@inproceedings{bai2023regret,
  title={Regret Analysis of Policy Gradient Algorithm for Infinite Horizon Average Reward Markov Decision Processes},
  author={Bai, Qinbo and Mondal, Washim Uddin and Aggarwal, Vaneet},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2024}
}

@article{mondal2024sample,
  title={Sample-Efficient Constrained Reinforcement Learning with General Parameterization},
  author={Mondal, Washim Uddin and Aggarwal, Vaneet},
  journal={arXiv preprint arXiv:2405.10624},
  year={2024}
}

@article{mondal2023mean,
  title={Mean-Field Control based Approximation of Multi-Agent Reinforcement Learning in Presence of a Non-decomposable Shared Global State},
  author={Mondal, Washim Uddin and Aggarwal, Vaneet and Ukkusuri, Satish V},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

@inproceedings{dorfman2022adapting,
  title={Adapting to mixing time in stochastic optimization with Markovian data},
  author={Dorfman, Ron and Levy, Kfir Yehuda},
  booktitle={International Conference on Machine Learning},
  pages={5429--5446},
  year={2022},
  organization={PMLR}
}
@inproceedings{liu2021cmix,
  title={Cmix: Deep multi-agent reinforcement learning with peak and average constraints},
  author={Liu, Chenyi and Geng, Nan and others},
  booktitle={Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13--17, 2021, Proceedings, Part I 21},
  pages={157--173},
  year={2021},
  organization={Springer}
}
@article{agarwal2022concave,
  title={Concave Utility Reinforcement Learning with Zero-Constraint Violations},
  author={Agarwal, Mridul and Bai, Qinbo and Aggarwal, Vaneet},
  journal={Transactions on Machine Learning Research},
  year={2022}
}
@article{osband2013more,
  title={(More) efficient reinforcement learning via posterior sampling},
  author={Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}
@article{agrawal2017optimistic,
  title={Optimistic posterior sampling for reinforcement learning: worst-case regret bounds},
  author={Agrawal, Shipra and Jia, Randy},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}
@article{liu2020improved,
  title={An improved analysis of (variance-reduced) policy gradient and natural policy gradient methods},
  author={Liu, Yanli and Zhang, Kaiqing and Basar, Tamer and Yin, Wotao},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7624--7636},
  year={2020}
}
@article{singh2020learning,
  title={Learning in Markov decision processes under constraints},
  author={Singh, Rahul and Gupta, Abhishek and Shroff, Ness B},
  journal={arXiv preprint arXiv:2002.12435},
  year={2020}
}
@inproceedings{agarwal2022regret,
  title={Regret guarantees for model-based reinforcement learning with long-term average constraints},
  author={Agarwal, Mridul and Bai, Qinbo and Aggarwal, Vaneet},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={22--31},
  year={2022},
  organization={PMLR}
}
@inproceedings{bai2023achieving,
  title={Achieving zero constraint violation for constrained reinforcement learning via conservative natural policy gradient primal-dual algorithm},
  author={Bai, Qinbo and Bedi, Amrit Singh and Aggarwal, Vaneet},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={6737--6744},
  year={2023}
}
@inproceedings{suttle2023beyond,
  title={Beyond Exponentially Fast Mixing in Average-Reward Reinforcement Learning via Multi-Level Monte Carlo Actor-Critic},
  author={Suttle, Wesley A and Bedi, Amrit and Patel, Bhrij and Sadler, Brian M and Koppel, Alec and Manocha, Dinesh},
  booktitle={International Conference on Machine Learning},
  pages={33240--33267},
  year={2023},
  organization={PMLR}
}


@article{germano2023best,
  title={A Best-of-Both-Worlds Algorithm for Constrained MDPs with Long-Term Constraints},
  author={Germano, Jacopo and Stradi, Francesco Emanuele and Genalti, Gianmarco and Castiglioni, Matteo and Marchesi, Alberto and Gatti, Nicola},
  journal={arXiv preprint arXiv:2304.14326},
  year={2023}
}

@article{monograph,
author = {Aggarwal, Vaneet and Mondal, Washim Uddin and Bai, Qinbo},
title = {Constrained Reinforcement Learning with Average Reward Objective: Model-Based and Model-Free Algorithms},
year = {2024},
issue_date = {Aug 2024},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {6},
number = {4},
issn = {2167-3888},
url = {https://doi.org/10.1561/2400000038},
doi = {10.1561/2400000038},
journal = {Found. Trends Optim.},
month = aug,
pages = {193â€“298},
numpages = {109}
}


@article{qiu2020upper,
  title={Upper confidence primal-dual reinforcement learning for CMDP with adversarial loss},
  author={Qiu, Shuang and Wei, Xiaohan and Yang, Zhuoran and Ye, Jieping and Wang, Zhaoran},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15277--15287},
  year={2020}
}
@article{efroni2020exploration,
  title={Exploration-exploitation in constrained mdps},
  author={Efroni, Yonathan and Mannor, Shie and Pirotta, Matteo},
  journal={arXiv preprint arXiv:2003.02189},
  year={2020}
}
@inproceedings{xu2021crpo,
  title={Crpo: A new approach for safe reinforcement learning with convergence guarantee},
  author={Xu, Tengyu and Liang, Yingbin and Lan, Guanghui},
  booktitle={International Conference on Machine Learning},
  pages={11480--11491},
  year={2021},
  organization={PMLR}
}
@article{ding2020natural,
  title={Natural policy gradient primal-dual method for constrained markov decision processes},
  author={Ding, Dongsheng and Zhang, Kaiqing and Basar, Tamer and Jovanovic, Mihailo},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={8378--8390},
  year={2020}
}
@inproceedings{bai2022achieving,
  title={Achieving zero constraint violation for constrained reinforcement learning via primal-dual approach},
  author={Bai, Qinbo and Bedi, Amrit Singh and Agarwal, Mridul and Koppel, Alec and Aggarwal, Vaneet},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={3682--3689},
  year={2022}
}
@inproceedings{wei2020model,
  title={Model-free reinforcement learning in infinite-horizon average-reward markov decision processes},
  author={Wei, Chen-Yu and Jahromi, Mehdi Jafarnia and Luo, Haipeng and Sharma, Hiteshi and Jain, Rahul},
  booktitle={International conference on machine learning},
  pages={10170--10180},
  year={2020},
  organization={PMLR}
}
@inproceedings{wei2022provably,
  title={A provably-efficient model-free algorithm for infinite-horizon average-reward constrained Markov decision processes},
  author={Wei, Honghao and Liu, Xin and Ying, Lei},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={3868--3876},
  year={2022}
}
@article{auer2008near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}
@inproceedings{wei2021learning,
  title={Learning infinite-horizon average-reward mdps with linear function approximation},
  author={Wei, Chen-Yu and Jahromi, Mehdi Jafarnia and Luo, Haipeng and Jain, Rahul},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3007--3015},
  year={2021},
  organization={PMLR}
}

@article{singh2022learning,
  title={Learning in Constrained Markov Decision Processes},
  author={Singh, Rahul and Gupta, Abhishek and Shroff, Ness B},
  journal={IEEE Transactions on Control of Network Systems},
  volume={10},
  number={1},
  pages={441--453},
  year={2022},
  publisher={IEEE}
}
@inproceedings{ghosh2022achieving,
  title={Achieving Sub-linear Regret in Infinite Horizon Average Reward Constrained MDP with Linear Function Approximation},
  author={Ghosh, Arnob and Zhou, Xingyu and Shroff, Ness},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}
@article{al2019deeppool,
  title={Deeppool: Distributed model-free algorithm for ride-sharing using deep reinforcement learning},
  author={Al-Abbasi, Abubakr O and Ghosh, Arnob and Aggarwal, Vaneet},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={20},
  number={12},
  pages={4714--4727},
  year={2019},
  publisher={IEEE}
}
@inproceedings{chen2022learning,
  title={Learning infinite-horizon average-reward Markov decision process with constraints},
  author={Chen, Liyu and Jain, Rahul and Luo, Haipeng},
  booktitle={International Conference on Machine Learning},
  pages={3246--3270},
  year={2022},
  organization={PMLR}
}

@inproceedings{pesquerel2022imed,
  title={IMED-RL: Regret optimal learning of ergodic Markov decision processes},
  author={Pesquerel, Fabien and Maillard, Odalric-Ambrym},
  booktitle={NeurIPS 2022-Thirty-sixth Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{gong2020duality,
  title={A Duality Approach for Regret Minimization in Average-Award Ergodic Markov Decision Processes},
  author={Gong, Hao and Wang, Mengdi},
  booktitle={Learning for Dynamics and Control},
  pages={862--883},
  year={2020},
  organization={PMLR}
}

@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}
@inproceedings{NEURIPS2018_d3b1fb02,
 author = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Is Q-Learning Provably Efficient?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/d3b1fb02964aa64e257f9f26a31f72cf-Paper.pdf},
 volume = {31},
 year = {2018}
}
@inproceedings{NEURIPS2020_5f7695de,
 author = {Ding, Dongsheng and Zhang, Kaiqing and Basar, Tamer and Jovanovic, Mihailo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {8378--8390},
 publisher = {Curran Associates, Inc.},
 title = {Natural Policy Gradient Primal-Dual Method for Constrained Markov Decision Processes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/5f7695debd8cde8db5abcb9f161b49ea-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{agarwal2021theory,
  title={On the theory of policy gradient methods: Optimality, approximation, and distribution shift},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={4431--4506},
  year={2021},
  publisher={JMLRORG}
}

@article{neu2014online,
  title={Online Markov Decision Processes Under Bandit Feedback},
  author={Neu, Gergely and Gyorgy, Andras and Szepesvari, Csaba and Antos, Andras},
  journal={IEEE Transactions on Automatic Control},
  volume={3},
  number={59},
  pages={676--691},
  year={2014}
}

@article{Mengdi2021,
  title={On the convergence and sample efficiency of variance-reduced policy gradient method},
  author={Zhang, Junyu and Ni, Chengzhuo and Szepesvari, Csaba and Wang, Mengdi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2228--2240},
  year={2021}
}


@InProceedings{Alekh2020, 
  title={Optimality and approximation with policy gradient methods in markov decision processes},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  booktitle={Conference on Learning Theory},
  pages={64--66},
  year={2020},
  organization={PMLR}
}

@inproceedings{Lingxiao2019,
  title={Neural Policy Gradient Methods: Global Optimality and Rates of Convergence},
  author={Wang, Lingxiao and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@InProceedings{Chi2019, 
	title = {Provably efficient reinforcement learning with linear function approximation}, author = {Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I}, 
	booktitle = {Proceedings of Thirty Third Conference on Learning Theory}, 
	pages = {2137--2143}, 
	year = {2020}, 
	editor = {Jacob Abernethy and Shivani Agarwal}, 
	volume = {125}, 
	series = {Proceedings of Machine Learning Research}, 
	address = {}, 
	month = {09--12 Jul}, 
	publisher = {PMLR}, 
}

@article{yang2020deep,
  title={Deep reinforcement learning-based intelligent reflecting surface for secure wireless communications},
  author={Yang, Helin and Xiong, Zehui and Zhao, Jun and Niyato, Dusit and Xiao, Liang and Wu, Qingqing},
  journal={IEEE Transactions on Wireless Communications},
  volume={20},
  number={1},
  pages={375--388},
  year={2020},
  publisher={IEEE}
}

@article{ling2023cooperating,
  title={Cooperating Graph Neural Networks with Deep Reinforcement Learning for Vaccine Prioritization},
  author={Lu Ling and Washim Uddin Mondal and Satish V. Ukkusuri},
 journal={arXiv preprint arXiv:2305.05163},
  year={2023}
}

@article{ouyang2017learning,
  title={Learning unknown markov decision processes: A thompson sampling approach},
  author={Ouyang, Yi and Gagrani, Mukul and Nayyar, Ashutosh and Jain, Rahul},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{mondal2023improved,
  title={Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes},
  author={Mondal, Washim Uddin and Aggarwal, Vaneet},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS) },
  year={2024}
}

@inproceedings{fruit2018efficient,
  title={Efficient bias-span-constrained exploration-exploitation in reinforcement learning},
  author={Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro and Ortner, Ronald},
  booktitle={International Conference on Machine Learning},
  pages={1578--1586},
  year={2018},
  organization={PMLR}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016},
  organization={PMLR}
}

@article{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}
@misc{Zeng2020,
    doi = {10.48550/ARXIV.2006.04338},
    url = {https://arxiv.org/abs/2006.04338},
    author = {Zeng, Sihan and Anwar, Aqeel and Doan, Thinh and Raychowdhury, Arijit and Romberg, Justin},
    keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {A Decentralized Policy Gradient Approach to Multi-task Reinforcement Learning},
    publisher = {arXiv},
    year = {2020},
    copyright = {arXiv.org perpetual, non-exclusive license}
}
@InProceedings{pmlr-v151-yuan22a,
  title = 	 { A general sample complexity analysis of vanilla policy gradient },
  author =       {Yuan, Rui and Gower, Robert M. and Lazaric, Alessandro},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3332--3380},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28--30 Mar},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v151/yuan22a/yuan22a.pdf},
  url = 	 {https://proceedings.mlr.press/v151/yuan22a.html},
  abstract = 	 { We adapt recent tools developed for the analysis of Stochastic Gradient Descent (SGD) in non-convex optimization to obtain convergence and sample complexity guarantees for the vanilla policy gradient (PG). Our only assumptions are that the expected return is smooth w.r.t. the policy parameters, that its $H$-step truncated gradient is close to the exact gradient, and a certain ABC assumption. This assumption requires the second moment of the estimated gradient to be bounded by $A \geq 0$ times the suboptimality gap, $B \geq 0$ times the norm of the full batch gradient and an additive constant $C \geq 0$, or any combination of aforementioned. We show that the ABC assumption is more general than the commonly used assumptions on the policy space to prove convergence to a stationary point. We provide a single convergence theorem that recovers the $\widetilde{\mathcal{O}}(\epsilon^{-4})$ sample complexity of PG. Our results also affords greater flexibility in the choice of hyper parameters such as the step size and places no restriction on the batch size $m$, including the single trajectory case (i.e., $m=1$). We then instantiate our theorem in different settings, where we both recover existing results and obtained improved sample complexity, e.g., for convergence to the global optimum for Fisher-non-degenerated parameterized policies. }
}
@inproceedings{ding2022global,
  title={On the global optimum convergence of momentum-based policy gradient},
  author={Ding, Yuhao and Zhang, Junzi and Lavaei, Javad},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1910--1934},
  year={2022},
  organization={PMLR}
}

@article{ding2023convergence,
  title={Convergence and sample complexity of natural policy gradient primal-dual methods for constrained MDPs},
  author={Ding, Dongsheng and Zhang, Kaiqing and Duan, Jiali and Ba{\c{s}}ar, Tamer and Jovanovi{\'c}, Mihailo R},
  journal={arXiv preprint arXiv:2206.02346},
  year={2022}
}

@article{bai2023provably,
  title={Provably Sample-Efficient Model-Free Algorithm for MDPs with Peak Constraints},
  author={Bai, Qinbo and Aggarwal, Vaneet and Gattami, Ather},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={60},
  pages={1--25},
  year={2023}
}
@inproceedings{fatkhullin2023stochastic,
  title={Stochastic policy gradient methods: Improved sample complexity for fisher-non-degenerate policies},
  author={Fatkhullin, Ilyas and Barakat, Anas and Kireeva, Anastasia and He, Niao},
  booktitle={International Conference on Machine Learning},
  pages={9827--9869},
  year={2023},
  organization={PMLR}
}
@inproceedings{yuan2022general,
  title={A general sample complexity analysis of vanilla policy gradient},
  author={Yuan, Rui and Gower, Robert M and Lazaric, Alessandro},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3332--3380},
  year={2022},
  organization={PMLR}
}