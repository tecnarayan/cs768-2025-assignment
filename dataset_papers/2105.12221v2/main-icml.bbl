\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Arora, S., Du, S.~S., Hu, W., Li, Z., and Wang, R.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock \emph{arXiv preprint arXiv:1901.08584}, 2019.

\bibitem[Auffinger et~al.(2013)Auffinger, Arous, and
  {\v{C}}ern{\`y}]{auffinger2013random}
Auffinger, A., Arous, G.~B., and {\v{C}}ern{\`y}, J.
\newblock Random matrices and complexity of spin glasses.
\newblock \emph{Communications on Pure and Applied Mathematics}, 66\penalty0
  (2):\penalty0 165--201, 2013.

\bibitem[Bray \& Dean(2007)Bray and Dean]{bray2007statistics}
Bray, A.~J. and Dean, D.~S.
\newblock Statistics of critical points of gaussian fields on large-dimensional
  spaces.
\newblock \emph{Physical review letters}, 98\penalty0 (15):\penalty0 150201,
  2007.

\bibitem[Brea et~al.(2019)Brea, Simsek, Illing, and Gerstner]{brea2019weight}
Brea, J., Simsek, B., Illing, B., and Gerstner, W.
\newblock Weight-space symmetry in deep networks gives rise to permutation
  saddles, connected by equal-loss valleys across the loss landscape.
\newblock \emph{arXiv preprint arXiv:1907.02911}, 2019.

\bibitem[Chizat \& Bach(2018)Chizat and Bach]{chizat2018global}
Chizat, L. and Bach, F.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock \emph{Advances in neural information processing systems},
  31:\penalty0 3036--3046, 2018.

\bibitem[Cooper(2020)]{cooper2020critical}
Cooper, Y.
\newblock The critical locus of overparameterized neural networks.
\newblock \emph{arXiv preprint arXiv:2005.04210}, 2020.

\bibitem[Dauphin et~al.(2014)Dauphin, Pascanu, Gulcehre, Cho, Ganguli, and
  Bengio]{dauphin2014identifying}
Dauphin, Y.~N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio, Y.
\newblock Identifying and attacking the saddle point problem in
  high-dimensional non-convex optimization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2933--2941, 2014.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and
  Hamprecht]{draxler2018essentially}
Draxler, F., Veschgini, K., Salmhofer, M., and Hamprecht, F.~A.
\newblock Essentially no barriers in neural network energy landscape.
\newblock \emph{arXiv preprint arXiv:1803.00885}, 2018.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1675--1685. PMLR, 2019.

\bibitem[Du et~al.(2017)Du, Jin, Lee, Jordan, Singh, and
  Poczos]{du2017gradient}
Du, S.~S., Jin, C., Lee, J.~D., Jordan, M.~I., Singh, A., and Poczos, B.
\newblock Gradient descent can take exponential time to escape saddle points.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1067--1077, 2017.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018.

\bibitem[Fort \& Jastrzebski(2019)Fort and Jastrzebski]{fort2019large}
Fort, S. and Jastrzebski, S.
\newblock Large scale structure of neural network loss landscapes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6709--6717, 2019.

\bibitem[Fort et~al.(2020)Fort, Dziugaite, Paul, Kharaghani, Roy, and
  Ganguli]{fort2020deep}
Fort, S., Dziugaite, G.~K., Paul, M., Kharaghani, S., Roy, D.~M., and Ganguli,
  S.
\newblock Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the neural tangent kernel.
\newblock \emph{arXiv preprint arXiv:2010.15110}, 2020.

\bibitem[Frankle \& Carbin(2018)Frankle and Carbin]{frankle2018lottery}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock \emph{arXiv preprint arXiv:1803.03635}, 2018.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020linear}
Frankle, J., Dziugaite, G.~K., Roy, D., and Carbin, M.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3259--3269. PMLR, 2020.

\bibitem[Freeman \& Bruna(2016)Freeman and Bruna]{freeman2016topology}
Freeman, C.~D. and Bruna, J.
\newblock Topology and geometry of half-rectified network optimization.
\newblock \emph{arXiv preprint arXiv:1611.01540}, 2016.

\bibitem[Fukumizu \& Amari(2000)Fukumizu and Amari]{fukumizu2000local}
Fukumizu, K. and Amari, S.-i.
\newblock Local minima and plateaus in hierarchical structures of multilayer
  perceptrons.
\newblock \emph{Neural networks}, 13\penalty0 (3):\penalty0 317--327, 2000.

\bibitem[Fukumizu et~al.(2019)Fukumizu, Yamaguchi, Mototake, and
  Tanaka]{fukumizu2019semi}
Fukumizu, K., Yamaguchi, S., Mototake, Y.-i., and Tanaka, M.
\newblock Semi-flat minima and saddle points by embedding neural networks to
  overparameterization.
\newblock \emph{arXiv preprint arXiv:1906.04868}, 2019.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{garipov2018loss}
Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D.~P., and Wilson, A.~G.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock \emph{Advances in Neural Information Processing Systems},
  31:\penalty0 8789--8798, 2018.

\bibitem[Geiger et~al.(2019)Geiger, Spigler, d'Ascoli, Sagun, Baity-Jesi,
  Biroli, and Wyart]{geiger2019jamming}
Geiger, M., Spigler, S., d'Ascoli, S., Sagun, L., Baity-Jesi, M., Biroli, G.,
  and Wyart, M.
\newblock Jamming transition as a paradigm to understand the loss landscape of
  deep neural networks.
\newblock \emph{Physical Review E}, 100\penalty0 (1):\penalty0 012115, 2019.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{Glorot10}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In Teh, Y.~W. and Titterington, M. (eds.), \emph{Proceedings of the
  Thirteenth International Conference on Artificial Intelligence and
  Statistics}, volume~9 of \emph{Proceedings of Machine Learning Research},
  pp.\  249--256, Chia Laguna Resort, Sardinia, Italy, 13--15 May 2010. JMLR
  Workshop and Conference Proceedings.
\newblock URL \url{http://proceedings.mlr.press/v9/glorot10a.html}.

\bibitem[G{\l}uch \& Urbanke(2021)G{\l}uch and Urbanke]{gluch2021noether}
G{\l}uch, G. and Urbanke, R.
\newblock Noether: The more things change, the more stay the same.
\newblock \emph{arXiv preprint arXiv:2104.05508}, 2021.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  8571--8580, 2018.

\bibitem[Jin et~al.(2017)Jin, Ge, Netrapalli, Kakade, and
  Jordan]{jin2017escape}
Jin, C., Ge, R., Netrapalli, P., Kakade, S.~M., and Jordan, M.~I.
\newblock How to escape saddle points efficiently.
\newblock \emph{arXiv preprint arXiv:1703.00887}, 2017.

\bibitem[Johnson()]{Johnson}
Johnson, S.~G.
\newblock The nlopt nonlinear-optimization package.
\newblock URL \url{http://github.com/stevengj/nlopt}.

\bibitem[{Kingma} \& {Ba}(2014){Kingma} and {Ba}]{Kingma14}
{Kingma}, D.~P. and {Ba}, J.
\newblock {Adam: A Method for Stochastic Optimization}.
\newblock \emph{ArXiv e-prints}, December 2014.
\newblock URL \url{https://arxiv.org/abs/1412.6980}.

\bibitem[Kuditipudi et~al.(2019)Kuditipudi, Wang, Lee, Zhang, Li, Hu, Ge, and
  Arora]{kuditipudi2019explaining}
Kuditipudi, R., Wang, X., Lee, H., Zhang, Y., Li, Z., Hu, W., Ge, R., and
  Arora, S.
\newblock Explaining landscape connectivity of low-cost solutions for
  multilayer nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  14601--14610, 2019.

\bibitem[Kung et~al.(2009)Kung, Rota, and Yan]{kung2009combinatorics}
Kung, J.~P., Rota, G.-C., and Yan, C.~H.
\newblock \emph{Combinatorics: the Rota way}.
\newblock Cambridge University Press, 2009.

\bibitem[Kunin et~al.(2020)Kunin, Sagastuy-Brena, Ganguli, Yamins, and
  Tanaka]{kunin2020neural}
Kunin, D., Sagastuy-Brena, J., Ganguli, S., Yamins, D.~L., and Tanaka, H.
\newblock Neural mechanics: Symmetry and broken conservation laws in deep
  learning dynamics.
\newblock \emph{arXiv preprint arXiv:2012.04728}, 2020.

\bibitem[Lee et~al.(2019{\natexlab{a}})Lee, Xiao, Schoenholz, Bahri, Novak,
  Sohl-Dickstein, and Pennington]{lee2019wide}
Lee, J., Xiao, L., Schoenholz, S.~S., Bahri, Y., Novak, R., Sohl-Dickstein, J.,
  and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{arXiv preprint arXiv:1902.06720}, 2019{\natexlab{a}}.

\bibitem[Lee et~al.(2020)Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and
  Sohl-Dickstein]{lee2020finite}
Lee, J., Schoenholz, S.~S., Pennington, J., Adlam, B., Xiao, L., Novak, R., and
  Sohl-Dickstein, J.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock \emph{arXiv preprint arXiv:2007.15801}, 2020.

\bibitem[Lee et~al.(2019{\natexlab{b}})Lee, Panageas, Piliouras, Simchowitz,
  Jordan, and Recht]{lee2019first}
Lee, J.~D., Panageas, I., Piliouras, G., Simchowitz, M., Jordan, M.~I., and
  Recht, B.
\newblock First-order methods almost always avoid strict saddle points.
\newblock \emph{Mathematical programming}, 176\penalty0 (1):\penalty0 311--337,
  2019{\natexlab{b}}.

\bibitem[Lengyel et~al.(2020)Lengyel, Petangoda, Falk, Highnam, Lazarou,
  Kolbeinsson, Deisenroth, and Jennings]{lengyel2020genni}
Lengyel, D., Petangoda, J., Falk, I., Highnam, K., Lazarou, M., Kolbeinsson,
  A., Deisenroth, M.~P., and Jennings, N.~R.
\newblock Genni: Visualising the geometry of equivalences for neural network
  identifiability.
\newblock \emph{arXiv preprint arXiv:2011.07407}, 2020.

\bibitem[Milne-Thomson(2000)]{milne2000calculus}
Milne-Thomson, L.~M.
\newblock \emph{The calculus of finite differences}.
\newblock American Mathematical Soc., 2000.

\bibitem[Nguyen(2019)]{nguyen2019connected}
Nguyen, Q.
\newblock On connected sublevel sets in deep learning.
\newblock \emph{arXiv preprint arXiv:1901.07417}, 2019.

\bibitem[Sagan(2013)]{sagan2013symmetric}
Sagan, B.~E.
\newblock \emph{The symmetric group: representations, combinatorial algorithms,
  and symmetric functions}, volume 203.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Sagun et~al.(2014)Sagun, Guney, Arous, and
  LeCun]{sagun2014explorations}
Sagun, L., Guney, V.~U., Arous, G.~B., and LeCun, Y.
\newblock Explorations on high dimensional landscapes.
\newblock \emph{arXiv preprint arXiv:1412.6615}, 2014.

\bibitem[Sagun et~al.(2017)Sagun, Evci, Guney, Dauphin, and
  Bottou]{sagun2017empirical}
Sagun, L., Evci, U., Guney, V.~U., Dauphin, Y., and Bottou, L.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1706.04454}, 2017.

\end{thebibliography}
