\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bjorck et~al.(2018)Bjorck, Gomes, Selman, and
  Weinberger]{bjorck2018understanding}
Bjorck, N., Gomes, C.~P., Selman, B., and Weinberger, K.~Q.
\newblock Understanding batch normalization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  7694--7705, 2018.

\bibitem[Bottou(2010)]{bottou2010large}
Bottou, L.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In \emph{Proceedings of COMPSTAT'2010}, pp.\  177--186. Springer,
  2010.

\bibitem[Caruana et~al.(2001)Caruana, Lawrence, and
  Giles]{caruana2001overfitting}
Caruana, R., Lawrence, S., and Giles, C.~L.
\newblock Overfitting in neural nets: Backpropagation, conjugate gradient, and
  early stopping.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  402--408, 2001.

\bibitem[Chaudhari \& Soatto(2018)Chaudhari and
  Soatto]{chaudhari2018stochastic}
Chaudhari, P. and Soatto, S.
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock In \emph{2018 Information Theory and Applications Workshop (ITA)},
  pp.\  1--10. IEEE, 2018.

\bibitem[De \& Smith(2020)De and Smith]{de2020batch}
De, S. and Smith, S.~L.
\newblock Batch normalization biases residual blocks towards the identity
  function in deep networks.
\newblock \emph{arXiv preprint arXiv:2002.10444}, 2020.

\bibitem[De et~al.(2017)De, Yadav, Jacobs, and Goldstein]{de2017automated}
De, S., Yadav, A., Jacobs, D., and Goldstein, T.
\newblock Automated inference with adaptive batches.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1504--1513,
  2017.

\bibitem[Gardiner et~al.(1985)]{gardiner1985handbook}
Gardiner, C.~W. et~al.
\newblock \emph{Handbook of stochastic methods}, volume~3.
\newblock springer Berlin, 1985.

\bibitem[Ge et~al.(2019)Ge, Kakade, Kidambi, and Netrapalli]{ge2019step}
Ge, R., Kakade, S.~M., Kidambi, R., and Netrapalli, P.
\newblock The step decay schedule: A near optimal, geometrically decaying
  learning rate procedure.
\newblock \emph{arXiv preprint arXiv:1904.12838}, 2019.

\bibitem[Goh(2017)]{goh2017momentum}
Goh, G.
\newblock Why momentum really works.
\newblock \emph{Distill}, 2\penalty0 (4):\penalty0 e6, 2017.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Goyal, P., Doll{\'a}r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola,
  A., Tulloch, A., Jia, Y., and He, K.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Heskes \& Kappen(1993)Heskes and Kappen]{heskes1993line}
Heskes, T.~M. and Kappen, B.
\newblock On-line learning processes in artificial neural networks.
\newblock In \emph{North-Holland Mathematical Library}, volume~51, pp.\
  199--233. Elsevier, 1993.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{hoffer2017train}
Hoffer, E., Hubara, I., and Soudry, D.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1731--1741, 2017.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{arXiv preprint arXiv:1502.03167}, 2015.

\bibitem[Jastrz{\k{e}}bski et~al.(2017)Jastrz{\k{e}}bski, Kenton, Arpit,
  Ballas, Fischer, Bengio, and Storkey]{jastrzkebski2017three}
Jastrz{\k{e}}bski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio,
  Y., and Storkey, A.
\newblock Three factors influencing minima in sgd.
\newblock \emph{arXiv preprint arXiv:1711.04623}, 2017.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kidambi et~al.(2018)Kidambi, Netrapalli, Jain, and
  Kakade]{kidambi2018insufficiency}
Kidambi, R., Netrapalli, P., Jain, P., and Kakade, S.
\newblock On the insufficiency of existing momentum schemes for stochastic
  optimization.
\newblock In \emph{2018 Information Theory and Applications Workshop (ITA)},
  pp.\  1--9. IEEE, 2018.

\bibitem[Krizhevsky(2014)]{krizhevsky2014one}
Krizhevsky, A.
\newblock One weird trick for parallelizing convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:1404.5997}, 2014.

\bibitem[Le~Roux et~al.(2012)Le~Roux, Schmidt, and Bach]{roux2012stochastic}
Le~Roux, N., Schmidt, M., and Bach, F.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2663--2671, 2012.

\bibitem[LeCun et~al.(2012)LeCun, Bottou, Orr, and
  M{\"u}ller]{lecun2012efficient}
LeCun, Y.~A., Bottou, L., Orr, G.~B., and M{\"u}ller, K.-R.
\newblock Efficient backprop.
\newblock In \emph{Neural networks: Tricks of the trade}, pp.\  9--48.
  Springer, 2012.

\bibitem[Li et~al.(2017)Li, Tai, et~al.]{li2017stochastic}
Li, Q., Tai, C., et~al.
\newblock Stochastic modified equations and adaptive stochastic gradient
  algorithms.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2101--2110. JMLR. org, 2017.

\bibitem[Li et~al.(2019)Li, Wei, and Ma]{li2019towards}
Li, Y., Wei, C., and Ma, T.
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  11669--11680, 2019.

\bibitem[Liu \& Belkin(2018{\natexlab{a}})Liu and Belkin]{liu2018accelerating}
Liu, C. and Belkin, M.
\newblock Accelerating sgd with momentum for over-parameterized learning.
\newblock \emph{arXiv preprint arXiv:1810.13395}, 2018{\natexlab{a}}.

\bibitem[Liu \& Belkin(2018{\natexlab{b}})Liu and Belkin]{liu2018mass}
Liu, C. and Belkin, M.
\newblock Mass: an accelerated stochastic method for over-parametrized
  learning.
\newblock \emph{arXiv preprint arXiv:1810.13395}, 2018{\natexlab{b}}.

\bibitem[Ma et~al.(2017)Ma, Bassily, and Belkin]{ma2017power}
Ma, S., Bassily, R., and Belkin, M.
\newblock The power of interpolation: Understanding the effectiveness of sgd in
  modern over-parametrized learning.
\newblock \emph{arXiv preprint arXiv:1712.06559}, 2017.

\bibitem[Mandt et~al.(2017)Mandt, Hoffman, and Blei]{mandt2017stochastic}
Mandt, S., Hoffman, M.~D., and Blei, D.~M.
\newblock Stochastic gradient descent as approximate bayesian inference.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 4873--4907, 2017.

\bibitem[McCandlish et~al.(2018)McCandlish, Kaplan, Amodei, and
  Team]{mccandlish2018empirical}
McCandlish, S., Kaplan, J., Amodei, D., and Team, O.~D.
\newblock An empirical model of large-batch training.
\newblock \emph{arXiv preprint arXiv:1812.06162}, 2018.

\bibitem[Nesterov(2013)]{nesterov2013introductory}
Nesterov, Y.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Orr \& Leen(1994)Orr and Leen]{orr1994momentum}
Orr, G.~B. and Leen, T.~K.
\newblock Momentum and optimal stochastic search.
\newblock In \emph{Proceedings of the 1993 Connectionist Models Summer School},
  pp.\  351--357. Psychology Press, 1994.

\bibitem[Park et~al.(2019)Park, Sohl-Dickstein, Le, and Smith]{park2019effect}
Park, D.~S., Sohl-Dickstein, J., Le, Q.~V., and Smith, S.~L.
\newblock The effect of network width on stochastic gradient descent and
  generalization: an empirical study.
\newblock \emph{arXiv preprint arXiv:1905.03776}, 2019.

\bibitem[Polyak(1964)]{polyak1964some}
Polyak, B.~T.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Prechelt(1998)]{prechelt1998early}
Prechelt, L.
\newblock Early stopping-but when?
\newblock In \emph{Neural Networks: Tricks of the trade}, pp.\  55--69.
  Springer, 1998.

\bibitem[Qian(1999)]{qian1999momentum}
Qian, N.
\newblock On the momentum term in gradient descent learning algorithms.
\newblock \emph{Neural networks}, 12\penalty0 (1):\penalty0 145--151, 1999.

\bibitem[Sankararaman et~al.(2019)Sankararaman, De, Xu, Huang, and
  Goldstein]{sankararaman2019impact}
Sankararaman, K.~A., De, S., Xu, Z., Huang, W.~R., and Goldstein, T.
\newblock The impact of neural network overparameterization on gradient
  confusion and stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:1904.06963}, 2019.

\bibitem[Santurkar et~al.(2018)Santurkar, Tsipras, Ilyas, and
  Madry]{santurkar2018does}
Santurkar, S., Tsipras, D., Ilyas, A., and Madry, A.
\newblock How does batch normalization help optimization?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2483--2493, 2018.

\bibitem[Shallue et~al.(2018)Shallue, Lee, Antognini, Sohl-Dickstein, Frostig,
  and Dahl]{shallue2018measuring}
Shallue, C.~J., Lee, J., Antognini, J., Sohl-Dickstein, J., Frostig, R., and
  Dahl, G.~E.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock \emph{arXiv preprint arXiv:1811.03600}, 2018.

\bibitem[Simsekli et~al.(2019)Simsekli, Sagun, and
  Gurbuzbalaban]{simsekli2019tail}
Simsekli, U., Sagun, L., and Gurbuzbalaban, M.
\newblock A tail-index analysis of stochastic gradient noise in deep neural
  networks.
\newblock \emph{arXiv preprint arXiv:1901.06053}, 2019.

\bibitem[Smith \& Le(2017)Smith and Le]{smith2017bayesian}
Smith, S.~L. and Le, Q.~V.
\newblock A bayesian perspective on generalization and stochastic gradient
  descent.
\newblock \emph{arXiv preprint arXiv:1710.06451}, 2017.

\bibitem[Smith et~al.(2017)Smith, Kindermans, Ying, and Le]{smith2017don}
Smith, S.~L., Kindermans, P.-J., Ying, C., and Le, Q.~V.
\newblock Don't decay the learning rate, increase the batch size.
\newblock \emph{arXiv preprint arXiv:1711.00489}, 2017.

\bibitem[Smith et~al.(2019)Smith, Elsen, and De]{smith2019momentum}
Smith, S.~L., Elsen, E., and De, S.
\newblock Momentum enables large batch training.
\newblock \emph{ICML workshop on Theoretical Physics in Deep Learning}, 2019.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013importance}
Sutskever, I., Martens, J., Dahl, G., and Hinton, G.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1139--1147, 2013.

\bibitem[Welling \& Teh(2011)Welling and Teh]{welling2011bayesian}
Welling, M. and Teh, Y.~W.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pp.\  681--688, 2011.

\bibitem[Yuan et~al.(2016)Yuan, Ying, and Sayed]{yuan2016influence}
Yuan, K., Ying, B., and Sayed, A.~H.
\newblock On the influence of momentum acceleration on online learning.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 6602--6667, 2016.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zaremba et~al.(2014)Zaremba, Sutskever, and
  Vinyals]{zaremba2014recurrent}
Zaremba, W., Sutskever, I., and Vinyals, O.
\newblock Recurrent neural network regularization.
\newblock \emph{arXiv preprint arXiv:1409.2329}, 2014.

\bibitem[Zhang et~al.(2019)Zhang, Li, Nado, Martens, Sachdeva, Dahl, Shallue,
  and Grosse]{zhang2019algorithmic}
Zhang, G., Li, L., Nado, Z., Martens, J., Sachdeva, S., Dahl, G.~E., Shallue,
  C.~J., and Grosse, R.
\newblock Which algorithmic choices matter at which batch sizes? insights from
  a noisy quadratic model.
\newblock \emph{arXiv preprint arXiv:1907.04164}, 2019.

\end{thebibliography}
