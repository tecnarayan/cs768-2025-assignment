@article{campbell1959,
  title={Convergent and discriminant validation by the multitrait-multimethod matrix},
  author={Campbell, Donald T. and Fiske, Donald W.},
  journal={Psychological Bulletin},
  volume={56},
  number={2},
  pages={81},
  year={1959},
  publisher={American Psychological Association},
  url={https://pubmed.ncbi.nlm.nih.gov/13634291/}
}

@article{messick1987,
  title={Validity},
  author={Messick, Samuel},
  journal={ETS Research Report Series},
  volume={1987},
  number={2},
  pages={i--208},
  year={1987},
  publisher={Wiley Online Library},
  url={https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2330-8516.1987.tb00244.x}
}

@inproceedings{chang2008,
author = {Chang, Ming-Wei and Ratinov, Lev and Roth, Dan and Srikumar, Vivek},
title = {Importance of Semantic Representation: Dataless Classification},
year = {2008},
isbn = {9781577353683},
publisher = {AAAI Press},
abstract = {Traditionally, text categorization has been studied as the problem of training of a classifier using labeled data. However, people can categorize documents into named categories without any explicit training because we know the meaning of category names. In this paper, we introduce Dataless Classification, a learning protocol that uses world knowledge to induce classifiers without the need for any labeled data. Like humans, a dataless classifier interprets a string of words as a set of semantic concepts. We propose a model for dataless classification and show that the label name alone is often sufficient to induce classifiers. Using Wikipedia as our source of world knowledge, we get 85.29% accuracy on tasks from the 20 Newsgroup dataset and 88.62% accuracy on tasks from a Yahoo! Answers dataset without any labeled or unlabeled data from the data sets. With unlabeled data, we can further improve the results and show quite competitive performance to a supervised learning algorithm that uses 100 labeled examples.},
booktitle = {Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 2},
pages = {830–835},
numpages = {6},
location = {Chicago, Illinois},
series = {AAAI'08}
}

@InProceedings{maas2011,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}

@article{loevinger1957,
author = {Jane Loevinger},
title ={Objective Tests as Instruments of Psychological Theory},
journal = {Psychological Reports},
volume = {3},
number = {3},
pages = {635-694},
year = {1957},
doi = {10.2466/pr0.1957.3.3.635},
URL = {https://doi.org/10.2466/pr0.1957.3.3.635},
eprint = {https://doi.org/10.2466/pr0.1957.3.3.635}
}

@article{pedregosa2011,
  author  = {Fabian Pedregosa and Ga{{\"e}}l Varoquaux and Alexandre Gramfort and Vincent Michel and Bertrand Thirion and Olivier Grisel and Mathieu Blondel and Peter Prettenhofer and Ron Weiss and Vincent Dubourg and Jake Vanderplas and Alexandre Passos and David Cournapeau and Matthieu Brucher and Matthieu Perrot and {{\'E}}douard Duchesnay},
  title   = {Scikit-learn: Machine Learning in Python},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {85},
  pages   = {2825-2830},
  url     = {http://jmlr.org/papers/v12/pedregosa11a.html}
}

@article{viljoen2021,
  title={A Relational Theory of Data Governance.},
  author={Viljoen, Salom{\'e}},
  journal={Yale Law Journal},
  volume={131},
  number={2},
  year={2021}
}

@article{birhane2021algorithmic,
title = {Algorithmic injustice: a relational ethics approach},
journal = {Patterns},
volume = {2},
number = {2},
pages = {100205},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100205},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921000155},
author = {Abeba Birhane},
keywords = {justice, ethics, Afro-feminism, relational epistemology, data science, complex systems, enaction, embodiment, artificial intelligence, machine learning},
abstract = {Summary
It has become trivial to point out that algorithmic systems increasingly pervade the social sphere. Improved efficiency—the hallmark of these systems—drives their mass integration into day-to-day life. However, as a robust body of research in the area of algorithmic injustice shows, algorithmic systems, especially when used to sort and predict social outcomes, are not only inadequate but also perpetuate harm. In particular, a persistent and recurrent trend within the literature indicates that society's most vulnerable are disproportionally impacted. When algorithmic injustice and harm are brought to the fore, most of the solutions on offer (1) revolve around technical solutions and (2) do not center disproportionally impacted communities. This paper proposes a fundamental shift—from rational to relational—in thinking about personhood, data, justice, and everything in between, and places ethics as something that goes above and beyond technical solutions. Outlining the idea of ethics built on the foundations of relationality, this paper calls for a rethinking of justice and ethics as a set of broad, contingent, and fluid concepts and down-to-earth practices that are best viewed as a habit and not a mere methodology for data science. As such, this paper mainly offers critical examinations and reflection and not “solutions.”}
}

@inproceedings{kasy2021,
author = {Kasy, Maximilian and Abebe, Rediet},
title = {Fairness, Equality, and Power in Algorithmic Decision-Making},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445919},
doi = {10.1145/3442188.3445919},
abstract = {Much of the debate on the impact of algorithms is concerned with fairness, defined as the absence of discrimination for individuals with the same "merit." Drawing on the theory of justice, we argue that leading notions of fairness suffer from three key limitations: they legitimize inequalities justified by "merit;" they are narrowly bracketed, considering only differences of treatment within the algorithm; and they consider between-group and not within-group differences. We contrast this fairness-based perspective with two alternate perspectives: the first focuses on inequality and the causal impact of algorithms and the second on the distribution of power. We formalize these perspectives drawing on techniques from causal inference and empirical economics, and characterize when they give divergent evaluations. We present theoretical results and empirical examples which demonstrate this tension. We further use these insights to present a guide for algorithmic auditing and discuss the importance of inequality- and power-centered frameworks in algorithmic decision-making.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {576–586},
numpages = {11},
keywords = {power, Algorithmic fairness, inequality, empirical economics, auditing},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@article{henzinger2022,
  title={Leximax Approximations and Representative Cohort Selection},
  author={Monika Henzinger and Charlotte Peale and Omer Reingold and Judy Hanwen Shen},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.01157}
}

@inproceedings{fish2022,
author = {Fish, Benjamin and Stark, Luke},
title = {It’s Not Fairness, and It’s Not Fair: The Failure of Distributional Equality and the Promise of Relational Equality in Complete-Information Hiring Games},
year = {2022},
isbn = {9781450394772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551624.3555296},
doi = {10.1145/3551624.3555296},
abstract = {Existing efforts to formulate computational definitions of fairness have largely focused on distributional notions of equality, defined through how resources or decisions are divided. Yet existing discrimination is often the result of unequal social relations, rather than simply an unequal distribution of resources. We show how optimizing for existing computational definitions of fairness fails to prevent unequal social relations by providing an example of a self-confirming equilibrium in a simple hiring market that is relationally unequal but satisfies existing distributional notions of fairness. We introduce a notion of blatant relational unfairness for complete-information games, and discuss how this definition helps initiate a new approach to incorporating relational equality into computational systems.},
booktitle = {Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {11},
numpages = {15},
location = {Arlington, VA, USA},
series = {EAAMO '22}
}

@inproceedings{
kumar2022,
title={Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution},
author={Ananya Kumar and Aditi Raghunathan and Robbie Matthew Jones and Tengyu Ma and Percy Liang},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=UYneFzXSJWh}
}

@article{dubins1961,
 ISSN = {00029890, 19300972},
 URL = {http://www.jstor.org/stable/2311357},
 author = {L. E. Dubins and E. H. Spanier},
 journal = {The American Mathematical Monthly},
 number = {1},
 pages = {1--17},
 publisher = {Mathematical Association of America},
 title = {How to Cut A Cake Fairly},
 urldate = {2022-05-26},
 volume = {68},
 year = {1961}
}



@inproceedings{dwork2012,
author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
title = {Fairness through Awareness},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090255},
doi = {10.1145/2090236.2090255},
abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {214–226},
numpages = {13},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@article{CitronPasquale2014,
title = {The Scored Society: Due Process for Automated Predictions},
author={Danielle Citron and Frank Pasquale},
journal = {Wash. L. Rev.},
issue = {89},
page = {1}, 
year = {2014}
}

@inproceedings{zhang2015,
 author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Character-level Convolutional Networks for Text Classification},
 url = {https://proceedings.neurips.cc/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},
 volume = {28},
 year = {2015}
}



@inproceedings{hardt2016,
  title={Equality of Opportunity in Supervised Learning},
  author={Moritz Hardt and Eric Price and Nathan Srebro},
  booktitle={NIPS},
  year={2016}
}

@article{corbettdavies2018,
  title={The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning},
  author={Sam Corbett-Davies and Sharad Goel},
  journal={ArXiv},
  year={2018},
  volume={abs/1808.00023}
}

@article{tzioumis2018,
  title={Demographic aspects of first names},
  author={Konstantinos Tzioumis},
  journal={Scientific Data},
  year={2018},
  volume={5}
}

@inproceedings{gibert2018,
    title = "{Hate Speech Dataset from a White Supremacy Forum}",
    author = "de Gibert, Ona  and
      Perez, Naiara  and
      Garc{\'\i}a-Pablos, Aitor  and
      Cuadros, Montse",
    booktitle = "Proceedings of the 2nd Workshop on Abusive Language Online ({ALW}2)",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5102",
    doi = "10.18653/v1/W18-5102",
    pages = "11--20",
}

@inproceedings{hashimoto2018,
  title={Fairness Without Demographics in Repeated Loss Minimization},
  author={Tatsunori B. Hashimoto and Megha Srivastava and Hongseok Namkoong and Percy Liang},
  booktitle={ICML},
  year={2018}
}

@InProceedings{Kearns2018,
  title = 	 {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
  author =       {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2564--2572},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kearns18a/kearns18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/kearns18a.html},
  abstract = 	 {The most prevalent notions of fairness in machine learning fix a small collection of pre-defined groups (such as race or gender), and then ask for approximate parity of some statistic of the classifier (such as false positive rate) across these groups. Constraints of this form are susceptible to fairness gerrymandering, in which a classifier is fair on each individual group, but badly violates the fairness constraint on structured subgroups, such as certain combinations of protected attribute values. We thus consider fairness across exponentially or infinitely many subgroups, defined by a structured class of functions over the protected attributes. We first prove that the problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is computationally equivalent to the problem of weak agnostic learning — which means it is hard in the worst case, even for simple structured subclasses. However, it also suggests that common heuristics for learning can be applied to successfully solve the auditing problem in practice. We then derive an algorithm that provably converges in a polynomial number of steps to the best subgroup-fair distribution over classifiers, given access to an oracle which can solve the agnostic learning problem. The algorithm is based on a formulation of subgroup fairness as a zero-sum game between a Learner (the primal player) and an Auditor (the dual player). We implement a variant of this algorithm using heuristic oracles, and show that we can effectively both audit and learn fair classifiers on a real dataset.}
}

@article{khani2019,
  title={Maximum Weighted Loss Discrepancy},
  author={Fereshte Khani and Aditi Raghunathan and Percy Liang},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.03518}
}

@inproceedings{raji2019,
author = {Raji, Inioluwa Deborah and Buolamwini, Joy},
title = {Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314244},
doi = {10.1145/3306618.3314244},
abstract = {Although algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, we struggle to understand the real-world impact of these audits, as scholarship on the impact of algorithmic audits on increasing algorithmic fairness and transparency in commercial systems is nascent. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of Gender Shades, the first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. This paper 1) outlines the audit design and structured disclosure procedure used in the Gender Shades study, 2) presents new performance metrics from targeted companies IBM, Microsoft and Megvii (Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018, 3) provides performance results on PPB by non-target companies Amazon and Kairos and, 4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within 7 months of the original audit, we find that all three targets released new API versions. All targets reduced accuracy disparities between males and females and darker and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup, that underwent a 17.7% - 30.4% reduction in error between audit periods. Minimizing these disparities led to a 5.72% to 8.3% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66% and 6.60% overall, and error rates of 31.37% and 22.50% for the darker female subgroup, respectively.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {429–435},
numpages = {7},
keywords = {ethics, artificial intelligence, machine learning, fairness, computer vision, facial recognition, commercial applications},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@article{bandi2019,
  title={From Detection of Individual Metastases to Classification of Lymph Node Status at the Patient Level: The CAMELYON17 Challenge},
  author={P{\'e}ter B{\'a}ndi and Oscar G. F. Geessink and Quirine F Manson and Marcory Crf van Dijk and Maschenka C. A. Balkenhol and Meyke Hermsen and Babak Ehteshami Bejnordi and Byungjae Lee and Kyunghyun Paeng and Aoxiao Zhong and Quanzheng Li and Farhad Ghazvinian Zanjani and Svitlana Zinger and Keisuke Fukuta and Daisuke Komura and Vlado Ovtcharov and Shenghua Cheng and Shaoqun Zeng and Jeppe Thagaard and Anders Bjorholm Dahl and Huangjing Lin and Hao Chen and Ludwig Jacobsson and Martin Hedlund and Melih Çetin and Eren Halıcı and Hunter Jackson and Richard Chen and Fabian Both and J{\"o}rg K.H. Franke and Heidi V. N. K{\"u}sters-Vandevelde and W. Vreuls and Peter Bult and Bram van Ginneken and Jeroen A. van der Laak and Geert J. S. Litjens},
  journal={IEEE Transactions on Medical Imaging},
  year={2019},
  volume={38},
  pages={550-560}
}

@inproceedings{dodge2019,
    title = "Show Your Work: Improved Reporting of Experimental Results",
    author = "Dodge, Jesse  and
      Gururangan, Suchin  and
      Card, Dallas  and
      Schwartz, Roy  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1224",
    doi = "10.18653/v1/D19-1224",
    pages = "2185--2194",
    abstract = "Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparisons, and provide code to allow researchers to use our technique.",
}

@inproceedings{romanov2019,
    title = "What{'}s in a Name? {R}educing Bias in Bios without Access to Protected Attributes",
    author = "Romanov, Alexey  and
      De-Arteaga, Maria  and
      Wallach, Hanna  and
      Chayes, Jennifer  and
      Borgs, Christian  and
      Chouldechova, Alexandra  and
      Geyik, Sahin  and
      Kenthapadi, Krishnaram  and
      Rumshisky, Anna  and
      Kalai, Adam",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1424",
    doi = "10.18653/v1/N19-1424",
    pages = "4187--4195",
    abstract = "There is a growing body of work that proposes methods for mitigating bias in machine learning systems. These methods typically rely on access to protected attributes such as race, gender, or age. However, this raises two significant challenges: (1) protected attributes may not be available or it may not be legal to use them, and (2) it is often desirable to simultaneously consider multiple protected attributes, as well as their intersections. In the context of mitigating bias in occupation classification, we propose a method for discouraging correlation between the predicted probability of an individual{'}s true occupation and a word embedding of their name. This method leverages the societal biases that are encoded in word embeddings, eliminating the need for access to protected attributes. Crucially, it only requires access to individuals{'} names at training time and not at deployment time. We evaluate two variations of our proposed method using a large-scale dataset of online biographies. We find that both variations simultaneously reduce race and gender biases, with almost no reduction in the classifier{'}s overall true positive rate.",
}


@article{dhariwal2020,
  title={Jukebox: A Generative Model for Music},
  author={Prafulla Dhariwal and Heewoo Jun and Christine Payne and Jong Wook Kim and Alec Radford and Ilya Sutskever},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.00341}
}

@inproceedings{shwartz2020,
    title = "{``}You are grounded!{''}: Latent Name Artifacts in Pre-trained Language Models",
    author = "Shwartz, Vered  and
      Rudinger, Rachel  and
      Tafjord, Oyvind",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.556",
    doi = "10.18653/v1/2020.emnlp-main.556",
    pages = "6850--6861",
    abstract = "Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models. We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associated with specific entities, as indicated by next token prediction (e.g., Trump). While helpful in some contexts, grounding happens also in under-specified or inappropriate contexts. For example, endings generated for {`}Donald is a{'} substantially differ from those of other names, and often have more-than-average negative sentiment. We demonstrate the potential effect on downstream tasks with reading comprehension probes where name perturbation changes the model answers. As a silver lining, our experiments suggest that additional pre-training on different corpora may mitigate this bias.",
}


@article{chen2021,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde and Jared Kaplan and Harrison Edwards and Yura Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and David W. Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Igor Babuschkin and S. Arun Balaji and Shantanu Jain and Andrew Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew M. Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.03374}
}

@article{singh2021,
  title={FLAVA: A Foundational Language And Vision Alignment Model},
  author={Amanpreet Singh and Ronghang Hu and Vedanuj Goswami and Guillaume Couairon and Wojciech Galuba and Marcus Rohrbach and Douwe Kiela},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.04482}
}

@InProceedings{ramesh2021,
  title = 	 {Zero-Shot Text-to-Image Generation},
  author =       {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8821--8831},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/ramesh21a.html},
  abstract = 	 {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.}
}


@article{KleinbergRaghavan2021,
	author = {Kleinberg, Jon and Raghavan, Manish},
	title = {Algorithmic monoculture and social welfare},
	volume = {118},
	number = {22},
	elocation-id = {e2018340118},
	year = {2021},
	doi = {10.1073/pnas.2018340118},
	publisher = {National Academy of Sciences},
	abstract = {Algorithmic monoculture is a growing concern in the use of algorithms for high-stakes screening decisions in areas such as employment and lending. If many firms use the same algorithm, even if it is more accurate than the alternatives, the resulting {\textquotedblleft}monoculture{\textquotedblright} may be susceptible to correlated failures, much as a monocultural system is in biological settings. To investigate this concern, we develop a model of selection under monoculture. We find that even without any assumption of shocks or correlated failures{\textemdash}i.e., under {\textquotedblleft}normal operations{\textquotedblright}{\textemdash}the quality of decisions may decrease when multiple firms use the same algorithm. Thus, the introduction of a more accurate algorithm may decrease social welfare{\textemdash}a kind of {\textquotedblleft}Braess{\textquoteright} paradox{\textquotedblright} for algorithmic decision-making.As algorithms are increasingly applied to screen applicants for high-stakes decisions in employment, lending, and other domains, concerns have been raised about the effects of algorithmic monoculture, in which many decision-makers all rely on the same algorithm. This concern invokes analogies to agriculture, where a monocultural system runs the risk of severe harm from unexpected shocks. Here, we show that the dangers of algorithmic monoculture run much deeper, in that monocultural convergence on a single algorithm by a group of decision-making agents, even when the algorithm is more accurate for any one agent in isolation, can reduce the overall quality of the decisions being made by the full collection of agents. Unexpected shocks are therefore not needed to expose the risks of monoculture; it can hurt accuracy even under {\textquotedblleft}normal{\textquotedblright} operations and even for algorithms that are more accurate when used by only a single decision-maker. Our results rely on minimal assumptions and involve the development of a probabilistic framework for analyzing systems that use multiple noisy estimates of a set of alternatives.There are no data underlying this work.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/118/22/e2018340118},
	eprint = {https://www.pnas.org/content/118/22/e2018340118.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article {rives2021,
	author = {Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C. Lawrence and Ma, Jerry and Fergus, Rob},
	title = {Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
	volume = {118},
	number = {15},
	elocation-id = {e2016239118},
	year = {2021},
	doi = {10.1073/pnas.2016239118},
	publisher = {National Academy of Sciences},
	abstract = {Learning biological properties from sequence data is a logical step toward generative and predictive artificial intelligence for biology. Here, we propose scaling a deep contextual language model with unsupervised learning to sequences spanning evolutionary diversity. We find that without prior knowledge, information emerges in the learned representations on fundamental properties of proteins such as secondary structure, contacts, and biological activity. We show the learned representations are useful across benchmarks for remote homology detection, prediction of secondary structure, long-range residue{\textendash}residue contacts, and mutational effect. Unsupervised representation learning enables state-of-the-art supervised prediction of mutational effect and secondary structure and improves state-of-the-art features for long-range contact prediction.In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.Pretrained models and datasets are available at https://github.com/facebookresearch/esm.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/118/15/e2016239118},
	eprint = {https://www.pnas.org/content/118/15/e2016239118.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@techreport{Kline2021,
  doi = {10.3386/w29053},
  url = {https://doi.org/10.3386/w29053},
  year = {2021},
  month = jul,
  publisher = {National Bureau of Economic Research},
  author = {Patrick Kline and Evan Rose and Christopher Walters},
  title = {Systemic Discrimination Among Large U.S. Employers},
  institution = {National Bureau of Economic Research}
}

@techreport{Lippens2021state,
address = {Essen},
author = {Louis Lippens and Siel Vermeiren and Stijn Baert},
copyright = {http://www.econstor.eu/dspace/Nutzungsbedingungen},
keywords = {J71; J23; J14; J15; J16; 330; hiring discrimination; unequal treatment; meta-analysis; correspondence experiment; audit study},
language = {eng},
number = {972},
publisher = {Global Labor Organization (GLO)},
title = {The state of hiring discrimination: A meta-analysis of (almost) all recent correspondence experiments},
type = {GLO Discussion Paper},
url = {http://hdl.handle.net/10419/244614},
year = {2021},
institution = {IZA Institute of Labor Economics}
}

@article{Way2019,
  doi = {10.1073/pnas.1817431116},
  url = {https://doi.org/10.1073/pnas.1817431116},
  year = {2019},
  month = apr,
  publisher = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {22},
  pages = {10729--10733},
  author = {Samuel F. Way and Allison C. Morgan and Daniel B. Larremore and Aaron Clauset},
  title = {Productivity,  prominence,  and the effects of academic environment},
  journal = {Proceedings of the National Academy of Sciences}
}

@article{liu2015,
  title={Deep Learning Face Attributes in the Wild},
  author={Ziwei Liu and Ping Luo and Xiaogang Wang and Xiaoou Tang},
  journal={2015 IEEE International Conference on Computer Vision (ICCV)},
  year={2015},
  pages={3730-3738}
}

@article{Heesen2016,
  doi = {10.1007/s11229-016-1146-5},
  url = {https://doi.org/10.1007/s11229-016-1146-5},
  year = {2016},
  month = sep,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {194},
  number = {11},
  pages = {4499--4518},
  author = {Remco Heesen},
  title = {Academic superstars: competent or lucky?},
  journal = {Synthese}
}

@InProceedings{buolamwini2018,
  title = 	 {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
  author = 	 {Buolamwini, Joy and Gebru, Timnit},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {77--91},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/buolamwini18a.html},
  abstract = 	 {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.}
}

@article {garg2018,
	author = {Garg, Nikhil and Schiebinger, Londa and Jurafsky, Dan and Zou, James},
	title = {Word embeddings quantify 100 years of gender and ethnic stereotypes},
	volume = {115},
	number = {16},
	pages = {E3635--E3644},
	year = {2018},
	doi = {10.1073/pnas.1720347115},
	publisher = {National Academy of Sciences},
	abstract = {Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science.Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts{\textemdash}e.g., the women{\textquoteright}s movement in the 1960s and Asian immigration into the United States{\textemdash}and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/115/16/E3635},
	eprint = {https://www.pnas.org/content/115/16/E3635.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@inproceedings{strubell2019,
    title = "Energy and Policy Considerations for Deep Learning in {NLP}",
    author = "Strubell, Emma  and
      Ganesh, Ananya  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1355",
    doi = "10.18653/v1/P19-1355",
    pages = "3645--3650",
    abstract = "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
}

@inproceedings{devlin2019,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{liu2019,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692}
}

@article{kornblith2019,
  title={Do Better ImageNet Models Transfer Better?},
  author={Simon Kornblith and Jonathon Shlens and Quoc V. Le},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={2656-2666}
}

@INPROCEEDINGS{zhao2019,  author={Zhao, Chen and Chen, Feng},  booktitle={2019 IEEE International Conference on Data Mining (ICDM)},   title={Rank-Based Multi-task Learning for Fair Regression},   year={2019},  volume={},  number={},  pages={916-925},  doi={10.1109/ICDM.2019.00102}}

@inproceedings{gururangan2019,
    title = "Variational Pretraining for Semi-supervised Text Classification",
    author = "Gururangan, Suchin  and
      Dang, Tam  and
      Card, Dallas  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1590",
    doi = "10.18653/v1/P19-1590",
    pages = "5880--5894",
    abstract = "We introduce VAMPIRE, a lightweight pretraining framework for effective text classification when data and computing resources are limited. We pretrain a unigram document model as a variational autoencoder on in-domain, unlabeled data and use its internal states as features in a downstream classifier. Empirically, we show the relative strength of VAMPIRE against computationally expensive contextual embeddings and other popular semi-supervised baselines under low resource settings. We also find that fine-tuning to in-domain data is crucial to achieving decent performance from contextual embeddings when working with limited supervision. We accompany this paper with code to pretrain and use VAMPIRE embeddings in downstream tasks.",
}

@inproceedings{gururangan2020,
    title = "Don{'}t Stop Pretraining: Adapt Language Models to Domains and Tasks",
    author = "Gururangan, Suchin  and
      Marasovi{\'c}, Ana  and
      Swayamdipta, Swabha  and
      Lo, Kyle  and
      Beltagy, Iz  and
      Downey, Doug  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.740",
    doi = "10.18653/v1/2020.acl-main.740",
    pages = "8342--8360",
    abstract = "Language models pretrained on text from a wide variety of sources form the foundation of today{'}s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task{'}s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.",
}

@article{damour2020,
  title={Underspecification Presents Challenges for Credibility in Modern Machine Learning},
  author={Alexander D'Amour and Katherine A. Heller and Dan I. Moldovan and Ben Adlam and Babak Alipanahi and Alex Beutel and Christina Chen and Jonathan Deaton and Jacob Eisenstein and Matthew D. Hoffman and Farhad Hormozdiari and Neil Houlsby and Shaobo Hou and Ghassen Jerfel and Alan Karthikesalingam and Mario Lucic and Yi-An Ma and Cory Y. McLean and Diana Mincu and Akinori Mitani and Andrea Montanari and Zachary Nado and Vivek Natarajan and Christopher Nielson and Thomas F. Osborne and Rajiv Raman and Kim Ramasamy and Rory Sayres and Jessica Schrouff and Martin G. Seneviratne and Shannon Sequeira and Harini Suresh and Victor Veitch and Max Vladymyrov and Xuezhi Wang and Kellie Webster and Steve Yadlowsky and Taedong Yun and Xiaohua Zhai and D. Sculley},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.03395}
}

@inproceedings{wolf2020,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.",
}

@inproceedings{cao2020,
    title = "Toward Gender-Inclusive Coreference Resolution",
    author = "Cao, Yang Trista  and
      Daum{\'e} III, Hal",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.418",
    doi = "10.18653/v1/2020.acl-main.418",
    pages = "4568--4595",
    abstract = "Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems. Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms.",
}

@article{kaplan2020,
  title={Scaling Laws for Neural Language Models},
  author={Jared Kaplan and Sam McCandlish and T. J. Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeff Wu and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.08361}
}

@article {manning2020,
	author = {Manning, Christopher D. and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
	title = {Emergent linguistic structure in artificial neural networks trained by self-supervision},
	volume = {117},
	number = {48},
	pages = {30046--30054},
	year = {2020},
	doi = {10.1073/pnas.1907367117},
	publisher = {National Academy of Sciences},
	abstract = {This paper explores the knowledge of linguistic structure learned by large artificial neural networks, trained via self-supervision, whereby the model simply tries to predict a masked word in a given context. Human language communication is via sequences of words, but language understanding requires constructing rich hierarchical structures that are never observed explicitly. The mechanisms for this have been a prime mystery of human language acquisition, while engineering work has mainly proceeded by supervised learning on treebanks of sentences hand labeled for this latent structure. However, we demonstrate that modern deep contextual language models learn major aspects of this structure, without any explicit supervision. We develop methods for identifying linguistic hierarchical structure emergent in artificial neural networks and demonstrate that components in these models focus on syntactic grammatical relationships and anaphoric coreference. Indeed, we show that a linear transformation of learned embeddings in these models captures parse tree distances to a surprising degree, allowing approximate reconstruction of the sentence tree structures normally assumed by linguists. These results help explain why these models have brought such large improvements across many language-understanding tasks.Code and most of the data to reproduce the analyses in this paper are freely available at https://github.com/clarkkev/attention-analysis and https://github.com/john-hewitt/structural-probes (55, 56); the BERT models are freely available at https://github.com/google-research/bert (20). The syntactic evaluations use the Penn Treebank-3 (PTB), which is available under license from he Linguistic Data consortium at https://catalog.ldc.upenn.edu/LDC99T42 (35).},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/117/48/30046},
	eprint = {https://www.pnas.org/content/117/48/30046.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@inproceedings{sagawa2020,
title={Distributionally Robust Neural Networks},
author={Shiori Sagawa* and Pang Wei Koh* and Tatsunori B. Hashimoto and Percy Liang},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ryxGuJrFvS}
}

@inproceedings{Binns2020,
  doi = {10.1145/3351095.3372864},
  url = {https://doi.org/10.1145/3351095.3372864},
  year = {2020},
  month = jan,
  publisher = {{ACM}},
  author = {Reuben Binns},
  title = {On the apparent conflict between individual and group fairness},
  booktitle = {Proceedings of the 2020 Conference on Fairness,  Accountability,  and Transparency}
}

@article{Bertrand2004,
  doi = {10.1257/0002828042002561},
  url = {https://doi.org/10.1257/0002828042002561},
  year = {2004},
  month = aug,
  publisher = {American Economic Association},
  volume = {94},
  number = {4},
  pages = {991--1013},
  author = {Marianne Bertrand and Sendhil Mullainathan},
  title = {Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination},
  journal = {American Economic Review}
}

@article{Riach1991,
title = {Testing for Racial Discrimination in the Labour Market},
author = {Riach, Peter and Rich, Judith},
year = {1991},
journal = {Cambridge Journal of Economics},
volume = {15},
number = {3},
pages = {239-56},
url = {https://EconPapers.repec.org/RePEc:oup:cambje:v:15:y:1991:i:3:p:239-56}
}

@article {Quillian2017,
	author = {Quillian, Lincoln and Pager, Devah and Hexel, Ole and Midtb{\o}en, Arnfinn H.},
	title = {Meta-analysis of field experiments shows no change in racial discrimination in hiring over time},
	volume = {114},
	number = {41},
	pages = {10870--10875},
	year = {2017},
	doi = {10.1073/pnas.1706255114},
	publisher = {National Academy of Sciences},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/114/41/10870},
	eprint = {https://www.pnas.org/content/114/41/10870.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}


@incollection{Bertrand2017,
title = {Field Experiments on Discrimination},
editor = {Abhijit Vinayak Banerjee and Esther Duflo},
series = {Handbook of Economic Field Experiments},
address = {Amsterdam},
publisher = {North-Holland},
volume = {1},
pages = {309-393},
year = {2017},
booktitle = {Handbook of Field Experiments},
issn = {2214-658X},
doi = {https://doi.org/10.1016/bs.hefe.2016.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S2214658X1630006X},
author = {M. Bertrand and E. Duflo}
}

@article{Oreopoulos2011,
Author = {Oreopoulos, Philip},
Title = {Why Do Skilled Immigrants Struggle in the Labor Market? A Field Experiment with Thirteen Thousand Resumes},
Journal = {American Economic Journal: Economic Policy},
Volume = {3},
Number = {4},
Year = {2011},
Month = {November},
Pages = {148-71},
DOI = {10.1257/pol.3.4.148},
URL = {https://www.aeaweb.org/articles?id=10.1257/pol.3.4.148}}

@article{Pager2009,
  doi = {10.1177/000312240907400505},
  url = {https://doi.org/10.1177/000312240907400505},
  year = {2009},
  month = oct,
  publisher = {{SAGE} Publications},
  volume = {74},
  number = {5},
  pages = {777--799},
  author = {Devah Pager and Bart Bonikowski and Bruce Western},
  title = {Discrimination in a Low-Wage Labor Market},
  journal = {American Sociological Review}
}

@article{Weichselbaumer2003,
  doi = {10.1016/s0927-5371(03)00074-5},
  url = {https://doi.org/10.1016/s0927-5371(03)00074-5},
  year = {2003},
  month = dec,
  publisher = {Elsevier {BV}},
  volume = {10},
  number = {6},
  pages = {629--642},
  author = {Doris Weichselbaumer},
  title = {Sexual orientation discrimination in hiring},
  journal = {Labour Economics}
}

@article{Jowell1970,
  doi = {10.1177/030639687001100401},
  url = {https://doi.org/10.1177/030639687001100401},
  year = {1970},
  month = apr,
  publisher = {{SAGE} Publications},
  volume = {11},
  number = {4},
  pages = {397--417},
  author = {Roger Jowell and Patricia Prescott-Clarke},
  title = {Racial Discrimination and White-collar Workers in Britain},
  journal = {Race}
}

@inproceedings{marx2020,
author = {Marx, Charles T. and Du Pin Calmon, Flavio and Ustun, Berk},
title = {Predictive Multiplicity in Classification},
year = {2020},
publisher = {JMLR.org},
abstract = {Prediction problems often admit competing models that perform almost equally well. This effect challenges key assumptions in machine learning when competing models assign conflicting predictions. In this paper, we define predictive multiplicity as the ability of a prediction problem to admit competing models with conflicting predictions. We introduce formal measures to evaluate the severity of predictive multiplicity and develop integer programming tools to compute them exactly for linear classification problems. We apply our tools to measure predictive multiplicity in recidivism prediction problems. Our results show that real-world datasets may admit competing models that assign wildly conflicting predictions, and motivate the need to measure and report predictive multiplicity in model development.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {628},
numpages = {10},
series = {ICML'20}
}

@InProceedings{chen2020,
  title = 	 {Generative Pretraining From Pixels},
  author =       {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1691--1703},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20s/chen20s.pdf},
  url = 	 {https://proceedings.mlr.press/v119/chen20s.html},
  abstract = 	 {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full fine-tuning, matching the top supervised pre-trained models. We are also competitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0% top-1 accuracy on a linear probe of our features.}
}

@inproceedings{brown2020,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@book{wightman1998,
  title={LSAC National Longitudinal Bar Passage Study},
  author={Wightman, L.F. and Ramsey, H. and Law School Admission Council},
  series={LSAC research report series},
  url={https://books.google.com/books?id=dtA7AQAAIAAJ},
  year={1998},
  publisher={Law School Admission Council}
}

@article{breiman2001,
  title={Statistical modeling: The two cultures},
  author={L. Breiman},
  journal={Quality Engineering},
  year={2001},
  volume={48},
  pages={81-82}
}

@article{rothchild2021,
  title={C5T5: Controllable Generation of Organic Molecules with Transformers},
  author={Daniel Rothchild and Alex Tamkin and Julie Yu and Ujval Misra and Joseph Gonzalez},
  journal={ArXiv},
  year={2021},
  volume={abs/2108.10307}
}

@inproceedings{lhoest2021,
    title = "Datasets: A Community Library for Natural Language Processing",
    author = "Lhoest, Quentin  and
      Villanova del Moral, Albert  and
      Jernite, Yacine  and
      Thakur, Abhishek  and
      von Platen, Patrick  and
      Patil, Suraj  and
      Chaumond, Julien  and
      Drame, Mariama  and
      Plu, Julien  and
      Tunstall, Lewis  and
      Davison, Joe  and
      {\v{S}}a{\v{s}}ko, Mario  and
      Chhablani, Gunjan  and
      Malik, Bhavitvya  and
      Brandeis, Simon  and
      Le Scao, Teven  and
      Sanh, Victor  and
      Xu, Canwen  and
      Patry, Nicolas  and
      McMillan-Major, Angelina  and
      Schmid, Philipp  and
      Gugger, Sylvain  and
      Delangue, Cl{\'e}ment  and
      Matussi{\`e}re, Th{\'e}o  and
      Debut, Lysandre  and
      Bekman, Stas  and
      Cistac, Pierric  and
      Goehringer, Thibault  and
      Mustar, Victor  and
      Lagunas, Fran{\c{c}}ois  and
      Rush, Alexander  and
      Wolf, Thomas",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-demo.21",
    doi = "10.18653/v1/2021.emnlp-demo.21",
    pages = "175--184",
    abstract = "The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.",
}


@inproceedings{khani2021,
author = {Khani, Fereshte and Liang, Percy},
title = {Removing Spurious Features Can Hurt Accuracy and Affect Groups Disproportionately},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445883},
doi = {10.1145/3442188.3445883},
abstract = {Spurious features interfere with the goal of obtaining robust models that perform well across many groups within the population. A natural remedy is to remove such features from the model. However, in this work, we show that removing spurious features can surprisingly decrease accuracy due to the inductive biases of overparameterized models. In noiseless overparameterized linear regression, we completely characterize how the removal of spurious features affects accuracy across different groups (more generally, test distributions). In addition, we show that removal of spurious features can decrease the accuracy even on balanced datasets (where each target co-occurs equally with each spurious feature); and it can inadvertently make the model more susceptible to other spurious features. Finally, we show that robust self-training produces models that no longer depend on spurious features without affecting their overall accuracy. The empirical results on the Toxic-Comment-Detection and CelebA datasets show that our results hold in non-linear models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {196–205},
numpages = {10},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{bender2021,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?}, 
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610–623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@book{hand2016,
  title={Measurement: A Very Short Introduction},
  author={Hand, D.J.},
  isbn={9780198779568},
  lccn={2016942911},
  series={Very short introductions},
  url={https://books.google.com/books?id=QBIBDQAAQBAJ},
  year={2016},
  publisher={Oxford University Press}
}


@inproceedings{jacobs2021,
author = {Abigail Z. Jacobs and Hanna Wallach},
title = {Measurement and Fairness},
year = {2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://arxiv.org/abs/1912.05511},
booktitle = {Proceedings of the 2021 Conference on Fairness, Accountability, and Transparency},
location = {Online},
series = {FAccT '21}
}

 @misc{Hirevue2021, 
 title={HireVue customers conduct over 1 million video interviews in just 30 Days}, 
 url={https://www.hirevue.com/press-release/hirevue-customers-conduct-over-1-million-video-interviews-in-just-30-days},
 journal={hirevue.com}, 
 author={Hirevue},  
 year={2021}, 
 month={Oct}}
 
@misc{BurningGlass,
url={https://www.burning-glass.com/},
title={Burning Glass Technologies},
author={Burning Glass Technologies},
year={2022},
month={January}
}

@article{sonderling2022promise,
author = {Keith E. Sonderling and Bradford J. Kelley and Lance Casimir},
year = {2022},
month = {11},
pages = {},
title = {The Promise and The Peril: Artificial Intelligence and Employment Discrimination},
volume = {77},
journal = {University of Miami Law Review},
url = {https://repository.law.miami.edu/umlr/vol77/iss1/3},
}


@article{fabris2022,
  title={Algorithmic Fairness Datasets: the Story so Far},
  author={Alessandro Fabris and Stefano Messina and Gianmaria Silvello and Gian Antonio Susto},
  journal={Data Mining and Knowledge Discovery},
  year={2022},
  volume={abs/2202.01711}
}

@article{quy2022,
author = {Le Quy, Tai and Roy, Arjun and Iosifidis, Vasileios and Zhang, Wenbin and Ntoutsi, Eirini},
year = {2022},
month = {05},
pages = {},
title = {A survey on datasets for fairness‐aware machine learning},
volume = {12},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
doi = {10.1002/widm.1452}
}

@inproceedings{semenova2022,
author = {Semenova, Lesia and Rudin, Cynthia and Parr, Ronald},
title = {On the Existence of Simpler Machine Learning Models},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533232},
doi = {10.1145/3531146.3533232},
abstract = {It is almost always easier to find an accurate-but-complex model than an accurate-yet-simple model. Finding optimal, sparse, accurate models of various forms (linear models with integer coefficients, decision sets, rule lists, decision trees) is generally NP-hard. We often do not know whether the search for a simpler model will be worthwhile, and thus we do not go to the trouble of searching for one. In this work, we ask an important practical question: can accurate-yet-simple models be proven to exist, or shown likely to exist, before explicitly searching for them? We hypothesize that there is an important reason that simple-yet-accurate models often do exist. This hypothesis is that the size of the Rashomon set is often large, where the Rashomon set is the set of almost-equally-accurate models from a function class. If the Rashomon set is large, it contains numerous accurate models, and perhaps at least one of them is the simple model we desire. In this work, we formally present the Rashomon ratio as a new gauge of simplicity for a learning problem, depending on a function class and a data set. The Rashomon ratio is the ratio of the volume of the set of accurate models to the volume of the hypothesis space, and it is different from standard complexity measures from statistical learning theory. Insight from studying the Rashomon ratio provides an easy way to check whether a simpler model might exist for a problem before finding it, namely whether several different machine learning methods achieve similar performance on the data. In that sense, the Rashomon ratio is a powerful tool for understanding why and when an accurate-yet-simple model might exist. If, as we hypothesize in this work, many real-world data sets admit large Rashomon sets, the implications are vast: it means that simple or interpretable models may often be used for high-stakes decisions without losing accuracy.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1827–1858},
numpages = {32},
keywords = {Simplicity, Interpretable Machine Learning, Rashomon Set, Generalization, Model Multiplicity},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}
 
@inproceedings{fishman2022,
author = {Nic Fishman and Leif Hancox-Li},
title = {Should attention be all we need? The epistemic and ethical implications of unification in machine learning},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://arxiv.org/abs/2205.08377},
booktitle = {Proceedings of the 2022 Conference on Fairness, Accountability, and Transparency},
location = {Seoul, South Korea},
series = {FAccT '22}
}

@misc{dua2019,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@misc{Hirevue2019,
url={https://www.hirevue.com/press-release/hirevue-receives-growth-investment-from-the-carlyle-group},
title={HireVue to Receive Growth Investment from New Majority Investor The Carlyle Group},
year={2019},
month={September},
journal={hirevue.com}, 
author={Hirevue}


}

@book{Moore2018,
title = {Digital Dominance: The Power of Google, Facebook, Amazon and Apple},
editor = {Martin Moore and Damian Tambini},
publisher = {Oxford University Press},
address = {New York, NY},
year = {2018},
month = {05},
day = {08},
pagecount = {304},
isbn = {9780190845131}
}


@article{lakhotia2021,
title = "On Generative Spoken Language Modeling from Raw Audio",
author = "Lakhotia, Kushal  and
  Kharitonov, Eugene  and
  Hsu, Wei-Ning  and
  Adi, Yossi  and
  Polyak, Adam  and
  Bolte, Benjamin  and
  Nguyen, Tu-Anh  and
  Copet, Jade  and
  Baevski, Alexei  and
  Mohamed, Abdelrahman  and
  Dupoux, Emmanuel",
journal = "Transactions of the Association for Computational Linguistics",
volume = "9",
year = "2021",
address = "Cambridge, MA",
publisher = "MIT Press",
url = "https://aclanthology.org/2021.tacl-1.79",
    doi = "10.1162/tacl_a_00430",
    pages = "1336--1354"
}

@inproceedings{
dosovitskiy2021,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@article{creel2022, title={The Algorithmic Leviathan: Arbitrariness, Fairness, and Opportunity in Algorithmic Decision-Making Systems}, volume={52}, DOI={10.1017/can.2022.3}, number={1}, journal={Canadian Journal of Philosophy}, publisher={Cambridge University Press}, author={Creel, Kathleen and Hellman, Deborah}, year={2022}, pages={26–43}}

@inbook{wang2021,
author = {Wang, Yuyan and Wang, Xuezhi and Beutel, Alex and Prost, Flavien and Chen, Jilin and Chi, Ed H.},
title = {Understanding and Improving Fairness-Accuracy Trade-Offs in Multi-Task Learning},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467326},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {1748–1757},
numpages = {10}
}

@inproceedings{radford2021,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  booktitle={ICML},
  year={2021}
}

@misc{benzaken2021,
  title={BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
  author={Elad Ben-Zaken and Shauli Ravfogel and Yoav Goldberg},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.10199}
}

@inproceedings{benzaken2022,
    title = "{B}it{F}it: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
    author = "Ben Zaken, Elad  and
      Goldberg, Yoav  and
      Ravfogel, Shauli",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.1",
    pages = "1--9",
    abstract = "We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods.Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
}


@article{Jain2021,
  author    = {Saachi Jain and
               Dimitris Tsipras and
               Aleksander Madry},
  title     = {Combining Diverse Feature Priors},
  journal   = {CoRR},
  volume    = {abs/2110.08220},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.08220},
  eprinttype = {arXiv},
  eprint    = {2110.08220},
  timestamp = {Fri, 22 Oct 2021 13:33:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-08220.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zhang2017,
  title={A Survey on Multi-Task Learning},
  author={Yu Zhang and Qiang Yang},
  journal={ArXiv},
  year={2017},
  volume={abs/1707.08114}
}

@misc{grgic2017,
      title={On Fairness, Diversity and Randomness in Algorithmic Decision Making}, 
      Author = {Nina Grgi\'{c}-Hla\u{c}a and Muhammad Bilal Zafar and Krishna P. Gummadi and Adrian Weller},
      year={2017},
      eprint={1706.10208},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@book{Raz1988,
author = {Joseph Raz},
title = {The Morality of Freedom},
publisher = {Oxford University Press},
address = {Oxford},
year = {1988}
}

@article {crenshaw1989,
  title={Demarginalizing the intersection of race and sex: A black feminist critique of antidiscrimination doctrine, feminist theory and antiracist politics},
  author={Crenshaw, Kimberl{\'e}},
  journal={University of Chicago Legal Forum},
  volume={Vol.1989, Article 8},
  year={1989},
  url={https://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=1052&context=uclf}
}

@misc{CurrentEmploymentStatistics,
url = {https://www.bls.gov/ces/},
author = {Division of Current Employment Statistics},
title = {Current Employment Statistics - CES (National)},
year = {2022}
}

@book{Anderson2016,
author = {Elizabeth Anderson},
title = {Liberty, Equality, and Private Government},
booktitle = {The Tanner Lectures on Human Values, Volume 35},
year = {2016},
publisher = {University of Utah Press},
bookeditor = {Mark Matheson},

}

@article{Anderson1999,
  doi = {10.1086/233897},
  url = {https://doi.org/10.1086/233897},
  year = {1999},
  month = jan,
  publisher = {University of Chicago Press},
  volume = {109},
  number = {2},
  pages = {287--337},
  author = {Elizabeth~S. Anderson},
  title = {What Is the Point of Equality?},
  journal = {Ethics}
}

@article{logan2021,
  title={Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models},
  author={Robert L Logan IV and Ivana Balavzevi'c and Eric Wallace and Fabio Petroni and Sameer Singh and Sebastian Riedel},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.13353}
}

@inproceedings{antoniak2021,
    title = "Bad Seeds: Evaluating Lexical Methods for Bias Measurement",
    author = "Antoniak, Maria  and
      Mimno, David",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.148",
    doi = "10.18653/v1/2021.acl-long.148",
    pages = "1889--1904"
}

@techreport{Engler2021,
institution = {The Brookings Institution},
type = {report},
title = {Enrollment algorithms are contributing to the crises of higher education},
author = {Alex Engler},
month = {September},
day = {14}, 
year = {2021},
url = {https://www.brookings.edu/research/enrollment-algorithms-are-contributing-to-the-crises-of-higher-education/}
}

@article{Lee2015,
  author    = {Stefan Lee and
               Senthil Purushwalkam and
               Michael Cogswell and
               David J. Crandall and
               Dhruv Batra},
  title     = {Why {M} Heads are Better than One: Training a Diverse Ensemble of
               Deep Networks},
  journal   = {CoRR},
  volume    = {abs/1511.06314},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06314},
  eprinttype = {arXiv},
  eprint    = {1511.06314},
  timestamp = {Mon, 13 Aug 2018 16:48:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LeePCCB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Gandy2021PanopticonsLeviathans,
	title = {Panopticons and {Leviathans}: {Oscar} {H}. {Gandy}, {Jr}. on {Algorithmic} {Life}},
	shorttitle = {Panopticons and {Leviathans}},
	url = {https://logicmag.io/commons/panopticons-and-leviathans-oscar-h-gandy-jr-on-algorithmic-life/},
	abstract = {A conversation about life and death in a computerized world.},
	year = {2020},
	month = {12},
	day = {20},
	journal = {Logic Magazine},
	author = {Gandy, Jr., Oscar H.}
}

@book{Gandy1993Panoptic,
author = {Gandy, Jr., Oscar H.},
title = {The Panoptic Sort: A Political Economy of Personal Information},
publisher = {Westview Press},
address = {Boulder, CO},
year = {1993}
}

@article{Hellman2021Compounding,
author = {Deborah Hellman},
title = {Big Data and Compounding Injustice},
year = {forthcoming},
journal = {Journal of Moral Philosophy} 
}

@INPROCEEDINGS{birhane2021,  
author={Birhane, Abeba and Prabhu, Vinay Uday},  
booktitle={2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},   
title={Large image datasets: A pyrrhic win for computer vision?},   
year={2021},  
volume={},  
number={},  
pages={1536-1546},  doi={10.1109/WACV48630.2021.00158}}

@article{birhane2021b,
  title={Multimodal datasets: misogyny, pornography, and malignant stereotypes},
  author={Abeba Birhane and Vinay Uday Prabhu and Emmanuel Kahembwe},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.01963}
}

@article{bommasani2021,
  title={On the Opportunities and Risks of Foundation Models},
  author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and S. Buch and D. Card and Rodrigo Castellon and Niladri S. Chatterji and Annie Chen and Kathleen Creel and Jared Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren E. Gillespie and Karan Goel and Noah D. Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas F. Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and O. Khattab and Pang Wei Koh and Mark S. Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir P. Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and J. F. Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Robert Reich and Hongyu Ren and Frieda Rong and Yusuf H. Roohani and Camilo Ruiz and Jackson K. Ryan and Christopher R'e and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishna Parasuram Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tram{\`e}r and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei A. Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
  journal={ArXiv},
  year={2021},
  volume={abs/2108.07258},
  url={https://crfm.stanford.edu/assets/report.pdf}
}

@inproceedings{ding2021,
title={Retiring Adult: New Datasets for Fair Machine Learning},
author={Frances Ding and Moritz Hardt and John Miller and Ludwig Schmidt},
booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
year={2021},
url={https://openreview.net/forum?id=bYi_2708mKK}
}

@article{metaxa2021,
url = {http://dx.doi.org/10.1561/1100000083},
year = {2021},
volume = {14},
journal = {Foundations and Trends® in Human–Computer Interaction},
title = {Auditing Algorithms: Understanding Algorithmic Systems from the Outside In},
doi = {10.1561/1100000083},
issn = {1551-3955},
number = {4},
pages = {272-344},
author = {Danaë Metaxa and Joon Sung Park and Ronald E. Robertson and Karrie Karahalios and Christo Wilson and Jeff Hancock and Christian Sandvig}
}

% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }