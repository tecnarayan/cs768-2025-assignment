\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Azizi et~al.(2021)Azizi, Mustafa, Ryan, Beaver, Freyberg, Deaton, Loh,
  Karthikesalingam, Kornblith, Chen, et~al.]{azizi2021big}
Azizi, S., Mustafa, B., Ryan, F., Beaver, Z., Freyberg, J., Deaton, J., Loh,
  A., Karthikesalingam, A., Kornblith, S., Chen, T., et~al.
\newblock Big self-supervised models advance medical image classification.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  3478--3488, 2021.

\bibitem[Banayeeanzade et~al.(2021)Banayeeanzade, Mirzaiezadeh, Hasani, and
  Soleymani]{banayeeanzade2021generative}
Banayeeanzade, M., Mirzaiezadeh, R., Hasani, H., and Soleymani, M.
\newblock Generative vs. discriminative: Rethinking the meta-continual
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Baxter(2000)]{baxter2000model}
Baxter, J.
\newblock A model of inductive bias learning.
\newblock \emph{Journal of artificial intelligence research}, 12:\penalty0
  149--198, 2000.

\bibitem[Borgeaud et~al.(2022)Borgeaud, Mensch, Hoffmann, Cai, Rutherford,
  Millican, Van Den~Driessche, Lespiau, Damoc, Clark,
  et~al.]{borgeaud2021improving}
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K.,
  Van Den~Driessche, G.~B., Lespiau, J.-B., Damoc, B., Clark, A., et~al.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock In \emph{International conference on machine learning}, pp.\
  2206--2240. PMLR, 2022.

\bibitem[Bottou \& Vapnik(1992)Bottou and Vapnik]{bottou1992local}
Bottou, L. and Vapnik, V.
\newblock Local learning algorithms.
\newblock \emph{Neural computation}, 4\penalty0 (6):\penalty0 888--900, 1992.

\bibitem[Bricken et~al.(2023)Bricken, Davies, Singh, Krotov, and
  Kreiman]{sdm_paper}
Bricken, T., Davies, X., Singh, D., Krotov, D., and Kreiman, G.
\newblock Sparse distributed memory is a continual learner.
\newblock \emph{International Conference for Learning Representations (ICLR)},
  2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron2020unsupervised}
Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 9912--9924, 2020.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J\'egou, Mairal, Bojanowski,
  and Joulin]{caron2021emerging}
Caron, M., Touvron, H., Misra, I., J\'egou, H., Mairal, J., Bojanowski, P., and
  Joulin, A.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{Proceedings of the International Conference on Computer
  Vision (ICCV)}, 2021.

\bibitem[Chandar et~al.(2016)Chandar, Ahn, Larochelle, Vincent, Tesauro, and
  Bengio]{chandar2016hierarchical}
Chandar, S., Ahn, S., Larochelle, H., Vincent, P., Tesauro, G., and Bengio, Y.
\newblock Hierarchical memory networks.
\newblock \emph{arXiv preprint arXiv:1605.07427}, 2016.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Swersky, Norouzi, and
  Hinton]{chen2020big}
Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G.~E.
\newblock Big self-supervised models are strong semi-supervised learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 22243--22255, 2020.

\bibitem[Chen \& Liu(2018)Chen and Liu]{chen2018lifelong}
Chen, Z. and Liu, B.
\newblock Lifelong machine learning.
\newblock \emph{Synthesis Lectures on Artificial Intelligence and Machine
  Learning}, 12\penalty0 (3):\penalty0 1--207, 2018.

\bibitem[Coates et~al.(2011)Coates, Ng, and Lee]{coates2011analysis}
Coates, A., Ng, A., and Lee, H.
\newblock An analysis of single-layer networks in unsupervised feature
  learning.
\newblock In \emph{Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pp.\  215--223. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Davidson \& Mozer(2020)Davidson and Mozer]{davidson2020sequential}
Davidson, G. and Mozer, M.~C.
\newblock Sequential mastery of multiple visual tasks: Networks naturally learn
  to learn and forget to forget.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  9282--9293, 2020.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Finn et~al.(2019)Finn, Rajeswaran, Kakade, and Levine]{finn2019online}
Finn, C., Rajeswaran, A., Kakade, S., and Levine, S.
\newblock Online meta-learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1920--1930. PMLR, 2019.

\bibitem[Gao et~al.(2022)Gao, Luo, Klabjan, and Zhang]{gao2022efficient}
Gao, Q., Luo, Z., Klabjan, D., and Zhang, F.
\newblock Efficient architecture search for continual learning.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  2022.

\bibitem[Goyal et~al.(2022)Goyal, Friesen, Banino, Weber, Ke, Badia, Guez,
  Mirza, Humphreys, Konyushova, et~al.]{goyal2022retrieval}
Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N.~R., Badia, A.~P., Guez,
  A., Mirza, M., Humphreys, P.~C., Konyushova, K., et~al.
\newblock Retrieval-augmented reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7740--7765. PMLR, 2022.

\bibitem[Guu et~al.(2020)Guu, Lee, Tung, Pasupat, and Chang]{guu2020realm}
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.
\newblock Retrieval augmented language model pre-training.
\newblock In \emph{International conference on machine learning}, pp.\
  3929--3938. PMLR, 2020.

\bibitem[Harrison et~al.(2020)Harrison, Sharma, Finn, and
  Pavone]{harrison2020continuous}
Harrison, J., Sharma, A., Finn, C., and Pavone, M.
\newblock Continuous meta-learning without tasks.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 17571--17581, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2020)He, Sygnowski, Galashov, Rusu, Teh, and
  Pascanu]{he2019task}
He, X., Sygnowski, J., Galashov, A., Rusu, A.~A., Teh, Y.~W., and Pascanu, R.
\newblock Task agnostic continual learning via meta learning.
\newblock In \emph{4th Lifelong Machine Learning Workshop at ICML 2020}, 2020.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, Dean,
  et~al.]{hinton2015distilling}
Hinton, G., Vinyals, O., Dean, J., et~al.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2\penalty0 (7), 2015.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameter}
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De~Laroussilhe, Q.,
  Gesmundo, A., Attariyan, M., and Gelly, S.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2790--2799. PMLR, 2019.

\bibitem[Huang et~al.(2019)Huang, Dong, Gong, and Zhu]{huang2019unsupervised}
Huang, J., Dong, Q., Gong, S., and Zhu, X.
\newblock Unsupervised deep learning by neighbourhood discovery.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2849--2858. PMLR, 2019.

\bibitem[Kanerva(1988)]{kanerva1988sparse}
Kanerva, P.
\newblock \emph{Sparse distributed memory}.
\newblock MIT press, 1988.

\bibitem[Kanerva(1992)]{kanerva1992sparse}
Kanerva, P.
\newblock Sparse distributed memory and related models.
\newblock Technical report, 1992.

\bibitem[Kawaguchi et~al.(2022)Kawaguchi, Deng, Luh, and
  Huang]{kawaguchi2022robustness}
Kawaguchi, K., Deng, Z., Luh, K., and Huang, J.
\newblock Robustness implies generalization via data-dependent generalization
  bounds.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10866--10894. PMLR, 2022.

\bibitem[Ke et~al.(2021)Ke, Liu, Ma, Xu, and Shu]{ke2021achieving}
Ke, Z., Liu, B., Ma, N., Xu, H., and Shu, L.
\newblock Achieving forgetting prevention and knowledge transfer in continual
  learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 22443--22456, 2021.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick2017overcoming}
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu,
  A.~A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kumar et~al.(2022)Kumar, Raghunathan, Jones, Ma, and
  Liang]{kumar2022fine}
Kumar, A., Raghunathan, A., Jones, R., Ma, T., and Liang, P.
\newblock Fine-tuning can distort pretrained features and underperform
  out-of-distribution.
\newblock \emph{International Conference on Learning Representations}, 2022.

\bibitem[Lample et~al.(2019)Lample, Sablayrolles, Ranzato, Denoyer, and
  J{\'e}gou]{lample2019large}
Lample, G., Sablayrolles, A., Ranzato, M., Denoyer, L., and J{\'e}gou, H.
\newblock Large memory layers with product keys.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Lee et~al.(2019)Lee, Chang, and Toutanova]{lee2019latent}
Lee, K., Chang, M.-W., and Toutanova, K.
\newblock Latent retrieval for weakly supervised open domain question
  answering.
\newblock \emph{arXiv preprint arXiv:1906.00300}, 2019.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal,
  K{\"u}ttler, Lewis, Yih, Rockt{\"a}schel, et~al.]{lewis2020retrieval}
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N.,
  K{\"u}ttler, H., Lewis, M., Yih, W.-t., Rockt{\"a}schel, T., et~al.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 9459--9474, 2020.

\bibitem[Li \& Hoiem(2017)Li and Hoiem]{li2017learning}
Li, Z. and Hoiem, D.
\newblock Learning without forgetting.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 40\penalty0 (12):\penalty0 2935--2947, 2017.

\bibitem[Liu et~al.(2021)Liu, Lamb, Kawaguchi, Goyal, Sun, Mozer, and
  Bengio]{liu2021discrete}
Liu, D., Lamb, A.~M., Kawaguchi, K., Goyal, A., Sun, C., Mozer, M.~C., and
  Bengio, Y.
\newblock Discrete-valued neural communication.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Mama et~al.(2021)Mama, Tyndel, Kadhim, Clifford, and
  Thurairatnam]{mama2021nwt}
Mama, R., Tyndel, M.~S., Kadhim, H., Clifford, C., and Thurairatnam, R.
\newblock Nwt: Towards natural audio-to-video generation with representation
  learning.
\newblock \emph{arXiv preprint arXiv:2106.04283}, 2021.

\bibitem[Nakano et~al.(2021)Nakano, Hilton, Balaji, Wu, Ouyang, Kim, Hesse,
  Jain, Kosaraju, Saunders, et~al.]{nakano2021webgpt}
Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C.,
  Jain, S., Kosaraju, V., Saunders, W., et~al.
\newblock Webgpt: Browser-assisted question-answering with human feedback.
\newblock \emph{arXiv preprint arXiv:2112.09332}, 2021.

\bibitem[Ostapenko et~al.(2022)Ostapenko, Lesort, Rodr{\'\i}guez, Arefin,
  Douillard, Rish, and Charlin]{ostapenko2022foundational}
Ostapenko, O., Lesort, T., Rodr{\'\i}guez, P., Arefin, M.~R., Douillard, A.,
  Rish, I., and Charlin, L.
\newblock Foundational models for continual learning: An empirical study of
  latent replay.
\newblock \emph{arXiv preprint arXiv:2205.00329}, 2022.

\bibitem[Panigrahy et~al.(2021)Panigrahy, Wang, and
  Zaheer]{panigrahy2021sketch}
Panigrahy, R., Wang, X., and Zaheer, M.
\newblock Sketch based memory for neural networks.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3169--3177. PMLR, 2021.

\bibitem[Pham et~al.(2021)Pham, Dai, Ghiasi, Kawaguchi, Liu, Yu, Yu, Chen,
  Luong, Wu, et~al.]{pham2021combined}
Pham, H., Dai, Z., Ghiasi, G., Kawaguchi, K., Liu, H., Yu, A.~W., Yu, J., Chen,
  Y.-T., Luong, M.-T., Wu, Y., et~al.
\newblock Combined scaling for open-vocabulary image classification.
\newblock \emph{arXiv e-prints}, pp.\  arXiv--2111, 2021.

\bibitem[Pourcel et~al.(2022)Pourcel, Vu, and French]{pourcel2022online}
Pourcel, J., Vu, N.-S., and French, R.~M.
\newblock Online task-free continual learning with dynamic sparse distributed
  memory.
\newblock In \emph{European Conference on Computer Vision}, pp.\  739--756.
  Springer, 2022.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8748--8763. PMLR, 2021.

\bibitem[Razavi et~al.(2019)Razavi, Van~den Oord, and
  Vinyals]{razavi2019generating}
Razavi, A., Van~den Oord, A., and Vinyals, O.
\newblock Generating diverse high-fidelity images with vq-vae-2.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Kolesnikov, Sperl, and
  Lampert]{rebuffi2017icarl}
Rebuffi, S.-A., Kolesnikov, A., Sperl, G., and Lampert, C.~H.
\newblock icarl: Incremental classifier and representation learning.
\newblock In \emph{Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pp.\  2001--2010, 2017.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Saha et~al.(2021)Saha, Garg, and Roy]{saha2021gradient}
Saha, G., Garg, I., and Roy, K.
\newblock Gradient projection memory for continual learning.
\newblock \emph{International Conference for Learning Representations (ICLR)},
  2021.

\bibitem[Serra et~al.(2018)Serra, Suris, Miron, and
  Karatzoglou]{serra2018overcoming}
Serra, J., Suris, D., Miron, M., and Karatzoglou, A.
\newblock Overcoming catastrophic forgetting with hard attention to the task.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4548--4557. PMLR, 2018.

\bibitem[Shen et~al.(2021)Shen, Dasgupta, and Navlakha]{shen2021algorithmic}
Shen, Y., Dasgupta, S., and Navlakha, S.
\newblock Algorithmic insights on continual learning from fruit flies.
\newblock \emph{arXiv preprint arXiv:2107.07617}, 2021.

\bibitem[Shin et~al.(2017)Shin, Lee, Kim, and Kim]{shin2017continual}
Shin, H., Lee, J.~K., Kim, J., and Kim, J.
\newblock Continual learning with deep generative replay.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Shin et~al.(2021)Shin, Lee, Lee, Lee, and Choi]{shin2021translation}
Shin, W., Lee, G., Lee, J., Lee, J., and Choi, E.
\newblock Translation-equivariant image quantizer for bi-directional image-text
  generation.
\newblock \emph{arXiv preprint arXiv:2112.00384}, 2021.

\bibitem[Sukhbaatar et~al.(2015)Sukhbaatar, Weston, Fergus,
  et~al.]{sukhbaatar2015end}
Sukhbaatar, S., Weston, J., Fergus, R., et~al.
\newblock End-to-end memory networks.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.

\bibitem[Sun et~al.(2021)Sun, Wang, Feng, Ding, Pang, Shang, Liu, Chen, Zhao,
  Lu, et~al.]{sun2021ernie}
Sun, Y., Wang, S., Feng, S., Ding, S., Pang, C., Shang, J., Liu, J., Chen, X.,
  Zhao, Y., Lu, Y., et~al.
\newblock Ernie 3.0: Large-scale knowledge enhanced pre-training for language
  understanding and generation.
\newblock \emph{arXiv preprint arXiv:2107.02137}, 2021.

\bibitem[Tang et~al.(2022)Tang, Peng, and Zheng]{tang2022learning}
Tang, Y.-M., Peng, Y.-X., and Zheng, W.-S.
\newblock Learning to imagine: Diversify memory for incremental learning using
  unlabeled data.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  9549--9558, 2022.

\bibitem[Thrun(1995)]{thrun1995learning}
Thrun, S.
\newblock Is learning the n-th thing any easier than learning the first?
\newblock \emph{Advances in neural information processing systems}, 8, 1995.

\bibitem[Thrun \& Pratt(2012)Thrun and Pratt]{thrun2012learning}
Thrun, S. and Pratt, L.
\newblock \emph{Learning to learn}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Tian et~al.(2022)Tian, Wu, Dai, Hu, and Jiang]{tian2022deeper}
Tian, R., Wu, Z., Dai, Q., Hu, H., and Jiang, Y.
\newblock Deeper insights into vits robustness towards common corruptions.
\newblock \emph{arXiv preprint arXiv:2204.12143}, 2022.

\bibitem[Trockman \& Kolter(2022)Trockman and Kolter]{trockman2022patches}
Trockman, A. and Kolter, J.~Z.
\newblock Patches are all you need?
\newblock \emph{arXiv preprint arXiv:2201.09792}, 2022.

\bibitem[Van~de Ven \& Tolias(2019)Van~de Ven and Tolias]{van2019three}
Van~de Ven, G.~M. and Tolias, A.~S.
\newblock Three scenarios for continual learning.
\newblock \emph{arXiv preprint arXiv:1904.07734}, 2019.

\bibitem[Van Den~Oord et~al.(2017)Van Den~Oord, Vinyals, et~al.]{van2017neural}
Van Den~Oord, A., Vinyals, O., et~al.
\newblock Neural discrete representation learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Verma et~al.(2021)Verma, Liang, Mehta, Rai, and
  Carin]{verma2021efficient}
Verma, V.~K., Liang, K.~J., Mehta, N., Rai, P., and Carin, L.
\newblock Efficient feature transformations for discriminative and generative
  continual learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  13865--13875, 2021.

\bibitem[Wang et~al.(2022)Wang, Zhang, Lee, Zhang, Sun, Ren, Su, Perot, Dy, and
  Pfister]{wang2021learning}
Wang, Z., Zhang, Z., Lee, C.-Y., Zhang, H., Sun, R., Ren, X., Su, G., Perot,
  V., Dy, J., and Pfister, T.
\newblock Learning to prompt for continual learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  139--149, 2022.

\bibitem[Webb et~al.(2021)Webb, Sinha, and Cohen]{webb2020emergent}
Webb, T.~W., Sinha, I., and Cohen, J.~D.
\newblock Emergent symbols through binding in external memory.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Xiao et~al.(2014)Xiao, Zhang, Yang, Peng, and Zhang]{xiao2014error}
Xiao, T., Zhang, J., Yang, K., Peng, Y., and Zhang, Z.
\newblock Error-driven incremental learning in deep convolutional neural
  network for large-scale image classification.
\newblock In \emph{Proceedings of the 22nd ACM international conference on
  Multimedia}, pp.\  177--186, 2014.

\bibitem[Yosinski et~al.(2014)Yosinski, Clune, Bengio, and
  Lipson]{yosinski2014transferable}
Yosinski, J., Clune, J., Bengio, Y., and Lipson, H.
\newblock How transferable are features in deep neural networks?
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Yu et~al.(2022)Yu, Li, Koh, Zhang, Pang, Qin, Ku, Xu, Baldridge, and
  Wu]{yu2021vector}
Yu, J., Li, X., Koh, J.~Y., Zhang, H., Pang, R., Qin, J., Ku, A., Xu, Y.,
  Baldridge, J., and Wu, Y.
\newblock Vector-quantized image modeling with improved vqgan.
\newblock \emph{International Conference on Learning Representations}, 2022.

\bibitem[Zeghidour et~al.(2021)Zeghidour, Luebs, Omran, Skoglund, and
  Tagliasacchi]{zeghidour2021soundstream}
Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., and Tagliasacchi, M.
\newblock Soundstream: An end-to-end neural audio codec.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, 2021.

\bibitem[Zeng et~al.(2019)Zeng, Chen, Cui, and Yu]{zeng2019continual}
Zeng, G., Chen, Y., Cui, B., and Yu, S.
\newblock Continual learning of context-dependent processing in neural
  networks.
\newblock \emph{Nature Machine Intelligence}, 1\penalty0 (8):\penalty0
  364--372, 2019.

\bibitem[Zenke et~al.(2017)Zenke, Poole, and Ganguli]{zenke2017continual}
Zenke, F., Poole, B., and Ganguli, S.
\newblock Continual learning through synaptic intelligence.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3987--3995. PMLR, 2017.

\bibitem[Zhou et~al.(2022{\natexlab{a}})Zhou, Yu, Xie, Xiao, Anandkumar, Feng,
  and Alvarez]{zhou2022understanding}
Zhou, D., Yu, Z., Xie, E., Xiao, C., Anandkumar, A., Feng, J., and Alvarez,
  J.~M.
\newblock Understanding the robustness in vision transformers.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  27378--27394. PMLR, 2022{\natexlab{a}}.

\bibitem[Zhou et~al.(2022{\natexlab{b}})Zhou, Yang, Loy, and
  Liu]{zhou2021learning}
Zhou, K., Yang, J., Loy, C.~C., and Liu, Z.
\newblock Learning to prompt for vision-language models.
\newblock \emph{International Journal of Computer Vision}, 130\penalty0
  (9):\penalty0 2337--2348, 2022{\natexlab{b}}.

\end{thebibliography}
