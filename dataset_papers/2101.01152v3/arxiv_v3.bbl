\begin{thebibliography}{57}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Allen{-}Zhu and Li(2019)}]{allenzhu.kernel}
\textsc{Allen{-}Zhu, Z.} and \textsc{Li, Y.} (2019).
\newblock What can resnet learn efficiently, going beyond kernels?
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Allen-Zhu et~al.(2019)Allen-Zhu, Li and
  Song}]{allenzhu2019convergence}
\textsc{Allen-Zhu, Z.}, \textsc{Li, Y.} and \textsc{Song, Z.} (2019).
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \textit{International Conference on Machine Learning (ICML)}.

\bibitem[{Angluin and Laird(1988)}]{angluin1988rcn}
\textsc{Angluin, D.} and \textsc{Laird, P.} (1988).
\newblock Learning from noisy examples.
\newblock \textit{Machine Learning} \textbf{2} 343--370.

\bibitem[{Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, Salakhutdinov and
  Wang}]{arora2019exact}
\textsc{Arora, S.}, \textsc{Du, S.~S.}, \textsc{Hu, W.}, \textsc{Li, Z.},
  \textsc{Salakhutdinov, R.} and \textsc{Wang, R.} (2019{\natexlab{a}}).
\newblock On exact computation with an infinitely wide neural net.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li and
  Wang}]{arora2019finegrained}
\textsc{Arora, S.}, \textsc{Du, S.~S.}, \textsc{Hu, W.}, \textsc{Li, Z.} and
  \textsc{Wang, R.} (2019{\natexlab{b}}).
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \textit{International Conference on Machine Learning (ICML)}.

\bibitem[{Awasthi et~al.(2015)Awasthi, Balcan, Haghtalab and
  Urner}]{awasthi2015massart}
\textsc{Awasthi, P.}, \textsc{Balcan, M.-F.}, \textsc{Haghtalab, N.} and
  \textsc{Urner, R.} (2015).
\newblock Efficient learning of linear separators under bounded noise.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Awasthi et~al.(2016)Awasthi, Balcan, Haghtalab and
  Zhang}]{awasthi20161bitcompressednoise}
\textsc{Awasthi, P.}, \textsc{Balcan, M.-F.}, \textsc{Haghtalab, N.} and
  \textsc{Zhang, H.} (2016).
\newblock Learning and 1-bit compressed sensing under asymmetric noise.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Awasthi et~al.(2017)Awasthi, Balcan and
  Long}]{awasthi2017acm.localization}
\textsc{Awasthi, P.}, \textsc{Balcan, M.~F.} and \textsc{Long, P.~M.} (2017).
\newblock The power of localization for efficiently learning linear separators
  with noise.
\newblock \textit{J. ACM} \textbf{63}.

\bibitem[{Balcan and Haghtalab(2021)}]{balcan2020noise}
\textsc{Balcan, M.-F.} and \textsc{Haghtalab, N.} (2021).
\newblock Noise in classification.
\newblock In \textit{Beyond Worst Case Analysis of Algorithms} (T.~Roughgarden,
  ed.), chap.~16. Cambridge University Press.

\bibitem[{Blum et~al.(1998)Blum, Frieze, Kannan and Vempala}]{blum1998rcn}
\textsc{Blum, A.}, \textsc{Frieze, A.}, \textsc{Kannan, R.} and
  \textsc{Vempala, S.} (1998).
\newblock A polynomial-time algorithm for learning noisy linear threshold
  functions.
\newblock \textit{Algorithmica} \textbf{22} 35--52.

\bibitem[{Brutzkus et~al.(2018)Brutzkus, Globerson, Malach and
  Shalev-Shwartz}]{brutzkus2018sgd}
\textsc{Brutzkus, A.}, \textsc{Globerson, A.}, \textsc{Malach, E.} and
  \textsc{Shalev-Shwartz, S.} (2018).
\newblock {SGD} learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock In \textit{International Conference on Learning Representations
  (ICLR)}.

\bibitem[{Cao and Gu(2019)}]{cao2019generalizationsgd}
\textsc{Cao, Y.} and \textsc{Gu, Q.} (2019).
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Cao and Gu(2020)}]{cao2019generalization}
\textsc{Cao, Y.} and \textsc{Gu, Q.} (2020).
\newblock Generalization error bounds of gradient descent for learning
  over-parameterized deep relu networks.
\newblock In \textit{the Thirty-Fourth AAAI Conference on Artificial
  Intelligence}.

\bibitem[{Chen et~al.(2020)Chen, Cao, Gu and Zhang}]{chen2020generalized}
\textsc{Chen, Z.}, \textsc{Cao, Y.}, \textsc{Gu, Q.} and \textsc{Zhang, T.}
  (2020).
\newblock A generalized neural tangent kernel analysis for two-layer neural
  networks.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Chen et~al.(2019)Chen, Cao, Zou and Gu}]{chen.polylog}
\textsc{Chen, Z.}, \textsc{Cao, Y.}, \textsc{Zou, D.} and \textsc{Gu, Q.}
  (2019).
\newblock How much over-parameterization is sufficient to learn deep relu
  networks?
\newblock \textit{arXiv} \textbf{abs/1911.12360}.

\bibitem[{Chizat et~al.(2019)Chizat, Oyallon and Bach}]{chizat2018note}
\textsc{Chizat, L.}, \textsc{Oyallon, E.} and \textsc{Bach, F.} (2019).
\newblock On lazy training in differentiable programming.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Daniely(2016)}]{daniely2016complexity}
\textsc{Daniely, A.} (2016).
\newblock Complexity theoretic limitations on learning halfspaces.
\newblock In \textit{ACM Symposium on Theory of Computing (STOC)}.

\bibitem[{Diakonikolas et~al.(2019)Diakonikolas, Gouleakis and
  Tzamos}]{diakonikolas2019massart}
\textsc{Diakonikolas, I.}, \textsc{Gouleakis, T.} and \textsc{Tzamos, C.}
  (2019).
\newblock Distribution-independent pac learning of halfspaces with massart
  noise.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Diakonikolas et~al.(2020{\natexlab{a}})Diakonikolas, Kane, Kontonis
  and Zarifis}]{diakonikolas2020sqlowerbound}
\textsc{Diakonikolas, I.}, \textsc{Kane, D.~M.}, \textsc{Kontonis, V.} and
  \textsc{Zarifis, N.} (2020{\natexlab{a}}).
\newblock Algorithms and sq lower bounds for pac learning one-hidden-layer relu
  networks.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Diakonikolas et~al.(2020{\natexlab{b}})Diakonikolas, Kontonis, Tzamos
  and Zarifis}]{diakonikolas2020massartstructured}
\textsc{Diakonikolas, I.}, \textsc{Kontonis, V.}, \textsc{Tzamos, C.} and
  \textsc{Zarifis, N.} (2020{\natexlab{b}}).
\newblock Learning halfspaces with massart noise under structured
  distributions.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Diakonikolas et~al.(2020{\natexlab{c}})Diakonikolas, Kontonis, Tzamos
  and Zarifis}]{diakonikolas2020nonconvex}
\textsc{Diakonikolas, I.}, \textsc{Kontonis, V.}, \textsc{Tzamos, C.} and
  \textsc{Zarifis, N.} (2020{\natexlab{c}}).
\newblock Non-convex sgd learns halfspaces with adversarial label noise.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Du et~al.(2018)Du, Lee, Li, Wang and Zhai}]{du2019deep}
\textsc{Du, S.~S.}, \textsc{Lee, J.~D.}, \textsc{Li, H.}, \textsc{Wang, L.} and
  \textsc{Zhai, X.} (2018).
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \textit{International Conference on Machine Learning (ICML)}.

\bibitem[{Du et~al.(2019)Du, Zhai, P{\'{o}}czos and Singh}]{du2019-1layer}
\textsc{Du, S.~S.}, \textsc{Zhai, X.}, \textsc{P{\'{o}}czos, B.} and
  \textsc{Singh, A.} (2019).
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \textit{International Conference on Learning Representations
  (ICLR)}.

\bibitem[{Fort et~al.(2020)Fort, Dziugaite, Paul, Kharaghani, Roy and
  Ganguli}]{fort2020ntk}
\textsc{Fort, S.}, \textsc{Dziugaite, G.~K.}, \textsc{Paul, M.},
  \textsc{Kharaghani, S.}, \textsc{Roy, D.~M.} and \textsc{Ganguli, S.} (2020).
\newblock Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the neural tangent kernel.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Frei et~al.(2019)Frei, Cao and Gu}]{frei2019resnet}
\textsc{Frei, S.}, \textsc{Cao, Y.} and \textsc{Gu, Q.} (2019).
\newblock Algorithm-dependent generalization bounds for overparameterized deep
  residual networks.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Frei et~al.(2020{\natexlab{a}})Frei, Cao and
  Gu}]{frei2020singleneuron}
\textsc{Frei, S.}, \textsc{Cao, Y.} and \textsc{Gu, Q.} (2020{\natexlab{a}}).
\newblock Agnostic learning of a single neuron with gradient descent.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Frei et~al.(2020{\natexlab{b}})Frei, Cao and Gu}]{frei2020halfspace}
\textsc{Frei, S.}, \textsc{Cao, Y.} and \textsc{Gu, Q.} (2020{\natexlab{b}}).
\newblock Agnostic learning of halfspaces with gradient descent via soft
  margins.
\newblock \textit{Preprint, arXiv:2010.00539} .

\bibitem[{Goel et~al.(2020)Goel, Gollakota, Jin, Karmalkar and
  Klivans}]{goel2020superpolynomial}
\textsc{Goel, S.}, \textsc{Gollakota, A.}, \textsc{Jin, Z.}, \textsc{Karmalkar,
  S.} and \textsc{Klivans, A.} (2020).
\newblock Superpolynomial lower bounds for learning one-layer neural networks
  using gradient descent.
\newblock In \textit{International Conference on Machine Learning (ICML)}.

\bibitem[{Goel et~al.(2019)Goel, Karmalkar and Klivans}]{goel2019relugaussian}
\textsc{Goel, S.}, \textsc{Karmalkar, S.} and \textsc{Klivans, A.~R.} (2019).
\newblock Time/accuracy tradeoffs for learning a relu with respect to gaussian
  marginals.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Goodfellow et~al.(2014)Goodfellow, Shlens and
  Szegedy}]{goodfellow2014explaining}
\textsc{Goodfellow, I.~J.}, \textsc{Shlens, J.} and \textsc{Szegedy, C.}
  (2014).
\newblock Explaining and harnessing adversarial examples.
\newblock \textit{arXiv preprint arXiv:1412.6572} .

\bibitem[{Hu et~al.(2020{\natexlab{a}})Hu, Li and Yu}]{hu2020simple}
\textsc{Hu, W.}, \textsc{Li, Z.} and \textsc{Yu, D.} (2020{\natexlab{a}}).
\newblock Simple and effective regularization methods for training on noisily
  labeled data with generalization guarantee.
\newblock In \textit{International Conference on Learning Representations
  (ICLR)}.

\bibitem[{Hu et~al.(2020{\natexlab{b}})Hu, Xiao, Adlam and
  Pennington}]{hu2020surprising}
\textsc{Hu, W.}, \textsc{Xiao, L.}, \textsc{Adlam, B.} and \textsc{Pennington,
  J.} (2020{\natexlab{b}}).
\newblock The surprising simplicity of the early-time learning dynamics of
  neural networks.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Jacot et~al.(2018)Jacot, Gabriel and Hongler}]{jacot2018ntk}
\textsc{Jacot, A.}, \textsc{Gabriel, F.} and \textsc{Hongler, C.} (2018).
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Ji and Telgarsky(2019)}]{jitelgarsky2019implicit}
\textsc{Ji, Z.} and \textsc{Telgarsky, M.} (2019).
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Ji and Telgarsky(2020{\natexlab{a}})}]{ji2020directional}
\textsc{Ji, Z.} and \textsc{Telgarsky, M.} (2020{\natexlab{a}}).
\newblock Directional convergence and alignment in deep learning.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Ji and Telgarsky(2020{\natexlab{b}})}]{jitelgarsky20.polylog}
\textsc{Ji, Z.} and \textsc{Telgarsky, M.} (2020{\natexlab{b}}).
\newblock Polylogarithmic width suffices for gradient descent to achieve
  arbitrarily small test error with shallow relu networks.
\newblock In \textit{International Conference on Learning Representations
  (ICLR)}.

\bibitem[{Kearns et~al.(1994)Kearns, Schapire and Sellie}]{kearns.agnostic}
\textsc{Kearns, M.~J.}, \textsc{Schapire, R.~E.} and \textsc{Sellie, L.~M.}
  (1994).
\newblock Toward efficient agnostic learning.
\newblock \textit{Machine Learning} \textbf{17} 115--141.

\bibitem[{Klivans et~al.(2009)Klivans, Long and
  Servedio}]{klivans2009maliciousnoise}
\textsc{Klivans, A.~R.}, \textsc{Long, P.~M.} and \textsc{Servedio, R.~A.}
  (2009).
\newblock Learning halfspaces with malicious noise.
\newblock \textit{Journal of Machine Learning Research (JMLR)} \textbf{10}
  2715--2740.

\bibitem[{Li et~al.(2019{\natexlab{a}})Li, Soltanolkotabi and
  Oymak}]{li2019labelnoise}
\textsc{Li, M.}, \textsc{Soltanolkotabi, M.} and \textsc{Oymak, S.}
  (2019{\natexlab{a}}).
\newblock Gradient descent with early stopping is provably robust to label
  noise for overparameterized neural networks.
\newblock In \textit{Conference on Artificial Intelligence and Statistics
  (AISTATS)}.

\bibitem[{Li et~al.(2020{\natexlab{a}})Li, Ma and Zhang}]{li2020relubeyondntk}
\textsc{Li, Y.}, \textsc{Ma, T.} and \textsc{Zhang, H.~R.}
  (2020{\natexlab{a}}).
\newblock Learning over-parametrized two-layer relu neural networks beyond ntk.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Li et~al.(2019{\natexlab{b}})Li, Wei and
  Ma}]{li2020largelearningrate}
\textsc{Li, Y.}, \textsc{Wei, C.} and \textsc{Ma, T.} (2019{\natexlab{b}}).
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Li et~al.(2020{\natexlab{b}})Li, X.Fang, Xu and
  Zhao}]{li2020implicitadversarial}
\textsc{Li, Y.}, \textsc{X.Fang, E.}, \textsc{Xu, H.} and \textsc{Zhao, T.}
  (2020{\natexlab{b}}).
\newblock Implicit bias of gradient descent based adversarial training on
  separable data.
\newblock In \textit{International Conference on Learning Representations
  (ICLR)}.

\bibitem[{Lyu and Li(2020)}]{lyuli2020implicitbias}
\textsc{Lyu, K.} and \textsc{Li, J.} (2020).
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock In \textit{International Conference on Learning Representations
  (ICLR)}.

\bibitem[{Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras and
  Vladu}]{madry2018adversarial}
\textsc{Madry, A.}, \textsc{Makelov, A.}, \textsc{Schmidt, L.},
  \textsc{Tsipras, D.} and \textsc{Vladu, A.} (2018).
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \textit{International Conference on Learning Representations
  (ICLR)}.

\bibitem[{Massart et~al.(2006)Massart, N{\'e}d{\'e}lec
  et~al.}]{massart2006noise}
\textsc{Massart, P.}, \textsc{N{\'e}d{\'e}lec, {\'E}.} \textsc{et~al.} (2006).
\newblock Risk bounds for statistical learning.
\newblock \textit{The Annals of Statistics} \textbf{34} 2326--2366.

\bibitem[{Mei et~al.(2019)Mei, Misiakiewicz and Montanari}]{mei2019mean}
\textsc{Mei, S.}, \textsc{Misiakiewicz, T.} and \textsc{Montanari, A.} (2019).
\newblock Mean-field theory of two-layers neural networks: dimension-free
  bounds and kernel limit.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Moroshko et~al.(2020)Moroshko, Gunasekar, Woodworth, Lee, Srebro and
  Soudry}]{moroshko2020implicit}
\textsc{Moroshko, E.}, \textsc{Gunasekar, S.}, \textsc{Woodworth, B.},
  \textsc{Lee, J.~D.}, \textsc{Srebro, N.} and \textsc{Soudry, D.} (2020).
\newblock Implicit bias in deep linear classification: Initialization scale vs
  training accuracy.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Nakkiran et~al.(2019)Nakkiran, Kaplun, Kalimeris, Yang, Edelman,
  Zhang and Barak}]{nakkiran2019sgd}
\textsc{Nakkiran, P.}, \textsc{Kaplun, G.}, \textsc{Kalimeris, D.},
  \textsc{Yang, T.}, \textsc{Edelman, B.~L.}, \textsc{Zhang, F.} and
  \textsc{Barak, B.} (2019).
\newblock Sgd on neural networks learns functions of increasing complexity.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Shah et~al.(2020)Shah, Tamuly, Raghunathan, Jain and
  Netrapalli}]{shah2020pitfalls}
\textsc{Shah, H.}, \textsc{Tamuly, K.}, \textsc{Raghunathan, A.}, \textsc{Jain,
  P.} and \textsc{Netrapalli, P.} (2020).
\newblock The pitfalls of simplicity bias in neural networks.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Shalev-Shwartz and Ben-David(2014)}]{shalevschwartz}
\textsc{Shalev-Shwartz, S.} and \textsc{Ben-David, S.} (2014).
\newblock \textit{Understanding Machine Learning: From Theory to Algorithms}.
\newblock Cambridge University Press, New York, NY, USA.

\bibitem[{Shamir(2018)}]{shamir2018resnetslinear}
\textsc{Shamir, O.} (2018).
\newblock Are resnets provably better than linear predictors?
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar and
  Srebro}]{soudry2018implicitbias}
\textsc{Soudry, D.}, \textsc{Hoffer, E.}, \textsc{Nacson, M.~S.},
  \textsc{Gunasekar, S.} and \textsc{Srebro, N.} (2018).
\newblock The implicit bias of gradient descent on separable data.
\newblock \textit{Journal of Machine Learning Research (JMLR)} \textbf{19}
  1--57.

\bibitem[{Wei et~al.(2019)Wei, Lee, Liu and Ma}]{wei2020regularization}
\textsc{Wei, C.}, \textsc{Lee, J.~D.}, \textsc{Liu, Q.} and \textsc{Ma, T.}
  (2019).
\newblock Regularization matters: Generalization and optimization of neural
  nets v.s. their induced kernel.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese,
  Golan, Soudry and Srebro}]{woodworth2020kernelrich}
\textsc{Woodworth, B.}, \textsc{Gunasekar, S.}, \textsc{Lee, J.~D.},
  \textsc{Moroshko, E.}, \textsc{Savarese, P.}, \textsc{Golan, I.},
  \textsc{Soudry, D.} and \textsc{Srebro, N.} (2020).
\newblock Kernel and rich regimes in overparametrized models.
\newblock In \textit{Conference on Learning Theory (COLT)}.

\bibitem[{Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht and
  Vinyals}]{zhang2017rethinkinggeneralization}
\textsc{Zhang, C.}, \textsc{Bengio, S.}, \textsc{Hardt, M.}, \textsc{Recht, B.}
  and \textsc{Vinyals, O.} (2017).
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \textit{International Conference on Learning Representations
  (ICLR)}.

\bibitem[{Zou et~al.(2019)Zou, Cao, Zhou and Gu}]{zou2019gradient}
\textsc{Zou, D.}, \textsc{Cao, Y.}, \textsc{Zhou, D.} and \textsc{Gu, Q.}
  (2019).
\newblock Gradient descent optimizes over-parameterized deep {ReLU} networks.
\newblock \textit{Machine Learning} .

\bibitem[{Zou and Gu(2019)}]{zou2019improved}
\textsc{Zou, D.} and \textsc{Gu, Q.} (2019).
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock In \textit{Advances in Neural Information Processing Systems
  (NeurIPS)}.

\end{thebibliography}
