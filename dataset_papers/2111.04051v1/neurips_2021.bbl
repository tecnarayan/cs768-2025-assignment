\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Wu et~al.(2020)Wu, Zhou, Liu, Yuan, Wang, Huang, and Wu]{wu2020multi}
Tong Wu, Pan Zhou, Kai Liu, Yali Yuan, Xiumin Wang, Huawei Huang, and
  Dapeng~Oliver Wu.
\newblock Multi-agent deep reinforcement learning for urban traffic light
  control in vehicular networks.
\newblock \emph{IEEE Transactions on Vehicular Technology}, 69\penalty0
  (8):\penalty0 8243--8256, 2020.

\bibitem[Sharma and Chauhan(2020)]{sharma2020distributed}
Anamika Sharma and Siddhartha Chauhan.
\newblock A distributed reinforcement learning based sensor node scheduling
  algorithm for coverage and connectivity maintenance in wireless sensor
  network.
\newblock \emph{Wireless Networks}, 26\penalty0 (6):\penalty0 4411--4429, 2020.

\bibitem[Yu et~al.(2019)Yu, Wang, Xu, Zhang, Ge, Ren, Sun, Chen, and
  Tan]{yu2019distributed}
Chao Yu, Xin Wang, Xin Xu, Minjie Zhang, Hongwei Ge, Jiankang Ren, Liang Sun,
  Bingcai Chen, and Guozhen Tan.
\newblock Distributed multiagent coordinated learning for autonomous driving in
  highways based on dynamic coordination graphs.
\newblock \emph{IEEE Transactions on Intelligent Transportation Systems},
  21\penalty0 (2):\penalty0 735--748, 2019.

\bibitem[Hernandez-Leal et~al.(2017)Hernandez-Leal, Kaisers, Baarslag, and
  de~Cote]{HernandezLeal2017ASO}
Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique~Munoz de~Cote.
\newblock A survey of learning in multiagent environments: Dealing with
  non-stationarity.
\newblock \emph{ArXiv}, abs/1707.09183, 2017.

\bibitem[Papoudakis et~al.(2019)Papoudakis, Christianos, Rahman, and
  Albrecht]{papoudakis2019dealing}
Georgios Papoudakis, Filippos Christianos, Arrasy Rahman, and Stefano~V.
  Albrecht.
\newblock Dealing with non-stationarity in multi-agent deep reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1906.04737}, 2019.

\bibitem[Foerster et~al.(2016)Foerster, Assael, de~Freitas, and
  Whiteson]{Foerster2016LearningTC}
Jakob Foerster, Ioannis~Alexandros Assael, Nando de~Freitas, and Shimon
  Whiteson.
\newblock Learning to communicate with deep multi-agent reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  29:\penalty0 2137--2145, 2016.

\bibitem[Lowe et~al.(2017)Lowe, Wu, Tamar, Harb, Pieter~Abbeel, and
  Mordatch]{Lowe2017MultiAgentAF}
Ryan Lowe, Yi~Wu, Aviv Tamar, Jean Harb, OpenAI Pieter~Abbeel, and Igor
  Mordatch.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock \emph{Advances in Neural Information Processing Systems},
  30:\penalty0 6379--6390, 2017.

\bibitem[Foerster et~al.(2018)Foerster, Farquhar, Afouras, Nardelli, and
  Whiteson]{foerster2018counterfactual}
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and
  Shimon Whiteson.
\newblock Counterfactual multi-agent policy gradients.
\newblock In \emph{Proceedings of the Conference of Association for the
  Advancement of Artificial Intelligence}, volume~32, page 2974â€“2982, 2018.

\bibitem[Wang et~al.(2020)Wang, Han, Wang, Dong, and Zhang]{wang2020off}
Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang.
\newblock Dop: Off-policy multi-agent decomposed policy gradients.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Yu et~al.(2021)Yu, Velu, Vinitsky, Wang, Bayen, and Wu]{Yu2021TheSE}
Chao Yu, Akash Velu, Eugene Vinitsky, Yu~Wang, A.~Bayen, and Yi~Wu.
\newblock The surprising effectiveness of mappo in cooperative, multi-agent
  games.
\newblock \emph{ArXiv}, abs/2103.01955, 2021.

\bibitem[Sunehag et~al.(2018)Sunehag, Lever, Gruslys, Czarnecki, Zambaldi,
  Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls,
  et~al.]{Sunehag2018ValueDecompositionNF}
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech~Marian Czarnecki, Vinicius
  Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel~Z Leibo, Karl
  Tuyls, et~al.
\newblock Value-decomposition networks for cooperative multi-agent learning
  based on team reward.
\newblock In \emph{International Conference on Autonomous Agents and MultiAgent
  Systems}, pages 2085--2087, 2018.

\bibitem[Rashid et~al.(2018)Rashid, Samvelyan, Schroeder, Farquhar, Foerster,
  and Whiteson]{Rashid2018QMIXMV}
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob
  Foerster, and Shimon Whiteson.
\newblock Qmix: Monotonic value function factorisation for deep multi-agent
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  4295--4304, 2018.

\bibitem[Son et~al.(2019)Son, Kim, Kang, Hostallero, and Yi]{Son2019QTRANLT}
Kyunghwan Son, Daewoo Kim, Wan~Ju Kang, David~Earl Hostallero, and Yung Yi.
\newblock Qtran: Learning to factorize with transformation for cooperative
  multi-agent reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  5887--5896, 2019.

\bibitem[Mahajan et~al.(2019)Mahajan, Rashid, Samvelyan, and
  Whiteson]{Mahajan2019MAVENMV}
Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson.
\newblock Maven: Multi-agent variational exploration.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 7613--7624, 2019.

\bibitem[Gupta et~al.(2017)Gupta, Egorov, and
  Kochenderfer]{gupta2017cooperative}
Jayesh~K Gupta, Maxim Egorov, and Mykel Kochenderfer.
\newblock Cooperative multi-agent control using deep reinforcement learning.
\newblock In \emph{International Conference on Autonomous Agents and Multiagent
  Systems}, pages 66--83, 2017.

\bibitem[Song et~al.(2019)Song, Wang, and Zhang]{song2019convergence}
Xinliang Song, Tonghan Wang, and Chongjie Zhang.
\newblock Convergence of multi-agent learning with a finite step size in
  general-sum games.
\newblock In \emph{International Conference on Autonomous Agents and MultiAgent
  Systems}, pages 935--943, 2019.

\bibitem[Deisenroth et~al.(2013)Deisenroth, Neumann, Peters,
  et~al.]{deisenroth2013survey}
Marc~Peter Deisenroth, Gerhard Neumann, Jan Peters, et~al.
\newblock A survey on policy search for robotics.
\newblock \emph{Foundations and trends in Robotics}, 2\penalty0 (1-2):\penalty0
  388--403, 2013.

\bibitem[Su et~al.(2021)Su, Adams, and Beling]{su2020value}
Jianyu Su, Stephen Adams, and Peter Beling.
\newblock Value-decomposition multi-agent actor-critics.
\newblock In \emph{Proceedings of the Conference of Association for the
  Advancement of Artificial Intelligence}, volume~35, pages 11352--11360, 2021.

\bibitem[Samvelyan et~al.(2019)Samvelyan, Rashid, Schroeder~de Witt, Farquhar,
  Nardelli, Rudner, Hung, Torr, Foerster, and Whiteson]{Samvelyan2019TheSM}
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder~de Witt, Gregory
  Farquhar, Nantas Nardelli, Tim~GJ Rudner, Chia-Man Hung, Philip~HS Torr,
  Jakob Foerster, and Shimon Whiteson.
\newblock The starcraft multi-agent challenge.
\newblock In \emph{International Conference on Autonomous Agents and MultiAgent
  Systems}, pages 2186--2188, 2019.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{sutton1999policy}
Richard~S Sutton, David~A McAllester, Satinder~P Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1057--1063, 2000.

\bibitem[Grondman et~al.(2012)Grondman, Busoniu, Lopes, and
  Babuska]{grondman2012survey}
Ivo Grondman, Lucian Busoniu, Gabriel~AD Lopes, and Robert Babuska.
\newblock A survey of actor-critic reinforcement learning: Standard and natural
  policy gradients.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics, Part C
  (Applications and Reviews)}, 42\penalty0 (6):\penalty0 1291--1307, 2012.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and
  Meger]{fujimoto2018addressing}
Scott Fujimoto, Herke Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning}, pages
  1587--1596, 2018.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{Schulman2017ProximalPO}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{ArXiv}, abs/1707.06347, 2017.

\bibitem[Claus and Boutilier(1998)]{claus1998dynamics}
Caroline Claus and Craig Boutilier.
\newblock The dynamics of reinforcement learning in cooperative multiagent
  systems.
\newblock \emph{Proceedings of the fifteenth national/tenth conference on
  Artificial intelligence/Innovative applications of artificial intelligence},
  1998\penalty0 (746-752):\penalty0 2, 1998.

\bibitem[Oliehoek and Amato(2016)]{Oliehoek2016ACI}
Frans~A Oliehoek and Christopher Amato.
\newblock \emph{A concise introduction to decentralized POMDPs}.
\newblock Springer, 2016.

\bibitem[Schulman et~al.(2015{\natexlab{a}})Schulman, Levine, Abbeel, Jordan,
  and Moritz]{Schulman2015TrustRP}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1889--1897, 2015{\natexlab{a}}.

\bibitem[Kakade and Langford(2002)]{Kakade2002ApproximatelyOA}
Sham Kakade and John Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  267--274, 2002.

\bibitem[Ye et~al.(2020{\natexlab{a}})Ye, Liu, Sun, Shi, Zhao, Wu, Yu, Yang,
  Wu, Guo, et~al.]{ye2020mastering}
Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu,
  Shaojie Yang, Xipeng Wu, Qingwei Guo, et~al.
\newblock Mastering complex control in moba games with deep reinforcement
  learning.
\newblock In \emph{Proceedings of the Conference of Association for the
  Advancement of Artificial Intelligence}, pages 6672--6679,
  2020{\natexlab{a}}.

\bibitem[Ye et~al.(2020{\natexlab{b}})Ye, Chen, Zhang, Chen, Yuan, Liu, Chen,
  Liu, Qiu, Yu, Yin, Shi, Wang, Shi, Fu, Yang, Huang, and Liu]{fullmoba}
Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo~Yuan, Bo~Liu, Jia Chen, Zhao
  Liu, Fuhao Qiu, Hongsheng Yu, Yinyuting Yin, Bei Shi, Liang Wang, Tengfei
  Shi, Qiang Fu, Wei Yang, Lanxiao Huang, and Wei Liu.
\newblock Towards playing full {MOBA} games with deep reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  21--632, 2020{\natexlab{b}}.

\bibitem[Spiros and Daniel(2002)]{spiros2002reinforcement}
K~Spiros and K~Daniel.
\newblock Reinforcement learning of coordination in cooperative mas.
\newblock In \emph{The 18th National Conference on Artificial Intelligence},
  pages 326--331, 2002.

\bibitem[Tampuu et~al.(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus, Aru,
  Aru, and Vicente]{tampuu2017multagent}
Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus,
  Juhan Aru, Jaan Aru, and Raul Vicente.
\newblock Multiagent cooperation and competition with deep reinforcement
  learning.
\newblock \emph{PloS one}, 12\penalty0 (4):\penalty0 e0172395, 2017.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{Lillicrap2016ContinuousCW}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Li and He(2020)]{Li2020MultiAgentTR}
Hepeng Li and Haibo He.
\newblock Multi-agent trust region policy optimization.
\newblock \emph{ArXiv}, abs/2010.07916, 2020.

\bibitem[Wen et~al.(2020)Wen, Chen, Yang, Tian, Li, Chen, and
  Wang]{wen2021multagent}
Ying Wen, Hui Chen, Yaodong Yang, Zheng Tian, Minne Li, Xu~Chen, and Jun Wang.
\newblock Multi-agent trust region learning.
\newblock 2020.
\newblock URL \url{https://openreview.net/forum?id=eHG7asK_v-k}.

\bibitem[Levin and Peres(2017)]{levin2017markov}
David~A Levin and Yuval Peres.
\newblock \emph{Markov chains and mixing times}, volume 107.
\newblock American Mathematical Soc., 2017.

\bibitem[Pollard(2000)]{Pollard2000Asymptopia}
David Pollard.
\newblock Asymptopia: an exposition of statistical asymptotic theory.
\newblock 2000.
\newblock URL \url{http://www. stat. yale. edu/pollard/Books/Asymptopia}.

\bibitem[Schulman et~al.(2015{\natexlab{b}})Schulman, Moritz, Levine, Jordan,
  and Abbeel]{Schulman2016HighDimensionalCC}
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter
  Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock \emph{arXiv preprint arXiv:1506.02438}, 2015{\natexlab{b}}.

\end{thebibliography}
