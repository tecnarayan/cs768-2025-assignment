\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Badanidiyuru and Vondr{\'{a}}k(2014)]{badanidiyuru2014fast}
Ashwinkumar Badanidiyuru and Jan Vondr{\'{a}}k.
\newblock {Fast algorithms for maximizing submodular functions}.
\newblock In \emph{Symposium on Discrete Algorithms, {SODA}}, pages 1497--1514,
  2014.

\bibitem[Badanidiyuru et~al.(2014)Badanidiyuru, Mirzasoleiman, Karbasi, and
  Krause]{badanidiyuru2014streaming}
Ashwinkumar Badanidiyuru, Baharan Mirzasoleiman, Amin Karbasi, and Andreas
  Krause.
\newblock {Streaming Submodular Maximization:Massive Data Summarization on the
  Fly}.
\newblock In \emph{International Conference on Knowledge Discovery and Data
  Mining, {KDD}}, pages 671--680, 2014.

\bibitem[Balkanski and Singer(2018)]{balkanski2018adaptive}
Eric Balkanski and Yaron Singer.
\newblock {The adaptive complexity of maximizing a submodular function}.
\newblock In \emph{{ACM} {SIGACT} Symposium on Theory of Computing, {STOC}},
  pages 1138--1151, 2018.

\bibitem[Balkanski et~al.(2016)Balkanski, Mirzasoleiman, Krause, and
  Singer]{balkanski2016tlearning}
Eric Balkanski, Baharan Mirzasoleiman, Andreas Krause, and Yaron Singer.
\newblock Learning sparse combinatorial representations via two-stage
  submodular maximization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2016.

\bibitem[Balkanski et~al.(2018)Balkanski, Breuer, and
  Singer]{balkanski2018nonmonotone}
Eric Balkanski, Adam Breuer, and Yaron Singer.
\newblock {Non-monotone Submodular Maximization in Exponentially Fewer
  Iterations}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2359--2370, 2018.

\bibitem[Balkanski et~al.(2019)Balkanski, Rubinstein, and
  Singer]{balkanski2018exponential}
Eric Balkanski, Aviad Rubinstein, and Yaron Singer.
\newblock {An Exponential Speedup in Parallel Running Time for Submodular
  Maximization without Loss in Approximation}.
\newblock In \emph{Symposium on Discrete Algorithms (SODA)}, pages 283--302,
  2019.

\bibitem[Bansal and Sviridenko(2006)]{bansal2006santa}
Nikhil Bansal and Maxim Sviridenko.
\newblock The santa claus problem.
\newblock In \emph{Proceedings of the Thirty-Eighth Annual ACM Symposium on
  Theory of Computing}, pages 31--40. ACM, 2006.

\bibitem[Barbosa et~al.(2015)Barbosa, Ene, Nguyen, and Ward]{barbosa2015power}
Rafael Barbosa, Alina Ene, Huy Nguyen, and Justin Ward.
\newblock The power of randomization: Distributed submodular maximization on
  massive datasets.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  1236--1244, 2015.

\bibitem[Barbosa et~al.(2016)Barbosa, Ene, Nguyen, and Ward]{barbosa2016new}
Rafael Barbosa, Alina Ene, Huy~L. Nguyen, and Justin Ward.
\newblock {A New Framework for Distributed Submodular Maximization}.
\newblock In \emph{Annual Symposium on Foundations of Computer Science,
  {FOCS}}, pages 645--654, 2016.

\bibitem[Buchbinder et~al.(2015)Buchbinder, Feldman, and
  Schwartz]{buchbinder2015online}
Niv Buchbinder, Moran Feldman, and Roy Schwartz.
\newblock Online submodular maximization with preemption.
\newblock In \emph{{ACM-SIAM} Symposium on Discrete Algorithms, {SODA}}, pages
  1202--1216, 2015.

\bibitem[Buchbinder et~al.(2017)Buchbinder, Feldman, and
  Schwartz]{buchbinder2017comparing}
Niv Buchbinder, Moran Feldman, and Roy Schwartz.
\newblock {Comparing Apples and Oranges: Query Trade-off in Submodular
  Maximization}.
\newblock \emph{Math. Oper. Res.}, 42\penalty0 (2):\penalty0 308--329, 2017.

\bibitem[Chakrabarti and Kale(2015)]{chakrabarti2015submodular}
Amit Chakrabarti and Sagar Kale.
\newblock {Submodular maximization meets streaming: matchings, matroids, and
  more}.
\newblock \emph{Math. Program.}, 154\penalty0 (1-2):\penalty0 225--247, 2015.

\bibitem[Chan et~al.(2016)Chan, Huang, Jiang, Kang, and Tang]{chan2017online}
TH~Chan, Zhiyi Huang, Shaofeng H-C Jiang, Ning Kang, and Zhihao~Gavin Tang.
\newblock Online submodular maximization with free disposal: Randomization
  beats 0.25 for partition matroids.
\newblock 2016.

\bibitem[Chekuri and Quanrud(2018)]{chekuri2018parallelizing}
Chandra Chekuri and Kent Quanrud.
\newblock {Parallelizing greedy for submodular set function maximization in
  matroids and beyond}.
\newblock \emph{CoRR}, abs/1811.12568, 2018.

\bibitem[Chekuri et~al.(2015)Chekuri, Gupta, and Quanrud]{chekuri2015streaming}
Chandra Chekuri, Shalmoli Gupta, and Kent Quanrud.
\newblock Streaming algorithms for submodular function maximization.
\newblock In \emph{International Colloquium on Automata, Languages, and
  Programming}, pages 318--330. Springer, 2015.

\bibitem[Chen et~al.(2016)Chen, Nguyen, and Zhang]{chen2016submodular}
Jiecao Chen, Huy~L. Nguyen, and Qin Zhang.
\newblock {Submodular Maximization over Sliding Windows}.
\newblock \emph{CoRR}, abs/1611.00129, 2016.

\bibitem[Chen et~al.(2018)Chen, Feldman, and Karbasi]{chen2018unconstrained}
Lin Chen, Moran Feldman, and Amin Karbasi.
\newblock Unconstrained submodular maximization with constant adaptive
  complexity.
\newblock \emph{CoRR}, abs/1811.06603, 2018.

\bibitem[Das and Kempe(2011)]{das2011submodular}
Abhimanyu Das and David Kempe.
\newblock {Submodular meets Spectral: Greedy Algorithms for Subset Selection,
  Sparse Approximation and Dictionary Selection}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  1057--1064, 2011.

\bibitem[Elenberg et~al.(2017)Elenberg, Dimakis, Feldman, and
  Karbasi]{elenberg2017streaming}
Ethan~R. Elenberg, Alexandros~G. Dimakis, Moran Feldman, and Amin Karbasi.
\newblock {Streaming Weak Submodularity: Interpreting Neural Networks on the
  Fly}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4047--4057, 2017.

\bibitem[Ene and Nguyen(2019)]{ene2018submodular}
Alina Ene and Huy~L. Nguyen.
\newblock {Submodular Maximization with Nearly-optimal Approximation and
  Adaptivity in Nearly-linear Time}.
\newblock In \emph{Symposium on Discrete Algorithms (SODA)}, pages 274--282,
  2019.

\bibitem[Epasto et~al.(2017)Epasto, Lattanzi, Vassilvitskii, and
  Zadimoghaddam]{epasto2017submodular}
Alessandro Epasto, Silvio Lattanzi, Sergei Vassilvitskii, and Morteza
  Zadimoghaddam.
\newblock {Submodular Optimization Over Sliding Windows}.
\newblock In \emph{WWW}, pages 421--430, 2017.

\bibitem[Fahrbach et~al.(2018)Fahrbach, Mirrokni, and
  Zadimoghaddam]{fahrbach2018nonmonotone}
Matthew Fahrbach, Vahab~S. Mirrokni, and Morteza Zadimoghaddam.
\newblock {Non-monotone Submodular Maximization with Nearly Optimal Adaptivity
  Complexity}.
\newblock \emph{CoRR}, abs/1808.06932, 2018.

\bibitem[Fahrbach et~al.(2019)Fahrbach, Mirrokni, and
  Zadimoghaddam]{fahrbach2018submodular}
Matthew Fahrbach, Vahab~S. Mirrokni, and Morteza Zadimoghaddam.
\newblock {Submodular Maximization with Nearly Optimal Approximation,
  Adaptivity and Query Complexity}.
\newblock In \emph{Symposium on Discrete Algorithms (SODA)}, pages 255--273,
  2019.

\bibitem[Feldman et~al.(2017)Feldman, Harshaw, and Karbasi]{feldman2017greed}
Moran Feldman, Christopher Harshaw, and Amin Karbasi.
\newblock {Greed Is Good: Near-Optimal Submodular Maximization via Greedy
  Optimization}.
\newblock In \emph{{Conference on Learning Theory}}, 2017.

\bibitem[Feldman et~al.(2018)Feldman, Karbasi, and Kazemi]{feldman2018do}
Moran Feldman, Amin Karbasi, and Ehsan Kazemi.
\newblock {Do Less, Get More: Streaming Submodular Maximization with
  Subsampling}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  730--740, 2018.

\bibitem[Herbrich et~al.(2003)Herbrich, Lawrence, and Seeger]{herbrich2003fast}
Ralf Herbrich, Neil~D Lawrence, and Matthias Seeger.
\newblock Fast sparse gaussian process methods: The informative vector machine.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  625--632, 2003.

\bibitem[Kazemi et~al.(2018)Kazemi, Zadimoghaddam, and
  Karbasi]{kazemi2018scalable}
Ehsan Kazemi, Morteza Zadimoghaddam, and Amin Karbasi.
\newblock {Scalable Deletion-Robust Submodular Maximization: Data Summarization
  with Privacy and Fairness Constraints}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  2549--2558, 2018.

\bibitem[Krause and Golovin(2012)]{krause12survey}
Andreas Krause and Daniel Golovin.
\newblock {Submodular Function Maximization}.
\newblock In \emph{Tractability: Practical Approaches to Hard Problems}.
  Cambridge University Press, 2012.

\bibitem[Kumar et~al.(2015)Kumar, Moseley, Vassilvitskii, and
  Vattani]{kumar2015fast}
Ravi Kumar, Benjamin Moseley, Sergei Vassilvitskii, and Andrea Vattani.
\newblock {Fast Greedy Algorithms in MapReduce and Streaming}.
\newblock \emph{{TOPC}}, 2\penalty0 (3):\penalty0 14:1--14:22, 2015.

\bibitem[Liu and Vondr{\'{a}}k(2018)]{liu2018submodular}
Paul Liu and Jan Vondr{\'{a}}k.
\newblock {Submodular Optimization in the MapReduce Model}.
\newblock \emph{CoRR}, abs/1810.01489, 2018.

\bibitem[Mirrokni and Zadimoghaddam(2015)]{mirrokni2015randomized}
Vahab Mirrokni and Morteza Zadimoghaddam.
\newblock {Randomized composable core-sets for distributed submodular
  maximization}.
\newblock In \emph{ACM on Symposium on Theory of Computing, , {STOC}}, pages
  153--162. ACM, 2015.

\bibitem[Mirzasoleiman et~al.(2015)Mirzasoleiman, Badanidiyuru, Karbasi,
  Vondrak, and Krause]{mirzasoleiman2015lazier}
Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrak, and
  Andreas Krause.
\newblock {Lazier than Lazy Greedy}.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, pages
  1812--1818, 2015.

\bibitem[Mirzasoleiman et~al.(2016{\natexlab{a}})Mirzasoleiman, Badanidiyuru,
  and Karbasi]{mirzasoleiman2016fast}
Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, and Amin Karbasi.
\newblock Fast constrained submodular maximization: Personalized data
  summarization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  1358--1367, 2016{\natexlab{a}}.

\bibitem[Mirzasoleiman et~al.(2016{\natexlab{b}})Mirzasoleiman, Karbasi,
  Sarkar, and Krause]{mirzasoleiman16distributed}
Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause.
\newblock {Distributed Submodular Maximization}.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 17:\penalty0
  1--44, 2016{\natexlab{b}}.

\bibitem[Mirzasoleiman et~al.(2016{\natexlab{c}})Mirzasoleiman, Zadimoghaddam,
  and Karbasi]{mirzasoleiman2016cover}
Baharan Mirzasoleiman, Morteza Zadimoghaddam, and Amin Karbasi.
\newblock {Fast Distributed Submodular Cover: Public-Private Data
  Summarization}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2016{\natexlab{c}}.

\bibitem[Mirzasoleiman et~al.(2017)Mirzasoleiman, Karbasi, and
  Krause]{mirzasoleiman2017deletion}
Baharan Mirzasoleiman, Amin Karbasi, and Andreas Krause.
\newblock {Deletion-Robust Submodular Maximization: Data Summarization with
  ``the Right to be Forgotten''}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  2449--2458, 2017.

\bibitem[Mirzasoleiman et~al.(2018)Mirzasoleiman, Jegelka, and
  Krause]{mirzasoleiman2018streaming}
Baharan Mirzasoleiman, Stefanie Jegelka, and Andreas Krause.
\newblock {Streaming Non-Monotone Submodular Maximization: Personalized Video
  Summarization on the Fly}.
\newblock In \emph{{AAAI} Conference on Artificial Intelligence}, 2018.

\bibitem[Mitrovic et~al.(2017{\natexlab{a}})Mitrovic, Bun, Krause, and
  Karbasi]{mitrovic2017differentially}
Marko Mitrovic, Mark Bun, Andreas Krause, and Amin Karbasi.
\newblock {Differentially Private Submodular Maximization: Data Summarization
  in Disguise}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  2478--2487, 2017{\natexlab{a}}.

\bibitem[Mitrovic et~al.(2018)Mitrovic, Kazemi, Zadimoghaddam, and
  Karbasi]{mitrovic2018data}
Marko Mitrovic, Ehsan Kazemi, Morteza Zadimoghaddam, and Amin Karbasi.
\newblock {Data Summarization at Scale: {A} Two-Stage Submodular Approach}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  3593--3602, 2018.

\bibitem[Mitrovic et~al.(2017{\natexlab{b}})Mitrovic, Bogunovic, Norouzi-Fard,
  Tarnawski, and Cevher]{mitrovic2017streaming}
Slobodan Mitrovic, Ilija Bogunovic, Ashkan Norouzi-Fard, Jakub~M Tarnawski, and
  Volkan Cevher.
\newblock {Streaming Robust Submodular Maximization: A Partitioned Thresholding
  Approach}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4560--4569, 2017{\natexlab{b}}.

\bibitem[Nemhauser et~al.(1978)Nemhauser, Wolsey, and
  Fisher]{nemhauser1978analysis}
George~L Nemhauser, Laurence~A Wolsey, and Marshall~L Fisher.
\newblock {An analysis of approximations for maximizing submodular set
  functions-I}.
\newblock \emph{Mathematical programming}, 14\penalty0 (1):\penalty0 265--294,
  1978.

\bibitem[Norouzi{-}Fard et~al.(2018)Norouzi{-}Fard, Tarnawski, Mitrovic,
  Zandieh, Mousavifar, and Svensson]{norouzifard2018beyond}
Ashkan Norouzi{-}Fard, Jakub Tarnawski, Slobodan Mitrovic, Amir Zandieh,
  Aidasadat Mousavifar, and Ola Svensson.
\newblock {Beyond 1/2-Approximation for Submodular Maximization on Massive Data
  Streams}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  3826--3835, 2018.

\bibitem[Stan et~al.(2017)Stan, Zadimoghaddam, Krause, and
  Karbasi]{stan2017probabilistic}
Serban Stan, Morteza Zadimoghaddam, Andreas Krause, and Amin Karbasi.
\newblock {Probabilistic Submodular Maximization in Sub-Linear Time}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\end{thebibliography}
