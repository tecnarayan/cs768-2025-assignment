\begin{thebibliography}{}

\bibitem[Bengio et~al., 2013]{bengio2013}
Bengio, Y., L{\'{e}}onard, N., and Courville, A.~C. (2013).
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock {\em CoRR}, abs/1308.3432.

\bibitem[Burda et~al., 2016]{burda2016}
Burda, Y., Grosse, R.~B., and Salakhutdinov, R. (2016).
\newblock Importance weighted autoencoders.
\newblock In {\em 4th International Conference on Learning Representations}.

\bibitem[Cao et~al., 2019]{cao2019}
Cao, N.~D., Aziz, W., and Titov, I. (2019).
\newblock Block neural autoregressive flow.
\newblock In {\em Proceedings of the Thirty-Fifth Conference on Uncertainty in
  Artificial Intelligence}, page 511.

\bibitem[Chen et~al., 2018]{chen2018}
Chen, X., Mishra, N., Rohaninejad, M., and Abbeel, P. (2018).
\newblock Pixelsnail: An improved autoregressive generative model.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning}, pages 863--871.

\bibitem[Child et~al., 2019]{child2019}
Child, R., Gray, S., Radford, A., and Sutskever, I. (2019).
\newblock Generating long sequences with sparse transformers.
\newblock {\em CoRR}, abs/1904.10509.

\bibitem[Dinh et~al., 2017]{dinh2017}
Dinh, L., Sohl-Dickstein, J., and Bengio, S. (2017).
\newblock {Density estimation using Real NVP}.
\newblock In {\em 5th International Conference on Learning Representations}.

\bibitem[Durkan et~al., 2019]{durkan2019}
Durkan, C., Bekasov, A., Murray, I., and Papamakarios, G. (2019).
\newblock Neural spline flows.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7509--7520.

\bibitem[Germain et~al., 2015]{germain2015}
Germain, M., Gregor, K., Murray, I., and Larochelle, H. (2015).
\newblock Made: Masked autoencoder for distribution estimation.
\newblock In {\em Proceedings of the 32nd International Conference on Machine
  Learning}, pages 881--889.

\bibitem[Goodfellow et~al., 2014]{goodfellow2014}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y. (2014).
\newblock Generative adversarial nets.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2672--2680.

\bibitem[Grathwohl et~al., 2019]{grathwohl2018}
Grathwohl, W., Chen, R. T.~Q., Bettencourt, J., Sutskever, I., and Duvenaud, D.
  (2019).
\newblock {FFJORD:} free-form continuous dynamics for scalable reversible
  generative models.
\newblock In {\em 7th International Conference on Learning Representations}.

\bibitem[Hinton, 2007]{hinton2007}
Hinton, G.~E. (2007).
\newblock Learning multiple layers of representation.
\newblock {\em Trends in Cognitive Sciences}, 11:428--434.

\bibitem[Hinton et~al., 2006]{hinton2006}
Hinton, G.~E., Osindero, S., and Teh, Y.~W. (2006).
\newblock A fast learning algorithm for deep belief nets.
\newblock {\em Neural Computation}, 18(7):1527--1554.

\bibitem[Ho et~al., 2019]{ho2019}
Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. (2019).
\newblock Flow++: Improving flow-based generative models with variational
  dequantization and architecture design.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}.

\bibitem[Hoogeboom et~al., 2020]{hoogeboom2020}
Hoogeboom, E., Cohen, T.~S., and Tomczak, J.~M. (2020).
\newblock Learning discrete distributions by dequantization.
\newblock {\em CoRR}, abs/2001.11235.

\bibitem[Hoogeboom et~al., 2019]{hoogeboom2019}
Hoogeboom, E., Peters, J. W.~T., van~den Berg, R., and Welling, M. (2019).
\newblock Integer discrete flows and lossless compression.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  12134--12144.

\bibitem[Huang et~al., 2018]{huang2018}
Huang, C., Krueger, D., Lacoste, A., and Courville, A.~C. (2018).
\newblock Neural autoregressive flows.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning}, pages 2083--2092.

\bibitem[Huang et~al., 2017]{huang2017}
Huang, G., Liu, Z., van~der Maaten, L., and Weinberger, K.~Q. (2017).
\newblock Densely connected convolutional networks.
\newblock In {\em 2017 {IEEE} Conference on Computer Vision and Pattern
  Recognition}, pages 2261--2269.

\bibitem[Jaini et~al., 2019]{jaini2019}
Jaini, P., Selby, K.~A., and Yu, Y. (2019).
\newblock Sum-of-squares polynomial flow.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, pages 3009--3018.

\bibitem[Kalchbrenner et~al., 2017]{kalchbrenner2017}
Kalchbrenner, N., van~den Oord, A., Simonyan, K., Danihelka, I., Vinyals, O.,
  Graves, A., and Kavukcuoglu, K. (2017).
\newblock Video pixel networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning}, pages 1771--1779.

\bibitem[Kingma and Ba, 2015]{kingma2014}
Kingma, D.~P. and Ba, J. (2015).
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em 3rd International Conference on Learning Representations}.

\bibitem[Kingma and Dhariwal, 2018]{kingma2018}
Kingma, D.~P. and Dhariwal, P. (2018).
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10236--10245.

\bibitem[Kingma et~al., 2016]{kingma2016}
Kingma, D.~P., Salimans, T., J{\'{o}}zefowicz, R., Chen, X., Sutskever, I., and
  Welling, M. (2016).
\newblock Improving variational autoencoders with inverse autoregressive flow.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4736--4744.

\bibitem[Kingma and Welling, 2014]{kingma2013}
Kingma, D.~P. and Welling, M. (2014).
\newblock Auto-encoding variational bayes.
\newblock In {\em 2nd International Conference on Learning Representations}.

\bibitem[Ma et~al., 2019]{ma2019}
Ma, X., Kong, X., Zhang, S., and Hovy, E.~H. (2019).
\newblock Macow: Masked convolutional generative flow.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5891--5900.

\bibitem[Menick and Kalchbrenner, 2019]{menick2019}
Menick, J. and Kalchbrenner, N. (2019).
\newblock Generating high fidelity images with subscale pixel networks and
  multidimensional upscaling.
\newblock In {\em 7th International Conference on Learning Representations}.

\bibitem[M{\"{u}}ller et~al., 2019]{muller2018}
M{\"{u}}ller, T., McWilliams, B., Rousselle, F., Gross, M., and Nov{\'{a}}k, J.
  (2019).
\newblock Neural importance sampling.
\newblock {\em {ACM} Trans. Graph.}, 38(5):145:1--145:19.

\bibitem[Oliva et~al., 2018]{oliva2018}
Oliva, J.~B., Dubey, A., Zaheer, M., P{\'{o}}czos, B., Salakhutdinov, R., Xing,
  E.~P., and Schneider, J. (2018).
\newblock Transformation autoregressive networks.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning}, pages 3895--3904.

\bibitem[Papamakarios et~al., 2017]{papamakarios2017}
Papamakarios, G., Murray, I., and Pavlakou, T. (2017).
\newblock Masked autoregressive flow for density estimation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2338--2347.

\bibitem[Parmar et~al., 2018]{parmar2018}
Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., and
  Tran, D. (2018).
\newblock Image transformer.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning}, pages 4052--4061.

\bibitem[Rezende and Mohamed, 2015]{rezende2015}
Rezende, D.~J. and Mohamed, S. (2015).
\newblock Variational inference with normalizing flows.
\newblock In {\em Proceedings of the 32nd International Conference on Machine
  Learning}, pages 1530--1538.

\bibitem[Rezende et~al., 2014]{rezende2014}
Rezende, D.~J., Mohamed, S., and Wierstra, D. (2014).
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In {\em Proceedings of the 31th International Conference on Machine
  Learning}, pages 1278--1286.

\bibitem[Salakhutdinov and Hinton, 2009]{salakhutdinov2009}
Salakhutdinov, R. and Hinton, G. (2009).
\newblock Deep boltzmann machines.
\newblock In {\em Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, volume~5, pages 448--455.

\bibitem[Salimans et~al., 2017]{salimans2017}
Salimans, T., Karpathy, A., Chen, X., and Kingma, D.~P. (2017).
\newblock Pixelcnn++: Improving the pixelcnn with discretized logistic mixture
  likelihood and other modifications.
\newblock In {\em 5th International Conference on Learning Representations}.

\bibitem[Song et~al., 2019]{song2019}
Song, Y., Meng, C., and Ermon, S. (2019).
\newblock Mintnet: Building invertible neural networks with masked
  convolutions.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  11002--11012.

\bibitem[Theis et~al., 2016]{theis2016}
Theis, L., van~den Oord, A., and Bethge, M. (2016).
\newblock A note on the evaluation of generative models.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Tran et~al., 2019]{tran2019}
Tran, D., Vafa, K., Agrawal, K.~K., Dinh, L., and Poole, B. (2019).
\newblock Discrete flows: Invertible generative models of discrete data.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  14692--14701.

\bibitem[Uria et~al., 2013]{uria2013}
Uria, B., Murray, I., and Larochelle, H. (2013).
\newblock Rnade: The real-valued neural autoregressive density-estimator.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2175--2183.

\bibitem[van~den Oord et~al., 2016a]{oord2016wavenet}
van~den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A.,
  Kalchbrenner, N., Senior, A.~W., and Kavukcuoglu, K. (2016a).
\newblock Wavenet: {A} generative model for raw audio.
\newblock {\em CoRR}, abs/1609.03499.

\bibitem[van~den Oord et~al., 2016b]{oord2016gatedpixelcnn}
van~den Oord, A., Kalchbrenner, N., Espeholt, L., Kavukcuoglu, K., Vinyals, O.,
  and Graves, A. (2016b).
\newblock Conditional image generation with pixelcnn decoders.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4790--4798.

\bibitem[van~den Oord et~al., 2016c]{oord2016pixelrnn}
van~den Oord, A., Kalchbrenner, N., and Kavukcuoglu, K. (2016c).
\newblock Pixel recurrent neural networks.
\newblock In {\em Proceedings of the 33nd International Conference on Machine
  Learning}, pages 1747--1756.

\end{thebibliography}
