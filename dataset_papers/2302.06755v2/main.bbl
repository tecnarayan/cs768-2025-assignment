\begin{thebibliography}{79}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Chu, Goodfellow, McMahan, Mironov, Talwar,
  and Zhang]{deep_learning_with_DP_OG_paper}
Abadi, M., Chu, A., Goodfellow, I., McMahan, H.~B., Mironov, I., Talwar, K.,
  and Zhang, L.
\newblock Deep learning with differential privacy.
\newblock In \emph{Proceedings of the 2016 ACM SIGSAC Conference on Computer
  and Communications Security}, CCS '16, pp.\  308â€“318, New York, NY, USA,
  2016. Association for Computing Machinery.
\newblock ISBN 9781450341394.
\newblock \doi{10.1145/2976749.2978318}.
\newblock URL \url{https://doi.org/10.1145/2976749.2978318}.

\bibitem[Agarwal et~al.(2016)Agarwal, Bullins, and Hazan]{neumann_unroll}
Agarwal, N., Bullins, B., and Hazan, E.
\newblock Second-order stochastic optimization for machine learning in linear
  time.
\newblock 2016.
\newblock \doi{10.48550/ARXIV.1602.03943}.
\newblock URL \url{https://arxiv.org/abs/1602.03943}.

\bibitem[Aitken \& Gur-Ari(2020)Aitken and Gur-Ari]{finite_dynamcis2}
Aitken, K. and Gur-Ari, G.
\newblock On the asymptotics of wide networks with polynomial activations.
\newblock \emph{ArXiv}, abs/2006.06687, 2020.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{Arora_Exact_NTK_calc}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R.~R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8141--8150. Curran Associates, Inc., 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019on}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R.~R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8141--8150. Curran Associates, Inc., 2019{\natexlab{b}}.

\bibitem[Babuschkin et~al.(2020)Babuschkin, Baumli, Bell, Bhupatiraju, Bruce,
  Buchlovsky, Budden, Cai, Clark, Danihelka, Fantacci, Godwin, Jones, Hemsley,
  Hennigan, Hessel, Hou, Kapturowski, Keck, Kemaev, King, Kunesch, Martens,
  Merzic, Mikulik, Norman, Quan, Papamakarios, Ring, Ruiz, Sanchez, Schneider,
  Sezener, Spencer, Srinivasan, Wang, Stokowiec, and Viola]{deepmind2020jax}
Babuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce, J., Buchlovsky,
  P., Budden, D., Cai, T., Clark, A., Danihelka, I., Fantacci, C., Godwin, J.,
  Jones, C., Hemsley, R., Hennigan, T., Hessel, M., Hou, S., Kapturowski, S.,
  Keck, T., Kemaev, I., King, M., Kunesch, M., Martens, L., Merzic, H.,
  Mikulik, V., Norman, T., Quan, J., Papamakarios, G., Ring, R., Ruiz, F.,
  Sanchez, A., Schneider, R., Sezener, E., Spencer, S., Srinivasan, S., Wang,
  L., Stokowiec, W., and Viola, F.
\newblock The {D}eep{M}ind {JAX} {E}cosystem, 2020.
\newblock URL \url{http://github.com/deepmind}.

\bibitem[Bachem et~al.(2016)Bachem, Lucic, Hassani, and Krause]{BachemLHK16}
Bachem, O., Lucic, M., Hassani, S.~H., and Krause, A.
\newblock Approximate k-means++ in sublinear time.
\newblock In Schuurmans, D. and Wellman, M.~P. (eds.), \emph{Proceedings of the
  Thirtieth {AAAI} Conference on Artificial Intelligence, February 12-17, 2016,
  Phoenix, Arizona, {USA}}, pp.\  1459--1467. {AAAI} Press, 2016.
\newblock URL
  \url{http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12147}.

\bibitem[Bae et~al.(2022)Bae, Ng, Lo, Ghassemi, and
  Grosse]{if_influence_is_answer}
Bae, J., Ng, N.~H., Lo, A., Ghassemi, M., and Grosse, R.~B.
\newblock If influence functions are the answer, then what is the question?
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=hzbguA9zMJ}.

\bibitem[Basu et~al.(2020)Basu, Pope, and Feizi]{influence_fns_are_fragile}
Basu, S., Pope, P., and Feizi, S.
\newblock Influence functions in deep learning are fragile, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.14651}.

\bibitem[Bengio(2000)]{implicit_gradients1}
Bengio, Y.
\newblock Gradient-based optimization of hyperparameters.
\newblock \emph{Neural Computation}, 12\penalty0 (8):\penalty0 1889--1900,
  2000.
\newblock \doi{10.1162/089976600300015187}.

\bibitem[Borsos et~al.(2020)Borsos, Mutny, and Krause]{borsos2020coresets}
Borsos, Z., Mutny, M., and Krause, A.
\newblock Coresets via bilevel optimization for continual learning and
  streaming.
\newblock In \emph{Proceedings of the Advances in Neural Information Processing
  Systems (NeurIPS)}, volume~33, pp.\  14879--14890, 2020.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., Necula, G., Paszke, A., Vander{P}las, J., Wanderman-{M}ilne, S., and
  Zhang, Q.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Carlini et~al.(2022)Carlini, Feldman, and Nasr]{DD_with_DP_gets_rekt}
Carlini, N., Feldman, V., and Nasr, M.
\newblock No free lunch in "privacy for free: How does dataset condensation
  help privacy", 2022.
\newblock URL \url{https://arxiv.org/abs/2209.14987}.

\bibitem[Cazenavette et~al.(2022{\natexlab{a}})Cazenavette, Wang, Torralba,
  Efros, and Zhu]{cazenavette2022dataset}
Cazenavette, G., Wang, T., Torralba, A., Efros, A.~A., and Zhu, J.-Y.
\newblock Dataset distillation by matching training trajectories.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  4750--4759, 2022{\natexlab{a}}.

\bibitem[Cazenavette et~al.(2022{\natexlab{b}})Cazenavette, Wang, Torralba,
  Efros, and Zhu]{mtt}
Cazenavette, G., Wang, T., Torralba, A., Efros, A.~A., and Zhu, J.-Y.
\newblock Dataset distillation by matching training trajectories.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 2022{\natexlab{b}}.

\bibitem[Chen \& Hagan(1999)Chen and Hagan]{implicit_gradients2}
Chen, D. and Hagan, M.
\newblock Optimal use of regularization and cross-validation in neural network
  modeling.
\newblock In \emph{IJCNN'99. International Joint Conference on Neural Networks.
  Proceedings (Cat. No.99CH36339)}, volume~2, pp.\  1275--1280 vol.2, 1999.
\newblock \doi{10.1109/IJCNN.1999.831145}.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{lazy_training}
Chizat, L., Oyallon, E., and Bach, F.~R.
\newblock On lazy training in differentiable programming.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Domke(2012)]{bilevel_hyper1}
Domke, J.
\newblock Generic methods for optimization-based modeling.
\newblock In Lawrence, N.~D. and Girolami, M. (eds.), \emph{Proceedings of the
  Fifteenth International Conference on Artificial Intelligence and
  Statistics}, volume~22 of \emph{Proceedings of Machine Learning Research},
  pp.\  318--326, La Palma, Canary Islands, 21--23 Apr 2012. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v22/domke12.html}.

\bibitem[Dong et~al.(2022)Dong, Zhao, and Lyu]{dd_with_dp}
Dong, T., Zhao, B., and Lyu, L.
\newblock Privacy for free: How does dataset condensation help privacy?
\newblock \emph{ArXiv}, abs/2206.00240, 2022.

\bibitem[Feldman \& Langberg(2011)Feldman and Langberg]{feldman2011unified}
Feldman, D. and Langberg, M.
\newblock A unified framework for approximating and clustering data.
\newblock In \emph{Proceedings of the forty-third annual ACM symposium on
  Theory of computing}, pp.\  569--578, 2011.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{maml}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock \emph{CoRR}, abs/1703.03400, 2017.
\newblock URL \url{http://arxiv.org/abs/1703.03400}.

\bibitem[Fort et~al.(2020)Fort, Dziugaite, Paul, Kharaghani, Roy, and
  Ganguli]{fortman}
Fort, S., Dziugaite, G.~K., Paul, M., Kharaghani, S., Roy, D.~M., and Ganguli,
  S.
\newblock Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the neural tangent kernel.
\newblock In \emph{NeurIPS}, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/hash/405075699f065e43581f27d67bb68478-Abstract.html}.

\bibitem[Hampel(1974)]{old_influence}
Hampel, F.~R.
\newblock The influence curve and its role in robust estimation.
\newblock \emph{Journal of the American Statistical Association}, 69\penalty0
  (346):\penalty0 383--393, 1974.
\newblock ISSN 01621459.
\newblock URL \url{http://www.jstor.org/stable/2285666}.

\bibitem[Hanin \& Nica(2020)Hanin and Nica]{Hanin2020Finite}
Hanin, B. and Nica, M.
\newblock Finite depth and width corrections to the neural tangent kernel.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SJgndT4KwB}.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock \emph{CoRR}, abs/1512.03385, 2015.
\newblock URL \url{http://arxiv.org/abs/1512.03385}.

\bibitem[Heek et~al.(2020)Heek, Levskaya, Oliver, Ritter, Rondepierre, Steiner,
  and van {Z}ee]{flax}
Heek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre, B., Steiner, A.,
  and van {Z}ee, M.
\newblock {F}lax: A neural network library and ecosystem for {JAX}, 2020.
\newblock URL \url{http://github.com/google/flax}.

\bibitem[Howard(2020)]{imagenette}
Howard, J.
\newblock A smaller subset of 10 easily classified classes from imagenet, and a
  little more french, 2020.
\newblock URL \url{https://github.com/fastai/imagenette/}.

\bibitem[Huggins et~al.(2016)Huggins, Campbell, and Broderick]{HugginsCB16}
Huggins, J.~H., Campbell, T., and Broderick, T.
\newblock Coresets for scalable bayesian logistic regression.
\newblock In Lee, D.~D., Sugiyama, M., von Luxburg, U., Guyon, I., and Garnett,
  R. (eds.), \emph{Proceedings of the Advances in Neural Information Processing
  Systems (NeurIPS)}, pp.\  4080--4088, 2016.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{batchnorm}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{CoRR}, abs/1502.03167, 2015.
\newblock URL \url{http://arxiv.org/abs/1502.03167}.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{Jacot2018ntk}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6980}.

\bibitem[Koh \& Liang(2017)Koh and Liang]{influence_fns}
Koh, P.~W. and Liang, P.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1885--1894. JMLR. org, 2017.

\bibitem[Krizhevsky(2009)]{cifar}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock pp.\  32--33, 2009.
\newblock URL
  \url{https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{alexnet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In Pereira, F., Burges, C., Bottou, L., and Weinberger, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, volume~25. Curran
  Associates, Inc., 2012.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf}.

\bibitem[Le \& Yang(2015)Le and Yang]{tiny_imagenet}
Le, Y. and Yang, X.~S.
\newblock Tiny imagenet visual recognition challenge.
\newblock 2015.

\bibitem[Lecun et~al.(1998)Lecun, Bottou, Bengio, and Haffner]{MNIST}
Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.
\newblock \doi{10.1109/5.726791}.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{wide_linear_models}
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J.,
  and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Lee et~al.(2020)Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and
  Sohl-Dickstein]{finite_vs_infinite}
Lee, J., Schoenholz, S., Pennington, J., Adlam, B., Xiao, L., Novak, R., and
  Sohl-Dickstein, J.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  15156--15172. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/ad086f59924fffe0773f8d0ca22ea712-Paper.pdf}.

\bibitem[Loo et~al.(2022{\natexlab{a}})Loo, Hasani, Amini, and Rus]{RFAD}
Loo, N., Hasani, R., Amini, A., and Rus, D.
\newblock Efficient dataset distillation using random feature approximation.
\newblock \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{a}}.

\bibitem[Loo et~al.(2022{\natexlab{b}})Loo, Hasani, Amini, and
  Rus]{loo2022evolution}
Loo, N., Hasani, R., Amini, A., and Rus, D.
\newblock Evolution of neural tangent kernels under benign and adversarial
  training.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{b}}.

\bibitem[Lucic et~al.(2016)Lucic, Bachem, and Krause]{lucic2016strong}
Lucic, M., Bachem, O., and Krause, A.
\newblock Strong coresets for hard and soft bregman clustering with
  applications to exponential family mixtures.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  1--9. PMLR,
  2016.

\bibitem[MacKay et~al.(2019)MacKay, Vicol, Lorraine, Duvenaud, and
  Grosse]{bilevel_hyper3}
MacKay, M., Vicol, P., Lorraine, J., Duvenaud, D., and Grosse, R.~B.
\newblock Self-tuning networks: Bilevel optimization of hyperparameters using
  structured best-response functions.
\newblock \emph{CoRR}, abs/1903.03088, 2019.
\newblock URL \url{http://arxiv.org/abs/1903.03088}.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and Adams]{bilevel_hyper2}
Maclaurin, D., Duvenaud, D., and Adams, R.~P.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In \emph{Proceedings of the 32nd International Conference on
  International Conference on Machine Learning - Volume 37}, ICML'15, pp.\
  2113â€“2122. JMLR.org, 2015.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry_pgd}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks, 2017.
\newblock URL \url{https://arxiv.org/abs/1706.06083}.

\bibitem[Martens(2010)]{CG_in_deep_learning}
Martens, J.
\newblock Deep learning via hessian-free optimization.
\newblock In \emph{Proceedings of the 27th International Conference on
  International Conference on Machine Learning}, ICML'10, pp.\  735â€“742,
  Madison, WI, USA, 2010. Omnipress.
\newblock ISBN 9781605589077.

\bibitem[Mirzasoleiman et~al.(2020)Mirzasoleiman, Bilmes, and
  Leskovec]{MirzasoleimanBL20}
Mirzasoleiman, B., Bilmes, J.~A., and Leskovec, J.
\newblock Coresets for data-efficient training of machine learning models.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  6950--6960. {PMLR},
  2020.
\newblock URL \url{http://proceedings.mlr.press/v119/mirzasoleiman20a.html}.

\bibitem[Munteanu et~al.(2018)Munteanu, Schwiegelshohn, Sohler, and
  Woodruff]{munteanu2018coresets}
Munteanu, A., Schwiegelshohn, C., Sohler, C., and Woodruff, D.
\newblock On coresets for logistic regression.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Nasr et~al.(2018)Nasr, Shokri, and
  Houmansadr]{dp_with_adversarial_reg}
Nasr, M., Shokri, R., and Houmansadr, A.
\newblock Machine learning with membership privacy using adversarial
  regularization.
\newblock In \emph{Proceedings of the 2018 ACM SIGSAC Conference on Computer
  and Communications Security}, CCS '18, pp.\  634â€“646, New York, NY, USA,
  2018. Association for Computing Machinery.
\newblock ISBN 9781450356930.
\newblock \doi{10.1145/3243734.3243855}.
\newblock URL \url{https://doi.org/10.1145/3243734.3243855}.

\bibitem[Nguyen et~al.(2021{\natexlab{a}})Nguyen, Chen, and Lee]{KIP1}
Nguyen, T., Chen, Z., and Lee, J.
\newblock Dataset meta-learning from kernel ridge-regression.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=l-PrrQrK0QR}.

\bibitem[Nguyen et~al.(2021{\natexlab{b}})Nguyen, Novak, Xiao, and Lee]{KIP2}
Nguyen, T., Novak, R., Xiao, L., and Lee, J.
\newblock Dataset distillation with infinitely wide convolutional networks.
\newblock In \emph{Thirty-Fifth Conference on Neural Information Processing
  Systems}, 2021{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=hXWPpJedrVP}.

\bibitem[Oktay et~al.(2021)Oktay, McGreivy, Aduol, Beatson, and
  Adams]{randomized_autodiff}
Oktay, D., McGreivy, N., Aduol, J., Beatson, A., and Adams, R.~P.
\newblock Randomized automatic differentiation.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=xpx9zj7CUlY}.

\bibitem[Pearlmutter(1994)]{perlmutter}
Pearlmutter, B.~A.
\newblock {Fast Exact Multiplication by the Hessian}.
\newblock \emph{Neural Computation}, 6\penalty0 (1):\penalty0 147--160, 01
  1994.
\newblock ISSN 0899-7667.
\newblock \doi{10.1162/neco.1994.6.1.147}.
\newblock URL \url{https://doi.org/10.1162/neco.1994.6.1.147}.

\bibitem[Pooladzandi et~al.(2022)Pooladzandi, Davini, and
  Mirzasoleiman]{PooladzandiDM22}
Pooladzandi, O., Davini, D., and Mirzasoleiman, B.
\newblock Adaptive second order coresets for data-efficient machine learning.
\newblock In Chaudhuri, K., Jegelka, S., Song, L., Szepesv{\'{a}}ri, C., Niu,
  G., and Sabato, S. (eds.), \emph{International Conference on Machine
  Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, volume
  162 of \emph{Proceedings of Machine Learning Research}, pp.\  17848--17869.
  {PMLR}, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/pooladzandi22a.html}.

\bibitem[Rajeswaran et~al.(2019)Rajeswaran, Finn, Kakade, and Levine]{imaml}
Rajeswaran, A., Finn, C., Kakade, S.~M., and Levine, S.
\newblock Meta-learning with implicit gradients.
\newblock \emph{CoRR}, abs/1909.04630, 2019.
\newblock URL \url{http://arxiv.org/abs/1909.04630}.

\bibitem[Rumelhart \& McClelland(1987)Rumelhart and McClelland]{BPTT2}
Rumelhart, D.~E. and McClelland, J.~L.
\newblock \emph{Learning Internal Representations by Error Propagation}, pp.\
  318--362.
\newblock 1987.

\bibitem[Russakovsky et~al.(2014)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei{-}Fei]{imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M.~S., Berg, A.~C., and Fei{-}Fei, L.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{CoRR}, abs/1409.0575, 2014.
\newblock URL \url{http://arxiv.org/abs/1409.0575}.

\bibitem[Salimans et~al.(2017)Salimans, Ho, Chen, Sidor, and
  Sutskever]{evolution_strategies}
Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I.
\newblock Evolution strategies as a scalable alternative to reinforcement
  learning, 2017.
\newblock URL \url{https://arxiv.org/abs/1703.03864}.

\bibitem[Sangermano et~al.(2022)Sangermano, Carta, Cossu, and
  Bacciu]{continual_dd}
Sangermano, M., Carta, A., Cossu, A., and Bacciu, D.
\newblock Sample condensation in online continual learning, 2022.
\newblock URL \url{https://arxiv.org/abs/2206.11849}.

\bibitem[Scellier \& Bengio(2016)Scellier and Bengio]{equilibrium_prop}
Scellier, B. and Bengio, Y.
\newblock Equilibrium propagation: Bridging the gap between energy-based models
  and backpropagation, 2016.
\newblock URL \url{https://arxiv.org/abs/1602.05179}.

\bibitem[Shokri et~al.(2016)Shokri, Stronati, and Shmatikov]{MIA}
Shokri, R., Stronati, M., and Shmatikov, V.
\newblock Membership inference attacks against machine learning models.
\newblock \emph{CoRR}, abs/1610.05820, 2016.
\newblock URL \url{http://arxiv.org/abs/1610.05820}.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{vgg}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{CoRR}, abs/1409.1556, 2014.

\bibitem[Such et~al.(2019)Such, Rawal, Lehman, Stanley, and Clune]{NAS_with_DD}
Such, F.~P., Rawal, A., Lehman, J., Stanley, K.~O., and Clune, J.
\newblock Generative teaching networks: Accelerating neural architecture search
  by learning to generate synthetic training data.
\newblock \emph{CoRR}, abs/1912.07768, 2019.
\newblock URL \url{http://arxiv.org/abs/1912.07768}.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{OGadversarial}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
  and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock \emph{--}, 12 2013.

\bibitem[Tukan et~al.(2020)Tukan, Maalouf, and Feldman]{TukanMF20}
Tukan, M., Maalouf, A., and Feldman, D.
\newblock Coresets for near-convex functions.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Proceedings of the Advances in Neural Information Processing
  Systems (NeurIPS)}, 2020.

\bibitem[Vicol et~al.(2021)Vicol, Metz, and
  Sohl{-}Dickstein]{persistent_evolution}
Vicol, P., Metz, L., and Sohl{-}Dickstein, J.
\newblock Unbiased gradient estimation in unrolled computation graphs with
  persistent evolution strategies.
\newblock \emph{CoRR}, abs/2112.13835, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.13835}.

\bibitem[Vicol et~al.(2022)Vicol, Lorraine, Pedregosa, Duvenaud, and
  Grosse]{implicit_BLO}
Vicol, P., Lorraine, J.~P., Pedregosa, F., Duvenaud, D., and Grosse, R.~B.
\newblock On implicit bias in overparameterized bilevel optimization.
\newblock In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and
  Sabato, S. (eds.), \emph{Proceedings of the 39th International Conference on
  Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning
  Research}, pp.\  22234--22259. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/vicol22a.html}.

\bibitem[Wang et~al.(2022)Wang, Zhao, Peng, Zhu, Yang, Wang, Huang, Bilen,
  Wang, and You]{wang2022cafe}
Wang, K., Zhao, B., Peng, X., Zhu, Z., Yang, S., Wang, S., Huang, G., Bilen,
  H., Wang, X., and You, Y.
\newblock Cafe: Learning to condense dataset by aligning features.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  12196--12205, 2022.

\bibitem[Wang et~al.(2018)Wang, Zhu, Torralba, and Efros]{wang2018dataset}
Wang, T., Zhu, J.-Y., Torralba, A., and Efros, A.~A.
\newblock Dataset distillation.
\newblock \emph{arXiv preprint arXiv:1811.10959}, 2018.

\bibitem[Welinder et~al.(2010)Welinder, Branson, Mita, Wah, Schroff, Belongie,
  and Perona]{CUB-200}
Welinder, P., Branson, S., Mita, T., Wah, C., Schroff, F., Belongie, S., and
  Perona, P.
\newblock {Caltech-UCSD Birds 200}.
\newblock Technical Report CNS-TR-2010-001, California Institute of Technology,
  2010.

\bibitem[Werbos(1990)]{BPTT3}
Werbos, P.
\newblock Backpropagation through time: what it does and how to do it.
\newblock \emph{Proceedings of the IEEE}, 78\penalty0 (10):\penalty0
  1550--1560, 1990.
\newblock \doi{10.1109/5.58337}.

\bibitem[Werbos(1988)]{BPTT1}
Werbos, P.~J.
\newblock Generalization of backpropagation with application to a recurrent gas
  market model.
\newblock \emph{Neural Networks}, 1\penalty0 (4):\penalty0 339--356, 1988.
\newblock ISSN 0893-6080.
\newblock \doi{https://doi.org/10.1016/0893-6080(88)90007-X}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/089360808890007X}.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{FASHION_MNIST}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{CoRR}, abs/1708.07747, 2017.
\newblock URL \url{http://arxiv.org/abs/1708.07747}.

\bibitem[Yu et~al.(2022)Yu, Wei, Karimireddy, Ma, and Jordan]{TCT}
Yu, Y., Wei, A., Karimireddy, S.~P., Ma, Y., and Jordan, M.~I.
\newblock Tct: Convexifying federated learning using bootstrapped neural
  tangent kernels, 2022.
\newblock URL \url{https://arxiv.org/abs/2207.06343}.

\bibitem[Zhao \& Bilen(2021{\natexlab{a}})Zhao and Bilen]{zhao2021dataset}
Zhao, B. and Bilen, H.
\newblock Dataset condensation with distribution matching.
\newblock \emph{arXiv preprint arXiv:2110.04181}, 2021{\natexlab{a}}.

\bibitem[Zhao \& Bilen(2021{\natexlab{b}})Zhao and Bilen]{zhao2021dsa}
Zhao, B. and Bilen, H.
\newblock Dataset condensation with differentiable siamese augmentation.
\newblock \emph{arXiv preprint arXiv:2102.08259}, 2021{\natexlab{b}}.

\bibitem[Zhao et~al.(2021)Zhao, Mopuri, and Bilen]{zhao2021DC}
Zhao, B., Mopuri, K.~R., and Bilen, H.
\newblock Dataset condensation with gradient matching.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=mSAKhLYLSsl}.

\bibitem[Zhou et~al.(2022)Zhou, Nezhadarya, and Ba]{frepo}
Zhou, Y., Nezhadarya, E., and Ba, J.
\newblock Dataset distillation using neural feature regression.
\newblock In \emph{Proceedings of the Advances in Neural Information Processing
  Systems (NeurIPS)}, 2022.

\bibitem[Zhuang et~al.(2020)Zhuang, Tang, Ding, Tatikonda, Dvornek,
  Papademetris, and Duncan]{adabelief}
Zhuang, J., Tang, T., Ding, Y., Tatikonda, S., Dvornek, N., Papademetris, X.,
  and Duncan, J.~S.
\newblock Adabelief optimizer: Adapting stepsizes by the belief in observed
  gradients.
\newblock \emph{arXiv preprint arXiv:2010.07468}, 2020.

\bibitem[Zucchet et~al.(2021)Zucchet, Schug, von Oswald, Zhao, and
  Sacramento]{eqm_prop2}
Zucchet, N., Schug, S., von Oswald, J., Zhao, D., and Sacramento, J.
\newblock A contrastive rule for meta-learning.
\newblock \emph{CoRR}, abs/2104.01677, 2021.
\newblock URL \url{https://arxiv.org/abs/2104.01677}.

\end{thebibliography}
