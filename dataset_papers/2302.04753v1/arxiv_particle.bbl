\begin{thebibliography}{10}

\bibitem{arjovsky2017wasserstein}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock Wasserstein generative adversarial networks.
\newblock In {\em International conference on machine learning}, pages
  214--223. PMLR, 2017.

\bibitem{chizat2018global}
Lenaic Chizat and Francis Bach.
\newblock \href{https://arxiv.org/pdf/1805.09545.pdf}{On the global convergence
  of gradient descent for over-parameterized models using optimal transport}.
\newblock {\em NeurIps}, 2019.

\bibitem{liu2016stein}
Qiang Liu and Dilin Wang.
\newblock Stein variational gradient descent: A general purpose bayesian
  inference algorithm.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{bredies2013inverse}
Kristian Bredies and Hanna~Katriina Pikkarainen.
\newblock Inverse problems in spaces of measures.
\newblock {\em ESAIM: Control, Optimisation and Calculus of Variations},
  19(1):190--218, 2013.

\bibitem{mccann1997convexity}
Robert~J McCann.
\newblock
  \href{https://www.ceremade.dauphine.fr/~carlier/McCann-convexity-displ.pdf}{A
  convexity principle for interacting gases}.
\newblock {\em Advances in mathematics}, 128(1):153--179, 1997.

\bibitem{ambrosio2005gradient}
Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar{\'e}.
\newblock {\em Gradient flows: in metric spaces and in the space of probability
  measures}.
\newblock Springer Science \& Business Media, 2005.

\bibitem{jordan1998variational}
Richard Jordan, David Kinderlehrer, and Felix Otto.
\newblock The variational formulation of the fokker--planck equation.
\newblock {\em SIAM journal on mathematical analysis}, 29(1):1--17, 1998.

\bibitem{absil2009optimization}
P-A Absil, Robert Mahony, and Rodolphe Sepulchre.
\newblock Optimization algorithms on matrix manifolds.
\newblock In {\em Optimization Algorithms on Matrix Manifolds}. Princeton
  University Press, 2009.

\bibitem{santambrogio2017euclidean}
Filippo Santambrogio.
\newblock \href{https://arxiv.org/pdf/1609.03890.pdf}{$\{$Euclidean, metric,
  and Wasserstein$\}$ gradient flows: an overview}.
\newblock {\em Bulletin of Mathematical Sciences}, 2017.

\bibitem{chizat2022sparse}
Lenaic Chizat.
\newblock Sparse optimization on measures with over-parameterized gradient
  descent.
\newblock {\em Mathematical Programming}, 194(1):487--532, 2022.

\bibitem{carrillo2018measure}
Jose~Antonio Carrillo, Marco~Di Francesco, Antonio Esposito, Simone Fagioli,
  and Markus Schmidtchen.
\newblock \href{https://arxiv.org/pdf/1810.10236.pdf}{Measure solutions to a
  system of continuity equations driven by Newtonian nonlocal interactions}.
\newblock {\em Discrete and Continuous Dynamical Systems}, 40(2):1191--1231,
  2020.

\bibitem{carrillo2022global}
Jos{\'e}~A Carrillo and Ruiwen Shu.
\newblock Global minimizers of a large class of anisotropic
  attractive-repulsive interaction energies in 2d.
\newblock {\em arXiv preprint arXiv:2202.09237}, 2022.

\bibitem{carrillo2021equilibrium}
JA~Carrillo, Joan Mateu, MG~Mora, Luca Rondi, Lucia Scardia, and Joan Verdera.
\newblock The equilibrium measure for an anisotropic nonlocal energy.
\newblock {\em Calculus of Variations and Partial Differential Equations},
  60(3):1--28, 2021.

\bibitem{nitanda2017stochastic}
Atsushi Nitanda and Taiji Suzuki.
\newblock Stochastic particle gradient descent for infinite ensembles.
\newblock {\em arXiv preprint arXiv:1712.05438}, 2017.

\bibitem{li2022sampling}
Lingxiao Li, Qiang Liu, Anna Korba, Mikhail Yurochkin, and Justin Solomon.
\newblock Sampling with mollified interaction energy descent.
\newblock {\em arXiv preprint arXiv:2210.13400}, 2022.

\bibitem{frank1956algorithm}
Marguerite Frank and Philip Wolfe.
\newblock An algorithm for quadratic programming.
\newblock {\em Naval research logistics quarterly}, 3(1-2):95--110, 1956.

\bibitem{dunn1979rates}
Joseph~C Dunn.
\newblock Rates of convergence for conditional gradient algorithms near
  singular and nonsingular extremals.
\newblock {\em SIAM Journal on Control and Optimization}, 17(2):187--211, 1979.

\bibitem{jaggi2013revisiting}
Martin Jaggi.
\newblock Revisiting frank-wolfe: Projection-free sparse convex optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  427--435. PMLR, 2013.

\bibitem{chizat2020implicit}
Lenaic Chizat and Francis Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock In {\em Conference on Learning Theory}, pages 1305--1338. PMLR, 2020.

\bibitem{javanmard}
Adel Javanmard, Marco Mondelli, and Andrea Montanari.
\newblock Analysis of a two-layer neural network via displacement convexity.
\newblock 2019.

\bibitem{hadi22}
Hadi Daneshmand and Francis Bach.
\newblock Polynomial-time sparse deconvolution, 2022.

\bibitem{zhang2016first}
Hongyi Zhang and Suvrit Sra.
\newblock First-order methods for geodesically convex optimization.
\newblock In {\em Conference on Learning Theory}, pages 1617--1638. PMLR, 2016.

\bibitem{udriste2013convex}
Constantin Udriste.
\newblock {\em Convex functions and optimization methods on Riemannian
  manifolds}, volume 297.
\newblock Springer Science \& Business Media, 2013.

\bibitem{baldi1989neural}
Pierre Baldi and Kurt Hornik.
\newblock Neural networks and principal component analysis: Learning from
  examples without local minima.
\newblock {\em Neural networks}, 2(1):53--58, 1989.

\bibitem{ge2017no}
Rong Ge, Chi Jin, and Yi~Zheng.
\newblock No spurious local minima in nonconvex low rank problems: A unified
  geometric analysis.
\newblock In {\em International Conference on Machine Learning}, pages
  1233--1242. PMLR, 2017.

\bibitem{javanmard2020analysis}
Adel Javanmard, Marco Mondelli, and Andrea Montanari.
\newblock Analysis of a two-layer neural network via displacement convexity.
\newblock {\em The Annals of Statistics}, 48(6):3619--3642, 2020.

\bibitem{jin2017escape}
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham~M Kakade, and Michael~I Jordan.
\newblock How to escape saddle points efficiently.
\newblock In {\em International Conference on Machine Learning}, pages
  1724--1732. PMLR, 2017.

\bibitem{daneshmand2018escaping}
Hadi Daneshmand, Jonas Kohler, Aurelien Lucchi, and Thomas Hofmann.
\newblock Escaping saddles with stochastic gradients.
\newblock In {\em International Conference on Machine Learning}, pages
  1155--1164. PMLR, 2018.

\bibitem{lee2016gradient}
Jason~D. Lee, Max Simchowitz, Michael~I. Jordan, and Benjamin Recht.
\newblock Gradient descent only converges to minimizers.
\newblock In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir, editors, {\em
  29th Annual Conference on Learning Theory}, volume~49 of {\em Proceedings of
  Machine Learning Research}, pages 1246--1257, Columbia University, New York,
  New York, USA, 23--26 Jun 2016. PMLR.

\bibitem{du2017gradient}
Simon~S Du, Chi Jin, Jason~D Lee, Michael~I Jordan, Aarti Singh, and Barnabas
  Poczos.
\newblock Gradient descent can take exponential time to escape saddle points.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{szekely2017energy}
G{\'a}bor~J Sz{\'e}kely and Maria~L Rizzo.
\newblock The energy of data.
\newblock {\em Annual Review of Statistics and Its Application}, 4(1):447--479,
  2017.

\bibitem{nesterov2006cubic}
Yurii Nesterov and Boris~T Polyak.
\newblock Cubic regularization of newton method and its global performance.
\newblock {\em Mathematical Programming}, 108(1):177--205, 2006.

\bibitem{anandkumar2014tensor}
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham~M Kakade, and Matus Telgarsky.
\newblock Tensor decompositions for learning latent variable models.
\newblock {\em Journal of machine learning research}, 15:2773--2832, 2014.

\bibitem{carrillo2009example}
Jos{\'e}~A Carrillo and Dejan Slep{\v{c}}ev.
\newblock Example of a displacement convex functional of first order.
\newblock {\em Calculus of Variations and Partial Differential Equations},
  36(4):547--564, 2009.

\bibitem{nesterov1999global}
Yurii Nesterov.
\newblock {\em Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem{ge2015escaping}
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.
\newblock Escaping from saddle pointsâ€”online stochastic gradient for tensor
  decomposition.
\newblock In {\em Conference on learning theory}, pages 797--842. PMLR, 2015.

\bibitem{xu2018first}
Yi~Xu, Rong Jin, and Tianbao Yang.
\newblock First-order stochastic algorithms for escaping from saddle points in
  almost linear time.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{zhang2017hitting}
Yuchen Zhang, Percy Liang, and Moses Charikar.
\newblock A hitting time analysis of stochastic gradient langevin dynamics.
\newblock In {\em Conference on Learning Theory}, pages 1980--2022. PMLR, 2017.

\bibitem{gretton2012kernel}
Arthur Gretton, Karsten~M Borgwardt, Malte~J Rasch, Bernhard Sch{\"o}lkopf, and
  Alexander Smola.
\newblock A kernel two-sample test.
\newblock {\em The Journal of Machine Learning Research}, 13(1):723--773, 2012.

\bibitem{xu2022accurate}
Lantian Xu, Anna Korba, and Dejan Slepcev.
\newblock Accurate quantization of measures via interacting particle-based
  optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  24576--24595. PMLR, 2022.

\bibitem{bach2017breaking}
Francis Bach.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock {\em The Journal of Machine Learning Research}, 18(1):629--681, 2017.

\bibitem{mcculloch1943logical}
Warren~S McCulloch and Walter Pitts.
\newblock A logical calculus of the ideas immanent in nervous activity.
\newblock {\em The bulletin of mathematical biophysics}, 5(4):115--133, 1943.

\bibitem{philipowski2007interacting}
Robert Philipowski.
\newblock Interacting diffusions approximating the porous medium equation and
  propagation of chaos.
\newblock {\em Stochastic processes and their applications}, 117(4):526--538,
  2007.

\end{thebibliography}
