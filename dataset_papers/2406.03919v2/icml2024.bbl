\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Boussif et~al.(2022)Boussif, Bengio, Benabbou, and Assouline]{magnet-pde-boussif:2022}
Boussif, O., Bengio, Y., Benabbou, L., and Assouline, D.
\newblock Magnet: Mesh agnostic neural pde solver.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 31972--31985, 2022.

\bibitem[Brandstetter et~al.(2022)Brandstetter, Worrall, and Welling]{mp-pde-brandstetter}
Brandstetter, J., Worrall, D.~E., and Welling, M.
\newblock Message passing neural {PDE} solvers.
\newblock \emph{CoRR}, abs/2202.03376, 2022.
\newblock URL \url{https://arxiv.org/abs/2202.03376}.

\bibitem[Cao(2021)]{galerkin-cao}
Cao, S.
\newblock Choose a transformer: Fourier or galerkin.
\newblock \emph{CoRR}, abs/2105.14995, 2021.
\newblock URL \url{https://arxiv.org/abs/2105.14995}.

\bibitem[Chen et~al.(2021)Chen, Fan, and Panda]{multi-scale-vit-chen}
Chen, C., Fan, Q., and Panda, R.
\newblock Crossvit: Cross-attention multi-scale vision transformer for image classification.
\newblock \emph{CoRR}, abs/2103.14899, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.14899}.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Wu, Grinspun, Zheng, and Chen]{implicit-neural-spatial-repr-pde-chen:2023}
Chen, H., Wu, R., Grinspun, E., Zheng, C., and Chen, P.~Y.
\newblock Implicit neural spatial representations for time-dependent pdes.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), \emph{International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  5162--5177. {PMLR}, 2023{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v202/chen23af.html}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Xiang, Cho, Chang, Pershing, Maia, Chiaramonte, Carlberg, and Grinspun]{crom-pde-yichen-chen:2023}
Chen, P.~Y., Xiang, J., Cho, D.~H., Chang, Y., Pershing, G.~A., Maia, H.~T., Chiaramonte, M.~M., Carlberg, K.~T., and Grinspun, E.
\newblock {CROM:} continuous reduced-order modeling of pdes using implicit neural representations.
\newblock In \emph{The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net, 2023{\natexlab{b}}.
\newblock URL \url{https://openreview.net/pdf?id=FUORz1tG8Og}.

\bibitem[Chu et~al.(2022)Chu, Liu, Zheng, Franz, Seidel, Theobalt, and Zayer]{physics-informed-neural-fields-chu:2022}
Chu, M., Liu, L., Zheng, Q., Franz, E., Seidel, H., Theobalt, C., and Zayer, R.
\newblock Physics informed neural fields for smoke reconstruction with sparse data.
\newblock \emph{{ACM} Trans. Graph.}, 41\penalty0 (4):\penalty0 119:1--119:14, 2022.
\newblock \doi{10.1145/3528223.3530169}.
\newblock URL \url{https://doi.org/10.1145/3528223.3530169}.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{transfromers-bert-devlin}
Devlin, J., Chang, M., Lee, K., and Toutanova, K.
\newblock {BERT:} pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{CoRR}, abs/1810.04805, 2018.
\newblock URL \url{http://arxiv.org/abs/1810.04805}.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{transformers-images-dosovitskiy}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{CoRR}, abs/2010.11929, 2020.
\newblock URL \url{https://arxiv.org/abs/2010.11929}.

\bibitem[Geneva \& Zabaras(2020)Geneva and Zabaras]{transformers-physical-systems-geneva:2020}
Geneva, N. and Zabaras, N.
\newblock Transformers for modeling physical systems.
\newblock \emph{CoRR}, abs/2010.03957, 2020.
\newblock URL \url{https://arxiv.org/abs/2010.03957}.

\bibitem[Gulati et~al.(2020)Gulati, Qin, Chiu, Parmar, Zhang, Yu, Han, Wang, Zhang, Wu, and Pang]{transformer-conformer-gulati}
Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., and Pang, R.
\newblock Conformer: Convolution-augmented transformer for speech recognition, 2020.

\bibitem[Gupta \& Brandstetter(2023)Gupta and Brandstetter]{generalized-pde-modeling-gupta:2023}
Gupta, J.~K. and Brandstetter, J.
\newblock Towards multi-spatiotemporal-scale generalized {PDE} modeling.
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=dPSTDbGtBY}.

\bibitem[Hao et~al.(2023)Hao, Wang, Su, Ying, Dong, Liu, Cheng, Song, and Zhu]{gnot-operator-learn-hao:2023}
Hao, Z., Wang, Z., Su, H., Ying, C., Dong, Y., Liu, S., Cheng, Z., Song, J., and Zhu, J.
\newblock {GNOT:} {A} general neural operator transformer for operator learning.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), \emph{International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  12556--12569. {PMLR}, 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/hao23c.html}.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and Fleuret]{linear-transformers-katharopoulos:2020}
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
\newblock Transformers are rnns: Fast autoregressive transformers with linear attention.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of \emph{Proceedings of Machine Learning Research}, pp.\  5156--5165. {PMLR}, 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/katharopoulos20a.html}.

\bibitem[Kovachki et~al.(2021)Kovachki, Li, Liu, Azizzadenesheli, Bhattacharya, Stuart, and Anandkumar]{neural-operators-kovachki}
Kovachki, N.~B., Li, Z., Liu, B., Azizzadenesheli, K., Bhattacharya, K., Stuart, A.~M., and Anandkumar, A.
\newblock Neural operator: Learning maps between function spaces.
\newblock \emph{CoRR}, abs/2108.08481, 2021.
\newblock URL \url{https://arxiv.org/abs/2108.08481}.

\bibitem[Krishnapriyan et~al.(2021)Krishnapriyan, Gholami, Zhe, Kirby, and Mahoney]{pinn-failure-modes-krishnapriyan:2021}
Krishnapriyan, A.~S., Gholami, A., Zhe, S., Kirby, R.~M., and Mahoney, M.~W.
\newblock Characterizing possible failure modes in physics-informed neural networks.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y.~N., Liang, P., and Vaughan, J.~W. (eds.), \emph{Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual}, pp.\  26548--26560, 2021.
\newblock URL \url{https://arxiv.org/abs/2109.01050}.

\bibitem[Li et~al.(2021)Li, Si, Li, Hsieh, and Bengio]{fourier-features-li}
Li, Y., Si, S., Li, G., Hsieh, C.-J., and Bengio, S.
\newblock Learnable fourier features for multi-dimensional spatial positional encoding, 2021.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya, Stuart, and Anandkumar]{fno-li}
Li, Z., Kovachki, N.~B., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A.~M., and Anandkumar, A.
\newblock Fourier neural operator for parametric partial differential equations.
\newblock \emph{CoRR}, abs/2010.08895, 2020{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2010.08895}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya, Stuart, and Anandkumar]{graph-neural-operator-li}
Li, Z., Kovachki, N.~B., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A.~M., and Anandkumar, A.
\newblock Neural operator: Graph kernel network for partial differential equations.
\newblock \emph{CoRR}, abs/2003.03485, 2020{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2003.03485}.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Meidani, and Farimani]{oformer-pde-li:2023}
Li, Z., Meidani, K., and Farimani, A.~B.
\newblock Transformer for partial differential equations' operator learning.
\newblock \emph{Trans. Mach. Learn. Res.}, 2023, 2023{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=EPPqt3uERT}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Shu, and Farimani]{scalable-transformer-pde-li:2023}
Li, Z., Shu, D., and Farimani, A.~B.
\newblock Scalable transformer for {PDE} surrogate modeling.
\newblock \emph{CoRR}, abs/2305.17560, 2023{\natexlab{b}}.
\newblock \doi{10.48550/arXiv.2305.17560}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2305.17560}.

\bibitem[Li et~al.(2023{\natexlab{c}})Li, Zheng, Kovachki, Jin, Chen, Liu, Azizzadenesheli, and Anandkumar]{physics-neural-operator-li}
Li, Z., Zheng, H., Kovachki, N., Jin, D., Chen, H., Liu, B., Azizzadenesheli, K., and Anandkumar, A.
\newblock Physics-informed neural operator for learning partial differential equations, 2023{\natexlab{c}}.

\bibitem[Lippe et~al.(2023)Lippe, Veeling, Perdikaris, Turner, and Brandstetter]{pde-refiner-long-term-rollouts-neural-pde-lippe:2023}
Lippe, P., Veeling, B.~S., Perdikaris, P., Turner, R.~E., and Brandstetter, J.
\newblock Pde-refiner: Achieving accurate long rollouts with neural {PDE} solvers.
\newblock \emph{CoRR}, abs/2308.05732, 2023.
\newblock \doi{10.48550/ARXIV.2308.05732}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2308.05732}.

\bibitem[Lu et~al.(2019)Lu, Jin, and Karniadakis]{deeponet-lu:2019}
Lu, L., Jin, P., and Karniadakis, G.~E.
\newblock Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators.
\newblock \emph{CoRR}, abs/1910.03193, 2019.
\newblock URL \url{http://arxiv.org/abs/1910.03193}.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and Adams]{hips-autograd-maclaurin:2015}
Maclaurin, D., Duvenaud, D., and Adams, R.~P.
\newblock Autograd: Effortless gradients in numpy.
\newblock In \emph{ICML 2015 AutoML workshop}, volume 238, 2015.

\bibitem[McCabe et~al.(2023)McCabe, Blancard, Parker, Ohana, Cranmer, Bietti, Eickenberg, Golkar, Krawezik, Lanusse, Pettee, Tesileanu, Cho, and Ho]{multi-physics-pretrain-avit-mccabe:2023}
McCabe, M., Blancard, B.~R., Parker, L.~H., Ohana, R., Cranmer, M.~D., Bietti, A., Eickenberg, M., Golkar, S., Krawezik, G., Lanusse, F., Pettee, M., Tesileanu, T., Cho, K., and Ho, S.
\newblock Multiple physics pretraining for physical surrogate models.
\newblock \emph{CoRR}, abs/2310.02994, 2023.
\newblock \doi{10.48550/ARXIV.2310.02994}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2310.02994}.

\bibitem[Mescheder et~al.(2019)Mescheder, Oechsle, Niemeyer, Nowozin, and Geiger]{occupancy-networks-inr-mescheder:2019}
Mescheder, L.~M., Oechsle, M., Niemeyer, M., Nowozin, S., and Geiger, A.
\newblock Occupancy networks: Learning 3d reconstruction in function space.
\newblock In \emph{{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2019, Long Beach, CA, USA, June 16-20, 2019}, pp.\  4460--4470. Computer Vision Foundation / {IEEE}, 2019.
\newblock \doi{10.1109/CVPR.2019.00459}.
\newblock URL \url{http://openaccess.thecvf.com/content\_CVPR\_2019/html/Mescheder\_Occupancy\_Networks\_Learning\_3D\_Reconstruction\_in\_Function\_Space\_CVPR\_2019\_paper.html}.

\bibitem[Ovadia et~al.(2023)Ovadia, Turkel, Kahana, and Karniadakis]{ditto-diffusion-temporal-transformer-ovadia:2023}
Ovadia, O., Turkel, E., Kahana, A., and Karniadakis, G.~E.
\newblock Ditto: Diffusion-inspired temporal transformer operator.
\newblock \emph{CoRR}, abs/2307.09072, 2023.
\newblock \doi{10.48550/ARXIV.2307.09072}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2307.09072}.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin, Desmaison, Antiga, and Lerer]{autodiff-pytorch-paszke:2017}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock In \emph{NIPS 2017 Workshop on Autodiff}, 2017.
\newblock URL \url{https://openreview.net/forum?id=BJJsrmfCZ}.

\bibitem[Perez et~al.(2018)Perez, Strub, de~Vries, Dumoulin, and Courville]{film-conditioning-perez:2018}
Perez, E., Strub, F., de~Vries, H., Dumoulin, V., and Courville, A.~C.
\newblock Film: Visual reasoning with a general conditioning layer.
\newblock In McIlraith, S.~A. and Weinberger, K.~Q. (eds.), \emph{Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018}, pp.\  3942--3951. {AAAI} Press, 2018.
\newblock \doi{10.1609/AAAI.V32I1.11671}.
\newblock URL \url{https://doi.org/10.1609/aaai.v32i1.11671}.

\bibitem[Rahman et~al.(2023)Rahman, Ross, and Azizzadenesheli]{unet-neural-op-uno-rahman:2023}
Rahman, M.~A., Ross, Z.~E., and Azizzadenesheli, K.
\newblock U-{NO}: U-shaped neural operators.
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=j3oQF9coJd}.

\bibitem[Raissi et~al.(2017)Raissi, Perdikaris, and Karniadakis]{pinn-raissi}
Raissi, M., Perdikaris, P., and Karniadakis, G.~E.
\newblock Physics informed deep learning (part {I):} data-driven solutions of nonlinear partial differential equations.
\newblock \emph{CoRR}, abs/1711.10561, 2017.
\newblock URL \url{http://arxiv.org/abs/1711.10561}.

\bibitem[Rebain et~al.(2023)Rebain, Matthews, Yi, Sharma, Lagun, and Tagliasacchi]{attention-vs-concat-neural-fields-conditioning-rebain:2023}
Rebain, D., Matthews, M.~J., Yi, K.~M., Sharma, G., Lagun, D., and Tagliasacchi, A.
\newblock Attention beats concatenation for conditioning neural fields.
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=GzqdMrFQsE}.

\bibitem[Ruoss et~al.(2023)Ruoss, Delétang, Genewein, Grau-Moya, Csordás, Bennani, Legg, and Veness]{rope-ruoss}
Ruoss, A., Delétang, G., Genewein, T., Grau-Moya, J., Csordás, R., Bennani, M., Legg, S., and Veness, J.
\newblock Randomized positional encodings boost length generalization of transformers, 2023.

\bibitem[Sanchez{-}Gonzalez et~al.(2020)Sanchez{-}Gonzalez, Godwin, Pfaff, Ying, Leskovec, and Battaglia]{learn2simulate-physics-sanchez-gonzalez:2020}
Sanchez{-}Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J., and Battaglia, P.~W.
\newblock Learning to simulate complex physics with graph networks.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of \emph{Proceedings of Machine Learning Research}, pp.\  8459--8468. {PMLR}, 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/sanchez-gonzalez20a.html}.

\bibitem[Serrano et~al.(2023)Serrano, Boudec, Koupa{\"{\i}}, Wang, Yin, Vittaut, and Gallinari]{operator-learning-neural-fields-general-geometries-serrano:2023}
Serrano, L., Boudec, L.~L., Koupa{\"{\i}}, A.~K., Wang, T.~X., Yin, Y., Vittaut, J., and Gallinari, P.
\newblock Operator learning with neural fields: Tackling pdes on general geometries.
\newblock \emph{CoRR}, abs/2306.07266, 2023.
\newblock \doi{10.48550/ARXIV.2306.07266}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2306.07266}.

\bibitem[Sitzmann et~al.(2020)Sitzmann, Martel, Bergman, Lindell, and Wetzstein]{siren-inr-sitzmann:2020}
Sitzmann, V., Martel, J. N.~P., Bergman, A.~W., Lindell, D.~B., and Wetzstein, G.
\newblock Implicit neural representations with periodic activation functions.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/53c04118df112c13a8c34b38343b9c10-Abstract.html}.

\bibitem[Takamoto et~al.(2022)Takamoto, Praditia, Leiteritz, MacKinlay, Alesiani, Pfl{\"u}ger, and Niepert]{pdebench-takamoto:2022}
Takamoto, M., Praditia, T., Leiteritz, R., MacKinlay, D., Alesiani, F., Pfl{\"u}ger, D., and Niepert, M.
\newblock Pdebench: An extensive benchmark for scientific machine learning.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Takamoto et~al.(2023)Takamoto, Alesiani, and Niepert]{cape-takamoto:2023}
Takamoto, M., Alesiani, F., and Niepert, M.
\newblock Learning neural {PDE} solvers with parameter-guided channel attention.
\newblock \emph{ICML}, abs/2304.14118, 2023.
\newblock \doi{10.48550/arXiv.2304.14118}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2304.14118}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{transformers-vaswani}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{CoRR}, abs/1706.03762, 2017.
\newblock URL \url{http://arxiv.org/abs/1706.03762}.

\bibitem[Xie et~al.(2021)Xie, Takikawa, Saito, Litany, Yan, Khan, Tombari, Tompkin, Sitzmann, and Sridhar]{neural-fields-xie}
Xie, Y., Takikawa, T., Saito, S., Litany, O., Yan, S., Khan, N., Tombari, F., Tompkin, J., Sitzmann, V., and Sridhar, S.
\newblock Neural fields in visual computing and beyond.
\newblock \emph{CoRR}, abs/2111.11426, 2021.
\newblock URL \url{https://arxiv.org/abs/2111.11426}.

\bibitem[Yin et~al.(2023)Yin, Kirchmeyer, Franceschi, Rakotomamonjy, and Gallinari]{dynamics-aware-implicit-neural-repr-dino-yin:2023}
Yin, Y., Kirchmeyer, M., Franceschi, J., Rakotomamonjy, A., and Gallinari, P.
\newblock Continuous {PDE} dynamics forecasting with implicit neural representations.
\newblock In \emph{The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net, 2023.
\newblock URL \url{https://openreview.net/pdf?id=B73niNjbPs}.

\end{thebibliography}
