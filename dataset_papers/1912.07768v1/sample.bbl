\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2018)Andrychowicz, Baker, Chociej, Jozefowicz,
  McGrew, Pachocki, Petron, Plappert, Powell, Ray,
  et~al.]{andrychowicz2018learning}
Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew,
  Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray,
  et~al.
\newblock Learning dexterous in-hand manipulation.
\newblock \emph{arXiv preprint arXiv:1808.00177}, 2018.

\bibitem[Baker et~al.(2017)Baker, Gupta, Raskar, and
  Naik]{baker2017accelerating}
Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik.
\newblock Accelerating neural architecture search using performance prediction.
\newblock \emph{arXiv preprint arXiv:1705.10823}, 2017.

\bibitem[Brock et~al.(2018)Brock, Donahue, and Simonyan]{brock2018large}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale gan training for high fidelity natural image synthesis.
\newblock \emph{arXiv preprint arXiv:1809.11096}, 2018.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Clune(2019)]{clune2019ai}
Jeff Clune.
\newblock Ai-gas: Ai-generating algorithms, an alternate paradigm for producing
  general artificial intelligence.
\newblock \emph{arXiv preprint arXiv:1905.10985}, 2019.

\bibitem[DeVries \& Taylor(2017)DeVries and Taylor]{devries2017improved}
Terrance DeVries and Graham~W Taylor.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock \emph{arXiv preprint arXiv:1708.04552}, 2017.

\bibitem[Elsayed et~al.(2018)Elsayed, Goodfellow, and
  Sohl{-}Dickstein]{DBLP:journals/corr/abs-1806-11146}
Gamaleldin~F. Elsayed, Ian~J. Goodfellow, and Jascha Sohl{-}Dickstein.
\newblock Adversarial reprogramming of neural networks.
\newblock \emph{CoRR}, abs/1806.11146, 2018.
\newblock URL \url{http://arxiv.org/abs/1806.11146}.

\bibitem[Elsken et~al.(2019)Elsken, Metzen, and Hutter]{elsken2018efficient}
Thomas Elsken, Jan~Hendrik Metzen, and Frank Hutter.
\newblock Efficient multi-objective neural architecture search via lamarckian
  evolution.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Fan et~al.(2018)Fan, Tian, Qin, Li, and Liu]{fan2018learning}
Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu.
\newblock Learning to teach.
\newblock \emph{arXiv preprint arXiv:1805.03643}, 2018.

\bibitem[Finn \& Levine(2017)Finn and Levine]{finn2017meta}
Chelsea Finn and Sergey Levine.
\newblock Meta-learning and universality: Deep representations and gradient
  descent can approximate any learning algorithm.
\newblock \emph{arXiv preprint arXiv:1710.11622}, 2017.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1126--1135. JMLR. org, 2017.

\bibitem[Fleiss(1993)]{fleiss1993review}
JL~Fleiss.
\newblock Review papers: The statistical basis of meta-analysis.
\newblock \emph{Statistical methods in medical research}, 2\penalty0
  (2):\penalty0 121--145, 1993.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2672--2680, 2014.

\bibitem[Graves et~al.(2017)Graves, Bellemare, Menick, Munos, and
  Kavukcuoglu]{curriculumGrave}
Alex Graves, Marc~G. Bellemare, Jacob Menick, R{\'{e}}mi Munos, and Koray
  Kavukcuoglu.
\newblock Automated curriculum learning for neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017}, pp.\
  1311--1320, 2017.

\bibitem[Griewank \& Walther(2000)Griewank and Walther]{griewank2000algorithm}
Andreas Griewank and Andrea Walther.
\newblock Algorithm 799: revolve: an implementation of checkpointing for the
  reverse or adjoint mode of computational differentiation.
\newblock \emph{ACM Transactions on Mathematical Software (TOMS)}, 26\penalty0
  (1):\penalty0 19--45, 2000.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  1026--1034, 2015.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and
  Dean]{journals/corr/HintonVD15}
Geoffrey~E. Hinton, Oriol Vinyals, and Jeffrey Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{CoRR}, abs/1503.02531, 2015.

\bibitem[Houthooft et~al.(2018)Houthooft, Chen, Isola, Stadie, Wolski,
  Jonathan~Ho, and Abbeel]{NIPS2018_7785}
Rein Houthooft, Yuhua Chen, Phillip Isola, Bradly Stadie, Filip Wolski, OpenAI
  Jonathan~Ho, and Pieter Abbeel.
\newblock Evolved policy gradients.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett (eds.), \emph{Advances in Neural Information Processing
  Systems 31}, pp.\  5405--5414. Curran Associates, Inc., 2018.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{arXiv preprint arXiv:1502.03167}, 2015.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Konyushkova et~al.(2017)Konyushkova, Sznitman, and
  Fua]{Konyushkova2017LearningAL}
Ksenia Konyushkova, Raphael Sznitman, and Pascal Fua.
\newblock Learning active learning from data.
\newblock In \emph{NIPS}, 2017.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Liu et~al.(2018{\natexlab{a}})Liu, Zoph, Neumann, Shlens, Hua, Li,
  Fei-Fei, Yuille, Huang, and Murphy]{liu2018progressive}
Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li,
  Li~Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy.
\newblock Progressive neural architecture search.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  19--34, 2018{\natexlab{a}}.

\bibitem[Liu et~al.(2018{\natexlab{b}})Liu, Simonyan, and Yang]{liu2018darts}
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
\newblock Darts: Differentiable architecture search.
\newblock \emph{arXiv preprint arXiv:1806.09055}, 2018{\natexlab{b}}.

\bibitem[Luo et~al.(2018)Luo, Tian, Qin, Chen, and Liu]{luo2018neural}
Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu.
\newblock Neural architecture optimization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  7816--7827, 2018.

\bibitem[Maas et~al.(2013)Maas, Hannun, and Ng]{maas2013rectifier}
Andrew~L Maas, Awni~Y Hannun, and Andrew~Y Ng.
\newblock Rectifier nonlinearities improve neural network acoustic models.
\newblock In \emph{Proc. icml}, volume~30, pp.\ ~3, 2013.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and
  Adams]{Maclaurin:2015:GHO:3045118.3045343}
Dougal Maclaurin, David Duvenaud, and Ryan~P. Adams.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In \emph{Proceedings of the 32Nd International Conference on
  International Conference on Machine Learning - Volume 37}, ICML'15, pp.\
  2113--2122. JMLR.org, 2015.

\bibitem[Metz et~al.(2018)Metz, Maheswaranathan, Cheung, and
  Sohl-Dickstein]{metz2018meta}
Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein.
\newblock Meta-learning update rules for unsupervised representation learning.
\newblock \emph{arXiv preprint arXiv:1804.00222}, 2018.

\bibitem[Metz et~al.(2019)Metz, Maheswaranathan, Nixon, Freeman, and
  Sohl-dickstein]{metz2019learned}
Luke Metz, Niru Maheswaranathan, Jeremy Nixon, Daniel Freeman, and Jascha
  Sohl-dickstein.
\newblock Learned optimizers that outperform on wall-clock and validation loss,
  2019.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1928--1937, 2016.

\bibitem[Nguyen et~al.(2015)Nguyen, Yosinski, and Clune]{nguyen2015deep}
Anh Nguyen, Jason Yosinski, and Jeff Clune.
\newblock Deep neural networks are easily fooled: High confidence predictions
  for unrecognizable images.
\newblock In \emph{In Computer Vision and Pattern Recognition (CVPR '15)},
  2015.

\bibitem[Pham et~al.(2018)Pham, Guan, Zoph, Le, and Dean]{pmlr-v80-pham18a}
Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean.
\newblock Efficient neural architecture search via parameters sharing.
\newblock In Jennifer Dy and Andreas Krause (eds.), \emph{Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pp.\  4095--4104,
  Stockholmsmässan, Stockholm Sweden, 10--15 Jul 2018. PMLR.

\bibitem[Real et~al.(2019)Real, Aggarwal, Huang, and Le]{real2019regularized}
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc~V Le.
\newblock Regularized evolution for image classifier architecture search.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  4780--4789, 2019.

\bibitem[Salimans \& Kingma(2016)Salimans and Kingma]{salimans2016weight}
Tim Salimans and Durk~P Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  901--909, 2016.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{GanModelCollapse}
Tim Salimans, Ian~J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
  and Xi~Chen.
\newblock Improved techniques for training gans.
\newblock In \emph{NIPS}, 2016.

\bibitem[Salimans et~al.(2017)Salimans, Ho, Chen, Sidor, and
  Sutskever]{salimans2017evolution}
Tim Salimans, Jonathan Ho, Xi~Chen, Szymon Sidor, and Ilya Sutskever.
\newblock Evolution strategies as a scalable alternative to reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1703.03864}, 2017.

\bibitem[Sener \& Savarese(2018)Sener and Savarese]{sener2018active}
Ozan Sener and Silvio Savarese.
\newblock Active learning for convolutional neural networks: A core-set
  approach.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Settles(2010)]{Settles10activelearning}
Burr Settles.
\newblock Active learning literature survey.
\newblock Technical report, 2010.

\bibitem[Siegelmann \& Sontag(1995)Siegelmann and
  Sontag]{siegelmann1995computational}
Hava~T Siegelmann and Eduardo~D Sontag.
\newblock On the computational power of neural nets.
\newblock \emph{Journal of computer and system sciences}, 50\penalty0
  (1):\penalty0 132--150, 1995.

\bibitem[Srivastava et~al.(2017)Srivastava, Valkov, Russell, Gutmann, and
  Sutton]{NIPS2017_6923}
Akash Srivastava, Lazar Valkov, Chris Russell, Michael~U. Gutmann, and Charles
  Sutton.
\newblock Veegan: Reducing mode collapse in gans using implicit variational
  learning.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett (eds.), \emph{Advances in Neural Information
  Processing Systems 30}, pp.\  3308--3318. Curran Associates, Inc., 2017.

\bibitem[Stanley et~al.(2017)Stanley, Lehman, and Soros]{stanley2017open}
Kenneth~O. Stanley, Joel Lehman, and Lisa Soros.
\newblock Open-endedness: The last grand challenge you’ve never heard of.
\newblock \emph{O'Reilly Online}, 2017.
\newblock URL \url{https://www.oreilly.com/ideas/
  open-endedness-the-last-grand-challenge-youve-never-heard-of}.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6199}, 2013.

\bibitem[Tobin et~al.(2017)Tobin, Fong, Ray, Schneider, Zaremba, and
  Abbeel]{tobin2017domain}
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and
  Pieter Abbeel.
\newblock Domain randomization for transferring deep neural networks from
  simulation to the real world.
\newblock In \emph{2017 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pp.\  23--30. IEEE, 2017.

\bibitem[Toneva et~al.(2018)Toneva, Sordoni, Combes, Trischler, Bengio, and
  Gordon]{toneva2018empirical}
Mariya Toneva, Alessandro Sordoni, Remi Tachet~des Combes, Adam Trischler,
  Yoshua Bengio, and Geoffrey~J Gordon.
\newblock An empirical study of example forgetting during deep neural network
  learning.
\newblock \emph{arXiv preprint arXiv:1812.05159}, 2018.

\bibitem[Tsang et~al.(2005)Tsang, Kwok, and Cheung]{article}
Ivor Tsang, James Kwok, and Pak-Ming Cheung.
\newblock Core vector machines: Fast svm training on very large data sets.
\newblock \emph{Journal of Machine Learning Research}, 6:\penalty0 363--392, 04
  2005.

\bibitem[Wang et~al.(2019{\natexlab{a}})Wang, Lehman, Clune, and
  Stanley]{wang2019paired}
Rui Wang, Joel Lehman, Jeff Clune, and Kenneth~O Stanley.
\newblock Paired open-ended trailblazer (poet): Endlessly generating
  increasingly complex and diverse learning environments and their solutions.
\newblock \emph{arXiv preprint arXiv:1901.01753}, 2019{\natexlab{a}}.

\bibitem[Wang et~al.(2019{\natexlab{b}})Wang, Zhu, Torralba, and
  Efros]{wang2019dataset}
Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei~A. Efros.
\newblock Dataset distillation, 2019{\natexlab{b}}.

\bibitem[Zhang et~al.(2018)Zhang, Ren, and Urtasun]{zhang2018graph}
Chris Zhang, Mengye Ren, and Raquel Urtasun.
\newblock Graph hypernetworks for neural architecture search.
\newblock \emph{arXiv preprint arXiv:1810.05749}, 2018.

\bibitem[Zoph \& Le(2017)Zoph and Le]{ZophNAS}
Barret Zoph and Quoc~V. Le.
\newblock Neural architecture search with reinforcement learning.
\newblock 2017.
\newblock URL \url{https://arxiv.org/abs/1611.01578}.

\end{thebibliography}
