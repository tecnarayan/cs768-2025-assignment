%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Andrea Zanette at 2021-06-10 16:21:23 -0700 


%% Saved with string encoding Unicode (UTF-8) 


@string{aaai = {AAAI Conference on Artificial Intelligence (AAAI)}}

@string{aamas = {International Conference on Autonomous Agents and Multiagent Systems (AAMAS)}}

@string{acc = {American Control Conference (ACC)}}

@string{aiaa_info = {AIAA Infotech@Aerospace Conference}}

@string{aiaa_jacic = {Journal of Aerospace Computing, Information, and Communication}}

@string{allerton = {Allerton Conference on Communication, Control, and Compution}}

@string{atio = {AIAA Aviation Technology, Integration, and Operations Conference (ATIO)}}

@string{cacm = {Communications of the ACM}}

@string{cdc = {IEEE Conference on Decision and Control (CDC)}}

@string{colt = {Conference on Learning Theory (COLT)}}

@string{cvpr = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)}}

@string{dasc = {Digital Avionics Systems Conference (DASC)}}

@string{ecml = {European Conference on Machine Learning (ECML)}}

@string{gnc = {AIAA Guidance, Navigation, and Control Conference (GNC)}}

@string{icaart = {International Conference on Agents and Artificial Intelligence (ICAART)}}

@string{icaps = {International Conference on Automated Planning and Scheduling (ICAPS)}}

@string{icassp = {International Conference on Acoustics, Speech, and Signal Processing (ICASSP)}}

@string{iclr = {International Conference on Learning Representations}}

@string{icml = {International Conference on Machine Learning (ICML)}}

@string{icmla = {International Conference on Machine Learning and Applications (ICMLA)}}

@string{icra = {IEEE International Conference on Robotics and Automation (ICRA)}}

@string{icslp = {International Conference on Spoken Language Processing (ICSLP)}}

@string{ieee_csm = {IEEE Control Systems Magazine}}

@string{ieee_j_ac = {IEEE Transactions on Automatic Control}}

@string{ieeeaero = {IEEE Aerospace Conference}}

@string{ieeeciaig = {IEEE Transactions on Computational Intelligence and AI in Games}}

@string{ieeecst = {IEEE Transactions on Control Systems Technology}}

@string{ieeetac = {IEEE Transactions on Automatic Control}}

@string{ieeetsp = {IEEE Transactions on Signal Processing}}

@string{ijcai = {International Joint Conference on Artificial Intelligence (IJCAI)}}

@string{interspeech = {Annual Conference of the International Speech Communication Association (INTERSPEECH)}}

@string{iros = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}}

@string{itsc = {IEEE International Conference on Intelligent Transportation Systems (ITSC)}}

@string{iv = {IEEE Intelligent Vehicles Symposium (IV)}}

@string{jair = {Journal of Artificial Intelligence Research}}

@string{jgcd = {AIAA Journal of Guidance, Control, and Dynamics}}

@string{jmlr = {Journal of Machine Learning Research}}

@string{jota = {Journal of Optimization Theory and Applications}}

@string{lion = {Learning and Intelligent Optimization (LION)}}

@string{mit = {Massachusetts Institute of Technology}}

@string{mitaa = {Massachusetts Institute of Technology, Department of Aeronautics and Astronautics}}

@string{mitee = {Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science}}

@string{mitme = {Massachusetts Institute of Technology, Department of Mechanical Engineering}}

@string{mor = {Mathematics of Operations Research}}

@string{nips = {Advances in Neural Information Processing Systems (NIPS)}}

@string{or = {Operations Research}}

@string{rss = {Robotics: Science and Systems}}

@string{sigcomm = {ACM Special Interest Group on Data Communication (SIGCOMM)}}

@string{suaa = {Stanford University, Department of Aeronautics and Astronautics}}

@string{suee = {Stanford University, Department of Electrical Engineering}}

@string{sume = {Stanford University, Department of Mechanical Engineering}}

@string{tac = {IEEE Transactions on Automatic Control}}

@string{tacas = {International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS)}}

@string{taes = {IEEE Transactions on Aerospace and Electronic Systems}}

@string{uai = {Conference on Uncertainty in Artificial Intelligence (UAI)}}


@inproceedings{lattimore2020learning,
	abstract = {The construction in the recent paper by Du et al. [2019] implies that searching for a near-optimal action in a bandit sometimes requires examining essentially all the actions, even if the learner is given linear features in R^d that approximate the rewards with a small uniform error. We use the Kiefer-Wolfowitz theorem to prove a positive result that by checking only a few actions, a learner can always find an action that is suboptimal with an error of at most O($\epsilon$$\sqrt{}$d) where $\epsilon$ is the approximation error of the features. Thus, features are useful when the approximation error is small relative to the dimensionality of the features. The idea is applied to stochastic bandits and reinforcement learning with a generative model where the learner has access to d-dimensional linear features that approximate the action-value functions for all policies to an accuracy of $\epsilon$. For linear bandits, we prove a bound on the regret of order d$\sqrt{}$(n log(k)) + $\epsilon$n$\sqrt{}$d log(n) with k the number of actions and n the horizon. For RL we show that approximate policy iteration can learn a policy that is optimal up to an additive error of order $\epsilon$$\sqrt{}$d/(1 − $\gamma$)^2 and using about d/($\epsilon$^2(1 − $\gamma$)^4) samples from the generative model. These bounds are independent of the finer details of the features. We also investigate how the structure of the feature set impacts the tradeoff between sample complexity and estimation error.},
	author = {Lattimore, Tor and Szepesvari, Csaba and Weisz, Gellert},
	booktitle = {Proceedings of the 37th International Conference on Machine Learning},
	date-added = {2021-06-10 16:20:43 -0700},
	date-modified = {2021-06-10 16:21:20 -0700},
	editor = {III, Hal Daum{\'e} and Singh, Aarti},
	month = {13--18 Jul},
	pages = {5662--5670},
	pdf = {http://proceedings.mlr.press/v119/lattimore20a/lattimore20a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Learning with Good Feature Representations in Bandits and in {RL} with a Generative Model},
	url = {http://proceedings.mlr.press/v119/lattimore20a.html},
	volume = {119},
	year = {2020},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v119/lattimore20a.html}}

@misc{amortila2020variant,
	archiveprefix = {arXiv},
	author = {Philip Amortila and Nan Jiang and Tengyang Xie},
	date-added = {2020-11-20 17:23:36 -0800},
	date-modified = {2020-11-20 17:23:36 -0800},
	eprint = {2011.01075},
	primaryclass = {cs.LG},
	title = {A Variant of the Wang-Foster-Kakade Lower Bound for the Discounted Setting},
	year = {2020}}

@inproceedings{xie2020Q,
	abstract = {We prove performance guarantees of two algorithms for approximating Q* in batch reinforcement learning. Compared to classical iterative methods such as Fitted Q-Iteration---whose performance loss incurs quadratic dependence on horizon---these methods estimate (some forms of) the Bellman error and enjoy linear-in-horizon error propagation, a property established for the first time for algorithms that rely solely on batch data and output stationary policies. One of the algorithms uses a novel and explicit importance-weighting correction to overcome the infamous "double sampling" difficulty in Bellman error estimation, and does not use any squared losses. Our analyses reveal its distinct characteristics and potential advantages compared to classical algorithms. },
	address = {Virtual},
	author = {Xie, Tengyang and Jiang, Nan},
	date-added = {2020-11-20 14:58:36 -0800},
	date-modified = {2020-11-20 14:58:49 -0800},
	editor = {Jonas Peters and David Sontag},
	month = {03--06 Aug},
	pages = {550--559},
	pdf = {http://proceedings.mlr.press/v124/xie20a/xie20a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Q* Approximation Schemes for Batch Reinforcement Learning: A Theoretical Comparison},
	url = {http://proceedings.mlr.press/v124/xie20a.html},
	volume = {124},
	year = {2020},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v124/xie20a.html}}

@inproceedings{xie2019towards,
	author = {Xie, Tengyang and Ma, Yifei and Wang, Yu-Xiang},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-11-20 14:49:01 -0800},
	date-modified = {2020-11-20 14:49:01 -0800},
	pages = {9668--9678},
	title = {Towards optimal off-policy evaluation for reinforcement learning with marginalized importance sampling},
	year = {2019}}

@inproceedings{liu2018breaking,
	author = {Liu, Qiang and Li, Lihong and Tang, Ziyang and Zhou, Dengyong},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-11-20 14:45:08 -0800},
	date-modified = {2020-11-20 14:45:08 -0800},
	pages = {5356--5366},
	title = {Breaking the curse of horizon: Infinite-horizon off-policy estimation},
	year = {2018}}

@article{li2015toward,
	author = {Li, Lihong and Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
	date-added = {2020-11-20 14:40:59 -0800},
	date-modified = {2020-11-20 14:40:59 -0800},
	title = {Toward minimax off-policy value estimation},
	year = {2015}}

@inproceedings{thomas2016data,
	author = {Thomas, Philip and Brunskill, Emma},
	booktitle = {International Conference on Machine Learning},
	date-added = {2020-11-20 14:17:13 -0800},
	date-modified = {2020-11-20 14:17:13 -0800},
	pages = {2139--2148},
	title = {Data-efficient off-policy policy evaluation for reinforcement learning},
	year = {2016}}

@article{precup2000eligibility,
	author = {Precup, Doina},
	date-added = {2020-11-20 14:13:28 -0800},
	date-modified = {2020-11-20 14:13:28 -0800},
	journal = {Computer Science Department Faculty Publication Series},
	pages = {80},
	title = {Eligibility traces for off-policy policy evaluation},
	year = {2000}}

@inproceedings{jiang2016doubly,
	author = {Jiang, Nan and Li, Lihong},
	booktitle = {International Conference on Machine Learning},
	date-added = {2020-11-20 14:07:54 -0800},
	date-modified = {2020-11-20 14:07:54 -0800},
	organization = {PMLR},
	pages = {652--661},
	title = {Doubly robust off-policy value evaluation for reinforcement learning},
	year = {2016}}

@article{liu2020provably,
	author = {Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
	date-added = {2020-11-15 18:29:15 -0800},
	date-modified = {2020-11-15 18:29:15 -0800},
	journal = {arXiv preprint arXiv:2007.08202},
	title = {Provably good batch reinforcement learning without great exploration},
	year = {2020}}

@misc{hao2020sparse,
	archiveprefix = {arXiv},
	author = {Botao Hao and Yaqi Duan and Tor Lattimore and Csaba Szepesv{\'a}ri and Mengdi Wang},
	date-added = {2020-11-11 10:35:18 -0800},
	date-modified = {2020-11-11 10:35:18 -0800},
	eprint = {2011.04019},
	primaryclass = {cs.LG},
	title = {Sparse Feature Selection Makes Batch Reinforcement Learning More Sample Efficient},
	year = {2020}}

@article{wagenmaker2020experimental,
	author = {Wagenmaker, Andrew and Katz-Samuels, Julian and Jamieson, Kevin},
	date-added = {2020-11-11 08:57:31 -0800},
	date-modified = {2020-11-11 08:57:31 -0800},
	journal = {arXiv preprint arXiv:2011.00576},
	title = {Experimental Design for Regret Minimization in Linear Bandits},
	year = {2020}}

@article{xie2020batch,
	author = {Xie, Tengyang and Jiang, Nan},
	date-added = {2020-11-03 13:22:08 -0800},
	date-modified = {2020-11-03 13:22:08 -0800},
	journal = {arXiv preprint arXiv:2008.04990},
	title = {Batch Value-function Approximation with Only Realizability},
	year = {2020}}

@article{bradtke1996linear,
	author = {Bradtke, Steven J and Barto, Andrew G},
	date-added = {2020-11-03 12:56:44 -0800},
	date-modified = {2020-11-03 12:56:44 -0800},
	journal = {Machine learning},
	number = {1-3},
	pages = {33--57},
	publisher = {Springer},
	title = {Linear least-squares algorithms for temporal difference learning},
	volume = {22},
	year = {1996}}

@article{cui2020plug,
	author = {Cui, Qiwen and Yang, Lin F},
	date-added = {2020-11-03 12:48:16 -0800},
	date-modified = {2020-11-03 12:48:16 -0800},
	journal = {arXiv preprint arXiv:2010.05673},
	title = {Is Plug-in Solver Sample-Efficient for Feature-based Reinforcement Learning?},
	year = {2020}}

@article{duan2020minimax,
	author = {Duan, Yaqi and Wang, Mengdi},
	date-added = {2020-11-03 12:19:13 -0800},
	date-modified = {2020-11-03 12:19:13 -0800},
	journal = {arXiv preprint arXiv:2002.09516},
	title = {Minimax-Optimal Off-Policy Evaluation with Linear Function Approximation},
	year = {2020}}

@inproceedings{agarwal2020optimality,
	author = {Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
	booktitle = {Conference on Learning Theory},
	date-added = {2020-11-03 12:11:38 -0800},
	date-modified = {2020-11-03 12:11:38 -0800},
	pages = {64--66},
	title = {Optimality and approximation with policy gradient methods in Markov decision processes},
	year = {2020}}

@article{marjani2020best,
	author = {Marjani, Aymen Al and Proutiere, Alexandre},
	date-added = {2020-11-03 10:04:35 -0800},
	date-modified = {2020-11-03 10:04:35 -0800},
	journal = {arXiv preprint arXiv:2009.13405},
	title = {Best Policy Identification in discounted MDPs: Problem-specific Sample Complexity},
	year = {2020}}

@book{pukelsheim2006optimal,
	author = {Pukelsheim, Friedrich},
	date-added = {2020-11-02 08:10:37 -0800},
	date-modified = {2020-11-02 08:10:37 -0800},
	publisher = {SIAM},
	title = {Optimal design of experiments},
	year = {2006}}

@article{yin2020near,
	author = {Yin, Ming and Bai, Yu and Wang, Yu-Xiang},
	date-added = {2020-11-01 12:28:59 -0800},
	date-modified = {2020-11-01 12:28:59 -0800},
	journal = {arXiv preprint arXiv:2007.03760},
	title = {Near Optimal Provable Uniform Convergence in Off-Policy Evaluation for Reinforcement Learning},
	year = {2020}}

@article{wang2020statistical,
	author = {Wang, Ruosong and Foster, Dean P and Kakade, Sham M},
	date-added = {2020-11-01 08:52:20 -0800},
	date-modified = {2020-11-01 08:52:20 -0800},
	journal = {arXiv preprint arXiv:2010.11895},
	title = {What are the Statistical Limits of Offline RL with Linear Function Approximation?},
	year = {2020}}

@article{weisz2020exponential,
	author = {Weisz, Gellert and Amortila, Philip and Szepesv{\'a}ri, Csaba},
	date-added = {2020-11-01 08:51:36 -0800},
	date-modified = {2020-11-01 08:51:36 -0800},
	journal = {arXiv preprint arXiv:2010.01374},
	title = {Exponential Lower Bounds for Planning in MDPs With Linearly-Realizable Optimal Action-Value Functions},
	year = {2020}}

@inproceedings{agarwal2019model,
	author = {Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
	booktitle = {Conference on Learning Theory},
	date-added = {2020-10-31 11:11:17 -0700},
	date-modified = {2020-10-31 11:11:17 -0700},
	pages = {67--83},
	title = {Model-based reinforcement learning with a generative model is minimax optimal},
	year = {2020}}

@conference{zanette2020provably,
	author = {Zanette, Andrea and Lazaric, Alessandro and Kochenderfer, Mykel J and Brunskill, Emma},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-10-12 17:47:05 -0700},
	date-modified = {2020-10-12 17:47:37 -0700},
	title = {Provably Efficient Reward-Agnostic Navigation with Linear Value Iteration},
	year = {2020}}

@article{ayoub2020model,
	author = {Ayoub, Alex and Jia, Zeyu and Szepesvari, Csaba and Wang, Mengdi and Yang, Lin F},
	date-added = {2020-10-12 17:41:28 -0700},
	date-modified = {2020-10-12 17:41:28 -0700},
	journal = {arXiv preprint arXiv:2006.01107},
	title = {Model-Based Reinforcement Learning with Value-Targeted Regression},
	year = {2020}}

@article{cai2019provably,
	author = {Cai, Qi and Yang, Zhuoran and Jin, Chi and Wang, Zhaoran},
	date-added = {2020-10-12 17:39:52 -0700},
	date-modified = {2020-10-12 17:39:52 -0700},
	journal = {arXiv preprint arXiv:1912.05830},
	title = {Provably efficient exploration in policy optimization},
	year = {2019}}

@article{zhou2020provably,
	author = {Zhou, Dongruo and He, Jiafan and Gu, Quanquan},
	date-added = {2020-10-12 17:37:21 -0700},
	date-modified = {2020-10-12 17:37:21 -0700},
	journal = {arXiv preprint arXiv:2006.13165},
	title = {Provably Efficient Reinforcement Learning for Discounted MDPs with Feature Mapping},
	year = {2020}}

@inproceedings{munos2003error,
	author = {Munos, R{\'e}mi},
	booktitle = {ICML},
	date-added = {2020-10-12 17:30:10 -0700},
	date-modified = {2020-10-12 17:30:10 -0700},
	pages = {560--567},
	title = {Error bounds for approximate policy iteration},
	volume = {3},
	year = {2003}}

@article{zhang2020reinforcement,
	author = {Zhang, Zihan and Ji, Xiangyang and Du, Simon S},
	date-added = {2020-10-12 17:11:41 -0700},
	date-modified = {2020-10-12 17:11:41 -0700},
	journal = {arXiv preprint arXiv:2009.13503},
	title = {Is Reinforcement Learning More Difficult Than Bandits? A Near-optimal Algorithm Escaping the Curse of Horizon},
	year = {2020}}

@article{li2020breaking,
	author = {Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
	date-added = {2020-10-12 17:07:51 -0700},
	date-modified = {2020-10-12 17:07:51 -0700},
	journal = {arXiv preprint arXiv:2005.12900},
	title = {Breaking the sample size barrier in model-based reinforcement learning with a generative model},
	year = {2020}}

@article{li2011concise,
	author = {Li, Shengqiao},
	date-added = {2020-10-08 17:16:41 -0700},
	date-modified = {2020-10-08 17:16:41 -0700},
	journal = {Asian Journal of Mathematics and Statistics},
	number = {1},
	pages = {66--70},
	title = {Concise formulas for the area and volume of a hyperspherical cap},
	volume = {4},
	year = {2011}}

@article{agarwal2020pc,
	author = {Agarwal, Alekh and Henaff, Mikael and Kakade, Sham and Sun, Wen},
	date-added = {2020-09-10 12:39:05 -0700},
	date-modified = {2020-09-10 12:39:05 -0700},
	journal = {arXiv preprint arXiv:2007.08459},
	title = {Pc-pg: Policy cover directed exploration for provable policy gradient learning},
	year = {2020}}

@inproceedings{mehta2017fast,
	author = {Mehta, Nishant},
	booktitle = {Artificial Intelligence and Statistics},
	date-added = {2020-09-10 10:44:05 -0700},
	date-modified = {2020-09-10 10:44:05 -0700},
	organization = {PMLR},
	pages = {1085--1093},
	title = {Fast rates with high probability in exp-concave statistical learning},
	year = {2017}}

@book{shalev2014understanding,
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	date-added = {2020-07-31 14:48:04 -0700},
	date-modified = {2020-07-31 14:48:04 -0700},
	publisher = {Cambridge university press},
	title = {Understanding machine learning: From theory to algorithms},
	year = {2014}}

@misc{gao2017properties,
	archiveprefix = {arXiv},
	author = {Bolin Gao and Lacra Pavel},
	date-added = {2020-06-30 20:18:17 -0700},
	date-modified = {2020-06-30 20:18:17 -0700},
	eprint = {1704.00805},
	primaryclass = {math.OC},
	title = {On the Properties of the Softmax Function with Application in Game Theory and Reinforcement Learning},
	year = {2017}}

@article{auer2002using,
	author = {Auer, Peter},
	date-added = {2020-06-28 16:11:04 -0700},
	date-modified = {2020-06-28 16:11:04 -0700},
	journal = {Journal of Machine Learning Research},
	number = {Nov},
	pages = {397--422},
	title = {Using confidence bounds for exploitation-exploration trade-offs},
	volume = {3},
	year = {2002}}

@inproceedings{krishnamurthy2018semiparametric,
	author = {Krishnamurthy, Akshay and Wu, Steven and Syrgkanis, Vasilis},
	booktitle = {35th International Conference on Machine Learning, ICML 2018},
	date-added = {2020-06-28 16:10:05 -0700},
	date-modified = {2020-06-28 16:10:05 -0700},
	organization = {International Machine Learning Society (IMLS)},
	pages = {4330--4349},
	title = {Semiparametric contextual bandits},
	year = {2018}}

@article{gopalan2016low,
	author = {Gopalan, Aditya and Maillard, Odalric-Ambrym and Zaki, Mohammadi},
	date-added = {2020-06-28 16:07:49 -0700},
	date-modified = {2020-06-28 16:07:49 -0700},
	journal = {arXiv preprint arXiv:1609.01508},
	title = {Low-rank bandits with latent mixtures},
	year = {2016}}

@inproceedings{ghosh2017misspecified,
	author = {Ghosh, Avishek and Chowdhury, Sayak Ray and Gopalan, Aditya},
	booktitle = {Thirty-First AAAI Conference on Artificial Intelligence},
	date-added = {2020-06-28 16:07:09 -0700},
	date-modified = {2020-06-28 16:07:09 -0700},
	title = {Misspecified linear bandits},
	year = {2017}}

@article{tossou2019near,
	author = {Tossou, Aristide and Basu, Debabrota and Dimitrakakis, Christos},
	date-added = {2020-06-28 16:01:25 -0700},
	date-modified = {2020-06-28 16:01:25 -0700},
	journal = {arXiv preprint arXiv:1905.12425},
	title = {Near-optimal optimistic reinforcement learning using empirical bernstein inequalities},
	year = {2019}}

@inproceedings{zhang2019regret,
	author = {Zhang, Zihan and Ji, Xiangyang},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-06-28 16:00:21 -0700},
	date-modified = {2020-06-28 16:00:21 -0700},
	pages = {2827--2836},
	title = {Regret minimization for reinforcement learning by evaluating the optimal bias function},
	year = {2019}}

@conference{misra2019kinematic,
	author = {Misra, Dipendra and Henaff, Mikael and Krishnamurthy, Akshay and Langford, John},
	booktitle = {International Conference on Machine Learning (ICML)},
	date-added = {2020-06-01 09:57:38 -0700},
	date-modified = {2020-06-04 13:14:40 -0700},
	journal = {arXiv preprint arXiv:1911.05815},
	title = {Kinematic State Abstraction and Provably Efficient Rich-Observation Reinforcement Learning},
	year = {2020}}

@inproceedings{du2019provablyefficient,
	abstract = {We study the exploration problem in episodic MDPs with rich observations generated from a small number of latent states. Under certain identifiability assumptions, we demonstrate how to estimate a mapping from the observations to latent states inductively through a sequence of regression and clustering steps---where previously decoded latent states provide labels for later regression problems---and use it to construct good exploration policies. We provide finite-sample guarantees on the quality of the learned state decoding function and exploration policies, and complement our theory with an empirical evaluation on a class of hard exploration problems. Our method exponentially improves over $Q$-learning with na{\"\i}ve exploration, even when $Q$-learning has cheating access to latent states.},
	address = {Long Beach, California, USA},
	author = {Du, Simon and Krishnamurthy, Akshay and Jiang, Nan and Agarwal, Alekh and Dudik, Miroslav and Langford, John},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	date-added = {2020-06-01 09:43:00 -0700},
	date-modified = {2020-06-01 09:43:34 -0700},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	month = {09--15 Jun},
	pages = {1665--1674},
	pdf = {http://proceedings.mlr.press/v97/du19b/du19b.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Provably efficient {RL} with Rich Observations via Latent State Decoding},
	url = {http://proceedings.mlr.press/v97/du19b.html},
	volume = {97},
	year = {2019},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v97/du19b.html}}

@article{kiefer1960equivalence,
	author = {Kiefer, Jack and Wolfowitz, Jacob},
	date-added = {2020-05-31 23:04:19 -0700},
	date-modified = {2020-05-31 23:04:19 -0700},
	journal = {Canadian Journal of Mathematics},
	pages = {363--366},
	publisher = {Cambridge University Press},
	title = {The equivalence of two extremum problems},
	volume = {12},
	year = {1960}}

@inproceedings{gopalan2015thompson,
	author = {Gopalan, Aditya and Mannor, Shie},
	booktitle = {Conference on Learning Theory},
	date-added = {2020-05-31 15:57:25 -0700},
	date-modified = {2020-05-31 15:57:25 -0700},
	pages = {861--898},
	title = {Thompson sampling for learning parameterized markov decision processes},
	year = {2015}}

@inproceedings{ouyang2017learning,
	author = {Ouyang, Yi and Gagrani, Mukul and Nayyar, Ashutosh and Jain, Rahul},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-05-31 15:56:26 -0700},
	date-modified = {2020-05-31 15:56:26 -0700},
	pages = {1333--1342},
	title = {Learning unknown markov decision processes: A thompson sampling approach},
	year = {2017}}

@book{lattimore2020bandit,
	author = {Lattimore, Tor and Szepesv{\'a}ri, Csaba},
	date-added = {2020-05-30 17:23:51 -0700},
	date-modified = {2020-05-30 17:24:55 -0700},
	publisher = {Cambridge University Press},
	title = {Bandit Algorithms},
	year = {2020}}

@misc{du2020agnostic,
	archiveprefix = {arXiv},
	author = {Simon S. Du and Jason D. Lee and Gaurav Mahajan and Ruosong Wang},
	date-added = {2020-05-30 16:28:25 -0700},
	date-modified = {2020-05-30 16:28:25 -0700},
	eprint = {2002.07125},
	primaryclass = {cs.LG},
	title = {Agnostic Q-learning with Function Approximation in Deterministic Systems: Tight Bounds on Approximation Error and Sample Complexity},
	year = {2020}}

@inproceedings{du2019provably,
	author = {Du, Simon S and Luo, Yuping and Wang, Ruosong and Zhang, Hanrui},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-05-30 09:07:41 -0700},
	date-modified = {2020-05-30 09:07:41 -0700},
	pages = {8058--8068},
	title = {Provably efficient Q-learning with function approximation via distribution shift error checking oracle},
	year = {2019}}

@misc{wang2020provably,
	archiveprefix = {arXiv},
	author = {Ruosong Wang and Ruslan Salakhutdinov and Lin F. Yang},
	date-added = {2020-05-26 14:46:46 -0700},
	date-modified = {2020-05-26 14:46:46 -0700},
	eprint = {2005.10804},
	primaryclass = {cs.LG},
	title = {Provably Efficient Reinforcement Learning with General Value Function Approximation},
	year = {2020}}

@article{wang2019optimism,
	author = {Wang, Yining and Wang, Ruosong and Du, Simon S and Krishnamurthy, Akshay},
	date-added = {2020-05-26 14:40:37 -0700},
	date-modified = {2020-05-26 14:40:37 -0700},
	journal = {arXiv preprint arXiv:1912.04136},
	title = {Optimism in Reinforcement Learning with Generalized Linear Function Approximation},
	year = {2019}}

@book{kochenderfer2019algorithms,
	author = {Kochenderfer, Mykel J and Wheeler, Tim A},
	date-added = {2020-05-25 13:32:03 -0700},
	date-modified = {2020-05-25 13:32:03 -0700},
	publisher = {Mit Press},
	title = {Algorithms for optimization},
	year = {2019}}

@misc{hazan2018provably,
	archiveprefix = {arXiv},
	author = {Elad Hazan and Sham M. Kakade and Karan Singh and Abby Van Soest},
	date-added = {2020-05-13 18:07:48 -0700},
	date-modified = {2020-05-13 18:07:48 -0700},
	eprint = {1812.02690},
	primaryclass = {cs.LG},
	title = {Provably Efficient Maximum Entropy Exploration},
	year = {2018}}

@conference{jin2020rewardfree,
	archiveprefix = {arXiv},
	author = {Chi Jin and Akshay Krishnamurthy and Max Simchowitz and Tiancheng Yu},
	booktitle = {International Conference on Machine Learning (ICML)},
	date-added = {2020-05-13 17:46:27 -0700},
	date-modified = {2020-06-02 11:17:56 -0700},
	eprint = {2002.02794},
	primaryclass = {cs.LG},
	title = {Reward-Free Exploration for Reinforcement Learning},
	year = {2020}}

@conference{zanette2020learning,
	archiveprefix = {arXiv},
	author = {Andrea Zanette and Alessandro Lazaric and Mykel Kochenderfer and Emma Brunskill},
	booktitle = {International Conference on Machine Learning (ICML)},
	date-added = {2020-04-01 15:05:33 -0700},
	date-modified = {2020-06-02 10:10:47 -0700},
	eprint = {2003.00153},
	primaryclass = {cs.LG},
	title = {Learning Near Optimal Policies with Low Inherent Bellman Error},
	year = {2020}}

@article{shreve1978alternative,
	author = {Shreve, Steven E and Bertsekas, Dimitri P},
	date-added = {2019-12-18 18:13:22 +0100},
	date-modified = {2019-12-18 18:13:22 +0100},
	journal = {SIAM Journal on control and optimization},
	number = {6},
	pages = {953--978},
	publisher = {SIAM},
	title = {Alternative theoretical frameworks for finite horizon discrete-time stochastic optimal control},
	volume = {16},
	year = {1978}}

@book{puterman1994markov,
	address = {New York, NY, USA},
	author = {Puterman, Martin L.},
	date-added = {2019-12-18 17:58:02 +0100},
	date-modified = {2019-12-18 17:58:02 +0100},
	isbn = {0471619779},
	publisher = {John Wiley \& Sons, Inc.},
	title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
	year = {1994}}

@inproceedings{dann2019policy,
	author = {Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma},
	booktitle = {International Conference on Machine Learning},
	date-added = {2019-12-18 17:46:26 +0100},
	date-modified = {2019-12-18 17:46:26 +0100},
	pages = {1507--1516},
	title = {Policy Certificates: Towards Accountable Reinforcement Learning},
	year = {2019}}

@article{de2000existence,
	author = {De Farias, Daniela Pucci and Van Roy, Benjamin},
	date-added = {2019-11-28 16:12:26 -0800},
	date-modified = {2019-11-28 16:12:26 -0800},
	journal = {Journal of Optimization theory and Applications},
	number = {3},
	pages = {589--608},
	publisher = {Springer},
	title = {On the existence of fixed points for approximate value iteration and temporal-difference learning},
	volume = {105},
	year = {2000}}

@inproceedings{chen2019information,
	author = {Chen, Jinglin and Jiang, Nan},
	booktitle = {International Conference on Machine Learning},
	date-added = {2019-11-27 14:37:35 -0800},
	date-modified = {2019-11-27 14:37:35 -0800},
	pages = {1042--1051},
	title = {Information-Theoretic Considerations in Batch Reinforcement Learning},
	year = {2019}}

@article{van2019comments,
	author = {Van Roy, Benjamin and Dong, Shi},
	date-added = {2019-11-26 16:14:57 -0800},
	date-modified = {2019-11-26 16:14:57 -0800},
	journal = {arXiv preprint arXiv:1911.07910},
	title = {Comments on the Du-Kakade-Wang-Yang Lower Bounds},
	year = {2019}}

@article{abbasi2019exploration,
	author = {Abbasi-Yadkori, Yasin and Lazic, Nevena and Szepesvari, Csaba and Weisz, Gellert},
	date-added = {2019-11-26 11:11:37 -0800},
	date-modified = {2019-11-26 11:11:37 -0800},
	journal = {arXiv preprint arXiv:1908.10479},
	title = {Exploration-Enhanced POLITEX},
	year = {2019}}

@inproceedings{abbasi2019politex,
	author = {Abbasi-Yadkori, Yasin and Bartlett, Peter and Bhatia, Kush and Lazic, Nevena and Szepesv{\'a}ri, Csaba and Weisz, Gell{\'e}rt},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	date-added = {2019-11-26 11:11:16 -0800},
	date-modified = {2019-11-26 11:11:16 -0800},
	pages = {3692--3702},
	title = {POLITEX: Regret bounds for policy iteration using expert prediction},
	volume = {97},
	year = {2019}}

@article{lazaric2012finite,
	author = {Lazaric, Alessandro and Ghavamzadeh, Mohammad and Munos, R{\'e}mi},
	date-added = {2019-11-26 10:56:14 -0800},
	date-modified = {2019-11-26 10:56:14 -0800},
	journal = {Journal of Machine Learning Research},
	number = {Oct},
	pages = {3041--3074},
	title = {Finite-sample analysis of least-squares policy iteration},
	volume = {13},
	year = {2012}}

@article{lagoudakis2003least,
	author = {Lagoudakis, Michail G and Parr, Ronald},
	date-added = {2019-11-26 10:05:33 -0800},
	date-modified = {2019-11-26 10:05:33 -0800},
	journal = {Journal of machine learning research},
	number = {Dec},
	pages = {1107--1149},
	title = {Least-squares policy iteration},
	volume = {4},
	year = {2003}}

@conference{zanette2020frequentist,
	author = {Zanette, Andrea and Brandfonbrener, David and Pirotta, Matteo and Lazaric, Alessandro},
	booktitle = {AISTATS},
	date-added = {2019-11-24 18:27:58 -0800},
	date-modified = {2020-05-31 15:11:41 -0700},
	journal = {arXiv preprint arXiv:1911.00567},
	title = {Frequentist Regret Bounds for Randomized Least-Squares Value Iteration},
	year = {2020}}

@conference{russo2019worst,
	author = {Russo, Daniel},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2019-11-24 18:27:38 -0800},
	date-modified = {2019-12-13 19:06:56 -0800},
	title = {Worst-Case Regret Bounds for Exploration via Randomized Value Functions},
	year = {2019}}

@article{abeille2017linear,
	author = {Abeille, Marc and Lazaric, Alessandro and others},
	date-added = {2019-11-24 18:26:01 -0800},
	date-modified = {2019-11-24 18:26:01 -0800},
	journal = {Electronic Journal of Statistics},
	number = {2},
	pages = {5165--5197},
	publisher = {The Institute of Mathematical Statistics and the Bernoulli Society},
	title = {Linear Thompson sampling revisited},
	volume = {11},
	year = {2017}}

@article{qian2018exploration,
	author = {Qian, Jian and Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro},
	date-added = {2019-11-24 17:51:40 -0800},
	date-modified = {2019-11-24 17:51:40 -0800},
	journal = {arXiv preprint arXiv:1812.04363},
	title = {Exploration Bonus for Regret Minimization in Undiscounted Discrete and Continuous Markov Decision Processes},
	year = {2018}}

@conference{efroni2019tight,
	author = {Efroni, Yonathan and Merlis, Nadav and Ghavamzadeh, Mohammad and Mannor, Shie},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2019-11-24 17:43:44 -0800},
	date-modified = {2019-12-13 19:05:38 -0800},
	title = {Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies},
	year = {2019}}

@inproceedings{jin2018q,
	author = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2019-11-24 17:36:05 -0800},
	date-modified = {2019-11-24 17:36:05 -0800},
	pages = {4863--4873},
	title = {Is q-learning provably efficient?},
	year = {2018}}

@article{du2019good,
	author = {Du, Simon S and Kakade, Sham M and Wang, Ruosong and Yang, Lin F},
	date-added = {2019-11-24 14:25:46 -0800},
	date-modified = {2019-11-24 14:25:46 -0800},
	journal = {arXiv preprint arXiv:1910.03016},
	title = {Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?},
	year = {2019}}

@conference{yang2020reinforcement,
	author = {Yang, Lin F and Wang, Mengdi},
	booktitle = {International Conference on Machine Learning (ICML)},
	date-added = {2019-11-20 10:24:07 -0800},
	date-modified = {2020-06-02 10:37:18 -0700},
	journal = {arXiv preprint arXiv:1905.10389},
	title = {Reinforcement Leaning in Feature Space: Matrix Bandit, Kernels, and Regret Bound},
	year = {2020}}

@conference{jin2020provably,
	author = {Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
	booktitle = {Conference on Learning Theory},
	date-added = {2019-11-20 10:23:44 -0800},
	date-modified = {2020-06-28 16:02:21 -0700},
	journal = {arXiv preprint arXiv:1907.05388},
	title = {Provably efficient reinforcement learning with linear function approximation},
	year = {2020}}

@article{vershynin2010introduction,
	author = {Vershynin, Roman},
	date-added = {2019-10-31 14:17:17 -0700},
	date-modified = {2019-10-31 14:17:17 -0700},
	journal = {arXiv preprint arXiv:1011.3027},
	title = {Introduction to the non-asymptotic analysis of random matrices},
	year = {2010}}

@book{wainwright2019high,
	author = {Wainwright, Martin J},
	date-added = {2019-09-17 14:27:14 -0700},
	date-modified = {2019-09-17 14:27:14 -0700},
	publisher = {Cambridge University Press},
	title = {High-dimensional statistics: A non-asymptotic viewpoint},
	volume = {48},
	year = {2019}}

@conference{zanette19limiting,
	author = {Andrea Zanette and Alessandro Lazaric and Mykel {J. Kochenderfer} and Emma Brunskill},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2019-07-19 19:32:28 +0200},
	date-modified = {2019-12-13 19:08:58 -0800},
	journal = {under Review},
	title = {Limiting Extrapolation in Linear Approximate Value Iteration},
	year = {2019}}

@inproceedings{zanette2019tighter,
	abstract = {Strong worst-case performance bounds for episodic reinforcement learning exist but fortunately in practice RL algorithms perform much better than such bounds would predict. Algorithms and theory that provide strong problem-dependent bounds could help illuminate the key features of what makes a RL problem hard and reduce the barrier to using RL algorithms in practice. As a step towards this we derive an algorithm and analysis for finite horizon discrete MDPs with state-of-the-art worst-case regret bounds and substantially tighter bounds if the RL environment has special features but without apriori knowledge of the environment from the algorithm. As a result of our analysis, we also help address an open learning theory question \cite{jiang2018open} about episodic MDPs with a constant upper-bound on the sum of rewards, providing a regret bound function of the number of episodes with no dependence on the horizon.},
	author = {Zanette, Andrea and Brunskill, Emma},
	booktitle = {International Conference on Machine Learning (ICML)},
	date-added = {2019-07-19 19:13:37 +0200},
	date-modified = {2019-07-19 19:15:28 +0200},
	pdf = {http://proceedings.mlr.press/v97/zanette19a/zanette19a.pdf},
	title = {Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds},
	url = {http://proceedings.mlr.press/v97/zanette19a.html},
	year = {2019},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v97/zanette19a.html}}

@article{zanette2017enriching,
	author = {Zanette, A and Ferronato, M and Janna, C},
	date-added = {2019-07-19 19:10:04 +0200},
	date-modified = {2019-07-19 19:10:04 +0200},
	journal = {International Journal for Numerical Methods in Engineering},
	number = {7},
	pages = {675--700},
	publisher = {Wiley Online Library},
	title = {Enriching the finite element method with meshfree particles in structural mechanics},
	volume = {110},
	year = {2017}}

@article{zanette2015enriching,
	author = {Zanette, Andrea and Ferronato, Massimiliano and Janna, Carlo},
	date-added = {2019-07-19 19:09:51 +0200},
	date-modified = {2019-07-19 19:09:51 +0200},
	journal = {PAMM},
	number = {1},
	pages = {691--692},
	publisher = {Wiley Online Library},
	title = {Enriching the Finite Element Method with meshfree techniques in structural mechanics},
	volume = {15},
	year = {2015}}

@conference{zanette2019b,
	author = {Andrea Zanette and Emma Brunskill and Mykel {J. Kochenderfer}},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2019-07-18 11:33:54 +0200},
	date-modified = {2019-12-13 19:08:41 -0800},
	journal = {under Review},
	title = {Almost Horizon-Free Structure-Aware Best Policy Identification with a Generative Model},
	year = {2019}}

@article{simchowitz2019non,
	author = {Simchowitz, Max and Jamieson, Kevin},
	date-added = {2019-05-25 11:47:16 -0700},
	date-modified = {2019-05-25 11:47:16 -0700},
	journal = {arXiv preprint arXiv:1905.03814},
	title = {Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs},
	year = {2019}}

@phdthesis{kakade2003sample,
	author = {Kakade, Sham Machandranath and others},
	date-added = {2019-05-25 11:19:23 -0700},
	date-modified = {2019-05-25 11:19:23 -0700},
	school = {University of London London, England},
	title = {On the sample complexity of reinforcement learning},
	year = {2003}}

@incollection{NIPS2011_4485,
	author = {Farahmand, Amir-massoud},
	booktitle = {Advances in Neural Information Processing Systems 24},
	date-added = {2019-05-24 13:13:14 -0700},
	date-modified = {2019-05-24 13:13:14 -0700},
	editor = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
	pages = {172--180},
	publisher = {Curran Associates, Inc.},
	title = {Action-Gap Phenomenon in Reinforcement Learning},
	url = {http://papers.nips.cc/paper/4485-action-gap-phenomenon-in-reinforcement-learning.pdf},
	year = {2011},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/4485-action-gap-phenomenon-in-reinforcement-learning.pdf}}

@article{burnetas1997optimal,
	author = {Burnetas, Apostolos N and Katehakis, Michael N},
	date-added = {2019-05-21 18:49:31 -0700},
	date-modified = {2019-05-21 18:49:31 -0700},
	journal = {Mathematics of Operations Research},
	number = {1},
	pages = {222--255},
	publisher = {INFORMS},
	title = {Optimal adaptive policies for Markov decision processes},
	volume = {22},
	year = {1997}}

@inproceedings{tewari2008optimistic,
	author = {Tewari, Ambuj and Bartlett, Peter L},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2019-05-21 18:49:09 -0700},
	date-modified = {2019-05-21 18:49:09 -0700},
	pages = {1505--1512},
	title = {Optimistic linear programming gives logarithmic regret for irreducible MDPs},
	year = {2008}}

@inproceedings{ok2018exploration,
	author = {Ok, Jungseul and Proutiere, Alexandre and Tranos, Damianos},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2019-05-21 18:48:30 -0700},
	date-modified = {2019-05-21 18:48:30 -0700},
	pages = {8874--8882},
	title = {Exploration in Structured Reinforcement Learning},
	year = {2018}}

@inproceedings{wang2007dual,
	author = {Wang, Tao and Bowling, Michael and Schuurmans, Dale},
	booktitle = {2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning},
	date-added = {2019-05-01 21:07:14 -0700},
	date-modified = {2019-05-01 21:07:14 -0700},
	organization = {IEEE},
	pages = {44--51},
	title = {Dual representations for dynamic programming and reinforcement learning},
	year = {2007}}

@inproceedings{munos1999barycentric,
	author = {Munos, Remi and Moore, Andrew W},
	booktitle = nips,
	title = {Barycentric interpolators for continuous space and time reinforcement learning},
	year = {1999}}

@article{ormoneit2002kernel,
	author = {Ormoneit, Dirk and Sen, {\'S}aunak},
	journal = {Machine Learning},
	number = {2-3},
	pages = {161--178},
	title = {Kernel-based reinforcement learning},
	volume = {49},
	year = {2002}}

@inproceedings{melo2008analysis,
	author = {Melo, Francisco S and Meyn, Sean P and Ribeiro, M Isabel},
	booktitle = icml,
	title = {An analysis of reinforcement learning with function approximation},
	year = {2008}}

@article{baxter_2001,
	author = {Baxter, J. and Bartlett, P. L.},
	doi = {10.1613/jair.806},
	journal = jair,
	pages = {319--350},
	title = {Infinite-Horizon Policy-Gradient Estimation},
	volume = {15},
	year = {2001},
	Bdsk-Url-1 = {http://dx.doi.org/10.1613/jair.806}}

@article{peters2008natural,
	author = {Peters, Jan and Schaal, Stefan},
	date-added = {2019-03-07 10:21:46 -0800},
	date-modified = {2019-03-07 10:21:46 -0800},
	journal = {Neurocomputing},
	number = {7-9},
	pages = {1180--1190},
	title = {Natural actor-critic},
	volume = {71},
	year = {2008}}

@inproceedings{azar2011dynamic,
	author = {Azar, Mohammad Gheshlaghi and Kappen, Bert and others},
	booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
	date-added = {2019-03-06 21:14:19 -0800},
	date-modified = {2019-03-06 21:14:19 -0800},
	title = {Dynamic policy programming with function approximation},
	year = {2011}}

@incollection{bartlett2003introduction,
	author = {Bartlett, Peter L},
	booktitle = {Advanced Lectures on Machine Learning},
	date-added = {2019-03-06 20:06:15 -0800},
	date-modified = {2019-03-06 20:06:15 -0800},
	pages = {184--202},
	publisher = {Springer},
	title = {An introduction to reinforcement learning theory: Value function methods},
	year = {2003}}

@inproceedings{kolter2011fixed,
	author = {Kolter, J Zico},
	booktitle = nips,
	date-added = {2019-03-06 17:47:27 -0800},
	date-modified = {2019-03-06 17:47:27 -0800},
	pages = {2169--2177},
	title = {The fixed points of off-policy TD},
	year = {2011}}

@article{liu2018proximal,
	author = {Liu, Bo and Gemp, Ian and Ghavamzadeh, Mohammad and Liu, Ji and Mahadevan, Sridhar and Petrik, Marek},
	date-added = {2019-03-06 17:45:18 -0800},
	date-modified = {2019-03-06 17:45:18 -0800},
	journal = jair,
	pages = {461--494},
	title = {Proximal gradient temporal difference learning: Stable reinforcement learning with polynomial sample complexity},
	volume = {63},
	year = {2018}}

@inproceedings{agarwal2014taming,
	author = {Agarwal, Alekh and Hsu, Daniel and Kale, Satyen and Langford, John and Li, Lihong and Schapire, Robert},
	booktitle = {International Conference on Machine Learning},
	pages = {1638--1646},
	title = {Taming the monster: A fast and simple algorithm for contextual bandits},
	year = {2014}}

@inproceedings{sutton2009fast,
	author = {Sutton, Richard S and Maei, Hamid Reza and Precup, Doina and Bhatnagar, Shalabh and Silver, David and Szepesv{\'a}ri, Csaba and Wiewiora, Eric},
	booktitle = icml,
	date-added = {2019-03-06 17:44:45 -0800},
	date-modified = {2019-03-06 17:44:45 -0800},
	pages = {993--1000},
	title = {Fast gradient-descent methods for temporal-difference learning with linear function approximation},
	year = {2009}}

@article{Ghiassian2018OnlineOP,
	author = {Sina Ghiassian and Andrew Patterson and Martha White and Richard S. Sutton and Adam White},
	date-added = {2019-03-06 16:48:34 -0800},
	date-modified = {2019-03-06 16:48:34 -0800},
	journal = {CoRR},
	title = {Online Off-policy Prediction},
	volume = {abs/1811.02597},
	year = {2018}}

@incollection{gordon1995stable,
	author = {Gordon, Geoffrey J},
	booktitle = icml,
	date-added = {2019-03-06 12:06:45 -0800},
	date-modified = {2019-03-06 12:06:45 -0800},
	pages = {261--268},
	title = {Stable function approximation in dynamic programming},
	year = {1995}}

@inproceedings{gordon1996stable,
	author = {Gordon, Geoffrey J},
	booktitle = nips,
	date-added = {2019-03-06 12:06:45 -0800},
	date-modified = {2019-03-06 12:06:45 -0800},
	title = {Stable fitted reinforcement learning},
	year = {1996}}

@inproceedings{farahmand2010error,
	author = {Farahmand, Amir-massoud and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
	booktitle = nips,
	date-added = {2019-03-06 11:59:52 -0800},
	date-modified = {2019-03-06 11:59:52 -0800},
	title = {Error propagation for approximate policy and value iteration},
	year = {2010}}

@article{munos2008finite,
	author = {Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
	date-added = {2019-03-06 11:20:54 -0800},
	date-modified = {2019-03-06 11:20:54 -0800},
	journal = jmlr,
	number = {May},
	pages = {815--857},
	title = {Finite-time bounds for fitted value iteration},
	volume = {9},
	year = {2008}}

@inproceedings{munos2005error,
	author = {Munos, R{\'e}mi},
	booktitle = aaai,
	date-added = {2019-03-06 09:47:41 -0800},
	date-modified = {2019-03-06 09:47:41 -0800},
	title = {Error bounds for approximate value iteration},
	year = {2005}}

@book{friedman2001elements,
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	date-added = {2019-03-06 09:44:43 -0800},
	date-modified = {2019-03-06 09:44:43 -0800},
	number = {10},
	publisher = {Springer},
	title = {The Elements of Statistical Learning},
	year = {2001}}

@book{powell2007approximate,
	author = {Powell, Warren B},
	date-added = {2019-03-06 09:28:01 -0800},
	date-modified = {2019-03-06 09:28:01 -0800},
	publisher = {Wiley},
	title = {Approximate Dynamic Programming: Solving the curses of dimensionality},
	year = {2007}}

@book{bertsekas1996neuro,
	author = {Bertsekas, Dimitri P and Tsitsiklis, John N},
	date-added = {2019-03-06 09:25:32 -0800},
	date-modified = {2019-03-06 09:25:32 -0800},
	publisher = {Athena Scientific},
	title = {Neuro-dynamic programming},
	year = {1996}}

@book{trefethen1997numerical,
	author = {Trefethen, Lloyd N and Bau III, David},
	date-added = {2019-03-04 20:35:07 -0800},
	date-modified = {2019-03-04 20:35:07 -0800},
	publisher = {SIAM},
	title = {Numerical Linear Algebra},
	year = {1997}}

@book{golub2012matrix,
	author = {Golub, Gene H and Van Loan, Charles F},
	date-added = {2019-03-04 20:34:36 -0800},
	date-modified = {2019-03-04 20:34:36 -0800},
	publisher = {JHU Press},
	title = {Matrix Computations},
	year = {2012}}

@inproceedings{soare2014best,
	author = {Soare, Marta and Lazaric, Alessandro and Munos, R{\'e}mi},
	booktitle = nips,
	date-added = {2019-03-04 19:55:28 -0800},
	date-modified = {2019-03-04 19:55:28 -0800},
	pages = {828--836},
	title = {Best-arm identification in linear bandits},
	year = {2014}}

@inproceedings{tsitsiklis1997analysis,
	author = {Tsitsiklis, John N and Van Roy, Benjamin},
	booktitle = nips,
	title = {Analysis of temporal-diffference learning with function approximation},
	year = {1997}}

@inproceedings{kumaraswamy2018context,
	author = {Kumaraswamy, Raksha and Schlegel, Matthew and White, Adam and White, Martha},
	booktitle = nips,
	title = {Context-dependent upper-confidence bounds for directed exploration},
	year = {2018}}

@incollection{baird1995residual,
	author = {Baird, Leemon},
	booktitle = icml,
	title = {Residual algorithms: Reinforcement learning with function approximation},
	year = {1995}}

@article{tsitsiklis1996feature,
	author = {Tsitsiklis, John N and Van Roy, Benjamin},
	date-added = {2019-01-14 07:43:45 -0800},
	date-modified = {2019-01-14 07:43:45 -0800},
	journal = {Machine Learning},
	number = {1-3},
	pages = {59--94},
	title = {Feature-based methods for large scale dynamic programming},
	volume = {22},
	year = {1996}}

@inproceedings{boyan1995generalization,
	author = {Boyan, Justin A and Moore, Andrew W},
	booktitle = nips,
	title = {Generalization in reinforcement learning: Safely approximating the value function},
	year = {1995}}

@inproceedings{city5203,
	abstract = {This paper gives specific divergence examples of value-iteration for several major Reinforcement Learning and Adaptive Dynamic Programming algorithms, when using a function approximator for the value function. These divergence examples differ from previous divergence examples in the literature, in that they are applicable for a greedy policy, i.e. in a ?value iteration? scenario. Perhaps surprisingly, with a greedy policy, it is also possible to get divergence for the algorithms TD(1) and Sarsa(1). In addition to these divergences, we also achieve divergence for the Adaptive Dynamic Programming algorithms HDP, DHP and GDHP.},
	author = {M. Fairbank and E. Alonso},
	booktitle = {International Joint Conference on Neural Networks (IJCNN)},
	date-added = {2019-01-14 07:31:12 -0800},
	date-modified = {2019-01-14 07:31:12 -0800},
	doi = {10.1109/IJCNN.2012.6252792},
	keywords = {Adaptive Dynamic Programming, Reinforcement Learning, Greedy Policy, Value Iteration, Divergence},
	note = {{\copyright} 2012 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.},
	publisher = {IEEE Press},
	title = {The divergence of reinforcement learning algorithms with value-iteration and function approximation},
	url = {http://openaccess.city.ac.uk/5203/},
	year = {2012},
	Bdsk-Url-1 = {http://openaccess.city.ac.uk/5203/},
	Bdsk-Url-2 = {https://doi.org/10.1109/IJCNN.2012.6252792}}

@inproceedings{osband2014near,
	author = {Osband, Ian and Van Roy, Benjamin},
	booktitle = nips,
	title = {Near-optimal reinforcement learning in factored MDPs},
	year = {2014}}

@inproceedings{bellemare2016unifying,
	author = {Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
	booktitle = nips,
	title = {Unifying count-based exploration and intrinsic motivation},
	year = {2016}}

@article{silver2017mastering,
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
	journal = {Nature},
	number = {7676},
	pages = {354},
	title = {Mastering the game of {G}o without human knowledge},
	volume = {550},
	year = {2017}}

@article{silver2016mastering,
	author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
	date-added = {2019-01-14 05:31:27 -0800},
	date-modified = {2019-01-14 05:31:27 -0800},
	journal = {Nature},
	number = {7587},
	pages = {484},
	title = {Mastering the game of Go with deep neural networks and tree search},
	volume = {529},
	year = {2016}}

@inproceedings{osband2016deep,
	author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
	booktitle = nips,
	title = {Deep exploration via bootstrapped {DQN}},
	year = {2016}}

@inproceedings{osband2016generalization,
	author = {Osband, Ian and Van Roy, Benjamin and Wen, Zheng},
	booktitle = icml,
	title = {Generalization and Exploration via Randomized Value Functions},
	year = {2016}}

@article{hossain2013constructing,
	author = {Hossain, M Zahid and Amin, M Ashraful},
	date-added = {2019-01-11 14:28:30 +0100},
	date-modified = {2019-01-11 14:28:30 +0100},
	journal = {American Journal of Computational Mathematics},
	number = {1},
	pages = {11},
	title = {On constructing approximate convex hull},
	volume = {3},
	year = {2013}}

@misc{sartipizadeh2016computing,
	archiveprefix = {arXiv},
	author = {Hossein Sartipizadeh and Tyrone L. Vincent},
	date-added = {2019-01-11 14:26:05 +0100},
	date-modified = {2019-01-11 14:26:05 +0100},
	eprint = {1603.04422},
	primaryclass = {cs.CG},
	title = {Computing the Approximate Convex Hull in High Dimensions},
	year = {2016}}

@misc{blum2017approximate,
	archiveprefix = {arXiv},
	author = {Avrim Blum and Vladimir Braverman and Ananya Kumar and Harry Lang and Lin F. Yang},
	date-added = {2019-01-11 14:24:06 +0100},
	date-modified = {2019-01-11 14:24:06 +0100},
	eprint = {1712.04564},
	primaryclass = {cs.CG},
	title = {Approximate Convex Hull of Data Streams},
	year = {2017}}

@misc{graham2017approximate,
	archiveprefix = {arXiv},
	author = {Robert Graham and Adam M. Oberman},
	date-added = {2019-01-11 14:11:53 +0100},
	date-modified = {2019-01-11 14:11:53 +0100},
	eprint = {1703.01350},
	primaryclass = {cs.CG},
	title = {Approximate Convex Hulls: sketching the convex hull using curvature},
	year = {2017}}

@book{boyd2004convex,
	author = {Boyd, Stephen and Vandenberghe, Lieven},
	date-added = {2019-01-11 11:55:54 +0100},
	date-modified = {2019-01-11 11:55:54 +0100},
	publisher = {Cambridge University Press},
	title = {Convex Optimization},
	year = {2004}}

@inproceedings{auer2007logarithmic,
	author = {Auer, Peter and Ortner, Ronald},
	booktitle = nips,
	title = {Logarithmic online regret bounds for undiscounted reinforcement learning},
	year = {2007}}

@inproceedings{krishnamurthy2016pac,
	author = {Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
	booktitle = nips,
	date-added = {2019-01-09 17:51:10 +0100},
	date-modified = {2019-01-09 17:51:10 +0100},
	pages = {1840--1848},
	title = {PAC reinforcement learning with rich observations},
	year = {2016}}

@inproceedings{kocsis2006bandit,
	author = {Kocsis, Levente and Szepesv{\'a}ri, Csaba},
	booktitle = ecml,
	date-added = {2019-01-09 17:49:40 +0100},
	date-modified = {2019-01-09 17:49:40 +0100},
	pages = {282--293},
	title = {Bandit based monte-carlo planning},
	year = {2006}}

@article{kearns2002sparse,
	author = {Kearns, Michael and Mansour, Yishay and Ng, Andrew Y},
	date-added = {2019-01-09 17:44:41 +0100},
	date-modified = {2019-01-09 17:44:41 +0100},
	journal = {Machine Learning},
	number = {2-3},
	pages = {193--208},
	publisher = {Springer},
	title = {A sparse sampling algorithm for near-optimal planning in large Markov decision processes},
	volume = {49},
	year = {2002}}

@article{dimakopoulou2018scalable,
	author = {Dimakopoulou, Maria and Osband, Ian and Van Roy, Benjamin},
	date-added = {2019-01-09 17:36:39 +0100},
	date-modified = {2019-01-09 17:36:39 +0100},
	journal = {arXiv preprint arXiv:1805.08948},
	title = {Scalable Coordinated Exploration in Concurrent Reinforcement Learning},
	year = {2018}}

@inproceedings{pazis2016efficient,
	author = {Pazis, Jason and Parr, Ronald},
	booktitle = aaai,
	date-added = {2019-01-09 17:33:22 +0100},
	date-modified = {2019-01-09 17:33:22 +0100},
	pages = {1977--1985},
	title = {Efficient PAC-Optimal Exploration in Concurrent, Continuous State MDPs with Delayed Updates.},
	year = {2016}}

@inproceedings{jong2007model,
	author = {Jong, Nicholas K and Stone, Peter},
	booktitle = {International Symposium on Abstraction, Reformulation, and Approximation},
	date-added = {2019-01-09 17:31:10 +0100},
	date-modified = {2019-01-09 17:31:10 +0100},
	organization = {Springer},
	pages = {258--272},
	title = {Model-based exploration in continuous state spaces},
	year = {2007}}

@inproceedings{kakade2003exploration,
	author = {Sham M. Kakade and Michael Kearns and John Langford},
	booktitle = icml,
	date-added = {2019-01-09 17:17:33 +0100},
	date-modified = {2019-01-09 17:17:45 +0100},
	title = {Exploration in Metric State Spaces},
	year = {2003}}

@inproceedings{pazis2013pac,
	acmid = {2891568},
	author = {Pazis, Jason and Parr, Ronald},
	booktitle = aaai,
	date-added = {2019-01-09 16:58:37 +0100},
	date-modified = {2019-01-09 16:58:52 +0100},
	location = {Bellevue, Washington},
	numpages = {8},
	pages = {774--781},
	title = {PAC Optimal Exploration in Continuous Space Markov Decision Processes},
	url = {http://dl.acm.org/citation.cfm?id=2891460.2891568},
	year = {2013},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2891460.2891568}}

@inproceedings{chen2018scalable,
	author = {Yichen Chen and Lihong Li and Mengdi Wang},
	booktitle = icml,
	date-added = {2019-01-09 15:58:56 +0100},
	date-modified = {2019-01-09 16:37:28 +0100},
	title = {Scalable Bilinear pi-Learning Using State and Action Features},
	year = {2018}}

@article{sun2018model,
	author = {Sun, Wen and Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
	date-added = {2019-01-09 15:32:53 +0100},
	date-modified = {2019-01-09 15:32:53 +0100},
	journal = {arXiv preprint arXiv:1811.08540},
	title = {Model-Based Reinforcement Learning in Contextual Decision Processes},
	year = {2018}}

@inproceedings{dann2018oracle,
	author = {Dann, Christoph and Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
	booktitle = nips,
	date-added = {2019-01-09 14:19:50 +0100},
	date-modified = {2019-01-09 14:19:50 +0100},
	pages = {1429--1439},
	title = {On Oracle-Efficient PAC RL with Rich Observations},
	year = {2018}}

@inproceedings{jiang17contextual,
	abstract = {This paper studies systematic exploration for reinforcement learning (RL) with rich observations and function approximation. We introduce contextual decision processes (CDPs), that unify most prior RL settings. Our first contribution is a complexity measure, the Bellman rank, that we show enables tractable learning of near-optimal behavior in CDPs and is naturally small for many well-studied RL models. Our second contribution is a new RL algorithm that does systematic exploration to learn near-optimal behavior in CDPs with low Bellman rank. The algorithm requires a number of samples that is polynomial in all relevant parameters but independent of the number of unique contexts. Our approach uses Bellman error minimization with optimistic exploration and provides new insights into efficient exploration for RL with function approximation.},
	address = {International Convention Centre, Sydney, Australia},
	author = {Nan Jiang and Akshay Krishnamurthy and Alekh Agarwal and John Langford and Robert E. Schapire},
	booktitle = icml,
	date-added = {2019-01-09 14:17:31 +0100},
	date-modified = {2019-01-09 14:17:44 +0100},
	editor = {Doina Precup and Yee Whye Teh},
	month = {06--11 Aug},
	pages = {1704--1713},
	pdf = {http://proceedings.mlr.press/v70/jiang17c/jiang17c.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Contextual Decision Processes with low {B}ellman rank are {PAC}-Learnable},
	url = {http://proceedings.mlr.press/v70/jiang17c.html},
	volume = {70},
	year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/jiang17c.html}}

@inproceedings{lattimore2012pac,
	author = {Lattimore, Tor and Hutter, Marcus},
	booktitle = {International Conference on Algorithmic Learning Theory},
	date-added = {2019-01-06 16:54:27 +0100},
	date-modified = {2019-01-06 16:54:27 +0100},
	organization = {Springer},
	pages = {320--334},
	title = {{PAC} bounds for discounted {MDP}s},
	year = {2012}}

@inproceedings{mansour1999complexity,
	author = {Mansour, Yishay and Singh, Satinder},
	booktitle = uai,
	date-added = {2019-01-06 16:53:50 +0100},
	date-modified = {2019-01-06 16:53:50 +0100},
	organization = {Morgan Kaufmann},
	pages = {401--408},
	title = {On the complexity of policy iteration},
	year = {1999}}

@inproceedings{radlinski2008learning,
	author = {Radlinski, Filip and Kleinberg, Robert and Joachims, Thorsten},
	booktitle = icml,
	date-added = {2019-01-05 18:20:33 +0100},
	date-modified = {2019-01-05 18:20:33 +0100},
	organization = {ACM},
	pages = {784--791},
	title = {Learning diverse rankings with multi-armed bandits},
	year = {2008}}

@inproceedings{pandey2007bandits,
	author = {Pandey, Sandeep and Agarwal, Deepak and Chakrabarti, Deepayan and Josifovski, Vanja},
	booktitle = {SIAM International Conference on Data Mining},
	date-added = {2019-01-05 18:20:02 +0100},
	date-modified = {2019-01-05 18:20:02 +0100},
	organization = {SIAM},
	pages = {216--227},
	title = {Bandits for taxonomies: A model-based approach},
	year = {2007}}

@inproceedings{agarwal2009explore,
	author = {Agarwal, Deepak and Chen, Bee-Chung and Elango, Pradheep},
	booktitle = {IEEE International Conference on Data Mining},
	title = {Explore/exploit schemes for web content optimization},
	year = {2009}}

@inproceedings{chakrabarti2009mortal,
	author = {Chakrabarti, Deepayan and Kumar, Ravi and Radlinski, Filip and Upfal, Eli},
	booktitle = nips,
	date-added = {2019-01-05 18:18:53 +0100},
	date-modified = {2019-01-05 18:18:53 +0100},
	pages = {273--280},
	title = {Mortal multi-armed bandits},
	year = {2009}}

@article{kearns2002near,
	author = {Kearns, Michael and Singh, Satinder},
	date-added = {2019-01-05 12:10:56 +0100},
	date-modified = {2019-01-05 12:10:56 +0100},
	journal = {Machine Learning},
	number = {2-3},
	pages = {209--232},
	publisher = {Springer},
	title = {Near-optimal reinforcement learning in polynomial time},
	volume = {49},
	year = {2002}}

@article{mannor2004sample,
	author = {Mannor, Shie and Tsitsiklis, John N},
	date-added = {2019-01-04 18:31:47 +0100},
	date-modified = {2019-01-04 18:31:47 +0100},
	journal = jmlr,
	number = {Jun},
	pages = {623--648},
	title = {The sample complexity of exploration in the multi-armed bandit problem},
	volume = {5},
	year = {2004}}

@book{sutton2018reinforcement,
	author = {Sutton, Richard S and Barto, Andrew G},
	date-added = {2019-01-04 13:31:11 +0100},
	date-modified = {2019-01-04 13:31:11 +0100},
	publisher = {MIT Press},
	title = {Reinforcement learning: An introduction},
	year = {2018}}

@article{liu2018simple,
	author = {Liu, Yao and Brunskill, Emma},
	date-added = {2018-12-28 20:30:04 +0100},
	date-modified = {2018-12-28 20:30:04 +0100},
	journal = {arXiv preprint arXiv:1805.09045},
	title = {When Simple Exploration is Sample Efficient: Identifying Sufficient Conditions for Random Exploration to Yield PAC RL Algorithms},
	year = {2018}}

@article{chen2017nearly,
	author = {Chen, Lijie and Li, Jian and Qiao, Mingda},
	date-added = {2018-12-28 19:51:10 +0100},
	date-modified = {2018-12-28 19:51:10 +0100},
	journal = {arXiv preprint arXiv:1702.03605},
	title = {Nearly instance optimal sample complexity bounds for top-k arm selection},
	year = {2017}}

@inproceedings{karnin2013almost,
	author = {Karnin, Zohar and Koren, Tomer and Somekh, Oren},
	booktitle = icml,
	date-added = {2018-12-28 16:15:07 +0100},
	date-modified = {2018-12-28 16:15:07 +0100},
	title = {Almost optimal exploration in multi-armed bandits},
	year = {2013}}

@article{kaufmann2016complexity,
	author = {Kaufmann, Emilie and Capp{\'e}, Olivier and Garivier, Aur{\'e}lien},
	date-added = {2018-12-28 16:07:58 +0100},
	date-modified = {2018-12-28 16:07:58 +0100},
	journal = jmlr,
	number = {1},
	pages = {1--42},
	publisher = {JMLR. org},
	title = {On the complexity of best-arm identification in multi-armed bandit models},
	volume = {17},
	year = {2016}}

@inproceedings{jamieson2014lil,
	author = {Jamieson, Kevin and Malloy, Matthew and Nowak, Robert and Bubeck, S{\'e}bastien},
	booktitle = colt,
	date-added = {2018-12-28 15:58:12 +0100},
	date-modified = {2018-12-28 15:58:12 +0100},
	pages = {423--439},
	title = {lil'ucb: An optimal exploration algorithm for multi-armed bandits},
	year = {2014}}

@inproceedings{jiang2016structural,
	author = {Jiang, Nan and Singh, Satinder P and Tewari, Ambuj},
	booktitle = ijcai,
	date-added = {2018-12-24 17:30:55 +0100},
	date-modified = {2018-12-24 17:30:55 +0100},
	pages = {1640--1647},
	title = {On Structural Properties of MDPs that Bound Loss Due to Shallow Planning.},
	year = {2016}}

@article{ortner2018regret,
	author = {Ortner, Ronald},
	date-added = {2018-12-24 11:36:33 +0100},
	date-modified = {2018-12-24 11:36:33 +0100},
	journal = {arXiv preprint arXiv:1808.01813},
	title = {Regret Bounds for Reinforcement Learning via Markov Chain Concentration},
	year = {2018}}

@article{strehl2009reinforcement,
	author = {Strehl, Alexander L and Li, Lihong and Littman, Michael L},
	date-added = {2018-12-24 11:22:17 +0100},
	date-modified = {2018-12-24 11:22:17 +0100},
	journal = jmlr,
	number = {Nov},
	pages = {2413--2444},
	title = {Reinforcement learning in finite MDPs: PAC analysis},
	volume = {10},
	year = {2009}}

@article{lattimore2014near,
	author = {Lattimore, Tor and Hutter, Marcus},
	date-added = {2018-12-24 11:21:07 +0100},
	date-modified = {2018-12-24 11:21:07 +0100},
	journal = {Theoretical Computer Science},
	pages = {125--143},
	publisher = {Elsevier},
	title = {Near-optimal {PAC} bounds for discounted {MDP}s},
	volume = {558},
	year = {2014}}

@inproceedings{carpentier2016tight,
	author = {Carpentier, Alexandra and Locatelli, Andrea},
	booktitle = colt,
	date-added = {2018-12-24 11:06:45 +0100},
	date-modified = {2018-12-24 11:06:45 +0100},
	pages = {590--604},
	title = {Tight (lower) bounds for the fixed budget best arm identification bandit problem},
	year = {2016}}

@inproceedings{bubeck2009pure,
	author = {Bubeck, S{\'e}bastien and Munos, R{\'e}mi and Stoltz, Gilles},
	booktitle = {International Conference on Algorithmic Learning Theory},
	date-added = {2018-12-24 10:43:25 +0100},
	date-modified = {2018-12-24 10:43:25 +0100},
	title = {Pure exploration in multi-armed bandits problems},
	year = {2009}}

@inproceedings{mnih2008empirical,
	author = {Mnih, Volodymyr and Szepesv{\'a}ri, Csaba and Audibert, Jean-Yves},
	booktitle = icml,
	date-added = {2018-12-24 10:36:50 +0100},
	date-modified = {2018-12-24 10:36:50 +0100},
	organization = {ACM},
	pages = {672--679},
	title = {Empirical bernstein stopping},
	year = {2008}}

@inproceedings{maron1994hoeffding,
	author = {Maron, Oded and Moore, Andrew W},
	booktitle = nips,
	date-added = {2018-12-24 10:33:57 +0100},
	date-modified = {2018-12-24 10:33:57 +0100},
	pages = {59--66},
	title = {Hoeffding races: Accelerating model selection search for classification and function approximation},
	year = {1994}}

@inproceedings{gabillon2012best,
	author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro},
	booktitle = nips,
	date-added = {2018-12-24 10:24:58 +0100},
	date-modified = {2018-12-24 10:24:58 +0100},
	pages = {3212--3220},
	title = {Best arm identification: A unified approach to fixed budget and fixed confidence},
	year = {2012}}

@book{nocedal2006numerical,
	author = {Nocedal, Jorge and Wright, Stephen},
	publisher = {Springer},
	title = {Numerical Optimization},
	year = {2006}}

@inproceedings{gabillon2011multi,
	author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro and Bubeck, S{\'e}bastien},
	booktitle = nips,
	date-added = {2018-12-24 10:18:40 +0100},
	date-modified = {2018-12-24 10:18:40 +0100},
	pages = {2222--2230},
	title = {Multi-bandit best arm identification},
	year = {2011}}

@inproceedings{brunskill2010policies,
	author = {Brunskill, Emma},
	booktitle = icaps,
	date-added = {2018-12-14 05:52:28 +0100},
	date-modified = {2018-12-14 05:52:28 +0100},
	pages = {218--221},
	title = {When Policies Can Be Trusted: Analyzing a Criteria to Identify Optimal Policies in MDPs with Unknown Model Parameters.},
	year = {2010}}

@conference{yang2019sample,
	author = {Yang, Lin F and Wang, Mengdi},
	booktitle = {International Conference on Machine Learning (ICML)},
	date-modified = {2019-12-13 19:07:53 -0800},
	journal = {arXiv preprint arXiv:1902.04779},
	title = {Sample-Optimal Parametric Q-Learning with Linear Transition Models},
	year = {2019}}

@inproceedings{jiang2018open,
	author = {Jiang, Nan and Agarwal, Alekh},
	booktitle = colt,
	date-added = {2018-12-10 13:48:34 -0800},
	date-modified = {2018-12-10 13:48:34 -0800},
	pages = {3395--3398},
	title = {Open Problem: The Dependence of Sample Complexity Lower Bounds on Planning Horizon},
	year = {2018}}

@article{white1993,
	author = {White, Douglas J},
	date-added = {2018-07-27 19:25:17 +0200},
	date-modified = {2018-07-27 19:25:24 +0200},
	journal = {Journal of the Operational Research Society},
	number = {11},
	pages = {1073--1096},
	publisher = {Taylor \& Francis},
	title = {A survey of applications of Markov decision processes},
	volume = {44},
	year = {1993}}

@inproceedings{petrusel2013,
	author = {Petrusel, Razvan},
	booktitle = {International Conference on Business Information Systems},
	date-added = {2018-07-27 19:20:02 +0200},
	date-modified = {2018-07-27 19:20:08 +0200},
	organization = {Springer},
	pages = {125--137},
	title = {Using {M}arkov decision process for recommendations based on aggregated decision data models},
	year = {2013}}

@article{espinosa2010,
	author = {Espinosa, Enrique D and Frausto, Juan and Rivera, Ernesto J},
	date-added = {2018-07-27 19:17:58 +0200},
	date-modified = {2018-07-27 19:18:05 +0200},
	journal = {Service Science},
	number = {4},
	pages = {245--269},
	title = {Markov decision processes for optimizing human workflows},
	volume = {2},
	year = {2010}}

@inproceedings{codemo2013,
	author = {Codemo, Claudio G and Erseghe, Tomaso and Zanella, Andrea},
	booktitle = {IEEE International Conference on Communications (ICC)},
	date-added = {2018-07-27 17:11:25 +0000},
	date-modified = {2018-07-27 17:11:34 +0000},
	title = {Energy storage optimization strategies for smart grids},
	year = {2013}}

@inproceedings{li2008,
	author = {Li, Yingzi and Niu, Jincang and Luan, Ru and Yue, Yuntao},
	booktitle = {IEEE International Conference on Electrical Machines and Systems},
	date-added = {2018-07-27 17:01:37 +0000},
	date-modified = {2018-07-27 17:01:45 +0000},
	title = {Research of multi-power structure optimization for grid-connected photovoltaic system based on Markov decision-making model},
	year = {2008}}

@inproceedings{kim2014,
	author = {Kim, Junhyuk and Kong, Peng-Yong and Song, Nah-Oak and Rhee, June-Koo Kevin and Al-Araji, Saleh},
	booktitle = {IEEE Wireless Communications and Networking Conference (WCNC)},
	date-added = {2018-07-27 16:20:44 +0000},
	date-modified = {2018-07-27 16:20:49 +0000},
	title = {{MDP} based dynamic base station management for power conservation in self-organizing networks},
	year = {2014}}

@inproceedings{Zanette18b,
	author = {Andrea Zanette and Junzi Zhang and Mykel {J. Kochenderfer}},
	booktitle = ecml,
	date-added = {2018-07-26 14:31:43 +0000},
	date-modified = {2018-10-03 09:56:27 -0700},
	title = {Robust Super-Level Set Estimation using Gaussian Processes},
	year = {2018}}

@inproceedings{Sidford18,
	author = {Aaron Sidford and Mengdi Wang and Xian Wu and Lin F. Yang and Yinyu Ye},
	booktitle = nips,
	title = {Near-Optimal Time and Sample Complexities for for Solving Discounted Markov Decision Process with a Generative Model},
	year = {2018}}

@inproceedings{Abbasi11,
	author = {Yasin Abbasi-Yadkori and David Pal and Csaba Szepesvari},
	booktitle = nips,
	date-added = {2018-06-07 20:43:50 +0000},
	date-modified = {2018-06-07 20:44:28 +0000},
	title = {Improved Algorithms for Linear Stochastic Bandits},
	year = {2011}}

@inproceedings{Bubeck2012,
	abstract = {We present a new bandit algorithm, SAO (Stochastic and Adversarial Optimal) whose regret is (essentially) optimal both for adversarial rewards and for stochastic rewards. Specifically, SAO combines the \emphO(√\emphn) worst-case regret of Exp3 (Auer et al., 2002b) and the (poly)logarithmic regret of UCB1 (Auer et al., 2002a) for stochastic rewards. Adversarial rewards and stochastic rewards are the two main settings in the literature on multi-armed bandits (MAB). Prior work on MAB treats them separately, and does not attempt to jointly optimize for both. This result falls into the general agenda to design algorithms that combine the optimal worst-case performance with improved guarantees for ``nice'' problem instances.},
	author = {S{\'e}bastien Bubeck and Aleksandrs Slivkins},
	booktitle = colt,
	date-added = {2018-06-06 21:48:05 +0000},
	date-modified = {2018-06-06 22:01:52 +0000},
	title = {The Best of Both Worlds: Stochastic and Adversarial Bandits},
	year = {2012},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v23/bubeck12b.html}}

@inproceedings{Dann17,
	author = {Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
	booktitle = nips,
	date-added = {2018-06-04 20:32:01 +0000},
	date-modified = {2018-06-04 21:23:52 +0000},
	title = {Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning},
	year = {2017},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/7154-unifying-pac-and-regret-uniform-pac-bounds-for-episodic-reinforcement-learning.pdf}}

@article{antos2008learning,
	author = {Antos, Andr{\'a}s and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
	date-added = {2018-05-27 06:02:22 +0000},
	date-modified = {2018-05-27 06:02:22 +0000},
	journal = {Machine Learning},
	number = {1},
	pages = {89--129},
	publisher = {Springer},
	title = {Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path},
	volume = {71},
	year = {2008}}

@inproceedings{maei2010toward,
	author = {Maei, Hamid Reza and Szepesv{\'a}ri, Csaba and Bhatnagar, Shalabh and Sutton, Richard S},
	booktitle = icml,
	date-added = {2018-05-27 05:51:46 +0000},
	date-modified = {2018-05-27 05:51:46 +0000},
	pages = {719--726},
	title = {Toward off-policy learning control with function approximation.},
	year = {2010}}

@article{burnetas1997,
	author = {Burnetas, Apostolos N and Katehakis, Michael N},
	date-added = {2018-05-27 05:36:33 +0000},
	date-modified = {2018-05-27 05:36:40 +0000},
	journal = {Mathematics of Operations Research},
	number = {1},
	pages = {222--255},
	publisher = {INFORMS},
	title = {Optimal adaptive policies for Markov decision processes},
	volume = {22},
	year = {1997}}

@inproceedings{Agrawal2017,
	author = {Agrawal, Shipra and Jia, Randy},
	booktitle = nips,
	date-added = {2018-05-27 05:31:36 +0000},
	date-modified = {2018-05-27 05:32:17 +0000},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {1184--1194},
	publisher = {Curran Associates, Inc.},
	title = {Optimistic posterior sampling for reinforcement learning: worst-case regret bounds},
	url = {http://papers.nips.cc/paper/6718-optimistic-posterior-sampling-for-reinforcement-learning-worst-case-regret-bounds.pdf},
	year = {2017},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/6718-optimistic-posterior-sampling-for-reinforcement-learning-worst-case-regret-bounds.pdf}}

@inproceedings{WR13,
	author = {Zheng Wen and Benjamin {Van Roy}},
	booktitle = nips,
	date-added = {2018-05-17 18:04:11 +0000},
	date-modified = {2018-05-17 18:07:05 +0000},
	title = {Efficient Exploration and Value Function Generalization in Deterministic Systems},
	year = {2013}}

@article{BLLLN09,
	author = {Emma Brunskill and {Bethany R.} Leffler and Lihong Li and {Michael L.} Littman and Nicholas Roy},
	date-added = {2018-05-17 17:58:35 +0000},
	date-modified = {2018-05-17 18:00:55 +0000},
	journal = jmlr,
	title = {Provably Efficient Learning with Typed Parametric Models},
	year = {2009}}

@inproceedings{BLLLR08,
	author = {Emma Brunskill and Bethany Leffler and Lihong Li and {Michael L.} Littman and Nicholas Roy},
	booktitle = uai,
	date-added = {2018-05-17 17:55:34 +0000},
	date-modified = {2018-05-17 17:57:40 +0000},
	title = {CORL: A Continuous-state Offset-dynamics Reinforcement Learner},
	year = {2008}}

@inproceedings{OVR13,
	author = {Ian Osband and Benjamin {Van Roy} and Daniel Russo},
	booktitle = nips,
	date-added = {2018-05-17 05:08:17 +0000},
	date-modified = {2018-05-17 05:09:10 +0000},
	title = {(More) Efficient Reinforcement Learning via Posterior Sampling},
	year = {2013}}

@inproceedings{ABM10,
	author = {{Jean-Yves} Audibert and Sebastien Bubeck and Remi Munos},
	booktitle = colt,
	date-added = {2018-05-16 22:14:50 +0000},
	date-modified = {2018-05-17 05:35:38 +0000},
	title = {Best Arm Identification in Multi-Armed Bandits},
	year = {2010}}

@inproceedings{Zanette18a,
	author = {Andrea Zanette and Emma Brunskill},
	booktitle = icml,
	date-added = {2018-05-16 19:10:09 +0000},
	date-modified = {2018-10-03 09:56:22 -0700},
	title = {Problem Dependent Reinforcement Learning Bounds Which Can Identify Bandit Structure in MDPs},
	year = {2018}}

@inproceedings{Azar12,
	author = {{Mohammad Gheshlaghi} Azar and Remi Munos and Hilbert J. Kappen},
	booktitle = icml,
	date-added = {2018-05-16 19:06:13 +0000},
	date-modified = {2018-06-14 04:05:16 +0000},
	title = {On the Sample Complexity of Reinforcement Learning with a Generative Model},
	year = {2012}}

@article{Azar2013_journal,
	abstract = {We consider the problems of learning the optimal action-value function and the optimal policy in discounted-reward Markov decision processes (MDPs). We prove new PAC bounds on the sample-complexity of two well-known model-based reinforcement learning (RL) algorithms in the presence of a generative model of the MDP: value iteration and policy iteration. The first result indicates that for an MDP with N state-action pairs and the discount factor $\gamma$∈[0,1) only O(Nlog(N/$\delta$)/((1−$\gamma$)3$\epsilon$2)) state-transition samples are required to find an $\epsilon$-optimal estimation of the action-value function with the probability (w.p.) 1−$\delta$. Further, we prove that, for small values of $\epsilon$, an order of O(Nlog(N/$\delta$)/((1−$\gamma$)3$\epsilon$2)) samples is required to find an $\epsilon$-optimal policy w.p. 1−$\delta$. We also prove a matching lower bound of $\Theta$(Nlog(N/$\delta$)/((1−$\gamma$)3$\epsilon$2)) on the sample complexity of estimating the optimal action-value function with $\epsilon$ accuracy. To the best of our knowledge, this is the first minimax result on the sample complexity of RL: the upper bounds match the lower bound in terms of N, $\epsilon$, $\delta$ and 1/(1−$\gamma$) up to a constant factor. Also, both our lower bound and upper bound improve on the state-of-the-art in terms of their dependence on 1/(1−$\gamma$).},
	author = {Gheshlaghi Azar, Mohammad and Munos, R{\'e}mi and Kappen, Hilbert J.},
	day = {01},
	doi = {10.1007/s10994-013-5368-1},
	journal = {Machine Learning},
	month = {06},
	number = {3},
	pages = {325--349},
	title = {Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model},
	url = {https://doi.org/10.1007/s10994-013-5368-1},
	volume = {91},
	year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10994-013-5368-1}}

@inproceedings{OV17,
	author = {Ian Osband and Benjamin Van Roy},
	booktitle = icml,
	date-added = {2018-05-13 04:00:45 +0000},
	date-modified = {2018-05-13 04:02:20 +0000},
	title = {Why is Posterior Sampling Better than Optimism for Reinforcement Learning?},
	year = {2017}}

@inproceedings{OV14,
	author = {Ian Osband and Benjamin Van Roy},
	booktitle = nips,
	date-added = {2018-05-11 01:09:01 +0000},
	date-modified = {2018-05-11 01:09:37 +0000},
	title = {Model-based Reinforcement Learning and the Eluder Dimension},
	year = {2014}}

@article{Audibert09,
	author = {Jean Yves Audibert and Remi Munos and Csaba Szepesvari},
	date-added = {2018-05-10 17:24:04 +0000},
	date-modified = {2018-10-02 12:13:21 -0700},
	journal = {Theoretical Computer Science},
	title = {Exploration-exploitation trade-off using variance estimates in multi-armed bandits},
	year = {2009}}

@article{KWY18,
	author = {Sham Kakade and Mengdi Wang and Lin F. Yang},
	date-added = {2018-04-24 22:55:55 +0000},
	date-modified = {2018-05-17 06:49:41 +0000},
	journal = {Arxiv},
	title = {Variance Reduction Methods for Sublinear Reinforcement Learning},
	year = {2018}}

@inproceedings{TM18,
	author = {{Mohammad Sadegh} Talebi and {Odalric-Ambrym} Maillard},
	booktitle = {ALT},
	date-added = {2018-04-24 22:48:09 +0000},
	date-modified = {2018-04-24 22:59:06 +0000},
	title = {Variance-Aware Regret Bounds for Undiscounted Reinforcement Learning in MDPs},
	year = {2018}}

@unpublished{Fruit18,
	author = {Ronan Fruit and Matteo Pirotta and Alessandro Lazaric and Ronald Ortner},
	date-added = {2018-04-24 22:28:31 +0000},
	date-modified = {2018-10-02 16:50:25 -0700},
	note = {https://arxiv.org/abs/1802.04020},
	title = {Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning},
	year = {2018}}

@book{bernstein,
	author = {S. N. Bernstein},
	date-added = {2018-04-03 20:08:22 +0000},
	date-modified = {2018-04-03 20:10:42 +0000},
	title = {Theory of Probability},
	year = {1927}}

@inproceedings{MP09,
	author = {Andreas Maurer and Massimiliano Pontil},
	booktitle = colt,
	date-added = {2018-02-21 23:13:25 +0000},
	date-modified = {2018-05-06 20:29:07 +0000},
	title = {Empirical Bernstein Bounds and Sample Variance Penalization},
	year = {2009}}

@techreport{Weissman03,
	author = {Tsachy Weissman and Erik Ordentlich and Gadiel Seroussi and Sergio Verdu and Marcelo J. Weinberger},
	date-added = {2018-02-21 23:05:29 +0000},
	date-modified = {2018-02-21 23:09:17 +0000},
	institution = {Hewlett-Packard Labs},
	title = {Inequalities for the l1 deviation of the empirical distribution},
	year = {2003}}

@article{EMM06,
	author = {Eyal {Even-Dar} and Shie Mannor and Yishay Mansour},
	date-modified = {2018-05-19 22:53:44 +0000},
	journal = jmlr,
	title = {Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems},
	year = {2006}}

@inproceedings{OV16,
	author = {Ian Osband and Benjamin {Van Roy}},
	booktitle = {Arxiv},
	date-added = {2018-01-28 23:13:58 +0000},
	date-modified = {2018-05-17 06:49:57 +0000},
	note = {https://arxiv.org/pdf/1608.02732.pdf},
	title = {On Lower Bounds for Regret in Reinforcement Learning},
	url = {https://arxiv.org/pdf/1608.02732.pdf},
	year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1608.02732.pdf}}

@inproceedings{modelselection,
	author = {Odalric-Ambrym Maillard and Ryabko, Daniil and R\'{e}mi Munos},
	booktitle = nips,
	date-added = {2017-12-07 00:50:38 +0000},
	date-modified = {2017-12-07 00:54:04 +0000},
	title = {Selecting the State-Representation in Reinforcement Learning},
	year = {2011}}

@inproceedings{BayesianClustering,
	author = {Travis Mandel and Yun-En Liu and Emma Brunskill and and Zoran Popovic},
	booktitle = ijcai,
	title = {Efficient Bayesian clustering for reinforcement learning},
	year = {2016}}

@inproceedings{InfinitePOMDPs,
	author = {Finale Doshi-Velez},
	booktitle = nips,
	title = {The Infinite Partially Observable Markov Decision Process},
	year = {2009}}

@inproceedings{Apprenticeship,
	author = {Pieter Abbeel and Andrew Y. Ng},
	booktitle = icml,
	title = {Apprenticeship learning via inverse reinforcement learning},
	year = {2004}}

@article{Hoeffding,
	author = {Wassily Hoeffding},
	date-added = {2017-11-30 00:02:47 +0000},
	date-modified = {2017-11-30 00:04:38 +0000},
	journal = {Journal of the American Statistical Association},
	title = {Probability inequalities for sums of bounded random variables},
	year = {1963}}

@book{stochasticorders,
	author = {Moshe Shaked and J. George Shanthikumar},
	date-added = {2017-11-21 04:40:22 +0000},
	date-modified = {2017-11-21 04:41:48 +0000},
	publisher = {Springer},
	title = {Stochastic Orders},
	year = {2007}}

@book{CL06,
	author = {Nicol\`{o} Cesa-Bianchi and G\'{a}bor Lugosi},
	date-added = {2017-11-18 21:50:17 +0000},
	date-modified = {2018-05-09 16:46:36 +0000},
	publisher = {Cambridge University Press},
	title = {Prediction, Learning, and Games},
	year = {2006}}

@article{POMDPsComplexity,
	author = {Christopher Lusena and Judy Goldsmith and Martin Mundhenk},
	journal = jair,
	title = {Nonapproximability results for partially observable Markov decision processes},
	year = {2001}}

@inproceedings{AG12,
	author = {Shipra Agrawal and Navin Goyal},
	booktitle = colt,
	date-modified = {2018-05-19 22:54:22 +0000},
	title = {Analysis of Thompson Sampling for the Multi-armed Bandit Problem},
	year = {2012}}

@article{Auer02,
	author = {Peter Auer and Nicolo Cesa Bianchi and Paul Fischer},
	date-added = {2017-11-12 01:59:40 +0000},
	date-modified = {2018-10-02 12:39:45 -0700},
	journal = {Machine Learning},
	title = {Finite-time Analysis of the Multiarmed Bandit Problem},
	year = {2002}}

@inproceedings{Maillard14,
	author = {Odalric-Ambrym Maillard and Timothy A. Mann and Shie Mannor},
	booktitle = nips,
	date-added = {2017-11-12 01:05:17 +0000},
	date-modified = {2018-10-02 11:55:56 -0700},
	title = {``How hard is my {MDP}?'' The distribution-norm to the rescue},
	year = {2014}}

@article{BC12,
	author = {S{\'e}bastien Bubeck and Nicol{\`o} Cesa-Bianchi},
	date-added = {2017-11-04 17:34:17 +0000},
	date-modified = {2018-05-10 17:19:45 +0000},
	journal = {Foundations and Trends in Machine Learning},
	title = {Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems},
	year = {2012}}

@inproceedings{Bartlett09,
	author = {Peter L. Bartlett and Ambuj Tewari},
	booktitle = uai,
	date-added = {2017-11-04 17:00:20 +0000},
	date-modified = {2018-10-02 16:46:07 -0700},
	title = {REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs},
	year = {2009}}

@inproceedings{Dann15,
	author = {Christoph Dann and Emma Brunskill},
	booktitle = nips,
	date-added = {2017-11-04 16:58:01 +0000},
	date-modified = {2018-06-06 21:35:15 +0000},
	title = {Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning},
	year = {2015}}

@inproceedings{Azar17,
	author = {Mohammad Gheshlaghi Azar and Ian Osband and Remi Munos},
	booktitle = icml,
	date-added = {2017-11-04 16:53:32 +0000},
	date-modified = {2018-06-06 21:35:49 +0000},
	title = {Minimax Regret Bounds for Reinforcement Learning},
	year = {2017}}

@inproceedings{Mnih2013,
	author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
	booktitle = nips,
	date-added = {2017-10-26 05:00:22 +0000},
	date-modified = {2018-07-27 07:55:08 +0000},
	title = {Playing Atari with Deep Reinforcement Learning},
	year = {2013}}

@article{Jaksch10,
	author = {Thomas Jaksch and Ronald Ortner and Peter Auer},
	date-modified = {2018-10-02 16:23:32 -0700},
	journal = jmlr,
	title = {Near-optimal Regret Bounds for Reinforcement Learning},
	year = {2010}}
