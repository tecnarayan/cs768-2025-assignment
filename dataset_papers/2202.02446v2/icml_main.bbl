\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2021)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2021theory}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G.
\newblock On the theory of policy gradient methods: Optimality, approximation,
  and distribution shift.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (98):\penalty0 1--76, 2021.

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'a}ri, and Munos]{antos2008learning}
Antos, A., Szepesv{\'a}ri, C., and Munos, R.
\newblock Learning near-optimal policies with bellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock \emph{Machine Learning}, 71\penalty0 (1):\penalty0 89--129, 2008.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{arjovsky2017wasserstein}
Arjovsky, M., Chintala, S., and Bottou, L.
\newblock Wasserstein generative adversarial networks.
\newblock In \emph{International conference on machine learning}, pp.\
  214--223. PMLR, 2017.

\bibitem[Baird(1995)]{baird1995residual}
Baird, L.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \emph{Machine Learning Proceedings 1995}, pp.\  30--37. Elsevier,
  1995.

\bibitem[Bertsekas \& Tsitsiklis(1995)Bertsekas and
  Tsitsiklis]{bertsekas1995neuro}
Bertsekas, D.~P. and Tsitsiklis, J.~N.
\newblock Neuro-dynamic programming: an overview.
\newblock In \emph{Proceedings of 1995 34th IEEE conference on decision and
  control}, volume~1, pp.\  560--564. IEEE, 1995.

\bibitem[Borkar(1997)]{borkar1997stochastic}
Borkar, V.~S.
\newblock Stochastic approximation with two time scales.
\newblock \emph{Systems \& Control Letters}, 29\penalty0 (5):\penalty0
  291--294, 1997.

\bibitem[Cesa-Bianchi \& Lugosi(2006)Cesa-Bianchi and
  Lugosi]{cesa2006prediction}
Cesa-Bianchi, N. and Lugosi, G.
\newblock \emph{Prediction, learning, and games}.
\newblock Cambridge university press, 2006.

\bibitem[Chen \& Jiang(2019)Chen and Jiang]{chen2019information}
Chen, J. and Jiang, N.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1042--1051, 2019.

\bibitem[Cheng et~al.(2019)Cheng, Yan, Ratliff, and Boots]{cheng2019predictor}
Cheng, C.-A., Yan, X., Ratliff, N., and Boots, B.
\newblock Predictor-corrector policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1151--1161. PMLR, 2019.

\bibitem[Cheng et~al.(2020)Cheng, Kolobov, and Agarwal]{cheng2020policy}
Cheng, C.-A., Kolobov, A., and Agarwal, A.
\newblock Policy improvement via imitation of multiple oracles.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Cheng et~al.(2021)Cheng, Kolobov, and Swaminathan]{cheng2021heuristic}
Cheng, C.-A., Kolobov, A., and Swaminathan, A.
\newblock Heuristic-guided reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 13550--13563, 2021.

\bibitem[Even-Dar et~al.(2009)Even-Dar, Kakade, and Mansour]{even2009online}
Even-Dar, E., Kakade, S.~M., and Mansour, Y.
\newblock Online markov decision processes.
\newblock \emph{Mathematics of Operations Research}, 34\penalty0 (3):\penalty0
  726--736, 2009.

\bibitem[Farahmand et~al.(2010)Farahmand, Munos, and
  Szepesv{\'a}ri]{farahmand2010error}
Farahmand, A.~M., Munos, R., and Szepesv{\'a}ri, C.
\newblock Error propagation for approximate policy and value iteration.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2010.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Fujimoto \& Gu(2021)Fujimoto and Gu]{fujimoto2021minimalist}
Fujimoto, S. and Gu, S.~S.
\newblock A minimalist approach to offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 20132--20145, 2021.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and
  Meger]{fujimoto2018addressing}
Fujimoto, S., Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1587--1596. PMLR, 2018.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Fujimoto, S., Meger, D., and Precup, D.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2052--2062, 2019.

\bibitem[Geist et~al.(2019)Geist, Scherrer, and Pietquin]{geist2019theory}
Geist, M., Scherrer, B., and Pietquin, O.
\newblock A theory of regularized markov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2160--2169. PMLR, 2019.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pp.\
  1861--1870. PMLR, 2018.

\bibitem[Jin et~al.(2021)Jin, Yang, and Wang]{jin2020pessimism}
Jin, Y., Yang, Z., and Wang, Z.
\newblock Is pessimism provably efficient for offline rl?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5084--5096. PMLR, 2021.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximately}
Kakade, S. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{ICML}, volume~2, pp.\  267--274, 2002.

\bibitem[Kakade(2001)]{kakade2001natural}
Kakade, S.~M.
\newblock A natural policy gradient.
\newblock \emph{Advances in neural information processing systems}, 14, 2001.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{kidambi2020morel}
Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T.
\newblock Morel: Model-based offline reinforcement learning.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{3rd International Conference on Learning Representations},
  2015.

\bibitem[Konda \& Tsitsiklis(2000)Konda and Tsitsiklis]{konda2000actor}
Konda, V.~R. and Tsitsiklis, J.~N.
\newblock Actor-critic algorithms.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1008--1014. Citeseer, 2000.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Nair, and
  Levine]{kostrikov2021offline}
Kostrikov, I., Nair, A., and Levine, S.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock \emph{arXiv preprint arXiv:2110.06169}, 2021.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and
  Levine]{kumar2019stabilizing}
Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 11784--11794, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1179--1191, 2020.

\bibitem[Laroche et~al.(2019)Laroche, Trichelair, and
  Des~Combes]{laroche2019safe}
Laroche, R., Trichelair, P., and Des~Combes, R.~T.
\newblock Safe policy improvement with baseline bootstrapping.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3652--3661. PMLR, 2019.

\bibitem[Liu et~al.(2020)Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020provably}
Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E.
\newblock Provably good batch reinforcement learning without great exploration.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Maei et~al.(2009)Maei, Szepesvari, Bhatnagar, Precup, Silver, and
  Sutton]{maei2009convergent}
Maei, H.~R., Szepesvari, C., Bhatnagar, S., Precup, D., Silver, D., and Sutton,
  R.~S.
\newblock Convergent temporal-difference learning with arbitrary smooth
  function approximation.
\newblock In \emph{NIPS}, pp.\  1204--1212, 2009.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[M{\"u}ller(1997)]{muller1997integral}
M{\"u}ller, A.
\newblock Integral probability metrics and their generating classes of
  functions.
\newblock \emph{Advances in Applied Probability}, 1997.

\bibitem[Munos(2003)]{munos2003error}
Munos, R.
\newblock Error bounds for approximate policy iteration.
\newblock In \emph{Proceedings of the Twentieth International Conference on
  International Conference on Machine Learning}, pp.\  560--567, 2003.

\bibitem[Munos \& Szepesv{\'a}ri(2008)Munos and
  Szepesv{\'a}ri]{munos2008finite}
Munos, R. and Szepesv{\'a}ri, C.
\newblock Finite-time bounds for fitted value iteration.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (5), 2008.

\bibitem[Neu et~al.(2017)Neu, Jonsson, and G{\'o}mez]{neu2017unified}
Neu, G., Jonsson, A., and G{\'o}mez, V.
\newblock A unified view of entropy-regularized markov decision processes.
\newblock \emph{arXiv preprint arXiv:1705.07798}, 2017.

\bibitem[Paine et~al.(2020)Paine, Paduraru, Michi, Gulcehre, Zolna, Novikov,
  Wang, and de~Freitas]{paine2020hyperparameter}
Paine, T.~L., Paduraru, C., Michi, A., Gulcehre, C., Zolna, K., Novikov, A.,
  Wang, Z., and de~Freitas, N.
\newblock Hyperparameter selection for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2007.09055}, 2020.

\bibitem[Rajeswaran et~al.(2020)Rajeswaran, Mordatch, and
  Kumar]{rajeswaran2020game}
Rajeswaran, A., Mordatch, I., and Kumar, V.
\newblock A game theoretic framework for model based reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7953--7963. PMLR, 2020.

\bibitem[Schoknecht \& Merke(2003)Schoknecht and Merke]{schoknecht2003td}
Schoknecht, R. and Merke, A.
\newblock Td (0) converges provably faster than the residual gradient
  algorithm.
\newblock In \emph{Proceedings of the 20th International Conference on Machine
  Learning (ICML-03)}, pp.\  680--687, 2003.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, van~den
  Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., van~den Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Sun et~al.(2017)Sun, Venkatraman, Gordon, Boots, and
  Bagnell]{sun2017deeply}
Sun, W., Venkatraman, A., Gordon, G.~J., Boots, B., and Bagnell, J.~A.
\newblock Deeply aggrevated: Differentiable imitation learning for sequential
  prediction.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3309--3318. PMLR, 2017.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Swamy et~al.(2021)Swamy, Choudhury, Bagnell, and Wu]{swamy2021moments}
Swamy, G., Choudhury, S., Bagnell, J.~A., and Wu, S.
\newblock Of moments and matching: A game-theoretic framework for closing the
  imitation gap.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10022--10032. PMLR, 2021.

\bibitem[Uehara et~al.(2021)Uehara, Zhang, and Sun]{uehara2021representation}
Uehara, M., Zhang, X., and Sun, W.
\newblock Representation learning for online and offline rl in low-rank mdps.
\newblock \emph{arXiv preprint arXiv:2110.04652}, 2021.

\bibitem[Von~Stackelberg(2010)]{von2010market}
Von~Stackelberg, H.
\newblock \emph{Market structure and equilibrium}.
\newblock Springer Science \& Business Media, 2010.

\bibitem[Wainwright(2019)]{wainwright2019high}
Wainwright, M.~J.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wang \& Ueda(2021)Wang and Ueda]{wang2021convergent}
Wang, Z.~T. and Ueda, M.
\newblock A convergent and efficient deep q network algorithm.
\newblock \emph{arXiv preprint arXiv:2106.15419}, 2021.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Wu, Y., Tucker, G., and Nachum, O.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Xie \& Jiang(2020)Xie and Jiang]{xie2020q}
Xie, T. and Jiang, N.
\newblock Q* approximation schemes for batch reinforcement learning: A
  theoretical comparison.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, pp.\
  550--559. PMLR, 2020.

\bibitem[Xie et~al.(2021)Xie, Cheng, Jiang, Mineiro, and
  Agarwal]{xie2021bellman}
Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A.
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 6683--6694, 2021.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{yu2020mopo}
Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J.~Y., Levine, S., Finn, C., and
  Ma, T.
\newblock Mopo: Model-based offline policy optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 14129--14142, 2020.

\bibitem[Yu et~al.(2021)Yu, Kumar, Rafailov, Rajeswaran, Levine, and
  Finn]{yu2021combo}
Yu, T., Kumar, A., Rafailov, R., Rajeswaran, A., Levine, S., and Finn, C.
\newblock Combo: Conservative offline model-based policy optimization.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 28954--28967, 2021.

\bibitem[Zanette et~al.(2021)Zanette, Wainwright, and
  Brunskill]{zanette2021provable}
Zanette, A., Wainwright, M.~J., and Brunskill, E.
\newblock Provable benefits of actor-critic methods for offline reinforcement
  learning.
\newblock \emph{Advances in neural information processing systems}, 34, 2021.

\bibitem[Zhang \& Jiang(2021)Zhang and Jiang]{zhang2021towards}
Zhang, S. and Jiang, N.
\newblock Towards hyperparameter-free policy selection for offline
  reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Zheng et~al.(2021)Zheng, Fiez, Alumbaugh, Chasnov, and
  Ratliff]{zheng2021stackelberg}
Zheng, L., Fiez, T., Alumbaugh, Z., Chasnov, B., and Ratliff, L.~J.
\newblock Stackelberg actor-critic: Game-theoretic reinforcement learning
  algorithms.
\newblock \emph{arXiv preprint arXiv:2109.12286}, 2021.

\end{thebibliography}
