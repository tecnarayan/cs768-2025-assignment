\begin{thebibliography}{81}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
  Ghemawat, S., Irving, G., Isard, M., et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{12th $\{$USENIX$\}$ Symposium on Operating Systems Design
  and Implementation ($\{$OSDI$\}$ 16)}, pp.\  265--283, 2016.

\bibitem[Alemi et~al.(2016)Alemi, Fischer, Dillon, and Murphy]{alemi2016deep}
Alemi, A.~A., Fischer, I., Dillon, J.~V., and Murphy, K.
\newblock Deep variational information bottleneck.
\newblock \emph{arXiv preprint arXiv:1612.00410}, 2016.

\bibitem[Alemi et~al.(2017)Alemi, Poole, Fischer, Dillon, Saurous, and
  Murphy]{alemi2017information}
Alemi, A.~A., Poole, B., Fischer, I., Dillon, J.~V., Saurous, R.~A., and
  Murphy, K.
\newblock An information-theoretic analysis of deep latent-variable models.
\newblock \emph{arXiv preprint arXiv:1711.00464}, 2017.

\bibitem[Ball{\'e} et~al.(2016)Ball{\'e}, Laparra, and
  Simoncelli]{balle2016end}
Ball{\'e}, J., Laparra, V., and Simoncelli, E.~P.
\newblock End-to-end optimized image compression.
\newblock \emph{arXiv preprint arXiv:1611.01704}, 2016.

\bibitem[Ball{\'e} et~al.(2018)Ball{\'e}, Minnen, Singh, Hwang, and
  Johnston]{balle2018variational}
Ball{\'e}, J., Minnen, D., Singh, S., Hwang, S.~J., and Johnston, N.
\newblock Variational image compression with a scale hyperprior.
\newblock \emph{arXiv preprint arXiv:1802.01436}, 2018.

\bibitem[Barber \& Agakov(2006)Barber and Agakov]{barber2006kernelized}
Barber, D. and Agakov, F.~V.
\newblock Kernelized infomax clustering.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  17--24, 2006.

\bibitem[Bengio et~al.(2007)Bengio, LeCun, et~al.]{bengio2007scaling}
Bengio, Y., LeCun, Y., et~al.
\newblock Scaling learning algorithms towards ai.
\newblock 2007.

\bibitem[Bengio et~al.(2014)Bengio, Laufer, Alain, and
  Yosinski]{bengio2014deep}
Bengio, Y., Laufer, E., Alain, G., and Yosinski, J.
\newblock Deep generative stochastic networks trainable by backprop.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  226--234, 2014.

\bibitem[Berlekamp et~al.(1978)Berlekamp, McEliece, and
  Van~Tilborg]{berlekamp1978inherent}
Berlekamp, E., McEliece, R., and Van~Tilborg, H.
\newblock On the inherent intractability of certain coding problems (corresp.).
\newblock \emph{IEEE Transactions on Information Theory}, 24\penalty0
  (3):\penalty0 384--386, 1978.

\bibitem[Bourtsoulatze et~al.(2018)Bourtsoulatze, Kurka, and
  Gunduz]{bourtsoulatze2018deep}
Bourtsoulatze, E., Kurka, D.~B., and Gunduz, D.
\newblock Deep joint source-channel coding for wireless image transmission.
\newblock \emph{arXiv preprint arXiv:1809.01733}, 2018.

\bibitem[Burda et~al.(2015)Burda, Grosse, and
  Salakhutdinov]{burda2015importance}
Burda, Y., Grosse, R., and Salakhutdinov, R.
\newblock Importance weighted autoencoders.
\newblock \emph{arXiv preprint arXiv:1509.00519}, 2015.

\bibitem[Cammerer et~al.(2017)Cammerer, Gruber, Hoydis, and ten
  Brink]{cammerer2017scaling}
Cammerer, S., Gruber, T., Hoydis, J., and ten Brink, S.
\newblock Scaling deep learning-based decoding of polar codes via partitioning.
\newblock In \emph{GLOBECOM 2017-2017 IEEE Global Communications Conference},
  pp.\  1--6. IEEE, 2017.

\bibitem[Chen \& Fossorier(2002)Chen and Fossorier]{chen2002near}
Chen, J. and Fossorier, M.~P.
\newblock Near optimum universal belief propagation based decoding of
  low-density parity check codes.
\newblock \emph{IEEE Transactions on communications}, 50\penalty0 (3):\penalty0
  406--414, 2002.

\bibitem[Chen et~al.(2016{\natexlab{a}})Chen, Duan, Houthooft, Schulman,
  Sutskever, and Abbeel]{chen2016infogan}
Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P.
\newblock Infogan: Interpretable representation learning by information
  maximizing generative adversarial nets.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2172--2180, 2016{\natexlab{a}}.

\bibitem[Chen et~al.(2016{\natexlab{b}})Chen, Kingma, Salimans, Duan, Dhariwal,
  Schulman, Sutskever, and Abbeel]{chen2016variational}
Chen, X., Kingma, D.~P., Salimans, T., Duan, Y., Dhariwal, P., Schulman, J.,
  Sutskever, I., and Abbeel, P.
\newblock Variational lossy autoencoder.
\newblock \emph{arXiv preprint arXiv:1611.02731}, 2016{\natexlab{b}}.

\bibitem[Csiszar(1982)]{csiszar1982linear}
Csiszar, I.
\newblock Linear codes for sources and source networks: Error exponents,
  universal coding.
\newblock \emph{IEEE Transactions on Information Theory}, 28\penalty0
  (4):\penalty0 585--592, 1982.

\bibitem[Diggle \& Gratton(1984)Diggle and Gratton]{diggle1984monte}
Diggle, P.~J. and Gratton, R.~J.
\newblock Monte carlo methods of inference for implicit statistical models.
\newblock \emph{Journal of the Royal Statistical Society. Series B
  (Methodological)}, pp.\  193--227, 1984.

\bibitem[Dorner et~al.(2017)Dorner, Cammerer, Hoydis, and ten
  Brink]{dorner2017deep}
Dorner, S., Cammerer, S., Hoydis, J., and ten Brink, S.
\newblock On deep learning-based communication over the air.
\newblock In \emph{Signals, Systems, and Computers, 2017 51st Asilomar
  Conference on}, pp.\  1791--1795. IEEE, 2017.

\bibitem[Farsad et~al.(2018)Farsad, Rao, and Goldsmith]{farsad2018deep}
Farsad, N., Rao, M., and Goldsmith, A.
\newblock Deep learning for joint source-channel coding of text.
\newblock \emph{arXiv preprint arXiv:1802.06832}, 2018.

\bibitem[Feldman et~al.(2005)Feldman, Wainwright, and Karger]{feldman2005using}
Feldman, J., Wainwright, M.~J., and Karger, D.~R.
\newblock Using linear programming to decode binary linear codes.
\newblock \emph{IEEE Transactions on Information Theory}, 51\penalty0
  (3):\penalty0 954--972, 2005.

\bibitem[Fossorier et~al.(1999)Fossorier, Mihaljevic, and
  Imai]{fossorier1999reduced}
Fossorier, M.~P., Mihaljevic, M., and Imai, H.
\newblock Reduced complexity iterative decoding of low-density parity check
  codes based on belief propagation.
\newblock \emph{IEEE Transactions on communications}, 47\penalty0 (5):\penalty0
  673--680, 1999.

\bibitem[Fouladi et~al.(2018)Fouladi, Emmons, Orbay, Wu, Wahby, and
  Winstein]{fouladi2018salsify}
Fouladi, S., Emmons, J., Orbay, E., Wu, C., Wahby, R.~S., and Winstein, K.
\newblock Salsify: Low-latency network video through tighter integration
  between a video codec and a transport protocol.
\newblock 2018.

\bibitem[Gallager(1962)]{gallager1962low}
Gallager, R.
\newblock Low-density parity-check codes.
\newblock \emph{IRE Transactions on information theory}, 8\penalty0
  (1):\penalty0 21--28, 1962.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2672--2680, 2014.

\bibitem[Google(2015)]{webp}
Google.
\newblock Webp compression study.
\newblock \url{https://developers.google.com/speed/webp/ docs/webp_study},
  2015.
\newblock [Online; accessed 22-January-2019].

\bibitem[Goyal et~al.(2017)Goyal, Ke, Ganguli, and
  Bengio]{goyal2017variational}
Goyal, A. G. A.~P., Ke, N.~R., Ganguli, S., and Bengio, Y.
\newblock Variational walkback: Learning a transition operator as a stochastic
  recurrent net.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4392--4402, 2017.

\bibitem[Grathwohl et~al.(2017)Grathwohl, Choi, Wu, Roeder, and
  Duvenaud]{grathwohl2017backpropagation}
Grathwohl, W., Choi, D., Wu, Y., Roeder, G., and Duvenaud, D.
\newblock Backpropagation through the void: Optimizing control variates for
  black-box gradient estimation.
\newblock \emph{arXiv preprint arXiv:1711.00123}, 2017.

\bibitem[Grover \& Ermon(2019)Grover and Ermon]{grover2018uae}
Grover, A. and Ermon, S.
\newblock Uncertainty autoencoders: Learning compressed representations via
  variational information maximization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2019.

\bibitem[Grover et~al.(2018)Grover, Gummadi, Lazaro-Gredilla, Schuurmans, and
  Ermon]{grover2018variational}
Grover, A., Gummadi, R., Lazaro-Gredilla, M., Schuurmans, D., and Ermon, S.
\newblock Variational rejection sampling.
\newblock In \emph{Proc. 21st International Conference on Artificial
  Intelligence and Statistics}, 2018.

\bibitem[Gruber et~al.(2017)Gruber, Cammerer, Hoydis, and ten
  Brink]{gruber2017deep}
Gruber, T., Cammerer, S., Hoydis, J., and ten Brink, S.
\newblock On deep learning-based channel decoding.
\newblock In \emph{Information Sciences and Systems (CISS), 2017 51st Annual
  Conference on}, pp.\  1--6. IEEE, 2017.

\bibitem[Gu et~al.(2015)Gu, Levine, Sutskever, and Mnih]{gu2015muprop}
Gu, S., Levine, S., Sutskever, I., and Mnih, A.
\newblock Muprop: Unbiased backpropagation for stochastic neural networks.
\newblock \emph{arXiv preprint arXiv:1511.05176}, 2015.

\bibitem[Higgins et~al.(2016)Higgins, Matthey, Pal, Burgess, Glorot, Botvinick,
  Mohamed, and Lerchner]{higgins2016beta}
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M.,
  Mohamed, S., and Lerchner, A.
\newblock beta-vae: Learning basic visual concepts with a constrained
  variational framework.
\newblock 2016.

\bibitem[Hinton \& Van~Camp(1993)Hinton and Van~Camp]{hinton1993keeping}
Hinton, G.~E. and Van~Camp, D.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In \emph{Proceedings of the sixth annual conference on Computational
  learning theory}, pp.\  5--13. ACM, 1993.

\bibitem[Hjelm et~al.(2018)Hjelm, Fedorov, Lavoie-Marchildon, Grewal,
  Trischler, and Bengio]{hjelm2018learning}
Hjelm, R.~D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Trischler, A.,
  and Bengio, Y.
\newblock Learning deep representations by mutual information estimation and
  maximization.
\newblock \emph{arXiv preprint arXiv:1808.06670}, 2018.

\bibitem[Honkela \& Valpola(2004)Honkela and Valpola]{honkela2004variational}
Honkela, A. and Valpola, H.
\newblock Variational learning and bits-back coding: an information-theoretic
  view to bayesian learning.
\newblock \emph{IEEE Transactions on Neural Networks}, 15\penalty0
  (4):\penalty0 800--810, 2004.

\bibitem[Hu et~al.(2017)Hu, Miyato, Tokui, Matsumoto, and
  Sugiyama]{hu2017learning}
Hu, W., Miyato, T., Tokui, S., Matsumoto, E., and Sugiyama, M.
\newblock Learning discrete representations via information maximizing
  self-augmented training.
\newblock \emph{arXiv preprint arXiv:1702.08720}, 2017.

\bibitem[Jang et~al.(2016)Jang, Gu, and Poole]{jang2016categorical}
Jang, E., Gu, S., and Poole, B.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock \emph{arXiv preprint arXiv:1611.01144}, 2016.

\bibitem[Kaiser \& Bengio(2018)Kaiser and Bengio]{kaiser2018discrete}
Kaiser, {\L}. and Bengio, S.
\newblock Discrete autoencoders for sequence models.
\newblock \emph{arXiv preprint arXiv:1801.09797}, 2018.

\bibitem[Kim et~al.(2018{\natexlab{a}})Kim, Jiang, Kannan, Oh, and
  Viswanath]{kim2018deepcode}
Kim, H., Jiang, Y., Kannan, S., Oh, S., and Viswanath, P.
\newblock Deepcode: Feedback codes via deep learning.
\newblock \emph{arXiv preprint arXiv:1807.00801}, 2018{\natexlab{a}}.

\bibitem[Kim et~al.(2018{\natexlab{b}})Kim, Jiang, Rana, Kannan, Oh, and
  Viswanath]{kim2018communication}
Kim, H., Jiang, Y., Rana, R., Kannan, S., Oh, S., and Viswanath, P.
\newblock Communication algorithms via deep learning.
\newblock \emph{arXiv preprint arXiv:1805.09317}, 2018{\natexlab{b}}.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Koetter \& Vontobel(2003)Koetter and Vontobel]{koetter2003graph}
Koetter, R. and Vontobel, P.~O.
\newblock Graph-covers and iterative decoding of finite length codes.
\newblock Citeseer, 2003.

\bibitem[Kostina \& Verd{\'u}(2013)Kostina and Verd{\'u}]{kostina2013lossy}
Kostina, V. and Verd{\'u}, S.
\newblock Lossy joint source-channel coding in the finite blocklength regime.
\newblock \emph{IEEE Transactions on Information Theory}, 59\penalty0
  (5):\penalty0 2545--2575, 2013.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Lake et~al.(2015)Lake, Salakhutdinov, and Tenenbaum]{lake2015human}
Lake, B.~M., Salakhutdinov, R., and Tenenbaum, J.~B.
\newblock Human-level concept learning through probabilistic program induction.
\newblock \emph{Science}, 350\penalty0 (6266):\penalty0 1332--1338, 2015.

\bibitem[LeCun(1998)]{lecun1998mnist}
LeCun, Y.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015faceattributes}
Liu, Z., Luo, P., Wang, X., and Tang, X.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of International Conference on Computer Vision
  (ICCV)}, 2015.

\bibitem[MacKay(2003)]{mackay2003information}
MacKay, D.~J.
\newblock \emph{Information theory, inference and learning algorithms}.
\newblock Cambridge university press, 2003.

\bibitem[Maddison et~al.(2016)Maddison, Mnih, and Teh]{maddison2016concrete}
Maddison, C.~J., Mnih, A., and Teh, Y.~W.
\newblock The concrete distribution: A continuous relaxation of discrete random
  variables.
\newblock \emph{arXiv preprint arXiv:1611.00712}, 2016.

\bibitem[Mnih \& Gregor(2014)Mnih and Gregor]{mnih2014neural}
Mnih, A. and Gregor, K.
\newblock Neural variational inference and learning in belief networks.
\newblock \emph{arXiv preprint arXiv:1402.0030}, 2014.

\bibitem[Mnih \& Rezende(2016)Mnih and Rezende]{mnih2016variational}
Mnih, A. and Rezende, D.~J.
\newblock Variational inference for monte carlo objectives.
\newblock \emph{arXiv preprint arXiv:1602.06725}, 2016.

\bibitem[Mohamed \& Lakshminarayanan(2016)Mohamed and
  Lakshminarayanan]{mohamed2016learning}
Mohamed, S. and Lakshminarayanan, B.
\newblock Learning in implicit generative models.
\newblock \emph{arXiv preprint arXiv:1610.03483}, 2016.

\bibitem[Nachmani et~al.(2016)Nachmani, Be'ery, and
  Burshtein]{nachmani2016learning}
Nachmani, E., Be'ery, Y., and Burshtein, D.
\newblock Learning to decode linear codes using deep learning.
\newblock In \emph{Communication, Control, and Computing (Allerton), 2016 54th
  Annual Allerton Conference on}, pp.\  341--346. IEEE, 2016.

\bibitem[Neal(1992)]{neal1992connectionist}
Neal, R.~M.
\newblock Connectionist learning of belief networks.
\newblock \emph{Artificial intelligence}, 56\penalty0 (1):\penalty0 71--113,
  1992.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.~Y.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In \emph{NIPS Workshop on Deep Learning and Unsupervised Feature
  Learning}. NIPS, 2011.

\bibitem[Pilc(1967)]{pilc1967coding}
Pilc, R.~J.
\newblock \emph{Coding theorems for discrete source-channel pairs.}
\newblock PhD thesis, Massachusetts Institute of Technology, 1967.

\bibitem[Radford et~al.(2015)Radford, Metz, and
  Chintala]{radford2015unsupervised}
Radford, A., Metz, L., and Chintala, S.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1511.06434}, 2015.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock \emph{arXiv preprint arXiv:1401.4082}, 2014.

\bibitem[Richardson \& Urbanke(2008)Richardson and
  Urbanke]{richardson2008modern}
Richardson, T. and Urbanke, R.
\newblock \emph{Modern coding theory}.
\newblock Cambridge university press, 2008.

\bibitem[Rolfe(2016)]{rolfe2016discrete}
Rolfe, J.~T.
\newblock Discrete variational autoencoders.
\newblock \emph{arXiv preprint arXiv:1609.02200}, 2016.

\bibitem[Roy et~al.(2018)Roy, Vaswani, Neelakantan, and Parmar]{roy2018theory}
Roy, A., Vaswani, A., Neelakantan, A., and Parmar, N.
\newblock Theory and experiments on vector quantized autoencoders.
\newblock \emph{arXiv preprint arXiv:1805.11063}, 2018.

\bibitem[Ruiz et~al.(2016)Ruiz, AUEB, and Blei]{ruiz2016generalized}
Ruiz, F.~R., AUEB, M. T.~R., and Blei, D.
\newblock The generalized reparameterization gradient.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  460--468, 2016.

\bibitem[Santurkar et~al.(2017)Santurkar, Budden, and
  Shavit]{santurkar2017generative}
Santurkar, S., Budden, D., and Shavit, N.
\newblock Generative compression.
\newblock \emph{arXiv preprint arXiv:1703.01467}, 2017.

\bibitem[Schulman et~al.(2015)Schulman, Heess, Weber, and
  Abbeel]{schulman2015gradient}
Schulman, J., Heess, N., Weber, T., and Abbeel, P.
\newblock Gradient estimation using stochastic computation graphs.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3528--3536, 2015.

\bibitem[Shannon(1948)]{shannon1948mathematical}
Shannon, C.~E.
\newblock A mathematical theory of communication.
\newblock \emph{Bell Syst. Tech. J.}, 27:\penalty0 623--656, 1948.

\bibitem[Shu et~al.(2018)Shu, Bui, Zhao, Kochenderfer, and
  Ermon]{shu2018amortized}
Shu, R., Bui, H.~H., Zhao, S., Kochenderfer, M.~J., and Ermon, S.
\newblock Amortized inference regularization.
\newblock In \emph{NIPS}, 2018.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and
  Ganguli]{sohl2015deep}
Sohl-Dickstein, J., Weiss, E.~A., Maheswaranathan, N., and Ganguli, S.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock \emph{arXiv preprint arXiv:1503.03585}, 2015.

\bibitem[Theis et~al.(2017)Theis, Shi, Cunningham, and
  Husz{\'a}r]{theis2017lossy}
Theis, L., Shi, W., Cunningham, A., and Husz{\'a}r, F.
\newblock Lossy image compression with compressive autoencoders.
\newblock \emph{arXiv preprint arXiv:1703.00395}, 2017.

\bibitem[Toderici et~al.(2015)Toderici, O'Malley, Hwang, Vincent, Minnen,
  Baluja, Covell, and Sukthankar]{toderici2015variable}
Toderici, G., O'Malley, S.~M., Hwang, S.~J., Vincent, D., Minnen, D., Baluja,
  S., Covell, M., and Sukthankar, R.
\newblock Variable rate image compression with recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1511.06085}, 2015.

\bibitem[Toderici et~al.(2017)Toderici, Vincent, Johnston, Hwang, Minnen, Shor,
  and Covell]{toderici2017full}
Toderici, G., Vincent, D., Johnston, N., Hwang, S.~J., Minnen, D., Shor, J.,
  and Covell, M.
\newblock Full resolution image compression with recurrent neural networks.
\newblock In \emph{CVPR}, pp.\  5435--5443, 2017.

\bibitem[Tucker et~al.(2017)Tucker, Mnih, Maddison, Lawson, and
  Sohl-Dickstein]{tucker2017rebar}
Tucker, G., Mnih, A., Maddison, C.~J., Lawson, J., and Sohl-Dickstein, J.
\newblock Rebar: Low-variance, unbiased gradient estimates for discrete latent
  variable models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2627--2636, 2017.

\bibitem[van~den Oord et~al.(2017)van~den Oord, Vinyals, et~al.]{van2017neural}
van~den Oord, A., Vinyals, O., et~al.
\newblock Neural discrete representation learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6306--6315, 2017.

\bibitem[Vincent et~al.(2008)Vincent, Larochelle, Bengio, and
  Manzagol]{vincent2008extracting}
Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A.
\newblock Extracting and composing robust features with denoising autoencoders.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pp.\  1096--1103. ACM, 2008.

\bibitem[Vincent et~al.(2010)Vincent, Larochelle, Lajoie, Bengio, and
  Manzagol]{vincent2010stacked}
Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P.-A.
\newblock Stacked denoising autoencoders: Learning useful representations in a
  deep network with a local denoising criterion.
\newblock \emph{Journal of machine learning research}, 11\penalty0
  (Dec):\penalty0 3371--3408, 2010.

\bibitem[Vontobel \& Koetter(2007)Vontobel and Koetter]{vontobel2007low}
Vontobel, P.~O. and Koetter, R.
\newblock On low-complexity linear-programming decoding of ldpc codes.
\newblock \emph{European transactions on telecommunications}, 18\penalty0
  (5):\penalty0 509--517, 2007.

\bibitem[Wang et~al.(2013)Wang, Chen, Smola, and Xing]{wang2013variance}
Wang, C., Chen, X., Smola, A.~J., and Xing, E.~P.
\newblock Variance reduction for stochastic gradient optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  181--189, 2013.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Zarcone et~al.(2018)Zarcone, Paiton, Anderson, Engel, Wong, and
  Olshausen]{zarcone2018joint}
Zarcone, R., Paiton, D., Anderson, A., Engel, J., Wong, H.~P., and Olshausen,
  B.
\newblock Joint source-channel coding with neural networks for analog data
  compression and storage.
\newblock In \emph{2018 Data Compression Conference}, pp.\  147--156. IEEE,
  2018.

\bibitem[Zhai \& Katsaggelos(2007)Zhai and Katsaggelos]{zhai2007joint}
Zhai, F. and Katsaggelos, A.
\newblock Joint source-channel video transmission.
\newblock \emph{Synthesis Lectures on Image, Video, and Multimedia Processing},
  3\penalty0 (1):\penalty0 1--136, 2007.

\bibitem[Zhao et~al.(2017)Zhao, Song, and Ermon]{zhao2017infovae}
Zhao, S., Song, J., and Ermon, S.
\newblock Infovae: Information maximizing variational autoencoders.
\newblock \emph{arXiv preprint arXiv:1706.02262}, 2017.

\bibitem[Zhao et~al.(2018)Zhao, Song, and Ermon]{zhao2018lagrangian}
Zhao, S., Song, J., and Ermon, S.
\newblock A lagrangian perspective on latent variable generative models.
\newblock In \emph{Proc. 34th Conference on Uncertainty in Artificial
  Intelligence}, 2018.

\end{thebibliography}
