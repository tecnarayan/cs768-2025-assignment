\begin{thebibliography}{91}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainslie et~al.(2020)Ainslie, Ontanon, Alberti, Cvicek, Fisher, Pham,
  Ravula, Sanghai, Wang, and Yang]{ainslie2020etc}
Ainslie, J., Ontanon, S., Alberti, C., Cvicek, V., Fisher, Z., Pham, P.,
  Ravula, A., Sanghai, S., Wang, Q., and Yang, L.
\newblock Etc: Encoding long and structured inputs in transformers.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  268--284, 2020.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Beltagy, I., Peters, M.~E., and Cohan, A.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Berman et~al.(2019)Berman, J{\'e}gou, Vedaldi, Kokkinos, and
  Douze]{berman2019multigrain}
Berman, M., J{\'e}gou, H., Vedaldi, A., Kokkinos, I., and Douze, M.
\newblock Multigrain: a unified image embedding for classes and instances.
\newblock \emph{arXiv preprint arXiv:1902.05509}, 2019.

\bibitem[Bochner(2020)]{bochner2020harmonic}
Bochner, S.
\newblock \emph{Harmonic analysis and the theory of probability}.
\newblock University of California press, 2020.

\bibitem[Bojar et~al.(2014)Bojar, Buck, Federmann, Haddow, Koehn, Leveling,
  Monz, Pecina, Post, Saint-Amand, et~al.]{bojar2014wmt}
Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz,
  C., Pecina, P., Post, M., Saint-Amand, H., et~al.
\newblock Findings of the 2014 workshop on statistical machine translation.
\newblock In \emph{Proceedings of the ninth workshop on statistical machine
  translation}, pp.\  12--58, 2014.

\bibitem[Carion et~al.(2020)Carion, Massa, Synnaeve, Usunier, Kirillov, and
  Zagoruyko]{carion2020detr}
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko,
  S.
\newblock End-to-end object detection with transformers.
\newblock In \emph{European Conference on Computer Vision}, pp.\  213--229.
  Springer, 2020.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Dao, Winsor, Song, Rudra, and
  R{\'e}]{chen2021scatterbrain}
Chen, B., Dao, T., Winsor, E., Song, Z., Rudra, A., and R{\'e}, C.
\newblock Scatterbrain: Unifying sparse and low-rank attention.
\newblock In \emph{Thirty-Fifth Conference on Neural Information Processing
  Systems}, 2021{\natexlab{a}}.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Panda, and Fan]{regionvit}
Chen, C.-F., Panda, R., and Fan, Q.
\newblock Regionvit: Regional-to-local attention for vision transformers.
\newblock \emph{arXiv preprint arXiv:2106.02689}, 2021{\natexlab{b}}.

\bibitem[Chen et~al.(2021{\natexlab{c}})Chen, Lu, Rajeswaran, Lee, Grover,
  Laskin, Abbeel, Srinivas, and Mordatch]{chen2021decision}
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P.,
  Srinivas, A., and Mordatch, I.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{arXiv preprint arXiv:2106.01345}, 2021{\natexlab{c}}.

\bibitem[Chen et~al.(2021{\natexlab{d}})Chen, Zeng, Ji, and
  Yang]{chen2021skyformer}
Chen, Y., Zeng, Q., Ji, H., and Yang, Y.
\newblock Skyformer: Remodel self-attention with gaussian kernel and
  nystr$\backslash$" om method.
\newblock In \emph{Thirty-Fifth Conference on Neural Information Processing
  Systems}, 2021{\natexlab{d}}.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Choromanski et~al.(2021)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and
  Weller]{choromanski2021rethinking}
Choromanski, K.~M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos,
  T., Hawkins, P., Davis, J.~Q., Mohiuddin, A., Kaiser, L., Belanger, D.~B.,
  Colwell, L.~J., and Weller, A.
\newblock Rethinking attention with performers.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Ua6zuk0WRH}.

\bibitem[Chu et~al.(2021)Chu, Tian, Wang, Zhang, Ren, Wei, Xia, and
  Shen]{twins}
Chu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., and Shen, C.
\newblock Twins: Revisiting the design of spatial attention in vision
  transformers.
\newblock \emph{arXiv preprint arXiv:2104.13840}, 2021.

\bibitem[Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le]{random-augment}
Cubuk, E.~D., Zoph, B., Shlens, J., and Le, Q.~V.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition Workshops}, pp.\  702--703, 2020.

\bibitem[Dai et~al.(2020)Dai, Lai, Yang, and Le]{dai2020funnel}
Dai, Z., Lai, G., Yang, Y., and Le, Q.
\newblock Funnel-transformer: Filtering out sequential redundancy for efficient
  language processing.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 4271--4282, 2020.

\bibitem[Daras et~al.(2020)Daras, Kitaev, Odena, and Dimakis]{daras2020smyrf}
Daras, G., Kitaev, N., Odena, A., and Dimakis, A.~G.
\newblock Smyrf-efficient attention using asymmetric clustering.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Dehghani et~al.(2019)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser]{dehghani2018universal}
Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L.
\newblock Universal transformers.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HyzdRiR9Y7}.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin-etal-2019-bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021vit}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[El-Nouby et~al.(2021)El-Nouby, Touvron, Caron, Bojanowski, Douze,
  Joulin, Laptev, Neverova, Synnaeve, Verbeek, et~al.]{el2021xcit}
El-Nouby, A., Touvron, H., Caron, M., Bojanowski, P., Douze, M., Joulin, A.,
  Laptev, I., Neverova, N., Synnaeve, G., Verbeek, J., et~al.
\newblock Xcit: Cross-covariance image transformers.
\newblock \emph{arXiv preprint arXiv:2106.09681}, 2021.

\bibitem[Fan et~al.(2020)Fan, Li, Xiong, Lo, and
  Feichtenhofer]{fan2020pyslowfast}
Fan, H., Li, Y., Xiong, B., Lo, W.-Y., and Feichtenhofer, C.
\newblock Pyslowfast.
\newblock \url{https://github.com/facebookresearch/slowfast}, 2020.

\bibitem[Goyal et~al.(2017)Goyal, Ebrahimi~Kahou, Michalski, Materzynska,
  Westphal, Kim, Haenel, Fruend, Yianilos, Mueller-Freitag,
  et~al.]{goyal2017ssv2}
Goyal, R., Ebrahimi~Kahou, S., Michalski, V., Materzynska, J., Westphal, S.,
  Kim, H., Haenel, V., Fruend, I., Yianilos, P., Mueller-Freitag, M., et~al.
\newblock The" something something" video database for learning and evaluating
  visual common sense.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  5842--5850, 2017.

\bibitem[Hoffer et~al.(2020)Hoffer, Ben-Nun, Hubara, Giladi, Hoefler, and
  Soudry]{repeat-augment}
Hoffer, E., Ben-Nun, T., Hubara, I., Giladi, N., Hoefler, T., and Soudry, D.
\newblock Augment your batch: Improving generalization through instance
  repetition.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  8129--8138, 2020.

\bibitem[Huang et~al.(2016)Huang, Sun, Liu, Sedra, and
  Weinberger]{stochastic-depth}
Huang, G., Sun, Y., Liu, Z., Sedra, D., and Weinberger, K.~Q.
\newblock Deep networks with stochastic depth.
\newblock In \emph{European conference on computer vision}, pp.\  646--661.
  Springer, 2016.

\bibitem[Jaegle et~al.(2021{\natexlab{a}})Jaegle, Borgeaud, Alayrac, Doersch,
  Ionescu, Ding, Koppula, Zoran, Brock, Shelhamer,
  et~al.]{jaegle2021perceiverio}
Jaegle, A., Borgeaud, S., Alayrac, J.-B., Doersch, C., Ionescu, C., Ding, D.,
  Koppula, S., Zoran, D., Brock, A., Shelhamer, E., et~al.
\newblock Perceiver io: A general architecture for structured inputs \&
  outputs.
\newblock \emph{arXiv preprint arXiv:2107.14795}, 2021{\natexlab{a}}.

\bibitem[Jaegle et~al.(2021{\natexlab{b}})Jaegle, Gimeno, Brock, Vinyals,
  Zisserman, and Carreira]{jaegle2021perceiver}
Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J.
\newblock Perceiver: General perception with iterative attention.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  4651--4664. PMLR,
  18--24 Jul 2021{\natexlab{b}}.
\newblock URL \url{https://proceedings.mlr.press/v139/jaegle21a.html}.

\bibitem[Jumper et~al.(2021)Jumper, Evans, Pritzel, Green, Figurnov,
  Ronneberger, Tunyasuvunakool, Bates, {\v{Z}}{\'\i}dek, Potapenko,
  et~al.]{jumper2021highly}
Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O.,
  Tunyasuvunakool, K., Bates, R., {\v{Z}}{\'\i}dek, A., Potapenko, A., et~al.
\newblock Highly accurate protein structure prediction with alphafold.
\newblock \emph{Nature}, 596\penalty0 (7873):\penalty0 583--589, 2021.

\bibitem[Kasai et~al.(2021{\natexlab{a}})Kasai, Pappas, Peng, Cross, and
  Smith]{kasai2021deepNMT}
Kasai, J., Pappas, N., Peng, H., Cross, J., and Smith, N.
\newblock Deep encoder, shallow decoder: Reevaluating non-autoregressive
  machine translation.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=KpfasTaLUpq}.

\bibitem[Kasai et~al.(2021{\natexlab{b}})Kasai, Peng, Zhang, Yogatama, Ilharco,
  Pappas, Mao, Chen, and Smith]{kasai2021t2r}
Kasai, J., Peng, H., Zhang, Y., Yogatama, D., Ilharco, G., Pappas, N., Mao, Y.,
  Chen, W., and Smith, N.~A.
\newblock Finetuning pretrained transformers into rnns.
\newblock \emph{arXiv preprint arXiv:2103.13076}, 2021{\natexlab{b}}.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{katharopoulos2020transformers_are_rnns}
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5156--5165. PMLR, 2020.

\bibitem[Kay et~al.(2017)Kay, Carreira, Simonyan, Zhang, Hillier,
  Vijayanarasimhan, Viola, Green, Back, Natsev, et~al.]{kay2017kinetics}
Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan,
  S., Viola, F., Green, T., Back, T., Natsev, P., et~al.
\newblock The kinetics human action video dataset.
\newblock \emph{arXiv preprint arXiv:1705.06950}, 2017.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{vae1}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{Kitaev2020reformer}
Kitaev, N., Kaiser, L., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkgNKkHtvB}.

\bibitem[Kondapaneni et~al.(2019)Kondapaneni, Vevoda, Grittmann, Sk\v{r}ivan,
  Slusallek, and K\v{r}iv\'{a}nek]{optimal_mis}
Kondapaneni, I., Vevoda, P., Grittmann, P., Sk\v{r}ivan, T., Slusallek, P., and
  K\v{r}iv\'{a}nek, J.
\newblock Optimal multiple importance sampling.
\newblock \emph{ACM Trans. Graph.}, 38\penalty0 (4), jul 2019.
\newblock ISSN 0730-0301.
\newblock \doi{10.1145/3306346.3323009}.
\newblock URL \url{https://doi.org/10.1145/3306346.3323009}.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009cifar}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lee et~al.(2019)Lee, Lee, Kim, Kosiorek, Choi, and Teh]{lee2019set}
Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh, Y.~W.
\newblock Set transformer: A framework for attention-based
  permutation-invariant neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3744--3753. PMLR, 2019.

\bibitem[Lee-Thorp et~al.(2021)Lee-Thorp, Ainslie, Eckstein, and
  Ontanon]{lee2021fnet}
Lee-Thorp, J., Ainslie, J., Eckstein, I., and Ontanon, S.
\newblock Fnet: Mixing tokens with fourier transforms.
\newblock \emph{arXiv preprint arXiv:2105.03824}, 2021.

\bibitem[Lin et~al.(2021)Lin, Wang, Liu, and Qiu]{lin2021survey}
Lin, T., Wang, Y., Liu, X., and Qiu, X.
\newblock A survey of transformers.
\newblock \emph{arXiv preprint arXiv:2106.04554}, 2021.

\bibitem[Linsley et~al.(2018)Linsley, Kim, Veerabadran, Windolf, and
  Serre]{linsley2018pathfinder}
Linsley, D., Kim, J., Veerabadran, V., Windolf, C., and Serre, T.
\newblock Learning long-range spatial dependencies with horizontal gated
  recurrent units.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pp.\  152--164, 2018.

\bibitem[Liu* et~al.(2018)Liu*, Saleh*, Pot, Goodrich, Sepassi, Kaiser, and
  Shazeer]{liu2018generating}
Liu*, P.~J., Saleh*, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L., and
  Shazeer, N.
\newblock Generating wikipedia by summarizing long sequences.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=Hyg0vbWC-}.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{liu2021swin}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pp.\  10012--10022, October 2021.

\bibitem[Loshchilov \& Hutter(2016)Loshchilov and Hutter]{cos-lr}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{arXiv preprint arXiv:1608.03983}, 2016.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{adamw}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Lu et~al.(2021)Lu, Yao, Zhang, Zhu, Xu, Gao, Xu, Xiang, and
  Zhang]{lu2021soft}
Lu, J., Yao, J., Zhang, J., Zhu, X., Xu, H., Gao, W., Xu, C., Xiang, T., and
  Zhang, L.
\newblock Soft: Softmax-free transformer with linear complexity.
\newblock In \emph{Thirty-Fifth Conference on Neural Information Processing
  Systems}, 2021.

\bibitem[Ma et~al.(2021)Ma, Kong, Wang, Zhou, May, Ma, and
  Zettlemoyer]{ma2021luna}
Ma, X., Kong, X., Wang, S., Zhou, C., May, J., Ma, H., and Zettlemoyer, L.
\newblock Luna: Linear unified nested attention.
\newblock \emph{arXiv preprint arXiv:2106.01540}, 2021.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and Potts]{maas2011imdb}
Maas, A., Daly, R.~E., Pham, P.~T., Huang, D., Ng, A.~Y., and Potts, C.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proceedings of the 49th annual meeting of the association
  for computational linguistics: Human language technologies}, pp.\  142--150,
  2011.

\bibitem[Nangia \& Bowman(2018)Nangia and Bowman]{nangia2018listops}
Nangia, N. and Bowman, S.~R.
\newblock Listops: A diagnostic dataset for latent tree learning.
\newblock \emph{arXiv preprint arXiv:1804.06028}, 2018.

\bibitem[Nguyen et~al.(2021)Nguyen, Suliafu, Osher, Chen, and
  Wang]{nguyen2021fmmformer}
Nguyen, T., Suliafu, V., Osher, S., Chen, L., and Wang, B.
\newblock Fmmformer: Efficient and flexible transformer via decomposed
  near-field and far-field attention.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Ott et~al.(2018)Ott, Edunov, Grangier, and Auli]{ott2018scalingNMT}
Ott, M., Edunov, S., Grangier, D., and Auli, M.
\newblock Scaling neural machine translation.
\newblock \emph{arXiv preprint arXiv:1806.00187}, 2018.

\bibitem[Owen(2013)]{mcbook}
Owen, A.~B.
\newblock \emph{Monte Carlo theory, methods and examples}.
\newblock 2013.

\bibitem[Parmar et~al.(2018)Parmar, Vaswani, Uszkoreit, Kaiser, Shazeer, Ku,
  and Tran]{parmar2018image}
Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., and
  Tran, D.
\newblock Image transformer.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4055--4064. PMLR, 2018.

\bibitem[Patrick et~al.(2021)Patrick, Campbell, Asano, Misra, Metze,
  Feichtenhofer, Vedaldi, and Henriques]{patrick2021motionformer}
Patrick, M., Campbell, D., Asano, Y., Misra, I., Metze, F., Feichtenhofer, C.,
  Vedaldi, A., and Henriques, J.~F.
\newblock Keeping your eye on the ball: Trajectory attention in video
  transformers.
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W.
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=mfQxdSMWOF}.

\bibitem[Peng et~al.(2021{\natexlab{a}})Peng, Kasai, Pappas, Yogatama, Wu,
  Kong, Schwartz, and Smith]{peng2021abc}
Peng, H., Kasai, J., Pappas, N., Yogatama, D., Wu, Z., Kong, L., Schwartz, R.,
  and Smith, N.~A.
\newblock Abc: Attention with bounded-memory control.
\newblock \emph{arXiv preprint arXiv:2110.02488}, 2021{\natexlab{a}}.

\bibitem[Peng et~al.(2021{\natexlab{b}})Peng, Pappas, Yogatama, Schwartz,
  Smith, and Kong]{peng2021rfa}
Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N., and Kong, L.
\newblock Random feature attention.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.

\bibitem[Pietruszka et~al.(2020)Pietruszka, Borchmann, and
  Garncarek]{pietruszka2020sparsifying}
Pietruszka, M., Borchmann, {\L}., and Garncarek, {\L}.
\newblock Sparsifying transformer models with trainable representation pooling.
\newblock \emph{arXiv preprint arXiv:2009.05169}, 2020.

\bibitem[Radev et~al.(2013)Radev, Muthukrishnan, Qazvinian, and
  Abu-Jbara]{radev2013aan}
Radev, D.~R., Muthukrishnan, P., Qazvinian, V., and Abu-Jbara, A.
\newblock The acl anthology network corpus.
\newblock \emph{Language Resources and Evaluation}, 47\penalty0 (4):\penalty0
  919--944, 2013.

\bibitem[Radosavovic et~al.(2020)Radosavovic, Kosaraju, Girshick, He, and
  Doll{\'a}r]{regnet}
Radosavovic, I., Kosaraju, R.~P., Girshick, R., He, K., and Doll{\'a}r, P.
\newblock Designing network design spaces.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  10428--10436, 2020.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020t5}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (140):\penalty0 1--67, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Rahimi \& Recht(2008)Rahimi and Recht]{random-features}
Rahimi, A. and Recht, B.
\newblock Random features for large-scale kernel machines.
\newblock In Platt, J., Koller, D., Singer, Y., and Roweis, S. (eds.),
  \emph{Advances in Neural Information Processing Systems}, volume~20. Curran
  Associates, Inc., 2008.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf}.

\bibitem[Ren et~al.(2021)Ren, Dai, Dai, Yang, Leskovec, Schuurmans, and
  Dai]{ren2021combiner}
Ren, H., Dai, H., Dai, Z., Yang, M., Leskovec, J., Schuurmans, D., and Dai, B.
\newblock Combiner: Full attention transformer with sparse computation cost.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and Wierstra]{vae2}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning}, volume~32, pp.\  1278--1286, 2014.

\bibitem[Rives et~al.(2021)Rives, Meier, Sercu, Goyal, Lin, Liu, Guo, Ott,
  Zitnick, Ma, and Fergus]{Rivese2016239118}
Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M.,
  Zitnick, C.~L., Ma, J., and Fergus, R.
\newblock Biological structure and function emerge from scaling unsupervised
  learning to 250 million protein sequences.
\newblock \emph{Proceedings of the National Academy of Sciences}, 118\penalty0
  (15), 2021.
\newblock ISSN 0027-8424.
\newblock \doi{10.1073/pnas.2016239118}.
\newblock URL \url{https://www.pnas.org/content/118/15/e2016239118}.

\bibitem[Roy et~al.(2021)Roy, Saffar, Vaswani, and Grangier]{roy2021routing}
Roy, A., Saffar, M., Vaswani, A., and Grangier, D.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  9:\penalty0 53--68, 2021.
\newblock \doi{10.1162/tacl_a_00353}.
\newblock URL \url{https://aclanthology.org/2021.tacl-1.4}.

\bibitem[Schlag et~al.(2021)Schlag, Irie, and Schmidhuber]{schlag2021linear}
Schlag, I., Irie, K., and Schmidhuber, J.
\newblock Linear transformers are secretly fast weight programmers.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9355--9366. PMLR, 2021.

\bibitem[Sennrich et~al.(2016)Sennrich, Haddow, and
  Birch]{sennrich-etal-2016-bpe}
Sennrich, R., Haddow, B., and Birch, A.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1715--1725,
  Berlin, Germany, August 2016. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P16-1162}.
\newblock URL \url{https://aclanthology.org/P16-1162}.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava14dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (56):\penalty0 1929--1958, 2014.
\newblock URL \url{http://jmlr.org/papers/v15/srivastava14a.html}.

\bibitem[Tay et~al.(2020{\natexlab{a}})Tay, Bahri, Yang, Metzler, and
  Juan]{yi2020sinkhorn}
Tay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C.
\newblock Sparse {S}inkhorn attention.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  9438--9447. PMLR,
  13--18 Jul 2020{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v119/tay20a.html}.

\bibitem[Tay et~al.(2020{\natexlab{b}})Tay, Dehghani, Bahri, and
  Metzler]{tay2020efficient}
Tay, Y., Dehghani, M., Bahri, D., and Metzler, D.
\newblock Efficient transformers: A survey.
\newblock \emph{arXiv preprint arXiv:2009.06732}, 2020{\natexlab{b}}.

\bibitem[Tay et~al.(2021{\natexlab{a}})Tay, Bahri, Metzler, Juan, Zhao, and
  Zheng]{tay2021synthesizer}
Tay, Y., Bahri, D., Metzler, D., Juan, D.-C., Zhao, Z., and Zheng, C.
\newblock Synthesizer: Rethinking self-attention for transformer models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10183--10192. PMLR, 2021{\natexlab{a}}.

\bibitem[Tay et~al.(2021{\natexlab{b}})Tay, Dehghani, Abnar, Shen, Bahri, Pham,
  Rao, Yang, Ruder, and Metzler]{tay2021long}
Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang,
  L., Ruder, S., and Metzler, D.
\newblock Long range arena : A benchmark for efficient transformers.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=qVyeW-grC2k}.

\bibitem[Titsias \& Lázaro-Gredilla(2014)Titsias and Lázaro-Gredilla]{vae3}
Titsias, M. and Lázaro-Gredilla, M.
\newblock Doubly stochastic variational bayes for non-conjugate inference.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning}, volume~32, pp.\  1971--1979, 2014.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J{\'e}gou]{touvron21adeit}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J{\'e}gou,
  H.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  10347--10357. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/touvron21a.html}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5998--6008, 2017.

\bibitem[Veach \& Guibas(1995)Veach and Guibas]{veach1995optimally}
Veach, E. and Guibas, L.~J.
\newblock Optimally combining sampling techniques for monte carlo rendering.
\newblock In \emph{Proceedings of the 22nd annual conference on Computer
  graphics and interactive techniques}, pp.\  419--428, 1995.

\bibitem[Vyas et~al.(2020)Vyas, Katharopoulos, and Fleuret]{vyas2020fast}
Vyas, A., Katharopoulos, A., and Fleuret, F.
\newblock Fast transformers with clustered attention.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Wang, S., Li, B.~Z., Khabsa, M., Fang, H., and Ma, H.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Xie, Li, Fan, Song, Liang, Lu,
  Luo, and Shao]{pvt}
Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P.,
  and Shao, L.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock \emph{arXiv preprint arXiv:2102.12122}, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Xie, Li, Fan, Song, Liang, Lu,
  Luo, and Shao]{pvtv2}
Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P.,
  and Shao, L.
\newblock Pvtv2: Improved baselines with pyramid vision transformer.
\newblock \emph{arXiv preprint arXiv:2106.13797}, 2021{\natexlab{b}}.

\bibitem[Wu et~al.(2021)Wu, Xiao, Codella, Liu, Dai, Yuan, and Zhang]{cvt}
Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., and Zhang, L.
\newblock Cvt: Introducing convolutions to vision transformers.
\newblock \emph{arXiv preprint arXiv:2103.15808}, 2021.

\bibitem[Xiao et~al.(2021)Xiao, Singh, Mintun, Darrell, Doll{\'a}r, and
  Girshick]{xiao2021early}
Xiao, T., Singh, M., Mintun, E., Darrell, T., Doll{\'a}r, P., and Girshick, R.
\newblock Early convolutions help transformers see better.
\newblock \emph{arXiv preprint arXiv:2106.14881}, 2021.

\bibitem[Xiong et~al.(2021)Xiong, Zeng, Chakraborty, Tan, Fung, Li, and
  Singh]{xiong2021nystromformer}
Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., and Singh, V.
\newblock Nystr{\"o}mformer: A nystr{\"o}m-based algorithm for approximating
  self-attention.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  14138--14148, 2021.

\bibitem[Yang et~al.(2021)Yang, Li, Zhang, Dai, Xiao, Yuan, and Gao]{focal}
Yang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., and Gao, J.
\newblock Focal attention for long-range interactions in vision transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Yun et~al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]{cutmix}
Yun, S., Han, D., Oh, S.~J., Chun, S., Choe, J., and Yoo, Y.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  6023--6032, 2019.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, and Ahmed]{zaheer2020bigbird}
Zaheer, M., Guruganesh, G., Dubey, K.~A., Ainslie, J., Alberti, C., Ontanon,
  S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed, A.
\newblock Big bird: Transformers for longer sequences.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  17283--17297. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf}.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and Lopez-Paz]{mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\bibitem[Zhang et~al.(2021)Zhang, Dai, Yang, Xiao, Yuan, Zhang, and Gao]{vil}
Zhang, P., Dai, X., Yang, J., Xiao, B., Yuan, L., Zhang, L., and Gao, J.
\newblock Multi-scale vision longformer: A new vision transformer for
  high-resolution image encoding.
\newblock \emph{arXiv preprint arXiv:2103.15358}, 2021.

\bibitem[Zheng et~al.(2021)Zheng, Pan, and Kong]{ripple}
Zheng, L., Pan, H., and Kong, L.
\newblock Ripple attention for visual perception with sub-quadratic complexity.
\newblock \emph{arXiv preprint arXiv:2110.02453}, 2021.

\bibitem[Zhong et~al.(2020)Zhong, Zheng, Kang, Li, and Yang]{random-erasing}
Zhong, Z., Zheng, L., Kang, G., Li, S., and Yang, Y.
\newblock Random erasing data augmentation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  13001--13008, 2020.

\bibitem[Zhu et~al.(2021)Zhu, Ping, Xiao, Shoeybi, Goldstein, Anandkumar, and
  Catanzaro]{zhu2021long-short}
Zhu, C., Ping, W., Xiao, C., Shoeybi, M., Goldstein, T., Anandkumar, A., and
  Catanzaro, B.
\newblock Long-short transformer: Efficient transformers for language and
  vision.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\end{thebibliography}
