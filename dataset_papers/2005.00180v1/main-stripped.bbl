\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Advani \& Saxe(2017)Advani and Saxe]{advani2017high}
Advani, M.~S. and Saxe, A.~M.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock \emph{arXiv preprint arXiv:1710.03667}, 2017.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Liang]{allen2019learning}
Allen-Zhu, Z., Li, Y., and Liang, Y.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6155--6166, 2019.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Arora, S., Du, S.~S., Hu, W., Li, Z., and Wang, R.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock \emph{arXiv preprint arXiv:1901.08584}, 2019.

\bibitem[Barbier et~al.(2019)Barbier, Krzakala, Macris, Miolane, and
  Zdeborov{\'a}]{barbier2019optimal}
Barbier, J., Krzakala, F., Macris, N., Miolane, L., and Zdeborov{\'a}, L.
\newblock Optimal errors and phase transitions in high-dimensional generalized
  linear models.
\newblock \emph{Proc. National Academy of Sciences}, 116\penalty0
  (12):\penalty0 5451--5460, 2019.

\bibitem[Bartlett et~al.(2019)Bartlett, Long, Lugosi, and
  Tsigler]{bartlett2019benign}
Bartlett, P.~L., Long, P.~M., Lugosi, G., and Tsigler, A.
\newblock Benign overfitting in linear regression.
\newblock \emph{arXiv preprint arXiv:1906.11300}, 2019.

\bibitem[Bayati \& Montanari(2011)Bayati and Montanari]{BayatiM:11}
Bayati, M. and Montanari, A.
\newblock The dynamics of message passing on dense graphs, with applications to
  compressed sensing.
\newblock \emph{IEEE Trans. Inform. Theory}, 57\penalty0 (2):\penalty0
  764--785, February 2011.

\bibitem[Belkin et~al.(2018)Belkin, Ma, and Mandal]{belkin2018understand}
Belkin, M., Ma, S., and Mandal, S.
\newblock To understand deep learning we need to understand kernel learning.
\newblock \emph{arXiv preprint arXiv:1802.01396}, 2018.

\bibitem[Belkin et~al.(2019{\natexlab{a}})Belkin, Hsu, Ma, and
  Mandal]{belkin2019reconciling}
Belkin, M., Hsu, D., Ma, S., and Mandal, S.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock \emph{Proc. National Academy of Sciences}, 116\penalty0
  (32):\penalty0 15849--15854, 2019{\natexlab{a}}.

\bibitem[Belkin et~al.(2019{\natexlab{b}})Belkin, Hsu, and Xu]{belkin2019two}
Belkin, M., Hsu, D., and Xu, J.
\newblock Two models of double descent for weak features.
\newblock \emph{arXiv preprint arXiv:1903.07571}, 2019{\natexlab{b}}.

\bibitem[Cakmak et~al.(2014)Cakmak, Winther, and Fleury]{cakmak2014samp}
Cakmak, B., Winther, O., and Fleury, B.~H.
\newblock {S-AMP}: Approximate message passing for general matrix ensembles.
\newblock In \emph{Proc.\ IEEE ITW}, 2014.

\bibitem[Daniely(2017)]{daniely2017sgd}
Daniely, A.
\newblock Sgd learns the conjugate kernel class of the network.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2422--2430, 2017.

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and Singer]{daniely2016toward}
Daniely, A., Frostig, R., and Singer, Y.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In \emph{Advances In Neural Information Processing Systems}, pp.\
  2253--2261, 2016.

\bibitem[Deng et~al.(2019)Deng, Kammoun, and Thrampoulidis]{deng2019model}
Deng, Z., Kammoun, A., and Thrampoulidis, C.
\newblock A model of double descent for high-dimensional binary linear
  classification.
\newblock \emph{arXiv preprint arXiv:1911.05822}, 2019.

\bibitem[Donoho et~al.(2009)Donoho, Maleki, and Montanari]{donoho2009message}
Donoho, D.~L., Maleki, A., and Montanari, A.
\newblock Message-passing algorithms for compressed sensing.
\newblock \emph{Proc. National Academy of Sciences}, 106\penalty0
  (45):\penalty0 18914--18919, 2009.

\bibitem[Donoho et~al.(2010)Donoho, Maleki, and Montanari]{DonohoMM:10-ITW1}
Donoho, D.~L., Maleki, A., and Montanari, A.
\newblock Message passing algorithms for compressed sensing.
\newblock In \emph{Proc. Inform. Theory Workshop}, pp.\  1--5, 2010.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018.

\bibitem[Fletcher et~al.(2016)Fletcher, Sahraee-Ardakan, Rangan, and
  Schniter]{fletcher2016expectation}
Fletcher, A., Sahraee-Ardakan, M., Rangan, S., and Schniter, P.
\newblock Expectation consistent approximate inference: Generalizations and
  convergence.
\newblock In \emph{Proc. IEEE Int. Symp. Information Theory (ISIT)}, pp.\
  190--194. IEEE, 2016.

\bibitem[Fletcher et~al.(2018)Fletcher, Rangan, and
  Schniter]{fletcher2018inference}
Fletcher, A.~K., Rangan, S., and Schniter, P.
\newblock Inference in deep networks in high dimensions.
\newblock \emph{Proc. IEEE Int. Symp. Information Theory}, 2018.

\bibitem[Gabri{\'e} et~al.(2018)Gabri{\'e}, Manoel, Luneau, Barbier, Macris,
  Krzakala, and Zdeborov{\'a}]{gabrie2018entropy}
Gabri{\'e}, M., Manoel, A., Luneau, C., Barbier, J., Macris, N., Krzakala, F.,
  and Zdeborov{\'a}, L.
\newblock Entropy and mutual information in models of deep neural networks.
\newblock In \emph{Proc.\ NIPS}, 2018.

\bibitem[Gerbelot et~al.(2020)Gerbelot, Abbara, and
  Krzakala]{gerbelot2020asymptotic}
Gerbelot, C., Abbara, A., and Krzakala, F.
\newblock Asymptotic errors for convex penalized linear regression beyond
  gaussian matrices.
\newblock \emph{arXiv preprint arXiv:2002.04372}, 2020.

\bibitem[Givens et~al.(1984)Givens, Shortt, et~al.]{givens1984class}
Givens, C.~R., Shortt, R.~M., et~al.
\newblock A class of wasserstein metrics for probability distributions.
\newblock \emph{The Michigan Mathematical Journal}, 31\penalty0 (2):\penalty0
  231--240, 1984.

\bibitem[Hastie et~al.(2019)Hastie, Montanari, Rosset, and
  Tibshirani]{hastie2019surprises}
Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R.~J.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock \emph{arXiv preprint arXiv:1903.08560}, 2019.

\bibitem[He et~al.(2017)He, Wen, and Jin]{he2017generalized}
He, H., Wen, C.-K., and Jin, S.
\newblock Generalized expectation consistent signal recovery for nonlinear
  measurements.
\newblock In \emph{2017 IEEE International Symposium on Information Theory
  (ISIT)}, pp.\  2333--2337. IEEE, 2017.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  8571--8580, 2018.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Ma \& Ping(2017)Ma and Ping]{ma2017orthogonal}
Ma, J. and Ping, L.
\newblock Orthogonal amp.
\newblock \emph{IEEE Access}, 5:\penalty0 2020--2033, 2017.

\bibitem[Manoel et~al.(2018)Manoel, Krzakala, Varoquaux, Thirion, and
  Zdeborov{\'a}]{manoel2018approximate}
Manoel, A., Krzakala, F., Varoquaux, G., Thirion, B., and Zdeborov{\'a}, L.
\newblock Approximate message-passing for convex optimization with
  non-separable penalties.
\newblock \emph{arXiv preprint arXiv:1809.06304}, 2018.

\bibitem[Mei \& Montanari(2019)Mei and Montanari]{mei2019generalization}
Mei, S. and Montanari, A.
\newblock The generalization error of random features regression: Precise
  asymptotics and double descent curve.
\newblock \emph{arXiv preprint arXiv:1908.05355}, 2019.

\bibitem[Montanari et~al.(2019)Montanari, Ruan, Sohn, and
  Yan]{montanari2019generalization}
Montanari, A., Ruan, F., Sohn, Y., and Yan, J.
\newblock The generalization error of max-margin linear classifiers:
  High-dimensional asymptotics in the overparametrized regime.
\newblock \emph{arXiv preprint arXiv:1911.01544}, 2019.

\bibitem[Muthukumar et~al.(2019)Muthukumar, Vodrahalli, and
  Sahai]{muthukumar2019harmless}
Muthukumar, V., Vodrahalli, K., and Sahai, A.
\newblock Harmless interpolation of noisy data in regression.
\newblock In \emph{2019 IEEE International Symposium on Information Theory
  (ISIT)}, pp.\  2299--2303. IEEE, 2019.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Li, Bhojanapalli, LeCun, and
  Srebro]{neyshabur2018towards}
Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N.
\newblock Towards understanding the role of over-parametrization in
  generalization of neural networks.
\newblock \emph{arXiv preprint arXiv:1805.12076}, 2018.

\bibitem[Opper \& Winther(2005)Opper and Winther]{opper2005expectation}
Opper, M. and Winther, O.
\newblock Expectation consistent approximate inference.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (Dec):\penalty0 2177--2204, 2005.

\bibitem[{Pandit} et~al.(2019){Pandit}, {Sahraee}, {Rangan}, and
  {Fletcher}]{pandit2019asymptotics}
{Pandit}, P., {Sahraee}, M., {Rangan}, S., and {Fletcher}, A.~K.
\newblock Asymptotics of {MAP} inference in deep networks.
\newblock In \emph{Proc. IEEE Int. Symp. Information Theory}, pp.\  842--846,
  2019.

\bibitem[Pandit et~al.(2019)Pandit, Sahraee-Ardakan, Rangan, Schniter, and
  Fletcher]{pandit2019inference}
Pandit, P., Sahraee-Ardakan, M., Rangan, S., Schniter, P., and Fletcher, A.~K.
\newblock Inference with deep generative priors in high dimensions.
\newblock \emph{arXiv preprint arXiv:1911.03409}, 2019.

\bibitem[Rangan et~al.(2019)Rangan, Schniter, and Fletcher]{rangan2019vamp}
Rangan, S., Schniter, P., and Fletcher, A.~K.
\newblock Vector approximate message passing.
\newblock \emph{IEEE Trans. Information Theory}, 65\penalty0 (10):\penalty0
  6664--6684, 2019.

\bibitem[Reeves(2017)]{reeves2017additivity}
Reeves, G.
\newblock Additivity of information in multilayer networks via additive
  gaussian noise transforms.
\newblock In \emph{Proc. 55th Annual Allerton Conf. Communication, Control, and
  Computing (Allerton)}, pp.\  1064--1070. IEEE, 2017.

\bibitem[Salehi et~al.(2019)Salehi, Abbasi, and Hassibi]{salehi2019impact}
Salehi, F., Abbasi, E., and Hassibi, B.
\newblock The impact of regularization on high-dimensional logistic regression.
\newblock \emph{arXiv preprint arXiv:1906.03761}, 2019.

\bibitem[Tulino et~al.(2004)Tulino, Verd{\'u}, et~al.]{tulino2004random}
Tulino, A.~M., Verd{\'u}, S., et~al.
\newblock Random matrix theory and wireless communications.
\newblock \emph{Foundations and Trends{\textregistered} in Communications and
  Information Theory}, 1\penalty0 (1):\penalty0 1--182, 2004.

\bibitem[Villani(2008)]{villani2008optimal}
Villani, C.
\newblock \emph{Optimal transport: old and new}, volume 338.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\end{thebibliography}
