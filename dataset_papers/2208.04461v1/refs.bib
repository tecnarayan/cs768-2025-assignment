@article{allingham2021sparse,
  title={{Sparse MOEs meet efficient ensembles}},
  author={Allingham, James Urquhart and Wenzel, Florian and Mariet, Zelda E and Mustafa, Basil and Puigcerver, Joan and Houlsby, Neil and Jerfel, Ghassen and Fortuin, Vincent and Lakshminarayanan, Balaji and Snoek, Jasper},
  journal={arXiv preprint arXiv:2110.03360},
  year={2021}
}

@article{wen2022transformers,
  title={Transformers in Time Series: A Survey},
  author={Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
  journal={arXiv preprint arXiv:2202.07125},
  year={2022}
}

@article{cifar,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@article{jacobs1991adaptive,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}

@article{jacobs1995methods,
  title={Methods for combining experts' probability assessments},
  author={Jacobs, Robert A},
  journal={Neural computation},
  volume={7},
  number={5},
  pages={867--888},
  year={1995},
  publisher={MIT Press}
}

@article{kang1996statistical,
  title={Statistical mechanics of the mixture of experts},
  author={Kang, Kukjin and Oh, Jong-Hoon},
  journal={Advances in neural information processing systems},
  volume={9},
  year={1996}
}

@article{riquelme2021scaling,
  title={Scaling vision with sparse mixture of experts},
  author={Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, Andr{\'e} and Keysers, Daniel and Houlsby, Neil},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{valiant2011estimating,
  title={Estimating the unseen: an n/log (n)-sample estimator for entropy and support size, shown optimal via new CLTs},
  author={Valiant, Gregory and Valiant, Paul},
  booktitle={Proceedings of the forty-third annual ACM symposium on Theory of computing},
  pages={685--694},
  year={2011}
}

@inproceedings{datar2004locality,
  title={Locality-sensitive hashing scheme based on p-stable distributions},
  author={Datar, Mayur and Immorlica, Nicole and Indyk, Piotr and Mirrokni, Vahab S},
  booktitle={Proceedings of the twentieth annual symposium on Computational geometry},
  pages={253--262},
  year={2004}
}

@inproceedings{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1806.07572},
  year={2018}
}

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{rudelson2009smallest,
  title={Smallest singular value of a random rectangular matrix},
  author={Rudelson, Mark and Vershynin, Roman},
  journal={Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences},
  volume={62},
  number={12},
  pages={1707--1739},
  year={2009},
  publisher={Wiley Online Library}
}

@article{chen2005condition,
  title={Condition numbers of Gaussian random matrices},
  author={Chen, Zizhong and Dongarra, Jack J},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={27},
  number={3},
  pages={603--620},
  year={2005},
  publisher={SIAM}
}

@article{du2021glam,
  title={GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  journal={arXiv preprint arXiv:2112.06905},
  year={2021}
}

@inproceedings{timpl2021understanding,
  title={Understanding the effect of sparsity on neural networks robustness},
  author={Timpl, Lukas and Entezari, Rahim and Sedghi, Hanie and Neyshabur, Behnam and Saukh, Olga},
  booktitle={ICML 2021 Workshop Overparameterization: Pitfalls \& Opportunities},
  year={2021},
  organization={ICML workshop}
}

@article{jordan1994hierarchical,
  title={{Hierarchical mixtures of experts and the EM algorithm}},
  author={Jordan, Michael I and Jacobs, Robert A},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={181--214},
  year={1994},
  publisher={MIT Press}
}

@article{roller2021hash,
  title={Hash Layers For Large Sparse Models},
  author={Roller, Stephen and Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason},
  journal={arXiv preprint arXiv:2106.04426},
  year={2021}
}

@article{daniely2020learning,
  title={Learning parities with neural networks},
  author={Daniely, Amit and Malach, Eran},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{fedus2021switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}

@article{jaszczur2021sparse,
  title={Sparse is Enough in Scaling Transformers},
  author={Jaszczur, Sebastian and Chowdhery, Aakanksha and Mohiuddin, Afroz and Kaiser, {\L}ukasz and Gajewski, Wojciech and Michalewski, Henryk and Kanerva, Jonni},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{shazeer2017outrageously,
  author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc V. and Hinton, Geoffrey E. and Dean, Jeff},
  biburl = {https://www.bibsonomy.org/bibtex/27d67fba2e196e06dfab34d778a49a48f/dblp},
  booktitle = {ICLR (Poster)},
  ee = {https://openreview.net/forum?id=B1ckMDqlg},
  interhash = {cc05819387ba513c5f22be3772b2e4f6},
  intrahash = {7d67fba2e196e06dfab34d778a49a48f},
  keywords = {dblp},
  publisher = {OpenReview.net},
  timestamp = {2019-07-26T11:44:03.000+0200},
  title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.},
  url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2017.html#ShazeerMMDLHD17},
  year = 2017
}

@inproceedings{shazeer2018mesh-tensorflow,
 author = {Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and Sepassi, Ryan and Hechtman, Blake},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Mesh-TensorFlow: Deep Learning for Supercomputers},
 url = {https://proceedings.neurips.cc/paper/2018/file/3a37abdeefe1dab1b30f7c5c7e581b93-Paper.pdf},
 volume = {31},
 year = {2018}
}


@inproceedings{lepikhin2021gshard,
  author    = {Dmitry Lepikhin and
               HyoukJoong Lee and
               Yuanzhong Xu and
               Dehao Chen and
               Orhan Firat and
               Yanping Huang and
               Maxim Krikun and
               Noam Shazeer and
               Zhifeng Chen},
  title     = {GShard: Scaling Giant Models with Conditional Computation and Automatic
               Sharding},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=qrwe7XHTmYb},
  timestamp = {Wed, 23 Jun 2021 17:36:40 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LepikhinLXCFHKS21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hoefler2021sparsity,
  title={Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks},
  author={Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
  journal={arXiv preprint arXiv:2102.00554},
  year={2021}
}

@inproceedings{panigrahy2021sketch,
  title={Sketch based Memory for Neural Networks},
  author={Panigrahy, Rina and Wang, Xin and Zaheer, Manzil},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3169--3177},
  year={2021},
  organization={PMLR}
}

@inproceedings{lample2019large,
  title={Large memory layers with product keys},
  author={Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc'Aurelio and Denoyer, Ludovic and Jegou, Herve},
  booktitle={Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  pages={8548--8559},
  year={2019}
}

@inproceedings{andoni2018approximate,
  title={Approximate nearest neighbor search in high dimensions},
  author={Andoni, Alexandr and Indyk, Piotr and Razenshteyn, Ilya},
  booktitle={Proceedings of the International Congress of Mathematicians: Rio de Janeiro 2018},
  pages={3287--3318},
  year={2018},
  organization={World Scientific}
}

@book{har2011geometric,
  title={Geometric approximation algorithms},
  author={Har-Peled, Sariel},
  number={173},
  year={2011},
  publisher={American Mathematical Soc.}
}

@inproceedings{andoni2006near,
  title={Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions},
  author={Andoni, Alexandr and Indyk, Piotr},
  booktitle={2006 47th annual IEEE symposium on foundations of computer science (FOCS'06)},
  pages={459--468},
  year={2006},
  organization={IEEE}
}

@inproceedings{andoni2015practical,
  title={Practical and optimal LSH for angular distance},
  author={Andoni, Alexandr and Indyk, Piotr and Laarhoven, Thijs and Razenshteyn, Ilya and Schmidt, Ludwig},
  booktitle={Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 1},
  pages={1225--1233},
  year={2015}
}

@inproceedings{ghazi2019recursive,
  title={Recursive sketches for modular deep learning},
  author={Ghazi, Badih and Panigrahy, Rina and Wang, Joshua},
  booktitle={International Conference on Machine Learning},
  pages={2211--2220},
  year={2019},
  organization={PMLR}
}

@article{li2020train,
  title={Train large, then compress: Rethinking model size for efficient training and inference of transformers},
  author={Li, Zhuohan and Wallace, Eric and Shen, Sheng and Lin, Kevin and Keutzer, Kurt and Klein, Dan and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2002.11794},
  year={2020}
}

@article{kim2020fastformers,
  title={Fastformers: Highly efficient transformer models for natural language understanding},
  author={Kim, Young Jin and Awadalla, Hany Hassan},
  journal={arXiv preprint arXiv:2010.13382},
  year={2020}
}

@article{artetxe2021efficient,
  title={Efficient Large Scale Language Modeling with Mixtures of Experts},
  author={Artetxe, Mikel and Bhosale, Shruti and Goyal, Naman and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Lin, Xi Victoria and Du, Jingfei and Iyer, Srinivasan and Pasunuru, Ramakanth and others},
  journal={arXiv preprint arXiv:2112.10684},
  year={2021}
}

@article{nie2021dense,
  title={Dense-to-Sparse Gate for Mixture-of-Experts},
  author={Nie, Xiaonan and Cao, Shijie and Miao, Xupeng and Ma, Lingxiao and Xue, Jilong and Miao, Youshan and Yang, Zichao and Yang, Zhi and Cui, Bin},
  journal={arXiv preprint arXiv:2112.14397},
  year={2021}
}

@article{wang2021ernie,
  title={ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation},
  author={Wang, Shuohuan and Sun, Yu and Xiang, Yang and Wu, Zhihua and Ding, Siyu and Gong, Weibao and Feng, Shikun and Shang, Junyuan and Zhao, Yanbin and Pang, Chao and others},
  journal={arXiv preprint arXiv:2112.12731},
  year={2021}
}

@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?��},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021}
}

@article{khan2021transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={arXiv preprint arXiv:2101.01169},
  year={2021}
}

@article{wu2020visual,
  title={Visual transformers: Token-based image representation and processing for computer vision},
  author={Wu, Bichen and Xu, Chenfeng and Dai, Xiaoliang and Wan, Alvin and Zhang, Peizhao and Yan, Zhicheng and Tomizuka, Masayoshi and Gonzalez, Joseph and Keutzer, Kurt and Vajda, Peter},
  journal={arXiv preprint arXiv:2006.03677},
  year={2020}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{chen2020mongoose,
  title={MONGOOSE: A learnable LSH framework for efficient neural network training},
  author={Chen, Beidi and Liu, Zichang and Peng, Binghui and Xu, Zhaozhuo and Li, Jonathan Lingjie and Dao, Tri and Song, Zhao and Shrivastava, Anshumali and Re, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{charikar2002similarity,
  title={Similarity estimation techniques from rounding algorithms},
  author={Charikar, Moses S},
  booktitle={Proceedings of the thiry-fourth annual ACM symposium on Theory of computing},
  pages={380--388},
  year={2002}
}

@article{chandar2016hierarchical,
  title={Hierarchical memory networks},
  author={Chandar, Sarath and Ahn, Sungjin and Larochelle, Hugo and Vincent, Pascal and Tesauro, Gerald and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1605.07427},
  year={2016}
}

@article{rae2016scaling,
  title={Scaling memory-augmented neural networks with sparse reads and writes},
  author={Rae, Jack W and Hunt, Jonathan J and Harley, Tim and Danihelka, Ivo and Senior, Andrew and Wayne, Greg and Graves, Alex and Lillicrap, Timothy P},
  journal={arXiv preprint arXiv:1610.09027},
  year={2016}
}

@inproceedings{chen2015compressing,
  title={Compressing neural networks with the hashing trick},
  author={Chen, Wenlin and Wilson, James and Tyree, Stephen and Weinberger, Kilian and Chen, Yixin},
  booktitle={International conference on machine learning},
  pages={2285--2294},
  year={2015},
  organization={PMLR}
}

@online{rmsprop,
    author = "Geoffrey Hinton",
    title = {{Lecture Notes, Toronto, Hinton, 2012, \url{http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}}},
    url  = "http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf",
    addendum = "(accessed: 01.27.2022)",
    keywords = "rmsprop"
}