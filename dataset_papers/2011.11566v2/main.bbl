\begin{thebibliography}{37}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l and
  Szepesv{\'a}ri}]{abbasi2011improved}
\textsc{Abbasi-Yadkori, Y.}, \textsc{P{\'a}l, D.} and \textsc{Szepesv{\'a}ri,
  C.} (2011).
\newblock Improved algorithms for linear stochastic bandits.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Auer and Ortner(2007)}]{auer2007logarithmic}
\textsc{Auer, P.} and \textsc{Ortner, R.} (2007).
\newblock Logarithmic online regret bounds for undiscounted reinforcement
  learning.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang and
  Yang}]{ayoub2020model}
\textsc{Ayoub, A.}, \textsc{Jia, Z.}, \textsc{Szepesvari, C.}, \textsc{Wang,
  M.} and \textsc{Yang, L.~F.} (2020).
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock \textit{arXiv preprint arXiv:2006.01107} .

\bibitem[{Azar et~al.(2017)Azar, Osband and Munos}]{azar2017minimax}
\textsc{Azar, M.~G.}, \textsc{Osband, I.} and \textsc{Munos, R.} (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \textit{Proceedings of the 34th International Conference on
  Machine Learning-Volume 70}. JMLR. org.

\bibitem[{Bartlett et~al.(2005)Bartlett, Bousquet, Mendelson
  et~al.}]{bartlett2005local}
\textsc{Bartlett, P.~L.}, \textsc{Bousquet, O.}, \textsc{Mendelson, S.}
  \textsc{et~al.} (2005).
\newblock Local rademacher complexities.
\newblock \textit{The Annals of Statistics} \textbf{33} 1497--1537.

\bibitem[{Besson and Kaufmann(2018)}]{besson2018doubling}
\textsc{Besson, L.} and \textsc{Kaufmann, E.} (2018).
\newblock What doubling tricks can and can't do for multi-armed bandits.
\newblock \textit{arXiv preprint arXiv:1803.06971} .

\bibitem[{Bubeck and Cesa-Bianchi(2012)}]{bubeck2012regret}
\textsc{Bubeck, S.} and \textsc{Cesa-Bianchi, N.} (2012).
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit
  problems.
\newblock \textit{Machine Learning} \textbf{5} 1--122.

\bibitem[{Cai et~al.(2019)Cai, Yang, Jin and Wang}]{cai2019provably}
\textsc{Cai, Q.}, \textsc{Yang, Z.}, \textsc{Jin, C.} and \textsc{Wang, Z.}
  (2019).
\newblock Provably efficient exploration in policy optimization.
\newblock \textit{arXiv preprint arXiv:1912.05830} .

\bibitem[{Cesa-Bianchi and Lugosi(2006)}]{cesa2006prediction}
\textsc{Cesa-Bianchi, N.} and \textsc{Lugosi, G.} (2006).
\newblock \textit{Prediction, learning, and games}.
\newblock Cambridge university press.

\bibitem[{Dani et~al.(2008)Dani, Hayes and Kakade}]{dani2008stochastic}
\textsc{Dani, V.}, \textsc{Hayes, T.~P.} and \textsc{Kakade, S.~M.} (2008).
\newblock Stochastic linear optimization under bandit feedback.
\newblock In \textit{Conference on Learning Theory}.

\bibitem[{Dann and Brunskill(2015)}]{dann2015sample}
\textsc{Dann, C.} and \textsc{Brunskill, E.} (2015).
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Du et~al.(2020)Du, Lee, Mahajan and Wang}]{du2020agnostic}
\textsc{Du, S.~S.}, \textsc{Lee, J.~D.}, \textsc{Mahajan, G.} and \textsc{Wang,
  R.} (2020).
\newblock Agnostic q-learning with function approximation in deterministic
  systems: Tight bounds on approximation error and sample complexity.
\newblock \textit{arXiv preprint arXiv:2002.07125} .

\bibitem[{Du et~al.(2019)Du, Luo, Wang and Zhang}]{du2019provably}
\textsc{Du, S.~S.}, \textsc{Luo, Y.}, \textsc{Wang, R.} and \textsc{Zhang, H.}
  (2019).
\newblock Provably efficient q-learning with function approximation via
  distribution shift error checking oracle.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Jaksch et~al.(2010)Jaksch, Ortner and Auer}]{jaksch2010near}
\textsc{Jaksch, T.}, \textsc{Ortner, R.} and \textsc{Auer, P.} (2010).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \textit{Journal of Machine Learning Research} \textbf{11} 1563--1600.

\bibitem[{Jia et~al.(2020)Jia, Yang, Szepesvari and Wang}]{jia2020model}
\textsc{Jia, Z.}, \textsc{Yang, L.}, \textsc{Szepesvari, C.} and \textsc{Wang,
  M.} (2020).
\newblock Model-based reinforcement learning with value-targeted regression .

\bibitem[{Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford and
  Schapire}]{jiang2017contextual}
\textsc{Jiang, N.}, \textsc{Krishnamurthy, A.}, \textsc{Agarwal, A.},
  \textsc{Langford, J.} and \textsc{Schapire, R.~E.} (2017).
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \textit{Proceedings of the 34th International Conference on
  Machine Learning-Volume 70}. JMLR. org.

\bibitem[{Jin et~al.(2018)Jin, Allen-Zhu, Bubeck and Jordan}]{jin2018q}
\textsc{Jin, C.}, \textsc{Allen-Zhu, Z.}, \textsc{Bubeck, S.} and
  \textsc{Jordan, M.~I.} (2018).
\newblock Is q-learning provably efficient?
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Jin et~al.(2020)Jin, Yang, Wang and Jordan}]{jin2019provably}
\textsc{Jin, C.}, \textsc{Yang, Z.}, \textsc{Wang, Z.} and \textsc{Jordan,
  M.~I.} (2020).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \textit{Conference on Learning Theory}.

\bibitem[{Lattimore and Szepesv{\'a}ri(2018)}]{lattimore2018bandit}
\textsc{Lattimore, T.} and \textsc{Szepesv{\'a}ri, C.} (2018).
\newblock Bandit algorithms.
\newblock \textit{preprint}  28.

\bibitem[{Li et~al.(2010)Li, Chu, Langford and Schapire}]{li2010contextual}
\textsc{Li, L.}, \textsc{Chu, W.}, \textsc{Langford, J.} and \textsc{Schapire,
  R.~E.} (2010).
\newblock A contextual-bandit approach to personalized news article
  recommendation.
\newblock In \textit{Proceedings of the 19th international conference on World
  wide web}.

\bibitem[{Mou et~al.(2020)Mou, Wen and Chen}]{mou2020sample}
\textsc{Mou, W.}, \textsc{Wen, Z.} and \textsc{Chen, X.} (2020).
\newblock On the sample complexity of reinforcement learning with policy space
  generalization.
\newblock \textit{arXiv preprint arXiv:2008.07353} .

\bibitem[{Ok et~al.(2018)Ok, Proutiere and Tranos}]{ok2018exploration}
\textsc{Ok, J.}, \textsc{Proutiere, A.} and \textsc{Tranos, D.} (2018).
\newblock Exploration in structured reinforcement learning.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Osband and Van~Roy(2016)}]{osband2016lower}
\textsc{Osband, I.} and \textsc{Van~Roy, B.} (2016).
\newblock On lower bounds for regret in reinforcement learning.
\newblock \textit{arXiv preprint arXiv:1608.02732} .

\bibitem[{Simchowitz and Jamieson(2019)}]{simchowitz2019non}
\textsc{Simchowitz, M.} and \textsc{Jamieson, K.~G.} (2019).
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Slivkins et~al.(2019)}]{slivkins2019introduction}
\textsc{Slivkins, A.} \textsc{et~al.} (2019).
\newblock Introduction to multi-armed bandits.
\newblock \textit{Foundations and Trends{\textregistered} in Machine Learning}
  \textbf{12} 1--286.

\bibitem[{Strehl et~al.(2006)Strehl, Li, Wiewiora, Langford and
  Littman}]{strehl2006pac}
\textsc{Strehl, A.~L.}, \textsc{Li, L.}, \textsc{Wiewiora, E.},
  \textsc{Langford, J.} and \textsc{Littman, M.~L.} (2006).
\newblock Pac model-free reinforcement learning.
\newblock In \textit{Proceedings of the 23rd international conference on
  Machine learning}. ACM.

\bibitem[{Tewari and Bartlett(2008)}]{tewari2008optimistic}
\textsc{Tewari, A.} and \textsc{Bartlett, P.~L.} (2008).
\newblock Optimistic linear programming gives logarithmic regret for
  irreducible mdps.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Wang et~al.(2019{\natexlab{a}})Wang, Dong, Chen and Wang}]{dong2019q}
\textsc{Wang, Y.}, \textsc{Dong, K.}, \textsc{Chen, X.} and \textsc{Wang, L.}
  (2019{\natexlab{a}}).
\newblock Q-learning with ucb exploration is sample efficient for
  infinite-horizon mdp.
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Wang et~al.(2019{\natexlab{b}})Wang, Wang, Du and
  Krishnamurthy}]{wang2019optimism}
\textsc{Wang, Y.}, \textsc{Wang, R.}, \textsc{Du, S.~S.} and
  \textsc{Krishnamurthy, A.} (2019{\natexlab{b}}).
\newblock Optimism in reinforcement learning with generalized linear function
  approximation.
\newblock \textit{arXiv preprint arXiv:1912.04136} .

\bibitem[{Watkins(1989)}]{watkins1989learning}
\textsc{Watkins, C. J. C.~H.} (1989).
\newblock \textit{Learning from delayed rewards.}
\newblock Ph.D. thesis, University of Cambridge.

\bibitem[{Yang et~al.(2020)Yang, Yang and Du}]{yang2020q}
\textsc{Yang, K.}, \textsc{Yang, L.~F.} and \textsc{Du, S.~S.} (2020).
\newblock $ q $-learning with logarithmic regret.
\newblock \textit{arXiv preprint arXiv:2006.09118} .

\bibitem[{Yang and Wang(2019)}]{yang2019sample}
\textsc{Yang, L.} and \textsc{Wang, M.} (2019).
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Zanette and Brunskill(2019)}]{zanette2019tighter}
\textsc{Zanette, A.} and \textsc{Brunskill, E.} (2019).
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Zanette et~al.(2020)Zanette, Lazaric, Kochenderfer and
  Brunskill}]{zanette2020learning}
\textsc{Zanette, A.}, \textsc{Lazaric, A.}, \textsc{Kochenderfer, M.} and
  \textsc{Brunskill, E.} (2020).
\newblock Learning near optimal policies with low inherent bellman error.
\newblock \textit{arXiv preprint arXiv:2003.00153} .

\bibitem[{Zhang et~al.(2020)Zhang, Zhou and Ji}]{zhang2020almost}
\textsc{Zhang, Z.}, \textsc{Zhou, Y.} and \textsc{Ji, X.} (2020).
\newblock Almost optimal model-free reinforcement learning via
  reference-advantage decomposition.
\newblock \textit{arXiv preprint arXiv:2004.10019} .

\bibitem[{Zhou et~al.(2020{\natexlab{a}})Zhou, Gu and
  Szepesvari}]{zhou2020nearly}
\textsc{Zhou, D.}, \textsc{Gu, Q.} and \textsc{Szepesvari, C.}
  (2020{\natexlab{a}}).
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock \textit{arXiv preprint arXiv:2012.08507} .

\bibitem[{Zhou et~al.(2020{\natexlab{b}})Zhou, He and Gu}]{zhou2020provably}
\textsc{Zhou, D.}, \textsc{He, J.} and \textsc{Gu, Q.} (2020{\natexlab{b}}).
\newblock Provably efficient reinforcement learning for discounted mdps with
  feature mapping.
\newblock \textit{arXiv preprint arXiv:2006.13165} .

\end{thebibliography}
