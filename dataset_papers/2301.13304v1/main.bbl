\begin{thebibliography}{}

\bibitem[Ahn et~al., 2019]{ahn2019variational}
Ahn, S., Hu, S.~X., Damianou, A., Lawrence, N.~D., and Dai, Z. (2019).
\newblock Variational information distillation for knowledge transfer.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 9163--9171.

\bibitem[Alain and Bengio, 2016]{alain2016understanding}
Alain, G. and Bengio, Y. (2016).
\newblock Understanding intermediate layers using linear classifier probes.
\newblock {\em arXiv preprint arXiv:1610.01644}.

\bibitem[Baykal et~al., 2022]{baykal2022robust}
Baykal, C., Trinh, K., Iliopoulos, F., Menghani, G., and Vee, E. (2022).
\newblock Robust active distillation.
\newblock {\em arXiv preprint arXiv:2210.01213}.

\bibitem[Beyer et~al., 2022]{beyer2022knowledge}
Beyer, L., Zhai, X., Royer, A., Markeeva, L., Anil, R., and Kolesnikov, A.
  (2022).
\newblock Knowledge distillation: A good teacher is patient and consistent.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10925--10934.

\bibitem[Bossard et~al., 2014]{bossard14}
Bossard, L., Guillaumin, M., and Van~Gool, L. (2014).
\newblock Food-101 -- mining discriminative components with random forests.
\newblock In {\em European Conference on Computer Vision}.

\bibitem[Chen et~al., 2019]{chen2019understanding}
Chen, P., Liao, B.~B., Chen, G., and Zhang, S. (2019).
\newblock Understanding and utilizing deep neural networks trained with noisy
  labels.
\newblock In {\em International Conference on Machine Learning}, pages
  1062--1070. PMLR.

\bibitem[Chen et~al., 2020]{chen2020big}
Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G.~E. (2020).
\newblock Big self-supervised models are strong semi-supervised learners.
\newblock {\em Advances in neural information processing systems},
  33:22243--22255.

\bibitem[Cheng et~al., 2020]{cheng2020explaining}
Cheng, X., Rao, Z., Chen, Y., and Zhang, Q. (2020).
\newblock Explaining knowledge distillation by quantifying the knowledge.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 12925--12935.

\bibitem[Dong et~al., 2019]{dong2019distillation}
Dong, B., Hou, J., Lu, Y., and Zhang, Z. (2019).
\newblock Distillation $\approx$ early stopping? harvesting dark knowledge
  utilizing anisotropic information retrieval for overparameterized neural
  network.
\newblock {\em arXiv preprint arXiv:1910.01255}.

\bibitem[Furlanello et~al., 2018]{furlanello2018born}
Furlanello, T., Lipton, Z., Tschannen, M., Itti, L., and Anandkumar, A. (2018).
\newblock Born again neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1607--1616. PMLR.

\bibitem[Gou et~al., 2021]{gou2021knowledge}
Gou, J., Yu, B., Maybank, S.~J., and Tao, D. (2021).
\newblock Knowledge distillation: A survey.
\newblock {\em International Journal of Computer Vision}, 129(6):1789--1819.

\bibitem[Griffin et~al., 2007]{griffin2007caltech}
Griffin, G., Holub, A., and Perona, P. (2007).
\newblock Caltech-256 object category dataset.

\bibitem[Hendrycks et~al., 2018]{hendrycks2018using}
Hendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K. (2018).
\newblock Using trusted data to train deep networks on labels corrupted by
  severe noise.
\newblock {\em Advances in neural information processing systems}, 31.

\bibitem[Hinton et~al., 2015]{hinton2015distilling}
Hinton, G., Vinyals, O., Dean, J., et~al. (2015).
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2(7).

\bibitem[Ji and Zhu, 2020]{ji2020knowledge}
Ji, G. and Zhu, Z. (2020).
\newblock Knowledge distillation in wide neural networks: Risk bound, data
  efficiency and imperfect teacher.
\newblock {\em Advances in Neural Information Processing Systems},
  33:20823--20833.

\bibitem[Kakade et~al., 2008]{kakade2008complexity}
Kakade, S.~M., Sridharan, K., and Tewari, A. (2008).
\newblock On the complexity of linear prediction: Risk bounds, margin bounds,
  and regularization.
\newblock {\em Advances in neural information processing systems}, 21.

\bibitem[Kaplun et~al., 2022]{kaplun2022knowledge}
Kaplun, G., Malach, E., Nakkiran, P., and Shalev-Shwartz, S. (2022).
\newblock Knowledge distillation: Bad models can be good role models.
\newblock {\em arXiv preprint arXiv:2203.14649}.

\bibitem[Krause et~al., 2013]{KrauseStarkDengFei-Fei_3DRR2013}
Krause, J., Stark, M., Deng, J., and Fei-Fei, L. (2013).
\newblock 3d object representations for fine-grained categorization.
\newblock In {\em 4th International IEEE Workshop on 3D Representation and
  Recognition (3dRR-13)}, Sydney, Australia.

\bibitem[Kumar et~al., 2022]{kumar2022fine}
Kumar, A., Raghunathan, A., Jones, R., Ma, T., and Liang, P. (2022).
\newblock Fine-tuning can distort pretrained features and underperform
  out-of-distribution.
\newblock {\em arXiv preprint arXiv:2202.10054}.

\bibitem[Li et~al., 2021]{li2021align}
Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., and Hoi, S. C.~H.
  (2021).
\newblock Align before fuse: Vision and language representation learning with
  momentum distillation.
\newblock {\em Advances in neural information processing systems},
  34:9694--9705.

\bibitem[Li et~al., 2017]{li2017learning}
Li, Y., Yang, J., Song, Y., Cao, L., Luo, J., and Li, L.-J. (2017).
\newblock Learning from noisy labels with distillation.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1910--1918.

\bibitem[Lopez-Paz et~al., 2015]{lopez2015unifying}
Lopez-Paz, D., Bottou, L., Sch{\"o}lkopf, B., and Vapnik, V. (2015).
\newblock Unifying distillation and privileged information.
\newblock {\em arXiv preprint arXiv:1511.03643}.

\bibitem[Menon et~al., 2021]{menon2021statistical}
Menon, A.~K., Rawat, A.~S., Reddi, S., Kim, S., and Kumar, S. (2021).
\newblock A statistical perspective on distillation.
\newblock In {\em International Conference on Machine Learning}, pages
  7632--7642. PMLR.

\bibitem[Mobahi et~al., 2020]{mobahi2020self}
Mobahi, H., Farajtabar, M., and Bartlett, P. (2020).
\newblock Self-distillation amplifies regularization in hilbert space.
\newblock {\em Advances in Neural Information Processing Systems},
  33:3351--3361.

\bibitem[Nilsback and Zisserman, 2008]{nilsback2008automated}
Nilsback, M.-E. and Zisserman, A. (2008).
\newblock Automated flower classification over a large number of classes.
\newblock In {\em 2008 Sixth Indian Conference on Computer Vision, Graphics \&
  Image Processing}, pages 722--729. IEEE.

\bibitem[Pham et~al., 2021]{pham2021meta}
Pham, H., Dai, Z., Xie, Q., and Le, Q.~V. (2021).
\newblock Meta pseudo labels.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 11557--11568.

\bibitem[Pham et~al., 2022]{pham2022revisiting}
Pham, M., Cho, M., Joshi, A., and Hegde, C. (2022).
\newblock Revisiting self-distillation.
\newblock {\em arXiv preprint arXiv:2206.08491}.

\bibitem[Phuong and Lampert, 2019]{phuong2019towards}
Phuong, M. and Lampert, C. (2019).
\newblock Towards understanding knowledge distillation.
\newblock In {\em International Conference on Machine Learning}, pages
  5142--5151. PMLR.

\bibitem[Sarfraz et~al., 2021]{sarfraz2021knowledge}
Sarfraz, F., Arani, E., and Zonooz, B. (2021).
\newblock Knowledge distillation beyond model compression.
\newblock In {\em 2020 25th International Conference on Pattern Recognition
  (ICPR)}, pages 6136--6143. IEEE.

\bibitem[Stanton et~al., 2021]{stanton2021does}
Stanton, S., Izmailov, P., Kirichenko, P., Alemi, A.~A., and Wilson, A.~G.
  (2021).
\newblock Does knowledge distillation really work?
\newblock {\em Advances in Neural Information Processing Systems},
  34:6906--6919.

\bibitem[Sun et~al., 2019]{sun2019patient}
Sun, S., Cheng, Y., Gan, Z., and Liu, J. (2019).
\newblock Patient knowledge distillation for bert model compression.
\newblock {\em arXiv preprint arXiv:1908.09355}.

\bibitem[Xie et~al., 2020]{xie2020self}
Xie, Q., Luong, M.-T., Hovy, E., and Le, Q.~V. (2020).
\newblock Self-training with noisy student improves imagenet classification.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 10687--10698.

\end{thebibliography}
