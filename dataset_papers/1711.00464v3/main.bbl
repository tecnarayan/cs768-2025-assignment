\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[{Achille} \& {Soatto}(2016){Achille} and {Soatto}]{infodropout}
{Achille}, A. and {Soatto}, S.
\newblock {Information Dropout: Learning Optimal Representations Through Noisy
  Computation}.
\newblock In \emph{Information Control and Learning}, September 2016.
\newblock URL \url{http://arxiv.org/abs/1611.01353}.

\bibitem[{Achille} \& {Soatto}(2017){Achille} and {Soatto}]{emergence}
{Achille}, A. and {Soatto}, S.
\newblock {Emergence of Invariance and Disentangling in Deep Representations}.
\newblock \emph{Proceedings of the ICML Workshop on Principled Approaches to
  Deep Learning}, 2017.

\bibitem[Agakov(2006)]{agakov2006variational}
Agakov, Felix~Vsevolodovich.
\newblock \emph{Variational Information Maximization in Stochastic
  Environments}.
\newblock PhD thesis, University of Edinburgh, 2006.

\bibitem[Alemi et~al.(2017)Alemi, Fischer, Dillon, and Murphy]{vib}
Alemi, Alexander~A, Fischer, Ian, Dillon, Joshua~V, and Murphy, Kevin.
\newblock {Deep Variational Information Bottleneck}.
\newblock In \emph{ICLR}, 2017.

\bibitem[{Ball{\'e}} et~al.(2017){Ball{\'e}}, {Laparra}, and
  {Simoncelli}]{endtoendcompression}
{Ball{\'e}}, J., {Laparra}, V., and {Simoncelli}, E.~P.
\newblock {End-to-end Optimized Image Compression}.
\newblock In \emph{ICLR}, 2017.

\bibitem[Barber \& Agakov(2003)Barber and Agakov]{agakov2003}
Barber, David and Agakov, Felix~V.
\newblock Information maximization in noisy channels : A variational approach.
\newblock In \emph{NIPS}. 2003.

\bibitem[Bell \& Sejnowski(1995)Bell and Sejnowski]{blindsource}
Bell, Anthony~J and Sejnowski, Terrence~J.
\newblock An information-maximization approach to blind separation and blind
  deconvolution.
\newblock \emph{Neural computation}, 7\penalty0 (6):\penalty0 1129--1159, 1995.

\bibitem[Bowman et~al.(2016)Bowman, Vilnis, Vinyals, Dai, Jozefowicz, and
  Bengio]{bowman2015generating}
Bowman, Samuel~R, Vilnis, Luke, Vinyals, Oriol, Dai, Andrew~M, Jozefowicz,
  Rafal, and Bengio, Samy.
\newblock Generating sentences from a continuous space.
\newblock \emph{CoNLL}, 2016.

\bibitem[{Chen} et~al.(2017){Chen}, {Kingma}, {Salimans}, {Duan}, {Dhariwal},
  {Schulman}, {Sutskever}, and {Abbeel}]{vlae}
{Chen}, X., {Kingma}, D.~P., {Salimans}, T., {Duan}, Y., {Dhariwal}, P.,
  {Schulman}, J., {Sutskever}, I., and {Abbeel}, P.
\newblock {Variational Lossy Autoencoder}.
\newblock In \emph{ICLR}, 2017.

\bibitem[Chen et~al.(2016)Chen, Duan, Houthooft, Schulman, Sutskever, and
  Abbeel]{infogan}
Chen, Xi, Duan, Yan, Houthooft, Rein, Schulman, John, Sutskever, Ilya, and
  Abbeel, Pieter.
\newblock Infogan: Interpretable representation learning by information
  maximizing generative adversarial nets.
\newblock \emph{arXiv preprint 1606.03657}, 2016.

\bibitem[Germain et~al.(2015)Germain, Gregor, Murray, and Larochelle]{made}
Germain, Mathieu, Gregor, Karol, Murray, Iain, and Larochelle, Hugo.
\newblock Made: Masked autoencoder for distribution estimation.
\newblock In \emph{ICML}, 2015.

\bibitem[Gregor et~al.(2016)Gregor, Besse, Rezende, Danihelka, and
  Wierstra]{conceptualcompression}
Gregor, Karol, Besse, Frederic, Rezende, Danilo~Jimenez, Danihelka, Ivo, and
  Wierstra, Daan.
\newblock Towards conceptual compression.
\newblock In \emph{Advances In Neural Information Processing Systems}, pp.\
  3549--3557, 2016.

\bibitem[Ha \& Eck(2018)Ha and Eck]{ha2018a}
Ha, David and Eck, Doug.
\newblock A neural representation of sketch drawings.
\newblock \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=Hy6GHpkCW}.

\bibitem[Higgins et~al.(2017)Higgins, Matthey, Pal, Burgess, Glorot, Botvinick,
  Mohamed, and Lerchner]{betavae}
Higgins, Irina, Matthey, Loic, Pal, Arka, Burgess, Christopher, Glorot, Xavier,
  Botvinick, Matthew, Mohamed, Shakir, and Lerchner, Alexander.
\newblock {$\beta$-VAE: Learning Basic Visual Concepts with a Constrained
  Variational Framework}.
\newblock In \emph{ICLR}, 2017.

\bibitem[Hinton \& Van~Camp(1993)Hinton and Van~Camp]{hinton1993keeping}
Hinton, Geoffrey~E and Van~Camp, Drew.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In \emph{Proc. of the Workshop on Computational Learning Theory},
  1993.

\bibitem[Hoffman \& Johnson(2016)Hoffman and Johnson]{surgery}
Hoffman, Matthew~D and Johnson, Matthew~J.
\newblock Elbo surgery: yet another way to carve up the variational evidence
  lower bound.
\newblock In \emph{{NIPS} Workshop in Advances in Approximate Bayesian
  Inference}, 2016.

\bibitem[Husz\'ar(2017)]{huszar}
Husz\'ar, Ferenc.
\newblock Is maximum likelihood useful for representation learning?, 2017.
\newblock URL
  \url{http://www.inference.vc/maximum-likelihood-for-representation-learning-2/}.

\bibitem[{Johnston} et~al.(2017){Johnston}, {Vincent}, {Minnen}, {Covell},
  {Singh}, {Chinen}, {Hwang}, {Shor}, and {Toderici}]{johnston2017improved}
{Johnston}, N., {Vincent}, D., {Minnen}, D., {Covell}, M., {Singh}, S.,
  {Chinen}, T., {Hwang}, S.~J., {Shor}, J., and {Toderici}, G.
\newblock {Improved Lossy Image Compression with Priming and Spatially Adaptive
  Bit Rates for Recurrent Networks}.
\newblock \emph{ArXiv e-prints}, 2017.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{vae}
Kingma, Diederik~P and Welling, Max.
\newblock {Auto-encoding variational Bayes}.
\newblock In \emph{ICLR}, 2014.

\bibitem[Kingma et~al.(2016)Kingma, Salimans, Jozefowicz, Chen, Sutskever, and
  Welling]{iaf}
Kingma, Diederik~P, Salimans, Tim, Jozefowicz, Rafal, Chen, Xi, Sutskever,
  Ilya, and Welling, Max.
\newblock Improved variational inference with inverse autoregressive flow.
\newblock In \emph{NIPS}. 2016.

\bibitem[Lake et~al.(2015)Lake, Salakhutdinov, and Tenenbaum]{lake2015human}
Lake, Brenden~M., Salakhutdinov, Ruslan, and Tenenbaum, Joshua~B.
\newblock Human-level concept learning through probabilistic program induction.
\newblock \emph{Science}, 350\penalty0 (6266):\penalty0 1332--1338, 2015.

\bibitem[Larochelle \& Murray(2011)Larochelle and Murray]{binarystaticmnist}
Larochelle, Hugo and Murray, Iain.
\newblock The neural autoregressive distribution estimator.
\newblock In \emph{AI/Statistics}, 2011.

\bibitem[Makhzani et~al.(2016)Makhzani, Shlens, Jaitly, and
  Goodfellow]{makhzani2015adversarial}
Makhzani, Alireza, Shlens, Jonathon, Jaitly, Navdeep, and Goodfellow, Ian.
\newblock Adversarial autoencoders.
\newblock In \emph{ICLR}, 2016.

\bibitem[Papamakarios et~al.(2017)Papamakarios, Murray, and Pavlakou]{maf}
Papamakarios, George, Murray, Iain, and Pavlakou, Theo.
\newblock Masked autoregressive flow for density estimation.
\newblock In \emph{NIPS}. 2017.

\bibitem[Phuong et~al.(2018)Phuong, Welling, Kushman, Tomioka, and
  Nowozin]{mutualae}
Phuong, Mary, Welling, Max, Kushman, Nate, Tomioka, Ryota, and Nowozin,
  Sebastian.
\newblock The mutual autoencoder: Controlling information in latent code
  representations, 2018.
\newblock URL \url{https://openreview.net/forum?id=HkbmWqxCZ}.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and Wierstra]{Rezende14}
Rezende, Danilo~Jimenez, Mohamed, Shakir, and Wierstra, Daan.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{ICML}, 2014.

\bibitem[Salimans et~al.(2017)Salimans, Karpathy, Chen, and Kingma]{pxpp}
Salimans, Tim, Karpathy, Andrej, Chen, Xi, and Kingma, Diederik~P.
\newblock Pixelcnn++: Improving the pixelcnn with discretized logistic mixture
  likelihood and other modifications.
\newblock In \emph{ICLR}, 2017.

\bibitem[Shamir et~al.(2010)Shamir, Sabato, and Tishby]{generalization}
Shamir, Ohad, Sabato, Sivan, and Tishby, Naftali.
\newblock Learning and generalization with the information bottleneck.
\newblock \emph{Theoretical Computer Science}, 411\penalty0 (29):\penalty0 2696
  -- 2711, 2010.

\bibitem[Slonim et~al.(2005)Slonim, Atwal, Tka{\v{c}}ik, and Bialek]{unsupib}
Slonim, Noam, Atwal, Gurinder~Singh, Tka{\v{c}}ik, Ga{\v{s}}per, and Bialek,
  William.
\newblock Information-based clustering.
\newblock \emph{PNAS}, 102\penalty0 (51):\penalty0 18297--18302, 2005.

\bibitem[Tishby \& Zaslavsky(2015)Tishby and Zaslavsky]{deeplearningib}
Tishby, N. and Zaslavsky, N.
\newblock Deep learning and the information bottleneck principle.
\newblock In \emph{2015 IEEE Information Theory Workshop (ITW)}, 2015.

\bibitem[Tishby et~al.(1999)Tishby, Pereira, and Biale]{Tishby99}
Tishby, N., Pereira, F.C., and Biale, W.
\newblock The information bottleneck method.
\newblock In \emph{The 37th annual Allerton Conf. on Communication, Control,
  and Computing}, pp.\  368--377, 1999.
\newblock URL \url{https://arxiv.org/abs/physics/0004057}.

\bibitem[{Tomczak} \& {Welling}(2017){Tomczak} and {Welling}]{vampprior}
{Tomczak}, J.~M. and {Welling}, M.
\newblock {VAE with a VampPrior}.
\newblock \emph{ArXiv e-prints}, 2017.

\bibitem[van~den Oord et~al.(2017)van~den Oord, Vinyals, and
  kavukcuoglu]{vqvae}
van~den Oord, Aaron, Vinyals, Oriol, and kavukcuoglu, koray.
\newblock Neural discrete representation learning.
\newblock In \emph{NIPS}. 2017.

\bibitem[Zhao et~al.(2017)Zhao, Song, and Ermon]{infovae}
Zhao, Shengjia, Song, Jiaming, and Ermon, Stefano.
\newblock Infovae: Information maximizing variational autoencoders.
\newblock \emph{arXiv preprint 1706.02262}, 2017.

\bibitem[Zhao et~al.(2018)Zhao, Song, and Ermon]{infoautoencoding}
Zhao, Shengjia, Song, Jiaming, and Ermon, Stefano.
\newblock The information-autoencoding family: A lagrangian perspective on
  latent variable generative modeling, 2018.
\newblock URL \url{https://openreview.net/forum?id=ryZERzWCZ}.

\end{thebibliography}
