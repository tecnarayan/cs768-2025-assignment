@article{konevcny2015federated,
  title={Federated optimization: Distributed optimization beyond the datacenter},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, Brendan and Ramage, Daniel},
  journal={arXiv preprint arXiv:1511.03575},
  year={2015}
}

@inproceedings{shokri2015privacy,
  title={Privacy-preserving deep learning},
  author={Shokri, Reza and Shmatikov, Vitaly},
  booktitle={Proceedings of the 22nd ACM SIGSAC conference on computer and communications security},
  pages={1310--1321},
  year={2015}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial Intelligence and Statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@article{hard2018federated,
  title={Federated learning for mobile keyboard prediction},
  author={Hard, Andrew and Rao, Kanishka and Mathews, Rajiv and Ramaswamy, Swaroop and Beaufays, Fran{\c{c}}oise and Augenstein, Sean and Eichner, Hubert and Kiddon, Chlo{\'e} and Ramage, Daniel},
  journal={arXiv preprint arXiv:1811.03604},
  year={2018}
}

@inproceedings{leroy2019federated,
  title={Federated learning for keyword spotting},
  author={Leroy, David and Coucke, Alice and Lavril, Thibaut and Gisselbrecht, Thibault and Dureau, Joseph},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6341--6345},
  year={2019},
  organization={IEEE}
}

@misc{cordis2019machine,
  title={Machine learning ledger orchestration for drug discovery},
  author={Cordis, E},
  year={2019}
}

@article{rieke2020future,
  title={The future of digital health with federated learning},
  author={Rieke, Nicola and Hancox, Jonny and Li, Wenqi and Milletari, Fausto and Roth, Holger R and Albarqouni, Shadi and Bakas, Spyridon and Galtier, Mathieu N and Landman, Bennett A and Maier-Hein, Klaus and others},
  journal={NPJ digital medicine},
  volume={3},
  number={1},
  pages={1--7},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{kairouz2019advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Keith and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={arXiv preprint arXiv:1912.04977},
  year={2019}
}

@article{dekel2012optimal,
  title={Optimal Distributed Online Prediction Using Mini-Batches.},
  author={Dekel, Ofer and Gilad-Bachrach, Ran and Shamir, Ohad and Xiao, Lin},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={1},
  year={2012}
}

@article{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={26},
  pages={315--323},
  year={2013},
  publisher={Citeseer}
}

@inproceedings{allen2016variance,
  title={Variance reduction for faster non-convex optimization},
  author={Allen-Zhu, Zeyuan and Hazan, Elad},
  booktitle={International conference on machine learning},
  pages={699--707},
  year={2016},
  organization={PMLR}
}

@inproceedings{reddi2016stochastic,
  title={Stochastic variance reduction for nonconvex optimization},
  author={Reddi, Sashank J and Hefny, Ahmed and Sra, Suvrit and Poczos, Barnabas and Smola, Alex},
  booktitle={International conference on machine learning},
  pages={314--323},
  year={2016},
  organization={PMLR}
}

@inproceedings{nguyen2017sarah,
  title={SARAH: A novel method for machine learning problems using stochastic recursive gradient},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  booktitle={International Conference on Machine Learning},
  pages={2613--2621},
  year={2017},
  organization={PMLR}
}

@article{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  journal={arXiv preprint arXiv:1807.01695},
  year={2018}
}

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@article{lin2018don,
  title={Don't Use Large Mini-Batches, Use Local SGD},
  author={Lin, Tao and Stich, Sebastian U and Patel, Kumar Kshitij and Jaggi, Martin},
  journal={arXiv preprint arXiv:1808.07217},
  year={2018}
}

@article{stich2018local,
  title={Local SGD converges fast and communicates little},
  author={Stich, Sebastian U},
  journal={arXiv preprint arXiv:1805.09767},
  year={2018}
}

@inproceedings{woodworth2020local,
  title={Is local SGD better than minibatch SGD?},
  author={Woodworth, Blake and Patel, Kumar Kshitij and Stich, Sebastian and Dai, Zhen and Bullins, Brian and Mcmahan, Brendan and Shamir, Ohad and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={10334--10343},
  year={2020},
  organization={PMLR}
}

@inproceedings{yu2019parallel,
  title={Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning},
  author={Yu, Hao and Yang, Sen and Zhu, Shenghuo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={5693--5700},
  year={2019}
}

@article{haddadpour2019local,
  title={Local sgd with periodic averaging: Tighter analysis and adaptive synchronization},
  author={Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad and Cadambe, Viveck R},
  journal={arXiv preprint arXiv:1910.13598},
  year={2019}
}

@article{haddadpour2019convergence,
  title={On the convergence of local descent methods in federated learning},
  author={Haddadpour, Farzin and Mahdavi, Mehrdad},
  journal={arXiv preprint arXiv:1910.14425},
  year={2019}
}

@inproceedings{khaled2020tighter,
  title={Tighter theory for local SGD on identical and heterogeneous data},
  author={Khaled, Ahmed and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4519--4529},
  year={2020},
  organization={PMLR}
}

@inproceedings{koloskova2020unified,
  title={A unified theory of decentralized SGD with changing topology and local updates},
  author={Koloskova, Anastasia and Loizou, Nicolas and Boreiri, Sadra and Jaggi, Martin and Stich, Sebastian},
  booktitle={International Conference on Machine Learning},
  pages={5381--5393},
  year={2020},
  organization={PMLR}
}






@inproceedings{karimireddy2020scaffold,
  title={Scaffold: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International Conference on Machine Learning},
  pages={5132--5143},
  year={2020},
  organization={PMLR}
}

@article{woodworth2020minibatch,
  title={Minibatch vs Local SGD for Heterogeneous Distributed Learning},
  author={Woodworth, Blake and Patel, Kumar Kshitij and Srebro, Nathan},
  journal={arXiv preprint arXiv:2006.04735},
  year={2020}
}

@article{reddi2016aide,
  title={Aide: Fast and communication efficient distributed optimization},
  author={Reddi, Sashank J and Kone{\v{c}}n{\`y}, Jakub and Richt{\'a}rik, Peter and P{\'o}cz{\'o}s, Barnab{\'a}s and Smola, Alex},
  journal={arXiv preprint arXiv:1608.06879},
  year={2016}
}


@article{arjevani2015communication,
  title={Communication complexity of distributed convex learning and optimization},
  author={Arjevani, Yossi and Shamir, Ohad},
  journal={Advances in neural information processing systems},
  volume={28},
  pages={1756--1764},
  year={2015}
}

@article{lei2017non,
  title={Non-convex finite-sum optimization via scsg methods},
  author={Lei, Lihua and Ju, Cheng and Chen, Jianbo and Jordan, Michael I},
  journal={arXiv preprint arXiv:1706.09156},
  year={2017}
}

@article{zhou2018stochastic,
  title={Stochastic nested variance reduction for nonconvex optimization},
  author={Zhou, Dongruo and Xu, Pan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1806.07811},
  year={2018}
}

@article{nguyen2019finite,
  title={Finite-sum smooth optimization with sarah},
  author={Nguyen, Lam M and van Dijk, Marten and Phan, Dzung T and Nguyen, Phuong Ha and Weng, Tsui-Wei and Kalagnanam, Jayant R},
  journal={arXiv preprint arXiv:1901.07648},
  year={2019}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{zhang2020fedpd,
  title={FedPD: A federated learning framework with optimal rates and adaptivity to non-IID data},
  author={Zhang, Xinwei and Hong, Mingyi and Dhople, Sairaj and Yin, Wotao and Liu, Yang},
  journal={arXiv preprint arXiv:2005.11418},
  year={2020}
}

@article{sharma2019parallel,
  title={Parallel Restarted SPIDER--Communication Efficient Distributed Nonconvex Optimization with Optimal Computation Complexity},
  author={Sharma, Pranay and Kafle, Swatantra and Khanduri, Prashant and Bulusu, Saikiran and Rajawat, Ketan and Varshney, Pramod K},
  journal={arXiv preprint arXiv:1912.06036},
  year={2019}
}

@article{karimireddy2020mime,
  title={Mime: Mimicking centralized stochastic algorithms in federated learning},
  author={Karimireddy, Sai Praneeth and Jaggi, Martin and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank J and Stich, Sebastian U and Suresh, Ananda Theertha},
  journal={arXiv preprint arXiv:2008.03606},
  year={2020}
}

@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on learning theory},
  pages={797--842},
  year={2015},
  organization={PMLR}
}

@inproceedings{jin2017escape,
  title={How to escape saddle points efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  booktitle={International Conference on Machine Learning},
  pages={1724--1732},
  year={2017},
  organization={PMLR}
}

@article{allen2017natasha,
  title={Natasha 2: Faster non-convex optimization than sgd},
  author={Allen-Zhu, Zeyuan},
  journal={arXiv preprint arXiv:1708.08694},
  year={2017}
}

@article{li2019ssrgd,
  title={SSRGD: Simple stochastic recursive gradient descent for escaping saddle points},
  author={Li, Zhize},
  journal={arXiv preprint arXiv:1904.09265},
  year={2019}
}

@article{das2020faster,
  title={Faster Non-Convex Federated Learning via Global and Local Momentum},
  author={Das, Rudrajit and Acharya, Anish and Hashemi, Abolfazl and Sanghavi, Sujay and Dhillon, Inderjit S and Topcu, Ufuk},
  journal={arXiv preprint arXiv:2012.04061},
  year={2020}
}

@inproceedings{murata2021bias,
  title={Bias-Variance Reduced Local SGD for Less Heterogeneous Federated Learning},
  author={Murata, Tomoya and Suzuki, Taiji},
  booktitle={International Conference on Machine Learning},
  pages={7872-7881},
  year={2021},
  organization={PMLR}
}

@article{jin2021nonconvex,
  title={On nonconvex optimization for machine learning: Gradients, stochasticity, and saddle points},
  author={Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M and Jordan, Michael I},
  journal={Journal of the ACM (JACM)},
  volume={68},
  number={2},
  pages={1--29},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{xu2017first,
  title={First-order stochastic algorithms for escaping from saddle points in almost linear time},
  author={Xu, Yi and Jin, Rong and Yang, Tianbao},
  journal={arXiv preprint arXiv:1711.01944},
  year={2017}
}

@article{allen2017neon2,
  title={Neon2: Finding local minima via first-order oracles},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:1711.06673},
  year={2017}
}

@inproceedings{reddi2018generic,
  title={A generic approach for escaping saddle points},
  author={Reddi, Sashank and Zaheer, Manzil and Sra, Suvrit and Poczos, Barnabas and Bach, Francis and Salakhutdinov, Ruslan and Smola, Alex},
  booktitle={International conference on artificial intelligence and statistics},
  pages={1233--1242},
  year={2018},
  organization={PMLR}
}

@inproceedings{ge2019stabilized,
  title={Stabilized SVRG: Simple variance reduction for nonconvex optimization},
  author={Ge, Rong and Li, Zhize and Wang, Weiyao and Wang, Xiang},
  booktitle={Conference on learning theory},
  pages={1394--1448},
  year={2019},
  organization={PMLR}
}

@inproceedings{vlaski2020second,
  title={Second-Order Guarantees in Federated Learning},
  author={Vlaski, Stefan and Rizk, Elsa and Sayed, Ali H},
  booktitle={2020 54th Asilomar Conference on Signals, Systems, and Computers},
  pages={915--922},
  year={2020},
  organization={IEEE}
}

@article{khanduriachieving,
  title={Achieving Optimal Sample and Communication Complexities for Non-IID Federated Learning},
  author={Khanduri, Prashant and Sharma, Pranay and Yang, Haibo and Hong, Mingyi and Liu, Jia and Rajawat, Ketan and Varshney, Pramod K}, 
  year={2021}
}

@article{chen2021losac,
  title={LoSAC: An Efficient Local Stochastic Average Control Method for Federated Optimization},
  author={Chen, Huiming and Wang, Huandong and Yao, Quanming and Li, Yong and Jin, Depeng and Yang, Qiang},
  journal={arXiv preprint arXiv:2112.07839},
  year={2021}
}

@article{jin2019short,
  title={A short note on concentration inequalities for random vectors with subgaussian norm},
  author={Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M and Jordan, Michael I},
  journal={arXiv preprint arXiv:1902.03736},
  year={2019}
}