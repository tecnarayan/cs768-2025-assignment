\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aghajanyan et~al.(2021)Aghajanyan, Gupta, and Zettlemoyer]{aghajanyan2021intrinsic}
Aghajanyan, A., Gupta, S., and Zettlemoyer, L.
\newblock Intrinsic dimensionality explains the effectiveness of language model fine-tuning.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  7319--7328, 2021.

\bibitem[Babakniya et~al.(2023)Babakniya, Elkordy, Ezzeldin, Liu, Song, El-Khamy, and Avestimehr]{babakniya2023slora}
Babakniya, S., Elkordy, A.~R., Ezzeldin, Y.~H., Liu, Q., Song, K.-B., El-Khamy, M., and Avestimehr, S.
\newblock {SLoRA}: Federated parameter efficient fine-tuning of language models.
\newblock \emph{arXiv preprint arXiv:2308.06522}, 2023.

\bibitem[Bai et~al.(2024)Bai, Chen, Qian, Yao, and Li]{bai2024federated}
Bai, J., Chen, D., Qian, B., Yao, L., and Li, Y.
\newblock Federated fine-tuning of large language models under heterogeneous language tasks and client resources.
\newblock \emph{arXiv preprint arXiv:2402.11505}, 2024.

\bibitem[Borzunov et~al.(2023)Borzunov, Ryabinin, Chumachenko, Baranchuk, Dettmers, Belkada, Samygin, and Raffel]{borzunov2023distributed}
Borzunov, A., Ryabinin, M., Chumachenko, A., Baranchuk, D., Dettmers, T., Belkada, Y., Samygin, P., and Raffel, C.
\newblock Distributed inference and fine-tuning of large language models over the internet.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Che et~al.(2023)Che, Liu, Zhou, Ren, Zhou, Sheng, Dai, and Dou]{che2023FedPepTAO}
Che, T., Liu, J., Zhou, Y., Ren, J., Zhou, J., Sheng, V., Dai, H., and Dou, D.
\newblock Federated learning of large language models with parameter-efficient prompt tuning and adaptive optimization.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pp.\  7871--7888, 2023.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Feng, Zhou, Yin, and Zheng]{chen2023position}
Chen, C., Feng, X., Zhou, J., Yin, J., and Zheng, X.
\newblock Federated large language model: A position paper.
\newblock \emph{arXiv preprint arXiv:2307.08925}, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Gao, Xie, Pan, Li, Li, Ding, and Zhou]{chen2023fsreal}
Chen, D., Gao, D., Xie, Y., Pan, X., Li, Z., Li, Y., Ding, B., and Zhou, J.
\newblock {FS-REAL}: Towards real-world cross-device federated learning.
\newblock In \emph{Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pp.\  3829â€“3841, 2023{\natexlab{b}}.

\bibitem[Chen et~al.(2023{\natexlab{c}})Chen, Yao, Gao, Ding, and Li]{chen2023efficient}
Chen, D., Yao, L., Gao, D., Ding, B., and Li, Y.
\newblock Efficient personalized federated learning via sparse model-adaptation.
\newblock In \emph{International Conference on Machine Learning, {ICML}}, volume 202, pp.\  5234--5256, 2023{\natexlab{c}}.

\bibitem[Chen et~al.(2024)Chen, Huang, Ma, Chen, Pan, Ge, Gao, Xie, Liu, Gao, Li, Ding, and Zhou]{chen2023datajuicer}
Chen, D., Huang, Y., Ma, Z., Chen, H., Pan, X., Ge, C., Gao, D., Xie, Y., Liu, Z., Gao, J., Li, Y., Ding, B., and Zhou, J.
\newblock Data-juicer: {A} one-stop data processing system for large language models.
\newblock In \emph{International Conference on Management of Data}, 2024.

\bibitem[Chen et~al.(2022)Chen, Liu, Meng, and Liang]{chen2022revisiting}
Chen, G., Liu, F., Meng, Z., and Liang, S.
\newblock Revisiting parameter-efficient tuning: Are we really there yet?
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, {EMNLP}}, pp.\  2612--2626, 2022.

\bibitem[Chen et~al.(2023{\natexlab{d}})Chen, Chen, Gu, and Deng]{chen2023fine}
Chen, J., Chen, H., Gu, B., and Deng, H.
\newblock Fine-grained theoretical analysis of federated zeroth-order optimization.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023{\natexlab{d}}.

\bibitem[Conover et~al.(2023)Conover, Hayes, Mathur, Xie, Wan, Shah, Ghodsi, Wendell, Zaharia, and Xin]{DatabricksBlog2023DollyV2}
Conover, M., Hayes, M., Mathur, A., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M., and Xin, R.
\newblock Free dolly: Introducing the world's first truly open instruction-tuned {LLM}, 2023.
\newblock URL \url{https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm}.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2023qlora}
Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L.
\newblock {QLoRA}: Efficient finetuning of quantized {LLMs}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Dong et~al.(2023)Dong, Moon, Xu, Malik, and Yu]{dong2023towards}
Dong, X.~L., Moon, S., Xu, Y.~E., Malik, K., and Yu, Z.
\newblock Towards next-generation intelligent assistants leveraging {LLM} techniques.
\newblock In \emph{Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pp.\  5792--5793, 2023.

\bibitem[Dorfman et~al.(2023)Dorfman, Vargaftik, Ben-Itzhak, and Levy]{dorfman2023docofl}
Dorfman, R., Vargaftik, S., Ben-Itzhak, Y., and Levy, K.~Y.
\newblock {DoCoFL}: downlink compression for cross-device federated learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  8356--8388. PMLR, 2023.

\bibitem[Fan et~al.(2023)Fan, Kang, Ma, Chen, Wei, Fan, and Yang]{fan2023-FATE-LLM}
Fan, T., Kang, Y., Ma, G., Chen, W., Wei, W., Fan, L., and Yang, Q.
\newblock {FATE-LLM:} {A} industrial grade federated learning framework for large language models.
\newblock \emph{CoRR}, abs/2310.10049, 2023.

\bibitem[Fang et~al.(2022)Fang, Yu, Jiang, Shi, Jones, and Zhou]{fang2022communication}
Fang, W., Yu, Z., Jiang, Y., Shi, Y., Jones, C.~N., and Zhou, Y.
\newblock Communication-efficient stochastic zeroth-order optimization for federated learning.
\newblock \emph{IEEE Transactions on Signal Processing}, 70:\penalty0 5058--5073, 2022.

\bibitem[Feng et~al.(2023)Feng, Pang, Du, Chen, Yan, and Lin]{feng2023baffle}
Feng, H., Pang, T., Du, C., Chen, W., Yan, S., and Lin, M.
\newblock Does federated learning really need backpropagation?
\newblock \emph{arXiv preprint arXiv:2301.12195}, 2023.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen{-}Zhu, Li, Wang, Wang, and Chen]{hu2022lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen{-}Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock {LoRA}: Low-rank adaptation of large language models.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR}}, 2022.

\bibitem[Huang et~al.(2016)Huang, Li, Pleiss, Liu, Hopcroft, and Weinberger]{huang2016snapshot}
Huang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J.~E., and Weinberger, K.~Q.
\newblock Snapshot ensembles: Train 1, get {M} for free.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Jiang et~al.(2023)Jiang, Liu, and Fan]{jiang2023low-parameter}
Jiang, J., Liu, X., and Fan, C.
\newblock Low-parameter federated learning with large language models.
\newblock \emph{arXiv preprint arXiv:2307.13896}, 2023.

\bibitem[Kairouz et~al.(2021)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji, Bonawitz, Charles, Cormode, Cummings, D'Oliveira, Eichner, Rouayheb, Evans, Gardner, Garrett, Gasc{\'{o}}n, Ghazi, Gibbons, Gruteser, Harchaoui, He, He, Huo, Hutchinson, Hsu, Jaggi, Javidi, Joshi, Khodak, Kone{\v{c}}n{\'y}, Korolova, Koushanfar, Koyejo, Lepoint, Liu, Mittal, Mohri, Nock, {\"{O}}zg{\"{u}}r, Pagh, Qi, Ramage, Raskar, Raykova, Song, Song, Stich, Sun, Suresh, Tram{\`{e}}r, Vepakomma, Wang, Xiong, Xu, Yang, Yu, Yu, and Zhao]{Kairouz2021federated}
Kairouz, P., McMahan, H.~B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.~N., Bonawitz, K.~A., Charles, Z., Cormode, G., Cummings, R., D'Oliveira, R. G.~L., Eichner, H., Rouayheb, S.~E., Evans, D., Gardner, J., Garrett, Z., Gasc{\'{o}}n, A., Ghazi, B., Gibbons, P.~B., Gruteser, M., Harchaoui, Z., He, C., He, L., Huo, Z., Hutchinson, B., Hsu, J., Jaggi, M., Javidi, T., Joshi, G., Khodak, M., Kone{\v{c}}n{\'y}, J., Korolova, A., Koushanfar, F., Koyejo, S., Lepoint, T., Liu, Y., Mittal, P., Mohri, M., Nock, R., {\"{O}}zg{\"{u}}r, A., Pagh, R., Qi, H., Ramage, D., Raskar, R., Raykova, M., Song, D., Song, W., Stich, S.~U., Sun, Z., Suresh, A.~T., Tram{\`{e}}r, F., Vepakomma, P., Wang, J., Xiong, L., Xu, Z., Yang, Q., Yu, F.~X., Yu, H., and Zhao, S.
\newblock Advances and open problems in federated learning.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 14\penalty0 (1-2):\penalty0 1--210, 2021.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{3rd International Conference on Learning Representations, {ICLR}}, 2015.

\bibitem[Kuang et~al.(2023)Kuang, Qian, Li, Chen, Gao, Pan, Xie, Li, Ding, and Zhou]{kuang2023federatedscope-LLM}
Kuang, W., Qian, B., Li, Z., Chen, D., Gao, D., Pan, X., Xie, Y., Li, Y., Ding, B., and Zhou, J.
\newblock {FederatedScope}-{LLM}: A comprehensive package for fine-tuning large language models in federated learning.
\newblock \emph{arXiv preprint arXiv:2309.00363}, 2023.

\bibitem[Lester et~al.(2021)Lester, Al{-}Rfou, and Constant]{lester2021power}
Lester, B., Al{-}Rfou, R., and Constant, N.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, {EMNLP}}, pp.\  3045--3059, 2021.

\bibitem[Li et~al.(2018)Li, Farkhoor, Liu, and Yosinski]{li2018measuring-intrinsic}
Li, C., Farkhoor, H., Liu, R., and Yosinski, J.
\newblock Measuring the intrinsic dimension of objective landscapes.
\newblock In \emph{International Conference on Learning Representations, {ICLR}}, 2018.

\bibitem[Li et~al.(2022)Li, Diao, Chen, and He]{li2022federated}
Li, Q., Diao, Y., Chen, Q., and He, B.
\newblock Federated learning on non-{IID} data silos: An experimental study.
\newblock In \emph{2022 IEEE 38th International Conference on Data Engineering, {ICDE}}, pp.\  965--978. IEEE, 2022.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Sahu, Zaheer, Sanjabi, Talwalkar, and Smith]{li2020fedprox}
Li, T., Sahu, A.~K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.
\newblock Federated optimization in heterogeneous networks.
\newblock In \emph{Proceedings of Machine Learning and Systems 2020, MLSys 2020}, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Huang, Yang, Wang, and Zhang]{li2020convergence}
Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z.
\newblock On the convergence of fedavg on non-iid data.
\newblock In \emph{International Conference on Learning Representations, {ICLR}}, 2020{\natexlab{b}}.

\bibitem[Li \& Chen(2021)Li and Chen]{li2021communication}
Li, Z. and Chen, L.
\newblock Communication-efficient decentralized zeroth-order method on heterogeneous data.
\newblock In \emph{International Conference on Wireless Communications and Signal Processing, {WCSP}}, pp.\  1--6, 2021.

\bibitem[Lin(2004)]{lin2004rouge}
Lin, C.-Y.
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In \emph{Text summarization branches out}, pp.\  74--81, 2004.

\bibitem[Ling et~al.(2024)Ling, Chen, Yao, Li, and Shen]{ling2024convergence}
Ling, Z., Chen, D., Yao, L., Li, Y., and Shen, Y.
\newblock On the convergence of zeroth-order federated tuning for large language models.
\newblock \emph{arXiv preprint arXiv:2402.05926}, 2024.

\bibitem[Liu et~al.(2018)Liu, Kailkhura, Chen, Ting, Chang, and Amini]{liu2018zeroth}
Liu, S., Kailkhura, B., Chen, P.-Y., Ting, P., Chang, S., and Amini, L.
\newblock Zeroth-order stochastic variance reduction for nonconvex optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Liu et~al.(2023)Liu, Zheng, Du, Ding, Qian, Yang, and Tang]{liu2023gpt}
Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., and Tang, J.
\newblock {GPT} understands, too.
\newblock \emph{AI Open}, 2023.

\bibitem[Malladi et~al.(2023)Malladi, Gao, Nichani, Damian, Lee, Chen, and Arora]{malladi2023mezo}
Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J.~D., Chen, D., and Arora, S.
\newblock Fine-tuning language models with just forward passes.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 53038--53075, 2023.

\bibitem[Mangrulkar et~al.(2022)Mangrulkar, Gugger, Debut, Belkada, Paul, and Bossan]{peft}
Mangrulkar, S., Gugger, S., Debut, L., Belkada, Y., Paul, S., and Bossan, B.
\newblock {PEFT}: State-of-the-art parameter-efficient fine-tuning methods.
\newblock \url{https://github.com/huggingface/peft}, 2022.

\bibitem[Maritan et~al.(2023)Maritan, Dey, and Schenato]{maritan2023fedzen}
Maritan, A., Dey, S., and Schenato, L.
\newblock {FedZeN}: Towards superlinear zeroth-order federated learning via incremental hessian estimation.
\newblock \emph{CoRR}, abs/2309.17174, 2023.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and y~Arcas]{mcmahan2017communication}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized data.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  1273--1282. PMLR, 2017.

\bibitem[Melas-Kyriazi \& Wang(2021)Melas-Kyriazi and Wang]{melas2021intrinisic}
Melas-Kyriazi, L. and Wang, F.
\newblock Intrinisic gradient compression for federated learning.
\newblock \emph{arXiv preprint arXiv:2112.02656}, 2021.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock {PyTorch}: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Pu et~al.(2023)Pu, Jain, Yin, and Kaplan]{pu2023empirical}
Pu, G., Jain, A., Yin, J., and Kaplan, R.
\newblock Empirical analysis of the strengths and weaknesses of peft techniques for llms.
\newblock \emph{arXiv preprint arXiv:2304.14999}, 2023.

\bibitem[Qin et~al.(2024)Qin, Yan, Zhou, and Deng]{qin2024blockdfl}
Qin, Z., Yan, X., Zhou, M., and Deng, S.
\newblock {BlockDFL}: A blockchain-based fully decentralized peer-to-peer federated learning framework.
\newblock In \emph{Proceedings of the ACM on Web Conference 2024}, pp.\  2914--2925, 2024.

\bibitem[Rahimi et~al.(2024)Rahimi, Bhatti, Park, Kousar, Kim, and Moon]{rahimi2023evofed}
Rahimi, M.~M., Bhatti, H.~I., Park, Y., Kousar, H., Kim, D.-Y., and Moon, J.
\newblock {EvoFed}: Leveraging evolutionary strategies for communication-efficient federated learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Rothchild et~al.(2020)Rothchild, Panda, Ullah, Ivkin, Stoica, Braverman, Gonzalez, and Arora]{Rothchild2020FetchSGD}
Rothchild, D., Panda, A., Ullah, E., Ivkin, N., Stoica, I., Braverman, V., Gonzalez, J., and Arora, R.
\newblock {FetchSGD}: Communication-efficient federated learning with sketching.
\newblock In \emph{International Conference on Machine Learning}, pp.\  8253--8265. PMLR, 2020.

\bibitem[Shu et~al.(2023)Shu, Lin, Dai, and Low]{shu2023Trajectory}
Shu, Y., Lin, X., Dai, Z., and Low, B. K.~H.
\newblock Federated zeroth-order optimization using trajectory-informed surrogate gradients.
\newblock \emph{arXiv preprint arXiv:2308.04077}, 2023.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and Potts]{socher2013recursive}
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.~D., Ng, A.~Y., and Potts, C.
\newblock Recursive deep models for semantic compositionality over a sentiment treebank.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in natural language processing}, pp.\  1631--1642, 2013.

\bibitem[Sun et~al.(2023)Sun, Ji, Ma, and Li]{sun2023comparative}
Sun, X., Ji, Y., Ma, B., and Li, X.
\newblock A comparative study between full-parameter and {LoRA}-based fine-tuning on chinese instruction data for instruction following large language model.
\newblock \emph{arXiv preprint arXiv:2304.08109}, 2023.

\bibitem[Tan et~al.(2022)Tan, Yu, Cui, and Yang]{tan2022towards}
Tan, A.~Z., Yu, H., Cui, L., and Yang, Q.
\newblock Towards personalized federated learning.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems}, 2022.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{alpaca}
Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T.~B.
\newblock Stanford {Alpaca}: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`{e}}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozi{\`{e}}re, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G.
\newblock {LLaMA}: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Villalobos et~al.(2022)Villalobos, Sevilla, Heim, Besiroglu, Hobbhahn, and Ho]{villalobos2022will}
Villalobos, P., Sevilla, J., Heim, L., Besiroglu, T., Hobbhahn, M., and Ho, A.
\newblock Will we run out of data? an analysis of the limits of scaling datasets in machine learning.
\newblock \emph{arXiv preprint arXiv:2211.04325}, 2022.

\bibitem[Wang et~al.(2022)Wang, Mishra, Alipoormolabashi, Kordi, Mirzaei, Naik, Ashok, Dhanasekaran, Arunkumar, Stap, Pathak, Karamanolakis, Lai, Purohit, Mondal, Anderson, Kuznia, Doshi, Pal, Patel, Moradshahi, Parmar, Purohit, Varshney, Kaza, Verma, Puri, Karia, Doshi, Sampat, Mishra, A, Patro, Dixit, and Shen]{supernaturalinstructions}
Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Naik, A., Ashok, A., Dhanasekaran, A.~S., Arunkumar, A., Stap, D., Pathak, E., Karamanolakis, G., Lai, H.~G., Purohit, I., Mondal, I., Anderson, J., Kuznia, K., Doshi, K., Pal, K.~K., Patel, M., Moradshahi, M., Parmar, M., Purohit, M., Varshney, N., Kaza, P.~R., Verma, P., Puri, R.~S., Karia, R., Doshi, S., Sampat, S.~K., Mishra, S., A, S.~R., Patro, S., Dixit, T., and Shen, X.
\newblock Super-naturalinstructions: Generalization via declarative instructions on 1600+ {NLP} tasks.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, {EMNLP}}, pp.\  5085--5109. Association for Computational Linguistics, 2022.

\bibitem[Wei et~al.(2022)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2022FLAN}
Wei, J., Bosma, M., Zhao, V.~Y., Guu, K., Yu, A.~W., Lester, B., Du, N., Dai, A.~M., and Le, Q.~V.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR}}, 2022.

\bibitem[Woisetschl{\"a}ger et~al.(2023)Woisetschl{\"a}ger, Isenko, Wang, Mayer, and Jacobsen]{woisetschlager2023very-edge}
Woisetschl{\"a}ger, H., Isenko, A., Wang, S., Mayer, R., and Jacobsen, H.-A.
\newblock Federated fine-tuning of {LLMs} on the very edge: The good, the bad, the ugly.
\newblock \emph{arXiv preprint arXiv:2310.03150}, 2023.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, et~al.]{wolf-etal-2020-transformers}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et~al.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations}, pp.\  38--45, 2020.

\bibitem[Xi et~al.(2023)Xi, Li, Chen, and Zhu]{xi2023-4bit}
Xi, H., Li, C., Chen, J., and Zhu, J.
\newblock Training transformers with 4-bit integers.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 49146--49168, 2023.

\bibitem[Xu et~al.(2023)Xu, Wu, Cai, Li, and Wang]{xu2023billion-sized}
Xu, M., Wu, Y., Cai, D., Li, X., and Wang, S.
\newblock {FwdLLM}: Efficient {FedLLM} using forward gradient.
\newblock \emph{arXiv preprint arXiv:2308.13894}, 2023.

\bibitem[Zelikman et~al.(2023)Zelikman, Huang, Liang, Haber, and Goodman]{zelikman2023onebyte}
Zelikman, E., Huang, Q., Liang, P., Haber, N., and Goodman, N.~D.
\newblock Just one byte (per gradient): {A} note on low-bandwidth decentralized language model finetuning using shared randomness.
\newblock \emph{CoRR}, abs/2306.10015, 2023.

\bibitem[Zhang et~al.(2024)Zhang, Vahidian, Kuo, Li, Zhang, Yu, Wang, and Chen]{zhang2023fedit}
Zhang, J., Vahidian, S., Kuo, M., Li, C., Zhang, R., Yu, T., Wang, G., and Chen, Y.
\newblock Towards building the federatedgpt: Federated instruction tuning.
\newblock In \emph{ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pp.\  6915--6919. IEEE, 2024.

\bibitem[Zhang et~al.(2023)Zhang, Yang, Dai, Wang, Yu, Qu, and Xu]{zhang2023-FedPETuning}
Zhang, Z., Yang, Y., Dai, Y., Wang, Q., Yu, Y., Qu, L., and Xu, Z.
\newblock {FedPETuning}: When federated learning meets the parameter-efficient tuning methods of pre-trained language models.
\newblock In \emph{Findings of the Association for Computational Linguistics: {ACL}}, pp.\  9963--9977, 2023.

\end{thebibliography}
