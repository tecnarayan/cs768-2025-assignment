\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{abbasi2011improved}
Abbasi-Yadkori, Y., P{\'a}l, D., and Szepesv{\'a}ri, C.
\newblock Improved algorithms for linear stochastic bandits.
\newblock \emph{Advances in neural information processing systems},
  24:\penalty0 2312--2320, 2011.

\bibitem[Agarwal et~al.(2017)Agarwal, Krishnamurthy, Langford, Luo,
  et~al.]{agarwal2017open}
Agarwal, A., Krishnamurthy, A., Langford, J., Luo, H., et~al.
\newblock Open problem: First-order regret bounds for contextual bandits.
\newblock In \emph{Conference on Learning Theory}, pp.\  4--7. PMLR, 2017.

\bibitem[Allen-Zhu et~al.(2018)Allen-Zhu, Bubeck, and Li]{allen2018make}
Allen-Zhu, Z., Bubeck, S., and Li, Y.
\newblock Make the minority great again: First-order regret bound for
  contextual bandits.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  186--194. PMLR, 2018.

\bibitem[Allenberg et~al.(2006)Allenberg, Auer, Gy{\"o}rfi, and
  Ottucs{\'a}k]{allenberg2006hannan}
Allenberg, C., Auer, P., Gy{\"o}rfi, L., and Ottucs{\'a}k, G.
\newblock Hannan consistency in on-line learning in case of unbounded losses
  under partial monitoring.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pp.\  229--243. Springer, 2006.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Gentile]{auer2002adaptive}
Auer, P., Cesa-Bianchi, N., and Gentile, C.
\newblock Adaptive and self-confident on-line learning algorithms.
\newblock \emph{Journal of Computer and System Sciences}, 64\penalty0
  (1):\penalty0 48--75, 2002.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang, and
  Yang]{ayoub2020model}
Ayoub, A., Jia, Z., Szepesvari, C., Wang, M., and Yang, L.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  463--474. PMLR, 2020.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  263--272. PMLR, 2017.

\bibitem[Bubeck \& Sellke(2020)Bubeck and Sellke]{bubeck2020first}
Bubeck, S. and Sellke, M.
\newblock First-order bayesian regret analysis of thompson sampling.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  196--233. PMLR, 2020.

\bibitem[Camilleri et~al.(2021)Camilleri, Jamieson, and
  Katz-Samuels]{camilleri2021high}
Camilleri, R., Jamieson, K., and Katz-Samuels, J.
\newblock High-dimensional experimental design and kernel bandits.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1227--1237. PMLR, 2021.

\bibitem[Catoni(2012)]{catoni2012challenging}
Catoni, O.
\newblock Challenging the empirical mean and empirical variance: a deviation
  study.
\newblock In \emph{Annales de l'IHP Probabilit{\'e}s et statistiques},
  volume~48, pp.\  1148--1185, 2012.

\bibitem[Cesa-Bianchi et~al.(2007)Cesa-Bianchi, Mansour, and
  Stoltz]{cesa2007improved}
Cesa-Bianchi, N., Mansour, Y., and Stoltz, G.
\newblock Improved second-order bounds for prediction with expert advice.
\newblock \emph{Machine Learning}, 66\penalty0 (2):\penalty0 321--352, 2007.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Dann, C., Lattimore, T., and Brunskill, E.
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1703.07710}, 2017.

\bibitem[Dann et~al.(2019)Dann, Li, Wei, and Brunskill]{dann2019policy}
Dann, C., Li, L., Wei, W., and Brunskill, E.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1507--1516. PMLR, 2019.

\bibitem[Dann et~al.(2021)Dann, Marinov, Mohri, and Zimmert]{dann2021beyond}
Dann, C., Marinov, T.~V., Mohri, M., and Zimmert, J.
\newblock Beyond value-function gaps: Improved instance-dependent regret bounds
  for episodic reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Du et~al.(2019)Du, Kakade, Wang, and Yang]{du2019good}
Du, S.~S., Kakade, S.~M., Wang, R., and Yang, L.~F.
\newblock Is a good representation sufficient for sample efficient
  reinforcement learning?
\newblock \emph{arXiv preprint arXiv:1910.03016}, 2019.

\bibitem[Du et~al.(2021)Du, Kakade, Lee, Lovett, Mahajan, Sun, and
  Wang]{du2021bilinear}
Du, S.~S., Kakade, S.~M., Lee, J.~D., Lovett, S., Mahajan, G., Sun, W., and
  Wang, R.
\newblock Bilinear classes: A structural framework for provable generalization
  in rl.
\newblock \emph{arXiv preprint arXiv:2103.10897}, 2021.

\bibitem[Foster \& Krishnamurthy(2021)Foster and
  Krishnamurthy]{foster2021efficient}
Foster, D.~J. and Krishnamurthy, A.
\newblock Efficient first-order contextual bandits: Prediction, allocation, and
  triangular discrimination.
\newblock In \emph{Thirty-Fifth Conference on Neural Information Processing
  Systems}, 2021.

\bibitem[Foster et~al.(2015)Foster, Rakhlin, and Sridharan]{foster2015adaptive}
Foster, D.~J., Rakhlin, A., and Sridharan, K.
\newblock Adaptive online learning.
\newblock \emph{arXiv preprint arXiv:1508.05170}, 2015.

\bibitem[Freedman(1975)]{freedman1975tail}
Freedman, D.~A.
\newblock On tail probabilities for martingales.
\newblock \emph{the Annals of Probability}, pp.\  100--118, 1975.

\bibitem[Freund \& Schapire(1997)Freund and Schapire]{freund1997decision}
Freund, Y. and Schapire, R.~E.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock \emph{Journal of computer and system sciences}, 55\penalty0
  (1):\penalty0 119--139, 1997.

\bibitem[Hazan \& Kale(2011)Hazan and Kale]{hazan2011better}
Hazan, E. and Kale, S.
\newblock Better algorithms for benign bandits.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0 (4), 2011.

\bibitem[He et~al.(2021)He, Zhou, and Gu]{he2021logarithmic}
He, J., Zhou, D., and Gu, Q.
\newblock Logarithmic regret for reinforcement learning with linear function
  approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4171--4180. PMLR, 2021.

\bibitem[Ito et~al.(2020)Ito, Hirahara, Soma, and Yoshida]{ito2020tight}
Ito, S., Hirahara, S., Soma, T., and Yoshida, Y.
\newblock Tight first-and second-order regret bounds for adversarial linear
  bandits.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2028--2038, 2020.

\bibitem[Jia et~al.(2020)Jia, Yang, Szepesvari, and Wang]{jia2020model}
Jia, Z., Yang, L., Szepesvari, C., and Wang, M.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{Learning for Dynamics and Control}, pp.\  666--686. PMLR,
  2020.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R.~E.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1704--1713. PMLR, 2017.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I.
\newblock Is q-learning provably efficient?
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pp.\  4868--4878, 2018.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Krishnamurthy, Simchowitz, and
  Yu]{jin2020reward}
Jin, C., Krishnamurthy, A., Simchowitz, M., and Yu, T.
\newblock Reward-free exploration for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4870--4879. PMLR, 2020{\natexlab{a}}.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Yang, Wang, and
  Jordan]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pp.\  2137--2143. PMLR,
  2020{\natexlab{b}}.

\bibitem[Jin et~al.(2021)Jin, Liu, and Miryoosefi]{jin2021bellman}
Jin, C., Liu, Q., and Miryoosefi, S.
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock \emph{arXiv preprint arXiv:2102.00815}, 2021.

\bibitem[Kakade(2003)]{kakade2003sample}
Kakade, S.~M.
\newblock \emph{On the sample complexity of reinforcement learning}.
\newblock PhD thesis, UCL (University College London), 2003.

\bibitem[Kearns \& Singh(2002)Kearns and Singh]{kearns2002near}
Kearns, M. and Singh, S.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine learning}, 49\penalty0 (2):\penalty0 209--232, 2002.

\bibitem[Kim et~al.(2021)Kim, Yang, and Jun]{kim2021improved}
Kim, Y., Yang, I., and Jun, K.-S.
\newblock Improved regret analysis for variance-adaptive linear bandits and
  horizon-free linear mixture mdps.
\newblock \emph{arXiv preprint arXiv:2111.03289}, 2021.

\bibitem[Koolen \& Van~Erven(2015)Koolen and Van~Erven]{koolen2015second}
Koolen, W.~M. and Van~Erven, T.
\newblock Second-order quantile methods for experts and combinatorial games.
\newblock In \emph{Conference on Learning Theory}, pp.\  1155--1175. PMLR,
  2015.

\bibitem[Lattimore \& Szepesv{\'a}ri(2020)Lattimore and
  Szepesv{\'a}ri]{lattimore2020bandit}
Lattimore, T. and Szepesv{\'a}ri, C.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Lee et~al.(2021)Lee, Luo, Wei, Zhang, and Zhang]{lee2021achieving}
Lee, C.-W., Luo, H., Wei, C.-Y., Zhang, M., and Zhang, X.
\newblock Achieving near instance-optimality and minimax-optimality in
  stochastic and adversarial linear bandits simultaneously.
\newblock \emph{arXiv preprint arXiv:2102.05858}, 2021.

\bibitem[Lugosi \& Mendelson(2019)Lugosi and Mendelson]{lugosi2019mean}
Lugosi, G. and Mendelson, S.
\newblock Mean estimation and regression under heavy-tailed distributions: A
  survey.
\newblock \emph{Foundations of Computational Mathematics}, 19\penalty0
  (5):\penalty0 1145--1190, 2019.

\bibitem[Luo \& Schapire(2015)Luo and Schapire]{luo2015achieving}
Luo, H. and Schapire, R.~E.
\newblock Achieving all with no parameters: Adanormalhedge.
\newblock In \emph{Conference on Learning Theory}, pp.\  1286--1304. PMLR,
  2015.

\bibitem[Lykouris et~al.(2018)Lykouris, Sridharan, and
  Tardos]{lykouris2018small}
Lykouris, T., Sridharan, K., and Tardos, {\'E}.
\newblock Small-loss bounds for online learning with partial information.
\newblock In \emph{Conference on Learning Theory}, pp.\  979--986. PMLR, 2018.

\bibitem[Neu(2015)]{neu2015first}
Neu, G.
\newblock First-order regret bounds for combinatorial semi-bandits.
\newblock In \emph{Conference on Learning Theory}, pp.\  1360--1375. PMLR,
  2015.

\bibitem[Sarkar \& Rakhlin(2019)Sarkar and Rakhlin]{sarkar2019near}
Sarkar, T. and Rakhlin, A.
\newblock Near optimal finite time identification of arbitrary linear dynamical
  systems.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5610--5618. PMLR, 2019.

\bibitem[Simchowitz \& Jamieson(2019)Simchowitz and
  Jamieson]{simchowitz2019non}
Simchowitz, M. and Jamieson, K.
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock \emph{arXiv preprint arXiv:1905.03814}, 2019.

\bibitem[Srebro et~al.(2010)Srebro, Sridharan, and
  Tewari]{srebro2010smoothness}
Srebro, N., Sridharan, K., and Tewari, A.
\newblock Smoothness, low noise and fast rates.
\newblock \emph{Advances in neural information processing systems}, 23, 2010.

\bibitem[Vapnik \& Chervonenkis(1971)Vapnik and
  Chervonenkis]{vapnik1971uniform}
Vapnik, V.~N. and Chervonenkis, A.~Y.
\newblock On the uniform convergence of relative frequencies of events to their
  probabilities.
\newblock 1971.

\bibitem[Vershynin(2010)]{vershynin2010introduction}
Vershynin, R.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \emph{arXiv preprint arXiv:1011.3027}, 2010.

\bibitem[Wang et~al.(2020)Wang, Du, Yang, and Kakade]{wang2020long}
Wang, R., Du, S.~S., Yang, L.~F., and Kakade, S.~M.
\newblock Is long horizon reinforcement learning more difficult than short
  horizon reinforcement learning?
\newblock \emph{arXiv preprint arXiv:2005.00527}, 2020.

\bibitem[Wang et~al.(2019)Wang, Wang, Du, and Krishnamurthy]{wang2019optimism}
Wang, Y., Wang, R., Du, S.~S., and Krishnamurthy, A.
\newblock Optimism in reinforcement learning with generalized linear function
  approximation.
\newblock \emph{arXiv preprint arXiv:1912.04136}, 2019.

\bibitem[Wang et~al.(2021)Wang, Wang, and Kakade]{wang2021exponential}
Wang, Y., Wang, R., and Kakade, S.~M.
\newblock An exponential lower bound for linearly-realizable mdps with constant
  suboptimality gap.
\newblock \emph{arXiv preprint arXiv:2103.12690}, 2021.

\bibitem[Wei \& Luo(2018)Wei and Luo]{wei2018more}
Wei, C.-Y. and Luo, H.
\newblock More adaptive algorithms for adversarial bandits.
\newblock In \emph{Conference On Learning Theory}, pp.\  1263--1291. PMLR,
  2018.

\bibitem[Wei et~al.(2020)Wei, Luo, and Agarwal]{wei2020taking}
Wei, C.-Y., Luo, H., and Agarwal, A.
\newblock Taking a hint: How to leverage loss predictors in contextual bandits?
\newblock In \emph{Conference on Learning Theory}, pp.\  3583--3634. PMLR,
  2020.

\bibitem[Weisz et~al.(2021)Weisz, Amortila, and
  Szepesv{\'a}ri]{weisz2021exponential}
Weisz, G., Amortila, P., and Szepesv{\'a}ri, C.
\newblock Exponential lower bounds for planning in mdps with
  linearly-realizable optimal action-value functions.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  1237--1264. PMLR, 2021.

\bibitem[Xu et~al.(2021)Xu, Ma, and Du]{xu2021fine}
Xu, H., Ma, T., and Du, S.~S.
\newblock Fine-grained gap-dependent bounds for tabular mdps via adaptive
  multi-step bootstrap.
\newblock \emph{arXiv preprint arXiv:2102.04692}, 2021.

\bibitem[Yang \& Wang(2019)Yang and Wang]{yang2019sample}
Yang, L. and Wang, M.
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6995--7004. PMLR, 2019.

\bibitem[Zanette \& Brunskill(2019)Zanette and Brunskill]{zanette2019tighter}
Zanette, A. and Brunskill, E.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7304--7312. PMLR, 2019.

\bibitem[Zanette et~al.(2020{\natexlab{a}})Zanette, Brandfonbrener, Brunskill,
  Pirotta, and Lazaric]{zanette2020frequentist}
Zanette, A., Brandfonbrener, D., Brunskill, E., Pirotta, M., and Lazaric, A.
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1954--1964. PMLR, 2020{\natexlab{a}}.

\bibitem[Zanette et~al.(2020{\natexlab{b}})Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
Zanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10978--10989. PMLR, 2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Ji, and
  Du]{zhang2020reinforcement}
Zhang, Z., Ji, X., and Du, S.~S.
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock \emph{arXiv preprint arXiv:2009.13503}, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Zhou, and Ji]{zhang2020almost}
Zhang, Z., Zhou, Y., and Ji, X.
\newblock Almost optimal model-free reinforcement learningvia
  reference-advantage decomposition.
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2021)Zhang, Yang, Ji, and Du]{zhang2021variance}
Zhang, Z., Yang, J., Ji, X., and Du, S.~S.
\newblock Variance-aware confidence set: Variance-dependent bound for linear
  bandits and horizon-free bound for linear mixture mdp.
\newblock \emph{arXiv preprint arXiv:2101.12745}, 2021.

\bibitem[Zhou et~al.(2020)Zhou, Gu, and Szepesvari]{zhou2020nearly}
Zhou, D., Gu, Q., and Szepesvari, C.
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock \emph{arXiv preprint arXiv:2012.08507}, 2020.

\bibitem[Zhou et~al.(2021)Zhou, He, and Gu]{zhou2021provably}
Zhou, D., He, J., and Gu, Q.
\newblock Provably efficient reinforcement learning for discounted mdps with
  feature mapping.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12793--12802. PMLR, 2021.

\end{thebibliography}
