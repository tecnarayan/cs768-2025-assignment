\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2018)Arora, Cohen, and Hazan]{arora2018optimization}
Arora, S., Cohen, N., and Hazan, E.~E.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock In \emph{35th International Conference on Machine Learning, ICML
  2018}, pp.\  372--389. International Machine Learning Society (IMLS), 2018.

\bibitem[Chaudhari et~al.(2019)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{chaudhari2019entropy}
Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C.,
  Chayes, J., Sagun, L., and Zecchina, R.
\newblock Entropy-{SGD}: {B}iasing gradient descent into wide valleys.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2019\penalty0 (12):\penalty0 124018, 2019.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1019--1028. JMLR. org, 2017.

\bibitem[Du et~al.(2018)Du, Hu, and Lee]{du2018algorithmic}
Du, S.~S., Hu, W., and Lee, J.~D.
\newblock Algorithmic regularization in learning deep homogeneous models:
  Layers are automatically balanced.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  384--395, 2018.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Gunasekar, S., Woodworth, B.~E., Bhojanapalli, S., Neyshabur, B., and Srebro,
  N.
\newblock Implicit regularization in matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6151--6159, 2017.

\bibitem[Gunasekar et~al.(2018{\natexlab{a}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018characterizing}
Gunasekar, S., Lee, J., Soudry, D., and Srebro, N.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock \emph{arXiv preprint arXiv:1802.08246}, 2018{\natexlab{a}}.

\bibitem[Gunasekar et~al.(2018{\natexlab{b}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018implicit}
Gunasekar, S., Lee, J.~D., Soudry, D., and Srebro, N.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9461--9471, 2018{\natexlab{b}}.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: {S}urpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  1026--1034, 2015.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997flat}
Hochreiter, S. and Schmidhuber, J.
\newblock Flat minima.
\newblock \emph{Neural Computation}, 9\penalty0 (1):\penalty0 1--42, 1997.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{hoffer2017train}
Hoffer, E., Hubara, I., and Soudry, D.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1731--1741, 2017.

\bibitem[Jastrz{\k{e}}bski et~al.(2017)Jastrz{\k{e}}bski, Kenton, Arpit,
  Ballas, Fischer, Bengio, and Storkey]{jastrzkebski2017three}
Jastrz{\k{e}}bski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio,
  Y., and Storkey, A.
\newblock Three factors influencing minima in {SGD}.
\newblock \emph{arXiv preprint arXiv:1711.04623}, 2017.

\bibitem[Ji \& Telgarsky(2019)Ji and Telgarsky]{ji2019gradient}
Ji, Z. and Telgarsky, M.~J.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock In \emph{7th International Conference on Learning Representations,
  ICLR 2019}, 2019.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[LeCun(1998)]{lecun1998mnist}
LeCun, Y.
\newblock The {MNIST} database of handwritten digits.
\newblock 1998.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6389--6399, 2018.

\bibitem[Lyu \& Li(2020)Lyu and Li]{lyu2020gradient}
Lyu, K. and Li, J.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Masters \& Luschi(2018)Masters and Luschi]{masters2018revisiting}
Masters, D. and Luschi, C.
\newblock Revisiting small batch training for deep neural networks.
\newblock \emph{arXiv preprint arXiv:1804.07612}, 2018.

\bibitem[Morgan \& Bourlard(1990)Morgan and Bourlard]{morgan1990generalization}
Morgan, N. and Bourlard, H.
\newblock Generalization and parameter estimation in feedforward nets: Some
  experiments.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  630--637, 1990.

\bibitem[Nar \& Sastry(2018)Nar and Sastry]{nar2018step}
Nar, K. and Sastry, S.
\newblock Step size matters in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3436--3444, 2018.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and
  Srebro]{neyshabur2014search}
Neyshabur, B., Tomioka, R., and Srebro, N.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1412.6614}, 2014.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{neyshabur2017exploring}
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N.
\newblock Exploring generalization in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5947--5956, 2017.

\bibitem[Simsekli et~al.(2019)Simsekli, Sagun, and
  Gurbuzbalaban]{simsekli2019tail}
Simsekli, U., Sagun, L., and Gurbuzbalaban, M.
\newblock A tail-index analysis of stochastic gradient noise in deep neural
  networks.
\newblock \emph{arXiv preprint arXiv:1901.06053}, 2019.

\bibitem[Smith \& Le(2017)Smith and Le]{smith2017bayesian}
Smith, S.~L. and Le, Q.~V.
\newblock A bayesian perspective on generalization and stochastic gradient
  descent.
\newblock \emph{arXiv preprint arXiv:1710.06451}, 2017.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Soudry, D., Hoffer, E., Nacson, M.~S., Gunasekar, S., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Strand(1974)]{strand1974theory}
Strand, O.~N.
\newblock Theory and methods related to the singular-function expansion and
  landweberâ€™s iteration for integral equations of the first kind.
\newblock \emph{SIAM Journal on Numerical Analysis}, 11\penalty0 (4):\penalty0
  798--825, 1974.

\bibitem[Wu et~al.(2018)Wu, Ma, and Weinan]{wu2018sgd}
Wu, L., Ma, C., and Weinan, E.
\newblock How {SGD} selects the global minima in over-parameterized learning: A
  dynamical stability perspective.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8279--8288, 2018.

\bibitem[Wu et~al.(2019)Wu, Dobriban, Ren, Wu, Li, Gunasekar, Ward, and
  Liu]{wu2019implicit}
Wu, X., Dobriban, E., Ren, T., Wu, S., Li, Z., Gunasekar, S., Ward, R., and
  Liu, Q.
\newblock Implicit regularization of normalization methods.
\newblock \emph{arXiv preprint arXiv:1911.07956}, 2019.

\bibitem[Zhang et~al.(2018)Zhang, Dauphin, and Ma]{zhang2018fixup}
Zhang, H., Dauphin, Y.~N., and Ma, T.
\newblock Fixup initialization: Residual learning without normalization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\end{thebibliography}
