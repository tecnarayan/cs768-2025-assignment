\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Yamins et~al.(2014)Yamins, Hong, Cadieu, Solomon, Seibert, and DiCarlo]{yamins2014performance}
Daniel~LK Yamins, Ha~Hong, Charles~F Cadieu, Ethan~A Solomon, Darren Seibert, and James~J DiCarlo.
\newblock Performance-optimized hierarchical models predict neural responses in higher visual cortex.
\newblock \emph{Proceedings of the national academy of sciences}, 111\penalty0 (23):\penalty0 8619--8624, 2014.

\bibitem[Yamins and DiCarlo(2016)]{yamins2016using}
Daniel~LK Yamins and James~J DiCarlo.
\newblock Using goal-driven deep learning models to understand sensory cortex.
\newblock \emph{Nature neuroscience}, 19\penalty0 (3):\penalty0 356--365, 2016.

\bibitem[Schrimpf et~al.(2018)Schrimpf, Kubilius, Hong, Majaj, Rajalingham, Issa, Kar, Bashivan, Prescott-Roy, Geiger, et~al.]{schrimpf2018brain}
Martin Schrimpf, Jonas Kubilius, Ha~Hong, Najib~J Majaj, Rishi Rajalingham, Elias~B Issa, Kohitij Kar, Pouya Bashivan, Jonathan Prescott-Roy, Franziska Geiger, et~al.
\newblock Brain-score: Which artificial neural network for object recognition is most brain-like?
\newblock \emph{BioRxiv}, page 407007, 2018.

\bibitem[Minni et~al.(2019)Minni, Ji-An, Moskovitz, Lindsay, Miller, Dipoppa, and Yang]{minni2019understanding}
Sun Minni, Li~Ji-An, Theodore Moskovitz, Grace Lindsay, Kenneth Miller, Mario Dipoppa, and Guangyu~Robert Yang.
\newblock Understanding the functional and structural differences across excitatory and inhibitory neurons.
\newblock \emph{bioRxiv}, page 680439, 2019.

\bibitem[Cueva and Wei(2018)]{cueva2018emergence}
Christopher~J Cueva and Xue-Xin Wei.
\newblock Emergence of grid-like representations by training recurrent neural networks to perform spatial localization.
\newblock \emph{arXiv preprint arXiv:1803.07770}, 2018.

\bibitem[Banino et~al.(2018)Banino, Barry, Uria, Blundell, Lillicrap, Mirowski, Pritzel, Chadwick, Degris, Modayil, et~al.]{banino2018vector}
Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexander Pritzel, Martin~J Chadwick, Thomas Degris, Joseph Modayil, et~al.
\newblock Vector-based navigation using grid-like representations in artificial agents.
\newblock \emph{Nature}, 557\penalty0 (7705):\penalty0 429--433, 2018.

\bibitem[Wang et~al.(2018)Wang, Kurth-Nelson, Kumaran, Tirumala, Soyer, Leibo, Hassabis, and Botvinick]{wang2018prefrontal}
Jane~X Wang, Zeb Kurth-Nelson, Dharshan Kumaran, Dhruva Tirumala, Hubert Soyer, Joel~Z Leibo, Demis Hassabis, and Matthew Botvinick.
\newblock Prefrontal cortex as a meta-reinforcement learning system.
\newblock \emph{Nature neuroscience}, 21\penalty0 (6):\penalty0 860--868, 2018.

\bibitem[Hanson et~al.(2018)Hanson, Çağlar, and Hanson]{Hanson2018}
Catherine Hanson, Leyla~Roskan Çağlar, and Stephen~Jos{\'e} Hanson.
\newblock Attentional bias in human category learning: The case of deep learning.
\newblock \emph{Frontiers in Psychology}, 9, 2018.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna~M. Wallach, Rob Fergus, S.~V.~N. Vishwanathan, and Roman Garnett, editors, \emph{Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}}, pages 5998--6008, 2017.

\bibitem[Ramsauer et~al.(2020)Ramsauer, Sch{\"a}fl, Lehner, Seidl, Widrich, Adler, Gruber, Holzleitner, Pavlovi{\'c}, Sandve, et~al.]{ramsauer2020hopfield}
Hubert Ramsauer, Bernhard Sch{\"a}fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovi{\'c}, Geir~Kjetil Sandve, et~al.
\newblock Hopfield networks is all you need.
\newblock \emph{arXiv preprint arXiv:2008.02217}, 2020.

\bibitem[Goldstein et~al.(2024)Goldstein, Grinstein-Dabush, Schain, Wang, Hong, Aubrey, Schain, Nastase, Zada, Ham, et~al.]{goldstein2024alignment}
Ariel Goldstein, Avigail Grinstein-Dabush, Mariano Schain, Haocheng Wang, Zhuoqiao Hong, Bobbi Aubrey, Mariano Schain, Samuel~A Nastase, Zaid Zada, Eric Ham, et~al.
\newblock Alignment of brain embeddings and artificial contextual embeddings in natural language points to common geometric patterns.
\newblock \emph{Nature communications}, 15\penalty0 (1):\penalty0 2768, 2024.

\bibitem[Schrimpf et~al.(2020)Schrimpf, Blank, Tuckute, Kauf, Hosseini, Kanwisher, Tenenbaum, and Fedorenko]{schrimpf2020neural}
Martin Schrimpf, Idan Blank, Greta Tuckute, Carina Kauf, Eghbal~A Hosseini, Nancy Kanwisher, Joshua Tenenbaum, and Evelina Fedorenko.
\newblock The neural architecture of language: Integrative reverse-engineering converges on a model for predictive processing.
\newblock \emph{BioRxiv}, 2020.

\bibitem[Antonello et~al.(2024)Antonello, Vaidya, and Huth]{antonello2024scaling}
Richard Antonello, Aditya Vaidya, and Alexander Huth.
\newblock Scaling laws for language encoding models in fmri.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Whittington et~al.(2021)Whittington, Warren, and Behrens]{whittington2021relating}
James~CR Whittington, Joseph Warren, and Timothy~EJ Behrens.
\newblock Relating transformers to models and neural representations of the hippocampal formation.
\newblock \emph{arXiv preprint arXiv:2112.04035}, 2021.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan, Mann, Askell, Bai, Chen, et~al.]{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et~al.
\newblock In-context learning and induction heads.
\newblock \emph{arXiv preprint arXiv:2209.11895}, 2022.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{Open{AI} blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Murdock and Bennet(1962)]{Murdock1962}
Murdock and B~Bennet.
\newblock The serial position effect of free recall.
\newblock \emph{Journal of Experimental Psychology}, 64:\penalty0 482--488, 1962.

\bibitem[Howard and Kahana(2002)]{howard2002distributed}
Marc~W Howard and Michael~J Kahana.
\newblock A distributed representation of temporal context.
\newblock \emph{Journal of mathematical psychology}, 46\penalty0 (3):\penalty0 269--299, 2002.

\bibitem[Ratcliff and McKoon(1981)]{Ratcliff1981}
Roger Ratcliff and Gail McKoon.
\newblock Does activation really spread?
\newblock \emph{Psychological Review}, 88\penalty0 (5):\penalty0 454--462, 1981.
\newblock \doi{10.1037/0033-295x.88.5.454}.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann, Askell, Bai, Chen, Conerly, et~al.]{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et~al.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 1, 2021.

\bibitem[Srivastava et~al.(2015)Srivastava, Greff, and Schmidhuber]{srivastava2015highway}
Rupesh~Kumar Srivastava, Klaus Greff, and J{\"u}rgen Schmidhuber.
\newblock Highway networks.
\newblock \emph{arXiv preprint arXiv:1505.00387}, 2015.

\bibitem[Reddy(2023)]{reddy2023mechanistic}
Gautam Reddy.
\newblock The mechanistic basis of data dependence and abrupt learning in an in-context classification task.
\newblock \emph{arXiv preprint arXiv:2312.03002}, 2023.

\bibitem[Singh et~al.(2024)Singh, Moskovitz, Hill, Chan, and Saxe]{singh2024needs}
Aaditya~K Singh, Ted Moskovitz, Felix Hill, Stephanie~CY Chan, and Andrew~M Saxe.
\newblock What needs to go right for an induction head? a mechanistic study of in-context learning circuits and their formation.
\newblock \emph{arXiv preprint arXiv:2404.07129}, 2024.

\bibitem[Nanda(2022)]{nandatransformerlens2022}
Neel Nanda.
\newblock Transformerlens, 2022.
\newblock URL \url{https://github.com/neelnanda-io/TransformerLens}.

\bibitem[Polyn et~al.(2009)Polyn, Norman, and Kahana]{polyn2009cmr}
Sean~M Polyn, Kenneth~A Norman, and Michael~J Kahana.
\newblock A context maintenance and retrieval model of organizational processes in free recall.
\newblock \emph{Psychological review}, 116\penalty0 (1):\penalty0 129, 2009.

\bibitem[Lohnas et~al.(2015)Lohnas, Polyn, and Kahana]{lohnas2015cmr2}
Lynn Lohnas, Sean Polyn, and Michael Kahana.
\newblock Expanding the scope of memory search: Modeling intralist and interlist effects in free recall.
\newblock \emph{Psychological review}, 122:\penalty0 337--363, 04 2015.
\newblock \doi{10.1037/a0039036}.

\bibitem[Cohen and Kahana(2019)]{cohen2019cmr3}
Rivka~T Cohen and Michael~J. Kahana.
\newblock Retrieved-context theory of memory in emotional disorders.
\newblock \emph{bioRxiv}, page 817486, 2019.

\bibitem[Zhang et~al.(2023)Zhang, Griffiths, and Norman]{zhang2023}
Qiong Zhang, Thomas~L. Griffiths, and Kenneth~A. Norman.
\newblock Optimal policies for free recall.
\newblock \emph{Psychological Review}, 130\penalty0 (4):\penalty0 1104--1124, 2023.
\newblock \doi{10.1037/rev0000375}.

\bibitem[Wang et~al.(2022)Wang, Variengien, Conmy, Shlegeris, and Steinhardt]{wang2022interpretability}
Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.
\newblock Interpretability in the wild: a circuit for indirect object identification in gpt-2 small.
\newblock \emph{arXiv preprint arXiv:2211.00593}, 2022.

\bibitem[Bansal et~al.(2022)Bansal, Gopalakrishnan, Dingliwal, Bodapati, Kirchhoff, and Roth]{bansal2022}
Hritik Bansal, Karthik Gopalakrishnan, Saket Dingliwal, S.~Bodapati, Katrin Kirchhoff, and Dan Roth.
\newblock Rethinking the role of scale for in-context learning: An interpretability-based case study at 66 billion scale.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}, 2022.

\bibitem[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, et~al.]{bai2023qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al.
\newblock Qwen technical report.
\newblock \emph{arXiv preprint arXiv:2309.16609}, 2023.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan, et~al.]{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}, 2024.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[McDougall et~al.(2023)McDougall, Conmy, Rushing, McGrath, and Nanda]{mcdougall2023}
Callum McDougall, Arthur Conmy, Cody Rushing, Thomas McGrath, and Neel Nanda.
\newblock Copy suppression: Comprehensively understanding an attention head, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.04625}.

\bibitem[Stachenfeld et~al.(2017)Stachenfeld, Botvinick, and Gershman]{stachenfeld2017hippocampus}
Kimberly~L Stachenfeld, Matthew~M Botvinick, and Samuel~J Gershman.
\newblock The hippocampus as a predictive map.
\newblock \emph{Nature neuroscience}, 20\penalty0 (11):\penalty0 1643--1653, 2017.

\bibitem[Schapiro et~al.(2017)Schapiro, Turk-Browne, Botvinick, and Norman]{schapiro2017complementary}
Anna~C Schapiro, Nicholas~B Turk-Browne, Matthew~M Botvinick, and Kenneth~A Norman.
\newblock Complementary learning systems within the hippocampus: a neural network modelling approach to reconciling episodic memory with statistical learning.
\newblock \emph{Philosophical Transactions of the Royal Society B: Biological Sciences}, 372\penalty0 (1711):\penalty0 20160049, 2017.

\bibitem[Ji-An et~al.(2023)Ji-An, Stefanini, Benna, and Fusi]{ji2023face}
Li~Ji-An, Fabio Stefanini, Marcus~K Benna, and Stefano Fusi.
\newblock Face familiarity detection with complex synapses.
\newblock \emph{Iscience}, 26\penalty0 (1), 2023.

\bibitem[Sakon and Kahana(2022)]{sakon2022hippocampal}
John~J Sakon and Michael~J Kahana.
\newblock Hippocampal ripples signal contextually mediated episodic recall.
\newblock \emph{Proceedings of the National Academy of Sciences}, 119\penalty0 (40):\penalty0 e2201657119, 2022.

\bibitem[Dimsdale-Zucker et~al.(2020)Dimsdale-Zucker, Ritchey, Ekstrom, Yonelinas, and Ranganath]{DimsdaleZucker2020}
Halle~R. Dimsdale-Zucker, Maureen Ritchey, Arne~D. Ekstrom, Andrew~P. Yonelinas, and Charan Ranganath.
\newblock Ca1 and ca3 differentially support spontaneous retrieval of episodic contexts within human hippocampal subfields.
\newblock \emph{Nature Communications}, 2020.

\bibitem[Howard et~al.(2005)Howard, Fotedar, Datey, and Hasselmo]{howard2005temporal}
Marc~W Howard, Mrigankka~S Fotedar, Aditya~V Datey, and Michael~E Hasselmo.
\newblock The temporal context model in spatial navigation and relational learning: toward a common explanation of medial temporal lobe function across domains.
\newblock \emph{Psychological review}, 112\penalty0 (1):\penalty0 75, 2005.

\bibitem[Salvatore and Zhang(2024)]{Salvatore2024}
Nikolaus Salvatore and Qiong Zhang.
\newblock Parallels between neural machine translation and human memory search: A cognitive modeling approach.
\newblock In \emph{Proceedings of the Annual Meeting of the Cognitive Science Society}, 2024.

\bibitem[Li et~al.(2024)Li, Jensen, and Mattar]{li2024neural}
Moufan Li, Kristopher~T Jensen, and Marcelo~G Mattar.
\newblock A neural network model trained on free recall learns the method of loci.
\newblock In \emph{Proceedings of the Annual Meeting of the Cognitive Science Society}, volume~46, 2024.

\bibitem[Giallanza et~al.(2024{\natexlab{a}})Giallanza, Campbell, and Cohen]{giallanza2024toward}
Tyler Giallanza, Declan Campbell, and Jonathan~D Cohen.
\newblock Toward the emergence of intelligent control: Episodic generalization and optimization.
\newblock \emph{Open Mind}, 8:\penalty0 688--722, 2024{\natexlab{a}}.

\bibitem[Kraus et~al.(2015)Kraus, Brandon, Robinson, Connerney, Hasselmo, and Eichenbaum]{kraus2015during}
Benjamin~J Kraus, Mark~P Brandon, Robert~J Robinson, Michael~A Connerney, Michael~E Hasselmo, and Howard Eichenbaum.
\newblock During running in place, grid cells integrate elapsed time and distance run.
\newblock \emph{Neuron}, 88\penalty0 (3):\penalty0 578--589, 2015.

\bibitem[Lu et~al.(2024)Lu, Hummos, and Norman]{Lu2024}
Qihong Lu, Ali Hummos, and Kenneth Norman.
\newblock Episodic memory supports the acquisition of structured task representations, 2024.

\bibitem[Giallanza et~al.(2024{\natexlab{b}})Giallanza, Campbell, and Cohen]{Giallanza2024}
Tyler Giallanza, Declan Campbell, and Jonathan~D. Cohen.
\newblock {Toward the Emergence of Intelligent Control: Episodic Generalization and Optimization}.
\newblock \emph{Open Mind}, 8:\penalty0 688--722, 2024{\natexlab{b}}.
\newblock ISSN 2470-2986.
\newblock \doi{10.1162/opmi_a_00143}.
\newblock URL \url{https://doi.org/10.1162/opmi\_a\_00143}.

\bibitem[Kumaran and McClelland(2012)]{Kumaran2012}
Dharshan Kumaran and James~L. McClelland.
\newblock Generalization through the recurrent interaction of episodic memories.
\newblock \emph{Psychological Review}, 119:\penalty0 573 -- 616, 2012.

\bibitem[Zhou et~al.(2023)Zhou, Talmi, Daw, and Mattar]{zhou2023tcmsr}
Corey~Y. Zhou, Deborah Talmi, Nathaniel~D. Daw, and Marcelo~G. Mattar.
\newblock Episodic retrieval for model-based evaluation in sequential decision tasks, 2023.

\bibitem[Gershman and Daw(2017)]{Gershman2017}
Samuel~J. Gershman and Nathaniel~D. Daw.
\newblock Reinforcement learning and episodic memory in humans and animals: An integrative framework.
\newblock \emph{Annual Review of Psychology}, 68:\penalty0 101–128, 2017.

\bibitem[Pal et~al.(2023)Pal, Sun, Yuan, Wallace, and Bau]{Pal2023}
Koyena Pal, Jiuding Sun, Andrew Yuan, Byron Wallace, and David Bau.
\newblock Future lens: Anticipating subsequent tokens from a single hidden state.
\newblock In Jing Jiang, David Reitter, and Shumin Deng, editors, \emph{Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)}, pages 548--560. Association for Computational Linguistics, 2023.
\newblock \doi{10.18653/v1/2023.conll-1.37}.
\newblock URL \url{https://aclanthology.org/2023.conll-1.37}.

\bibitem[Liu et~al.(2024)Liu, Lin, Hewitt, Paranjape, Bevilacqua, Petroni, and Liang]{liu2024lost}
Nelson~F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.
\newblock Lost in the middle: How language models use long contexts.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 12:\penalty0 157--173, 2024.

\bibitem[Murphy et~al.(2021)Murphy, Friedman, and Castel]{Murphy2021}
Dillon Murphy, Michael Friedman, and Alan Castel.
\newblock Metacognitive control, serial position effects, and effective transfer to self-paced study.
\newblock \emph{Memory \& Cognition}, 50:\penalty0 144--159, 2021.
\newblock \doi{10.3758/s13421-021-01204-y}.

\bibitem[Morton and Polyn(2016)]{morton2016}
Neal~W Morton and Sean~M Polyn.
\newblock A predictive framework for evaluating models of semantic organization in free recall.
\newblock \emph{Journal of memory and language}, 86:\penalty0 119--140, 2016.
\newblock \doi{10.1016/j.jml.2015.10.002}.

\bibitem[Naim et~al.(2019)Naim, Katkov, Recanatesi, and Tsodyks]{Naim2019}
Michelangelo Naim, Mikhail Katkov, Stefano Recanatesi, and Michail Tsodyks.
\newblock Emergence of hierarchical organization in memory for random material.
\newblock \emph{Scientific Reports}, 9:\penalty0 10448, 07 2019.
\newblock \doi{10.1038/s41598-019-46908-z}.

\bibitem[Baldassano et~al.(2016)Baldassano, Chen, Zadbood, Pillow, Hasson, and Norman]{Baldassano2016}
Christopher~A. Baldassano, Janice Chen, Asieh Zadbood, Jonathan~W. Pillow, Uri Hasson, and Kenneth~A. Norman.
\newblock Discovering event structure in continuous narrative perception and memory.
\newblock \emph{Neuron}, 95:\penalty0 709--721.e5, 2016.

\bibitem[Kozachkov et~al.(2023)Kozachkov, Kastanenka, and Krotov]{kozachkov2023building}
Leo Kozachkov, Ksenia~V Kastanenka, and Dmitry Krotov.
\newblock Building transformers from neurons and astrocytes.
\newblock \emph{Proceedings of the National Academy of Sciences}, 120\penalty0 (34):\penalty0 e2219150120, 2023.

\bibitem[Lee et~al.()Lee, Jiang, and Berg-Kirkpatrick]{leeattention}
Ivan Lee, Nan Jiang, and Taylor Berg-Kirkpatrick.
\newblock Is attention required for icl? exploring the relationship between model architecture and in-context learning ability.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[Crosbie and Shutova(2024)]{crosbie2024induction}
Joy Crosbie and Ekaterina Shutova.
\newblock Induction heads as an essential mechanism for pattern matching in in-context learning.
\newblock \emph{arXiv preprint arXiv:2407.07011}, 2024.

\end{thebibliography}
