\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{Ba2016}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv:1607.06450}, 2016.

\bibitem[Bao et~al.(2022)Bao, Li, Cao, and Zhu]{bao2022all}
Fan Bao, Chongxuan Li, Yue Cao, and Jun Zhu.
\newblock All are worth words: a vit backbone for score-based diffusion models.
\newblock In \emph{NeurIPS 2022 Workshop on Score-Based Methods}, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{Brown2020}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Chang et~al.(2022)Chang, Zhang, Jiang, Liu, and Freeman]{Chang2022}
Huiwen Chang, Han Zhang, Lu~Jiang, Ce~Liu, and William~T Freeman.
\newblock {MaskGIT}: Masked generative image {Transformer}.
\newblock In \emph{CVPR}, 2022.

\bibitem[Chang et~al.(2023)Chang, Zhang, Barber, Maschinot, Lezama, Jiang,
  Yang, Murphy, Freeman, Rubinstein, Li, and Krishnan]{Chang2023}
Huiwen Chang, Han Zhang, Jarred Barber, AJ~Maschinot, Jose Lezama, Lu~Jiang,
  Ming-Hsuan Yang, Kevin Murphy, William~T Freeman, Michael Rubinstein,
  Yuanzhen Li, and Dilip Krishnan.
\newblock Muse: Text-to-image generation via masked generative {Transformers}.
\newblock In \emph{ICML}, 2023.

\bibitem[Chen et~al.(2020)Chen, Radford, Child, Wu, Jun, Luan, and
  Sutskever]{Chen2020}
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and
  Ilya Sutskever.
\newblock Generative pretraining from pixels.
\newblock In \emph{ICML}, 2020.

\bibitem[Chen et~al.(2018)Chen, Mishra, Rohaninejad, and
  Abbeel]{chen2018pixelsnail}
Xi~Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel.
\newblock {PixelSNAIL}: An improved autoregressive generative model.
\newblock In \emph{ICML}, 2018.

\bibitem[Chi et~al.(2023)Chi, Feng, Du, Xu, Cousineau, Burchfiel, and
  Song]{Chi2023}
Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin
  Burchfiel, and Shuran Song.
\newblock Diffusion policy: Visuomotor policy learning via action diffusion.
\newblock In \emph{RSS}, 2023.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock {ImageNet}: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Dhariwal \& Nichol(2021)Dhariwal and Nichol]{Dhariwal2021}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat {GANs} on image synthesis.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{Dosovitskiy2021}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: {Transformers} for image recognition
  at scale.
\newblock In \emph{ICLR}, 2021.

\bibitem[Elfwing et~al.(2018)Elfwing, Uchibe, and Doya]{elfwing2018sigmoid}
Stefan Elfwing, Eiji Uchibe, and Kenji Doya.
\newblock Sigmoid-weighted linear units for neural network function
  approximation in reinforcement learning.
\newblock \emph{Neural networks}, 2018.

\bibitem[Esser et~al.(2021)Esser, Rombach, and Ommer]{Esser2021}
Patrick Esser, Robin Rombach, and Bjorn Ommer.
\newblock Taming {Transformers} for high-resolution image synthesis.
\newblock In \emph{CVPR}, 2021.

\bibitem[Gao et~al.(2023)Gao, Zhou, Cheng, and Yan]{Gao2023}
Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan.
\newblock Masked diffusion {Transformer} is a strong image synthesizer.
\newblock In \emph{ICCV}, 2023.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian~J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{NeurIPS}, 2014.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{Goyal2017}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch {SGD}: Training {ImageNet} in 1 hour.
\newblock \emph{arXiv:1706.02677}, 2017.

\bibitem[Gregor et~al.(2014)Gregor, Danihelka, Mnih, Blundell, and
  Wierstra]{Gregor2014}
Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra.
\newblock Deep autoregressive networks.
\newblock In \emph{ICML}, 2014.

\bibitem[Gumbel(1954)]{gumbel1954statistical}
Emil~Julius Gumbel.
\newblock Statistical theory of extreme valuse and some practical applications.
\newblock \emph{Nat. Bur. Standards Appl. Math. Ser. 33}, 1954.

\bibitem[Hatamizadeh et~al.(2023)Hatamizadeh, Song, Liu, Kautz, and
  Vahdat]{hatamizadeh2023diffit}
Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat.
\newblock Diffi{T}: Diffusion vision {Transformers} for image generation.
\newblock \emph{arXiv:2312.02139}, 2023.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{He2016}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and
  Girshick]{he2022masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{CVPR}, 2022.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{Heusel2017}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock {GANs} trained by a two time-scale update rule converge to a local
  nash equilibrium.
\newblock In \emph{NIP}, 2017.

\bibitem[Ho \& Salimans(2022)Ho and Salimans]{ho2022classifier}
Jonathan Ho and Tim Salimans.
\newblock Classifier-free diffusion guidance.
\newblock \emph{arXiv:2207.12598}, 2022.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{Ho2020}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Karras et~al.(2023)Karras, Aittala, Lehtinen, Hellsten, Aila, and
  Laine]{karras2023analyzing}
Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and
  Samuli Laine.
\newblock Analyzing and improving the training dynamics of diffusion models.
\newblock \emph{arXiv:2312.02696}, 2023.

\bibitem[Kingma \& Gao(2023)Kingma and Gao]{Kingma2023}
Diederik Kingma and Ruiqi Gao.
\newblock Understanding diffusion objectives as the {ELBO} with simple data
  augmentation.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Kolesnikov et~al.(2022)Kolesnikov, Susano~Pinto, Beyer, Zhai, Harmsen,
  and Houlsby]{Kolesnikov2022}
Alexander Kolesnikov, Andr{\'e} Susano~Pinto, Lucas Beyer, Xiaohua Zhai,
  Jeremiah Harmsen, and Neil Houlsby.
\newblock {UViM}: A unified modeling approach for vision with learned guiding
  codes.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Krasin et~al.(2017)Krasin, Duerig, Alldrin, Ferrari, Abu-El-Haija,
  Kuznetsova, Rom, Uijlings, Popov, Veit, Belongie, Gomes, Gupta, Sun, Chechik,
  Cai, Feng, Narayanan, and Murphy]{Krasin2017}
Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija,
  Alina Kuznetsova, Hassan Rom, Jasper Uijlings, Stefan Popov, Andreas Veit,
  Serge Belongie, Victor Gomes, Abhinav Gupta, Chen Sun, Gal Chechik, David
  Cai, Zheyun Feng, Dhyanesh Narayanan, and Kevin Murphy.
\newblock Openimages: A public dataset for large-scale multi-label and
  multi-class image classification.
\newblock 2017.

\bibitem[Li et~al.(2023)Li, Chang, Mishra, Zhang, Katabi, and Krishnan]{Li2023}
Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip
  Krishnan.
\newblock {MAGE}: Masked generative encoder to unify representation learning
  and image synthesis.
\newblock In \emph{CVPR}, 2023.

\bibitem[Li et~al.(2024)Li, Bornschein, and Chen]{li2024denoising}
Yazhe Li, Jorg Bornschein, and Ting Chen.
\newblock Denoising autoregressive representation learning.
\newblock \emph{arXiv preprint arXiv:2403.05196}, 2024.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{Loshchilov2019}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{ICLR}, 2019.

\bibitem[Mentzer et~al.(2024)Mentzer, Minnen, Agustsson, and
  Tschannen]{Mentzer2024}
Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen.
\newblock Finite scalar quantization: {VQ-VAE} made simple.
\newblock In \emph{ICLR}, 2024.

\bibitem[Nichol \& Dhariwal(2021)Nichol and Dhariwal]{Nichol2021}
Alexander~Quinn Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock In \emph{ICML}, 2021.

\bibitem[{Octo Model Team} et~al.(2024){Octo Model Team}, Ghosh, Walke,
  Pertsch, Black, Mees, Dasari, Hejna, Xu, Luo, Kreiman, Tan, Sanketi, Vuong,
  Xiao, Sadigh, Finn, and Levine]{OMT2024}
{Octo Model Team}, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier
  Mees, Sudeep Dasari, Joey Hejna, Charles Xu, Jianlan Luo, Tobias Kreiman,
  {You Liang} Tan, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea
  Finn, and Sergey Levine.
\newblock Octo: An open-source generalist robot policy.
\newblock In \emph{RSS}, 2024.

\bibitem[{OpenAI}(2024)]{OpenAI2024}
{OpenAI}.
\newblock {Consistency Decoder}, 2024.
\newblock URL \url{https://github.com/openai/consistencydecoder}.

\bibitem[Parmar et~al.(2018)Parmar, Vaswani, Uszkoreit, Kaiser, Shazeer, Ku,
  and Tran]{parmar2018image}
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer,
  Alexander Ku, and Dustin Tran.
\newblock Image {Transformer}.
\newblock In \emph{ICML}, 2018.

\bibitem[Peebles \& Xie(2023)Peebles and Xie]{Peebles2023}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with {Transformers}.
\newblock In \emph{ICCV}, 2023.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{Radford2018}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{Radford2019}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{Ramesh2021}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In \emph{ICML}, 2021.

\bibitem[Razavi et~al.(2019)Razavi, Van~den Oord, and Vinyals]{Razavi2019}
Ali Razavi, Aaron Van~den Oord, and Oriol Vinyals.
\newblock Generating diverse high-fidelity images with {VQ-VAE-2}.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and
  Ommer]{Rombach2022}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{CVPR}, 2022.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{Salimans2016}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen.
\newblock Improved techniques for training {GANs}.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Shazeer(2019)]{Shazeer2019}
Noam Shazeer.
\newblock Fast {Transformer} decoding: One write-head is all you need.
\newblock \emph{arXiv:1911.02150}, 2019.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and
  Ganguli]{SohlDickstein2015}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{ICML}, 2015.

\bibitem[Song \& Ermon(2019)Song and Ermon]{Song2019}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Song et~al.(2021)Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and
  Poole]{Song2021}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock In \emph{ICLR}, 2021.

\bibitem[Tschannen et~al.(2023)Tschannen, Eastwood, and
  Mentzer]{tschannen2023givt}
Michael Tschannen, Cian Eastwood, and Fabian Mentzer.
\newblock {GIVT}: Generative infinite-vocabulary {Transformers}.
\newblock \emph{arXiv:2312.02116}, 2023.

\bibitem[van~den Oord et~al.(2016{\natexlab{a}})van~den Oord, Kalchbrenner,
  Espeholt, Vinyals, Graves, and Kavukcuoglu]{Oord2016}
Aaron van~den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex
  Graves, and Koray Kavukcuoglu.
\newblock Conditional image generation with {PixelCNN} decoders.
\newblock In \emph{NeurIPS}, 2016{\natexlab{a}}.

\bibitem[van~den Oord et~al.(2016{\natexlab{b}})van~den Oord, Kalchbrenner, and
  Kavukcuoglu]{Oord2016a}
Aaron van~den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu.
\newblock Pixel recurrent neural networks.
\newblock In \emph{ICML}, 2016{\natexlab{b}}.

\bibitem[van~den Oord et~al.(2017)van~den Oord, Vinyals, and
  Kavukcuoglu]{Oord2017}
Aaron van~den Oord, Oriol Vinyals, and Koray Kavukcuoglu.
\newblock Neural discrete representation learning.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{Vaswani2017}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Wei et~al.(2023)Wei, Mangalam, Huang, Li, Fan, Xu, Wang, Xie, Yuille,
  and Feichtenhofer]{wei2023diffusion}
Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao Li, Haoqi Fan, Hu~Xu,
  Huiyu Wang, Cihang Xie, Alan Yuille, and Christoph Feichtenhofer.
\newblock Diffusion models as masked autoencoders.
\newblock In \emph{ICCV}, 2023.

\bibitem[Yu et~al.(2023)Yu, Cheng, Sohn, Lezama, Zhang, Chang, Hauptmann, Yang,
  Hao, Essa, and Jiang]{Yu2023}
Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos{\'e} Lezama, Han Zhang, Huiwen Chang,
  Alexander~G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, and Lu~Jiang.
\newblock {MAGVIT}: Masked generative video {Transformer}.
\newblock In \emph{CVPR}, 2023.

\bibitem[Yu et~al.(2024)Yu, Lezama, Gundavarapu, Versari, Sohn, Minnen, Cheng,
  Gupta, Gu, Hauptmann, Gong, Yang, Irfan~Essa, and Jiang]{Yu2024}
Lijun Yu, Jos{\'e} Lezama, Nitesh~B Gundavarapu, Luca Versari, Kihyuk Sohn,
  David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander~G Hauptmann,
  Boqing Gong, Ming-Hsuan Yang, David A.~Ross Irfan~Essa, and Lu~Jiang.
\newblock Language model beats diffusion--tokenizer is key to visual
  generation.
\newblock In \emph{ICLR}, 2024.

\bibitem[Zhang et~al.(2018)Zhang, Isola, Efros, Shechtman, and Wang]{Zhang2018}
Richard Zhang, Phillip Isola, Alexei~A Efros, Eli Shechtman, and Oliver Wang.
\newblock The unreasonable effectiveness of deep features as a perceptual
  metric.
\newblock In \emph{CVPR}, 2018.

\end{thebibliography}
