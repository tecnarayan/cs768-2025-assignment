\begin{thebibliography}{10}

\bibitem{lecun95hbtnn}
Yann LeCun, Yoshua Bengio, et~al.
\newblock Convolutional networks for images, speech, and time series.
\newblock {\em The handbook of brain theory and neural networks},
  3361(10):1995, 1995.

\bibitem{goodfellow16book}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock {\em Deep Learning}.
\newblock MIT Press, 2016.
\newblock \url{http://www.deeplearningbook.org}.

\bibitem{salakhutdinov09aistats}
Ruslan Salakhutdinov and Geoffrey Hinton.
\newblock Deep boltzmann machines.
\newblock In {\em Proceedings of the Twelth International Conference on
  Artificial Intelligence and Statistics}, volume~5 of {\em Proceedings of
  Machine Learning Research}, pages 448--455, Hilton Clearwater Beach Resort,
  Clearwater Beach, Florida USA, 2009. PMLR.

\bibitem{kingma14iclr}
Diederik~P. Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock In {\em 2nd International Conference on Learning Representations,
  {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track
  Proceedings}, 2014.

\bibitem{goodfellow20acm}
Ian~J. Goodfellow, Jean Pouget{-}Abadie, Mehdi Mirza, Bing Xu, David
  Warde{-}Farley, Sherjil Ozair, Aaron~C. Courville, and Yoshua Bengio.
\newblock Generative adversarial networks.
\newblock {\em Commun. {ACM}}, 63(11):139--144, 2020.

\bibitem{rezende15icml}
Danilo~Jimenez Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In {\em Proceedings of the 32nd International Conference on Machine
  Learning, {ICML} 2015, Lille, France, 6-11 July 2015}, volume~37 of {\em
  {JMLR} Workshop and Conference Proceedings}, pages 1530--1538. JMLR.org,
  2015.

\bibitem{vanoord16icml}
Aaron Van~Oord, Nal Kalchbrenner, and Koray Kavukcuoglu.
\newblock Pixel recurrent neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1747--1756. PMLR, 2016.

\bibitem{bardenet17jmlr}
R{\'{e}}mi Bardenet, Arnaud Doucet, and Christopher~C. Holmes.
\newblock On markov chain monte carlo methods for tall data.
\newblock {\em J. Mach. Learn. Res.}, 18:47:1--47:43, 2017.

\bibitem{hinton02nc}
Geoffrey~E. Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock {\em Neural Comput.}, 14(8):1771--1800, 2002.

\bibitem{neal11book}
Radford~M Neal et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock {\em Handbook of markov chain monte carlo}, 2(11):2, 2011.

\bibitem{salimans17iclr}
Tim Salimans, Andrej Karpathy, Xi~Chen, and Diederik~P. Kingma.
\newblock Pixelcnn++: Improving the pixelcnn with discretized logistic mixture
  likelihood and other modifications.
\newblock In {\em 5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}, 2017.

\bibitem{sohl-dickstein15icml}
Jascha Sohl{-}Dickstein, Eric~A. Weiss, Niru Maheswaranathan, and Surya
  Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In {\em Proceedings of the 32nd International Conference on Machine
  Learning, {ICML} 2015, Lille, France, 6-11 July 2015}, volume~37 of {\em
  {JMLR} Workshop and Conference Proceedings}, pages 2256--2265. JMLR.org,
  2015.

\bibitem{ho20nips}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In {\em Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem{song20arxiv}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock {\em CoRR}, abs/2010.02502, 2020.

\bibitem{radford16iclr}
Alec Radford, Luke Metz, and Soumith Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock In {\em 4th International Conference on Learning Representations,
  {ICLR} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track
  Proceedings}, 2016.

\bibitem{lucas19nips}
Thomas Lucas, Konstantin Shmelkov, Karteek Alahari, Cordelia Schmid, and Jakob
  Verbeek.
\newblock Adaptive density estimation for generative models.
\newblock In {\em Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pages 11993--12003, 2019.

\bibitem{dinh14iclr}
Laurent Dinh, David Krueger, and Yoshua Bengio.
\newblock {NICE:} non-linear independent components estimation.
\newblock In {\em 3rd International Conference on Learning Representations,
  {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings},
  2015.

\bibitem{dinh17iclr}
Laurent Dinh, Jascha Sohl{-}Dickstein, and Samy Bengio.
\newblock Density estimation using real {NVP}.
\newblock In {\em 5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}, 2017.

\bibitem{kingma19nips}
Diederik~P. Kingma and Prafulla Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In {\em Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
  December 3-8, 2018, Montr{\'{e}}al, Canada}, pages 10236--10245, 2018.

\bibitem{jacobsen18iclr}
J{\"{o}}rn{-}Henrik Jacobsen, Arnold W.~M. Smeulders, and Edouard Oyallon.
\newblock i-revnet: Deep invertible networks.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}, 2018.

\bibitem{ren19nips}
Jie Ren, Peter~J. Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark~A.
  DePristo, Joshua~V. Dillon, and Balaji Lakshminarayanan.
\newblock Likelihood ratios for out-of-distribution detection.
\newblock In {\em Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pages 14680--14691, 2019.

\bibitem{nalisnick19icml}
Eric~T. Nalisnick, Akihiro Matsukawa, Yee~Whye Teh, Dilan G{\"{o}}r{\"{u}}r,
  and Balaji Lakshminarayanan.
\newblock Hybrid models with deep and invertible features.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  volume~97 of {\em Proceedings of Machine Learning Research}, pages
  4723--4732. {PMLR}, 2019.

\bibitem{muller19acm}
Thomas M{\"{u}}ller, Brian McWilliams, Fabrice Rousselle, Markus Gross, and Jan
  Nov{\'{a}}k.
\newblock Neural importance sampling.
\newblock {\em {ACM} Trans. Graph.}, 38(5):145:1--145:19, 2019.

\bibitem{chen20icml}
Jianfei Chen, Cheng Lu, Biqi Chenli, Jun Zhu, and Tian Tian.
\newblock Vflow: More expressive generative flows with variational data
  augmentation.
\newblock In {\em International Conference on Machine Learning}, pages
  1660--1669. PMLR, 2020.

\bibitem{tan19icml}
Mingxing Tan and Quoc~V. Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  volume~97 of {\em Proceedings of Machine Learning Research}, pages
  6105--6114. {PMLR}, 2019.

\bibitem{huang20arxiv}
Chin-Wei Huang, Laurent Dinh, and Aaron Courville.
\newblock Augmented normalizing flows: Bridging the gap between generative
  flows and latent variable models.
\newblock {\em arXiv preprint arXiv:2002.07101}, 2020.

\bibitem{ho19icml}
Jonathan Ho, Xi~Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel.
\newblock Flow++: Improving flow-based generative models with variational
  dequantization and architecture design.
\newblock In {\em International Conference on Machine Learning}, pages
  2722--2730. PMLR, 2019.

\bibitem{xiong21arxiv}
Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung,
  Yin Li, and Vikas Singh.
\newblock Nystr{\"{o}}mformer: {A} nystr{\"{o}}m-based algorithm for
  approximating self-attention.
\newblock {\em CoRR}, abs/2102.03902, 2021.

\bibitem{huang17cvpr}
Gao Huang, Zhuang Liu, Laurens van~der Maaten, and Kilian~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em 2017 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017}, pages
  2261--2269. {IEEE} Computer Society, 2017.

\bibitem{ioffe15icml}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em Proceedings of the 32nd International Conference on Machine
  Learning, {ICML} 2015, Lille, France, 6-11 July 2015}, volume~37 of {\em
  {JMLR} Workshop and Conference Proceedings}, pages 448--456. JMLR.org, 2015.

\bibitem{vaswani17nips}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
  Long Beach, CA, {USA}}, pages 5998--6008, 2017.

\bibitem{wang18cvpr}
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.
\newblock Non-local neural networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7794--7803, 2018.

\bibitem{dosovitskiy20arxiv}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em CoRR}, abs/2010.11929, 2020.

\bibitem{nielsen20nips}
Didrik Nielsen, Priyank Jaini, Emiel Hoogeboom, Ole Winther, and Max Welling.
\newblock Survae flows: Surjections to bridge the gap between vaes and flows.
\newblock In {\em Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem{krizhevsky12}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock {\em University of Toronto}, 05 2012.

\bibitem{imagenet15ijcv}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
  Alexander~C. Berg, and Li~Fei-Fei.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock {\em International Journal of Computer Vision (IJCV)},
  115(3):211--252, 2015.

\bibitem{liu15iccv}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In {\em Proceedings of International Conference on Computer Vision
  (ICCV)}, December 2015.

\bibitem{parmar18icml}
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer,
  Alexander Ku, and Dustin Tran.
\newblock Image transformer.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, volume~80 of {\em Proceedings of Machine Learning Research}, pages
  4052--4061. {PMLR}, 2018.

\bibitem{roy21tacl}
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock {\em Trans. Assoc. Comput. Linguistics}, 9:53--68, 2021.

\bibitem{gregor16nips}
Karol Gregor, Frederic Besse, Danilo~Jimenez Rezende, Ivo Danihelka, and Daan
  Wierstra.
\newblock Towards conceptual compression.
\newblock In {\em Advances in Neural Information Processing Systems 29: Annual
  Conference on Neural Information Processing Systems 2016, December 5-10,
  2016, Barcelona, Spain}, pages 3549--3557, 2016.

\bibitem{vahdat18icml}
Arash Vahdat, William~G. Macready, Zhengbing Bian, Amir Khoshaman, and Evgeny
  Andriyash.
\newblock {DVAE++:} discrete variational autoencoders with overlapping
  transformations.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, volume~80 of {\em Proceedings of Machine Learning Research}, pages
  5042--5051. {PMLR}, 2018.

\bibitem{kingma16nips}
Diederik~P Kingma, Tim Salimans, Rafal Jozefowicz, Xi~Chen, Ilya Sutskever, and
  Max Welling.
\newblock Improving variational inference with inverse autoregressive flow.
\newblock {\em arXiv preprint arXiv:1606.04934}, 2016.

\bibitem{maaloe19nips}
Lars Maal{\o}e, Marco Fraccaro, Valentin Li{\'{e}}vin, and Ole Winther.
\newblock {BIVA:} {A} very deep hierarchy of latent variables for generative
  modeling.
\newblock In {\em Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pages 6548--6558, 2019.

\bibitem{samarth21arxiv}
Samarth Sinha and Adji~B. Dieng.
\newblock Consistency regularization for variational auto-encoders.
\newblock {\em CoRR}, abs/2105.14859, 2021.

\bibitem{dongjun21arxiv}
Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il{-}Chul Moon.
\newblock Score matching model for unbounded data score.
\newblock {\em CoRR}, abs/2106.05527, 2021.

\bibitem{nichol21arxiv}
Alex Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock {\em CoRR}, abs/2102.09672, 2021.

\bibitem{kingma21arxiv}
Diederik~P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho.
\newblock Variational diffusion models.
\newblock {\em CoRR}, abs/2107.00630, 2021.

\bibitem{oord16nips}
A{\"{a}}ron van~den Oord, Nal Kalchbrenner, Lasse Espeholt, Koray Kavukcuoglu,
  Oriol Vinyals, and Alex Graves.
\newblock Conditional image generation with pixelcnn decoders.
\newblock In {\em Advances in Neural Information Processing Systems 29: Annual
  Conference on Neural Information Processing Systems 2016, December 5-10,
  2016, Barcelona, Spain}, pages 4790--4798, 2016.

\bibitem{chen18icml}
Xi~Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel.
\newblock Pixelsnail: An improved autoregressive generative model.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, volume~80 of {\em Proceedings of Machine Learning Research}, pages
  863--871. {PMLR}, 2018.

\bibitem{menick19iclr}
Jacob Menick and Nal Kalchbrenner.
\newblock Generating high fidelity images with subscale pixel networks and
  multidimensional upscaling.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}, 2019.

\bibitem{yu20nips}
Jason~J. Yu, Konstantinos~G. Derpanis, and Marcus~A. Brubaker.
\newblock Wavelet flow: Fast training of high resolution normalizing flows.
\newblock In {\em Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem{chen19nips}
Tian~Qi Chen, Jens Behrmann, David Duvenaud, and J{\"{o}}rn{-}Henrik Jacobsen.
\newblock Residual flows for invertible generative modeling.
\newblock In {\em Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pages 9913--9923, 2019.

\bibitem{perugachi21arxiv}
Yura Perugachi-Diaz, Jakub~M Tomczak, and Sandjai Bhulai.
\newblock Invertible densenets with concatenated lipswish.
\newblock {\em arXiv preprint arXiv:2102.02694}, 2021.

\bibitem{bhattacharyya20cvpr}
Apratim Bhattacharyya, Shweta Mahajan, Mario Fritz, Bernt Schiele, and Stefan
  Roth.
\newblock Normalizing flows with multi-scale autoregressive priors.
\newblock In {\em 2020 {IEEE/CVF} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2020, Seattle, WA, USA, June 13-19, 2020}, pages
  8412--8421. Computer Vision Foundation / {IEEE}, 2020.

\bibitem{ma19nips}
Xuezhe Ma, Xiang Kong, Shanghang Zhang, and Eduard~H. Hovy.
\newblock Macow: Masked convolutional generative flow.
\newblock In {\em Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pages 5891--5900, 2019.

\bibitem{vahdat20nips}
Arash Vahdat and Jan Kautz.
\newblock {NVAE:} {A} deep hierarchical variational autoencoder.
\newblock In {\em Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem{sadeghi19arxiv}
Hossein Sadeghi, Evgeny Andriyash, Walter Vinci, Lorenzo Buffoni, and
  Mohammad~H. Amin.
\newblock Pixelvae++: Improved pixelvae with discrete prior.
\newblock {\em CoRR}, abs/1908.09948, 2019.

\bibitem{razavi19iclr}
Ali Razavi, A{\"{a}}ron van~den Oord, Ben Poole, and Oriol Vinyals.
\newblock Preventing posterior collapse with delta-vaes.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}, 2019.

\bibitem{heusel17nips}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock In {\em Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
  Long Beach, CA, {USA}}, pages 6626--6637, 2017.

\bibitem{ostrovski18icml}
Georg Ostrovski, Will Dabney, and R{\'{e}}mi Munos.
\newblock Autoregressive quantile networks for generative modeling.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, volume~80 of {\em Proceedings of Machine Learning Research}, pages
  3933--3942. {PMLR}, 2018.

\bibitem{behrmann19icml}
Jens Behrmann, Will Grathwohl, Ricky~TQ Chen, David Duvenaud, and
  J{\"o}rn-Henrik Jacobsen.
\newblock Invertible residual networks.
\newblock In {\em International Conference on Machine Learning}, pages
  573--582. PMLR, 2019.

\bibitem{zhao20nips}
Shengyu Zhao, Zhijian Liu, Ji~Lin, Jun{-}Yan Zhu, and Song Han.
\newblock Differentiable augmentation for data-efficient {GAN} training.
\newblock In {\em Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem{gulrajani17nips}
Ishaan Gulrajani, Faruk Ahmed, Mart{\'{\i}}n Arjovsky, Vincent Dumoulin, and
  Aaron~C. Courville.
\newblock Improved training of wasserstein gans.
\newblock In {\em Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
  Long Beach, CA, {USA}}, pages 5767--5777, 2017.

\bibitem{xiao20arxiv}
Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vahdat.
\newblock {VAEBM:} {A} symbiosis between variational autoencoders and
  energy-based models.
\newblock {\em CoRR}, abs/2010.00654, 2020.

\bibitem{perugachi20aabi}
Yura Perugachi-Diaz, Jakub~M. Tomczak, and Sandjai Bhulai.
\newblock Invertible densenets.
\newblock In {\em 3rd Symposium on Advances in Approximate Bayesian Inference},
  pages 1--11, 2020.

\bibitem{mishra18iclr}
Nikhil Mishra, Mostafa Rohaninejad, Xi~Chen, and Pieter Abbeel.
\newblock A simple neural attentive meta-learner.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}, 2018.

\bibitem{srinivasan21acm}
Ramya Srinivasan and Ajay Chander.
\newblock Biases in {AI} systems.
\newblock {\em Commun. {ACM}}, 64(8):44--49, 2021.

\bibitem{lacoste19arxiv}
Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres.
\newblock Quantifying the carbon emissions of machine learning.
\newblock {\em arXiv preprint arXiv:1910.09700}, 2019.

\end{thebibliography}
