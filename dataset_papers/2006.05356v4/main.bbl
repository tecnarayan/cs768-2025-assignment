\begin{thebibliography}{10}

\bibitem{Thompson1933}
William Thompson.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock {\em Biometrika}, 1933.

\bibitem{Shahriari2016}
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan~P. Adams, and Nando de~Freitas.
\newblock Taking the human out of the loop: A review of {B}ayesian
  optimization.
\newblock {\em Proceedings of the IEEE}, 2016.

\bibitem{Chowdhury2017bandit}
Sayak~Ray Chowdhury and Aditya Gopalan.
\newblock On kernelized multi-armed bandits.
\newblock In {\em International Conference on Machine Learning}, 2017.

\bibitem{baptista2018bayesian}
Ricardo Baptista and Matthias Poloczek.
\newblock Bayesian optimization of combinatorial structures.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{eriksson2021scalable}
David Eriksson and Matthias Poloczek.
\newblock Scalable constrained bayesian optimization.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, 2021.

\bibitem{daulton2019thompson}
Samuel Daulton, Shaun Singh, Vashist Avadhanula, Drew Dimmery, and Eytan
  Bakshy.
\newblock Thompson sampling for contextual bandit problems with auxiliary
  safety constraints.
\newblock {\em arXiv preprint arXiv:1911.00638}, 2019.

\bibitem{chevalier2013fast}
Cl{\'e}ment Chevalier and David Ginsbourger.
\newblock Fast computation of the multi-points expected improvement with
  applications in batch selection.
\newblock In {\em International Conference on Learning and Intelligent
  Optimization}, 2013.

\bibitem{gonzalez2016batch}
Javier Gonz{\'a}lez, Zhenwen Dai, Philipp Hennig, and Neil Lawrence.
\newblock Batch {B}ayesian optimization via local penalization.
\newblock In {\em Artificial intelligence and statistics}, 2016.

\bibitem{wu2016parallel}
Jian Wu and Peter~I Frazier.
\newblock The parallel knowledge gradient method for batch {B}ayesian
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, 2016.

\bibitem{moss2021gibbon}
Henry~B Moss, David~S Leslie, Javier Gonzalez, and Paul Rayson.
\newblock Gibbon: General-purpose information-based bayesian optimisation.
\newblock {\em arXiv preprint arXiv:2102.03324}, 2021.

\bibitem{jalali2017comparison}
Hamed Jalali, Inneke Van~Nieuwenhuyse, and Victor Picheny.
\newblock Comparison of kriging-based algorithms for simulation optimization
  with heterogeneous noise.
\newblock {\em European Journal of Operational Research}, 2017.

\bibitem{binois2019replication}
Micka{\"e}l Binois, Jiangeng Huang, Robert~B Gramacy, and Mike Ludkovski.
\newblock Replication or exploration? sequential design for stochastic
  simulation experiments.
\newblock {\em Technometrics}, 2019.

\bibitem{hernandez2017parallel}
Jos{\'e}~Miguel Hern{\'a}ndez-Lobato, James Requeima, Edward~O Pyzer-Knapp, and
  Al{\'a}n Aspuru-Guzik.
\newblock Parallel and distributed {T}hompson sampling for large-scale
  accelerated exploration of chemical space.
\newblock In {\em International Conference on Machine Learning}, 2017.

\bibitem{kandasamy2018parallelised}
Kirthevasan Kandasamy, Akshay Krishnamurthy, Jeff Schneider, and Barnab{\'a}s
  P{\'o}czos.
\newblock Parallelised {B}ayesian optimisation via {T}hompson sampling.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, 2018.

\bibitem{snoek2015scalable}
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish,
  Narayanan Sundaram, Mostofa Patwary, Mr~Prabhat, and Ryan Adams.
\newblock Scalable {B}ayesian optimization using deep neural networks.
\newblock In {\em International conference on machine learning}, pages
  2171--2180, 2015.

\bibitem{wang2018batched}
Zi~Wang, Clement Gehring, Pushmeet Kohli, and Stefanie Jegelka.
\newblock Batched large-scale {B}ayesian optimization in high-dimensional
  spaces.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, 2018.

\bibitem{Rasmussen2006}
Carl~E Rasmussen and Christopher~KI Williams.
\newblock {\em {Gaussian Processes for Machine Learning}}.
\newblock MIT Press, 2006.

\bibitem{diggle1998model}
Peter~J Diggle, Jonathan~A Tawn, and Rana~A Moyeed.
\newblock Model-based geostatistics.
\newblock {\em Journal of the Royal Statistical Society: Series C}, 1998.

\bibitem{eriksson2019scalable}
David Eriksson, Michael Pearce, Jacob Gardner, Ryan~D Turner, and Matthias
  Poloczek.
\newblock Scalable global optimization via local {B}ayesian optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{Titsias2009Variational}
M.~K. Titsias.
\newblock Variational learning of inducing variables in sparse {G}aussian
  processes.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, 2009.

\bibitem{mcintire2016sparse}
Mitchell McIntire, Daniel Ratner, and Stefano Ermon.
\newblock Sparse gaussian processes for bayesian optimization.
\newblock In {\em Associateion for Uncertainty in Artificial Intelligence},
  2016.

\bibitem{griffiths2017constrained}
Ryan-Rhys Griffiths and Jos{\'e}~Miguel Hern{\'a}ndez-Lobato.
\newblock Constrained {B}ayesian optimization for automatic chemical design.
\newblock {\em arXiv preprint arXiv:1709.05501}, 2017.

\bibitem{yang2019sparse}
Ang Yang, Cheng Li, Santu Rana, Sunil Gupta, and Svetha Venkatesh.
\newblock Sparse spectrum {G}aussian process for {B}ayesian optimization.
\newblock {\em arXiv preprint arXiv:1906.08898}, 2019.

\bibitem{wilson2020efficientsampling}
James~T. Wilson, Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky,
  and Marc~Peter Deisenroth.
\newblock Efficiently sampling functions from {G}aussian process posteriors.
\newblock {\em International Conference on Machine Learning}, 2020.

\bibitem{Phan2019TSExample}
My~Phan, Yasin Abbasi~Yadkori, and Justin Domke.
\newblock Thompson sampling and approximate inference.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{Burt2019Rates}
David Burt, Carl~Edward Rasmussen, and Mark Van Der~Wilk.
\newblock Rates of convergence for sparse variational {G}aussian process
  regression.
\newblock In {\em International Conference on Machine Learning}, 2019.

\bibitem{Russo2018TutorialTS}
Daniel~J. Russo, Benjamin Van~Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen.
\newblock A tutorial on thompson sampling.
\newblock {\em Foundational Trends in Machine Learning}, 2018.

\bibitem{Russo2014}
D.~Russo and B.~Van~Roy.
\newblock Learning to optimize via posterior sampling.
\newblock {\em Mathematics of Operations Research}, 2014.

\bibitem{srinivas2010gaussian}
Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger.
\newblock Gaussian process optimization in the bandit setting: no regret and
  experimental design.
\newblock In {\em International Conference on Machine Learning}, 2010.

\bibitem{Calandriello2019Adaptive}
Daniele Calandriello, Luigi Carratino, Alessandro Lazaric, Michal Valko, and
  Lorenzo Rosasco.
\newblock Gaussian process optimization with adaptive sketching: scalable and
  no regret.
\newblock In {\em Conference on Learning Theory}, 2019.

\bibitem{berlinet2011reproducing}
Alain Berlinet and Christine Thomas-Agnan.
\newblock {\em Reproducing kernel Hilbert spaces in probability and
  statistics}.
\newblock Springer Science \& Business Media, 2011.

\bibitem{Hensman2013}
James Hensman, Nicol{\'{o}} Fusi, and Neil~D. Lawrence.
\newblock Gaussian processes for big data.
\newblock In {\em Uncertainty in Artificial Intelligence ({UAI} 2013)}, 2013.

\bibitem{hensman2017variational}
James Hensman, Nicolas Durrande, Arno Solin, et~al.
\newblock Variational {F}ourier features for {G}aussian processes.
\newblock {\em Journal of Machine Learning Research}, 2017.

\bibitem{Dutordoir2020spherical}
Vincent Dutordoir, Nicolas Durrande, and James Hensman.
\newblock {Sparse {G}aussian Processes with Spherical Harmonic Features}.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, 2020.

\bibitem{lazaro2009inter}
Miguel L{\'a}zaro-Gredilla and Anibal Figueiras-Vidal.
\newblock Inter-domain {G}aussian processes for sparse inference using inducing
  features.
\newblock In {\em Advances in Neural Information Processing Systems}, 2009.

\bibitem{Kanagawa2018}
Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, and Bharath~K
  Sriperumbudur.
\newblock Gaussian processes and kernel methods: A review on connections and
  equivalences.
\newblock {\em arXiv preprint arXiv:1807.02582}, 2018.

\bibitem{Borovitskiy2020}
Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, and Marc~P.
  Deisenroth.
\newblock {Matern {G}aussian processes on {R}iemannian manifolds}.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{zhu1997gaussian}
Huaiyu Zhu, Christopher~KI Williams, Richard Rohwer, and Michal Morciniec.
\newblock Gaussian regression and optimal finite dimensional linear models,
  1997.

\bibitem{solin2020hilbert}
Arno Solin and Simo S{\"a}rkk{\"a}.
\newblock {H}ilbert space methods for reduced-rank {G}aussian process
  regression.
\newblock {\em Statistics and Computing}, 2020.

\bibitem{hernandez2014predictive}
Jos{\'e}~Miguel Hern{\'a}ndez-Lobato, Matthew~W Hoffman, and Zoubin Ghahramani.
\newblock Predictive entropy search for efficient global optimization of
  black-box functions.
\newblock {\em Advances in Neural Information Processing Systems}, 2014.

\bibitem{Hernandez2014features}
Jos\'{e}~Miguel Hern\'{a}ndez-Lobato, Matthew~W Hoffman, and Zoubin Ghahramani.
\newblock Predictive entropy search for efficient global optimization of
  black-box functions.
\newblock In {\em Advances in Neural Information Processing Systems}, 2014.

\bibitem{bochner1959lectures}
Salomon Bochner et~al.
\newblock {\em Lectures on {F}ourier integrals}.
\newblock Princeton University Press, 1959.

\bibitem{Mutny2018SGPTS}
Mojmir Mutny and Andreas Krause.
\newblock Efficient high dimensional {B}ayesian optimization with additivity
  and quadrature fourier features.
\newblock In {\em Advances in Neural Information Processing Systems 31}, 2018.

\bibitem{cover1999elements}
Thomas~M Cover.
\newblock {\em Elements of information theory}.
\newblock John Wiley \& Sons, 1999.

\bibitem{Snoek2012practicalBO}
Jasper Snoek, Hugo Larochelle, and Ryan~P Adams.
\newblock Practical {B}ayesian optimization of machine learning algorithms.
\newblock In {\em Advances in Neural Information Processing Systems 25}, 2012.

\bibitem{MaternEigenvaluessantin2016}
Gabriele Santin and Robert Schaback.
\newblock Approximation of eigenfunctions in kernel-based spaces.
\newblock {\em Advances in Computational Mathematics}, 2016.

\bibitem{SEEigenvalues}
Mikhail Belkin.
\newblock Approximation beats concentration? an approximation view on inference
  with smooth radial kernels.
\newblock In {\em Conference On Learning Theory}, 2018.

\bibitem{Gabriel2020practicalfeature}
Gabriel Riutort-Mayol1, Paul-Christian Burkner, Michael~R. Andersen, Arno
  Solin, and Aki Vehtari.
\newblock Practical {H}ilbert space approximate {B}ayesian {G}aussian processes
  for probabilistic programming.
\newblock {\em arXiv preprint arXiv:2004.11408}, 2020.

\bibitem{vakili2021information}
Sattar Vakili, Kia Khezeli, and Victor Picheny.
\newblock On information gain and regret bounds in {G}aussian process bandits.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, 2021.

\bibitem{trieste}
Joel Berkeley, Henry~B. Moss, Artem Artemev, Sergio Pascual-Diaz, Uri Granta,
  Hrvoje Stojic, Ivo Couckuyt, Jixiang Quing, Loka Satrio, and Victor Picheny.
\newblock Trieste, 2021.

\bibitem{matthews2017gpflow}
Alexander G de~G Matthews, Mark van~der Wilk, Tom Nickson, Keisuke Fujii,
  Alexis Boukouvalas, Pablo Le{\'o}n-Villagr{\'a}, Zoubin Ghahramani, and James
  Hensman.
\newblock Gpflow: A gaussian process library using tensorflow.
\newblock {\em Journal of Machine Learning Research}, 2017.

\bibitem{dutordoir2021gpflux}
Vincent Dutordoir, Hugh Salimbeni, Eric Hambro, John McLeod, Felix Leibfried,
  Artem Artemev, Mark van~der Wilk, James Hensman, Marc~P Deisenroth, and
  ST~John.
\newblock Gpflux: A library for deep {G}aussian processes.
\newblock {\em arXiv preprint arXiv:2104.05674}, 2021.

\bibitem{jones1998efficient}
Donald~R Jones, Matthias Schonlau, and William~J Welch.
\newblock Efficient global optimization of expensive black-box functions.
\newblock {\em Journal of Global optimization}, 1998.

\bibitem{huang2006global}
Deng Huang, Theodore~T Allen, William~I Notz, and Ning Zeng.
\newblock Global optimization of stochastic black-box systems via sequential
  kriging meta-models.
\newblock {\em Journal of global optimization}, 2006.

\bibitem{hennig2012entropy}
Philipp Hennig and Christian~J Schuler.
\newblock Entropy search for information-efficient global optimization.
\newblock {\em Journal of Machine Learning Research}, 2012.

\bibitem{gomez2016design}
Rafael G{\'o}mez-Bombarelli, Jorge Aguilera-Iparraguirre, Timothy~D Hirzel,
  David Duvenaud, Dougal Maclaurin, Martin~A Blood-Forsythe, Hyun~Sik Chae,
  Markus Einzinger, Dong-Gwang Ha, Tony Wu, et~al.
\newblock Design of efficient molecular organic light-emitting diodes by a
  high-throughput virtual screening and experimental approach.
\newblock {\em Nature Materials}, 2016.

\bibitem{moss2020boss}
Henry~B Moss, Daniel Beck, Javier Gonz{\'a}lez, David~S Leslie, and Paul
  Rayson.
\newblock Boss: Bayesian optimization over string spaces.
\newblock {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{hachmann2011harvard}
Johannes Hachmann, Roberto Olivares-Amaya, Sule Atahan-Evrenk, Carlos
  Amador-Bedolla, Roel~S S{\'a}nchez-Carrera, Aryeh Gold-Parker, Leslie Vogt,
  Anna~M Brockway, and Al{\'a}n Aspuru-Guzik.
\newblock The {H}arvard clean energy project: large-scale computational
  screening and design of organic photovoltaics on the world community grid.
\newblock {\em The Journal of Physical Chemistry Letters}, 2011.

\bibitem{rogers2010extended}
David Rogers and Mathew Hahn.
\newblock Extended-connectivity fingerprints.
\newblock {\em Journal of chemical information and modeling}, 2010.

\bibitem{moss2020gaussian}
Henry~B Moss and Ryan-Rhys Griffiths.
\newblock Gaussian process molecule property prediction with flowmo.
\newblock {\em Advances in Neural Information Processing Systems: Workshop on
  Machine Learning for Molecules.}, 2020.

\bibitem{cho2012kernel}
Youngmin Cho.
\newblock {\em Kernel methods for deep learning}.
\newblock PhD thesis, UC San Diego, 2012.

\bibitem{cutajar2017random}
Kurt Cutajar, Edwin~V Bonilla, Pietro Michiardi, and Maurizio Filippone.
\newblock Random feature expansions for deep {G}aussian processes.
\newblock In {\em International Conference on Machine Learning}, 2017.

\bibitem{Jian2017NPHard}
Prateek Jain and Purushottam Kar.
\newblock Non-convex optimization for machine learning.
\newblock {\em Foundational Trends in Machine Learning}, 2017.

\bibitem{Dani2008}
V.~Dani, T.~P. Hayes, and S.~M. Kakade.
\newblock Stochastic linear optimization under bandit feedback.
\newblock In {\em Conference on Learning Theory}, 2008.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{riche2021revisiting}
Rodolphe~Le Riche and Victor Picheny.
\newblock Revisiting {B}ayesian optimization in the light of the coco
  benchmark.
\newblock {\em arXiv preprint arXiv:2103.16649}, 2021.

\bibitem{liu1989limited}
Dong~C Liu and Jorge Nocedal.
\newblock On the limited memory bfgs method for large scale optimization.
\newblock {\em Mathematical programming}, 1989.

\end{thebibliography}
