\begin{thebibliography}{10}

\bibitem{bamieh2002distributed}
B.~Bamieh, F.~Paganini, and M.~A. Dahleh.
\newblock Distributed control of spatially invariant systems.
\newblock {\em IEEE Transactions on automatic control}, 47(7):1091--1107, 2002.

\bibitem{bertsekas2007dpbook}
D.~P. Bertsekas.
\newblock {\em Dynamic Programming and Optimal Control, Vol. II}.
\newblock Athena Scientific, 3rd edition, 2007.

\bibitem{bertsekas1996neuro}
D.~P. Bertsekas and J.~N. Tsitsiklis.
\newblock {\em Neuro-dynamic programming}, volume~5.
\newblock Athena Scientific Belmont, MA, 1996.

\bibitem{bhandari2018finite}
J.~Bhandari, D.~Russo, and R.~Singal.
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock In {\em Conference On Learning Theory}, pages 1691--1692. PMLR, 2018.

\bibitem{bremaud2013markov}
P.~Bremaud.
\newblock {\em Markov Chains: Gibbs Fields, Monte Carlo Simulation, and
  Queues}.
\newblock Texts in Applied Mathematics. Springer New York, 2013.

\bibitem{marl_bu2008comprehensive}
L.~Bu, R.~Babu, B.~De~Schutter, et~al.
\newblock A comprehensive survey of multiagent reinforcement learning.
\newblock {\em IEEE Transactions on Systems, Man, and Cybernetics, Part C
  (Applications and Reviews)}, 38(2):156--172, 2008.

\bibitem{application_chakrabarti2008epidemic}
D.~Chakrabarti, Y.~Wang, C.~Wang, J.~Leskovec, and C.~Faloutsos.
\newblock Epidemic thresholds in real networks.
\newblock {\em ACM Transactions on Information and System Security (TISSEC)},
  10(4):1, 2008.

\bibitem{marl_claus1998dynamics}
C.~Claus and C.~Boutilier.
\newblock The dynamics of reinforcement learning in cooperative multiagent
  systems.
\newblock {\em AAAI/IAAI}, 1998:746--752, 1998.

\bibitem{Dann2018OnOP}
C.~Dann, N.~Jiang, A.~Krishnamurthy, A.~Agarwal, J.~Langford, and R.~E.
  Schapire.
\newblock On oracle-efficient pac rl with rich observations.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1422--1432, 2018.

\bibitem{pmlr-v97-doan19a}
T.~Doan, S.~Maguluri, and J.~Romberg.
\newblock Finite-time analysis of distributed {TD}(0) with linear function
  approximation on multi-agent reinforcement learning.
\newblock In K.~Chaudhuri and R.~Salakhutdinov, editors, {\em Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of {\em
  Proceedings of Machine Learning Research}, pages 1626--1635, Long Beach,
  California, USA, 09--15 Jun 2019. PMLR.

\bibitem{doan2019finitetime}
T.~T. Doan.
\newblock Finite-time analysis and restarting scheme for linear two-time-scale
  stochastic approximation, 2019.

\bibitem{dong2019q}
K.~Dong, Y.~Wang, X.~Chen, and L.~Wang.
\newblock Q-learning with {UCB} exploration is sample efficient for
  infinite-horizon {MDP}.
\newblock {\em arXiv preprint arXiv:1901.09311}, 2019.

\bibitem{duan2016benchmarking}
Y.~Duan, X.~Chen, R.~Houthooft, J.~Schulman, and P.~Abbeel.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In {\em International Conference on Machine Learning}, pages
  1329--1338, 2016.

\bibitem{easley2012networks}
D.~Easley, J.~Kleinberg, et~al.
\newblock Networks, crowds, and markets: Reasoning about a highly connected
  world.
\newblock {\em Significance}, 9:43--44, 2012.

\bibitem{foerster2018counterfactual}
J.~Foerster, G.~Farquhar, T.~Afouras, N.~Nardelli, and S.~Whiteson.
\newblock Counterfactual multi-agent policy gradients.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem{gamarnik2013correlation}
D.~Gamarnik.
\newblock Correlation decay method for decision, optimization, and inference in
  large-scale networks.
\newblock In {\em Theory Driven by Influential Applications}, pages 108--121.
  INFORMS, 2013.

\bibitem{gamarnik2014correlation}
D.~Gamarnik, D.~A. Goldberg, and T.~Weber.
\newblock Correlation decay in random decision networks.
\newblock {\em Mathematics of Operations Research}, 39(2):229--261, 2014.

\bibitem{gu2020qlearning}
H.~Gu, X.~Guo, X.~Wei, and R.~Xu.
\newblock Q-learning for mean-field controls, 2020.

\bibitem{gu2020q}
H.~Gu, X.~Guo, X.~Wei, and R.~Xu.
\newblock Q-learning for mean-field controls.
\newblock {\em arXiv preprint arXiv:2002.04131}, 2020.

\bibitem{Lee2019AUS}
D.~hwan Lee and N.~He.
\newblock A unified switching system perspective and {O.D.E.} analysis of
  q-learning algorithms.
\newblock {\em ArXiv}, abs/1912.02270, 2019.

\bibitem{Jiang2018NotesOS}
N.~Jiang.
\newblock Notes on state abstractions.
\newblock \url{http://nanjiang.web.engr.illinois.edu/files/cs598/note4.pdf},
  2018.

\bibitem{Jiang2015AbstractionSI}
N.~Jiang, A.~Kulesza, and S.~Singh.
\newblock Abstraction selection in model-based reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  179--188, 2015.

\bibitem{Jong2005StateAD}
N.~K. Jong and P.~Stone.
\newblock State abstraction discovery from irrelevant state variables.
\newblock In {\em IJCAI}, volume~8, pages 752--757, 2005.

\bibitem{tabular-lower-bound}
T.~Lattimore and M.~Hutter.
\newblock {PAC} bounds for discounted {MDP}s.
\newblock In N.~H. Bshouty, G.~Stoltz, N.~Vayatis, and T.~Zeugmann, editors,
  {\em Algorithmic Learning Theory}, pages 320--334, Berlin, Heidelberg, 2012.
  Springer Berlin Heidelberg.

\bibitem{li2019reinforcement}
D.~Li, D.~Zhao, Q.~Zhang, and Y.~Chen.
\newblock Reinforcement learning and deep learning based lateral control for
  autonomous driving [application notes].
\newblock {\em IEEE Computational Intelligence Magazine}, 14(2):83--98, 2019.

\bibitem{Li2006TowardsAU}
L.~Li, T.~J. Walsh, and M.~L. Littman.
\newblock Towards a unified theory of state abstraction for {MDP}s.
\newblock In {\em ISAIM}, 2006.

\bibitem{application_llas2003nonequilibrium}
M.~Llas, P.~M. Gleiser, J.~M. L{\'o}pez, and A.~D{\'\i}az-Guilera.
\newblock Nonequilibrium phase transition in a model for the propagation of
  innovations among economic agents.
\newblock {\em Physical Review E}, 68(6):066101, 2003.

\bibitem{lowe2017multi}
R.~Lowe, Y.~Wu, A.~Tamar, J.~Harb, O.~P. Abbeel, and I.~Mordatch.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6379--6390, 2017.

\bibitem{matignon2012independent}
L.~Matignon, G.~J. Laurent, and N.~Le~Fort-Piat.
\newblock Independent reinforcement learners in cooperative {Markov} games: a
  survey regarding coordination problems.
\newblock {\em The Knowledge Engineering Review}, 27(1):1--31, 2012.

\bibitem{epi_mei2017dynamics}
W.~Mei, S.~Mohagheghi, S.~Zampieri, and F.~Bullo.
\newblock On the dynamics of deterministic epidemic propagation over networks.
\newblock {\em Annual Reviews in Control}, 44:116--128, 2017.

\bibitem{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529, 2015.

\bibitem{motee2008optimal}
N.~Motee and A.~Jadbabaie.
\newblock Optimal control of spatially distributed systems.
\newblock {\em IEEE Transactions on Automatic Control}, 53(7):1616--1629, 2008.

\bibitem{Backpressure}
M.~J. {Neely}.
\newblock Optimal backpressure routing for wireless networks with
  multi-receiver diversity.
\newblock In {\em 2006 40th Annual Conference on Information Sciences and
  Systems}, pages 18--25, 2006.

\bibitem{complexity_papadimitriou1999complexity}
C.~H. Papadimitriou and J.~N. Tsitsiklis.
\newblock The complexity of optimal queuing network control.
\newblock {\em Mathematics of Operations Research}, 24(2):293--305, 1999.

\bibitem{qu2019exploiting}
G.~Qu and N.~Li.
\newblock Exploiting fast decaying and locality in multi-agent {MDP} with tree
  dependence structure.
\newblock In {\em 2019 IEEE 58th Conference on Decision and Control (CDC)},
  pages 6479--6486. IEEE, 2019.

\bibitem{qu2020scalable}
G.~Qu, Y.~Lin, A.~Wierman, and N.~Li.
\newblock Scalable multi-agent reinforcement learning for networked systems
  with average reward.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{qu2020finite}
G.~Qu and A.~Wierman.
\newblock Finite-time analysis of asynchronous stochastic approximation and $ q
  $-learning.
\newblock In {\em Conference on Learning Theory}, pages 3185--3205. PMLR, 2020.

\bibitem{qu2019scalable}
G.~Qu, A.~Wierman, and N.~Li.
\newblock Scalable reinforcement learning of localized policies for multi-agent
  networked systems.
\newblock In {\em Learning for Dynamics and Control}, pages 256--266. PMLR,
  2020.

\bibitem{roberts1975aloha}
L.~G. Roberts.
\newblock Aloha packet system with and without slots and capture.
\newblock {\em ACM SIGCOMM Computer Communication Review}, 5(2):28--42, 1975.

\bibitem{ruhi2016improved}
N.~A. Ruhi, C.~Thrampoulidis, and B.~Hassibi.
\newblock Improved bounds on the epidemic threshold of exact sis models on
  complex networks.
\newblock In {\em 2016 IEEE 55th Conference on Decision and Control (CDC)},
  pages 3560--3565. IEEE, 2016.

\bibitem{shah2018q}
D.~Shah and Q.~Xie.
\newblock {Q}-learning with nearest neighbors.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3111--3121, 2018.

\bibitem{silver2016mastering}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~Van Den~Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em nature}, 529(7587):484, 2016.

\bibitem{Singh1994ReinforcementLW}
S.~P. Singh, T.~Jaakkola, and M.~I. Jordan.
\newblock Reinforcement learning with soft state aggregation.
\newblock In {\em Advances in neural information processing systems}, pages
  361--368, 1995.

\bibitem{srikant2019finite}
R.~Srikant and L.~Ying.
\newblock Finite-time error bounds for linear stochastic approximation and {TD}
  learning.
\newblock In {\em COLT}, 2019.

\bibitem{subramanian2019reinforcement}
J.~Subramanian and A.~Mahajan.
\newblock Reinforcement learning in stationary mean-field games.
\newblock In {\em Proceedings of the 18th International Conference on
  Autonomous Agents and MultiAgent Systems}, pages 251--259, 2019.

\bibitem{sunehag2018value}
P.~Sunehag, G.~Lever, A.~Gruslys, W.~M. Czarnecki, V.~F. Zambaldi,
  M.~Jaderberg, M.~Lanctot, N.~Sonnerat, J.~Z. Leibo, K.~Tuyls, et~al.
\newblock Value-decomposition networks for cooperative multi-agent learning
  based on team reward.
\newblock In {\em AAMAS}, pages 2085--2087, 2018.

\bibitem{PolicyGradientTheorem}
R.~S. Sutton, D.~McAllester, S.~Singh, and Y.~Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In {\em Proceedings of the 12th International Conference on Neural
  Information Processing Systems}, NIPS’99, page 1057–1063, Cambridge, MA,
  USA, 1999. MIT Press.

\bibitem{tsitsiklis1994asynchronous}
J.~N. Tsitsiklis.
\newblock Asynchronous stochastic approximation and {Q}-learning.
\newblock {\em Machine learning}, 16(3):185--202, 1994.

\bibitem{TDwithFuncApprox}
J.~N. {Tsitsiklis} and B.~{Van Roy}.
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock {\em IEEE Transactions on Automatic Control}, 42(5):674--690, 1997.

\bibitem{tsitsiklis1997analysis}
J.~N. Tsitsiklis and B.~Van~Roy.
\newblock Analysis of temporal-diffference learning with function
  approximation.
\newblock In {\em Advances in neural information processing systems}, pages
  1075--1081, 1997.

\bibitem{application_communication}
W.~Vogels, R.~van Renesse, and K.~Birman.
\newblock The power of epidemics: Robust communication for large-scale
  distributed systems.
\newblock {\em SIGCOMM Comput. Commun. Rev.}, 33(1):131--135, Jan. 2003.

\bibitem{wainwright2019stochastic}
M.~J. Wainwright.
\newblock Stochastic approximation with cone-contractive operators: Sharp
  $\ell_infty $-bounds for {Q-learning}.
\newblock {\em arXiv preprint arXiv:1905.06265}, 2019.

\bibitem{hierarchyDependence}
S.~{Wang}, V.~{Venkateswaran}, and X.~{Zhang}.
\newblock Fundamental analysis of full-duplex gains in wireless networks.
\newblock {\em IEEE/ACM Transactions on Networking}, 25(3):1401--1416, 2017.

\bibitem{wu2020finite}
Y.~Wu, W.~Zhang, P.~Xu, and Q.~Gu.
\newblock A finite time analysis of two time-scale actor critic methods, 2020.

\bibitem{tengyu2019twotimescale}
T.~Xu, S.~Zou, and Y.~Liang.
\newblock Two time-scale off-policy {TD} learning: Non-asymptotic analysis over
  {Markovian} samples.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  10634--10644. Curran Associates, Inc., 2019.

\bibitem{yang2018mean}
Y.~Yang, R.~Luo, M.~Li, M.~Zhou, W.~Zhang, and J.~Wang.
\newblock Mean field multi-agent reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  5571--5580. PMLR, 2018.

\bibitem{zhang2019multi}
K.~Zhang, Z.~Yang, and T.~Ba{\c{s}}ar.
\newblock Multi-agent reinforcement learning: A selective overview of theories
  and algorithms.
\newblock {\em arXiv preprint arXiv:1911.10635}, 2019.

\bibitem{zhang2018fully}
K.~Zhang, Z.~Yang, H.~Liu, T.~Zhang, and T.~Ba{\c{s}}ar.
\newblock Fully decentralized multi-agent reinforcement learning with networked
  agents.
\newblock {\em arXiv preprint arXiv:1802.08757}, 2018.

\bibitem{zhang2016control}
R.~Zhang and M.~Pavone.
\newblock Control of robotic mobility-on-demand systems: a queueing-theoretical
  perspective.
\newblock {\em The International Journal of Robotics Research},
  35(1-3):186--203, 2016.

\bibitem{zocca2019temporal}
A.~Zocca.
\newblock Temporal starvation in multi-channel csma networks: an analytical
  framework.
\newblock {\em Queueing Systems}, 91(3-4):241--263, 2019.

\end{thebibliography}
