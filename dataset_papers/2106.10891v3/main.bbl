\begin{thebibliography}{82}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arazo et~al.(2019)Arazo, Ortego, Albert, O’Connor, and
  McGuinness]{arazo2019unsupervised}
Eric Arazo, Diego Ortego, Paul Albert, Noel O’Connor, and Kevin McGuinness.
\newblock Unsupervised label noise modeling and loss correction.
\newblock In \emph{International Conference on Machine Learning}, pages
  312--321. PMLR, 2019.

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{\k{e}}bski, Ballas, Krueger, Bengio,
  Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{pmlr-v70-arpit17a}
Devansh Arpit, Stanis{\l}aw Jastrz{\k{e}}bski, Nicolas Ballas, David Krueger,
  Emmanuel Bengio, Maxinder~S Kanwal, Tegan Maharaj, Asja Fischer, Aaron
  Courville, Yoshua Bengio, et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, pages 233--242, 2017.

\bibitem[Blum et~al.(2003)Blum, Kalai, and Wasserman]{blum2003noise}
Avrim Blum, Adam Kalai, and Hal Wasserman.
\newblock Noise-tolerant learning, the parity problem, and the statistical
  query model.
\newblock \emph{Journal of the ACM}, 50\penalty0 (4):\penalty0 506--519, 2003.

\bibitem[Bottou(2012)]{bottou2012stochastic}
L{\'e}on Bottou.
\newblock Stochastic gradient descent tricks.
\newblock In \emph{Neural networks: Tricks of the trade}, pages 421--436.
  Springer, 2012.

\bibitem[Carmon et~al.(2019)Carmon, Raghunathan, Schmidt, Liang, and
  Duchi]{carmon2019unlabeled}
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John~C Duchi.
\newblock Unlabeled data improves adversarial robustness.
\newblock \emph{arXiv preprint arXiv:1905.13736}, 2019.

\bibitem[Chen et~al.(2020)Chen, Ye, Chen, Zhao, and Heng]{chen2020beyond}
Pengfei Chen, Junjie Ye, Guangyong Chen, Jingwei Zhao, and Pheng-Ann Heng.
\newblock Beyond class-conditional assumption: A primary attempt to combat
  instance-dependent label noise.
\newblock \emph{arXiv preprint arXiv:2012.05458}, 2020.

\bibitem[Chen et~al.(2021)Chen, Chen, Ye, Zhao, and Heng]{chen2021noise}
Pengfei Chen, Guangyong Chen, Junjie Ye, Jingwei Zhao, and Pheng-Ann Heng.
\newblock Noise against noise: stochastic label noise helps combat inherent
  label noise.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Chen and Gupta(2015)]{chen2015webly}
Xinlei Chen and Abhinav Gupta.
\newblock Webly supervised learning of convolutional networks.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1431--1439, 2015.

\bibitem[Cimpoi et~al.(2014)Cimpoi, Maji, Kokkinos, Mohamed, and
  Vedaldi]{cimpoi2014describing}
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea
  Vedaldi.
\newblock Describing textures in the wild.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3606--3613, 2014.

\bibitem[Fatras et~al.(2021)Fatras, Damodaran, Lobry, Flamary, Tuia, and
  Courty]{fatras2021wasserstein}
Kilian Fatras, Bharath~Bhushan Damodaran, Sylvain Lobry, Remi Flamary, Devis
  Tuia, and Nicolas Courty.
\newblock Wasserstein adversarial regularization for learning with label noise.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2021.

\bibitem[Feng et~al.(2020)Feng, Shu, Lin, Lv, Li, and An]{feng2020can}
Lei Feng, Senlin Shu, Zhuoyi Lin, Fengmao Lv, Li~Li, and Bo~An.
\newblock Can cross entropy loss be robust to label noise?
\newblock In \emph{International Joint Conference on Artificial Intelligence},
  pages 2206--2212, 2020.

\bibitem[Ghosh et~al.(2017)Ghosh, Kumar, and Sastry]{ghosh2017robust}
Aritra Ghosh, Himanshu Kumar, and PS~Sastry.
\newblock Robust loss functions under label noise for deep neural networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~31, page 1919–1925, 2017.

\bibitem[Goodfellow et~al.(2014{\natexlab{a}})Goodfellow, Pouget-Abadie, Mirza,
  Xu, Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian~J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1406.2661}, 2014{\natexlab{a}}.

\bibitem[Goodfellow et~al.(2014{\natexlab{b}})Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014{\natexlab{b}}.

\bibitem[Han et~al.(2018)Han, Yao, Yu, Niu, Xu, Hu, Tsang, and
  Sugiyama]{han2018co}
Bo~Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and
  Masashi Sugiyama.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8527--8537, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pages 770--778, 2016.

\bibitem[Hein et~al.(2019)Hein, Andriushchenko, and Bitterwolf]{hein2019relu}
Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf.
\newblock Why relu networks yield high-confidence predictions far away from the
  training data and how to mitigate the problem.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 41--50, 2019.

\bibitem[Hendrycks and Gimpel(2016)]{hendrycks2016baseline}
Dan Hendrycks and Kevin Gimpel.
\newblock A baseline for detecting misclassified and out-of-distribution
  examples in neural networks.
\newblock \emph{arXiv preprint arXiv:1610.02136}, 2016.

\bibitem[Hendrycks et~al.(2018)Hendrycks, Mazeika, Wilson, and
  Gimpel]{hendrycks2018using}
Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel.
\newblock Using trusted data to train deep networks on labels corrupted by
  severe noise.
\newblock \emph{arXiv preprint arXiv:1802.05300}, 2018.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Mazeika, and
  Dietterich]{hendrycks2019oe}
Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich.
\newblock Deep anomaly detection with outlier exposure.
\newblock \emph{Proceedings of the International Conference on Learning
  Representations}, 2019.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{arXiv preprint arXiv:2006.11239}, 2020.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997flat}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Flat minima.
\newblock \emph{Neural Computation}, 9\penalty0 (1):\penalty0 1--42, 1997.

\bibitem[Hu et~al.(2019)Hu, Li, and Yu]{hu2019simple}
Wei Hu, Zhiyuan Li, and Dingli Yu.
\newblock Simple and effective regularization methods for training on noisily
  labeled data with generalization guarantee.
\newblock \emph{arXiv preprint arXiv:1905.11368}, 2019.

\bibitem[Jiang et~al.(2017)Jiang, Zhou, Leung, Li, and
  Fei-Fei]{jiang2017mentornet}
Lu~Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li~Fei-Fei.
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock \emph{arXiv preprint arXiv:1712.05055}, 2017.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kleinberg et~al.(2018)Kleinberg, Li, and
  Yuan]{kleinberg2018alternative}
Bobby Kleinberg, Yuanzhi Li, and Yang Yuan.
\newblock An alternative view: When does sgd escape local minima?
\newblock In \emph{Proceedings of International Conference on Machine
  Learning}, pages 2698--2707. PMLR, 2018.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Laine and Aila(2016)]{laine2016temporal}
Samuli Laine and Timo Aila.
\newblock Temporal ensembling for semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1610.02242}, 2016.

\bibitem[Lee et~al.(2017)Lee, Lee, Lee, and Shin]{lee2017training}
Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin.
\newblock Training confidence-calibrated classifiers for detecting
  out-of-distribution samples.
\newblock \emph{arXiv preprint arXiv:1711.09325}, 2017.

\bibitem[Lee et~al.(2019)Lee, Yun, Lee, Lee, Li, and Shin]{lee2019robust}
Kimin Lee, Sukmin Yun, Kibok Lee, Honglak Lee, Bo~Li, and Jinwoo Shin.
\newblock Robust inference via generative classifiers for handling noisy
  labels.
\newblock In \emph{Proceedings of International Conference on Machine
  Learning}, pages 3763--3772. PMLR, 2019.

\bibitem[Lee et~al.(2021)Lee, Park, Lee, Yi, Lee, and Yoon]{lee2021removing}
Saehyung Lee, Changhwa Park, Hyungyu Lee, Jihun Yi, Jonghyun Lee, and Sungroh
  Yoon.
\newblock Removing undesirable feature contributions using out-of-distribution
  data.
\newblock 2021.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6391--–6401, 2018.

\bibitem[Li et~al.(2020)Li, Socher, and Hoi]{li2020dividemix}
Junnan Li, Richard Socher, and Steven~CH Hoi.
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:2002.07394}, 2020.

\bibitem[Liu and Tao(2015)]{liu2015classification}
Tongliang Liu and Dacheng Tao.
\newblock Classification with noisy labels by importance reweighting.
\newblock \emph{IEEE Transactions on pattern analysis and machine
  intelligence}, 38\penalty0 (3):\penalty0 447--461, 2015.

\bibitem[Liu et~al.(2020)Liu, Wang, Owens, and Li]{liu2020energy}
Weitang Liu, Xiaoyun Wang, John~D Owens, and Yixuan Li.
\newblock Energy-based out-of-distribution detection.
\newblock \emph{arXiv preprint arXiv:2010.03759}, 2020.

\bibitem[Lukasik et~al.(2020)Lukasik, Bhojanapalli, Menon, and
  Kumar]{lukasik2020does}
Michal Lukasik, Srinadh Bhojanapalli, Aditya Menon, and Sanjiv Kumar.
\newblock Does label smoothing mitigate label noise?
\newblock In \emph{Proceedings of International Conference on Machine
  Learning}, pages 6448--6458. PMLR, 2020.

\bibitem[Lyu and Tsang(2019)]{lyu2019curriculum}
Yueming Lyu and Ivor~W Tsang.
\newblock Curriculum loss: Robust learning and generalization against label
  corruption.
\newblock \emph{arXiv preprint arXiv:1905.10045}, 2019.

\bibitem[Ma et~al.(2020)Ma, Huang, Wang, Romano, Erfani, and
  Bailey]{ma2020normalized}
Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James
  Bailey.
\newblock Normalized loss functions for deep learning with noisy labels.
\newblock In \emph{International Conference on Machine Learning}, pages
  6543--6553. PMLR, 2020.

\bibitem[Mahajan et~al.(2018)Mahajan, Girshick, Ramanathan, He, Paluri, Li,
  Bharambe, and Van Der~Maaten]{mahajan2018exploring}
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
  Yixuan Li, Ashwin Bharambe, and Laurens Van Der~Maaten.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock In \emph{Proceedings of the European Conference on Computer Vision},
  pages 181--196, 2018.

\bibitem[Malach and Shalev-Shwartz(2017)]{malach2017decoupling}
Eran Malach and Shai Shalev-Shwartz.
\newblock Decoupling" when to update" from" how to update".
\newblock \emph{arXiv preprint arXiv:1706.02613}, 2017.

\bibitem[Menon et~al.(2020)Menon, Rawat, Kumar, and Reddi]{menon2020can}
Aditya~Krishna Menon, Ankit~Singh Rawat, Sanjiv Kumar, and Sashank Reddi.
\newblock Can gradient clipping mitigate label noise?
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Miyato et~al.(2018)Miyato, Maeda, Koyama, and
  Ishii]{miyato2018virtual}
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii.
\newblock Virtual adversarial training: a regularization method for supervised
  and semi-supervised learning.
\newblock \emph{IEEE transactions on Pattern Analysis and Machine
  Intelligence}, 41\penalty0 (8):\penalty0 1979--1993, 2018.

\bibitem[Nakkiran et~al.(2020)Nakkiran, Neyshabur, and
  Sedghi]{nakkiran2020deep}
Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi.
\newblock The deep bootstrap: Good online learners are good offline
  generalizers.
\newblock \emph{arXiv preprint arXiv:2010.08127}, 2020.

\bibitem[Netzer et~al.(2011{\natexlab{a}})Netzer, Wang, Coates, Bissacco, Wu,
  and Ng]{yuval2011svhn}
Yuval Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In \emph{NIPS Workshop on Deep Learning and Unsupervised Feature
  Learning}, 2011{\natexlab{a}}.

\bibitem[Netzer et~al.(2011{\natexlab{b}})Netzer, Wang, Coates, Bissacco, Wu,
  and Ng]{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011{\natexlab{b}}.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{neyshabur2017exploring}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro.
\newblock Exploring generalization in deep learning.
\newblock \emph{arXiv preprint arXiv:1706.08947}, 2017.

\bibitem[Nguyen et~al.(2019)Nguyen, Mummadi, Ngo, Nguyen, Beggel, and
  Brox]{nguyen2019self}
Duc~Tam Nguyen, Chaithanya~Kumar Mummadi, Thi Phuong~Nhung Ngo, Thi Hoai~Phuong
  Nguyen, Laura Beggel, and Thomas Brox.
\newblock Self: Learning to filter noisy labels with self-ensembling.
\newblock \emph{arXiv preprint arXiv:1910.01842}, 2019.

\bibitem[Papadopoulos et~al.(2021)Papadopoulos, Rajati, Shaikh, and
  Wang]{papadopoulos2021outlier}
Aristotelis-Angelos Papadopoulos, Mohammad~Reza Rajati, Nazim Shaikh, and
  Jiamian Wang.
\newblock Outlier exposure with confidence control for out-of-distribution
  detection.
\newblock \emph{Neurocomputing}, 441:\penalty0 138--150, 2021.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{NEURIPS2019_9015}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8024--8035. Curran Associates, Inc., 2019.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Krishna~Menon, Nock, and
  Qu]{patrini2017making}
Giorgio Patrini, Alessandro Rozza, Aditya Krishna~Menon, Richard Nock, and
  Lizhen Qu.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1944--1952, 2017.

\bibitem[Reed et~al.(2014)Reed, Lee, Anguelov, Szegedy, Erhan, and
  Rabinovich]{reed2014training}
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan,
  and Andrew Rabinovich.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock \emph{arXiv preprint arXiv:1412.6596}, 2014.

\bibitem[Ren et~al.(2018)Ren, Zeng, Yang, and Urtasun]{ren2018learning}
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun.
\newblock Learning to reweight examples for robust deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  4334--4343. PMLR, 2018.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
  Alexander~C. Berg, and Li~Fei-Fei.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Sachdeva et~al.(2021)Sachdeva, Cordeiro, Belagiannis, Reid, and
  Carneiro]{sachdeva2021evidentialmix}
Ragav Sachdeva, Filipe~R Cordeiro, Vasileios Belagiannis, Ian Reid, and Gustavo
  Carneiro.
\newblock Evidentialmix: Learning with combined open-set and closed-set noisy
  labels.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision}, pages 3607--3615, 2021.

\bibitem[Shu et~al.(2019)Shu, Xie, Yi, Zhao, Zhou, Xu, and Meng]{shu2019meta}
Jun Shu, Qi~Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng.
\newblock Meta-weight-net: Learning an explicit mapping for sample weighting.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1919--1930, 2019.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pages 2818--2826, 2016.

\bibitem[Tanaka et~al.(2018)Tanaka, Ikami, Yamasaki, and
  Aizawa]{tanaka2018joint}
Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa.
\newblock Joint optimization framework for learning with noisy labels.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 5552--5560, 2018.

\bibitem[Uesato et~al.(2019)Uesato, Alayrac, Huang, Stanforth, Fawzi, and
  Kohli]{uesato2019labels}
Jonathan Uesato, Jean-Baptiste Alayrac, Po-Sen Huang, Robert Stanforth,
  Alhussein Fawzi, and Pushmeet Kohli.
\newblock Are labels required for improving adversarial robustness?
\newblock \emph{arXiv preprint arXiv:1905.13725}, 2019.

\bibitem[Van~Rooyen et~al.(2015)Van~Rooyen, Menon, and
  Williamson]{van2015learning}
Brendan Van~Rooyen, Aditya Menon, and Robert~C Williamson.
\newblock Learning with symmetric label noise: The importance of being
  unhinged.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10--18, 2015.

\bibitem[Wang et~al.(2018)Wang, Liu, Ma, Bailey, Zha, Song, and
  Xia]{wang2018iterative}
Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le~Song, and
  Shu-Tao Xia.
\newblock Iterative learning with open-set noisy labels.
\newblock In \emph{Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pages 8688--8696, 2018.

\bibitem[Wang et~al.(2019)Wang, Ma, Chen, Luo, Yi, and
  Bailey]{wang2019symmetric}
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey.
\newblock Symmetric cross entropy for robust learning with noisy labels.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 322--330, 2019.

\bibitem[Wei et~al.(2020{\natexlab{a}})Wei, Feng, Chen, and
  An]{wei2020combating}
Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo~An.
\newblock Combating noisy labels by agreement: A joint training method with
  co-regularization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 13726--13735, 2020{\natexlab{a}}.

\bibitem[Wei et~al.(2020{\natexlab{b}})Wei, Feng, Wang, and
  An]{wei2020metainfonet}
Hongxin Wei, Lei Feng, Rundong Wang, and Bo~An.
\newblock Metainfonet: Learning task-guided information for sample reweighting.
\newblock \emph{arXiv preprint arXiv:2012.05273}, 2020{\natexlab{b}}.

\bibitem[Wu et~al.(2020)Wu, Hu, Xiong, Huan, Braverman, and Zhu]{wu2020noisy}
Jingfeng Wu, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, and
  Zhanxing Zhu.
\newblock On the noisy gradient descent that generalizes as sgd.
\newblock In \emph{International Conference on Machine Learning}, pages
  10367--10376. PMLR, 2020.

\bibitem[Xia et~al.(2020{\natexlab{a}})Xia, Liu, Han, Gong, Wang, Ge, and
  Chang]{xia2020robust}
Xiaobo Xia, Tongliang Liu, Bo~Han, Chen Gong, Nannan Wang, Zongyuan Ge, and
  Yi~Chang.
\newblock Robust early-learning: Hindering the memorization of noisy labels.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.

\bibitem[Xia et~al.(2020{\natexlab{b}})Xia, Liu, Han, Wang, Deng, Li, and
  Mao]{xia2020extended}
Xiaobo Xia, Tongliang Liu, Bo~Han, Nannan Wang, Jiankang Deng, Jiatong Li, and
  Yinian Mao.
\newblock Extended t: Learning with mixed closed-set and open-set noisy labels.
\newblock \emph{arXiv preprint arXiv:2012.00932}, 2020{\natexlab{b}}.

\bibitem[Xia et~al.(2020{\natexlab{c}})Xia, Liu, Han, Wang, Gong, Liu, Niu,
  Tao, and Sugiyama]{xia2020parts}
Xiaobo Xia, Tongliang Liu, Bo~Han, Nannan Wang, Mingming Gong, Haifeng Liu,
  Gang Niu, Dacheng Tao, and Masashi Sugiyama.
\newblock Parts-dependent label noise: Towards instance-dependent label noise.
\newblock \emph{arXiv preprint arXiv:2006.07836}, 2020{\natexlab{c}}.

\bibitem[Xiao et~al.(2010)Xiao, Hays, Ehinger, Oliva, and
  Torralba]{xiao2010sun}
Jianxiong Xiao, James Hays, Krista~A Ehinger, Aude Oliva, and Antonio Torralba.
\newblock Sun database: Large-scale scene recognition from abbey to zoo.
\newblock In \emph{proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3485--3492. IEEE, 2010.

\bibitem[Xiao et~al.(2015)Xiao, Xia, Yang, Huang, and Wang]{xiao2015learning}
Tong Xiao, Tian Xia, Yi~Yang, Chang Huang, and Xiaogang Wang.
\newblock Learning from massive noisy labeled data for image classification.
\newblock In \emph{Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pages 2691--2699, 2015.

\bibitem[Xie et~al.(2016)Xie, Wang, Wei, Wang, and Tian]{xie2016disturblabel}
Lingxi Xie, Jingdong Wang, Zhen Wei, Meng Wang, and Qi~Tian.
\newblock Disturblabel: Regularizing cnn on the loss layer.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4753--4762, 2016.

\bibitem[Xu et~al.(2015)Xu, Ehinger, Zhang, Finkelstein, Kulkarni, and
  Xiao]{xu2015turkergaze}
Pingmei Xu, Krista~A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev~R
  Kulkarni, and Jianxiong Xiao.
\newblock Turkergaze: Crowdsourcing saliency with webcam based eye tracking.
\newblock \emph{arXiv preprint arXiv:1504.06755}, 2015.

\bibitem[Xu et~al.(2019)Xu, Cao, Kong, and Wang]{xu2019l_dmi}
Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang.
\newblock L\_dmi: A novel information-theoretic loss function for training deep
  nets robust to label noise.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6222--6233, 2019.

\bibitem[Yan et~al.(2014)Yan, Rosales, Fung, Subramanian, and
  Dy]{yan2014learning}
Yan Yan, R{\'o}mer Rosales, Glenn Fung, Ramanathan Subramanian, and Jennifer
  Dy.
\newblock Learning from multiple annotators with varying expertise.
\newblock \emph{Machine Learning}, 95\penalty0 (3):\penalty0 291--327, 2014.

\bibitem[Yu et~al.(2015)Yu, Seff, Zhang, Song, Funkhouser, and
  Xiao]{yu2015lsun}
Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong
  Xiao.
\newblock Lsun: Construction of a large-scale image dataset using deep learning
  with humans in the loop.
\newblock \emph{arXiv preprint arXiv:1506.03365}, 2015.

\bibitem[Yu et~al.(2019)Yu, Han, Yao, Niu, Tsang, and Sugiyama]{yu2019does}
Xingrui Yu, Bo~Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama.
\newblock How does disagreement help generalization against label corruption?
\newblock In \emph{International Conference on Machine Learning}, pages
  7164--7173. PMLR, 2019.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zeiler and Fergus(2014)]{zeiler2014visualizing}
Matthew~D Zeiler and Rob Fergus.
\newblock Visualizing and understanding convolutional networks.
\newblock In \emph{European Conference on Computer Vision}, pages 818--833.
  Springer, 2014.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{Proceedings of International Conference on Learning
  Representations}, 2016.

\bibitem[Zhang and Sabuncu(2018)]{zhang2018generalized}
Zhilu Zhang and Mert~R Sabuncu.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock \emph{arXiv preprint arXiv:1805.07836}, 2018.

\bibitem[Zheng et~al.(2020)Zheng, Wu, Goswami, Goswami, Metaxas, and
  Chen]{zheng2020error}
Songzhu Zheng, Pengxiang Wu, Aman Goswami, Mayank Goswami, Dimitris Metaxas,
  and Chao Chen.
\newblock Error-bounded correction of noisy labels.
\newblock In \emph{International Conference on Machine Learning}, pages
  11447--11457. PMLR, 2020.

\bibitem[Zhou et~al.(2017)Zhou, Lapedriza, Khosla, Oliva, and
  Torralba]{zhou2017places}
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba.
\newblock Places: A 10 million image database for scene recognition.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 40\penalty0 (6):\penalty0 1452--1464, 2017.

\end{thebibliography}
