\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{GPAM{\etalchar{+}}14}

\bibitem[ABG{\etalchar{+}}21]{abnar2021gradual}
Samira Abnar, Rianne van~den Berg, Golnaz Ghiasi, Mostafa Dehghani, Nal
  Kalchbrenner, and Hanie Sedghi.
\newblock Gradual domain adaptation in the wild: When intermediate
  distributions are absent.
\newblock {\em arXiv preprint arXiv:2106.06080}, 2021.

\bibitem[AUH{\etalchar{+}}19]{alayrac2019labels}
Jean-Baptiste Alayrac, Jonathan Uesato, Po-Sen Huang, Alhussein Fawzi, Robert
  Stanforth, and Pushmeet Kohli.
\newblock Are labels required for improving adversarial robustness?
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[BM19]{blanchet2019quantifying}
Jose Blanchet and Karthyek Murthy.
\newblock Quantifying distributional model risk via optimal transport.
\newblock {\em Mathematics of Operations Research}, 44(2):565--600, 2019.

\bibitem[CRS{\etalchar{+}}19]{carmon2019unlabeled}
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John~C Duchi, and Percy~S
  Liang.
\newblock Unlabeled data improves adversarial robustness.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem[CWZ{\etalchar{+}}20]{cui2020gradually}
Shuhao Cui, Shuhui Wang, Junbao Zhuo, Chi Su, Qingming Huang, and Qi~Tian.
\newblock Gradually vanishing bridge for adversarial domain adaptation.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 12455--12464, 2020.

\bibitem[GK23]{gao2023distributionally}
Rui Gao and Anton Kleywegt.
\newblock Distributionally robust stochastic optimization with wasserstein
  distance.
\newblock {\em Mathematics of Operations Research}, 48(2):603--655, 2023.

\bibitem[GPAM{\etalchar{+}}14]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock {\em Advances in neural information processing systems}, 27, 2014.

\bibitem[HJ14]{honorio2014tight}
Jean Honorio and Tommi Jaakkola.
\newblock Tight bounds for the expected risk of linear classifiers and
  pac-bayes finite-sample guarantees.
\newblock In {\em Artificial Intelligence and Statistics}, pages 384--392.
  PMLR, 2014.

\bibitem[HWLZ23]{he2023gradual}
Yifei He, Haoxiang Wang, Bo~Li, and Han Zhao.
\newblock Gradual domain adaptation: Theory and algorithms.
\newblock {\em arXiv preprint arXiv:2310.13852}, 2023.

\bibitem[JSZ{\etalchar{+}}15]{jaderberg2015spatial}
Max Jaderberg, Karen Simonyan, Andrew Zisserman, et~al.
\newblock Spatial transformer networks.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem[KML20]{kumar2020understanding}
Ananya Kumar, Tengyu Ma, and Percy Liang.
\newblock Understanding self-training for gradual domain adaptation.
\newblock In {\em International conference on machine learning}, pages
  5468--5479. PMLR, 2020.

\bibitem[KRdBG21]{kirchmeyer2021mapping}
Matthieu Kirchmeyer, Alain Rakotomamonjy, Emmanuel de~Bezenac, and Patrick
  Gallinari.
\newblock Mapping conditional distributions for domain adaptation under
  generalized target shift.
\newblock {\em arXiv preprint arXiv:2110.15057}, 2021.

\bibitem[SH22]{sagawa2022gradual}
Shogo Sagawa and Hideitsu Hino.
\newblock Gradual domain adaptation via normalizing flows.
\newblock {\em arXiv preprint arXiv:2206.11492}, 2022.

\bibitem[SNVD17]{sinha2017certifying}
Aman Sinha, Hongseok Namkoong, Riccardo Volpi, and John Duchi.
\newblock Certifying some distributional robustness with principled adversarial
  training.
\newblock {\em arXiv preprint arXiv:1710.10571}, 2017.

\bibitem[Wai19]{wainwright2019high}
Martin~J Wainwright.
\newblock {\em High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge university press, 2019.

\bibitem[WLZ22]{wang2022understanding}
Haoxiang Wang, Bo~Li, and Han Zhao.
\newblock Understanding gradual domain adaptation: Improved analysis, optimal
  path and beyond.
\newblock In {\em International Conference on Machine Learning}, pages
  22784--22801. PMLR, 2022.

\bibitem[WSCM20]{wei2020theoretical}
Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma.
\newblock Theoretical analysis of self-training with deep networks on unlabeled
  data.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem[XZLS23]{xiao2023energy}
Zehao Xiao, Xiantong Zhen, Shengcai Liao, and Cees~GM Snoek.
\newblock Energy-based test sample adaptation for domain generalization.
\newblock {\em arXiv preprint arXiv:2302.11215}, 2023.

\bibitem[ZDJZ21]{zhang2021gradual}
Yabin Zhang, Bin Deng, Kui Jia, and Lei Zhang.
\newblock Gradual domain adaptation via self-training of auxiliary models.
\newblock {\em arXiv preprint arXiv:2106.09890}, 2021.

\bibitem[ZZW23]{zhuang2023gradual}
Zhan Zhuang, Yu~Zhang, and Ying Wei.
\newblock Gradual domain adaptation via gradient flow.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2023.

\end{thebibliography}
