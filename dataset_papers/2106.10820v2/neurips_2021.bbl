\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{bai2019deep}
S.~Bai, J.~Z. Kolter, and V.~Koltun.
\newblock Deep equilibrium models.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  690--701, 2019.

\bibitem{borges-volker-etal-2019-hdt}
E.~Borges~V{\"o}lker, M.~Wendt, F.~Hennig, and A.~K{\"o}hn.
\newblock {HDT}-{UD}: A very large {U}niversal {D}ependencies treebank for
  {G}erman.
\newblock {\em Workshop on Universal Dependencies}, pages 46--57, 2019.

\bibitem{jax2018github}
J.~Bradbury, R.~Frostig, P.~Hawkins, M.~J. Johnson, C.~Leary, D.~Maclaurin,
  G.~Necula, A.~Paszke, J.~Vander{P}las, S.~Wanderman-{M}ilne, and Q.~Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.

\bibitem{chang2018antisymmetricrnn}
B.~Chang, M.~Chen, E.~Haber, and E.~H. Chi.
\newblock Antisymmetric{RNN}: {A} dynamical system view on recurrent neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{chang2018reversible}
B.~Chang, L.~Meng, E.~Haber, L.~Ruthotto, D.~Begert, and E.~Holtham.
\newblock Reversible architectures for arbitrarily deep residual neural
  networks.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2018.

\bibitem{chang2017multi}
B.~Chang, L.~Meng, E.~Haber, F.~Tung, and D.~Begert.
\newblock Multi-level residual networks from dynamical systems view.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{chen2018neural}
T.~Q. Chen, Y.~Rubanova, J.~Bettencourt, and D.~K. Duvenaud.
\newblock Neural ordinary differential equations.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6571--6583, 2018.

\bibitem{cranmer2020lagrangian}
M.~Cranmer, S.~Greydanus, S.~Hoyer, P.~Battaglia, D.~Spergel, and S.~Ho.
\newblock Lagrangian neural networks.
\newblock In {\em ICLR 2020 Workshop on Integration of Deep Neural Models and
  Differential Equations}, 2020.

\bibitem{dupont2019augmented}
E.~Dupont, A.~Doucet, and Y.~W. Teh.
\newblock Augmented neural odes.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3134--3144, 2019.

\bibitem{Elbayad2020Depth-Adaptive}
M.~Elbayad, J.~Gu, E.~Grave, and M.~Auli.
\newblock Depth-adaptive transformer.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{erichson2020lipschitz}
N.~B. Erichson, O.~Azencot, A.~Queiruga, and M.~W. Mahoney.
\newblock Lipschitz recurrent neural networks.
\newblock {\em arXiv preprint arXiv:2006.12070}, 2020.

\bibitem{Fan2020Reducing}
A.~Fan, E.~Grave, and A.~Joulin.
\newblock Reducing transformer depth on demand with structured dropout.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{gholami2019anode}
A.~Gholami, K.~Keutzer, and G.~Biros.
\newblock {ANODE}: unconditionally accurate memory-efficient gradients for
  neural {ODEs}.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, pages 730--736, 2019.

\bibitem{grathwohl2018ffjord}
W.~Grathwohl, R.~T. Chen, J.~Bettencourt, I.~Sutskever, and D.~Duvenaud.
\newblock {FFJORD}: Free-form continuous dynamics for scalable reversible
  generative models.
\newblock {\em arXiv preprint arXiv:1810.01367}, 2018.

\bibitem{greydanus2019hamiltonian}
S.~Greydanus, M.~Dzamba, and J.~Yosinski.
\newblock Hamiltonian neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  15353--15363, 2019.

\bibitem{gusak2020towards}
J.~Gusak, L.~Markeeva, T.~Daulbaev, A.~Katrutsa, A.~Cichocki, and I.~Oseledets.
\newblock Towards understanding normalization in neural {ODE}s.
\newblock In {\em ICLR 2020 Workshop on Integration of Deep Neural Models and
  Differential Equations}, 2020.

\bibitem{haber2017stable}
E.~Haber and L.~Ruthotto.
\newblock Stable architectures for deep neural networks.
\newblock {\em Inverse Problems}, 34(1):014004, 2017.

\bibitem{hairer1993solving}
E.~Hairer, S.~P. N{\o}rsett, and G.~Wanner.
\newblock {\em Solving ordinary differential equations. I. Nonstiff problems}.
\newblock Springer-Verlag, 1993.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the Conference on Computer Vision and Pattern
  Recognition}, pages 770--778, 2016.

\bibitem{flax2020github}
J.~Heek, A.~Levskaya, A.~Oliver, M.~Ritter, B.~Rondepierre, A.~Steiner, and
  M.~van {Z}ee.
\newblock {F}lax: A neural network library and ecosystem for {JAX}, 2020.

\bibitem{huang2016deep}
G.~Huang, Y.~Sun, Z.~Liu, D.~Sedra, and K.~Q. Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In {\em Proceedings of the European Conference on Computer Vision},
  pages 646--661. Springer, 2016.

\bibitem{ioffe2015batch}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em International conference on machine learning}, pages
  448--456, 2015.

\bibitem{jordan1965calculus}
C.~Jordan and K.~Jord{\'a}n.
\newblock {\em Calculus of finite differences}, volume~33.
\newblock American Mathematical Soc., 1965.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{KGC17_TR}
J.~Kukacka, V.~Golkov, and D.~Cremers.
\newblock Regularization for deep learning: A taxonomy.
\newblock Technical Report Preprint: arXiv:1710.10686, 2017.

\bibitem{kwong2006norm}
M.~K. Kwong and A.~Zettl.
\newblock {\em Norm inequalities for derivatives and differences}.
\newblock Springer, 2006.

\bibitem{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{li2020deep}
X.~Li, A.~Cooper~Stickland, Y.~Tang, and X.~Kong.
\newblock Deep transformers with latent depth.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{lim2020understanding}
S.~H. Lim.
\newblock Understanding recurrent neural networks using nonequilibrium response
  theory.
\newblock {\em arXiv preprint arXiv:2006.11052}, 2020.

\bibitem{lim2021noisy}
S.~H. Lim, N.~B. Erichson, L.~Hodgkinson, and M.~W. Mahoney.
\newblock Noisy recurrent neural networks.
\newblock {\em arXiv preprint arXiv:2102.04877}, 2021.

\bibitem{lu2017beyond}
Y.~Lu, A.~Zhong, Q.~Li, and B.~Dong.
\newblock Beyond finite layer neural networks: {B}ridging deep architectures
  and numerical differential equations.
\newblock In {\em International Conference on Machine Learning}, pages
  5181--5190, 2018.

\bibitem{lu2018beyond}
Y.~Lu, A.~Zhong, Q.~Li, and B.~Dong.
\newblock Beyond finite layer neural networks: Bridging deep architectures and
  numerical differential equations.
\newblock In {\em International Conference on Machine Learning}, pages
  3276--3285. PMLR, 2018.

\bibitem{Mah12}
M.~W. Mahoney.
\newblock Approximate computation and implicit regularization for very
  large-scale data analysis.
\newblock In {\em ACM Symposium on Principles of Database Systems}, pages
  143--154, 2012.

\bibitem{massaroli2020dissecting}
S.~Massaroli, M.~Poli, J.~Park, A.~Yamashita, and H.~Asma.
\newblock Dissecting neural odes.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{Ney17_TR}
B.~Neyshabur.
\newblock Implicit regularization in deep learning.
\newblock Technical Report Preprint: arXiv:1709.01953, 2017.

\bibitem{nivre2020universal}
J.~Nivre, M.-C. de~Marneffe, F.~Ginter, J.~Haji{\v{c}}, C.~D. Manning,
  S.~Pyysalo, S.~Schuster, F.~Tyers, and D.~Zeman.
\newblock Universal dependencies v2: An evergrowing multilingual treebank
  collection.
\newblock {\em arXiv preprint arXiv:2004.10643}, 2020.

\bibitem{queiruga2020continuous}
A.~F. Queiruga, N.~B. Erichson, D.~Taylor, and M.~W. Mahoney.
\newblock Continuous-in-depth neural networks.
\newblock {\em arXiv preprint arXiv:2008.02389}, 2020.

\bibitem{rachowicz2006fully}
W.~Rachowicz, D.~Pardo, and L.~Demkowicz.
\newblock Fully automatic {$hp$}-adaptivity in three dimensions.
\newblock {\em Computer methods in applied mechanics and engineering},
  195(37-40):4816--4842, 2006.

\bibitem{rubanova2019latent}
Y.~Rubanova, R.~T. Chen, and D.~Duvenaud.
\newblock Latent {ODEs} for irregularly-sampled time series.
\newblock In {\em International Conference on Neural Information Processing
  Systems}, pages 5320--5330, 2019.

\bibitem{rusch2021coupled}
T.~K. Rusch and S.~Mishra.
\newblock Coupled oscillatory recurrent neural network ({coRNN}): An accurate
  and (gradient) stable architecture for learning long time dependencies.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{rusch2021unicornn}
T.~K. Rusch and S.~Mishra.
\newblock Unicornn: A recurrent model for learning very long time dependencies.
\newblock {\em arXiv preprint arXiv:2103.05487}, 2021.

\bibitem{ruthotto2019deep}
L.~Ruthotto and E.~Haber.
\newblock Deep neural networks motivated by partial differential equations.
\newblock {\em Journal of Mathematical Imaging and Vision}, pages 1--13, 2019.

\bibitem{weinan2017proposal}
E.~Weinan.
\newblock A proposal on machine learning via dynamical systems.
\newblock {\em Communications in Mathematics and Statistics}, 5(1):1--11, 2017.

\bibitem{wriggers2008nonlinear}
P.~Wriggers.
\newblock {\em Nonlinear finite element methods}.
\newblock Springer Science \& Business Media, 2008.

\bibitem{xu2021infinitely}
W.~Xu, R.~T. Chen, X.~Li, and D.~Duvenaud.
\newblock Infinitely deep bayesian neural networks with stochastic differential
  equations.
\newblock {\em arXiv preprint arXiv:2102.06559}, 2021.

\bibitem{zagoruyko2016wide}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock {\em arXiv preprint arXiv:1605.07146}, 2016.

\bibitem{Zeldes2017}
A.~Zeldes.
\newblock The {GUM} corpus: Creating multilayer resources in the classroom.
\newblock {\em Language Resources and Evaluation}, 51(3):581--612, 2017.

\bibitem{zhang2019anodev2}
T.~Zhang, Z.~Yao, A.~Gholami, J.~E. Gonzalez, K.~Keutzer, M.~W. Mahoney, and
  G.~Biros.
\newblock {ANODEV2}: A coupled neural {ODE} framework.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5152--5162, 2019.

\bibitem{zhong2019symplectic}
Y.~D. Zhong, B.~Dey, and A.~Chakraborty.
\newblock Symplectic {ODE-Net}: {L}earning {H}amiltonian dynamics with control.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{zhu2021neural}
Q.~Zhu, Y.~Guo, and W.~Lin.
\newblock Neural delay differential equations.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{zhuang2020adaptive}
J.~Zhuang, N.~Dvornek, X.~Li, S.~Tatikonda, X.~Papademetris, and J.~Duncan.
\newblock Adaptive checkpoint adjoint method for gradient estimation in neural
  {ODE}.
\newblock In {\em International Conference on Machine Learning}, volume 119,
  pages 11639--11649, 2020.

\end{thebibliography}
