\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2016)Andrychowicz, Denil, Gomez, Hoffman, Pfau,
  Schaul, Shillingford, and de~Freitas]{andrychowicz2016learning}
Andrychowicz, Marcin, Denil, Misha, Gomez, Sergio, Hoffman, Matthew~W, Pfau,
  David, Schaul, Tom, Shillingford, Brendan, and de~Freitas, Nando.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and Bengio]{bahdanau2015neural}
Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{iclr}, 2015.

\bibitem[Bengio et~al.(1995)Bengio, Bengio, and Cloutier]{bengio:1995}
Bengio, S., Bengio, Y., and Cloutier, J.
\newblock On the search for new learning rules for {ANN}s.
\newblock \emph{Neural Processing Letters}, 2\penalty0 (4):\penalty0 26--30,
  1995.

\bibitem[Bengio et~al.(1990)Bengio, Bengio, and Cloutier]{bengio:1990}
Bengio, Yoshua, Bengio, Samy, and Cloutier, Jocelyn.
\newblock \emph{Learning a synaptic learning rule}.
\newblock Universit{\'e} de Montr{\'e}al, D{\'e}partement d'informatique et de
  recherche op{\'e}rationnelle, 1990.

\bibitem[Bengio et~al.(1992)Bengio, Bengio, Cloutier, and Gecsei]{Bengio+al-92}
Bengio, Yoshua, Bengio, Samy, Cloutier, Jocelyn, and Gecsei, Jan.
\newblock On the optimization of a synaptic learning rule.
\newblock In \emph{in Conference on Optimality in Biological and Artificial
  Networks}, 1992.

\bibitem[Chen et~al.(2016)Chen, Hoffman, Colmenarejo, Denil, Lillicrap, and
  de~Freitas]{Chen2016learning}
Chen, Yutian, Hoffman, Matthew~W., Colmenarejo, Sergio~Gomez, Denil, Misha,
  Lillicrap, Timothy~P., and de~Freitas, Nando.
\newblock Learning to learn for global optimization of black box functions.
\newblock arXiv Report 1611.03824, 2016.

\bibitem[Cho et~al.(2014)Cho, Van~Merri{\"e}nboer, Bahdanau, and
  Bengio]{cho2014properties}
Cho, Kyunghyun, Van~Merri{\"e}nboer, Bart, Bahdanau, Dzmitry, and Bengio,
  Yoshua.
\newblock On the properties of neural machine translation: Encoder-decoder
  approaches.
\newblock \emph{arXiv preprint arXiv:1409.1259}, 2014.

\bibitem[Duan et~al.(2016)Duan, Schulman, Chen, Bartlett, Sutskever, and
  Abbeel]{Duan2016}
Duan, Yan, Schulman, John, Chen, Xi, Bartlett, Peter, Sutskever, Ilya, and
  Abbeel, Pieter.
\newblock Rl$^2$: Fast reinforcement learning via slow reinforcement learning.
\newblock Technical report, UC Berkeley and OpenAI, 2016.

\bibitem[Funk(2015)]{smorms3}
Funk, Simon.
\newblock {RMSprop} loses to {SMORMS3} - beware the epsilon!, 2015.
\newblock URL \url{sifter.org/$\sim$simon/journal/20150420.html}.

\bibitem[Harlow(1949)]{harlow1949formation}
Harlow, Harry~F.
\newblock The formation of learning sets.
\newblock \emph{Psychological review}, 56\penalty0 (1):\penalty0 51, 1949.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016identity}
He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European Conference on Computer Vision}, pp.\  630--645.
  Springer, 2016.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Younger, and
  Conwell]{hochreiter:2001}
Hochreiter, Sepp, Younger, A~Steven, and Conwell, Peter~R.
\newblock Learning to learn using gradient descent.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pp.\  87--94. Springer, 2001.

\bibitem[Kehoe(1988)]{kehoe1988layered}
Kehoe, E~James.
\newblock A layered network model of associative learning: learning to learn
  and configuration.
\newblock \emph{Psychological review}, 95\penalty0 (4):\penalty0 411, 1988.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2015adam}
Kingma, Diederik and Ba, Jimmy.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{iclr}, 2015.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1097--1105, 2012.

\bibitem[Lake et~al.(2016)Lake, Ullman, Tenenbaum, and Gershman]{lake:2016}
Lake, Brenden~M, Ullman, Tomer~D, Tenenbaum, Joshua~B, and Gershman, Samuel~J.
\newblock Building machines that learn and think like people.
\newblock arXiv Report 1604.00289, 2016.

\bibitem[Li \& Malik(2017)Li and Malik]{Li2017learning}
Li, SKe and Malik, Jitendra.
\newblock Learning to optimize.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Naik \& Mammone(1992)Naik and Mammone]{naik:1992}
Naik, Devang~K and Mammone, RJ.
\newblock Meta{-}neural networks that learn by learning.
\newblock In \emph{International Joint Conference on Neural Networks},
  volume~1, pp.\  437--442. IEEE, 1992.

\bibitem[Nesterov(1983{\natexlab{a}})]{nesterov1983method}
Nesterov, Yurii.
\newblock A method of solving a convex programming problem with convergence
  rate o (1/k2).
\newblock In \emph{Soviet Mathematics Doklady}, volume~27, pp.\  372--376,
  1983{\natexlab{a}}.

\bibitem[Nesterov(1983{\natexlab{b}})]{nesterov:1983}
Nesterov, Yurii.
\newblock A method of solving a convex programming problem with convergence
  rate o (1/k2).
\newblock In \emph{Soviet Mathematics Doklady}, volume~27, pp.\  372--376,
  1983{\natexlab{b}}.

\bibitem[Ravi \& Larochelle(2017)Ravi and Larochelle]{Ravi2017optimization}
Ravi, Sachin and Larochelle, Hugo.
\newblock Optimization as a model for few-shot learning.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Runarsson \& Jonsson(2000)Runarsson and Jonsson]{runarsson:2000}
Runarsson, Thomas~Philip and Jonsson, Magnus~Thor.
\newblock Evolution and design of distributed learning rules.
\newblock In \emph{IEEE Symposium on Combinations of Evolutionary Computation
  and Neural Networks}, pp.\  59--63. IEEE, 2000.

\bibitem[Santoro et~al.(2016)Santoro, Bartunov, Botvinick, Wierstra, and
  Lillicrap]{santoro:2016}
Santoro, ADAM, Bartunov, Sergey, Botvinick, Matthew, Wierstra, Daan, and
  Lillicrap, Timothy.
\newblock Meta-learning with memory-augmented neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Schmidhuber(1987)]{schmidhuber:1987}
Schmidhuber, Jurgen.
\newblock \emph{Evolutionary Principles in Self-Referential Learning. On
  Learning how to Learn: The Meta-Meta-Meta...-Hook}.
\newblock PhD thesis, Institut f. Informatik, Tech. Univ. Munich, 1987.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, David, Huang, Aja, Maddison, Chris~J, Guez, Arthur, Sifre, Laurent, Van
  Den~Driessche, George, Schrittwieser, Julian, Antonoglou, Ioannis,
  Panneershelvam, Veda, Lanctot, Marc, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Surjanovic \& Bingham(2013)Surjanovic and
  Bingham]{optimization-functions}
Surjanovic, Sonja and Bingham, Derek.
\newblock Optimization test functions and datasets, 2013.
\newblock URL \url{http://www.sfu.ca/~ssurjano/optimization.html}.

\bibitem[Sutton(1992)]{sutton:1992}
Sutton, Richard~S.
\newblock Adapting bias by gradient descent: An incremental version of
  delta-bar-delta.
\newblock In \emph{Association for the Advancement of Artificial Intelligence},
  pp.\  171--176, 1992.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Szegedy, Christian, Vanhoucke, Vincent, Ioffe, Sergey, Shlens, Jon, and Wojna,
  Zbigniew.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  2818--2826, 2016.

\bibitem[Thrun \& Pratt(1998)Thrun and Pratt]{thrun:1998}
Thrun, Sebastian and Pratt, Lorien.
\newblock \emph{Learning to learn}.
\newblock Springer Science and Business Media, 1998.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{tieleman:2012}
Tieleman, Tijmen and Hinton, Geoffrey.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural Networks for Machine Learning}, 4:\penalty0 2,
  2012.

\bibitem[Tseng(1998)]{tseng:1998}
Tseng, Paul.
\newblock An incremental gradient (-projection) method with momentum term and
  adaptive stepsize rule.
\newblock \emph{Journal on Optimization}, 8\penalty0 (2):\penalty0 506--531,
  1998.

\bibitem[Wang et~al.(2016)Wang, Kurth{-}Nelson, Tirumala, Soyer, Leibo, Munos,
  Blundell, Kumaran, and Botvinick]{Wang2016learning}
Wang, Jane~X., Kurth{-}Nelson, Zeb, Tirumala, Dhruva, Soyer, Hubert, Leibo,
  Joel~Z., Munos, R{\'{e}}mi, Blundell, Charles, Kumaran, Dharshan, and
  Botvinick, Matt.
\newblock Learning to reinforcement learn.
\newblock arXiv Report 1611.05763, 2016.

\bibitem[Ward(1937)]{ward1937reminiscence}
Ward, Lewis~B.
\newblock Reminiscence and rote learning.
\newblock \emph{Psychological Monographs}, 49\penalty0 (4):\penalty0 i, 1937.

\bibitem[Zoph \& Le(2017)Zoph and Le]{ZophLe2017}
Zoph, Barret and Le, Quoc~V.
\newblock Neural architecture search with reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\end{thebibliography}
