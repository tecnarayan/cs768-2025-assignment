@inproceedings{tan2021long-form,
  author       = {Bowen Tan and
                  Zichao Yang and
                  Maruan Al{-}Shedivat and
                  Eric P. Xing and
                  Zhiting Hu},
  editor       = {Kristina Toutanova and
                  Anna Rumshisky and
                  Luke Zettlemoyer and
                  Dilek Hakkani{-}T{\"{u}}r and
                  Iz Beltagy and
                  Steven Bethard and
                  Ryan Cotterell and
                  Tanmoy Chakraborty and
                  Yichao Zhou},
  title        = {Progressive Generation of Long Text with Pretrained Language Models},
  booktitle    = {Proceedings of the 2021 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2021, Online, June 6-11, 2021},
  pages        = {4313--4324},
  publisher    = {Association for Computational Linguistics},
  year         = {2021}
}

@article{dong2023bamboo,
  author       = {Zican Dong and
                  Tianyi Tang and
                  Junyi Li and
                  Wayne Xin Zhao and
                  Ji{-}Rong Wen},
  title        = {{BAMBOO:} {A} Comprehensive Benchmark for Evaluating Long Text Modeling
                  Capacities of Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2309.13345},
  year         = {2023}
}

@article{bai2023longbench,
  author       = {Yushi Bai and
                  Xin Lv and
                  Jiajie Zhang and
                  Hongchang Lyu and
                  Jiankai Tang and
                  Zhidian Huang and
                  Zhengxiao Du and
                  Xiao Liu and
                  Aohan Zeng and
                  Lei Hou and
                  Yuxiao Dong and
                  Jie Tang and
                  Juanzi Li},
  title        = {LongBench: {A} Bilingual, Multitask Benchmark for Long Context Understanding},
  journal      = {CoRR},
  volume       = {abs/2308.14508},
  year         = {2023}
}

@article{li2023LooGLE,
  author       = {Jiaqi Li and
                  Mengmeng Wang and
                  Zilong Zheng and
                  Muhan Zhang},
  title        = {LooGLE: Can Long-Context Language Models Understand Long Contexts?},
  journal      = {CoRR},
  volume       = {abs/2311.04939},
  year         = {2023}
}


@inproceedings{shaw2018rpe,
  author       = {Peter Shaw and
                  Jakob Uszkoreit and
                  Ashish Vaswani},
  editor       = {Marilyn A. Walker and
                  Heng Ji and
                  Amanda Stent},
  title        = {Self-Attention with Relative Position Representations},
  booktitle    = {Proceedings of the 2018 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short
                  Papers)},
  pages        = {464--468},
  publisher    = {Association for Computational Linguistics},
  year         = {2018}
}


@inproceedings{wang2019spe,
  author       = {Xing Wang and
                  Zhaopeng Tu and
                  Longyue Wang and
                  Shuming Shi},
  editor       = {Kentaro Inui and
                  Jing Jiang and
                  Vincent Ng and
                  Xiaojun Wan},
  title        = {Self-Attention with Structural Position Representations},
  booktitle    = {Proceedings of the 2019 Conference on Empirical Methods in Natural
                  Language Processing and the 9th International Joint Conference on
                  Natural Language Processing, {EMNLP-IJCNLP} 2019, Hong Kong, China,
                  November 3-7, 2019},
  pages        = {1403--1409},
  publisher    = {Association for Computational Linguistics},
  year         = {2019}
}

@article{wang2024templora,
  title={With Greater Text Comes Greater Necessity: Inference-Time Training Helps Long Text Generation},
  author={Wang, Y and Ma, D and Cai, D},
  journal={arXiv preprint arXiv:2401.11504},
  year={2024}
}

@article{zhou2023recurrentgpt,
  title={Recurrentgpt: Interactive generation of (arbitrarily) long text},
  author={Zhou, Wangchunshu and Jiang, Yuchen Eleanor and Cui, Peng and Wang, Tiannan and Xiao, Zhenxin and Hou, Yifan and Cotterell, Ryan and Sachan, Mrinmaya},
  journal={arXiv preprint arXiv:2305.13304},
  year={2023}
}

@inproceedings{shao2024assisting,
      title={{Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models}}, 
      author={Yijia Shao and Yucheng Jiang and Theodore A. Kanell and Peter Xu and Omar Khattab and Monica S. Lam},
      year={2024},
      booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}
}

@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={ACM New York, NY}
}

@article{fan2018hierarchicalstory,
  title={Hierarchical neural story generation},
  author={Fan, Angela and Lewis, Mike and Dauphin, Yann},
  journal={arXiv preprint arXiv:1805.04833},
  year={2018}
}

@article{wu2021recursively,
  title={Recursively summarizing books with human feedback},
  author={Wu, Jeff and Ouyang, Long and Ziegler, Daniel M and Stiennon, Nisan and Lowe, Ryan and Leike, Jan and Christiano, Paul},
  journal={arXiv preprint arXiv:2109.10862},
  year={2021}
}

@article{chang2023booookscore,
  title={BooookScore: A systematic exploration of book-length summarization in the era of LLMs},
  author={Chang, Yapei and Lo, Kyle and Goyal, Tanya and Iyyer, Mohit},
  journal={arXiv preprint arXiv:2310.00785},
  year={2023}
}

@article{cho2018traingcohernt,
  title={Towards coherent and cohesive long-form text generation},
  author={Cho, Woon Sang and Zhang, Pengchuan and Zhang, Yizhe and Li, Xiujun and Galley, Michel and Brockett, Chris and Wang, Mengdi and Gao, Jianfeng},
  journal={arXiv preprint arXiv:1811.00511},
  year={2018}
}

@article{bosselut2018awardcoherent,
  title={Discourse-aware neural rewards for coherent text generation},
  author={Bosselut, Antoine and Celikyilmaz, Asli and He, Xiaodong and Gao, Jianfeng and Huang, Po-Sen and Choi, Yejin},
  journal={arXiv preprint arXiv:1805.03766},
  year={2018}
}

@article{zhou2023contextfaithfulness,
  title={Context-faithful prompting for large language models},
  author={Zhou, Wenxuan and Zhang, Sheng and Poon, Hoifung and Chen, Muhao},
  journal={arXiv preprint arXiv:2303.11315},
  year={2023}
}

@article{liu2024probingstructure,
  title={Probing structured semantics understanding and generation of language models via question answering},
  author={Liu, Jinxin and Cao, Shulin and Shi, Jiaxin and Zhang, Tingjian and Hou, Lei and Li, Juanzi},
  journal={arXiv preprint arXiv:2401.05777},
  year={2024}
}

@article{zhang2024style,
  title={Distilling Text Style Transfer With Self-Explanation From LLMs},
  author={Zhang, Chiyu and Cai, Honglong and Wu, Yuexin and Hou, Le and Abdul-Mageed, Muhammad and others},
  journal={arXiv preprint arXiv:2403.01106},
  year={2024}
}

@article{schramowski2022ethics,
  title={Large pre-trained language models contain human-like biases of what is right and wrong to do},
  author={Schramowski, Patrick and Turan, Cigdem and Andersen, Nico and Rothkopf, Constantin A and Kersting, Kristian},
  journal={Nature Machine Intelligence},
  volume={4},
  number={3},
  pages={258--268},
  year={2022},
}

@article{menick2022teachingcitations,
  title={Teaching language models to support answers with verified quotes},
  author={Menick, Jacob and Trebacz, Maja and Mikulik, Vladimir and Aslanides, John and Song, Francis and Chadwick, Martin and Glaese, Mia and Young, Susannah and Campbell-Gillingham, Lucy and Irving, Geoffrey and others},
  journal={arXiv preprint arXiv:2203.11147},
  year={2022}
}

@article{balepur2023expository,
author={Balepur, Nishant and Huang, Jie and Chang, Kevin Chen-Chuan},
  title={Expository text generation: Imitate, retrieve, paraphrase},
  journal={arXiv preprint arXiv:2305.03276},
  year={2023}
}

@article{wang2019paperrobot,
  title={PaperRobot: Incremental draft generation of scientific ideas},
  author={Wang, Qingyun and Huang, Lifu and Jiang, Zhiying and Knight, Kevin and Ji, Heng and Bansal, Mohit and Luan, Yi},
  journal={arXiv preprint arXiv:1905.07870},
  year={2019}
}



@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{shi2023large,
  title={Large language models can be easily distracted by irrelevant context},
  author={Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed H and Sch{\"a}rli, Nathanael and Zhou, Denny},
  booktitle={International Conference on Machine Learning},
  pages={31210--31227},
  year={2023},
  organization={PMLR}
}

@article{pandalm2024,
      title={PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization}, 
      author={Wang, Yidong and Yu, Zhuohao and Zeng, Zhengran and Yang, Linyi and Wang, Cunxiang and Chen, Hao and Jiang, Chaoya and Xie, Rui and Wang, Jindong and Xie, Xing and Ye, Wei and Zhang, Shikun and Zhang, Yue},
      booktitle={International Conference on Learning Representations (ICLR)},
      year={2024}
}





@inproceedings{kirillov2023segment,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4015--4026},
  year={2023}
}

@article{zheng2024judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{yu2024kieval,
  title={KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models},
  author={Yu, Zhuohao and Gao, Chang and Yao, Wenjin and Wang, Yidong and Ye, Wei and Wang, Jindong and Xie, Xing and Zhang, Yue and Zhang, Shikun},
  booktitle = "Annual Meeting of the Association for Computational Linguistics 2024",
    year = "2024",
}

@inproceedings{li2023long,
  title={How Long Can Context Length of Open-Source LLMs truly Promise?},
  author={Li, Dacheng and Shao, Rulin and Xie, Anze and Sheng, Ying and Zheng, Lianmin and Gonzalez, Joseph and Stoica, Ion and Ma, Xuezhe and Zhang, Hao},
  booktitle={NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following},
  year={2023}
}

@article{li2024long,
  title={Long-context LLMs Struggle with Long In-context Learning},
  author={Li, Tianle and Zhang, Ge and Do, Quy Duc and Yue, Xiang and Chen, Wenhu},
  journal={arXiv preprint arXiv:2404.02060},
  year={2024}
}

@inproceedings{gao2023citation,
    title = "Enabling Large Language Models to Generate Text with Citations",
    author = "Gao, Tianyu  and
      Yen, Howard  and
      Yu, Jiatong  and
      Chen, Danqi",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    year = "2023",
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}

@article{pouyanfar2018survey,
  title={A survey on deep learning: Algorithms, techniques, and applications},
  author={Pouyanfar, Samira and Sadiq, Saad and Yan, Yilin and Tian, Haiman and Tao, Yudong and Reyes, Maria Presa and Shyu, Mei-Ling and Chen, Shu-Ching and Iyengar, Sundaraja S},
  journal={ACM Computing Surveys (CSUR)},
  volume={51},
  number={5},
  pages={1--36},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{chang2023survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  year={2023},
  publisher={ACM New York, NY}
}

@misc{nussbaum2024nomic,
      title={Nomic Embed: Training a Reproducible Long Context Text Embedder}, 
      author={Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar},
      year={2024},
      eprint={2402.01613},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{khan2022transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={ACM computing surveys (CSUR)},
  volume={54},
  number={10s},
  pages={1--41},
  year={2022},
  publisher={ACM New York, NY}
}

@article{chen2023extending,
  title={Extending context window of large language models via positional interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal={arXiv preprint arXiv:2306.15595},
  year={2023}
}

@inproceedings{chen2023longlora,
  title={LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models},
  author={Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{wang2024augmenting,
  title={Augmenting language models with long-term memory},
  author={Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@article{kaddour2023challenges,
  title={Challenges and applications of large language models},
  author={Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  journal={arXiv preprint arXiv:2307.10169},
  year={2023}
}

@article{wang2023surveyfact,
  title={Survey on factuality in large language models: Knowledge, retrieval and domain-specificity},
  author={Wang, Cunxiang and Liu, Xiaoze and Yue, Yuanhao and Tang, Xiangru and Zhang, Tianhang and Jiayang, Cheng and Yao, Yunzhi and Gao, Wenyang and Hu, Xuming and Qi, Zehan and others},
  journal={arXiv preprint arXiv:2310.07521},
  year={2023}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@inproceedings{jiang2023active,
  title={Active Retrieval Augmented Generation},
  author={Jiang, Zhengbao and Xu, Frank F and Gao, Luyu and Sun, Zhiqing and Liu, Qian and Dwivedi-Yu, Jane and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={7969--7992},
  year={2023}
}

@article{gao2023retrieval,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}