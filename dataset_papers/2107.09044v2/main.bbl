\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2018)Agarwal, Beygelzimer, Dudik, Langford, and
  Wallach]{agarwal2018reductions}
Agarwal, A., Beygelzimer, A., Dudik, M., Langford, J., and Wallach, H.
\newblock A reductions approach to fair classification.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  60--69, 2018.

\bibitem[Badgeley et~al.(2019)Badgeley, Zech, Oakden-Rayner, Glicksberg, Liu,
  Gale, McConnell, Percha, Snyder, and Dudley]{badgeley2019deep}
Badgeley, M.~A., Zech, J.~R., Oakden-Rayner, L., Glicksberg, B.~S., Liu, M.,
  Gale, W., McConnell, M.~V., Percha, B., Snyder, T.~M., and Dudley, J.~T.
\newblock Deep learning predicts hip fracture using confounding patient and
  healthcare variables.
\newblock \emph{npj Digital Medicine}, 2, 2019.

\bibitem[Ben-Tal et~al.(2013)Ben-Tal, den Hertog, Waegenaere, Melenberg, and
  Rennen]{bental2013robust}
Ben-Tal, A., den Hertog, D., Waegenaere, A.~D., Melenberg, B., and Rennen, G.
\newblock Robust solutions of optimization problems affected by uncertain
  probabilities.
\newblock \emph{Management Science}, 59:\penalty0 341--357, 2013.

\bibitem[Biggio et~al.(2013)Biggio, Corona, Maiorca, Nelson, {\v{S}}rndi{\'c},
  Laskov, Giacinto, and Roli]{biggio2013evasion}
Biggio, B., Corona, I., Maiorca, D., Nelson, B., {\v{S}}rndi{\'c}, N., Laskov,
  P., Giacinto, G., and Roli, F.
\newblock Evasion attacks against machine learning at test time.
\newblock In \emph{Joint European conference on machine learning and knowledge
  discovery in databases}, pp.\  387--402, 2013.

\bibitem[Blanchard et~al.(2011)Blanchard, Lee, and
  Scott]{blanchard2011generalizing}
Blanchard, G., Lee, G., and Scott, C.
\newblock Generalizing from several related classification tasks to a new
  unlabeled sample.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2178--2186, 2011.

\bibitem[Blodgett et~al.(2016)Blodgett, Green, and O'Connor]{blodgett2016}
Blodgett, S.~L., Green, L., and O'Connor, B.
\newblock Demographic dialectal variation in social media: A case study of
  {A}frican-{A}merican {E}nglish.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)},
  pp.\  1119--1130, 2016.

\bibitem[Borkan et~al.(2019)Borkan, Dixon, Sorensen, Thain, and
  Vasserman]{borkan2019nuanced}
Borkan, D., Dixon, L., Sorensen, J., Thain, N., and Vasserman, L.
\newblock Nuanced metrics for measuring unintended bias with real data for text
  classification.
\newblock In \emph{World Wide Web (WWW)}, pp.\  491--500, 2019.

\bibitem[Byrd \& Lipton(2019)Byrd and Lipton]{byrd2019effect}
Byrd, J. and Lipton, Z.
\newblock What is the effect of importance weighting in deep learning?
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  872--881, 2019.

\bibitem[Cao et~al.(2019)Cao, Wei, Gaidon, Arechiga, and Ma]{cao2019learning}
Cao, K., Wei, C., Gaidon, A., Arechiga, N., and Ma, T.
\newblock Learning imbalanced datasets with label-distribution-aware margin
  loss.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Cao et~al.(2020)Cao, Chen, Lu, Arechiga, Gaidon, and
  Ma]{cao2020heteroskedastic}
Cao, K., Chen, Y., Lu, J., Arechiga, N., Gaidon, A., and Ma, T.
\newblock Heteroskedastic and imbalanced deep learning with adaptive
  regularization.
\newblock \emph{arXiv preprint arXiv:2006.15766}, 2020.

\bibitem[Creager et~al.(2021)Creager, Jacobsen, and
  Zemel]{creager2021environment}
Creager, E., Jacobsen, J.-H., and Zemel, R.
\newblock Environment inference for invariant learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  2189--2200, 2021.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Association for Computational Linguistics (ACL)}, pp.\
  4171--4186, 2019.

\bibitem[Duchi et~al.(2016)Duchi, Glynn, and Namkoong]{duchi2016}
Duchi, J., Glynn, P., and Namkoong, H.
\newblock Statistics of robust optimization: A generalized empirical likelihood
  approach.
\newblock \emph{arXiv}, 2016.

\bibitem[Duchi et~al.(2019)Duchi, Hashimoto, and
  Namkoong]{duchi2019distributionally}
Duchi, J., Hashimoto, T., and Namkoong, H.
\newblock Distributionally robust losses against mixture covariate shifts.
\newblock
  \url{https://cs.stanford.edu/~thashim/assets/publications/condrisk.pdf},
  2019.

\bibitem[Goel et~al.(2020)Goel, Gu, Li, and R{\'e}]{goel2020model}
Goel, K., Gu, A., Li, Y., and R{\'e}, C.
\newblock Model patching: Closing the subgroup performance gap with data
  augmentation.
\newblock \emph{arXiv preprint arXiv:2008.06775}, 2020.

\bibitem[Gururangan et~al.(2018)Gururangan, Swayamdipta, Levy, Schwartz,
  Bowman, and Smith]{gururangan2018annotation}
Gururangan, S., Swayamdipta, S., Levy, O., Schwartz, R., Bowman, S., and Smith,
  N.~A.
\newblock Annotation artifacts in natural language inference data.
\newblock In \emph{Association for Computational Linguistics (ACL)}, pp.\
  107--112, 2018.

\bibitem[Hardt et~al.(2016)Hardt, Price, and Srebo]{hardt2016}
Hardt, M., Price, E., and Srebo, N.
\newblock Equality of opportunity in supervised learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  3315--3323, 2016.

\bibitem[Hashimoto et~al.(2018)Hashimoto, Srivastava, Namkoong, and
  Liang]{hashimoto2018repeated}
Hashimoto, T.~B., Srivastava, M., Namkoong, H., and Liang, P.
\newblock Fairness without demographics in repeated loss minimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR)}, 2016.

\bibitem[Hovy \& Søgaard(2015)Hovy and Søgaard]{hovy2015}
Hovy, D. and Søgaard, A.
\newblock Tagging performance correlates with age.
\newblock In \emph{Association for Computational Linguistics (ACL)}, pp.\
  483--488, 2015.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  448--456, 2015.

\bibitem[Khani et~al.(2019)Khani, Raghunathan, and Liang]{khani2019mwld}
Khani, F., Raghunathan, A., and Liang, P.
\newblock Maximum weighted loss discrepancy.
\newblock \emph{arXiv preprint arXiv:1906.03518}, 2019.

\bibitem[Kim et~al.(2019)Kim, Ghorbani, and Zou]{kim2019multiaccuracy}
Kim, M.~P., Ghorbani, A., and Zou, J.
\newblock Multiaccuracy: Black-box post-processing for fairness in
  classification.
\newblock In \emph{Association for the Advancement of Artificial Intelligence
  (AAAI)}, pp.\  247--254, 2019.

\bibitem[Koh et~al.(2021)Koh, Sagawa, Marklund, Xie, Zhang, Balsubramani, Hu,
  Yasunaga, Phillips, Gao, Lee, David, Stavness, Guo, Earnshaw, Haque, Beery,
  Leskovec, Kundaje, Pierson, Levine, Finn, and Liang]{koh2021wilds}
Koh, P.~W., Sagawa, S., Marklund, H., Xie, S.~M., Zhang, M., Balsubramani, A.,
  Hu, W., Yasunaga, M., Phillips, R.~L., Gao, I., Lee, T., David, E., Stavness,
  I., Guo, W., Earnshaw, B.~A., Haque, I.~S., Beery, S., Leskovec, J., Kundaje,
  A., Pierson, E., Levine, S., Finn, C., and Liang, P.
\newblock {WILDS}: A benchmark of in-the-wild distribution shifts.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Lam \& Zhou(2015)Lam and Zhou]{lam2015quantifying}
Lam, H. and Zhou, E.
\newblock Quantifying input uncertainty in stochastic optimization.
\newblock In \emph{2015 Winter Simulation Conference}, 2015.

\bibitem[Levy et~al.(2020)Levy, Carmon, Duchi, and Sidford]{levy2020large}
Levy, D., Carmon, Y., Duchi, J.~C., and Sidford, A.
\newblock Large-scale methods for distributionally robust optimization.
\newblock \emph{arXiv preprint arXiv:2010.05893}, 2020.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015deep}
Liu, Z., Luo, P., Wang, X., and Tang, X.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  3730--3738, 2015.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017sgdr}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[McCoy et~al.(2019)McCoy, Pavlick, and Linzen]{mccoy2019right}
McCoy, R.~T., Pavlick, E., and Linzen, T.
\newblock Right for the wrong reasons: Diagnosing syntactic heuristics in
  natural language inference.
\newblock In \emph{Association for Computational Linguistics (ACL)}, 2019.

\bibitem[Mohri et~al.(2019)Mohri, Sivek, and Suresh]{mohri2019agnostic}
Mohri, M., Sivek, G., and Suresh, A.~T.
\newblock Agnostic federated learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  4615--4625, 2019.

\bibitem[Muandet et~al.(2013)Muandet, Balduzzi, and
  Sch{\"o}lkopf]{muandet2013domain}
Muandet, K., Balduzzi, D., and Sch{\"o}lkopf, B.
\newblock Domain generalization via invariant feature representation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  10--18, 2013.

\bibitem[Nam et~al.(2020)Nam, Cha, Ahn, Lee, and Shin]{nam2020learning}
Nam, J., Cha, H., Ahn, S., Lee, J., and Shin, J.
\newblock Learning from failure: Training debiased classifier from biased
  classifier.
\newblock \emph{arXiv preprint arXiv:2007.02561}, 2020.

\bibitem[Namkoong \& Duchi(2017)Namkoong and Duchi]{namkoong2017variance}
Namkoong, H. and Duchi, J.
\newblock Variance regularization with convex objectives.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2017.

\bibitem[Oakden-Rayner et~al.(2020)Oakden-Rayner, Dunnmon, Carneiro, and
  R{\'e}]{oakden2020hidden}
Oakden-Rayner, L., Dunnmon, J., Carneiro, G., and R{\'e}, C.
\newblock Hidden stratification causes clinically meaningful failures in
  machine learning for medical imaging.
\newblock In \emph{Proceedings of the ACM Conference on Health, Inference, and
  Learning}, pp.\  151--159, 2020.

\bibitem[Oren et~al.(2019)Oren, Sagawa, Hashimoto, and Liang]{oren2019drolm}
Oren, Y., Sagawa, S., Hashimoto, T., and Liang, P.
\newblock Distributionally robust language modeling.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)},
  2019.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch, 2017.

\bibitem[Pezeshki et~al.(2020)Pezeshki, Kaba, Bengio, Courville, Precup, and
  Lajoie]{pezeshki2020gradient}
Pezeshki, M., Kaba, S.-O., Bengio, Y., Courville, A., Precup, D., and Lajoie,
  G.
\newblock Gradient starvation: A learning proclivity in neural networks.
\newblock \emph{arXiv preprint arXiv:2011.09468}, 2020.

\bibitem[Pleiss et~al.(2017)Pleiss, Raghavan, Wu, Kleinberg, and
  Weinberger]{pleiss2017}
Pleiss, G., Raghavan, M., Wu, F., Kleinberg, J., and Weinberger, K.~Q.
\newblock On fairness and calibration.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  5684--5693, 2017.

\bibitem[Ren et~al.(2018)Ren, Zeng, Yang, and Urtasun]{ren2018reweighting}
Ren, M., Zeng, W., Yang, B., and Urtasun, R.
\newblock Learning to reweight examples for robust deep learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Rockafellar \& Uryasev(2000)Rockafellar and
  Uryasev]{rockafellar2000optimization}
Rockafellar, R.~T. and Uryasev, S.
\newblock Optimization of conditional value-at-risk.
\newblock \emph{Journal of Risk}, 2:\penalty0 21--41, 2000.

\bibitem[Sagawa et~al.(2020{\natexlab{a}})Sagawa, Koh, Hashimoto, and
  Liang]{sagawa2020group}
Sagawa, S., Koh, P.~W., Hashimoto, T.~B., and Liang, P.
\newblock Distributionally robust neural networks for group shifts: On the
  importance of regularization for worst-case generalization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020{\natexlab{a}}.

\bibitem[Sagawa et~al.(2020{\natexlab{b}})Sagawa, Raghunathan, Koh, and
  Liang]{sagawa2020overparameterization}
Sagawa, S., Raghunathan, A., Koh, P.~W., and Liang, P.
\newblock An investigation of why overparameterization exacerbates spurious
  correlations.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2020{\natexlab{b}}.

\bibitem[Shimodaira(2000)]{shimodaira2000improving}
Shimodaira, H.
\newblock Improving predictive inference under covariate shift by weighting the
  log-likelihood function.
\newblock \emph{Journal of Statistical Planning and Inference}, 90:\penalty0
  227--244, 2000.

\bibitem[Shu et~al.(2019)Shu, Xie, Yi, Zhao, Zhou, Xu, and Meng]{shu2019meta}
Shu, J., Xie, Q., Yi, L., Zhao, Q., Zhou, S., Xu, Z., and Meng, D.
\newblock {Meta-Weight-Net}: Learning an explicit mapping for sample weighting.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  1919--1930, 2019.

\bibitem[Sohoni et~al.(2020)Sohoni, Dunnmon, Angus, Gu, and
  R{\'e}]{sohoni2020no}
Sohoni, N.~S., Dunnmon, J.~A., Angus, G., Gu, A., and R{\'e}, C.
\newblock No subclass left behind: Fine-grained robustness in coarse-grained
  classification problems.
\newblock \emph{arXiv preprint arXiv:2011.12945}, 2020.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2014intriguing}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
  and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2014.

\bibitem[Tatman(2017)]{tatman2017}
Tatman, R.
\newblock Gender and dialect bias in {Y}ou{T}ube's automatic captions.
\newblock In \emph{Workshop on Ethics in Natural Langauge Processing},
  volume~1, pp.\  53--59, 2017.

\bibitem[Utama et~al.(2020)Utama, Moosavi, and Gurevych]{utama2020towards}
Utama, P.~A., Moosavi, N.~S., and Gurevych, I.
\newblock Towards debiasing {NLU} models from unknown biases.
\newblock \emph{arXiv preprint arXiv:2009.12303}, 2020.

\bibitem[Wah et~al.(2011)Wah, Branson, Welinder, Perona, and
  Belongie]{wah2011cub}
Wah, C., Branson, S., Welinder, P., Perona, P., and Belongie, S.
\newblock The {Caltech}-{UCSD} {Birds}-200-2011 dataset.
\newblock Technical report, California Institute of Technology, 2011.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{williams2018broad}
Williams, A., Nangia, N., and Bowman, S.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{Association for Computational Linguistics (ACL)}, pp.\
  1112--1122, 2018.

\bibitem[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, and Brew]{wolf2019transformers}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
  Rault, T., Louf, R., Funtowicz, M., and Brew, J.
\newblock {HuggingFace}'s transformers: State-of-the-art natural language
  processing.
\newblock \emph{arXiv preprint arXiv:1910.03771}, 2019.

\bibitem[Woodworth et~al.(2017)Woodworth, Gunasekar, Ohannessian, and
  Srebro]{woodworth2017}
Woodworth, B., Gunasekar, S., Ohannessian, M.~I., and Srebro, N.
\newblock Learning non-discriminatory predictors.
\newblock In \emph{Conference on Learning Theory (COLT)}, pp.\  1920--1953,
  2017.

\bibitem[Yaghoobzadeh et~al.(2019)Yaghoobzadeh, Mehri, Tachet, Hazen, and
  Sordoni]{yaghoobzadeh2019increasing}
Yaghoobzadeh, Y., Mehri, S., Tachet, R., Hazen, T.~J., and Sordoni, A.
\newblock Increasing robustness to spurious correlations using forgettable
  examples.
\newblock \emph{arXiv preprint arXiv:1911.03861}, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Menon, Veit, Bhojanapalli, Kumar, and
  Sra]{zhang2020coping}
Zhang, J., Menon, A., Veit, A., Bhojanapalli, S., Kumar, S., and Sra, S.
\newblock Coping with label shift via distributionally robust optimisation.
\newblock \emph{arXiv preprint arXiv:2010.12230}, 2020.

\bibitem[Zhang \& Sabuncu(2018)Zhang and Sabuncu]{zhang2018generalized}
Zhang, Z. and Sabuncu, M.~R.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Zhou et~al.(2017)Zhou, Lapedriza, Khosla, Oliva, and
  Torralba]{zhou2017places}
Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., and Torralba, A.
\newblock Places: A 10 million image database for scene recognition.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 40\penalty0 (6):\penalty0 1452--1464, 2017.

\end{thebibliography}
