\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aitchison(2019)]{aitchison2019bigger}
Aitchison, L.
\newblock Why bigger is not always better: on finite and infinite neural
  networks.
\newblock \emph{arXiv preprint arXiv:1910.08013}, 2019.

\bibitem[Aitchison(2020)]{aitchison2020statistical}
Aitchison, L.
\newblock A statistical theory of cold posteriors in deep neural networks.
\newblock \emph{arXiv preprint arXiv:2008.05912}, 2020.

\bibitem[Aitchison et~al.(2020)Aitchison, Yang, and Ober]{aitchison2020deep}
Aitchison, L., Yang, A.~X., and Ober, S.~W.
\newblock Deep kernel processes.
\newblock \emph{arXiv preprint arXiv:2010.01590}, 2020.

\bibitem[Ashukha et~al.(2020)Ashukha, Lyzhov, Molchanov, and
  Vetrov]{ashukha2020pitfalls}
Ashukha, A., Lyzhov, A., Molchanov, D., and Vetrov, D.
\newblock Pitfalls of in-domain uncertainty estimation and ensembling in deep
  learning.
\newblock \emph{arXiv preprint arXiv:2002.06470}, 2020.

\bibitem[Bae et~al.(2018)Bae, Zhang, and Grosse]{bae2018eigenvalue}
Bae, J., Zhang, G., and Grosse, R.
\newblock Eigenvalue corrected noisy natural gradient.
\newblock \emph{arXiv preprint arXiv:1811.12565}, 2018.

\bibitem[Bartlett(1933)]{bartlett1933on}
Bartlett, M.~S.
\newblock On the theory of statistical regression.
\newblock \emph{Proceedings of the Royal Society of Edinburgh}, 53:\penalty0
  260--283, 1933.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell2015weight}
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D.
\newblock Weight uncertainty in neural networks.
\newblock \emph{arXiv preprint arXiv:1505.05424}, 2015.

\bibitem[Burt et~al.(2020)Burt, Rasmussen, and van~der
  Wilk]{burt2020convergence}
Burt, D.~R., Rasmussen, C.~E., and van~der Wilk, M.
\newblock Convergence of sparse variational inference in {G}aussian processes
  regression.
\newblock \emph{Journal of Machine Learning Research}, 21:\penalty0 1--63,
  2020.

\bibitem[Chafaï(2015)]{chafai_2015}
Chafaï, D.
\newblock Bartlett decomposition and other factorizations, 2015.
\newblock URL
  \url{http://djalil.chafai.net/blog/2015/10/20/bartlett-decomposition-and-other-factorizations/}.

\bibitem[Damianou \& Lawrence(2013)Damianou and Lawrence]{damianou2013deep}
Damianou, A. and Lawrence, N.
\newblock Deep {G}aussian processes.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  207--215,
  2013.

\bibitem[Dusenberry et~al.(2020)Dusenberry, Jerfel, Wen, Ma, Snoek, Heller,
  Lakshminarayanan, and Tran]{dusenberry2020efficient}
Dusenberry, M.~W., Jerfel, G., Wen, Y., Ma, Y.-a., Snoek, J., Heller, K.,
  Lakshminarayanan, B., and Tran, D.
\newblock Efficient and scalable {B}ayesian neural nets with rank-1 factors.
\newblock \emph{arXiv preprint arXiv:2005.07186}, 2020.

\bibitem[Dutordoir et~al.(2019)Dutordoir, van~der Wilk, Artemev, Tomczak, and
  Hensman]{dutordoir2019translation}
Dutordoir, V., van~der Wilk, M., Artemev, A., Tomczak, M., and Hensman, J.
\newblock Translation insensitivity for deep convolutional {G}aussian
  processes.
\newblock \emph{arXiv preprint arXiv:1902.05888}, 2019.

\bibitem[Farquhar et~al.(2020)Farquhar, Smith, and Gal]{farquhar2020try}
Farquhar, S., Smith, L., and Gal, Y.
\newblock Try depth instead of weight correlations: Mean-field is a less
  restrictive assumption for deeper networks.
\newblock \emph{arXiv preprint arXiv:2002.03704}, 2020.

\bibitem[Foong et~al.(2019{\natexlab{a}})Foong, Burt, Li, and
  Turner]{foong2019pathologies}
Foong, A.~Y., Burt, D.~R., Li, Y., and Turner, R.~E.
\newblock Pathologies of factorised {G}aussian and {MC} dropout posteriors in
  {B}ayesian neural networks.
\newblock \emph{arXiv preprint arXiv:1909.00719}, 2019{\natexlab{a}}.

\bibitem[Foong et~al.(2019{\natexlab{b}})Foong, Li, Hern{\'a}ndez-Lobato, and
  Turner]{foong2019between}
Foong, A.~Y., Li, Y., Hern{\'a}ndez-Lobato, J.~M., and Turner, R.~E.
\newblock 'in-between' uncertainty in {B}ayesian neural networks.
\newblock \emph{arXiv preprint arXiv:1906.11537}, 2019{\natexlab{b}}.

\bibitem[Fortuin et~al.(2021)Fortuin, Garriga-Alonso, Wenzel, R{\"a}tsch,
  Turner, van~der Wilk, and Aitchison]{fortuin2021bayesian}
Fortuin, V., Garriga-Alonso, A., Wenzel, F., R{\"a}tsch, G., Turner, R.,
  van~der Wilk, M., and Aitchison, L.
\newblock Bayesian neural network priors revisited.
\newblock \emph{arXiv preprint arXiv:2102.06571}, 2021.

\bibitem[Gal \& Ghahramani(2015)Gal and Ghahramani]{gal2015dropout}
Gal, Y. and Ghahramani, Z.
\newblock Dropout as a {B}ayesian approximation: Representing model uncertainty
  in deep learning.
\newblock \emph{arXiv preprint arXiv:1506.02142}, 2015.

\bibitem[Ghosh \& Doshi-Velez(2017)Ghosh and Doshi-Velez]{ghosh2017model}
Ghosh, S. and Doshi-Velez, F.
\newblock Model selection in {B}ayesian neural networks via horseshoe priors.
\newblock \emph{arXiv preprint arXiv:1705.10388}, 2017.

\bibitem[Graves(2011)]{graves2011practical}
Graves, A.
\newblock Practical variational inference for neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2348--2356, 2011.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1321--1330. JMLR. org, 2017.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  {I}mage{N}et classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  1026--1034, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Heinonen et~al.(2016)Heinonen, Mannerstr{\"o}m, Rousu, Kaski, and
  L{\"a}hdesm{\"a}ki]{heinonen2016non}
Heinonen, M., Mannerstr{\"o}m, H., Rousu, J., Kaski, S., and
  L{\"a}hdesm{\"a}ki, H.
\newblock Non-stationary {G}aussian process regression with {H}amiltonian
  {M}onte {C}arlo.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  732--740.
  PMLR, 2016.

\bibitem[Hern{\'a}ndez-Lobato \& Adams(2015)Hern{\'a}ndez-Lobato and
  Adams]{hernandez2015probabilistic}
Hern{\'a}ndez-Lobato, J.~M. and Adams, R.
\newblock Probabilistic backpropagation for scalable learning of {B}ayesian
  neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1861--1869, 2015.

\bibitem[Hinton \& Van~Camp(1993)Hinton and Van~Camp]{hinton1993keeping}
Hinton, G.~E. and Van~Camp, D.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In \emph{Proceedings of the sixth annual conference on Computational
  learning theory}, pp.\  5--13, 1993.

\bibitem[Hoffman \& Blei(2015)Hoffman and Blei]{hoffman2015structured}
Hoffman, M.~D. and Blei, D.~M.
\newblock Structured stochastic variational inference.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  361--369,
  2015.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  448--456. PMLR, 2015.

\bibitem[Jordan et~al.(1999)Jordan, Ghahramani, Jaakkola, and
  Saul]{jordan1999introduction}
Jordan, M.~I., Ghahramani, Z., Jaakkola, T.~S., and Saul, L.~K.
\newblock An introduction to variational methods for graphical models.
\newblock \emph{Machine learning}, 37\penalty0 (2):\penalty0 183--233, 1999.

\bibitem[Karaletsos \& Bui(2020)Karaletsos and Bui]{karaletsos2020hierarchical}
Karaletsos, T. and Bui, T.~D.
\newblock Hierarchical {G}aussian process priors for {B}ayesian neural network
  weights.
\newblock \emph{arXiv preprint arXiv:2002.04033}, 2020.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and
  Welling]{kingma2015variational}
Kingma, D.~P., Salimans, T., and Welling, M.
\newblock Variational dropout and the local reparameterization trick.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2575--2583, 2015.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem[Krueger et~al.(2017)Krueger, Huang, Islam, Turner, Lacoste, and
  Courville]{krueger2017bayesian}
Krueger, D., Huang, C.-W., Islam, R., Turner, R., Lacoste, A., and Courville,
  A.
\newblock Bayesian hypernetworks.
\newblock \emph{arXiv preprint arXiv:1710.04759}, 2017.

\bibitem[Li et~al.(2019)Li, Wang, Yu, Du, Hu, Salakhutdinov, and
  Arora]{li2019enhanced}
Li, Z., Wang, R., Yu, D., Du, S.~S., Hu, W., Salakhutdinov, R., and Arora, S.
\newblock Enhanced convolutional neural tangent kernels.
\newblock \emph{arXiv preprint arXiv:1911.00809}, 2019.

\bibitem[Lindinger et~al.(2020)Lindinger, Reeb, Lippert, and
  Rakitsch]{lindinger2020beyond}
Lindinger, J., Reeb, D., Lippert, C., and Rakitsch, B.
\newblock Beyond the mean-field: Structured deep {G}aussian processes improve
  the predictive uncertainties.
\newblock \emph{arXiv preprint arXiv:2005.11110}, 2020.

\bibitem[Louizos \& Welling(2016)Louizos and Welling]{louizos2016structured}
Louizos, C. and Welling, M.
\newblock Structured and efficient variational deep learning with matrix
  {G}aussian posteriors.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1708--1716, 2016.

\bibitem[Louizos \& Welling(2017)Louizos and
  Welling]{louizos2017multiplicative}
Louizos, C. and Welling, M.
\newblock Multiplicative normalizing flows for variational {B}ayesian neural
  networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2218--2227. JMLR. org, 2017.

\bibitem[Nabarro et~al.(2021)Nabarro, Ganev, Garriga-Alonso, Fortuin, van~der
  Wilk, and Aitchison]{nabarro2021data}
Nabarro, S., Ganev, S., Garriga-Alonso, A., Fortuin, V., van~der Wilk, M., and
  Aitchison, L.
\newblock Data augmentation in {B}ayesian neural networks and the cold
  posterior effect.
\newblock \emph{arXiv preprint}, 2021.

\bibitem[Naeini et~al.(2015)Naeini, Cooper, and
  Hauskrecht]{naeini2015obtaining}
Naeini, M.~P., Cooper, G., and Hauskrecht, M.
\newblock Obtaining well calibrated probabilities using {B}ayesian binning.
\newblock In \emph{Twenty-Ninth AAAI Conference on Artificial Intelligence},
  2015.

\bibitem[Neal(1996)]{neal1996priors}
Neal, R.~M.
\newblock Priors for infinite networks.
\newblock In \emph{Bayesian Learning for Neural Networks}, pp.\  29--53.
  Springer, 1996.

\bibitem[Neal et~al.(2011)]{neal2011mcmc}
Neal, R.~M. et~al.
\newblock {MCMC} using {H}amiltonian dynamics.
\newblock \emph{Handbook of Markov chain Monte Carlo}, 2\penalty0
  (11):\penalty0 2, 2011.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.~Y.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Ober et~al.(2021)Ober, Rasmussen, and van~der Wilk]{ober2021promises}
Ober, S.~W., Rasmussen, C.~E., and van~der Wilk, M.
\newblock The promises and pitfalls of deep kernel learning.
\newblock \emph{arXiv preprint arXiv:2102.12108}, 2021.

\bibitem[Osawa et~al.(2019)Osawa, Swaroop, Khan, Jain, Eschenhagen, Turner, and
  Yokota]{osawa2019practical}
Osawa, K., Swaroop, S., Khan, M. E.~E., Jain, A., Eschenhagen, R., Turner,
  R.~E., and Yokota, R.
\newblock Practical deep learning with {B}ayesian principles.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4289--4301, 2019.

\bibitem[Pawlowski et~al.(2017)Pawlowski, Brock, Lee, Rajchl, and
  Glocker]{pawlowski2017implicit}
Pawlowski, N., Brock, A., Lee, M.~C., Rajchl, M., and Glocker, B.
\newblock Implicit weight uncertainty in neural networks.
\newblock \emph{arXiv preprint arXiv:1711.01297}, 2017.

\bibitem[Pearl(1988)]{pearl1988probabilistic}
Pearl, J.
\newblock \emph{Probabilistic Reasoning in Intelligent Systems: Networks of
  Plausible Inference}.
\newblock Morgan Kaufmann, 1988.

\bibitem[Ranganath et~al.(2016)Ranganath, Tran, and
  Blei]{ranganath2016hierarchical}
Ranganath, R., Tran, D., and Blei, D.
\newblock Hierarchical variational models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  324--333. PMLR, 2016.

\bibitem[Rasmussen \& Williams(2006)Rasmussen and
  Williams]{rasmussen2006gaussian}
Rasmussen, C.~E. and Williams, C.~K.
\newblock Gaussian {P}rocesses for {M}achine {L}earning.
\newblock \emph{ISBN-13 978-0-262-18253-9}, 2006.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock \emph{arXiv preprint arXiv:1401.4082}, 2014.

\bibitem[Ritter et~al.(2018)Ritter, Botev, and Barber]{ritter2018scalable}
Ritter, H., Botev, A., and Barber, D.
\newblock A scalable {L}aplace approximation for neural networks.
\newblock In \emph{6th International Conference on Learning Representations,
  ICLR 2018-Conference Track Proceedings}, volume~6. International Conference
  on Representation Learning, 2018.

\bibitem[Salimbeni \& Deisenroth(2017)Salimbeni and
  Deisenroth]{salimbeni2017doubly}
Salimbeni, H. and Deisenroth, M.
\newblock Doubly stochastic variational inference for deep {G}aussian
  processes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4588--4599, 2017.

\bibitem[Shi et~al.(2019)Shi, Titsias, and Mnih]{shi2019sparse}
Shi, J., Titsias, M.~K., and Mnih, A.
\newblock Sparse orthogonal variational inference for {G}aussian processes.
\newblock \emph{arXiv preprint arXiv:1910.10596}, 2019.

\bibitem[Shridhar et~al.(2019)Shridhar, Laumann, and
  Liwicki]{shridhar2019comprehensive}
Shridhar, K., Laumann, F., and Liwicki, M.
\newblock A comprehensive guide to {B}ayesian convolutional neural network with
  variational inference.
\newblock \emph{arXiv preprint arXiv:1901.02731}, 2019.

\bibitem[Sobolev \& Vetrov(2019)Sobolev and Vetrov]{sobolev2019importance}
Sobolev, A. and Vetrov, D.
\newblock Importance weighted hierarchical variational inference.
\newblock \emph{arXiv preprint arXiv:1905.03290}, 2019.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{tieleman2012lecture}
Tieleman, T. and Hinton, G.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, 4\penalty0
  (2):\penalty0 26--31, 2012.

\bibitem[Trippe \& Turner(2018)Trippe and Turner]{trippe2018overpruning}
Trippe, B. and Turner, R.
\newblock Overpruning in variational {B}ayesian neural networks.
\newblock \emph{arXiv preprint arXiv:1801.06230}, 2018.

\bibitem[Ustyuzhaninov et~al.(2020)Ustyuzhaninov, Kazlauskaite, Kaiser, Bodin,
  Campbell, and Ek]{ustyuzhaninov2019compositional}
Ustyuzhaninov, I., Kazlauskaite, I., Kaiser, M., Bodin, E., Campbell, N.~D.,
  and Ek, C.~H.
\newblock Compositional uncertainty in deep {G}aussian processes.
\newblock \emph{UAI}, 2020.

\bibitem[Wenzel et~al.(2020)Wenzel, Roth, Veeling, {\'S}wi{\k{a}}tkowski, Tran,
  Mandt, Snoek, Salimans, Jenatton, and Nowozin]{wenzel2020good}
Wenzel, F., Roth, K., Veeling, B.~S., {\'S}wi{\k{a}}tkowski, J., Tran, L.,
  Mandt, S., Snoek, J., Salimans, T., Jenatton, R., and Nowozin, S.
\newblock How good is the {B}ayes posterior in deep neural networks really?
\newblock \emph{arXiv preprint arXiv:2002.02405}, 2020.

\bibitem[Yao et~al.(2019)Yao, Pan, Ghosh, and Doshi-Velez]{yao2019quality}
Yao, J., Pan, W., Ghosh, S., and Doshi-Velez, F.
\newblock Quality of uncertainty quantification for {B}ayesian neural network
  inference.
\newblock \emph{arXiv preprint arXiv:1906.09686}, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Sun, Duvenaud, and Grosse]{zhang2017noisy}
Zhang, G., Sun, S., Duvenaud, D., and Grosse, R.
\newblock Noisy natural gradient as variational inference.
\newblock \emph{arXiv preprint arXiv:1712.02390}, 2017.

\end{thebibliography}
