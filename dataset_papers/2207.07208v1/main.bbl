\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andriushchenko \& Hein(2019)Andriushchenko and
  Hein]{andriushchenko2019provably}
Andriushchenko, M. and Hein, M.
\newblock Provably robust boosted decision stumps and trees against adversarial
  attacks.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Athalye et~al.(2018)Athalye, Carlini, and Wagner]{AthEtAl2018}
Athalye, A., Carlini, N., and Wagner, D.~A.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock In \emph{ICML}, 2018.

\bibitem[Bertsimas et~al.(2018)Bertsimas, Dunn, Pawlowski, and
  Zhuo]{bertsimas2018robust}
Bertsimas, D., Dunn, J., Pawlowski, C., and Zhuo, Y.~D.
\newblock Robust classification.
\newblock \emph{INFORMS Journal on Optimization}, 1:\penalty0 2--34, 2018.

\bibitem[Biggio et~al.(2013)Biggio, Corona, Maiorca, Nelson, {\v{S}}rndi{\'c},
  Laskov, Giacinto, and Roli]{biggio2013evasion}
Biggio, B., Corona, I., Maiorca, D., Nelson, B., {\v{S}}rndi{\'c}, N., Laskov,
  P., Giacinto, G., and Roli, F.
\newblock Evasion attacks against machine learning at test time.
\newblock In \emph{Joint European conference on machine learning and knowledge
  discovery in databases}, pp.\  387--402. Springer, 2013.

\bibitem[Carlini et~al.(2019)Carlini, Athalye, Papernot, Brendel, Rauber,
  Tsipras, Goodfellow, Madry, and Kurakin]{carlini2019evaluating}
Carlini, N., Athalye, A., Papernot, N., Brendel, W., Rauber, J., Tsipras, D.,
  Goodfellow, I., Madry, A., and Kurakin, A.
\newblock On evaluating adversarial robustness.
\newblock \emph{arXiv preprint arXiv:1902.06705}, 2019.

\bibitem[Chen et~al.(2019)Chen, Zhang, Boning, and Hsieh]{chen2019robust}
Chen, H., Zhang, H., Boning, D., and Hsieh, C.-J.
\newblock Robust decision trees against adversarial examples.
\newblock In \emph{ICML}, 2019.

\bibitem[Cohen et~al.(2019)Cohen, Rosenfeld, and Kolter]{CohenARXIV2019}
Cohen, J.~M., Rosenfeld, E., and Kolter, J.~Z.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Crammer et~al.(2003)Crammer, Gilad-bachrach, Navot, and
  Tishby]{NIPS2002_bbaa9d6a}
Crammer, K., Gilad-bachrach, R., Navot, A., and Tishby, N.
\newblock Margin analysis of the lvq algorithm.
\newblock In \emph{NeurIPS}, 2003.

\bibitem[Croce \& Hein(2020{\natexlab{a}})Croce and Hein]{croce2020provable}
Croce, F. and Hein, M.
\newblock Provable robustness against all adversarial $l_p$-perturbations for
  $p\geq 1$.
\newblock In \emph{ICLR}, 2020{\natexlab{a}}.

\bibitem[Croce \& Hein(2020{\natexlab{b}})Croce and Hein]{croce2020reliable}
Croce, F. and Hein, M.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks, 2020{\natexlab{b}}.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and Szegedy]{GooShlSze2015}
Goodfellow, I.~J., Shlens, J., and Szegedy, C.
\newblock Explaining and harnessing adversarial examples.
\newblock In \emph{ICLR}, 2015.

\bibitem[Gowal et~al.(2018)Gowal, Dvijotham, Stanforth, Bunel, Qin, Uesato,
  Arandjelovic, Mann, and Kohli]{GowEtAl18}
Gowal, S., Dvijotham, K., Stanforth, R., Bunel, R., Qin, C., Uesato, J.,
  Arandjelovic, R., Mann, T.~A., and Kohli, P.
\newblock On the effectiveness of interval bound propagation for training
  verifiably robust models.
\newblock preprint, arXiv:1810.12715v3, 2018.

\bibitem[Hein \& Andriushchenko(2017)Hein and Andriushchenko]{HeiAnd2017}
Hein, M. and Andriushchenko, M.
\newblock Formal guarantees on the robustness of a classifier against
  adversarial manipulation.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Huang et~al.(2021)Huang, Zhang, Shi, Kolter, and
  Anandkumar]{huang2021training}
Huang, Y., Zhang, H., Shi, Y., Kolter, J.~Z., and Anandkumar, A.
\newblock Training certifiably robust neural networks with efficient local
  lipschitz bounds.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Jeong et~al.(2021)Jeong, Park, Kim, Lee, Kim, and
  Shin]{jeong2021smoothmix}
Jeong, J., Park, S., Kim, M., Lee, H.-C., Kim, D., and Shin, J.
\newblock Smoothmix: Training confidence-calibrated smoothed classifiers for
  certified robustness.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Kantchelian et~al.(2016)Kantchelian, Tygar, and
  Joseph]{kantchelian2016evasion}
Kantchelian, A., Tygar, J., and Joseph, A.
\newblock Evasion and hardening of tree ensemble classifiers.
\newblock In \emph{ICML}, 2016.

\bibitem[Kireev et~al.(2021)Kireev, Andriushchenko, and
  Flammarion]{kireev2022effectiveness}
Kireev, K., Andriushchenko, M., and Flammarion, N.
\newblock On the effectiveness of adversarial training against common
  corruptions.
\newblock \emph{arXiv preprint, arXiv:2103.02325}, 2021.

\bibitem[Kohonen(1995)]{Kohonen1995}
Kohonen, T.
\newblock \emph{Learning Vector Quantization}, pp.\  175--189.
\newblock Springer Berlin Heidelberg, 1995.

\bibitem[Laidlaw et~al.(2021)Laidlaw, Singla, and Feizi]{laidlaw2021perceptual}
Laidlaw, C., Singla, S., and Feizi, S.
\newblock Perceptual adversarial robustness: Defense against unseen threat
  models.
\newblock In \emph{ICLR}, 2021.

\bibitem[Lee et~al.(2020)Lee, Lee, and Park]{Lee2020Lipschitz}
Lee, S., Lee, J., and Park, S.
\newblock Lipschitz-certifiable training with a tight outer bound.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Leino et~al.(2021)Leino, Wang, and
  Fredrikson]{leino2021globallyrobust}
Leino, K., Wang, Z., and Fredrikson, M.
\newblock Globally-robust neural networks.
\newblock In \emph{ICML}, 2021.

\bibitem[Li et~al.(2020)Li, Qi, Xie, and Li]{li2020sokcertified}
Li, L., Qi, X., Xie, T., and Li, B.
\newblock Sok: Certified robustness for deep neural networks.
\newblock \emph{arXiv preprint arXiv:2009.04131}, 2020.

\bibitem[Li et~al.(2019)Li, Haque, Anil, Lucas, Grosse, and
  Jacobsen]{li2019preventing}
Li, Q., Haque, S., Anil, C., Lucas, J., Grosse, R.~B., and Jacobsen, J.-H.
\newblock Preventing gradient attenuation in lipschitz constrained
  convolutional networks.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Mirman et~al.(2018)Mirman, Gehr, and Vechev]{MirGehVec2018}
Mirman, M., Gehr, T., and Vechev, M.
\newblock Differentiable abstract interpretation for provably robust neural
  networks.
\newblock In \emph{ICML}, 2018.

\bibitem[Papernot et~al.(2016)Papernot, McDaniel, and
  Goodfellow]{papernot2016transferability}
Papernot, N., McDaniel, P., and Goodfellow, I.
\newblock Transferability in machine learning: from phenomena to black-box
  attacks using adversarial samples, 2016.

\bibitem[Russu et~al.(2016)Russu, Demontis, Biggio, Fumera, and
  Roli]{russu2016secure}
Russu, P., Demontis, A., Biggio, B., Fumera, G., and Roli, F.
\newblock Secure kernel machines against evasion attacks.
\newblock In \emph{ACM workshop on AI and security}. ACM, 2016.

\bibitem[Saralajew et~al.(2020)Saralajew, Holdijk, and
  Villmann]{Saralajew2020fast}
Saralajew, S., Holdijk, L., and Villmann, T.
\newblock Fast adversarial robustness certification of nearest prototype
  classifiers for arbitrary seminorms.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Singla et~al.(2022)Singla, Singla, and Feizi]{singla2022improved}
Singla, S., Singla, S., and Feizi, S.
\newblock Improved deterministic l2 robustness on {CIFAR}-10 and {CIFAR}-100.
\newblock In \emph{ICLR}, 2022.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{SzeEtAl2014}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
  and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock In \emph{ICLR}, pp.\  2503--2511, 2014.

\bibitem[Tjeng \& Tedrake(2017)Tjeng and Tedrake]{TjeTed2017}
Tjeng, V. and Tedrake, R.
\newblock Verifying neural networks with mixed integer programming.
\newblock preprint, arXiv:1711.07356v1, 2017.

\bibitem[Tramer et~al.(2020)Tramer, Carlini, Brendel, and
  Madry]{tramer2020adaptive}
Tramer, F., Carlini, N., Brendel, W., and Madry, A.
\newblock On adaptive attacks to adversarial example defenses.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Trockman \& Kolter(2021)Trockman and
  Kolter]{trockman2021orthogonalizing}
Trockman, A. and Kolter, J.~Z.
\newblock Orthogonalizing convolutional layers with the cayley transform.
\newblock In \emph{ICLR}, 2021.

\bibitem[Wang et~al.(2019)Wang, Liu, Yi, Zhou, and Hsieh]{wang2019evaluating}
Wang, L., Liu, X., Yi, J., Zhou, Z.-H., and Hsieh, C.-J.
\newblock Evaluating the robustness of nearest neighbor classifiers: A
  primal-dual perspective.
\newblock \emph{arXiv preprint, arXiv:1906.03972}, 2019.

\bibitem[Wang et~al.(2018)Wang, Jha, and Chaudhuri]{wang2018analyzing}
Wang, Y., Jha, S., and Chaudhuri, K.
\newblock Analyzing the robustness of nearest neighbors to adversarial
  examples.
\newblock In \emph{ICML}, 2018.

\bibitem[Wang et~al.(2004)Wang, Bovik, Sheikh, and Simoncelli]{wang2004image}
Wang, Z., Bovik, A.~C., Sheikh, H.~R., and Simoncelli, E.~P.
\newblock Image quality assessment: from error visibility to structural
  similarity.
\newblock \emph{IEEE transactions on image processing}, 13\penalty0
  (4):\penalty0 600--612, 2004.

\bibitem[Wong \& Kolter(2018)Wong and Kolter]{WonKol2018}
Wong, E. and Kolter, J.~Z.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock In \emph{ICML}, 2018.

\bibitem[Wong et~al.(2018)Wong, Schmidt, Metzen, and Kolter]{WonEtAl18}
Wong, E., Schmidt, F., Metzen, J.~H., and Kolter, J.~Z.
\newblock Scaling provable adversarial defenses.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Xiao et~al.(2019)Xiao, Tjeng, Shafiullah, and Madry]{xiao2019training}
Xiao, K.~Y., Tjeng, V., Shafiullah, N.~M., and Madry, A.
\newblock Training for faster adversarial robustness verification via inducing
  relu stability.
\newblock In \emph{ICLR}, 2019.

\bibitem[Xu et~al.(2009)Xu, Caramanis, and Mannor]{XuCarMan2009}
Xu, H., Caramanis, C., and Mannor, S.
\newblock Robustness and regularization of support vector machines.
\newblock \emph{Journal of Machine Learning Research}, 10:\penalty0 1485--1510,
  2009.

\bibitem[Zhang et~al.(2021)Zhang, Cai, Lu, He, and Wang]{pmlr-v139-zhang21b}
Zhang, B., Cai, T., Lu, Z., He, D., and Wang, L.
\newblock Towards certifying l-infinity robustness using neural networks with
  l-inf-dist neurons.
\newblock In \emph{ICML}, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Chen, Xiao, Gowal, Stanforth, Li, Boning, and
  Hsieh]{zhang2019stable}
Zhang, H., Chen, H., Xiao, C., Gowal, S., Stanforth, R., Li, B., Boning, D.,
  and Hsieh, C.-J.
\newblock Towards stable and efficient training of verifiably robust neural
  networks.
\newblock In \emph{ICLR}, 2020.

\bibitem[Zhang et~al.(2018)Zhang, Isola, Efros, Shechtman, and
  Wang]{zhang2018unreasonable}
Zhang, R., Isola, P., Efros, A.~A., Shechtman, E., and Wang, O.
\newblock The unreasonable effectiveness of deep features as a perceptual
  metric.
\newblock In \emph{CVPR}, 2018.

\end{thebibliography}
