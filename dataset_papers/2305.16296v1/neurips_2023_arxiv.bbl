\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ajalloeian and Stich(2020)]{AjallStich}
Ahmad Ajalloeian and Sebastian~U Stich.
\newblock Analysis of {SGD} with biased gradient estimators.
\newblock \emph{arXiv preprint arXiv:2008.00051}, 2020.

\bibitem[Aji and Heafield(2017)]{aji2017sparse}
Alham~Fikri Aji and Kenneth Heafield.
\newblock Sparse communication for distributed gradient descent.
\newblock \emph{arXiv preprint arXiv:1704.05021}, 2017.

\bibitem[Alistarh et~al.(2018)Alistarh, Hoefler, Johansson, Konstantinov,
  Khirirat, and Renggli]{alistarh2018sparse}
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit
  Khirirat, and Cedric Renggli.
\newblock The convergence of sparsified gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~31, pages 5973--5983, 2018.

\bibitem[Beznosikov et~al.(2020)Beznosikov, Horv{\'a}th, Richt{\'a}rik, and
  Safaryan]{BezHorRichSaf}
Aleksandr Beznosikov, Samuel Horv{\'a}th, Peter Richt{\'a}rik, and Mher
  Safaryan.
\newblock On biased compression for distributed learning.
\newblock \emph{arXiv preprint arXiv:2002.12410}, 2020.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and Nocedal]{BotCurNoce}
L\'{e}on Bottou, Frank Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{SIAM Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Chang and Lin(2011)]{chang2011libsvm}
Chih-Chung Chang and Chih-Jen Lin.
\newblock {LIBSVM}: a library for support vector machines.
\newblock \emph{{ACM} {T}ransactions on {I}ntelligent {S}ystems and
  {T}echnology (TIST)}, 2\penalty0 (3):\penalty0 1--27, 2011.

\bibitem[Chen et~al.(2021)Chen, Shen, Huang, and Liu]{chen2021quantized}
Congliang Chen, Li~Shen, Haozhi Huang, and Wei Liu.
\newblock Quantized adam with error feedback.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology (TIST)},
  12\penalty0 (5):\penalty0 1--26, 2021.

\bibitem[Chen et~al.(2017)Chen, Zhang, Sharma, Yi, and
  Hsieh]{ChenZhangSharmaYiHsieh}
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh.
\newblock Zoo: Zeroth order optimization based black-box attacks to deep neural
  networks without training substitute models.
\newblock \emph{Proceedings of the 10th ACM Workshop on Artificial Intelligence
  and Security}, pages 15--26, 2017.

\bibitem[Condat et~al.(2022)Condat, Yi, and Richt{\'a}rik]{CondYiRich}
Laurent Condat, Kai Yi, and Peter Richt{\'a}rik.
\newblock Ef-bv: A unified theory of error feedback and variance reduction
  mechanisms for biased and unbiased compression in distributed optimization.
\newblock \emph{arXiv preprint arXiv:2205.04180}, 2022.

\bibitem[Cordonnier(2018)]{cordonnier2018convex}
Jean-Baptiste Cordonnier.
\newblock Convex optimization using sparsified stochastic gradient descent with
  memory.
\newblock Technical report, 2018.

\bibitem[Danilova and Gorbunov(2022)]{danilova2022distributed}
Marina Danilova and Eduard Gorbunov.
\newblock Distributed methods with absolute compression and error compensation.
\newblock In \emph{Mathematical Optimization Theory and Operations Research:
  Recent Trends: 21st International Conference, MOTOR 2022, Petrozavodsk,
  Russia, July 2--6, 2022, Revised Selected Papers}, pages 163--177. Springer,
  2022.

\bibitem[d'Aspremont(2008)]{DAspremont}
Alexandre d'Aspremont.
\newblock Smooth optimization with approximate gradient.
\newblock \emph{SIAM Journal on Optimization}, 19\penalty0 (3):\penalty0
  1171--1183, 2008.

\bibitem[Devolder et~al.(2014)Devolder, Glineur, and
  Nesterov]{DevolderGlineurNesterov}
Olivier Devolder, Fran\c{c}ois Glineur, and Yurii Nesterov.
\newblock First-order methods of smooth convex optimization with inexact
  oracle.
\newblock \emph{Math. Program.}, 146\penalty0 (1-2):\penalty0 37--75, 2014.

\bibitem[Dutta et~al.(2020)Dutta, Bergou, Abdelmoniem, Ho, Sahu, Canini, and
  Kalnis]{dutta2020discrepancy}
Aritra Dutta, El~Houcine Bergou, Ahmed~M Abdelmoniem, Chen-Yu Ho, Atal~Narayan
  Sahu, Marco Canini, and Panos Kalnis.
\newblock On the discrepancy between the theoretical analysis and practical
  implementations of compressed communication for distributed deep learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 3817--3824, 2020.

\bibitem[Fatkhullin et~al.(2021)Fatkhullin, Sokolov, Gorbunov, Li, and
  Richt\'{a}rik]{EF21BW}
Ilyas Fatkhullin, Igor Sokolov, Eduard Gorbunov, Zhize Li, and Peter
  Richt\'{a}rik.
\newblock {EF21} with bells \& whistles: practical algorithmic extensions of
  modern error feedback.
\newblock \emph{arXiv preprint arXiv:2110.03294}, 2021.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and Courville]{GoodBenCour}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock \emph{Deep Learning}.
\newblock The MIT Press, 2016.

\bibitem[Gorbunov et~al.(2020)Gorbunov, Kovalev, Makarenko, and
  Richt{\'a}rik]{gorbunov2020linearly}
Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richt{\'a}rik.
\newblock Linearly converging error compensated {SGD}.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 20889--20900, 2020.

\bibitem[Gower et~al.(2019)Gower, Loizou, Qian, Sailanbayev, Shulgin, and
  Richt{\'a}rik]{gower2019sgd}
Robert~Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor
  Shulgin, and Peter Richt{\'a}rik.
\newblock Sgd: General analysis and improved rates.
\newblock In \emph{International conference on machine learning}, pages
  5200--5209. PMLR, 2019.

\bibitem[Gupta et~al.(2015)Gupta, Agrawal, Gopalakrishnan, and
  Narayanan]{gupta2015deep}
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
\newblock Deep learning with limited numerical precision.
\newblock In \emph{International conference on machine learning}, pages
  1737--1746. PMLR, 2015.

\bibitem[Horv{\'a}th et~al.(2022)Horv{\'a}th, Ho, Horvath, Sahu, Canini, and
  Richt{\'a}rik]{CNAT}
Samuel Horv{\'a}th, Chen-Yu Ho, Ludovit Horvath, Atal~Narayan Sahu, Marco
  Canini, and Peter Richt{\'a}rik.
\newblock Natural compression for distributed deep learning.
\newblock In \emph{Mathematical and Scientific Machine Learning}, pages
  129--141. PMLR, 2022.

\bibitem[Hu et~al.(2021)Hu, Seiler, and Lessard]{HuSeiLes}
Bin Hu, Peter Seiler, and Laurent Lessard.
\newblock Analysis of biased stochastic gradient descent using sequential
  semidefinite programs.
\newblock \emph{Mathematical Programming}, 187:\penalty0 383--408, 2021.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{KarNutSch}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-\l ojasiewicz condition.
\newblock \emph{Machine Learning and Knowledge Discovery in Databases}, pages
  795â€“--811, 2016.

\bibitem[Karimireddy et~al.(2018)Karimireddy, Stich, and Jaggi]{KariStichJaggi}
Sai~Praneeth Karimireddy, Sebastian Stich, and Martin Jaggi.
\newblock Adaptive balancing of gradient and update computation times using
  global geometry and approximate subproblems.
\newblock 2018.

\bibitem[Karimireddy et~al.(2019)Karimireddy, Rebjock, Stich, and
  Jaggi]{karimireddy2019ef}
Sai~Praneeth Karimireddy, Quentin Rebjock, Sebastian~U. Stich, and Martin
  Jaggi.
\newblock Error feedback fixes {S}ign{SGD} and other gradient compression
  schemes.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  volume~97, pages 3252--3261, 2019.

\bibitem[Khaled and Richt{\'a}rik(2023)]{khaled2022better}
Ahmed Khaled and Peter Richt{\'a}rik.
\newblock Better theory for {SGD} in the nonconvex world.
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=AU4qHN2VkS}.
\newblock Survey Certification.

\bibitem[Khirirat et~al.(2018{\natexlab{a}})Khirirat, Feyzmahdavian, and
  Johansson]{khirirat2018distributed}
Sarit Khirirat, Hamid~Reza Feyzmahdavian, and Mikael Johansson.
\newblock Distributed learning with compressed gradients.
\newblock \emph{arXiv preprint arXiv:1806.06573}, 2018{\natexlab{a}}.

\bibitem[Khirirat et~al.(2018{\natexlab{b}})Khirirat, Johansson, and
  Alistarh]{khirirat2018gradient}
Sarit Khirirat, Mikael Johansson, and Dan Alistarh.
\newblock Gradient compression for communication-limited convex optimization.
\newblock In \emph{2018 IEEE Conference on Decision and Control (CDC)}, pages
  166--171. IEEE, 2018{\natexlab{b}}.

\bibitem[Khirirat et~al.(2020)Khirirat, Magn{\'u}sson, and
  Johansson]{khirirat2020compressed}
Sarit Khirirat, Sindri Magn{\'u}sson, and Mikael Johansson.
\newblock Compressed gradient methods with hessian-aided error compensation.
\newblock \emph{IEEE Transactions on Signal Processing}, 69:\penalty0
  998--1011, 2020.

\bibitem[Khirirat et~al.(2022)Khirirat, Magn{\'u}sson, and
  Johansson]{khirirat2022eco}
Sarit Khirirat, Sindri Magn{\'u}sson, and Mikael Johansson.
\newblock Eco-fedsplit: Federated learning with error-compensated compression.
\newblock In \emph{ICASSP 2022-2022 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 5952--5956. IEEE, 2022.

\bibitem[Lei et~al.(2019)Lei, Hu, Li, and Tang]{LeiHuLiTang}
Yunwei Lei, Ting Hu, Guiying Li, and Ke~Tang.
\newblock Stochastic gradient descent for nonconvex learning without bounded
  gradient assumptions.
\newblock pages 1--7, 2019.

\bibitem[Liu et~al.(2018)Liu, Kailkhura, Chen, Ting, Chang, and
  Amini]{LiuKailChenTingChangAmini}
Sijia Liu, Bhavya Kailkhura, Pin-Yu Chen, Paishun Ting, Shiyu Chang, and Lisa
  Amini.
\newblock Zeroth-order stochastic variance re- duction for nonconvex
  optimization.
\newblock \emph{Advances in neural information processing systems (NeurIPS)},
  31:\penalty0 3727--3737, 2018.

\bibitem[Mishchenko et~al.(2021)Mishchenko, Wang, Kovalev, and
  Richt{\'a}rik]{mishchenko2021intsgd}
Konstantin Mishchenko, Bokun Wang, Dmitry Kovalev, and Peter Richt{\'a}rik.
\newblock Intsgd: Adaptive floatless compression of stochastic gradients.
\newblock \emph{arXiv preprint arXiv:2102.08374}, 2021.

\bibitem[Moosavi-Dezfooli et~al.(2016)Moosavi-Dezfooli, Fawzi, Fawzi, and
  Frossard]{MoosFawziFrossard}
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
  Frossard.
\newblock Universal adversarial perturbations.
\newblock \emph{arXiv preprints arXiv:1610.08401}, 2016.

\bibitem[Nemirovsky and Yudin(1983)]{NemirovskyYudin}
Arkadi Nemirovsky and David Yudin.
\newblock \emph{Problwm Complexity and Method Efficiency in Optimization}.
\newblock Wiley, New York, 1983.

\bibitem[Nesterov and Spokoiny(2017)]{NestSpok}
Yurii Nesterov and Vladimir Spokoiny.
\newblock Random gradient-free minimization of convex functions.
\newblock \emph{Found. Comput. Math.}, 17\penalty0 (2):\penalty0 527--566,
  2017.

\bibitem[Niu et~al.(2011)Niu, Recht, Re, and Wright]{NiuRechtReWright}
Feng Niu, Benjamin Recht, Christopher Re, and Stephen Wright.
\newblock Hogwild: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~24, pages 693--701, 2011.

\bibitem[Polyak(1963)]{PolyakAsn}
Boris Polyak.
\newblock Gradient methods for minimizing functionals.
\newblock \emph{U.S.S.R. Comput. Math. Math. Phys.}, 3\penalty0 (4):\penalty0
  864--878, 1963.

\bibitem[Polyak(1987)]{Polyak}
Boris Polyak.
\newblock \emph{Introduction to Optimization}.
\newblock OptimizationSoftware, Inc., 1987.

\bibitem[Richt\'{a}rik et~al.(2021)Richt\'{a}rik, Sokolov, and
  Fatkhullin]{EF21}
Peter Richt\'{a}rik, Igor Sokolov, and Ilyas Fatkhullin.
\newblock {EF21}: A new, simpler, theoretically better, and practically faster
  error feedback.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Richt{\'a}rik et~al.(2022)Richt{\'a}rik, Sokolov, Gasanov, Fatkhullin,
  Li, and Gorbunov]{richtarik20223pc}
Peter Richt{\'a}rik, Igor Sokolov, Elnur Gasanov, Ilyas Fatkhullin, Zhize Li,
  and Eduard Gorbunov.
\newblock 3pc: Three point compressors for communication-efficient distributed
  training and a better theory for lazy aggregation.
\newblock In \emph{International Conference on Machine Learning}, pages
  18596--18648. PMLR, 2022.

\bibitem[Robbins and Monro(1951)]{RobbinsMonro:1951}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{Annals of Mathematical Statistics}, 22:\penalty0 400--407,
  1951.

\bibitem[Sahu et~al.(2021)Sahu, Dutta, M~Abdelmoniem, Banerjee, Canini, and
  Kalnis]{sahu2021rethinking}
Atal Sahu, Aritra Dutta, Ahmed M~Abdelmoniem, Trambak Banerjee, Marco Canini,
  and Panos Kalnis.
\newblock Rethinking gradient sparsification as total error minimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 8133--8146, 2021.

\bibitem[Sapio et~al.(2019)Sapio, Canini, Ho, Nelson, Kalnis, Kim,
  Krishnamurthy, Moshref, Ports, and Richt{\'a}rik]{sapio2019scaling}
Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob Nelson, Panos Kalnis, Changhoon
  Kim, Arvind Krishnamurthy, Masoud Moshref, Dan~RK Ports, and Peter
  Richt{\'a}rik.
\newblock Scaling distributed machine learning with in-network aggregation.
\newblock \emph{arXiv preprint arXiv:1903.06701}, 2019.

\bibitem[Sapio et~al.(2021)Sapio, Canini, Ho, Nelson, Kalnis, Kim,
  Krishnamurthy, Moshref, Ports, and
  Richt\'{a}rik]{SapCanHoNelKalKinKrisMoshPorRich}
Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob Nelson, Panos Kalnis, Changhoon
  Kim, Arvind Krishnamurthy, Masoud Moshref, Dan Ports, and Peter
  Richt\'{a}rik.
\newblock Scaling distributed machine learning with in-network aggregation.
\newblock In \emph{In 18th USENIX Symposium on Networked Systems Design and
  Implementation (NSDI 21)}, pages 785--808, 2021.

\bibitem[Schmidt et~al.(2011)Schmidt, Roux, and Bach]{SchmidtRouxBach}
Mark Schmidt, Nicolas Roux, and Francis Bach.
\newblock Convergence rates of inexact proximal-gradient methods for convex
  optimization.
\newblock \emph{Advances in neural information processing systems (NeurIPS)},
  24:\penalty0 1458--1466, 2011.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{shai_book}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock \emph{Understanding machine learning: from theory to algorithms}.
\newblock Cambridge University Press, 2014.

\bibitem[Stich and Karimireddy(2020)]{stich2020error}
Sebastian~U Stich and Sai~Praneeth Karimireddy.
\newblock The error-feedback framework: Better rates for sgd with delayed
  gradients and compressed updates.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 9613--9648, 2020.

\bibitem[Stich et~al.(2018)Stich, Cordonnier, and Jaggi]{Stich-EF-NIPS2018}
Sebastian~U. Stich, J.-B. Cordonnier, and Martin Jaggi.
\newblock Sparsified {SGD} with memory.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Str{\"o}m(2015)]{strom2015scalable}
Nikko Str{\"o}m.
\newblock Scalable distributed dnn training using commodity gpu cloud
  computing.
\newblock 2015.

\bibitem[Sun(2020)]{Sun}
Ruo-Yu Sun.
\newblock Optimization for deep learning: An overview.
\newblock \emph{Journal of the Operations Research Society of China},
  8\penalty0 (2):\penalty0 249--294, 2020.

\bibitem[Tang et~al.(2020)Tang, Lian, Yu, Zhang, and Liu]{DoubleSqueeze}
Hanlin Tang, Xiangru Lian, Chen Yu, Tong Zhang, and Ji~Liu.
\newblock {D}ouble{S}queeze: {P}arallel stochastic gradient descent with
  double-pass error-compensated compression.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, 2020.

\bibitem[Tappenden et~al.(2016)Tappenden, Richt{\'a}rik, and
  Gondzio]{TappendenRichtarikGondzio}
Rachael Tappenden, Peter Richt{\'a}rik, and Jacek Gondzio.
\newblock Inexact coordinate descent: Complexity and preconditioning.
\newblock \emph{Journal of Optimization Theory and Applications}, 170:\penalty0
  144--176, 2016.

\bibitem[Wangni et~al.(2018)Wangni, Wang, Liu, and Zhang]{WangniWangLiuZhang}
Jianqiao Wangni, Jialei Wang, Ji~Liu, and Tong Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~31, pages 1306--1316, 2018.

\end{thebibliography}
