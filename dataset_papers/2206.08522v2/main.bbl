\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{ahmed2020causalworld}
Ahmed, O., Tr{\"a}uble, F., Goyal, A., Neitz, A., Bengio, Y., Sch{\"o}lkopf,
  B., W{\"u}thrich, M., Bauer, S.: Causalworld: A robotic manipulation
  benchmark for causal structure and transfer learning. arXiv preprint
  arXiv:2010.04296  (2020)

\bibitem{anderson2018vision}
Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., S{\"u}nderhauf, N.,
  Reid, I., Gould, S., Van Den~Hengel, A.: Vision-and-language navigation:
  Interpreting visually-grounded navigation instructions in real environments
  pp. 3674--3683 (2018)

\bibitem{antol2015vqa}
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh,
  D.: Vqa: Visual question answering pp. 2425--2433 (2015)

\bibitem{chen2020imram}
Chen, H., Ding, G., Liu, X., Lin, Z., Liu, J., Han, J.: Imram: Iterative
  matching with recurrent attention memory for cross-modal image-text retrieval
  pp. 12655--12663 (2020)

\bibitem{das2018embodied}
Das, A., Datta, S., Gkioxari, G., Lee, S., Parikh, D., Batra, D.: Embodied
  question answering pp. 1--10 (2018)

\bibitem{ehsani2021manipulathor}
Ehsani, K., Han, W., Herrasti, A., VanderBilt, E., Weihs, L., Kolve, E.,
  Kembhavi, A., Mottaghi, R.: Manipulathor: A framework for visual object
  manipulation pp. 4497--4506 (2021)

\bibitem{gao2017video}
Gao, L., Guo, Z., Zhang, H., Xu, X., Shen, H.T.: Video captioning with
  attention-based lstm and semantic consistency. IEEE Transactions on
  Multimedia  \textbf{19}(9),  2045--2055 (2017)

\bibitem{goyal2021zero}
Goyal, P., Mooney, R.J., Niekum, S.: Zero-shot task adaptation using natural
  language. arXiv preprint arXiv:2106.02972  (2021)

\bibitem{james2019pyrep}
James, S., Freese, M., Davison, A.J.: Pyrep: Bringing v-rep to deep robot
  learning. arXiv preprint arXiv:1906.11176  (2019)

\bibitem{james2020rlbench}
James, S., Ma, Z., Arrojo, D.R., Davison, A.J.: Rlbench: The robot learning
  benchmark \& learning environment. IEEE Robotics and Automation Letters
  \textbf{5}(2),  3019--3026 (2020)

\bibitem{kurita2020generative}
Kurita, S., Cho, K.: Generative language-grounded policy in vision-and-language
  navigation with bayes' rule. arXiv preprint arXiv:2009.07783  (2020)

\bibitem{li2021igibson}
Li, C., Xia, F., Mart{\'\i}n-Mart{\'\i}n, R., Lingelbach, M., Srivastava, S.,
  Shen, B., Vainio, K., Gokmen, C., Dharan, G., Jain, T., et~al.: Igibson 2.0:
  Object-centric simulation for robot learning of everyday household tasks.
  arXiv preprint arXiv:2108.03272  (2021)

\bibitem{liu2019clevr}
Liu, R., Liu, C., Bai, Y., Yuille, A.L.: Clevr-ref+: Diagnosing visual
  reasoning with referring expressions pp. 4185--4194 (2019)

\bibitem{liu2021structformer}
Liu, W., Paxton, C., Hermans, T., Fox, D.: Structformer: Learning spatial
  structure for language-guided semantic rearrangement of novel objects. arXiv
  preprint arXiv:2110.10189  (2021)

\bibitem{lu2019vilbert}
Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic
  visiolinguistic representations for vision-and-language tasks. Advances in
  neural information processing systems  \textbf{32} (2019)

\bibitem{lynch2020language}
Lynch, C., Sermanet, P.: Language conditioned imitation learning over
  unstructured data. arXiv preprint arXiv:2005.07648  (2020)

\bibitem{mandlekar2021matters}
Mandlekar, A., Xu, D., Wong, J., Nasiriany, S., Wang, C., Kulkarni, R.,
  Fei-Fei, L., Savarese, S., Zhu, Y., Mart{\'\i}n-Mart{\'\i}n, R.: What matters
  in learning from offline human demonstrations for robot manipulation. arXiv
  preprint arXiv:2108.03298  (2021)

\bibitem{mees2022matters}
Mees, O., Hermann, L., Burgard, W.: What matters in language conditioned
  robotic imitation learning. arXiv preprint arXiv:2204.06252  (2022)

\bibitem{mees2022calvin}
Mees, O., Hermann, L., Rosete-Beas, E., Burgard, W.: Calvin: A benchmark for
  language-conditioned policy learning for long-horizon robot manipulation
  tasks. IEEE Robotics and Automation Letters (RA-L)  \textbf{7}(3),
  7327--7334 (2022)

\bibitem{ten2017grasp}
ten Pas, A., Gualtieri, M., Saenko, K., Platt, R.: Grasp pose detection in
  point clouds. The International Journal of Robotics Research
  \textbf{36}(13-14),  1455--1473 (2017)

\bibitem{pashevich2021episodic}
Pashevich, A., Schmid, C., Sun, C.: Episodic transformer for
  vision-and-language navigation pp. 15942--15952 (2021)

\bibitem{radford2021learning}
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.: Learning transferable visual
  models from natural language supervision pp. 8748--8763 (2021)

\bibitem{6696520}
{Rohmer}, E., {Singh}, S.P.N., {Freese}, M.: V-rep: A versatile and scalable
  robot simulation framework pp. 1321--1326 (2013).
  \doi{10.1109/IROS.2013.6696520}

\bibitem{shao2021concept2robot}
Shao, L., Migimatsu, T., Zhang, Q., Yang, K., Bohg, J.: Concept2robot: Learning
  manipulation concepts from instructions and human demonstrations. The
  International Journal of Robotics Research  \textbf{40}(12-14),  1419--1434
  (2021)

\bibitem{shridhar2022cliport}
Shridhar, M., Manuelli, L., Fox, D.: Cliport: What and where pathways for
  robotic manipulation pp. 894--906 (2022)

\bibitem{ALFRED20}
Shridhar, M., Thomason, J., Gordon, D., Bisk, Y., Han, W., Mottaghi, R.,
  Zettlemoyer, L., Fox, D.: {ALFRED: A Benchmark for Interpreting Grounded
  Instructions for Everyday Tasks}  (2020),
  \url{https://arxiv.org/abs/1912.01734}

\bibitem{srivastava2022behavior}
Srivastava, S., Li, C., Lingelbach, M., Mart{\'\i}n-Mart{\'\i}n, R., Xia, F.,
  Vainio, K.E., Lian, Z., Gokmen, C., Buch, S., Liu, K., et~al.: Behavior:
  Benchmark for everyday household activities in virtual, interactive, and
  ecological environments pp. 477--490 (2022)

\bibitem{stepputtis2020languageconditioned}
Stepputtis, S., Campbell, J., Phielipp, M., Lee, S., Baral, C., Amor, H.B.:
  Language-conditioned imitation learning for robot manipulation tasks (2020)

\bibitem{sucan2012the-open-motion-planning-library}
{\c{S}}ucan, I.A., Moll, M., Kavraki, L.E.: The {O}pen {M}otion {P}lanning
  {L}ibrary. {IEEE} Robotics \& Automation Magazine  \textbf{19}(4),  72--82
  (December 2012). \doi{10.1109/MRA.2012.2205651},
  \url{https://ompl.kavrakilab.org}

\bibitem{szot2021habitat}
Szot, A., Clegg, A., Undersander, E., Wijmans, E., Zhao, Y., Turner, J.,
  Maestre, N., Mukadam, M., Chaplot, D.S., Maksymets, O., et~al.: Habitat 2.0:
  Training home assistants to rearrange their habitat. Advances in Neural
  Information Processing Systems  \textbf{34} (2021)

\bibitem{wang2018reconstruction}
Wang, B., Ma, L., Zhang, W., Liu, W.: Reconstruction network for video
  captioning pp. 7622--7631 (2018)

\bibitem{yi2019clevrer}
Yi, K., Gan, C., Li, Y., Kohli, P., Wu, J., Torralba, A., Tenenbaum, J.B.:
  Clevrer: Collision events for video representation and reasoning. arXiv
  preprint arXiv:1910.01442  (2019)

\bibitem{yu2020meta}
Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., Levine, S.:
  Meta-world: A benchmark and evaluation for multi-task and meta reinforcement
  learning pp. 1094--1100 (2020)

\bibitem{yuan2022sornet}
Yuan, W., Paxton, C., Desingh, K., Fox, D.: Sornet: Spatial object-centric
  representations for sequential manipulation pp. 148--157 (2022)

\bibitem{zeng2020transporter}
Zeng, A., Florence, P., Tompson, J., Welker, S., Chien, J., Attarian, M.,
  Armstrong, T., Krasin, I., Duong, D., Sindhwani, V., Lee, J.: Transporter
  networks: Rearranging the visual world for robotic manipulation. Conference
  on Robot Learning (CoRL)  (2020)

\bibitem{zhang2021invigorate}
Zhang, H., Lu, Y., Yu, C., Hsu, D., La, X., Zheng, N.: Invigorate: Interactive
  visual grounding and grasping in clutter. arXiv preprint arXiv:2108.11092
  (2021)

\bibitem{zhang2020context}
Zhang, Q., Lei, Z., Zhang, Z., Li, S.Z.: Context-aware attention network for
  image-text retrieval pp. 3536--3545 (2020)

\bibitem{zhu2020robosuite}
Zhu, Y., Wong, J., Mandlekar, A., Mart{\'\i}n-Mart{\'\i}n, R.: robosuite: A
  modular simulation framework and benchmark for robot learning. arXiv preprint
  arXiv:2009.12293  (2020)

\end{thebibliography}
