\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdullah et~al.(2019)Abdullah, Ren, Ammar, Milenkovic, Luo, Zhang, and
  Wang]{abdullah2019wasserstein}
Mohammed~Amin Abdullah, Hang Ren, Haitham~Bou Ammar, Vladimir Milenkovic, Rui
  Luo, Mingtian Zhang, and Jun Wang.
\newblock Wasserstein robust reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1907.13196}, 2019.

\bibitem[Agarwal et~al.(2021)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2021theory}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock On the theory of policy gradient methods: Optimality, approximation,
  and distribution shift.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (98):\penalty0 1--76, 2021.

\bibitem[Asadi and Littman(2017)]{Asadi2016}
Kavosh Asadi and Michael~L Littman.
\newblock An alternative softmax operator for reinforcement learning.
\newblock In \emph{Proc. International Conference on Machine Learning (ICML)},
  volume~70, pages 243--252. JMLR, 2017.

\bibitem[Atkeson and Morimoto(2003)]{atkeson2003nonparametric}
Christopher~G Atkeson and Jun Morimoto.
\newblock Nonparametric representation of policies and value functions: A
  trajectory-based approach.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NIPS)}, pages 1643--1650, 2003.

\bibitem[author={Dalal, Gal and Sz{\"o}r{\'e}nyi, Bal{\'a}zs and Thoppe, Gugan
  and Mannor, Shie}(2018)]{Dalal2018a}
author={Dalal, Gal and Sz{\"o}r{\'e}nyi, Bal{\'a}zs and Thoppe, Gugan and
  Mannor, Shie}.
\newblock Finite sample analyses for {TD}(0) with function approximation.
\newblock In \emph{Proc. AAAI Conference on Artificial Intelligence (AAAI)},
  pages 6144--6160, 2018.

\bibitem[Badrinath and Kalathil(2021)]{badrinath2021robust}
Kishan~Panaganti Badrinath and Dileep Kalathil.
\newblock Robust reinforcement learning using least squares policy iteration
  with provable performance guarantees.
\newblock In \emph{Proc. International Conference on Machine Learning (ICML)},
  pages 511--520. PMLR, 2021.

\bibitem[Bagnell et~al.(2001)Bagnell, Ng, and Schneider]{bagnell2001solving}
J~Andrew Bagnell, Andrew~Y Ng, and Jeff~G Schneider.
\newblock Solving uncertain markov decision.
\newblock 09 2001.

\bibitem[Baird(1995)]{baird1995residual}
Leemon Baird.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \emph{Machine Learning Proceedings}, pages 30--37. Elsevier, 1995.

\bibitem[Beck and Srikant(2012)]{beck2012error}
Carolyn~L Beck and Rayadurgam Srikant.
\newblock Error bounds for constant step-size {Q}-learning.
\newblock \emph{Systems \& control letters}, 61\penalty0 (12):\penalty0
  1203--1208, 2012.

\bibitem[Bhandari and Russo(2019)]{bhandari2019global}
Jalaj Bhandari and Daniel Russo.
\newblock Global optimality guarantees for policy gradient methods.
\newblock \emph{arXiv preprint arXiv:1906.01786}, 2019.

\bibitem[Bhandari et~al.(2018)Bhandari, Russo, and Singal]{bhandari2018finite}
Jalaj Bhandari, Daniel Russo, and Raghav Singal.
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock In \emph{Proc. Annual Conference on Learning Theory (CoLT)}, pages
  1691--1692. PMLR, 2018.

\bibitem[Bhatnagar et~al.(2009)Bhatnagar, Precup, Silver, Sutton, Maei, and
  Szepesv{\'a}ri]{bhatnagar2009convergent}
Shalabh Bhatnagar, Doina Precup, David Silver, Richard~S Sutton, Hamid Maei,
  and Csaba Szepesv{\'a}ri.
\newblock Convergent temporal-difference learning with arbitrary smooth
  function approximation.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NIPS)}, volume~22, pages 1204--1212, 2009.

\bibitem[Borkar and Meyn(2000)]{borkar2000ode}
Vivek~S Borkar and Sean~P Meyn.
\newblock The ode method for convergence of stochastic approximation and
  reinforcement learning.
\newblock \emph{SIAM Journal on Control and Optimization}, 38\penalty0
  (2):\penalty0 447--469, 2000.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock {OpenAI Gym}.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Cai et~al.(2019)Cai, Yang, Lee, and Wang]{cai2019neural}
Qi~Cai, Zhuoran Yang, Jason~D Lee, and Zhaoran Wang.
\newblock Neural temporal-difference learning converges to global optima.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 11312--11322, 2019.

\bibitem[Cen et~al.(2020)Cen, Cheng, Chen, Wei, and Chi]{cen2020fast}
Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi.
\newblock Fast global convergence of natural policy gradient methods with
  entropy regularization.
\newblock \emph{arXiv preprint arXiv:2007.06558}, 2020.

\bibitem[Chen et~al.(2019)Chen, Zhang, Doan, Maguluri, and
  Clarke]{chen2019performance}
Zaiwei Chen, Sheng Zhang, Thinh~T Doan, Siva~Theja Maguluri, and John-Paul
  Clarke.
\newblock Performance of {Q}-learning with linear function approuimation:
  Stability and finite-time analysis.
\newblock \emph{arXiv preprint arXiv:1905.11425}, 2019.

\bibitem[Dalal et~al.(2018)Dalal, Sz{\"o}r{\'e}nyi, Thoppe, and
  Mannor]{dalal2018finite}
Gal Dalal, Bal{\'a}zs Sz{\"o}r{\'e}nyi, Gugan Thoppe, and Shie Mannor.
\newblock Finite sample analysis of two-timescale stochastic approximation with
  applications to reinforcement learning.
\newblock \emph{Proceedings of Machine Learning Research}, 75:\penalty0 1--35,
  2018.

\bibitem[Dalal et~al.(2020)Dalal, Sz{\"o}r{\'e}nyi, and Thoppe]{dalal2020tale}
Gal Dalal, Bal{\'a}zs Sz{\"o}r{\'e}nyi, and Gugan Thoppe.
\newblock A tale of two-timescale reinforcement learning with the tightest
  finite-time bound.
\newblock In \emph{Proc. Conference on Artificial Intelligence (AAAI)}, pages
  3701--3708, 2020.

\bibitem[Doan(2021)]{doan2021finite}
Thinh~T Doan.
\newblock Finite-time analysis and restarting scheme for linear two-time-scale
  stochastic approximation.
\newblock \emph{SIAM Journal on Control and Optimization}, 59\penalty0
  (4):\penalty0 2798--2819, 2021.

\bibitem[Even-Dar et~al.(2003)Even-Dar, Mansour, and
  Bartlett]{even2003learning}
Eyal Even-Dar, Yishay Mansour, and Peter Bartlett.
\newblock Learning rates for {Q}-learning.
\newblock \emph{Journal of machine learning Research}, 5\penalty0 (1), 2003.

\bibitem[Gupta et~al.(2019)Gupta, Srikant, and Ying]{gupta2019finite}
Harsh Gupta, R~Srikant, and Lei Ying.
\newblock Finite-time performance bounds and adaptive learning rate selection
  for two time-scale reinforcement learning.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 4706--4715, 2019.

\bibitem[Hou et~al.(2020)Hou, Pang, Hong, Lan, Ma, and Yin]{hou2020robust}
Linfang Hou, Liang Pang, Xin Hong, Yanyan Lan, Zhiming Ma, and Dawei Yin.
\newblock Robust reinforcement learning with wasserstein constraint.
\newblock \emph{arXiv preprint arXiv:2006.00945}, 2020.

\bibitem[Huang et~al.(2017)Huang, Papernot, Goodfellow, Duan, and
  Abbeel]{huang2017adversarial}
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel.
\newblock Adversarial attacks on neural network policies.
\newblock In \emph{Proc. International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Huber(1965)]{hub65}
P.~J. Huber.
\newblock A robust version of the probability ratio test.
\newblock \emph{Ann. Math. Statist.}, 36:\penalty0 1753--1758, 1965.

\bibitem[Iyengar(2005)]{iyengar2005robust}
Garud~N Iyengar.
\newblock Robust dynamic programming.
\newblock \emph{Mathematics of Operations Research}, 30\penalty0 (2):\penalty0
  257--280, 2005.

\bibitem[Kaledin et~al.(2020)Kaledin, Moulines, Naumov, Tadic, and
  Wai]{kaledin2020finite}
Maxim Kaledin, Eric Moulines, Alexey Naumov, Vladislav Tadic, and Hoi-To Wai.
\newblock Finite time analysis of linear two-timescale stochastic approximation
  with {M}arkovian noise.
\newblock In \emph{Proc. Annual Conference on Learning Theory (CoLT)}, pages
  2144--2203. PMLR, 2020.

\bibitem[Kos and Song(2017)]{kos2017delving}
Jernej Kos and Dawn Song.
\newblock Delving into adversarial attacks on deep policies.
\newblock In \emph{Proc. International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Kumar et~al.(2019)Kumar, Koppel, and Ribeiro]{kumar2019sample}
Harshat Kumar, Alec Koppel, and Alejandro Ribeiro.
\newblock On the sample complexity of actor-critic method for reinforcement
  learning with function approximation.
\newblock \emph{arXiv preprint arXiv:1910.08412}, 2019.

\bibitem[Li et~al.(2020)Li, Wei, Chi, Gu, and Chen]{li2020sample}
Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen.
\newblock Sample complexity of asynchronous {Q}-learning: Sharper analysis and
  variance reduction.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Li et~al.(2021)Li, Cai, Chen, Gu, Wei, and Chi]{li2021q}
Gen Li, Changxiao Cai, Yuxin Chen, Yuantao Gu, Yuting Wei, and Yuejie Chi.
\newblock Is {Q}-learning minimax optimal? a tight sample complexity analysis.
\newblock \emph{arXiv preprint arXiv:2102.06548}, 2021.

\bibitem[Lim et~al.(2013)Lim, Xu, and Mannor]{lim2013reinforcement}
Shiau~Hong Lim, Huan Xu, and Shie Mannor.
\newblock Reinforcement learning in robust markov decision processes.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NIPS)}, pages 701--709, 2013.

\bibitem[Lin et~al.(2017)Lin, Hong, Liao, Shih, Liu, and Sun]{lin2017tactics}
Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and
  Min Sun.
\newblock Tactics of adversarial attack on deep reinforcement learning agents.
\newblock In \emph{Proc. International Joint Conferences on Artificial
  Intelligence (IJCAI)}, pages 3756--3762, 2017.

\bibitem[Liu et~al.(2015)Liu, Liu, Ghavamzadeh, Mahadevan, and
  Petrik]{liu2015finite}
Bo~Liu, Ji~Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik.
\newblock Finite-sample analysis of proximal gradient td algorithms.
\newblock In \emph{Proc. International Conference on Uncertainty in Artificial
  Intelligence (UAI)}, pages 504--513. Citeseer, 2015.

\bibitem[Ma et~al.(2020)Ma, Zhou, and Zou]{ma2020variance}
Shaocong Ma, Yi~Zhou, and Shaofeng Zou.
\newblock Variance-reduced off-policy {TDC} learning: Non-asymptotic
  convergence analysis.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~33, pages 14796--14806, 2020.

\bibitem[Ma et~al.(2021)Ma, Zhou, and Zou]{ma2021greedygq}
Shaocong Ma, Yi~Zhou, and Shaofeng Zou.
\newblock Greedy-{GQ} with variance reduction: Finite-time analysis and
  improved complexity.
\newblock In \emph{Proc. International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Maei(2011)]{maei2011gradient}
Hamid~Reza Maei.
\newblock Gradient temporal-difference learning algorithms.
\newblock \emph{Thesis, University of Alberta}, 2011.

\bibitem[Maei et~al.(2010)Maei, Szepesv{\'a}ri, Bhatnagar, and
  Sutton]{maei2010toward}
Hamid~Reza Maei, Csaba Szepesv{\'a}ri, Shalabh Bhatnagar, and Richard~S Sutton.
\newblock Toward off-policy learning control with function approximation.
\newblock In \emph{Proc. International Conference on Machine Learning (ICML)},
  pages 719--726, 2010.

\bibitem[Mandlekar et~al.(2017)Mandlekar, Zhu, Garg, Fei-Fei, and
  Savarese]{mandlekar2017adversarially}
Ajay Mandlekar, Yuke Zhu, Animesh Garg, Li~Fei-Fei, and Silvio Savarese.
\newblock Adversarially robust policy learning: Active construction of
  physically-plausible perturbations.
\newblock In \emph{2017 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pages 3932--3939. IEEE, 2017.

\bibitem[Mei et~al.(2020)Mei, Xiao, Szepesvari, and Schuurmans]{mei2020global}
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans.
\newblock On the global convergence rates of softmax policy gradient methods.
\newblock In \emph{Proc. International Conference on Machine Learning (ICML)},
  pages 6820--6829. PMLR, 2020.

\bibitem[Morimoto and Doya(2005)]{morimoto2005robust}
Jun Morimoto and Kenji Doya.
\newblock Robust reinforcement learning.
\newblock \emph{Neural computation}, 17\penalty0 (2):\penalty0 335--359, 2005.

\bibitem[Nilim and El~Ghaoui(2004)]{nilim2004robustness}
Arnab Nilim and Laurent El~Ghaoui.
\newblock Robustness in {Markov} decision problems with uncertain transition
  matrices.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NIPS)}, pages 839--846, 2004.

\bibitem[Pattanaik et~al.(2018)Pattanaik, Tang, Liu, Bommannan, and
  Chowdhary]{pattanaik2018robust}
Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish
  Chowdhary.
\newblock Robust deep reinforcement learning with adversarial attacks.
\newblock In \emph{Proc. International Conference on Autonomous Agents and
  MultiAgent Systems}, pages 2040--2042, 2018.

\bibitem[Pinto et~al.(2017)Pinto, Davidson, Sukthankar, and
  Gupta]{pinto2017robust}
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta.
\newblock Robust adversarial reinforcement learning.
\newblock In \emph{Proc. International Conference on Machine Learning (ICML)},
  pages 2817--2826. PMLR, 2017.

\bibitem[Qiu et~al.(2019)Qiu, Yang, Ye, and Wang]{qiu2019finite}
Shuang Qiu, Zhuoran Yang, Jieping Ye, and Zhaoran Wang.
\newblock On the finite-time convergence of actor-critic algorithm.
\newblock In \emph{Proc. Optimization Foundations for Reinforcement Learning
  Workshop at Advances in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem[Qu and Wierman(2020)]{qu2020finite}
Guannan Qu and Adam Wierman.
\newblock Finite-time analysis of asynchronous stochastic approximation and
  {Q}-learning.
\newblock In \emph{Proc. Annual Conference on Learning Theory (CoLT)}, pages
  3185--3205. PMLR, 2020.

\bibitem[Rajeswaran et~al.(2017)Rajeswaran, Ghotra, Ravindran, and
  Levine]{rajeswaran2017epopt}
Aravind Rajeswaran, Sarvjeet Ghotra, Balaraman Ravindran, and Sergey Levine.
\newblock Epopt: Learning robust neural network policies using model ensembles.
\newblock In \emph{Proc. International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Roy et~al.(2017)Roy, Xu, and Pokutta]{roy2017reinforcement}
Aurko Roy, Huan Xu, and Sebastian Pokutta.
\newblock Reinforcement learning under model mismatch.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NIPS)}, pages 3046--3055, 2017.

\bibitem[Satia and Lave~Jr(1973)]{satia1973markovian}
Jay~K Satia and Roy~E Lave~Jr.
\newblock {M}arkovian decision processes with uncertain transition
  probabilities.
\newblock \emph{Operations Research}, 21\penalty0 (3):\penalty0 728--740, 1973.

\bibitem[Srikant and Ying(2019)]{srikant2019}
R.~Srikant and Lei Ying.
\newblock Finite-time error bounds for linear stochastic approximation and {TD}
  learning.
\newblock In \emph{Proc. Annual Conference on Learning Theory (CoLT)}, pages
  2803--2830, 2019.

\bibitem[Sun et~al.(2020)Sun, Wang, Giannakis, Yang, and Yang]{sun2020finite}
Jun Sun, Gang Wang, Georgios~B Giannakis, Qinmin Yang, and Zaiyue Yang.
\newblock Finite-sample analysis of decentralized temporal-difference learning
  with linear function approximation.
\newblock In \emph{Proc. International Conference on Artifical Intelligence and
  Statistics (AISTATS)}, 2020.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S. Sutton and Andrew~G. Barto.
\newblock \emph{Reinforcement Learning: An Introduction, Second Edition}.
\newblock The MIT Press, Cambridge, Massachusetts, 2018.

\bibitem[Sutton et~al.(2008)Sutton, Szepesv{\'a}ri, and
  Maei]{sutton2008convergent}
Richard~S Sutton, Csaba Szepesv{\'a}ri, and Hamid~Reza Maei.
\newblock A convergent {O}(n) algorithm for off-policy temporal-difference
  learning with linear function approximation.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NIPS)}, volume~21, pages 1609--1616. MIT Press, 2008.

\bibitem[Sutton et~al.(2009)Sutton, Maei, Precup, Bhatnagar, Silver,
  Szepesv{\'a}ri, and Wiewiora]{sutton2009fast}
Richard~S Sutton, Hamid~Reza Maei, Doina Precup, Shalabh Bhatnagar, David
  Silver, Csaba Szepesv{\'a}ri, and Eric Wiewiora.
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In \emph{Proc. International Conference on Machine Learning (ICML)},
  pages 993--1000, 2009.

\bibitem[Tamar et~al.(2014)Tamar, Mannor, and Xu]{tamar2014scaling}
Aviv Tamar, Shie Mannor, and Huan Xu.
\newblock Scaling up robust mdps using function approximation.
\newblock In \emph{Proc. International Conference on Machine Learning (ICML)},
  pages 181--189. PMLR, 2014.

\bibitem[Vinitsky et~al.(2020)Vinitsky, Du, Parvate, Jang, Abbeel, and
  Bayen]{vinitsky2020robust}
Eugene Vinitsky, Yuqing Du, Kanaad Parvate, Kathy Jang, Pieter Abbeel, and
  Alexandre Bayen.
\newblock Robust reinforcement learning using adversarial populations.
\newblock \emph{arXiv preprint arXiv:2008.01825}, 2020.

\bibitem[Wai et~al.(2019)Wai, Hong, Yang, Wang, and Tang]{wai2019variance}
Hoi-To Wai, Mingyi Hong, Zhuoran Yang, Zhaoran Wang, and Kexin Tang.
\newblock Variance reduced policy evaluation with smooth function
  approximation.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~32, pages 5784--5795, 2019.

\bibitem[Wainwright(2019)]{wainwright2019variance}
Martin~J Wainwright.
\newblock Variance-reduced {Q}-learning is minimax optimal.
\newblock \emph{arXiv preprint arXiv:1906.04697}, 2019.

\bibitem[Wang et~al.(2020)Wang, Cai, Yang, and Wang]{Wang2020Neural}
Lingxiao Wang, Qi~Cai, Zhuoran Yang, and Zhaoran Wang.
\newblock Neural policy gradient methods: Global optimality and rates of
  convergence.
\newblock In \emph{Proc. International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Wang and Zou(2020)]{wang2020finite}
Yue Wang and Shaofeng Zou.
\newblock Finite-sample analysis of {Greedy-GQ} with linear function
  approximation under {M}arkovian noise.
\newblock In \emph{Proc. International Conference on Uncertainty in Artificial
  Intelligence (UAI)}, pages 11--20. PMLR, 2020.

\bibitem[Wang et~al.(2021)Wang, Zou, and Zhou]{wang2021finite}
Yue Wang, Shaofeng Zou, and Yi~Zhou.
\newblock Finite-sample analysis for two time-scale non-linear {TDC} with
  general smooth function approximation.
\newblock \emph{arXiv preprint arXiv:2104.02836}, 2021.

\bibitem[Wiesemann et~al.(2013)Wiesemann, Kuhn, and
  Rustem]{wiesemann2013robust}
Wolfram Wiesemann, Daniel Kuhn, and Ber{\c{c}} Rustem.
\newblock Robust {M}arkov decision processes.
\newblock \emph{Mathematics of Operations Research}, 38\penalty0 (1):\penalty0
  153--183, 2013.

\bibitem[Wu et~al.(2020)Wu, Zhang, Xu, and Gu]{wu2020finite}
Yue Wu, Weitong Zhang, Pan Xu, and Quanquan Gu.
\newblock A finite time analysis of two time-scale actor critic methods.
\newblock \emph{arXiv preprint arXiv:2005.01350}, 2020.

\bibitem[Xu and Liang(2021)]{xu2021sample}
Tengyu Xu and Yingbin Liang.
\newblock Sample complexity bounds for two timescale value-based reinforcement
  learning algorithms.
\newblock In \emph{Proc. International Conference on Artifical Intelligence and
  Statistics (AISTATS)}, pages 811--819. PMLR, 2021.

\bibitem[Xu et~al.(2019)Xu, Zou, and Liang]{xu2019two}
Tengyu Xu, Shaofeng Zou, and Yingbin Liang.
\newblock Two time-scale off-policy {TD} learning: Non-asymptotic analysis over
  {Markovian} samples.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 10633--10643, 2019.

\bibitem[Yang et~al.(2019)Yang, Chen, Hong, and Wang]{yang2019provably}
Zhuoran Yang, Yongxin Chen, Mingyi Hong, and Zhaoran Wang.
\newblock Provably global convergence of actor-critic: A case for linear
  quadratic regulator with ergodic cost.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 8353--8365, 2019.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Hu, and
  Basar]{zhang2020stability}
Kaiqing Zhang, Bin Hu, and Tamer Basar.
\newblock On the stability and convergence of robust adversarial reinforcement
  learning: A case study on linear quadratic systems.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~33, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Sun, Tao, Genc, Mallya, and
  Basar]{zhang2020robust}
Kaiqing Zhang, Tao Sun, Yunzhe Tao, Sahika Genc, Sunil Mallya, and Tamer Basar.
\newblock Robust multi-agent reinforcement learning with model uncertainty.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~33, 2020{\natexlab{b}}.

\bibitem[Zou et~al.(2019)Zou, Xu, and Liang]{zou2019finite}
Shaofeng Zou, Tengyu Xu, and Yingbin Liang.
\newblock Finite-sample analysis for {SARSA} with linear function
  approximation.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 8665--8675, 2019.

\end{thebibliography}
