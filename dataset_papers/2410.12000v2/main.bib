@book{DeuflhardTextBook,
author = {Peter Deuflhard and Andreas Hohmann},
title = {Numerical Analysis in Modern Scientific Computing},
publisher = {Springer},
year = {2003}}

@inproceedings{
li2023selfconsistent,
title={Self-Consistent Velocity Matching of Probability Flows},
author={Lingxiao Li and Samuel Hurault and Justin Solomon},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=C6fvJ2RfsL}
}

@InProceedings{pmlr-v178-shen22a,
  title = 	 {Self-Consistency of the Fokker Planck Equation},
  author =       {Shen, Zebang and Wang, Zhenfu and Kale, Satyen and Ribeiro, Alejandro and Karbasi, Amin and Hassani, Hamed},
  booktitle = 	 {Proceedings of Thirty Fifth Conference on Learning Theory},
  pages = 	 {817--841},
  year = 	 {2022},
  editor = 	 {Loh, Po-Ling and Raginsky, Maxim},
  volume = 	 {178},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--05 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v178/shen22a/shen22a.pdf},
  url = 	 {https://proceedings.mlr.press/v178/shen22a.html},
  abstract = 	 {The Fokker-Planck equation (FPE) is the partial differential equation that governs the density evolution of the Ito process and is of great importance to the literature of statistical physics and machine learning. The FPE can be regarded as a continuity equation where the change of the density is completely determined by a time varying velocity field. Importantly, this velocity field also depends on the current density function. As a result, the ground-truth velocity field can be shown to be the solution of a fixed-point equation, a property that we call self-consistency. In this paper, we exploit this concept to design a potential function of the hypothesis velocity fields, and prove that, if such a function diminishes to zero during the training procedure, the trajectory of the densities generated by the hypothesis velocity fields converges to the solution of the FPE in the Wasserstein-2 sense. The proposed potential function is amenable to neural-network based parameterization as the stochastic gradient with respect to the parameter can be efficiently computed. Once a parameterized model, such as Neural Ordinary Differential Equation is trained, we can generate the entire trajectory to the FPE.}
}


@article{Boffi_2023,
doi = {10.1088/2632-2153/ace2aa},
url = {https://dx.doi.org/10.1088/2632-2153/ace2aa},
year = {2023},
month = {jul},
publisher = {IOP Publishing},
volume = {4},
number = {3},
pages = {035012},
author = {Nicholas M Boffi and Eric Vanden-Eijnden},
title = {Probability flow solution of the Fokker–Planck equation},
journal = {Machine Learning: Science and Technology},
abstract = {The method of choice for integrating the time-dependent Fokker–Planck equation (FPE) in high-dimension is to generate samples from the solution via integration of the associated stochastic differential equation (SDE). Here, we study an alternative scheme based on integrating an ordinary differential equation that describes the flow of probability. Acting as a transport map, this equation deterministically pushes samples from the initial density onto samples from the solution at any later time. Unlike integration of the stochastic dynamics, the method has the advantage of giving direct access to quantities that are challenging to estimate from trajectories alone, such as the probability current, the density itself, and its entropy. The probability flow equation depends on the gradient of the logarithm of the solution (its ‘score’), and so is a-priori unknown. To resolve this dependence, we model the score with a deep neural network that is learned on-the-fly by propagating a set of samples according to the instantaneous probability current. We show theoretically that the proposed approach controls the Kullback–Leibler (KL) divergence from the learned solution to the target, while learning on external samples from the SDE does not control either direction of the KL divergence. Empirically, we consider several high-dimensional FPEs from the physics of interacting particle systems. We find that the method accurately matches analytical solutions when they are available as well as moments computed via Monte-Carlo when they are not. Moreover, the method offers compelling predictions for the global entropy production rate that out-perform those obtained from learning on stochastic trajectories, and can effectively capture non-equilibrium steady-state probability currents over long time intervals.}
}


@inproceedings{NIPS2013_af21d0c9,
 author = {Cuturi, Marco},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf},
 volume = {26},
 year = {2013}
}

@InProceedings{pmlr-v84-genevay18a,
  title = 	 {Learning Generative Models with Sinkhorn Divergences},
  author = 	 {Genevay, Aude and Peyre, Gabriel and Cuturi, Marco},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1608--1617},
  year = 	 {2018},
  editor = 	 {Storkey, Amos and Perez-Cruz, Fernando},
  volume = 	 {84},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--11 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v84/genevay18a/genevay18a.pdf},
  url = 	 {https://proceedings.mlr.press/v84/genevay18a.html},
  abstract = 	 {The ability to compare two degenerate probability distributions, that is two distributions supported on low-dimensional manifolds in much higher-dimensional spaces, is a crucial factor in the estimation of generative mod- els.It is therefore no surprise that optimal transport (OT) metrics and their ability to handle measures with non-overlapping sup- ports have emerged as a promising tool. Yet, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational bur- den of evaluating OT losses, (ii) their instability and lack of smoothness, (iii) the difficulty to estimate them, as well as their gradients, in high dimension. This paper presents the first tractable method to train large scale generative models using an OT-based loss called Sinkhorn loss which tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into a differentiable and more robust quantity that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations with seam- less GPU execution. Additionally, Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Energy distance/Maximum Mean Discrepancy (MMD) losses, thus allowing to find a sweet spot leveraging the geometry of OT on the one hand, and the favorable high-dimensional sample complexity of MMD, which comes with un- biased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.}
}


@article{schmid2010dynamic,
  title={Dynamic mode decomposition of numerical and experimental data},
  author={Schmid, P. J.},
  journal={Journal of Fluid Mechanics},
  volume={656},
  pages={5--28},
  year={2010},
  publisher={Cambridge University Press}
}

@article{tu2013dynamic,
  title = {On dynamic mode decomposition:  Theory and applications},
journal = {Journal of Computational Dynamics},
volume = {1},
number = {2},
pages = {391-421},
year = {2014},
author = {Jonathan H. Tu and Clarence W. Rowley and Dirk M. Luchtenburg and Steven L. Brunton and J. Nathan Kutz}
}

% DMD for non-linear
@article{mezic2005spectral,
  title={Spectral properties of dynamical systems, model reduction and decompositions},
  author={Mezi{\'c}, Igor},
  journal={Nonlinear Dynamics},
  volume={41},
  number={1-3},
  pages={309--325},
  year={2005},
  publisher={Springer}
}

@inproceedings{10.5555/3504035.3504518,
author = {Perez, Ethan and Strub, Florian and de Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
title = {FiLM: visual reasoning with a general conditioning layer},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning — answering image-related questions which require a multi-step, high-level process — a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {483},
numpages = {10},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@InProceedings{pmlr-v119-tong20a,
  title = 	 {{T}rajectory{N}et: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics},
  author =       {Tong, Alexander and Huang, Jessie and Wolf, Guy and Van Dijk, David and Krishnaswamy, Smita},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {9526--9536},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/tong20a/tong20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/tong20a.html},
  abstract = 	 {It is increasingly common to encounter data in the form of cross-sectional population measurements over time, particularly in biomedical settings. Recent attempts to model individual trajectories from this data use optimal transport to create pairwise matchings between time points. However, these methods cannot model non-linear paths common in many underlying dynamic systems. We establish a link between continuous normalizing flows and dynamic optimal transport to model the expected paths of points over time. Continuous normalizing flows are generally under constrained, as they are allowed to take an arbitrary path from the source to the target distribution. We present \emph{TrajectoryNet}, which controls the continuous paths taken between distributions. We show how this is particularly applicable for studying cellular dynamics in data from single-cell RNA sequencing (scRNA-seq) technologies, and that TrajectoryNet improves upon recently proposed static optimal transport-based models that can be used for interpolating cellular distributions.}
}


@InProceedings{pmlr-v151-bunne22a,
  title = 	 { Proximal Optimal Transport Modeling of Population Dynamics },
  author =       {Bunne, Charlotte and Papaxanthos, Laetitia and Krause, Andreas and Cuturi, Marco},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {6511--6528},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28--30 Mar},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v151/bunne22a/bunne22a.pdf},
  url = 	 {https://proceedings.mlr.press/v151/bunne22a.html},
  abstract = 	 { We propose a new approach to model the collective dynamics of a population of particles evolving with time. As is often the case in challenging scientific applications, notably single-cell genomics, measuring features for these particles requires destroying them. As a result, the population can only be monitored with periodic snapshots, obtained by sampling a few particles that are sacrificed in exchange for measurements. Given only access to these snapshots, can we reconstruct likely individual trajectories for all other particles? We propose to model these trajectories as collective realizations of a causal Jordan-Kinderlehrer-Otto (JKO) flow of measures: The JKO scheme posits that the new configuration taken by a population at time t+1 is one that trades off an improvement, in the sense that it decreases an energy, while remaining close (in Wasserstein distance) to the previous configuration observed at t. In order to learn such an energy using only snapshots, we propose JKOnet, a neural architecture that computes (in end-to-end differentiable fashion) the JKO flow given a parametric energy and initial configuration of points. We demonstrate the good performance and robustness of the JKOnet fitting procedure, compared to a more direct forward method. }
}


@article{williams2015data,
	title={A data--driven approximation of the {K}oopman operator: Extending dynamic mode decomposition},
	author={Williams, Matthew O and Kevrekidis, Ioannis G and Rowley, Clarence W},
	journal={Journal of Nonlinear Science},
	volume={25},
	number={6},
	pages={1307--1346},
	year={2015},
	publisher={Springer}
}

@article{brunton2016koopman,
  title={Koopman invariant subspaces and finite linear representations of nonlinear dynamical systems for control},
  author={Brunton, Steven L and Brunton, Bingni W and Proctor, Joshua L and Kutz, J Nathan},
  journal={PLOS One},
  volume={11},
  number={2},
  pages={e0150171},
  year={2016},
  publisher={Public Library of Science San Francisco, CA USA}
}

@book{kutz2016dynamic,
  title={Dynamic mode decomposition: data-driven modeling of complex systems},
  author={Kutz, J Nathan and Brunton, Steven L and Brunton, Bingni W and Proctor, Joshua L},
  year={2016},
  publisher={SIAM}
}

@article{rowley2009spectral,
  title={Spectral analysis of nonlinear flows},
  author={Rowley, C. W. and Mezi{\'c}, I. and Bagheri, S. and Schlatter, P. and Henningson, D. S.},
  journal={Journal of Fluid Mechanics},
  volume={641},
  pages={115--127},
  year={2009},
  publisher={Cambridge University Press}
}

@inproceedings{NEURIPS2022_39235c56,
 author = {Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {8633--8646},
 publisher = {Curran Associates, Inc.},
 title = {Video Diffusion Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/39235c56aef13fb05a6adc95eb9d8d66-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@book{GhanemTextbook,
author = {Roger G. Ghanem and Pol D. Spanos},
title = {Stochastic Finite Elements: A Spectral Approach},
year = {1991},
publisher = {Springer}}

@article{doi:10.1137/130932715,
author = {Benner, Peter and Gugercin, Serkan and Willcox, Karen},
title = {A Survey of Projection-Based Model Reduction Methods for Parametric Dynamical Systems},
journal = {SIAM Review},
volume = {57},
number = {4},
pages = {483-531},
year = {2015},
}

@article{annurev:/content/journals/10.1146/annurev-fluid-121021-025220,
   author = "Kramer, Boris and Peherstorfer, Benjamin and Willcox, Karen E.",
   title = "Learning Nonlinear Reduced Models from Data with Operator Inference", 
   journal= "Annual Review of Fluid Mechanics",
   year = "2024",
   volume = "56",
   number = "Volume 56, 2024",
   pages = "521-548",
   doi = "https://doi.org/10.1146/annurev-fluid-121021-025220",
   url = "https://www.annualreviews.org/content/journals/10.1146/annurev-fluid-121021-025220",
   publisher = "Annual Reviews",
   issn = "1545-4479",
   type = "Journal Article",
   keywords = "structure preservation",
   keywords = "nonlinear model reduction",
   keywords = "scientific machine learning",
   keywords = "data-driven modeling",
   keywords = "Operator Inference",
   
  }

@article{RozzaPateraSurvey,
year={2007},
issn={1134-3060},
journal={Archives of Computational Methods in Engineering},
volume={15},
number={3},
title={Reduced basis approximation and a posteriori error estimation for affinely parametrized elliptic coercive partial differential equations},
author={Rozza, G. and Huynh, D.B.P. and Patera, A.},
pages={1--47},
}

@book{Hughes2012,
title = {The Finite Element Method: Linear Static and Dynamic Finite Element Analysis},
author = {Thomas J. R. Hughes},
publisher = {Dover Publications},
year = {2012}}

@book{LeVeque_2002, place={Cambridge}, series={Cambridge Texts in Applied Mathematics}, title={Finite Volume Methods for Hyperbolic Problems}, publisher={Cambridge University Press}, author={LeVeque, Randall J.}, year={2002}, collection={Cambridge Texts in Applied Mathematics}}


@article{PWG17MultiSurvey,
title = {Survey of multifidelity methods in uncertainty propagation, inference, and optimization},
author = {Peherstorfer, B. and Willcox, K. and Gunzburger, M.},
journal = {SIAM Review},
volume = {60},
number = {3},
pages = {550-591},
year = {2018},
}

@article{
norcliffe2023faster,
title={Faster Training of Neural {ODE}s Using Gau{\ss}{\textendash}Legendre Quadrature},
author={Alexander Luke Ian Norcliffe and Marc Peter Deisenroth},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=f0FSDAy1bU},
note={}
}

@article{https://doi.org/10.1029/2020MS002405,
author = {Rasp, Stephan and Thuerey, Nils},
title = {Data-Driven Medium-Range Weather Prediction With a Resnet Pretrained on Climate Simulations: A New Model for WeatherBench},
journal = {Journal of Advances in Modeling Earth Systems},
volume = {13},
number = {2},
pages = {e2020MS002405},
keywords = {deep learning, machine learning, numerical weather forecasting},
doi = {https://doi.org/10.1029/2020MS002405},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002405},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002405},
note = {e2020MS002405 2020MS002405},
abstract = {Abstract Numerical weather prediction has traditionally been based on the models that discretize the dynamical and physical equations of the atmosphere. Recently, however, the rise of deep learning has created increased interest in purely data-driven medium-range weather forecasting with first studies exploring the feasibility of such an approach. To accelerate progress in this area, the WeatherBench benchmark challenge was defined. Here, we train a deep residual convolutional neural network (Resnet) to predict geopotential, temperature and precipitation at 5.625° resolution up to 5 days ahead. To avoid overfitting and improve forecast skill, we pretrain the model using historical climate model output before fine-tuning on reanalysis data. The resulting forecasts outperform previous submissions to WeatherBench and are comparable in skill to a physical baseline at similar resolution. We also analyze how the neural network creates its predictions and find that, for the case studies analyzed, the model has learned physically reasonable correlations. Finally, we perform scaling experiments to estimate the potential skill of data-driven approaches at higher resolutions.},
year = {2021}
}


@INPROCEEDINGS{9683134,
  author={Lambert, Nathan and Wilcox, Albert and Zhang, Howard and Pister, Kristofer S. J. and Calandra, Roberto},
  booktitle={2021 60th IEEE Conference on Decision and Control (CDC)}, 
  title={Learning Accurate Long-term Dynamics for Model-based Reinforcement Learning}, 
  year={2021},
  volume={},
  number={},
  pages={2880-2887},
  keywords={Uncertainty;Computational modeling;Supervised learning;Fitting;Reinforcement learning;Predictive models;Trajectory},
  doi={10.1109/CDC45484.2021.9683134}}

@inproceedings{
harvey2022flexible,
title={Flexible Diffusion Modeling of Long Videos},
author={William Harvey and Saeid Naderiparizi and Vaden Masrani and Christian Dietrich Weilbach and Frank Wood},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=0RTJcuvHtIu}
}

@inproceedings{NEURIPS2020_4c5bcfec,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6840--6851},
 publisher = {Curran Associates, Inc.},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 volume = {33},
 year = {2020}
}


@InProceedings{pmlr-v37-sohl-dickstein15,
  title = 	 {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2256--2265},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract = 	 {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}


@InProceedings{Davtyan_2023_ICCV,
    author    = {Davtyan, Aram and Sameni, Sepehr and Favaro, Paolo},
    title     = {Efficient Video Prediction via Sparsely Conditioned Flow Matching},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {23263-23274}
}

@misc{blattmann2023stable,
      title={Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets}, 
      author={Andreas Blattmann and Tim Dockhorn and Sumith Kulal and Daniel Mendelevitch and Maciej Kilian and Dominik Lorenz and Yam Levi and Zion English and Vikram Voleti and Adam Letts and Varun Jampani and Robin Rombach},
      year={2023},
      eprint={2311.15127},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{pmlr-v139-rasul21a,
  title = 	 {Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting},
  author =       {Rasul, Kashif and Seward, Calvin and Schuster, Ingmar and Vollgraf, Roland},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8857--8868},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/rasul21a/rasul21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/rasul21a.html},
  abstract = 	 {In this work, we propose TimeGrad, an autoregressive model for multivariate probabilistic time series forecasting which samples from the data distribution at each time step by estimating its gradient. To this end, we use diffusion probabilistic models, a class of latent variable models closely connected to score matching and energy-based methods. Our model learns gradients by optimizing a variational bound on the data likelihood and at inference time converts white noise into a sample of the distribution of interest through a Markov chain using Langevin sampling. We demonstrate experimentally that the proposed autoregressive denoising diffusion model is the new state-of-the-art multivariate probabilistic forecasting method on real-world data sets with thousands of correlated dimensions. We hope that this method is a useful tool for practitioners and lays the foundation for future research in this area.}
}


@inproceedings{
lienen2024from,
title={From Zero to Turbulence: Generative Modeling for 3D Flow Simulation},
author={Marten Lienen and David L{\"u}dke and Jan Hansen-Palmus and Stephan G{\"u}nnemann},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=ZhlwoC1XaN}
}

@article{10.1214/23-AAP1969,
author = {Hugo Lavenant and Stephen Zhang and Young-Heon Kim and Geoffrey Schiebinger},
title = {{Toward a mathematical theory of trajectory inference}},
volume = {34},
journal = {The Annals of Applied Probability},
number = {1A},
publisher = {Institute of Mathematical Statistics},
pages = {428 -- 500},
keywords = {Convex optimization, developmental biology, Optimal transport, single-cell RNA-sequencing, Stochastic processes, Trajectory inference},
year = {2024},
doi = {10.1214/23-AAP1969},
URL = {https://doi.org/10.1214/23-AAP1969}
}

@article{chen2024probabilistic,
      title={Probabilistic Forecasting with Stochastic Interpolants and {F}\"ollmer Processes}, 
      author={Yifan Chen and Mark Goldstein and Mengjian Hua and Michael S. Albergo and Nicholas M. Boffi and Eric Vanden-Eijnden},
      year={2024},
      volume={2403.13724},
      journal={arXiv},
      primaryClass={cs.LG}
}

@article{annurev:/content/journals/10.1146/annurev-physchem-042018-052331,
   author = "Noé, Frank and Tkatchenko, Alexandre and Müller, Klaus-Robert and Clementi, Cecilia",
   title = "Machine Learning for Molecular Simulation", 
   journal= "Annual Review of Physical Chemistry",
   year = "2020",
   volume = "71",
   number = "Volume 71, 2020",
   pages = "361-390",
   doi = "https://doi.org/10.1146/annurev-physchem-042018-052331",
   url = "https://www.annualreviews.org/content/journals/10.1146/annurev-physchem-042018-052331",
   publisher = "Annual Reviews",
   issn = "1545-1593",
   type = "Journal Article",
   keywords = "molecular simulation",
   keywords = "machine learning",
   keywords = "coarse graining",
   keywords = "kinetics",
   keywords = "quantum mechanics",
   keywords = "neural networks",
   abstract = "Machine learning (ML) is transforming all areas of science. The complex and time-consuming calculations in molecular simulations are particularly suitable for an ML revolution and have already been profoundly affected by the application of existing ML methods. Here we review recent ML methods for molecular simulation, with particular focus on (deep) neural networks for the prediction of quantum-mechanical energies and forces, on coarse-grained molecular dynamics, on the extraction of free energy surfaces and kinetics, and on generative network approaches to sample molecular equilibrium structures and compute thermodynamics. To explain these methods and illustrate open methodological problems, we review some important principles of molecular physics and describe how they can be incorporated into ML structures. Finally, we identify and describe a list of open challenges for the interface between ML and molecular simulation.",
  }


@inproceedings{10.5555/3618408.3619484,
author = {Neklyudov, Kirill and Brekelmans, Rob and Severo, Daniel and Makhzani, Alireza},
title = {Action matching: learning stochastic dynamics from samples},
year = {2023},
publisher = {JMLR.org},
abstract = {Learning the continuous dynamics of a system from snapshots of its temporal marginals is a problem which appears throughout natural sciences and machine learning, including in quantum systems, single-cell biological data, and generative modeling. In these settings, we assume access to cross-sectional samples that are uncorrelated over time, rather than full trajectories of samples. In order to better understand the systems under observation, we would like to learn a model of the underlying process that allows us to propagate samples in time and thereby simulate entire individual trajectories. In this work, we propose Action Matching, a method for learning a rich family of dynamics using only independent samples from its time evolution. We derive a tractable training objective, which does not rely on explicit assumptions about the underlying dynamics and does not require back-propagation through differential equations or optimal transport solvers. Inspired by connections with optimal transport, we derive extensions of Action Matching to learn stochastic differential equations and dynamics involving creation and destruction of probability mass. Finally, we showcase applications of Action Matching by achieving competitive performance in a diverse set of experiments from biology, physics, and generative modeling.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1076},
numpages = {32},
location = {, Honolulu, Hawaii, USA, },
series = {ICML'23}
}



@InProceedings{pmlr-v48-hashimoto16,
  title = 	 {Learning Population-Level Diffusions with Generative {RNNs}},
  author = 	 {Hashimoto, Tatsunori and Gifford, David and Jaakkola, Tommi},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2417--2426},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/hashimoto16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/hashimoto16.html},
  abstract = 	 {We estimate stochastic processes that govern the dynamics of evolving populations such as cell differentiation. The problem is challenging since longitudinal trajectory measurements of individuals in a population are rarely available due to experimental cost and/or privacy. We show that cross-sectional samples from an evolving population suffice for recovery within a class of processes even if samples are available only at a few distinct time points. We provide a stratified analysis of recoverability conditions, and establish that reversibility is sufficient for recoverability. For estimation, we derive a natural loss and regularization, and parameterize the processes as diffusive recurrent neural networks. We demonstrate the approach in the context of uncovering complex cellular dynamics known as the ‘epigenetic landscape’ from existing biological assays.}
}


@inproceedings{
holzschuh2023solving,
title={Solving Inverse Physics Problems with Score Matching},
author={Benjamin Holzschuh and Simona Vegetti and Nils Thuerey},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=2BpoGPSDCR}
}

@article{doi:10.1098/rspa.2021.0162,
author = {Lee, Kookjin  and Parish, Eric J. },
title = {Parameterized neural ordinary differential equations: applications to computational physics problems},
journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
volume = {477},
number = {2253},
pages = {20210162},
year = {2021},
doi = {10.1098/rspa.2021.0162},

URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.2021.0162},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rspa.2021.0162}
,
    abstract = { This work proposes an extension of neural ordinary differential equations (NODEs) by introducing an additional set of ODE input parameters to NODEs. This extension allows NODEs to learn multiple dynamics specified by the input parameter instances. Our extension is inspired by the concept of parameterized ODEs, which are widely investigated in computational science and engineering contexts, where characteristics of the governing equations vary over the input parameters. We apply the proposed parameterized NODEs (PNODEs) for learning latent dynamics of complex dynamical processes that arise in computational physics, which is an essential component for enabling rapid numerical simulations for time-critical physics applications. For this, we propose an encoder–decoder-type framework, which models latent dynamics as PNODEs. We demonstrate the effectiveness of PNODEs on benchmark problems from computational physics. }
}


@article{tzen2019neural,
      title={Neural Stochastic Differential Equations: Deep Latent Gaussian Models in the Diffusion Limit}, 
      author={Belinda Tzen and Maxim Raginsky},
      year={2019},
      volume={1905.09883},
      journal={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{NEURIPS2022_09116662,
 author = {Salvi, Cristopher and Lemercier, Maud and Gerasimovics, Andris},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {1333--1344},
 publisher = {Curran Associates, Inc.},
 title = {Neural Stochastic PDEs: Resolution-Invariant Learning of Continuous Spatiotemporal Dynamics},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/091166620a04a289c555f411d8899049-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@InProceedings{pmlr-v139-kidger21b,
  title = 	 {Neural {SDEs} as Infinite-Dimensional {GANs}},
  author =       {Kidger, Patrick and Foster, James and Li, Xuechen and Lyons, Terry J},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {5453--5463},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/kidger21b/kidger21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/kidger21b.html},
  abstract = 	 {Stochastic differential equations (SDEs) are a staple of mathematical modelling of temporal dynamics. However, a fundamental limitation has been that such models have typically been relatively inflexible, which recent work introducing Neural SDEs has sought to solve. Here, we show that the current classical approach to fitting SDEs may be approached as a special case of (Wasserstein) GANs, and in doing so the neural and classical regimes may be brought together. The input noise is Brownian motion, the output samples are time-evolving paths produced by a numerical solver, and by parameterising a discriminator as a Neural Controlled Differential Equation (CDE), we obtain Neural SDEs as (in modern machine learning parlance) continuous-time generative time series models. Unlike previous work on this problem, this is a direct extension of the classical approach without reference to either prespecified statistics or density functions. Arbitrary drift and diffusions are admissible, so as the Wasserstein loss has a unique global minima, in the infinite data limit \textit{any} SDE may be learnt.}
}

@article{albergo2023stochastic,
      title={Stochastic Interpolants: A Unifying Framework for Flows and Diffusions}, 
      author={Michael S. Albergo and Nicholas M. Boffi and Eric Vanden-Eijnden},
      year={2023},
      volume={2303.08797},
      journal={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
albergo2023building,
title={Building Normalizing Flows with Stochastic Interpolants},
author={Michael Samuel Albergo and Eric Vanden-Eijnden},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=li7qeBbCR1t}
}

@ARTICLE{6795935,
  author={Vincent, Pascal},
  journal={Neural Computation}, 
  title={A Connection Between Score Matching and Denoising Autoencoders}, 
  year={2011},
  volume={23},
  number={7},
  pages={1661-1674},
  keywords={},
  doi={10.1162/NECO_a_00142}}

@article{UY2023224,
title = {Operator inference with roll outs for learning reduced models from scarce and low-quality data},
journal = {Computers \& Mathematics with Applications},
volume = {145},
pages = {224-239},
year = {2023},
author = {Wayne Isaac Tan Uy and Dirk Hartmann and Benjamin Peherstorfer},
}%  

@InProceedings{pmlr-v118-li20a,
  title = 	 {Scalable Gradients and Variational Inference for
 Stochastic Differential Equations },
  author =       {Li, Xuechen and Wong, Ting-Kam Leonard and Chen, Ricky T. Q. and Duvenaud, David K.},
  booktitle = 	 {Proceedings of The 2nd Symposium on
 Advances in Approximate Bayesian Inference},
  pages = 	 {1--28},
  year = 	 {2020},
  editor = 	 {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
  volume = 	 {118},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {08 Dec},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v118/li20a/li20a.pdf},
  url = 	 {https://proceedings.mlr.press/v118/li20a.html},
  abstract = 	 { We derive reverse-mode (or adjoint) automatic differentiation for solutions of stochastic differential equations (SDEs), allowing time-efficient and constant-memory computation of pathwise gradients, a continuous-time analogue of the reparameterization trick. Specifically, we construct a backward SDE whose solution is the gradient and provide conditions under which numerical solutions converge. We also combine our stochastic adjoint approach with a stochastic variational inference scheme for continuous-time SDE models, allowing us to learn distributions over functions using stochastic gradient descent. Our latent SDE model achieves competitive performance compared to existing approaches on time series modeling.}
}

@book{RobertBook,
title = {{M}onte {C}arlo Statistical Methods},
author = {C. Robert and G. Casella},
publisher = {Springer},
year = {2004}
}

@inproceedings{song2020generativealg,
 author = {Song, Yang and Ermon, Stefano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Modeling by Estimating Gradients of the Data Distribution},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{
song2021scorebased,
title={Score-Based Generative Modeling through Stochastic Differential Equations},
author={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=PxTIG12RRHS}
}

@inproceedings{song2019sliced,
  author    = {Yang Song and
               Sahaj Garg and
               Jiaxin Shi and
               Stefano Ermon},
  title     = {Sliced Score Matching: {A} Scalable Approach to Density and Score
               Estimation},
  booktitle = {Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial
               Intelligence, {UAI} 2019, Tel Aviv, Israel, July 22-25, 2019},
  pages     = {204},
  year      = {2019},
  url       = {http://auai.org/uai2019/proceedings/papers/204.pdf},
}

@article{JMLR:v6:hyvarinen05a,
  author  = {Aapo Hyv{{\"a}}rinen},
  title   = {Estimation of Non-Normalized Statistical Models by Score Matching},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {24},
  pages   = {695--709},
  url     = {http://jmlr.org/papers/v6/hyvarinen05a.html}
}

@inproceedings{NEURIPS2019_21be9a4b,
 author = {Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Augmented Neural {ODEs}},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/21be9a4bd4f81549a9d1d241981cec3c-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{NEURIPS2018_69386f6b,
 author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Ordinary Differential Equations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{PeterReiterer_1998,
doi = {10.1088/0305-4470/31/34/015},
url = {https://dx.doi.org/10.1088/0305-4470/31/34/015},
year = {1998},
month = {aug},
publisher = {},
volume = {31},
number = {34},
pages = {7121},
author = {Peter Reiterer and  Claudia Lainscsek and  Ferdinand Schürrer and  Christophe Letellier and  Jean Maquet},
title = {A nine-dimensional Lorenz system to study high-dimensional chaos},
journal = {Journal of Physics A: Mathematical and General},
abstract = {We examine the dynamics of three-dimensional cells with square planform in dissipative Rayleigh-Bénard convection. By applying a triple Fourier series ansatz up to second order, we obtain a system of nine nonlinear ordinary differential equations from the governing hydrodynamic equations. Depending on two control parameters, namely the Rayleigh number and the Prandtl number, the asymptotic behaviour can be stationary, periodic, quasiperiodic or chaotic. A period-doubling cascade is identified as a route to chaos. Hereafter, the asymptotic behaviour progressively evolves towards a hyperchaotic attractor. For given values of control parameters beyond the accumulation point, we observe a low-dimensional chaotic attractor as is currently done for dissipative systems. Although the correlation dimension strongly suggests that this attractor could be embedded in a three-dimensional space, a topological characterization reveals that a higher-dimensional space must be used. Thus, we reconstruct a four-dimensional model which is found to be in agreement with the properties of the original dynamics. The nine-dimensional Lorenz model could therefore play a significant role in developing tools to characterize chaotic attractors embedded in phase space with a dimension greater than 3.}
}

@inproceedings{
hu_lora_2021,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@article{BRUNA2024112588,
title = {Neural {Galerkin} schemes with active learning for high-dimensional evolution equations},
journal = {Journal of Computational Physics},
volume = {496},
pages = {112588},
year = {2024},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.112588},
url = {https://www.sciencedirect.com/science/article/pii/S0021999123006836},
author = {Joan Bruna and Benjamin Peherstorfer and Eric Vanden-Eijnden},
keywords = {Scientific computing, Machine learning, Deep neural networks, Partial differential equations, Active learning, Monte Carlo},
abstract = {Deep neural networks have been shown to provide accurate function approximations in high dimensions. However, fitting network parameters requires informative training data that are often challenging to collect in science and engineering applications. This work proposes Neural Galerkin schemes based on deep learning that generate training data with active learning for numerically solving high-dimensional partial differential equations. Neural Galerkin schemes build on the Dirac-Frenkel variational principle to train networks by minimizing the residual sequentially over time, which enables adaptively collecting new training data in a self-informed manner that is guided by the dynamics described by the partial differential equations. This is in contrast to other machine learning methods that aim to fit network parameters globally in time without taking into account training data acquisition. Our finding is that the active form of gathering training data of the proposed Neural Galerkin schemes is key for numerically realizing the expressive power of networks in high dimensions. Numerical experiments demonstrate that Neural Galerkin schemes have the potential to enable simulating phenomena and processes with many variables for which traditional and other deep-learning-based solvers fail, especially when features of the solutions evolve locally such as in high-dimensional wave propagation problems and interacting particle systems described by Fokker-Planck and kinetic equations.}
}%   

@inproceedings{
cachay2023dyffusion,
title={{DY}ffusion: A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting},
author={Salva R{\"u}hling Cachay and Bo Zhao and Hailey James and Rose Yu},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=WRGldGm5Hz}
}

@article{P22AMS,
title = {Breaking the {Kolmogorov} Barrier with Nonlinear Model Reduction},
author = {Peherstorfer, B.},
journal = {Notices of the American Mathematical Society},
volume = {69},
pages = {725-733},
year = {2022},
}

@Article{Benner2015,
author={Benner, Peter
and Redmann, Martin},
title={Model reduction for stochastic systems},
journal={Stochastic Partial Differential Equations: Analysis and Computations},
year={2015},
month={Sep},
day={01},
volume={3},
number={3},
pages={291-338},
abstract={To solve a stochastic linear evolution equation numerically, finite dimensional approximations are commonly used. If one uses the well-known Galerkin scheme, one might end up with a sequence of ordinary stochastic linear equations of high order. To reduce the high dimension for practical computations we consider balanced truncation as a model order reduction technique. This approach is well-known from deterministic control theory and successfully employed in practice for decades. So, we generalize balanced truncation for controlled linear systems with Levy noise, discuss properties of the reduced order model, provide an error bound, and give some examples.},
issn={2194-041X},
doi={10.1007/s40072-015-0050-1},
url={https://doi.org/10.1007/s40072-015-0050-1}
}



@article{CHEN2024112984,
title = {Learning stochastic dynamical system via flow map operator},
journal = {Journal of Computational Physics},
volume = {508},
pages = {112984},
year = {2024},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2024.112984},
url = {https://www.sciencedirect.com/science/article/pii/S002199912400233X},
author = {Yuan Chen and Dongbin Xiu},
keywords = {Data driven modeling, Stochastic dynamical systems, Deep neural networks, Generative adversarial networks, Stochastic differential equations},
abstract = {We present a numerical framework for learning unknown stochastic dynamical systems using measurement data. Termed stochastic flow map learning (sFML), the new framework is an extension of flow map learning (FML) that was developed for learning deterministic dynamical systems. For learning stochastic systems, we define a stochastic flow map that is a superposition of two sub-flow maps: a deterministic sub-map and a stochastic sub-map. The stochastic training data are used to construct the deterministic sub-map first, followed by the stochastic sub-map. The deterministic sub-map takes the form of residual network (ResNet), similar to the work of FML for deterministic systems. For the stochastic sub-map, we employ a generative model, particularly generative adversarial networks (GANs) in this paper. The final constructed stochastic flow map then defines a stochastic evolution model that is a weak approximation, in term of distribution, of the unknown stochastic system. A comprehensive set of numerical examples are presented to demonstrate the flexibility and effectiveness of the proposed sFML method for various types of stochastic systems.}
}

@InProceedings{BP24COLORA,
  title = 	 {{C}o{L}o{RA}: Continuous low-rank adaptation for reduced implicit neural modeling of parameterized partial differential equations},
  author =       {Berman, Jules and Peherstorfer, Benjamin},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {3565--3583},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
}

@article{CHENG2014630,
title = {Energy-conserving discontinuous {Galerkin} methods for the {Vlasov}–{A}mpère system},
journal = {Journal of Computational Physics},
volume = {256},
pages = {630-655},
year = {2014},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2013.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0021999113006189},
author = {Yingda Cheng and Andrew J. Christlieb and Xinghui Zhong},
keywords = {Vlasov–Ampère system, Energy conservation, Discontinuous Galerkin methods, Landau damping, Two-stream instability, Bump-on-tail instability},
abstract = {In this paper, we propose energy-conserving numerical schemes for the Vlasov–Ampère (VA) systems. The VA system is a model used to describe the evolution of probability density function of charged particles under self consistent electric field in plasmas. It conserves many physical quantities, including the total energy which is comprised of the kinetic and electric energy. Unlike the total particle number conservation, the total energy conservation is challenging to achieve. For simulations in longer time ranges, negligence of this fact could cause unphysical results, such as plasma self heating or cooling. In this paper, we develop the first Eulerian solvers that can preserve fully discrete total energy conservation. The main components of our solvers include explicit or implicit energy-conserving temporal discretizations, an energy-conserving operator splitting for the VA equation and discontinuous Galerkin finite element methods for the spatial discretizations. We validate our schemes by rigorous derivations and benchmark numerical examples such as Landau damping, two-stream instability and bump-on-tail instability.}
}%    

@ARTICLE{Banks20102198,
	author = {Banks, Jeffrey William and Hittinger, Jeffrey Alan Furst},
	title = {A new class of nonlinear finite-volume methods for {Vlasov} simulation},
	year = {2010},
	journal = {IEEE Transactions on Plasma Science},
	volume = {38},
	number = {9 PART 1},
	pages = {2198 – 2207},
	doi = {10.1109/TPS.2010.2056937},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956610596&doi=10.1109%2fTPS.2010.2056937&partnerID=40&md5=c695a00bb753e9bdb541dd6e96919c95},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
}%  

@article{HITTINGER2013118,
title = {Block-structured adaptive mesh refinement algorithms for {Vlasov} simulation},
journal = {Journal of Computational Physics},
volume = {241},
pages = {118-140},
year = {2013},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2013.01.030},
url = {https://www.sciencedirect.com/science/article/pii/S0021999113000740},
author = {J.A.F. Hittinger and J.W. Banks},
keywords = {Kinetic simulations, Vlasov, Finite-volume methods, Adaptive mesh refinement},
abstract = {Direct discretization of continuum kinetic equations, like the Vlasov equation, are under-utilized because the distribution function generally exists in a high-dimensional (>3D) space and computational cost increases geometrically with dimension. We propose to use high-order finite-volume techniques with block-structured adaptive mesh refinement (AMR) to reduce the computational cost. The primary complication comes from a solution state comprised of variables of different dimensions. We develop the algorithms required to extend standard single-dimension block structured AMR to the multi-dimension case. Specifically, algorithms for reduction and injection operations that transfer data between mesh hierarchies of different dimensions are explained in detail. In addition, modifications to the basic AMR algorithm that enable the use of high-order spatial and temporal discretizations are discussed. Preliminary results for a standard 1D+1V Vlasov–Poisson test problem are presented. Results indicate that there is potential for significant savings for some classes of Vlasov problems.}
}%     