\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrews et~al.(2016)Andrews, Tanay, Morton, and
  Griffin]{andrews2016transfer}
Andrews, J., Tanay, T., Morton, E.~J., and Griffin, L.~D.
\newblock Transfer representation-learning for anomaly detection.
\newblock JMLR, 2016.

\bibitem[Austern \& Zhou(2020)Austern and Zhou]{austern2020asymptotics}
Austern, M. and Zhou, W.
\newblock Asymptotics of cross-validation.
\newblock \emph{arXiv preprint arXiv:2001.11111}, 2020.

\bibitem[Balcan et~al.(2019)Balcan, Khodak, and Talwalkar]{balcan2019}
Balcan, M.-F., Khodak, M., and Talwalkar, A.
\newblock Provable guarantees for gradient-based meta-learning.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pp.\  424--433. PMLR, 09--15
  Jun 2019.

\bibitem[Bartlett et~al.(2006)Bartlett, Jordan, and
  McAuliffe]{bartlett2006convexity}
Bartlett, P.~L., Jordan, M.~I., and McAuliffe, J.~D.
\newblock Convexity, classification, and risk bounds.
\newblock \emph{Journal of the American Statistical Association}, 101\penalty0
  (473):\penalty0 138--156, 2006.

\bibitem[Bayle et~al.(2020)Bayle, Bayle, Janson, and Mackey]{bayle2020cross}
Bayle, P., Bayle, A., Janson, L., and Mackey, L.
\newblock Cross-validation confidence intervals for test error.
\newblock \emph{arXiv preprint arXiv:2007.12671}, 2020.

\bibitem[Ben-David et~al.(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,
  and Vaughan]{ben2010theory}
Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan,
  J.~W.
\newblock A theory of learning from different domains.
\newblock \emph{Machine learning}, 79\penalty0 (1):\penalty0 151--175, 2010.

\bibitem[Blitzer et~al.(2007{\natexlab{a}})Blitzer, Crammer, Kulesza, Pereira,
  and Wortman]{blitzer2007learning}
Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Wortman, J.
\newblock Learning bounds for domain adaptation.
\newblock \emph{Advances in neural information processing systems}, 20,
  2007{\natexlab{a}}.

\bibitem[Blitzer et~al.(2007{\natexlab{b}})Blitzer, Dredze, and
  Pereira]{blitzer2007biographies}
Blitzer, J., Dredze, M., and Pereira, F.
\newblock Biographies, bollywood, boom-boxes and blenders: Domain adaptation
  for sentiment classification.
\newblock In \emph{Proceedings of the 45th annual meeting of the association of
  computational linguistics}, pp.\  440--447, 2007{\natexlab{b}}.

\bibitem[Bousmalis et~al.(2018)Bousmalis, Irpan, Wohlhart, Bai, Kelcey,
  Kalakrishnan, Downs, Ibarz, Pastor, Konolige, et~al.]{bousmalis2018using}
Bousmalis, K., Irpan, A., Wohlhart, P., Bai, Y., Kelcey, M., Kalakrishnan, M.,
  Downs, L., Ibarz, J., Pastor, P., Konolige, K., et~al.
\newblock Using simulation and domain adaptation to improve efficiency of deep
  robotic grasping.
\newblock In \emph{2018 IEEE international conference on robotics and
  automation (ICRA)}, pp.\  4243--4250. IEEE, 2018.

\bibitem[Bousquet \& Elisseeff(2002)Bousquet and
  Elisseeff]{bousquet2002stability}
Bousquet, O. and Elisseeff, A.
\newblock Stability and generalization.
\newblock \emph{The Journal of Machine Learning Research}, 2:\penalty0
  499--526, 2002.

\bibitem[Bousquet et~al.(2020)Bousquet, Klochkov, and
  Zhivotovskiy]{bousquet2020sharper}
Bousquet, O., Klochkov, Y., and Zhivotovskiy, N.
\newblock Sharper bounds for uniformly stable algorithms.
\newblock In \emph{Conference on Learning Theory}, pp.\  610--626. PMLR, 2020.

\bibitem[Campi et~al.(2021)Campi, Peters, Azzaoui, and
  Matsui]{campi2021machine}
Campi, M., Peters, G.~W., Azzaoui, N., and Matsui, T.
\newblock Machine learning mitigants for speech based cyber risk.
\newblock \emph{IEEE Access}, 9:\penalty0 136831--136860, 2021.

\bibitem[Campi et~al.(2023)Campi, Peters, and Toczydlowska]{campi2023ataxic}
Campi, M., Peters, G.~W., and Toczydlowska, D.
\newblock Ataxic speech disorders and parkinson’s disease diagnostics via
  stochastic embedding of empirical mode decomposition.
\newblock \emph{Plos one}, 18\penalty0 (4):\penalty0 e0284667, 2023.

\bibitem[Celisse \& Mary-Huard(2018)Celisse and
  Mary-Huard]{celisse2018theoretical}
Celisse, A. and Mary-Huard, T.
\newblock Theoretical analysis of cross-validation for estimating the risk of
  the k-nearest neighbor classifier.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2373--2426, 2018.

\bibitem[Chandola et~al.(2009)Chandola, Banerjee, and
  Kumar]{chandola2009anomaly}
Chandola, V., Banerjee, A., and Kumar, V.
\newblock Anomaly detection: A survey.
\newblock \emph{ACM computing surveys (CSUR)}, 41\penalty0 (3):\penalty0 1--58,
  2009.

\bibitem[Charles \& Papailiopoulos(2018)Charles and Papailiopoulos]{charles18a}
Charles, Z. and Papailiopoulos, D.
\newblock Stability and generalization of learning algorithms that converge to
  global optima.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  745--754. PMLR, 10--15 Jul 2018.

\bibitem[Cho \& Hariharan(2019)Cho and Hariharan]{cho2019efficacy}
Cho, J.~H. and Hariharan, B.
\newblock On the efficacy of knowledge distillation.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pp.\  4794--4802, 2019.

\bibitem[Colombo et~al.(2022{\natexlab{a}})Colombo, Dadalto, Staerman, Noiry,
  and Piantanida]{colombo2022beyond}
Colombo, P., Dadalto, E., Staerman, G., Noiry, N., and Piantanida, P.
\newblock Beyond mahalanobis distance for textual ood detection.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 17744--17759, 2022{\natexlab{a}}.

\bibitem[Colombo et~al.(2022{\natexlab{b}})Colombo, Staerman, Noiry, and
  Piantanida]{colombo2022learning}
Colombo, P., Staerman, G., Noiry, N., and Piantanida, P.
\newblock Learning disentangled textual representations via statistical
  measures of similarity.
\newblock \emph{arXiv preprint arXiv:2205.03589}, 2022{\natexlab{b}}.

\bibitem[Cortes et~al.(2015)Cortes, Mohri, and
  Mu{\~n}oz~Medina]{cortes2015adaptation}
Cortes, C., Mohri, M., and Mu{\~n}oz~Medina, A.
\newblock Adaptation algorithm and theory based on generalized discrepancy.
\newblock In \emph{Proceedings of the 21th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pp.\  169--178, 2015.

\bibitem[Darrin et~al.(2023)Darrin, Staerman, Gomes, Cheung, Piantanida, and
  Colombo]{darrin2023unsupervised}
Darrin, M., Staerman, G., Gomes, E. D.~C., Cheung, J.~C., Piantanida, P., and
  Colombo, P.
\newblock Unsupervised layer-wise score aggregation for textual ood detection.
\newblock \emph{arXiv preprint arXiv:2302.09852}, 2023.

\bibitem[Denevi et~al.(2019)Denevi, Ciliberto, Grazzi, and Pontil]{denevi19a}
Denevi, G., Ciliberto, C., Grazzi, R., and Pontil, M.
\newblock Learning-to-learn stochastic gradient descent with biased
  regularization.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pp.\  1566--1575. PMLR,
  09--15 Jun 2019.

\bibitem[Denevi et~al.(2020)Denevi, Pontil, and Ciliberto]{Denevi2020}
Denevi, G., Pontil, M., and Ciliberto, C.
\newblock The advantage of conditional meta-learning for biased regularization
  and fine tuning.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  964--974. Curran Associates, Inc., 2020.

\bibitem[Devroye \& Wagner(1979)Devroye and Wagner]{DEvroy-79}
Devroye, L. and Wagner, T.
\newblock Distribution-free performance bounds for potential function rules.
\newblock \emph{IEEE Transactions on Information Theory}, 25\penalty0
  (5):\penalty0 601--604, 1979.
\newblock \doi{10.1109/TIT.1979.1056087}.

\bibitem[Dhouib \& Redko(2018)Dhouib and Redko]{dhouib2018revisiting}
Dhouib, S. and Redko, I.
\newblock Revisiting similarity learning for domain adaptation.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Dredze et~al.(2007)Dredze, Blitzer, Talukdar, Ganchev, Gra{\c{c}}a,
  and Pereira]{dredze2007frustratingly}
Dredze, M., Blitzer, J., Talukdar, P.~P., Ganchev, K., Gra{\c{c}}a, J.~V., and
  Pereira, F.
\newblock Frustratingly hard domain adaptation for dependency parsing.
\newblock 2007.

\bibitem[Du et~al.(2017)Du, Koushik, Singh, and P{\'o}czos]{du2017hypothesis}
Du, S.~S., Koushik, J., Singh, A., and P{\'o}czos, B.
\newblock Hypothesis transfer learning via transformation functions.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Dugas et~al.(2000)Dugas, Bengio, B{\'e}lisle, Nadeau, and
  Garcia]{dugas2000incorporating}
Dugas, C., Bengio, Y., B{\'e}lisle, F., Nadeau, C., and Garcia, R.
\newblock Incorporating second-order functional knowledge for better option
  pricing.
\newblock \emph{Advances in neural information processing systems}, 13, 2000.

\bibitem[Elisseeff et~al.(2005)Elisseeff, Evgeniou, and Pontil]{elisseeff05a}
Elisseeff, A., Evgeniou, T., and Pontil, M.
\newblock Stability of randomized learning algorithms.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0 (3):\penalty0
  55--79, 2005.

\bibitem[Freund \& Schapire(1997)Freund and Schapire]{freund1997decision}
Freund, Y. and Schapire, R.~E.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock \emph{Journal of computer and system sciences}, 55\penalty0
  (1):\penalty0 119--139, 1997.

\bibitem[Golovanov et~al.(2019)Golovanov, Kurbanov, Nikolenko, Truskovskyi,
  Tselousov, and Wolf]{golovanov2019large}
Golovanov, S., Kurbanov, R., Nikolenko, S., Truskovskyi, K., Tselousov, A., and
  Wolf, T.
\newblock Large-scale transfer learning for natural language generation.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  6053--6058, 2019.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
Hardt, M., Recht, B., and Singer, Y.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{International conference on machine learning}, pp.\
  1225--1234. PMLR, 2016.

\bibitem[Kearns \& Ron(1999)Kearns and Ron]{kearns1999algorithmic}
Kearns, M. and Ron, D.
\newblock Algorithmic stability and sanity-check bounds for leave-one-out
  cross-validation.
\newblock \emph{Neural computation}, 11\penalty0 (6):\penalty0 1427--1453,
  1999.

\bibitem[Khodak et~al.(2019)Khodak, Balcan, and Talwalkar]{Khodak2019}
Khodak, M., Balcan, M.-F.~F., and Talwalkar, A.~S.
\newblock Adaptive gradient-based meta-learning methods.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Klochkov \& Zhivotovskiy(2021)Klochkov and
  Zhivotovskiy]{nikita2021stability}
Klochkov, Y. and Zhivotovskiy, N.
\newblock Stability and deviation optimal risk bounds with convergence rate $ o
  (1/n) $.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 5065--5076, 2021.

\bibitem[Kumar et~al.(2013)Kumar, Lokshtanov, Vassilvitskii, and
  Vattani]{kumar2013near}
Kumar, R., Lokshtanov, D., Vassilvitskii, S., and Vattani, A.
\newblock Near-optimal bounds for cross-validation via loss stability.
\newblock In \emph{International Conference on Machine Learning}, pp.\  27--35.
  PMLR, 2013.

\bibitem[Kutin \& Niyogi(2002)Kutin and Niyogi]{kutin2002}
Kutin, S. and Niyogi, P.
\newblock Almost-everywhere algorithmic stability and generalization error.
\newblock In \emph{Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence}, UAI'02, pp.\  275–282, San Francisco, CA, USA,
  2002. Morgan Kaufmann Publishers Inc.
\newblock ISBN 1558608974.

\bibitem[Kuzborskij \& Orabona(2013)Kuzborskij and Orabona]{kuzborskij2013}
Kuzborskij, I. and Orabona, F.
\newblock Stability and hypothesis transfer learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  942--950. PMLR, 2013.

\bibitem[Kuzborskij \& Orabona(2017)Kuzborskij and Orabona]{kuzborskij2017fast}
Kuzborskij, I. and Orabona, F.
\newblock Fast rates by transferring from auxiliary hypotheses.
\newblock \emph{Machine Learning}, 106\penalty0 (2):\penalty0 171--195, 2017.

\bibitem[Laforgue et~al.(2021)Laforgue, Staerman, and
  Cl{\'e}men{\c{c}}on]{laforgue2021generalization}
Laforgue, P., Staerman, G., and Cl{\'e}men{\c{c}}on, S.
\newblock Generalization bounds in the presence of outliers: a median-of-means
  study.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5937--5947. PMLR, 2021.

\bibitem[Li \& Bilmes(2007)Li and Bilmes]{li2007bayesian}
Li, X. and Bilmes, J.
\newblock A bayesian divergence prior for classiffier adaptation.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  275--282.
  PMLR, 2007.

\bibitem[Liu et~al.(2019)Liu, Shi, Ji, and Jia]{liu2019survey}
Liu, R., Shi, Y., Ji, C., and Jia, M.
\newblock A survey of sentiment analysis based on transfer learning.
\newblock \emph{IEEE Access}, 7:\penalty0 85401--85412, 2019.

\bibitem[Mansour et~al.(2009)Mansour, Mohri, and
  Rostamizadeh]{mansour2009domain}
Mansour, Y., Mohri, M., and Rostamizadeh, A.
\newblock Domain adaptation: Learning bounds and algorithms.
\newblock \emph{arXiv preprint arXiv:0902.3430}, 2009.

\bibitem[Morvant et~al.(2012)Morvant, Habrard, and
  Ayache]{morvant2012parsimonious}
Morvant, E., Habrard, A., and Ayache, S.
\newblock Parsimonious unsupervised and semi-supervised domain adaptation with
  good similarity functions.
\newblock \emph{Knowledge and Information Systems}, 33\penalty0 (2):\penalty0
  309--349, 2012.

\bibitem[Nesterov et~al.(2018)]{nesterov2018lectures}
Nesterov, Y. et~al.
\newblock \emph{Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem[Orabona et~al.(2009)Orabona, Castellini, Caputo, Fiorilla, and
  Sandini]{orabona2009}
Orabona, F., Castellini, C., Caputo, B., Fiorilla, A.~E., and Sandini, G.
\newblock Model adaptation with least-squares svm for adaptive hand
  prosthetics.
\newblock In \emph{2009 IEEE International Conference on Robotics and
  Automation}, pp.\  2897--2903. IEEE, 2009.

\bibitem[Perrot \& Habrard(2015)Perrot and Habrard]{perrot2015theoretical}
Perrot, M. and Habrard, A.
\newblock A theoretical analysis of metric hypothesis transfer learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1708--1717. PMLR, 2015.

\bibitem[Picot et~al.(2023)Picot, Granese, Staerman, Romanelli, Messina,
  Piantanida, and Colombo]{picothalfspace}
Picot, M., Granese, F., Staerman, G., Romanelli, M., Messina, F., Piantanida,
  P., and Colombo, P.
\newblock A halfspace-mass depth-based method for adversarial attack detection.
\newblock \emph{Transactions on Machine Learning Research}, 2023.

\bibitem[Rifkin et~al.(2003)Rifkin, Yeo, Poggio, et~al.]{rifkin2003regularized}
Rifkin, R., Yeo, G., Poggio, T., et~al.
\newblock Regularized least-squares classification.
\newblock \emph{Nato Science Series Sub Series III Computer and Systems
  Sciences}, 190:\penalty0 131--154, 2003.

\bibitem[Ruder et~al.(2019)Ruder, Peters, Swayamdipta, and
  Wolf]{ruder2019transfer}
Ruder, S., Peters, M.~E., Swayamdipta, S., and Wolf, T.
\newblock Transfer learning in natural language processing.
\newblock In \emph{Proceedings of the 2019 conference of the North American
  chapter of the association for computational linguistics: Tutorials}, pp.\
  15--18, 2019.

\bibitem[Sch{\"o}lkopf et~al.(2001)Sch{\"o}lkopf, Herbrich, and
  Smola]{scholkopf2001generalized}
Sch{\"o}lkopf, B., Herbrich, R., and Smola, A.~J.
\newblock A generalized representer theorem.
\newblock In \emph{International conference on computational learning theory},
  pp.\  416--426. Springer, 2001.

\bibitem[Shafahi et~al.(2020)Shafahi, Saadatpanah, Zhu, Ghiasi, Studer, Jacobs,
  and Goldstein]{shafahi2020adversarially}
Shafahi, A., Saadatpanah, P., Zhu, C., Ghiasi, A., Studer, C., Jacobs, D., and
  Goldstein, T.
\newblock Adversarially robust transfer learning.
\newblock In \emph{8th International Conference on Learning Representations
  (ICLR 2020)(virtual)}. International Conference on Learning Representations,
  2020.

\bibitem[Staerman et~al.(2020)Staerman, Mozharovskyi, Cl{\'e}men,
  et~al.]{staerman2020area}
Staerman, G., Mozharovskyi, P., Cl{\'e}men, S., et~al.
\newblock The area of the convex hull of sampled curves: a robust functional
  statistical depth measure.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  570--579. PMLR, 2020.

\bibitem[Staerman et~al.(2021{\natexlab{a}})Staerman, Laforgue, Mozharovskyi,
  and d’Alch{\'e} Buc]{staerman2021ot}
Staerman, G., Laforgue, P., Mozharovskyi, P., and d’Alch{\'e} Buc, F.
\newblock When ot meets mom: Robust estimation of wasserstein distance.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  136--144. PMLR, 2021{\natexlab{a}}.

\bibitem[Staerman et~al.(2021{\natexlab{b}})Staerman, Mozharovskyi, Colombo,
  Cl{\'e}men{\c{c}}on, and d'Alch{\'e} Buc]{staerman2021pseudo}
Staerman, G., Mozharovskyi, P., Colombo, P., Cl{\'e}men{\c{c}}on, S., and
  d'Alch{\'e} Buc, F.
\newblock A pseudo-metric between probability distributions based on
  depth-trimmed regions.
\newblock \emph{arXiv preprint arXiv:2103.12711}, 2021{\natexlab{b}}.

\bibitem[Staerman et~al.(2022{\natexlab{a}})Staerman, Adjakossa, Mozharovskyi,
  Hofer, Sen~Gupta, and Cl{\'e}men{\c{c}}on]{staerman2022functional}
Staerman, G., Adjakossa, E., Mozharovskyi, P., Hofer, V., Sen~Gupta, J., and
  Cl{\'e}men{\c{c}}on, S.
\newblock Functional anomaly detection: a benchmark study.
\newblock \emph{International Journal of Data Science and Analytics}, pp.\
  1--17, 2022{\natexlab{a}}.

\bibitem[Staerman et~al.(2022{\natexlab{b}})Staerman, Allain, Gramfort, and
  Moreau]{staerman2022fadin}
Staerman, G., Allain, C., Gramfort, A., and Moreau, T.
\newblock Fadin: Fast discretized inference for hawkes processes with general
  parametric kernels.
\newblock \emph{arXiv preprint arXiv:2210.04635}, 2022{\natexlab{b}}.

\bibitem[Vershynin(2018)]{vershynin_2018}
Vershynin, R.
\newblock \emph{High-Dimensional Probability: An Introduction with Applications
  in Data Science}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics.
  Cambridge University Press, 2018.
\newblock \doi{10.1017/9781108231596}.

\bibitem[Wang et~al.(2018)Wang, Zhou, Lu, Maleki, and
  Mirrokni]{wang2018approximate}
Wang, S., Zhou, W., Lu, H., Maleki, A., and Mirrokni, V.
\newblock Approximate leave-one-out for fast parameter tuning in high
  dimensions.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  5228--5237. PMLR, 10--15 Jul 2018.

\bibitem[Wang et~al.(2019)Wang, Dai, P{\'o}czos, and
  Carbonell]{wang2019characterizing}
Wang, Z., Dai, Z., P{\'o}czos, B., and Carbonell, J.
\newblock Characterizing and avoiding negative transfer.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pp.\  11293--11302, 2019.

\bibitem[Weiss et~al.(2016)Weiss, Khoshgoftaar, and Wang]{weiss2016survey}
Weiss, K., Khoshgoftaar, T.~M., and Wang, D.
\newblock A survey of transfer learning.
\newblock \emph{Journal of Big data}, 3\penalty0 (1):\penalty0 1--40, 2016.

\bibitem[Wibisono et~al.(2009)Wibisono, Rosasco, and
  Poggio]{wibisono2009sufficient}
Wibisono, A., Rosasco, L., and Poggio, T.
\newblock Sufficient conditions for uniform stability of regularization
  algorithms.
\newblock \emph{Computer Science and Artificial Intelligence Laboratory
  Technical Report, MIT-CSAIL-TR-2009-060}, 2009.

\bibitem[Zhang et~al.(2012)Zhang, Zhang, and Ye]{zhang2012generalization}
Zhang, C., Zhang, L., and Ye, J.
\newblock Generalization bounds for domain adaptation.
\newblock \emph{Advances in neural information processing systems}, 25, 2012.

\bibitem[Zhang(2004)]{Zhang2004}
Zhang, T.
\newblock Statistical behavior and consistency of classification methods based
  on convex risk minimization.
\newblock \emph{The Annals of Statistics}, 32\penalty0 (1):\penalty0 56--85,
  2004.

\bibitem[Zhang et~al.(2019)Zhang, Liu, Long, and Jordan]{zhang2019bridging}
Zhang, Y., Liu, T., Long, M., and Jordan, M.
\newblock Bridging theory and algorithm for domain adaptation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7404--7413. PMLR, 2019.

\end{thebibliography}
