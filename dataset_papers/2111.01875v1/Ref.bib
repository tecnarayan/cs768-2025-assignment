############################### JOURNALS
@STRING{TPAMI = "{IEEE} Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)"}
@STRING{IJCV = "International Journal of Computer Vision (IJCV)"}
@STRING{PR = "Pattern Recognition"}
@STRING{TNNLS = "{IEEE} Transactions on Neural Networks and Learning Systems (T-NN)"}
@STRING{JMLR = "Journal of Machine Learning Research (JMLR)"}
@STRING{TASP = "{IEEE} Transactions on Acoustics, Speech, and Signal Processing"}
@STRING{SPL = "{IEEE} Signal Processing Letters"}
@STRING{TSP = "{IEEE} Transactions on Signal Processing (TSP)"}
@STRING{SAIT = "{IEEE} Journal on Selected Areas in Information Theory"}
@STRING{MP = "Mathematical Programming"}
@STRING{JASA = "Journal of American Statistical Association"}


############################### CONFERENCES
@STRING{CVPR = "Conference on Computer Vision and Pattern Recognition (CVPR)"}
@STRING{ICCV = "International Conference on Computer Vision (ICCV)"}
@STRING{ICML = "International Conference on Machine Learning (ICML)"}
@STRING{ICMLW = "International Conference on Machine Learning Workshops"}
@STRING{ICLR = "International Conference on Learning Representations (ICLR)"}
@STRING{ICLRW = "International Conference on Learning Representations Workshops"}
@STRING{NIPS = "Advances in neural information processing systems (NeurIPS)"}
@STRING{NIPSW = "NeurIPS Workshops"}
@STRING{AISTATS = "International Conference on Artificial Intelligence and Statistics (AISTATS)"}
@STRING{IJCAI = "International Joint Conferences on Artificial Intelligence (IJCAI)"}
@STRING{AAAI = "AAAI Conference on Artificial Intelligence"}
@STRING{FAT = "Conference on Fairness, Accountability, and Transparency"}
@STRING{Allerton = "Annual Allerton Conference on Communication, Control, and Computing"}
@STRING{COLT = "Conference on Learning Theory"}

@INPROCEEDINGS{zhang2017understanding,
author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht,  Benjamin and Vinyals, Oriol},
booktitle=ICLR,
title={Understanding deep learning requires rethinking generalization},
year={2017},}

@article{zou2018stochastic,
  title={Stochastic gradient descent optimizes over-parameterized deep {ReLU} networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:1811.08888v3},
  year={2018}
}


@INPROCEEDINGS{brutzkus2017globally,
  title={Globally optimal gradient descent for a {ConvNet} with {Gaussian} inputs},
  author={Brutzkus, Alon and Globerson, Amir},
  booktitle=ICML,
  year={2017},}


@INPROCEEDINGS{du2018power,
  title={On the power of over-parametrization in neural networks with quadratic activation},
  author={Du, Simon S. and Lee, Jason D.},
  booktitle=ICML,
  year={2018}
}

@INPROCEEDINGS{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  booktitle=NIPS,
  year={2018}
}


@article{song2019quadratic,
  title={Quadratic Suffices for Over-parametrization via Matrix {Chernoff} Bound},
  author={Song, Zhao and Yang, Xin},
  journal={arXiv preprint arXiv:1906.03593v2},
  year={2019}
}

@INPROCEEDINGS{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S. and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
booktitle=ICLR,
 year={2019}
}

@ARTICLE{oymak2019towards,
  title={Towards moderate overparameterization: global convergence guarantees for training shallow neural networks},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  journal=SAIT,
  year={2020},
  volume={1},
 pages={84--105},}
}

@INPROCEEDINGS{kawaguchi2019gradient,
  title={Gradient Descent Finds Global Minima for Generalizable Deep Neural Networks of Practical Sizes},
  author={Kawaguchi, Kenji and Huang, Jiaoyang},
  booktitle=Allerton,
  year={2019}
}

@INPROCEEDINGS{chizat2019lazy,
  title={On Lazy Training in Differentiable Programming},
  author={Chizat, L\'{e}na\"{i}c and Oyallon, Edouard and Bach, Francis},
  booktitle=NIPS,
  year={2019}
}

@INPROCEEDINGS{yehudai2019power,
  title={On the power and limitations of random features for understanding neural networks},
  author={Yehudai, Gilad and Shamir, Ohad},
  booktitle=NIPS,
  year={2019}
}

@INPROCEEDINGS{ghorbani2019limitations,
	title={Limitations of Lazy Training of Two-layers Neural Networks},
	author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle=NIPS,
  year={2019}
}

@INPROCEEDINGS{jacot2018neural,
	title={Neural tangent kernel: Convergence and generalization in neural networks},
	author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl\'{e}ment},
  booktitle=NIPS,
	year={2018}
}

@INPROCEEDINGS{oymak2018overparameterized,
  title={Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
 booktitle=ICML,
  year={2019}
}

@INPROCEEDINGS{allen2018convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle=ICML,
  year={2019}
}



@INPROCEEDINGS{zou2019improved,
  title={An Improved Analysis of Training Over-parameterized Deep Neural Networks},
  author={Zou, Difan and Gu, Quanquan},
  booktitle=NIPS,
  year={2019}
}

@INPROCEEDINGS{ji2019polylogarithmic,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow {ReLU} networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle=ICLR,
  year={2020}
}

@INPROCEEDINGS{chen2019much,
  title={How Much Over-parameterization Is Sufficient to Learn Deep {ReLU} Networks?},
  author={Chen, Zixiang and Cao, Yuan and Zou, Difan and Gu, Quanquan},
  booktitle=ICLR,
  year={2021}
}

@INPROCEEDINGS{oymak2020noise,
  title={Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks},
  author={Mingchen Li and Mahdi Soltanolkotabi and Samet Oymak},
 booktitle=AISTATS,
  year={2020}
}

@INPROCEEDINGS{Eftekhari2020linear,
  title={Training Linear Neural Networks: Non-Local Convergence and Complexity Results},
  author={Armin Eftekhari},
 booktitle=ICML,
  year={2020}
}

@INPROCEEDINGS{Su2019approx,
  title={On Learning Over-parameterized Neural Networks:
A Functional Approximation Perspective},
  author={Lili Su and Pengkun Yang},
 booktitle=NIPS,
  year={2019}
}

@INPROCEEDINGS{Daniely2020,
  title={Neural Networks Learning and Memorization with (almost) no Over-Parameterization},
  author={Amit Daniely},
 booktitle=NIPS,
  year={2020}
}


@INPROCEEDINGS{MFT2019,
  title={Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
  author={Song Mei and Theodor Misiakiewicz and Andrea Montanari},
 booktitle=COLT,
  year={2019}
}

@INPROCEEDINGS{Pyr2020,
  title={Global Convergence of Deep Networks with One Wide Layer Followed by Pyramidal Topology},
  author={Quynh Nguyen and Marco Mondelli},
 booktitle=NIPS,
  year={2020}
}

@INPROCEEDINGS{MFT2020,
  title={A Mean Field Analysis Of Deep ResNet And Beyond: Towards Provably Optimization Via Overparameterization From Depth},
  author={Yiping Lu and Chao Ma and Yulong Lu and Jianfeng Lu and Lexing Ying},
 booktitle=ICML,
  year={2020}
}

@ARTICLE{bolte2017error,
	title={From error bounds to the complexity of first-order descent methods for convex functions},
	author={Bolte, J{\'e}r{\^o}me and Nguyen, Trong Phong and Peypouquet, Juan and Suter, Bruce W},
	journal=MP,
	volume={165},
	pages={471--507},
	year={2017},}
	
	
@ARTICLE{chi2018nonconvex,
  title={Nonconvex optimization meets low-rank matrix factorization: An overview},
  author={Chi, Yuejie and Lu, Yue M and Chen, Yuxin},
  journal=TSP,
	volume={67},
	pages={5239--5269},
	year={2019},}
		

@BOOK{nocedal2006numerical,
author = {Nocedal, J. and Wright, S.},
title = {Numerical Optimization},
year = {2006},
publisher = {Springer New York},
}
		
		
@article{hendrycks2016gaussian,
  title={Gaussian error linear units ({GELU})},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415v4},
  year={2020}
}

@INPROCEEDINGS{Dugas2000,
title = {Incorporating Second-Order Functional Knowledge for Better Option Pricing},
author = {Charles Dugas and Bengio, Yoshua and Fran\c{c}ois B\'{e}lisle and Claude Nadeau and Ren\'{e} Garcia},
booktitle = NIPS,
year = {2000},}

@article{Xu2015,
       author = {{Xu}, Bing and {Wang}, Naiyan and {Chen}, Tianqi and {Li}, Mu},
        title = {Empirical Evaluation of Rectified Activations in Convolutional Network},
  journal={arXiv preprint arXiv:1505.00853v2},
  year={2020}
}

@INPROCEEDINGS{clevert2015fast,
  title={Fast and Accurate Deep Network Learning by Exponential Linear Units ({ELUs})},
  author={Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  booktitle = ICLR,
  year = {2016},}

@INPROCEEDINGS{gulrajani2017improved,
  title={Improved training of {Wasserstein} {GANs}},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  booktitle=NIPS,
  year={2017}
}

@INPROCEEDINGS{Kumar2017,
title = {Semi-supervised Learning with {GANs}: Manifold
Invariance with Improved Inference},
author = {Kumar, Abhishek and Sattigeri, Prasanna and Fletcher, Tom},
booktitle = NIPS,
year = {2017},}

@INPROCEEDINGS{kim2018memorization,
title={Memorization Precedes Generation: Learning Unsupervised {GAN}s with Memory Networks},
author={Youngjin Kim and Minjung Kim and Gunhee Kim},
booktitle=ICLR,
year={2018},}

@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805v2},
  year={2019}
}

@INPROCEEDINGS{du2017gradient,
  title={Gradient descent can take exponential time to escape saddle points},
  author={Du, Simon S. and Jin, Chi and Lee, Jason D and Jordan, Michael I. and Singh, Aarti and P\'oczos, Barnab\'as},
  booktitle=NIPS,
  year={2017}
}

@BOOK{olver2010nist,
  title={NIST Handbook of Mathematical Functions Paperback and {CD-ROM}},
  author={Olver, Frank W. J.   and  Lozier, Daniel W.  and Boisvert,  Ronald F.  and Clark, Charles W. },
  year={2010},
  publisher={Cambridge University Press}
}

@BOOK{vershynin2010introduction,
  title={Introduction to the Non-asymptotic Analysis of Random Matrices},
  author={Vershynin, Roman},
  year={2012},
  publisher={Cambridge University Press}
}

@ARTICLE{Hoeffding,
  title={Probability inequalities for sums of bounded random variables},
  author={Hoeffding, Wassily},
  journal=JASA,
	volume={58},
	pages={13--30},
	year={1963},}

@BOOK{LeCuninit,
  title={Efficient {BackProp.} In Neural networks: Tricks of the Trade},
  author={Yann A. LeCun and L\'{e}on Bottou and Genevieve B. Orr and Klaus-Robert M\"{u}ller},
  year={2012},
  publisher={Springer}
}

@INPROCEEDINGS{Heinit,
  title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on {ImageNet} Classification},
  author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  booktitle=CVPR,
  year={2015}
}
	
@article{MNIST,
title={Gradient-based learning applied to document recognition},
author={Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
journal={Proceedings of the IEEE},
year={1998},
volume={86},
pages={2278--2324},}

@INPROCEEDINGS{CIFAR10,
author={A. Krizhevsky},
note={Technical report,
University of Toronto, 2009},
title={Learning multiple layers of features from tiny images},}
%year={2009},}


@INPROCEEDINGS{torch,
author={A. Paszke and S. Gross and F. Massa and A. Lerer and J. Bradbury and G. Chanan and T. Killeen and Z. Lin and N. Gimelshein and L. Antiga and A. Desmaison and A. Kopf and E. Yang and Z. DeVito and M. Raison and A. Tejani and S. Chilamkurthy and B. Steiner and L. Fang and J. Bai and S. Chintala},
booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
title={{PyTorch}: An Imperative Style, High-Performance Deep Learning Library},
year={2019},}

@article{mei2018mean,
  title={A mean field view of the landscape of two-layer neural networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={33},
  pages={E7665--E7671},
  year={2018},
  publisher={National Acad Sciences}
}

@INPROCEEDINGS{FabianADMM,
  title={Fast and Provable {ADMM} for
Learning with Generative Priors},
  author={Fabian Latorre and Armin Eftekhari and Volkan Cevher},
  booktitle=NIPS,
  year={2019}
}

@INPROCEEDINGS{liu2020loss,
  title={Loss landscapes and optimization in over-parameterized non-linear systems and neural networks},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  booktitle=NIPS,
  year={2020}
}


@INPROCEEDINGS{liu2020linearity,
  title={On the linearity of large non-linear models: when and why the tangent kernel is constant},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  booktitle=NIPS,
  year={2020}
}

@INPROCEEDINGS{Arora2020Harnessing,
title={Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks},
author={Sanjeev Arora and Simon S. Du and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang and Dingli Yu},
booktitle=ICLR,
year={2020}
}

@INPROCEEDINGS{lee2020finite,
  title={Finite Versus Infinite Neural Networks: an Empirical Study},
  author={Jaehoon Lee and Samuel S. Schoenholz and Jeffrey Pennington and Ben Adlam and Lechao Xiao and Roman Novak and Jascha Sohl-Dickstein},
  booktitle=NIPS,
  year={2020}
}

@article{luo2021phase,
  title={Phase diagram for two-layer {ReLU} neural networks at infinite-width limit},
  author={Luo, Tao and Xu, Zhi-Qin John and Ma, Zheng and Zhang, Yaoyu},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={71},
  pages={1--47},
  year={2021}
}