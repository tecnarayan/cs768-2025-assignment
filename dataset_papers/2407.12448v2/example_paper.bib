Replay Buffer
@inproceedings{per,
  author    = {Tom Schaul and
               John Quan and
               Ioannis Antonoglou and
               David Silver},
  title     = {Prioritized Experience Replay},
  booktitle = {Proceedings of the 4th International Conference on Learning Representations (ICLR'16)},
  address   = {San Juan, Puerto Rico},
  year      = {2016},
}

@article{deeper,
  author    = {Shangtong Zhang and
               Richard S. Sutton},
  title     = {A Deeper Look at Experience Replay},
  journal   = {CoRR},
  volume    = {abs/1712.01275},
  year      = {2017}
}

@inproceedings{revisit,
  author    = {William Fedus and
               Prajit Ramachandran and
               Rishabh Agarwal and
               Yoshua Bengio and
               Hugo Larochelle and
               Mark Rowland and
               Will Dabney},
  title     = {Revisiting Fundamentals of Experience Replay},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML'20)},
  address   = {Virtual Event},
  year = {2020}
}

@article{sequence,
  author    = {Marc Brittain and
               Joshua R. Bertram and
               Xuxi Yang and
               Peng Wei},
  title     = {Prioritized Sequence Experience Replay},
  journal   = {CoRR},
  volume    = {abs/1905.12726},
  year      = {2019}
}

@inproceedings{bottleneck,
  author    = {Justin Fu and
               Aviral Kumar and
               Matthew Soh and
               Sergey Levine},
  title     = {Diagnosing Bottlenecks in Deep Q-learning Algorithms},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML'19)},
  address   = {Long Beach, USA},
  year      = {2019},
}

@inproceedings{equivalence,
  author    = {Scott Fujimoto and
               David Meger and
               Doina Precup},
  title     = {An Equivalence between Loss Functions and Non-Uniform Sampling in Experience Replay},
  booktitle = {Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS'20)},
  address  = {Virtual Event},
  year      = {2020},
}

@inproceedings{ere,
  author    = {Che Wang and
               Yanqiu Wu and
               Quan Vuong and
               Keith Ross},
  title     = {Striving for Simplicity and Performance in Off-Policy {DRL:} Output Normalization and Non-Uniform Sampling},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML'20)},
  address   = {Virtual Event},
  year      = {2020}
}


@inproceedings{discor,
  author    = {Aviral Kumar and
               Abhishek Gupta and
               Sergey Levine},
  title     = {DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction},
  booktitle = {Proceedings of 33rd conference on Neural Information Processing Systems (NeurIPS'20)},
  address   = {Virtual Event},
  year      = {2020}
}



@inproceedings{lfiw,
  title     = {Experience replay with likelihood-free importance weights},
  author    = {Samarth Sinha and
               Jiaming Song and
               Animesh Garg and
               Stefano Ermon},
  booktitle = {Learning for Dynamics and Control Conference (L4RC'22)},
  address  = {Stanford, USA},
  year      = {2022}
}

@inproceedings{liu2021regret,
  title     = {Regret Minimization Experience Replay in Off-Policy Reinforcement Learning},
  author    = {Liu, Xu-Hui and Xue, Zhenghai and Pang, Jingcheng and Jiang, Shengyi and Xu, Feng and Yu, Yang},
  booktitle   = {Proceedings of 34th conference on Neural Information Processing Systems  (NeurIPS'21)},
  address = {Virtual Event},
  year      = {2021}
}

@inproceedings{topological,
  title     = {Topological Experience Replay},
  author    = {Hong, Zhang-Wei and Chen, Tao and Lin, Yen-Chen and Pajarinen, Joni and Agrawal, Pulkit},
  booktitle = {International Conference on Learning Representations (ICLR'21)},
  address = {Virtual Event},
  year      = {2021}
}

@inproceedings{ebu,
  title     = {Sample-efficient deep reinforcement learning via episodic backward update},
  author    = {Lee, Su Young and Sungik, Choi and Chung, Sae-Young},
  booktitle   = {Proceedings of 32nd conference on Neural Information Processing Systems  (NeurIPS'19)},
  year      = {2019}
}

@inproceedings{predictive,
  title     = {Predictive PER: balancing priority and diversity towards stable deep reinforcement learning},
  author    = {Lee, Sanghwa and Lee, Jaeyoung and Hasuo, Ichiro},
  booktitle = {2021 International Joint Conference on Neural Networks (IJCNN'21)},
  pages     = {1--10},
  year      = {2021},
  organization={IEEE}
}

Representation learning

@article{nce,
  author       = {Michael Gutmann and
                  Aapo Hyv{\"{a}}rinen},
  title        = {Noise-Contrastive Estimation of Unnormalized Statistical Models, with
                  Applications to Natural Image Statistics},
  journal      = {Journal of Machine Learning Research},
  volume       = {13},
  pages        = {307--361},
  year         = {2012}
}

@article{infonce,
  author       = {A{\"{a}}ron van den Oord and
                  Yazhe Li and
                  Oriol Vinyals},
  title        = {Representation Learning with Contrastive Predictive Coding},
  journal      = {CoRR},
  volume       = {abs/1807.03748},
  year         = {2018}
}

@inproceedings{kakade,
  author    = {Sham M. Kakade and
               John Langford},
  title     = {Approximately Optimal Approximate Reinforcement Learning},
  booktitle = {Proceedings of the 19th International Conference on Machine Learning (ICML'02)},
  address   = {Sydney, Australia},
  year      = {2002},
}


@book{sutton,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

RL algorithm
@inproceedings{trpo,
  author    = {John Schulman and
               Sergey Levine and
               Pieter Abbeel and
               Michael I. Jordan and
               Philipp Moritz},
  title     = {Trust Region Policy Optimization},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML'15)},
  pages     = {1889--1897},
  address   = {Lille, France},
  year      = {2015}

}

@article{ppo,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017}
}

@inproceedings{a3c,
  author    = {Volodymyr Mnih and
               Adri{\`{a}} Puigdom{\`{e}}nech Badia and
               Mehdi Mirza and
               Alex Graves and
               Timothy P. Lillicrap and
               Tim Harley and
               David Silver and
               Koray Kavukcuoglu},
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  booktitle = {Proceedings of the 33nd International Conference on Machine Learning (ICML'16)},
  pages     = {1928--1937},
  year      = {2016}
}

@inproceedings{c51,
  author    = {Marc G. Bellemare and
               Will Dabney and
               R{\'{e}}mi Munos},
  title     = {A Distributional Perspective on Reinforcement Learning},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML'17)},
  address   = {Sydney, Australia},
  year      = {2017}
}

@inproceedings{dpg,
  author    = {David Silver and
               Guy Lever and
               Nicolas Heess and
               Thomas Degris and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {Deterministic Policy Gradient Algorithms},
  booktitle = {Proceedings of the 31th International Conference on Machine Learning (ICML'14)},
  address   = {Beijing, China},

  year      = {2014}
}

@inproceedings{sac,
  author    = {Tuomas Haarnoja and
               Aurick Zhou and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML'18)},
  address   = {Stockholmsm{\"{a}}ssan, Sweden},
  year      = {2018},
}

@article{dqn,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Andrei A. Rusu and
               Joel Veness and
               Marc G. Bellemare and
               Alex Graves and
               Martin A. Riedmiller and
               Andreas Fidjeland and
               Georg Ostrovski and
               Stig Petersen and
               Charles Beattie and
               Amir Sadik and
               Ioannis Antonoglou and
               Helen King and
               Dharshan Kumaran and
               Daan Wierstra and
               Shane Legg and
               Demis Hassabis},
  title     = {Human-level control through deep reinforcement learning},
  journal   = {Nature},
  volume    = {518},
  number    = {7540},
  pages     = {529--533},
  year      = {2015}
}

@inproceedings{td3,
  author    = {Scott Fujimoto and
               Herke van Hoof and
               David Meger},
  title     = {Addressing Function Approximation Error in Actor-Critic Methods},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML'18)},
  address   = {Stockholmsm{\"{a}}ssan, Sweden},
  year      = {2018}
}

@inproceedings{rainbow,
  author    = {Matteo Hessel and
               Joseph Modayil and
               Hado van Hasselt and
               Tom Schaul and
               Georg Ostrovski and
               Will Dabney and
               Dan Horgan and
               Bilal Piot and
               Mohammad Gheshlaghi Azar and
               David Silver},
  title     = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
  booktitle = {Proceedings of the 32nd Conference on Artificial Intelligence (AAAI'18)},
  address   = {New Orleans, LA},
  year      = {2018}
}

@inproceedings{ddpg,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  booktitle={Proceedings of the 4th International Conference on Learning Representations (ICLR'16)},
  address   = {San Juan, Puerto Rico},
  year={2016}
}

@article{redq,
  author    = {Xinyue Chen and
               Che Wang and
               Zijian Zhou and
               Keith Ross},
  title     = {Randomized Ensembled Double Q-Learning: Learning Fast Without a Model},
  journal   = {CoRR},
  volume    = {abs/2101.05982},
  year      = {2021}
}

@inproceedings{sunrise,
  author    = {Kimin Lee and
               Michael Laskin and
               Aravind Srinivas and
               Pieter Abbeel},
  title     = {{SUNRISE:} {A} Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning (ICML'21)},
  address   = {Virtual Event},
  year      = {2021}
}



offline RL
@inproceedings{bcq,
  author    = {Scott Fujimoto and
               David Meger and
               Doina Precup},
  title     = {Off-Policy Deep Reinforcement Learning without Exploration},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML'19)},
  address   = {Long Beach, USA},
  year      = {2019}
}

@article{hybrid,
  author       = {Xue{-}Kun Jin and
                  Xu{-}Hui Liu and
                  Shengyi Jiang and
                  Yang Yu},
  title        = {Hybrid Value Estimation for Off-policy Evaluation and Offline Reinforcement
                  Learning},
  journal      = {CoRR},
  volume       = {abs/2206.02000},
  year         = {2022}
}

@inproceedings{CQL,
  author    = {Aviral Kumar and
               Aurick Zhou and
               George Tucker and
               Sergey Levine},
  title     = {Conservative Q-Learning for Offline Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems 33 (NeurIPS'20)},
  address   = {Virtual Event},
  year      = {2020}
}

@inproceedings{td3bc,
  author    = {Scott Fujimoto and
               Shixiang Shane Gu},
  title     = {A Minimalist Approach to Offline Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems 34 (NeurIPS'21)},
  address   = {Virtual Event},
  year      = {2021},
}

@inproceedings{BEAR,
  author    = {Aviral Kumar and
               Justin Fu and
               Matthew Soh and
               George Tucker and
               Sergey Levine},
  title     = {Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction},
  booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS'19)},
  address   = {Vancouver, Canada},
  year      = {2019}

}

@article{d4rl,
  author    = {Justin Fu and
               Aviral Kumar and
               Ofir Nachum and
               George Tucker and
               Sergey Levine},
  title     = {{D4RL:} Datasets for Deep Data-Driven Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/2004.07219},
  year      = {2020},
}


other
@inproceedings{rs1,
  author    = {Xiting Wang and
               Yiru Chen and
               Jie Yang and
               Le Wu and
               Zhengtao Wu and
               Xing Xie},
  title     = {A Reinforcement Learning Framework for Explainable Recommendation},
  booktitle = {Proceedings of the 18th International Conference on Data Mining (ICDM'18)},
  address   = {Singapore},
  year      = {2018}
}

@inproceedings{rs2,
  author    = {Xiangyu Zhao and
               Liang Zhang and
               Zhuoye Ding and
               Long Xia and
               Jiliang Tang and
               Dawei Yin},
  title     = {Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning},
  booktitle = {Proceedings of the 24th International Conference on Knowledge Discovery {\&} Data Mining (KDD'18)},
  address   = {London, UK},
  year      = {2018}
}

@inproceedings{rc1,
  author    = {Xue Bin Peng and
               Erwin Coumans and
               Tingnan Zhang and
               Tsang{-}Wei Edward Lee and
               Jie Tan and
               Sergey Levine},
  title     = {Learning Agile Robotic Locomotion Skills by Imitating Animals},
  booktitle = {Proceedings of the 14th Robotics: Science and Systems (RSS'20)},
  address   = {Virtual Event},
  year      = {2020}
}

@inproceedings{fqi,
  author       = {Jinglin Chen and
                  Nan Jiang},
  title        = {Information-Theoretic Considerations in Batch Reinforcement Learning},
  booktitle    = {Proceedings of the 36th International Conference on Machine Learning (ICML'19)},
  address      = {Long Beach, USA},
  year         = {2019}
}

offline to online
@inproceedings{balanced,
  author       = {Seunghyun Lee and
                  Younggyo Seo and
                  Kimin Lee and
                  Pieter Abbeel and
                  Jinwoo Shin},
  title        = {Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic
                  Q-Ensemble},
  booktitle    = {Conference on Robot Learning (CoRL'21)},
  address      = {London, UK},
  year         = {2021}
}


@article{awac,
  author       = {Ashvin Nair and
                  Murtaza Dalal and
                  Abhishek Gupta and
                  Sergey Levine},
  title        = {Accelerating Online Reinforcement Learning with Offline Datasets},
  journal      = {CoRR},
  volume       = {abs/2006.09359},
  year         = {2020}
}



@article{calql,
  author       = {Mitsuhiko Nakamoto and
                  Yuexiang Zhai and
                  Anikait Singh and
                  Max Sobol Mark and
                  Yi Ma and
                  Chelsea Finn and
                  Aviral Kumar and
                  Sergey Levine},
  title        = {Cal-QL: Calibrated Offline {RL} Pre-Training for Efficient Online
                  Fine-Tuning},
  journal      = {CoRR},
  volume       = {abs/2303.05479},
  year         = {2023}
}

@inproceedings{iql,
  author       = {Ilya Kostrikov and
                  Ashvin Nair and
                  Sergey Levine},
  title        = {Offline Reinforcement Learning with Implicit Q-Learning},
  booktitle    = {Proceedings of the 10-th International Conference on Learning Representations (ICLR'22)},
  address    = {Virtual Event},
  year         = {2022}
}

@inproceedings{pex,
  author       = {Haichao Zhang and
                  Wei Xu and
                  Haonan Yu},
  title        = {Policy Expansion for Bridging Offline-to-Online Reinforcement Learning},
  booktitle    = {The Eleventh International Conference on Learning Representations(ICLR'23)},
  address = {Kigali, Rwanda},
  year         = {2023}
}

@inproceedings{efficientonline,
  author       = {Philip J. Ball and
                  Laura M. Smith and
                  Ilya Kostrikov and
                  Sergey Levine},
  title        = {Efficient Online Reinforcement Learning with Offline Data},
  booktitle    = {International Conference on Machine Learning (ICML'23)},
  year        = {2023},
  address     = {Honolulu, USA}
}

@article{adaptivebc,
  author       = {Yi Zhao and
                  Rinu Boney and
                  Alexander Ilin and
                  Juho Kannala and
                  Joni Pajarinen},
  title        = {Adaptive Behavior Cloning Regularization for Stable Offline-to-Online
                  Reinforcement Learning},
  journal      = {CoRR},
  volume       = {abs/2210.13846},
  year         = {2022}
}

@article{oncefamily,
  author       = {Shenzhi Wang and
                  Qisen Yang and
                  Jiawei Gao and
                  Matthieu Gaetan Lin and
                  Hao Chen and
                  Liwei Wu and
                  Ning Jia and
                  Shiji Song and
                  Gao Huang},
  title        = {Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online
                  Reinforcement Learning},
  journal      = {CoRR},
  volume       = {abs/2310.17966},
  year         = {2023}
}


@article{proto,
  author       = {Jianxiong Li and
                  Xiao Hu and
                  Haoran Xu and
                  Jingjing Liu and
                  Xianyuan Zhan and
                  Ya{-}Qin Zhang},
  title        = {{PROTO:} Iterative Policy Regularized Offline-to-Online Reinforcement
                  Learning},
  journal      = {CoRR},
  volume       = {abs/2305.15669},
  year         = {2023},
}

@inproceedings{moto,
  title={MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot Learning},
  author={Rafailov, Rafael and Hatch, Kyle Beltran and Kolev, Victor and Martin, John D and Phielipp, Mariano and Finn, Chelsea},
  booktitle={Conference on Robot Learning (CoRL'23)},
  pages={3654--3671},
  year={2023}
}

@inproceedings{deepqdemo,
  title={Deep q-learning from demonstrations},
  author={Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Osband, Ian and others},
  booktitle={Proceedings of the 32nd AAAI conference on artificial intelligence (AAAI'18)},
  year={2018},
  address = {New Orleans, USA}
}

@article{learnfromdemo,
  title={Learning from demonstration},
  author={Schaal, Stefan},
  journal={Advances in neural information processing systems},
  volume={9},
  year={1996}
}

@inproceedings{hybridrl,
    author = {Song, Yuda and Zhou, Yifei and Sekhari, Ayush and Bagnell, J Andrew and Krishnamurthy, Akshay and Sun, Wen},
    title = {Hybrid rl: Using both offline and online data can make rl efficient},
    booktitle = {The 11th International Conference on Learning Representations (ICLR'22)},
    address = {Virtual Event},
    year = {2022}}

@article{leveragingdemo,
  title={Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards},
  author={Vecerik, Mel and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Roth{\"o}rl, Thomas and Lampe, Thomas and Riedmiller, Martin},
  journal   = {CoRR},
  volume    = {abs/1707.08817},
  year={2017}
}



model based 
@inproceedings{MBPO,
  author    = {Michael Janner and
               Justin Fu and
               Marvin Zhang and
               Sergey Levine},
  title     = {When to Trust Your Model: {M}odel-Based Policy Optimization},
  booktitle = {Proceedings of the 32nd Neural Information Processing Systems (NeurIPS'19)},
  address   = {Vancouver, Canada},
  year      = {2019}
}

@inproceedings{mopo,
  title={Mopo: Model-based offline policy optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James Y and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  booktitle   = {Proceedings of 33rd conference on Neural Information Processing Systems  (NeurIPS'20)},
  address = {Virtual Event},
  year      = {2020}
}

@inproceedings{mobile,
  title={Model-Bellman inconsistency for model-based offline reinforcement learning},
  author={Sun, Yihao and Zhang, Jiaji and Jia, Chengxing and Lin, Haoxin and Ye, Junyin and Yu, Yang},
  booktitle={International Conference on Machine Learning},
  pages={33177--33194},
  year={2023},
  organization={PMLR}
}

@inproceedings{combo,
  title={Combo: Conservative offline model-based policy optimization},
  author={Yu, Tianhe and Kumar, Aviral and Rafailov, Rafael and Rajeswaran, Aravind and Levine, Sergey and Finn, Chelsea},
  booktitle   = {Proceedings of 34th conference on Neural Information Processing Systems  (NeurIPS'21)},
  address = {Virtual Event},
  year      = {2021}
}

@article{rambo,
  title={Rambo-rl: Robust adversarial model-based offline reinforcement learning},
  author={Rigter, Marc and Lacerda, Bruno and Hawes, Nick},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={16082--16097},
  year={2022}
}

@inproceedings{morel,
  author    = {Rahul Kidambi and
               Aravind Rajeswaran and
               Praneeth Netrapalli and
               Thorsten Joachims},
  title     = {MOReL: Model-Based Offline Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems 33 (NeurIPS'20)},
  address   = {virtual event},
  year      = {2020},
}


diffusion
@article{conditiondiffusioncontrol,
  author       = {Bogdan Mazoure and
                  Walter Talbott and
                  Miguel {\'{A}}ngel Bautista and
                  R. Devon Hjelm and
                  Alexander Toshev and
                  Joshua M. Susskind},
  title        = {Value function estimation using conditional diffusion models for control},
  journal      = {CoRR},
  volume       = {abs/2306.07290},
  year         = {2023},
}
@article{policyguidedtrajdiff,
  author       = {Marc Rigter and
                  Jun Yamada and
                  Ingmar Posner},
  title        = {World Models via Policy-Guided Trajectory Diffusion},
  journal      = {CoRR},
  volume       = {abs/2312.08533},
  year         = {2023},
}

@article{imagecondiff,
  author       = {Andreas Lugmayr and
                  Martin Danelljan and
                  Andr{\'{e}}s Romero and
                  Fisher Yu and
                  Radu Timofte and
                  Luc Van Gool},
  title        = {RePaint: Inpainting using Denoising Diffusion Probabilistic Models},
  journal      = {CoRR},
  volume       = {abs/2201.09865},
  year         = {2022},
}

@inproceedings{conddiff,
  author       = {Jooyoung Choi and
                  Sungwon Kim and
                  Yonghyun Jeong and
                  Youngjune Gwon and
                  Sungroh Yoon},
  title        = {{ILVR:} Conditioning Method for Denoising Diffusion Probabilistic
                  Models},
  booktitle    = {Proceedings of the 2021 International Conference on Computer Vision (ICCV'21)},
  address     = {Montreal, Canada},
  year         = {2021},
}


@inproceedings{elucidate,
  author       = {Tero Karras and
                  Miika Aittala and
                  Timo Aila and
                  Samuli Laine},
  title        = {Elucidating the Design Space of Diffusion-Based Generative Models},
  booktitle    = {Proceedings of the 36th Neural Information Processing Systems (NeurIPS'22)},
  address      = {LA, USA},
  year = {2022}
}

@inproceedings{diff,
  author       = {Jonathan Ho and
                  Ajay Jain and
                  Pieter Abbeel},
  title        = {Denoising Diffusion Probabilistic Models},
  booktitle    = {Proceedings of the 33rd Neural Information Processing Systems (NeurIPS'20)},
  year         = {2020},
  address      = {Virtual Event}
}

@inproceedings{contrastive,
  author       = {Cheng Lu and
                  Huayu Chen and
                  Jianfei Chen and
                  Hang Su and
                  Chongxuan Li and
                  Jun Zhu},
  title        = {Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling
                  in Offline Reinforcement Learning},
  booktitle    = {Proceedings of the 40th International Conference on Machine Learning(ICML'23)},
  address      = {Honolulu, HI},
  volume       = {202},
  pages        = {22825--22855},
  year         = {2023}
}

@inproceedings{SahariaCSLWDGLA22,
  author       = {Chitwan Saharia and
                  William Chan and
                  Saurabh Saxena and
                  Lala Li and
                  Jay Whang and
                  Emily L. Denton and
                  Seyed Kamyar Seyed Ghasemipour and
                  Raphael Gontijo Lopes and
                  Burcu Karagol Ayan and
                  Tim Salimans and
                  Jonathan Ho and
                  David J. Fleet and
                  Mohammad Norouzi},
  title        = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  booktitle    = {Proceedings of the 35th Neural Information Processing Systems (NeurIPS'22)},
  address      = {New Orleans, USA},
  year         = {2022},
}

@inproceedings{NicholDRSMMSC22,
  author       = {Alexander Quinn Nichol and
                  Prafulla Dhariwal and
                  Aditya Ramesh and
                  Pranav Shyam and
                  Pamela Mishkin and
                  Bob McGrew and
                  Ilya Sutskever and
                  Mark Chen},
  title        = {{GLIDE:} Towards Photorealistic Image Generation and Editing with
                  Text-Guided Diffusion Models},
  booktitle    = {Proceedings of the 39th International Conference on Machine Learning (ICML'22)},
  address      = {Baltimore, USA},
  year         = {2022},
}

@inproceedings{NicholD21,
  author       = {Alexander Quinn Nichol and
                  Prafulla Dhariwal},
  editor       = {Marina Meila and
                  Tong Zhang},
  title        = {Improved Denoising Diffusion Probabilistic Models},
  booktitle    = {Proceedings of the 38th International Conference on Machine Learning (ICML'21)},
  address      = {Virtual Event},
  year         = {2021},
}

@inproceedings{DecisionDiffuser,
  author       = {Anurag Ajay and
                  Yilun Du and
                  Abhi Gupta and
                  Joshua B. Tenenbaum and
                  Tommi S. Jaakkola and
                  Pulkit Agrawal},
  title        = {Is Conditional Generative Modeling all you need for Decision Making?},
  booktitle    = {The Eleventh International Conference on Learning Representations (ICLR'23)},
  address      = {Kigali, Rwanda},
  year         = {2023},
}

@inproceedings{DiffusionQL,
  author       = {Zhendong Wang and
                  Jonathan J. Hunt and
                  Mingyuan Zhou},
  title        = {Diffusion Policies as an Expressive Policy Class for Offline Reinforcement
                  Learning},
  booktitle    = {The Eleventh International Conference on Learning Representations (ICLR'23)},
  address      = {Kigali, Rwanda},
  year         = {2023},
}

@article{synther,
  author       = {Cong Lu and
                  Philip J. Ball and
                  Jack Parker{-}Holder},
  title        = {Synthetic Experience Replay},
  journal      = {CoRR},
  volume       = {abs/2303.06614},
  year         = {2023},
}

@inproceedings{diffuser,
  title={Planning with Diffusion for Flexible Behavior Synthesis},
  author={Janner, Michael and Du, Yilun and Tenenbaum, Joshua and Levine, Sergey},
  booktitle={Proceedings of the 39th International Conference on Machine Learning (ICML'22)},
  address={Baltimore, USA},
  year={2022},
}

@article{ddim,
  author       = {Jiaming Song and
                  Chenlin Meng and
                  Stefano Ermon},
  title        = {Denoising Diffusion Implicit Models},
  journal      = {CoRR},
  volume       = {abs/2010.02502},
  year         = {2020},
}













@inproceedings{rob1,
  author       = {Yuke Zhu and
                  Ziyu Wang and
                  Josh Merel and
                  Andrei A. Rusu and
                  Tom Erez and
                  Serkan Cabi and
                  Saran Tunyasuvunakool and
                  J{\'{a}}nos Kram{\'{a}}r and
                  Raia Hadsell and
                  Nando de Freitas and
                  Nicolas Heess},
  title        = {Reinforcement and Imitation Learning for Diverse Visuomotor Skills},
  booktitle    = {Proceedings of the 12th Robotics: Science and Systems (RSS'18)},
  address      = {PA, USA}, 
  year         = {2018}
}

@inproceedings{rob2,
  author       = {Aravind Rajeswaran and
                  Vikash Kumar and
                  Abhishek Gupta and
                  Giulia Vezzani and
                  John Schulman and
                  Emanuel Todorov and
                  Sergey Levine},
  title        = {Learning Complex Dexterous Manipulation with Deep Reinforcement Learning
                  and Demonstrations},
  booktitle    = {Proceedings of the 12th Robotics: Science and Systems (RSS'18)},
  address      = {PA, USA},
  year         = {2018}
}

@inproceedings{rob3,
  author       = {Henry Zhu and
                  Abhishek Gupta and
                  Aravind Rajeswaran and
                  Sergey Levine and
                  Vikash Kumar},
  title        = {Dexterous Manipulation with Deep Reinforcement Learning: Efficient,
                  General, and Low-Cost},
  booktitle    = {Proceedings of the 36th International Conference on Robotics and Automation (ICRA'19)},
  address      = {Montreal, Canada},
  year         = {2019}
}

@inproceedings{opal,
  author       = {Anurag Ajay and
                  Aviral Kumar and
                  Pulkit Agrawal and
                  Sergey Levine and
                  Ofir Nachum},
  title        = {{OPAL:} Offline Primitive Discovery for Accelerating Offline Reinforcement
                  Learning},
  booktitle    = {Proceedings of the 9th International Conference on Learning Representations (ICLR'21)},
  address      = {Virtual Event},
  year         = {2021}
}

@inproceedings{relay,
  author       = {Abhishek Gupta and
                  Vikash Kumar and
                  Corey Lynch and
                  Sergey Levine and
                  Karol Hausman},
  title        = {Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and
                  Reinforcement Learning},
  booktitle    = {Proceedings of the 3rd Annual Conference on Robot Learning (CoRL'19)},
  address      = {Osaka, Japan},
  year         = {2019}
}

@inproceedings{dualdice,
  author       = {Ofir Nachum and
                  Yinlam Chow and
                  Bo Dai and
                  Lihong Li},
  title        = {DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution
                  Corrections},
  booktitle    = {Proceedings of the 32nd Neural Information Processing Systems (NeurIPS'19)},
  address      = {Vancouver, Canada},
  pages        = {2315--2325},
  year         = {2019}
}

@article{instructed_diffuser,
  author       = {Jifeng Hu and
                  Yanchao Sun and
                  Sili Huang and
                  Siyuan Guo and
                  Hechang Chen and
                  Li Shen and
                  Lichao Sun and
                  Yi Chang and
                  Dacheng Tao},
  title        = {Instructed Diffuser with Temporal Condition Guidance for Offline Reinforcement
                  Learning},
  journal      = {CoRR},
  volume       = {abs/2306.04875},
  year         = {2023},
}

@inproceedings{
corl,
  title={{CORL}: Research-oriented Deep Offline Reinforcement Learning Library},
  author={Denis Tarasov and Alexander Nikulin and Dmitry Akimov and Vladislav Kurenkov and Sergey Kolesnikov},
  booktitle={3rd Offline RL Workshop: Offline RL as a ''Launchpad''},
  year={2022}
}

@incollection{vc,
  author       = {Ulrike von Luxburg and
                  Bernhard Sch{\"{o}}lkopf},
  title        = {Statistical Learning Theory: Models, Concepts, and Results},
  booktitle    = {Inductive Logic},
  volume       = {10},
  pages        = {651--706},
  year         = {2011},
  publisher={Elsevier}
}

@article{nearly,
  title={Nearly Minimax Optimal Adversarial Imitation Learning with Known and Unknown Transitions},
  author={Xu, Tian and Li, Ziniu and Yu, Yang},
  journal={CoRR abs/2106.10424},
  year={2021}
}

@inproceedings{sfbc,
  author       = {Huayu Chen and
                  Cheng Lu and
                  Chengyang Ying and
                  Hang Su and
                  Jun Zhu},
  title        = {Offline Reinforcement Learning via High-Fidelity Generative Behavior
                  Modeling},
  booktitle    = {The 11th International Conference on Learning Representations (ICLR'23)},
  address      = {Kigali, Rwanda},
  year         = {2023}
}

@inproceedings{gan,
  author       = {Ian J. Goodfellow and
                  Jean Pouget{-}Abadie and
                  Mehdi Mirza and
                  Bing Xu and
                  David Warde{-}Farley and
                  Sherjil Ozair and
                  Aaron C. Courville and
                  Yoshua Bengio},
  title        = {Generative Adversarial Nets},
  booktitle    = {Advances in Neural Information Processing Systems 27 (NeurIPS'24)},
  address      = {Montreal, Canada},
  pages        = {2672--2680},
  year         = {2014}
}

@article{dvf,
  author       = {Bogdan Mazoure and
                  Walter Talbott and
                  Miguel {\'{A}}ngel Bautista and
                  R. Devon Hjelm and
                  Alexander Toshev and
                  Joshua M. Susskind},
  title        = {Value function estimation using conditional diffusion models for control},
  journal      = {CoRR},
  volume       = {abs/2306.07290},
  year         = {2023}
}

@article{diffusion_trajectory,
  author       = {Marc Rigter and
                  Jun Yamada and
                  Ingmar Posner},
  title        = {World Models via Policy-Guided Trajectory Diffusion},
  journal      = {CoRR},
  volume       = {abs/2312.08533},
  year         = {2023}
}

@inproceedings{foda,
  author       = {Ruifeng Chen and
                  Xu{-}Hui Liu and
                  Tian{-}Shuo Liu and
                  Shengyi Jiang and
                  Feng Xu and
                  Yang Yu},
  title        = {Foresight Distribution Adjustment for Off-policy Reinforcement Learning},
  booktitle    = {Proceedings of the 23rd International Conference on Autonomous Agents
                  and Multiagent Systems (AAMAS'24)},
  pages        = {317--325},
  year         = {2024},
  address   = {Auckland, New Zealand}
}

@inproceedings{ada,
  author       = {Chengxing Jia and
                  Fuxiang Zhang and
                  Yi{-}Chen Li and
                  Chenxiao Gao and
                  Xu{-}Hui Liu and
                  Lei Yuan and
                  Zongzhang Zhang and
                  Yang Yu},
  title        = {Disentangling Policy from Offline Task Representation Learning via
                  Adversarial Data Augmentation},
  booktitle    = {Proceedings of the 23rd International Conference on Autonomous Agents
                  and Multiagent Systems (AAMAS'24)},
  pages        = {944--953},
  year         = {2024},
  address   = {Auckland, New Zealand}
}


@inproceedings{xionghui,
  author       = {Xiong{-}Hui Chen and
                  Yang Yu and
                  Zhengmao Zhu and
                  Zhihua Yu and
                  Zhenjun Chen and
                  Chenghe Wang and
                  Yinan Wu and
                  Rong{-}Jun Qin and
                  Hongqiu Wu and
                  Ruijin Ding and
                  Fangsheng Huang},
  title        = {Adversarial Counterfactual Environment Model Learning},
  booktitle    = {Advances in Neural Information Processing Systems 36 (NeurIPS'23)},
  address      = {New Orleans, LA},
  year         = {2023}
}

@article{policyguideddiffusion,
  author       = {Matthew Thomas Jackson and
                  Michael T. Matthews and
                  Cong Lu and
                  Benjamin Ellis and
                  Shimon Whiteson and
                  Jakob N. Foerster},
  title        = {Policy-Guided Diffusion},
  journal      = {CoRR},
  volume       = {abs/2404.06356},
  year         = {2024}
}

@article{Wang_Wang_Zhou_Li_Li_2022, title={Sample-Efficient Reinforcement Learning via Conservative Model-Based Actor-Critic}, volume={36}, abstractNote={Model-based reinforcement learning algorithms, which aim to learn a model of the environment to make decisions, are more sample efficient than their model-free counterparts. The sample efficiency of model-based approaches relies on whether the model can well approximate the environment. However, learning an accurate model is challenging, especially in complex and noisy environments. To tackle this problem, we propose the conservative model-based actor-critic (CMBAC), a novel approach that achieves high sample efficiency without the strong reliance on accurate learned models. Specifically, CMBAC learns multiple estimates of the Q-value function from a set of inaccurate models and uses the average of the bottom-k estimates---a conservative estimate---to optimize the policy. An appealing feature of CMBAC is that the conservative estimates effectively encourage the agent to avoid unreliable “promising actions”---whose values are high in only a small fraction of the models. Experiments demonstrate that CMBAC significantly outperforms state-of-the-art approaches in terms of sample efficiency on several challenging control tasks, and the proposed method is more robust than previous methods in noisy environments.}, number={8}, journal={Proceedings of the AAAI Conference on Artificial Intelligence (AAAI'22)}, author={Wang, Zhihai and Wang, Jie and Zhou, Qi and Li, Bin and Li, Houqiang}, year={2022}, month={Jun.}, pages={8612-8620} }

@article{Ling_Wang_Wang_2024, title={Learning to Stop Cut Generation for Efficient Mixed-Integer Linear Programming}, volume={38},
abstractNote={Cutting planes (cuts) play an important role in solving mixed-integer linear programs (MILPs), as they significantly tighten the dual bounds and improve the solving performance. A key problem for cuts is when to stop cuts generation, which is important for the efficiency of solving MILPs. However, many modern MILP solvers employ hard-coded heuristics to tackle this problem, which tends to neglect underlying patterns among MILPs from certain applications. To address this challenge, we formulate the cuts generation stopping problem as a reinforcement learning problem and propose a novel hybrid graph representation model (HYGRO) to learn effective stopping strategies. An appealing feature of HYGRO is that it can effectively capture both the dynamic and static features of MILPs, enabling dynamic decision-making for the stopping strategies. To the best of our knowledge, HYGRO is the first data-driven method to tackle the cuts generation stopping problem. By integrating our approach with modern solvers, experiments demonstrate that HYGRO significantly improves the efficiency of solving MILPs compared to competitive baselines, achieving up to 31% improvement.}, number={18}, journal={Proceedings of the AAAI Conference on Artificial Intelligence (AAAI'24)}, author={Ling, Haotian and Wang, Zhihai and Wang, Jie}, year={2024}, month={Mar.}, pages={20759-20767} }

@article{wang2024learning,
title={Learning to Cut via Hierarchical Sequence/Set Model for Efficient Mixed-Integer Programming},
author={Jie Wang and Zhihai Wang and Xijun Li and Yufei Kuang and Zhihao Shi and Fangzhou Zhu and Mingxuan Yuan and Jia Zeng and Yongdong Zhang and Feng Wu},
year={2024},
journal      = {CoRR},
volume       = {abs/2008.06319}
}

@inproceedings{
wang2023learning,
title={Learning Cut Selection for Mixed-Integer Linear Programming via Hierarchical Sequence Model},
author={Zhihai Wang and Xijun Li and Jie Wang and Yufei Kuang and Mingxuan Yuan and Jia Zeng and Yongdong Zhang and Feng Wu},
booktitle={The Eleventh International Conference on Learning Representations (ICLR'23) },
year={2023}
}

@article{hubbs2020or,
  title={Or-gym: A reinforcement learning library for operations research problems},
  author={Hubbs, Christian D and Perez, Hector D and Sarwar, Owais and Sahinidis, Nikolaos V and Grossmann, Ignacio E and Wassick, John M},
  journal      = {CoRR},
  volume       = {abs/2008.06319},
  year={2020}
}
