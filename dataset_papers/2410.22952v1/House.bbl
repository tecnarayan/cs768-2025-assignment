% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{hu2021lora}
E.~J. Hu, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, L.~Wang, W.~Chen \emph{et~al.}, ``Lora: Low-rank adaptation of large language models,'' in \emph{International Conference on Learning Representations}, 2021.

\bibitem{chen2022adaptformer}
S.~Chen, C.~Ge, Z.~Tong, J.~Wang, Y.~Song, J.~Wang, and P.~Luo, ``Adaptformer: Adapting vision transformers for scalable visual recognition,'' \emph{Advances in Neural Information Processing Systems}, vol.~35, pp. 16\,664--16\,678, 2022.

\bibitem{dong2023efficient}
W.~Dong, D.~Yan, Z.~Lin, and P.~Wang, ``Efficient adaptation of large vision transformer via adapter re-composing,'' in \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem{zhuang2020comprehensive}
F.~Zhuang, Z.~Qi, K.~Duan, D.~Xi, Y.~Zhu, H.~Zhu, H.~Xiong, and Q.~He, ``A comprehensive survey on transfer learning,'' \emph{Proceedings of the IEEE}, vol. 109, no.~1, pp. 43--76, 2020.

\bibitem{pan2009survey}
S.~J. Pan and Q.~Yang, ``A survey on transfer learning,'' \emph{IEEE Transactions on knowledge and data engineering}, vol.~22, no.~10, pp. 1345--1359, 2009.

\bibitem{iman2023review}
M.~Iman, H.~R. Arabnia, and K.~Rasheed, ``A review of deep transfer learning and recent advancements,'' \emph{Technologies}, vol.~11, no.~2, p.~40, 2023.

\bibitem{ying2018transfer}
W.~Ying, Y.~Zhang, J.~Huang, and Q.~Yang, ``Transfer learning via learning to transfer,'' in \emph{International Conference on Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2018, pp. 5085--5094.

\bibitem{yan2018two}
Q.~Yan, D.~Gong, and Y.~Zhang, ``Two-stream convolutional networks for blind image quality assessment,'' \emph{IEEE Transactions on Image Processing}, vol.~28, no.~5, pp. 2200--2211, 2018.

\bibitem{su2020blindly}
S.~Su, Q.~Yan, Y.~Zhu, C.~Zhang, X.~Ge, J.~Sun, and Y.~Zhang, ``Blindly assess image quality in the wild guided by a self-adaptive hyper network,'' in \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 2020.

\bibitem{zhang2013multi}
Y.~Zhang, H.~Zhang, N.~M. Nasrabadi, and T.~S. Huang, ``Multi-metric learning for multi-sensor fusion based classification,'' \emph{Information Fusion}, vol.~14, no.~4, pp. 431--440, 2013.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei, ``Imagenet: A large-scale hierarchical image database,'' in \emph{2009 IEEE conference on computer vision and pattern recognition}, 2009.

\bibitem{tan2021efficientnetv2}
M.~Tan and Q.~Le, ``Efficientnetv2: Smaller models and faster training,'' in \emph{International conference on machine learning}, 2021.

\bibitem{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai, T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly \emph{et~al.}, ``An image is worth 16x16 words: Transformers for image recognition at scale,'' in \emph{International Conference on Learning Representations}, 2020.

\bibitem{liu2021swin}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo, ``Swin transformer: Hierarchical vision transformer using shifted windows,'' in \emph{Proceedings of the IEEE/CVF international conference on computer vision}, 2021.

\bibitem{yun2021re}
S.~Yun, S.~J. Oh, B.~Heo, D.~Han, J.~Choe, and S.~Chun, ``Re-labeling imagenet: from single to multi-labels, from global to localized labels,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2021.

\bibitem{wang2021scaled}
C.-Y. Wang, A.~Bochkovskiy, and H.-Y.~M. Liao, ``Scaled-yolov4: Scaling cross stage partial network,'' in \emph{Proceedings of the IEEE/cvf conference on computer vision and pattern recognition}, 2021.

\bibitem{carion2020end}
N.~Carion, F.~Massa, G.~Synnaeve, N.~Usunier, A.~Kirillov, and S.~Zagoruyko, ``End-to-end object detection with transformers,'' in \emph{European conference on computer vision}, 2020.

\bibitem{cheng2021per}
B.~Cheng, A.~Schwing, and A.~Kirillov, ``Per-pixel classification is not all you need for semantic segmentation,'' \emph{Advances in neural information processing systems}, 2021.

\bibitem{kirillov2020pointrend}
A.~Kirillov, Y.~Wu, K.~He, and R.~Girshick, ``Pointrend: Image segmentation as rendering,'' in \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 2020.

\bibitem{he2022masked}
K.~He, X.~Chen, S.~Xie, Y.~Li, P.~Doll{\'a}r, and R.~Girshick, ``Masked autoencoders are scalable vision learners,'' in \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 2022.

\bibitem{chen2021empirical}
X.~Chen, S.~Xie, and K.~He, ``An empirical study of training self-supervised vision transformers,'' in \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, 2021.

\bibitem{houlsby2019parameter}
N.~Houlsby, A.~Giurgiu, S.~Jastrzebski, B.~Morrone, Q.~De~Laroussilhe, A.~Gesmundo, M.~Attariyan, and S.~Gelly, ``Parameter-efficient transfer learning for nlp,'' in \emph{International Conference on Machine Learning}, 2019.

\bibitem{zaken2022bitfit}
E.~B. Zaken, Y.~Goldberg, and S.~Ravfogel, ``Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models,'' in \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, 2022, pp. 1--9.

\bibitem{jia2022visual}
M.~Jia, L.~Tang, B.-C. Chen, C.~Cardie, S.~Belongie, B.~Hariharan, and S.-N. Lim, ``Visual prompt tuning,'' in \emph{European Conference on Computer Vision}, 2022.

\bibitem{lian2022scaling}
D.~Lian, D.~Zhou, J.~Feng, and X.~Wang, ``Scaling \& shifting your features: A new baseline for efficient model tuning,'' \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem{jie2023fact}
S.~Jie and Z.-H. Deng, ``Fact: Factor-tuning for lightweight adaptation on vision transformer,'' in \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 2023.

\bibitem{zhang2023adaptive}
Q.~Zhang, M.~Chen, A.~Bukharin, P.~He, Y.~Cheng, W.~Chen, and T.~Zhao, ``Adaptive budget allocation for parameter-efficient fine-tuning,'' in \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{hyeon2021fedpara}
N.~Hyeon-Woo, M.~Ye-Bin, and T.-H. Oh, ``Fedpara: Low-rank hadamard product for communication-efficient federated learning,'' \emph{arXiv preprint arXiv:2108.06098}, 2021.

\bibitem{dong2024low}
W.~Dong, X.~Zhang, B.~Chen, D.~Yan, Z.~Lin, Q.~Yan, P.~Wang, and Y.~Yang, ``Low-rank rescaled vision transformer fine-tuning: A residual design approach,'' \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, 2024.

\bibitem{householder1958unitary}
A.~S. Householder, ``Unitary triangularization of a nonsymmetric matrix,'' \emph{Journal of the ACM (JACM)}, vol.~5, no.~4, pp. 339--342, 1958.

\bibitem{trefethen2022numerical}
L.~N. Trefethen and D.~Bau, \emph{Numerical linear algebra}.\hskip 1em plus 0.5em minus 0.4em\relax SIAM, 2022.

\bibitem{francis1961qr}
J.~G. Francis, ``The qr transformation a unitary analogue to the lr transformationâ€”part 1,'' \emph{The Computer Journal}, vol.~4, no.~3, pp. 265--271, 1961.

\bibitem{horn2012matrix}
R.~A. Horn and C.~R. Johnson, \emph{Matrix analysis}.\hskip 1em plus 0.5em minus 0.4em\relax Cambridge university press, 2012.

\bibitem{steiner2021train}
A.~Steiner, A.~Kolesnikov, X.~Zhai, R.~Wightman, J.~Uszkoreit, and L.~Beyer, ``How to train your vit? data, augmentation, and regularization in vision transformers,'' \emph{arXiv preprint arXiv:2106.10270}, 2021.

\bibitem{loshchilov2018decoupled}
I.~Loshchilov and F.~Hutter, ``Decoupled weight decay regularization,'' in \emph{International Conference on Learning Representations}, 2018.

\bibitem{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen, Z.~Lin, N.~Gimelshein, L.~Antiga \emph{et~al.}, ``Pytorch: An imperative style, high-performance deep learning library,'' \emph{Advances in neural information processing systems}, 2019.

\bibitem{wah2011caltech}
C.~Wah, S.~Branson, P.~Welinder, P.~Perona, and S.~Belongie, ``The caltech-ucsd birds-200-2011 dataset,'' 2011.

\bibitem{van2015building}
G.~Van~Horn, S.~Branson, R.~Farrell, S.~Haber, J.~Barry, P.~Ipeirotis, P.~Perona, and S.~Belongie, ``Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection,'' in \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, 2015.

\bibitem{nilsback2008automated}
M.-E. Nilsback and A.~Zisserman, ``Automated flower classification over a large number of classes,'' in \emph{2008 Sixth Indian conference on computer vision, graphics \& image processing}, 2008.

\bibitem{khosla2011novel}
A.~Khosla, N.~Jayadevaprakash, B.~Yao, and F.-F. Li, ``Novel dataset for fine-grained image categorization: Stanford dogs,'' in \emph{Proc. CVPR workshop on fine-grained visual categorization (FGVC)}, vol.~2, no.~1, 2011.

\bibitem{gebru2017fine}
T.~Gebru, J.~Krause, Y.~Wang, D.~Chen, J.~Deng, and L.~Fei-Fei, ``Fine-grained car detection for visual census estimation,'' in \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 2017.

\bibitem{zhai2019large}
X.~Zhai, J.~Puigcerver, A.~Kolesnikov, P.~Ruyssen, C.~Riquelme, M.~Lucic, J.~Djolonga, A.~S. Pinto, M.~Neumann, A.~Dosovitskiy \emph{et~al.}, ``A large-scale study of representation learning with the visual task adaptation benchmark,'' \emph{arXiv preprint arXiv:1910.04867}, 2019.

\end{thebibliography}
