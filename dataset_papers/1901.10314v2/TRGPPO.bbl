\begin{thebibliography}{24}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare2013arcade}
Marc~G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{Brockman2016OpenAI}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym, 2016.

\bibitem[Chen et~al.(2018)Chen, Peng, and Zhang]{Chen2018AnAC}
Gang Chen, Yiming Peng, and Mengjie Zhang.
\newblock An adaptive clipping approach for proximal policy optimization.
\newblock \emph{CoRR}, abs/1804.06461, 2018.

\bibitem[Dhariwal et~al.(2017)Dhariwal, Hesse, Klimov, Nichol, Plappert,
  Radford, Schulman, Sidor, and Wu]{baselines}
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias
  Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu.
\newblock Openai baselines.
\newblock \url{https://github.com/openai/baselines}, 2017.

\bibitem[{Duan} et~al.(2016){Duan}, {Chen}, {Houthooft}, {Schulman}, and
  {Abbeel}]{duan2016benchmarking}
Yan {Duan}, Xi~{Chen}, Rein {Houthooft}, John {Schulman}, and Pieter {Abbeel}.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock \emph{International Conference on Machine Learning}, pages
  1329--1338, 2016.

\bibitem[Fakoor et~al.(2019)Fakoor, Chaudhari, and Smola]{fakoor2019p3o}
Rasool Fakoor, Pratik Chaudhari, and Alexander~J Smola.
\newblock P3o: Policy-on policy-off policy optimization.
\newblock In \emph{Uncertainty in Artificial Intelligence}, 2019.

\bibitem[{Fortunato} et~al.(2018){Fortunato}, {Azar}, {Piot}, {Menick},
  {Hessel}, {Osband}, {Graves}, {Mnih}, {Munos}, {Hassabis}, {Pietquin},
  {Blundell}, and {Legg}]{fortunato2018noisy}
Meire {Fortunato}, Mohammad~Gheshlaghi {Azar}, Bilal {Piot}, Jacob {Menick},
  Matteo {Hessel}, Ian {Osband}, Alex {Graves}, Volodymyr {Mnih}, Remi {Munos},
  Demis {Hassabis}, Olivier {Pietquin}, Charles {Blundell}, and Shane {Legg}.
\newblock Noisy networks for exploration.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[{Haarnoja} et~al.(2017){Haarnoja}, {Tang}, {Abbeel}, and
  {Levine}]{haarnoja2017reinforcement}
Tuomas {Haarnoja}, Haoran {Tang}, Pieter {Abbeel}, and Sergey {Levine}.
\newblock Reinforcement learning with deep energy-based policies.
\newblock \emph{International Conference on Machine Learning}, pages
  1352--1361, 2017.

\bibitem[{Haarnoja} et~al.(2018){Haarnoja}, {Zhou}, {Abbeel}, and
  {Levine}]{haarnoja2018soft}
Tuomas {Haarnoja}, Aurick {Zhou}, Pieter {Abbeel}, and Sergey {Levine}.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock \emph{International Conference on Machine Learning}, pages
  1856--1865, 2018.

\bibitem[{Levine} et~al.(2016){Levine}, {Finn}, {Darrell}, and
  {Abbeel}]{levine2016end}
Sergey {Levine}, Chelsea {Finn}, Trevor {Darrell}, and Pieter {Abbeel}.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1334--1373, 2016.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529, 2015.

\bibitem[{Osband} et~al.(2016){Osband}, {Blundell}, {Pritzel}, and
  {Roy}]{osband2016deep}
Ian {Osband}, Charles {Blundell}, Alexander {Pritzel}, and Benjamin~Van {Roy}.
\newblock Deep exploration via bootstrapped dqn.
\newblock \emph{Advances in Neural Information Processing Systems}, pages
  4026--4034, 2016.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and
  Darrell]{pathak2017curiosity}
Deepak Pathak, Pulkit Agrawal, Alexei~A Efros, and Trevor Darrell.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{International Conference on Machine Learning (ICML)}, volume
  2017, 2017.

\bibitem[Peters and Schaal(2008)]{peters2008reinforcement}
Jan Peters and Stefan Schaal.
\newblock Reinforcement learning of motor skills with policy gradients.
\newblock \emph{Neural networks}, 21\penalty0 (4):\penalty0 682--697, 2008.

\bibitem[Powell(1970)]{powell1970hybrid}
Michael~JD Powell.
\newblock A hybrid method for nonlinear equations.
\newblock \emph{Numerical methods for nonlinear algebraic equations}, 1970.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1889--1897, 2015.

\bibitem[{Schulman} et~al.(2016){Schulman}, {Moritz}, {Levine}, {Jordan}, and
  {Abbeel}]{schulman2016high}
John {Schulman}, Philipp {Moritz}, Sergey {Levine}, Michael~I. {Jordan}, and
  Pieter {Abbeel}.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock \emph{International Conference on Learning Representations}, 2016.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Silver et~al.(2017)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2017mastering}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
  Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
  Graepel, et~al.
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock \emph{arXiv preprint arXiv:1712.01815}, 2017.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{NIPS1999_1713}
Richard~S Sutton, David~A. McAllester, Satinder~P. Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock pages 1057--1063, 2000.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{Todorov2012MuJoCo}
E~Todorov, T~Erez, and Y~Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{IEEE/RSJ International Conference on Intelligent Robots and
  Systems}, pages 5026--5033, 2012.

\bibitem[Wang et~al.(2019)Wang, He, Tan, and Gan]{wang2019truly}
Yuhui Wang, Hao He, Xiaoyang Tan, and Yaozhong Gan.
\newblock Truly proximal policy optimization.
\newblock In \emph{Uncertainty in Artificial Intelligence}, 2019.

\bibitem[{Wu} et~al.(2017){Wu}, {Mansimov}, {Grosse}, {Liao}, and
  {Ba}]{wu2017scalable}
Yuhuai {Wu}, Elman {Mansimov}, Roger~B. {Grosse}, Shun {Liao}, and Jimmy {Ba}.
\newblock Scalable trust-region method for deep reinforcement learning using
  kronecker-factored approximation.
\newblock \emph{Advances in Neural Information Processing Systems}, pages
  5279--5288, 2017.

\bibitem[Ziebart et~al.(2010)Ziebart, Bagnell, and Dey]{Ziebart2010ModelingIV}
Brian~D. Ziebart, J.~Andrew Bagnell, and Anind~K. Dey.
\newblock Modeling interaction via the principle of maximum causal entropy.
\newblock In \emph{ICML}, 2010.

\end{thebibliography}
