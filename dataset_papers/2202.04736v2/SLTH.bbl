\begin{thebibliography}{70}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alabdulmohsin et~al.(2021)Alabdulmohsin, Markeeva, Keysers, and
  Tolstikhin]{alabdulmohsin2021generalized}
Alabdulmohsin, I., Markeeva, L., Keysers, D., and Tolstikhin, I.
\newblock A generalized lottery ticket hypothesis.
\newblock \emph{arXiv preprint arXiv:2107.06825}, 2021.

\bibitem[Bartlett et~al.(2021)Bartlett, Montanari, and
  Rakhlin]{bartlett2021deep}
Bartlett, P.~L., Montanari, A., and Rakhlin, A.
\newblock Deep learning: a statistical viewpoint.
\newblock \emph{arXiv preprint arXiv:2103.09177}, 2021.

\bibitem[Bartoldson et~al.(2019)Bartoldson, Morcos, Barbu, and
  Erlebacher]{bartoldson2019generalization}
Bartoldson, B.~R., Morcos, A.~S., Barbu, A., and Erlebacher, G.
\newblock The generalization-stability tradeoff in neural network pruning.
\newblock \emph{arXiv preprint arXiv:1906.03728}, 2019.

\bibitem[Blalock et~al.(2020)Blalock, Ortiz, Frankle, and
  Guttag]{blalock2020state}
Blalock, D., Ortiz, J. J.~G., Frankle, J., and Guttag, J.
\newblock What is the state of neural network pruning?
\newblock \emph{arXiv preprint arXiv:2003.03033}, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Chen et~al.(2018)Chen, Oh, Fan, and Pistoia]{chen2018sc}
Chen, C.-F., Oh, J., Fan, Q., and Pistoia, M.
\newblock Sc-conv: Sparse-complementary convolution for efficient model
  utilization on cnns.
\newblock In \emph{2018 IEEE International Symposium on Multimedia (ISM)}, pp.\
   97--100. IEEE, 2018.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Frankle, Chang, Liu, Zhang,
  Carbin, and Wang]{chen2020lottery2}
Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Carbin, M., and Wang, Z.
\newblock The lottery tickets hypothesis for supervised and self-supervised
  pre-training in computer vision models.
\newblock \emph{arXiv preprint arXiv:2012.06908}, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Frankle, Chang, Liu, Zhang, Wang,
  and Carbin]{chen2020lottery}
Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang, Z., and Carbin, M.
\newblock The lottery ticket hypothesis for pre-trained bert networks.
\newblock \emph{arXiv preprint arXiv:2007.12223}, 2020{\natexlab{b}}.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Cheng, Gan, Liu, and
  Wang]{chen2021ultra}
Chen, T., Cheng, Y., Gan, Z., Liu, J., and Wang, Z.
\newblock Ultra-data-efficient gan training: Drawing a lottery ticket first,
  then training it toughly.
\newblock \emph{arXiv preprint arXiv:2103.00397}, 2021{\natexlab{a}}.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Sui, Chen, Zhang, and
  Wang]{chen2021unified}
Chen, T., Sui, Y., Chen, X., Zhang, A., and Wang, Z.
\newblock A unified lottery ticket hypothesis for graph neural networks,
  2021{\natexlab{b}}.

\bibitem[Chen(2018)]{chen2018escort}
Chen, X.
\newblock Escort: Efficient sparse convolutional neural networks on gpus.
\newblock \emph{arXiv preprint arXiv:1802.10280}, 2018.

\bibitem[Chen et~al.(2021{\natexlab{c}})Chen, Chen, Zhang, and
  Wang]{chen2021you}
Chen, X., Chen, T., Zhang, Z., and Wang, Z.
\newblock You are caught stealing my winning lottery ticket! making a lottery
  ticket claim its ownership.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{c}}.

\bibitem[Chen et~al.(2021{\natexlab{d}})Chen, Zhang, Sui, and
  Chen]{chen2021gans}
Chen, X., Zhang, Z., Sui, Y., and Chen, T.
\newblock {\{}GAN{\}}s can play lottery tickets too.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{d}}.
\newblock URL \url{https://openreview.net/forum?id=1AoMhc_9jER}.

\bibitem[Chetlur et~al.(2014)Chetlur, Woolley, Vandermersch, Cohen, Tran,
  Catanzaro, and Shelhamer]{chetlur2014cudnn}
Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catanzaro, B.,
  and Shelhamer, E.
\newblock cudnn: Efficient primitives for deep learning.
\newblock \emph{arXiv preprint arXiv:1410.0759}, 2014.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Dong et~al.(2019)Dong, Liu, Zhao, Li, Li, Wang, and
  Feng]{dong2019acorns}
Dong, X., Liu, L., Zhao, P., Li, G., Li, J., Wang, X., and Feng, X.
\newblock Acorns: A framework for accelerating deep neural networks with input
  sparsity.
\newblock In \emph{2019 28th International Conference on Parallel Architectures
  and Compilation Techniques (PACT)}, pp.\  178--191. IEEE, 2019.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018.

\bibitem[Frankle \& Carbin(2019)Frankle and Carbin]{frankle2018the}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJl-b3RcF7}.

\bibitem[Frankle et~al.(2020{\natexlab{a}})Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020linear}
Frankle, J., Dziugaite, G.~K., Roy, D., and Carbin, M.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3259--3269. PMLR, 2020{\natexlab{a}}.

\bibitem[Frankle et~al.(2020{\natexlab{b}})Frankle, Schwab, and
  Morcos]{Frankle2020The}
Frankle, J., Schwab, D.~J., and Morcos, A.~S.
\newblock The early phase of neural network training.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=Hkl1iRNFwS}.

\bibitem[Gale et~al.(2019)Gale, Elsen, and Hooker]{gale2019state}
Gale, T., Elsen, E., and Hooker, S.
\newblock The state of sparsity in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1902.09574}, 2019.

\bibitem[Gan et~al.(2021)Gan, Chen, Li, Chen, Cheng, Wang, and
  Liu]{gan2021playing}
Gan, Z., Chen, Y.-C., Li, L., Chen, T., Cheng, Y., Wang, S., and Liu, J.
\newblock Playing lottery tickets with vision and language.
\newblock \emph{arXiv preprint arXiv:2104.11832}, 2021.

\bibitem[Han et~al.(2015{\natexlab{a}})Han, Mao, and Dally]{han2015deep}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015{\natexlab{a}}.

\bibitem[Han et~al.(2015{\natexlab{b}})Han, Pool, Tran, and
  Dally]{han2015learning}
Han, S., Pool, J., Tran, J., and Dally, W.~J.
\newblock Learning both weights and connections for efficient neural networks.
\newblock \emph{arXiv preprint arXiv:1506.02626}, 2015{\natexlab{b}}.

\bibitem[Han et~al.(2016)Han, Liu, Mao, Pu, Pedram, Horowitz, and
  Dally]{han2016eie}
Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M.~A., and Dally,
  W.~J.
\newblock Eie: efficient inference engine on compressed deep neural network.
\newblock In \emph{ISCA}, 2016.

\bibitem[Hanson \& Pratt(1988)Hanson and Pratt]{hanson1988comparing}
Hanson, S. and Pratt, L.
\newblock Comparing biases for minimal network construction with
  back-propagation.
\newblock \emph{Advances in neural information processing systems}, 1:\penalty0
  177--185, 1988.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2017)He, Zhang, and Sun]{he2017channel}
He, Y., Zhang, X., and Sun, J.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, pp.\  1389--1397, 2017.

\bibitem[Hoefler et~al.(2021)Hoefler, Alistarh, Ben-Nun, Dryden, and
  Peste]{hoefler2021sparsity}
Hoefler, T., Alistarh, D., Ben-Nun, T., Dryden, N., and Peste, A.
\newblock Sparsity in deep learning: Pruning and growth for efficient inference
  and training in neural networks.
\newblock \emph{arXiv preprint arXiv:2102.00554}, 2021.

\bibitem[Hong et~al.(2018)Hong, Sukumaran-Rajam, Bandyopadhyay, Kim, Kurt,
  Nisa, Sabhlok, \c{C}ataly\"{u}rek, Parthasarathy, and
  Sadayappan]{hong2018efficient}
Hong, C., Sukumaran-Rajam, A., Bandyopadhyay, B., Kim, J., Kurt, S.~E., Nisa,
  I., Sabhlok, S., \c{C}ataly\"{u}rek, U.~V., Parthasarathy, S., and
  Sadayappan, P.
\newblock Efficient sparse-matrix multi-vector product on gpus.
\newblock Association for Computing Machinery, 2018.

\bibitem[Hong et~al.(2019)Hong, Sukumaran-Rajam, Nisa, Singh, and
  Sadayappan]{hong2019adaptive}
Hong, C., Sukumaran-Rajam, A., Nisa, I., Singh, K., and Sadayappan, P.
\newblock Adaptive sparse tiling for sparse matrix multiplication.
\newblock In \emph{Proceedings of the 24th Symposium on Principles and Practice
  of Parallel Programming}, pp.\  300--314, 2019.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{howard2017mobilenets}
Howard, A.~G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T.,
  Andreetto, M., and Adam, H.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{arXiv preprint arXiv:1704.04861}, 2017.

\bibitem[Hu et~al.(2016)Hu, Peng, Tai, and Tang]{hu2016network}
Hu, H., Peng, R., Tai, Y.-W., and Tang, C.-K.
\newblock Network trimming: A data-driven neuron pruning approach towards
  efficient deep architectures.
\newblock \emph{arXiv preprint arXiv:1607.03250}, 2016.

\bibitem[Jiang et~al.(2020)Jiang, Hong, and Agrawal]{10.1145/3332466.3374546}
Jiang, P., Hong, C., and Agrawal, G.
\newblock A novel data transformation and execution strategy for accelerating
  sparse matrix multiplication on gpus.
\newblock PPoPP '20. Association for Computing Machinery, 2020.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R.,
  Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Le \& Yang(2015)Le and Yang]{le2015tiny}
Le, Y. and Yang, X.
\newblock Tiny imagenet visual recognition challenge.
\newblock \emph{CS 231N}, 7:\penalty0 7, 2015.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{lecun1990optimal}
LeCun, Y., Denker, J.~S., and Solla, S.~A.
\newblock Optimal brain damage.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  598--605, 1990.

\bibitem[Lee et~al.(2019{\natexlab{a}})Lee, Ajanthan, and Torr]{lee2018snip}
Lee, N., Ajanthan, T., and Torr, P.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019{\natexlab{a}}.

\bibitem[Lee et~al.(2019{\natexlab{b}})Lee, Ajanthan, and Torr]{snip}
Lee, N., Ajanthan, T., and Torr, P.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=B1VZqjAcYX}.

\bibitem[Li et~al.(2016)Li, Kadav, Durdanovic, Samet, and Graf]{li2016pruning}
Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H.~P.
\newblock Pruning filters for efficient convnets.
\newblock \emph{arXiv preprint arXiv:1608.08710}, 2016.

\bibitem[Lin et~al.(2020)Lin, Ji, Zhang, Zhang, Wu, and Tian]{lin2020channel}
Lin, M., Ji, R., Zhang, Y., Zhang, B., Wu, Y., and Tian, Y.
\newblock Channel pruning via automatic structure search.
\newblock \emph{arXiv preprint arXiv:2001.08565}, 2020.

\bibitem[Liu et~al.(2017)Liu, Li, Shen, Huang, Yan, and Zhang]{liu2017learning}
Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., and Zhang, C.
\newblock Learning efficient convolutional networks through network slimming.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  2736--2744, 2017.

\bibitem[Louizos et~al.(2017)Louizos, Welling, and Kingma]{louizos2017learning}
Louizos, C., Welling, M., and Kingma, D.~P.
\newblock Learning sparse neural networks through $ l\_0 $ regularization.
\newblock \emph{arXiv preprint arXiv:1712.01312}, 2017.

\bibitem[Ma et~al.(2021{\natexlab{a}})Ma, Chen, Hu, You, Xie, and
  Wang]{ma2021good}
Ma, H., Chen, T., Hu, T.-K., You, C., Xie, X., and Wang, Z.
\newblock Good students play big lottery better.
\newblock \emph{arXiv preprint arXiv:2101.03255}, 2021{\natexlab{a}}.

\bibitem[Ma et~al.(2020)Ma, Guo, Niu, Lin, Tang, Ma, Ren, and
  Wang]{ma2020pconv}
Ma, X., Guo, F.-M., Niu, W., Lin, X., Tang, J., Ma, K., Ren, B., and Wang, Y.
\newblock Pconv: The missing but desirable sparsity in dnn weight pruning for
  real-time execution on mobile devices.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  5117--5124, 2020.

\bibitem[Ma et~al.(2021{\natexlab{b}})Ma, Yuan, Shen, Chen, Chen, Chen, Liu,
  Qin, Liu, Wang, et~al.]{ma2021sanity}
Ma, X., Yuan, G., Shen, X., Chen, T., Chen, X., Chen, X., Liu, N., Qin, M.,
  Liu, S., Wang, Z., et~al.
\newblock Sanity checks for lottery tickets: Does your winning ticket really
  win the jackpot?
\newblock \emph{arXiv preprint arXiv:2107.00166}, 2021{\natexlab{b}}.

\bibitem[Mao et~al.(2017)Mao, Han, Pool, Li, Liu, Wang, and
  Dally]{mao2017exploring}
Mao, H., Han, S., Pool, J., Li, W., Liu, X., Wang, Y., and Dally, W.~J.
\newblock Exploring the regularity of sparse structure in convolutional neural
  networks.
\newblock \emph{arXiv preprint arXiv:1705.08922}, 2017.

\bibitem[Molchanov et~al.(2019)Molchanov, Mallya, Tyree, Frosio, and
  Kautz]{molchanov2019importance}
Molchanov, P., Mallya, A., Tyree, S., Frosio, I., and Kautz, J.
\newblock Importance estimation for neural network pruning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  11264--11272, 2019.

\bibitem[Niu et~al.(2020)Niu, Ma, Lin, Wang, Qian, Lin, Wang, and
  Ren]{niu2020patdnn}
Niu, W., Ma, X., Lin, S., Wang, S., Qian, X., Lin, X., Wang, Y., and Ren, B.
\newblock Patdnn: Achieving real-time dnn execution on mobile devices with
  pattern-based weight pruning.
\newblock \emph{arXiv preprint arXiv:2001.00138}, 2020.

\bibitem[Park et~al.(2016)Park, Li, Wen, Tang, Li, Chen, and
  Dubey]{park2016faster}
Park, J., Li, S., Wen, W., Tang, P. T.~P., Li, H., Chen, Y., and Dubey, P.
\newblock Faster cnns with direct sparse convolutions and guided pruning.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Peng et~al.(2017)Peng, Fu, Liu, and Hsu]{peng2017adaptive}
Peng, K.-Y., Fu, S.-Y., Liu, Y.-P., and Hsu, W.-C.
\newblock Adaptive runtime exploiting sparsity in tensor of deep learning
  neural network on heterogeneous systems.
\newblock In \emph{2017 International Conference on Embedded Computer Systems:
  Architectures, Modeling, and Simulation (SAMOS)}, pp.\  105--112. IEEE, 2017.

\bibitem[Radu et~al.(2019)Radu, Kaszyk, Wen, Turner, Cano, Crowley, Franke,
  Storkey, and O'Boyle]{radu2019performance}
Radu, V., Kaszyk, K., Wen, Y., Turner, J., Cano, J., Crowley, E.~J., Franke,
  B., Storkey, A., and O'Boyle, M.
\newblock Performance aware convolutional neural network channel pruning for
  embedded gpus.
\newblock In \emph{2019 IEEE International Symposium on Workload
  Characterization (IISWC)}, pp.\  24--34. IEEE, 2019.

\bibitem[Ren et~al.(2018)Ren, Zhang, Ye, Li, Xu, Qian, Lin, and
  Wang]{ren2018admmnn}
Ren, A., Zhang, T., Ye, S., Li, J., Xu, W., Qian, X., Lin, X., and Wang, Y.
\newblock Admm-nn: An algorithm-hardware co-design framework of dnns using
  alternating direction method of multipliers, 2018.

\bibitem[Renda et~al.(2020)Renda, Frankle, and Carbin]{Renda2020Comparing}
Renda, A., Frankle, J., and Carbin, M.
\newblock Comparing rewinding and fine-tuning in neural network pruning.
\newblock In \emph{8th International Conference on Learning Representations},
  2020.

\bibitem[Ruder(2016)]{ruder2016overview}
Ruder, S.
\newblock An overview of gradient descent optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1609.04747}, 2016.

\bibitem[Rumi et~al.(2020)Rumi, Ma, Wang, and Jiang]{rumi2020accelerating}
Rumi, M.~A., Ma, X., Wang, Y., and Jiang, P.
\newblock Accelerating sparse cnn inference on gpus with performance-aware
  weight pruning.
\newblock In \emph{Proceedings of the ACM International Conference on Parallel
  Architectures and Compilation Techniques}, pp.\  267--278, 2020.

\bibitem[Shangguan et~al.(2019)Shangguan, Li, Liang, Alvarez, and
  McGraw]{shangguan2019optimizing}
Shangguan, Y., Li, J., Liang, Q., Alvarez, R., and McGraw, I.
\newblock Optimizing speech recognition for the edge.
\newblock \emph{arXiv preprint arXiv:1909.12408}, 2019.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Tanaka et~al.(2020)Tanaka, Kunin, Yamins, and
  Ganguli]{tanaka2020pruning}
Tanaka, H., Kunin, D., Yamins, D.~L., and Ganguli, S.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock In \emph{Advances in Neural Information Processing Systems 33
  pre-proceedings}, 2020.

\bibitem[Tang et~al.(2020)Tang, Wang, Xu, Tao, Xu, Xu, and Xu]{tang2020scop}
Tang, Y., Wang, Y., Xu, Y., Tao, D., Xu, C., Xu, C., and Xu, C.
\newblock Scop: Scientific control for reliable neural network pruning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 10936--10947, 2020.

\bibitem[Wang et~al.(2020)Wang, Zhang, and Grosse]{Wang2020Picking}
Wang, C., Zhang, G., and Grosse, R.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SkgsACVKPH}.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li]{wen2016learning}
Wen, W., Wu, C., Wang, Y., Chen, Y., and Li, H.
\newblock Learning structured sparsity in deep neural networks.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, pp.\  2074--2082, 2016.

\bibitem[You et~al.(2020)You, Li, Xu, Fu, Wang, Chen, Baraniuk, Wang, and
  Lin]{You2020Drawing}
You, H., Li, C., Xu, P., Fu, Y., Wang, Y., Chen, X., Baraniuk, R.~G., Wang, Z.,
  and Lin, Y.
\newblock Drawing early-bird tickets: Toward more efficient training of deep
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=BJxsrgStvr}.

\bibitem[Yu et~al.(2020)Yu, Edunov, Tian, and Morcos]{yu2019playing}
Yu, H., Edunov, S., Tian, Y., and Morcos, A.~S.
\newblock Playing the lottery with rewards and multiple languages: lottery
  tickets in rl and nlp.
\newblock In \emph{8th International Conference on Learning Representations},
  2020.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zhang et~al.(2018)Zhang, Ye, Zhang, Tang, Wen, Fardad, and
  Wang]{Zhang_2018}
Zhang, T., Ye, S., Zhang, K., Tang, J., Wen, W., Fardad, M., and Wang, Y.
\newblock A systematic {DNN} weight pruning framework using alternating
  direction method of multipliers.
\newblock In \emph{ECCV}, 2018.

\bibitem[Zhang et~al.(2021)Zhang, Chen, Chen, and Wang]{pmlr-v139-zhang21c}
Zhang, Z., Chen, X., Chen, T., and Wang, Z.
\newblock Efficient lottery ticket finding: Less data is more.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  12380--12390. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/zhang21c.html}.

\bibitem[Zhou et~al.(2021)Zhou, Ma, Zhu, Liu, Zhang, Yuan, Sun, and
  Li]{zhou2021learning}
Zhou, A., Ma, Y., Zhu, J., Liu, J., Zhang, Z., Yuan, K., Sun, W., and Li, H.
\newblock Learning n: M fine-grained structured sparse neural networks from
  scratch.
\newblock \emph{arXiv preprint arXiv:2102.04010}, 2021.

\bibitem[Zhu et~al.(2019)Zhu, Zhang, Gu, and Xie]{Zhu2019STC}
Zhu, M., Zhang, T., Gu, Z., and Xie, Y.
\newblock Sparse tensor core: Algorithm and hardware co-design for vector-wise
  sparse neural networks on modern gpus.
\newblock In \emph{Proceedings of the 52nd Annual IEEE/ACM International
  Symposium on Microarchitecture}, pp.\  359--371, 2019.

\end{thebibliography}
