\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[{Bateni} et~al.(2020){Bateni}, {Goyal}, {Masrani}, {Wood}, and
  {Sigal}]{BAT-20}
Peyman {Bateni}, Raghav {Goyal}, Vaden {Masrani}, Frank {Wood}, and Leonid
  {Sigal}.
\newblock Improved few-shot visual classification.
\newblock In \emph{Proc. Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2020.

\bibitem[Buzzega et~al.(2020)Buzzega, Boschini, Porrello, Abati, and
  Calderara]{buzzega2020dark}
Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone
  Calderara.
\newblock Dark experience for general continual learning: A strong, simple
  baseline.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15920--15930, 2020.

\bibitem[Chaudhry et~al.(2018)Chaudhry, Dokania, Ajanthan, and
  Torr]{Chaudhry_2018_ECCV}
Arslan Chaudhry, Puneet~K. Dokania, Thalaiyasingam Ajanthan, and Philip H.~S.
  Torr.
\newblock Riemannian walk for incremental learning: Understanding forgetting
  and intransigence.
\newblock In \emph{Proc. European Conference on Computer Vision (ECCV)},
  September 2018.

\bibitem[Chen(1996)]{Chen.96}
C.~L.~Philip Chen.
\newblock A rapid supervised learning neural network for function interpolation
  and approximation.
\newblock \emph{IEEE Transactions on Neural Networks}, 7:\penalty0 1220--1230,
  1996.

\bibitem[Chen et~al.(2023)Chen, Wu, Han, Jia, and Jiang]{chen2023promptfusion}
Haoran Chen, Zuxuan Wu, Xintong Han, Menglin Jia, and Yu-Gang Jiang.
\newblock Prompt{F}usion: {D}ecoupling stability and plasticity for continual
  learning.
\newblock arXiv:2303.07223, Submitted 13 March 2023, 2023.

\bibitem[Chen et~al.(2022)Chen, GE, Tong, Wang, Song, Wang, and
  Luo]{chen2022adaptformer}
Shoufa Chen, Chongjian GE, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and
  Ping Luo.
\newblock Adapt{F}ormer: Adapting vision transformers for scalable visual
  recognition.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~35,
  pages 16664--16678. Curran Associates, Inc., 2022.

\bibitem[De~Lange and Tuytelaars(2021)]{Lange21}
Matthias De~Lange and Tinne Tuytelaars.
\newblock Continual prototype evolution: Learning online from non-stationary
  data streams.
\newblock In \emph{Proc. IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pages 8230--8239, 2021.

\bibitem[Ding et~al.(2022)Ding, Liu, Tian, Yang, and Ding]{ding2022dont}
Yuxuan Ding, Lingqiao Liu, Chunna Tian, Jingyuan Yang, and Haoxuan Ding.
\newblock Don't stop learning: Towards continual learning for the {CLIP} model.
\newblock arXiv: 2207.09248, 2022.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock arXiv:2010.11929 (ICLR 2020), 2020.

\bibitem[Ermis et~al.(2022)Ermis, Zappella, Wistuba, Rawal, and
  Archambeau]{Ermis22}
Beyza Ermis, Giovanni Zappella, Martin Wistuba, Aditya Rawal, and Cedric
  Archambeau.
\newblock Memory efficient continual learning with transformers.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~35,
  pages 10629--10642. Curran Associates, Inc., 2022.

\bibitem[Hayes and Kanan(2020)]{hayes2020lifelong}
Tyler~L. Hayes and Christopher Kanan.
\newblock Lifelong machine learning with deep streaming linear discriminant
  analysis.
\newblock arXiv:1909.01520, 2020.

\bibitem[He et~al.(2022)He, Zhou, Ma, Berg-Kirkpatrick, and
  Neubig]{he2022unified}
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham
  Neubig.
\newblock Towards a unified view of parameter-efficient transfer learning.
\newblock arXiv: 2110.04366 (ICLR 2022), 2022.

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Basart, Mu, Kadavath,
  Wang, Dorundo, Desai, Zhu, Parajuli, Guo, Song, Steinhardt, and
  Gilmer]{Hendrycks_ImageNetR}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan
  Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob
  Steinhardt, and Justin Gilmer.
\newblock The many faces of robustness: {A} critical analysis of
  out-of-distribution generalization.
\newblock In \emph{Proc. IEEE/CVF International Conference on Computer Vision,
  {ICCV} 2021, Montreal, QC, Canada, October 10-17, 2021}, pages 8320--8329.
  {IEEE}, 2021{\natexlab{a}}.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Zhao, Basart,
  Steinhardt, and Song]{Hendrycks_2021_CVPR}
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
\newblock Natural adversarial examples.
\newblock In \emph{Proc. IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 15262--15271, June 2021{\natexlab{b}}.

\bibitem[Hocquet et~al.(2020)Hocquet, Bichler, and Querlioz]{Hocquet20}
Guillaume Hocquet, Olivier Bichler, and Damien Querlioz.
\newblock {OvA-INN: C}ontinual learning with invertible neural networks.
\newblock In \emph{Proc. International Joint Conference on Neural Networks
  (IJCNN)}, pages 1--7, 2020.

\bibitem[Hou et~al.(2019)Hou, Pan, Loy, Wang, and Lin]{UCIR}
Saihui Hou, Xinyu Pan, Chen~Change Loy, Zilei Wang, and Dahua Lin.
\newblock Learning a unified classifier incrementally via rebalancing.
\newblock In \emph{Proc. 2019 IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 831--839, 2019.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, and
  Chen]{hu2021lora}
Edward Hu, Yelong Shen, Phil Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Lu~Wang, and
  Weizhu Chen.
\newblock {LoRA: L}ow-rank adaptation of large language models.
\newblock arXiv:2106.09685 (ICLR 2022), 2021.

\bibitem[Huang et~al.(2006)Huang, Zhu, and Siew]{Huang.06}
Guang-Bin Huang, Qin-Yu Zhu, and Chee-Kheong Siew.
\newblock Extreme learning machine: {T}heory and applications.
\newblock \emph{Neurocomputing}, 70:\penalty0 489--501, 2006.

\bibitem[Ilharco et~al.(2021)Ilharco, Wortsman, Wightman, Gordon, Carlini,
  Taori, Dave, Shankar, Namkoong, Miller, Hajishirzi, Farhadi, and
  Schmidt]{ilharco_gabriel_2021_5143773}
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas
  Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John
  Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.
\newblock {OpenCLIP}, July 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5143773}.

\bibitem[Janson et~al.(2023)Janson, Zhang, Aljundi, and
  Elhoseiny]{janson2023simple}
Paul Janson, Wenxuan Zhang, Rahaf Aljundi, and Mohamed Elhoseiny.
\newblock A simple baseline that questions the use of pretrained-models in
  continual learning.
\newblock arXiv: 2210.04428 (2022 NeurIPS Workshop on Distribution Shifts),
  2023.

\bibitem[Jia et~al.(2022)Jia, Tang, Chen, Cardie, Belongie, Hariharan, and
  Lim]{jia2022vpt}
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath
  Hariharan, and Ser-Nam Lim.
\newblock Visual prompt tuning.
\newblock In \emph{Proc. European Conference on Computer Vision (ECCV)}, 2022.

\bibitem[Jie et~al.(2022)Jie, Deng, and Li]{Jie_2022_CVPR}
Shibo Jie, Zhi-Hong Deng, and Ziheng Li.
\newblock Alleviating representational shift for continual fine-tuning.
\newblock In \emph{Proc. IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) Workshops}, pages 3810--3819, June 2022.

\bibitem[Kessy et~al.(2018)Kessy, Lewin, and Strimmer]{Kessy18}
Agnan Kessy, Alex Lewin, and Korbinian Strimmer.
\newblock Optimal whitening and decorrelation.
\newblock \emph{The American Statistician}, 72:\penalty0 309--314, 2018.

\bibitem[Kornblith et~al.(2019)Kornblith, Shlens, and Le]{Kornblith.18}
Simon Kornblith, Jonathon Shlens, and Quoc~V. Le.
\newblock Do better imagenet models transfer better?
\newblock In \emph{Proc. {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2019, Long Beach, CA, USA, June 16-20, 2019}, pages
  2661--2671. Computer Vision Foundation / {IEEE}, 2019.

\bibitem[Krause et~al.(2013)Krause, Stark, Deng, and Fei-Fei]{Krause13}
Jonathan Krause, Michael Stark, Jia Deng, and Li~Fei-Fei.
\newblock {3D} object representations for fine-grained categorization.
\newblock In \emph{Proc. IEEE International Conference on Computer Vision
  Workshops}, pages 554--561, 2013.

\bibitem[Krizhevsky(2009)]{Krizhevsky}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Master's thesis, Dept of CS, University of Toronto. See {\tt
  {http://www.cs.toronto.edu/~kriz/cifar.html}}), 2009.

\bibitem[Li et~al.(2022)Li, Huang, Paudel, Wang, Shahbazi, Hong, and
  Gool]{li2022continual}
Chuqiao Li, Zhiwu Huang, Danda~Pani Paudel, Yabin Wang, Mohamad Shahbazi,
  Xiaopeng Hong, and Luc~Van Gool.
\newblock A continual deepfake detection benchmark: Dataset, methods, and
  essentials.
\newblock arXiv:2205.05467, 2022.

\bibitem[Lian et~al.(2022)Lian, Zhou, Feng, and Wang]{NEURIPS2022_00bb4e41}
Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang.
\newblock Scaling \& shifting your features: {A} new baseline for efficient
  model tuning.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~35,
  pages 109--123. Curran Associates, Inc., 2022.

\bibitem[Lomonaco and Maltoni(2017)]{CORE50}
Vincenzo Lomonaco and Davide Maltoni.
\newblock Core50: {A} new dataset and benchmark for continuous object
  recognition.
\newblock In Sergey Levine, Vincent Vanhoucke, and Ken Goldberg, editors,
  \emph{Proceedings of the 1st Annual Conference on Robot Learning}, volume~78
  of \emph{Proceedings of Machine Learning Research}, pages 17--26. PMLR,
  13--15 Nov 2017.

\bibitem[Lopez-Paz and Ranzato(2017)]{NIPS2017_f8752278}
David Lopez-Paz and Marc{\textquotesingle}Aurelio Ranzato.
\newblock Gradient episodic memory for continual learning.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Mai et~al.(2021)Mai, Li, Kim, and Sanner]{Mai21}
Zheda Mai, Ruiwen Li, Hyunwoo Kim, and Scott Sanner.
\newblock Supervised contrastive replay: Revisiting the nearest class mean
  classifier in online class-incremental continual learning.
\newblock In \emph{Proc. {IEEE} Conference on Computer Vision and Pattern
  Recognition Workshops, {CVPR} Workshops 2021, virtual, June 19-25, 2021},
  pages 3589--3599. Computer Vision Foundation / {IEEE}, 2021.

\bibitem[McDonnell et~al.(2016)McDonnell, McKilliam, and
  de~Chazal]{McDonnell.16}
Mark~D. McDonnell, Robby~G. McKilliam, and Philip de~Chazal.
\newblock On the importance of pair-wise feature correlations for image
  classification.
\newblock In \emph{Proc. 2016 International Joint Conference on Neural Networks
  (IJCNN)}, pages 2290--2297, 2016.

\bibitem[Mehta et~al.(2021)Mehta, Patil, Chandar, and
  Strubell]{mehta2021empirical}
Sanket~Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell.
\newblock An empirical investigation of the role of pre-training in lifelong
  learning.
\newblock arXiv:2112.09153, 2021.

\bibitem[Mensink et~al.(2012)Mensink, Verbeek, Perronnin, and
  Csurka]{Mensink2012MetricLF}
Thomas Mensink, Jakob~J. Verbeek, Florent Perronnin, and Gabriela Csurka.
\newblock Metric learning for large scale image classification: Generalizing to
  new classes at near-zero cost.
\newblock In \emph{Proc. European Conference on Computer Vision}, 2012.

\bibitem[Murphy(2013)]{murphy2013machine}
Kevin~P. Murphy.
\newblock \emph{Machine learning : A probabilistic perspective}.
\newblock MIT Press, Cambridge, Mass. [u.a.], 2013.

\bibitem[Pang et~al.(2005)Pang, Ozawa, and Kasabov]{Pang05}
Shaoning Pang, S.~Ozawa, and N.~Kasabov.
\newblock Incremental linear discriminant analysis for classification of data
  streams.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics, Part B
  (Cybernetics)}, 35:\penalty0 905--914, 2005.

\bibitem[Panos et~al.(2023)Panos, Kobe, Reino, Aljundi, and
  Turner]{panos2023session}
Aristeidis Panos, Yuriko Kobe, Daniel~Olmeda Reino, Rahaf Aljundi, and
  Richard~E. Turner.
\newblock First session adaptation: A strong replay-free baseline for
  class-incremental learning.
\newblock In \emph{Proc. IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pages 18820--18830, October 2023.

\bibitem[Pao and Takefuji(1991)]{Pao.91}
Yoh-Han Pao and Yoshiyasu Takefuji.
\newblock Functional-link net computing: Theory, system architecture, and
  functionalities.
\newblock \emph{Computing Magazine}, 3:\penalty0 7679--7682, 1991.

\bibitem[Pelosin(2022)]{pelosin2022simpler}
Francesco Pelosin.
\newblock Simpler is better: {O}ff-the-shelf continual learning through
  pretrained backbones.
\newblock arXiv:2205.01586, 2022.

\bibitem[Peng et~al.(2019)Peng, Bai, Xia, Huang, Saenko, and
  Wang]{peng2019moment}
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo~Wang.
\newblock Moment matching for multi-source domain adaptation.
\newblock In \emph{Proc. IEEE International Conference on Computer Vision},
  pages 1406--1415, 2019.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{Radford}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{Proc. International Conference on Machine Learning}, pages
  8748--8763. PMLR, 2021.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Kolesnikov, Sperl, and Lampert]{icarl}
Sylvestre{-}Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph~H.
  Lampert.
\newblock icarl: Incremental classifier and representation learning.
\newblock In \emph{Proc. {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017}, pages
  5533--5542. {IEEE} Computer Society, 2017.

\bibitem[Schmidt et~al.(1992)Schmidt, Kraaijveld, and Duin]{Schmidt.92}
Wouter~F. Schmidt, Martin~A. Kraaijveld, and Robert P.~W. Duin.
\newblock Feedforward neural networks with random weights.
\newblock In \emph{Proc. 11th IAPR International Conference on Pattern
  Recognition. Vol.II. Conference B: Pattern Recognition Methodology and
  Systems}, pages 1--4, 1992.

\bibitem[Shanahan et~al.(2021)Shanahan, Kaplanis, and Mitrovic]{Shanahan21}
Murray Shanahan, Christos Kaplanis, and Jovana Mitrovic.
\newblock Encoders and ensembles for task-free continual learning.
\newblock arXiv:2105.13327, 2021.

\bibitem[Shysheya et~al.(2023)Shysheya, Bronskill, Patacchiola, Nowozin, and
  Turner]{shysheya2023fit}
Aliaksandra Shysheya, John Bronskill, Massimiliano Patacchiola, Sebastian
  Nowozin, and Richard~E Turner.
\newblock {FiT: P}arameter efficient few-shot transfer learning for
  personalized and federated image classification.
\newblock arXiv:2206.08671, 2023.

\bibitem[Smith et~al.(2023{\natexlab{a}})Smith, Karlinsky, Gutta,
  Cascante-Bonilla, Kim, Arbelle, Panda, Feris, and Kira]{smith2023codaprompt}
James~Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola Cascante-Bonilla,
  Donghyun Kim, Assaf Arbelle, Rameswar Panda, Rogerio Feris, and Zsolt Kira.
\newblock {CODA-Prompt: COntinual Decomposed Attention-based} prompting for
  rehearsal-free continual learning.
\newblock In \emph{Proc. IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 11909--11919, June 2023{\natexlab{a}}.

\bibitem[Smith et~al.(2023{\natexlab{b}})Smith, Tian, Halbe, Hsu, and
  Kira]{smith2023closer}
James~Seale Smith, Junjiao Tian, Shaunak Halbe, Yen-Chang Hsu, and Zsolt Kira.
\newblock A closer look at rehearsal-free continual learning.
\newblock arXiv:2203.17269 (Revised 3 April, 2023{\natexlab{b}}.

\bibitem[Soutif{-}Cormerais et~al.(2021)Soutif{-}Cormerais, Masana, van~de
  Weijer, and Twardowski]{Soutif}
Albin Soutif{-}Cormerais, Marc Masana, Joost van~de Weijer, and Bartlomiej
  Twardowski.
\newblock On the importance of cross-task features for class-incremental
  learning.
\newblock arXiv:2106.11930, 2021.

\bibitem[Sun et~al.(2023)Sun, Ye, Zhan, and Liu]{sun2023pilot}
Hai-Long Sun, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan.
\newblock {PILOT: A} Pre-Trained Model-Based Continual Learning Toolbox.
\newblock arXiv:2309.07117, 2023.

\bibitem[Tsang et~al.(2018)Tsang, Cheng, and Liu]{tsang2018detecting}
Michael Tsang, Dehua Cheng, and Yan Liu.
\newblock Detecting statistical interactions from neural network weights.
\newblock In \emph{Proc. Intl Conference on Learning Representations (ICLR)},
  2018.

\bibitem[van~de Ven et~al.(2022)van~de Ven, Tuytelaars, and Tolias]{Gido22}
Gido~M. van~de Ven, Tinne Tuytelaars, and Andreas~S. Tolias.
\newblock Three types of incremental learning.
\newblock \emph{Nature Machine Intelligence}, pages 1185--1197, 2022.

\bibitem[Wah et~al.(2011)Wah, Branson, Welinder, Perona, and Belongie]{CUB}
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge
  Belongie.
\newblock The {Caltech-UCSD Birds-200-2011 Dataset}.
\newblock Technical report, California Institute of Technology, 2011.
\newblock Technical Report CNS-TR-2011-001.

\bibitem[Wang et~al.(2023)Wang, Zhang, Su, and Zhu]{wang2023comprehensive}
Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu.
\newblock A comprehensive survey of continual learning: Theory, method and
  application.
\newblock arXiv: 2302.00487, 2023.

\bibitem[Wang et~al.(2017)Wang, Fu, Fu, and
  Wang]{DBLP:journals/corr/abs-1708-05123}
Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang.
\newblock Deep {\&} cross network for ad click predictions.
\newblock arXiv:1708.05123 (ADKDD'17), 2017.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Huang, and Hong]{S-prompts}
Yabin Wang, Zhiwu Huang, and Xiaopeng Hong.
\newblock {S-Prompts} learning with pre-trained transformers: {A}n {O}ccam’s
  razor for domain incremental learning.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~35,
  pages 5682--5695. Curran Associates, Inc., 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Zhang, Ebrahimi, Sun, Zhang, Lee,
  Ren, Su, Perot, Dy, and Pfister]{DualPrompt}
Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee,
  Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister.
\newblock Dual{P}rompt: {C}omplementary prompting for rehearsal-free continual
  learning.
\newblock In \emph{Computer Vision – ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23–27, 2022, Proceedings, Part XXVI}, page 631–648,
  Berlin, Heidelberg, 2022{\natexlab{b}}. Springer-Verlag.
\newblock ISBN 978-3-031-19808-3.

\bibitem[Wang et~al.(2022{\natexlab{c}})Wang, Zhang, Lee, Zhang, Sun, Ren, Su,
  Perot, Dy, and Pfister]{L2P}
Zifeng Wang, Zizhao Zhang, Chen{-}Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren,
  Guolong Su, Vincent Perot, Jennifer~G. Dy, and Tomas Pfister.
\newblock Learning to prompt for continual learning.
\newblock In \emph{Proc. {IEEE/CVF} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022}, pages
  139--149. {IEEE}, 2022{\natexlab{c}}.

\bibitem[Webb and Copsey(2011)]{Webb}
Andrew~R. Webb and Keith~D. Copsey.
\newblock \emph{Statistical Pattern Recognition}.
\newblock John Wiley \& Sons, Ltd, 2011.

\bibitem[Wu et~al.(2022)Wu, Swaminathan, Li, Ravichandran, Vasconcelos,
  Bhotika, and Soatto]{Wu_2022_CVPR}
Tz-Ying Wu, Gurumurthy Swaminathan, Zhizhong Li, Avinash Ravichandran, Nuno
  Vasconcelos, Rahul Bhotika, and Stefano Soatto.
\newblock Class-incremental learning with strong pre-trained models.
\newblock In \emph{Proc. IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 9601--9610, 2022.

\bibitem[Yan et~al.(2022)Yan, Gong, Liu, van~den Hengel, and
  Shi]{yan2022learning}
Qingsen Yan, Dong Gong, Yuhang Liu, Anton van~den Hengel, and Javen~Qinfeng
  Shi.
\newblock Learning {B}ayesian sparse networks with full experience replay for
  continual learning.
\newblock In \emph{Proc. IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pages 109--118, 2022.

\bibitem[Zeno et~al.(2019)Zeno, Golan, Hoffer, and Soudry]{zeno2019task}
Chen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry.
\newblock Task agnostic continual learning using online variational {B}ayes.
\newblock arXiv:1803.10123, 2019.

\bibitem[Zhai et~al.(2019)Zhai, Puigcerver, Kolesnikov, Ruyssen, Riquelme,
  Lucic, Djolonga, Pinto, Neumann, Dosovitskiy, Beyer, Bachem, Tschannen,
  Michalski, Bousquet, Gelly, and Houlsby]{vtab}
Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos
  Riquelme, Mario Lucic, Josip Djolonga, Andr{\'{e}}~Susano Pinto, Maxim
  Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen,
  Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby.
\newblock The visual task adaptation benchmark.
\newblock arXiv:1910.04867, 2019.

\bibitem[Zhang et~al.(2023)Zhang, Wang, Kang, Chen, and Wei]{zhang2023slca}
Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen, and Yunchao Wei.
\newblock {SLCA: S}low learner with classifier alignment for continual learning
  on a pre-trained model.
\newblock In \emph{Proc. IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pages 19148--19158, October 2023.

\bibitem[Zhang et~al.(2022)Zhang, Yin, Shao, and Liu]{OmniBenchmark}
Yuanhan Zhang, Zhenfei Yin, Jing Shao, and Ziwei Liu.
\newblock Benchmarking omni-vision representation through the lens of visual
  realms.
\newblock In Shai Avidan, Gabriel~J. Brostow, Moustapha Ciss{\'{e}},
  Giovanni~Maria Farinella, and Tal Hassner, editors, \emph{Computer Vision -
  {ECCV} 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27,
  2022, Proceedings, Part {VII}}, volume 13667 of \emph{Lecture Notes in
  Computer Science}, pages 594--611. Springer, 2022.

\bibitem[Zhou et~al.(2023)Zhou, Ye, Zhan, and Liu]{zhou2023revisiting}
Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu.
\newblock Revisiting class-incremental learning with pre-trained models:
  {G}eneralizability and adaptivity are all you need.
\newblock arXiv:2303.07338, Submitted 13 March, 2023.



\end{thebibliography}
