\begin{thebibliography}{88}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdullah et~al.(2022)Abdullah, Hassan, and Mustafa]{abdullah2022review}
Abdullah, A.~A., Hassan, M.~M., and Mustafa, Y.~T.
\newblock A review on bayesian deep learning in healthcare: Applications and challenges.
\newblock \emph{IEEE Access}, 10:\penalty0 36538--36562, 2022.

\bibitem[Abe et~al.(2022)Abe, Buchanan, Pleiss, Zemel, and Cunningham]{abe2022deep}
Abe, T., Buchanan, E.~K., Pleiss, G., Zemel, R., and Cunningham, J.~P.
\newblock Deep ensembles work, but are they necessary?
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 33646--33660, 2022.

\bibitem[Abe et~al.(2023)Abe, Buchanan, Pleiss, and Cunningham]{abe2023pathologies}
Abe, T., Buchanan, E.~K., Pleiss, G., and Cunningham, J.~P.
\newblock Pathologies of predictive diversity in deep ensembles.
\newblock \emph{arXiv preprint arXiv:2302.00704}, 2023.

\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and Man{\'e}]{amodei2016concrete}
Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Man{\'e}, D.
\newblock Concrete problems in ai safety.
\newblock \emph{arXiv preprint arXiv:1606.06565}, 2016.

\bibitem[Asuncion \& Newman(2007)Asuncion and Newman]{asuncion2007uci}
Asuncion, A. and Newman, D.
\newblock Uci machine learning repository, 2007.

\bibitem[Ayhan \& Berens(2022)Ayhan and Berens]{ayhan2022test}
Ayhan, M.~S. and Berens, P.
\newblock Test-time data augmentation for estimation of heteroscedastic aleatoric uncertainty in deep neural networks.
\newblock In \emph{Medical Imaging with Deep Learning}, 2022.

\bibitem[Baek \& Kr{\"o}ger(2023)Baek and Kr{\"o}ger]{baek2023safety}
Baek, W.-J. and Kr{\"o}ger, T.
\newblock Safety evaluation of robot systems via uncertainty quantification.
\newblock \emph{arXiv preprint arXiv:2302.10644}, 2023.

\bibitem[Begoli et~al.(2019)Begoli, Bhattacharya, and Kusnezov]{begoli2019need}
Begoli, E., Bhattacharya, T., and Kusnezov, D.
\newblock The need for uncertainty quantification in machine-assisted medical decision making.
\newblock \emph{Nature Machine Intelligence}, 1\penalty0 (1):\penalty0 20--23, 2019.

\bibitem[Brando et~al.(2022)Brando, Center, Rodriguez-Serrano, Vitri{\`a}, et~al.]{brando2022deep}
Brando, A., Center, B.~S., Rodriguez-Serrano, J., Vitri{\`a}, J., et~al.
\newblock Deep non-crossing quantiles through the partial derivative.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  7902--7914. PMLR, 2022.

\bibitem[Caflisch(1998)]{caflisch1998monte}
Caflisch, R.~E.
\newblock Monte carlo and quasi-monte carlo methods.
\newblock \emph{Acta numerica}, 7:\penalty0 1--49, 1998.

\bibitem[Chung et~al.(2021)Chung, Neiswanger, Char, and Schneider]{chung2021beyond}
Chung, Y., Neiswanger, W., Char, I., and Schneider, J.
\newblock Beyond pinball loss: Quantile methods for calibrated uncertainty quantification.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 10971--10984, 2021.

\bibitem[Daxberger et~al.(2021)Daxberger, Kristiadi, Immer, Eschenhagen, Bauer, and Hennig]{daxberger2021laplace}
Daxberger, E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., and Hennig, P.
\newblock Laplace redux-effortless bayesian deep learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 20089--20103, 2021.

\bibitem[Dewolf et~al.(2023)Dewolf, Baets, and Waegeman]{dewolf2023valid}
Dewolf, N., Baets, B.~D., and Waegeman, W.
\newblock Valid prediction intervals for regression problems.
\newblock \emph{Artificial Intelligence Review}, 56\penalty0 (1):\penalty0 577--613, 2023.

\bibitem[Dietterich(2017)]{dietterich2017steps}
Dietterich, T.~G.
\newblock Steps toward robust artificial intelligence.
\newblock \emph{Ai Magazine}, 38\penalty0 (3):\penalty0 3--24, 2017.

\bibitem[Dunsmore(1968)]{dunsmore1968bayesian}
Dunsmore, I.
\newblock A bayesian approach to calibration.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Methodological)}, 30\penalty0 (2):\penalty0 396--405, 1968.

\bibitem[Feldman et~al.(2021)Feldman, Bates, and Romano]{feldman2021improving}
Feldman, S., Bates, S., and Romano, Y.
\newblock Improving conditional coverage via orthogonal quantile regression.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 2060--2071, 2021.

\bibitem[Fitzenberger et~al.(2001)Fitzenberger, Koenker, and Machado]{fitzenberger2001economic}
Fitzenberger, B., Koenker, R., and Machado, J.~A.
\newblock \emph{Economic applications of quantile regression}.
\newblock Springer Science \& Business Media, 2001.

\bibitem[Fort et~al.(2019)Fort, Hu, and Lakshminarayanan]{fort2019deep}
Fort, S., Hu, H., and Lakshminarayanan, B.
\newblock Deep ensembles: A loss landscape perspective.
\newblock \emph{arXiv preprint arXiv:1912.02757}, 2019.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{gal2016dropout}
Gal, Y. and Ghahramani, Z.
\newblock Dropout as a bayesian approximation: Representing model uncertainty in deep learning.
\newblock In \emph{international conference on machine learning}, pp.\  1050--1059. PMLR, 2016.

\bibitem[Gasthaus et~al.(2019)Gasthaus, Benidis, Wang, Rangapuram, Salinas, Flunkert, and Januschowski]{gasthaus2019probabilistic}
Gasthaus, J., Benidis, K., Wang, Y., Rangapuram, S.~S., Salinas, D., Flunkert, V., and Januschowski, T.
\newblock Probabilistic forecasting with spline quantile function rnns.
\newblock In \emph{The 22nd international conference on artificial intelligence and statistics}, pp.\  1901--1910. PMLR, 2019.

\bibitem[Gawlikowski et~al.(2023)Gawlikowski, Tassi, Ali, Lee, Humt, Feng, Kruspe, Triebel, Jung, Roscher, et~al.]{gawlikowski2023survey}
Gawlikowski, J., Tassi, C. R.~N., Ali, M., Lee, J., Humt, M., Feng, J., Kruspe, A., Triebel, R., Jung, P., Roscher, R., et~al.
\newblock A survey of uncertainty in deep neural networks.
\newblock \emph{Artificial Intelligence Review}, pp.\  1--77, 2023.

\bibitem[Ghahramani(1996)]{ghahramani1996kin}
Ghahramani, Z.
\newblock The kin datasets, 1996.

\bibitem[Gneiting et~al.(2007)Gneiting, Balabdaoui, and Raftery]{gneiting2007probabilistic}
Gneiting, T., Balabdaoui, F., and Raftery, A.~E.
\newblock Probabilistic forecasts, calibration and sharpness.
\newblock \emph{Journal of the Royal Statistical Society Series B: Statistical Methodology}, 69\penalty0 (2):\penalty0 243--268, 2007.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and Courville]{goodfellow2016deep}
Goodfellow, I., Bengio, Y., and Courville, A.
\newblock \emph{Deep learning}.
\newblock MIT press, 2016.

\bibitem[Goodwin et~al.(2010)Goodwin, {\"O}nkal, and Thomson]{goodwin2010forecasts}
Goodwin, P., {\"O}nkal, D., and Thomson, M.
\newblock Do forecasts expressed as prediction intervals improve production planning decisions?
\newblock \emph{European Journal of Operational Research}, 205\penalty0 (1):\penalty0 195--201, 2010.

\bibitem[Graves(2011)]{graves2011practical}
Graves, A.
\newblock Practical variational inference for neural networks.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Greenfeld \& Shalit(2020)Greenfeld and Shalit]{greenfeld2020robust}
Greenfeld, D. and Shalit, U.
\newblock Robust learning with the hilbert-schmidt independence criterion.
\newblock In \emph{International Conference on Machine Learning}, pp.\  3759--3768. PMLR, 2020.

\bibitem[Grinsztajn et~al.(2022)Grinsztajn, Oyallon, and Varoquaux]{grinsztajn2022tree}
Grinsztajn, L., Oyallon, E., and Varoquaux, G.
\newblock Why do tree-based models still outperform deep learning on typical tabular data?
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 507--520, 2022.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock In \emph{International conference on machine learning}, pp.\  1321--1330. PMLR, 2017.

\bibitem[Hekler et~al.(2023)Hekler, Brinker, and Buettner]{hekler2023test}
Hekler, A., Brinker, T.~J., and Buettner, F.
\newblock Test time augmentation meets post-hoc calibration: uncertainty quantification under real-world conditions.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pp.\  14856--14864, 2023.

\bibitem[Hinton \& Van~Camp(1993)Hinton and Van~Camp]{hinton1993keeping}
Hinton, G.~E. and Van~Camp, D.
\newblock Keeping the neural networks simple by minimizing the description length of the weights.
\newblock In \emph{Proceedings of the sixth annual conference on Computational learning theory}, pp.\  5--13, 1993.

\bibitem[H{\"u}llermeier \& Waegeman(2021)H{\"u}llermeier and Waegeman]{hullermeier2021aleatoric}
H{\"u}llermeier, E. and Waegeman, W.
\newblock Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods.
\newblock \emph{Machine Learning}, 110:\penalty0 457--506, 2021.

\bibitem[Hunter \& Lange(2000)Hunter and Lange]{hunter2000quantile}
Hunter, D.~R. and Lange, K.
\newblock Quantile regression via an mm algorithm.
\newblock \emph{Journal of Computational and Graphical Statistics}, 9\penalty0 (1):\penalty0 60--77, 2000.

\bibitem[Hwang \& Ding(1997)Hwang and Ding]{hwang1997prediction}
Hwang, J.~G. and Ding, A.~A.
\newblock Prediction intervals for artificial neural networks.
\newblock \emph{Journal of the American Statistical Association}, 92\penalty0 (438):\penalty0 748--757, 1997.

\bibitem[Jeffares et~al.(2023)Jeffares, Liu, Crabb{\'e}, and van~der Schaar]{jeffares2023joint}
Jeffares, A., Liu, T., Crabb{\'e}, J., and van~der Schaar, M.
\newblock Joint training of deep ensembles fails due to learner collusion.
\newblock \emph{arXiv preprint arXiv:2301.11323}, 2023.

\bibitem[Kendall \& Gal(2017)Kendall and Gal]{kendall2017uncertainties}
Kendall, A. and Gal, Y.
\newblock What uncertainties do we need in bayesian deep learning for computer vision?
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Khosravi et~al.(2010)Khosravi, Nahavandi, Creighton, and Atiya]{khosravi2010lower}
Khosravi, A., Nahavandi, S., Creighton, D., and Atiya, A.~F.
\newblock Lower upper bound estimation method for construction of neural network-based prediction intervals.
\newblock \emph{IEEE transactions on neural networks}, 22\penalty0 (3):\penalty0 337--346, 2010.

\bibitem[Khosravi et~al.(2011)Khosravi, Nahavandi, Creighton, and Atiya]{khosravi2011comprehensive}
Khosravi, A., Nahavandi, S., Creighton, D., and Atiya, A.~F.
\newblock Comprehensive review of neural network-based prediction intervals and new advances.
\newblock \emph{IEEE Transactions on neural networks}, 22\penalty0 (9):\penalty0 1341--1356, 2011.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Koenker(2017)]{koenker2017quantile}
Koenker, R.
\newblock Quantile regression: 40 years on.
\newblock \emph{Annual review of economics}, 9:\penalty0 155--176, 2017.

\bibitem[Koenker \& Bassett~Jr(1978)Koenker and Bassett~Jr]{koenker1978regression}
Koenker, R. and Bassett~Jr, G.
\newblock Regression quantiles.
\newblock \emph{Econometrica: journal of the Econometric Society}, pp.\  33--50, 1978.

\bibitem[Koenker \& Hallock(2001)Koenker and Hallock]{koenker2001quantile}
Koenker, R. and Hallock, K.~F.
\newblock Quantile regression.
\newblock \emph{Journal of economic perspectives}, 15\penalty0 (4):\penalty0 143--156, 2001.

\bibitem[Kompa et~al.(2021)Kompa, Snoek, and Beam]{kompa2021second}
Kompa, B., Snoek, J., and Beam, A.~L.
\newblock Second opinion needed: communicating uncertainty in medical machine learning.
\newblock \emph{NPJ Digital Medicine}, 4\penalty0 (1):\penalty0 4, 2021.

\bibitem[Kuleshov et~al.(2018)Kuleshov, Fenner, and Ermon]{kuleshov2018accurate}
Kuleshov, V., Fenner, N., and Ermon, S.
\newblock Accurate uncertainties for deep learning using calibrated regression.
\newblock In \emph{International conference on machine learning}, pp.\  2796--2804. PMLR, 2018.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and Blundell]{lakshminarayanan2017simple}
Lakshminarayanan, B., Pritzel, A., and Blundell, C.
\newblock Simple and scalable predictive uncertainty estimation using deep ensembles.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Meinshausen \& Ridgeway(2006)Meinshausen and Ridgeway]{meinshausen2006quantile}
Meinshausen, N. and Ridgeway, G.
\newblock Quantile regression forests.
\newblock \emph{Journal of machine learning research}, 7\penalty0 (6), 2006.

\bibitem[Nix \& Weigend(1994)Nix and Weigend]{nix1994estimating}
Nix, D.~A. and Weigend, A.~S.
\newblock Estimating the mean and variance of the target probability distribution.
\newblock In \emph{Proceedings of 1994 ieee international conference on neural networks (ICNN'94)}, volume~1, pp.\  55--60. IEEE, 1994.

\bibitem[Osawa et~al.(2019)Osawa, Swaroop, Khan, Jain, Eschenhagen, Turner, and Yokota]{osawa2019practical}
Osawa, K., Swaroop, S., Khan, M. E.~E., Jain, A., Eschenhagen, R., Turner, R.~E., and Yokota, R.
\newblock Practical deep learning with bayesian principles.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Papadopoulos(2008)]{papadopoulos2008inductive}
Papadopoulos, H.
\newblock Inductive conformal prediction: Theory and application to neural networks.
\newblock In \emph{Tools in artificial intelligence}. Citeseer, 2008.

\bibitem[Park et~al.(2022)Park, Maddix, Aubet, Kan, Gasthaus, and Wang]{park2022learning}
Park, Y., Maddix, D., Aubet, F.-X., Kan, K., Gasthaus, J., and Wang, Y.
\newblock Learning quantile functions without quantile crossing for distribution-free time series forecasting.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  8127--8150. PMLR, 2022.

\bibitem[Pearce et~al.(2018)Pearce, Brintrup, Zaki, and Neely]{pearce2018high}
Pearce, T., Brintrup, A., Zaki, M., and Neely, A.
\newblock High-quality prediction intervals for deep learning: A distribution-free, ensembled approach.
\newblock In \emph{International conference on machine learning}, pp.\  4075--4084. PMLR, 2018.

\bibitem[Pereyra et~al.(2017)Pereyra, Tucker, Chorowski, Kaiser, and Hinton]{pereyra2017regularizing}
Pereyra, G., Tucker, G., Chorowski, J., Kaiser, L., and Hinton, G.
\newblock Regularizing neural networks by penalizing confident output distributions, 2017.
\newblock URL \url{https://openreview.net/forum?id=HkCjNI5ex}.

\bibitem[Platt et~al.(1999)]{platt1999probabilistic}
Platt, J. et~al.
\newblock Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.
\newblock \emph{Advances in large margin classifiers}, 10\penalty0 (3):\penalty0 61--74, 1999.

\bibitem[Rahaman et~al.(2021)]{rahaman2021uncertainty}
Rahaman, R. et~al.
\newblock Uncertainty quantification and deep ensembles.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 20063--20075, 2021.

\bibitem[Rasmussen et~al.(2006)Rasmussen, Williams, et~al.]{rasmussen2006gaussian}
Rasmussen, C.~E., Williams, C.~K., et~al.
\newblock \emph{Gaussian processes for machine learning}, volume~1.
\newblock Springer, 2006.

\bibitem[Romano et~al.(2019)Romano, Patterson, and Candes]{romano2019conformalized}
Romano, Y., Patterson, E., and Candes, E.
\newblock Conformalized quantile regression.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Sagi \& Rokach(2018)Sagi and Rokach]{sagi2018ensemble}
Sagi, O. and Rokach, L.
\newblock Ensemble learning: A survey.
\newblock \emph{Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery}, 8\penalty0 (4):\penalty0 e1249, 2018.

\bibitem[Savelli \& Joslyn(2013)Savelli and Joslyn]{savelli2013advantages}
Savelli, S. and Joslyn, S.
\newblock The advantages of predictive interval forecasts for non-expert users and the impact of visualizations.
\newblock \emph{Applied Cognitive Psychology}, 27\penalty0 (4):\penalty0 527--541, 2013.

\bibitem[Seber \& Lee(2003)Seber and Lee]{seber2003linear}
Seber, G.~A. and Lee, A.~J.
\newblock \emph{Linear regression analysis}, volume 330.
\newblock John Wiley \& Sons, 2003.

\bibitem[Seedat et~al.(2023)Seedat, Jeffares, Imrie, and van~der Schaar]{seedat2023improving}
Seedat, N., Jeffares, A., Imrie, F., and van~der Schaar, M.
\newblock Improving adaptive conformal prediction using self-supervised learning.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  10160--10177. PMLR, 2023.

\bibitem[Sesia \& Cand{\`e}s(2020)Sesia and Cand{\`e}s]{sesia2020comparison}
Sesia, M. and Cand{\`e}s, E.~J.
\newblock A comparison of some conformal quantile regression methods.
\newblock \emph{Stat}, 9\penalty0 (1):\penalty0 e261, 2020.

\bibitem[Sesia \& Romano(2021)Sesia and Romano]{sesia2021conformal}
Sesia, M. and Romano, Y.
\newblock Conformal prediction using conditional histograms.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 6304--6315, 2021.

\bibitem[Skafte et~al.(2019)Skafte, J{\o}rgensen, and Hauberg]{skafte2019reliable}
Skafte, N., J{\o}rgensen, M., and Hauberg, S.
\newblock Reliable training and estimation of variance networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Steinwart \& Christmann(2011)Steinwart and Christmann]{steinwart2011estimating}
Steinwart, I. and Christmann, A.
\newblock Estimating conditional quantiles with the help of the pinball loss.
\newblock 2011.

\bibitem[Su et~al.(2023)Su, Li, He, Han, Feng, Ding, and Miao]{su2023uncertainty}
Su, S., Li, Y., He, S., Han, S., Feng, C., Ding, C., and Miao, F.
\newblock Uncertainty quantification of collaborative detection for self-driving.
\newblock In \emph{2023 IEEE International Conference on Robotics and Automation (ICRA)}, pp.\  5588--5594. IEEE, 2023.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and Wojna]{szegedy2016rethinking}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  2818--2826, 2016.

\bibitem[Tagasovska \& Lopez-Paz(2019)Tagasovska and Lopez-Paz]{tagasovska2019single}
Tagasovska, N. and Lopez-Paz, D.
\newblock Single-model uncertainties for deep learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Takeuchi et~al.(2006)Takeuchi, Le, Sears, Smola, et~al.]{takeuchi2006nonparametric}
Takeuchi, I., Le, Q., Sears, T., Smola, A., et~al.
\newblock Nonparametric quantile estimation.
\newblock 2006.

\bibitem[Tierney \& Kadane(1986)Tierney and Kadane]{tierney1986accurate}
Tierney, L. and Kadane, J.~B.
\newblock Accurate approximations for posterior moments and marginal densities.
\newblock \emph{Journal of the american statistical association}, 81\penalty0 (393):\penalty0 82--86, 1986.

\bibitem[Tomani et~al.(2021)Tomani, Gruber, Erdem, Cremers, and Buettner]{tomani2021post}
Tomani, C., Gruber, S., Erdem, M.~E., Cremers, D., and Buettner, F.
\newblock Post-hoc uncertainty calibration for domain drift scenarios.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  10124--10132, 2021.

\bibitem[Vapnik(2006)]{vapnik2006estimation}
Vapnik, V.
\newblock \emph{Estimation of dependences based on empirical data}.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Vovk et~al.(2005)Vovk, Gammerman, and Shafer]{vovk2005algorithmic}
Vovk, V., Gammerman, A., and Shafer, G.
\newblock \emph{Algorithmic learning in a random world}, volume~29.
\newblock Springer, 2005.

\bibitem[Wang et~al.(2022)Wang, Xu, Zou, Zhang, and Zhang]{wang2022deep}
Wang, Y., Xu, H., Zou, R., Zhang, L., and Zhang, F.
\newblock A deep asymmetric laplace neural network for deterministic and probabilistic wind power forecasting.
\newblock \emph{Renewable Energy}, 196:\penalty0 497--517, 2022.

\bibitem[Wen et~al.(2019)Wen, Tran, and Ba]{wen2019batchensemble}
Wen, Y., Tran, D., and Ba, J.
\newblock Batchensemble: an alternative approach to efficient ensemble and lifelong learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Wilson(2020)]{wilson2020case}
Wilson, A.~G.
\newblock The case for bayesian deep learning.
\newblock \emph{arXiv preprint arXiv:2001.10995}, 2020.

\bibitem[Wilson \& Izmailov(2020)Wilson and Izmailov]{wilson2020bayesian}
Wilson, A.~G. and Izmailov, P.
\newblock Bayesian deep learning and a probabilistic perspective of generalization.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 4697--4708, 2020.

\bibitem[Wilson et~al.(2016{\natexlab{a}})Wilson, Hu, Salakhutdinov, and Xing]{wilson2016deep}
Wilson, A.~G., Hu, Z., Salakhutdinov, R., and Xing, E.~P.
\newblock Deep kernel learning.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  370--378. PMLR, 2016{\natexlab{a}}.

\bibitem[Wilson et~al.(2016{\natexlab{b}})Wilson, Hu, Salakhutdinov, and Xing]{wilson2016stochastic}
Wilson, A.~G., Hu, Z., Salakhutdinov, R.~R., and Xing, E.~P.
\newblock Stochastic variational deep kernel learning.
\newblock \emph{Advances in neural information processing systems}, 29, 2016{\natexlab{b}}.

\bibitem[Winkler(1972)]{winkler1972decision}
Winkler, R.~L.
\newblock A decision-theoretic approach to interval estimation.
\newblock \emph{Journal of the American Statistical Association}, 67\penalty0 (337):\penalty0 187--191, 1972.

\bibitem[Xie et~al.(2016)Xie, Wang, Wei, Wang, and Tian]{xie2016disturblabel}
Xie, L., Wang, J., Wei, Z., Wang, M., and Tian, Q.
\newblock Disturblabel: Regularizing cnn on the loss layer.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  4753--4762, 2016.

\bibitem[Yao et~al.(2019)Yao, Pan, Ghosh, and Doshi-Velez]{yao2019quality}
Yao, J., Pan, W., Ghosh, S., and Doshi-Velez, F.
\newblock Quality of uncertainty quantification for bayesian neural network inference.
\newblock \emph{arXiv preprint arXiv:1906.09686}, 2019.

\bibitem[Yu \& Jones(1998)Yu and Jones]{yu1998local}
Yu, K. and Jones, M.
\newblock Local linear quantile regression.
\newblock \emph{Journal of the American statistical Association}, 93\penalty0 (441):\penalty0 228--237, 1998.

\bibitem[Yu et~al.(2003)Yu, Lu, and Stander]{yu2003quantile}
Yu, K., Lu, Z., and Stander, J.
\newblock Quantile regression: applications and current research areas.
\newblock \emph{Journal of the Royal Statistical Society Series D: The Statistician}, 52\penalty0 (3):\penalty0 331--350, 2003.

\bibitem[Zadrozny \& Elkan(2001)Zadrozny and Elkan]{zadrozny2001obtaining}
Zadrozny, B. and Elkan, C.
\newblock Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.
\newblock In \emph{Icml}, volume~1, pp.\  609--616, 2001.

\bibitem[Zadrozny \& Elkan(2002)Zadrozny and Elkan]{zadrozny2002transforming}
Zadrozny, B. and Elkan, C.
\newblock Transforming classifier scores into accurate multiclass probability estimates.
\newblock In \emph{Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining}, pp.\  694--699, 2002.

\bibitem[Zhang et~al.(2018)Zhang, B{\"u}tepage, Kjellstr{\"o}m, and Mandt]{zhang2018advances}
Zhang, C., B{\"u}tepage, J., Kjellstr{\"o}m, H., and Mandt, S.
\newblock Advances in variational inference.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 41\penalty0 (8):\penalty0 2008--2026, 2018.

\bibitem[Zhang et~al.(2023)Zhang, Wen, and Wu]{zhang2023contextual}
Zhang, Y., Wen, H., and Wu, Q.
\newblock A contextual bandit approach for value-oriented prediction interval forecasting.
\newblock \emph{IEEE Transactions on Smart Grid}, 2023.

\bibitem[Zhao et~al.(2020)Zhao, Ma, and Ermon]{zhao2020individual}
Zhao, S., Ma, T., and Ermon, S.
\newblock Individual calibration with randomized forecasting.
\newblock In \emph{International Conference on Machine Learning}, pp.\  11387--11397. PMLR, 2020.

\end{thebibliography}
