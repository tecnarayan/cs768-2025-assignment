\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adams et~al.(2017)Adams, Jeffrey, Julia, Lucas, Mark, Nithum, and Will]{jigsaw-toxic-comment-classification-challenge}
Adams, C., Jeffrey, S., Julia, E., Lucas, D., Mark, M., Nithum, and Will, C.
\newblock Toxic comment classification challenge, 2017.

\bibitem[Aghajanyan et~al.(2020{\natexlab{a}})Aghajanyan, Shrivastava, Gupta, Goyal, Zettlemoyer, and Gupta]{aghajanyan2020better}
Aghajanyan, A., Shrivastava, A., Gupta, A., Goyal, N., Zettlemoyer, L., and Gupta, S.
\newblock Better fine-tuning by reducing representational collapse.
\newblock \emph{arXiv preprint arXiv:2008.03156}, 2020{\natexlab{a}}.

\bibitem[Aghajanyan et~al.(2020{\natexlab{b}})Aghajanyan, Zettlemoyer, and Gupta]{aghajanyan2020intrinsic}
Aghajanyan, A., Zettlemoyer, L., and Gupta, S.
\newblock Intrinsic dimensionality explains the effectiveness of language model fine-tuning.
\newblock \emph{arXiv preprint arXiv:2012.13255}, 2020{\natexlab{b}}.

\bibitem[Balestriero \& Baraniuk(2018)Balestriero and Baraniuk]{balestriero2018hard}
Balestriero, R. and Baraniuk, R.~G.
\newblock From hard to soft: Understanding deep network nonlinearities via vector quantization and statistical inference.
\newblock \emph{arXiv preprint arXiv:1810.09274}, 2018.

\bibitem[Balestriero \& Baraniuk(2020)Balestriero and Baraniuk]{balestriero2020mad}
Balestriero, R. and Baraniuk, R.~G.
\newblock Mad max: Affine spline insights into deep learning.
\newblock \emph{Proceedings of the IEEE}, 109\penalty0 (5):\penalty0 704--727, 2020.

\bibitem[Balestriero et~al.(2019)Balestriero, Cosentino, Aazhang, and Baraniuk]{balestriero2019geometry}
Balestriero, R., Cosentino, R., Aazhang, B., and Baraniuk, R.
\newblock The geometry of deep networks: Power diagram subdivision.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Balestriero et~al.(2018)]{balestriero2018spline}
Balestriero, R. et~al.
\newblock A spline theory of deep learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  374--383. PMLR, 2018.

\bibitem[Belinkov(2022)]{belinkov2022probing}
Belinkov, Y.
\newblock Probing classifiers: Promises, shortcomings, and advances.
\newblock \emph{Computational Linguistics}, 48\penalty0 (1):\penalty0 207--219, 2022.

\bibitem[Bennett(1969)]{bennett1969intrinsic}
Bennett, R.
\newblock The intrinsic dimensionality of signal collections.
\newblock \emph{IEEE Transactions on Information Theory}, 15\penalty0 (5):\penalty0 517--525, 1969.

\bibitem[Boix-Adsera et~al.(2023)Boix-Adsera, Littwin, Abbe, Bengio, and Susskind]{boix2023transformers}
Boix-Adsera, E., Littwin, E., Abbe, E., Bengio, S., and Susskind, J.
\newblock Transformers learn through gradual rank increase.
\newblock \emph{arXiv preprint arXiv:2306.07042}, 2023.

\bibitem[Bourgeade et~al.(2023)Bourgeade, Chiril, Benamara, and Moriceau]{bourgeade2023did}
Bourgeade, T., Chiril, P., Benamara, F., and Moriceau, V.
\newblock What did you learn to hate? a topic-oriented analysis of generalization in hate speech detection.
\newblock In \emph{Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics}, pp.\  3477--3490, 2023.

\bibitem[Bradley(1997)]{bradley1997use}
Bradley, A.~P.
\newblock The use of the area under the roc curve in the evaluation of machine learning algorithms.
\newblock \emph{Pattern recognition}, 30\penalty0 (7):\penalty0 1145--1159, 1997.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Burns et~al.(2022)Burns, Ye, Klein, and Steinhardt]{burns2022discovering}
Burns, C., Ye, H., Klein, D., and Steinhardt, J.
\newblock Discovering latent knowledge in language models without supervision.
\newblock \emph{arXiv preprint arXiv:2212.03827}, 2022.

\bibitem[Campadelli et~al.(2015)Campadelli, Casiraghi, Ceruti, and Rozza]{campadelli2015intrinsic}
Campadelli, P., Casiraghi, E., Ceruti, C., and Rozza, A.
\newblock Intrinsic dimension estimation: Relevant techniques and a benchmark framework.
\newblock \emph{Mathematical Problems in Engineering}, 2015.

\bibitem[Caselli et~al.(2020)Caselli, Basile, Mitrovi{\'c}, and Granitzer]{caselli2020hatebert}
Caselli, T., Basile, V., Mitrovi{\'c}, J., and Granitzer, M.
\newblock Hatebert: Retraining bert for abusive language detection in english.
\newblock \emph{arXiv preprint arXiv:2010.12472}, 2020.

\bibitem[Chen et~al.(2020)Chen, Frankle, Chang, Liu, Zhang, Wang, and Carbin]{chen2020lottery}
Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang, Z., and Carbin, M.
\newblock The lottery ticket hypothesis for pre-trained bert networks.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 15834--15846, 2020.

\bibitem[Chughtai et~al.(2023)Chughtai, Chan, and Nanda]{chughtai2023toy}
Chughtai, B., Chan, L., and Nanda, N.
\newblock A toy model of universality: Reverse engineering how networks learn group operations.
\newblock \emph{arXiv preprint arXiv:2302.03025}, 2023.

\bibitem[Conover et~al.(2023)Conover, Hayes, Mathur, Xie, Wan, Shah, Ghodsi, Wendell, Zaharia, and Xin]{DatabricksBlog2023DollyV2}
Conover, M., Hayes, M., Mathur, A., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M., and Xin, R.
\newblock Free dolly: Introducing the world's first truly open instruction-tuned llm.
\newblock \emph{online}, 2023.

\bibitem[Cosentino et~al.(2022)Cosentino, Sengupta, Avestimehr, Soltanolkotabi, Ortega, Willke, and Tepper]{cosentino2022toward}
Cosentino, R., Sengupta, A., Avestimehr, S., Soltanolkotabi, M., Ortega, A., Willke, T., and Tepper, M.
\newblock Toward a geometrical understanding of self-supervised contrastive learning.
\newblock \emph{arXiv preprint arXiv:2205.06926}, 2022.

\bibitem[Dar et~al.(2022)Dar, Geva, Gupta, and Berant]{dar2022analyzing}
Dar, G., Geva, M., Gupta, A., and Berant, J.
\newblock Analyzing transformers in embedding space.
\newblock \emph{arXiv preprint arXiv:2209.02535}, 2022.

\bibitem[Dong et~al.(2021)Dong, Cordonnier, and Loukas]{dong2021attention}
Dong, Y., Cordonnier, J.-B., and Loukas, A.
\newblock Attention is not all you need: Pure attention loses rank doubly exponentially with depth.
\newblock In \emph{International Conference on Machine Learning}, pp.\  2793--2803. PMLR, 2021.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann, Askell, Bai, Chen, Conerly, et~al.]{elhage2021mathematical}
Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., et~al.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 1, 2021.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, et~al.]{gao2020pile}
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et~al.
\newblock The {P}ile: An 800{GB} dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Hartvigsen et~al.(2022)Hartvigsen, Gabriel, Palangi, Sap, Ray, and Kamar]{hartvigsen2022toxigen}
Hartvigsen, T., Gabriel, S., Palangi, H., Sap, M., Ray, D., and Kamar, E.
\newblock {T}oxi{G}en: A large-scale machine-generated dataset for adversarial and implicit hate speech detection.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association of Computational Linguistics}, 2022.

\bibitem[Hernandez \& Andreas(2021)Hernandez and Andreas]{hernandez2021low}
Hernandez, E. and Andreas, J.
\newblock The low-dimensional linear geometry of contextualized word representations.
\newblock \emph{arXiv preprint arXiv:2105.07109}, 2021.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d.~L., Hendricks, L.~A., Welbl, J., Clark, A., et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Humayun et~al.(2022{\natexlab{a}})Humayun, Balestriero, and Baraniuk]{humayun2022magnet}
Humayun, A.~I., Balestriero, R., and Baraniuk, R.
\newblock Ma{GNET}: Uniform sampling from deep generative network manifolds without retraining.
\newblock In \emph{International Conference on Learning Representations}, 2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=r5qumLiYwf9}.

\bibitem[Humayun et~al.(2022{\natexlab{b}})Humayun, Balestriero, and Baraniuk]{humayun2022polarity}
Humayun, A.~I., Balestriero, R., and Baraniuk, R.
\newblock Polarity sampling: Quality and diversity control of pre-trained generative networks via singular values.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  10641--10650, 2022{\natexlab{b}}.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Jiang, A.~Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.~S., Casas, D. d.~l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Jing et~al.(2021)Jing, Vincent, LeCun, and Tian]{jing2021understanding}
Jing, L., Vincent, P., LeCun, Y., and Tian, Y.
\newblock Understanding dimensional collapse in contrastive self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2110.09348}, 2021.

\bibitem[Kim et~al.(2022)Kim, Park, and Han]{kim2022generalizable}
Kim, Y., Park, S., and Han, Y.-S.
\newblock Generalizable implicit hate speech detection using contrastive learning.
\newblock In \emph{Proceedings of the 29th International Conference on Computational Linguistics}, pp.\  6667--6679, 2022.

\bibitem[Mathew et~al.(2021)Mathew, Saha, Yimam, Biemann, Goyal, and Mukherjee]{mathew2021hatexplain}
Mathew, B., Saha, P., Yimam, S.~M., Biemann, C., Goyal, P., and Mukherjee, A.
\newblock Hatexplain: A benchmark dataset for explainable hate speech detection.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~35, pp.\  14867--14875, 2021.

\bibitem[Noci et~al.(2022)Noci, Anagnostidis, Biggio, Orvieto, Singh, and Lucchi]{noci2022signal}
Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S.~P., and Lucchi, A.
\newblock Signal propagation in transformers: Theoretical perspectives and the role of rank collapse.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27198--27211, 2022.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Pope et~al.(2021)Pope, Zhu, Abdelkader, Goldblum, and Goldstein]{pope2021intrinsic}
Pope, P., Zhu, C., Abdelkader, A., Goldblum, M., and Goldstein, T.
\newblock The intrinsic dimension of images and its impact on learning.
\newblock \emph{arXiv preprint arXiv:2104.08894}, 2021.

\bibitem[Ravichander et~al.(2020)Ravichander, Belinkov, and Hovy]{ravichander2020probing}
Ravichander, A., Belinkov, Y., and Hovy, E.
\newblock Probing the probing paradigm: Does probing accuracy entail task relevance?
\newblock \emph{arXiv preprint arXiv:2005.00719}, 2020.

\bibitem[Shekkizhar \& Ortega(2020)Shekkizhar and Ortega]{shekkizhar2020graph}
Shekkizhar, S. and Ortega, A.
\newblock Graph construction from data by non-negative kernel regression.
\newblock In \emph{Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP)}, pp.\  3892--3896. IEEE, 2020.

\bibitem[Song \& Zhong(2023)Song and Zhong]{song2023uncovering}
Song, J. and Zhong, Y.
\newblock Uncovering hidden geometry in transformers via disentangling position and context.
\newblock \emph{arXiv preprint arXiv:2310.04861}, 2023.

\bibitem[Su et~al.(2023)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2023roformer}
Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, pp.\  127063, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Trockman \& Kolter(2023)Trockman and Kolter]{trockman2023mimetic}
Trockman, A. and Kolter, J.~Z.
\newblock Mimetic initialization of self-attention layers.
\newblock \emph{arXiv preprint arXiv:2305.09828}, 2023.

\bibitem[Van~Aken et~al.(2018)Van~Aken, Risch, Krestel, and L{\"o}ser]{van2018challenges}
Van~Aken, B., Risch, J., Krestel, R., and L{\"o}ser, A.
\newblock Challenges for toxic comment classification: An in-depth error analysis.
\newblock \emph{arXiv preprint arXiv:1809.07572}, 2018.

\bibitem[Varadhan \& Manocha(2004)Varadhan and Manocha]{varadhan2004accurate}
Varadhan, G. and Manocha, D.
\newblock Accurate minkowski sum approximation of polyhedral models.
\newblock In \emph{12th Pacific Conference on Computer Graphics and Applications, 2004. PG 2004. Proceedings.}, pp.\  392--401. IEEE, 2004.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2022)Wang, Variengien, Conmy, Shlegeris, and Steinhardt]{wang2022interpretability}
Wang, K., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J.
\newblock Interpretability in the wild: a circuit for indirect object identification in gpt-2 small.
\newblock \emph{arXiv preprint arXiv:2211.00593}, 2022.

\bibitem[Wang \& Chang(2022)Wang and Chang]{wang2022toxicity}
Wang, Y.-S. and Chang, Y.
\newblock Toxicity detection with generative prompt-based inference.
\newblock \emph{arXiv preprint arXiv:2205.12390}, 2022.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.~V., Zhou, D., et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Wu, Xu, Cao, Du, and Psounis]{zhang2023efficient}
Zhang, J., Wu, Q., Xu, Y., Cao, C., Du, Z., and Psounis, K.
\newblock Efficient toxic content detection by bootstrapping and distilling large language models.
\newblock \emph{arXiv preprint arXiv:2312.08303}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Luo, Chuang, Fang, Gaitskell, Hartvigsen, Wu, Fox, Meng, and Glass]{zhang2023interpretable}
Zhang, T., Luo, H., Chuang, Y.-S., Fang, W., Gaitskell, L., Hartvigsen, T., Wu, X., Fox, D., Meng, H., and Glass, J.
\newblock Interpretable unified language checking.
\newblock \emph{arXiv preprint arXiv:2304.03728}, 2023{\natexlab{b}}.

\bibitem[Zhao et~al.(2023)Zhao, Chen, Yang, Liu, Deng, Cai, Wang, Yin, and Du]{zhao2023explainability}
Zhao, H., Chen, H., Yang, F., Liu, N., Deng, H., Cai, H., Wang, S., Yin, D., and Du, M.
\newblock Explainability for large language models: A survey.
\newblock \emph{arXiv preprint arXiv:2309.01029}, 2023.

\bibitem[Zhou(2021)]{zhou2021challenges}
Zhou, X.
\newblock \emph{Challenges in automated debiasing for toxic language detection}.
\newblock University of Washington, 2021.

\end{thebibliography}
