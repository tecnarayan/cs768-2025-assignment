\begin{thebibliography}{10}

\bibitem{banner2019post}
Ron Banner, Yury Nahshan, and Daniel Soudry.
\newblock Post training 4-bit quantization of convolutional networks for
  rapid-deployment.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{bengio2013estimating}
Yoshua Bengio, Nicholas L{\'e}onard, and Aaron Courville.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock {\em arXiv preprint arXiv:1308.3432}, 2013.

\bibitem{cai2017deep}
Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos.
\newblock Deep learning with low precision by half-wave gaussian quantization.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 5918--5926, 2017.

\bibitem{chen2020addernet}
Hanting Chen, Yunhe Wang, Chunjing Xu, Boxin Shi, Chao Xu, Qi~Tian, and Chang
  Xu.
\newblock Addernet: Do we really need multiplications in deep learning?
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 1468--1477, 2020.

\bibitem{chen2021empirical}
Xinghao Chen, Chang Xu, Minjing Dong, Chunjing Xu, and Yunhe Wang.
\newblock An empirical study of adder neural networks for object detection.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{choi2018pact}
Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang,
  Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan.
\newblock Pact: Parameterized clipping activation for quantized neural
  networks.
\newblock {\em arXiv preprint arXiv:1805.06085}, 2018.

\bibitem{choi2021qimera}
Kanghyun Choi, Deokki Hong, Noseong Park, Youngsok Kim, and Jinho Lee.
\newblock Qimera: Data-free quantization with synthetic boundary supporting
  samples.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{courbariaux2015binaryconnect}
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock In {\em Advances in neural information processing systems}, pages
  3123--3131, 2015.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{dong2020hawq}
Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael~W Mahoney, and
  Kurt Keutzer.
\newblock Hawq-v2: Hessian aware trace-weighted quantization of neural
  networks.
\newblock {\em Advances in neural information processing systems},
  33:18518--18529, 2020.

\bibitem{esser2019learned}
Steven~K Esser, Jeffrey~L McKinstry, Deepika Bablani, Rathinakumar Appuswamy,
  and Dharmendra~S Modha.
\newblock Learned step size quantization.
\newblock {\em arXiv preprint arXiv:1902.08153}, 2019.

\bibitem{deepcompression}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock In {\em ICLR}, 2016.

\bibitem{hartigan1979algorithm}
John~A Hartigan and Manchek~A Wong.
\newblock Algorithm as 136: A k-means clustering algorithm.
\newblock {\em Journal of the royal statistical society. series c (applied
  statistics)}, 28(1):100--108, 1979.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{horowitz20141}
Mark Horowitz.
\newblock 1.1 computing's energy problem (and what we can do about it).
\newblock In {\em 2014 IEEE International Solid-State Circuits Conference
  Digest of Technical Papers (ISSCC)}, pages 10--14. IEEE, 2014.

\bibitem{howard2017mobilenets}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{mindspore}
Huawei.
\newblock Mindspore.
\newblock \url{https://www.mindspore.cn/}, 2020.

\bibitem{hubara2016binarized}
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
  Bengio.
\newblock Binarized neural networks.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{jacob2018quantization}
Benoit Jacob, Skirmantas Kligys, Bo~Chen, Menglong Zhu, Matthew Tang, Andrew
  Howard, Hartwig Adam, and Dmitry Kalenichenko.
\newblock Quantization and training of neural networks for efficient
  integer-arithmetic-only inference.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2704--2713, 2018.

\bibitem{krishnamoorthi2018quantizing}
Raghuraman Krishnamoorthi.
\newblock Quantizing deep convolutional networks for efficient inference: A
  whitepaper.
\newblock {\em arXiv preprint arXiv:1806.08342}, 2018.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{lee2021network}
Junghyup Lee, Dohyung Kim, and Bumsub Ham.
\newblock Network quantization with element-wise gradient scaling.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 6448--6457, 2021.

\bibitem{li2021winograd}
Wenshuo Li, Hanting Chen, Mingqiang Huang, Xinghao Chen, Chunjing Xu, and Yunhe
  Wang.
\newblock Winograd algorithm for addernet.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, pages 6307--6315, 2021.

\bibitem{li2021brecq}
Yuhang Li, Ruihao Gong, Xu~Tan, Yang Yang, Peng Hu, Qi~Zhang, Fengwei Yu, Wei
  Wang, and Shi Gu.
\newblock Brecq: Pushing the limit of post-training quantization by block
  reconstruction.
\newblock {\em arXiv preprint arXiv:2102.05426}, 2021.

\bibitem{lin2020rotated}
Mingbao Lin, Rongrong Ji, Zihan Xu, Baochang Zhang, Yan Wang, Yongjian Wu,
  Feiyue Huang, and Chia-Wen Lin.
\newblock Rotated binary neural network.
\newblock {\em Advances in neural information processing systems},
  33:7474--7485, 2020.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em European conference on computer vision}, pages 740--755.
  Springer, 2014.

\bibitem{liu2020reactnet}
Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng.
\newblock Reactnet: Towards precise binary neural network with generalized
  activation functions.
\newblock In {\em European Conference on Computer Vision}, pages 143--159.
  Springer, 2020.

\bibitem{nagel2020up}
Markus Nagel, Rana~Ali Amjad, Mart Van~Baalen, Christos Louizos, and Tijmen
  Blankevoort.
\newblock Up or down? adaptive rounding for post-training quantization.
\newblock In {\em International Conference on Machine Learning}, pages
  7197--7206. PMLR, 2020.

\bibitem{nie2021ghostsr}
Ying Nie, Kai Han, Zhenhua Liu, An~Xiao, Yiping Deng, Chunjing Xu, and Yunhe
  Wang.
\newblock Ghostsr: Learning ghost features for efficient image
  super-resolution.
\newblock {\em arXiv preprint arXiv:2101.08525}, 2021.

\bibitem{niemulti}
Ying Nie, Kai Han, and Yunhe Wang.
\newblock Multi-bit adaptive distillation for binary neural networks.

\bibitem{rastegari2016xnor}
M.~Rastegari, V.~Ordonez, J.~Redmon, and A.~Farhadi.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In {\em European Conference on Computer Vision}, pages 525--542.
  Springer, 2016.

\bibitem{shomron2021post}
Gil Shomron, Freddy Gabbay, Samer Kurzum, and Uri Weiser.
\newblock Post-training sparsity-aware quantization.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{shu2021adder}
Han Shu, Jiahao Wang, Hanting Chen, Lin Li, Yujiu Yang, and Yunhe Wang.
\newblock Adder attention for vision transformer.
\newblock {\em Advances in Neural Information Processing Systems},
  34:19899--19909, 2021.

\bibitem{sze2017efficient}
Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel~S Emer.
\newblock Efficient processing of deep neural networks: A tutorial and survey.
\newblock {\em Proceedings of the IEEE}, 105(12):2295--2329, 2017.

\bibitem{tian2019fcos}
Zhi Tian, Chunhua Shen, Hao Chen, and Tong He.
\newblock Fcos: Fully convolutional one-stage object detection.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 9627--9636, 2019.

\bibitem{wang2021addernet}
Yunhe Wang, Mingqiang Huang, Kai Han, Hanting Chen, Wei Zhang, Chunjing Xu, and
  Dacheng Tao.
\newblock Addernet and its minimalist hardware design for energy-efficient
  artificial intelligence.
\newblock {\em arXiv preprint arXiv:2101.10015}, 2021.

\bibitem{wei2022qdrop}
Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei Yu.
\newblock Qdrop: Randomly dropping quantization for extremely low-bit
  post-training quantization.
\newblock {\em arXiv preprint arXiv:2203.05740}, 2022.

\bibitem{xu2021learning}
Yixing Xu, Kai Han, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe Wang.
\newblock Learning frequency domain approximation for binary neural networks.
\newblock {\em Advances in Neural Information Processing Systems},
  34:25553--25565, 2021.

\bibitem{xu2019positive}
Yixing Xu, Yunhe Wang, Hanting Chen, Kai Han, Chunjing Xu, Dacheng Tao, and
  Chang Xu.
\newblock Positive-unlabeled compression on the cloud.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{xu2020kernel}
Yixing Xu, Chang Xu, Xinghao Chen, Wei Zhang, Chunjing Xu, and Yunhe Wang.
\newblock Kernel based progressive distillation for adder neural networks.
\newblock In {\em Proceedings of the Neural Information Processing Systems},
  2020.

\bibitem{you2020shiftaddnet}
Haoran You, Xiaohan Chen, Yongan Zhang, Chaojian Li, Sicheng Li, Zihao Liu,
  Zhangyang Wang, and Yingyan Lin.
\newblock Shiftaddnet: A hardware-inspired deep network.
\newblock {\em Advances in Neural Information Processing Systems},
  33:2771--2783, 2020.

\bibitem{zhang2018lq}
Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua.
\newblock Lq-nets: Learned quantization for highly accurate and compact deep
  neural networks.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 365--382, 2018.

\bibitem{zhou2016dorefa}
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He~Wen, and Yuheng Zou.
\newblock Dorefa-net: Training low bitwidth convolutional neural networks with
  low bitwidth gradients.
\newblock {\em arXiv preprint arXiv:1606.06160}, 2016.

\end{thebibliography}
