\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bussi and Parrinello(2007)]{bussi2007accurate}
G.~Bussi and M.~Parrinello.
\newblock Accurate sampling using langevin dynamics.
\newblock \emph{Physical Review E}, 75\penalty0 (5):\penalty0 056707, 2007.

\bibitem[Dalalyan and Karagulyan(2019)]{dalalyan2019user}
A.~S. Dalalyan and A.~Karagulyan.
\newblock User-friendly guarantees for the langevin monte carlo with inaccurate
  gradient.
\newblock \emph{Stochastic Processes and their Applications}, 129\penalty0
  (12):\penalty0 5278--5311, 2019.

\bibitem[Dasgupta and Freund(2008)]{dasgupta2008random}
S.~Dasgupta and Y.~Freund.
\newblock Random projection trees and low dimensional manifolds.
\newblock In \emph{Proceedings of the fortieth annual ACM symposium on Theory
  of computing}, pages 537--546, 2008.

\bibitem[Efron(2011)]{efron2011tweedie}
B.~Efron.
\newblock Tweedieâ€™s formula and selection bias.
\newblock \emph{Journal of the American Statistical Association}, 106\penalty0
  (496):\penalty0 1602--1614, 2011.

\bibitem[Girolami and Calderhead(2011)]{girolami2011riemann}
M.~Girolami and B.~Calderhead.
\newblock Riemann manifold langevin and hamiltonian monte carlo methods.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 73\penalty0 (2):\penalty0 123--214, 2011.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
J.~Ho, A.~Jain, and P.~Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{arXiv preprint arXiv:2006.11239}, 2020.

\bibitem[Hutchinson(1989)]{hutchinson1989stochastic}
M.~F. Hutchinson.
\newblock A stochastic estimator of the trace of the influence matrix for
  laplacian smoothing splines.
\newblock \emph{Communications in Statistics-Simulation and Computation},
  18\penalty0 (3):\penalty0 1059--1076, 1989.

\bibitem[Hyv{\"a}rinen(2005)]{hyvarinen2005estimation}
A.~Hyv{\"a}rinen.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (Apr):\penalty0 695--709, 2005.

\bibitem[Kong et~al.(2020)Kong, Ping, Huang, Zhao, and
  Catanzaro]{kong2020diffwave}
Z.~Kong, W.~Ping, J.~Huang, K.~Zhao, and B.~Catanzaro.
\newblock Diffwave: A versatile diffusion model for audio synthesis.
\newblock \emph{arXiv preprint arXiv:2009.09761}, 2020.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Martens and Grosse(2015)]{martens2015optimizing}
J.~Martens and R.~Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pages
  2408--2417. PMLR, 2015.

\bibitem[Mou et~al.(2019)Mou, Ma, Wainwright, Bartlett, and
  Jordan]{mou2019high}
W.~Mou, Y.-A. Ma, M.~J. Wainwright, P.~L. Bartlett, and M.~I. Jordan.
\newblock High-order langevin diffusion yields an accelerated mcmc algorithm.
\newblock \emph{arXiv preprint arXiv:1908.10859}, 2019.

\bibitem[Narayanan and Mitter(2010)]{narayanan2010sample}
H.~Narayanan and S.~Mitter.
\newblock Sample complexity of testing the manifold hypothesis.
\newblock In \emph{Proceedings of the 23rd International Conference on Neural
  Information Processing Systems-Volume 2}, pages 1786--1794, 2010.

\bibitem[Pang et~al.(2020)Pang, Xu, Li, Song, Ermon, and
  Zhu]{pang2020efficient}
T.~Pang, K.~Xu, C.~Li, Y.~Song, S.~Ermon, and J.~Zhu.
\newblock Efficient learning of generative models via finite-difference score
  matching.
\newblock \emph{arXiv preprint arXiv:2007.03317}, 2020.

\bibitem[Raphan and Simoncelli(2011)]{raphan2011least}
M.~Raphan and E.~P. Simoncelli.
\newblock Least squares estimation without priors or supervision.
\newblock \emph{Neural computation}, 23\penalty0 (2):\penalty0 374--420, 2011.

\bibitem[Robbins(2020)]{robbins2020empirical}
H.~Robbins.
\newblock \emph{An empirical Bayes approach to statistics}.
\newblock University of California Press, 2020.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015u}
O.~Ronneberger, P.~Fischer, and T.~Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{International Conference on Medical image computing and
  computer-assisted intervention}, pages 234--241. Springer, 2015.

\bibitem[Sabanis et~al.(2019)Sabanis, Zhang, et~al.]{sabanis2019higher}
S.~Sabanis, Y.~Zhang, et~al.
\newblock Higher order langevin monte carlo algorithm.
\newblock \emph{Electronic Journal of Statistics}, 13\penalty0 (2):\penalty0
  3805--3850, 2019.

\bibitem[Saremi and Hyvarinen(2019)]{saremi2019neural}
S.~Saremi and A.~Hyvarinen.
\newblock Neural empirical bayes.
\newblock \emph{Journal of Machine Learning Research}, 20:\penalty0 1--23,
  2019.

\bibitem[Saremi et~al.(2018)Saremi, Mehrjou, Sch{\"o}lkopf, and
  Hyv{\"a}rinen]{saremi2018deep}
S.~Saremi, A.~Mehrjou, B.~Sch{\"o}lkopf, and A.~Hyv{\"a}rinen.
\newblock Deep energy estimator networks.
\newblock \emph{arXiv preprint arXiv:1805.08306}, 2018.

\bibitem[Saul and Roweis(2003)]{saul2003think}
L.~K. Saul and S.~T. Roweis.
\newblock Think globally, fit locally: unsupervised learning of low dimensional
  manifolds.
\newblock \emph{Departmental Papers (CIS)}, page~12, 2003.

\bibitem[Song et~al.(2017)Song, Zhao, and Ermon]{song2017nice}
J.~Song, S.~Zhao, and S.~Ermon.
\newblock A-nice-mc: Adversarial training for mcmc.
\newblock \emph{arXiv preprint arXiv:1706.07561}, 2017.

\bibitem[Song and Ermon(2019)]{song2019generative}
Y.~Song and S.~Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  11918--11930, 2019.

\bibitem[Song and Ermon(2020)]{song2020improved}
Y.~Song and S.~Ermon.
\newblock Improved techniques for training score-based generative models.
\newblock \emph{arXiv preprint arXiv:2006.09011}, 2020.

\bibitem[Song and Kingma(2021)]{song2021train}
Y.~Song and D.~P. Kingma.
\newblock How to train your energy-based models.
\newblock \emph{arXiv preprint arXiv:2101.03288}, 2021.

\bibitem[Song et~al.(2019)Song, Garg, Shi, and Ermon]{song2019sliced}
Y.~Song, S.~Garg, J.~Shi, and S.~Ermon.
\newblock Sliced score matching: A scalable approach to density and score
  estimation.
\newblock \emph{arXiv preprint arXiv:1905.07088}, 2019.

\bibitem[Stein(1981)]{stein1981estimation}
C.~M. Stein.
\newblock Estimation of the mean of a multivariate normal distribution.
\newblock \emph{The annals of Statistics}, pages 1135--1151, 1981.

\bibitem[Stramer and Tweedie(1999)]{stramer1999langevin}
O.~Stramer and R.~Tweedie.
\newblock Langevin-type models i: Diffusions with given stationary
  distributions and their discretizations.
\newblock \emph{Methodology and Computing in Applied Probability}, 1\penalty0
  (3):\penalty0 283--306, 1999.

\bibitem[Vincent(2011)]{vincent2011connection}
P.~Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock \emph{Neural computation}, 23\penalty0 (7):\penalty0 1661--1674,
  2011.

\bibitem[Wang et~al.(2020)Wang, Cheng, Yueru, Zhu, and
  Zhang]{wang2020wasserstein}
Z.~Wang, S.~Cheng, L.~Yueru, J.~Zhu, and B.~Zhang.
\newblock A wasserstein minimum velocity approach to learning unnormalized
  models.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3728--3738. PMLR, 2020.

\bibitem[Welling and Teh(2011)]{welling2011bayesian}
M.~Welling and Y.~W. Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 681--688. Citeseer, 2011.

\bibitem[Zhou et~al.(2020)Zhou, Shi, and Zhu]{zhou2020nonparametric}
Y.~Zhou, J.~Shi, and J.~Zhu.
\newblock Nonparametric score estimators.
\newblock In \emph{International Conference on Machine Learning}, pages
  11513--11522. PMLR, 2020.

\end{thebibliography}
