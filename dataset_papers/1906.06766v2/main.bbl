\begin{thebibliography}{10}

\bibitem{achille2017critical}
Alessandro Achille, Matteo Rovere, and Stefano Soatto.
\newblock Critical learning periods in deep neural networks.
\newblock {\em arXiv preprint arXiv:1711.08856}, 2017.

\bibitem{anandkumar2016homotopy}
Anima Anandkumar, Yuan Deng, Rong Ge, and Hossein Mobahi.
\newblock Homotopy analysis for tensor pca.
\newblock {\em arXiv preprint arXiv:1610.09322}, 2016.

\bibitem{arora2018optimization}
Sanjeev Arora, Nadav Cohen, and Elad Hazan.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock {\em arXiv preprint arXiv:1802.06509}, 2018.

\bibitem{ba2014deep}
Jimmy Ba and Rich Caruana.
\newblock Do deep nets really need to be deep?
\newblock In {\em Advances in neural information processing systems}, pages
  2654--2662, 2014.

\bibitem{zec2}
Carlo Baldassi, Christian Borgs, Jennifer~T Chayes, Alessandro Ingrosso, Carlo
  Lucibello, Luca Saglietti, and Riccardo Zecchina.
\newblock Unreasonable effectiveness of learning neural networks: From
  accessible states and robust ensembles to basic algorithmic schemes.
\newblock {\em Proceedings of the National Academy of Sciences},
  113(48):E7655--E7662, 2016.

\bibitem{zec1}
Carlo Baldassi, Fabrizio Pittorino, and Riccardo Zecchina.
\newblock Shaping the learning landscape in neural networks around wide flat
  minima.
\newblock {\em arXiv preprint arXiv:1905.07833}, 2019.

\bibitem{belkin2018reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine learning and the bias-variance trade-off.
\newblock {\em arXiv preprint arXiv:1812.11118}, 2018.

\bibitem{bucilu2006model}
Cristian Bucilu«é, Rich Caruana, and Alexandru Niculescu-Mizil.
\newblock Model compression.
\newblock In {\em Proceedings of the 12th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 535--541. ACM, 2006.

\bibitem{chaudhari2016entropy}
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi,
  Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock {\em arXiv preprint arXiv:1611.01838}, 2016.

\bibitem{chen2015net2net}
Tianqi Chen, Ian Goodfellow, and Jonathon Shlens.
\newblock Net2net: Accelerating learning via knowledge transfer.
\newblock {\em arXiv preprint arXiv:1511.05641}, 2015.

\bibitem{coates2011selecting}
Adam Coates and Andrew~Y Ng.
\newblock Selecting receptive fields in deep networks.
\newblock In {\em Advances in neural information processing systems}, pages
  2528--2536, 2011.

\bibitem{draxler2018essentially}
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred~A Hamprecht.
\newblock Essentially no barriers in neural network energy landscape.
\newblock {\em arXiv preprint arXiv:1803.00885}, 2018.

\bibitem{du2017gradient}
Simon~S Du, Jason~D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent learns one-hidden-layer cnn: Don't be afraid of
  spurious local minima.
\newblock {\em arXiv preprint arXiv:1712.00779}, 2017.

\bibitem{du2018many}
Simon~S Du, Yining Wang, Xiyu Zhai, Sivaraman Balakrishnan, Ruslan~R
  Salakhutdinov, and Aarti Singh.
\newblock How many samples are needed to estimate a convolutional neural
  network?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  373--383, 2018.

\bibitem{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock {\em arXiv preprint arXiv:1803.03635}, 2018.

\bibitem{freeman2016topology}
C~Daniel Freeman and Joan Bruna.
\newblock Topology and geometry of deep rectified network optimization
  landscapes.
\newblock {\em arXiv preprint arXiv:1611.01540}, 2016.

\bibitem{garipov2018loss}
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry~P Vetrov, and
  Andrew~G Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8789--8798, 2018.

\bibitem{geiger2019scaling}
Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun,
  St{\'e}phane d'Ascoli, Giulio Biroli, Cl{\'e}ment Hongler, and Matthieu
  Wyart.
\newblock Scaling description of generalization with number of parameters in
  deep learning.
\newblock {\em arXiv preprint arXiv:1901.01608}, 2019.

\bibitem{geiger18}
Mario Geiger, Stefano Spigler, {St{\'e}phane} d'Ascoli, Levent Sagun, Marco
  Baity-Jesi, Giulio Biroli, and Matthieu Wyart.
\newblock The jamming transition as a paradigm to understand the loss landscape
  of deep neural networks.
\newblock {\em arXiv preprint arXiv:1809.09349}, 2018.

\bibitem{golatkar2019time}
Aditya Golatkar, Alessandro Achille, and Stefano Soatto.
\newblock Time matters in regularizing deep networks: Weight decay and data
  augmentation affect early learning dynamics, matter little near convergence.
\newblock {\em arXiv preprint arXiv:1905.13277}, 2019.

\bibitem{gunasekar2018characterizing}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock {\em arXiv preprint arXiv:1802.08246}, 2018.

\bibitem{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock {\em arXiv preprint arXiv:1510.00149}, 2015.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{jastrzkebski2017three}
Stanis{\l}aw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja
  Fischer, Yoshua Bengio, and Amos Storkey.
\newblock Three factors influencing minima in sgd.
\newblock {\em arXiv preprint arXiv:1711.04623}, 2017.

\bibitem{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock {\em arXiv preprint arXiv:1609.04836}, 2016.

\bibitem{cifar-10}
Alex Krizhevsky and Geoffrey Hinton.
\newblock {Learning Multiple Layers of Features from Tiny Images}.
\newblock {\em Technical Report}, pages 1--60, 2009.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem{lee2018learning}
Jaeho Lee and Maxim Raginsky.
\newblock Learning finite-dimensional coding schemes with nonlinear
  reconstruction maps.
\newblock {\em arXiv preprint arXiv:1812.09658}, 2018.

\bibitem{long2019size}
Philip~M Long and Hanie Sedghi.
\newblock Size-free generalization bounds for convolutional neural networks.
\newblock {\em arXiv preprint arXiv:1905.12600}, 2019.

\bibitem{neal2018modern}
Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna,
  Simon Lacoste-Julien, and Ioannis Mitliagkas.
\newblock A modern take on the bias-variance tradeoff in neural networks.
\newblock {\em arXiv preprint arXiv:1810.08591}, 2018.

\bibitem{neyshabur2018towards}
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan
  Srebro.
\newblock Towards understanding the role of over-parametrization in
  generalization of neural networks.
\newblock {\em arXiv preprint arXiv:1805.12076}, 2018.

\bibitem{novak2018bayesian}
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron,
  Daniel~A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein.
\newblock Bayesian deep convolutional networks with many channels are gaussian
  processes.
\newblock {\em arXiv preprint arXiv:1810.05148}, 2018.

\bibitem{nowlan1992simplifying}
Steven~J Nowlan and Geoffrey~E Hinton.
\newblock Simplifying neural networks by soft weight-sharing.
\newblock {\em Neural computation}, 4(4):473--493, 1992.

\bibitem{sagun2017empirical}
Levent Sagun, Utku Evci, V.~U\u{g}ur G\"uney, Yann Dauphin, and L\'eon Bottou.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock {\em ICLR 2018 Workshop Contribution, arXiv:1706.04454}, 2017.

\bibitem{saxena2016convolutional}
Shreyas Saxena and Jakob Verbeek.
\newblock Convolutional neural fabrics.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4053--4061, 2016.

\bibitem{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock {\em Journal of Machine Learning Research}, 19(70), 2018.

\bibitem{venturi2018neural}
Luca Venturi, Afonso Bandeira, and Joan Bruna.
\newblock Neural networks with finite intrinsic dimension have no spurious
  valleys.
\newblock {\em arXiv preprint arXiv:1802.06384}, 2018.

\bibitem{wu2017towards}
Lei Wu, Zhanxing Zhu, et~al.
\newblock Towards understanding generalization of deep learning: Perspective of
  loss landscapes.
\newblock {\em arXiv preprint arXiv:1706.10239}, 2017.

\bibitem{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em arXiv preprint arXiv:1611.03530}, 2016.

\end{thebibliography}
