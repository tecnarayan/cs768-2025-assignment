% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{none/global//global/global}
    \entry{vafaeikia_brief_2020}{misc}{}
      \name{author}{3}{}{%
        {{hash=2df36819e0fde50af878f870bc073ee9}{%
           family={Vafaeikia},
           familyi={V\bibinitperiod},
           given={Partoo},
           giveni={P\bibinitperiod}}}%
        {{hash=fd3c17439d0cc09977c85539fe10d766}{%
           family={Namdar},
           familyi={N\bibinitperiod},
           given={Khashayar},
           giveni={K\bibinitperiod}}}%
        {{hash=27d3d76ef3c8fe87b15e7798b8f5f3d0}{%
           family={Khalvati},
           familyi={K\bibinitperiod},
           given={Farzad},
           giveni={F\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{6f47ec1d96b716a78a4b8d654e4e7fc3}
      \strng{fullhash}{6a6fc54994bab4b7c3a4e678315265d8}
      \strng{bibnamehash}{6a6fc54994bab4b7c3a4e678315265d8}
      \strng{authorbibnamehash}{6a6fc54994bab4b7c3a4e678315265d8}
      \strng{authornamehash}{6f47ec1d96b716a78a4b8d654e4e7fc3}
      \strng{authorfullhash}{6a6fc54994bab4b7c3a4e678315265d8}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multi-task learning (MTL) optimizes several learning tasks simultaneously and leverages their shared information to improve generalization and the prediction of the model for each task. Auxiliary tasks can be added to the main task to ultimately boost the performance. In this paper, we provide a brief review on the recent deep multi-task learning (dMTL) approaches followed by methods on selecting useful auxiliary tasks that can be used in dMTL to improve the performance of the model for the main task.}
      \field{month}{7}
      \field{note}{arXiv:2007.01126 [cs, stat]}
      \field{title}{A {Brief} {Review} of {Deep} {Multi}-task {Learning} and {Auxiliary} {Task} {Learning}}
      \field{urlday}{29}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2020}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2007.01126
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2007.01126
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2007.01126
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{zhang_survey_2022}{article}{}
      \name{author}{2}{}{%
        {{hash=9a4f4a1ff661cd600eb26523a5ba8bb4}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Yu},
           giveni={Y\bibinitperiod}}}%
        {{hash=55242d2a60270145342841e4d4238da0}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Qiang},
           giveni={Q\bibinitperiod}}}%
      }
      \strng{namehash}{6cd6b62eb7a185a0d85f22af0ef449f2}
      \strng{fullhash}{6cd6b62eb7a185a0d85f22af0ef449f2}
      \strng{bibnamehash}{6cd6b62eb7a185a0d85f22af0ef449f2}
      \strng{authorbibnamehash}{6cd6b62eb7a185a0d85f22af0ef449f2}
      \strng{authornamehash}{6cd6b62eb7a185a0d85f22af0ef449f2}
      \strng{authorfullhash}{6cd6b62eb7a185a0d85f22af0ef449f2}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.}
      \field{issn}{1558-2191}
      \field{journaltitle}{IEEE Transactions on Knowledge and Data Engineering}
      \field{month}{12}
      \field{note}{Conference Name: IEEE Transactions on Knowledge and Data Engineering}
      \field{number}{12}
      \field{title}{A {Survey} on {Multi}-{Task} {Learning}}
      \field{volume}{34}
      \field{year}{2022}
      \field{pages}{5586\bibrangedash 5609}
      \range{pages}{24}
      \verb{doi}
      \verb 10.1109/TKDE.2021.3070203
      \endverb
      \keyw{Classification algorithms,Computational modeling,Data models,Multi-task learning,Supervised learning,Task analysis,Training,Transfer learning,artificial intelligence,machine learning}
    \endentry
    \entry{du_survey_2022}{misc}{}
      \name{author}{4}{}{%
        {{hash=8b52fd6470026437f20bf1d0e1302c61}{%
           family={Du},
           familyi={D\bibinitperiod},
           given={Yifan},
           giveni={Y\bibinitperiod}}}%
        {{hash=eb761f0ec974830dfebae92d776d3bbe}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Zikang},
           giveni={Z\bibinitperiod}}}%
        {{hash=0da53871c5bda3ae9ea7b5d31b6a8874}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Junyi},
           giveni={J\bibinitperiod}}}%
        {{hash=64942cca497eff6cd6c5b264afdc5dd6}{%
           family={Zhao},
           familyi={Z\bibinitperiod},
           given={Wayne\bibnamedelima Xin},
           giveni={W\bibinitperiod\bibinitdelim X\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{77fbf46f54315ecd9cd20bd302fe1b15}
      \strng{fullhash}{501aa6271bc2f86a48769c326b64f07c}
      \strng{bibnamehash}{501aa6271bc2f86a48769c326b64f07c}
      \strng{authorbibnamehash}{501aa6271bc2f86a48769c326b64f07c}
      \strng{authornamehash}{77fbf46f54315ecd9cd20bd302fe1b15}
      \strng{authorfullhash}{501aa6271bc2f86a48769c326b64f07c}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{As transformer evolves, pre-trained models have advanced at a breakneck pace in recent years. They have dominated the mainstream techniques in natural language processing (NLP) and computer vision (CV). How to adapt pre-training to the field of Vision-and-Language (V-L) learning and improve downstream task performance becomes a focus of multimodal learning. In this paper, we review the recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As the core content, we first briefly introduce several ways to encode raw images and texts to single-modal embeddings before pre-training. Then, we dive into the mainstream architectures of VL-PTMs in modeling the interaction between text and image representations. We further present widely-used pre-training tasks, and then we introduce some common downstream tasks. We finally conclude this paper and present some promising research directions. Our survey aims to provide researchers with synthesis and pointer to related research.}
      \field{month}{7}
      \field{note}{arXiv:2202.10936 [cs]}
      \field{title}{A {Survey} of {Vision}-{Language} {Pre}-{Trained} {Models}}
      \field{urlday}{29}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2202.10936
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2202.10936
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2202.10936
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
    \endentry
    \entry{zhou_comprehensive_2023}{misc}{}
      \name{author}{19}{}{%
        {{hash=1e9b71f8cade0a6d2e770ca52781b0fe}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Ce},
           giveni={C\bibinitperiod}}}%
        {{hash=9885f16be76c4e68e9c2aaf363a53600}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Qian},
           giveni={Q\bibinitperiod}}}%
        {{hash=02821a48003406ef06e1ee4dfa9fca52}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Chen},
           giveni={C\bibinitperiod}}}%
        {{hash=94658a16389c695058255d9358d09cdd}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod}}}%
        {{hash=c8c0e2e8882c8cbe0bbeb873ae3dc3df}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Yixin},
           giveni={Y\bibinitperiod}}}%
        {{hash=d5e3c419562fb365f03276bd56ded037}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Guangjing},
           giveni={G\bibinitperiod}}}%
        {{hash=accfaf086a407bfb746d7875478d9130}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Kai},
           giveni={K\bibinitperiod}}}%
        {{hash=3004cb6c2779bd6516c4f23dfd55d275}{%
           family={Ji},
           familyi={J\bibinitperiod},
           given={Cheng},
           giveni={C\bibinitperiod}}}%
        {{hash=b679eaee9b69b41850bc86ad80ccc4af}{%
           family={Yan},
           familyi={Y\bibinitperiod},
           given={Qiben},
           giveni={Q\bibinitperiod}}}%
        {{hash=8bc1c2c96c880e5bde99cc1aab8ec6ef}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Lifang},
           giveni={L\bibinitperiod}}}%
        {{hash=a7a55881cb261787fefdc7f2311dab03}{%
           family={Peng},
           familyi={P\bibinitperiod},
           given={Hao},
           giveni={H\bibinitperiod}}}%
        {{hash=221d23a1cc1702c31044af688c0b7c39}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Jianxin},
           giveni={J\bibinitperiod}}}%
        {{hash=9cecf65b3c7f3407db3d54a3f9966210}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Jia},
           giveni={J\bibinitperiod}}}%
        {{hash=15a5322a1b261fe95ac6e04758acf39c}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Ziwei},
           giveni={Z\bibinitperiod}}}%
        {{hash=4b1c47330a6364bd6222728a5bd478e1}{%
           family={Xie},
           familyi={X\bibinitperiod},
           given={Pengtao},
           giveni={P\bibinitperiod}}}%
        {{hash=1fa4dfdd5ecf049d39575fe7ba1ec9f3}{%
           family={Xiong},
           familyi={X\bibinitperiod},
           given={Caiming},
           giveni={C\bibinitperiod}}}%
        {{hash=56af930e1f5e2c9a791b71e7b347c0ba}{%
           family={Pei},
           familyi={P\bibinitperiod},
           given={Jian},
           giveni={J\bibinitperiod}}}%
        {{hash=ed7b2de2f08bfa6638d5d174d55d839b}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Philip\bibnamedelima S.},
           giveni={P\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=48972a9b89e478f69f1b1e95205c6278}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Lichao},
           giveni={L\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{e2534b515ba5bc3bea6164305c1b6a44}
      \strng{fullhash}{4ea2d287d2e82c29e9335141795fcbcf}
      \strng{bibnamehash}{e2534b515ba5bc3bea6164305c1b6a44}
      \strng{authorbibnamehash}{e2534b515ba5bc3bea6164305c1b6a44}
      \strng{authornamehash}{e2534b515ba5bc3bea6164305c1b6a44}
      \strng{authorfullhash}{4ea2d287d2e82c29e9335141795fcbcf}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. The review covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning. Additionally, it explores advanced PFMs used for different data modalities and unified PFMs that consider data quality and quantity. The review also discusses research related to the fundamentals of PFMs, such as model efficiency and compression, security, and privacy. Finally, the study provides key implications, future research directions, challenges, and open problems in the field of PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and the user-friendly interactive ability for artificial general intelligence.}
      \field{month}{5}
      \field{note}{arXiv:2302.09419 [cs]}
      \field{shorttitle}{A {Comprehensive} {Survey} on {Pretrained} {Foundation} {Models}}
      \field{title}{A {Comprehensive} {Survey} on {Pretrained} {Foundation} {Models}: {A} {History} from {BERT} to {ChatGPT}}
      \field{urlday}{29}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2023}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2302.09419
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2302.09419
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2302.09419
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
    \endentry
    \entry{bommasani_opportunities_2022}{misc}{}
      \name{author}{114}{}{%
        {{hash=3963a2fcc8de83e028bc855365b882d9}{%
           family={Bommasani},
           familyi={B\bibinitperiod},
           given={Rishi},
           giveni={R\bibinitperiod}}}%
        {{hash=b2756a270aa1e5d4a20fa3ed9b6e1f8c}{%
           family={Hudson},
           familyi={H\bibinitperiod},
           given={Drew\bibnamedelima A.},
           giveni={D\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=91a5d92d019d0302b9a66aa6ba4b1bc8}{%
           family={Adeli},
           familyi={A\bibinitperiod},
           given={Ehsan},
           giveni={E\bibinitperiod}}}%
        {{hash=e8fc96447f9340e4184d491b95fce5a5}{%
           family={Altman},
           familyi={A\bibinitperiod},
           given={Russ},
           giveni={R\bibinitperiod}}}%
        {{hash=3ffe90aedf033aa6eb6d5c26bced7177}{%
           family={Arora},
           familyi={A\bibinitperiod},
           given={Simran},
           giveni={S\bibinitperiod}}}%
        {{hash=000a6cbf8aca9234835772b7972b889f}{%
           family={Arx},
           familyi={A\bibinitperiod},
           given={Sydney},
           giveni={S\bibinitperiod},
           prefix={von},
           prefixi={v\bibinitperiod}}}%
        {{hash=bb1df70dce464605ddd9cf83485d8db1}{%
           family={Bernstein},
           familyi={B\bibinitperiod},
           given={Michael\bibnamedelima S.},
           giveni={M\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=49e0ca937af486456ef5f0603eafb7dc}{%
           family={Bohg},
           familyi={B\bibinitperiod},
           given={Jeannette},
           giveni={J\bibinitperiod}}}%
        {{hash=ed22853aa493ba4023443a010c9f6f8b}{%
           family={Bosselut},
           familyi={B\bibinitperiod},
           given={Antoine},
           giveni={A\bibinitperiod}}}%
        {{hash=d94fdf38ad71e8210b4505c8fbb7fa87}{%
           family={Brunskill},
           familyi={B\bibinitperiod},
           given={Emma},
           giveni={E\bibinitperiod}}}%
        {{hash=7bd9f54569cb8c0301da00ed49b5a752}{%
           family={Brynjolfsson},
           familyi={B\bibinitperiod},
           given={Erik},
           giveni={E\bibinitperiod}}}%
        {{hash=af31d8923236f1a7e9a0df8b744d2d55}{%
           family={Buch},
           familyi={B\bibinitperiod},
           given={Shyamal},
           giveni={S\bibinitperiod}}}%
        {{hash=65c8e5336738df29fabfe51e91d52c3b}{%
           family={Card},
           familyi={C\bibinitperiod},
           given={Dallas},
           giveni={D\bibinitperiod}}}%
        {{hash=392132fc28203fa2ae31b59cf05b0c7a}{%
           family={Castellon},
           familyi={C\bibinitperiod},
           given={Rodrigo},
           giveni={R\bibinitperiod}}}%
        {{hash=ef6ded1d36f3b5b10bb5f2447bc40818}{%
           family={Chatterji},
           familyi={C\bibinitperiod},
           given={Niladri},
           giveni={N\bibinitperiod}}}%
        {{hash=2fe24d54b2fea8307a1f608cd0d97174}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Annie},
           giveni={A\bibinitperiod}}}%
        {{hash=0b13ebd89013d6c24d2c6e82545a0cd8}{%
           family={Creel},
           familyi={C\bibinitperiod},
           given={Kathleen},
           giveni={K\bibinitperiod}}}%
        {{hash=e7ca1a184acbd133d9c401f184441c69}{%
           family={Davis},
           familyi={D\bibinitperiod},
           given={Jared\bibnamedelima Quincy},
           giveni={J\bibinitperiod\bibinitdelim Q\bibinitperiod}}}%
        {{hash=4aade6eed920a6b296d0230dbd4706a5}{%
           family={Demszky},
           familyi={D\bibinitperiod},
           given={Dora},
           giveni={D\bibinitperiod}}}%
        {{hash=a53bb69fcb99ecc1daaa649ffe82ca5c}{%
           family={Donahue},
           familyi={D\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod}}}%
        {{hash=9d1efdb590f09cc2d86d91fe65b21dd5}{%
           family={Doumbouya},
           familyi={D\bibinitperiod},
           given={Moussa},
           giveni={M\bibinitperiod}}}%
        {{hash=fb99a22b9a7e4d6e6827455fa420fc5f}{%
           family={Durmus},
           familyi={D\bibinitperiod},
           given={Esin},
           giveni={E\bibinitperiod}}}%
        {{hash=160929a7e6ceecdf3f7c9a13abd5ea35}{%
           family={Ermon},
           familyi={E\bibinitperiod},
           given={Stefano},
           giveni={S\bibinitperiod}}}%
        {{hash=60b817775846213e2bec953071cfba95}{%
           family={Etchemendy},
           familyi={E\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
        {{hash=5e0326e7704dfc4d1bc0ad89e1192f67}{%
           family={Ethayarajh},
           familyi={E\bibinitperiod},
           given={Kawin},
           giveni={K\bibinitperiod}}}%
        {{hash=cd00ce5bc45f687c432e52e0fa1a7aa6}{%
           family={Fei-Fei},
           familyi={F\bibinithyphendelim F\bibinitperiod},
           given={Li},
           giveni={L\bibinitperiod}}}%
        {{hash=058e82495825ae376c6a96a12169e6ee}{%
           family={Finn},
           familyi={F\bibinitperiod},
           given={Chelsea},
           giveni={C\bibinitperiod}}}%
        {{hash=c7fc2ec4e4f0cbfedeff8c8a385eeb90}{%
           family={Gale},
           familyi={G\bibinitperiod},
           given={Trevor},
           giveni={T\bibinitperiod}}}%
        {{hash=240ae9ab7eae3e2d948d764d2f55d6ec}{%
           family={Gillespie},
           familyi={G\bibinitperiod},
           given={Lauren},
           giveni={L\bibinitperiod}}}%
        {{hash=b28818adf394a54c714e3a5c8313377e}{%
           family={Goel},
           familyi={G\bibinitperiod},
           given={Karan},
           giveni={K\bibinitperiod}}}%
        {{hash=d98a11597b0645b5e5c970ce0a562a96}{%
           family={Goodman},
           familyi={G\bibinitperiod},
           given={Noah},
           giveni={N\bibinitperiod}}}%
        {{hash=4eed4dfc949da3211d651ebd8c4c670b}{%
           family={Grossman},
           familyi={G\bibinitperiod},
           given={Shelby},
           giveni={S\bibinitperiod}}}%
        {{hash=5c5188998b64714185b6cc228eb9a4fd}{%
           family={Guha},
           familyi={G\bibinitperiod},
           given={Neel},
           giveni={N\bibinitperiod}}}%
        {{hash=be228979341f812e59d6e0fe757fe105}{%
           family={Hashimoto},
           familyi={H\bibinitperiod},
           given={Tatsunori},
           giveni={T\bibinitperiod}}}%
        {{hash=50374f86d34cc54b8a5788ccbcce4cfe}{%
           family={Henderson},
           familyi={H\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
        {{hash=8d557c63d5d40248206f4499cd9ac310}{%
           family={Hewitt},
           familyi={H\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
        {{hash=2e146a981c17f3c9b6f92736f5fde3c1}{%
           family={Ho},
           familyi={H\bibinitperiod},
           given={Daniel\bibnamedelima E.},
           giveni={D\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=faf318ef4acda4567253c563b330f7ff}{%
           family={Hong},
           familyi={H\bibinitperiod},
           given={Jenny},
           giveni={J\bibinitperiod}}}%
        {{hash=78e1c9958ee71476268e13a238fae1e5}{%
           family={Hsu},
           familyi={H\bibinitperiod},
           given={Kyle},
           giveni={K\bibinitperiod}}}%
        {{hash=dc6a25244d65d1c3b76b8da8ba84b54c}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Jing},
           giveni={J\bibinitperiod}}}%
        {{hash=b0badbf499a1ef5e7bc2f893186e6a5c}{%
           family={Icard},
           familyi={I\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
        {{hash=acdd7450ea2182de698a25065127e049}{%
           family={Jain},
           familyi={J\bibinitperiod},
           given={Saahil},
           giveni={S\bibinitperiod}}}%
        {{hash=3147296c99a3f829087becd1a4eaec08}{%
           family={Jurafsky},
           familyi={J\bibinitperiod},
           given={Dan},
           giveni={D\bibinitperiod}}}%
        {{hash=4b08aa37d9caa8d3d167a189b7fdabdc}{%
           family={Kalluri},
           familyi={K\bibinitperiod},
           given={Pratyusha},
           giveni={P\bibinitperiod}}}%
        {{hash=0cf3ec76164d190258b742372fecf5af}{%
           family={Karamcheti},
           familyi={K\bibinitperiod},
           given={Siddharth},
           giveni={S\bibinitperiod}}}%
        {{hash=f26de96c576b2e86930c2310426c0e1d}{%
           family={Keeling},
           familyi={K\bibinitperiod},
           given={Geoff},
           giveni={G\bibinitperiod}}}%
        {{hash=cce7236c358c86293a63037af2316088}{%
           family={Khani},
           familyi={K\bibinitperiod},
           given={Fereshte},
           giveni={F\bibinitperiod}}}%
        {{hash=7c1f69df368375faab52ad37587dfd85}{%
           family={Khattab},
           familyi={K\bibinitperiod},
           given={Omar},
           giveni={O\bibinitperiod}}}%
        {{hash=b7309d5979903f9e56ec43ebf0d594ad}{%
           family={Koh},
           familyi={K\bibinitperiod},
           given={Pang\bibnamedelima Wei},
           giveni={P\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
        {{hash=4a65f24db77dbeffe37e9a16299dbafe}{%
           family={Krass},
           familyi={K\bibinitperiod},
           given={Mark},
           giveni={M\bibinitperiod}}}%
        {{hash=e9cf8ade7c94f1084ea6832928a36fef}{%
           family={Krishna},
           familyi={K\bibinitperiod},
           given={Ranjay},
           giveni={R\bibinitperiod}}}%
        {{hash=e8a00cdf6113b597eb2a9766a7b801fb}{%
           family={Kuditipudi},
           familyi={K\bibinitperiod},
           given={Rohith},
           giveni={R\bibinitperiod}}}%
        {{hash=1047181628ae2fc1a1cf6271af00ce82}{%
           family={Kumar},
           familyi={K\bibinitperiod},
           given={Ananya},
           giveni={A\bibinitperiod}}}%
        {{hash=ced0403c07861802e74bf36cea3d1460}{%
           family={Ladhak},
           familyi={L\bibinitperiod},
           given={Faisal},
           giveni={F\bibinitperiod}}}%
        {{hash=27d62aaa219df51eaa434fbedee686b4}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Mina},
           giveni={M\bibinitperiod}}}%
        {{hash=75502ac96d1848352bed21df4be905a0}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Tony},
           giveni={T\bibinitperiod}}}%
        {{hash=900d107125ff0ca84698cb909e4f6c51}{%
           family={Leskovec},
           familyi={L\bibinitperiod},
           given={Jure},
           giveni={J\bibinitperiod}}}%
        {{hash=1a3f3794d6fe2b30f59da78d4d7036b1}{%
           family={Levent},
           familyi={L\bibinitperiod},
           given={Isabelle},
           giveni={I\bibinitperiod}}}%
        {{hash=3e100909d77fa0e509e19ac0b35b6306}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Xiang\bibnamedelima Lisa},
           giveni={X\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=b15f49c62474eb455b840b4fd198e965}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Xuechen},
           giveni={X\bibinitperiod}}}%
        {{hash=5bd4a91e09b499d15e82bc929acf1e34}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Tengyu},
           giveni={T\bibinitperiod}}}%
        {{hash=644cd66cda9a4ffbe195d6884eb9da21}{%
           family={Malik},
           familyi={M\bibinitperiod},
           given={Ali},
           giveni={A\bibinitperiod}}}%
        {{hash=2214edb8305f7ccd7cdc310b3a8ae1b4}{%
           family={Manning},
           familyi={M\bibinitperiod},
           given={Christopher\bibnamedelima D.},
           giveni={C\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=9efbf4a9894da6afdb78d41966429558}{%
           family={Mirchandani},
           familyi={M\bibinitperiod},
           given={Suvir},
           giveni={S\bibinitperiod}}}%
        {{hash=b9441ad4287a1b5f251258a6fd6afb3a}{%
           family={Mitchell},
           familyi={M\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod}}}%
        {{hash=3897179e7392bf966ae8398b2607acae}{%
           family={Munyikwa},
           familyi={M\bibinitperiod},
           given={Zanele},
           giveni={Z\bibinitperiod}}}%
        {{hash=df590fc759747c7e8bfe23768561e640}{%
           family={Nair},
           familyi={N\bibinitperiod},
           given={Suraj},
           giveni={S\bibinitperiod}}}%
        {{hash=8af4e4ca915f01ba9eebacf5762a5542}{%
           family={Narayan},
           familyi={N\bibinitperiod},
           given={Avanika},
           giveni={A\bibinitperiod}}}%
        {{hash=b34c3675fa6935732be9302df09d61e7}{%
           family={Narayanan},
           familyi={N\bibinitperiod},
           given={Deepak},
           giveni={D\bibinitperiod}}}%
        {{hash=ddb6b648f08c540de2055befac601da3}{%
           family={Newman},
           familyi={N\bibinitperiod},
           given={Ben},
           giveni={B\bibinitperiod}}}%
        {{hash=f236bd1ea193b63083b7b8f9a6693fab}{%
           family={Nie},
           familyi={N\bibinitperiod},
           given={Allen},
           giveni={A\bibinitperiod}}}%
        {{hash=0e3d5d93aebbb520ce0d66f1493e3182}{%
           family={Niebles},
           familyi={N\bibinitperiod},
           given={Juan\bibnamedelima Carlos},
           giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=a4b30e51359854c0d89aa15a6279a424}{%
           family={Nilforoshan},
           familyi={N\bibinitperiod},
           given={Hamed},
           giveni={H\bibinitperiod}}}%
        {{hash=f9610120104f363e5e025e37f0481a00}{%
           family={Nyarko},
           familyi={N\bibinitperiod},
           given={Julian},
           giveni={J\bibinitperiod}}}%
        {{hash=d533c61283c37125fbd7dff715b3b13b}{%
           family={Ogut},
           familyi={O\bibinitperiod},
           given={Giray},
           giveni={G\bibinitperiod}}}%
        {{hash=56609998fd4fad40ac1db49f3bf74a71}{%
           family={Orr},
           familyi={O\bibinitperiod},
           given={Laurel},
           giveni={L\bibinitperiod}}}%
        {{hash=ab1d41a4277989255e218a39ef990918}{%
           family={Papadimitriou},
           familyi={P\bibinitperiod},
           given={Isabel},
           giveni={I\bibinitperiod}}}%
        {{hash=a6d2d4802375c9b3367011ada5e50822}{%
           family={Park},
           familyi={P\bibinitperiod},
           given={Joon\bibnamedelima Sung},
           giveni={J\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=329b34d0a2be175814c185a16c2314fc}{%
           family={Piech},
           familyi={P\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod}}}%
        {{hash=ddcb5844dd617114f2e297527ab57781}{%
           family={Portelance},
           familyi={P\bibinitperiod},
           given={Eva},
           giveni={E\bibinitperiod}}}%
        {{hash=699d626ef41a1a827d623529c1a45e5a}{%
           family={Potts},
           familyi={P\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
        {{hash=7bb584b014f043d94e43a8395735799e}{%
           family={Raghunathan},
           familyi={R\bibinitperiod},
           given={Aditi},
           giveni={A\bibinitperiod}}}%
        {{hash=59d55d52c7b75f92715c4f7d81be2940}{%
           family={Reich},
           familyi={R\bibinitperiod},
           given={Rob},
           giveni={R\bibinitperiod}}}%
        {{hash=f136c99c06f10d114554179d4722b864}{%
           family={Ren},
           familyi={R\bibinitperiod},
           given={Hongyu},
           giveni={H\bibinitperiod}}}%
        {{hash=b645d37e24a7c66bcf7a6422ecbbad15}{%
           family={Rong},
           familyi={R\bibinitperiod},
           given={Frieda},
           giveni={F\bibinitperiod}}}%
        {{hash=0c985a6824f10b568d5b3a5db148a9f0}{%
           family={Roohani},
           familyi={R\bibinitperiod},
           given={Yusuf},
           giveni={Y\bibinitperiod}}}%
        {{hash=af87918b9edf77ea2ebe3722099ea779}{%
           family={Ruiz},
           familyi={R\bibinitperiod},
           given={Camilo},
           giveni={C\bibinitperiod}}}%
        {{hash=9ac8a375a7855a6db42abb4107d7ea4e}{%
           family={Ryan},
           familyi={R\bibinitperiod},
           given={Jack},
           giveni={J\bibinitperiod}}}%
        {{hash=eb1f5a2ba08ea12db36c871fe601f5f1}{%
           family={Ré},
           familyi={R\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
        {{hash=ad742818738efaa3c358f9def3944504}{%
           family={Sadigh},
           familyi={S\bibinitperiod},
           given={Dorsa},
           giveni={D\bibinitperiod}}}%
        {{hash=7a0090b3fe3fd8202defc8955e942abb}{%
           family={Sagawa},
           familyi={S\bibinitperiod},
           given={Shiori},
           giveni={S\bibinitperiod}}}%
        {{hash=d592c130d7c5ac98ff68ac9690dae0d0}{%
           family={Santhanam},
           familyi={S\bibinitperiod},
           given={Keshav},
           giveni={K\bibinitperiod}}}%
        {{hash=a7b4dd0087a3c49f89ad850cb8bede41}{%
           family={Shih},
           familyi={S\bibinitperiod},
           given={Andy},
           giveni={A\bibinitperiod}}}%
        {{hash=402d4f01c8bc2ccf3772e8538e4d4a6d}{%
           family={Srinivasan},
           familyi={S\bibinitperiod},
           given={Krishnan},
           giveni={K\bibinitperiod}}}%
        {{hash=1209f7e3599bd390bab72d9cca887763}{%
           family={Tamkin},
           familyi={T\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=9590cdbe06768705c6571bce7802fcc2}{%
           family={Taori},
           familyi={T\bibinitperiod},
           given={Rohan},
           giveni={R\bibinitperiod}}}%
        {{hash=7bc57e0261c064c9e837697f8b5fc22e}{%
           family={Thomas},
           familyi={T\bibinitperiod},
           given={Armin\bibnamedelima W.},
           giveni={A\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
        {{hash=ab482c2931db00ecd5b1e89bbe011cb5}{%
           family={Tramèr},
           familyi={T\bibinitperiod},
           given={Florian},
           giveni={F\bibinitperiod}}}%
        {{hash=de94dfef4697a78dc56ab69c23558df5}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Rose\bibnamedelima E.},
           giveni={R\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=f547ec885594e76da25ad82058ba8850}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={William},
           giveni={W\bibinitperiod}}}%
        {{hash=8680ef845bbdd349f96424ad986a47f8}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Bohan},
           giveni={B\bibinitperiod}}}%
        {{hash=3a7d08fff1af967fe4513ac63790e97d}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Jiajun},
           giveni={J\bibinitperiod}}}%
        {{hash=954bef435a12336c9decd19360a640f5}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Yuhuai},
           giveni={Y\bibinitperiod}}}%
        {{hash=3e4a7fd97d3e0bb7770a140ba997150f}{%
           family={Xie},
           familyi={X\bibinitperiod},
           given={Sang\bibnamedelima Michael},
           giveni={S\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=f95714ad66afcfa71b317b3bec07d01d}{%
           family={Yasunaga},
           familyi={Y\bibinitperiod},
           given={Michihiro},
           giveni={M\bibinitperiod}}}%
        {{hash=adf64964c8c9d5f144d937e11ccf61ba}{%
           family={You},
           familyi={Y\bibinitperiod},
           given={Jiaxuan},
           giveni={J\bibinitperiod}}}%
        {{hash=00dc518b94dfa6349c86e70661a1a5ef}{%
           family={Zaharia},
           familyi={Z\bibinitperiod},
           given={Matei},
           giveni={M\bibinitperiod}}}%
        {{hash=3214f9487d07db5ea11795e7d6ea416d}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=733e3732b7e3ce5cf2b04cf5e9bd9cf1}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Tianyi},
           giveni={T\bibinitperiod}}}%
        {{hash=0062b3d93aad0fef37c9aa27dc6d165b}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Xikun},
           giveni={X\bibinitperiod}}}%
        {{hash=4a029fd7cea6a9c680cd411bbb8ad43c}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Yuhui},
           giveni={Y\bibinitperiod}}}%
        {{hash=3c6953622ba83af60dc476561349a5b8}{%
           family={Zheng},
           familyi={Z\bibinitperiod},
           given={Lucia},
           giveni={L\bibinitperiod}}}%
        {{hash=d123e225dfb2478fcb0d08b27c81491b}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Kaitlyn},
           giveni={K\bibinitperiod}}}%
        {{hash=86c032bbc45c3b616f0a1170befc0e82}{%
           family={Liang},
           familyi={L\bibinitperiod},
           given={Percy},
           giveni={P\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{e1331c79138f819881e39bc36f1b80e0}
      \strng{fullhash}{38016a5c6e9130115c95746c93874ec8}
      \strng{bibnamehash}{e1331c79138f819881e39bc36f1b80e0}
      \strng{authorbibnamehash}{e1331c79138f819881e39bc36f1b80e0}
      \strng{authornamehash}{e1331c79138f819881e39bc36f1b80e0}
      \strng{authorfullhash}{38016a5c6e9130115c95746c93874ec8}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.}
      \field{month}{7}
      \field{note}{arXiv:2108.07258 [cs]}
      \field{title}{On the {Opportunities} and {Risks} of {Foundation} {Models}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2108.07258
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2108.07258
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2108.07258
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning}
    \endentry
    \entry{jaderberg_reinforcement_2016}{misc}{}
      \name{author}{7}{}{%
        {{hash=7dc1446dea7ff50b2b02fb83780cc9c6}{%
           family={Jaderberg},
           familyi={J\bibinitperiod},
           given={Max},
           giveni={M\bibinitperiod}}}%
        {{hash=f7d23cfe4ca0e6bf7a8c251bfa78aca6}{%
           family={Mnih},
           familyi={M\bibinitperiod},
           given={Volodymyr},
           giveni={V\bibinitperiod}}}%
        {{hash=2e540193e28e5b3aa63fef2e515c637b}{%
           family={Czarnecki},
           familyi={C\bibinitperiod},
           given={Wojciech\bibnamedelima Marian},
           giveni={W\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=a56e72b23778a835bdade7d0511e43a3}{%
           family={Schaul},
           familyi={S\bibinitperiod},
           given={Tom},
           giveni={T\bibinitperiod}}}%
        {{hash=698896c85bec4f93adca09e2a21cdf78}{%
           family={Leibo},
           familyi={L\bibinitperiod},
           given={Joel\bibnamedelima Z.},
           giveni={J\bibinitperiod\bibinitdelim Z\bibinitperiod}}}%
        {{hash=89dbd30410c2085cd059f32c57d4593e}{%
           family={Silver},
           familyi={S\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=481308b301166b521c74fde6566e97e6}{%
           family={Kavukcuoglu},
           familyi={K\bibinitperiod},
           given={Koray},
           giveni={K\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{6373e6f74e9bf37ec2dc2f2504c49358}
      \strng{fullhash}{5d476a02818667a4b28554bcc296609f}
      \strng{bibnamehash}{6373e6f74e9bf37ec2dc2f2504c49358}
      \strng{authorbibnamehash}{6373e6f74e9bf37ec2dc2f2504c49358}
      \strng{authornamehash}{6373e6f74e9bf37ec2dc2f2504c49358}
      \strng{authorfullhash}{5d476a02818667a4b28554bcc296609f}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880{\textbackslash}\% expert human performance, and a challenging suite of first-person, three-dimensional {\textbackslash}emph\{Labyrinth\} tasks leading to a mean speedup in learning of 10\${\textbackslash}times\$ and averaging 87{\textbackslash}\% expert human performance on Labyrinth.}
      \field{month}{11}
      \field{note}{arXiv:1611.05397 [cs]}
      \field{title}{Reinforcement {Learning} with {Unsupervised} {Auxiliary} {Tasks}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{year}{2016}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1611.05397
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1611.05397
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1611.05397
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
    \endentry
    \entry{wang_overwriting_2023}{misc}{}
      \name{author}{2}{}{%
        {{hash=530b12684720166b10a7734d1fedca62}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Angelina},
           giveni={A\bibinitperiod}}}%
        {{hash=2a74ec28b731b015747e2af5f0b519e1}{%
           family={Russakovsky},
           familyi={R\bibinitperiod},
           given={Olga},
           giveni={O\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{5ff07878bac730dafb506bf81d3792d8}
      \strng{fullhash}{5ff07878bac730dafb506bf81d3792d8}
      \strng{bibnamehash}{5ff07878bac730dafb506bf81d3792d8}
      \strng{authorbibnamehash}{5ff07878bac730dafb506bf81d3792d8}
      \strng{authornamehash}{5ff07878bac730dafb506bf81d3792d8}
      \strng{authorfullhash}{5ff07878bac730dafb506bf81d3792d8}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Transfer learning is beneficial by allowing the expressive features of models pretrained on large-scale datasets to be finetuned for the target task of smaller, more domain-specific datasets. However, there is a concern that these pretrained models may come with their own biases which would propagate into the finetuned model. In this work, we investigate bias when conceptualized as both spurious correlations between the target task and a sensitive attribute as well as underrepresentation of a particular group in the dataset. Under both notions of bias, we find that (1) models finetuned on top of pretrained models can indeed inherit their biases, but (2) this bias can be corrected for through relatively minor interventions to the finetuning dataset, and often with a negligible impact to performance. Our findings imply that careful curation of the finetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model.}
      \field{month}{8}
      \field{note}{arXiv:2303.06167 [cs]}
      \field{title}{Overwriting {Pretrained} {Bias} with {Finetuning} {Data}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{year}{2023}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2303.06167
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2303.06167
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2303.06167
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computers and Society,Computer Science - Machine Learning}
    \endentry
    \entry{steed_upstream_2022}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=89e83674fb1257abcd38e198acb81820}{%
           family={Steed},
           familyi={S\bibinitperiod},
           given={Ryan},
           giveni={R\bibinitperiod}}}%
        {{hash=805fb0accdd23e8f8ce8b8534165e1a9}{%
           family={Panda},
           familyi={P\bibinitperiod},
           given={Swetasudha},
           giveni={S\bibinitperiod}}}%
        {{hash=910ba66005a50848a5285266f8c20c46}{%
           family={Kobren},
           familyi={K\bibinitperiod},
           given={Ari},
           giveni={A\bibinitperiod}}}%
        {{hash=791c4d0f7de912f6acf41f2d69a3d7e1}{%
           family={Wick},
           familyi={W\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Dublin, Ireland}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{86af774dcc4b31dc5ecbe2494a2820ca}
      \strng{fullhash}{c1551d333db71353d92354ed3b2cc5f8}
      \strng{bibnamehash}{c1551d333db71353d92354ed3b2cc5f8}
      \strng{authorbibnamehash}{c1551d333db71353d92354ed3b2cc5f8}
      \strng{authornamehash}{86af774dcc4b31dc5ecbe2494a2820ca}
      \strng{authorfullhash}{c1551d333db71353d92354ed3b2cc5f8}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{A few large, homogenous, pre-trained models undergird many machine learning systems — and often, these models contain harmful stereotypes learned from the internet. We investigate the bias transfer hypothesis: the theory that social biases (such as stereotypes) internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning. For two classification tasks, we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier's discriminatory behavior after fine-tuning. Regression analysis suggests that downstream disparities are better explained by biases in the fine-tuning dataset. Still, pre-training plays a role: simple alterations to co-occurrence rates in the fine-tuning dataset are ineffective when the model has been pre-trained. Our results encourage practitioners to focus more on dataset quality and context-specific harms.}
      \field{booktitle}{Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})}
      \field{month}{5}
      \field{shorttitle}{Upstream {Mitigation} {Is} \textit{ {N}}ot {All} {You} {Need}}
      \field{title}{Upstream {Mitigation} {Is} \textit{ {N}}ot {All} {You} {Need}: {Testing} the {Bias} {Transfer} {Hypothesis} in {Pre}-{Trained} {Language} {Models}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{3524\bibrangedash 3542}
      \range{pages}{19}
      \verb{doi}
      \verb 10.18653/v1/2022.acl-long.247
      \endverb
      \verb{urlraw}
      \verb https://aclanthology.org/2022.acl-long.247
      \endverb
      \verb{url}
      \verb https://aclanthology.org/2022.acl-long.247
      \endverb
    \endentry
    \entry{neyshabur_implicit_2017}{misc}{}
      \name{author}{1}{}{%
        {{hash=e5f9e207dc41c0bc364d3e1d7be37cf1}{%
           family={Neyshabur},
           familyi={N\bibinitperiod},
           given={Behnam},
           giveni={B\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{e5f9e207dc41c0bc364d3e1d7be37cf1}
      \strng{fullhash}{e5f9e207dc41c0bc364d3e1d7be37cf1}
      \strng{bibnamehash}{e5f9e207dc41c0bc364d3e1d7be37cf1}
      \strng{authorbibnamehash}{e5f9e207dc41c0bc364d3e1d7be37cf1}
      \strng{authornamehash}{e5f9e207dc41c0bc364d3e1d7be37cf1}
      \strng{authorfullhash}{e5f9e207dc41c0bc364d3e1d7be37cf1}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In an attempt to better understand generalization in deep learning, we study several possible explanations. We show that implicit regularization induced by the optimization method is playing a key role in generalization and success of deep learning models. Motivated by this view, we study how different complexity measures can ensure generalization and explain how optimization algorithms can implicitly regularize complexity measures. We empirically investigate the ability of these measures to explain different observed phenomena in deep learning. We further study the invariances in neural networks, suggest complexity measures and optimization algorithms that have similar invariances to those in neural networks and evaluate them on a number of learning tasks.}
      \field{month}{9}
      \field{note}{arXiv:1709.01953 [cs]}
      \field{title}{Implicit {Regularization} in {Deep} {Learning}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{year}{2017}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1709.01953
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1709.01953
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1709.01953
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{gunasekar_implicit_2018}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=177e105ef582d94f5b6cfd058d7384ff}{%
           family={Gunasekar},
           familyi={G\bibinitperiod},
           given={Suriya},
           giveni={S\bibinitperiod}}}%
        {{hash=f62754c1d0d862b8ab5ca448259623ba}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Jason\bibnamedelima D},
           giveni={J\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=07b30cf03223931cc6e46fff4b5b42e4}{%
           family={Soudry},
           familyi={S\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=34451c97b5a5bad6fcb6513573d1f94d}{%
           family={Srebro},
           familyi={S\bibinitperiod},
           given={Nati},
           giveni={N\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{c0826ba6005a39325a5d917b542a0403}
      \strng{fullhash}{b1430481257d5fc1d09ddc566f651c75}
      \strng{bibnamehash}{b1430481257d5fc1d09ddc566f651c75}
      \strng{authorbibnamehash}{b1430481257d5fc1d09ddc566f651c75}
      \strng{authornamehash}{c0826ba6005a39325a5d917b542a0403}
      \strng{authorfullhash}{b1430481257d5fc1d09ddc566f651c75}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Advances in {Neural} {Information} {Processing} {Systems}}
      \field{title}{Implicit {Bias} of {Gradient} {Descent} on {Linear} {Convolutional} {Networks}}
      \field{urlday}{28}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{volume}{31}
      \field{year}{2018}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2018/hash/0e98aeeb54acf612b9eb4e48a269814c-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2018/hash/0e98aeeb54acf612b9eb4e48a269814c-Abstract.html
      \endverb
    \endentry
    \entry{soudry_implicit_2018}{article}{}
      \name{author}{5}{}{%
        {{hash=07b30cf03223931cc6e46fff4b5b42e4}{%
           family={Soudry},
           familyi={S\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=823c4e365a399e7d553e258d6bcb9ae1}{%
           family={Hoffer},
           familyi={H\bibinitperiod},
           given={Elad},
           giveni={E\bibinitperiod}}}%
        {{hash=afba2ed234d5565b0fa614a01e2ddf84}{%
           family={Nacson},
           familyi={N\bibinitperiod},
           given={Mor\bibnamedelima Shpigel},
           giveni={M\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=177e105ef582d94f5b6cfd058d7384ff}{%
           family={Gunasekar},
           familyi={G\bibinitperiod},
           given={Suriya},
           giveni={S\bibinitperiod}}}%
        {{hash=4a4fc057d8331655775a21a8994f3339}{%
           family={Srebro},
           familyi={S\bibinitperiod},
           given={Nathan},
           giveni={N\bibinitperiod}}}%
      }
      \strng{namehash}{4fca6eb2a9539fac732f866cd77da2af}
      \strng{fullhash}{b79f69c64422a2642754da86abe0cb59}
      \strng{bibnamehash}{b79f69c64422a2642754da86abe0cb59}
      \strng{authorbibnamehash}{b79f69c64422a2642754da86abe0cb59}
      \strng{authornamehash}{4fca6eb2a9539fac732f866cd77da2af}
      \strng{authorfullhash}{b79f69c64422a2642754da86abe0cb59}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We examine gradient descent on unregularized logistic regression problems, with homogeneous linear predictors on linearly separable datasets. We show the predictor converges to the direction of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization in more complex models and with other optimization methods.}
      \field{issn}{1532-4435}
      \field{journaltitle}{The Journal of Machine Learning Research}
      \field{month}{1}
      \field{number}{1}
      \field{title}{The implicit bias of gradient descent on separable data}
      \field{volume}{19}
      \field{year}{2018}
      \field{pages}{2822\bibrangedash 2878}
      \range{pages}{57}
      \keyw{generalization,gradient descent,implicit regularization,logistic regression,margin}
    \endentry
    \entry{lyu_gradient_2020}{misc}{}
      \name{author}{2}{}{%
        {{hash=4842e97a9933135ceb9c94e51b7d1a3e}{%
           family={Lyu},
           familyi={L\bibinitperiod},
           given={Kaifeng},
           giveni={K\bibinitperiod}}}%
        {{hash=2bb35bf59aaca26e59bbdb0d0192f3b3}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Jian},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{531c8301934c0f7e765455d79cf39c6b}
      \strng{fullhash}{531c8301934c0f7e765455d79cf39c6b}
      \strng{bibnamehash}{531c8301934c0f7e765455d79cf39c6b}
      \strng{authorbibnamehash}{531c8301934c0f7e765455d79cf39c6b}
      \strng{authornamehash}{531c8301934c0f7e765455d79cf39c6b}
      \strng{authorfullhash}{531c8301934c0f7e765455d79cf39c6b}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper, we study the implicit regularization of the gradient descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient descent or gradient flow (i.e., gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its smoothed version converge to the objective value at a KKT point of the optimization problem. Our results generalize the previous results for logistic regression with one-layer or multi-layer linear networks, and provide more quantitative convergence results with weaker assumptions than previous results for homogeneous smooth neural networks. We conduct several experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. Finally, as margin is closely related to robustness, we discuss potential benefits of training longer for improving the robustness of the model.}
      \field{month}{12}
      \field{note}{arXiv:1906.05890 [cs, stat]}
      \field{title}{Gradient {Descent} {Maximizes} the {Margin} of {Homogeneous} {Neural} {Networks}}
      \field{urlday}{28}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2020}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1906.05890
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1906.05890
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1906.05890
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
    \endentry
    \entry{chizat_implicit_2020}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=4ce6a1097274a99bfca90cb3c6b9460b}{%
           family={Chizat},
           familyi={C\bibinitperiod},
           given={Lénaïc},
           giveni={L\bibinitperiod}}}%
        {{hash=93da2819ab01d8a5e7bae39ce6f17c1f}{%
           family={Bach},
           familyi={B\bibinitperiod},
           given={Francis},
           giveni={F\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{eea989e0ef9299667585121feaa33c2f}
      \strng{fullhash}{eea989e0ef9299667585121feaa33c2f}
      \strng{bibnamehash}{eea989e0ef9299667585121feaa33c2f}
      \strng{authorbibnamehash}{eea989e0ef9299667585121feaa33c2f}
      \strng{authornamehash}{eea989e0ef9299667585121feaa33c2f}
      \strng{authorfullhash}{eea989e0ef9299667585121feaa33c2f}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Neural networks trained to minimize the logistic (a.k.a. cross-entropy) loss with gradient-based methods are observed to perform well in many supervised classification tasks. Towards understanding this phenomenon, we analyze the training and generalization behavior of infinitely wide two-layer neural networks with homogeneous activations. We show that the limits of the gradient flow on exponentially tailed losses can be fully characterized as a max-margin classifier in a certain non-Hilbertian space of functions. In presence of hidden low-dimensional structures, the resulting margin is independent of the ambiant dimension, which leads to strong generalization bounds. In contrast, training only the output layer implicitly solves a kernel support vector machine, which a priori does not enjoy such an adaptivity. Our analysis of training is non-quantitative in terms of running time but we prove computational guarantees in simplified settings by showing equivalences with online mirror descent. Finally, numerical experiments suggest that our analysis describes well the practical behavior of two-layer neural networks with ReLU activation and confirm the statistical benefits of this implicit bias.}
      \field{booktitle}{Proceedings of {Thirty} {Third} {Conference} on {Learning} {Theory}}
      \field{month}{7}
      \field{note}{ISSN: 2640-3498}
      \field{title}{Implicit {Bias} of {Gradient} {Descent} for {Wide} {Two}-layer {Neural} {Networks} {Trained} with the {Logistic} {Loss}}
      \field{urlday}{28}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2020}
      \field{urldateera}{ce}
      \field{pages}{1305\bibrangedash 1338}
      \range{pages}{34}
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v125/chizat20a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v125/chizat20a.html
      \endverb
    \endentry
    \entry{savarese_how_2019}{article}{}
      \name{author}{4}{}{%
        {{hash=f134a417323419d25620c665207e87c9}{%
           family={Savarese},
           familyi={S\bibinitperiod},
           given={Pedro},
           giveni={P\bibinitperiod}}}%
        {{hash=e419144d3b6138db49ef952c6a1a4273}{%
           family={Evron},
           familyi={E\bibinitperiod},
           given={Itay},
           giveni={I\bibinitperiod}}}%
        {{hash=07b30cf03223931cc6e46fff4b5b42e4}{%
           family={Soudry},
           familyi={S\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=4a4fc057d8331655775a21a8994f3339}{%
           family={Srebro},
           familyi={S\bibinitperiod},
           given={Nathan},
           giveni={N\bibinitperiod}}}%
      }
      \strng{namehash}{ec9546b08b8a17df255446657f7de7b6}
      \strng{fullhash}{19e41907bc50777302b5de61526c2ba2}
      \strng{bibnamehash}{19e41907bc50777302b5de61526c2ba2}
      \strng{authorbibnamehash}{19e41907bc50777302b5de61526c2ba2}
      \strng{authornamehash}{ec9546b08b8a17df255446657f7de7b6}
      \strng{authorfullhash}{19e41907bc50777302b5de61526c2ba2}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We consider the question of what functions can be captured by ReLU networks with an unbounded number of units (infinite width), but where the overall network Euclidean norm (sum of squares of all weights in the system, except for an unregularized bias term for each unit) is bounded; or equivalently what is the minimal norm required to approximate a given function. For functions f : R → R and a single hidden layer, we show that the minimal network norm for representing f is max(´ {|}f00(x){|} dx, {|}f0(-∞) + f0(+∞){|}), and hence the minimal norm fit for a sample is given by a linear spline interpolation.}
      \field{journaltitle}{Proceedings of Machine Learning Research}
      \field{shorttitle}{How do infinite width bounded norm networks look in function space?}
      \field{title}{How do infinite width bounded norm networks look in function space?: 32nd {Conference} on {Learning} {Theory}, {COLT} 2019}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{volume}{99}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{2667\bibrangedash 2690}
      \range{pages}{24}
      \verb{urlraw}
      \verb http://www.scopus.com/inward/record.url?scp=85132757852&partnerID=8YFLogxK
      \endverb
      \verb{url}
      \verb http://www.scopus.com/inward/record.url?scp=85132757852&partnerID=8YFLogxK
      \endverb
    \endentry
    \entry{ongie_function_2019}{article}{}
      \name{author}{4}{}{%
        {{hash=a3423cac9ce9b0962cd4e7b88584100a}{%
           family={Ongie},
           familyi={O\bibinitperiod},
           given={Greg},
           giveni={G\bibinitperiod}}}%
        {{hash=c29866332917f3b90a0475c571f190c2}{%
           family={Willett},
           familyi={W\bibinitperiod},
           given={Rebecca},
           giveni={R\bibinitperiod}}}%
        {{hash=07b30cf03223931cc6e46fff4b5b42e4}{%
           family={Soudry},
           familyi={S\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=4a4fc057d8331655775a21a8994f3339}{%
           family={Srebro},
           familyi={S\bibinitperiod},
           given={Nathan},
           giveni={N\bibinitperiod}}}%
      }
      \strng{namehash}{9ed15d772aeccc499c85e0170b777d80}
      \strng{fullhash}{7264fa815d478b246f2970851036a639}
      \strng{bibnamehash}{7264fa815d478b246f2970851036a639}
      \strng{authorbibnamehash}{7264fa815d478b246f2970851036a639}
      \strng{authornamehash}{9ed15d772aeccc499c85e0170b777d80}
      \strng{authorfullhash}{7264fa815d478b246f2970851036a639}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{arXiv preprint arXiv:1910.01635}
      \field{title}{A function space view of bounded norm infinite width relu nets: {The} multivariate case}
      \field{year}{2019}
    \endentry
    \entry{dai_representation_2021}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=aa5317752ed0616c8f9ba5a095a8032f}{%
           family={Dai},
           familyi={D\bibinitperiod},
           given={Zhen},
           giveni={Z\bibinitperiod}}}%
        {{hash=67d2468427ffa64009ac16ade6f99ce5}{%
           family={Karzand},
           familyi={K\bibinitperiod},
           given={Mina},
           giveni={M\bibinitperiod}}}%
        {{hash=4a4fc057d8331655775a21a8994f3339}{%
           family={Srebro},
           familyi={S\bibinitperiod},
           given={Nathan},
           giveni={N\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{c4bddb9e028b95dbeb101f92f306956d}
      \strng{fullhash}{92274a565a971bbe61d86eee5d5df5d7}
      \strng{bibnamehash}{92274a565a971bbe61d86eee5d5df5d7}
      \strng{authorbibnamehash}{92274a565a971bbe61d86eee5d5df5d7}
      \strng{authornamehash}{c4bddb9e028b95dbeb101f92f306956d}
      \strng{authorfullhash}{92274a565a971bbe61d86eee5d5df5d7}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{booktitle}{Advances in {Neural} {Information} {Processing} {Systems}}
      \field{shorttitle}{Representation {Costs} of {Linear} {Neural} {Networks}}
      \field{title}{Representation {Costs} of {Linear} {Neural} {Networks}: {Analysis} and {Design}}
      \field{urlday}{13}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{34}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{26884\bibrangedash 26896}
      \range{pages}{13}
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper_files/paper/2021/hash/e22cb9d6bbb4c290a94e4fff4d68a831-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper_files/paper/2021/hash/e22cb9d6bbb4c290a94e4fff4d68a831-Abstract.html
      \endverb
    \endentry
    \entry{rahaman_spectral_2018}{article}{}
      \name{author}{8}{}{%
        {{hash=d4c778f9c80602fb7b4cd44117a0d8a0}{%
           family={Rahaman},
           familyi={R\bibinitperiod},
           given={Nasim},
           giveni={N\bibinitperiod}}}%
        {{hash=2d7e0f228a4066de5e2bbaf703a16d30}{%
           family={Baratin},
           familyi={B\bibinitperiod},
           given={Aristide},
           giveni={A\bibinitperiod}}}%
        {{hash=0f0c297dd56ddda3c4590124b80d0472}{%
           family={Arpit},
           familyi={A\bibinitperiod},
           given={Devansh},
           giveni={D\bibinitperiod}}}%
        {{hash=4b0006f485d3207563598a41053661bd}{%
           family={Draxler},
           familyi={D\bibinitperiod},
           given={Felix},
           giveni={F\bibinitperiod}}}%
        {{hash=d2827b813d1d73df44360201e3296bea}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Min},
           giveni={M\bibinitperiod}}}%
        {{hash=c184ed19622b27feb7d52546cc5932f6}{%
           family={Hamprecht},
           familyi={H\bibinitperiod},
           given={Fred},
           giveni={F\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
        {{hash=ccec1ccd2e1aa86960eb2e872c6b7020}{%
           family={Courville},
           familyi={C\bibinitperiod},
           given={Aaron},
           giveni={A\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{9da4ff5c16c96a7276e20d967deb609d}
      \strng{fullhash}{1fac4512de9d78d322d57a7b5e9b0379}
      \strng{bibnamehash}{9da4ff5c16c96a7276e20d967deb609d}
      \strng{authorbibnamehash}{9da4ff5c16c96a7276e20d967deb609d}
      \strng{authornamehash}{9da4ff5c16c96a7276e20d967deb609d}
      \strng{authorfullhash}{1fac4512de9d78d322d57a7b5e9b0379}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100\% accuracy. In this work we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior. Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples. We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets easier with increasing manifold complexity, and present a theoretical understanding of this behavior. Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.}
      \field{month}{9}
      \field{title}{On the {Spectral} {Bias} of {Neural} {Networks}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{year}{2018}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://openreview.net/forum?id=r1gR2sC9FX
      \endverb
      \verb{url}
      \verb https://openreview.net/forum?id=r1gR2sC9FX
      \endverb
    \endentry
    \entry{mulayoff_implicit_2021}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=7d4c242b2977625c3405dfb3038e3a07}{%
           family={Mulayoff},
           familyi={M\bibinitperiod},
           given={Rotem},
           giveni={R\bibinitperiod}}}%
        {{hash=7a20f965a29ab60ffefed7fc106e2719}{%
           family={Michaeli},
           familyi={M\bibinitperiod},
           given={Tomer},
           giveni={T\bibinitperiod}}}%
        {{hash=07b30cf03223931cc6e46fff4b5b42e4}{%
           family={Soudry},
           familyi={S\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{83a1f7fe4a0c44f3ff2e419c5f68b1c6}
      \strng{fullhash}{01e74f98da602c9938ab4fdd19a0f64a}
      \strng{bibnamehash}{01e74f98da602c9938ab4fdd19a0f64a}
      \strng{authorbibnamehash}{01e74f98da602c9938ab4fdd19a0f64a}
      \strng{authornamehash}{83a1f7fe4a0c44f3ff2e419c5f68b1c6}
      \strng{authorfullhash}{01e74f98da602c9938ab4fdd19a0f64a}
      \field{sortinit}{9}
      \field{sortinithash}{0a5ebc79d83c96b6579069544c73c7d4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{booktitle}{Advances in {Neural} {Information} {Processing} {Systems}}
      \field{shorttitle}{The {Implicit} {Bias} of {Minima} {Stability}}
      \field{title}{The {Implicit} {Bias} of {Minima} {Stability}: {A} {View} from {Function} {Space}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{volume}{34}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{17749\bibrangedash 17761}
      \range{pages}{13}
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2021/hash/944a5ae3483ed5c1e10bbccb7942a279-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2021/hash/944a5ae3483ed5c1e10bbccb7942a279-Abstract.html
      \endverb
    \endentry
    \entry{huh_low-rank_2023}{misc}{}
      \name{author}{6}{}{%
        {{hash=ced92cad45ea391d36f3f77da971067b}{%
           family={Huh},
           familyi={H\bibinitperiod},
           given={Minyoung},
           giveni={M\bibinitperiod}}}%
        {{hash=f2800380ecd0add8d852a779e92ca497}{%
           family={Mobahi},
           familyi={M\bibinitperiod},
           given={Hossein},
           giveni={H\bibinitperiod}}}%
        {{hash=0c65190dcbd461dee172354f7938ae43}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
        {{hash=a170d7309569d2d129f97b02387ac9b4}{%
           family={Cheung},
           familyi={C\bibinitperiod},
           given={Brian},
           giveni={B\bibinitperiod}}}%
        {{hash=8386009211558835d056bfe419103c76}{%
           family={Agrawal},
           familyi={A\bibinitperiod},
           given={Pulkit},
           giveni={P\bibinitperiod}}}%
        {{hash=cae9f806bc99a5f19fadea538fc2db04}{%
           family={Isola},
           familyi={I\bibinitperiod},
           given={Phillip},
           giveni={P\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{acb5d912bb2a97f8919561f9b7fba88c}
      \strng{fullhash}{6e177658c965454244f8348850ce6a51}
      \strng{bibnamehash}{acb5d912bb2a97f8919561f9b7fba88c}
      \strng{authorbibnamehash}{acb5d912bb2a97f8919561f9b7fba88c}
      \strng{authornamehash}{acb5d912bb2a97f8919561f9b7fba88c}
      \strng{authorfullhash}{6e177658c965454244f8348850ce6a51}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Modern deep neural networks are highly over-parameterized compared to the data on which they are trained, yet they often generalize remarkably well. A flurry of recent work has asked: why do deep networks not overfit to their training data? In this work, we make a series of empirical observations that investigate and extend the hypothesis that deeper networks are inductively biased to find solutions with lower effective rank embeddings. We conjecture that this bias exists because the volume of functions that maps to low effective rank embedding increases with depth. We show empirically that our claim holds true on finite width linear and non-linear models on practical learning paradigms and show that on natural data, these are often the solutions that generalize well. We then show that the simplicity bias exists at both initialization and after training and is resilient to hyper-parameters and learning methods. We further demonstrate how linear over-parameterization of deep non-linear models can be used to induce low-rank bias, improving generalization performance on CIFAR and ImageNet without changing the modeling capacity.}
      \field{month}{3}
      \field{note}{arXiv:2103.10427 [cs]}
      \field{title}{The {Low}-{Rank} {Simplicity} {Bias} in {Deep} {Networks}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{year}{2023}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2103.10427
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2103.10427
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2103.10427
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
    \endentry
    \entry{refinetti_neural_2023}{misc}{}
      \name{author}{3}{}{%
        {{hash=c53e92702f56a48885982ea8ee0c5708}{%
           family={Refinetti},
           familyi={R\bibinitperiod},
           given={Maria},
           giveni={M\bibinitperiod}}}%
        {{hash=d0c7b6856f10ba53911f1d5ffbc4ca35}{%
           family={Ingrosso},
           familyi={I\bibinitperiod},
           given={Alessandro},
           giveni={A\bibinitperiod}}}%
        {{hash=ea04adf284afb4578bfd45d13af18cd6}{%
           family={Goldt},
           familyi={G\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{3560786cbac0ac146e6ceb7a181745ec}
      \strng{fullhash}{5ff1502424ba61205ba7184188cfaec4}
      \strng{bibnamehash}{5ff1502424ba61205ba7184188cfaec4}
      \strng{authorbibnamehash}{5ff1502424ba61205ba7184188cfaec4}
      \strng{authornamehash}{3560786cbac0ac146e6ceb7a181745ec}
      \strng{authorfullhash}{5ff1502424ba61205ba7184188cfaec4}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The ability of deep neural networks to generalise well even when they interpolate their training data has been explained using various "simplicity biases". These theories postulate that neural networks avoid overfitting by first learning simple functions, say a linear classifier, before learning more complex, non-linear functions. Meanwhile, data structure is also recognised as a key ingredient for good generalisation, yet its role in simplicity biases is not yet understood. Here, we show that neural networks trained using stochastic gradient descent initially classify their inputs using lower-order input statistics, like mean and covariance, and exploit higher-order statistics only later during training. We first demonstrate this distributional simplicity bias (DSB) in a solvable model of a neural network trained on synthetic data. We empirically demonstrate DSB in a range of deep convolutional networks and visual transformers trained on CIFAR10, and show that it even holds in networks pre-trained on ImageNet. We discuss the relation of DSB to other simplicity biases and consider its implications for the principle of Gaussian universality in learning.}
      \field{month}{5}
      \field{note}{arXiv:2211.11567 [cond-mat, stat]}
      \field{title}{Neural networks trained with {SGD} learn distributions of increasing complexity}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{year}{2023}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2211.11567
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2211.11567
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2211.11567
      \endverb
      \keyw{Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,Statistics - Machine Learning}
    \endentry
    \entry{razin_implicit_2020}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=25315d27b173683535bc8330cf044475}{%
           family={Razin},
           familyi={R\bibinitperiod},
           given={Noam},
           giveni={N\bibinitperiod}}}%
        {{hash=049bbc3640fa48f818ad49526f7efcf7}{%
           family={Cohen},
           familyi={C\bibinitperiod},
           given={Nadav},
           giveni={N\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{fab221db668ed999650673b7712970d7}
      \strng{fullhash}{fab221db668ed999650673b7712970d7}
      \strng{bibnamehash}{fab221db668ed999650673b7712970d7}
      \strng{authorbibnamehash}{fab221db668ed999650673b7712970d7}
      \strng{authornamehash}{fab221db668ed999650673b7712970d7}
      \strng{authorfullhash}{fab221db668ed999650673b7712970d7}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Mathematically characterizing the implicit regularization induced by gradient-based optimization is a longstanding pursuit in the theory of deep learning. A widespread hope is that a characterization based on minimization of norms may apply, and a standard test-bed for studying this prospect is matrix factorization (matrix completion via linear neural networks). It is an open question whether norms can explain the implicit regularization in matrix factorization. The current paper resolves this open question in the negative, by proving that there exist natural matrix factorization problems on which the implicit regularization drives all norms (and quasi-norms) towards infinity. Our results suggest that, rather than perceiving the implicit regularization via norms, a potentially more useful interpretation is minimization of rank. We demonstrate empirically that this interpretation extends to a certain class of non-linear neural networks, and hypothesize that it may be key to explaining generalization in deep learning.}
      \field{booktitle}{Advances in {Neural} {Information} {Processing} {Systems}}
      \field{title}{Implicit {Regularization} in {Deep} {Learning} {May} {Not} {Be} {Explainable} by {Norms}}
      \field{urlday}{29}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{volume}{33}
      \field{year}{2020}
      \field{urldateera}{ce}
      \field{pages}{21174\bibrangedash 21187}
      \range{pages}{14}
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2020/hash/f21e255f89e0f258accbe4e984eef486-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2020/hash/f21e255f89e0f258accbe4e984eef486-Abstract.html
      \endverb
    \endentry
    \entry{boursier_gradient_2022}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=9e4e7dc476c0d646e94a4c62610e9d9d}{%
           family={Boursier},
           familyi={B\bibinitperiod},
           given={Etienne},
           giveni={E\bibinitperiod}}}%
        {{hash=e7abbaf3659900c715981a63c38087a0}{%
           family={Pillaud-Vivien},
           familyi={P\bibinithyphendelim V\bibinitperiod},
           given={Loucas},
           giveni={L\bibinitperiod}}}%
        {{hash=e2d7053f85144a452931150e4ba9f19c}{%
           family={Flammarion},
           familyi={F\bibinitperiod},
           given={Nicolas},
           giveni={N\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{d5ed4035116aa9880e1ae61c3deb5b22}
      \strng{fullhash}{3cce00dceba3de9546c91f22c6a7d978}
      \strng{bibnamehash}{3cce00dceba3de9546c91f22c6a7d978}
      \strng{authorbibnamehash}{3cce00dceba3de9546c91f22c6a7d978}
      \strng{authornamehash}{d5ed4035116aa9880e1ae61c3deb5b22}
      \strng{authorfullhash}{3cce00dceba3de9546c91f22c6a7d978}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The training of neural networks by gradient descent methods is a cornerstone of the deep learning revolution. Yet, despite some recent progress, a complete theory explaining its success is still missing. This article presents, for orthogonal input vectors, a precise description of the gradient flow dynamics of training one-hidden layer ReLU neural networks for the mean squared error at small initialisation. In this setting, despite non-convexity, we show that the gradient flow converges to zero loss and characterise its implicit bias towards minimum variation norm. Furthermore, some interesting phenomena are highlighted: a quantitative description of the initial alignment phenomenon and a proof that the process follows a specific saddle to saddle dynamics.}
      \field{month}{5}
      \field{title}{Gradient flow dynamics of shallow {ReLU} networks for square loss and orthogonal inputs}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://openreview.net/forum?id=L74c-iUxQ1I
      \endverb
      \verb{url}
      \verb https://openreview.net/forum?id=L74c-iUxQ1I
      \endverb
    \endentry
    \entry{maurer_benefit_2016}{article}{}
      \name{author}{3}{}{%
        {{hash=b8a9d235ee4f408b42fe52d057c5dd80}{%
           family={Maurer},
           familyi={M\bibinitperiod},
           given={Andreas},
           giveni={A\bibinitperiod}}}%
        {{hash=8b5942a7f10e47d565321b3dce4730d3}{%
           family={Pontil},
           familyi={P\bibinitperiod},
           given={Massimiliano},
           giveni={M\bibinitperiod}}}%
        {{hash=a4ad88ba00428c727fd7a7295f097c6d}{%
           family={Romera-Paredes},
           familyi={R\bibinithyphendelim P\bibinitperiod},
           given={Bernardino},
           giveni={B\bibinitperiod}}}%
      }
      \strng{namehash}{f0562ad972cbbcc5bfc452b7c6900e62}
      \strng{fullhash}{4db3c9b74f7653c10eba3b0dcfef5a23}
      \strng{bibnamehash}{4db3c9b74f7653c10eba3b0dcfef5a23}
      \strng{authorbibnamehash}{4db3c9b74f7653c10eba3b0dcfef5a23}
      \strng{authornamehash}{f0562ad972cbbcc5bfc452b7c6900e62}
      \strng{authorfullhash}{4db3c9b74f7653c10eba3b0dcfef5a23}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We discuss a general method to learn data representations from multiple tasks. We provide a justification for this method in both settings of multitask learning and learning-to-learn. The method is illustrated in detail in the special case of linear feature learning. Conditions on the theoretical advantage offered by multitask representation learning over independent task learning are established. In particular, focusing on the important example of half-space learning, we derive the regime in which multitask representation learning is beneficial over independent task learning, as a function of the sample size, the number of tasks and the intrinsic data dimensionality. Other potential applications of our results include multitask feature learning in reproducing kernel Hilbert spaces and multilayer, deep networks.}
      \field{issn}{1533-7928}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{number}{81}
      \field{title}{The {Benefit} of {Multitask} {Representation} {Learning}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{volume}{17}
      \field{year}{2016}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 32}
      \range{pages}{32}
      \verb{urlraw}
      \verb http://jmlr.org/papers/v17/15-242.html
      \endverb
      \verb{url}
      \verb http://jmlr.org/papers/v17/15-242.html
      \endverb
    \endentry
    \entry{wu_understanding_2020}{misc}{}
      \name{author}{3}{}{%
        {{hash=d391f69afd4ca19c6ef23ac44f186e61}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Sen},
           giveni={S\bibinitperiod}}}%
        {{hash=122166376c8b74b7a6aac5227bedfd14}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Hongyang\bibnamedelima R.},
           giveni={H\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=eb1f5a2ba08ea12db36c871fe601f5f1}{%
           family={Ré},
           familyi={R\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{df4567d40241815c0f758189902a628a}
      \strng{fullhash}{aa3b1128cd600c58fc8a82303232fd9a}
      \strng{bibnamehash}{aa3b1128cd600c58fc8a82303232fd9a}
      \strng{authorbibnamehash}{aa3b1128cd600c58fc8a82303232fd9a}
      \strng{authornamehash}{df4567d40241815c0f758189902a628a}
      \strng{authorfullhash}{aa3b1128cd600c58fc8a82303232fd9a}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks' data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks' embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtain a 2.35\% GLUE score average improvement on 5 GLUE tasks over BERT-LARGE using our alignment method. We also design an SVD-based task reweighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.}
      \field{month}{5}
      \field{note}{arXiv:2005.00944 [cs]}
      \field{title}{Understanding and {Improving} {Information} {Transfer} in {Multi}-{Task} {Learning}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{year}{2020}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2005.00944
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2005.00944
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2005.00944
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning}
    \endentry
    \entry{collins_provable_2024}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=87af5de512e5c32d789edd5f1918d58c}{%
           family={Collins},
           familyi={C\bibinitperiod},
           given={Liam},
           giveni={L\bibinitperiod}}}%
        {{hash=2ae4b63ed737cd7c460502a3d6aa594a}{%
           family={Hassani},
           familyi={H\bibinitperiod},
           given={Hamed},
           giveni={H\bibinitperiod}}}%
        {{hash=cdc208189d9717c3f2ecb7869ad7a68a}{%
           family={Soltanolkotabi},
           familyi={S\bibinitperiod},
           given={Mahdi},
           giveni={M\bibinitperiod}}}%
        {{hash=a5cf1aa4b616c0bb6335bc3307393ab9}{%
           family={Mokhtari},
           familyi={M\bibinitperiod},
           given={Aryan},
           giveni={A\bibinitperiod}}}%
        {{hash=137123f327c1ffb363287fa6de2f2c2e}{%
           family={Shakkottai},
           familyi={S\bibinitperiod},
           given={Sanjay},
           giveni={S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{ae9b7f99660c5038bf61f434e6abe335}
      \strng{fullhash}{2ee88da18e9426c8ad199daeb90a1c51}
      \strng{bibnamehash}{2ee88da18e9426c8ad199daeb90a1c51}
      \strng{authorbibnamehash}{2ee88da18e9426c8ad199daeb90a1c51}
      \strng{authornamehash}{ae9b7f99660c5038bf61f434e6abe335}
      \strng{authorfullhash}{2ee88da18e9426c8ad199daeb90a1c51}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{An increasingly popular machine learning paradigm is to pretrain a neural network (NN) on many tasks offline, then adapt it to downstream tasks, often by re-training only the last linear layer of the network. This approach yields strong downstream performance in a variety of contexts, demonstrating that multitask pretraining leads to effective feature learning. Although several recent theoretical studies have shown that shallow NNs learn meaningful features when either (i) they are trained on a *single* task or (ii) they are *linear*, very little is known about the closer-to-practice case of *nonlinear* NNs trained on *multiple* tasks. In this work, we present the first results proving that feature learning occurs during training with a nonlinear model on multiple tasks. Our key insight is that multi-task pretraining induces a pseudo-contrastive loss that favors representations that align points that typically have the same label across tasks. Using this observation, we show that when the tasks are binary classification tasks with labels depending on the projection of the data onto an \$r\$-dimensional subspace within the \$d{\textbackslash}gg r\$-dimensional input space, a simple gradient-based multitask learning algorithm on a two-layer ReLU NN recovers this projection, allowing for generalization to downstream tasks with sample and neuron complexity independent of \$d\$. In contrast, we show that with high probability over the draw of a single task, training on this single task cannot guarantee to learn all \$r\$ ground-truth features.}
      \field{month}{6}
      \field{title}{Provable {Multi}-{Task} {Representation} {Learning} by {Two}-{Layer} {ReLU} {Neural} {Networks}}
      \field{urlday}{30}
      \field{urlmonth}{10}
      \field{urlyear}{2024}
      \field{year}{2024}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://openreview.net/forum?id=M8UbECx485
      \endverb
      \verb{url}
      \verb https://openreview.net/forum?id=M8UbECx485
      \endverb
    \endentry
    \entry{braun_exact_2022}{article}{}
      \name{author}{4}{}{%
        {{hash=4b7d5150083c0f2389dbe2e045f25427}{%
           family={Braun},
           familyi={B\bibinitperiod},
           given={Lukas},
           giveni={L\bibinitperiod}}}%
        {{hash=687b5443d6e9dca7e9b9f5dd84cf1f5a}{%
           family={Dominé},
           familyi={D\bibinitperiod},
           given={Clémentine},
           giveni={C\bibinitperiod}}}%
        {{hash=265b9c348156b39acd0a12448b8b9e2f}{%
           family={Fitzgerald},
           familyi={F\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod}}}%
        {{hash=628311f82785c7d3fadf7482ea821c44}{%
           family={Saxe},
           familyi={S\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{89c9b2607206de226030a33c2aa4147d}
      \strng{fullhash}{d701dae0587438df34a0a4cfe0cc700a}
      \strng{bibnamehash}{d701dae0587438df34a0a4cfe0cc700a}
      \strng{authorbibnamehash}{d701dae0587438df34a0a4cfe0cc700a}
      \strng{authornamehash}{89c9b2607206de226030a33c2aa4147d}
      \strng{authorfullhash}{d701dae0587438df34a0a4cfe0cc700a}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Advances in Neural Information Processing Systems}
      \field{month}{12}
      \field{title}{Exact learning dynamics of deep linear networks with prior knowledge}
      \field{urlday}{30}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{volume}{35}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{6615\bibrangedash 6629}
      \range{pages}{15}
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper_files/paper/2022/hash/2b3bb2c95195130977a51b3bb251c40a-Abstract-Conference.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper_files/paper/2022/hash/2b3bb2c95195130977a51b3bb251c40a-Abstract-Conference.html
      \endverb
    \endentry
    \entry{shachaf_theoretical_2021}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=546e821940b317b7fe3c20e78de1c120}{%
           family={Shachaf},
           familyi={S\bibinitperiod},
           given={Gal},
           giveni={G\bibinitperiod}}}%
        {{hash=b73d179cb38c337fca11643785220aac}{%
           family={Brutzkus},
           familyi={B\bibinitperiod},
           given={Alon},
           giveni={A\bibinitperiod}}}%
        {{hash=e8b604d0a7af5ee88043f8e31a4c6871}{%
           family={Globerson},
           familyi={G\bibinitperiod},
           given={Amir},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{9ecd31dfd9fc03959e9936ac348b9d60}
      \strng{fullhash}{0541d3418ed2fd7e0c29ed0399a82a2b}
      \strng{bibnamehash}{0541d3418ed2fd7e0c29ed0399a82a2b}
      \strng{authorbibnamehash}{0541d3418ed2fd7e0c29ed0399a82a2b}
      \strng{authornamehash}{9ecd31dfd9fc03959e9936ac348b9d60}
      \strng{authorfullhash}{0541d3418ed2fd7e0c29ed0399a82a2b}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Fine-tuning is a common practice in deep learning, achieving excellent generalization results on downstream tasks using relatively little training data. Although widely used in practice, it is not well understood theoretically. Here we analyze the sample complexity of this scheme for regression with linear teachers in several settings. Intuitively, the success of fine-tuning depends on the similarity between the source tasks and the target task. But what is the right way of measuring this similarity? We show that the relevant measure has to do with the relation between the source task, the target task and the covariance structure of the target data. In the setting of linear regression, we show that under realistic settings there can be substantial sample complexity reduction when the above measure is low. For deep linear regression, we propose a novel result regarding the inductive bias of gradient-based training when the network is initialized with pretrained weights. Using this result we show that the similarity measure for this setting is also affected by the depth of the network. We conclude with results on shallow ReLU models, and analyze the dependence of sample complexity there on source and target tasks. We empirically demonstrate our results for both synthetic and realistic data.}
      \field{booktitle}{Advances in {Neural} {Information} {Processing} {Systems}}
      \field{title}{A {Theoretical} {Analysis} of {Fine}-tuning with {Linear} {Teachers}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{volume}{34}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{15382\bibrangedash 15394}
      \range{pages}{13}
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2021/hash/82039d16dce0aab3913b6a7ac73deff7-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2021/hash/82039d16dce0aab3913b6a7ac73deff7-Abstract.html
      \endverb
    \endentry
    \entry{evron_how_2022}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=e419144d3b6138db49ef952c6a1a4273}{%
           family={Evron},
           familyi={E\bibinitperiod},
           given={Itay},
           giveni={I\bibinitperiod}}}%
        {{hash=609ace7cd4540190d711798fdf8b4d3d}{%
           family={Moroshko},
           familyi={M\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod}}}%
        {{hash=4ddb6996bd47eabf820620ea28aa9140}{%
           family={Ward},
           familyi={W\bibinitperiod},
           given={Rachel},
           giveni={R\bibinitperiod}}}%
        {{hash=4a4fc057d8331655775a21a8994f3339}{%
           family={Srebro},
           familyi={S\bibinitperiod},
           given={Nathan},
           giveni={N\bibinitperiod}}}%
        {{hash=07b30cf03223931cc6e46fff4b5b42e4}{%
           family={Soudry},
           familyi={S\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{e582b6db6824b21ee0720b6e2d512898}
      \strng{fullhash}{89dcaf52a90d0a539a246f6f85223cf4}
      \strng{bibnamehash}{89dcaf52a90d0a539a246f6f85223cf4}
      \strng{authorbibnamehash}{89dcaf52a90d0a539a246f6f85223cf4}
      \strng{authornamehash}{e582b6db6824b21ee0720b6e2d512898}
      \strng{authorfullhash}{89dcaf52a90d0a539a246f6f85223cf4}
      \field{extraname}{1}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Conference on {Learning} {Theory}}
      \field{title}{How catastrophic can catastrophic forgetting be in linear regression?}
      \field{year}{2022}
      \field{pages}{4028\bibrangedash 4079}
      \range{pages}{52}
    \endentry
    \entry{lin_theory_2023}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=36595e014ac713cb3bf1ba2df20fa26a}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Sen},
           giveni={S\bibinitperiod}}}%
        {{hash=0beafb74195dc2b9219e95b6ae9d83a8}{%
           family={Ju},
           familyi={J\bibinitperiod},
           given={Peizhong},
           giveni={P\bibinitperiod}}}%
        {{hash=be4ec9d52691f418d2df8f3c182bf506}{%
           family={Liang},
           familyi={L\bibinitperiod},
           given={Yingbin},
           giveni={Y\bibinitperiod}}}%
        {{hash=4a3ad8132d8c58b9e9718104d432bc73}{%
           family={Shroff},
           familyi={S\bibinitperiod},
           given={Ness},
           giveni={N\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{a73825b16c6f6b78c0a2eb536f2c1c71}
      \strng{fullhash}{977fdcd68a1d3191164605944f93a777}
      \strng{bibnamehash}{977fdcd68a1d3191164605944f93a777}
      \strng{authorbibnamehash}{977fdcd68a1d3191164605944f93a777}
      \strng{authornamehash}{a73825b16c6f6b78c0a2eb536f2c1c71}
      \strng{authorfullhash}{977fdcd68a1d3191164605944f93a777}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Continual learning (CL), which aims to learn a sequence of tasks, has attracted significant recent attention. However, most work has focused on the experimental performance of CL, and theoretical studies of CL are still limited. In particular, there is a lack of understanding on what factors are important and how they affect "catastrophic forgetting" and generalization performance. To fill this gap, our theoretical analysis, under overparameterized linear models, provides the first-known explicit form of the expected forgetting and generalization error for a general CL setup with an arbitrary number of tasks. Further analysis of such a key result yields a number of theoretical explanations about how overparameterization, task similarity, and task ordering affect both forgetting and generalization error of CL. More interestingly, by conducting experiments on real datasets using deep neural networks (DNNs), we show that some of these insights even go beyond the linear models and can be carried over to practical setups. In particular, we use concrete examples to show that our results not only explain some interesting empirical observations in recent studies, but also motivate better practical algorithm designs of CL.}
      \field{booktitle}{Proceedings of the 40th {International} {Conference} on {Machine} {Learning}}
      \field{month}{7}
      \field{note}{ISSN: 2640-3498}
      \field{title}{Theory on {Forgetting} and {Generalization} of {Continual} {Learning}}
      \field{urlday}{29}
      \field{urlmonth}{10}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{urldateera}{ce}
      \field{pages}{21078\bibrangedash 21100}
      \range{pages}{23}
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v202/lin23f.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v202/lin23f.html
      \endverb
    \endentry
    \entry{evron_continual_2023}{inproceedings}{}
      \name{author}{7}{}{%
        {{hash=e419144d3b6138db49ef952c6a1a4273}{%
           family={Evron},
           familyi={E\bibinitperiod},
           given={Itay},
           giveni={I\bibinitperiod}}}%
        {{hash=609ace7cd4540190d711798fdf8b4d3d}{%
           family={Moroshko},
           familyi={M\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod}}}%
        {{hash=e9002fcd1710c995ea2052fec425188a}{%
           family={Buzaglo},
           familyi={B\bibinitperiod},
           given={Gon},
           giveni={G\bibinitperiod}}}%
        {{hash=95ef6e43dc305e5c9a5e5713dcc18da9}{%
           family={Khriesh},
           familyi={K\bibinitperiod},
           given={Maroun},
           giveni={M\bibinitperiod}}}%
        {{hash=c675aee01411d53939853db844b9b552}{%
           family={Marjieh},
           familyi={M\bibinitperiod},
           given={Badea},
           giveni={B\bibinitperiod}}}%
        {{hash=4a4fc057d8331655775a21a8994f3339}{%
           family={Srebro},
           familyi={S\bibinitperiod},
           given={Nathan},
           giveni={N\bibinitperiod}}}%
        {{hash=07b30cf03223931cc6e46fff4b5b42e4}{%
           family={Soudry},
           familyi={S\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{e582b6db6824b21ee0720b6e2d512898}
      \strng{fullhash}{2ba3e10362efec2ed1eb0e643338c108}
      \strng{bibnamehash}{e582b6db6824b21ee0720b6e2d512898}
      \strng{authorbibnamehash}{e582b6db6824b21ee0720b6e2d512898}
      \strng{authornamehash}{e582b6db6824b21ee0720b6e2d512898}
      \strng{authorfullhash}{2ba3e10362efec2ed1eb0e643338c108}
      \field{extraname}{2}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{International {Conference} on {Machine} {Learning}}
      \field{title}{Continual learning in linear classification on separable data}
      \field{year}{2023}
      \field{pages}{9440\bibrangedash 9484}
      \range{pages}{45}
    \endentry
    \entry{goldfarb_joint_2024}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=fc2d61c7a2d60b8f2e3b7d3ca5172d27}{%
           family={Goldfarb},
           familyi={G\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=e419144d3b6138db49ef952c6a1a4273}{%
           family={Evron},
           familyi={E\bibinitperiod},
           given={Itay},
           giveni={I\bibinitperiod}}}%
        {{hash=6a014b03c94209e2bb9e072b00f1de78}{%
           family={Weinberger},
           familyi={W\bibinitperiod},
           given={Nir},
           giveni={N\bibinitperiod}}}%
        {{hash=07b30cf03223931cc6e46fff4b5b42e4}{%
           family={Soudry},
           familyi={S\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=f898235958e87e52dd2335fbb91b5553}{%
           family={HAnd},
           familyi={H\bibinitperiod},
           given={PAul},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{54a49d5074c9c837f3835155707129fb}
      \strng{fullhash}{cd16191627835f6d366583c4b8e31d41}
      \strng{bibnamehash}{cd16191627835f6d366583c4b8e31d41}
      \strng{authorbibnamehash}{cd16191627835f6d366583c4b8e31d41}
      \strng{authornamehash}{54a49d5074c9c837f3835155707129fb}
      \strng{authorfullhash}{cd16191627835f6d366583c4b8e31d41}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{The {Twelfth} {International} {Conference} on {Learning} {Representations}}
      \field{title}{The {Joint} {Effect} of {Task} {Similarity} and {Overparameterization} on {Catastrophic} {Forgetting}—{An} {Analytical} {Model}}
      \field{year}{2024}
    \endentry
    \entry{lee_continual_2021}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=5ecc89fd659773a7fb5a01c49f2901f0}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
        {{hash=ea04adf284afb4578bfd45d13af18cd6}{%
           family={Goldt},
           familyi={G\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
        {{hash=628311f82785c7d3fadf7482ea821c44}{%
           family={Saxe},
           familyi={S\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{11bd9251d8a5fabf22f68b13e2907b36}
      \strng{fullhash}{7ec1e5e5f41b6f823b3ef2d6aae2ead6}
      \strng{bibnamehash}{7ec1e5e5f41b6f823b3ef2d6aae2ead6}
      \strng{authorbibnamehash}{7ec1e5e5f41b6f823b3ef2d6aae2ead6}
      \strng{authornamehash}{11bd9251d8a5fabf22f68b13e2907b36}
      \strng{authorfullhash}{7ec1e5e5f41b6f823b3ef2d6aae2ead6}
      \field{extraname}{1}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Continual learning\{—\}the ability to learn many tasks in sequence\{—\}is critical for artificial learning systems. Yet standard training methods for deep networks often suffer from catastrophic forgetting, where learning new tasks erases knowledge of the earlier tasks. While catastrophic forgetting labels the problem, the theoretical reasons for interference between tasks remain unclear. Here, we attempt to narrow this gap between theory and practice by studying continual learning in the teacher-student setup. We extend previous analytical work on two-layer networks in the teacher-student setup to multiple teachers. Using each teacher to represent a different task, we investigate how the relationship between teachers affects the amount of forgetting and transfer exhibited by the student when the task switches. In line with recent work, we find that when tasks depend on similar features, intermediate task similarity leads to greatest forgetting. However, feature similarity is only one way in which tasks may be related. The teacher-student approach allows us to disentangle task similarity at the level of {\textbackslash}emph\{readouts\} (hidden-to-output weights) as well as {\textbackslash}emph\{features\} (input-to-hidden weights). We find a complex interplay between both types of similarity, initial transfer/forgetting rates, maximum transfer/forgetting, and the long-time (post-switch) amount of transfer/forgetting. Together, these results help illuminate the diverse factors contributing to catastrophic forgetting.}
      \field{booktitle}{Proceedings of the 38th {International} {Conference} on {Machine} {Learning}}
      \field{month}{7}
      \field{note}{ISSN: 2640-3498}
      \field{shorttitle}{Continual {Learning} in the {Teacher}-{Student} {Setup}}
      \field{title}{Continual {Learning} in the {Teacher}-{Student} {Setup}: {Impact} of {Task} {Similarity}}
      \field{urlday}{29}
      \field{urlmonth}{10}
      \field{urlyear}{2024}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{6109\bibrangedash 6119}
      \range{pages}{11}
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v139/lee21e.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v139/lee21e.html
      \endverb
    \endentry
    \entry{lee_maslows_2022}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=5ecc89fd659773a7fb5a01c49f2901f0}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
        {{hash=1d671b4f5d58173319bd9fb10085b2c9}{%
           family={Mannelli},
           familyi={M\bibinitperiod},
           given={Stefano\bibnamedelima Sarao},
           giveni={S\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=326600b0ad340d8e0cd5df377b62e046}{%
           family={Clopath},
           familyi={C\bibinitperiod},
           given={Claudia},
           giveni={C\bibinitperiod}}}%
        {{hash=ea04adf284afb4578bfd45d13af18cd6}{%
           family={Goldt},
           familyi={G\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
        {{hash=628311f82785c7d3fadf7482ea821c44}{%
           family={Saxe},
           familyi={S\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{11bd9251d8a5fabf22f68b13e2907b36}
      \strng{fullhash}{1a789d481368f8b7934f9b69a2461b0a}
      \strng{bibnamehash}{1a789d481368f8b7934f9b69a2461b0a}
      \strng{authorbibnamehash}{1a789d481368f8b7934f9b69a2461b0a}
      \strng{authornamehash}{11bd9251d8a5fabf22f68b13e2907b36}
      \strng{authorfullhash}{1a789d481368f8b7934f9b69a2461b0a}
      \field{extraname}{2}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Continual learning—learning new tasks in sequence while maintaining performance on old tasks—remains particularly challenging for artificial neural networks. Surprisingly, the amount of forgetting does not increase with the dissimilarity between the learned tasks, but appears to be worst in an intermediate similarity regime. In this paper we theoretically analyse both a synthetic teacher-student framework and a real data setup to provide an explanation of this phenomenon that we name Maslow’s Hammer hypothesis. Our analysis reveals the presence of a trade-off between node activation and node re-use that results in worst forgetting in the intermediate regime. Using this understanding we reinterpret popular algorithmic interventions for catastrophic interference in terms of this trade-off, and identify the regimes in which they are most effective.}
      \field{booktitle}{Proceedings of the 39th {International} {Conference} on {Machine} {Learning}}
      \field{month}{6}
      \field{note}{ISSN: 2640-3498}
      \field{shorttitle}{Maslow’s {Hammer} in {Catastrophic} {Forgetting}}
      \field{title}{Maslow’s {Hammer} in {Catastrophic} {Forgetting}: {Node} {Re}-{Use} vs. {Node} {Activation}}
      \field{urlday}{29}
      \field{urlmonth}{10}
      \field{urlyear}{2024}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{12455\bibrangedash 12477}
      \range{pages}{23}
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v162/lee22g.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v162/lee22g.html
      \endverb
    \endentry
    \entry{dery_should_2021}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=ff5be1430cd335b59989936a4b95cfb4}{%
           family={Dery},
           familyi={D\bibinitperiod},
           given={Lucio\bibnamedelima M.},
           giveni={L\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=a832081e707e8fdbdc66a81ecc0d0823}{%
           family={Michel},
           familyi={M\bibinitperiod},
           given={Paul},
           giveni={P\bibinitperiod}}}%
        {{hash=70e0fe957473b80f1b1b5dcb8e99d31e}{%
           family={Talwalkar},
           familyi={T\bibinitperiod},
           given={Ameet},
           giveni={A\bibinitperiod}}}%
        {{hash=fcd2ecb2a6a4a3db4c9e66dddf4660b1}{%
           family={Neubig},
           familyi={N\bibinitperiod},
           given={Graham},
           giveni={G\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{762de2d0c1a5694e2fc363e0290491cf}
      \strng{fullhash}{b9d07b95784afb84a30f438fe7cc3aa7}
      \strng{bibnamehash}{b9d07b95784afb84a30f438fe7cc3aa7}
      \strng{authorbibnamehash}{b9d07b95784afb84a30f438fe7cc3aa7}
      \strng{authornamehash}{762de2d0c1a5694e2fc363e0290491cf}
      \strng{authorfullhash}{b9d07b95784afb84a30f438fe7cc3aa7}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{In most settings of practical concern, machine learning practitioners know in advance what end-task they wish to boost with auxiliary tasks. However, widely used methods for leveraging auxiliary data like pre-training and its continued-pretraining variant are end-task agnostic: they rarely, if ever, exploit knowledge of the target task. We study replacing end-task agnostic continued training of pre-trained language models with end-task aware training of said models. We argue that for sufficiently important end-tasks, the benefits of leveraging auxiliary data in a task-aware fashion can justify forgoing the traditional approach of obtaining generic, end-task agnostic representations as with (continued) pre-training. On three different low-resource NLP tasks from two domains, we demonstrate that multi-tasking the end-task and auxiliary objectives results in significantly better downstream task performance than the widely-used task-agnostic continued pre-training paradigm of Gururangan et al. (2020). We next introduce an online meta-learning algorithm that learns a set of multi-task weights to better balance among our multiple auxiliary objectives, achieving further improvements on end-task performance and data efficiency.}
      \field{month}{10}
      \field{shorttitle}{Should {We} {Be} {Pre}-training?}
      \field{title}{Should {We} {Be} {Pre}-training? {An} {Argument} for {End}-task {Aware} {Training} as an {Alternative}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{year}{2021}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://openreview.net/forum?id=2bO2x8NAIMB
      \endverb
      \verb{url}
      \verb https://openreview.net/forum?id=2bO2x8NAIMB
      \endverb
    \endentry
    \entry{weller_when_2022}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=a390033bb496c7e945d44bfae3e5cb96}{%
           family={Weller},
           familyi={W\bibinitperiod},
           given={Orion},
           giveni={O\bibinitperiod}}}%
        {{hash=32e811df2ade2aa290350cc893ccca2c}{%
           family={Seppi},
           familyi={S\bibinitperiod},
           given={Kevin},
           giveni={K\bibinitperiod}}}%
        {{hash=6adacaf607ef85a7b55da44661636929}{%
           family={Gardner},
           familyi={G\bibinitperiod},
           given={Matt},
           giveni={M\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Dublin, Ireland}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{6dd8ae93139218f4ee6c5e54d1d36a5e}
      \strng{fullhash}{b8a497e5ac7cf404f6197e9a7ae602fa}
      \strng{bibnamehash}{b8a497e5ac7cf404f6197e9a7ae602fa}
      \strng{authorbibnamehash}{b8a497e5ac7cf404f6197e9a7ae602fa}
      \strng{authornamehash}{6dd8ae93139218f4ee6c5e54d1d36a5e}
      \strng{authorfullhash}{b8a497e5ac7cf404f6197e9a7ae602fa}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Transfer learning (TL) in natural language processing (NLP) has seen a surge of interest in recent years, as pre-trained models have shown an impressive ability to transfer to novel tasks. Three main strategies have emerged for making use of multiple supervised datasets during fine-tuning: training on an intermediate task before training on the target task (STILTs), using multi-task learning (MTL) to train jointly on a supplementary task and the target task (pairwise MTL), or simply using MTL to train jointly on all available datasets (MTL-ALL). In this work, we compare all three TL methods in a comprehensive analysis on the GLUE dataset suite. We find that there is a simple heuristic for when to use one of these techniques over the other: pairwise MTL is better than STILTs when the target task has fewer instances than the supporting task and vice versa. We show that this holds true in more than 92\% of applicable cases on the GLUE dataset and validate this hypothesis with experiments varying dataset size. The simplicity and effectiveness of this heuristic is surprising and warrants additional exploration by the TL community. Furthermore, we find that MTL-ALL is worse than the pairwise methods in almost every case. We hope this study will aid others as they choose between TL methods for NLP tasks.}
      \field{booktitle}{Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})}
      \field{month}{5}
      \field{title}{When to {Use} {Multi}-{Task} {Learning} vs {Intermediate} {Fine}-{Tuning} for {Pre}-{Trained} {Encoder} {Transfer} {Learning}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{272\bibrangedash 282}
      \range{pages}{11}
      \verb{doi}
      \verb 10.18653/v1/2022.acl-short.30
      \endverb
      \verb{urlraw}
      \verb https://aclanthology.org/2022.acl-short.30
      \endverb
      \verb{url}
      \verb https://aclanthology.org/2022.acl-short.30
      \endverb
    \endentry
    \entry{kumar_fine-tuning_2022}{misc}{}
      \name{author}{5}{}{%
        {{hash=1047181628ae2fc1a1cf6271af00ce82}{%
           family={Kumar},
           familyi={K\bibinitperiod},
           given={Ananya},
           giveni={A\bibinitperiod}}}%
        {{hash=7bb584b014f043d94e43a8395735799e}{%
           family={Raghunathan},
           familyi={R\bibinitperiod},
           given={Aditi},
           giveni={A\bibinitperiod}}}%
        {{hash=13e69d41b4f2805a58fd893ba7f70124}{%
           family={Jones},
           familyi={J\bibinitperiod},
           given={Robbie},
           giveni={R\bibinitperiod}}}%
        {{hash=5bd4a91e09b499d15e82bc929acf1e34}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Tengyu},
           giveni={T\bibinitperiod}}}%
        {{hash=86c032bbc45c3b616f0a1170befc0e82}{%
           family={Liang},
           familyi={L\bibinitperiod},
           given={Percy},
           giveni={P\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{36fda4636a488f42eef12fcb0bead251}
      \strng{fullhash}{17575fde5a9680e926744627ccaa11a1}
      \strng{bibnamehash}{17575fde5a9680e926744627ccaa11a1}
      \strng{authorbibnamehash}{17575fde5a9680e926744627ccaa11a1}
      \strng{authornamehash}{36fda4636a488f42eef12fcb0bead251}
      \strng{authorfullhash}{17575fde5a9680e926744627ccaa11a1}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer -- the "head"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR \${\textbackslash}to\$ STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2\% higher accuracy ID but 7\% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head -- this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1\% better ID, 10\% better OOD than full fine-tuning).}
      \field{month}{2}
      \field{note}{arXiv:2202.10054 [cs]}
      \field{title}{Fine-{Tuning} can {Distort} {Pretrained} {Features} and {Underperform} {Out}-of-{Distribution}}
      \field{urlday}{29}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2202.10054
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2202.10054
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2202.10054
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
    \endentry
    \entry{kornblith_better_2019}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=7fc0edf04cce351dda404579ede3d0d1}{%
           family={Kornblith},
           familyi={K\bibinitperiod},
           given={Simon},
           giveni={S\bibinitperiod}}}%
        {{hash=9b81f43a31f185cea38a7a03620764eb}{%
           family={Shlens},
           familyi={S\bibinitperiod},
           given={Jon},
           giveni={J\bibinitperiod}}}%
        {{hash=c636f146591d51579a8119b777394878}{%
           family={Le},
           familyi={L\bibinitperiod},
           given={Quoc\bibnamedelima V.},
           giveni={Q\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
      }
      \strng{namehash}{85d441f32944674f6dd87a3cae8f2d38}
      \strng{fullhash}{3ef07d258224044af9be9965bee700ec}
      \strng{bibnamehash}{3ef07d258224044af9be9965bee700ec}
      \strng{authorbibnamehash}{3ef07d258224044af9be9965bee700ec}
      \strng{authornamehash}{85d441f32944674f6dd87a3cae8f2d38}
      \strng{authorfullhash}{3ef07d258224044af9be9965bee700ec}
      \field{extraname}{1}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Do better {ImageNet} models transfer better?}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://arxiv.org/pdf/1805.08974.pdf
      \endverb
      \verb{url}
      \verb https://arxiv.org/pdf/1805.08974.pdf
      \endverb
    \endentry
    \entry{woodworth_kernel_2020}{inproceedings}{}
      \name{author}{8}{}{%
        {{hash=b2aaf462ebb6eaede0ea55107eb076e5}{%
           family={Woodworth},
           familyi={W\bibinitperiod},
           given={Blake},
           giveni={B\bibinitperiod}}}%
        {{hash=177e105ef582d94f5b6cfd058d7384ff}{%
           family={Gunasekar},
           familyi={G\bibinitperiod},
           given={Suriya},
           giveni={S\bibinitperiod}}}%
        {{hash=f62754c1d0d862b8ab5ca448259623ba}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Jason\bibnamedelima D.},
           giveni={J\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=609ace7cd4540190d711798fdf8b4d3d}{%
           family={Moroshko},
           familyi={M\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod}}}%
        {{hash=f134a417323419d25620c665207e87c9}{%
           family={Savarese},
           familyi={S\bibinitperiod},
           given={Pedro},
           giveni={P\bibinitperiod}}}%
        {{hash=6a7762e3c43a89027a7633a4c41572a0}{%
           family={Golan},
           familyi={G\bibinitperiod},
           given={Itay},
           giveni={I\bibinitperiod}}}%
        {{hash=07b30cf03223931cc6e46fff4b5b42e4}{%
           family={Soudry},
           familyi={S\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=4a4fc057d8331655775a21a8994f3339}{%
           family={Srebro},
           familyi={S\bibinitperiod},
           given={Nathan},
           giveni={N\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{1115cb24dddb52ca9e56aa2ff1eb5910}
      \strng{fullhash}{6817ab00cfe1eb301477d5d8a8e960b4}
      \strng{bibnamehash}{1115cb24dddb52ca9e56aa2ff1eb5910}
      \strng{authorbibnamehash}{1115cb24dddb52ca9e56aa2ff1eb5910}
      \strng{authornamehash}{1115cb24dddb52ca9e56aa2ff1eb5910}
      \strng{authorfullhash}{6817ab00cfe1eb301477d5d8a8e960b4}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A recent line of work studies overparametrized neural networks in the “kernel regime,” i.e. when during training the network behaves as a kernelized linear predictor, and thus, training with gradient descent has the effect of finding the corresponding minimum RKHS norm solution. This stands in contrast to other studies which demonstrate how gradient descent on overparametrized networks can induce rich implicit biases that are not RKHS norms. Building on an observation by {\textbackslash}citet\{chizat2018note\}, we show how the {\textbackslash}textbf\{{\textbackslash}textit\{scale of the initialization\}\} controls the transition between the “kernel” (aka lazy) and “rich” (aka active) regimes and affects generalization properties in multilayer homogeneous models. We provide a complete and detailed analysis for a family of simple depth-DDD linear networks that exhibit an interesting and meaningful transition between the kernel and rich regimes, and highlight an interesting role for the {\textbackslash}emph\{width\} of the models. We further demonstrate this transition empirically for matrix factorization and multilayer non-linear networks.}
      \field{booktitle}{Proceedings of {Thirty} {Third} {Conference} on {Learning} {Theory}}
      \field{month}{7}
      \field{note}{ISSN: 2640-3498}
      \field{title}{Kernel and {Rich} {Regimes} in {Overparametrized} {Models}}
      \field{urlday}{28}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2020}
      \field{urldateera}{ce}
      \field{pages}{3635\bibrangedash 3673}
      \range{pages}{39}
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v125/woodworth20a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v125/woodworth20a.html
      \endverb
    \endentry
    \entry{pesme_implicit_2021}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=f740d202b79f1b02cf11f471fe68f3ae}{%
           family={Pesme},
           familyi={P\bibinitperiod},
           given={Scott},
           giveni={S\bibinitperiod}}}%
        {{hash=e7abbaf3659900c715981a63c38087a0}{%
           family={Pillaud-Vivien},
           familyi={P\bibinithyphendelim V\bibinitperiod},
           given={Loucas},
           giveni={L\bibinitperiod}}}%
        {{hash=e2d7053f85144a452931150e4ba9f19c}{%
           family={Flammarion},
           familyi={F\bibinitperiod},
           given={Nicolas},
           giveni={N\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{b90edde2615180aa7475067b58319fd3}
      \strng{fullhash}{89231d4fe8d587b00ab613893dd05db7}
      \strng{bibnamehash}{89231d4fe8d587b00ab613893dd05db7}
      \strng{authorbibnamehash}{89231d4fe8d587b00ab613893dd05db7}
      \strng{authornamehash}{b90edde2615180aa7475067b58319fd3}
      \strng{authorfullhash}{89231d4fe8d587b00ab613893dd05db7}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Understanding the implicit bias of training algorithms is of crucial importance in order to explain the success of overparametrised neural networks. In this paper, we study the dynamics of stochastic gradient descent over diagonal linear networks through its continuous time version, namely stochastic gradient flow. We explicitly characterise the solution chosen by the stochastic flow and prove that it always enjoys better generalisation properties than that of gradient flow.Quite surprisingly, we show that the convergence speed of the training loss controls the magnitude of the biasing effect: the slower the convergence, the better the bias. To fully complete our analysis, we provide convergence guarantees for the dynamics. We also give experimental results which support our theoretical claims. Our findings highlight the fact that structured noise can induce better generalisation and they help explain the greater performances of stochastic gradient descent over gradient descent observed in practice.}
      \field{month}{11}
      \field{shorttitle}{Implicit {Bias} of {SGD} for {Diagonal} {Linear} {Networks}}
      \field{title}{Implicit {Bias} of {SGD} for {Diagonal} {Linear} {Networks}: a {Provable} {Benefit} of {Stochasticity}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{year}{2021}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://openreview.net/forum?id=vvi7KqHQiA
      \endverb
      \verb{url}
      \verb https://openreview.net/forum?id=vvi7KqHQiA
      \endverb
    \endentry
    \entry{azulay_implicit_2021}{inproceedings}{}
      \name{author}{7}{}{%
        {{hash=c100c888adc0d7768135e0fe6b6c42a4}{%
           family={Azulay},
           familyi={A\bibinitperiod},
           given={Shahar},
           giveni={S\bibinitperiod}}}%
        {{hash=609ace7cd4540190d711798fdf8b4d3d}{%
           family={Moroshko},
           familyi={M\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod}}}%
        {{hash=afba2ed234d5565b0fa614a01e2ddf84}{%
           family={Nacson},
           familyi={N\bibinitperiod},
           given={Mor\bibnamedelima Shpigel},
           giveni={M\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=ad541818477113edab1f6b9b63335fd4}{%
           family={Woodworth},
           familyi={W\bibinitperiod},
           given={Blake\bibnamedelima E.},
           giveni={B\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=4a4fc057d8331655775a21a8994f3339}{%
           family={Srebro},
           familyi={S\bibinitperiod},
           given={Nathan},
           giveni={N\bibinitperiod}}}%
        {{hash=e8b604d0a7af5ee88043f8e31a4c6871}{%
           family={Globerson},
           familyi={G\bibinitperiod},
           given={Amir},
           giveni={A\bibinitperiod}}}%
        {{hash=07b30cf03223931cc6e46fff4b5b42e4}{%
           family={Soudry},
           familyi={S\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{0fc9a449f6c65139b326f447715d4dec}
      \strng{fullhash}{07ecb6248054432b8201aca89414b8ab}
      \strng{bibnamehash}{0fc9a449f6c65139b326f447715d4dec}
      \strng{authorbibnamehash}{0fc9a449f6c65139b326f447715d4dec}
      \strng{authornamehash}{0fc9a449f6c65139b326f447715d4dec}
      \strng{authorfullhash}{07ecb6248054432b8201aca89414b8ab}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Recent work has highlighted the role of initialization scale in determining the structure of the solutions that gradient methods converge to. In particular, it was shown that large initialization leads to the neural tangent kernel regime solution, whereas small initialization leads to so called “rich regimes”. However, the initialization structure is richer than the overall scale alone and involves relative magnitudes of different weights and layers in the network. Here we show that these relative scales, which we refer to as initialization shape, play an important role in determining the learned model. We develop a novel technique for deriving the inductive bias of gradient-flow and use it to obtain closed-form implicit regularizers for multiple cases of interest.}
      \field{booktitle}{Proceedings of the 38th {International} {Conference} on {Machine} {Learning}}
      \field{month}{7}
      \field{shorttitle}{On the {Implicit} {Bias} of {Initialization} {Shape}}
      \field{title}{On the {Implicit} {Bias} of {Initialization} {Shape}: {Beyond} {Infinitesimal} {Mirror} {Descent}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{468\bibrangedash 477}
      \range{pages}{10}
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v139/azulay21a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v139/azulay21a.html
      \endverb
    \endentry
    \entry{haochen_shape_2021}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=801ca70c89f3be997d6b24b2812eb3ab}{%
           family={HaoChen},
           familyi={H\bibinitperiod},
           given={Jeff\bibnamedelima Z.},
           giveni={J\bibinitperiod\bibinitdelim Z\bibinitperiod}}}%
        {{hash=f82aaeda30fe4e41d8d329eafa383e52}{%
           family={Wei},
           familyi={W\bibinitperiod},
           given={Colin},
           giveni={C\bibinitperiod}}}%
        {{hash=4ee154247efd44bd9bd762630838b953}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Jason},
           giveni={J\bibinitperiod}}}%
        {{hash=5bd4a91e09b499d15e82bc929acf1e34}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Tengyu},
           giveni={T\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{7f4b83c7f57b63546a9718ce74551898}
      \strng{fullhash}{501627355c3842f15d8ca924bc8c0ef9}
      \strng{bibnamehash}{501627355c3842f15d8ca924bc8c0ef9}
      \strng{authorbibnamehash}{501627355c3842f15d8ca924bc8c0ef9}
      \strng{authornamehash}{7f4b83c7f57b63546a9718ce74551898}
      \strng{authorfullhash}{501627355c3842f15d8ca924bc8c0ef9}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The noise in stochastic gradient descent (SGD) provides a crucial implicit regularization effect for training overparameterized models. Prior theoretical work largely focuses on spherical Gaussian noise, whereas empirical studies demonstrate the phenomenon that parameter-dependent noise — induced by mini-batches or label perturbation — is far more effective than Gaussian noise. This paper theoretically characterizes this phenomenon on a quadratically-parameterized model introduced by Vaskevicius et al. and Woodworth et al. We show that in an over-parameterized setting, SGD with label noise recovers the sparse ground-truth with an arbitrary initialization, whereas SGD with Gaussian noise or gradient descent overfits to dense solutions with large norms. Our analysis reveals that parameter-dependent noise introduces a bias towards local minima with smaller noise variance, whereas spherical Gaussian noise does not.}
      \field{booktitle}{Proceedings of {Thirty} {Fourth} {Conference} on {Learning} {Theory}}
      \field{month}{7}
      \field{shorttitle}{Shape {Matters}}
      \field{title}{Shape {Matters}: {Understanding} the {Implicit} {Bias} of the {Noise} {Covariance}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{2315\bibrangedash 2357}
      \range{pages}{43}
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v134/haochen21a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v134/haochen21a.html
      \endverb
    \endentry
    \entry{moroshko_implicit_2020}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=609ace7cd4540190d711798fdf8b4d3d}{%
           family={Moroshko},
           familyi={M\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod}}}%
        {{hash=ad541818477113edab1f6b9b63335fd4}{%
           family={Woodworth},
           familyi={W\bibinitperiod},
           given={Blake\bibnamedelima E},
           giveni={B\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=177e105ef582d94f5b6cfd058d7384ff}{%
           family={Gunasekar},
           familyi={G\bibinitperiod},
           given={Suriya},
           giveni={S\bibinitperiod}}}%
        {{hash=f62754c1d0d862b8ab5ca448259623ba}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Jason\bibnamedelima D},
           giveni={J\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=34451c97b5a5bad6fcb6513573d1f94d}{%
           family={Srebro},
           familyi={S\bibinitperiod},
           given={Nati},
           giveni={N\bibinitperiod}}}%
        {{hash=07b30cf03223931cc6e46fff4b5b42e4}{%
           family={Soudry},
           familyi={S\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{9e0c9278a655ca847fbfe91e9fc8db16}
      \strng{fullhash}{bc04ba3333f242166b4da992016078cd}
      \strng{bibnamehash}{9e0c9278a655ca847fbfe91e9fc8db16}
      \strng{authorbibnamehash}{9e0c9278a655ca847fbfe91e9fc8db16}
      \strng{authornamehash}{9e0c9278a655ca847fbfe91e9fc8db16}
      \strng{authorfullhash}{bc04ba3333f242166b4da992016078cd}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We provide a detailed asymptotic study of gradient flow trajectories and their implicit optimization bias when minimizing the exponential loss over "diagonal linear networks". This is the simplest model displaying a transition between "kernel" and non-kernel ("rich" or "active") regimes. We show how the transition is controlled by the relationship between the initialization scale and how accurately we minimize the training loss. Our results indicate that some limit behavior of gradient descent only kick in at ridiculous training accuracies (well beyond 10{\textasciicircum}-100). Moreover, the implicit bias at reasonable initialization scales and training accuracies is more complex and not captured by these limits.}
      \field{booktitle}{Advances in {Neural} {Information} {Processing} {Systems}}
      \field{shorttitle}{Implicit {Bias} in {Deep} {Linear} {Classification}}
      \field{title}{Implicit {Bias} in {Deep} {Linear} {Classification}: {Initialization} {Scale} vs {Training} {Accuracy}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{volume}{33}
      \field{year}{2020}
      \field{urldateera}{ce}
      \field{pages}{22182\bibrangedash 22193}
      \range{pages}{12}
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2020/hash/fc2022c89b61c76bbef978f1370660bf-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2020/hash/fc2022c89b61c76bbef978f1370660bf-Abstract.html
      \endverb
    \endentry
    \entry{ji_gradient_2020}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=24b318d147ffe07cf3d183806694085a}{%
           family={Ji},
           familyi={J\bibinitperiod},
           given={Ziwei},
           giveni={Z\bibinitperiod}}}%
        {{hash=f22170ade97bd945149242679a0c806c}{%
           family={Dudík},
           familyi={D\bibinitperiod},
           given={Miroslav},
           giveni={M\bibinitperiod}}}%
        {{hash=08900c47ecc2f4dbcb934ad47bb6db9b}{%
           family={Schapire},
           familyi={S\bibinitperiod},
           given={Robert\bibnamedelima E},
           giveni={R\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=cc58bc38a2fd6209406cca9b6a4d0730}{%
           family={Telgarsky},
           familyi={T\bibinitperiod},
           given={Matus},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{7f1de58d3d36035f2dcbf85d331900f1}
      \strng{fullhash}{a333a1f6af3439f9697f899efc6f6ef6}
      \strng{bibnamehash}{a333a1f6af3439f9697f899efc6f6ef6}
      \strng{authorbibnamehash}{a333a1f6af3439f9697f899efc6f6ef6}
      \strng{authornamehash}{7f1de58d3d36035f2dcbf85d331900f1}
      \strng{authorfullhash}{a333a1f6af3439f9697f899efc6f6ef6}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Conference on {Learning} {Theory}}
      \field{title}{Gradient descent follows the regularization path for general losses}
      \field{year}{2020}
      \field{pages}{2109\bibrangedash 2136}
      \range{pages}{28}
    \endentry
    \entry{yang_better_2022}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=13b67d4d83b1d3002d9480fa84cdc9ee}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Liu},
           giveni={L\bibinitperiod}}}%
        {{hash=865a737b376cc4efcca11d91d9909a96}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Jifan},
           giveni={J\bibinitperiod}}}%
        {{hash=8cbdc93e24da5e2064fa4425b1cfc219}{%
           family={Shenouda},
           familyi={S\bibinitperiod},
           given={Joseph},
           giveni={J\bibinitperiod}}}%
        {{hash=ab0562ce82f5b7a093c659f376824faf}{%
           family={Papailiopoulos},
           familyi={P\bibinitperiod},
           given={Dimitris},
           giveni={D\bibinitperiod}}}%
        {{hash=3de3c2300042d3868ce512b74ee27993}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Kangwook},
           giveni={K\bibinitperiod}}}%
        {{hash=59c0420f59aab17e94d3acb1497457d0}{%
           family={Nowak},
           familyi={N\bibinitperiod},
           given={Robert\bibnamedelima D.},
           giveni={R\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{4f0ef627241c9004d6a7e972cf3722db}
      \strng{fullhash}{5acec0c811fb756f898ed01cb29c36a3}
      \strng{bibnamehash}{4f0ef627241c9004d6a7e972cf3722db}
      \strng{authorbibnamehash}{4f0ef627241c9004d6a7e972cf3722db}
      \strng{authornamehash}{4f0ef627241c9004d6a7e972cf3722db}
      \strng{authorfullhash}{5acec0c811fb756f898ed01cb29c36a3}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Weight decay is one of the most widely used forms of regularization in deep learning, and has been shown to improve generalization and robustness. The optimization objective driving weight decay is a sum of losses plus a term proportional to the sum of squared weights. This paper argues that stochastic gradient descent (SGD) may be an inefficient algorithm for this objective. For neural networks with ReLU activations, solutions to the weight decay objective are equivalent to those of a different objective in which the regularization term is instead a sum of products of \${\textbackslash}ell\_2\$ (not squared) norms of the input and output weights associated each ReLU. This alternative (and effectively equivalent) regularization suggests a novel proximal gradient algorithm for network training. Theory and experiments support the new training approach, showing that it can converge much faster to the sparse solutions it shares with standard weight decay training.}
      \field{month}{11}
      \field{shorttitle}{A {Better} {Way} to {Decay}}
      \field{title}{A {Better} {Way} to {Decay}: {Proximal} {Gradient} {Training} {Algorithms} for {Neural} {Nets}}
      \field{urlday}{30}
      \field{urlmonth}{10}
      \field{urlyear}{2024}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://openreview.net/forum?id=4y1xh8jClhC
      \endverb
      \verb{url}
      \verb https://openreview.net/forum?id=4y1xh8jClhC
      \endverb
    \endentry
    \entry{parhi_near-minimax_2023}{article}{}
      \name{author}{2}{}{%
        {{hash=4f70d4c3e3d8eebc1c1c6cdcf7b6978a}{%
           family={Parhi},
           familyi={P\bibinitperiod},
           given={Rahul},
           giveni={R\bibinitperiod}}}%
        {{hash=59c0420f59aab17e94d3acb1497457d0}{%
           family={Nowak},
           familyi={N\bibinitperiod},
           given={Robert\bibnamedelima D.},
           giveni={R\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
      }
      \strng{namehash}{71a6dce4f0c843affb4ef5d460b21e4d}
      \strng{fullhash}{71a6dce4f0c843affb4ef5d460b21e4d}
      \strng{bibnamehash}{71a6dce4f0c843affb4ef5d460b21e4d}
      \strng{authorbibnamehash}{71a6dce4f0c843affb4ef5d460b21e4d}
      \strng{authornamehash}{71a6dce4f0c843affb4ef5d460b21e4d}
      \strng{authorfullhash}{71a6dce4f0c843affb4ef5d460b21e4d}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We study the problem of estimating an unknown function from noisy data using shallow ReLU neural networks. The estimators we study minimize the sum of squared data-fitting errors plus a regularization term proportional to the squared Euclidean norm of the network weights. This minimization corresponds to the common approach of training a neural network with weight decay. We quantify the performance (mean-squared error) of these neural network estimators when the data-generating function belongs to the second-order Radon-domain bounded variation space. This space of functions was recently proposed as the natural function space associated with shallow ReLU neural networks. We derive a minimax lower bound for the estimation problem for this function space and show that the neural network estimators are minimax optimal up to logarithmic factors. This minimax rate is immune to the curse of dimensionality. We quantify an explicit gap between neural networks and linear methods (which include kernel methods) by deriving a linear minimax lower bound for the estimation problem, showing that linear methods necessarily suffer the curse of dimensionality in this function space. As a result, this paper sheds light on the phenomenon that neural networks seem to break the curse of dimensionality.}
      \field{issn}{1557-9654}
      \field{journaltitle}{IEEE Transactions on Information Theory}
      \field{month}{2}
      \field{note}{Conference Name: IEEE Transactions on Information Theory}
      \field{number}{2}
      \field{title}{Near-{Minimax} {Optimal} {Estimation} {With} {Shallow} {ReLU} {Neural} {Networks}}
      \field{urlday}{30}
      \field{urlmonth}{10}
      \field{urlyear}{2024}
      \field{volume}{69}
      \field{year}{2023}
      \field{urldateera}{ce}
      \field{pages}{1125\bibrangedash 1140}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1109/TIT.2022.3208653
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/abstract/document/9899453
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/abstract/document/9899453
      \endverb
      \keyw{Biological neural networks,Estimation,Neural networks,Neurons,Noise measurement,Radon,TV,Training,function approximation,nonparametric function estimation,ridge functions,sparsity}
    \endentry
    \entry{shenouda_variation_2024}{article}{}
      \name{author}{4}{}{%
        {{hash=8cbdc93e24da5e2064fa4425b1cfc219}{%
           family={Shenouda},
           familyi={S\bibinitperiod},
           given={Joseph},
           giveni={J\bibinitperiod}}}%
        {{hash=4f70d4c3e3d8eebc1c1c6cdcf7b6978a}{%
           family={Parhi},
           familyi={P\bibinitperiod},
           given={Rahul},
           giveni={R\bibinitperiod}}}%
        {{hash=3de3c2300042d3868ce512b74ee27993}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Kangwook},
           giveni={K\bibinitperiod}}}%
        {{hash=59c0420f59aab17e94d3acb1497457d0}{%
           family={Nowak},
           familyi={N\bibinitperiod},
           given={Robert\bibnamedelima D.},
           giveni={R\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
      }
      \strng{namehash}{ce842da0146e6a6bac64e4d0f53be4b5}
      \strng{fullhash}{4b14c11c5413cc3dafb98c11e5fda74c}
      \strng{bibnamehash}{4b14c11c5413cc3dafb98c11e5fda74c}
      \strng{authorbibnamehash}{4b14c11c5413cc3dafb98c11e5fda74c}
      \strng{authornamehash}{ce842da0146e6a6bac64e4d0f53be4b5}
      \strng{authorfullhash}{4b14c11c5413cc3dafb98c11e5fda74c}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{This paper introduces a novel theoretical framework for the analysis of vector-valued neural networks through the development of vector-valued variation spaces, a new class of reproducing kernel Banach spaces. These spaces emerge from studying the regularization effect of weight decay in training networks with activation functions like the rectified linear unit (ReLU). This framework offers a deeper understanding of multi-output networks and their function-space characteristics. A key contribution of this work is the development of a representer theorem for the vector-valued variation spaces. This representer theorem establishes that shallow vector-valued neural networks are the solutions to data-fitting problems over these infinite-dimensional spaces, where the network widths are bounded by the square of the number of training data. This observation reveals that the norm associated with these vector-valued variation spaces encourages the learning of features that are useful for multiple tasks, shedding new light on multi-task learning with neural networks. Finally, this paper develops a connection between weight-decay regularization and the multi-task lasso problem. This connection leads to novel bounds for layer widths in deep networks that depend on the intrinsic dimensions of the training data representations. This insight not only deepens the understanding of the deep network architectural requirements, but also yields a simple convex optimization method for deep neural network compression. The performance of this compression procedure is evaluated on various architectures.}
      \field{issn}{1533-7928}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{number}{231}
      \field{shorttitle}{Variation {Spaces} for {Multi}-{Output} {Neural} {Networks}}
      \field{title}{Variation {Spaces} for {Multi}-{Output} {Neural} {Networks}: {Insights} on {Multi}-{Task} {Learning} and {Network} {Compression}}
      \field{urlday}{30}
      \field{urlmonth}{10}
      \field{urlyear}{2024}
      \field{volume}{25}
      \field{year}{2024}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 40}
      \range{pages}{40}
      \verb{urlraw}
      \verb http://jmlr.org/papers/v25/23-0677.html
      \endverb
      \verb{url}
      \verb http://jmlr.org/papers/v25/23-0677.html
      \endverb
    \endentry
    \entry{yuan_model_2006}{article}{}
      \name{author}{2}{}{%
        {{hash=c2a6a57443a5f803f5b84beffaeb709c}{%
           family={Yuan},
           familyi={Y\bibinitperiod},
           given={Ming},
           giveni={M\bibinitperiod}}}%
        {{hash=83462145d1960e68e830e4b0dd4314ef}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Yi},
           giveni={Y\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{3988dcaf156748c8725fb1bad463040f}
      \strng{fullhash}{3988dcaf156748c8725fb1bad463040f}
      \strng{bibnamehash}{3988dcaf156748c8725fb1bad463040f}
      \strng{authorbibnamehash}{3988dcaf156748c8725fb1bad463040f}
      \strng{authornamehash}{3988dcaf156748c8725fb1bad463040f}
      \strng{authorfullhash}{3988dcaf156748c8725fb1bad463040f}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Summary. We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.}
      \field{issn}{1467-9868}
      \field{journaltitle}{Journal of the Royal Statistical Society: Series B (Statistical Methodology)}
      \field{note}{\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2005.00532.x}
      \field{number}{1}
      \field{title}{Model selection and estimation in regression with grouped variables}
      \field{urlday}{28}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{volume}{68}
      \field{year}{2006}
      \field{urldateera}{ce}
      \field{pages}{49\bibrangedash 67}
      \range{pages}{19}
      \verb{doi}
      \verb 10.1111/j.1467-9868.2005.00532.x
      \endverb
      \verb{urlraw}
      \verb https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00532.x
      \endverb
      \verb{url}
      \verb https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00532.x
      \endverb
      \keyw{Analysis of variance,Lasso,Least angle regression,Non-negative garrotte,Piecewise linear solution path}
    \endentry
    \entry{lubana_how_2022}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=0edffb1e519b3d9e9ee47491e9cd10ce}{%
           family={Lubana},
           familyi={L\bibinitperiod},
           given={Ekdeep\bibnamedelima Singh},
           giveni={E\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=0b69e7d0e30b077a7c527c88304b483a}{%
           family={Trivedi},
           familyi={T\bibinitperiod},
           given={Puja},
           giveni={P\bibinitperiod}}}%
        {{hash=b8b397a632f3ed91f239f0c6508724c2}{%
           family={Koutra},
           familyi={K\bibinitperiod},
           given={Danai},
           giveni={D\bibinitperiod}}}%
        {{hash=539d11a90cad5711439ae06549f0c6a5}{%
           family={Dick},
           familyi={D\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{85ba6cc836dce4fede4a48b3053c6de9}
      \strng{fullhash}{186656c144e3d1dfcce40da4448245e9}
      \strng{bibnamehash}{186656c144e3d1dfcce40da4448245e9}
      \strng{authorbibnamehash}{186656c144e3d1dfcce40da4448245e9}
      \strng{authornamehash}{85ba6cc836dce4fede4a48b3053c6de9}
      \strng{authorfullhash}{186656c144e3d1dfcce40da4448245e9}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Conference on {Lifelong} {Learning} {Agents}}
      \field{title}{How do quadratic regularizers prevent catastrophic forgetting: {The} role of interpolation}
      \field{year}{2022}
      \field{pages}{819\bibrangedash 837}
      \range{pages}{19}
    \endentry
    \entry{he_deep_2016}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=6b4b60e909e78633945f3f9c9dc83e01}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kaiming},
           giveni={K\bibinitperiod}}}%
        {{hash=5e72bc22dbcf0984c6d113d280e36990}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Xiangyu},
           giveni={X\bibinitperiod}}}%
        {{hash=bb295293acacd54387339079ebbe4ead}{%
           family={Ren},
           familyi={R\bibinitperiod},
           given={Shaoqing},
           giveni={S\bibinitperiod}}}%
        {{hash=f85751488058842b5777c7b4074077b5}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Jian},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{fullhash}{42c4b52dc3a62cebabbc11c73e1afb53}
      \strng{bibnamehash}{42c4b52dc3a62cebabbc11c73e1afb53}
      \strng{authorbibnamehash}{42c4b52dc3a62cebabbc11c73e1afb53}
      \strng{authornamehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{authorfullhash}{42c4b52dc3a62cebabbc11c73e1afb53}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Deep {Residual} {Learning} for {Image} {Recognition}}
      \field{urlday}{16}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2016}
      \field{urldateera}{ce}
      \field{pages}{770\bibrangedash 778}
      \range{pages}{9}
      \verb{urlraw}
      \verb https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html
      \endverb
      \verb{url}
      \verb https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html
      \endverb
    \endentry
    \entry{krizhevsky_learning_2009}{article}{}
      \true{moreauthor}
      \true{morelabelname}
      \name{author}{2}{}{%
        {{hash=c5e3a676e2ac1164b3afcd539c131fc9}{%
           family={Krizhevsky},
           familyi={K\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{295643bee33894e6a58e8c12888e0009}
      \strng{fullhash}{295643bee33894e6a58e8c12888e0009}
      \strng{bibnamehash}{295643bee33894e6a58e8c12888e0009}
      \strng{authorbibnamehash}{295643bee33894e6a58e8c12888e0009}
      \strng{authornamehash}{295643bee33894e6a58e8c12888e0009}
      \strng{authorfullhash}{295643bee33894e6a58e8c12888e0009}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{note}{Publisher: Toronto, ON, Canada}
      \field{title}{Learning multiple layers of features from tiny images}
      \field{year}{2009}
    \endentry
    \entry{deng_imagenet_2009}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=0ae7fdc13773f928525f673b05f37149}{%
           family={Deng},
           familyi={D\bibinitperiod},
           given={Jia},
           giveni={J\bibinitperiod}}}%
        {{hash=7d87c5957b07153c7f18918b92830bf8}{%
           family={Dong},
           familyi={D\bibinitperiod},
           given={Wei},
           giveni={W\bibinitperiod}}}%
        {{hash=d5670b2600fea169724521e252d9d09d}{%
           family={Socher},
           familyi={S\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
        {{hash=2afdae52015b97674d81efea449edce2}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Li-Jia},
           giveni={L\bibinithyphendelim J\bibinitperiod}}}%
        {{hash=4838f7fdd28d5cefb28f3b3c734976d4}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Kai},
           giveni={K\bibinitperiod}}}%
        {{hash=cd00ce5bc45f687c432e52e0fa1a7aa6}{%
           family={Fei-Fei},
           familyi={F\bibinithyphendelim F\bibinitperiod},
           given={Li},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{990420f755e01028377fcad1464c9706}
      \strng{fullhash}{a16fdd05c52c264b99fe98f4a5e24c60}
      \strng{bibnamehash}{990420f755e01028377fcad1464c9706}
      \strng{authorbibnamehash}{990420f755e01028377fcad1464c9706}
      \strng{authornamehash}{990420f755e01028377fcad1464c9706}
      \strng{authorfullhash}{a16fdd05c52c264b99fe98f4a5e24c60}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}}
      \field{title}{{ImageNet}: {A} large-scale hierarchical image database}
      \field{year}{2009}
      \field{pages}{248\bibrangedash 255}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1109/CVPR.2009.5206848
      \endverb
      \keyw{Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine}
    \endentry
    \entry{gao_theory_2017}{misc}{}
      \name{author}{7}{}{%
        {{hash=3240a868e65a4675cc008a8878d7165d}{%
           family={Gao},
           familyi={G\bibinitperiod},
           given={Peiran},
           giveni={P\bibinitperiod}}}%
        {{hash=525c1e2b83fcd94cd1abee41418c8bc3}{%
           family={Trautmann},
           familyi={T\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod}}}%
        {{hash=e69f887a0b7bee315207da34bab760d1}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Byron},
           giveni={B\bibinitperiod}}}%
        {{hash=fc689c9981a30218680453dcedd2ae8f}{%
           family={Santhanam},
           familyi={S\bibinitperiod},
           given={Gopal},
           giveni={G\bibinitperiod}}}%
        {{hash=c4c4c1720bc486b2fdc1515c69a46e55}{%
           family={Ryu},
           familyi={R\bibinitperiod},
           given={Stephen},
           giveni={S\bibinitperiod}}}%
        {{hash=ee808638f3c277a1ddbd2a3ac4557d7c}{%
           family={Shenoy},
           familyi={S\bibinitperiod},
           given={Krishna},
           giveni={K\bibinitperiod}}}%
        {{hash=05b3e391388084df874ede60f2210c12}{%
           family={Ganguli},
           familyi={G\bibinitperiod},
           given={Surya},
           giveni={S\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {bioRxiv}%
      }
      \strng{namehash}{0e7ce4b9320aec79538f85661a655295}
      \strng{fullhash}{33cfdde59f42772f4d1e44213af9a745}
      \strng{bibnamehash}{0e7ce4b9320aec79538f85661a655295}
      \strng{authorbibnamehash}{0e7ce4b9320aec79538f85661a655295}
      \strng{authornamehash}{0e7ce4b9320aec79538f85661a655295}
      \strng{authorfullhash}{33cfdde59f42772f4d1e44213af9a745}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In many experiments, neuroscientists tightly control behavior, record many trials, and obtain trial-averaged firing rates from hundreds of neurons in circuits containing billions of behaviorally relevant neurons. Di-mensionality reduction methods reveal a striking simplicity underlying such multi-neuronal data: they can be reduced to a low-dimensional space, and the resulting neural trajectories in this space yield a remarkably insightful dynamical portrait of circuit computation. This simplicity raises profound and timely conceptual questions. What are its origins and its implications for the complexity of neural dynamics? How would the situation change if we recorded more neurons? When, if at all, can we trust dynamical portraits obtained from measuring an infinitesimal fraction of task relevant neurons? We present a theory that answers these questions, and test it using physiological recordings from reaching monkeys. This theory reveals conceptual insights into how task complexity governs both neural dimensionality and accurate recovery of dynamic portraits, thereby providing quantitative guidelines for future large-scale experimental design.}
      \field{month}{11}
      \field{title}{A theory of multineuronal dimensionality, dynamics and measurement}
      \field{urlday}{16}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2017}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1101/214262
      \endverb
      \verb{urlraw}
      \verb https://www.biorxiv.org/content/10.1101/214262v2
      \endverb
      \verb{url}
      \verb https://www.biorxiv.org/content/10.1101/214262v2
      \endverb
    \endentry
    \entry{giaffar_effective_2023}{misc}{}
      \name{author}{3}{}{%
        {{hash=f955230bb14386f3ba0db59f6db6a931}{%
           family={Giaffar},
           familyi={G\bibinitperiod},
           given={Hamza},
           giveni={H\bibinitperiod}}}%
        {{hash=ab06850c9b2667191441e1fe7fc1b3b6}{%
           family={Buxó},
           familyi={B\bibinitperiod},
           given={Camille\bibnamedelima Rullán},
           giveni={C\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=83f1807a6d1d2ae9f57697ad4c696259}{%
           family={Aoi},
           familyi={A\bibinitperiod},
           given={Mikio},
           giveni={M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {bioRxiv}%
      }
      \strng{namehash}{b2373149e13672b218c5819cbbe65ccb}
      \strng{fullhash}{bbf9ead8724f8f8976224dc09f2571bc}
      \strng{bibnamehash}{bbf9ead8724f8f8976224dc09f2571bc}
      \strng{authorbibnamehash}{bbf9ead8724f8f8976224dc09f2571bc}
      \strng{authornamehash}{b2373149e13672b218c5819cbbe65ccb}
      \strng{authorfullhash}{bbf9ead8724f8f8976224dc09f2571bc}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{A number of recent studies have sought to understand the behavior of artificial and biological neural networks by comparing representations across layers, networks and brain areas. Simultaneously, there has been growing interest in using dimensionality of a dataset as a proxy for computational complexity. At the intersection of these topics, studies exploring the dimensionality of shared computational and representational subspaces have relied on model-based methods, but a standard, model-free measure is lacking. Here we present a candidate measure for shared dimensionality that we call the effective number of shared dimensions (ENSD). The ENSD can be applied to data matrices sharing at least one dimension, reduces to the well-known participation ratio when both data sets are equivalent and has a number of other robust and intuitive mathematical properties. Notably, the ENSD can be written as a similarity metric that is a re-scaled version of centered kernel alignment (CKA) but additionally describes the dimensionality of the aligned subspaces. Unlike methods like canonical correlation analysis (CCA), the ENSD is robust to cases where data is sparse or low rank. We demonstrate its utility and computational efficiency by a direct comparison of CKA and ENSD on across-layer similarities in convolutional neural networks as well as by recovering results from recent studies in neuroscience on communication subspaces between brain regions. Finally, we demonstrate how the ENSD and its constituent statistics allow us to perform a variety of multi-modal analyses of multivariate datasets. Specifically, we use connectomic data to probe the alignment of parallel pathways in the fly olfactory system, revealing novel results in the interaction between innate and learned olfactory representations. Altogether, we show that the ENSD is an interpretable and computationally efficient model-free measure of shared dimensionality and that it can be used to probe shared structure in a wide variety of data types.}
      \field{month}{7}
      \field{shorttitle}{The effective number of shared dimensions}
      \field{title}{The effective number of shared dimensions: {A} simple method for revealing shared structure between datasets}
      \field{urlday}{16}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2023}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1101/2023.07.27.550815
      \endverb
      \verb{urlraw}
      \verb https://www.biorxiv.org/content/10.1101/2023.07.27.550815v1
      \endverb
      \verb{url}
      \verb https://www.biorxiv.org/content/10.1101/2023.07.27.550815v1
      \endverb
    \endentry
    \entry{dosovitskiy_image_2020}{article}{}
      \true{moreauthor}
      \true{morelabelname}
      \name{author}{10}{}{%
        {{hash=72308762399d6ab62bbd9391c64c7bfd}{%
           family={Dosovitskiy},
           familyi={D\bibinitperiod},
           given={Alexey},
           giveni={A\bibinitperiod}}}%
        {{hash=836254958eac09b30d45dd67a75ce4fa}{%
           family={Beyer},
           familyi={B\bibinitperiod},
           given={Lucas},
           giveni={L\bibinitperiod}}}%
        {{hash=6807e3c00242c6bf6a3179905040471b}{%
           family={Kolesnikov},
           familyi={K\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
        {{hash=b398fca6879fde89a032b8e7de2f26b5}{%
           family={Weissenborn},
           familyi={W\bibinitperiod},
           given={Dirk},
           giveni={D\bibinitperiod}}}%
        {{hash=00b950307c7d096765bb6df00c7c5c3a}{%
           family={Zhai},
           familyi={Z\bibinitperiod},
           given={Xiaohua},
           giveni={X\bibinitperiod}}}%
        {{hash=1a47640139e63b9b5969c969dfe95def}{%
           family={Unterthiner},
           familyi={U\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
        {{hash=965baf6fa5686a42b1564330c5e11f2f}{%
           family={Dehghani},
           familyi={D\bibinitperiod},
           given={Mostafa},
           giveni={M\bibinitperiod}}}%
        {{hash=06829adce8713f0ace444279dcd7328f}{%
           family={Minderer},
           familyi={M\bibinitperiod},
           given={Matthias},
           giveni={M\bibinitperiod}}}%
        {{hash=c95269b4a2e23e1e526be521c7ec6ad4}{%
           family={Heigold},
           familyi={H\bibinitperiod},
           given={Georg},
           giveni={G\bibinitperiod}}}%
        {{hash=450963f966620925cb8fecd32f7a6ee1}{%
           family={Gelly},
           familyi={G\bibinitperiod},
           given={Sylvain},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{d96316bc7578db680aba71d4226354c8}
      \strng{fullhash}{e2f7b82b6bcaba0b27eef166b225c54a}
      \strng{bibnamehash}{d96316bc7578db680aba71d4226354c8}
      \strng{authorbibnamehash}{d96316bc7578db680aba71d4226354c8}
      \strng{authornamehash}{d96316bc7578db680aba71d4226354c8}
      \strng{authorfullhash}{e2f7b82b6bcaba0b27eef166b225c54a}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{arXiv preprint arXiv:2010.11929}
      \field{title}{An image is worth 16x16 words: {Transformers} for image recognition at scale}
      \field{year}{2020}
    \endentry
    \entry{paszke_pytorch_2019}{article}{}
      \true{moreauthor}
      \true{morelabelname}
      \name{author}{10}{}{%
        {{hash=56bf0b340039cf8594436a624ff548a9}{%
           family={Paszke},
           familyi={P\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod}}}%
        {{hash=4ba5062e5919c814aceec188d54c01f2}{%
           family={Gross},
           familyi={G\bibinitperiod},
           given={Sam},
           giveni={S\bibinitperiod}}}%
        {{hash=e5dfae4582081d649e3a0d5342050016}{%
           family={Massa},
           familyi={M\bibinitperiod},
           given={Francisco},
           giveni={F\bibinitperiod}}}%
        {{hash=b5815e1692fa2d0c1f44eecf509bd7c4}{%
           family={Lerer},
           familyi={L\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod}}}%
        {{hash=b75383e6b48c8360c7a60031424c85cf}{%
           family={Bradbury},
           familyi={B\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod}}}%
        {{hash=f897ed422c34d95af2e22778dfc2607e}{%
           family={Chanan},
           familyi={C\bibinitperiod},
           given={Gregory},
           giveni={G\bibinitperiod}}}%
        {{hash=046269e070246feb6f394141db80ed87}{%
           family={Killeen},
           familyi={K\bibinitperiod},
           given={Trevor},
           giveni={T\bibinitperiod}}}%
        {{hash=c40352c194e60a3ef458ee7e8685afb5}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Zeming},
           giveni={Z\bibinitperiod}}}%
        {{hash=6e45f49ec618e619efad90c8e8a61f0c}{%
           family={Gimelshein},
           familyi={G\bibinitperiod},
           given={Natalia},
           giveni={N\bibinitperiod}}}%
        {{hash=f65a80959d520337ae99a0798515036c}{%
           family={Antiga},
           familyi={A\bibinitperiod},
           given={Luca},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{724e74fc18651eb78eb82fbcd1d9dfb1}
      \strng{fullhash}{48a7dff01ec9361a039ff178d435ae04}
      \strng{bibnamehash}{724e74fc18651eb78eb82fbcd1d9dfb1}
      \strng{authorbibnamehash}{724e74fc18651eb78eb82fbcd1d9dfb1}
      \strng{authornamehash}{724e74fc18651eb78eb82fbcd1d9dfb1}
      \strng{authorfullhash}{48a7dff01ec9361a039ff178d435ae04}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Advances in neural information processing systems}
      \field{title}{Pytorch: {An} imperative style, high-performance deep learning library}
      \field{volume}{32}
      \field{year}{2019}
    \endentry
    \entry{kornblith_similarity_2019}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=7fc0edf04cce351dda404579ede3d0d1}{%
           family={Kornblith},
           familyi={K\bibinitperiod},
           given={Simon},
           giveni={S\bibinitperiod}}}%
        {{hash=e34c5aa67281924804d54d88e58ca38a}{%
           family={Norouzi},
           familyi={N\bibinitperiod},
           given={Mohammad},
           giveni={M\bibinitperiod}}}%
        {{hash=d09ea8631bf43468107b5ef02c5195aa}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Honglak},
           giveni={H\bibinitperiod}}}%
        {{hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{85d441f32944674f6dd87a3cae8f2d38}
      \strng{fullhash}{91ab27d387c86b37d5cad66ed3c2dfa3}
      \strng{bibnamehash}{91ab27d387c86b37d5cad66ed3c2dfa3}
      \strng{authorbibnamehash}{91ab27d387c86b37d5cad66ed3c2dfa3}
      \strng{authornamehash}{85d441f32944674f6dd87a3cae8f2d38}
      \strng{authorfullhash}{91ab27d387c86b37d5cad66ed3c2dfa3}
      \field{extraname}{2}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.}
      \field{booktitle}{Proceedings of the 36th {International} {Conference} on {Machine} {Learning}}
      \field{month}{5}
      \field{title}{Similarity of {Neural} {Network} {Representations} {Revisited}}
      \field{urlday}{16}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{3519\bibrangedash 3529}
      \range{pages}{11}
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v97/kornblith19a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v97/kornblith19a.html
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput

