\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{agarwal2009information}
Alekh Agarwal, Martin~J Wainwright, Peter Bartlett, and Pradeep Ravikumar.
\newblock Information-theoretic lower bounds on the oracle complexity of convex
  optimization.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  22, 2009.

\bibitem{alghunaim2021unified}
Sulaiman~A Alghunaim and Kun Yuan.
\newblock A unified and refined convergence analysis for non-convex
  decentralized learning.
\newblock {\em arXiv preprint arXiv:2110.09993}, 2021.

\bibitem{allen2018make}
Zeyuan Allen-Zhu.
\newblock How to make the gradients small stochastically: Even faster convex
  and nonconvex sgd.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  31, 2018.

\bibitem{Arjevani2019LowerBF}
Yossi Arjevani, Yair Carmon, John~C. Duchi, Dylan~J. Foster, Nathan Srebro, and
  Blake~E. Woodworth.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock {\em ArXiv}, abs/1912.02365, 2019.

\bibitem{assran2019stochastic}
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Mike Rabbat.
\newblock Stochastic gradient push for distributed deep learning.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  344--353, 2019.

\bibitem{benjamini2014mixing}
Itai Benjamini, Gady Kozma, and Nicholas Wormald.
\newblock The mixing time of the giant component of a random graph.
\newblock {\em Random Structures \& Algorithms}, 45(3):383--407, 2014.

\bibitem{boyd2005mixing}
Stephen~P Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah.
\newblock Mixing times for random walks on geometric random graphs.
\newblock In {\em ALENEX/ANALCO}, pages 240--249, 2005.

\bibitem{Cam1986AsymptoticMI}
Lucien~Le Cam.
\newblock Asymptotic methods in statistical decision theory.
\newblock 1986.

\bibitem{carmon2020lower}
Yair Carmon, John~C Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Lower bounds for finding stationary points i.
\newblock {\em Mathematical Programming}, 184(1):71--120, 2020.

\bibitem{carmon2021lower}
Yair Carmon, John~C Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Lower bounds for finding stationary points ii: first-order methods.
\newblock {\em Mathematical Programming}, 185(1):315--355, 2021.

\bibitem{chen2012diffusion}
Jianshu Chen and Ali~H Sayed.
\newblock Diffusion adaptation strategies for distributed optimization and
  learning over networks.
\newblock {\em IEEE Transactions on Signal Processing}, 60(8):4289--4305, 2012.

\bibitem{Chen2022OnWG}
Yu Chen, Sanjeev Khanna, and Huani Li.
\newblock On weighted graph sparsification by linear sketching.
\newblock {\em ArXiv}, 2022.

\bibitem{chen2021accelerating}
Yiming Chen, Kun Yuan, Yingya Zhang, Pan Pan, Yinghui Xu, and Wotao Yin.
\newblock Accelerating gossip {SGD} with periodic global averaging.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2021.

\bibitem{Csiszr2011InformationT}
Imre Csisz{\'a}r and J{\'a}nos K{\"o}rner.
\newblock {\em Information Theory - Coding Theorems for Discrete Memoryless
  Systems, Second Edition}.
\newblock Cambridge University Press, 2011.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 248--255. Ieee, 2009.

\bibitem{di2016next}
P. Di~Lorenzo and G. Scutari.
\newblock Next: In-network nonconvex optimization.
\newblock {\em IEEE Transactions on Signal and Information Processing over
  Networks}, 2(2):120--136, 2016.

\bibitem{diakonikolas2019lower}
Jelena Diakonikolas and Crist{\'o}bal Guzm{\'a}n.
\newblock Lower bounds for parallel and randomized convex optimization.
\newblock In {\em Conference on Learning Theory}, pages 1132--1157. PMLR, 2019.

\bibitem{duchi2011dual}
John~C Duchi, Alekh Agarwal, and Martin~J Wainwright.
\newblock Dual averaging for distributed optimization: Convergence analysis and
  network scaling.
\newblock {\em IEEE Transactions on Automatic control}, 57(3):592--606, 2011.

\bibitem{dasdsafcvzx}
Pavel Dvurechensky.
\newblock Gradient method with inexact oracle for composite non-convex
  optimization, 2017.

\bibitem{fang2018spider}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock Spider: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{hayes2000computing}
Brian Hayes.
\newblock Computing science: Graph theory in practice: Part ii.
\newblock {\em American Scientist}, 88(2):104--109, 2000.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 770--778, 2016.

\bibitem{Horvath2019StochasticDL}
Samuel Horvath, D. Kovalev, Konstantin Mishchenko, Sebastian~U. Stich, and
  Peter Richt{\'a}rik.
\newblock Stochastic distributed learning with gradient quantization and
  variance reduction.
\newblock {\em arXiv}, 2019.

\bibitem{huang2022lb}
Xinmeng Huang, Yiming Chen, Wotao Yin, and Kun Yuan.
\newblock Lower bounds and nearly optimal algorithms in distributed learning
  with communication compression.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2022.

\bibitem{karimi2016linear}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In {\em Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 795--811. Springer, 2016.

\bibitem{karimireddy2020scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
  Stich, and Ananda~Theertha Suresh.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  5132--5143. PMLR, 2020.

\bibitem{koloskova2021improved}
Anastasiia Koloskova, Tao Lin, and Sebastian~U Stich.
\newblock An improved analysis of gradient tracking for decentralized machine
  learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{koloskova2020unified}
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and
  Sebastian~U Stich.
\newblock A unified theory of decentralized sgd with changing topology and
  local updates.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  1--12, 2020.

\bibitem{Koloskova2020AUT}
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and
  Sebastian~U. Stich.
\newblock A unified theory of decentralized sgd with changing topology and
  local updates.
\newblock In {\em International Conference on Machine Learning}, 2020.

\bibitem{kovalev2021lower}
Dmitry Kovalev, Elnur Gasanov, Alexander Gasnikov, and Peter Richtarik.
\newblock Lower bounds and optimal algorithms for smooth and strongly convex
  decentralized optimization over time-varying networks.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  34, 2021.

\bibitem{kovalev2020optimal}
Dmitry Kovalev, Adil Salim, and Peter Richt{\'a}rik.
\newblock Optimal and practical algorithms for smooth and strongly convex
  decentralized optimization.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  33:18342--18352, 2020.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{lan2012optimal}
Guanghui Lan.
\newblock An optimal method for stochastic composite optimization.
\newblock {\em Mathematical Programming}, 133(1):365--397, 2012.

\bibitem{lan2018optimal}
Guanghui Lan and Yi Zhou.
\newblock An optimal randomized incremental gradient method.
\newblock {\em Mathematical programming}, 171(1):167--215, 2018.

\bibitem{li2019convergence}
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.
\newblock On the convergence of fedavg on non-iid data.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{li2017decentralized}
Z. Li, W. Shi, and M. Yan.
\newblock A decentralized proximal-gradient method with network independent
  step-sizes and separated convergence rates.
\newblock {\em IEEE Transactions on Signal Processing}, July 2019.
\newblock early acces. Also available on arXiv:1704.07807.

\bibitem{lian2017can}
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? {A}
  case study for decentralized parallel stochastic gradient descent.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 5330--5340, 2017.

\bibitem{lian2018asynchronous}
Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu.
\newblock Asynchronous decentralized parallel stochastic gradient descent.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  3043--3052, 2018.

\bibitem{lin2021quasi}
Tao Lin, Sai~Praneeth Karimireddy, Sebastian~U Stich, and Martin Jaggi.
\newblock Quasi-global momentum: Accelerating decentralized deep learning on
  heterogeneous data.
\newblock In {\em International Conference on Machine Learning}, 2021.

\bibitem{liu2011accelerated}
Ji Liu and A~Stephen Morse.
\newblock Accelerated linear iterations for distributed averaging.
\newblock {\em Annual Reviews in Control}, 35(2):160--165, 2011.

\bibitem{lu2019gnsd}
Songtao Lu, Xinwei Zhang, Haoran Sun, and Mingyi Hong.
\newblock Gnsd: A gradient-tracking based nonconvex stochastic algorithm for
  decentralized optimization.
\newblock In {\em 2019 IEEE Data Science Workshop (DSW)}, pages 315--321. IEEE,
  2019.

\bibitem{lu2021optimal}
Yucheng Lu and Christopher De~Sa.
\newblock Optimal complexity in decentralized training.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  7111--7123. PMLR, 2021.

\bibitem{mateos2010distributed}
Gonzalo Mateos, Juan~Andr{\'e}s Bazerque, and Georgios~B Giannakis.
\newblock Distributed sparse linear regression.
\newblock {\em IEEE Transactions on Signal Processing}, 58(10):5262--5276,
  2010.

\bibitem{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera y
  Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem{nazari2019dadam}
Parvin Nazari, Davoud~Ataee Tarzanagh, and George Michailidis.
\newblock Dadam: A consensus-based distributed adaptive gradient method for
  online optimization.
\newblock {\em arXiv preprint arXiv:1901.09109}, 2019.

\bibitem{nedic2014distributed}
Angelia Nedi{\'c} and Alex Olshevsky.
\newblock Distributed optimization over time-varying directed graphs.
\newblock {\em IEEE Transactions on Automatic Control}, 60(3):601--615, 2014.

\bibitem{nedic2018network}
Angelia Nedi{\'c}, Alex Olshevsky, and Michael~G Rabbat.
\newblock Network topology and communication-computation tradeoffs in
  decentralized optimization.
\newblock {\em Proceedings of the IEEE}, 106(5):953--976, 2018.

\bibitem{nedic2017achieving}
A. Nedic, A. Olshevsky, and W. Shi.
\newblock Achieving geometric convergence for distributed optimization over
  time-varying graphs.
\newblock {\em SIAM Journal on Optimization}, 27(4):2597--2633, 2017.

\bibitem{nedic2009distributed}
Angelia Nedic and Asuman Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock {\em IEEE Transactions on Automatic Control}, 54(1):48--61, 2009.

\bibitem{nesterov2003introductory}
Yurii Nesterov.
\newblock {\em Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 8024--8035, 2019.

\bibitem{qu2018harnessing}
G. Qu and N. Li.
\newblock Harnessing smoothness to accelerate distributed optimization.
\newblock {\em IEEE Transactions on Control of Network Systems},
  5(3):1245--1260, 2018.

\bibitem{rakhlin2012making}
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  1571--1578, 2012.

\bibitem{pmlr-v28-ramdas13}
Aaditya Ramdas and Aarti Singh.
\newblock Optimal rates for stochastic convex optimization under tsybakov noise
  condition.
\newblock In {\em International Conference on Machine Learning}, pages
  365--373, 2013.

\bibitem{Rogozin2021AnAM}
Alexander Rogozin, Mikhail~Mikhailovich Bochko, Pavel~E. Dvurechensky,
  Alexander~V. Gasnikov, and Vladislav Lukoshkin.
\newblock An accelerated method for decentralized distributed stochastic
  optimization over time-varying graphs.
\newblock {\em IEEE Conference on Decision and Control (CDC)}, 2021.

\bibitem{rogozin2021towards}
Alexander Rogozin, Vladislav Lukoshkin, Alexander Gasnikov, Dmitry Kovalev, and
  Egor Shulgin.
\newblock Towards accelerated rates for distributed optimization over
  time-varying networks.
\newblock In {\em International Conference on Optimization and Applications},
  2021.

\bibitem{sayed2014adaptive}
Ali~H Sayed.
\newblock Adaptive networks.
\newblock {\em Proceedings of the IEEE}, 102(4):460--497, 2014.

\bibitem{scaman2017optimal}
Kevin Scaman, Francis Bach, S{\'e}bastien Bubeck, Yin~Tat Lee, and Laurent
  Massouli{\'e}.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  3027--3036, 2017.

\bibitem{scaman2018optimal}
Kevin Scaman, Francis Bach, S{\'e}bastien Bubeck, Laurent Massouli{\'e}, and
  Yin~Tat Lee.
\newblock Optimal algorithms for non-smooth distributed optimization in
  networks.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 2740--2749, 2018.

\bibitem{shi2015extra}
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin.
\newblock {EXTRA}: An exact first-order algorithm for decentralized consensus
  optimization.
\newblock {\em SIAM Journal on Optimization}, 25(2):944--966, 2015.

\bibitem{shi2014linear}
Wei Shi, Qing Ling, Kun Yuan, Gang Wu, and Wotao Yin.
\newblock On the linear convergence of the admm in decentralized consensus
  optimization.
\newblock {\em IEEE Transactions on Signal Processing}, 62(7):1750--1761, 2014.

\bibitem{stich2019local}
Sebastian~Urban Stich.
\newblock Local sgd converges fast and communicates little.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{sun2019distributed}
Haoran Sun and Mingyi Hong.
\newblock Distributed non-convex first-order optimization and information
  processing: Lower complexity bounds and rate optimal algorithms.
\newblock {\em IEEE Transactions on Signal processing}, 67(22):5912--5928,
  2019.

\bibitem{tang2018d}
Hanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, and Ji Liu.
\newblock $ d^2$: Decentralized training over decentralized data.
\newblock In {\em International Conference on Machine Learning}, pages
  4848--4856, 2018.

\bibitem{tang2019doublesqueeze}
Hanlin Tang, Chen Yu, Xiangru Lian, Tong Zhang, and Ji Liu.
\newblock Doublesqueeze: Parallel stochastic gradient descent with double-pass
  error-compensated compression.
\newblock In {\em International Conference on Machine Learning}, pages
  6155--6165. PMLR, 2019.

\bibitem{uribe2020dual}
C{\'e}sar~A Uribe, Soomin Lee, Alexander Gasnikov, and Angelia Nedi{\'c}.
\newblock A dual approach for optimal algorithms in distributed optimization
  over networks.
\newblock {\em Optimization Methods and Software}, pages 1--40, 2020.

\bibitem{wang2019matcha}
Jianyu Wang, Anit~Kumar Sahu, Zhouyi Yang, Gauri Joshi, and Soummya Kar.
\newblock {MATCHA}: Speeding up decentralized {SGD} via matching decomposition
  sampling.
\newblock {\em arXiv preprint arXiv:1905.09435}, 2019.

\bibitem{watts1999small}
Duncan~J Watts.
\newblock Small worlds: the dynamics of networks between order and randomness,
  1999.

\bibitem{watts1998collective}
Duncan~J Watts and Steven~H Strogatz.
\newblock Collective dynamics of ‘small-world’networks.
\newblock {\em nature}, 393(6684):440--442, 1998.

\bibitem{wu2012robustness}
Jun Wu, Mauricio Barahona, Yue-jin Tan, and Hong-zhong Deng.
\newblock Robustness of random graphs based on graph spectra.
\newblock {\em Chaos: An Interdisciplinary Journal of Nonlinear Science},
  22(4):043101, 2012.

\bibitem{xin2020improved}
Ran Xin, Usman~A Khan, and Soummya Kar.
\newblock An improved convergence analysis for decentralized online stochastic
  non-convex optimization.
\newblock {\em IEEE Transactions on Signal Processing}, 2020.

\bibitem{xu2015augmented}
Jinming Xu, Shanying Zhu, Yeng~Chai Soh, and Lihua Xie.
\newblock Augmented distributed gradient methods for multi-agent optimization
  under uncoordinated constant stepsizes.
\newblock In {\em IEEE Conference on Decision and Control (CDC)}, pages
  2055--2060, Osaka, Japan, 2015.

\bibitem{ying2021exponential}
Bicheng Ying, Kun Yuan, Yiming Chen, Hanbin Hu, Pan Pan, and Wotao Yin.
\newblock Exponential graph is provably efficient for decentralized deep
  training.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  34, 2021.

\bibitem{yu2019linear}
Hao Yu, Rong Jin, and Sen Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  sgd for distributed non-convex optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  7184--7193. PMLR, 2019.

\bibitem{yuan2021removing}
Kun Yuan, Sulaiman~A Alghunaim, and Xinmeng Huang.
\newblock Removing data heterogeneity influence enhances network topology
  dependence of decentralized sgd.
\newblock {\em arXiv preprint arXiv:2105.08023}, 2021.

\bibitem{yuan2021decentlam}
Kun Yuan, Yiming Chen, Xinmeng Huang, Yingya Zhang, Pan Pan, Yinghui Xu, and
  Wotao Yin.
\newblock {DecentLaM}: Decentralized momentum {SGD} for large-batch deep
  training.
\newblock In {\em 2021 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, 2021.

\bibitem{yuan2016convergence}
Kun Yuan, Qing Ling, and Wotao Yin.
\newblock On the convergence of decentralized gradient descent.
\newblock {\em SIAM Journal on Optimization}, 26(3):1835--1854, 2016.

\bibitem{yuan2017exact1}
Kun Yuan, Bicheng Ying, Xiaochuan Zhao, and Ali~H. Sayed.
\newblock Exact dffusion for distributed optimization and learning -- {Part I:
  Algorithm development}.
\newblock {\em IEEE Transactions on Signal Processing}, 67(3):708 -- 723, 2019.

\bibitem{yurochkin2019bayesian}
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia
  Hoang, and Yasaman Khazaeni.
\newblock Bayesian nonparametric federated learning of neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  7252--7261. PMLR, 2019.

\bibitem{zhang2019decentralized}
Jiaqi Zhang and Keyou You.
\newblock Decentralized stochastic gradient tracking for non-convex empirical
  risk minimization.
\newblock {\em arXiv preprint arXiv:1909.02712}, 2019.

\end{thebibliography}
