\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Balandat et~al.(2019)Balandat, Karrer, Jiang, Daulton, Letham, Wilson,
  and Bakshy]{balandat2019botorch}
M.~Balandat, B.~Karrer, D.~R. Jiang, S.~Daulton, B.~Letham, A.~G. Wilson, and
  E.~Bakshy.
\newblock {BoTorch: Programmable Bayesian Optimization in PyTorch}.
\newblock \emph{arXiv preprint arXiv:1910.06403}, 2019.

\bibitem[Bertsekas(2017)]{bertsekas2017dynamic}
D.~P. Bertsekas.
\newblock \emph{{Dynamic programming and optimal control}}, volume~1.
\newblock Athena scientific, 2017.

\bibitem[Bertsekas and Tsitsiklis(1996)]{bertsekas1996neuro}
D.~P. Bertsekas and J.~N. Tsitsiklis.
\newblock \emph{Neuro-dynamic programming}.
\newblock Athena Scientific, 1996.

\bibitem[Desautels et~al.(2014)Desautels, Krause, and
  Burdick]{desautels2014parallelizing}
T.~Desautels, A.~Krause, and J.~W. Burdick.
\newblock {Parallelizing exploration-exploitation tradeoffs in Gaussian process
  bandit optimization}.
\newblock \emph{Journal of Machine Learning Research}, 15:\penalty0 3873--3923,
  2014.

\bibitem[Eggensperger et~al.(2015)Eggensperger, Hutter, Hoos, and
  Leyton-Brown]{eggensperger2015efficient}
K.~Eggensperger, F.~Hutter, H.~Hoos, and K.~Leyton-Brown.
\newblock Efficient benchmarking of hyperparameter optimizers via surrogates.
\newblock In \emph{Twenty-Ninth AAAI Conference on Artificial Intelligence},
  2015.

\bibitem[Eggensperger et~al.(2018)Eggensperger, Lindauer, Hoos, Hutter, and
  Leyton-Brown]{eggensperger2018efficient}
K.~Eggensperger, M.~Lindauer, H.~H. Hoos, F.~Hutter, and K.~Leyton-Brown.
\newblock Efficient benchmarking of algorithm configurators via model-based
  surrogates.
\newblock \emph{Machine Learning}, 107\penalty0 (1):\penalty0 15--41, 2018.

\bibitem[Frazier(2018)]{frazier2018tutorial}
P.~I. Frazier.
\newblock A tutorial on bayesian optimization.
\newblock \emph{arXiv preprint arXiv:1807.02811}, 2018.

\bibitem[Gardner et~al.(2018)Gardner, Pleiss, Weinberger, Bindel, and
  Wilson]{gardner2018gpytorch}
J.~Gardner, G.~Pleiss, K.~Q. Weinberger, D.~Bindel, and A.~G. Wilson.
\newblock Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu
  acceleration.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7576--7586, 2018.

\bibitem[Ginsbourger and Le~Riche(2010)]{ginsbourger2010towards}
D.~Ginsbourger and R.~Le~Riche.
\newblock {Towards Gaussian process-based optimization with finite time
  horizon}.
\newblock In \emph{Advances in Model-Oriented Design and Analysis (\acro{mODa})
  9}, pages 89--96, 2010.

\bibitem[Ginsbourger et~al.(2010)Ginsbourger, Le~Riche, and
  Carraro]{ginsbourger2010kriging}
D.~Ginsbourger, R.~Le~Riche, and L.~Carraro.
\newblock Kriging is well-suited to parallelize optimization.
\newblock In \emph{Computational Intelligence in Expensive Optimization
  Problems}, pages 131--162. 2010.

\bibitem[Gonz{\'a}lez et~al.(2016)Gonz{\'a}lez, Osborne, and
  Lawrence]{gonzalez2016glasses}
J.~Gonz{\'a}lez, M.~Osborne, and N.~D. Lawrence.
\newblock {GLASSES: Relieving the myopia of Bayesian optimisation}.
\newblock In \emph{Proceedings of the 19th International Conference on
  Artificial Intelligence and Statistics (\acro{AISTATS})}, 2016.

\bibitem[Griffiths and Hern{\'a}ndez-Lobato(2020)]{griffiths2020constrained}
R.-R. Griffiths and J.~M. Hern{\'a}ndez-Lobato.
\newblock {Constrained Bayesian optimization for automatic chemical design
  using variational autoencoders}.
\newblock \emph{Chemical Science}, 2020.

\bibitem[Jiang et~al.(2017)Jiang, Malkomes, Converse, Shofner, Moseley, and
  Garnett]{jiang2017efficient}
S.~Jiang, G.~Malkomes, G.~Converse, A.~Shofner, B.~Moseley, and R.~Garnett.
\newblock {Efficient nonmyopic active search}.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning (\acro{ICML})}, 2017.

\bibitem[Jiang et~al.(2018)Jiang, Malkomes, Abbott, Moseley, and
  Garnett]{jiang2018efficient}
S.~Jiang, G.~Malkomes, M.~Abbott, B.~Moseley, and R.~Garnett.
\newblock Efficient nonmyopic batch active search.
\newblock In \emph{Advances in Neural Information Processing Systems
  (\acro{NeurIPS}) 31}, 2018.

\bibitem[Jiang et~al.(2020)Jiang, Chai, Gonzalez, and
  Garnett]{jiang2019binoculars}
S.~Jiang, H.~Chai, J.~Gonzalez, and R.~Garnett.
\newblock {BINOCULARS for Efficient, Nonmyopic Sequential Experimental Design}.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (\acro{ICML})}, 2020.

\bibitem[Kandasamy et~al.(2018)Kandasamy, Neiswanger, Schneider, Poczos, and
  Xing]{kandasamy2018neural}
K.~Kandasamy, W.~Neiswanger, J.~Schneider, B.~Poczos, and E.~P. Xing.
\newblock Neural architecture search with bayesian optimisation and optimal
  transport.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2016--2025, 2018.

\bibitem[{Kingma} and {Welling}(2013)]{kingma2013reparam}
D.~P. {Kingma} and M.~{Welling}.
\newblock {Auto-Encoding Variational Bayes}.
\newblock \emph{arXiv e-prints}, page arXiv:1312.6114, Dec 2013.

\bibitem[Lam et~al.(2016)Lam, Willcox, and Wolpert]{lam2016bayesian}
R.~Lam, K.~Willcox, and D.~H. Wolpert.
\newblock {Bayesian optimization with a finite budget: an approximate dynamic
  programming approach}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (\acro{NeurIPS}) 29}, 2016.

\bibitem[Malkomes and Garnett(2018)]{malkomes2018automating}
G.~Malkomes and R.~Garnett.
\newblock {Automating Bayesian optimization with Bayesian optimization}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (\acro{NeurIPS}) 31}, 2018.

\bibitem[Mo{\v{c}}kus(1974{\natexlab{a}})]{movckus1974bayesian}
J.~Mo{\v{c}}kus.
\newblock {On Bayesian methods for seeking the extremum}.
\newblock In \emph{Optimization Techniques IFIP Technical Conference}, pages
  400--404. Springer, 1974{\natexlab{a}}.

\bibitem[Mo{\v{c}}kus(1974{\natexlab{b}})]{movckus1975bayesian}
J.~Mo{\v{c}}kus.
\newblock {On Bayesian methods for seeking the extremum}.
\newblock In \emph{Optimization Techniques IFIP Technical Conference}, pages
  400--404. Springer, 1974{\natexlab{b}}.

\bibitem[Osborne et~al.(2009)Osborne, Garnett, and
  Roberts]{osborne2009gaussian}
M.~A. Osborne, R.~Garnett, and S.~J. Roberts.
\newblock {Gaussian processes for global optimization}.
\newblock In \emph{The 3rd International Conference on Learning and Intelligent
  Optimization (LION3)}, 2009.

\bibitem[Pleiss et~al.(2018{\natexlab{a}})Pleiss, Gardner, Weinberger, and
  Wilson]{pleiss2018constant}
G.~Pleiss, J.~R. Gardner, K.~Q. Weinberger, and A.~G. Wilson.
\newblock Constant-time predictive distributions for gaussian processes.
\newblock \emph{arXiv preprint arXiv:1803.06058}, 2018{\natexlab{a}}.

\bibitem[Pleiss et~al.(2018{\natexlab{b}})Pleiss, Gardner, Weinberger, and
  Wilson]{pleiss2018love}
G.~Pleiss, J.~R. Gardner, K.~Q. Weinberger, and A.~G. Wilson.
\newblock Constant-time predictive distributions for gaussian processes.
\newblock \emph{CoRR}, abs/1803.06058, 2018{\natexlab{b}}.

\bibitem[Shahriari et~al.(2016)Shahriari, Swersky, Wang, Adams, and
  de~Freitas]{shahriari2016taking}
B.~Shahriari, K.~Swersky, Z.~Wang, R.~P. Adams, and N.~de~Freitas.
\newblock {Taking the human out of the loop: a review of Bayesian
  optimization}.
\newblock \emph{Proceedings of the IEEE}, 104\penalty0 (1):\penalty0 148--175,
  2016.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{snoek2012practical}
J.~Snoek, H.~Larochelle, and R.~P. Adams.
\newblock {Practical Bayesian optimization of machine learning algorithms}.
\newblock In \emph{{Advances in Neural Information Processing Systems
  (\acro{NeurIPS}) 25}}, pages 2951--2959, 2012.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{{Reinforcement learning: An introduction}}.
\newblock MIT press, 2018.

\bibitem[Wang et~al.(2016)Wang, Clark, Liu, and Frazier]{wang2016parallel}
J.~Wang, S.~C. Clark, E.~Liu, and P.~I. Frazier.
\newblock {Parallel Bayesian global optimization of expensive functions}.
\newblock \emph{arXiv preprint arXiv:1602.05149}, 2016.

\bibitem[Wang and Jegelka(2017)]{wangICML2017b}
Z.~Wang and S.~Jegelka.
\newblock {Max-value entropy search for efficient Bayesian optimization}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Wilson et~al.(2018)Wilson, Hutter, and Deisenroth]{wilson2018maxbo}
J.~Wilson, F.~Hutter, and M.~Deisenroth.
\newblock {Maximizing acquisition functions for Bayesian optimization}.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pages
  9905--9916. 2018.

\bibitem[Wu and Frazier(2016)]{wu2016parallel}
J.~Wu and P.~Frazier.
\newblock {The parallel knowledge gradient method for batch Bayesian
  optimization}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (\acro{NeurIPS}) 29}, 2016.

\bibitem[Wu and Frazier(2019)]{wu2019practical}
J.~Wu and P.~Frazier.
\newblock {Practical two-step lookahead Bayesian optimization}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (\acro{NeurIPS}) 32}, 2019.

\bibitem[Yue and Al~Kontar(2020)]{yue2019why}
X.~Yue and R.~Al~Kontar.
\newblock {Why non-myopic Bayesian optimization is promising and how far should
  we look-ahead? A study via rollout}.
\newblock In \emph{Proceedings of the 23rd International Conference on
  Artificial Intelligence and Statistics (\acro{AISTATS})}, 2020.

\bibitem[Zhang et~al.(2019)Zhang, Jiang, Cui, Garnett, and Chen]{zhang2019d}
M.~Zhang, S.~Jiang, Z.~Cui, R.~Garnett, and Y.~Chen.
\newblock D-vae: A variational autoencoder for directed acyclic graphs.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1586--1598, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Apley, and Chen]{zhang2020bayesian}
Y.~Zhang, D.~W. Apley, and W.~Chen.
\newblock {Bayesian optimization for materials design with mixed quantitative
  and qualitative variables}.
\newblock \emph{Scientific Reports}, 10\penalty0 (4924), 2020.
\newblock \doi{https://doi.org/10.1038/s41598-020-60652-9}.

\end{thebibliography}
