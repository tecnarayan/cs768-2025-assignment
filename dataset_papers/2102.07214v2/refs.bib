@article{alistarh2020improved,
  title={Improved Communication Lower Bounds for Distributed Optimisation},
  author={Alistarh, Dan and Korhonen, Janne H},
  journal={arXiv preprint arXiv:2010.08222},
  year={2020}
}

@article{ben2019demystifying,
  title={Demystifying parallel and distributed deep learning: An in-depth concurrency analysis},
  author={Ben-Nun, Tal and Hoefler, Torsten},
  journal={ACM Computing Surveys (CSUR)},
  volume={52},
  number={4},
  pages={1--43},
  year={2019},
  publisher={ACM New York, NY, USA}
}


@article{alistarh2018convergence,
  title={The convergence of sparsified gradient methods},
  author={Alistarh, Dan and Hoefler, Torsten and Johansson, Mikael and Khirirat, Sarit and Konstantinov, Nikola and Renggli, C{\'e}dric},
  journal={arXiv preprint arXiv:1809.10505},
  year={2018}
}

@article{wang2018giant,
  title={Giant: Globally improved approximate newton method for distributed optimization},
  author={Wang, Shusen and Roosta, Fred and Xu, Peng and Mahoney, Michael W},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  pages={2332--2342},
  year={2018}
}

@inproceedings{shamir2014communication,
  title={Communication-efficient distributed optimization using an approximate newton-type method},
  author={Shamir, Ohad and Srebro, Nati and Zhang, Tong},
  booktitle={International conference on machine learning},
  pages={1000--1008},
  year={2014},
  organization={PMLR}
}

@inproceedings{zhang2015disco,
  title={DiSCO: Distributed optimization for self-concordant empirical loss},
  author={Zhang, Yuchen and Lin, Xiao},
  booktitle={International conference on machine learning},
  pages={362--370},
  year={2015},
  organization={PMLR}
}


@article{reddi2016aide,
  title={AIDE: Fast and communication efficient distributed optimization},
  author={Reddi, Sashank J and Kone{\v{c}}n{\`y}, Jakub and Richt{\'a}rik, Peter and P{\'o}cz{\'o}s, Barnab{\'a}s and Smola, Alex},
  journal={arXiv preprint arXiv:1608.06879},
  year={2016}
}

@article{zhang2020distributed,
  title={Distributed adaptive Newton methods with globally superlinear convergence},
  author={Zhang, Jiaqi and You, Keyou and Ba{\c{s}}ar, Tamer},
  journal={arXiv preprint arXiv:2002.07378},
  year={2020}
}

@article{NUQSGD,
  author    = {Ali Ramezani{-}Kebrya and
               Fartash Faghri and
               Daniel M. Roy},
  title     = {{NUQSGD:} Improved Communication Efficiency for Data-parallel {SGD}
               via Nonuniform Quantization},
  journal   = {CoRR},
  volume    = {abs/1908.06077},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.06077},
  archivePrefix = {arXiv},
  eprint    = {1908.06077},
  timestamp = {Sat, 23 Jan 2021 01:20:27 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-06077.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{khirirat2018gradient,
  title={Gradient compression for communication-limited convex optimization},
  author={Khirirat, Sarit and Johansson, Mikael and Alistarh, Dan},
  booktitle={2018 IEEE Conference on Decision and Control (CDC)},
  pages={166--171},
  year={2018},
  organization={IEEE}
}

@InProceedings{pmlr-v70-suresh17a, 
title = {Distributed Mean Estimation with Limited Communication}, 
author = {Ananda Theertha Suresh and Felix X. Yu and Sanjiv Kumar and H. Brendan McMahan}, 
booktitle = {Proceedings of the 34th International Conference on Machine Learning}, 
pages = {3329--3337}, 
year = {2017}, 
editor = {Doina Precup and Yee Whye Teh}, 
volume = {70}, series = {Proceedings of Machine Learning Research}, 
}

@article{LibSVM,
 author = {Chang, Chih-Chung and Lin, Chih-Jen},
 title = {{LIBSVM}: A library for support vector machines},
 journal = {ACM Transactions on Intelligent Systems and Technology},
 volume = {2},
 issue = {3},
 year = {2011},
 pages = {27:1--27:27},
 note =	 {Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}}
}

@inproceedings{NIPS2015_5731,
    title = {Communication Complexity of Distributed Convex Learning and Optimization},
    author = {Arjevani, Yossi and Shamir, Ohad},
    booktitle = {Advances in Neural Information Processing Systems 28 (NIPS 2015)},
    pages = {1756--1764},
    year = {2015},
}

@inproceedings{hendrikx2020statistically,
  title={Statistically preconditioned accelerated gradient method for distributed optimization},
  author={Hendrikx, Hadrien and Xiao, Lin and Bubeck, Sebastien and Bach, Francis and Massoulie, Laurent},
  booktitle={International Conference on Machine Learning},
  pages={4203--4227},
  year={2020},
  organization={PMLR}
}

@article{ghadikolaei2020communication,
  title={Communication-efficient Variance-reduced Stochastic Gradient Descent},
  author={Ghadikolaei, Hossein S and Magn{\'u}sson, Sindri},
  journal={arXiv preprint arXiv:2003.04686},
  year={2020}
}


@inproceedings{PS,
  title={Scaling distributed machine learning with the parameter server},
  author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
  booktitle={Proc.\ 11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 2014)},
  pages={583--598},
  year={2014}
}

@inproceedings{NIPS2013_4902,
    title = {Information-theoretic lower bounds for distributed statistical estimation with communication constraints},
    author = {Zhang, Yuchen and Duchi, John and Jordan, Michael I and Wainwright, Martin J},
    booktitle = {Advances in Neural Information Processing Systems 26 (NIPS 2013)},
    pages = {2328--2336},
    year = {2013},
}

@inproceedings{NIPS2014_5386,
title = {Fundamental Limits of Online and Distributed Algorithms for Statistical Learning and Estimation},
author = {Shamir, Ohad},
booktitle = {Advances in Neural Information Processing Systems 27 (NIPS 2014)},
pages = {163--171},
year = {2014},
}



@article{jaggi2014communication,
  title={Communication-efficient distributed dual coordinate ascent},
  author={Jaggi, Martin and Smith, Virginia and Tak{\'a}{\v{c}}, Martin and Terhorst, Jonathan and Krishnan, Sanjay and Hofmann, Thomas and Jordan, Michael I},
  journal={arXiv preprint arXiv:1409.1458},
  year={2014}
}

@inproceedings{scaman2017optimal,
  title={Optimal algorithms for smooth and strongly convex distributed optimization in networks},
  author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Lee, Yin Tat and Massouli{\'e}, Laurent},
  booktitle={international conference on machine learning},
  pages={3027--3036},
  year={2017},
  organization={PMLR}
}

@misc{chen2019gradient,
  author= {Yuxin Chen},
  title = {Gradient methods for unconstrained problems},
  howpublished = {\url{http://www.princeton.edu/~yc5/ele522_optimization/lectures/grad_descent_unconstrained.pdf}},
  note = {Princeton University, Fall 2019},
  year=2019
}

@article{magnusson2020maintaining,
  title={On maintaining linear convergence of distributed learning and optimization under limited communication},
  author={Magn{\'u}sson, Sindri and Shokri-Ghadikolaei, Hossein and Li, Na},
  journal={IEEE Transactions on Signal Processing},
  volume={68},
  pages={6101--6116},
  year={2020},
  publisher={IEEE}
}

@TECHREPORT{RePEc:cor:louvrp:1927,
title = {Cubic regularization of Newton method and its global performance},
author = {Nesterov, Yurii and Polyak, B.T.},
year = {2006},
institution = {Université catholique de Louvain, Center for Operations Research and Econometrics (CORE)},
type = {LIDAM Reprints CORE},
number = {1927},
url = {https://EconPapers.repec.org/RePEc:cor:louvrp:1927}
}

@inproceedings{ye2018communication,
  title={Communication-computation efficient gradient coding},
  author={Ye, Min and Abbe, Emmanuel},
  booktitle={International Conference on Machine Learning},
  pages={5610--5619},
  year={2018},
  organization={PMLR}
}

@article{khirirat2018distributed,
  title={Distributed learning with compressed gradients},
  author={Khirirat, Sarit and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
  journal={arXiv preprint arXiv:1806.06573},
  year={2018}
}

@article{jordan2018communication,
  title={Communication-efficient distributed statistical inference},
  author={Jordan, Michael I and Lee, Jason D and Yang, Yun},
  journal={Journal of the American Statistical Association},
  year={2018},
  publisher={Taylor \& Francis}
}

@inproceedings{
davies2021new,
title={New Bounds For Distributed Mean Estimation and Variance Reduction},
author={Peter Davies and Vijaykrishna Gurunanthan and Niusha Moshrefi and Saleh Ashkboos and Dan Alistarh},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=t86MwoUCCNe}
}

@misc{safaryan2021fednl,
      title={FedNL: Making Newton-Type Methods Applicable to Federated Learning}, 
      author={Mher Safaryan and Rustem Islamov and Xun Qian and Peter Richtárik},
      year={2021},
      eprint={2106.02969},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{nguyen2018sgd,
  title={SGD and Hogwild! convergence without the bounded gradients assumption},
  author={Nguyen, Lam and Nguyen, Phuong Ha and Dijk, Marten and Richt{\'a}rik, Peter and Scheinberg, Katya and Tak{\'a}c, Martin},
  booktitle={International Conference on Machine Learning},
  pages={3750--3758},
  year={2018},
  organization={PMLR}
}

@article{islamov2021distributed,
  title={Distributed Second Order Methods with Fast Rates and Compressed Communication},
  author={Islamov, Rustem and Qian, Xun and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2102.07158. Accepted to ICML 2021.},
  year={2021}
}

@article{niu2011hogwild,
  title={Hogwild!: A lock-free approach to parallelizing stochastic gradient descent},
  author={Niu, Feng and Recht, Benjamin and R{\'e}, Christopher and Wright, Stephen J},
  journal={arXiv preprint arXiv:1106.5730},
  year={2011}
}

@article{alistarh2016qsgd,
  title={QSGD: Communication-efficient SGD via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  journal={arXiv preprint arXiv:1610.02132},
  year={2016}
}



@inproceedings{mendler2020randomized,
  title={Randomized Block-Diagonal Preconditioning for Parallel Learning},
  author={Mendler-D{\"u}nner, Celestine and Lucchi, Aurelien},
  booktitle={International Conference on Machine Learning},
  pages={6841--6851},
  year={2020},
  organization={PMLR}
}

@INPROCEEDINGS{4048825,
  author={J. N. {Tsitsiklis} and Z. {Luo}},
  booktitle={1986 25th IEEE Conference on Decision and Control}, 
  title={Communication complexity of convex optimization}, 
  year={1986},
  volume={},
  number={},
  pages={608-611},
  doi={10.1109/CDC.1986.267379}}

@inproceedings{vempala2020communication,
  title={The communication complexity of optimization},
  author={Vempala, Santosh S and Wang, Ruosong and Woodruff, David P},
  booktitle={Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={1733--1752},
  year={2020},
  organization={SIAM}
}

