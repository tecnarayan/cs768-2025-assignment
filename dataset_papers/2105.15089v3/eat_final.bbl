\begin{thebibliography}{10}

\bibitem{vln_r2r}
Peter Anderson, Qi~Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko
  S{\"u}nderhauf, Ian Reid, Stephen Gould, and Anton Van Den~Hengel.
\newblock Vision-and-language navigation: Interpreting visually-grounded
  navigation instructions in real environments.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3674--3683, 2018.

\bibitem{attn_sit}
Sara Atito, Muhammad Awais, and Josef Kittler.
\newblock Sit: Self-supervised vision transformer.
\newblock {\em arXiv preprint arXiv:2104.03602}, 2021.

\bibitem{attn_improve5}
Irwan Bello.
\newblock Lambdanetworks: Modeling long-range interactions without attention.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{attn_timesformer}
Gedas Bertasius, Heng Wang, and Lorenzo Torresani.
\newblock Is space-time attention all you need for video understanding?
\newblock {\em arXiv preprint arXiv:2102.05095}, 2021.

\bibitem{curve1}
T.~Bially.
\newblock Space-filling curves: Their generation and their application to
  bandwidth reduction.
\newblock {\em IEEE Transactions on Information Theory}, 15(6):658--664, 1969.

\bibitem{curve7}
Christian B{\"o}hm.
\newblock Space-filling curves for high-performance data mining.
\newblock {\em arXiv preprint arXiv:2008.01684}, 2020.

\bibitem{attn_gpt3}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em arXiv preprint arXiv:2005.14165}, 2020.

\bibitem{attn_detr}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
  Kirillov, and Sergey Zagoruyko.
\newblock End-to-end object detection with transformers.
\newblock In {\em European Conference on Computer Vision}, pages 213--229.
  Springer, 2020.

\bibitem{lpea3}
Andreina~I Castillo, Carlos Chac{\'o}n-D{\'\i}az, Neysa Rodr{\'\i}guez-Murillo,
  Helvecio~D Coletta-Filho, and Rodrigo~PP Almeida.
\newblock Impacts of local population history and ecology on the evolution of a
  globally dispersed pathogen.
\newblock {\em BMC genomics}, 21:1--20, 2020.

\bibitem{attn_ipt}
Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei
  Ma, Chunjing Xu, Chao Xu, and Wen Gao.
\newblock Pre-trained image processing transformer.
\newblock {\em arXiv preprint arXiv:2012.00364}, 2020.

\bibitem{attn_transunet}
Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le~Lu,
  Alan~L Yuille, and Yuyin Zhou.
\newblock Transunet: Transformers make strong encoders for medical image
  segmentation.
\newblock {\em arXiv preprint arXiv:2102.04306}, 2021.

\bibitem{attn_igpt}
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and
  Ilya Sutskever.
\newblock Generative pretraining from pixels.
\newblock In {\em International Conference on Machine Learning}, pages
  1691--1703. PMLR, 2020.

\bibitem{attn_mocov3}
Xinlei Chen, Saining Xie, and Kaiming He.
\newblock An empirical study of training self-supervised visual transformers.
\newblock {\em arXiv preprint arXiv:2104.02057}, 2021.

\bibitem{lpea2}
Ziyi Chen and Lishan Kang.
\newblock Multi-population evolutionary algorithm for solving constrained
  optimization problems.
\newblock In {\em IFIP International Conference on Artificial Intelligence
  Applications and Innovations}, pages 381--395. Springer, 2005.

\bibitem{attn_improve3}
Krzysztof~Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared~Quincy Davis, Afroz
  Mohiuddin, Lukasz Kaiser, David~Benjamin Belanger, Lucy~J Colwell, and Adrian
  Weller.
\newblock Rethinking attention with performers.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{attn_cpvt}
Xiangxiang Chu, Zhi Tian, Bo~Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and
  Chunhua Shen.
\newblock Conditional positional encodings for vision transformers.
\newblock {\em Arxiv preprint 2102.10882}, 2021.

\bibitem{attn_coatnet}
Zihang Dai, Hanxiao Liu, Quoc~V Le, and Mingxing Tan.
\newblock Coatnet: Marrying convolution and attention for all data sizes.
\newblock {\em arXiv preprint arXiv:2106.04803}, 2021.

\bibitem{dea2}
Swagatam Das and Ponnuthurai~Nagaratnam Suganthan.
\newblock Differential evolution: A survey of the state-of-the-art.
\newblock {\em IEEE transactions on evolutionary computation}, 15(1):4--31,
  2010.

\bibitem{attn_convit}
St{\'e}phane d'Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio
  Biroli, and Levent Sagun.
\newblock Convit: Improving vision transformers with soft convolutional
  inductive biases.
\newblock {\em arXiv preprint arXiv:2103.10697}, 2021.

\bibitem{imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{attn_bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{attn_notattention}
Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas.
\newblock Attention is not all you need: Pure attention loses rank doubly
  exponentially with depth.
\newblock {\em arXiv preprint arXiv:2103.03404}, 2021.

\bibitem{attn_vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{dea5}
Manolis Georgioudakis and Vagelis Plevris.
\newblock A comparative study of differential evolution variants in constrained
  structural optimization.
\newblock {\em Frontiers in Built Environment}, 6:102, 2020.

\bibitem{s13}
Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and
  Jian Sun.
\newblock Single path one-shot neural architecture search with uniform
  sampling.
\newblock In {\em European Conference on Computer Vision}, pages 544--560.
  Springer, 2020.

\bibitem{attn_tnt}
Kai Han, An~Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.
\newblock Transformer in transformer.
\newblock {\em arXiv preprint arXiv:2103.00112}, 2021.

\bibitem{tir_fashion200k}
Xintong Han, Zuxuan Wu, Phoenix~X Huang, Xiao Zhang, Menglong Zhu, Yuan Li,
  Yang Zhao, and Larry~S Davis.
\newblock Automatic spatially-aware fashion concept discovery.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1463--1471, 2017.

\bibitem{lpea5}
Phan Thi~Hong Hanh, Pham~Dinh Thanh, and Huynh Thi~Thanh Binh.
\newblock Evolutionary algorithm and multifactorial evolutionary algorithm on
  clustered shortest-path tree problem.
\newblock {\em Information Sciences}, 553:280--304, 2021.

\bibitem{mea2}
William~Eugene Hart, Natalio Krasnogor, and James~E Smith.
\newblock Memetic evolutionary algorithms.
\newblock In {\em Recent advances in memetic algorithms}, pages 3--27.
  Springer, 2005.

\bibitem{s16}
Ahmad Hassanat, Khalid Almohammadi, Esra’ Alkafaween, Eman Abunawas, Awni
  Hammouri, and VB~Prasath.
\newblock Choosing mutation and crossover ratios for genetic algorithms—a
  review with a new dynamic approach.
\newblock {\em Information}, 10(12):390, 2019.

\bibitem{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{attn_improve1}
Ruining He, Anirudh Ravula, Bhargav Kanagal, and Joshua Ainslie.
\newblock Realformer: Transformer likes residual attention.
\newblock {\em arXiv e-prints}, pages arXiv--2012, 2020.

\bibitem{hilbert}
David Hilbert.
\newblock {\"U}ber die stetige abbildung einer linie auf ein
  fl{\"a}chenst{\"u}ck.
\newblock In {\em Dritter Band: Analysis{\textperiodcentered} Grundlagen der
  Mathematik{\textperiodcentered} Physik Verschiedenes}, pages 1--2. Springer,
  1935.

\bibitem{tir_mitstates}
Phillip Isola, Joseph~J Lim, and Edward~H Adelson.
\newblock Discovering states and transformations in image collections.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1383--1391, 2015.

\bibitem{s15}
Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech~M Czarnecki, Jeff
  Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan,
  et~al.
\newblock Population based training of neural networks.
\newblock {\em arXiv preprint arXiv:1711.09846}, 2017.

\bibitem{attn_transgan}
Yifan Jiang, Shiyu Chang, and Zhangyang Wang.
\newblock Transgan: Two transformers can make one strong gan.
\newblock {\em arXiv preprint arXiv:2102.07074}, 2021.

\bibitem{s12}
Shauharda Khadka and Kagan Tumer.
\newblock Evolution-guided policy gradient in reinforcement learning.
\newblock In {\em Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 1196--1208, 2018.

\bibitem{attn_improve4}
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{vln_pta}
Federico Landi, Lorenzo Baraldi, Marcella Cornia, Massimiliano Corsini, and
  Rita Cucchiara.
\newblock Perceive, transform, and act: Multi-modal attention networks for
  vision-and-language navigation.
\newblock {\em arXiv preprint arXiv:1911.12377}, 2019.

\bibitem{attn_bossnas}
Changlin Li, Tao Tang, Guangrun Wang, Jiefeng Peng, Bing Wang, Xiaodan Liang,
  and Xiaojun Chang.
\newblock Bossnas: Exploring hybrid cnn-transformers with block-wisely
  self-supervised neural architecture search.
\newblock {\em arXiv preprint arXiv:2103.12424}, 2021.

\bibitem{lpea4}
Xiaoyu Li, Lei Wang, Qiaoyong Jiang, and Ning Li.
\newblock Differential evolution algorithm with multi-population cooperation
  and multi-strategy integration.
\newblock {\em Neurocomputing}, 421:285--302, 2021.

\bibitem{attn_localvit}
Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van~Gool.
\newblock Localvit: Bringing locality to vision transformers.
\newblock {\em arXiv preprint arXiv:2104.05707}, 2021.

\bibitem{attn_swintransformer}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock {\em arXiv preprint arXiv:2103.14030}, 2021.

\bibitem{lpea1}
David~E McCauley.
\newblock Genetic consequences of local population extinction and
  recolonization.
\newblock {\em Trends in Ecology \& Evolution}, 6(1):5--8, 1991.

\bibitem{curve2}
Mohamed~F Mokbel, Walid~G Aref, and Ibrahim Kamel.
\newblock Analysis of multi-dimensional space-filling curves.
\newblock {\em GeoInformatica}, 7(3):179--209, 2003.

\bibitem{zorder}
Guy~M Morton.
\newblock A computer oriented geodetic data base and a new technique in file
  sequencing.
\newblock {\em empty}, 1966.

\bibitem{mea1}
Pablo Moscato et~al.
\newblock On evolution, search, optimization, genetic algorithms and martial
  arts: Towards memetic algorithms.
\newblock {\em Caltech concurrent computation program, C3P Report}, 826:1989,
  1989.

\bibitem{attn_fdsl}
Kodai Nakashima, Hirokatsu Kataoka, Asato Matsumoto, Kenji Iwata, and Nakamasa
  Inoue.
\newblock Can vision transformers learn without natural images?
\newblock {\em arXiv preprint arXiv:2103.13023}, 2021.

\bibitem{dea3}
Nikhil Padhye, Pulkit Mittal, and Kalyanmoy Deb.
\newblock Differential evolution: Performances and analyses.
\newblock In {\em 2013 IEEE Congress on Evolutionary Computation}, pages
  1960--1967. IEEE, 2013.

\bibitem{peano}
Giuseppe Peano.
\newblock Sur une courbe, qui remplit toute une aire plane.
\newblock {\em Mathematische Annalen}, 36(1):157--160, 1890.

\bibitem{attn_elmo}
Matthew~E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
  Kenton Lee, and Luke Zettlemoyer.
\newblock Deep contextualized word representations.
\newblock {\em arXiv preprint arXiv:1802.05365}, 2018.

\bibitem{dea4}
Adam~P Piotrowski and Jaros{\l}aw~J Napi{\'o}rkowski.
\newblock The grouping differential evolution algorithm for multi-dimensional
  optimization problems.
\newblock {\em Control and Cybernetics}, 39:527--550, 2010.

\bibitem{attn_gpt1}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock {\em empty}, 2018.

\bibitem{attn_gpt2}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{reddi2018adaptive}
S~Reddi, Manzil Zaheer, Devendra Sachan, Satyen Kale, and Sanjiv Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock In {\em Proceeding of 32nd Conference on Neural Information
  Processing Systems (NIPS 2018)}, 2018.

\bibitem{curve4}
Hans Sagan.
\newblock {\em Space-filling curves}.
\newblock Springer Science \& Business Media, 2012.

\bibitem{openaies}
Tim Salimans, Jonathan Ho, Xi~Chen, Szymon Sidor, and Ilya Sutskever.
\newblock Evolution strategies as a scalable alternative to reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:1703.03864}, 2017.

\bibitem{gradcam}
Ramprasaath~R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam,
  Devi Parikh, and Dhruv Batra.
\newblock Grad-cam: Visual explanations from deep networks via gradient-based
  localization.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 618--626, 2017.

\bibitem{dynamic2}
Anabela Sim{\~o}es and Ernesto Costa.
\newblock The influence of population and memory sizes on the evolutionary
  algorithm’s performance for dynamic environments.
\newblock In {\em Workshops on Applications of Evolutionary Computation}, pages
  705--714. Springer, 2009.

\bibitem{ea1}
Andrew~N Sloss and Steven Gustafson.
\newblock 2019 evolutionary algorithms review.
\newblock {\em arXiv preprint arXiv:1906.08870}, 2019.

\bibitem{attn_botnet}
Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel,
  and Ashish Vaswani.
\newblock Bottleneck transformers for visual recognition.
\newblock {\em arXiv preprint arXiv:2101.11605}, 2021.

\bibitem{dea1}
Rainer Storn and Kenneth Price.
\newblock Differential evolution--a simple and efficient heuristic for global
  optimization over continuous spaces.
\newblock {\em Journal of global optimization}, 11(4):341--359, 1997.

\bibitem{dynamic1}
Kay~Chen Tan, Tong~Heng Lee, and Eik~Fun Khor.
\newblock Evolutionary algorithms with dynamic population size and local
  exploration for multiobjective optimization.
\newblock {\em IEEE Transactions on Evolutionary Computation}, 5(6):565--588,
  2001.

\bibitem{attn_deit}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock {\em arXiv preprint arXiv:2012.12877}, 2020.

\bibitem{attn_cait}
Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and
  Herv{\'e} J{\'e}gou.
\newblock Going deeper with image transformers.
\newblock {\em arXiv preprint arXiv:2103.17239}, 2021.

\bibitem{attn_medt}
Jeya Maria~Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu, and Vishal~M
  Patel.
\newblock Medical transformer: Gated axial-attention for medical image
  segmentation.
\newblock {\em arXiv preprint arXiv:2102.10662}, 2021.

\bibitem{curve3}
Levi Valgaerts.
\newblock Space-filling curves an introduction.
\newblock {\em Technical University Munich}, 2005.

\bibitem{attn_halonets}
Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake
  Hechtman, and Jonathon Shlens.
\newblock Scaling local self-attention for parameter efficient visual
  backbones.
\newblock {\em arXiv preprint arXiv:2103.12731}, 2021.

\bibitem{attn}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem{ea2}
Pradnya~A Vikhar.
\newblock Evolutionary algorithms: A critical review and its future prospects.
\newblock In {\em 2016 International conference on global trends in signal
  processing, information computing and communication (ICGTSPICC)}, pages
  261--265. IEEE, 2016.

\bibitem{tir_css}
Nam Vo, Lu~Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li~Fei-Fei, and James
  Hays.
\newblock Composing text and image for image retrieval-an empirical odyssey.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 6439--6448, 2019.

\bibitem{attn_fat}
Zhaoyi Wan, Haoran Chen, Jielei Zhang, Wentao Jiang, Cong Yao, and Jiebo Luo.
\newblock Facial attribute transformers for precise and robust makeup transfer.
\newblock {\em arXiv preprint arXiv:2104.02894}, 2021.

\bibitem{attn_hat}
Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, and
  Song Han.
\newblock Hat: Hardware-aware transformers for efficient natural language
  processing.
\newblock {\em arXiv preprint arXiv:2005.14187}, 2020.

\bibitem{attn_improve2}
Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock {\em arXiv preprint arXiv:2006.04768}, 2020.

\bibitem{attn_pvt}
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong
  Lu, Ping Luo, and Ling Shao.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock {\em arXiv preprint arXiv:2102.12122}, 2021.

\bibitem{attn_visualtransformers}
Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan,
  Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter Vajda.
\newblock Visual transformers: Token-based image representation and processing
  for computer vision.
\newblock {\em arXiv preprint arXiv:2006.03677}, 2020.

\bibitem{attn_cvt}
Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu~Yuan, and Lei
  Zhang.
\newblock Cvt: Introducing convolutions to vision transformers.
\newblock {\em arXiv preprint arXiv:2103.15808}, 2021.

\bibitem{s14}
Zhaohui Yang, Yunhe Wang, Xinghao Chen, Boxin Shi, Chao Xu, Chunjing Xu,
  Qi~Tian, and Chang Xu.
\newblock Cars: Continuous evolution for efficient neural architecture search.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 1829--1838, 2020.

\bibitem{attn_ceit}
Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu.
\newblock Incorporating convolution designs into visual transformers.
\newblock {\em arXiv preprint arXiv:2103.11816}, 2021.

\bibitem{attn_t2tvit}
Li~Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis~EH Tay, Jiashi
  Feng, and Shuicheng Yan.
\newblock Tokens-to-token vit: Training vision transformers from scratch on
  imagenet.
\newblock {\em arXiv preprint arXiv:2101.11986}, 2021.

\bibitem{attn_setr}
Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang,
  Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip~HS Torr, et~al.
\newblock Rethinking semantic segmentation from a sequence-to-sequence
  perspective with transformers.
\newblock {\em arXiv preprint arXiv:2012.15840}, 2020.

\bibitem{attn_deepvit}
Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Qibin Hou,
  and Jiashi Feng.
\newblock Deepvit: Towards deeper vision transformer.
\newblock {\em arXiv preprint arXiv:2103.11886}, 2021.

\bibitem{curve6}
Liang Zhou, Chris~R Johnson, and Daniel Weiskopf.
\newblock Data-driven space-filling curves.
\newblock {\em IEEE Transactions on Visualization and Computer Graphics}, 2020.

\bibitem{dcnv2}
Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai.
\newblock Deformable convnets v2: More deformable, better results.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 9308--9316, 2019.

\bibitem{attn_deformabledetr}
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
\newblock Deformable detr: Deformable transformers for end-to-end object
  detection.
\newblock {\em arXiv preprint arXiv:2010.04159}, 2020.

\end{thebibliography}
