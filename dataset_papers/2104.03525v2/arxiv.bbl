\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{zhu19}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock \emph{arXiv preprint arXiv:1811.03962}, 2019.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{Arora19}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock \emph{CoRR}, abs/1904.11955, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, and
  Wang]{Arora2019FineGrainedAO}
Arora, S., Du, S.~S., Hu, W., Li, Z., and Wang, R.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning},
  2019{\natexlab{b}}.

\bibitem[Arora et~al.(2020)Arora, Du, Li, Salakhutdinov, Wang, and
  Yu]{Arora2020Harnessing}
Arora, S., Du, S.~S., Li, Z., Salakhutdinov, R., Wang, R., and Yu, D.
\newblock Harnessing the power of infinitely wide deep nets on small-data
  tasks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Ash et~al.(2020)Ash, Zhang, Krishnamurthy, Langford, and
  Agarwal]{badge}
Ash, J.~T., Zhang, C., Krishnamurthy, A., Langford, J., and Agarwal, A.
\newblock Deep batch active learning by diverse, uncertain gradient lower
  bounds.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Athiwaratkun et~al.(2019)Athiwaratkun, Finzi, Izmailov, and
  Wilson]{fastSWA}
Athiwaratkun, B., Finzi, M., Izmailov, P., and Wilson, A.~G.
\newblock There are many consistent explanations of unlabeled data: Why you
  should average.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Ben-David et~al.(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,
  and Vaughan]{Hdivergence}
Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan,
  J.
\newblock A theory of learning from different domains.
\newblock \emph{Machine Learning}, 79:\penalty0 151--175, 2010.

\bibitem[Berthelot et~al.(2019)Berthelot, Carlini, Goodfellow, Papernot,
  Oliver, and Raffel]{berthelot2019mixmatch}
Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., and
  Raffel, C.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1905.02249}, 2019.

\bibitem[Berthelot et~al.(2020)Berthelot, Carlini, Cubuk, Kurakin, Sohn, Zhang,
  and Raffel]{remixmatch}
Berthelot, D., Carlini, N., Cubuk, E.~D., Kurakin, A., Sohn, K., Zhang, H., and
  Raffel, C.
\newblock Remixmatch: Semi-supervised learning with distribution matching and
  augmentation anchoring.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Bordelon et~al.(2021)Bordelon, Canatar, and
  Pehlevan]{bordelon2021spectrum}
Bordelon, B., Canatar, A., and Pehlevan, C.
\newblock Spectrum dependent learning curves in kernel regression and wide
  neural networks, 2021.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du19}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\
  1675--1685, Long Beach, California, USA, 09--15 Jun 2019. PMLR.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{bayesian_dnn}
Gal, Y. and Ghahramani, Z.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In Balcan, M.~F. and Weinberger, K.~Q. (eds.), \emph{Proceedings of
  The 33rd International Conference on Machine Learning}, volume~48 of
  \emph{Proceedings of Machine Learning Research}, pp.\  1050--1059, New York,
  New York, USA, 20--22 Jun 2016. PMLR.

\bibitem[Gissin \& Shalev-Shwartz(2019)Gissin and
  Shalev-Shwartz]{ShalevSchwartz}
Gissin, D. and Shalev-Shwartz, S.
\newblock Discriminative active learning.
\newblock \emph{arXiv preprint arXiv:1907.06347}, 2019.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{calibration}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning - Volume 70}, ICML'17, pp.\  1321–1330. JMLR.org, 2017.

\bibitem[Hanneke(2007)]{Henneke_agnostic}
Hanneke, S.
\newblock A bound on the label complexity of agnostic active learning.
\newblock In \emph{Proceedings of the 24th International Conference on Machine
  Learning}, ICML '07, pp.\  353–360, New York, NY, USA, 2007. Association
  for Computing Machinery.
\newblock ISBN 9781595937933.

\bibitem[He et~al.(2019)He, Huang, and Yuan]{asymmetric}
He, H., Huang, G., and Yuan, Y.
\newblock Asymmetric valleys: Beyond sharp and flat local minima.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32, pp.\  2553--2564. Curran
  Associates, Inc., 2019.

\bibitem[Hsu \& Lin(2015)Hsu and Lin]{ALBL}
Hsu, W.-N. and Lin, H.-T.
\newblock Active learning by learning.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  29\penalty0 (1), Feb. 2015.

\bibitem[Huang et~al.(2016)Huang, Child, Rao, Liu, Satheesh, and Coates]{EGL}
Huang, J., Child, R., Rao, V., Liu, H., Satheesh, S., and Coates, A.
\newblock Active learning for speech recognition: the power of gradients.
\newblock \emph{arXiv preprint}, 12 2016.

\bibitem[Kim et~al.(2020)Kim, Hur, Park, Yang, Hwang, and Shin]{darp}
Kim, J., Hur, Y., Park, S., Yang, E., Hwang, S.~J., and Shin, J.
\newblock Distribution aligning refinery of pseudo-label for imbalanced
  semi-supervised learning.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  14567--14579. Curran Associates, Inc., 2020.

\bibitem[Kirsch et~al.(2019)Kirsch, van Amersfoort, and Gal]{batchbald}
Kirsch, A., van Amersfoort, J., and Gal, Y.
\newblock Batchbald: Efficient and diverse batch acquisition for deep bayesian
  active learning.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 32}, pp.\  7026--7037. Curran Associates,
  Inc., 2019.

\bibitem[Laine \& Aila(2017)Laine and Aila]{pi_model}
Laine, S. and Aila, T.
\newblock Temporal ensembling for semi-supervised learning.
\newblock In \emph{ICLR (Poster)}. OpenReview.net, 2017.

\bibitem[Lee(2013)]{pseudolabel}
Lee, D.-H.
\newblock Pseudo-label : The simple and efficient semi-supervised learning
  method for deep neural networks.
\newblock \emph{ICML 2013 Workshop : Challenges in Representation Learning
  (WREPL)}, 07 2013.

\bibitem[Lee et~al.(2021)Lee, Shin, and Kim]{lee2021abc}
Lee, H., Shin, S., and Kim, H.
\newblock {ABC}: Auxiliary balanced classifier for class-imbalanced
  semi-supervised learning.
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W.
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Lee et~al.(2020)Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and
  Sohl-Dickstein]{Pennington}
Lee, J., Schoenholz, S.~S., Pennington, J., Adlam, B., Xiao, L., Novak, R., and
  Sohl-Dickstein, J.
\newblock Finite versus infinite neural networks: An empirical study.
\newblock In \emph{Proceedings of the 34th International Conference on Neural
  Information Processing Systems}, NIPS'20, Red Hook, NY, USA, 2020. Curran
  Associates Inc.
\newblock ISBN 9781713829546.

\bibitem[Oliver et~al.(2018)Oliver, Odena, Raffel, Cubuk, and
  Goodfellow]{OliverRealistic}
Oliver, A., Odena, A., Raffel, C.~A., Cubuk, E.~D., and Goodfellow, I.
\newblock Realistic evaluation of deep semi-supervised learning algorithms.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  3235--3246. Curran Associates,
  Inc., 2018.

\bibitem[Roth \& Small(2006)Roth and Small]{margin}
Roth, D. and Small, K.
\newblock Margin-based active learning for structured output spaces.
\newblock In F{\"u}rnkranz, J., Scheffer, T., and Spiliopoulou, M. (eds.),
  \emph{Machine Learning: ECML 2006}, pp.\  413--424, Berlin, Heidelberg, 2006.
  Springer Berlin Heidelberg.

\bibitem[Sener \& Savarese(2017)Sener and Savarese]{coreset}
Sener, O. and Savarese, S.
\newblock Active learning for convolutional neural networks: A core-set
  approach.
\newblock \emph{arXiv preprint arXiv:1708.00489}, 2017.

\bibitem[Sohn et~al.(2020)Sohn, Berthelot, Li, Zhang, Carlini, Cubuk, Kurakin,
  Zhang, and Raffel]{fixmatch}
Sohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N., Cubuk, E.~D.,
  Kurakin, A., Zhang, H., and Raffel, C.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and
  confidence.
\newblock \emph{arXiv preprint arXiv:2001.07685}, 2020.

\bibitem[Song et~al.(2019)Song, Berthelot, and Rostamizadeh]{MixMatchAL}
Song, S., Berthelot, D., and Rostamizadeh, A.
\newblock Combining mixmatch and active learning for better accuracy with fewer
  labels.
\newblock \emph{arXiv preprint arXiv:1912.00594}, 2019.

\bibitem[Stathopoulos \& Wu(2002)Stathopoulos and Wu]{lobpcg}
Stathopoulos, A. and Wu, K.
\newblock A block orthogonalization procedure with constant synchronization
  requirements.
\newblock \emph{SIAM Journal on Scientific Computing}, 23\penalty0
  (6):\penalty0 2165--2182, 2002.

\bibitem[Tarvainen \& Valpola(2017)Tarvainen and Valpola]{mean_teacher}
Tarvainen, A. and Valpola, H.
\newblock Mean teachers are better role models: Weight-averaged consistency
  targets improve semi-supervised deep learning results.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30}, pp.\  1195--1204. Curran Associates,
  Inc., 2017.

\bibitem[Wang \& Shang(2014)Wang and Shang]{uncertainty}
Wang, D. and Shang, Y.
\newblock A new active labeling method for deep learning.
\newblock In \emph{2014 International joint conference on neural networks
  (IJCNN)}, pp.\  112--119. IEEE, 2014.

\bibitem[Wang et~al.(2016)Wang, Zhang, Li, Zhang, and Lin]{CEAL}
Wang, K., Zhang, D., Li, Y., Zhang, R., and Lin, L.
\newblock Cost-effective active learning for deep image classification.
\newblock \emph{IEEE Transactions on Circuits and Systems for Video
  Technology}, 27\penalty0 (12):\penalty0 2591--2600, 2016.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and Komodakis]{wide_resnet}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock In \emph{BMVC}, 2016.

\end{thebibliography}
