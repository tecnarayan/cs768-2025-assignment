\begin{thebibliography}{26}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, and Yang]{agarwal_model-based_2020}
Alekh Agarwal, Sham Kakade, and Lin~F. Yang.
\newblock Model-{Based} {Reinforcement} {Learning} with a {Generative} {Model} is {Minimax} {Optimal}, April 2020.
\newblock URL \url{http://arxiv.org/abs/1906.03804}.
\newblock arXiv:1906.03804 [cs, math, stat] version: 3.

\bibitem[Azar et~al.(2012)Azar, Munos, and Kappen]{azar_sample_2012}
Mohammad~Gheshlaghi Azar, Remi Munos, and Bert Kappen.
\newblock On the {Sample} {Complexity} of {Reinforcement} {Learning} with a {Generative} {Model}, June 2012.
\newblock URL \url{http://arxiv.org/abs/1206.6461}.
\newblock arXiv:1206.6461 [cs, stat].

\bibitem[Bartlett and Tewari(2012)]{bartlett_regal_2012}
Peter~L. Bartlett and Ambuj Tewari.
\newblock {REGAL}: {A} {Regularization} based {Algorithm} for {Reinforcement} {Learning} in {Weakly} {Communicating} {MDPs}, May 2012.
\newblock URL \url{https://arxiv.org/abs/1205.2661v1}.

\bibitem[Fruit et~al.(2018)Fruit, Pirotta, Lazaric, and Ortner]{fruit_efficient_2018}
Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner.
\newblock Efficient {Bias}-{Span}-{Constrained} {Exploration}-{Exploitation} in {Reinforcement} {Learning}, July 2018.
\newblock URL \url{http://arxiv.org/abs/1802.04020}.
\newblock arXiv:1802.04020 [cs, stat].

\bibitem[Fruit et~al.(2019)Fruit, Pirotta, and Lazaric]{fruit_near_2019}
Ronan Fruit, Matteo Pirotta, and Alessandro Lazaric.
\newblock Near {Optimal} {Exploration}-{Exploitation} in {Non}-{Communicating} {Markov} {Decision} {Processes}, March 2019.
\newblock URL \url{http://arxiv.org/abs/1807.02373}.
\newblock arXiv:1807.02373 [cs, stat].

\bibitem[Gheshlaghi~Azar et~al.(2013)Gheshlaghi~Azar, Munos, and Kappen]{gheshlaghi_azar_minimax_2013}
Mohammad Gheshlaghi~Azar, RÃ©mi Munos, and Hilbert~J. Kappen.
\newblock Minimax {PAC} bounds on the sample complexity of reinforcement learning with a generative model.
\newblock \emph{Machine Learning}, 91\penalty0 (3):\penalty0 325--349, June 2013.
\newblock ISSN 1573-0565.
\newblock \doi{10.1007/s10994-013-5368-1}.
\newblock URL \url{https://doi.org/10.1007/s10994-013-5368-1}.

\bibitem[Jin et~al.(2024)Jin, Gummadi, Zhou, and Blanchet]{jin_feasible_2024}
Ying Jin, Ramki Gummadi, Zhengyuan Zhou, and Jose Blanchet.
\newblock Feasible \${Q}\$-{Learning} for {Average} {Reward} {Reinforcement} {Learning}.
\newblock In \emph{Proceedings of {The} 27th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}}, pages 1630--1638. PMLR, April 2024.
\newblock URL \url{https://proceedings.mlr.press/v238/jin24b.html}.
\newblock ISSN: 2640-3498.

\bibitem[Jin and Sidford(2020)]{jin_efficiently_2020}
Yujia Jin and Aaron Sidford.
\newblock Efficiently {Solving} {MDPs} with {Stochastic} {Mirror} {Descent}, August 2020.
\newblock URL \url{https://arxiv.org/abs/2008.12776v1}.

\bibitem[Jin and Sidford(2021)]{jin_towards_2021}
Yujia Jin and Aaron Sidford.
\newblock Towards {Tight} {Bounds} on the {Sample} {Complexity} of {Average}-reward {MDPs}, June 2021.
\newblock URL \url{http://arxiv.org/abs/2106.07046}.
\newblock arXiv:2106.07046 [cs, math].

\bibitem[Kearns and Singh(1998)]{kearns_finite-sample_1998}
Michael Kearns and Satinder Singh.
\newblock Finite-{Sample} {Convergence} {Rates} for {Q}-{Learning} and {Indirect} {Algorithms}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}}, volume~11. MIT Press, 1998.
\newblock URL \url{https://proceedings.neurips.cc/paper/1998/hash/99adff456950dd9629a5260c4de21858-Abstract.html}.

\bibitem[Levin and Peres(2017)]{levin_markov_2017}
David~A. Levin and Yuval Peres.
\newblock \emph{Markov {Chains} and {Mixing} {Times}}.
\newblock American Mathematical Soc., October 2017.
\newblock ISBN 978-1-4704-2962-1.

\bibitem[Li et~al.(2020)Li, Wei, Chi, Gu, and Chen]{li_breaking_2020}
Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen.
\newblock Breaking the {Sample} {Size} {Barrier} in {Model}-{Based} {Reinforcement} {Learning} with a {Generative} {Model}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}}, volume~33, pages 12861--12872. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/96ea64f3a1aa2fd00c72faacf0cb8ac9-Abstract.html}.

\bibitem[Li et~al.(2022)Li, Wu, and Lan]{li_stochastic_2022}
Tianjiao Li, Feiyang Wu, and Guanghui Lan.
\newblock Stochastic first-order methods for average-reward {Markov} decision processes, May 2022.
\newblock URL \url{https://arxiv.org/abs/2205.05800v5}.

\bibitem[Puterman(2014)]{puterman_markov_2014}
Martin~L. Puterman.
\newblock \emph{Markov {Decision} {Processes}: {Discrete} {Stochastic} {Dynamic} {Programming}}.
\newblock John Wiley \& Sons, August 2014.
\newblock ISBN 978-1-118-62587-3.

\bibitem[Sidford et~al.(2018)Sidford, Wang, Wu, Yang, and Ye]{sidford_near-optimal_2018}
Aaron Sidford, Mengdi Wang, Xian Wu, Lin Yang, and Yinyu Ye.
\newblock Near-{Optimal} {Time} and {Sample} {Complexities} for {Solving} {Markov} {Decision} {Processes} with a {Generative} {Model}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}}, volume~31. Curran Associates, Inc., 2018.
\newblock URL \url{https://proceedings.neurips.cc/paper/2018/hash/bb03e43ffe34eeb242a2ee4a4f125e56-Abstract.html}.

\bibitem[Sobel(1982)]{sobel_variance_1982}
Matthew~J. Sobel.
\newblock The variance of discounted {Markov} decision processes.
\newblock \emph{Journal of Applied Probability}, 19\penalty0 (4):\penalty0 794--802, December 1982.
\newblock ISSN 0021-9002, 1475-6072.
\newblock \doi{10.2307/3213832}.
\newblock URL \url{https://www.cambridge.org/core/journals/journal-of-applied-probability/article/abs/variance-of-discounted-markov-decision-processes/AA4549BFA70081B27C0092F4BF9C661A}.
\newblock Publisher: Cambridge University Press.

\bibitem[Tarbouriech et~al.(2021)Tarbouriech, Pirotta, Valko, and Lazaric]{tarbouriech_provably_2021}
Jean Tarbouriech, Matteo Pirotta, Michal Valko, and Alessandro Lazaric.
\newblock A {Provably} {Efficient} {Sample} {Collection} {Strategy} for {Reinforcement} {Learning}, November 2021.
\newblock URL \url{http://arxiv.org/abs/2007.06437}.
\newblock arXiv:2007.06437 [cs, stat].

\bibitem[Wainwright(2019{\natexlab{a}})]{wainwright_high-dimensional_2019}
Martin~J. Wainwright.
\newblock \emph{High-{Dimensional} {Statistics}: {A} {Non}-{Asymptotic} {Viewpoint}}.
\newblock Cambridge University Press, 1 edition, February 2019{\natexlab{a}}.
\newblock ISBN 978-1-108-62777-1 978-1-108-49802-9.
\newblock \doi{10.1017/9781108627771}.
\newblock URL \url{https://www.cambridge.org/core/product/identifier/9781108627771/type/book}.

\bibitem[Wainwright(2019{\natexlab{b}})]{wainwright_variance-reduced_2019}
Martin~J. Wainwright.
\newblock Variance-reduced \${Q}\$-learning is minimax optimal, August 2019{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/1906.04697}.
\newblock arXiv:1906.04697 [cs, math, stat].

\bibitem[Wang et~al.(2022)Wang, Wang, and Yang]{wang_near_2022}
Jinghan Wang, Mengdi Wang, and Lin~F. Yang.
\newblock Near {Sample}-{Optimal} {Reduction}-based {Policy} {Learning} for {Average} {Reward} {MDP}, December 2022.
\newblock URL \url{http://arxiv.org/abs/2212.00603}.
\newblock arXiv:2212.00603 [cs].

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Blanchet, and Glynn]{wang_optimal_2023}
Shengbo Wang, Jose Blanchet, and Peter Glynn.
\newblock Optimal {Sample} {Complexity} of {Reinforcement} {Learning} for {Mixing} {Discounted} {Markov} {Decision} {Processes}, February 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2302.07477v3}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Blanchet, and Glynn]{wang_optimal_2023-1}
Shengbo Wang, Jose Blanchet, and Peter Glynn.
\newblock Optimal {Sample} {Complexity} for {Average} {Reward} {Markov} {Decision} {Processes}, October 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2310.08833v1}.

\bibitem[Wei et~al.(2020)Wei, Jafarnia-Jahromi, Luo, Sharma, and Jain]{wei_model-free_2020}
Chen-Yu Wei, Mehdi Jafarnia-Jahromi, Haipeng Luo, Hiteshi Sharma, and Rahul Jain.
\newblock Model-free {Reinforcement} {Learning} in {Infinite}-horizon {Average}-reward {Markov} {Decision} {Processes}, February 2020.
\newblock URL \url{http://arxiv.org/abs/1910.07072}.
\newblock arXiv:1910.07072 [cs, stat].

\bibitem[Yu(1997)]{yu1997assouad}
Bin Yu.
\newblock Assouad, {Fano}, and {Le Cam}.
\newblock In \emph{Festschrift for Lucien Le Cam}, pages 423--435. Springer, 1997.

\bibitem[Zhang and Ji(2019)]{zhang_regret_2019}
Zihan Zhang and Xiangyang Ji.
\newblock Regret {Minimization} for {Reinforcement} {Learning} by {Evaluating} the {Optimal} {Bias} {Function}, December 2019.
\newblock URL \url{http://arxiv.org/abs/1906.05110}.
\newblock arXiv:1906.05110 [cs, stat] version: 3.

\bibitem[Zhang and Xie(2023)]{zhang_sharper_2023}
Zihan Zhang and Qiaomin Xie.
\newblock Sharper {Model}-free {Reinforcement} {Learning} for {Average}-reward {Markov} {Decision} {Processes}, June 2023.
\newblock URL \url{http://arxiv.org/abs/2306.16394}.
\newblock arXiv:2306.16394 [cs].

\end{thebibliography}
