\begin{thebibliography}{10}

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in Neural Information Processing Systems}, 33:6840--6851, 2020.

\bibitem{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695, 2022.

\bibitem{dhariwal2021diffusion}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock {\em Advances in Neural Information Processing Systems}, 34:8780--8794, 2021.

\bibitem{song2023consistency}
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.
\newblock Consistency models.
\newblock {\em arXiv preprint arXiv:2303.01469}, 2023.

\bibitem{luhman2021knowledge}
Eric Luhman and Troy Luhman.
\newblock Knowledge distillation in iterative generative models for improved sampling speed.
\newblock {\em arXiv preprint arXiv:2101.02388}, 2021.

\bibitem{kim2023consistency}
Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon.
\newblock Consistency trajectory models: Learning probability flow ode trajectory of diffusion.
\newblock In {\em International Conference on Learning Representations}, 2024.

\bibitem{esser2024scaling}
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M{\"u}ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et~al.
\newblock Scaling rectified flow transformers for high-resolution image synthesis.
\newblock In {\em Forty-first International Conference on Machine Learning}, 2024.

\bibitem{flux2024}
Black-Forest.
\newblock Flux.
\newblock \url{https://blackforestlabs.ai/announcing-black-forest-labs/}, 2024.

\bibitem{song2020denoising}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.

\bibitem{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash equilibrium.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{karras2022elucidating}
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
\newblock Elucidating the design space of diffusion-based generative models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:26565--26577, 2022.

\bibitem{lu2022dpm}
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.
\newblock Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.
\newblock {\em Advances in Neural Information Processing Systems}, 35:5775--5787, 2022.

\bibitem{pernias2023wurstchen}
Pablo Pernias, Dominic Rampas, Mats~Leon Richter, Christopher Pal, and Marc Aubreville.
\newblock W{\"u}rstchen: An efficient architecture for large-scale text-to-image diffusion models.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{aneja2021contrastive}
Jyoti Aneja, Alex Schwing, Jan Kautz, and Arash Vahdat.
\newblock A contrastive learning approach for training variational autoencoder priors.
\newblock {\em Advances in neural information processing systems}, 34:480--493, 2021.

\bibitem{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock {\em arXiv preprint arXiv:1605.07146}, 2016.

\bibitem{ronneberger2015u}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In {\em Medical image computing and computer-assisted intervention--MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18}, pages 234--241. Springer, 2015.

\bibitem{karras2017progressive}
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
\newblock Progressive growing of gans for improved quality, stability, and variation.
\newblock {\em arXiv preprint arXiv:1710.10196}, 2017.

\bibitem{karras2019style}
Tero Karras, Samuli Laine, and Timo Aila.
\newblock A style-based generator architecture for generative adversarial networks.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 4401--4410, 2019.

\bibitem{mescheder2018training}
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.
\newblock Which training methods for gans do actually converge?
\newblock In {\em International conference on machine learning}, pages 3481--3490. PMLR, 2018.

\bibitem{nagarajan2017gradient}
Vaishnavh Nagarajan and J~Zico Kolter.
\newblock Gradient descent gan optimization is locally stable.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{ho2021classifier}
Jonathan Ho and Tim Salimans.
\newblock Classifier-free diffusion guidance.
\newblock In {\em NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications}, 2021.

\bibitem{kang2023scaling}
Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park.
\newblock Scaling up gans for text-to-image synthesis.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 10124--10134, 2023.

\bibitem{sauer2023stylegan}
Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila.
\newblock Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis.
\newblock In {\em International conference on machine learning}, pages 30105--30118. PMLR, 2023.

\bibitem{sauer2023adversarial}
Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach.
\newblock Adversarial diffusion distillation.
\newblock {\em arXiv preprint arXiv:2311.17042}, 2023.

\bibitem{sauer2024fast}
Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach.
\newblock Fast high-resolution image synthesis with latent adversarial diffusion distillation.
\newblock {\em arXiv preprint arXiv:2403.12015}, 2024.

\bibitem{meng2023distillation}
Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.
\newblock On distillation of guided diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 14297--14306, 2023.

\bibitem{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock {\em Advances in neural information processing systems}, 27, 2014.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In {\em International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{thomee2016yfcc100m}
Bart Thomee, David~A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li.
\newblock Yfcc100m: The new data in multimedia research.
\newblock {\em Communications of the ACM}, 59(2):64--73, 2016.

\bibitem{schuhmann2022laion}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation image-text models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:25278--25294, 2022.

\bibitem{esser2021taming}
Patrick Esser, Robin Rombach, and Bjorn Ommer.
\newblock Taming transformers for high-resolution image synthesis.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 12873--12883, 2021.

\bibitem{salimans2016improved}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi~Chen.
\newblock Improved techniques for training gans.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{kynkaanniemi2019improved}
Tuomas Kynk{\"a}{\"a}nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila.
\newblock Improved precision and recall metric for assessing generative models.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{jabri2022scalable}
Allan Jabri, David Fleet, and Ting Chen.
\newblock Scalable adaptive computation for iterative generation.
\newblock {\em arXiv preprint arXiv:2212.11972}, 2022.

\bibitem{hoogeboom2023simple}
Emiel Hoogeboom, Jonathan Heek, and Tim Salimans.
\newblock simple diffusion: End-to-end diffusion for high resolution images.
\newblock In {\em International Conference on Machine Learning}, pages 13213--13232. PMLR, 2023.

\bibitem{kingma2023understanding}
Diederik~P Kingma and Ruiqi Gao.
\newblock Understanding diffusion objectives as the elbo with simple data augmentation.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem{sauer2022stylegan}
Axel Sauer, Katja Schwarz, and Andreas Geiger.
\newblock Stylegan-xl: Scaling stylegan to large diverse datasets.
\newblock In {\em ACM SIGGRAPH 2022 conference proceedings}, pages 1--10, 2022.

\bibitem{peebles2023scalable}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 4195--4205, 2023.

\bibitem{karras2023analyzing}
Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine.
\newblock Analyzing and improving the training dynamics of diffusion models.
\newblock {\em arXiv preprint arXiv:2312.02696}, 2023.

\bibitem{henighan2020scaling}
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom~B Brown, Prafulla Dhariwal, Scott Gray, et~al.
\newblock Scaling laws for autoregressive generative modeling.
\newblock {\em arXiv preprint arXiv:2010.14701}, 2020.

\bibitem{ho2022cascaded}
Jonathan Ho, Chitwan Saharia, William Chan, David~J Fleet, Mohammad Norouzi, and Tim Salimans.
\newblock Cascaded diffusion models for high fidelity image generation.
\newblock {\em The Journal of Machine Learning Research}, 23(1):2249--2281, 2022.

\bibitem{chung2022diffusion}
Hyungjin Chung, Jeongsol Kim, Michael~T Mccann, Marc~L Klasky, and Jong~Chul Ye.
\newblock Diffusion posterior sampling for general noisy inverse problems.
\newblock {\em arXiv preprint arXiv:2209.14687}, 2022.

\bibitem{zhang2023adding}
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.
\newblock Adding conditional control to text-to-image diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 3836--3847, 2023.

\bibitem{he2023manifold}
Yutong He, Naoki Murata, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Dongjun Kim, Wei-Hsiang Liao, Yuki Mitsufuji, J~Zico Kolter, Ruslan Salakhutdinov, et~al.
\newblock Manifold preserving guided diffusion.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{shoemake1985animating}
Ken Shoemake.
\newblock Animating rotation with quaternion curves.
\newblock In {\em Proceedings of the 12th annual conference on Computer graphics and interactive techniques}, pages 245--254, 1985.

\bibitem{changpinyo2021cc12m}
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.
\newblock {Conceptual 12M}: Pushing web-scale image-text pre-training to recognize long-tail visual concepts.
\newblock In {\em CVPR}, 2021.

\bibitem{kakaobrain2022coyo-700m}
Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim.
\newblock Coyo-700m: Image-text pair dataset.
\newblock \url{https://github.com/kakaobrain/coyo-dataset}, 2022.

\bibitem{Radford2021LearningTV}
Alec Radford, Jong~Wook Kim, Chris Hallacy, A.~Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
\newblock Learning transferable visual models from natural language supervision.
\newblock In {\em ICML}, 2021.

\bibitem{deepfloyd}
DeepFloyd Lab.
\newblock If by deepfloyd lab at stabilityai.
\newblock \url{https://github.com/deep-floyd/IF}, 2023.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}, pages 740--755. Springer, 2014.

\bibitem{salimans2021progressive}
Tim Salimans and Jonathan Ho.
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{luo2023latent}
Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao.
\newblock Latent consistency models: Synthesizing high-resolution images with few-step inference.
\newblock {\em arXiv preprint arXiv:2310.04378}, 2023.

\bibitem{liu2023instaflow}
Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et~al.
\newblock Instaflow: One step is enough for high-quality diffusion-based text-to-image generation.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{xu2023ufogen}
Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou.
\newblock Ufogen: You forward once large scale text-to-image generation via diffusion gans.
\newblock {\em arXiv preprint arXiv:2311.09257}, 2023.

\bibitem{liu2024scott}
Hongjian Liu, Qingsong Xie, Zhijie Deng, Chen Chen, Shixiang Tang, Fueyang Fu, Zheng-jun Zha, and Haonan Lu.
\newblock Scott: Accelerating diffusion models with stochastic consistency distillation.
\newblock {\em arXiv preprint arXiv:2403.01505}, 2024.

\bibitem{balaji2022ediff}
Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et~al.
\newblock ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers.
\newblock {\em arXiv preprint arXiv:2211.01324}, 2022.

\bibitem{saharia2022photorealistic}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily~L Denton, Kamyar Ghasemipour, Raphael Gontijo~Lopes, Burcu Karagol~Ayan, Tim Salimans, et~al.
\newblock Photorealistic text-to-image diffusion models with deep language understanding.
\newblock {\em Advances in Neural Information Processing Systems}, 35:36479--36494, 2022.

\bibitem{chen2023pixart}
Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et~al.
\newblock Pixart-$\alpha$: Fast training of diffusion transformer for photorealistic text-to-image synthesis.
\newblock {\em arXiv preprint arXiv:2310.00426}, 2023.

\bibitem{yin2023one}
Tianwei Yin, Micha{\"e}l Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William~T Freeman, and Taesung Park.
\newblock One-step diffusion with distribution matching distillation.
\newblock {\em arXiv preprint arXiv:2311.18828}, 2023.

\bibitem{zhou2022towards}
Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun.
\newblock Towards language-free training for text-to-image generation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 17907--17917, 2022.

\bibitem{opennsfw}
Bosco Yung.
\newblock Open-nsfw 2.
\newblock \url{https://github.com/bhky/opennsfw2}, 2021.

\bibitem{man}
Gant Laborde.
\newblock \url{https://github.com/GantMan/nsfw_model}.

\bibitem{nsfw_text_1}
Roman Inflianskas.
\newblock \url{https://github.com/rominf/profanity-filter/blob/master/profanity_filter/data/en_profane_words.txt}.

\bibitem{nsfw_text_2}
Jaclyn Brockschmidt.
\newblock \url{https://github.com/snguyenthanh/better_profanity/blob/master/better_profanity/profanity_wordlist.txt}.

\bibitem{nsfw_text_3}
Jamie Dubs and Ryan Lewis.
\newblock \url{https://gist.github.com/ryanlewis/a37739d710ccdb4b406d}.

\bibitem{touvron2021training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through attention.
\newblock In {\em International conference on machine learning}, pages 10347--10357. PMLR, 2021.

\bibitem{tan2019efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural networks.
\newblock In {\em International conference on machine learning}, pages 6105--6114. PMLR, 2019.

\bibitem{zhao2020differentiable}
Shengyu Zhao, Zhijian Liu, Ji~Lin, Jun-Yan Zhu, and Song Han.
\newblock Differentiable augmentation for data-efficient gan training.
\newblock {\em Advances in neural information processing systems}, 33:7559--7570, 2020.

\bibitem{liu2019variance}
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock {\em arXiv preprint arXiv:1908.03265}, 2019.

\bibitem{oquab2023dinov2}
Maxime Oquab, Timoth{\'e}e Darcet, Th{\'e}o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et~al.
\newblock Dinov2: Learning robust visual features without supervision.
\newblock {\em arXiv preprint arXiv:2304.07193}, 2023.

\bibitem{betker2023improving}
James Betker, Gabriel Goh, Li~Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et~al.
\newblock Improving image generation with better captions.
\newblock {\em Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf}, 2(3):8, 2023.

\bibitem{liu2024visual}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock {\em Advances in neural information processing systems}, 36, 2024.

\bibitem{caron2021emerging}
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv{\'e} J{\'e}gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 9650--9660, 2021.

\bibitem{dettmers20218}
Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer.
\newblock 8-bit optimizers via block-wise quantization.
\newblock {\em arXiv preprint arXiv:2110.02861}, 2021.

\bibitem{saumard2014log}
Adrien Saumard and Jon~A Wellner.
\newblock Log-concavity and strong log-concavity: a review.
\newblock {\em Statistics surveys}, 8:45, 2014.

\bibitem{tang2024contractive}
Wenpin Tang and Hanyang Zhao.
\newblock Contractive diffusion probabilistic models.
\newblock {\em arXiv preprint arXiv:2401.13115}, 2024.

\bibitem{lyu2023sampling}
Junlong Lyu, Zhitang Chen, and Shoubo Feng.
\newblock Sampling is as easy as keeping the consistency: convergence guarantee for consistency models.
\newblock 2023.

\bibitem{gao2023wasserstein}
Xuefeng Gao, Hoang~M Nguyen, and Lingjiong Zhu.
\newblock Wasserstein convergence guarantees for a general class of score-based generative models.
\newblock {\em arXiv preprint arXiv:2311.11003}, 2023.

\bibitem{asner1970total}
Bernard~A Asner, Jr.
\newblock On the total nonnegativity of the hurwitz matrix.
\newblock {\em SIAM Journal on Applied Mathematics}, 18(2):407--414, 1970.

\bibitem{bhatia2002stability}
Nam~Parshad Bhatia and Giorgio~P Szeg{\"o}.
\newblock {\em Stability theory of dynamical systems}.
\newblock Springer Science \& Business Media, 2002.

\bibitem{mescheder2017numerics}
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger.
\newblock The numerics of gans.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{balduzzi2018mechanics}
David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel.
\newblock The mechanics of n-player differentiable games.
\newblock In {\em International Conference on Machine Learning}, pages 354--363. PMLR, 2018.

\bibitem{gemp2018global}
Ian Gemp and Sridhar Mahadevan.
\newblock Global convergence to the equilibrium of gans using variational inequalities.
\newblock {\em arXiv preprint arXiv:1808.01531}, 2018.

\bibitem{wang2019solvable}
Chuang Wang, Hong Hu, and Yue Lu.
\newblock A solvable high-dimensional model of gan.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{qin2020training}
Chongli Qin, Yan Wu, Jost~Tobias Springenberg, Andy Brock, Jeff Donahue, Timothy Lillicrap, and Pushmeet Kohli.
\newblock Training generative adversarial networks by solving ordinary differential equations.
\newblock {\em Advances in Neural Information Processing Systems}, 33:5599--5609, 2020.

\end{thebibliography}
