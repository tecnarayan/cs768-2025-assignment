@inproceedings{arora2018optimization,
	title={On the optimization of deep networks: Implicit acceleration by overparameterization},
	author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad E},
	booktitle={35th International Conference on Machine Learning, ICML 2018},
	pages={372--389},
	year={2018},
	organization={International Machine Learning Society (IMLS)}
}


@article{wu2019implicit,
	title={Implicit Regularization of Normalization Methods},
	author={Wu, Xiaoxia and Dobriban, Edgar and Ren, Tongzheng and Wu, Shanshan and Li, Zhiyuan and Gunasekar, Suriya and Ward, Rachel and Liu, Qiang},
	journal={arXiv preprint arXiv:1911.07956},
	year={2019}
}


@article{strand1974theory,
	title={Theory and methods related to the singular-function expansion and Landweberâ€™s iteration for integral equations of the first kind},
	author={Strand, Otto Neall},
	journal={SIAM Journal on Numerical Analysis},
	volume={11},
	number={4},
	pages={798--825},
	year={1974},
	publisher={SIAM}
}


@inproceedings{morgan1990generalization,
	title={Generalization and parameter estimation in feedforward nets: Some experiments},
	author={Morgan, Nelson and Bourlard, Herv{\'e}},
	booktitle={Advances in neural information processing systems},
	pages={630--637},
	year={1990}
}


@inproceedings{nar2018step,
  title={Step size matters in deep learning},
  author={Nar, Kamil and Sastry, Shankar},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3436--3444},
  year={2018}
}

@article{soudry2018implicit,
	title={The implicit bias of gradient descent on separable data},
	author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
	journal={The Journal of Machine Learning Research},
	volume={19},
	number={1},
	pages={2822--2878},
	year={2018},
	publisher={JMLR. org}
}

@article{neyshabur2014search,
	title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
	author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
	journal={arXiv preprint arXiv:1412.6614},
	year={2014}
}

@inproceedings{wilson2017marginal,
	title={The marginal value of adaptive gradient methods in machine learning},
	author={Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
	booktitle={Advances in Neural Information Processing Systems},
	pages={4148--4158},
	year={2017}
}

@article{zhang2016understanding,
	title={Understanding deep learning requires rethinking generalization},
	author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	journal={arXiv preprint arXiv:1611.03530},
	year={2016}
}

@inproceedings{neyshabur2017exploring,
	title={Exploring generalization in deep learning},
	author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
	booktitle={Advances in Neural Information Processing Systems},
	pages={5947--5956},
	year={2017}
}

@article{soudry2016no,
	title={No bad local minima: Data independent training error guarantees for multilayer neural networks},
	author={Soudry, Daniel and Carmon, Yair},
	journal={arXiv preprint arXiv:1605.08361},
	year={2016}
}

@inproceedings{gunasekar2018implicit,
	title={Implicit bias of gradient descent on linear convolutional networks},
	author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
	booktitle={Advances in Neural Information Processing Systems},
	pages={9461--9471},
	year={2018}
}


@inproceedings{gunasekar2017implicit,
	title={Implicit regularization in matrix factorization},
	author={Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
	booktitle={Advances in Neural Information Processing Systems},
	pages={6151--6159},
	year={2017}
}

@inproceedings{du2018algorithmic,
	title={Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced},
	author={Du, Simon S and Hu, Wei and Lee, Jason D},
	booktitle={Advances in Neural Information Processing Systems},
	pages={384--395},
	year={2018}
}


@inproceedings{tian2017analytical,
	title={An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis},
	author={Tian, Yuandong},
	booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	pages={3404--3413},
	year={2017},
	organization={JMLR. org}
}

@article{saxe2013exact,
	title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
	author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
	journal={arXiv preprint arXiv:1312.6120},
	year={2013}
}


@article{keskar2016large,
	title={On large-batch training for deep learning: Generalization gap and sharp minima},
	author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
	journal={arXiv preprint arXiv:1609.04836},
	year={2016}
}

@inproceedings{wu2018sgd,
	title={How {SGD} selects the global minima in over-parameterized learning: A dynamical stability perspective},
	author={Wu, Lei and Ma, Chao and Weinan, E},
	booktitle={Advances in Neural Information Processing Systems},
	pages={8279--8288},
	year={2018}
}


@article{simsekli2019tail,
	title={A tail-index analysis of stochastic gradient noise in deep neural networks},
	author={Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
	journal={arXiv preprint arXiv:1901.06053},
	year={2019}
}

@article{jastrzkebski2017three,
	title={Three factors influencing minima in {SGD}},
	author={Jastrz{\k{e}}bski, Stanis{\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
	journal={arXiv preprint arXiv:1711.04623},
	year={2017}
}

@inproceedings{hoffer2017train,
	title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
	author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1731--1741},
	year={2017}
}

@article{masters2018revisiting,
	title={Revisiting small batch training for deep neural networks},
	author={Masters, Dominic and Luschi, Carlo},
	journal={arXiv preprint arXiv:1804.07612},
	year={2018}
}

@article{smith2017don,
	title={Don't decay the learning rate, increase the batch size},
	author={Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
	journal={arXiv preprint arXiv:1711.00489},
	year={2017}
}


@article{sagun2017empirical,
	title={Empirical analysis of the {H}essian of over-parametrized neural networks},
	author={Sagun, Levent and Evci, Utku and Guney, V Ugur and Dauphin, Yann and Bottou, Leon},
	journal={arXiv preprint arXiv:1706.04454},
	year={2017}
}

@article{sagun2016eigenvalues,
	title={Eigenvalues of the {H}essian in deep learning: Singularity and beyond},
	author={Sagun, Levent and Bottou, Leon and LeCun, Yann},
	journal={arXiv preprint arXiv:1611.07476},
	year={2016}
}

@inproceedings{dinh2017sharp,
	title={Sharp minima can generalize for deep nets},
	author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
	booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	pages={1019--1028},
	year={2017},
	organization={JMLR. org}
}


@inproceedings{krizhevsky2012imagenet,
	title={Imagenet classification with deep convolutional neural networks},
	author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle={Advances in neural information processing systems},
	pages={1097--1105},
	year={2012}
}

@article{kingma2014adam,
	title={Adam: A method for stochastic optimization},
	author={Kingma, Diederik P and Ba, Jimmy},
	journal={arXiv preprint arXiv:1412.6980},
	year={2014}
}

@article{smith2017bayesian,
	title={A bayesian perspective on generalization and stochastic gradient descent},
	author={Smith, Samuel L and Le, Quoc V},
	journal={arXiv preprint arXiv:1710.06451},
	year={2017}
}

@inproceedings{
	lyu2020gradient,
	title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},
	author={Kaifeng Lyu and Jian Li},
	booktitle={International Conference on Learning Representations},
	year={2020}
}

@article{gunasekar2018characterizing,
	title={Characterizing implicit bias in terms of optimization geometry},
	author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
	journal={arXiv preprint arXiv:1802.08246},
	year={2018}
}

@article{lecun1998mnist,
	title={The {MNIST} database of handwritten digits},
	author={LeCun, Yann},
	year={1998}
}

@article{hochreiter1997flat,
	title={Flat minima},
	author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	journal={Neural Computation},
	volume={9},
	number={1},
	pages={1--42},
	year={1997},
	publisher={MIT Press}
}

@article{chaudhari2019entropy,
	title={Entropy-{SGD}: {B}iasing gradient descent into wide valleys},
	author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
	journal={Journal of Statistical Mechanics: Theory and Experiment},
	volume={2019},
	number={12},
	pages={124018},
	year={2019},
	publisher={IOP Publishing}
}

@inproceedings{ji2019gradient,
	title={Gradient descent aligns the layers of deep linear networks},
	author={Ji, Ziwei and Telgarsky, Matus Jan},
	booktitle={7th International Conference on Learning Representations, ICLR 2019},
	year={2019}
}

@inproceedings{desjardins2015natural,
	title={Natural neural networks},
	author={Desjardins, Guillaume and Simonyan, Karen and Pascanu, Razvan and others},
	booktitle={Advances in Neural Information Processing Systems},
	pages={2071--2079},
	year={2015}
}

@article{arpit2016normalization,
	title={Normalization propagation: A parametric technique for removing internal covariate shift in deep networks},
	author={Arpit, Devansh and Zhou, Yingbo and Kota, Bhargava U and Govindaraju, Venu},
	journal={arXiv preprint arXiv:1603.01431},
	year={2016}
}

@inproceedings{neyshabur2015pathsgd,
	title={Path-{SGD}: Path-normalized optimization in deep neural networks},
	author={Neyshabur, Behnam and Salakhutdinov, Ruslan R and Srebro, Nati},
	booktitle={Advances in Neural Information Processing Systems},
	pages={2422--2430},
	year={2015}
}

@inproceedings{li2018visualizing,
	title={Visualizing the loss landscape of neural nets},
	author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	booktitle={Advances in Neural Information Processing Systems},
	pages={6389--6399},
	year={2018}
}

@inproceedings{zhang2018fixup,
	title={Fixup Initialization: Residual Learning Without Normalization},
	author={Zhang, Hongyi and Dauphin, Yann N and Ma, Tengyu},
	booktitle={International Conference on Learning Representations},
	year={2018}
}

@inproceedings{he2015delving,
	title={Delving deep into rectifiers: {S}urpassing human-level performance on imagenet classification},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={Proceedings of the IEEE international conference on computer vision},
	pages={1026--1034},
	year={2015}
}