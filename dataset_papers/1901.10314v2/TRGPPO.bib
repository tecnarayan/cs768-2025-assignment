@inproceedings{conti2018improving,
  title={Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents},
  author={Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth and Clune, Jeff},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5027--5038},
  year={2018}
}
 
@article{agrawal1995continuum,
  title={The continuum-armed bandit problem},
  author={Agrawal, Rajeev},
  journal={SIAM journal on control and optimization},
  volume={33},
  number={6},
  pages={1926--1951},
  year={1995},
  publisher={SIAM}
}

@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={International Conference on Machine Learning (ICML)},
  volume={2017},
  year={2017}
}
@article{bellemare2013arcade,
  title={The arcade learning environment: An evaluation platform for general agents},
  author={Bellemare, Marc G and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={47},
  pages={253--279},
  year={2013}
}
@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}
@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016}
}
@incollection{NIPS2017_6868,    title = {\#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning},    author = {Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Xi Chen, OpenAI and Duan, Yan and Schulman, John and DeTurck, Filip and Abbeel, Pieter},    booktitle = {Advances in Neural Information Processing Systems 30},    editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},    pages = {2753--2762},    year = {2017},    publisher = {Curran Associates, Inc.}
}
@incollection{NIPS2016_6383,    title = {Unifying Count-Based Exploration and Intrinsic Motivation},    author = {Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},    booktitle = {Advances in Neural Information Processing Systems 29},    editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},    pages = {1471--1479},    year = {2016},    publisher = {Curran Associates, Inc.}}
@inproceedings{Ziebart2010ModelingIV,    title={Modeling Interaction via the Principle of Maximum Causal Entropy},    author={Brian D. Ziebart and J. Andrew Bagnell and Anind K. Dey},    booktitle={ICML},    year={2010}}
@article{peters2008reinforcement,
  title={Reinforcement learning of motor skills with policy gradients},
  author={Peters, Jan and Schaal, Stefan},
  journal={Neural networks},
  volume={21},
  number={4},
  pages={682--697},
  year={2008},
  publisher={Elsevier}
}
@article{NIPS1999_1786,
	title = {Actor-Critic Algorithms},
	author = {Vijay R. Konda and John N. Tsitsiklis},
	booktitle = {Advances in Neural Information Processing Systems 12},
	editor = {S. A. Solla and T. K. Leen and K. M\"{u}ller},
	pages = {1008--1014},
	year = {2000},
	publisher = {MIT Press}
}
@article{powell1970hybrid,
  title={A hybrid method for nonlinear equations},
  author={Powell, Michael JD},
  journal={Numerical methods for nonlinear algebraic equations},
  year={1970},
  publisher={Gordon and Breach}
}
@article{NIPS1999_1713,    title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},    author = {Sutton, Richard S and David A. McAllester and Satinder P. Singh and Mansour, Yishay},    booktitle = {Advances in Neural Information Processing Systems 12},    editor = {S. A. Solla and T. K. Leen and K. M\"{u}ller},    pages = {1057--1063},    year = {2000},    publisher = {MIT Press}
}
@article{haarnoja2017reinforcement,
	title="Reinforcement Learning with Deep Energy-Based Policies",
	author="Tuomas {Haarnoja} and Haoran {Tang} and Pieter {Abbeel} and Sergey {Levine}",
	journal="International Conference on Machine Learning",
	pages="1352--1361",
	year="2017"
}
@misc{burda2018exploration,
	title={Exploration by Random Network Distillation},
	author={Yuri Burda and Harrison Edwards and Amos Storkey and Oleg Klimov},
	year={2018},
	eprint={1810.12894},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}
@misc{houthooft2016vime,
	title={VIME: Variational Information Maximizing Exploration},
	author={Rein Houthooft and Xi Chen and Yan Duan and John Schulman and Filip De Turck and Pieter Abbeel},
	year={2016},
	eprint={1605.09674},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}
@article{levine2016end,
	title="End-to-end training of deep visuomotor policies",
	author="Sergey {Levine} and Chelsea {Finn} and Trevor {Darrell} and Pieter {Abbeel}",
	journal="Journal of Machine Learning Research",
	volume="17",
	number="1",
	pages="1334--1373",
	year="2016"
}
@article{haarnoja2018soft,
	title="Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
	author="Tuomas {Haarnoja} and Aurick {Zhou} and Pieter {Abbeel} and Sergey {Levine}",
	journal="International Conference on Machine Learning",
	pages="1856--1865",
	year="2018"
}
@article{fortunato2018noisy,
	title="Noisy Networks For Exploration",
	author="Meire {Fortunato} and Mohammad Gheshlaghi {Azar} and Bilal {Piot} and Jacob {Menick} and Matteo {Hessel} and Ian {Osband} and Alex {Graves} and Volodymyr {Mnih} and Remi {Munos} and Demis {Hassabis} and Olivier {Pietquin} and Charles {Blundell} and Shane {Legg}",
	journal="International Conference on Learning Representations",
	year="2018"
}
@article{osband2016deep,
	title="Deep Exploration via Bootstrapped DQN.",
	author="Ian {Osband} and Charles {Blundell} and Alexander {Pritzel} and Benjamin Van {Roy}",
	journal="Advances in Neural Information Processing Systems",
	pages="4026--4034",
	year="2016"
}
@article{duan2016benchmarking,
	title="Benchmarking deep reinforcement learning for continuous control",
	author="Yan {Duan} and Xi {Chen} and Rein {Houthooft} and John {Schulman} and Pieter {Abbeel}",
	journal="International Conference on Machine Learning",
	pages="1329--1338",
	year="2016"
}
@article{szepesvari2010algorithms,
  title={Algorithms for reinforcement learning},
  author={Szepesv{\'a}ri, Csaba},
  journal={Synthesis lectures on artificial intelligence and machine learning},
  volume={4},
  number={1},
  pages={1--103},
  year={2010},
  publisher={Morgan \& Claypool Publishers}
}
@article{Chen2018AnAC,
	title={An Adaptive Clipping Approach for Proximal Policy Optimization},
	author={Gang Chen and Yiming Peng and Mengjie Zhang},
	journal={CoRR},
	year={2018},
	volume={abs/1804.06461}
}

@article{Hmlinen2018PPOCMAPP,
	title={PPO-CMA: Proximal Policy Optimization with Covariance Matrix Adaptation},
	author={Perttu H{\"a}m{\"a}l{\"a}inen and Amin Babadi and Xiaoxiao Ma and Jaakko Lehtinen},
	journal={CoRR},
	year={2018},
	volume={abs/1810.02541}
}
@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2018}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}



@article{Silver2014,
abstract = {R{\'{e}}sum{\'{e}}: In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic pol-icy gradient has a particularly appealing form: it is the expected gradient of the action-value func-tion. This simple form ... $\backslash$n},
author = {Silver, D and Lever, G and Heess, N and Degris, T and Wierstra, D},
file = {:E$\backslash$:/rl/actor-critic/Deterministic Policy Gradient Algorithms - 2014.pdf:pdf},
journal = {International Conference on Machine Learning},
title = {{Deterministic Policy Gradient Algorithms}},
year = {2014}
}

@article{Gu2016,
archivePrefix = {arXiv},
arxivId = {1611.02247},
author = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E. and Levine, Sergey},
eprint = {1611.02247},
file = {:E$\backslash$:/rl/model-based/Q-Prop Sample-Efficient Policy Gradient with An Off-Policy Critic - 2016 - Gu et al.pdf:pdf},
pages = {1--13},
title = {{Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic}},
url = {http://arxiv.org/abs/1611.02247},
year = {2016}
}
@article{schulman2016high,
	title="High-Dimensional Continuous Control Using Generalized Advantage Estimation",
	author="John {Schulman} and Philipp {Moritz} and Sergey {Levine} and Michael I. {Jordan} and Pieter {Abbeel}",
	journal="International Conference on Learning Representations",
	year="2016"
}




@article{kaelbling1998planning,
	title={Planning and acting in partially observable stochastic domains},
	author={Kaelbling, Leslie Pack and Littman, Michael L and Cassandra, Anthony R},
	journal={Artificial Intelligence},
	volume={101},
	number={1},
	pages={99--134},
	year={1998}
}
@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={ICML},
  volume={2},
  pages={267--274},
  year={2002}
}
@article{ilyas2018deep,
  title={Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?},
  author={Ilyas, Andrew and Engstrom, Logan and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander},
  journal={arXiv preprint arXiv:1811.02553},
  year={2018}
}
@book{little1987statistical,
	title="Statistical analysis with missing data",
	author="Roderick J A {Little} and Donald B {Rubin}",
	year="1987"
}

@article{Maxim2015deep,
	title="Deep reinforcement learning with pomdps",
	author="Maxim Egorov",
	journal="",
	year="2015"
}
@article{hausknecht2015deep,
	title="Deep Recurrent Q-Learning for Partially Observable MDPs",
	author="Matthew J. {Hausknecht} and Peter {Stone}",
	journal="arXiv preprint arXiv:1507.06527",
	year="2015"
}

@article{igl2018deep,
	title="Deep Variational Reinforcement Learning for POMDPs",
	author="Maximilian {Igl} and Luisa M. {Zintgraf} and Tuan Anh {Le} and Frank {Wood} and Shimon {Whiteson}",
	journal="International Conference on Machine Learning",
	pages="2122--2131",
	year="2018"
}

@article{zhu2017improving,
	title="On Improving Deep Reinforcement Learning for POMDPs.",
	author="Pengfei {Zhu} and Xin {Li} and Pascal {Poupart}",
	journal="arXiv preprint arXiv:1804.06309",
	year="2017"
}


@book{gelman2006data,
	title="Data Analysis Using Regression and Multilevel/Hierarchical Models",
	author="Andrew {Gelman} and Yu-Sung {Su}",
	year="2006"
}

@inproceedings{rodriguez2000reinforcement,
	title="Reinforcement Learning Using Approximate Belief States",
	author="Andres C. {Rodriguez} and Ronald {Parr} and Daphne {Koller}",
	booktitle="Advances in Neural Information Processing Systems 12",
	pages="1036--1042",
	year="2000"
}
@inproceedings{mcallister2017data,
	title="Data-Efficient Reinforcement Learning in Continuous State-Action Gaussian-POMDPs",
	author="Rowan {McAllister} and Carl Edward {Rasmussen}",
	booktitle="NIPS 2017",
	pages="2040--2049",
	year="2017"
}
@book{schafer1997analysis,
  title={Analysis of incomplete multivariate data},
  author={Schafer, Joseph L},
  year={1997},
  publisher={CRC press}
}
@article{silver2017mastering,
	title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
	author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
	journal={arXiv preprint arXiv:1712.01815},
	year={2017}
} 
@article{Schulman2016HighDimensional,
	title={High-Dimensional Continuous Control Using Generalized Advantage Estimation},
	author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael I and Abbeel, Pieter},
	journal={International Conference on Learning Representations},
	year={2016}}
}

@article{wu2017scalable,
	title="Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation",
	author="Yuhuai {Wu} and Elman {Mansimov} and Roger B. {Grosse} and Shun {Liao} and Jimmy {Ba}",
	journal="Advances in Neural Information Processing Systems",
	pages="5279--5288",
	year="2017"
}

@inproceedings{wang2019truly,
	title="Truly Proximal Policy Optimization",
	author={Wang, Yuhui and He, Hao and Tan, Xiaoyang and Gan, Yaozhong},
	booktitle="Uncertainty in Artificial Intelligence",
	year="2019"
}

@inproceedings{fakoor2019p3o,
  title={P3O: Policy-on Policy-off Policy Optimization},
  author={Fakoor, Rasool and Chaudhari, Pratik and Smola, Alexander J},
booktitle="Uncertainty in Artificial Intelligence",
  year={2019}
}

@article{mnih2015human,
	title={Human-level control through deep reinforcement learning},
	author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
	journal={Nature},
	volume={518},
	number={7540},
	pages={529},
	year={2015},
	publisher={Nature Publishing Group}
}
@article{Lizotte2008,
	author = {Lizotte, Daniel J and Gunter, Lacey and Laber, Eric B and Murphy, Susan A},
	file = {:media/d/rl/missingdata/Missing Data and Uncertainty in Batch Reinforcement Learning - 2008 - Lizotte et al.pdf:pdf},
	journal = {NIPS-08 Workshop on Model Uncertainty and Risk in Reinforcement Learning},
	pages = {1--8},
	title = {{Missing Data and Uncertainty in Batch Reinforcement Learning}},
	year = {2008}
}

@inproceedings{munk2016learning,
	title={Learning state representation for deep actor-critic control},
	author={Munk, Jelle and Kober, Jens and Babu{\v{s}}ka, Robert},
	booktitle={Decision and Control (CDC), 2016 IEEE 55th Conference on},
	pages={4667--4673},
	year={2016},
	organization={IEEE}
}

@inproceedings{schulman2015trust,
	title={Trust region policy optimization},
	author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
	booktitle={International Conference on Machine Learning},
	pages={1889--1897},
	year={2015}
}
@article{mania2018simple,
  title={Simple random search provides a competitive approach to reinforcement learning},
  author={Mania, Horia and Guy, Aurelia and Recht, Benjamin},
  journal={arXiv preprint arXiv:1803.07055},
  year={2018}
}

@misc{Brockman2016OpenAI,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}
@inproceedings{Todorov2012MuJoCo,
  title={MuJoCo: A physics engine for model-based control},
  author={Todorov, E and Erez, T and Tassa, Y},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026-5033},
  year={2012},
}
@article{framling2004reinforcement,
  title={Reinforcement learning in a noisy environment: light-seeking robot},
  author={Fr{\"a}mling, Kary},
  journal={WSEAS Transactions on Systems},
  volume={3},
  number={2},
  pages={714--719},
  year={2004}
}
@misc{jette1997reinforcement,
  author = {Randløv, Jette and Alstrøm, Preben},
  title = {Reinforcement Learning based on Incomplete State Data},
  year = 1997,
  url = {https://pdfs.semanticscholar.org/ea66/983af0df5e9b26ab68c2ed2b18ead79cb19d.pdf},
  urldate = {2018-03-21}
}
@inproceedings{rajeswaran2017towards,
  title={Towards generalization and simplicity in continuous control},
  author={Rajeswaran, Aravind and Lowrey, Kendall and Todorov, Emanuel V and Kakade, Sham M},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6553--6564},
  year={2017}
}
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
@misc{baselines,
  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title = {OpenAI Baselines},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openai/baselines}},
}
