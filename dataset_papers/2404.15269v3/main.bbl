\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Agrawal \& Carpuat(2022)Agrawal and Carpuat]{Agrawal2022AnIL}
Sweta Agrawal and Marine Carpuat.
\newblock An imitation learning curriculum for text editing with non-autoregressive models.
\newblock \emph{ArXiv}, abs/2203.09486, 2022.

\bibitem[Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, Chen, Olsson, Olah, Hernandez, Drain, Ganguli, Li, Tran-Johnson, Perez, Kerr, Mueller, Ladish, Landau, Ndousse, Lukosuite, Lovitt, Sellitto, Elhage, Schiefer, Mercado, DasSarma, Lasenby, Larson, Ringer, Johnston, Kravec, Showk, Fort, Lanham, Telleen-Lawton, Conerly, Henighan, Hume, Bowman, Hatfield-Dodds, Mann, Amodei, Joseph, McCandlish, Brown, and Kaplan]{Bai2022ConstitutionalAH}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer~El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel~R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan.
\newblock Constitutional ai: Harmlessness from ai feedback, 2022.

\bibitem[Balachandran et~al.(2022)Balachandran, Hajishirzi, Cohen, and Tsvetkov]{Balachandran2022CorrectingDF}
Vidhisha Balachandran, Hannaneh Hajishirzi, William Cohen, and Yulia Tsvetkov.
\newblock Correcting diverse factual errors in abstractive summarization via post-editing and language model infilling.
\newblock \emph{ArXiv}, abs/2210.12378, 2022.

\bibitem[Bar(2022)]{Bar_PaperTweet}
Nitsan Bar.
\newblock Papertweet.
\newblock \url{https://github.com/bnitsan/PaperTweet/}, 2022.

\bibitem[Botha et~al.(2018)Botha, Faruqui, Alex, Baldridge, and Das]{Botha2018LearningTS}
Jan~A. Botha, Manaal Faruqui, John Alex, Jason Baldridge, and Dipanjan Das.
\newblock Learning to split and rephrase from wikipedia edit history.
\newblock \emph{ArXiv}, abs/1808.09468, 2018.

\bibitem[Brody et~al.(2020)Brody, Alon, and Yahav]{Brody2020ASM}
Shaked Brody, Uri Alon, and Eran Yahav.
\newblock A structural model for contextual code changes.
\newblock \emph{Proceedings of the ACM on Programming Languages}, 4:\penalty0 1 -- 28, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Brynjolfsson et~al.(2023)Brynjolfsson, Li, and Raymond]{brynjolfsson2023generative}
Erik Brynjolfsson, Danielle Li, and Lindsey~R Raymond.
\newblock Generative ai at work.
\newblock Technical report, National Bureau of Economic Research, 2023.

\bibitem[Cao et~al.(2020)Cao, Dong, Wu, and Cheung]{Cao2020FactualEC}
Mengyao Cao, Yue Dong, Jiapeng Wu, and Jackie Chi~Kit Cheung.
\newblock Factual error correction for abstractive summarization models.
\newblock \emph{ArXiv}, abs/2010.08712, 2020.

\bibitem[Chang et~al.(2023)Chang, Brantley, Ramamurthy, Misra, and Sun]{chang2023learning}
Jonathan~D Chang, Kiante Brantley, Rajkumar Ramamurthy, Dipendra Misra, and Wen Sun.
\newblock Learning to generate better than your llm.
\newblock \emph{arXiv preprint arXiv:2306.11816}, 2023.

\bibitem[Chen et~al.(2018)Chen, Kommrusch, Tufano, Pouchet, Poshyvanyk, and Martin]{Chen2018SequenceRSL}
Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-No{\"e}l Pouchet, Denys Poshyvanyk, and Monperrus Martin.
\newblock Sequencer: Sequence-to-sequence learning for end-to-end program repair.
\newblock \emph{IEEE Transactions on Software Engineering}, 47:\penalty0 1943--1959, 2018.

\bibitem[Cheng et~al.(2023)Cheng, Kolobov, Misra, Nie, and Swaminathan]{cheng2023llf}
Ching-An Cheng, Andrey Kolobov, Dipendra Misra, Allen Nie, and Adith Swaminathan.
\newblock Llf-bench: Benchmark for interactive learning from language feedback.
\newblock \emph{arXiv preprint arXiv:2312.06853}, 2023.

\bibitem[Chong et~al.(2023)Chong, Kong, Wu, Liu, Jin, Yang, Fan, Fan, and Yang]{Chong2023LeveragingPT}
Ruining Chong, Cunliang Kong, Liu Wu, Zhenghao Liu, Ziye Jin, Liner Yang, Yange Fan, Hanghang Fan, and Erhong Yang.
\newblock Leveraging prefix transfer for multi-intent text revision.
\newblock \emph{Annual Meeting of the Association for Computational Linguistics}, 2023.

\bibitem[Clement et~al.(2019)Clement, Bierbaum, O'Keeffe, and Alemi]{clement2019arxiv}
Colin~B. Clement, Matthew Bierbaum, Kevin~P. O'Keeffe, and Alexander~A. Alemi.
\newblock On the use of arxiv as a dataset, 2019.

\bibitem[D'Arcy et~al.(2023)D'Arcy, Ross, Bransom, Kuehl, Bragg, Hope, and Downey]{DArcy2023ARIESAC}
Mike D'Arcy, Alexis Ross, Erin Bransom, Bailey Kuehl, Jonathan Bragg, Tom Hope, and Doug Downey.
\newblock Aries: A corpus of scientific paper edits made in response to peer reviews.
\newblock \emph{ArXiv}, abs/2306.12587, 2023.

\bibitem[Deng et~al.(2022)Deng, Wang, Hsieh, Wang, Guo, Shu, Song, Xing, and Hu]{deng2022rlprompt}
Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric~P Xing, and Zhiting Hu.
\newblock Rlprompt: Optimizing discrete text prompts with reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2205.12548}, 2022.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{Devlin2019BERTPO}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{North American Chapter of the Association for Computational Linguistics}, 2019.

\bibitem[Dohmke(2022)]{githubcopilot}
Thomas Dohmke.
\newblock Github copilot is generally available to all developers.
\newblock \url{https://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/}, 2022.
\newblock Accessed: April-20-2024.

\bibitem[Du et~al.(2022)Du, Raheja, Kumar, Kim, Lopez, and Kang]{Du2022UnderstandingIR}
Wanyu Du, Vipul Raheja, Dhruv Kumar, Zae~Myung Kim, Melissa Lopez, and Dongyeop Kang.
\newblock Understanding iterative revision from human-written text.
\newblock \emph{ArXiv}, abs/2203.03802, 2022.

\bibitem[Faltings et~al.(2020)Faltings, Galley, Hintz, Brockett, Quirk, Gao, and Dolan]{Faltings2020TextEB}
Felix Faltings, Michel Galley, Gerold Hintz, Chris Brockett, Chris Quirk, Jianfeng Gao, and Bill Dolan.
\newblock Text editing by command.
\newblock \emph{ArXiv}, abs/2010.12826, 2020.

\bibitem[Fernandes et~al.(2023)Fernandes, Madaan, Liu, Farinhas, Martins, Bertsch, de~Souza, Zhou, Wu, Neubig, and Martins]{Fernandes2023BridgingTG}
Patrick Fernandes, Aman Madaan, Emmy Liu, Ant{\'o}nio Farinhas, Pedro~Henrique Martins, Amanda Bertsch, Jos{\'e} G.~C. de~Souza, Shuyan Zhou, Tongshuang~Sherry Wu, Graham Neubig, and Andr{\'e} F.~T. Martins.
\newblock Bridging the gap: A survey on integrating (human) feedback for natural language generation.
\newblock \emph{ArXiv}, abs/2305.00955, 2023.

\bibitem[Foundation(2022)]{wikidump}
Wikimedia Foundation.
\newblock Wikimedia downloads.
\newblock \url{https://dumps.wikimedia.org}, 2022.

\bibitem[Garivier et~al.(2016)Garivier, Lattimore, and Kaufmann]{garivier2016explore}
Aur{\'e}lien Garivier, Tor Lattimore, and Emilie Kaufmann.
\newblock On explore-then-commit strategies.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Guo et~al.(2024)Guo, Zhang, Liu, Liu, Khalman, Llinares, Rame, Mesnard, Zhao, Piot, et~al.]{guo2024direct}
Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et~al.
\newblock Direct language model alignment from online ai feedback.
\newblock \emph{arXiv preprint arXiv:2402.04792}, 2024.

\bibitem[Guu et~al.(2017)Guu, Hashimoto, Oren, and Liang]{Guu2017GeneratingSB}
Kelvin Guu, Tatsunori~B. Hashimoto, Yonatan Oren, and Percy Liang.
\newblock Generating sentences by editing prototypes.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 6:\penalty0 437--450, 2017.

\bibitem[Hancock et~al.(2019)Hancock, Bordes, Mazar{\'e}, and Weston]{Hancock2019LearningFD}
Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazar{\'e}, and Jason Weston.
\newblock Learning from dialogue after deployment: Feed yourself, chatbot!
\newblock \emph{Annual Meeting of the Association for Computational Linguistics}, 2019.

\bibitem[Hua et~al.(2019)Hua, Nikolov, Badugu, and Wang]{hua-etal-2019-argument}
Xinyu Hua, Mitko Nikolov, Nikhil Badugu, and Lu~Wang.
\newblock Argument mining for understanding peer reviews.
\newblock \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, June 2019.

\bibitem[Kershaw \& Koeling(2020)Kershaw and Koeling]{Kershaw2020ElsevierOC}
Daniel~James Kershaw and R.~Koeling.
\newblock Elsevier oa cc-by corpus.
\newblock \emph{ArXiv}, abs/2008.00774, 2020.
\newblock \doi{https://doi.org/10.48550/arXiv.2008.00774}.
\newblock URL \url{https://elsevier.digitalcommonsdata.com/datasets/zm33cdndxs}.

\bibitem[Kim et~al.(2022)Kim, Du, Raheja, Kumar, and Kang]{Kim2022ImprovingIT}
Zae~Myung Kim, Wanyu Du, Vipul Raheja, Dhruv Kumar, and Dongyeop Kang.
\newblock Improving iterative text revision by learning where to edit from other revision tasks.
\newblock \emph{ArXiv}, abs/2212.01350, 2022.

\bibitem[Laban et~al.(2023)Laban, Vig, Kryscinski, Joty, Xiong, and Wu]{Laban2023SWiPEAD}
Philippe Laban, Jesse Vig, Wojciech Kryscinski, Shafiq~R. Joty, Caiming Xiong, and Chien-Sheng Wu.
\newblock Swipe: A dataset for document-level simplification of wikipedia pages.
\newblock \emph{Annual Meeting of the Association for Computational Linguistics}, 2023.

\bibitem[Lee et~al.(2024)Lee, Gero, Chung, Shum, Raheja, Shen, Venugopalan, Wambsganss, Zhou, Alghamdi, August, Bhat, Choksi, Dutta, Guo, Hoque, Kim, Neshaei, Sergeyuk, Shibani, Shrivastava, Shroff, Stark, Sterman, Wang, Bosselut, Buschek, Chang, Chen, Kreminski, Park, Pea, Rho, Shen, and Siangliulue]{Lee2024ADS}
Mina Lee, Katy~Ilonka Gero, John Joon~Young Chung, Simon~Buckingham Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David Zhou, Emad~A. Alghamdi, Tal August, Avinash Bhat, Madiha~Zahrah Choksi, Senjuti Dutta, Jin~L.C. Guo, Md.~Naimul Hoque, Yewon Kim, Seyed~Parsa Neshaei, Agnia Sergeyuk, Antonette Shibani, Disha Shrivastava, Lila Shroff, Jessi Stark, S.~Sterman, Sitong Wang, Antoine Bosselut, Daniel Buschek, Joseph~Chee Chang, Sherol Chen, Max Kreminski, Joonsuk Park, Roy Pea, Eugenia~H. Rho, Shannon~Zejiang Shen, and Pao Siangliulue.
\newblock A design space for intelligent and interactive writing assistants.
\newblock \emph{Conference on Human Factors in Computing Systems}, abs/2403.14117, 2024.

\bibitem[Levenshtein(1965)]{Levenshtein1965BinaryCC}
Vladimir~I. Levenshtein.
\newblock Binary codes capable of correcting deletions, insertions, and reversals.
\newblock \emph{Soviet physics. Doklady}, 10:\penalty0 707--710, 1965.

\bibitem[Li et~al.(2023)Li, Zhang, Mei, Kong, and Bendersky]{li2023automatic}
Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, and Michael Bendersky.
\newblock Automatic prompt rewriting for personalized text generation.
\newblock \emph{arXiv preprint arXiv:2310.00152}, 2023.

\bibitem[Li et~al.(2016)Li, Miller, Chopra, Ranzato, and Weston]{Li2016DialogueLW}
Jiwei Li, Alexander~H. Miller, Sumit Chopra, Marc'Aurelio Ranzato, and Jason Weston.
\newblock Dialogue learning with human-in-the-loop.
\newblock \emph{ArXiv}, abs/1611.09823, 2016.

\bibitem[Li et~al.(2022)Li, Sharma, Lu, Cheung, and Reddy]{Li2022UsingIF}
Zichao Li, Prakhar Sharma, Xing~Han Lu, Jackie Chi~Kit Cheung, and Siva Reddy.
\newblock Using interactive feedback to improve the accuracy and explainability of question answering systems post-deployment.
\newblock \emph{ArXiv}, abs/2204.03025, 2022.

\bibitem[Liu et~al.(2023)Liu, Jia, Zhang, Zhuang, Liu, and Vosoughi]{Liu2023SecondTA}
Ruibo Liu, Chenyan Jia, Ge~Zhang, Ziyu Zhuang, Tony~X. Liu, and Soroush Vosoughi.
\newblock Second thoughts are best: Learning to re-align with human values from text edits.
\newblock \emph{ArXiv}, abs/2301.00355, 2023.

\bibitem[Liu et~al.(2022)Liu, Deb, Teruel, Halfaker, Radev, and Awadallah]{Liu2022OnIS}
Yixin Liu, Budhaditya Deb, Milagro Teruel, Aaron~L Halfaker, Dragomir~R. Radev, and Ahmed~Hassan Awadallah.
\newblock On improving summarization factual consistency from natural language feedback.
\newblock \emph{Annual Meeting of the Association for Computational Linguistics}, 2022.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and Potts]{maas-EtAl:2011:ACL-HLT2011}
Andrew~L. Maas, Raymond~E. Daly, Peter~T. Pham, Dan Huang, Andrew~Y. Ng, and Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock \emph{Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies}, June 2011.

\bibitem[Malaviya et~al.(2023)Malaviya, Lee, Roth, and Yatskar]{Malaviya2023PachinkoPI}
Chaitanya Malaviya, Subin Lee, Dan Roth, and Mark Yatskar.
\newblock Pachinko: Patching interpretable qa models through natural language feedback.
\newblock \emph{ArXiv}, abs/2311.09558, 2023.

\bibitem[Mallinson et~al.(2020)Mallinson, Severyn, Malmi, and Garrido]{Mallinson2020FELIXFT}
Jonathan Mallinson, Aliaksei Severyn, Eric Malmi, and Guillermo Garrido.
\newblock Felix: Flexible text editing through tagging and insertion.
\newblock \emph{ArXiv}, abs/2003.10687, 2020.

\bibitem[Marrese-Taylor et~al.(2020)Marrese-Taylor, Reid, and Matsuo]{MarreseTaylor2020VariationalIF}
Edison Marrese-Taylor, Machel Reid, and Yutaka Matsuo.
\newblock Variational inference for learning representations of natural language edits.
\newblock \emph{ArXiv}, abs/2004.09143, 2020.

\bibitem[Marrese-Taylor et~al.(2023)Marrese-Taylor, Reid, and Solano]{MarreseTaylor2023EditAR}
Edison Marrese-Taylor, Machel Reid, and Alfredo Solano.
\newblock Edit aware representation learning via levenshtein prediction.
\newblock \emph{The Fourth Workshop on Insights from Negative Results in NLP}, 2023.

\bibitem[Misra et~al.(2024)Misra, Pacchiano, and Schapire]{misra2024provable}
Dipendra Misra, Aldo Pacchiano, and Robert~E Schapire.
\newblock Provable interactive learning with hindsight instruction feedback.
\newblock \emph{arXiv preprint arXiv:2404.09123}, 2024.

\bibitem[Mita et~al.(2022)Mita, Sakaguchi, Hagiwara, Mizumoto, Suzuki, and Inui]{Mita2022TowardsAD}
Masato Mita, Keisuke Sakaguchi, Masato Hagiwara, Tomoya Mizumoto, Jun Suzuki, and Kentaro Inui.
\newblock Towards automated document revision: Grammatical error correction, fluency edits, and beyond.
\newblock \emph{ArXiv}, abs/2205.11484, 2022.

\bibitem[Mysore et~al.(2023)Mysore, Lu, Wan, Yang, Menezes, Baghaee, Gonzalez, Neville, and Safavi]{mysore2023pearl}
Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Steve Menezes, Tina Baghaee, Emmanuel~Barajas Gonzalez, Jennifer Neville, and Tara Safavi.
\newblock Pearl: Personalizing large language model writing assistants with generation-calibrated retrievers.
\newblock \emph{arXiv preprint arXiv:2311.09180}, 2023.

\bibitem[Nakano et~al.(2021)Nakano, Hilton, Balaji, Wu, Ouyang, Kim, Hesse, Jain, Kosaraju, Saunders, Jiang, Cobbe, Eloundou, Krueger, Button, Knight, Chess, and Schulman]{Nakano2021WebGPTBQ}
Reiichiro Nakano, Jacob Hilton, S.~Arun Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu~Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman.
\newblock Webgpt: Browser-assisted question-answering with human feedback.
\newblock \emph{ArXiv}, 2021.

\bibitem[Nguyen et~al.(2021)Nguyen, Misra, Schapire, Dud{\'\i}k, and Shafto]{nguyen2021interactive}
Khanh~X Nguyen, Dipendra Misra, Robert Schapire, Miroslav Dud{\'\i}k, and Patrick Shafto.
\newblock Interactive learning from activity description.
\newblock \emph{International Conference on Machine Learning}, pp.\  8096--8108, 2021.

\bibitem[Ouyang et~al.(2022{\natexlab{a}})Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe]{Ouyang2022TrainingLM}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke~E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul~Francis Christiano, Jan Leike, and Ryan~J. Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{ArXiv}, 2022{\natexlab{a}}.

\bibitem[Ouyang et~al.(2022{\natexlab{b}})Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 27730--27744, 2022{\natexlab{b}}.

\bibitem[Petrak et~al.(2023)Petrak, Moosavi, Tian, Rozanov, and Gurevych]{Petrak2023LearningFF}
Dominic Petrak, Nafise~Sadat Moosavi, Ye~Tian, Nikolai Rozanov, and Iryna Gurevych.
\newblock Learning from free-text human feedback - collect new datasets or extend existing ones?
\newblock \emph{ArXiv}, abs/2310.15758, 2023.

\bibitem[Rajagopal et~al.(2022)Rajagopal, Zhang, Gamon, Jauhar, Yang, and Hovy]{Rajagopal2022OneDM}
Dheeraj Rajagopal, Xuchao Zhang, Michael Gamon, Sujay~Kumar Jauhar, Diyi Yang, and Eduard~H. Hovy.
\newblock One document, many revisions: A dataset for classification and description of edit intents.
\newblock \emph{International Conference on Language Resources and Evaluation}, 2022.

\bibitem[Reid \& Neubig(2022)Reid and Neubig]{Reid2022LearningTM}
Machel Reid and Graham Neubig.
\newblock Learning to model editing processes.
\newblock \emph{Conference on Empirical Methods in Natural Language Processing}, 2022.

\bibitem[Reid et~al.(2023)Reid, Hellendoorn, and Neubig]{Reid2023DiffusERDV}
Machel Reid, Vincent~J. Hellendoorn, and Graham Neubig.
\newblock Diffuser: Diffusion via edit-based reconstruction.
\newblock \emph{International Conference on Learning Representations}, 2023.

\bibitem[Saunders et~al.(2022)Saunders, Yeh, Wu, Bills, Long, Ward, and Leike]{Saunders2022SelfcritiquingMF}
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Ouyang Long, Jonathan Ward, and Jan Leike.
\newblock Self-critiquing models for assisting human evaluators.
\newblock \emph{ArXiv}, abs/2206.05802, 2022.

\bibitem[Scheurer et~al.(2023)Scheurer, Campos, Korbak, Chan, Chen, Cho, and Perez]{Scheurer2023TrainingLM}
J'er'emy Scheurer, Jon~Ander Campos, Tomasz Korbak, Jun~Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez.
\newblock Training language models with language feedback at scale.
\newblock \emph{ArXiv}, abs/2303.16755, 2023.

\bibitem[See et~al.(2017)See, Liu, and Manning]{see-etal-2017-get}
Abigail See, Peter~J. Liu, and Christopher~D. Manning.
\newblock Get to the point: Summarization with pointer-generator networks.
\newblock \emph{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, July 2017.

\bibitem[Shen et~al.(2023)Shen, August, Siangliulue, Lo, Bragg, Hammerbacher, Downey, Chang, and Sontag]{shen2023beyond}
Zejiang Shen, Tal August, Pao Siangliulue, Kyle Lo, Jonathan Bragg, Jeff Hammerbacher, Doug Downey, Joseph~Chee Chang, and David Sontag.
\newblock Beyond summarization: Designing ai support for real-world expository writing tasks.
\newblock \emph{arXiv preprint arXiv:2304.02623}, 2023.

\bibitem[Shi et~al.(2022)Shi, Dinan, Shuster, Weston, and Xu]{Shi2022WhenLG}
Weiyan Shi, Emily Dinan, Kurt Shuster, Jason Weston, and Jing Xu.
\newblock When life gives you lemons, make cherryade: Converting feedback from bad responses into good labels.
\newblock \emph{ArXiv}, abs/2210.15893, 2022.

\bibitem[Song et~al.(2020)Song, Tan, Qin, Lu, and Liu]{Song2020MPNetMA}
Kaitao Song, Xu~Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.
\newblock Mpnet: Masked and permuted pre-training for language understanding.
\newblock \emph{ArXiv}, abs/2004.09297, 2020.

\bibitem[Stahlberg \& Kumar(2020)Stahlberg and Kumar]{Stahlberg2020Seq2EditsST}
Felix Stahlberg and Shankar Kumar.
\newblock Seq2edits: Sequence transduction using span-level edit operations.
\newblock \emph{Conference on Empirical Methods in Natural Language Processing}, 2020.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano]{Stiennon2020LearningTS}
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel~M. Ziegler, Ryan~J. Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano.
\newblock Learning to summarize from human feedback.
\newblock \emph{ArXiv}, abs/2009.01325, 2020.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Wang et~al.(2023)Wang, Chilton, and Nickerson]{wang2023writing}
Sitong Wang, Lydia~B Chilton, and Jeffrey~V Nickerson.
\newblock Writing with generative ai: Multi-modal and multi-dimensional tools for journalists.
\newblock \emph{The Second Workshop on Intelligent and Interactive Writing Assistants at ACM CHI}, 2023.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, hsin Chi, Xia, Le, and Zhou]{Wei2022ChainOT}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Huai hsin Chi, F.~Xia, Quoc Le, and Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language models.
\newblock \emph{ArXiv}, abs/2201.11903, 2022.

\bibitem[Weston(2016)]{Weston2016DialogbasedLL}
Jason Weston.
\newblock Dialog-based language learning.
\newblock \emph{ArXiv}, abs/1604.06045, 2016.

\bibitem[Xu et~al.(2022)Xu, Ung, Komeili, Arora, Boureau, and Weston]{Xu2022LearningNS}
Jing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora, Y-Lan Boureau, and Jason Weston.
\newblock Learning new skills after deployment: Improving open-domain internet-driven dialogue with human feedback.
\newblock \emph{Annual Meeting of the Association for Computational Linguistics}, 2022.

\bibitem[Yao et~al.(2021)Yao, Xu, Yin, Sun, and Neubig]{Yao2021LearningSE}
Ziyu Yao, Frank~F. Xu, Pengcheng Yin, Huan Sun, and Graham Neubig.
\newblock Learning structural edits via incremental tree transformations.
\newblock \emph{ArXiv}, abs/2101.12087, 2021.

\bibitem[Yin et~al.(2018)Yin, Neubig, Allamanis, Brockschmidt, and Gaunt]{Yin2018LearningTR}
Pengcheng Yin, Graham Neubig, Miltiadis Allamanis, Marc Brockschmidt, and Alexander~L. Gaunt.
\newblock Learning to represent edits.
\newblock \emph{ArXiv}, abs/1810.13337, 2018.

\bibitem[Zhang et~al.(2022)Zhang, Panthaplackel, Nie, Li, and Gligori{\'c}]{Zhang2022CoditT5PF}
Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi~Jessy Li, and Milo{\v s} Gligori{\'c}.
\newblock Coditt5: Pretraining for source code and natural language editing.
\newblock \emph{Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering}, 2022.

\bibitem[Zhang* et~al.(2020)Zhang*, Kishore*, Wu*, Weinberger, and Artzi]{bert-score}
Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian~Q. Weinberger, and Yoav Artzi.
\newblock Bertscore: Evaluating text generation with bert.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving]{Ziegler2019FineTuningLM}
Daniel~M. Ziegler, Nisan Stiennon, Jeff Wu, Tom~B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving.
\newblock Fine-tuning language models from human preferences.
\newblock \emph{ArXiv}, 2019.

\end{thebibliography}
