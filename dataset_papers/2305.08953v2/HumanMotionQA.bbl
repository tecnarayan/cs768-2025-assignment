\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Asghari-Esfeden et~al.(2020)Asghari-Esfeden, Sznaier, and
  Camps]{asghari2020dynamic}
Asghari-Esfeden, S., Sznaier, M., and Camps, O.
\newblock Dynamic motion representation for human action recognition.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision}, pp.\  557--566, 2020.

\bibitem[Athanasiou et~al.(2022)Athanasiou, Petrovich, Black, and Varol]{TEACH}
Athanasiou, N., Petrovich, M., Black, M.~J., and Varol, G.
\newblock Teach: Temporal action composition for 3d humans.
\newblock In \emph{Proceedings of the International Conference on 3D Vision},
  2022.

\bibitem[Avsec et~al.(2021)Avsec, Weilert, Shrikumar, Krueger, Alexandari,
  Dalal, Fropf, McAnany, Gagneur, Kundaje, et~al.]{BPNET}
Avsec, {\v{Z}}., Weilert, M., Shrikumar, A., Krueger, S., Alexandari, A.,
  Dalal, K., Fropf, R., McAnany, C., Gagneur, J., Kundaje, A., et~al.
\newblock Base-resolution models of transcription-factor binding reveal soft
  motif syntax.
\newblock \emph{Nature Genetics}, 53\penalty0 (3):\penalty0 354--366, 2021.

\bibitem[Caba~Heilbron et~al.(2015)Caba~Heilbron, Escorcia, Ghanem, and
  Carlos~Niebles]{ActivityNet}
Caba~Heilbron, F., Escorcia, V., Ghanem, B., and Carlos~Niebles, J.
\newblock Activitynet: A large-scale video benchmark for human activity
  understanding.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  961--970, 2015.

\bibitem[Caetano et~al.(2019)Caetano, Sena, Br{\'e}mond, Dos~Santos, and
  Schwartz]{caetano2019skelemotion}
Caetano, C., Sena, J., Br{\'e}mond, F., Dos~Santos, J.~A., and Schwartz, W.~R.
\newblock Skelemotion: A new representation of skeleton joint sequences based
  on motion information for 3d action recognition.
\newblock In \emph{Proceedings of the IEEE International Conference on Advanced
  Video and Signal Based Surveillance}, pp.\  1--8, 2019.

\bibitem[Cai et~al.(2021)Cai, Jiang, Han, Jia, and Lu]{cai2021jolo}
Cai, J., Jiang, N., Han, X., Jia, K., and Lu, J.
\newblock Jolo-gcn: mining joint-centered light-weight information for
  skeleton-based action recognition.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision}, pp.\  2735--2744, 2021.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Zhang, Yuan, Li, Deng, and
  Hu]{chen2021channel}
Chen, Y., Zhang, Z., Yuan, C., Li, B., Deng, Y., and Hu, W.
\newblock Channel-wise topology refinement graph convolution for skeleton-based
  action recognition.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  13359--13368, 2021{\natexlab{a}}.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Mao, Wu, Wong, Tenenbaum, and
  Gan]{chen2021grounding}
Chen, Z., Mao, J., Wu, J., Wong, K.-Y.~K., Tenenbaum, J.~B., and Gan, C.
\newblock Grounding physical concepts of objects and events through dynamic
  visual reasoning.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2021{\natexlab{b}}.

\bibitem[Cheng et~al.(2020)Cheng, Zhang, He, Chen, Cheng, and
  Lu]{cheng2020skeleton}
Cheng, K., Zhang, Y., He, X., Chen, W., Cheng, J., and Lu, H.
\newblock Skeleton-based action recognition with shift graph convolutional
  network.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  183--192, 2020.

\bibitem[Chereshnev \& Kert{\'e}sz-Farkas(2018)Chereshnev and
  Kert{\'e}sz-Farkas]{chereshnev2018hugadb}
Chereshnev, R. and Kert{\'e}sz-Farkas, A.
\newblock Hugadb: Human gait database for activity recognition from wearable
  inertial sensor networks.
\newblock In \emph{Proceedings of the International Conference on Analysis of
  Images, Social Networks and Texts}, pp.\  131--141. Springer, 2018.

\bibitem[Choutas et~al.(2018)Choutas, Weinzaepfel, Revaud, and
  Schmid]{choutas2018potion}
Choutas, V., Weinzaepfel, P., Revaud, J., and Schmid, C.
\newblock Potion: Pose motion representation for action recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  7024--7033, 2018.

\bibitem[Du et~al.(2015)Du, Wang, and Wang]{du2015hierarchical}
Du, Y., Wang, W., and Wang, L.
\newblock Hierarchical recurrent neural network for skeleton based action
  recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  1110--1118, 2015.

\bibitem[Duan et~al.(2022)Duan, Zhao, Chen, Lin, and Dai]{duan2022revisiting}
Duan, H., Zhao, Y., Chen, K., Lin, D., and Dai, B.
\newblock Revisiting skeleton-based action recognition.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  2969--2978, 2022.

\bibitem[Filtjens et~al.(2022)Filtjens, Vanrumste, and
  Slaets]{filtjens2022skeleton}
Filtjens, B., Vanrumste, B., and Slaets, P.
\newblock Skeleton-based action segmentation with multi-stage spatial-temporal
  graph convolutional neural networks.
\newblock \emph{IEEE Transactions on Emerging Topics in Computing}, 2022.

\bibitem[Guo et~al.(2022)Guo, Zou, Zuo, Wang, Ji, Li, and
  Cheng]{guo2022generating}
Guo, C., Zou, S., Zuo, X., Wang, S., Ji, W., Li, X., and Cheng, L.
\newblock Generating diverse and natural 3d human motions from text.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  5152--5161, 2022.

\bibitem[Hong et~al.(2022)Hong, Du, Lin, Tenenbaum, and Gan]{hong20223d}
Hong, Y., Du, Y., Lin, C., Tenenbaum, J., and Gan, C.
\newblock 3d concept grounding on neural fields.
\newblock In \emph{Proceedings of Advances in Neural Information Processing
  Systems}, 2022.

\bibitem[Hsu et~al.(2023)Hsu, Mao, and Wu]{hsu2023ns3d}
Hsu, J., Mao, J., and Wu, J.
\newblock Ns3d: Neuro-symbolic grounding of 3d objects and relations.
\newblock \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2023.

\bibitem[Jiang et~al.(2022)Jiang, Ye, Gopinath, Won, Winkler, and
  Liu]{jiang2022transformer}
Jiang, Y., Ye, Y., Gopinath, D., Won, J., Winkler, A.~W., and Liu, C.~K.
\newblock Transformer inertial poser: real-time human motion reconstruction
  from sparse imus with simultaneous terrain generation.
\newblock In \emph{SIGGRAPH Asia 2022 Conference Papers}, pp.\  1--9, 2022.

\bibitem[Johnson et~al.(2017)Johnson, Hariharan, Van Der~Maaten, Fei-Fei,
  Lawrence~Zitnick, and Girshick]{johnson2017clevr}
Johnson, J., Hariharan, B., Van Der~Maaten, L., Fei-Fei, L., Lawrence~Zitnick,
  C., and Girshick, R.
\newblock Clevr: A diagnostic dataset for compositional language and elementary
  visual reasoning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  2901--2910, 2017.

\bibitem[Ke et~al.(2017)Ke, Bennamoun, An, Sohel, and Boussaid]{ke2017new}
Ke, Q., Bennamoun, M., An, S., Sohel, F., and Boussaid, F.
\newblock A new representation of skeleton sequences for 3d action recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  3288--3297, 2017.

\bibitem[Kim et~al.(2022)Kim, Kim, and Choi]{kim2022flame}
Kim, J., Kim, J., and Choi, S.
\newblock Flame: Free-form language-based motion synthesis \& editing.
\newblock \emph{arXiv preprint arXiv:2209.00349}, 2022.

\bibitem[Liu et~al.(2017)Liu, Hu, Li, Song, and Liu]{liu2017pku}
Liu, C., Hu, Y., Li, Y., Song, S., and Liu, J.
\newblock Pku-mmd: A large scale benchmark for skeleton-based human action
  understanding.
\newblock In \emph{Proceedings of the Workshop on Visual Analysis in Smart and
  Connected Communities}, pp.\  1--8, 2017.

\bibitem[Liu et~al.(2020)Liu, Zhang, Chen, Wang, and
  Ouyang]{liu2020disentangling}
Liu, Z., Zhang, H., Chen, Z., Wang, Z., and Ouyang, W.
\newblock Disentangling and unifying graph convolutions for skeleton-based
  action recognition.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  143--152, 2020.

\bibitem[Luo et~al.(2021)Luo, Hachiuma, Yuan, and Kitani]{luo2021dynamics}
Luo, Z., Hachiuma, R., Yuan, Y., and Kitani, K.
\newblock Dynamics-regulated kinematic policy for egocentric pose estimation.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 25019--25032, 2021.

\bibitem[Mahmood et~al.(2019)Mahmood, Ghorbani, Troje, Pons-Moll, and
  Black]{AMASS}
Mahmood, N., Ghorbani, N., Troje, N.~F., Pons-Moll, G., and Black, M.~J.
\newblock Amass: Archive of motion capture as surface shapes.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  5442--5451, 2019.

\bibitem[Mao et~al.(2019)Mao, Gan, Kohli, Tenenbaum, and Wu]{NSCL}
Mao, J., Gan, C., Kohli, P., Tenenbaum, J.~B., and Wu, J.
\newblock The neuro-symbolic concept learner: Interpreting scenes, words, and
  sentences from natural supervision.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2019.

\bibitem[Mart{\'\i}nez-Gonz{\'a}lez et~al.(2021)Mart{\'\i}nez-Gonz{\'a}lez,
  Villamizar, and Odobez]{POTR}
Mart{\'\i}nez-Gonz{\'a}lez, A., Villamizar, M., and Odobez, J.-M.
\newblock Pose transformers (potr): Human motion prediction with
  non-autoregressive transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  2276--2284, 2021.

\bibitem[Niemann et~al.(2020)Niemann, Reining, Moya~Rueda, Nair, Steffens,
  Fink, and Ten~Hompel]{niemann2020lara}
Niemann, F., Reining, C., Moya~Rueda, F., Nair, N.~R., Steffens, J.~A., Fink,
  G.~A., and Ten~Hompel, M.
\newblock Lara: Creating a dataset for human activity recognition in logistics
  using semantic attributes.
\newblock \emph{Sensors}, 20\penalty0 (15):\penalty0 4083, 2020.

\bibitem[Petrovich et~al.(2022)Petrovich, Black, and Varol]{petrovich2022temos}
Petrovich, M., Black, M.~J., and Varol, G.
\newblock Temos: Generating diverse human motions from textual descriptions.
\newblock In \emph{Proceedings of the European Conference on Computer Vision},
  pp.\  480--497. Springer, 2022.

\bibitem[Punnakkal et~al.(2021)Punnakkal, Chandrasekaran, Athanasiou,
  Quiros-Ramirez, and Black]{BABEL}
Punnakkal, A.~R., Chandrasekaran, A., Athanasiou, N., Quiros-Ramirez, A., and
  Black, M.~J.
\newblock Babel: bodies, action and behavior with english labels.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  722--731, 2021.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{CLIP}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, pp.\  8748--8763, 2021.

\bibitem[Sedmidubsky et~al.(2019)Sedmidubsky, Elias, and
  Zezula]{temporal-localization}
Sedmidubsky, J., Elias, P., and Zezula, P.
\newblock Benchmarking search and annotation in continuous human skeleton
  sequences.
\newblock In \emph{Proceedings of the International Conference on Multimedia
  Retrieval}, pp.\  38--42, 2019.

\bibitem[Shahroudy et~al.(2016)Shahroudy, Liu, Ng, and Wang]{NTU}
Shahroudy, A., Liu, J., Ng, T.-T., and Wang, G.
\newblock Ntu rgb+d: A large scale dataset for 3d human activity analysis.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  1010--1019, 2016.

\bibitem[Shi et~al.(2019)Shi, Zhang, Cheng, and Lu]{AGCN}
Shi, L., Zhang, Y., Cheng, J., and Lu, H.
\newblock Two-stream adaptive graph convolutional networks for skeleton-based
  action recognition.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  12026--12035, 2019.

\bibitem[Shi et~al.(2020)Shi, Zhang, Cheng, and Lu]{shi2020decoupled}
Shi, L., Zhang, Y., Cheng, J., and Lu, H.
\newblock Decoupled spatial-temporal attention network for skeleton-based
  action-gesture recognition.
\newblock In \emph{Proceedings of the Asian Conference on Computer Vision},
  2020.

\bibitem[Sun et~al.(2022)Sun, Zhou, Black, and Chandrasekaran]{sun2022locate}
Sun, J., Zhou, B., Black, M.~J., and Chandrasekaran, A.
\newblock Locate: End-to-end localization of actions in 3d with transformers.
\newblock \emph{arXiv preprint arXiv:2203.10719}, 2022.

\bibitem[Tevet et~al.(2022)Tevet, Gordon, Hertz, Bermano, and
  Cohen-Or]{MotionCLIP}
Tevet, G., Gordon, B., Hertz, A., Bermano, A.~H., and Cohen-Or, D.
\newblock Motionclip: Exposing human motion generation to clip space.
\newblock In \emph{Proceedings of the European Conference on Computer Vision},
  pp.\  358--374. Springer, 2022.

\bibitem[Xu et~al.(2022)Xu, Lan, Zeng, and Lu]{xu2022skeleton}
Xu, L., Lan, C., Zeng, W., and Lu, C.
\newblock Skeleton-based mutually assisted interacted object localization and
  human action recognition.
\newblock \emph{IEEE Transactions on Multimedia}, 2022.

\bibitem[Yan et~al.(2018)Yan, Xiong, and Lin]{yan2018spatial}
Yan, S., Xiong, Y., and Lin, D.
\newblock Spatial temporal graph convolutional networks for skeleton-based
  action recognition.
\newblock In \emph{Proceedings of the AAAI conference on Artificial
  Intelligence}, 2018.

\bibitem[Yao et~al.(2018)Yao, Lin, Shi, and Ranasinghe]{yao2018efficient}
Yao, R., Lin, G., Shi, Q., and Ranasinghe, D.~C.
\newblock Efficient dense labelling of human activity sequences from wearables
  using fully convolutional networks.
\newblock \emph{Pattern Recognition}, 78:\penalty0 252--266, 2018.

\bibitem[Yi et~al.(2018)Yi, Wu, Gan, Torralba, Kohli, and
  Tenenbaum]{yi2018neural}
Yi, K., Wu, J., Gan, C., Torralba, A., Kohli, P., and Tenenbaum, J.
\newblock Neural-symbolic vqa: Disentangling reasoning from vision and language
  understanding.
\newblock In \emph{Proceedings of Advances in Neural Information Processing
  Systems}, 2018.

\bibitem[Zhang et~al.(2022)Zhang, Cai, Pan, Hong, Guo, Yang, and
  Liu]{zhang2022motiondiffuse}
Zhang, M., Cai, Z., Pan, L., Hong, F., Guo, X., Yang, L., and Liu, Z.
\newblock Motiondiffuse: Text-driven human motion generation with diffusion
  model.
\newblock \emph{arXiv preprint arXiv:2208.15001}, 2022.

\end{thebibliography}
