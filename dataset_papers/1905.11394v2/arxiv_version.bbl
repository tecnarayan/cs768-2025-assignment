\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu(2017)]{allen2017katyusha}
Zeyuan Allen-Zhu.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock In \emph{Proceedings of Symposium on Theory of Computing}, pages
  1200--1205, 2017.

\bibitem[Baccelli et~al.(1992)Baccelli, Cohen, Olsder, and
  Quadrat]{baccelli1992synchronization}
Fran{\c{c}}ois Baccelli, Guy Cohen, Geert~Jan Olsder, and Jean-Pierre Quadrat.
\newblock \emph{Synchronization and Linearity: an Algebra for Discrete Event
  Systems}.
\newblock John Wiley \& Sons Ltd, 1992.

\bibitem[Bottou(2010)]{bottou2010large}
L{\'e}on Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In \emph{Proceedings of COMPSTAT}, pages 177--186. Springer, 2010.

\bibitem[Boyd et~al.(2006)Boyd, Ghosh, Prabhakar, and Shah]{boyd2006randomized}
Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah.
\newblock Randomized gossip algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 52\penalty0
  (6):\penalty0 2508--2530, 2006.

\bibitem[Bubeck(2015)]{bubeck2015convex}
S{\'e}bastien Bubeck.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  8\penalty0 (3-4):\penalty0 231--357, 2015.

\bibitem[Chen et~al.(2016)Chen, Pan, Monga, Bengio, and
  Jozefowicz]{chen2016revisiting}
Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and Rafal Jozefowicz.
\newblock Revisiting distributed synchronous {SGD}.
\newblock \emph{arXiv preprint arXiv:1604.00981}, 2016.

\bibitem[Colin et~al.(2016)Colin, Bellet, Salmon, and
  Cl{\'e}men{\c{c}}on]{colin2016gossip}
Igor Colin, Aur{\'e}lien Bellet, Joseph Salmon, and St{\'e}phan
  Cl{\'e}men{\c{c}}on.
\newblock Gossip dual averaging for decentralized optimization of pairwise
  functions.
\newblock In \emph{Proceedings of the International Conference on International
  Conference on Machine Learning-Volume 48}, pages 1388--1396, 2016.

\bibitem[Defazio(2016)]{defazio2016simple}
Aaron Defazio.
\newblock A simple practical accelerated method for finite sums.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  676--684, 2016.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1646--1654, 2014.

\bibitem[Duchi et~al.(2012)Duchi, Agarwal, and Wainwright]{duchi2012dual}
John~C Duchi, Alekh Agarwal, and Martin~J Wainwright.
\newblock Dual averaging for distributed optimization: Convergence analysis and
  network scaling.
\newblock \emph{IEEE Transactions on Automatic Control}, 57\penalty0
  (3):\penalty0 592--606, 2012.

\bibitem[Fercoq and Richt{\'a}rik(2015)]{fercoq2015accelerated}
Olivier Fercoq and Peter Richt{\'a}rik.
\newblock Accelerated, parallel, and proximal coordinate descent.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (4):\penalty0
  1997--2023, 2015.

\bibitem[He et~al.(2018)He, Bian, and Jaggi]{he2018cola}
Lie He, An~Bian, and Martin Jaggi.
\newblock Cola: Decentralized linear learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4536--4546, 2018.

\bibitem[Hendrikx et~al.(2019)Hendrikx, Bach, and
  Massouli{\'e}]{hendrikx2018accelerated}
Hadrien Hendrikx, Francis Bach, and Laurent Massouli{\'e}.
\newblock Accelerated decentralized optimization with local updates for smooth
  and strongly convex objectives.
\newblock In \emph{Artificial Intelligence and Statistics}, 2019.

\bibitem[Johansson et~al.(2009)Johansson, Rabi, and
  Johansson]{johansson2009randomized}
Bj{\"o}rn Johansson, Maben Rabi, and Mikael Johansson.
\newblock A randomized incremental subgradient method for distributed
  optimization in networked systems.
\newblock \emph{SIAM Journal on Optimization}, 20\penalty0 (3):\penalty0
  1157--1170, 2009.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  315--323, 2013.

\bibitem[Koloskova et~al.(2019)Koloskova, Stich, and
  Jaggi]{koloskova2019decentralized}
Anastasia Koloskova, Sebastian~U Stich, and Martin Jaggi.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock \emph{International Conference on Machine Learning}, 2019.

\bibitem[Leblond et~al.(2017)Leblond, Pedregosa, and
  Lacoste-Julien]{leblond2016asaga}
R{\'e}mi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien.
\newblock {ASAGA}: Asynchronous parallel {SAGA}.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 46--54, 2017.

\bibitem[Lee and Sidford(2013)]{lee2013efficient}
Yin~Tat Lee and Aaron Sidford.
\newblock Efficient accelerated coordinate descent methods and faster
  algorithms for solving linear systems.
\newblock In \emph{Annual Symposium on Foundations of Computer Science (FOCS)},
  pages 147--156, 2013.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
Xiangru Lian, Ce~Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5330--5340, 2017.

\bibitem[Lin et~al.(2014)Lin, Lu, and Xiao]{lin2014accelerated}
Qihang Lin, Zhaosong Lu, and Lin Xiao.
\newblock An accelerated proximal coordinate gradient method.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3059--3067, 2014.

\bibitem[Lin et~al.(2015)Lin, Lu, and Xiao]{lin2015accelerated}
Qihang Lin, Zhaosong Lu, and Lin Xiao.
\newblock An accelerated randomized proximal coordinate gradient method and its
  application to regularized empirical risk minimization.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (4):\penalty0
  2244--2273, 2015.

\bibitem[Lin et~al.(2018)Lin, Stich, and Jaggi]{lin2018don}
Tao Lin, Sebastian~U. Stich, and Martin Jaggi.
\newblock Don't use large mini-batches, use local {SGD}.
\newblock \emph{arXiv preprint arXiv:1808.07217}, 2018.

\bibitem[Mokhtari and Ribeiro(2016)]{mokhtari2016dsa}
Aryan Mokhtari and Alejandro Ribeiro.
\newblock {DSA}: Decentralized double stochastic averaging gradient algorithm.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2165--2199, 2016.

\bibitem[Nedic and Ozdaglar(2009)]{nedic2009distributed}
Angelia Nedic and Asuman Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 54\penalty0
  (1):\penalty0 48--61, 2009.

\bibitem[Nedic et~al.(2017)Nedic, Olshevsky, and Shi]{nedic2017achieving}
Angelia Nedic, Alex Olshevsky, and Wei Shi.
\newblock Achieving geometric convergence for distributed optimization over
  time-varying graphs.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0
  2597--2633, 2017.

\bibitem[Nesterov(2013)]{nesterov2013introductory}
Yurii Nesterov.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Bourse},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Nesterov and Stich(2017)]{nesterov2017efficiency}
Yurii Nesterov and Sebastian~U. Stich.
\newblock Efficiency of the accelerated coordinate descent method on structured
  optimization problems.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (1):\penalty0
  110--123, 2017.

\bibitem[Parikh and Boyd(2014)]{parikh2014proximal}
Neal Parikh and Stephen Boyd.
\newblock Proximal algorithms.
\newblock \emph{Foundations and Trends{\textregistered} in Optimization},
  1\penalty0 (3):\penalty0 127--239, 2014.

\bibitem[Patel and Dieuleveut(2019)]{patel2019communication}
Kumar~Kshitij Patel and Aymeric Dieuleveut.
\newblock Communication trade-offs for synchronized distributed {SGD} with
  large step size.
\newblock \emph{arXiv preprint arXiv:1904.11325}, 2019.

\bibitem[Recht et~al.(2011)Recht, Re, Wright, and Niu]{recht2011hogwild}
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.
\newblock Hogwild: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  693--701, 2011.

\bibitem[Scaman et~al.(2017)Scaman, Bach, Bubeck, Lee, and
  Massouli{\'e}]{scaman2017optimal}
Kevin Scaman, Francis Bach, S{\'e}bastien Bubeck, Yin~Tat Lee, and Laurent
  Massouli{\'e}.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  3027--3036, 2017.

\bibitem[Schmidt et~al.(2017)Schmidt, Le~Roux, and Bach]{schmidt2017minimizing}
Mark Schmidt, Nicolas Le~Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, 162\penalty0 (1-2):\penalty0
  83--112, 2017.

\bibitem[Shalev-Shwartz and Zhang(2013)]{shalev2013stochastic}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock \emph{Journal of Machine Learning Research}, 14\penalty0
  (Feb):\penalty0 567--599, 2013.

\bibitem[Shalev-Shwartz and Zhang(2014)]{shalev2014accelerated}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Accelerated proximal stochastic dual coordinate ascent for
  regularized loss minimization.
\newblock In \emph{International Conference on Machine Learning}, pages 64--72,
  2014.

\bibitem[Shen et~al.(2018)Shen, Mokhtari, Zhou, Zhao, and
  Qian]{shen2018towards}
Zebang Shen, Aryan Mokhtari, Tengfei Zhou, Peilin Zhao, and Hui Qian.
\newblock Towards more efficient stochastic decentralized learning: Faster
  convergence and sparse communication.
\newblock In \emph{International Conference on Machine Learning}, pages
  4631--4640, 2018.

\bibitem[Shi et~al.(2015)Shi, Ling, Wu, and Yin]{shi2015extra}
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin.
\newblock Extra: An exact first-order algorithm for decentralized consensus
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0
  944--966, 2015.

\bibitem[Tang et~al.(2018)Tang, Lian, Yan, Zhang, and Liu]{tang2018d}
Hanlin Tang, Xiangru Lian, Ming Yan, Ce~Zhang, and Ji~Liu.
\newblock ${D}^2$: Decentralized training over decentralized data.
\newblock In \emph{International Conference on Machine Learning}, pages
  4855--4863, 2018.

\bibitem[Xiao et~al.(2019)Xiao, Yu, Lin, and Chen]{xiao2017dscovr}
Lin Xiao, Adams~Wei Yu, Qihang Lin, and Weizhu Chen.
\newblock {DSCOVR}: Randomized primal-dual block coordinate algorithms for
  asynchronous distributed optimization.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (43):\penalty0 1--58, 2019.

\bibitem[Zeng et~al.(2018)Zeng, Chaintreau, Towsley, and
  Xia]{zeng2018throughput}
Yun Zeng, Augustin Chaintreau, Don Towsley, and Cathy~H Xia.
\newblock Throughput scalability analysis of fork-join queueing networks.
\newblock \emph{Operations Research}, 66\penalty0 (6):\penalty0 1728--1743,
  2018.

\end{thebibliography}
