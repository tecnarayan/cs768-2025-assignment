\begin{thebibliography}{}

\bibitem[Abernethy et~al., 2011]{abernethy2011blackwell}
Abernethy, J., Bartlett, P.~L., and Hazan, E. (2011).
\newblock Blackwell approachability and no-regret learning are equivalent.
\newblock In {\em Proceedings of the 24th Annual Conference on Learning
  Theory}, pages 27--46. JMLR Workshop and Conference Proceedings.

\bibitem[Altman, 1999]{altman1999constrained}
Altman, E. (1999).
\newblock {\em Constrained Markov decision processes}, volume~7.
\newblock CRC Press.

\bibitem[Azar et~al., 2017]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R. (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  263--272. PMLR.

\bibitem[Babaioff et~al., 2015]{DynPricing-ec12}
Babaioff, M., Dughmi, S., Kleinberg, R.~D., and Slivkins, A. (2015).
\newblock Dynamic pricing with limited supply.
\newblock {\em TEAC}, 3(1):4.
\newblock Special issue for \emph{13th ACM EC}, 2012.

\bibitem[Besbes and Zeevi, 2009]{BZ09}
Besbes, O. and Zeevi, A. (2009).
\newblock Dynamic pricing without knowing the demand function: Risk bounds and
  near-optimal algorithms.
\newblock {\em Operations Research}, 57(6):1407--1420.

\bibitem[Blackwell et~al., 1956]{blackwell1956analog}
Blackwell, D. et~al. (1956).
\newblock An analog of the minimax theorem for vector payoffs.
\newblock {\em Pacific Journal of Mathematics}, 6(1):1--8.

\bibitem[Brantley et~al., 2020]{miryoosefi2020}
Brantley, K., Dudik, M., Lykouris, T., Miryoosefi, S., Simchowitz, M.,
  Slivkins, A., and Sun, W. (2020).
\newblock Constrained episodic reinforcement learning in concave-convex and
  knapsack settings.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 16315--16326. Curran Associates, Inc.

\bibitem[Ding et~al., 2021]{ding2021provably}
Ding, D., Wei, X., Yang, Z., Wang, Z., and Jovanovic, M. (2021).
\newblock Provably efficient safe exploration via primal-dual policy
  optimization.
\newblock In Banerjee, A. and Fukumizu, K., editors, {\em Proceedings of The
  24th International Conference on Artificial Intelligence and Statistics},
  volume 130 of {\em Proceedings of Machine Learning Research}, pages
  3304--3312. PMLR.

\bibitem[Efroni et~al., 2020]{efroni2020exploration}
Efroni, Y., Mannor, S., and Pirotta, M. (2020).
\newblock Exploration-exploitation in constrained mdps.
\newblock {\em arXiv preprint arXiv:2003.02189}.

\bibitem[Filar and Vrieze, 2012]{filar2012competitive}
Filar, J. and Vrieze, K. (2012).
\newblock {\em Competitive Markov decision processes}.
\newblock Springer Science \& Business Media.

\bibitem[Freund and Schapire, 1999]{freund1999adaptive}
Freund, Y. and Schapire, R.~E. (1999).
\newblock Adaptive game playing using multiplicative weights.
\newblock {\em Games and Economic Behavior}, 29(1-2):79--103.

\bibitem[Jabbari et~al., 2017]{jabbari2017fairness}
Jabbari, S., Joseph, M., Kearns, M., Morgenstern, J., and Roth, A. (2017).
\newblock Fairness in reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  1617--1626. PMLR.

\bibitem[Jin et~al., 2020a]{jin2020reward}
Jin, C., Krishnamurthy, A., Simchowitz, M., and Yu, T. (2020a).
\newblock Reward-free exploration for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  4870--4879. PMLR.

\bibitem[Jin et~al., 2019]{jin2019short}
Jin, C., Netrapalli, P., Ge, R., Kakade, S.~M., and Jordan, M.~I. (2019).
\newblock A short note on concentration inequalities for random vectors with
  subgaussian norm.
\newblock {\em arXiv preprint arXiv:1902.03736}.

\bibitem[Jin et~al., 2020b]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I. (2020b).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In {\em Conference on Learning Theory}, pages 2137--2143. PMLR.

\bibitem[Le et~al., 2019]{Hoang2019BPLUC}
Le, H.~M., Voloshin, C., and Yue, Y. (2019).
\newblock Batch policy learning under constraints.
\newblock {\em CoRR}, abs/1903.08738.

\bibitem[Liu et~al., 2020]{liu2020sharp}
Liu, Q., Yu, T., Bai, Y., and Jin, C. (2020).
\newblock A sharp analysis of model-based reinforcement learning with
  self-play.
\newblock {\em arXiv preprint arXiv:2010.01604}.

\bibitem[Mao et~al., 2016]{Mao2016RLSystems}
Mao, H., Alizadeh, M., Menache, I., and Kandula, S. (2016).
\newblock Resource management with deep reinforcement learning.
\newblock In {\em Proceedings of the 15th ACM Workshop on Hot Topics in
  Networks}, page 50–56, New York, NY, USA. Association for Computing
  Machinery.

\bibitem[Miryoosefi et~al., 2019]{miryoosefi2019}
Miryoosefi, S., Brantley, K., Daume~III, H., Dudik, M., and Schapire, R.~E.
  (2019).
\newblock Reinforcement learning with convex constraints.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32, pages 14093--14102. Curran Associates, Inc.

\bibitem[Neumann, 1928]{neumann1928theorie}
Neumann, J.~v. (1928).
\newblock Zur theorie der gesellschaftsspiele.
\newblock {\em Mathematische annalen}, 100(1):295--320.

\bibitem[Puterman, 2014]{puterman2014markov}
Puterman, M.~L. (2014).
\newblock {\em Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons.

\bibitem[Qiu et~al., 2020]{qiu2020upper}
Qiu, S., Wei, X., Yang, Z., Ye, J., and Wang, Z. (2020).
\newblock Upper confidence primal-dual optimization: Stochastically constrained
  markov decision processes with adversarial losses and unknown transitions.
\newblock {\em arXiv preprint arXiv:2003.00660}.

\bibitem[Rockafellar, 2015]{rockafellar2015convex}
Rockafellar, R.~T. (2015).
\newblock {\em Convex analysis}.
\newblock Princeton university press.

\bibitem[Singh et~al., 2020]{singh2020learning}
Singh, R., Gupta, A., and Shroff, N.~B. (2020).
\newblock Learning in markov decision processes under constraints.
\newblock {\em arXiv preprint arXiv:2002.12435}.

\bibitem[Sun et~al., 2019]{sun2019provably}
Sun, W., Vemula, A., Boots, B., and Bagnell, J.~A. (2019).
\newblock Provably efficient imitation learning from observation alone.
\newblock {\em arXiv preprint arXiv:1905.10948}.

\bibitem[Syed and Schapire, 2007]{SyedSchapire2008}
Syed, U. and Schapire, R.~E. (2007).
\newblock A game-theoretic approach to apprenticeship learning.
\newblock In {\em Proceedings of the 20th International Conference on Neural
  Information Processing Systems}, NIPS’07, page 1449–1456, Red Hook, NY,
  USA. Curran Associates Inc.

\bibitem[Tessler et~al., 2019]{tessler2018reward}
Tessler, C., Mankowitz, D.~J., and Mannor, S. (2019).
\newblock Reward constrained policy optimization.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Vinyals et~al., 2019]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al. (2019).
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock {\em Nature}, 575(7782):350--354.

\bibitem[Wang et~al., 2020a]{wang2020reward}
Wang, R., Du, S.~S., Yang, L.~F., and Salakhutdinov, R. (2020a).
\newblock On reward-free reinforcement learning with linear function
  approximation.
\newblock {\em arXiv preprint arXiv:2006.11274}.

\bibitem[Wang et~al., 2020b]{wang2020provably}
Wang, R., Salakhutdinov, R., and Yang, L.~F. (2020b).
\newblock Provably efficient reinforcement learning with general value function
  approximation.
\newblock {\em arXiv preprint arXiv:2005.10804}.

\bibitem[Wu et~al., 2020]{wu2020accommodating}
Wu, J., Braverman, V., and Yang, L.~F. (2020).
\newblock Accommodating picky customers: Regret bound and exploration
  complexity for multi-objective reinforcement learning.
\newblock {\em arXiv preprint arXiv:2011.13034}.

\bibitem[Yu et~al., 2021]{yu2021provably}
Yu, T., Tian, Y., Zhang, J., and Sra, S. (2021).
\newblock Provably efficient algorithms for multi-objective competitive rl.
\newblock {\em arXiv preprint arXiv:2102.03192}.

\bibitem[Zanette et~al., 2020]{zanette2020provably}
Zanette, A., Lazaric, A., Kochenderfer, M.~J., and Brunskill, E. (2020).
\newblock Provably efficient reward-agnostic navigation with linear value
  iteration.
\newblock {\em arXiv preprint arXiv:2008.07737}.

\bibitem[Zhang et~al., 2020]{zhang2020task}
Zhang, X., Singla, A., et~al. (2020).
\newblock Task-agnostic exploration in reinforcement learning.
\newblock {\em arXiv preprint arXiv:2006.09497}.

\bibitem[Ziebart et~al., 2008]{ziebart2008maximum}
Ziebart, B.~D., Maas, A.~L., Bagnell, J.~A., and Dey, A.~K. (2008).
\newblock Maximum entropy inverse reinforcement learning.
\newblock In {\em Aaai}, volume~8, pages 1433--1438. Chicago, IL, USA.

\bibitem[Zinkevich, 2003]{zinkevich2003online}
Zinkevich, M. (2003).
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In {\em Proceedings of the 20th international conference on machine
  learning (icml-03)}, pages 928--936.

\end{thebibliography}
