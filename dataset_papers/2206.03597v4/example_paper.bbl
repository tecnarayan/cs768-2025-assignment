\begin{thebibliography}{68}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen et~al.(2021)Allen, Parikh, Gottesman, and
  Konidaris]{DBLP:conf/nips/AllenPGK21}
Allen, C., Parikh, N., Gottesman, O., and Konidaris, G.
\newblock Learning markov state abstractions for deep reinforcement learning.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y.~N., Liang, P., and
  Vaughan, J.~W. (eds.), \emph{Advances in Neural Information Processing
  Systems 34: Annual Conference on Neural Information Processing Systems 2021,
  NeurIPS 2021, December 6-14, 2021, virtual}, pp.\  8229--8241, 2021.

\bibitem[Bagaria et~al.(2021)Bagaria, Senthil, and
  Konidaris]{DBLP:conf/icml/BagariaS021}
Bagaria, A., Senthil, J.~K., and Konidaris, G.
\newblock Skill discovery for exploration and planning using deep skill graphs.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021,
  Virtual Event}, volume 139 of \emph{Proceedings of Machine Learning
  Research}, pp.\  521--531. {PMLR}, 2021.

\bibitem[Barreto et~al.(2019)Barreto, Borsa, Hou, Comanici, Ayg{\"u}n, Hamel,
  Toyama, Hunt, Mourad, Silver, and Precup]{Barreto2019TheOK}
Barreto, A., Borsa, D., Hou, S., Comanici, G., Ayg{\"u}n, E., Hamel, P.,
  Toyama, D., Hunt, J.~J., Mourad, S., Silver, D., and Precup, D.
\newblock The option keyboard: Combining skills in reinforcement learning.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Bellman \& Kalaba(1959)Bellman and Kalaba]{Bellman1959OnAC}
Bellman, R. and Kalaba, R.~E.
\newblock On adaptive control processes.
\newblock \emph{Ire Transactions on Automatic Control}, 4:\penalty0 1--9, 1959.

\bibitem[Bester et~al.(2019)Bester, James, and
  Konidaris]{Bester2019MultiPassQF}
Bester, C.~J., James, S., and Konidaris, G.~D.
\newblock Multi-pass q-networks for deep reinforcement learning with
  parameterised action spaces.
\newblock \emph{ArXiv}, abs/1905.04388, 2019.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{1606.01540}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym, 2016.

\bibitem[Campos et~al.(2020)Campos, Trott, Xiong, Socher, i~Nieto, and
  Torres]{Campos2020ExploreDA}
Campos, V., Trott, A., Xiong, C., Socher, R., i~Nieto, X.~G., and Torres, J.
\newblock Explore, discover and learn: Unsupervised discovery of state-covering
  skills.
\newblock In \emph{ICML}, 2020.

\bibitem[Chitnis et~al.(2020)Chitnis, Tulsiani, Gupta, and
  Gupta]{DBLP:conf/icra/ChitnisT0020}
Chitnis, R., Tulsiani, S., Gupta, S., and Gupta, A.
\newblock Efficient bimanual manipulation using learned task schemas.
\newblock In \emph{2020 {IEEE} International Conference on Robotics and
  Automation, {ICRA} 2020, Paris, France, May 31 - August 31, 2020}, pp.\
  1149--1155. {IEEE}, 2020.

\bibitem[Co-Reyes et~al.(2018)Co-Reyes, Liu, Gupta, Eysenbach, Abbeel, and
  Levine]{CoReyes2018SelfConsistentTA}
Co-Reyes, J.~D., Liu, Y., Gupta, A., Eysenbach, B., Abbeel, P., and Levine, S.
\newblock Self-consistent trajectory autoencoder: Hierarchical reinforcement
  learning with trajectory embeddings.
\newblock In \emph{ICML}, 2018.

\bibitem[da~Silva et~al.(2012)da~Silva, Konidaris, and
  Barto]{DBLP:conf/icml/SilvaKB12}
da~Silva, B.~C., Konidaris, G.~D., and Barto, A.~G.
\newblock Learning parameterized skills.
\newblock In \emph{Proceedings of the 29th International Conference on Machine
  Learning, {ICML} 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012}.
  icml.cc / Omnipress, 2012.

\bibitem[Dalal et~al.(2021)Dalal, Pathak, and
  Salakhutdinov]{DBLP:conf/nips/DalalPS21}
Dalal, M., Pathak, D., and Salakhutdinov, R.
\newblock Accelerating robotic reinforcement learning via parameterized action
  primitives.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y.~N., Liang, P., and
  Vaughan, J.~W. (eds.), \emph{Advances in Neural Information Processing
  Systems 34: Annual Conference on Neural Information Processing Systems 2021,
  NeurIPS 2021, December 6-14, 2021, virtual}, pp.\  21847--21859, 2021.

\bibitem[Dorfman et~al.(2021)Dorfman, Shenfeld, and
  Tamar]{DBLP:conf/nips/DorfmanST21}
Dorfman, R., Shenfeld, I., and Tamar, A.
\newblock Offline meta reinforcement learning - identifiability challenges and
  effective data collection strategies.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y.~N., Liang, P., and
  Vaughan, J.~W. (eds.), \emph{Advances in Neural Information Processing
  Systems 34: Annual Conference on Neural Information Processing Systems 2021,
  NeurIPS 2021, December 6-14, 2021, virtual}, pp.\  4607--4618, 2021.

\bibitem[Doshi-Velez \& Konidaris(2016)Doshi-Velez and
  Konidaris]{DoshiVelez2016HiddenPM}
Doshi-Velez, F. and Konidaris, G.~D.
\newblock Hidden parameter markov decision processes: A semiparametric
  regression approach for discovering latent task parametrizations.
\newblock \emph{IJCAI : proceedings of the conference}, 2016:\penalty0
  1432--1440, 2016.

\bibitem[Eysenbach et~al.(2019)Eysenbach, Gupta, Ibarz, and
  Levine]{DBLP:conf/iclr/EysenbachGIL19}
Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S.
\newblock Diversity is all you need: Learning skills without a reward function.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}, 2019.

\bibitem[Fan et~al.(2019)Fan, Su, Zhang, and Yu]{Fan2019HybridAR}
Fan, Z., Su, R., Zhang, W., and Yu, Y.
\newblock Hybrid actor-critic reinforcement learning in parameterized action
  space.
\newblock In \emph{IJCAI}, 2019.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{Finn2017ModelAgnosticMF}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Frans et~al.(2018)Frans, Ho, Chen, Abbeel, and
  Schulman]{Frans2018MetaLS}
Frans, K., Ho, J., Chen, X., Abbeel, P., and Schulman, J.
\newblock Meta learning shared hierarchies.
\newblock \emph{ArXiv}, abs/1710.09767, 2018.

\bibitem[Fu et~al.(2019)Fu, Tang, Hao, Lei, Chen, and Fan]{Fu2019DeepMR}
Fu, H., Tang, H., Hao, J., Lei, Z., Chen, Y., and Fan, C.
\newblock Deep multi-agent reinforcement learning with discrete-continuous
  hybrid action spaces.
\newblock In \emph{IJCAI}, 2019.

\bibitem[Fu et~al.(2021)Fu, Tang, Hao, Chen, Feng, Li, and
  Liu]{DBLP:conf/aaai/FuTHCFLL21}
Fu, H., Tang, H., Hao, J., Chen, C., Feng, X., Li, D., and Liu, W.
\newblock Towards effective context for meta-reinforcement learning: an
  approach based on contrastive learning.
\newblock In \emph{Thirty-Fifth {AAAI} Conference on Artificial Intelligence,
  {AAAI} 2021, 2021}, pp.\  7457--7465. {AAAI} Press, 2021.

\bibitem[Fu et~al.(2022)Fu, Yu, Littman, and Konidaris]{DBLP:conf/nips/FuYL022}
Fu, H., Yu, S., Littman, M.~L., and Konidaris, G.
\newblock Model-based lifelong reinforcement learning with bayesian
  exploration.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Fu et~al.(2023)Fu, Yao, Gottesman, Doshi-Velez, and
  Konidaris]{fu2023performance}
Fu, H., Yao, J., Gottesman, O., Doshi-Velez, F., and Konidaris, G.
\newblock Performance bounds for model and policy transfer in hidden-parameter
  mdps.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and
  Meger]{Fujimoto2018AddressingFA}
Fujimoto, S., van Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock \emph{ArXiv}, abs/1802.09477, 2018.

\bibitem[Gelada et~al.(2019)Gelada, Kumar, Buckman, Nachum, and
  Bellemare]{DBLP:conf/icml/GeladaKBNB19}
Gelada, C., Kumar, S., Buckman, J., Nachum, O., and Bellemare, M.~G.
\newblock Deepmdp: Learning continuous latent space models for representation
  learning.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June
  2019, Long Beach, California, {USA}}, volume~97 of \emph{Proceedings of
  Machine Learning Research}, pp.\  2170--2179. {PMLR}, 2019.

\bibitem[Goyal et~al.(2020)Goyal, Sodhani, Binas, Peng, Levine, and
  Bengio]{Goyal2020ReinforcementLW}
Goyal, A., Sodhani, S., Binas, J., Peng, X.~B., Levine, S., and Bengio, Y.
\newblock Reinforcement learning with competitive ensembles of
  information-constrained primitives.
\newblock \emph{ArXiv}, abs/1906.10667, 2020.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{Haarnoja2018SoftAO}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{ICML}, 2018.

\bibitem[Harrison et~al.(2020)Harrison, Sharma, Finn, and
  Pavone]{DBLP:conf/nips/HarrisonSFP20}
Harrison, J., Sharma, A., Finn, C., and Pavone, M.
\newblock Continuous meta-learning without tasks.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Hausknecht \& Stone(2016)Hausknecht and
  Stone]{DBLP:journals/corr/HausknechtS15a}
Hausknecht, M.~J. and Stone, P.
\newblock Deep reinforcement learning in parameterized action space.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{4th International
  Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico,
  May 2-4, 2016, Conference Track Proceedings}, 2016.

\bibitem[Hausman et~al.(2018)Hausman, Springenberg, Wang, Heess, and
  Riedmiller]{Hausman2018LearningAE}
Hausman, K., Springenberg, J.~T., Wang, Z., Heess, N. M.~O., and Riedmiller,
  M.~A.
\newblock Learning an embedding space for transferable robot skills.
\newblock In \emph{ICLR}, 2018.

\bibitem[Heess et~al.(2016)Heess, Wayne, Tassa, Lillicrap, Riedmiller, and
  Silver]{Heess2016LearningAT}
Heess, N. M.~O., Wayne, G., Tassa, Y., Lillicrap, T.~P., Riedmiller, M.~A., and
  Silver, D.
\newblock Learning and transfer of modulated locomotor controllers.
\newblock \emph{ArXiv}, abs/1610.05182, 2016.

\bibitem[Jang et~al.(2017)Jang, Gu, and Poole]{Jang2017CategoricalRW}
Jang, E., Gu, S.~S., and Poole, B.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock \emph{ArXiv}, abs/1611.01144, 2017.

\bibitem[Kaelbling \& Lozano{-}P{\'{e}}rez(2011)Kaelbling and
  Lozano{-}P{\'{e}}rez]{DBLP:conf/icra/KaelblingL11}
Kaelbling, L.~P. and Lozano{-}P{\'{e}}rez, T.
\newblock Hierarchical task and motion planning in the now.
\newblock In \emph{{IEEE} International Conference on Robotics and Automation,
  {ICRA} 2011, Shanghai, China, 9-13 May 2011}, pp.\  1470--1477. {IEEE}, 2011.

\bibitem[Killian et~al.(2017)Killian, Daulton, Konidaris, and
  Doshi{-}Velez]{DBLP:journals/corr/KillianDKD17}
Killian, T.~W., Daulton, S., Konidaris, G.~D., and Doshi{-}Velez, F.
\newblock Robust and efficient transfer learning with hidden-parameter markov
  decision processes.
\newblock \emph{CoRR}, abs/1706.06544, 2017.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{Kingma2014AutoEncodingVB}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{CoRR}, abs/1312.6114, 2014.

\bibitem[Kwon(2021)]{DBLP:conf/iclr/Kwon21}
Kwon, T.
\newblock Variational intrinsic control revisited.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.

\bibitem[Laskin et~al.(2020)Laskin, Srinivas, and
  Abbeel]{DBLP:conf/icml/LaskinSA20}
Laskin, M., Srinivas, A., and Abbeel, P.
\newblock {CURL:} contrastive unsupervised representations for reinforcement
  learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  5639--5650. {PMLR},
  2020.

\bibitem[Lee et~al.(2020)Lee, Seo, Lee, Lee, and
  Shin]{DBLP:conf/icml/LeeSLLS20}
Lee, K., Seo, Y., Lee, S., Lee, H., and Shin, J.
\newblock Context-aware dynamics model for generalization in model-based
  reinforcement learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  5757--5766. {PMLR},
  2020.

\bibitem[Li et~al.(2021)Li, Tang, Zheng, Hao, Li, Wang, Meng, and
  Wang]{Li2021HyARAD}
Li, B., Tang, H., Zheng, Y., Hao, J., Li, P., Wang, Z.~Y., Meng, Z., and Wang,
  L.
\newblock Hyar: Addressing discrete-continuous action reinforcement learning
  via hybrid action representation.
\newblock \emph{ArXiv}, abs/2109.05490, 2021.

\bibitem[Masson et~al.(2016)Masson, Ranchod, and
  Konidaris]{DBLP:conf/aaai/MassonRK16}
Masson, W., Ranchod, P., and Konidaris, G.~D.
\newblock Reinforcement learning with parameterized actions.
\newblock In Schuurmans, D. and Wellman, M.~P. (eds.), \emph{AAAI, 2016}, pp.\
  1934--1940, 2016.

\bibitem[Mishra et~al.(2018)Mishra, Rohaninejad, Chen, and
  Abbeel]{DBLP:conf/iclr/MishraR0A18}
Mishra, N., Rohaninejad, M., Chen, X., and Abbeel, P.
\newblock A simple neural attentive meta-learner.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}. OpenReview.net, 2018.

\bibitem[M{\"u}ller(2008)]{Mller2008DynamicTW}
M{\"u}ller, M.
\newblock Dynamic time warping.
\newblock In \emph{Information retrieval for music and motion}, 2008.

\bibitem[Nachum et~al.(2018)Nachum, Gu, Lee, and
  Levine]{Nachum2018DataEfficientHR}
Nachum, O., Gu, S.~S., Lee, H., and Levine, S.
\newblock Data-efficient hierarchical reinforcement learning.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Nam et~al.(2022)Nam, Sun, Pertsch, Hwang, and
  Lim]{Nam2022SkillbasedML}
Nam, T., Sun, S.-H., Pertsch, K., Hwang, S.~J., and Lim, J.~J.
\newblock Skill-based meta-reinforcement learning.
\newblock \emph{ArXiv}, abs/2204.11828, 2022.

\bibitem[Nasiriany et~al.(2022)Nasiriany, Liu, and
  Zhu]{DBLP:conf/icra/NasirianyLZ22}
Nasiriany, S., Liu, H., and Zhu, Y.
\newblock Augmenting reinforcement learning with behavior primitives for
  diverse manipulation tasks.
\newblock In \emph{2022 International Conference on Robotics and Automation,
  {ICRA} 2022, Philadelphia, PA, USA, May 23-27, 2022}, pp.\  7477--7484.
  {IEEE}, 2022.

\bibitem[Neunert et~al.(2019)Neunert, Abdolmaleki, Wulfmeier, Lampe,
  Springenberg, Hafner, Romano, Buchli, Heess, and
  Riedmiller]{Neunert2019ContinuousDiscreteRL}
Neunert, M., Abdolmaleki, A., Wulfmeier, M., Lampe, T., Springenberg, J.~T.,
  Hafner, R., Romano, F., Buchli, J., Heess, N. M.~O., and Riedmiller, M.~A.
\newblock Continuous-discrete reinforcement learning for hybrid control in
  robotics.
\newblock \emph{ArXiv}, abs/2001.00449, 2019.

\bibitem[Ni et~al.(2022)Ni, Eysenbach, and
  Salakhutdinov]{DBLP:conf/icml/NiES22}
Ni, T., Eysenbach, B., and Salakhutdinov, R.
\newblock Recurrent model-free {RL} can be a strong baseline for many pomdps.
\newblock In Chaudhuri, K., Jegelka, S., Song, L., Szepesv{\'{a}}ri, C., Niu,
  G., and Sabato, S. (eds.), \emph{International Conference on Machine
  Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, volume
  162 of \emph{Proceedings of Machine Learning Research}, pp.\  16691--16723.
  {PMLR}, 2022.

\bibitem[Notin et~al.(2021)Notin, Hern{\'a}ndez-Lobato, and
  Gal]{Notin2021ImprovingBO}
Notin, P., Hern{\'a}ndez-Lobato, J.~M., and Gal, Y.
\newblock Improving black-box optimization in vae latent space using decoder
  uncertainty.
\newblock In \emph{Neural Information Processing Systems}, 2021.

\bibitem[Parr \& Friston(2018)Parr and Friston]{Parr2018TheDA}
Parr, T. and Friston, K.~J.
\newblock The discrete and continuous brain: From decisions to movementâ€”and
  back again.
\newblock \emph{Neural Computation}, 30:\penalty0 2319 -- 2347, 2018.

\bibitem[Qureshi et~al.(2020)Qureshi, Johnson, Qin, Henderson, Boots, and
  Yip]{Qureshi2020ComposingTP}
Qureshi, A.~H., Johnson, J.~J., Qin, Y., Henderson, T., Boots, B., and Yip,
  M.~C.
\newblock Composing task-agnostic policies with deep reinforcement learning.
\newblock In \emph{ICLR}, 2020.

\bibitem[Raffin et~al.(2021)Raffin, Hill, Gleave, Kanervisto, Ernestus, and
  Dormann]{stable-baselines3}
Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., and Dormann, N.
\newblock Stable-baselines3: Reliable reinforcement learning implementations.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (268):\penalty0 1--8, 2021.

\bibitem[Raileanu et~al.(2020)Raileanu, Goldstein, Szlam, and
  Fergus]{DBLP:conf/icml/RaileanuGSF20}
Raileanu, R., Goldstein, M., Szlam, A., and Fergus, R.
\newblock Fast adaptation to new environments via policy-dynamics value
  functions.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  7920--7931. {PMLR},
  2020.

\bibitem[Rakelly et~al.(2019)Rakelly, Zhou, Finn, Levine, and
  Quillen]{DBLP:conf/icml/RakellyZFLQ19}
Rakelly, K., Zhou, A., Finn, C., Levine, S., and Quillen, D.
\newblock Efficient off-policy meta-reinforcement learning via probabilistic
  context variables.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019}, volume~97 of \emph{Proceedings of Machine Learning
  Research}, pp.\  5331--5340. {PMLR}, 2019.

\bibitem[Rao et~al.(2021)Rao, Sadeghi, Hasenclever, Wulfmeier, Zambelli,
  Vezzani, Tirumala, Aytar, Merel, Heess, and Hadsell]{Rao2021LearningTM}
Rao, D., Sadeghi, F., Hasenclever, L., Wulfmeier, M., Zambelli, M., Vezzani,
  G., Tirumala, D., Aytar, Y., Merel, J., Heess, N. M.~O., and Hadsell, R.
\newblock Learning transferable motor skills with hierarchical latent mixture
  policies.
\newblock \emph{ArXiv}, abs/2112.05062, 2021.

\bibitem[Riedmiller et~al.(2018)Riedmiller, Hafner, Lampe, Neunert, Degrave,
  de~Wiele, Mnih, Heess, and Springenberg]{Riedmiller2018LearningBP}
Riedmiller, M.~A., Hafner, R., Lampe, T., Neunert, M., Degrave, J., de~Wiele,
  T.~V., Mnih, V., Heess, N. M.~O., and Springenberg, J.~T.
\newblock Learning by playing - solving sparse reward tasks from scratch.
\newblock \emph{ArXiv}, abs/1802.10567, 2018.

\bibitem[Sharma et~al.(2020{\natexlab{a}})Sharma, Ahn, Levine, Kumar, Hausman,
  and Gu]{DBLP:conf/rss/SharmaALKHG20}
Sharma, A., Ahn, M., Levine, S., Kumar, V., Hausman, K., and Gu, S.
\newblock Emergent real-world robotic skills via unsupervised off-policy
  reinforcement learning.
\newblock In Toussaint, M., Bicchi, A., and Hermans, T. (eds.), \emph{Robotics:
  Science and Systems XVI, Virtual Event / Corvalis, Oregon, USA, July 12-16,
  2020}, 2020{\natexlab{a}}.

\bibitem[Sharma et~al.(2020{\natexlab{b}})Sharma, Gu, Levine, Kumar, and
  Hausman]{DBLP:conf/iclr/SharmaGLKH20}
Sharma, A., Gu, S., Levine, S., Kumar, V., and Hausman, K.
\newblock Dynamics-aware unsupervised discovery of skills.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020{\natexlab{b}}.

\bibitem[Silver et~al.(2021)Silver, Chitnis, Tenenbaum, Kaelbling, and
  Lozano{-}P{\'{e}}rez]{DBLP:conf/iros/SilverCTKL21}
Silver, T., Chitnis, R., Tenenbaum, J.~B., Kaelbling, L.~P., and
  Lozano{-}P{\'{e}}rez, T.
\newblock Learning symbolic operators for task and motion planning.
\newblock In \emph{{IEEE/RSJ} International Conference on Intelligent Robots
  and Systems, {IROS} 2021, Prague, Czech Republic, September 27 - Oct. 1,
  2021}, pp.\  3182--3189. {IEEE}, 2021.

\bibitem[Silver et~al.(2022)Silver, Chitnis, Kumar, McClinton, Lozano-Perez,
  Kaelbling, and Tenenbaum]{Silver2022PredicateIF}
Silver, T., Chitnis, R., Kumar, N., McClinton, W., Lozano-Perez, T., Kaelbling,
  L.~P., and Tenenbaum, J.~B.
\newblock Predicate invention for bilevel planning.
\newblock 2022.

\bibitem[Sodhani et~al.(2022)Sodhani, Meier, Pineau, and
  Zhang]{DBLP:conf/l4dc/SodhaniMP022}
Sodhani, S., Meier, F., Pineau, J., and Zhang, A.
\newblock Block contextual mdps for continual learning.
\newblock In Firoozi, R., Mehr, N., Yel, E., Antonova, R., Bohg, J., Schwager,
  M., and Kochenderfer, M.~J. (eds.), \emph{Learning for Dynamics and Control
  Conference, {L4DC} 2022, 23-24 June 2022, Stanford University, Stanford, CA,
  {USA}}, volume 168 of \emph{Proceedings of Machine Learning Research}, pp.\
  608--623. {PMLR}, 2022.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{Sutton1999BetweenMA}
Sutton, R.~S., Precup, D., and Singh, S.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning.
\newblock \emph{Artif. Intell.}, 112:\penalty0 181--211, 1999.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and
  Tassa]{DBLP:conf/iros/TodorovET12}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: {A} physics engine for model-based control.
\newblock In \emph{2012 {IEEE/RSJ} International Conference on Intelligent
  Robots and Systems, {IROS} 2012, Vilamoura, Algarve, Portugal, October 7-12,
  2012}, pp.\  5026--5033. {IEEE}, 2012.

\bibitem[Wang et~al.(2016)Wang, Kurth{-}Nelson, Tirumala, Soyer, Leibo, Munos,
  Blundell, Kumaran, and Botvinick]{DBLP:journals/corr/WangKTSLMBKB16}
Wang, J.~X., Kurth{-}Nelson, Z., Tirumala, D., Soyer, H., Leibo, J.~Z., Munos,
  R., Blundell, C., Kumaran, D., and Botvinick, M.
\newblock Learning to reinforcement learn.
\newblock \emph{CoRR}, abs/1611.05763, 2016.

\bibitem[Xiong et~al.(2018)Xiong, Wang, Yang, Sun, Han, Zheng, Fu, Zhang, Liu,
  and Liu]{Xiong2018ParametrizedDQ}
Xiong, J., Wang, Q., Yang, Z., Sun, P., Han, L., Zheng, Y., Fu, H., Zhang, T.,
  Liu, J., and Liu, H.
\newblock Parametrized deep q-networks learning: Reinforcement learning with
  discrete-continuous hybrid action space.
\newblock \emph{ArXiv}, abs/1810.06394, 2018.

\bibitem[Yu et~al.(2019)Yu, Quillen, He, Julian, Hausman, Finn, and
  Levine]{DBLP:conf/corl/YuQHJHFL19}
Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S.
\newblock Meta-world: {A} benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock In Kaelbling, L.~P., Kragic, D., and Sugiura, K. (eds.), \emph{3rd
  Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 -
  November 1, 2019, Proceedings}, volume 100 of \emph{Proceedings of Machine
  Learning Research}, pp.\  1094--1100. {PMLR}, 2019.

\bibitem[Zhang et~al.(2021)Zhang, McAllister, Calandra, Gal, and
  Levine]{DBLP:conf/iclr/0001MCGL21}
Zhang, A., McAllister, R.~T., Calandra, R., Gal, Y., and Levine, S.
\newblock Learning invariant representations for reinforcement learning without
  reconstruction.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.

\bibitem[Zhao et~al.(2022)Zhao, Abbeel, and
  James]{DBLP:journals/corr/abs-2206-03271}
Zhao, M., Abbeel, P., and James, S.
\newblock On the effectiveness of fine-tuning versus meta-reinforcement
  learning.
\newblock \emph{CoRR}, abs/2206.03271, 2022.
\newblock \doi{10.48550/arXiv.2206.03271}.

\bibitem[Zhou et~al.(2020)Zhou, Bajracharya, and Held]{DBLP:conf/corl/ZhouBH20}
Zhou, W., Bajracharya, S., and Held, D.
\newblock {PLAS:} latent action space for offline reinforcement learning.
\newblock In Kober, J., Ramos, F., and Tomlin, C.~J. (eds.), \emph{4th
  Conference on Robot Learning, CoRL 2020, 16-18 November 2020, Virtual Event /
  Cambridge, MA, {USA}}, volume 155 of \emph{Proceedings of Machine Learning
  Research}, pp.\  1719--1735. {PMLR}, 2020.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and
  Dey]{Ziebart2008MaximumEI}
Ziebart, B.~D., Maas, A.~L., Bagnell, J.~A., and Dey, A.~K.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{AAAI}, 2008.

\bibitem[Zintgraf et~al.(2020)Zintgraf, Shiarlis, Igl, Schulze, Gal, Hofmann,
  and Whiteson]{Zintgraf2020VariBADAV}
Zintgraf, L.~M., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hofmann, K., and
  Whiteson, S.
\newblock Varibad: A very good method for bayes-adaptive deep rl via
  meta-learning.
\newblock \emph{ArXiv}, abs/1910.08348, 2020.

\end{thebibliography}
