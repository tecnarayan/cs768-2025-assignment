\begin{thebibliography}{76}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadal et~al.(2020)Abadal, Jain, Guirado, L{\'o}pez-Alonso, and
  Alarc{\'o}n]{abadal2020computing}
Abadal, S., Jain, A., Guirado, R., L{\'o}pez-Alonso, J., and Alarc{\'o}n, E.
\newblock Computing graph neural networks: A survey from algorithms to
  accelerators.
\newblock \emph{arXiv preprint arXiv:2010.00130}, 2020.

\bibitem[Adhikari et~al.(2017)Adhikari, Zhang, Amiri, Bharadwaj, and
  Prakash]{adhikari2017propagation}
Adhikari, B., Zhang, Y., Amiri, S.~E., Bharadwaj, A., and Prakash, B.~A.
\newblock Propagation-based temporal network summarization.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering},
  30\penalty0 (4):\penalty0 729--742, 2017.

\bibitem[Auten et~al.(2020)Auten, Tomei, and Kumar]{auten2020hardware}
Auten, A., Tomei, M., and Kumar, R.
\newblock Hardware acceleration of graph neural networks.
\newblock In \emph{2020 57th ACM/IEEE Design Automation Conference (DAC)}, pp.\
   1--6. IEEE, 2020.

\bibitem[Battaglia et~al.(2016)Battaglia, Pascanu, Lai, Rezende, and
  kavukcuoglu]{battaglia2016interaction}
Battaglia, P., Pascanu, R., Lai, M., Rezende, D.~J., and kavukcuoglu, K.
\newblock Interaction networks for learning about objects, relations and
  physics.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, 2016.

\bibitem[Bertsekas \& Rheinboldt(1982)Bertsekas and
  Rheinboldt]{bertsekas1982constrained}
Bertsekas, D.~P. and Rheinboldt, W.
\newblock Constrained optimization and lagrange multiplier methods.
\newblock 1982.

\bibitem[Bruna et~al.(2013)Bruna, Zaremba, Szlam, and LeCun]{bruna2013spectral}
Bruna, J., Zaremba, W., Szlam, A., and LeCun, Y.
\newblock Spectral networks and locally connected networks on graphs.
\newblock \emph{arXiv preprint arXiv:1312.6203}, 2013.

\bibitem[Busch et~al.(2020)Busch, Pi, and Seidl]{busch2020pushnet}
Busch, J., Pi, J., and Seidl, T.
\newblock Pushnet: Efficient and adaptive neural message passing.
\newblock \emph{arXiv preprint arXiv:2003.02228}, 2020.

\bibitem[Calandriello et~al.(2018)Calandriello, Lazaric, Koutis, and
  Valko]{calandriello2018improved}
Calandriello, D., Lazaric, A., Koutis, I., and Valko, M.
\newblock Improved large-scale graph learning through ridge spectral
  sparsification.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  688--697. PMLR, 2018.

\bibitem[Chakeri et~al.(2016)Chakeri, Farhidzadeh, and
  Hall]{chakeri2016spectral}
Chakeri, A., Farhidzadeh, H., and Hall, L.~O.
\newblock Spectral sparsification in spectral clustering.
\newblock In \emph{2016 23rd international conference on pattern recognition
  (icpr)}, pp.\  2301--2306. IEEE, 2016.

\bibitem[Chen et~al.(2018{\natexlab{a}})Chen, Ma, and Xiao]{chen2018fastgcn}
Chen, J., Ma, T., and Xiao, C.
\newblock Fastgcn: Fast learning with graph convolutional networks via
  importance sampling.
\newblock In \emph{International Conference on Learning Representations},
  2018{\natexlab{a}}.

\bibitem[Chen et~al.(2018{\natexlab{b}})Chen, Zhu, and
  Song]{chen2018stochastic}
Chen, J., Zhu, J., and Song, L.
\newblock Stochastic training of graph convolutional networks with variance
  reduction.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  942--950. PMLR, 2018{\natexlab{b}}.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Frankle, Chang, Liu, Zhang,
  Carbin, and Wang]{chen2020lottery2}
Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Carbin, M., and Wang, Z.
\newblock The lottery tickets hypothesis for supervised and self-supervised
  pre-training in computer vision models.
\newblock \emph{arXiv preprint arXiv:2012.06908}, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Frankle, Chang, Liu, Zhang, Wang,
  and Carbin]{chen2020lottery}
Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang, Z., and Carbin, M.
\newblock The lottery ticket hypothesis for pre-trained bert networks.
\newblock \emph{arXiv preprint arXiv:2007.12223}, 2020{\natexlab{b}}.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Cheng, Gan, Liu, and
  Wang]{chen2021ultra}
Chen, T., Cheng, Y., Gan, Z., Liu, J., and Wang, Z.
\newblock Ultra-data-efficient gan training: Drawing a lottery ticket first,
  then training it toughly.
\newblock \emph{arXiv preprint arXiv:2103.00397}, 2021{\natexlab{a}}.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Zhang, Liu, Chang, and
  Wang]{chen2021long}
Chen, T., Zhang, Z., Liu, S., Chang, S., and Wang, Z.
\newblock Long live the lottery: The existence of winning tickets in lifelong
  learning.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=LXMSvPmsm0g}.

\bibitem[Chen et~al.(2020{\natexlab{c}})Chen, Cheng, Wang, Gan, Wang, and
  Liu]{chen2020earlybert}
Chen, X., Cheng, Y., Wang, S., Gan, Z., Wang, Z., and Liu, J.
\newblock Earlybert: Efficient bert training via early-bird lottery tickets.
\newblock \emph{arXiv preprint arXiv:2101.00063}, 2020{\natexlab{c}}.

\bibitem[Chen et~al.(2019)Chen, Villar, Chen, and Bruna]{chen2019equivalence}
Chen, Z., Villar, S., Chen, L., and Bruna, J.
\newblock On the equivalence between graph isomorphism testing and function
  approximation with gnns.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Cheng et~al.(2017)Cheng, Wang, Zhou, and Zhang]{cheng2017survey}
Cheng, Y., Wang, D., Zhou, P., and Zhang, T.
\newblock A survey of model compression and acceleration for deep neural
  networks.
\newblock \emph{arXiv preprint arXiv:1710.09282}, 2017.

\bibitem[Chiang et~al.(2019)Chiang, Liu, Si, Li, Bengio, and
  Hsieh]{chiang2019cluster}
Chiang, W.-L., Liu, X., Si, S., Li, Y., Bengio, S., and Hsieh, C.-J.
\newblock Cluster-gcn: An efficient algorithm for training deep and large graph
  convolutional networks.
\newblock In \emph{Proceedings of the 25th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pp.\  257--266, 2019.

\bibitem[Defferrard et~al.(2016)Defferrard, Bresson, and
  Vandergheynst]{defferrard2016convolutional}
Defferrard, M., Bresson, X., and Vandergheynst, P.
\newblock Convolutional neural networks on graphs with fast localized spectral
  filtering.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pp.\  3844--3852, 2016.

\bibitem[Dwivedi et~al.(2020)Dwivedi, Joshi, Laurent, Bengio, and
  Bresson]{dwivedi2020benchmarking}
Dwivedi, V.~P., Joshi, C.~K., Laurent, T., Bengio, Y., and Bresson, X.
\newblock Benchmarking graph neural networks.
\newblock \emph{arXiv preprint arXiv:2003.00982}, 2020.

\bibitem[Eden et~al.(2018)Eden, Jain, Pinar, Ron, and
  Seshadhri]{eden2018provable}
Eden, T., Jain, S., Pinar, A., Ron, D., and Seshadhri, C.
\newblock Provable and practical approximations for the degree distribution
  using sublinear graph samples.
\newblock In \emph{Proceedings of the 2018 World Wide Web Conference}, pp.\
  449--458, 2018.

\bibitem[Evci et~al.(2019)Evci, Pedregosa, Gomez, and
  Elsen]{evci2019difficulty}
Evci, U., Pedregosa, F., Gomez, A., and Elsen, E.
\newblock The difficulty of training sparse neural networks.
\newblock \emph{arXiv}, abs/1906.10732, 2019.

\bibitem[Frankle \& Carbin(2018)Frankle and Carbin]{frankle2018lottery}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Frankle et~al.(2019)Frankle, Dziugaite, Roy, and
  Carbin]{frankle2019linear}
Frankle, J., Dziugaite, G.~K., Roy, D.~M., and Carbin, M.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock \emph{arXiv}, abs/1912.05671, 2019.

\bibitem[Freeman(1977)]{freeman1977set}
Freeman, L.~C.
\newblock A set of measures of centrality based on betweenness.
\newblock \emph{Sociometry}, pp.\  35--41, 1977.

\bibitem[Gale et~al.(2019)Gale, Elsen, and Hooker]{gale2019state}
Gale, T., Elsen, E., and Hooker, S.
\newblock The state of sparsity in deep neural networks.
\newblock \emph{arXiv}, abs/1902.09574, 2019.

\bibitem[Gan et~al.(2021)Gan, Chen, Li, Chen, Cheng, Wang, and
  Liu]{gan2021playing}
Gan, Z., Chen, Y.-C., Li, L., Chen, T., Cheng, Y., Wang, S., and Liu, J.
\newblock Playing lottery tickets with vision and language.
\newblock \emph{arXiv preprint arXiv:2104.11832}, 2021.

\bibitem[Geng et~al.(2020)Geng, Li, Shi, Wu, Wang, Li, Haghi, Tumeo, Che,
  Reinhardt, et~al.]{geng2020awb}
Geng, T., Li, A., Shi, R., Wu, C., Wang, T., Li, Y., Haghi, P., Tumeo, A., Che,
  S., Reinhardt, S., et~al.
\newblock Awb-gcn: A graph convolutional network accelerator with runtime
  workload rebalancing.
\newblock In \emph{2020 53rd Annual IEEE/ACM International Symposium on
  Microarchitecture (MICRO)}, pp.\  922--936. IEEE, 2020.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and
  Dahl]{gilmer2017neural}
Gilmer, J., Schoenholz, S.~S., Riley, P.~F., Vinyals, O., and Dahl, G.~E.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1263--1272. PMLR, 2017.

\bibitem[Hamilton et~al.(2017)Hamilton, Ying, and
  Leskovec]{hamilton2017inductive}
Hamilton, W., Ying, Z., and Leskovec, J.
\newblock Inductive representation learning on large graphs.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1024--1034, 2017.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{han2015deep}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock In \emph{4th International Conference on Learning Representations},
  2016.

\bibitem[Hu et~al.(2020)Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta, and
  Leskovec]{hu2020open}
Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and
  Leskovec, J.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock \emph{arXiv preprint arXiv:2005.00687}, 2020.

\bibitem[H{\"u}bler et~al.(2008)H{\"u}bler, Kriegel, Borgwardt, and
  Ghahramani]{hubler2008metropolis}
H{\"u}bler, C., Kriegel, H.-P., Borgwardt, K., and Ghahramani, Z.
\newblock Metropolis algorithms for representative subgraph sampling.
\newblock In \emph{2008 Eighth IEEE International Conference on Data Mining},
  pp.\  283--292. IEEE, 2008.

\bibitem[Kalibhat et~al.(2020)Kalibhat, Balaji, and Feizi]{kalibhat2020winning}
Kalibhat, N.~M., Balaji, Y., and Feizi, S.
\newblock Winning lottery tickets in deep generative models, 2020.

\bibitem[Karimi et~al.(2019)Karimi, Wu, Wang, and Shen]{karimi2019explainable}
Karimi, M., Wu, D., Wang, Z., and Shen, Y.
\newblock Explainable deep relational networks for predicting compound-protein
  affinities and contacts.
\newblock \emph{arXiv preprint arXiv:1912.12553}, 2019.

\bibitem[Kiningham et~al.(2020)Kiningham, Re, and Levis]{kiningham2020grip}
Kiningham, K., Re, C., and Levis, P.
\newblock Grip: A graph neural network acceleratorarchitecture.
\newblock \emph{arXiv preprint arXiv:2007.13828}, 2020.

\bibitem[Kipf \& Welling(2016)Kipf and Welling]{kipf2016semi}
Kipf, T.~N. and Welling, M.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock \emph{arXiv preprint arXiv:1609.02907}, 2016.

\bibitem[Leskovec \& Faloutsos(2006)Leskovec and
  Faloutsos]{leskovec2006sampling}
Leskovec, J. and Faloutsos, C.
\newblock Sampling from large graphs.
\newblock In \emph{Proceedings of the 12th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pp.\  631--636, 2006.

\bibitem[Li et~al.(2019)Li, Muller, Thabet, and Ghanem]{li2019deepgcns}
Li, G., Muller, M., Thabet, A., and Ghanem, B.
\newblock Deepgcns: Can gcns go as deep as cnns?
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  9267--9276, 2019.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Xiong, Thabet, and
  Ghanem]{li2020deepergcn}
Li, G., Xiong, C., Thabet, A., and Ghanem, B.
\newblock Deepergcn: All you need to train deeper gcns.
\newblock \emph{arXiv preprint arXiv:2006.07739}, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Zhang, Tian, Jin, Fardad, and
  Zafarani]{li2020sgcn}
Li, J., Zhang, T., Tian, H., Jin, S., Fardad, M., and Zafarani, R.
\newblock Sgcn: A graph sparsifier based on graph convolutional networks.
\newblock In \emph{Pacific-Asia Conference on Knowledge Discovery and Data
  Mining}, pp.\  275--287. Springer, 2020{\natexlab{b}}.

\bibitem[Liu et~al.(2019)Liu, Sun, Zhou, Huang, and Darrell]{liu2018rethinking}
Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T.
\newblock Rethinking the value of network pruning.
\newblock In \emph{7th International Conference on Learning Representations},
  2019.

\bibitem[Luce \& Perry(1949)Luce and Perry]{Luce1949AMO}
Luce, R. and Perry, A.~D.
\newblock A method of matrix analysis of group structure.
\newblock \emph{Psychometrika}, 14:\penalty0 95--116, 1949.

\bibitem[Ma et~al.(2021)Ma, Chen, Hu, You, Xie, and Wang]{ma2021good}
Ma, H., Chen, T., Hu, T.-K., You, C., Xie, X., and Wang, Z.
\newblock Good students play big lottery better.
\newblock \emph{arXiv preprint arXiv:2101.03255}, 2021.

\bibitem[Mavromatis \& Karypis(2020)Mavromatis and
  Karypis]{mavromatis2020graph}
Mavromatis, C. and Karypis, G.
\newblock Graph infoclust: Leveraging cluster-level node information for
  unsupervised graph representation learning.
\newblock \emph{arXiv preprint arXiv:2009.06946}, 2020.

\bibitem[Monti et~al.(2017)Monti, Boscaini, Masci, Rodola, Svoboda, and
  Bronstein]{monti2017geometric}
Monti, F., Boscaini, D., Masci, J., Rodola, E., Svoboda, J., and Bronstein,
  M.~M.
\newblock Geometric deep learning on graphs and manifolds using mixture model
  cnns.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  5115--5124, 2017.

\bibitem[Morris et~al.(2019)Morris, Ritzert, Fey, Hamilton, Lenssen, Rattan,
  and Grohe]{morris2019weisfeiler}
Morris, C., Ritzert, M., Fey, M., Hamilton, W.~L., Lenssen, J.~E., Rattan, G.,
  and Grohe, M.
\newblock Weisfeiler and leman go neural: Higher-order graph neural networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2019.

\bibitem[Murphy et~al.(2019)Murphy, Srinivasan, Rao, and
  Ribeiro]{murphy2019relational}
Murphy, R., Srinivasan, B., Rao, V., and Ribeiro, B.
\newblock Relational pooling for graph representations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4663--4673, 2019.

\bibitem[Narayanan(2005)]{narayanan2005betweenness}
Narayanan, S.
\newblock \emph{The betweenness centrality of biological networks}.
\newblock PhD thesis, Virginia Tech, 2005.

\bibitem[Naumov et~al.(2010)Naumov, Chien, Vandermersch, and
  Kapasi]{naumov2010cusparse}
Naumov, M., Chien, L., Vandermersch, P., and Kapasi, U.
\newblock Cusparse library.
\newblock In \emph{GPU Technology Conference}, 2010.

\bibitem[Qu et~al.(2019)Qu, Bengio, and Tang]{qu2019gmnn}
Qu, M., Bengio, Y., and Tang, J.
\newblock Gmnn: Graph markov neural networks.
\newblock \emph{arXiv preprint arXiv:1905.06214}, 2019.

\bibitem[Renda et~al.(2020)Renda, Frankle, and Carbin]{renda2020comparing}
Renda, A., Frankle, J., and Carbin, M.
\newblock Comparing rewinding and fine-tuning in neural network pruning.
\newblock In \emph{8th International Conference on Learning Representations},
  2020.

\bibitem[Savarese et~al.(2020)Savarese, Silva, and Maire]{savarese2020winning}
Savarese, P., Silva, H., and Maire, M.
\newblock Winning the lottery with continuous sparsification.
\newblock In \emph{Advances in Neural Information Processing Systems 33
  pre-proceedings}, 2020.

\bibitem[Scarselli et~al.(2008)Scarselli, Gori, Tsoi, Hagenbuchner, and
  Monfardini]{scarselli2008graph}
Scarselli, F., Gori, M., Tsoi, A.~C., Hagenbuchner, M., and Monfardini, G.
\newblock The graph neural network model.
\newblock \emph{IEEE Transactions on Neural Networks}, 2008.

\bibitem[Simonovsky \& Komodakis(2017)Simonovsky and
  Komodakis]{simonovsky2017dynamic}
Simonovsky, M. and Komodakis, N.
\newblock Dynamic edge-conditioned filters in convolutional neural networks on
  graphs.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  3693--3702, 2017.

\bibitem[Tailor et~al.(2021)Tailor, Fernandez-Marques, and
  Lane]{tailor2021degreequant}
Tailor, S.~A., Fernandez-Marques, J., and Lane, N.~D.
\newblock Degree-quant: Quantization-aware training for graph neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=NSBrFgJAHg}.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2017)Veli{\v{c}}kovi{\'c}, Cucurull,
  Casanova, Romero, Lio, and Bengio]{velivckovic2017graph}
Veli{\v{c}}kovi{\'c}, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and
  Bengio, Y.
\newblock Graph attention networks.
\newblock \emph{arXiv preprint arXiv:1710.10903}, 2017.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2018)Veli{\v{c}}kovi{\'c}, Cucurull,
  Casanova, Romero, Li{\`o}, and Bengio]{velivckovic2018graph}
Veli{\v{c}}kovi{\'c}, P., Cucurull, G., Casanova, A., Romero, A., Li{\`o}, P.,
  and Bengio, Y.
\newblock Graph attention networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Verma et~al.(2019)Verma, Qu, Lamb, Bengio, Kannala, and
  Tang]{verma2019graphmix}
Verma, V., Qu, M., Lamb, A., Bengio, Y., Kannala, J., and Tang, J.
\newblock Graphmix: Regularized training of graph neural networks for
  semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1909.11715}, 2019.

\bibitem[Voudigari et~al.(2016)Voudigari, Salamanos, Papageorgiou, and
  Yannakoudakis]{voudigari2016rank}
Voudigari, E., Salamanos, N., Papageorgiou, T., and Yannakoudakis, E.~J.
\newblock Rank degree: An efficient algorithm for graph sampling.
\newblock In \emph{2016 IEEE/ACM International Conference on Advances in Social
  Networks Analysis and Mining (ASONAM)}, pp.\  120--129. IEEE, 2016.

\bibitem[Wang et~al.(2014)Wang, Zhang, Shen, Zhang, Lu, Wu, and
  Wang]{wang2014intel}
Wang, E., Zhang, Q., Shen, B., Zhang, G., Lu, X., Wu, Q., and Wang, Y.
\newblock Intel math kernel library.
\newblock In \emph{High-Performance Computing on the Intel{\textregistered}
  Xeon Phi™}, pp.\  167--188. Springer, 2014.

\bibitem[Wang et~al.(2020)Wang, Guan, Sun, Niu, Wang, Zheng, and
  Han]{wang2020gnn}
Wang, Z., Guan, Y., Sun, G., Niu, D., Wang, Y., Zheng, H., and Han, Y.
\newblock Gnn-pim: A processing-in-memory architecture for graph neural
  networks.
\newblock In \emph{Conference on Advanced Computer Architecture}, pp.\  73--86.
  Springer, 2020.

\bibitem[Xu et~al.(2018)Xu, Hu, Leskovec, and Jegelka]{xu2018powerful}
Xu, K., Hu, W., Leskovec, J., and Jegelka, S.
\newblock How powerful are graph neural networks?
\newblock \emph{arXiv preprint arXiv:1810.00826}, 2018.

\bibitem[Xu et~al.(2019)Xu, Hu, Leskovec, and Jegelka]{xu2018how}
Xu, K., Hu, W., Leskovec, J., and Jegelka, S.
\newblock How powerful are graph neural networks?
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=ryGs6iA5Km}.

\bibitem[Yan et~al.(2020)Yan, Deng, Hu, Liang, Feng, Ye, Zhang, Fan, and
  Xie]{yan2020hygcn}
Yan, M., Deng, L., Hu, X., Liang, L., Feng, Y., Ye, X., Zhang, Z., Fan, D., and
  Xie, Y.
\newblock Hygcn: A gcn accelerator with hybrid architecture.
\newblock In \emph{2020 IEEE International Symposium on High Performance
  Computer Architecture (HPCA)}, pp.\  15--29. IEEE, 2020.

\bibitem[Ying et~al.(2018)Ying, You, Morris, Ren, Hamilton, and
  Leskovec]{ying2018hierarchical}
Ying, Z., You, J., Morris, C., Ren, X., Hamilton, W., and Leskovec, J.
\newblock Hierarchical graph representation learning with differentiable
  pooling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4800--4810, 2018.

\bibitem[You et~al.(2020{\natexlab{a}})You, Li, Xu, Fu, Wang, Chen, Baraniuk,
  Wang, and Lin]{You:2019tz}
You, H., Li, C., Xu, P., Fu, Y., Wang, Y., Chen, X., Baraniuk, R.~G., Wang, Z.,
  and Lin, Y.
\newblock Drawing early-bird tickets: Toward more efficient training of deep
  networks.
\newblock In \emph{8th International Conference on Learning Representations},
  2020{\natexlab{a}}.

\bibitem[You et~al.(2020{\natexlab{b}})You, Chen, Sui, Chen, Wang, and
  Shen]{you2020graph}
You, Y., Chen, T., Sui, Y., Chen, T., Wang, Z., and Shen, Y.
\newblock Graph contrastive learning with augmentations.
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{b}}.

\bibitem[You et~al.(2020{\natexlab{c}})You, Chen, Wang, and
  Shen]{pmlr-v119-you20a}
You, Y., Chen, T., Wang, Z., and Shen, Y.
\newblock When does self-supervision help graph convolutional networks?
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  10871--10880. PMLR,
  13--18 Jul 2020{\natexlab{c}}.
\newblock URL \url{http://proceedings.mlr.press/v119/you20a.html}.

\bibitem[You et~al.(2020{\natexlab{d}})You, Chen, Wang, and Shen]{you2020l2}
You, Y., Chen, T., Wang, Z., and Shen, Y.
\newblock L2-gcn: Layer-wise and learned efficient training of graph
  convolutional networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  2127--2135, 2020{\natexlab{d}}.

\bibitem[Yu et~al.(2020)Yu, Edunov, Tian, and Morcos]{yu2019playing}
Yu, H., Edunov, S., Tian, Y., and Morcos, A.~S.
\newblock Playing the lottery with rewards and multiple languages: lottery
  tickets in rl and nlp.
\newblock In \emph{8th International Conference on Learning Representations},
  2020.

\bibitem[Zhang \& Chen(2018)Zhang and Chen]{zhang2018link}
Zhang, M. and Chen, Y.
\newblock Link prediction based on graph neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5165--5175, 2018.

\bibitem[Zhao(2015)]{zhao2015gsparsify}
Zhao, P.
\newblock gsparsify: Graph motif based sparsification for graph clustering.
\newblock In \emph{Proceedings of the 24th ACM International on Conference on
  Information and Knowledge Management}, pp.\  373--382, 2015.

\bibitem[Zheng et~al.(2020)Zheng, Zong, Cheng, Song, Ni, Yu, Chen, and
  Wang]{zheng2020robust}
Zheng, C., Zong, B., Cheng, W., Song, D., Ni, J., Yu, W., Chen, H., and Wang,
  W.
\newblock Robust graph representation learning via neural sparsification.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Zhou et~al.(2018)Zhou, Cui, Zhang, Yang, Liu, Wang, Li, and
  Sun]{zhou2018graph}
Zhou, J., Cui, G., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., and Sun, M.
\newblock Graph neural networks: A review of methods and applications.
\newblock \emph{arXiv preprint arXiv:1812.08434}, 2018.

\end{thebibliography}
