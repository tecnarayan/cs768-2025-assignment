\begin{thebibliography}{10}

\bibitem{ardila2019common}
Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis~M Tyers, and Gregor Weber.
\newblock Common voice: A massively-multilingual speech corpus.
\newblock {\em arXiv preprint arXiv:1912.06670}, 2019.

\bibitem{baevski2020wav2vec2}
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.
\newblock wav2vec 2.0: A framework for self-supervised learning of speech representations.
\newblock {\em Advances in Neural Information Processing Systems}, 33:12449--12460, 2020.

\bibitem{baker2009developments}
Janet~M Baker, Li~Deng, James Glass, Sanjeev Khudanpur, Chin-Hui Lee, Nelson Morgan, and Douglas O'Shaughnessy.
\newblock Developments and directions in speech recognition and understanding, {Part 1 [DSP Education]}.
\newblock {\em IEEE Signal Processing Magazine}, 26(3):75--80, 2009.

\bibitem{barrault2023seamless}
Lo{\"\i}c Barrault, Yu-An Chung, Mariano~Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, et~al.
\newblock Seamless: Multilingual expressive and streaming speech translation.
\newblock {\em arXiv preprint arXiv:2312.05187}, 2023.

\bibitem{bell2020adaptation}
Peter Bell, Joachim Fainberg, Ondrej Klejch, Jinyu Li, Steve Renals, and Pawel Swietojanski.
\newblock Adaptation algorithms for neural network-based speech recognition: An overview.
\newblock {\em IEEE Open Journal of Signal Processing}, 2:33--66, 2020.

\bibitem{chen2022noise}
Chen Chen, Nana Hou, Yuchen Hu, Shashank Shirol, and Eng~Siong Chng.
\newblock Noise-robust speech recognition with 10 minutes unparalleled in-domain data.
\newblock In {\em Proc. ICSSP}, pages 4298--4302, 2022.

\bibitem{chen2023hyporadise}
Chen Chen, Yuchen Hu, Chao-Han~Huck Yang, Sabato~Marco Siniscalchi, Pin-Yu Chen, and Ensiong Chng.
\newblock Hyporadise: An open baseline for generative speech recognition with large language models.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2023.

\bibitem{chen2021source}
Cheng Chen, Quande Liu, Yueming Jin, Qi~Dou, and Pheng-Ann Heng.
\newblock Source-free domain adaptive fundus image segmentation with denoised pseudo-labeling.
\newblock In {\em Proc. MICCAI}, pages 225--235, 2021.

\bibitem{beier}
Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu.
\newblock Recall and learn: Fine-tuning deep pretrained language models with less forgetting.
\newblock {\em arXiv preprint arXiv:2004.12651}, 2020.

\bibitem{chen2022wavlm}
Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu~Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et~al.
\newblock Wavlm: Large-scale self-supervised pre-training for full stack speech processing.
\newblock {\em IEEE Journal of Selected Topics in Signal Processing}, 16(6):1505--1518, 2022.

\bibitem{chiu2018state}
Chung-Cheng Chiu, Tara~N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron~J Weiss, Kanishka Rao, Ekaterina Gonina, et~al.
\newblock State-of-the-art speech recognition with sequence-to-sequence models.
\newblock In {\em Proc. ICASSP}, pages 4774--4778, 2018.

\bibitem{chung2017lip}
Joon~Son Chung, Andrew Senior, Oriol Vinyals, and Andrew Zisserman.
\newblock Lip reading sentences in the wild.
\newblock In {\em Proc. CVPR}, pages 3444--3453, 2017.

\bibitem{conneau2023fleurs}
Alexis Conneau, Min Ma, Simran Khanuja, Yu~Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna.
\newblock Fleurs: Few-shot learning evaluation of universal representations of speech.
\newblock In {\em Proc. SLT}, pages 798--805, 2023.

\bibitem{deng2014autoencoder}
Jun Deng, Zixing Zhang, Florian Eyben, and Bj{\"o}rn Schuller.
\newblock Autoencoder-based unsupervised domain adaptation for speech emotion recognition.
\newblock {\em IEEE Signal Processing Letters}, 21(9):1068--1072, 2014.

\bibitem{ding2022source}
Ning Ding, Yixing Xu, Yehui Tang, Chao Xu, Yunhe Wang, and Dacheng Tao.
\newblock Source-free domain adaptation via distribution estimation.
\newblock In {\em Proc. CVPR}, pages 7212--7222, 2022.

\bibitem{du2013student}
Fengning Du.
\newblock Student perspectives of self-directed language learning: Implications for teaching and research.
\newblock {\em International Journal for the Scholarship of Teaching and Learning}, 7(2):24, 2013.

\bibitem{evermann2000posterior}
Gunnar Evermann and Phil Woodland.
\newblock Posterior probability decoding, confidence estimation and system combination.
\newblock In {\em Proc. STW}, pages 78--81, Baltimore, 2000.

\bibitem{fang2022source}
Yuqi Fang, Pew-Thian Yap, Weili Lin, Hongtu Zhu, and Mingxia Liu.
\newblock Source-free unsupervised domain adaptation: A survey.
\newblock {\em Neural Networks}, page 106230, 2024.

\bibitem{fleuret2021uncertainty}
Francois Fleuret et~al.
\newblock Uncertainty reduction for model adaptation in semantic segmentation.
\newblock In {\em Proc. CVPR}, pages 9613--9623, 2021.

\bibitem{fomicheva2020unsupervised}
Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Fr{\'e}d{\'e}ric Blain, Francisco Guzm{\'a}n, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia.
\newblock Unsupervised quality estimation for neural machine translation.
\newblock {\em Transactions of the Association for Computational Linguistics}, 8:539--555, 2020.

\bibitem{font2013freesound}
Frederic Font, Gerard Roma, and Xavier Serra.
\newblock Freesound technical demo.
\newblock In {\em Proc. ACM MM}, pages 411--412, 2013.

\bibitem{franzrebdomain}
Carlos Franzreb and Tim Polzehl.
\newblock Domain adversarial training for {G}erman accented speech recognition.
\newblock In {\em Proc. DAGA}, pages 1413--1416, 2023.

\bibitem{ganin2015unsupervised}
Yaroslav Ganin and Victor Lempitsky.
\newblock Unsupervised domain adaptation by backpropagation.
\newblock In {\em Proc. ICML}, pages 1180--1189, 2015.

\bibitem{godfrey1992switchboard}
John~J Godfrey, Edward~C Holliman, and Jane McDaniel.
\newblock {SwitchBoard}: {T}elephone speech corpus for research and development.
\newblock In {\em Proc. ICASSP}, pages 517--520, 1992.

\bibitem{graff2014rats}
David Graff, Kevin Walker, Stephanie~M Strassel, Xiaoyi Ma, Karen Jones, and Ann Sawyer.
\newblock The {RATS} collection: Supporting {HLT} research with degraded audio data.
\newblock In {\em Proc. LREC}, pages 1970--1977, 2014.

\bibitem{hansen2015speaker}
John Hansen and Taufiq Hasan.
\newblock Speaker recognition by machines and humans: {A} tutorial review.
\newblock {\em IEEE Signal Processing Magazine}, 32(6):74--99, 2015.

\bibitem{hemphill1990atis}
Charles~T. Hemphill, John~J. Godfrey, and George~R. Doddington.
\newblock The {ATIS} spoken language systems pilot corpus.
\newblock In {\em Proc. WSNL}, Hidden Valley, 1990.

\bibitem{hernandez2018ted}
Fran{\c{c}}ois Hernandez, Vincent Nguyen, Sahar Ghannay, Natalia Tomashenko, and Yannick Esteve.
\newblock {TED-LIUM 3}: {T}wice as much data and corpus repartition for experiments on speaker adaptation.
\newblock In {\em Proc. SPECOM}, pages 198--208, 2018.

\bibitem{hinton2012deep}
Geoffrey Hinton, Li~Deng, Dong Yu, George~E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara~N. Sainath, et~al.
\newblock Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups.
\newblock {\em IEEE Signal Processing Magazine}, 29(6):82--97, 2012.

\bibitem{hosseini2018multi}
Ehsan Hosseini-Asl, Yingbo Zhou, Caiming Xiong, and Richard Socher.
\newblock A multi-discriminator {CycleGAN} for unsupervised non-parallel speech domain adaptation.
\newblock In {\em Proc. Interspeech}, pages 3758--3762, 2018.

\bibitem{hsu2017unsupervised}
Wei-Ning Hsu, Yu~Zhang, and James Glass.
\newblock Unsupervised domain adaptation for robust speech recognition via variational autoencoder-based data augmentation.
\newblock In {\em Proc. ASRU}, pages 16--23, 2017.

\bibitem{hu2021redat}
Hu~Hu, Xuesong Yang, Zeynab Raeesy, Jinxi Guo, Gokce Keskin, Harish Arsikere, Ariya Rastrow, Andreas Stolcke, and Roland Maas.
\newblock {reDAT}: {A}ccent-invariant representation for end-to-end asr by domain adversarial training with relabeling.
\newblock In {\em Proc. ICASSP}, pages 6408--6412, 2021.

\bibitem{hu2024large}
Yuchen Hu, Chen Chen, Chao-Han~Huck Yang, Ruizhe Li, Chao Zhang, Pin-Yu Chen, and EnSiong Chng.
\newblock Large language models are efficient learners of noise-robust speech recognition.
\newblock {\em arXiv preprint arXiv:2401.10446}, 2024.

\bibitem{huang2021model}
Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu.
\newblock Model adaptation: Historical contrastive learning for unsupervised domain adaptation without source data.
\newblock {\em Advances in Neural Information Processing Systems}, 34:3635--3649, 2021.

\bibitem{huang2023opera}
Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu.
\newblock Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation.
\newblock In {\em Proc. CVPR}, 2024.

\bibitem{huang2021context}
Shuangping Huang, Yu~Luo, Zhenzhou Zhuang, Jin-Gang Yu, Mengchao He, and Yongpan Wang.
\newblock Context-aware selective label smoothing for calibrating sequence recognition model.
\newblock In {\em Proc. ACM MM}, pages 4591--4599, 2021.

\bibitem{hwang2022large}
Dongseong Hwang, Ananya Misra, Zhouyuan Huo, Nikhil Siddhartha, Shefali Garg, David Qiu, Khe~Chai Sim, Trevor Strohman, Fran{\c{c}}oise Beaufays, and Yanzhang He.
\newblock Large-scale {ASR} domain adaptation using self-and semi-supervised learning.
\newblock In {\em Proc. ICASSP}, pages 6627--6631, 2022.

\bibitem{jardri2007self}
Renaud Jardri, Delphine Pins, Maxime Bubrovszky, Pascal Despretz, Jean-Pierre Pruvo, Marc Steinling, and Pierre Thomas.
\newblock Self awareness and speech processing: {A}n {fMRI} study.
\newblock {\em NeuroImage}, 35(4):1645--1653, 2007.

\bibitem{kalgaonkar2015estimating}
Kaustubh Kalgaonkar, Chaojun Liu, Yifan Gong, and Kaisheng Yao.
\newblock Estimating confidence scores on asr results using recurrent neural networks.
\newblock In {\em 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages 4999--5003. IEEE, 2015.

\bibitem{kamath2019transfer}
Uday Kamath, John Liu, James Whitaker, Uday Kamath, John Liu, and James Whitaker.
\newblock Transfer learning: {S}cenarios, self-taught learning, and multitask learning.
\newblock {\em Deep Learning for NLP and Speech Recognition}, pages 463--493, 2019.

\bibitem{kendall2017uncertainties}
Alex Kendall and Yarin Gal.
\newblock What uncertainties do we need in {B}ayesian deep learning for computer vision?
\newblock {\em Advances in Neural Information Processing systems}, 30:1--11, 2017.

\bibitem{kendall2021corpus}
Tyler Kendall and Charlie Farrington.
\newblock The corpus of regional {A}frican {A}merican language. version 2021.07. eugene, or: The online resources for african american language project, 2021.

\bibitem{khurana2021unsupervised}
Sameer Khurana, Niko Moritz, Takaaki Hori, and Jonathan Le~Roux.
\newblock Unsupervised domain adaptation for speech recognition via uncertainty driven self-training.
\newblock In {\em Proc. ICASSP}, pages 6553--6557, 2021.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em Proc. ICLR}, pages 1--13, 2015.

\bibitem{kolen2001field}
John~F. Kolen and Stefan~C. Kremer.
\newblock {\em A field guide to dynamical recurrent networks}.
\newblock John Wiley \& Sons, 2001.

\bibitem{kundu2020universal}
Jogendra~Nath Kundu, Naveen Venkat, R~Venkatesh Babu, et~al.
\newblock Universal source-free domain adaptation.
\newblock In {\em Proc. CVPR}, pages 4544--4553, 2020.

\bibitem{lee2023feature}
JoonHo Lee and Gyemin Lee.
\newblock Feature alignment by uncertainty and self-training for source-free unsupervised domain adaptation.
\newblock {\em Neural Networks}, 161:682--692, 2023.

\bibitem{li2014overview}
Jinyu Li, Li~Deng, Yifan Gong, and Reinhold Haeb-Umbach.
\newblock An overview of noise-robust automatic speech recognition.
\newblock {\em IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 22(4):745--777, 2014.

\bibitem{li2017large}
Jinyu Li, Michael~L Seltzer, Xi~Wang, Rui Zhao, and Yifan Gong.
\newblock Large-scale domain adaptation via teacher-student learning.
\newblock {\em Interspeech 2017}, 2017.

\bibitem{li2001robust}
Qi~Li, Jinsong Zheng, Qiru Zhou, and Chin-Hui Lee.
\newblock Robust, real-time endpoint detector with energy normalization for asr in adverse environments.
\newblock In {\em Proc. ICASSP}, volume~1, pages 233--236, 2001.

\bibitem{li2019bi}
Qiujia Li, PM~Ness, Anton Ragni, and Mark~JF Gales.
\newblock Bi-directional lattice recurrent neural networks for confidence estimation.
\newblock In {\em ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages 6755--6759. IEEE, 2019.

\bibitem{li2021confidence}
Qiujia Li, David Qiu, Yu~Zhang, Bo~Li, Yanzhang He, Phil Woodland, Liangliang Cao, and Trevor Strohman.
\newblock Confidence estimation for attention-based sequence-to-sequence models for speech recognition.
\newblock In {\em Proc. ICASSP}, pages 6388--6392, 2021.

\bibitem{li2021free}
Xianfeng Li, Weijie Chen, Di~Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang.
\newblock A free lunch for unsupervised domain adaptive object detection without source data.
\newblock In {\em Proc. AAAI}, volume~35, pages 8474--8481, 2021.

\bibitem{liang2020we}
Jian Liang, Dapeng Hu, and Jiashi Feng.
\newblock Do we really need to access the source data? {S}ource hypothesis transfer for unsupervised domain adaptation.
\newblock In {\em Proc. ICML}, pages 6028--6039, 2020.

\bibitem{lin2017focal}
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll{\'a}r.
\newblock Focal loss for dense object detection.
\newblock In {\em Proc. ICCV}, pages 2980--2988, 2017.

\bibitem{liu2022source}
Xinyu Liu and Yixuan Yuan.
\newblock A source-free domain adaptive polyp detection framework with style diversification flow.
\newblock {\em IEEE Transactions on Medical Imaging}, 41(7):1897--1908, 2022.

\bibitem{luan2014semi}
Yi~Luan, Daisuke Saito, Yosuke Kashiwagi, Nobuaki Minematsu, and Keikichi Hirose.
\newblock Semi-supervised noise dictionary adaptation for exemplar-based noise robust speech recognition.
\newblock In {\em Proc. ICASSP}, pages 1745--1748, 2014.

\bibitem{ma2022robust}
Han Ma, Qiaoling Zhang, Roubing Tang, Lu~Zhang, and Yubo Jia.
\newblock Robust speech recognition using teacher-student learning domain adaptation.
\newblock {\em IEICE Transactions on Information and Systems}, 105(12):2112--2118, 2022.

\bibitem{malinin2020uncertainty}
Andrey Malinin and Mark Gales.
\newblock Uncertainty estimation in autoregressive structured prediction.
\newblock In {\em Proc. ICLR}, pages 1--31, 2021.

\bibitem{mangu2000finding}
Lidia Mangu, Eric Brill, and Andreas Stolcke.
\newblock Finding consensus in speech recognition: word error minimization and other applications of confusion networks.
\newblock {\em Computer Speech \& Language}, 14(4):373--400, 2000.

\bibitem{marevcek2020multilingual}
David Mare{\v{c}}ek, Hande Celikkanat, Miikka Silfverberg, Vinit Ravishankar, and J{\"o}rg Tiedemann.
\newblock Are multilingual neural machine translation models better at capturing linguistic features?
\newblock {\em The Prague Bulletin of Mathematical Linguistics}, pages 143--162, 2020.

\bibitem{meng2017unsupervised}
Zhong Meng, Zhuo Chen, Vadim Mazalov, Jinyu Li, and Yifan Gong.
\newblock Unsupervised adaptation with domain separation networks for robust speech recognition.
\newblock In {\em Proc. ASRU}, pages 214--221, 2017.

\bibitem{meng2018adversarial}
Zhong Meng, Jinyu Li, Yifan Gong, et~al.
\newblock Adversarial feature-mapping for speech enhancement.
\newblock In {\em Proc. Interspeech}, pages 3259--3263, 2018.

\bibitem{meng2019conditional}
Zhong Meng, Jinyu Li, Yong Zhao, and Yifan Gong.
\newblock Conditional teacher-student learning.
\newblock In {\em Proc. ICASSP}, pages 6445--6449, 2019.

\bibitem{nayak2021mining}
Gaurav~Kumar Nayak, Konda~Reddy Mopuri, Saksham Jain, and Anirban Chakraborty.
\newblock Mining data impressions from deep models as substitute for the unavailable training data.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 44(11):8465--8481, 2021.

\bibitem{panayotov2015librispeech}
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur.
\newblock Librispeech: {A}n {ASR} corpus based on public domain audio books.
\newblock In {\em Proc. ICASSP}, pages 5206--5210, 2015.

\bibitem{peng2024owsm}
Yifan Peng, Jinchuan Tian, William Chen, Siddhant Arora, Brian Yan, Yui Sudo, Muhammad Shakeel, Kwanghee Choi, Jiatong Shi, Xuankai Chang, et~al.
\newblock {OWSM} v3.1: {B}etter and faster open whisper-style speech models based on e-branchformer.
\newblock {\em arXiv preprint arXiv:2401.16658}, 2024.

\bibitem{prasad2021investigation}
Archiki Prasad, Preethi Jyothi, and Rajbabu Velmurugan.
\newblock An investigation of end-to-end models for robust speech recognition.
\newblock In {\em Proc. ICASSP}, pages 6893--6897, 2021.

\bibitem{pullin201517}
Graham Pullin and Shannon Hennig.
\newblock 17 ways to say yes: {T}oward nuanced tone of voice in {AAC} and speech technology.
\newblock {\em Augmentative and Alternative Communication}, 31(2):170--180, 2015.

\bibitem{nv2024canary}
Krishna~C. Puvvada, Piotr Zelasko, He~Huang, Oleksii Hrinchuk, Nithin~Rao Koluguri, Somshubra Majumdar, Elena Rastorgueva, Kunal Dhawan, Zhehuai Chen, Vitaly Larukhin, Jagadeesh Balam, and Boris Ginsburg.
\newblock New standard for speech recognition and translation from the nvidia nemo canary model.
\newblock 2024.

\bibitem{quirk2019investigating}
Randolph Quirk and Jan Svartvik.
\newblock {\em Investigating linguistic acceptability}.
\newblock 2019.

\bibitem{radford2023robust}
Alec Radford, Jong~Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.
\newblock Robust speech recognition via large-scale weak supervision.
\newblock In {\em International Conference on Machine Learning}, pages 28492--28518. PMLR, 2023.

\bibitem{ragni2018confidence}
Anton Ragni, Qiujia Li, Mark~J.F. Gales, and Yongqiang Wang.
\newblock Confidence estimation and deletion prediction using bidirectional recurrent neural networks.
\newblock In {\em Proc. SLT}, pages 204--211, 2018.

\bibitem{raina2007self}
Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer, and Andrew~Y Ng.
\newblock Self-taught learning: {T}ransfer learning from unlabeled data.
\newblock In {\em Proc. ICML}, pages 759--766, 2007.

\bibitem{seigel2011combining}
Matthew~Stephen Seigel, Phil Woodland, et~al.
\newblock Combining information sources for confidence estimation with {CRF} models.
\newblock In {\em Proc. Interspeech}, pages 905--908, 2011.

\bibitem{shi2023uncertain}
Yuxin Shi and Yuhong Sheng.
\newblock Uncertain quantile autoregressive model.
\newblock {\em Communications in Statistics-Simulation and Computation}, pages 1--21, 2023.

\bibitem{stan2021privacy}
Serban Stan and Mohammad Rostami.
\newblock Domain adaptation for the segmentation of confidential medical images.
\newblock {\em arXiv preprint arXiv:2101.00522}, 2021.

\bibitem{sun2017unsupervised}
Sining Sun, Binbin Zhang, Lei Xie, and Yanning Zhang.
\newblock An unsupervised deep domain adaptation approach for robust speech recognition.
\newblock {\em NeuroComputing}, 257:79--87, 2017.

\bibitem{thomas2013deep}
Samuel Thomas, Michael~L. Seltzer, Kenneth Church, and Hynek Hermansky.
\newblock Deep neural network features and semi-supervised training for low resource speech recognition.
\newblock In {\em Proc. ICASSP}, pages 6704--6708, 2013.

\bibitem{vaicenavicius2019evaluating}
Juozas Vaicenavicius, David Widmann, Carl Andersson, Fredrik Lindsten, Jacob Roll, and Thomas Sch{\"o}n.
\newblock Evaluating model calibration in classification.
\newblock In {\em Proc. ICAIS}, pages 3459--3467, 2019.

\bibitem{vig2019analyzing}
Jesse Vig and Yonatan Belinkov.
\newblock Analyzing the structure of attention in a transformer language model.
\newblock In {\em Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP}, pages 63--76, 2019.

\bibitem{vincent2016chime4}
Emmanuel Vincent, Shinji Watanabe, Jon Barker, and Ricard Marxer.
\newblock The 4th chime speech separation and recognition challenge.
\newblock {\em URL: http://spandh. dcs. shef. ac. uk/chime\_challenge/(last accessed on 1 August, 2018)}, 2016.

\bibitem{watanabe2017hybrid}
Shinji Watanabe, Takaaki Hori, Suyoun Kim, John~R Hershey, and Tomoki Hayashi.
\newblock Hybrid ctc/attention architecture for end-to-end speech recognition.
\newblock {\em IEEE Journal of Selected Topics in Signal Processing}, 11(8):1240--1253, 2017.

\bibitem{watanabe2020chime}
Shinji Watanabe, Michael Mandel, Jon Barker, Emmanuel Vincent, Ashish Arora, Xuankai Chang, Sanjeev Khudanpur, Vimal Manohar, Daniel Povey, Desh Raj, et~al.
\newblock Chime-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings.
\newblock In {\em CHiME 2020-6th International Workshop on Speech Processing in Everyday Environments}, 2020.

\bibitem{wotherspoon2021improved}
Shannon Wotherspoon, William Hartmann, Matthew Snover, and Owen Kimball.
\newblock Improved data selection for domain adaptation in {ASR}.
\newblock In {\em Proc. ICASSP}, pages 7018--7022, 2021.

\bibitem{xu2022denoising}
Zhe Xu, Donghuan Lu, Yixin Wang, Jie Luo, Dong Wei, Yefeng Zheng, and Raymond Kai-yu Tong.
\newblock Denoising for relaxing: Unsupervised domain adaptive fundus image segmentation without source data.
\newblock In {\em Proc. MICCAI}, pages 214--224, 2022.

\bibitem{yang2021exploiting}
Shiqi Yang, Joost van~de Weijer, Luis Herranz, Shangling Jui, et~al.
\newblock Exploiting the intrinsic neighborhood structure for source-free domain adaptation.
\newblock {\em Advances in Neural Information Processing Systems}, 34:29393--29405, 2021.

\bibitem{yarowsky1995unsupervised}
David Yarowsky.
\newblock Unsupervised word sense disambiguation rivaling supervised methods.
\newblock In {\em Proc. ACL}, pages 189--196, 1995.

\bibitem{yu2010unsupervised}
Kai Yu, Mark Gales, Lan Wang, and Phil Woodland.
\newblock Unsupervised training and directed manual transcription for {LVCSR}.
\newblock {\em Speech Communication}, 52(7-8):652--663, 2010.

\bibitem{zhang2022does}
Shuai Zhang, Meng Wang, Sijia~Liu Liu, Pin-Yu~Chen Chen, and Jinjun Xiong.
\newblock How does unlabeled data improve generalization in self-training? a one-hidden-layer theoretical analysis.
\newblock In {\em the Tenth International Conference on Learning Representations (ICLR)}, 2022.

\bibitem{zhu2023boosting}
Han Zhu, Gaofeng Cheng, Jindong Wang, Wenxin Hou, Pengyuan Zhang, and Yonghong Yan.
\newblock Boosting cross-domain speech recognition with self-supervision.
\newblock {\em IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 32:471--485, 2023.

\end{thebibliography}
