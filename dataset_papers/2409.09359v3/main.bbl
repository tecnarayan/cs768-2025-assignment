\begin{thebibliography}{10}

\bibitem{alabdulmohsin2022revisiting}
Ibrahim Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai.
\newblock Revisiting neural scaling laws in language and vision.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{batra2021emerging}
Rohit Batra, Le~Song, and Rampi Ramprasad.
\newblock Emerging materials intelligence ecosystems propelled by machine learning.
\newblock {\em Nature Reviews Materials}, 6(8):655--678, 2021.

\bibitem{bowers2023top}
Matthew Bowers, Theo~X Olausson, Lionel Wong, Gabriel Grand, Joshua~B Tenenbaum, Kevin Ellis, and Armando Solar-Lezama.
\newblock Top-down synthesis for library learning.
\newblock {\em Proceedings of the ACM on Programming Languages}, 7(POPL):1182--1213, 2023.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877--1901, 2020.

\bibitem{caballero2023broken}
Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger.
\newblock Broken neural scaling laws.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{chaudhuri2021neurosymbolic}
Swarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, Yisong Yue, et~al.
\newblock Neurosymbolic programming.
\newblock {\em Foundations and Trends{\textregistered} in Programming Languages}, 7(3):158--243, 2021.

\bibitem{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock {\em arXiv preprint arXiv:2107.03374}, 2021.

\bibitem{chen2021spreadsheetcoder}
Xinyun Chen, Petros Maniatis, Rishabh Singh, Charles Sutton, Hanjun Dai, Max Lin, and Denny Zhou.
\newblock Spreadsheetcoder: Formula prediction from semi-structured context.
\newblock In {\em International Conference on Machine Learning}, pages 1661--1672. PMLR, 2021.

\bibitem{llmmutate24}
Mia Chiquier, Utkarsh Mall, and Carl Vondrick.
\newblock Evolving interpretable visual classifiers with large language models, 2024.

\bibitem{cranmer2023interpretable}
Miles Cranmer.
\newblock Interpretable machine learning for science with pysr and symbolicregression. jl.
\newblock {\em arXiv preprint arXiv:2305.01582}, 2023.

\bibitem{cranmer2020discovering}
Miles Cranmer, Alvaro Sanchez-Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel, and Shirley Ho.
\newblock Discovering symbolic models from deep learning with inductive biases.
\newblock In {\em Neural Information Processing Systems}, 2020.

\bibitem{davis2023discovery}
Benjamin~L Davis and Zehao Jin.
\newblock Discovery of a planar black hole mass scaling relation for spiral galaxies.
\newblock {\em The Astrophysical Journal Letters}, 956(1):L22, 2023.

\bibitem{robustfill}
Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel{-}rahman Mohamed, and Pushmeet Kohli.
\newblock Robustfill: Neural program learning under noisy {I/O}.
\newblock In {\em ICML}, 2017.

\bibitem{ec2}
Kevin Ellis, Lucas Morales, Mathias Sabl{\'e}-Meyer, Armando Solar-Lezama, and Josh Tenenbaum.
\newblock Learning libraries of subroutines for neurally--guided {Bayesian} program induction.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 7805--7815, 2018.

\bibitem{ellis2020dreamcoder}
Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lucas Morales, Luke Hewitt, Armando Solar-Lezama, and Joshua~B Tenenbaum.
\newblock Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep {Bayesian} program learning.
\newblock {\em arXiv preprint arXiv:2006.08381}, 2020.

\bibitem{srivastava2023beyond}
Aarohi~Srivastava et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock {\em Transactions on Machine Learning Research}, 2023.

\bibitem{dubey2024llama3herdmodels}
Abhimanyu~Dubey et~al.
\newblock The llama 3 herd of models, 2024.

\bibitem{gerwin1974information}
Donald Gerwin.
\newblock Information processing, data inferences, and scientific generalization.
\newblock {\em Behavioral Science}, 19(5):314--325, 1974.

\bibitem{grand2023lilo}
Gabriel Grand, Lionel Wong, Matthew Bowers, Theo~X Olausson, Muxin Liu, Joshua~B Tenenbaum, and Jacob Andreas.
\newblock Lilo: Learning interpretable libraries by compressing and documenting code.
\newblock {\em arXiv preprint arXiv:2310.19791}, 2023.

\bibitem{grundner2024data}
Arthur Grundner, Tom Beucler, Pierre Gentine, and Veronika Eyring.
\newblock Data-driven equation discovery of a cloud cover parameterization.
\newblock {\em Journal of Advances in Modeling Earth Systems}, 16(3):e2023MS003763, 2024.

\bibitem{gupta2023visual}
Tanmay Gupta and Aniruddha Kembhavi.
\newblock Visual programming: Compositional visual reasoning without training.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 14953--14962, 2023.

\bibitem{hernandez2019fast}
Alberto Hernandez, Adarsh Balasubramanian, Fenglin Yuan, Simon~AM Mason, and Tim Mueller.
\newblock Fast, accurate, and transferable many-body interatomic potentials by symbolic regression.
\newblock {\em npj Computational Materials}, 5(1):112, 2019.

\bibitem{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock {\em arXiv preprint arXiv:2203.15556}, 2022.

\bibitem{hopcroftullman}
John~E. Hopcroft, Rajeev Motwani, and Jeffrey~D. Ullman.
\newblock {\em Introduction to automata theory, languages, and computation, 3rd Edition}.
\newblock Pearson international edition. Addison-Wesley, 2007.

\bibitem{kwon2023efficient}
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody~Hao Yu, Joseph~E. Gonzalez, Hao Zhang, and Ion Stoica.
\newblock Efficient memory management for large language model serving with pagedattention.
\newblock In {\em Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles}, 2023.

\bibitem{la2021contemporary}
William La~Cava, Patryk Orzechowski, Bogdan Burlacu, Fabr{\'\i}cio~Olivetti de~Fran{\c{c}}a, Marco Virgolin, Ying Jin, Michael Kommenda, and Jason~H Moore.
\newblock Contemporary symbolic regression methods and their relative performance.
\newblock {\em arXiv preprint arXiv:2107.14351}, 2021.

\bibitem{lake2015human}
Brenden~M Lake, Ruslan Salakhutdinov, and Joshua~B Tenenbaum.
\newblock Human-level concept learning through probabilistic program induction.
\newblock {\em Science}, 350(6266):1332--1338, 2015.

\bibitem{landajuela2022unified}
Mikel Landajuela, Chak~Shing Lee, Jiachen Yang, Ruben Glatt, Claudio~P Santiago, Ignacio Aravena, Terrell Mundhenk, Garrett Mulcahy, and Brenden~K Petersen.
\newblock A unified framework for deep symbolic regression.
\newblock {\em Advances in Neural Information Processing Systems}, 35:33985--33998, 2022.

\bibitem{Langley1977BACONAP}
Pat Langley.
\newblock Bacon: A production system that discovers empirical laws.
\newblock In {\em International Joint Conference on Artificial Intelligence}, 1977.

\bibitem{lemos2023rediscovering}
Pablo Lemos, Niall Jeffrey, Miles Cranmer, Shirley Ho, and Peter Battaglia.
\newblock Rediscovering orbital mechanics with machine learning.
\newblock {\em Machine Learning: Science and Technology}, 4(4):045002, 2023.

\bibitem{li2023starcoder}
Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al.
\newblock Starcoder: may the source be with you!
\newblock {\em arXiv preprint arXiv:2305.06161}, 2023.

\bibitem{li2022competition}
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R{\'e}mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal~Lago, et~al.
\newblock Competition-level code generation with alphacode.
\newblock {\em Science}, 378(6624):1092--1097, 2022.

\bibitem{makke2024interpretable}
Nour Makke and Sanjay Chawla.
\newblock Interpretable scientific discovery with symbolic regression: a review.
\newblock {\em Artificial Intelligence Review}, 57(1):2, 2024.

\bibitem{merler2024context}
Matteo Merler, Nicola Dainese, and Katsiaryna Haitsiukevich.
\newblock In-context symbolic regression: Leveraging language models for function discovery.
\newblock {\em arXiv preprint arXiv:2404.19094}, 2024.

\bibitem{meyerson2024languagemodelcrossovervariation}
Elliot Meyerson, Mark~J. Nelson, Herbie Bradley, Adam Gaier, Arash Moradi, Amy~K. Hoover, and Joel Lehman.
\newblock Language model crossover: Variation through few-shot prompting, 2024.

\bibitem{murali2017neural}
Vijayaraghavan Murali, Letao Qi, Swarat Chaudhuri, and Chris Jermaine.
\newblock Neural sketch learning for conditional program generation.
\newblock {\em ICLR}, 2018.

\bibitem{dso_srbench_image}
Deep Symbolic~Optimization Organization.
\newblock Srbench symbolic solution.
\newblock \url{https://github.com/dso-org/deep-symbolic-optimization/blob/master/images/srbench_symbolic-solution.png}.
\newblock Accessed: 2024-05-22.

\bibitem{petersen2019deep}
Brenden~K Petersen, Mikel Landajuela, T~Nathan Mundhenk, Claudio~P Santiago, Soo~K Kim, and Joanne~T Kim.
\newblock Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients.
\newblock {\em arXiv preprint arXiv:1912.04871}, 2019.

\bibitem{real2019regularized}
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc~V Le.
\newblock Regularized evolution for image classifier architecture search.
\newblock In {\em Proceedings of the aaai conference on artificial intelligence}, volume~33, pages 4780--4789, 2019.

\bibitem{romera2024mathematical}
Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M~Pawan Kumar, Emilien Dupont, Francisco~JR Ruiz, Jordan~S Ellenberg, Pengming Wang, Omar Fawzi, et~al.
\newblock Mathematical discoveries from program search with large language models.
\newblock {\em Nature}, 625(7995):468--475, 2024.

\bibitem{schmidt2009distilling}
Michael Schmidt and Hod Lipson.
\newblock Distilling free-form natural laws from experimental data.
\newblock {\em Science}, 324(5923):81--85, 2009.

\bibitem{Schmidt2010AgefitnessPO}
Michael~D. Schmidt and Hod Lipson.
\newblock Age-fitness pareto optimization.
\newblock In {\em Annual Conference on Genetic and Evolutionary Computation}, 2010.

\bibitem{shah2020learning}
Ameesh Shah, Eric Zhan, Jennifer~J Sun, Abhinav Verma, Yisong Yue, and Swarat Chaudhuri.
\newblock Learning differentiable programs with admissible neural heuristics.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{shin2019program}
Richard Shin, Miltiadis Allamanis, Marc Brockschmidt, and Oleksandr Polozov.
\newblock Program synthesis and semantic parsing with learned code idioms.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 10825--10835, 2019.

\bibitem{shojaee2024llm}
Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir~Barati Farimani, and Chandan~K Reddy.
\newblock Llm-sr: Scientific equation discovery via programming with large language models.
\newblock {\em arXiv preprint arXiv:2404.18400}, 2024.

\bibitem{snell2024scalingllmtesttimecompute}
Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar.
\newblock Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024.

\bibitem{gplearn}
Trevor Stephens.
\newblock gplearn: Genetic programming in python, with a scikit-learn inspired api, 2024.
\newblock Accessed: 2024-05-22.

\bibitem{suris2023vipergpt}
D{\'\i}dac Sur{\'\i}s, Sachit Menon, and Carl Vondrick.
\newblock Vipergpt: Visual inference via python execution for reasoning.
\newblock {\em arXiv preprint arXiv:2303.08128}, 2023.

\bibitem{udrescu2020ai}
Silviu-Marian Udrescu and Max Tegmark.
\newblock Ai feynman: A physics-inspired method for symbolic regression.
\newblock {\em Science Advances}, 6(16):eaay2631, 2020.

\bibitem{verstyuk2022machine}
Sergiy Verstyuk and Michael~R Douglas.
\newblock Machine learning the gravity equation for international trade.
\newblock {\em Available at SSRN 4053795}, 2022.

\bibitem{virgolin2020machine}
Marco Virgolin, Ziyuan Wang, Tanja Alderliesten, and Peter~AN Bosman.
\newblock Machine learning for the prediction of pseudorealistic pediatric abdominal phantoms for radiation dose reconstruction.
\newblock {\em Journal of Medical Imaging}, 7(4):046501--046501, 2020.

\bibitem{wigner1990unreasonable}
Eugene~P Wigner.
\newblock The unreasonable effectiveness of mathematics in the natural sciences.
\newblock In {\em Mathematics and science}, pages 291--306. World Scientific, 1990.

\bibitem{wong2021leveraging}
Catherine Wong, Kevin~M Ellis, Joshua Tenenbaum, and Jacob Andreas.
\newblock Leveraging language to learn program abstractions and search heuristics.
\newblock In {\em International conference on machine learning}, pages 11193--11204. PMLR, 2021.

\bibitem{zelikman2023parsel}
Eric Zelikman, Qian Huang, Gabriel Poesia, Noah Goodman, and Nick Haber.
\newblock Parsel: Algorithmic reasoning with language models by composing decompositions.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\end{thebibliography}
