\begin{thebibliography}{68}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alon and Yahav(2021)]{alon2021on}
Uri Alon and Eran Yahav.
\newblock On the bottleneck of graph neural networks and its practical
  implications.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Andres et~al.(2016)Andres, Deuschel, and Slowik]{andres2016heat}
Sebastian Andres, Jean-Dominique Deuschel, and Martin Slowik.
\newblock Heat kernel estimates for random walks with degenerate weights.
\newblock \emph{Electronic Journal of Probability}, 21:\penalty0 1--21, 2016.

\bibitem[Beaini et~al.(2021)Beaini, Passaro, L{\'e}tourneau, Hamilton, Corso,
  and Li{\`o}]{beaini2021directional_dgn}
Dominique Beaini, Saro Passaro, Vincent L{\'e}tourneau, Will Hamilton, Gabriele
  Corso, and Pietro Li{\`o}.
\newblock Directional graph networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  748--758. PMLR, 2021.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and
  Cohan]{DBLP:journals/corr/abs-2004-05150}
Iz~Beltagy, Matthew~E. Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock \emph{CoRR}, abs/2004.05150, 2020.

\bibitem[Bodnar et~al.(2021)Bodnar, Frasca, Otter, Wang, Lio, Montufar, and
  Bronstein]{bodnar2021weisfeiler_CIN}
Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido~F
  Montufar, and Michael Bronstein.
\newblock Weisfeiler and {L}ehman go cellular: {CW} networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 2625--2640, 2021.

\bibitem[Bouritsas et~al.(2022)Bouritsas, Frasca, Zafeiriou, and
  Bronstein]{bouritsas2022improving_GSN}
Giorgos Bouritsas, Fabrizio Frasca, Stefanos~P Zafeiriou, and Michael
  Bronstein.
\newblock Improving graph neural network expressivity via subgraph isomorphism
  counting.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2022.

\bibitem[Bresson and Laurent(2017)]{bresson2017GatedGCN}
Xavier Bresson and Thomas Laurent.
\newblock {Residual Gated Graph ConvNets}.
\newblock \emph{arXiv:1711.07553}, 2017.

\bibitem[Chelombiev et~al.(2021)Chelombiev, Justus, Orr, Dietrich, Gressmann,
  Koliousis, and Luschi]{chelombiev2021groupbert}
Ivan Chelombiev, Daniel Justus, Douglas Orr, Anastasia Dietrich, Frithjof
  Gressmann, Alexandros Koliousis, and Carlo Luschi.
\newblock Groupbert: Enhanced transformer architecture with efficient grouped
  structures.
\newblock \emph{arXiv:2106.05822}, 2021.

\bibitem[Chen et~al.(2022)Chen, O'Bray, and Borgwardt]{chen2022SAT}
Dexiong Chen, Leslie O'Bray, and Karsten Borgwardt.
\newblock Structure-aware transformer for graph representation learning.
\newblock \emph{Proceedings of the 39th International Conference on Machine
  Learning}, 2022.

\bibitem[Chen et~al.(2019)Chen, Chen, Villar, and Bruna]{chen2019equivalence}
Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna.
\newblock On the equivalence between graph isomorphism testing and function
  approximation with gnns.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Choromanski et~al.(2021{\natexlab{a}})Choromanski, Lin, Chen, Zhang,
  Sehanobish, Likhosherstov, Parker-Holder, Sarlos, Weller, and
  Weingarten]{choromanski2021blocktoeplitz}
Krzysztof Choromanski, Han Lin, Haoxian Chen, Tianyi Zhang, Arijit Sehanobish,
  Valerii Likhosherstov, Jack Parker-Holder, Tamas Sarlos, Adrian Weller, and
  Thomas Weingarten.
\newblock From block-{T}oeplitz matrices to differential equations on graphs:
  towards a general theory for scalable masked transformers.
\newblock \emph{arXiv:2107.07999}, 2021{\natexlab{a}}.

\bibitem[Choromanski et~al.(2021{\natexlab{b}})Choromanski, Likhosherstov,
  Dohan, Song, Gane, Sarl{\'{o}}s, Hawkins, Davis, Mohiuddin, Kaiser, Belanger,
  Colwell, and Weller]{DBLP:conf/iclr/ChoromanskiLDSG21}
Krzysztof~Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tam{\'{a}}s Sarl{\'{o}}s, Peter Hawkins, Jared~Quincy Davis,
  Afroz Mohiuddin, Lukasz Kaiser, David~Benjamin Belanger, Lucy~J. Colwell, and
  Adrian Weller.
\newblock Rethinking attention with performers.
\newblock In \emph{9th International Conference on Learning Representations},
  2021{\natexlab{b}}.

\bibitem[Corso et~al.(2020)Corso, Cavalleri, Beaini, Li{\`o}, and
  Veli{\v{c}}kovi{\'c}]{corso2020principal_pna}
Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li{\`o}, and Petar
  Veli{\v{c}}kovi{\'c}.
\newblock Principal neighbourhood aggregation for graph nets.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 13260--13271, 2020.

\bibitem[Dwivedi and Bresson(2020)]{dwivedi2020generalization}
Vijay~Prakash Dwivedi and Xavier Bresson.
\newblock A generalization of transformer networks to graphs.
\newblock \emph{arXiv:2012.09699}, 2020.

\bibitem[Dwivedi et~al.(2020)Dwivedi, Joshi, Laurent, Bengio, and
  Bresson]{dwivedi2020benchmarking}
Vijay~Prakash Dwivedi, Chaitanya~K Joshi, Thomas Laurent, Yoshua Bengio, and
  Xavier Bresson.
\newblock Benchmarking graph neural networks.
\newblock \emph{arXiv:2003.00982}, 2020.

\bibitem[Dwivedi et~al.(2022{\natexlab{a}})Dwivedi, Luu, Laurent, Bengio, and
  Bresson]{dwivedi2022LPE}
Vijay~Prakash Dwivedi, Anh~Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier
  Bresson.
\newblock Graph neural networks with learnable structural and positional
  representations.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{a}}.

\bibitem[Dwivedi et~al.(2022{\natexlab{b}})Dwivedi, Rampášek, Galkin, Parviz,
  Wolf, Luu, and Beaini]{dwivedi2022LRGB}
Vijay~Prakash Dwivedi, Ladislav Rampášek, Mikhail Galkin, Ali Parviz, Guy
  Wolf, Anh~Tuan Luu, and Dominique Beaini.
\newblock Long range graph benchmark.
\newblock \emph{Neural Information Processing Systems (NeurIPS 2022), Track on
  Datasets and Benchmarks}, 2022{\natexlab{b}}.

\bibitem[d’Ascoli et~al.(2021)d’Ascoli, Touvron, Leavitt, Morcos, Biroli,
  and Sagun]{dascoli2021convit}
St{\'e}phane d’Ascoli, Hugo Touvron, Matthew~L Leavitt, Ari~S Morcos, Giulio
  Biroli, and Levent Sagun.
\newblock Convit: Improving vision transformers with soft convolutional
  inductive biases.
\newblock In \emph{International Conference on Machine Learning}, pages
  2286--2296. PMLR, 2021.

\bibitem[Ertl and Schuffenhauer(2009)]{ertl2009estimation_sascore}
Peter Ertl and Ansgar Schuffenhauer.
\newblock Estimation of synthetic accessibility score of drug-like molecules
  based on molecular complexity and fragment contributions.
\newblock \emph{Journal of cheminformatics}, 1\penalty0 (1):\penalty0 1--11,
  2009.

\bibitem[Fey and Lenssen(2019)]{FeyLenssen2019PyG}
Matthias Fey and Jan~Eric Lenssen.
\newblock Fast graph representation learning with {PyTorch Geometric}.
\newblock In \emph{ICLR Workshop on Representation Learning on Graphs and
  Manifolds}, 2019.

\bibitem[Freitas et~al.(2021)Freitas, Dong, Neil, and Chau]{freitas2021malnet}
Scott Freitas, Yuxiao Dong, Joshua Neil, and Duen~Horng Chau.
\newblock A large-scale database for graph representation learning.
\newblock In \emph{35th Conference on Neural Information Processing Systems:
  Datasets and Benchmarks Track}, 2021.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and
  Dahl]{gilmer2017neural}
Justin Gilmer, Samuel~S Schoenholz, Patrick~F Riley, Oriol Vinyals, and
  George~E Dahl.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{International conference on machine learning}, pages
  1263--1272. PMLR, 2017.

\bibitem[Gu et~al.(2022)Gu, Goel, and Re]{gu2022efficiently}
Albert Gu, Karan Goel, and Christopher Re.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Guo et~al.(2021)Guo, Han, Wu, Xu, Tang, Xu, and Wang]{guo2021cmt}
Jianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe
  Wang.
\newblock {CMT}: Convolutional neural networks meet vision transformers.
\newblock \emph{arXiv:2107.06263}, 2021.

\bibitem[Han et~al.(2022)Han, Wang, Chen, Chen, Guo, Liu, Tang, Xiao, Xu, Xu,
  et~al.]{han2022survey_vision_transformer}
Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu,
  Yehui Tang, An~Xiao, Chunjing Xu, Yixing Xu, et~al.
\newblock A survey on vision transformer.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2022.

\bibitem[Hu et~al.(2019)Hu, Liu, Gomes, Zitnik, Liang, Pande, and
  Leskovec]{hu2019strategies_GINE}
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande,
  and Jure Leskovec.
\newblock Strategies for pre-training graph neural networks.
\newblock \emph{arXiv:1905.12265}, 2019.

\bibitem[Hu et~al.(2020)Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta, and
  Leskovec]{hu2020ogb}
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,
  Michele Catasta, and Jure Leskovec.
\newblock Open {Graph} {Benchmark}: {Datasets} for {Machine} {Learning} on
  {Graphs}.
\newblock \emph{34th Conference on Neural Information Processing Systems},
  2020.

\bibitem[Hu et~al.(2021)Hu, Fey, Ren, Nakata, Dong, and Leskovec]{hu2021ogblsc}
Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure
  Leskovec.
\newblock {OGB}-{LSC}: A large-scale challenge for machine learning on graphs.
\newblock In \emph{35th Conference on Neural Information Processing Systems:
  Datasets and Benchmarks Track}, 2021.

\bibitem[Hussain et~al.(2022)Hussain, Zaki, and Subramanian]{hussain2022EGT}
Md~Shamim Hussain, Mohammed~J Zaki, and Dharmashankar Subramanian.
\newblock Global self-attention as a replacement for graph convolution.
\newblock In \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, pages 655--665, 2022.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batchnorm}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pages
  448--456. PMLR, 2015.

\bibitem[Jain et~al.(2021)Jain, Wu, Wright, Mirhoseini, Gonzalez, and
  Stoica]{jain2021graphtrans}
Paras Jain, Zhanghao Wu, Matthew Wright, Azalia Mirhoseini, Joseph~E Gonzalez,
  and Ion Stoica.
\newblock Representing long-range context for graph neural networks with global
  attention.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Kalyan et~al.(2021)Kalyan, Rajasekharan, and
  Sangeetha]{kalyan2021ammus_transformer_survey}
Katikapalli~Subramanyam Kalyan, Ajit Rajasekharan, and Sivanesan Sangeetha.
\newblock Ammus: A survey of transformer-based pretrained models in natural
  language processing.
\newblock \emph{arXiv:2108.05542}, 2021.

\bibitem[Kipf and Welling(2016)]{kipf2016semi}
Thomas~N Kipf and Max Welling.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock \emph{arXiv:1609.02907}, 2016.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and
  Levskaya]{DBLP:conf/iclr/KitaevKL20}
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In \emph{8th International Conference on Learning Representations},
  2020.

\bibitem[Koutis and Le(2019)]{koutis2019spectral}
Ioannis Koutis and Huong Le.
\newblock Spectral modification of graphs for improved spectral clustering.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Kreuzer et~al.(2021)Kreuzer, Beaini, Hamilton, L{\'e}tourneau, and
  Tossou]{kreuzer2021rethinking}
Devin Kreuzer, Dominique Beaini, William~L. Hamilton, Vincent L{\'e}tourneau,
  and Prudencio Tossou.
\newblock Rethinking graph transformers with spectral attention.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Kurin et~al.(2020)Kurin, Igl, Rockt{\"a}schel, Boehmer, and
  Whiteson]{kurin2020my_body}
Vitaly Kurin, Maximilian Igl, Tim Rockt{\"a}schel, Wendelin Boehmer, and Shimon
  Whiteson.
\newblock My body is a cage: the role of morphology in graph-based incompatible
  control.
\newblock \emph{arXiv:2010.01856}, 2020.

\bibitem[Li et~al.(2020)Li, Wang, Wang, and Leskovec]{li2020distance}
Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec.
\newblock Distance encoding: Design provably more powerful neural networks for
  graph representation learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 4465--4478, 2020.

\bibitem[Lim et~al.(2022)Lim, Robinson, Zhao, Smidt, Sra, Maron, and
  Jegelka]{lim2022sign}
Derek Lim, Joshua Robinson, Lingxiao Zhao, Tess Smidt, Suvrit Sra, Haggai
  Maron, and Stefanie Jegelka.
\newblock Sign and basis invariant networks for spectral graph representation
  learning.
\newblock \emph{arXiv:2202.13013}, 2022.

\bibitem[Liu et~al.(2022)Liu, Yu, Liao, Li, Lin, Liu, and
  Dustdar]{liu2022pyraformer}
Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex~X. Liu, and
  Schahram Dustdar.
\newblock Pyraformer: Low-complexity pyramidal attention for long-range time
  series modeling and forecasting.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Loukas(2020)]{loukas2020graph}
Andreas Loukas.
\newblock What graph neural networks cannot learn: depth vs width.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Maron et~al.(2019)Maron, Ben-Hamu, Serviansky, and
  Lipman]{maron2019provably}
Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman.
\newblock Provably powerful graph networks.
\newblock \emph{arXiv:1905.11136}, 2019.

\bibitem[Mialon et~al.(2021)Mialon, Chen, Selosse, and
  Mairal]{mialon2021graphit}
Gr{\'e}goire Mialon, Dexiong Chen, Margot Selosse, and Julien Mairal.
\newblock {GraphiT}: Encoding graph structure in transformers.
\newblock \emph{arXiv:2106.05667}, 2021.

\bibitem[Morris et~al.(2019)Morris, Ritzert, Fey, Hamilton, Lenssen, Rattan,
  and Grohe]{morris2019}
Christopher Morris, Martin Ritzert, Matthias Fey, William~L. Hamilton, Jan~Eric
  Lenssen, Gaurav Rattan, and Martin Grohe.
\newblock Weisfeiler and {L}eman go neural: Higher-order graph neural networks.
\newblock In \emph{The Thirty-Third {AAAI} Conference on Artificial
  Intelligence}, pages 4602--4609. {AAAI} Press, 2019.

\bibitem[Murphy et~al.(2019)Murphy, Srinivasan, Rao, and
  Ribeiro]{murphy2019relational}
Ryan Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro.
\newblock Relational pooling for graph representations.
\newblock In \emph{International Conference on Machine Learning}, pages
  4663--4673. PMLR, 2019.

\bibitem[Oono and Suzuki(2020)]{Oono2020Graph}
Kenta Oono and Taiji Suzuki.
\newblock Graph neural networks exponentially lose expressive power for node
  classification.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Park et~al.(2022)Park, Chang, Lee, Kim, and won Hwang]{park2022GRPE}
Wonpyo Park, Woonggi Chang, Donggeon Lee, Juntae Kim, and Seung won Hwang.
\newblock {GRPE}: Relative positional encoding for graph transformer.
\newblock \emph{arXiv:22201.12787}, 2022.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (140):\penalty0 1--67, 2020.

\bibitem[Sato(2020)]{sato2020survey}
Ryoma Sato.
\newblock A survey on the expressive power of graph neural networks.
\newblock \emph{arXiv:2003.04078}, 2020.

\bibitem[Shi et~al.(2022)Shi, Zheng, Ke, Shen, You, He, Luo, Liu, He, and
  Liu]{shi2022benchgraphormer}
Yu~Shi, Shuxin Zheng, Guolin Ke, Yifei Shen, Jiacheng You, Jiyan He, Shengjie
  Luo, Chang Liu, Di~He, and Tie-Yan Liu.
\newblock Benchmarking graphormer on large-scale molecular modeling datasets.
\newblock \emph{arXiv:2203.04810}, 2022.

\bibitem[Tay et~al.(2021)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang,
  Ruder, and Metzler]{tay2021long}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Toenshoff et~al.(2021)Toenshoff, Ritzert, Wolf, and
  Grohe]{toenshoff2021CRaWl}
Jan Toenshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe.
\newblock Graph learning with 1d convolutions on random walks.
\newblock \emph{arXiv:2102.08786}, 2021.

\bibitem[Topping et~al.(2021)Topping, Di~Giovanni, Chamberlain, Dong, and
  Bronstein]{topping2021understanding_ricci}
Jake Topping, Francesco Di~Giovanni, Benjamin~Paul Chamberlain, Xiaowen Dong,
  and Michael~M Bronstein.
\newblock Understanding over-squashing and bottlenecks on graphs via curvature.
\newblock \emph{arXiv:2111.14522}, 2021.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Veličković et~al.(2018)Veličković, Cucurull, Casanova, Romero,
  Liò, and Bengio]{velickovic2018GAT}
Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
  Liò, and Yoshua Bengio.
\newblock Graph attention networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Wang et~al.(2022)Wang, Yin, Zhang, and Li]{wang2022equivstable}
Haorui Wang, Haoteng Yin, Muhan Zhang, and Pan Li.
\newblock Equivariant and stable positional encoding for more powerful graph
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{Wang2020LinformerSW}
Sinong Wang, Belinda~Z. Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv:2006.04768}, 2020.

\bibitem[Weisfeiler and Leman(1968)]{weisfeiler1968reduction}
Boris Weisfeiler and Andrei Leman.
\newblock The reduction of a graph to canonical form and the algebra which
  appears therein.
\newblock \emph{NTI, Series}, 2\penalty0 (9):\penalty0 12--16, 1968.

\bibitem[Wildman and Crippen(1999)]{wildman1999prediction_logp}
Scott~A Wildman and Gordon~M Crippen.
\newblock Prediction of physicochemical parameters by atomic contributions.
\newblock \emph{Journal of chemical information and computer sciences},
  39\penalty0 (5):\penalty0 868--873, 1999.

\bibitem[Xu et~al.(2019)Xu, Hu, Leskovec, and Jegelka]{xu2018how}
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
\newblock How powerful are graph neural networks?
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Yang et~al.(2022)Yang, Wang, Shen, Qi, and Yin]{yang2022ExpC}
Mingqi Yang, Renjian Wang, Yanming Shen, Heng Qi, and Baocai Yin.
\newblock Breaking the expression bottleneck of graph neural networks.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 2022.

\bibitem[Ying et~al.(2021{\natexlab{a}})Ying, Cai, Luo, Zheng, Ke, He, Shen,
  and Liu]{ying2021graphormer}
Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di~He,
  Yanming Shen, and Tie-Yan Liu.
\newblock Do transformers really perform badly for graph representation?
\newblock In \emph{Advances in Neural Information Processing Systems},
  2021{\natexlab{a}}.

\bibitem[Ying et~al.(2021{\natexlab{b}})Ying, Yang, Zheng, Ke, Luo, Cai, Wu,
  Wang, Shen, and He]{ying2021first}
Chengxuan Ying, Mingqi Yang, Shuxin Zheng, Guolin Ke, Shengjie Luo, Tianle Cai,
  Chenglin Wu, Yuxin Wang, Yanming Shen, and Di~He.
\newblock First place solution of {KDD Cup} 2021 \& {OGB} large-scale challenge
  graph prediction track.
\newblock \emph{arXiv:2106.08279}, 2021{\natexlab{b}}.

\bibitem[You et~al.(2020)You, Ying, and Leskovec]{you2020design}
Jiaxuan You, Rex Ying, and Jure Leskovec.
\newblock Design space for graph neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Onta{\~{n}}{\'{o}}n, Pham, Ravula, Wang, Yang, and
  Ahmed]{DBLP:conf/nips/ZaheerGDAAOPRWY20}
Manzil Zaheer, Guru Guruganesh, Kumar~Avinava Dubey, Joshua Ainslie, Chris
  Alberti, Santiago Onta{\~{n}}{\'{o}}n, Philip Pham, Anirudh Ravula, Qifan
  Wang, Li~Yang, and Amr Ahmed.
\newblock {Big Bird}: Transformers for longer sequences.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Li, Xia, Wang, and Jin]{zhang2021labeling}
Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Jin.
\newblock Labeling trick: A theory of using graph neural networks for
  multi-node representation learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Zhao et~al.(2022)Zhao, Jin, Akoglu, and Shah]{zhao2021stars}
Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah.
\newblock From stars to subgraphs: Uplifting any {GNN} with local structure
  awareness.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\end{thebibliography}
