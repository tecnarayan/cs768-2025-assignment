\begin{thebibliography}{10}

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under gradient descent.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{gur2018gradient}
Guy Gur-Ari, Daniel~A Roberts, and Ethan Dyer.
\newblock Gradient descent happens in a tiny subspace.
\newblock {\em arXiv preprint arXiv:1812.04754}, 2018.

\bibitem{brokman2024enhancing}
Jonathan Brokman, Roy Betser, Rotem Turjeman, Tom Berkov, Ido Cohen, and Guy Gilboa.
\newblock Enhancing neural training via a correlated dynamics model.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{chizat2019lazy}
Lenaic Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{woodworth2020kernel}
Blake Woodworth, Suriya Gunasekar, Jason~D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In {\em Conference on Learning Theory}, pages 3635--3673. PMLR, 2020.

\bibitem{chandramoorthy2022generalization}
Nisha Chandramoorthy, Andreas Loukas, Khashayar Gatmiry, and Stefanie Jegelka.
\newblock On the generalization of learning algorithms that do not converge.
\newblock {\em Advances in Neural Information Processing Systems}, 35:34241--34257, 2022.

\bibitem{kunin2023limiting}
Daniel Kunin, Javier Sagastuy-Brena, Lauren Gillespie, Eshed Margalit, Hidenori Tanaka, Surya Ganguli, and Daniel~LK Yamins.
\newblock The limiting dynamics of sgd: Modified loss, phase-space oscillations, and anomalous diffusion.
\newblock {\em Neural Computation}, 36(1):151--174, 2023.

\bibitem{achille2018critical}
Alessandro Achille, Matteo Rovere, and Stefano Soatto.
\newblock Critical learning periods in deep networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{zilly2021plasticity}
Julian Zilly, Alessandro Achille, Andrea Censi, and Emilio Frazzoli.
\newblock On plasticity, invariance, and mutually frozen weights in sequential task learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34:12386--12399, 2021.

\bibitem{frankle2020linear}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel Roy, and Michael Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In {\em International Conference on Machine Learning}, pages 3259--3269. PMLR, 2020.

\bibitem{wilson1983renormalization}
Kenneth~G Wilson.
\newblock The renormalization group and critical phenomena.
\newblock {\em Reviews of Modern Physics}, 55(3):583, 1983.

\bibitem{guckenheimer2013nonlinear}
John Guckenheimer and Philip Holmes.
\newblock {\em Nonlinear oscillations, dynamical systems, and bifurcations of vector fields}, volume~42.
\newblock Springer Science \& Business Media, 2013.

\bibitem{mermin1979topological}
N~David Mermin.
\newblock The topological theory of defects in ordered media.
\newblock {\em Reviews of Modern Physics}, 51(3):591, 1979.

\bibitem{hecht1990algebraic}
Robert Hecht-Nielsen.
\newblock On the algebraic structure of feedforward network weight spaces.
\newblock In {\em Advanced Neural Computers}, pages 129--135. Elsevier, 1990.

\bibitem{entezari2022the}
Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur.
\newblock The role of permutation invariance in linear mode connectivity of neural networks.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{ainsworth2023git}
Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa.
\newblock Git re-basin: Merging models modulo permutation symmetries.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{wiggins1996introduction}
Stephen Wiggins.
\newblock {\em Introduction to applied nonlinear dynamical systems and Chaos}.
\newblock Springer, 1996.

\bibitem{skufca2008concept}
Joseph~D Skufca and Erik~M Bollt.
\newblock A concept of homeomorphic defect for defining mostly conjugate dynamical systems.
\newblock {\em Chaos: An Interdisciplinary Journal of Nonlinear Science}, 18(1), 2008.

\bibitem{koopman1931hamiltonian}
Bernard~O Koopman.
\newblock Hamiltonian systems and transformation in hilbert space.
\newblock {\em Proceedings of the National Academy of Sciences}, 17(5):315--318, 1931.

\bibitem{mezic2005spectral}
Igor Mezi{\'c}.
\newblock Spectral properties of dynamical systems, model reduction and decompositions.
\newblock {\em Nonlinear Dynamics}, 41:309--325, 2005.

\bibitem{budivsic2012applied}
Marko Budi{\v{s}}i{\'c}, Ryan Mohr, and Igor Mezi{\'c}.
\newblock Applied koopmanism.
\newblock {\em Chaos: An Interdisciplinary Journal of Nonlinear Science}, 22(4), 2012.

\bibitem{mezic2020spectrum}
Igor Mezi{\'c}.
\newblock Spectrum of the koopman operator, spectral expansions in functional spaces, and state-space geometry.
\newblock {\em Journal of Nonlinear Science}, 30(5):2091--2145, 2020.

\bibitem{amid2020reparameterizing}
Ehsan Amid and Manfred~KK Warmuth.
\newblock Reparameterizing mirror descent as gradient descent.
\newblock {\em Advances in Neural Information Processing Systems}, 33:8430--8439, 2020.

\bibitem{amid2020winnowing}
Ehsan Amid and Manfred~K Warmuth.
\newblock Winnowing with gradient descent.
\newblock In {\em Conference on Learning Theory}, pages 163--182. PMLR, 2020.

\bibitem{ghai2022non}
Udaya Ghai, Zhou Lu, and Elad Hazan.
\newblock Non-convex online learning via algorithmic equivalence.
\newblock {\em Advances in Neural Information Processing Systems}, 35:22161--22172, 2022.

\bibitem{frankle2020early}
Jonathan Frankle, David~J. Schwab, and Ari~S. Morcos.
\newblock The early phase of neural network training.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{power2022grokking}
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
\newblock Grokking: Generalization beyond overfitting on small algorithmic datasets.
\newblock {\em arXiv preprint arXiv:2201.02177}, 2022.

\bibitem{nanda2022progress}
Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt.
\newblock Progress measures for grokking via mechanistic interpretability.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2022.

\bibitem{saad1995exact}
David Saad and Sara~A Solla.
\newblock Exact solution for on-line learning in multilayer neural networks.
\newblock {\em Physical Review Letters}, 74(21):4337, 1995.

\bibitem{goldt2019dynamics}
Sebastian Goldt, Madhu Advani, Andrew~M Saxe, Florent Krzakala, and Lenka Zdeborov{\'a}.
\newblock Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{dogra2020optimizing}
Akshunna~S Dogra and William~T Redman.
\newblock Optimizing neural networks via koopman operator theory.
\newblock {\em Advances in Neural Information Processing Systems}, 33:2087--2097, 2020.

\bibitem{tano2020accelerating}
Mauricio~E Tano, Gavin~D Portwood, and Jean~C Ragusa.
\newblock Accelerating training in artificial neural networks with dynamic mode decomposition.
\newblock {\em arXiv preprint arXiv:2006.14371}, 2020.

\bibitem{luo2024quack}
Di~Luo, Jiayu Shen, Rumen Dangovski, and Marin Soljacic.
\newblock Quack: Accelerating gradient-based quantum optimization with koopman operator learning.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{vsimanek2022learning}
Petr {\v{S}}im{\'a}nek, Daniel Va{\v{s}}ata, and Pavel Kord{\'\i}k.
\newblock Learning to optimize with dynamic mode decomposition.
\newblock In {\em 2022 International Joint Conference on Neural Networks (IJCNN)}, pages 1--8. IEEE, 2022.

\bibitem{hataya2024glocal}
Ryuichiro Hataya and Yoshinobu Kawahara.
\newblock Glocal hypergradient estimation with koopman operator.
\newblock {\em arXiv preprint arXiv:2402.02741}, 2024.

\bibitem{mohr2021applications}
Ryan Mohr, Maria Fonoberova, Iva Manojlovic, Aleksandr Andrejcuk, Zlatko Drmac, Yannis~G Kevrekidis, and Igor Mezic.
\newblock Applications of koopman mode analysis to neural networks.
\newblock In {\em AAAI Spring Symposium: MLPS}, 2021.

\bibitem{redman2022an}
William~T Redman, Maria Fonoberova, Ryan Mohr, Yannis Kevrekidis, and Igor Mezic.
\newblock An operator theoretic view on pruning deep neural networks.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{wang2023dynamic}
Fanqi Wang, Landon Harris, Weisheng Tang, Hairong Qi, Dan Wilson, and Igor Mezic.
\newblock Dynamic training guided by training dynamics.
\newblock 2023.

\bibitem{mauroy2013isostables}
Alexandre Mauroy, Igor Mezi{\'c}, and Jeff Moehlis.
\newblock Isostables, isochrons, and koopman spectrum for the action--angle representation of stable fixed point dynamics.
\newblock {\em Physica D: Nonlinear Phenomena}, 261:19--30, 2013.

\bibitem{ziyin2023symmetry}
Liu Ziyin.
\newblock Symmetry leads to structured constraint of learning.
\newblock {\em arXiv preprint arXiv:2309.16932}, 2023.

\bibitem{vcrnjaric2020koopman}
Nelida {\v{C}}rnjari{\'c}-{\v{Z}}ic, Senka Ma{\'c}e{\v{s}}i{\'c}, and Igor Mezi{\'c}.
\newblock Koopman operator spectrum for random dynamical systems.
\newblock {\em Journal of Nonlinear Science}, 30:2007--2056, 2020.

\bibitem{brunton2016extracting}
Bingni~W Brunton, Lise~A Johnson, Jeffrey~G Ojemann, and J~Nathan Kutz.
\newblock Extracting spatial--temporal coherent patterns in large-scale neural recordings using dynamic mode decomposition.
\newblock {\em Journal of neuroscience methods}, 258:1--15, 2016.

\bibitem{brunton2017chaos}
Steven~L Brunton, Bingni~W Brunton, Joshua~L Proctor, Eurika Kaiser, and J~Nathan Kutz.
\newblock Chaos as an intermittently forced linear system.
\newblock {\em Nature communications}, 8(1):19, 2017.

\bibitem{avila2020data}
Allan~M Avila and Igor Mezi{\'c}.
\newblock Data-driven analysis and forecasting of highway traffic dynamics.
\newblock {\em Nature Communications}, 11(1):1--16, 2020.

\bibitem{hogg2020exponentially}
James Hogg, Maria Fonoberova, and Igor Mezi{\'c}.
\newblock Exponentially decaying modes and long-term prediction of sea ice concentration using koopman mode decomposition.
\newblock {\em Scientific reports}, 10(1):16313, 2020.

\bibitem{mezic2024koopman}
Igor Mezi{\'c}, Zlatko Drma{\v{c}}, Nelida {\v{C}}rnjari{\'c}, Senka Ma{\'c}e{\v{s}}i{\'c}, Maria Fonoberova, Ryan Mohr, Allan~M Avila, Iva Manojlovi{\'c}, and Aleksandr Andrej{\v{c}}uk.
\newblock A koopman operator-based prediction algorithm and its application to covid-19 pandemic and influenza cases.
\newblock {\em Scientific reports}, 14(1):5788, 2024.

\bibitem{schmid2010dynamic}
Peter~J Schmid.
\newblock Dynamic mode decomposition of numerical and experimental data.
\newblock {\em Journal of fluid mechanics}, 656:5--28, 2010.

\bibitem{rowley2009spectral}
Clarence~W Rowley, Igor Mezi{\'c}, Shervin Bagheri, Philipp Schlatter, and Dan~S Henningson.
\newblock Spectral analysis of nonlinear flows.
\newblock {\em Journal of fluid mechanics}, 641:115--127, 2009.

\bibitem{williams2015data}
Matthew~O Williams, Ioannis~G Kevrekidis, and Clarence~W Rowley.
\newblock A data--driven approximation of the koopman operator: Extending dynamic mode decomposition.
\newblock {\em Journal of Nonlinear Science}, 25:1307--1346, 2015.

\bibitem{arbabi2017ergodic}
Hassan Arbabi and Igor Mezic.
\newblock Ergodic theory, dynamic mode decomposition, and computation of spectral properties of the koopman operator.
\newblock {\em SIAM Journal on Applied Dynamical Systems}, 16(4):2096--2126, 2017.

\bibitem{askham2018variable}
Travis Askham and J~Nathan Kutz.
\newblock Variable projection methods for an optimized dynamic mode decomposition.
\newblock {\em SIAM Journal on Applied Dynamical Systems}, 17(1):380--416, 2018.

\bibitem{drmac2018data}
Zlatko Drmac, Igor Mezic, and Ryan Mohr.
\newblock Data driven modal decompositions: analysis and enhancements.
\newblock {\em SIAM Journal on Scientific Computing}, 40(4):A2253--A2285, 2018.

\bibitem{colbrook2024rigorous}
Matthew~J Colbrook and Alex Townsend.
\newblock Rigorous data-driven computation of spectral properties of koopman operators for dynamical systems.
\newblock {\em Communications on Pure and Applied Mathematics}, 77(1):221--283, 2024.

\bibitem{takens2006detecting}
Floris Takens.
\newblock Detecting strange attractors in turbulence.
\newblock In {\em Dynamical Systems and Turbulence, Warwick 1980: proceedings of a symposium held at the University of Warwick 1979/80}, pages 366--381. Springer, 2006.

\bibitem{avila2023spectral}
Allan~M Avila and Igor Mezi{\'c}.
\newblock Spectral properties of pullback operators on vector bundles of a dynamical system.
\newblock {\em SIAM Journal on Applied Dynamical Systems}, 22(4):3059--3092, 2023.

\bibitem{kantorovich1960mathematical}
Leonid~V Kantorovich.
\newblock Mathematical methods of organizing and planning production.
\newblock {\em Management science}, 6(4):366--422, 1960.

\bibitem{vaserstein1969markov}
Leonid~Nisonovich Vaserstein.
\newblock Markov processes over denumerable products of spaces, describing large systems of automata.
\newblock {\em Problemy Peredachi Informatsii}, 5(3):64--72, 1969.

\bibitem{hendrycks2016gaussian}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock {\em arXiv preprint arXiv:1606.08415}, 2016.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.

\bibitem{liu2022omnigrok}
Ziming Liu, Eric~J Michaud, and Max Tegmark.
\newblock Omnigrok: Grokking beyond algorithmic data.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2022.

\bibitem{minegishi2023grokking}
Gouki Minegishi, Yusuke Iwasawa, and Yutaka Matsuo.
\newblock Grokking tickets: Lottery tickets accelerate grokking.
\newblock {\em arXiv preprint arXiv:2310.19470}, 2023.

\bibitem{notsawo2023predicting}
Pascal Notsawo~Jr, Hattie Zhou, Mohammad Pezeshki, Irina Rish, Guillaume Dumas, et~al.
\newblock Predicting grokking long before it happens: A look into the loss landscape of models which grok.
\newblock {\em arXiv preprint arXiv:2306.13253}, 2023.

\bibitem{lessard2016analysis}
Laurent Lessard, Benjamin Recht, and Andrew Packard.
\newblock Analysis and design of optimization algorithms via integral quadratic constraints.
\newblock {\em SIAM Journal on Optimization}, 26(1):57--95, 2016.

\bibitem{zhao2021automatic}
Shipu Zhao, Laurent Lessard, and Madeleine Udell.
\newblock An automatic system to detect equivalence between iterative algorithms.
\newblock {\em arXiv preprint arXiv:2105.04684}, 2021.

\bibitem{dietrich2020koopman}
Felix Dietrich, Thomas~N Thiem, and Ioannis~G Kevrekidis.
\newblock On the koopman operator of algorithms.
\newblock {\em SIAM Journal on Applied Dynamical Systems}, 19(2):860--885, 2020.

\bibitem{redman2022algorithmic}
William~T Redman, Maria Fonoberova, Ryan Mohr, Ioannis~G Kevrekidis, and Igor Mezi{\'c}.
\newblock Algorithmic (semi-) conjugacy via koopman operator theory.
\newblock In {\em 2022 IEEE 61st Conference on Decision and Control (CDC)}, pages 6006--6011. IEEE, 2022.

\bibitem{ostrow2024beyond}
Mitchell Ostrow, Adam Eisen, Leo Kozachkov, and Ila Fiete.
\newblock Beyond geometry: Comparing the temporal structure of computation in neural circuits with dynamical similarity analysis.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{mitsos2018optimal}
Alexander Mitsos, Jaromi{\l} Najman, and Ioannis~G Kevrekidis.
\newblock Optimal deterministic algorithm generation.
\newblock {\em Journal of Global Optimization}, 71:891--913, 2018.

\bibitem{guo2022personalized}
Yue Guo, Felix Dietrich, Tom Bertalan, Danimir~T Doncevic, Manuel Dahmen, Ioannis~G Kevrekidis, and Qianxiao Li.
\newblock Personalized algorithm generation: A case study in learning ode integrators.
\newblock {\em SIAM Journal on Scientific Computing}, 44(4):A1911--A1933, 2022.

\bibitem{doncevic2024recursively}
Danimir~T Doncevic, Alexander Mitsos, Yue Guo, Qianxiao Li, Felix Dietrich, Manuel Dahmen, and Ioannis~G Kevrekidis.
\newblock A recursively recurrent neural network (r2n2) architecture for learning iterative algorithms.
\newblock {\em SIAM Journal on Scientific Computing}, 46(2):A719--A743, 2024.

\bibitem{bertalan2020transformations}
Tom Bertalan, Felix Dietrich, and Ioannis~G Kevrekidis.
\newblock Transformations between deep neural networks.
\newblock {\em arXiv preprint arXiv:2007.05646}, 2020.

\bibitem{blalock2020state}
Davis Blalock, Jose~Javier Gonzalez~Ortiz, Jonathan Frankle, and John Guttag.
\newblock What is the state of neural network pruning?
\newblock {\em Proceedings of machine learning and systems}, 2:129--146, 2020.

\end{thebibliography}
