\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{ICML}, pages 8748--8763, 2021.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Kim, Li, Kornblith, Roelofs,
  Lopes, Hajishirzi, Farhadi, Namkoong, et~al.]{wortsman2022robust}
Mitchell Wortsman, Gabriel Ilharco, Jong~Wook Kim, Mike Li, Simon Kornblith,
  Rebecca Roelofs, Raphael~Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi,
  Hongseok Namkoong, et~al.
\newblock Robust fine-tuning of zero-shot models.
\newblock In \emph{CVPR}, pages 7959--7971, 2022.

\bibitem[Cho et~al.(2022)Cho, Yoon, Kale, Dernoncourt, Bui, and
  Bansal]{Cho2022CLIPReward}
Jaemin Cho, Seunghyun Yoon, Ajinkya Kale, Franck Dernoncourt, Trung Bui, and
  Mohit Bansal.
\newblock Fine-grained image captioning with clip reward.
\newblock In \emph{Findings of NAACL}, 2022.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and
  Duerig]{jia2021scaling}
Chao Jia, Yinfei Yang, Ye~Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
  Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In \emph{ICML}, pages 4904--4916, 2021.

\bibitem[Xu et~al.(2017)Xu, Zhu, Choy, and Fei-Fei]{xu2017scene}
Danfei Xu, Yuke Zhu, Christopher~B Choy, and Li~Fei-Fei.
\newblock Scene graph generation by iterative message passing.
\newblock In \emph{CVPR}, pages 5410--5419, 2017.

\bibitem[Tang et~al.(2020)Tang, Niu, Huang, Shi, and Zhang]{tang2020unbiased}
Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, and Hanwang Zhang.
\newblock Unbiased scene graph generation from biased training.
\newblock In \emph{CVPR}, pages 3716--3725, 2020.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Chen, Shao, Xiao, Zhang, and
  Xiao]{li2022rethinking}
Xingchen Li, Long Chen, Jian Shao, Shaoning Xiao, Songyang Zhang, and Jun Xiao.
\newblock Rethinking the evaluation of unbiased scene graph generation.
\newblock In \emph{BMVC}, 2022{\natexlab{a}}.

\bibitem[He et~al.(2022)He, Gao, Song, and Li]{hetao2022}
Tao He, Lianli Gao, Jingkuan Song, and Yuan-Fang Li.
\newblock Towards open-vocabulary scene graph generation with prompt-based
  finetuning.
\newblock In \emph{ECCV}, 2022.

\bibitem[Li et~al.(2023)Li, Chen, Xiao, Yang, Wang, and
  Chen]{li2023compositional}
Lin Li, Guikun Chen, Jun Xiao, Yi~Yang, Chunping Wang, and Long Chen.
\newblock Compositional feature augmentation for unbiased scene graph
  generation.
\newblock In \emph{ICCV}, pages 21685--21695, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{NIPS}, 2020.

\bibitem[Menon and Vondrick(2022)]{menon2022visual}
Sachit Menon and Carl Vondrick.
\newblock Visual classification via description from large language models.
\newblock In \emph{ICLR}, 2022.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and
  Iwasawa]{kojima2022large}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
  Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock In \emph{NIPS}, 2022.

\bibitem[Krishna et~al.(2017)Krishna, Zhu, Groth, Johnson, Hata, Kravitz, Chen,
  Kalantidis, Li, Shamma, et~al.]{krishna2017visual}
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
  Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David~A Shamma, et~al.
\newblock Visual genome: Connecting language and vision using crowdsourced
  dense image annotations.
\newblock \emph{IJCV}, 2017.

\bibitem[Hudson and Manning(2019)]{hudson2019gqa}
Drew~A Hudson and Christopher~D Manning.
\newblock Gqa: A new dataset for real-world visual reasoning and compositional
  question answering.
\newblock In \emph{CVPR}, pages 6700--6709, 2019.

\bibitem[Chao et~al.(2015)Chao, Wang, He, Wang, and Deng]{chao2015hico}
Yu-Wei Chao, Zhan Wang, Yugeng He, Jiaxuan Wang, and Jia Deng.
\newblock Hico: A benchmark for recognizing human-object interactions in
  images.
\newblock In \emph{ICCV}, pages 1017--1025, 2015.

\bibitem[Gupta and Malik(2015)]{gupta2015visual}
Saurabh Gupta and Jitendra Malik.
\newblock Visual semantic role labeling.
\newblock \emph{arXiv preprint arXiv:1505.04474}, 2015.

\bibitem[Yan et~al.(2022)Yan, Chang, Luo, Liu, Zhang, and
  Zheng]{yan2022semantics}
Caixia Yan, Xiaojun Chang, Minnan Luo, Huan Liu, Xiaoqin Zhang, and Qinghua
  Zheng.
\newblock Semantics-guided contrastive network for zero-shot object detection.
\newblock \emph{TPAMI}, 2022.

\bibitem[Zhang et~al.(2022)Zhang, Zhang, Li, and Smola]{zhang2022automatic}
Zhuosheng Zhang, Aston Zhang, Mu~Li, and Alex Smola.
\newblock Automatic chain of thought prompting in large language models.
\newblock \emph{arXiv preprint arXiv:2210.03493}, 2022.

\bibitem[zel(2018)]{zellers2018neural}
Neural motifs: Scene graph parsing with global context.
\newblock In \emph{CVPR}, 2018.

\bibitem[Bui et~al.(2022)Bui, Han, and Poon]{bui2022sg}
Anh~Duc Bui, Soyeon~Caren Han, and Josiah Poon.
\newblock Sg-shuffle: Multi-aspect shuffle transformer for scene graph
  generation.
\newblock In \emph{Australasian Joint Conference on Artificial Intelligence},
  pages 87--101. Springer, 2022.

\bibitem[Dong et~al.(2022)Dong, Gan, Song, Wu, Cheng, and Nie]{dong2022stacked}
Xingning Dong, Tian Gan, Xuemeng Song, Jianlong Wu, Yuan Cheng, and Liqiang
  Nie.
\newblock Stacked hybrid-attention and group collaborative learning for
  unbiased scene graph generation.
\newblock In \emph{CVPR}, pages 19427--19436, 2022.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Chen, Huang, Zhang, Zhang, and
  Xiao]{li2022devil}
Lin Li, Long Chen, Yifeng Huang, Zhimeng Zhang, Songyang Zhang, and Jun Xiao.
\newblock The devil is in the labels: Noisy label correction for robust scene
  graph generation.
\newblock In \emph{CVPR}, pages 18869--18878, 2022{\natexlab{b}}.

\bibitem[Chao et~al.(2018)Chao, Liu, Liu, Zeng, and Deng]{chao2018learning}
Yu-Wei Chao, Yunfan Liu, Xieyang Liu, Huayi Zeng, and Jia Deng.
\newblock Learning to detect human-object interactions.
\newblock In \emph{WACV}, pages 381--389, 2018.

\bibitem[Teng and Wang(2022)]{teng2022structured}
Yao Teng and Limin Wang.
\newblock Structured sparse r-cnn for direct scene graph generation.
\newblock In \emph{CVPR}, pages 19437--19446, 2022.

\bibitem[Kato et~al.(2018)Kato, Li, and Gupta]{kato2018compositional}
Keizo Kato, Yin Li, and Abhinav Gupta.
\newblock Compositional learning for human object interaction.
\newblock In \emph{ECCV}, pages 234--251, 2018.

\bibitem[Kim et~al.(2020)Kim, Sun, Choi, Lin, and Kweon]{kim2020detecting}
Dong-Jin Kim, Xiao Sun, Jinsoo Choi, Stephen Lin, and In~So Kweon.
\newblock Detecting human-object interactions with action co-occurrence priors.
\newblock In \emph{ECCV}, pages 718--736, 2020.

\bibitem[Liao et~al.(2022)Liao, Zhang, Lu, Wang, Li, and Liu]{liao2022gen}
Yue Liao, Aixi Zhang, Miao Lu, Yongliang Wang, Xiaobo Li, and Si~Liu.
\newblock Gen-vlkt: Simplify association and enhance interaction understanding
  for hoi detection.
\newblock In \emph{CVPR}, pages 20123--20132, 2022.

\bibitem[Chen et~al.(2023)Chen, Li, Luo, and Xiao]{chen2023addressing}
Guikun Chen, Lin Li, Yawei Luo, and Jun Xiao.
\newblock Addressing predicate overlap in scene graph generation with semantic
  granularity controller.
\newblock In \emph{ICME}, pages 78--83. IEEE, 2023.

\bibitem[Gao et~al.(2023)Gao, Chen, Zhang, Xiao, and Sun]{gao2023compositional}
Kaifeng Gao, Long Chen, Hanwang Zhang, Jun Xiao, and Qianru Sun.
\newblock Compositional prompt tuning with motion cues for open-vocabulary
  video relation detection.
\newblock In \emph{ICLR}, 2023.

\bibitem[Liu et~al.(2023)Liu, Yuan, Fu, Jiang, Hayashi, and Neubig]{liu2023pre}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
  Graham Neubig.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting
  methods in natural language processing.
\newblock \emph{ACM Computing Surveys}, 55\penalty0 (9):\penalty0 1--35, 2023.

\bibitem[Gao et~al.(2022)Gao, Xing, Niebles, Li, Xu, Liu, and
  Xiong]{gao2022open}
Mingfei Gao, Chen Xing, Juan~Carlos Niebles, Junnan Li, Ran Xu, Wenhao Liu, and
  Caiming Xiong.
\newblock Open vocabulary object detection with pseudo bounding-box labels.
\newblock In \emph{ECCV}, pages 266--282, 2022.

\bibitem[Mikolov et~al.(2013)Mikolov, Chen, Corrado, and
  Dean]{mikolov2013efficient}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Efficient estimation of word representations in vector space.
\newblock \emph{arXiv preprint arXiv:1301.3781}, 2013.

\bibitem[Pennington et~al.(2014)Pennington, Socher, and
  Manning]{pennington2014glove}
Jeffrey Pennington, Richard Socher, and Christopher~D Manning.
\newblock Glove: Global vectors for word representation.
\newblock In \emph{EMNLP}, pages 1532--1543, 2014.

\bibitem[Wang et~al.(2018)Wang, Ye, and Gupta]{wang2018zero}
Xiaolong Wang, Yufei Ye, and Abhinav Gupta.
\newblock Zero-shot recognition via semantic embeddings and knowledge graphs.
\newblock In \emph{CVPR}, pages 6857--6866, 2018.

\bibitem[Chuang et~al.(2020)Chuang, Robinson, Lin, Torralba, and
  Jegelka]{chuang2020debiased}
Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Stefanie
  Jegelka.
\newblock Debiased contrastive learning.
\newblock In \emph{NIPS}, 2020.

\bibitem[Zareian et~al.(2021)Zareian, Rosa, Hu, and Chang]{zareian2021open}
Alireza Zareian, Kevin~Dela Rosa, Derek~Hao Hu, and Shih-Fu Chang.
\newblock Open-vocabulary object detection using captions.
\newblock In \emph{CVPR}, pages 14393--14402, 2021.

\bibitem[Gu et~al.(2022)Gu, Lin, Kuo, and Cui]{gu2022open}
Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
\newblock Open-vocabulary object detection via vision and language knowledge
  distillation.
\newblock In \emph{ICLR}, 2022.

\bibitem[Wang and Li(2023)]{wang2023learning}
Tao Wang and Nan Li.
\newblock Learning to detect and segment for open vocabulary object detection.
\newblock In \emph{CVPR}, 2023.

\bibitem[Liu et~al.(2021)Liu, Shen, Zhang, Dolan, Carin, and
  Chen]{liu2021makes}
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu
  Chen.
\newblock What makes good in-context examples for gpt-$3 $?
\newblock \emph{arXiv preprint arXiv:2101.06804}, 2021.

\bibitem[kan(2021)]{kan2021zero}
Zero-shot scene graph relation prediction through commonsense knowledge
  integration.
\newblock In \emph{ECML PKDD}, 2021.

\bibitem[li2(2023)]{li2023decomposed}
Decomposed prototype learning for few-shot scene graph generation.
\newblock \emph{arXiv preprint arXiv:2303.10863}, 2023.

\bibitem[yu2(2023)]{yu2023visually}
Visually-prompted language model for fine-grained scene graph generation in an
  open world.
\newblock In \emph{ICCV}, 2023.

\end{thebibliography}
