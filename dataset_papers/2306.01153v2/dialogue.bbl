\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Chen et~al.(2020)Chen, Meng, Li, Chen, Xu, Xu, and
  Zhou]{chen2020bridging}
Chen, X., Meng, F., Li, P., Chen, F., Xu, S., Xu, B., and Zhou, J.
\newblock Bridging the gap between prior and posterior knowledge selection for
  knowledge-grounded dialogue generation.
\newblock In \emph{Proceedings of the 2020 conference on empirical methods in
  natural language processing (EMNLP)}, pp.\  3426--3437, 2020.

\bibitem[Cremer et~al.(2018)Cremer, Li, and Duvenaud]{cremer2018inference}
Cremer, C., Li, X., and Duvenaud, D.
\newblock Inference suboptimality in variational autoencoders.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1078--1086. PMLR, 2018.

\bibitem[Dinan et~al.(2018)Dinan, Roller, Shuster, Fan, Auli, and
  Weston]{dinan2018wizard}
Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston, J.
\newblock Wizard of wikipedia: Knowledge-powered conversational agents.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Dinan et~al.(2019)Dinan, Roller, Shuster, Fan, Auli, and
  Weston]{dinan2019wizard}
Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston, J.
\newblock {W}izard of {W}ikipedia: Knowledge-powered conversational agents.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2019.

\bibitem[Durmus et~al.(2020)Durmus, He, and Diab]{durmus2020feqa}
Durmus, E., He, H., and Diab, M.
\newblock Feqa: A question answering evaluation framework for faithfulness
  assessment in abstractive summarization.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  5055--5070, 2020.

\bibitem[Feng et~al.(2021)Feng, Patel, Wan, and Joshi]{feng2021multidoc2dial}
Feng, S., Patel, S.~S., Wan, H., and Joshi, S.
\newblock Multidoc2dial: Modeling dialogues grounded in multiple documents.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  6162--6176, 2021.

\bibitem[Fu et~al.(2022)Fu, Zhao, Tao, Wen, and Yan]{fu2022there}
Fu, T., Zhao, X., Tao, C., Wen, J.-R., and Yan, R.
\newblock There are a thousand hamlets in a thousand peopleâ€™s eyes: Enhancing
  knowledge-grounded dialogue with personal memory.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  3901--3913,
  2022.

\bibitem[Ghazvininejad et~al.(2018)Ghazvininejad, Brockett, Chang, Dolan, Gao,
  Yih, and Galley]{ghazvininejad2018knowledge}
Ghazvininejad, M., Brockett, C., Chang, M.-W., Dolan, B., Gao, J., Yih, W.-t.,
  and Galley, M.
\newblock A knowledge-grounded neural conversation model.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Han et~al.(2017)Han, Lu, Zhu, and Wu]{han2017alternating}
Han, T., Lu, Y., Zhu, S.-C., and Wu, Y.~N.
\newblock Alternating back-propagation for generator network.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~31, 2017.

\bibitem[Izacard \& Grave(2021)Izacard and Grave]{izacard2021leveraging}
Izacard, G. and Grave, {\'E}.
\newblock Leveraging passage retrieval with generative models for open domain
  question answering.
\newblock In \emph{Proceedings of the 16th Conference of the European Chapter
  of the Association for Computational Linguistics: Main Volume}, pp.\
  874--880, 2021.

\bibitem[Ji et~al.(2022{\natexlab{a}})Ji, Lee, Frieske, Yu, Su, Xu, Ishii,
  Bang, Madotto, and Fung]{ji2022survey}
Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y.,
  Madotto, A., and Fung, P.
\newblock Survey of hallucination in natural language generation.
\newblock \emph{ACM Computing Surveys}, 2022{\natexlab{a}}.

\bibitem[Ji et~al.(2022{\natexlab{b}})Ji, Liu, Lee, Yu, Wilie, Zeng, and
  Fung]{ji2022rho}
Ji, Z., Liu, Z., Lee, N., Yu, T., Wilie, B., Zeng, M., and Fung, P.
\newblock Rho: Reducing hallucination in open-domain dialogues with knowledge
  grounding.
\newblock \emph{arXiv preprint arXiv:2212.01588}, 2022{\natexlab{b}}.

\bibitem[Kim et~al.(2019)Kim, Ahn, and Kim]{kim2019sequential}
Kim, B., Ahn, J., and Kim, G.
\newblock Sequential latent knowledge selection for knowledge-grounded
  dialogue.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{kingma13auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{2nd International
  Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April
  14-16, 2014, Conference Track Proceedings}, 2014.
\newblock URL \url{http://arxiv.org/abs/1312.6114}.

\bibitem[Langevin(1908)]{langevin1908theory}
Langevin, P.
\newblock \emph{On the theory of Brownian motion}.
\newblock 1908.

\bibitem[Lewis et~al.(2020)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer]{lewis2020bart}
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O.,
  Stoyanov, V., and Zettlemoyer, L.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock In \emph{ACL 2020}, 2020.

\bibitem[Li \& Han(2022)Li and Han]{li2022learning}
Li, H. and Han, T.
\newblock Learning sparse latent representations for generator model.
\newblock \emph{arXiv preprint arXiv:2209.09949}, 2022.

\bibitem[Li et~al.(2016)Li, Galley, Brockett, Gao, and Dolan]{li2016diversity}
Li, J., Galley, M., Brockett, C., Gao, J., and Dolan, W.~B.
\newblock A diversity-promoting objective function for neural conversation
  models.
\newblock In \emph{Proceedings of the 2016 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp.\  110--119, 2016.

\bibitem[Li et~al.(2020)Li, Xu, Wu, Zhao, Zhao, and Tao]{li2020zero}
Li, L., Xu, C., Wu, W., Zhao, Y., Zhao, X., and Tao, C.
\newblock Zero-resource knowledge-grounded dialogue generation.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 8475--8485, 2020.

\bibitem[Lian et~al.()Lian, Xie, Wang, Peng, and Wu]{lianlearning}
Lian, R., Xie, M., Wang, F., Peng, J., and Wu, H.
\newblock Learning to select knowledge for response generation in dialog
  systems.

\bibitem[Lin(2004)]{lin2004rouge}
Lin, C.-Y.
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In \emph{Text summarization branches out}, pp.\  74--81, 2004.

\bibitem[Liu et~al.(2021)Liu, Zhao, Li, Ren, Zhang, and Yin]{liu2021three}
Liu, S., Zhao, X., Li, B., Ren, F., Zhang, L., and Yin, S.
\newblock A three-stage learning framework for low-resource knowledge-grounded
  dialogue generation.
\newblock \emph{arXiv preprint arXiv:2109.04096}, 2021.

\bibitem[Meng et~al.(2020)Meng, Ren, Chen, Sun, Ren, Tu, and
  Rijke]{meng2020dukenet}
Meng, C., Ren, P., Chen, Z., Sun, W., Ren, Z., Tu, Z., and Rijke, M.~d.
\newblock Dukenet: A dual knowledge interaction network for knowledge-grounded
  conversation.
\newblock In \emph{Proceedings of the 43rd International ACM SIGIR Conference
  on Research and Development in Information Retrieval}, pp.\  1151--1160,
  2020.

\bibitem[Moghe et~al.(2018)Moghe, Arora, Banerjee, and
  Khapra]{moghe-etal-2018-towards}
Moghe, N., Arora, S., Banerjee, S., and Khapra, M.~M.
\newblock Towards exploiting background knowledge for building conversation
  systems.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  2322--2332, Brussels, Belgium,
  October-November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-1255}.
\newblock URL \url{https://aclanthology.org/D18-1255}.

\bibitem[Nijkamp et~al.(2019)Nijkamp, Hill, Zhu, and Wu]{nijkamp2019learning}
Nijkamp, E., Hill, M., Zhu, S.-C., and Wu, Y.~N.
\newblock Learning non-convergent non-persistent short-run mcmc toward
  energy-based model.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[OpenAI(2023)]{openai_2023}
OpenAI.
\newblock Chatgpt: Optimizing language models for dialogue, Jan 2023.
\newblock URL \url{https://openai.com/blog/chatgpt/}.

\bibitem[Pang et~al.(2020)Pang, Han, Nijkamp, Zhu, and Wu]{bo_latent}
Pang, B., Han, T., Nijkamp, E., Zhu, S.-C., and Wu, Y.~N.
\newblock Learning latent space energy-based prior model.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  21994--22008. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/fa3060edb66e6ff4507886f9912e1ab9-Paper.pdf}.

\bibitem[Pang et~al.(2021{\natexlab{a}})Pang, Nijkamp, Han, and
  Wu]{pang2021generative}
Pang, B., Nijkamp, E., Han, T., and Wu, Y.~N.
\newblock Generative text modeling through short run inference.
\newblock \emph{arXiv preprint arXiv:2106.02513}, 2021{\natexlab{a}}.

\bibitem[Pang et~al.(2021{\natexlab{b}})Pang, Zhao, Xie, and
  Wu]{Pang_2021_CVPR}
Pang, B., Zhao, T., Xie, X., and Wu, Y.~N.
\newblock Trajectory prediction with latent belief energy-based model.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pp.\  11814--11824, June 2021{\natexlab{b}}.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu]{papineni2002bleu}
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th annual meeting of the Association
  for Computational Linguistics}, pp.\  311--318, 2002.

\bibitem[Rashkin et~al.(2021)Rashkin, Reitter, Tomar, and
  Das]{rashkin2021increasing}
Rashkin, H., Reitter, D., Tomar, G.~S., and Das, D.
\newblock Increasing faithfulness in knowledge-grounded dialogue with
  controllable features.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pp.\  704--718, 2021.

\bibitem[Robbins \& Monro(1985)Robbins and Monro]{kl}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock In \emph{Herbert Robbins Selected Papers}, pp.\  102--109. Springer,
  1985.

\bibitem[Roller et~al.(2021)Roller, Dinan, Goyal, Ju, Williamson, Liu, Xu, Ott,
  Smith, Boureau, et~al.]{roller2021recipes}
Roller, S., Dinan, E., Goyal, N., Ju, D., Williamson, M., Liu, Y., Xu, J., Ott,
  M., Smith, E.~M., Boureau, Y.-L., et~al.
\newblock Recipes for building an open-domain chatbot.
\newblock In \emph{EACL}, 2021.

\bibitem[Scialom et~al.(2021)Scialom, Dray, Lamprier, Piwowarski, Staiano,
  Wang, and Gallinari]{scialom2021questeval}
Scialom, T., Dray, P.-A., Lamprier, S., Piwowarski, B., Staiano, J., Wang, A.,
  and Gallinari, P.
\newblock Questeval: Summarization asks for fact-based evaluation.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  6594--6604, 2021.

\bibitem[Serban et~al.(2016)Serban, Sordoni, Bengio, Courville, and
  Pineau]{serban2016building}
Serban, I., Sordoni, A., Bengio, Y., Courville, A., and Pineau, J.
\newblock Building end-to-end dialogue systems using generative hierarchical
  neural network models.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~30, 2016.

\bibitem[Shuster et~al.(2021)Shuster, Poff, Chen, Kiela, and
  Weston]{shuster2021retrieval}
Shuster, K., Poff, S., Chen, M., Kiela, D., and Weston, J.
\newblock Retrieval augmentation reduces hallucination in conversation.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2021}, pp.\  3784--3803, 2021.

\bibitem[Tieleman(2008)]{tieleman2008training}
Tieleman, T.
\newblock Training restricted boltzmann machines using approximations to the
  likelihood gradient.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pp.\  1064--1071, 2008.

\bibitem[Xie et~al.(2022)Xie, Zhu, Li, and Li]{xie2022tale}
Xie, J., Zhu, Y., Li, J., and Li, P.
\newblock A tale of two flows: cooperative learning of langevin flow and
  normalizing flow toward energy-based model.
\newblock \emph{arXiv preprint arXiv:2205.06924}, 2022.

\bibitem[Xu et~al.(2021)Xu, Ishii, Winata, Lin, Madotto, Liu, Xu, and
  Fung]{xu2021caire}
Xu, Y., Ishii, E., Winata, G.~I., Lin, Z., Madotto, A., Liu, Z., Xu, P., and
  Fung, P.
\newblock Caire in dialdoc21: Data augmentation for information seeking
  dialogue system.
\newblock In \emph{Proceedings of the 1st Workshop on Document-grounded
  Dialogue and Conversational Question Answering (DialDoc 2021)}, pp.\  46--51,
  2021.

\bibitem[Xu et~al.(2022)Xu, Ishii, Cahyawijaya, Liu, Winata, Madotto, Su, and
  Fung]{xu2022retrieval}
Xu, Y., Ishii, E., Cahyawijaya, S., Liu, Z., Winata, G.~I., Madotto, A., Su,
  D., and Fung, P.
\newblock Retrieval-free knowledge-grounded dialogue response generation with
  adapters.
\newblock In \emph{Proceedings of the Second DialDoc Workshop on
  Document-grounded Dialogue and Conversational Question Answering}, pp.\
  93--107, 2022.

\bibitem[Yang et~al.(2022)Yang, Lin, Li, Meng, Wang, Wang, and
  Zhou]{yang2022take}
Yang, C., Lin, Z., Li, J., Meng, F., Wang, W., Wang, L., and Zhou, J.
\newblock Take: Topic-shift aware knowledge selection for dialogue generation.
\newblock In \emph{Proceedings of the 29th International Conference on
  Computational Linguistics}, pp.\  253--265, 2022.

\bibitem[Zhan et~al.(2021)Zhan, Shen, Chen, and Zhang]{zhan2021colv}
Zhan, H., Shen, L., Chen, H., and Zhang, H.
\newblock Colv: A collaborative latent variable model for knowledge-grounded
  dialogue generation.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  2250--2261, 2021.

\bibitem[Zhang et~al.(2021)Zhang, Duckworth, Ippolito, and
  Neelakantan]{zhang2021trading}
Zhang, H., Duckworth, D., Ippolito, D., and Neelakantan, A.
\newblock Trading off diversity and quality in natural language generation.
\newblock In \emph{Proceedings of the Workshop on Human Evaluation of NLP
  Systems (HumEval)}, pp.\  25--33, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Sun, Galley, Chen, Brockett, Gao, Gao, Liu,
  and Dolan]{zhang2020dialogpt}
Zhang, Y., Sun, S., Galley, M., Chen, Y.-C., Brockett, C., Gao, X., Gao, J.,
  Liu, J., and Dolan, W.~B.
\newblock Dialogpt: Large-scale generative pre-training for conversational
  response generation.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics: System Demonstrations}, pp.\  270--278, 2020.

\bibitem[Zhao et~al.(2019)Zhao, Wu, Tao, Xu, Zhao, and Yan]{zhao2019low}
Zhao, X., Wu, W., Tao, C., Xu, C., Zhao, D., and Yan, R.
\newblock Low-resource knowledge-grounded dialogue generation.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Zhao et~al.(2020)Zhao, Wu, Xu, Tao, Zhao, and Yan]{zhao2020knowledge}
Zhao, X., Wu, W., Xu, C., Tao, C., Zhao, D., and Yan, R.
\newblock Knowledge-grounded dialogue generation with pre-trained language
  models.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  3377--3390, 2020.

\end{thebibliography}
