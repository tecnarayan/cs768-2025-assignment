\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Uehara et~al.(2020)Uehara, Huang, and Jiang]{uehara2019minimax}
Masatoshi Uehara, Jiawei Huang, and Nan Jiang.
\newblock {Minimax Weight and Q-Function Learning for Off-Policy Evaluation}.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (ICML-20)}, 2020.

\bibitem[Precup et~al.(2000)Precup, Sutton, and Singh]{precup2000eligibility}
Doina Precup, Richard~S Sutton, and Satinder~P Singh.
\newblock {Eligibility Traces for Off-Policy Policy Evaluation}.
\newblock In \emph{Proceedings of the 17th International Conference on Machine
  Learning}, pages 759--766, 2000.

\bibitem[Jiang and Li(2016)]{jiang2016doubly}
Nan Jiang and Lihong Li.
\newblock {Doubly Robust Off-policy Value Evaluation for Reinforcement
  Learning}.
\newblock In \emph{Proceedings of The 33rd International Conference on Machine
  Learning}, volume~48, pages 652--661, 2016.

\bibitem[Li et~al.(2015)Li, Munos, and Szepesv{\'a}ri]{li2015minimax}
Lihong Li, R{\'e}mi Munos, and Csaba Szepesv{\'a}ri.
\newblock Toward minimax off-policy value estimation.
\newblock In \emph{Proceedings of the 18th International Conference on
  Artificial Intelligence and Statistics}, 2015.

\bibitem[Liu et~al.(2018)Liu, Li, Tang, and Zhou]{liu2018breaking}
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5361--5371, 2018.

\bibitem[Feng et~al.(2020)Feng, Ren, Tang, and Liu]{feng2020accountable}
Yihao Feng, Tongzheng Ren, Ziyang Tang, and Qiang Liu.
\newblock Accountable off-policy evaluation with kernel bellman statistics.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (ICML-20)}, 2020.

\bibitem[Dai et~al.(2020)Dai, Nachum, Chow, Li, Szepesvari, and
  Schuurmans]{dai2020coindice}
Bo~Dai, Ofir Nachum, Yinlam Chow, Lihong Li, Csaba Szepesvari, and Dale
  Schuurmans.
\newblock {CoinDICE: Off-Policy Confidence Interval Estimation}.
\newblock In \emph{Advances in neural information processing systems}, 2020.

\bibitem[Xie et~al.(2019)Xie, Ma, and Wang]{XieTengyang2019OOEf}
Tengyang Xie, Yifei Ma, and Yu-Xiang Wang.
\newblock Towards optimal off-policy evaluation for reinforcement learning with
  marginalized importance sampling.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pages
  9665--9675. 2019.

\bibitem[Nachum et~al.(2019{\natexlab{a}})Nachum, Chow, Dai, and
  Li]{ChowYinlam2019DBEo}
Ofir Nachum, Yinlam Chow, Bo~Dai, and Lihong Li.
\newblock Dualdice: Behavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock In \emph{Advances in Neural Information Processing Systems 32}.
  2019{\natexlab{a}}.

\bibitem[Liu et~al.(2020)Liu, Bacon, and Brunskill]{liu2019understanding}
Yao Liu, Pierre-Luc Bacon, and Emma Brunskill.
\newblock Understanding the curse of horizon in off-policy evaluation via
  conditional importance sampling.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (ICML-20)}, 2020.

\bibitem[Liu et~al.(2019)Liu, Swaminathan, Agarwal, and Brunskill]{liu2019off}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Off-policy policy gradient with state distribution correction.
\newblock In \emph{Proceedings of the 35th Conference on Uncertainty in
  Artificial Intelligence (UAI-19)}, 2019.

\bibitem[Rowland et~al.(2020)Rowland, Harutyunyan, van Hasselt, Borsa, Schaul,
  Munos, and Dabney]{rowland2019conditional}
Mark Rowland, Anna Harutyunyan, Hado van Hasselt, Diana Borsa, Tom Schaul,
  R{\'e}mi Munos, and Will Dabney.
\newblock Conditional importance sampling for off-policy learning.
\newblock 108:\penalty0 45--55, 2020.

\bibitem[Liao et~al.(2019)Liao, Klasnja, and Murphy]{liao2019off}
Peng Liao, Predrag Klasnja, and Susan Murphy.
\newblock Off-policy estimation of long-term average outcomes with applications
  to mobile health.
\newblock \emph{arXiv preprint arXiv:1912.13088}, 2019.

\bibitem[Zhang et~al.(2019)Zhang, Dai, Li, and Schuurmans]{zhang2019gendice}
Ruiyi Zhang, Bo~Dai, Lihong Li, and Dale Schuurmans.
\newblock Gendice: Generalized offline estimation of stationary values.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Liu, and Whiteson]{zhang2020gradientdice}
Shangtong Zhang, Bo~Liu, and Shimon Whiteson.
\newblock {GradientDICE: Rethinking Generalized Offline Estimation of
  Stationary Values}.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (ICML-20)}, 2020.

\bibitem[Feng et~al.(2019)Feng, Li, and Liu]{feng2019kernel}
Yihao Feng, Lihong Li, and Qiang Liu.
\newblock A kernel loss for solving the bellman equation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  15430--15441, 2019.

\bibitem[Dud{\'\i}k et~al.(2011)Dud{\'\i}k, Langford, and Li]{dudik2011doubly}
Miroslav Dud{\'\i}k, John Langford, and Lihong Li.
\newblock Doubly {R}obust {P}olicy {E}valuation and {L}earning.
\newblock In \emph{Proceedings of the 28th International Conference on Machine
  Learning}, pages 1097--1104, 2011.

\bibitem[Kallus and Uehara(2019)]{KallusNathan2019EBtC}
Nathan Kallus and Masatoshi Uehara.
\newblock Efficiently breaking the curse of horizon: Double reinforcement
  learning in infinite-horizon processes.
\newblock \emph{arXiv preprint arXiv:1909.05850}, 2019.

\bibitem[Tang et~al.(2020)Tang, Feng, Li, Zhou, and Liu]{tang2019harnessing}
Ziyang Tang, Yihao Feng, Lihong Li, Dengyong Zhou, and Qiang Liu.
\newblock Harnessing infinite-horizon off-policy evaluation: Double robustness
  via duality.
\newblock \emph{ICLR 2020(To appear)}, 2020.

\bibitem[Nachum et~al.(2019{\natexlab{b}})Nachum, Dai, Kostrikov, Chow, Li, and
  Schuurmans]{nachum2019algaedice}
Ofir Nachum, Bo~Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale
  Schuurmans.
\newblock Algaedice: Policy gradient from arbitrary experience.
\newblock \emph{arXiv preprint arXiv:1912.02074}, 2019{\natexlab{b}}.

\bibitem[Nachum and Dai(2020)]{nachum2020reinforcement}
Ofir Nachum and Bo~Dai.
\newblock Reinforcement learning via fenchel-rockafellar duality.
\newblock \emph{arXiv preprint arXiv:2001.01866}, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem[Nilim and El~Ghaoui(2005)]{nilim2005robust}
Arnab Nilim and Laurent El~Ghaoui.
\newblock Robust control of markov decision processes with uncertain transition
  matrices.
\newblock \emph{Operations Research}, 53\penalty0 (5):\penalty0 780--798, 2005.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning}, pages
  2052--2062, 2019.

\bibitem[Brafman and Tennenholtz(2003)]{brafman2003r}
Ronen~I Brafman and Moshe Tennenholtz.
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{The Journal of Machine Learning Research}, 3:\penalty0
  213--231, 2003.

\bibitem[Kakade(2003)]{kakade2003sample}
Sham~Machandranath Kakade.
\newblock \emph{On the sample complexity of reinforcement learning}.
\newblock PhD thesis, University of College London, 2003.

\bibitem[Munos and Szepesv{\'a}ri(2008)]{munos2008finite}
R{\'e}mi Munos and Csaba Szepesv{\'a}ri.
\newblock Finite-time bounds for fitted value iteration.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0
  (May):\penalty0 815--857, 2008.

\bibitem[Farahmand et~al.(2010)Farahmand, Szepesv{\'a}ri, and
  Munos]{farahmand2010error}
Amir-massoud Farahmand, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Error {P}ropagation for {A}pproximate {P}olicy and {V}alue
  {I}teration.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  568--576, 2010.

\bibitem[Chen and Jiang(2019)]{chen2019information}
Jinglin Chen and Nan Jiang.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, pages 1042--1051, 2019.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E.
  Schapire.
\newblock Contextual decision processes with low {B}ellman rank are
  {PAC}-learnable.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Neumann(1928)]{neumann1928theorie}
J~v Neumann.
\newblock Zur theorie der gesellschaftsspiele.
\newblock \emph{Mathematische annalen}, 100\penalty0 (1):\penalty0 295--320,
  1928.

\bibitem[Sion et~al.(1958)]{sion1958general}
Maurice Sion et~al.
\newblock On general minimax theorems.
\newblock \emph{Pacific Journal of mathematics}, 8\penalty0 (1):\penalty0
  171--176, 1958.

\bibitem[Voloshin et~al.(2019)Voloshin, Le, Jiang, and
  Yue]{voloshin2019empirical}
Cameron Voloshin, Hoang~M Le, Nan Jiang, and Yisong Yue.
\newblock Empirical study of off-policy policy evaluation for reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1911.06854}, 2019.

\bibitem[Jiang(2018)]{nan_rmax_notes}
Nan Jiang.
\newblock \emph{{CS 598: Notes on Rmax exploration}}.
\newblock {University of Illinois at Urbana-Champaign}, 2018.
\newblock \url{http://nanjiang.cs.illinois.edu/files/cs598/note7.pdf}.

\bibitem[M{\"u}ller(1997)]{muller1997integral}
Alfred M{\"u}ller.
\newblock Integral probability metrics and their generating classes of
  functions.
\newblock \emph{Advances in Applied Probability}, 29\penalty0 (2):\penalty0
  429--443, 1997.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Wang(2017)]{wang2017primal}
Mengdi Wang.
\newblock Primal-dual $\pi$ learning: Sample complexity and sublinear run time
  for ergodic markov decision problems.
\newblock \emph{arXiv preprint arXiv:1710.06100}, 2017.

\end{thebibliography}
