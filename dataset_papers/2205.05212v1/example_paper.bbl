\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Abbeel, and Zaremba]{andrychowicz2017hindsight}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P.,
  McGrew, B., Tobin, J., Abbeel, P., and Zaremba, W.
\newblock Hindsight experience replay.
\newblock \emph{arXiv preprint arXiv:1707.01495}, 2017.

\bibitem[Argall et~al.(2009)Argall, Chernova, Veloso, and
  Browning]{argall2009survey}
Argall, B.~D., Chernova, S., Veloso, M., and Browning, B.
\newblock A survey of robot learning from demonstration.
\newblock \emph{Robotics and autonomous systems}, 57\penalty0 (5):\penalty0
  469--483, 2009.

\bibitem[Baram et~al.(2017)Baram, Anschel, Caspi, and Mannor]{baram2017end}
Baram, N., Anschel, O., Caspi, I., and Mannor, S.
\newblock End-to-end differentiable adversarial imitation learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  390--399. PMLR, 2017.

\bibitem[Bloesch et~al.(2022)Bloesch, Humplik, Patraucean, Hafner, Haarnoja,
  Byravan, Siegel, Tunyasuvunakool, Casarini, Batchelor,
  et~al.]{bloesch2022towards}
Bloesch, M., Humplik, J., Patraucean, V., Hafner, R., Haarnoja, T., Byravan,
  A., Siegel, N.~Y., Tunyasuvunakool, S., Casarini, F., Batchelor, N., et~al.
\newblock Towards real robot learning in the wild: A case study in bipedal
  locomotion.
\newblock In \emph{Conference on Robot Learning}, pp.\  1502--1511. PMLR, 2022.

\bibitem[Brys et~al.(2015)Brys, Harutyunyan, Suay, Chernova, Taylor, and
  Now{\'e}]{brys2015reinforcement}
Brys, T., Harutyunyan, A., Suay, H.~B., Chernova, S., Taylor, M.~E., and
  Now{\'e}, A.
\newblock Reinforcement learning from demonstration through shaping.
\newblock In \emph{Twenty-fourth international joint conference on artificial
  intelligence}, 2015.

\bibitem[Burda et~al.(2018)Burda, Edwards, Storkey, and
  Klimov]{burda2018exploration}
Burda, Y., Edwards, H., Storkey, A., and Klimov, O.
\newblock Exploration by random network distillation.
\newblock \emph{arXiv preprint arXiv:1810.12894}, 2018.

\bibitem[Campos et~al.(2020)Campos, Trott, Xiong, Socher, Gir{\'o}-i Nieto, and
  Torres]{campos2020explore}
Campos, V., Trott, A., Xiong, C., Socher, R., Gir{\'o}-i Nieto, X., and Torres,
  J.
\newblock Explore, discover and learn: Unsupervised discovery of state-covering
  skills.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1317--1327. PMLR, 2020.

\bibitem[Chatzilygeroudis et~al.(2018)Chatzilygeroudis, Vassiliades, and
  Mouret]{chatzilygeroudis2018reset}
Chatzilygeroudis, K., Vassiliades, V., and Mouret, J.-B.
\newblock Reset-free trial-and-error learning for robot damage recovery.
\newblock \emph{Robotics and Autonomous Systems}, 100:\penalty0 236--250, 2018.

\bibitem[Co-Reyes et~al.(2020)Co-Reyes, Sanjeev, Berseth, Gupta, and
  Levine]{co2020ecological}
Co-Reyes, J.~D., Sanjeev, S., Berseth, G., Gupta, A., and Levine, S.
\newblock Ecological reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.12478}, 2020.

\bibitem[Even-Dar et~al.(2005)Even-Dar, Kakade, and
  Mansour]{even2005reinforcement}
Even-Dar, E., Kakade, S.~M., and Mansour, Y.
\newblock Reinforcement learning in pomdps without resets.
\newblock 2005.

\bibitem[Eysenbach et~al.(2017)Eysenbach, Gu, Ibarz, and
  Levine]{eysenbach2017leave}
Eysenbach, B., Gu, S., Ibarz, J., and Levine, S.
\newblock Leave no trace: Learning to reset for safe and autonomous
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1711.06782}, 2017.

\bibitem[Eysenbach et~al.(2018)Eysenbach, Gupta, Ibarz, and
  Levine]{eysenbach2018diversity}
Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S.
\newblock Diversity is all you need: Learning skills without a reward function.
\newblock \emph{arXiv preprint arXiv:1802.06070}, 2018.

\bibitem[Finn et~al.(2016)Finn, Levine, and Abbeel]{finn2016guided}
Finn, C., Levine, S., and Abbeel, P.
\newblock Guided cost learning: Deep inverse optimal control via policy
  optimization.
\newblock In \emph{International conference on machine learning}, pp.\  49--58.
  PMLR, 2016.

\bibitem[Fu et~al.(2018)Fu, Singh, Ghosh, Yang, and Levine]{fu2018variational}
Fu, J., Singh, A., Ghosh, D., Yang, L., and Levine, S.
\newblock Variational inverse control with events: A general framework for
  data-driven reward definition.
\newblock \emph{arXiv preprint arXiv:1805.11686}, 2018.

\bibitem[Ghasemipour et~al.(2020)Ghasemipour, Zemel, and
  Gu]{ghasemipour2020divergence}
Ghasemipour, S. K.~S., Zemel, R., and Gu, S.
\newblock A divergence minimization perspective on imitation learning methods.
\newblock In \emph{Conference on Robot Learning}, pp.\  1259--1277. PMLR, 2020.

\bibitem[Goodfellow(2016)]{goodfellow2016nips}
Goodfellow, I.
\newblock Nips 2016 tutorial: Generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1701.00160}, 2016.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Gregor et~al.(2016)Gregor, Rezende, and
  Wierstra]{gregor2016variational}
Gregor, K., Rezende, D.~J., and Wierstra, D.
\newblock Variational intrinsic control.
\newblock \emph{arXiv preprint arXiv:1611.07507}, 2016.

\bibitem[Gupta et~al.(2021)Gupta, Yu, Zhao, Kumar, Rovinsky, Xu, Devlin, and
  Levine]{gupta2021reset}
Gupta, A., Yu, J., Zhao, T.~Z., Kumar, V., Rovinsky, A., Xu, K., Devlin, T.,
  and Levine, S.
\newblock Reset-free reinforcement learning via multi-task learning: Learning
  dexterous manipulation behaviors without human intervention.
\newblock \emph{arXiv preprint arXiv:2104.11203}, 2021.

\bibitem[Ha et~al.(2020)Ha, Xu, Tan, Levine, and Tan]{ha2020learning}
Ha, S., Xu, P., Tan, Z., Levine, S., and Tan, J.
\newblock Learning to walk in the real world with minimal human effort.
\newblock \emph{arXiv preprint arXiv:2002.08550}, 2020.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pp.\
  1861--1870. PMLR, 2018.

\bibitem[Han et~al.(2015)Han, Levine, and Abbeel]{han2015learning}
Han, W., Levine, S., and Abbeel, P.
\newblock Learning compound multi-step controllers under unknown dynamics.
\newblock In \emph{2015 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pp.\  6435--6442. IEEE, 2015.

\bibitem[Hazan et~al.(2019)Hazan, Kakade, Singh, and
  Van~Soest]{hazan2019provably}
Hazan, E., Kakade, S., Singh, K., and Van~Soest, A.
\newblock Provably efficient maximum entropy exploration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2681--2691. PMLR, 2019.

\bibitem[Hester et~al.(2018)Hester, Vecerik, Pietquin, Lanctot, Schaul, Piot,
  Horgan, Quan, Sendonaris, Osband, et~al.]{hester2018deep}
Hester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T., Piot, B.,
  Horgan, D., Quan, J., Sendonaris, A., Osband, I., et~al.
\newblock Deep q-learning from demonstrations.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Ho \& Ermon(2016)Ho and Ermon]{ho2016generative}
Ho, J. and Ermon, S.
\newblock Generative adversarial imitation learning.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 4565--4573, 2016.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximately}
Kakade, S. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{In Proc. 19th International Conference on Machine Learning}.
  Citeseer, 2002.

\bibitem[Kostrikov et~al.(2018)Kostrikov, Agrawal, Dwibedi, Levine, and
  Tompson]{kostrikov2018discriminator}
Kostrikov, I., Agrawal, K.~K., Dwibedi, D., Levine, S., and Tompson, J.
\newblock Discriminator-actor-critic: Addressing sample inefficiency and reward
  bias in adversarial imitation learning.
\newblock \emph{arXiv preprint arXiv:1809.02925}, 2018.

\bibitem[Lee et~al.(2019)Lee, Eysenbach, Parisotto, Xing, Levine, and
  Salakhutdinov]{lee2019efficient}
Lee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine, S., and Salakhutdinov,
  R.
\newblock Efficient exploration via state marginal matching.
\newblock \emph{arXiv preprint arXiv:1906.05274}, 2019.

\bibitem[Lu et~al.(2020)Lu, Grover, Abbeel, and Mordatch]{lu2020reset}
Lu, K., Grover, A., Abbeel, P., and Mordatch, I.
\newblock Reset-free lifelong learning with skill-space planning.
\newblock \emph{arXiv preprint arXiv:2012.03548}, 2020.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Nair et~al.(2018)Nair, McGrew, Andrychowicz, Zaremba, and
  Abbeel]{nair2018overcoming}
Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., and Abbeel, P.
\newblock Overcoming exploration in reinforcement learning with demonstrations.
\newblock In \emph{2018 IEEE international conference on robotics and
  automation (ICRA)}, pp.\  6292--6299. IEEE, 2018.

\bibitem[Ng et~al.(2000)Ng, Russell, et~al.]{ng2000algorithms}
Ng, A.~Y., Russell, S.~J., et~al.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{Icml}, volume~1, pp.\ ~2, 2000.

\bibitem[Nowozin et~al.(2016)Nowozin, Cseke, and Tomioka]{nowozin2016f}
Nowozin, S., Cseke, B., and Tomioka, R.
\newblock f-gan: Training generative neural samplers using variational
  divergence minimization.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pp.\  271--279, 2016.

\bibitem[Rafailov et~al.(2021)Rafailov, Yu, Rajeswaran, and
  Finn]{rafailov2021visual}
Rafailov, R., Yu, T., Rajeswaran, A., and Finn, C.
\newblock Visual adversarial imitation learning using variational models.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Rajeswaran et~al.(2017)Rajeswaran, Kumar, Gupta, Vezzani, Schulman,
  Todorov, and Levine]{rajeswaran2017learning}
Rajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schulman, J., Todorov, E.,
  and Levine, S.
\newblock Learning complex dexterous manipulation with deep reinforcement
  learning and demonstrations.
\newblock \emph{arXiv preprint arXiv:1709.10087}, 2017.

\bibitem[Rivest \& Schapire(1993)Rivest and Schapire]{rivest1993inference}
Rivest, R.~L. and Schapire, R.~E.
\newblock Inference of finite automata using homing sequences.
\newblock \emph{Information and Computation}, 103\penalty0 (2):\penalty0
  299--347, 1993.

\bibitem[Sharma et~al.(2019)Sharma, Gu, Levine, Kumar, and
  Hausman]{sharma2019dynamics}
Sharma, A., Gu, S., Levine, S., Kumar, V., and Hausman, K.
\newblock Dynamics-aware unsupervised discovery of skills.
\newblock \emph{arXiv preprint arXiv:1907.01657}, 2019.

\bibitem[Sharma et~al.(2021)Sharma, Gupta, Levine, Hausman, and
  Finn]{sharma2021autonomouscurr}
Sharma, A., Gupta, A., Levine, S., Hausman, K., and Finn, C.
\newblock Autonomous reinforcement learning via subgoal curricula.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Sharma et~al.(2022)Sharma, Xu, Sardana, Gupta, Hausman, Levine, and
  Finn]{sharma2021autonomous}
Sharma, A., Xu, K., Sardana, N., Gupta, A., Hausman, K., Levine, S., and Finn,
  C.
\newblock Autonomous reinforcement learning: Formalism and benchmarking.
\newblock \emph{International Conference on Learning Representations}, 2022.

\bibitem[Singh et~al.(2019)Singh, Yang, Hartikainen, Finn, and
  Levine]{singh2019end}
Singh, A., Yang, L., Hartikainen, K., Finn, C., and Levine, S.
\newblock End-to-end robotic reinforcement learning without reward engineering.
\newblock \emph{arXiv preprint arXiv:1904.07854}, 2019.

\bibitem[Smith et~al.(2021)Smith, Kew, Peng, Ha, Tan, and
  Levine]{smith2021legged}
Smith, L., Kew, J.~C., Peng, X.~B., Ha, S., Tan, J., and Levine, S.
\newblock Legged robots that keep on learning: Fine-tuning locomotion policies
  in the real world.
\newblock \emph{arXiv preprint arXiv:2110.05457}, 2021.

\bibitem[Torabi et~al.(2019)Torabi, Warnell, and Stone]{torabi2019adversarial}
Torabi, F., Warnell, G., and Stone, P.
\newblock Adversarial imitation learning from state-only demonstrations.
\newblock In \emph{Proceedings of the 18th International Conference on
  Autonomous Agents and MultiAgent Systems}, pp.\  2229--2231, 2019.

\bibitem[Vecerik et~al.(2017)Vecerik, Hester, Scholz, Wang, Pietquin, Piot,
  Heess, Roth{\"o}rl, Lampe, and Riedmiller]{vecerik2017leveraging}
Vecerik, M., Hester, T., Scholz, J., Wang, F., Pietquin, O., Piot, B., Heess,
  N., Roth{\"o}rl, T., Lampe, T., and Riedmiller, M.
\newblock Leveraging demonstrations for deep reinforcement learning on robotics
  problems with sparse rewards.
\newblock \emph{arXiv preprint arXiv:1707.08817}, 2017.

\bibitem[Xu et~al.(2020)Xu, Verma, Finn, and Levine]{xu2020continual}
Xu, K., Verma, S., Finn, C., and Levine, S.
\newblock Continual learning of control primitives: Skill discovery via
  reset-games.
\newblock \emph{arXiv preprint arXiv:2011.05286}, 2020.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\bibitem[Zhu et~al.(2020{\natexlab{a}})Zhu, Yu, Gupta, Shah, Hartikainen,
  Singh, Kumar, and Levine]{zhu20ingredients}
Zhu, H., Yu, J., Gupta, A., Shah, D., Hartikainen, K., Singh, A., Kumar, V.,
  and Levine, S.
\newblock The ingredients of real world robotic reinforcement learning.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.

\bibitem[Zhu et~al.(2020{\natexlab{b}})Zhu, Lin, Dai, and Zhou]{zhu2020off}
Zhu, Z., Lin, K., Dai, B., and Zhou, J.
\newblock Off-policy imitation learning from observations.
\newblock In \emph{the Thirty-fourth Annual Conference on Neural Information
  Processing Systems (NeurIPS 2020)}, 2020{\natexlab{b}}.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, Dey,
  et~al.]{ziebart2008maximum}
Ziebart, B.~D., Maas, A.~L., Bagnell, J.~A., Dey, A.~K., et~al.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{Aaai}, volume~8, pp.\  1433--1438. Chicago, IL, USA, 2008.

\bibitem[Ziebart et~al.(2010)Ziebart, Bagnell, and Dey]{ziebart2010modeling}
Ziebart, B.~D., Bagnell, J.~A., and Dey, A.~K.
\newblock Modeling interaction via the principle of maximum causal entropy.
\newblock In \emph{ICML}, 2010.

\end{thebibliography}
