\begin{thebibliography}{10}

\bibitem{rashid2018qmix}
T.~Rashid, M.~Samvelyan, C.~Schroeder, G.~Farquhar, J.~Foerster, and
  S.~Whiteson, ``Qmix: Monotonic value function factorisation for deep
  multi-agent reinforcement learning,'' in {\em International Conference on
  Machine Learning (ICML)}, pp.~4295--4304, 2018.

\bibitem{wang2020Qplex}
J.~Wang, Z.~Ren, T.~Liu, Y.~Yu, and C.~Zhang, ``Qplex: Duplex dueling
  multi-agent q-learning,'' in {\em International Conference on Learning
  Representations (ICLR)}, 2021.

\bibitem{foerster2018counterfactual}
J.~Foerster, G.~Farquhar, T.~Afouras, N.~Nardelli, and S.~Whiteson,
  ``Counterfactual multi-agent policy gradients,'' in {\em AAAI conference on
  artificial intelligence (AAAI)}, vol.~32, 2018.

\bibitem{lowe2017multi}
R.~Lowe, Y.~I. Wu, A.~Tamar, J.~Harb, O.~Pieter~Abbeel, and I.~Mordatch,
  ``Multi-agent actor-critic for mixed cooperative-competitive environments,''
  {\em Advances in Neural Information Processing Systems (NeurIPS)}, vol.~30,
  2017.

\bibitem{ye2020towards}
D.~Ye, G.~Chen, W.~Zhang, S.~Chen, B.~Yuan, B.~Liu, J.~Chen, Z.~Liu, F.~Qiu,
  H.~Yu, {\em et~al.}, ``Towards playing full moba games with deep
  reinforcement learning,'' {\em Advances in Neural Information Processing
  Systems (NeurIPS)}, vol.~33, pp.~621--632, 2020.

\bibitem{berner2019dota}
C.~Berner, G.~Brockman, B.~Chan, V.~Cheung, P.~Debiak, C.~Dennison, D.~Farhi,
  Q.~Fischer, S.~Hashme, C.~Hesse, {\em et~al.}, ``Dota 2 with large scale deep
  reinforcement learning,'' {\em arXiv preprint arXiv:1912.06680}, 2019.

\bibitem{bulling2022combining}
N.~Bulling and V.~Goranko, ``Combining quantitative and qualitative reasoning
  in concurrent multi-player games,'' {\em Autonomous Agents and Multi-Agent
  Systems (AAMAS)}, vol.~36, no.~1, pp.~1--33, 2022.

\bibitem{cao2012overview}
Y.~Cao, W.~Yu, W.~Ren, and G.~Chen, ``An overview of recent progress in the
  study of distributed multi-agent coordination,'' {\em IEEE Transactions on
  Industrial Informatics}, vol.~9, no.~1, pp.~427--438, 2012.

\bibitem{kiran2021deep}
B.~R. Kiran, I.~Sobh, V.~Talpaert, P.~Mannion, A.~A. Al~Sallab, S.~Yogamani,
  and P.~P{\'e}rez, ``Deep reinforcement learning for autonomous driving: A
  survey,'' {\em IEEE Transactions on Intelligent Transportation Systems
  (T-ITS)}, 2021.

\bibitem{wu2020multi}
T.~Wu, P.~Zhou, K.~Liu, Y.~Yuan, X.~Wang, H.~Huang, and D.~O. Wu, ``Multi-agent
  deep reinforcement learning for urban traffic light control in vehicular
  networks,'' {\em IEEE Transactions on Vehicular Technology}, vol.~69, no.~8,
  pp.~8243--8256, 2020.

\bibitem{zhou2020drle}
P.~Zhou, X.~Chen, Z.~Liu, T.~Braud, P.~Hui, and J.~Kangasharju, ``Drle:
  Decentralized reinforcement learning at the edge for traffic light control in
  the iov,'' {\em IEEE Transactions on Intelligent Transportation Systems
  (T-ITS)}, vol.~22, no.~4, pp.~2262--2273, 2020.

\bibitem{tesauro2002pricing}
G.~Tesauro and J.~O. Kephart, ``Pricing in agent economies using multi-agent
  q-learning,'' {\em Autonomous Agents and Multi-Agent Systems (AAMAS)},
  vol.~5, no.~3, pp.~289--304, 2002.

\bibitem{zhang2011coordinated}
C.~Zhang and V.~Lesser, ``Coordinated multi-agent reinforcement learning in
  networked distributed pomdps,'' in {\em AAAI Conference on Artificial
  Intelligence (AAAI)}, vol.~25, 2011.

\bibitem{seraj2022embodied}
E.~Seraj, ``Embodied team intelligence in multi-robot systems.,'' in {\em
  Autonomous Agents and Multi-Agent Systems (AAMAS)}, pp.~1869--1871, 2022.

\bibitem{koller1999computing}
D.~Koller and R.~Parr, ``Computing factored value functions for policies in
  structured mdps,'' in {\em International Joint Conferences on Artificial
  Intelligence (IJCAI)}, vol.~99, pp.~1332--1339, 1999.

\bibitem{gronauer2022multi}
S.~Gronauer and K.~Diepold, ``Multi-agent deep reinforcement learning: a
  survey,'' {\em Artificial Intelligence Review}, vol.~55, no.~2, pp.~895--943,
  2022.

\bibitem{oroojlooy2022review}
A.~Oroojlooy and D.~Hajinezhad, ``A review of cooperative multi-agent deep
  reinforcement learning,'' {\em Applied Intelligence}, pp.~1--46, 2022.

\bibitem{oliehoek2008optimal}
F.~A. Oliehoek, M.~T. Spaan, and N.~Vlassis, ``Optimal and approximate q-value
  functions for decentralized pomdps,'' {\em Journal of Artificial Intelligence
  Research}, vol.~32, pp.~289--353, 2008.

\bibitem{kraemer2016multi}
L.~Kraemer and B.~Banerjee, ``Multi-agent reinforcement learning as a rehearsal
  for decentralized planning,'' {\em Neurocomputing}, vol.~190, pp.~82--94,
  2016.

\bibitem{hausknecht2015deep}
M.~Hausknecht and P.~Stone, ``Deep recurrent q-learning for partially
  observable mdps,'' {\em arXiv preprint arXiv:1507.06527}, 2015.

\bibitem{schaul2016prioritized}
T.~Schaul, J.~Quan, I.~Antonoglou, and D.~Silver, ``Prioritized experience
  replay,'' in {\em International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem{samvelyan19smac}
M.~Samvelyan, T.~Rashid, C.~S. de~Witt, G.~Farquhar, N.~Nardelli, T.~G.~J.
  Rudner, C.-M. Hung, P.~H.~S. Torr, J.~Foerster, and S.~Whiteson, ``{The}
  {StarCraft} {Multi}-{Agent} {Challenge},'' {\em CoRR}, vol.~abs/1902.04043,
  2019.

\bibitem{kurach2020google}
K.~Kurach, A.~Raichuk, P.~Stanczyk, M.~Zajac, O.~Bachem, L.~Espeholt,
  C.~Riquelme, D.~Vincent, M.~Michalski, O.~Bousquet, {\em et~al.}, ``Google
  research football: A novel reinforcement learning environment,'' in {\em AAAI
  Conference on Artificial Intelligence (AAAI)}, vol.~34, pp.~4501--4510, 2020.

\bibitem{peng2021facmac}
B.~Peng, T.~Rashid, C.~Schroeder~de Witt, P.-A. Kamienny, P.~Torr,
  W.~B{\"o}hmer, and S.~Whiteson, ``Facmac: Factored multi-agent centralised
  policy gradients,'' {\em Advances in Neural Information Processing Systems
  (NeurIPS)}, vol.~34, pp.~12208--12221, 2021.

\bibitem{oliehoek2016concise}
F.~A. Oliehoek and C.~Amato, {\em A concise introduction to decentralized
  POMDPs}.
\newblock Springer, 2016.

\bibitem{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, {\em et~al.},
  ``Human-level control through deep reinforcement learning,'' {\em Nature},
  vol.~518, no.~7540, pp.~529--533, 2015.

\bibitem{lin1992self}
L.-J. Lin, ``Self-improving reactive agents based on reinforcement learning,
  planning and teaching,'' {\em Machine Learning}, vol.~8, no.~3, pp.~293--321,
  1992.

\bibitem{van2016deep}
H.~Van~Hasselt, A.~Guez, and D.~Silver, ``Deep reinforcement learning with
  double q-learning,'' in {\em AAAI Conference on Artificial Intelligence
  (AAAI)}, vol.~30, 2016.

\bibitem{smith2018don}
S.~L. Smith, P.-J. Kindermans, C.~Ying, and Q.~V. Le, ``Don't decay the
  learning rate, increase the batch size,'' in {\em International Conference on
  Learning Representations (ICLR)}, 2018.

\bibitem{jeon2022maser}
J.~Jeon, W.~Kim, W.~Jung, and Y.~Sung, ``Maser: Multi-agent reinforcement
  learning with subgoals generated from experience replay buffer,'' in {\em
  International Conference on Machine Learning (ICML)}, pp.~10041--10052, PMLR,
  2022.

\bibitem{weber2022remember}
P.~Weber, D.~W{\"a}lchli, M.~Zeqiri, and P.~Koumoutsakos, ``Remember and forget
  experience replay for multi-agent reinforcement learning,'' {\em arXiv
  preprint arXiv:2203.13319}, 2022.

\bibitem{fan2020prioritized}
S.~Fan, G.~Song, B.~Yang, and X.~Jiang, ``Prioritized experience replay in
  multi-actor-attention-critic for reinforcement learning,'' in {\em Journal of
  Physics: Conference Series}, vol.~1631, 2020.

\bibitem{sunehag2017value}
P.~Sunehag, G.~Lever, A.~Gruslys, W.~M. Czarnecki, V.~Zambaldi, M.~Jaderberg,
  M.~Lanctot, N.~Sonnerat, J.~Z. Leibo, K.~Tuyls, {\em et~al.},
  ``Value-decomposition networks for cooperative multi-agent learning,'' {\em
  arXiv preprint arXiv:1706.05296}, 2017.

\end{thebibliography}
