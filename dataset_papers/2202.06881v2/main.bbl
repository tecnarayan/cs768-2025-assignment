\begin{thebibliography}{74}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Atlas et~al.(1990)Atlas, Cohn, and Ladner]{atlas_training_1990}
Atlas, L., Cohn, D., and Ladner, R.
\newblock Training connectionist networks with queries and selective sampling.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~2, pp.\  566--573, 1990.

\bibitem[Bennett \& Carvalho(2010)Bennett and Carvalho]{bennett2010online}
Bennett, P.~N. and Carvalho, V.~R.
\newblock Online stratified sampling: evaluating classifiers at web-scale.
\newblock In \emph{International conference on Information and knowledge
  management}, volume~19, pp.\  1581--1584, 2010.

\bibitem[Briol et~al.(2019)Briol, Oates, Girolami, Osborne, and
  Sejdinovic]{briol2019probabilistic}
Briol, F.-X., Oates, C.~J., Girolami, M., Osborne, M.~A., and Sejdinovic, D.
\newblock Probabilistic integration: A role in statistical computation?
\newblock \emph{Statistical Science}, 34\penalty0 (1):\penalty0 1--22, 2019.

\bibitem[Brochu et~al.(2010)Brochu, Cora, and De~Freitas]{brochu2010tutorial}
Brochu, E., Cora, V.~M., and De~Freitas, N.
\newblock A tutorial on bayesian optimization of expensive cost functions, with
  application to active user modeling and hierarchical reinforcement learning.
\newblock \emph{arXiv:1012.2599}, 2010.

\bibitem[Bugallo et~al.(2017)Bugallo, Elvira, Martino, Luengo, Miguez, and
  Djuric]{bugallo2017adaptive}
Bugallo, M.~F., Elvira, V., Martino, L., Luengo, D., Miguez, J., and Djuric,
  P.~M.
\newblock Adaptive importance sampling: The past, the present, and the future.
\newblock \emph{IEEE Signal Processing Magazine}, 34\penalty0 (4):\penalty0
  60--79, 2017.

\bibitem[Chaloner \& Verdinelli(1995)Chaloner and
  Verdinelli]{chaloner1995bayesian}
Chaloner, K. and Verdinelli, I.
\newblock Bayesian experimental design: A review.
\newblock \emph{Statistical Science}, pp.\  273--304, 1995.

\bibitem[Chaudhuri et~al.(2015)Chaudhuri, Kakade, Netrapalli, and
  Sanghavi]{chaudhuri2015convergence}
Chaudhuri, K., Kakade, S.~M., Netrapalli, P., and Sanghavi, S.
\newblock Convergence rates of active learning for maximum likelihood
  estimation.
\newblock \emph{Advances in Neural Information Processing Systems}, 28, 2015.

\bibitem[Chen \& Choe(2019)Chen and Choe]{chen2019importance}
Chen, Y.-C. and Choe, Y.
\newblock Importance sampling and its optimality for stochastic simulation
  models.
\newblock \emph{Electronic Journal of Statistics}, 13\penalty0 (2):\penalty0
  3386--3423, 2019.

\bibitem[Corneanu et~al.(2020)Corneanu, Escalera, and
  Martinez]{corneanu2020computing}
Corneanu, C.~A., Escalera, S., and Martinez, A.~M.
\newblock Computing the testing error without a testing set.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  2020.

\bibitem[Cozad et~al.(2014)Cozad, Sahinidis, and Miller]{cozad2014learning}
Cozad, A., Sahinidis, N.~V., and Miller, D.~C.
\newblock Learning surrogate models for simulation-based optimization.
\newblock \emph{AIChE Journal}, 60\penalty0 (6):\penalty0 2211--2227, 2014.

\bibitem[Deng \& Zheng(2021)Deng and Zheng]{deng2021labels}
Deng, W. and Zheng, L.
\newblock Are labels always necessary for classifier accuracy evaluation?
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  2021.

\bibitem[Deng et~al.(2021)Deng, Gould, and Zheng]{deng2021does}
Deng, W., Gould, S., and Zheng, L.
\newblock What does rotation prediction tell us about classifier accuracy under
  varying testing environments?
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Der~Kiureghian \& Ditlevsen(2009)Der~Kiureghian and
  Ditlevsen]{der2009aleatory}
Der~Kiureghian, A. and Ditlevsen, O.
\newblock Aleatory or epistemic? does it matter?
\newblock \emph{Structural safety}, 31\penalty0 (2):\penalty0 105--112, 2009.

\bibitem[DeVries \& Taylor(2017)DeVries and Taylor]{devries2017improved}
DeVries, T. and Taylor, G.~W.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock \emph{arXiv:1708.04552}, 2017.

\bibitem[Eggensperger et~al.(2015)Eggensperger, Hutter, Hoos, and
  Leyton-Brown]{eggensperger2015efficient}
Eggensperger, K., Hutter, F., Hoos, H., and Leyton-Brown, K.
\newblock Efficient benchmarking of hyperparameter optimizers via surrogates.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2015.

\bibitem[Eggensperger et~al.(2018)Eggensperger, Lindauer, Hoos, Hutter, and
  Leyton-Brown]{eggensperger2018efficient}
Eggensperger, K., Lindauer, M., Hoos, H.~H., Hutter, F., and Leyton-Brown, K.
\newblock Efficient benchmarking of algorithm configurators via model-based
  surrogates.
\newblock \emph{Machine Learning}, 2018.

\bibitem[Farquhar et~al.(2020)Farquhar, Osborne, and
  Gal]{pmlr-v108-farquhar20a}
Farquhar, S., Osborne, M.~A., and Gal, Y.
\newblock Radial {B}ayesian neural networks: Beyond discrete support in
  large-scale {B}ayesian deep learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, volume~23, pp.\  1352--1362, 2020.

\bibitem[Farquhar et~al.(2021)Farquhar, Gal, and
  Rainforth]{Farquhar2021Statistical}
Farquhar, S., Gal, Y., and Rainforth, T.
\newblock On statistical bias in active learning: How and when to fix it.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Filstroff et~al.(2021)Filstroff, Sundin, Mikkola, Tiulpin,
  Kylm{\"a}oja, and Kaski]{filstroff2021targeted}
Filstroff, L., Sundin, I., Mikkola, P., Tiulpin, A., Kylm{\"a}oja, J., and
  Kaski, S.
\newblock Targeted active learning for bayesian decision-making.
\newblock \emph{arXiv:2106.04193}, 2021.

\bibitem[Foster et~al.(2021)Foster, Ivanova, Malik, and
  Rainforth]{foster2021deep}
Foster, A., Ivanova, D.~R., Malik, I., and Rainforth, T.
\newblock Deep adaptive design: Amortizing sequential bayesian experimental
  design.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Gal et~al.(2017)Gal, Islam, and Ghahramani]{gal2017deep}
Gal, Y., Islam, R., and Ghahramani, Z.
\newblock Deep bayesian active learning with image data.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1183--1192, 2017.

\bibitem[Guerra et~al.(2008)Guerra, Prud{\^e}ncio, and
  Ludermir]{guerra2008predicting}
Guerra, S.~B., Prud{\^e}ncio, R.~B., and Ludermir, T.~B.
\newblock Predicting the performance of learning algorithms using support
  vector machines as meta-regressors.
\newblock In \emph{International Conference on Artificial Neural Networks},
  2008.

\bibitem[Hanneke(2009)]{hanneke2009adaptive}
Hanneke, S.
\newblock Adaptive rates of convergence in active learning.
\newblock In \emph{COLT}, 2009.

\bibitem[Hanneke(2011)]{hanneke2011rates}
Hanneke, S.
\newblock Rates of convergence in active learning.
\newblock \emph{The Annals of Statistics}, pp.\  333--361, 2011.

\bibitem[{He} et~al.(2016){He}, {Zhang}, {Ren}, and {Sun}]{resnet}
{He}, K., {Zhang}, X., {Ren}, S., and {Sun}, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition}, pp.\
   770--778, 2016.

\bibitem[He et~al.(2021)He, Zhao, and Chu]{he2021automl}
He, X., Zhao, K., and Chu, X.
\newblock Automl: A survey of the state-of-the-art.
\newblock \emph{Knowledge-Based Systems}, 2021.

\bibitem[Houlsby et~al.(2011)Houlsby, Husz{\'a}r, Ghahramani, and
  Lengyel]{houlsby2011bayesian}
Houlsby, N., Husz{\'a}r, F., Ghahramani, Z., and Lengyel, M.
\newblock Bayesian active learning for classification and preference learning.
\newblock \emph{arXiv:1112.5745}, 2011.

\bibitem[Imberg et~al.(2020)Imberg, Jonasson, and
  Axelson-Fisk]{imberg_optimal_2020}
Imberg, H., Jonasson, J., and Axelson-Fisk, M.
\newblock Optimal sampling in unbiased active learning.
\newblock \emph{Artificial Intelligence and Statistics}, 23, 2020.

\bibitem[Ji et~al.(2021)Ji, Logan, Smyth, and Steyvers]{ji2020active}
Ji, D., Logan, R.~L., Smyth, P., and Steyvers, M.
\newblock Active bayesian assessment of black-box classifiers.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, volume~35, pp.\
   7935--7944, 2021.

\bibitem[Kahn \& Marshall(1953)Kahn and Marshall]{kahn1953methods}
Kahn, H. and Marshall, A.~W.
\newblock Methods of reducing sample size in monte carlo computations.
\newblock \emph{Journal of the Operations Research Society of America},
  1:\penalty0 263--278, 1953.

\bibitem[Kanagawa \& Hennig(2019)Kanagawa and Hennig]{kanagawa2019convergence}
Kanagawa, M. and Hennig, P.
\newblock Convergence guarantees for adaptive bayesian quadrature methods.
\newblock In \emph{Advances in neural information processing systems}, 2019.

\bibitem[Karamchandani et~al.(1989)Karamchandani, Bjerager, and
  Cornell]{karamchandani1989adaptive}
Karamchandani, A., Bjerager, P., and Cornell, C.
\newblock Adaptive importance sampling.
\newblock In \emph{Structural Safety and Reliability}, pp.\  855--862. ASCE,
  1989.

\bibitem[Katariya et~al.(2012)Katariya, Iyer, and Sarawagi]{katariya2012active}
Katariya, N., Iyer, A., and Sarawagi, S.
\newblock Active evaluation of classifiers on large datasets.
\newblock In \emph{International Conference on Data Mining}, volume~12, pp.\
  329--338, 2012.

\bibitem[Kendall \& Gal(2017)Kendall and Gal]{kendall_what_2017}
Kendall, A. and Gal, Y.
\newblock What {Uncertainties} {Do} {We} {Need} in {Bayesian} {Deep} {Learning}
  for {Computer} {Vision}?
\newblock \emph{Advances In Neural Information Processing Systems}, 2017.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv:1412.6980}, 2014.

\bibitem[Kirsch et~al.(2021)Kirsch, Rainforth, and Gal]{kirsch2021active}
Kirsch, A., Rainforth, T., and Gal, Y.
\newblock Active learning under pool set distribution shift and noisy data.
\newblock \emph{arXiv:2106.11719}, 2021.

\bibitem[Kleijnen(2009)]{kleijnen2009kriging}
Kleijnen, J.~P.
\newblock Kriging metamodeling in simulation: A review.
\newblock \emph{European journal of operational research}, 192\penalty0
  (3):\penalty0 707--716, 2009.

\bibitem[Kossen et~al.(2021)Kossen, Farquhar, Gal, and
  Rainforth]{kossen2021active}
Kossen, J., Farquhar, S., Gal, Y., and Rainforth, T.
\newblock Active testing: Sample-efficient model evaluation.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem[Kumar \& Raj(2018)Kumar and Raj]{kumar2018classifier}
Kumar, A. and Raj, B.
\newblock Classifier risk estimation under limited labeling resources.
\newblock In \emph{Pacific-Asia Conference on Knowledge Discovery and Data
  Mining}, pp.\  3--15, 2018.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{deep_ensembles}
Lakshminarayanan, B., Pritzel, A., and Blundell, C.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30, pp.\  6402--6413, 2017.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Lindley(1956)]{lindley1956}
Lindley, D.~V.
\newblock On a measure of the information provided by an experiment.
\newblock \emph{The Annals of Mathematical Statistics}, pp.\  986--1005, 1956.

\bibitem[Lowell et~al.(2019)Lowell, Lipton, and Wallace]{lowell_practical_2019}
Lowell, D., Lipton, Z.~C., and Wallace, B.~C.
\newblock Practical {Obstacles} to {Deploying} {Active} {Learning}.
\newblock \emph{Empirical Methods in Natural Language Processing}, November
  2019.

\bibitem[MacKay(1992)]{mackay1992practical}
MacKay, D.~J.
\newblock A practical bayesian framework for backpropagation networks.
\newblock \emph{Neural computation}, 4\penalty0 (3):\penalty0 448--472, 1992.

\bibitem[Moulines \& Bach(2011)Moulines and Bach]{bach_convergence}
Moulines, E. and Bach, F.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In Shawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., and
  Weinberger, K.~Q. (eds.), \emph{Advances in Neural Information Processing
  Systems}, volume~24, 2011.

\bibitem[Oh \& Berger(1992)Oh and Berger]{oh1992adaptive}
Oh, M.-S. and Berger, J.~O.
\newblock Adaptive importance sampling in monte carlo integration.
\newblock \emph{Journal of Statistical Computation and Simulation}, 41\penalty0
  (3-4):\penalty0 143--168, 1992.

\bibitem[O'Hagan(1991)]{o1991bayes}
O'Hagan, A.
\newblock Bayes--hermite quadrature.
\newblock \emph{Journal of statistical planning and inference}, 29\penalty0
  (3):\penalty0 245--260, 1991.

\bibitem[Owen \& Zhou(2000)Owen and Zhou]{owen2000safe}
Owen, A. and Zhou, Y.
\newblock Safe and effective importance sampling.
\newblock \emph{Journal of the American Statistical Association}, 95:\penalty0
  135--143, 2000.

\bibitem[Owen(2013)]{owen2013monte}
Owen, A.~B.
\newblock Monte carlo theory, methods and examples, 2013.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 32}, pp.\  8024--8035, 2019.

\bibitem[Qian et~al.(2005)Qian, Seepersad, Joseph, Allen, and
  Jeff~Wu]{qian2006building}
Qian, Z., Seepersad, C.~C., Joseph, V.~R., Allen, J.~K., and Jeff~Wu, C.
\newblock Building surrogate models based on detailed and approximate
  simulations.
\newblock \emph{Journal of Mechanical Design}, 128\penalty0 (4):\penalty0
  668--677, 2005.

\bibitem[Raj \& Bach(2021)Raj and Bach]{raj2021convergence}
Raj, A. and Bach, F.
\newblock Convergence of uncertainty sampling for active learning.
\newblock \emph{arXiv:2110.15784}, 2021.

\bibitem[Rasmussen(2003)]{rasmussen2003gaussian}
Rasmussen, C.~E.
\newblock Gaussian processes in machine learning.
\newblock In \emph{Summer school on machine learning}, pp.\  63--71, 2003.

\bibitem[Rasmussen \& Ghahramani(2003)Rasmussen and
  Ghahramani]{rasmussen2003bayesian}
Rasmussen, C.~E. and Ghahramani, Z.
\newblock Bayesian monte carlo.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  505--512, 2003.

\bibitem[Redyuk et~al.(2019)Redyuk, Schelter, Rukat, Markl, and
  Biessmann]{redyuk2019learning}
Redyuk, S., Schelter, S., Rukat, T., Markl, V., and Biessmann, F.
\newblock Learning to validate the predictions of black box machine learning
  models on unseen data.
\newblock In \emph{Proceedings of the Workshop on Human-In-the-Loop Data
  Analytics}, 2019.

\bibitem[Sabato \& Munos(2014)Sabato and Munos]{sabato2014active}
Sabato, S. and Munos, R.
\newblock Active regression by stratification.
\newblock \emph{Advances in Neural Information Processing Systems}, 27, 2014.

\bibitem[Sawade et~al.(2010)Sawade, Landwehr, Bickel, and
  Scheffer]{sawade2010active}
Sawade, C., Landwehr, N., Bickel, S., and Scheffer, T.
\newblock Active risk estimation.
\newblock In \emph{International Conference on Machine Learning}, volume~27,
  pp.\  951â€“958, 2010.

\bibitem[Sebastiani \& Wynn(2000)Sebastiani and Wynn]{sebastiani2000maximum}
Sebastiani, P. and Wynn, H.~P.
\newblock Maximum entropy sampling and optimal {B}ayesian experimental design.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 62\penalty0 (1):\penalty0 145--157, 2000.

\bibitem[Sener \& Savarese(2018)Sener and Savarese]{sener_active_2018}
Sener, O. and Savarese, S.
\newblock Active learning for convolutional neural networks: A core-set
  approach.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Settles(2010)]{Settles2010}
Settles, B.
\newblock Active {Learning} {Literature} {Survey}.
\newblock \emph{Machine Learning}, 2010.

\bibitem[Shahriari et~al.(2015)Shahriari, Swersky, Wang, Adams, and
  De~Freitas]{shahriari2015taking}
Shahriari, B., Swersky, K., Wang, Z., Adams, R.~P., and De~Freitas, N.
\newblock Taking the human out of the loop: A review of bayesian optimization.
\newblock \emph{Proceedings of the IEEE}, 104, 2015.

\bibitem[Sun et~al.(2021)Sun, Hou, Li, and Zheng]{sun2021label}
Sun, X., Hou, Y., Li, H., and Zheng, L.
\newblock Label-free model evaluation with semi-structured dataset
  representations.
\newblock \emph{arXiv:2112.00694}, 2021.

\bibitem[Talagala et~al.(2022)Talagala, Li, and Kang]{talagala2022fformpp}
Talagala, T.~S., Li, F., and Kang, Y.
\newblock Fformpp: Feature-based forecast model performance prediction.
\newblock \emph{International Journal of Forecasting}, 2022.

\bibitem[Van~Rossum \& Drake(2009)Van~Rossum and Drake]{python}
Van~Rossum, G. and Drake, F.~L.
\newblock \emph{Python 3 Reference Manual}.
\newblock CreateSpace, Scotts Valley, CA, 2009.
\newblock ISBN 1441412697.

\bibitem[Vanschoren(2018)]{vanschoren2018meta}
Vanschoren, J.
\newblock Meta-learning: A survey.
\newblock \emph{arXiv:1810.03548}, 2018.

\bibitem[Wang et~al.(2021)Wang, Sun, and Grosse]{wang2021beyond}
Wang, C., Sun, S., and Grosse, R.
\newblock Beyond marginal uncertainty: How accurately can bayesian regression
  models estimate posterior predictive correlations?
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  2476--2484, 2021.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{fmnist}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-{MNIST}: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\bibitem[Yilmaz et~al.(2021)Yilmaz, Hayes, Habib, Burgess, and
  Barber]{yilmaz2021sample}
Yilmaz, E., Hayes, P., Habib, R., Burgess, J., and Barber, D.
\newblock Sample efficient model evaluation.
\newblock \emph{arXiv:2109.12043}, 2021.

\bibitem[Yu et~al.(2006)Yu, Bi, and Tresp]{yu2006active}
Yu, K., Bi, J., and Tresp, V.
\newblock Active learning via transductive experimental design.
\newblock In \emph{International Conference on Machine learning}, volume~23,
  2006.

\bibitem[Yu \& Zhu(2020)Yu and Zhu]{yu2020hyper}
Yu, T. and Zhu, H.
\newblock Hyper-parameter optimization: A review of algorithms and
  applications.
\newblock \emph{arXiv:2003.05689}, 2020.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and Komodakis]{BMVC2016_87}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock In \emph{British Machine Vision Conference}, pp.\  87.1--87.12, 2016.

\bibitem[Zhang et~al.(2018)Zhang, B{\"u}tepage, Kjellstr{\"o}m, and
  Mandt]{zhang2018advances}
Zhang, C., B{\"u}tepage, J., Kjellstr{\"o}m, H., and Mandt, S.
\newblock Advances in variational inference.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 41\penalty0 (8):\penalty0 2008--2026, 2018.

\end{thebibliography}
