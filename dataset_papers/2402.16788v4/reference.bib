@article{ubaru2017fast,
  title={Fast estimation of tr(f(A)) via stochastic Lanczos quadrature},
  author={Ubaru, Shashanka and Chen, Jie and Saad, Yousef},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={38},
  number={4},
  pages={1075--1099},
  year={2017},
  publisher={SIAM}
}

@article{han2017approximating,
  title={Approximating spectral sums of large-scale matrices using stochastic Chebyshev approximations},
  author={Han, Insu and Malioutov, Dmitry and Avron, Haim and Shin, Jinwoo},
  journal={SIAM Journal on Scientific Computing},
  volume={39},
  number={4},
  pages={A1558--A1585},
  year={2017},
  publisher={SIAM}
}

@article{lin2016approximating,
  title={Approximating spectral densities of large matrices},
  author={Lin, Lin and Saad, Yousef and Yang, Chao},
  journal={SIAM review},
  volume={58},
  number={1},
  pages={34--65},
  year={2016},
  publisher={SIAM}
}

@article{bai1996some,
  title={Some large-scale matrix computation problems},
  author={Bai, Zhaojun and Fahey, Gark and Golub, Gene},
  journal={Journal of Computational and Applied Mathematics},
  volume={74},
  number={1-2},
  pages={71--89},
  year={1996},
  publisher={Elsevier}
}

@article{hutchinson1989stochastic,
  title={A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines},
  author={Hutchinson, Michael F},
  journal={Communications in Statistics-Simulation and Computation},
  volume={18},
  number={3},
  pages={1059--1076},
  year={1989},
  publisher={Taylor \& Francis}
}

@book{burden2011numerical,
  title={Numerical analysis},
  author={Burden, Richard L},
  year={2011},
  publisher={Brooks/Cole Cengage Learning}
}

@book{golub2009matrices,
  title={Matrices, moments and quadrature with applications},
  author={Golub, Gene H and Meurant, G{\'e}rard},
  volume={30},
  year={2009},
  publisher={Princeton University Press}
}

@article{lanczos1950iteration,
  title={An iteration method for the solution of the eigenvalue problem of linear differential and integral operators},
  author={Lanczos, Cornelius},
  year={1950},
  publisher={United States Governm. Press Office Los Angeles, CA}
}

@article{golub1994estimates,
  title={Estimates in quadratic formulas},
  author={Golub, Gene H and Strako{\v{s}}, Zden{\v{e}}k},
  journal={Numerical Algorithms},
  volume={8},
  pages={241--268},
  year={1994},
  publisher={Springer}
}

@article{bai1996bounds,
  title={Bounds for the trace of the inverse and the determinant of symmetric positive definite matrices},
  author={Bai, Zhaojun and Golub, Gene H},
  journal={Annals of Numerical Mathematics},
  volume={4},
  pages={29--38},
  year={1996},
  publisher={SCIENCE PUBLISHERS}
}

@inproceedings{ghorbani2019investigation,
  title={An investigation into neural net optimization via hessian eigenvalue density},
  author={Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
  booktitle={International Conference on Machine Learning},
  pages={2232--2241},
  year={2019},
  organization={PMLR}
}


@article{golub1969calculation,
  title={Calculation of Gauss quadrature rules},
  author={Golub, Gene H and Welsch, John H},
  journal={Mathematics of computation},
  volume={23},
  number={106},
  pages={221--230},
  year={1969}
}

@article{brezinski1990direct,
  title={A direct proof of the Christoffel-Darboux identity and its equivalence to the recurrence relationship},
  author={Brezinski, Claude},
  journal={Journal of Computational and Applied Mathematics},
  volume={32},
  number={1-2},
  pages={17--25},
  year={1990},
  publisher={Elsevier}
}


@article{epperson2013introduction,
  title={An introduction to numerical methods and analysis},
  author={Epperson, James F},
  year={2013},
  publisher={John Wiley \& Sons, Inc.}
}


@book{golub2013matrix,
  title={Matrix computations},
  author={Golub, Gene H and Van Loan, Charles F},
  year={2013},
  publisher={JHU press}
}


@article{avron2011randomized,
  title={Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix},
  author={Avron, Haim and Toledo, Sivan},
  journal={Journal of the ACM (JACM)},
  volume={58},
  number={2},
  pages={1--34},
  year={2011},
  publisher={ACM New York, NY, USA}
}


@article{pearlmutter1994fast,
  title={Fast exact multiplication by the Hessian},
  author={Pearlmutter, Barak A},
  journal={Neural computation},
  volume={6},
  number={1},
  pages={147--160},
  year={1994},
  publisher={MIT Press}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

  @misc{enwiki:1191539517,
    author = "{Wikipedia}",
    title = "Gaussian quadrature --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2023",
    url = "https://en.wikipedia.org/w/index.php?title=Gaussian_quadrature&oldid=1191539517",
    note = "[Online; accessed 20-January-2024]"
  }


@book{cullum2002lanczos,
  title={Lanczos algorithms for large symmetric eigenvalue computations: Vol. I: Theory},
  author={Cullum, Jane K and Willoughby, Ralph A},
  year={2002},
  publisher={SIAM}
}

@book{saad2011numerical,
  title={Numerical methods for large eigenvalue problems: revised edition},
  author={Saad, Yousef},
  year={2011},
  publisher={SIAM}
}

@inproceedings{yao2020pyhessian,
  title={Pyhessian: Neural networks through the lens of the hessian},
  author={Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W},
  booktitle={2020 IEEE international conference on big data (Big data)},
  pages={581--590},
  year={2020},
  organization={IEEE}
}


@article{beck2013convergence,
  title={On the convergence of block coordinate descent type methods},
  author={Beck, Amir and Tetruashvili, Luba},
  journal={SIAM journal on Optimization},
  volume={23},
  number={4},
  pages={2037--2060},
  year={2013},
  publisher={SIAM}
}

@article{forsythe1955best,
  title={On best conditioned matrices},
  author={Forsythe, George E and Straus, Ernst G},
  journal={Proceedings of the American Mathematical Society},
  volume={6},
  number={3},
  pages={340--345},
  year={1955},
  publisher={JSTOR}
}

@article{qu2022optimal,
  title={Optimal Diagonal Preconditioning: Theory and Practice},
  author={Qu, Zhaonan and Gao, Wenzhi and Hinder, Oliver and Ye, Yinyu and Zhou, Zhengyuan},
  journal={arXiv preprint arXiv:2209.00809},
  year={2022}
}

@article{sun2021worst,
  title={Worst-case complexity of cyclic coordinate descent: O (n\^{} 2) O (n 2) gap with randomized version},
  author={Sun, Ruoyu and Ye, Yinyu},
  journal={Mathematical Programming},
  volume={185},
  pages={487--520},
  year={2021},
  publisher={Springer}
}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={pmlr}
}

@article{sun2019optimization,
  title={Optimization for deep learning: theory and algorithms},
  author={Sun, Ruoyu},
  journal={arXiv preprint arXiv:1912.08957},
  year={2019}
}

@article{jiang2023does,
  title={How Does Adaptive Optimization Impact Local Neural Network Geometry?},
  author={Jiang, Kaiqi and Malik, Dhruv and Li, Yuanzhi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@inproceedings{rasley2020deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}



@misc{gokaslan2019openwebtext,
  title={Openwebtext corpus},
  author={Gokaslan, Aaron and Cohen, Vanya and Pavlick, Ellie and Tellex, Stefanie},
  year={2019}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}


@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}

@article{luo2023came,
  title={CAME: Confidence-guided Adaptive Memory Efficient Optimization},
  author={Luo, Yang and Ren, Xiaozhe and Zheng, Zangwei and Jiang, Zhuo and Jiang, Xin and You, Yang},
  journal={arXiv preprint arXiv:2307.02047},
  year={2023}
}

@article{liu2023sophia,
  title={Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training},
  author={Liu, Hong and Li, Zhiyuan and Hall, David and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2305.14342},
  year={2023}
}

@misc{lit-gpt-2023,
  author       = {Lightning AI},
  title        = {Lit-GPT},
  howpublished = {\url{https://github.com/Lightning-AI/lit-gpt}},
  year         = {2023},
}


@inproceedings{bock2019non,
  title={Non-convergence and limit cycles in the Adam optimizer},
  author={Bock, Sebastian and Wei{\ss}, Martin},
  booktitle={Artificial Neural Networks and Machine Learning--ICANN 2019: Deep Learning: 28th International Conference on Artificial Neural Networks, Munich, Germany, September 17--19, 2019, Proceedings, Part II 28},
  pages={232--243},
  year={2019},
  organization={Springer}
}

@article{da2020general,
  title={A general system of differential equations to model first-order adaptive algorithms},
  author={Da Silva, Andr{\'e} Belotto and Gazeau, Maxime},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5072--5113},
  year={2020},
  publisher={JMLRORG}
}

 @misc{enwiki:1182147775,
    author = "{Wikipedia contributors}",
    title = "Gershgorin circle theorem --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2023",
    url = "https://en.wikipedia.org/w/index.php?title=Gershgorin_circle_theorem&oldid=1182147775",
    note = "[Online; accessed 30-January-2024]"
  }


@book{nesterov2013introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{goh2017why,
  author = {Goh, Gabriel},
  title = {Why Momentum Really Works},
  journal = {Distill},
  year = {2017},
  url = {http://distill.pub/2017/momentum},
  doi = {10.23915/distill.00006}
}

@inproceedings{goujaud2022super,
  title={Super-acceleration with cyclical step-sizes},
  author={Goujaud, Baptiste and Scieur, Damien and Dieuleveut, Aymeric and Taylor, Adrien B and Pedregosa, Fabian},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3028--3065},
  year={2022},
  organization={PMLR}
}


@techreport{collobert2004large,
  title={Large scale machine learning},
  author={Collobert, Ronan},
  year={2004},
  institution={Universit{\'e} de Paris VI}
}

@article{roux2007topmoumoute,
  title={Topmoumoute online natural gradient algorithm},
  author={Roux, Nicolas and Manzagol, Pierre-Antoine and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={20},
  year={2007}
}

@inproceedings{martens2015optimizing,
  title={Optimizing neural networks with kronecker-factored approximate curvature},
  author={Martens, James and Grosse, Roger},
  booktitle={International conference on machine learning},
  pages={2408--2417},
  year={2015},
  organization={PMLR}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{kopf2023openassistant,
  title={OpenAssistant Conversations--Democratizing Large Language Model Alignment},
  author={K{\"o}pf, Andreas and Kilcher, Yannic and von R{\"u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi-Rui and Stevens, Keith and Barhoum, Abdullah and Duc, Nguyen Minh and Stanley, Oliver and Nagyfi, Rich{\'a}rd and others},
  journal={arXiv preprint arXiv:2304.07327},
  year={2023}
}
@article{peng2023instruction,
  title={Instruction tuning with gpt-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@Misc{llama-factory,
  title = {LLaMA Factory},
  author = {hiyouga},
  howpublished = {\url{https://github.com/hiyouga/LLaMA-Factory}},
  year = {2023}
}



@article{lv2023full,
  title={Full Parameter Fine-tuning for Large Language Models with Limited Resources},
  author={Lv, Kai and Yang, Yuqing and Liu, Tengxiao and Gao, Qinghui and Guo, Qipeng and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2306.09782},
  year={2023}
}

@article{press2016using,
  title={Using the output embedding to improve language models},
  author={Press, Ofir and Wolf, Lior},
  journal={arXiv preprint arXiv:1608.05859},
  year={2016}
}


@article{zhang2020adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15383--15393},
  year={2020}
}


@article{zhang2019gradient,
  title={Why gradient clipping accelerates training: A theoretical justification for adaptivity},
  author={Zhang, Jingzhao and He, Tianxing and Sra, Suvrit and Jadbabaie, Ali},
  journal={arXiv preprint arXiv:1905.11881},
  year={2019}
}

@article{crawshaw2022robustness,
  title={Robustness to unbounded smoothness of generalized signsgd},
  author={Crawshaw, Michael and Liu, Mingrui and Orabona, Francesco and Zhang, Wei and Zhuang, Zhenxun},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9955--9968},
  year={2022}
}

@article{kunstner2023noise,
  title={Noise is not the main factor behind the gap between sgd and adam on transformers, but sign descent might be},
  author={Kunstner, Frederik and Chen, Jacques and Lavington, Jonathan Wilder and Schmidt, Mark},
  journal={arXiv preprint arXiv:2304.13960},
  year={2023}
}


@article{sagun2016eigenvalues,
  title={Eigenvalues of the hessian in deep learning: Singularity and beyond},
  author={Sagun, Levent and Bottou, Leon and LeCun, Yann},
  journal={arXiv preprint arXiv:1611.07476},
  year={2016}
}

@article{papyan2018full,
  title={The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample size},
  author={Papyan, Vardan},
  journal={arXiv preprint arXiv:1811.07062},
  year={2018}
}


@article{adams2018estimating,
  title={Estimating the spectral density of large implicit matrices},
  author={Adams, Ryan P and Pennington, Jeffrey and Johnson, Matthew J and Smith, Jamie and Ovadia, Yaniv and Patton, Brian and Saunderson, James},
  journal={arXiv preprint arXiv:1802.03451},
  year={2018}
}


@article{sagun2017empirical,
  title={Empirical analysis of the hessian of over-parametrized neural networks},
  author={Sagun, Levent and Evci, Utku and Guney, V Ugur and Dauphin, Yann and Bottou, Leon},
  journal={arXiv preprint arXiv:1706.04454},
  year={2017}
}


@inproceedings{sankar2021deeper,
  title={A deeper look at the hessian eigenspectrum of deep neural networks and its applications to regularization},
  author={Sankar, Adepu Ravi and Khasbage, Yash and Vigneswaran, Rahul and Balasubramanian, Vineeth N},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={11},
  pages={9481--9488},
  year={2021}
}

@article{liao2021hessian,
  title={Hessian eigenspectra of more realistic nonlinear models},
  author={Liao, Zhenyu and Mahoney, Michael W},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={20104--20117},
  year={2021}
}


@misc{ enwiki:1169517501,
    author = "{Wikipedia contributors}",
    title = "Jensen–Shannon divergence --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2023",
    url = "https://en.wikipedia.org/w/index.php?title=Jensen%E2%80%93Shannon_divergence&oldid=1169517501",
    note = "[Online; accessed 2-February-2024]"
  }

@article{lv2023adalomo,
  title={AdaLomo: Low-memory Optimization with Adaptive Learning Rate},
  author={Lv, Kai and Yan, Hang and Guo, Qipeng and Lv, Haijun and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2310.10195},
  year={2023}
}

@inproceedings{balles2018dissecting,
  title={Dissecting adam: The sign, magnitude and variance of stochastic gradients},
  author={Balles, Lukas and Hennig, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={404--413},
  year={2018},
  organization={PMLR}
}

@article{papyan2019measurements,
  title={Measurements of three-level hierarchical structure in the outliers in the spectrum of deepnet hessians},
  author={Papyan, Vardan},
  journal={arXiv preprint arXiv:1901.08244},
  year={2019}
}

@article{van1969condition,
  title={Condition numbers and equilibration of matrices},
  author={Van der Sluis, Abraham},
  journal={Numerische Mathematik},
  volume={14},
  number={1},
  pages={14--23},
  year={1969},
  publisher={Springer-Verlag Berlin/Heidelberg}
}

@article{noci2022signal,
  title={Signal propagation in transformers: Theoretical perspectives and the role of rank collapse},
  author={Noci, Lorenzo and Anagnostidis, Sotiris and Biggio, Luca and Orvieto, Antonio and Singh, Sidak Pal and Lucchi, Aurelien},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27198--27211},
  year={2022}
}

@article{liu2020understanding,
  title={Understanding the difficulty of training transformers},
  author={Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
  journal={arXiv preprint arXiv:2004.08249},
  year={2020}
}

@article{liu2019variance,
  title={On the variance of the adaptive learning rate and beyond. arXiv 2019},
  author={Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  journal={arXiv preprint arXiv:1908.03265},
  year={2019}
}

@inproceedings{xiong2020layer,
  title={On layer normalization in the transformer architecture},
  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle={International Conference on Machine Learning},
  pages={10524--10533},
  year={2020},
  organization={PMLR}
}

@inproceedings{dong2021attention,
  title={Attention is not all you need: Pure attention loses rank doubly exponentially with depth},
  author={Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
  booktitle={International Conference on Machine Learning},
  pages={2793--2803},
  year={2021},
  organization={PMLR}
}

@inproceedings{huang2020improving,
  title={Improving transformer optimization through better initialization},
  author={Huang, Xiao Shi and Perez, Felipe and Ba, Jimmy and Volkovs, Maksims},
  booktitle={International Conference on Machine Learning},
  pages={4475--4483},
  year={2020},
  organization={PMLR}
}

@article{wang2022deepnet,
  title={Deepnet: Scaling transformers to 1,000 layers},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dongdong and Wei, Furu},
  journal={arXiv preprint arXiv:2203.00555},
  year={2022}
}


@article{zhang2019improving,
  title={Improving deep transformer with depth-scaled initialization and merged attention},
  author={Zhang, Biao and Titov, Ivan and Sennrich, Rico},
  journal={arXiv preprint arXiv:1908.11365},
  year={2019}
}

@article{wortsman2023small,
  title={Small-scale proxies for large-scale Transformer training instabilities},
  author={Wortsman, Mitchell and Liu, Peter J and Xiao, Lechao and Everett, Katie and Alemi, Alex and Adlam, Ben and Co-Reyes, John D and Gur, Izzeddin and Kumar, Abhishek and Novak, Roman and others},
  journal={arXiv preprint arXiv:2309.14322},
  year={2023}
}



@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}

@article{chen2018best,
  title={The best of both worlds: Combining recent advances in neural machine translation},
  author={Chen, Mia Xu and Firat, Orhan and Bapna, Ankur and Johnson, Melvin and Macherey, Wolfgang and Foster, George and Jones, Llion and Parmar, Niki and Schuster, Mike and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:1804.09849},
  year={2018}
}

@inproceedings{zhai2023stabilizing,
  title={Stabilizing transformer training by preventing attention entropy collapse},
  author={Zhai, Shuangfei and Likhomanenko, Tatiana and Littwin, Etai and Busbridge, Dan and Ramapuram, Jason and Zhang, Yizhe and Gu, Jiatao and Susskind, Joshua M},
  booktitle={International Conference on Machine Learning},
  pages={40770--40803},
  year={2023},
  organization={PMLR}
}

@article{yang2022tensor,
  title={Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer},
  author={Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2203.03466},
  year={2022}
}

@inproceedings{bachlechner2021rezero,
  title={Rezero is all you need: Fast convergence at large depth},
  author={Bachlechner, Thomas and Majumder, Bodhisattwa Prasad and Mao, Henry and Cottrell, Gary and McAuley, Julian},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={1352--1361},
  year={2021},
  organization={PMLR}
}

@article{wang2019learning,
  title={Learning deep transformer models for machine translation},
  author={Wang, Qiang and Li, Bei and Xiao, Tong and Zhu, Jingbo and Li, Changliang and Wong, Derek F and Chao, Lidia S},
  journal={arXiv preprint arXiv:1906.01787},
  year={2019}
}

@article{nguyen2019transformers,
  title={Transformers without tears: Improving the normalization of self-attention},
  author={Nguyen, Toan Q and Salazar, Julian},
  journal={arXiv preprint arXiv:1910.05895},
  year={2019}
}

@inproceedings{dehghani2023scaling,
  title={Scaling vision transformers to 22 billion parameters},
  author={Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas Peter and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and others},
  booktitle={International Conference on Machine Learning},
  pages={7480--7512},
  year={2023},
  organization={PMLR}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{yang2023baichuan,
  title={Baichuan 2: Open large-scale language models},
  author={Yang, Aiyuan and Xiao, Bin and Wang, Bingning and Zhang, Borong and Bian, Ce and Yin, Chao and Lv, Chenxu and Pan, Da and Wang, Dian and Yan, Dong and others},
  journal={arXiv preprint arXiv:2309.10305},
  year={2023}
}

@article{merrill2020effects,
  title={Effects of parameter norm growth during transformer training: Inductive bias from gradient descent},
  author={Merrill, William and Ramanujan, Vivek and Goldberg, Yoav and Schwartz, Roy and Smith, Noah},
  journal={arXiv preprint arXiv:2010.09697},
  year={2020}
}

@article{zhang2022adam,
  title={Adam can converge without any modification on update rules},
  author={Zhang, Yushun and Chen, Congliang and Shi, Naichen and Sun, Ruoyu and Luo, Zhi-Quan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={28386--28399},
  year={2022}
}


@inproceedings{chen2021heavy,
  title={Heavy-tailed noise does not explain the gap between SGD and Adam on Transformers},
  author={Chen, Jacques and Kunstner, Frederik and Schmidt, Mark},
  booktitle={13th Annual Workshop on Optimization for Machine Learning},
  year={2021}
}
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}


@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@article{gur2018gradient,
  title={Gradient descent happens in a tiny subspace},
  author={Gur-Ari, Guy and Roberts, Daniel A and Dyer, Ethan},
  journal={arXiv preprint arXiv:1812.04754},
  year={2018}
}


@article{openai2020scaling,
  title={Scaling laws for neural language models},
  author={OpenAI},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{zhang2019algorithmic,
  title={Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model},
  author={Zhang, Guodong and Li, Lala and Nado, Zachary and Martens, James and Sachdeva, Sushant and Dahl, George and Shallue, Chris and Grosse, Roger B},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{gilmer2021loss,
  title={A loss curvature perspective on training instability in deep learning},
  author={Gilmer, Justin and Ghorbani, Behrooz and Garg, Ankush and Kudugunta, Sneha and Neyshabur, Behnam and Cardoze, David and Dahl, George and Nado, Zachary and Firat, Orhan},
  journal={arXiv preprint arXiv:2110.04369},
  year={2021}
}

@inproceedings{cohen2020gradient,
  title={Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability},
  author={Cohen, Jeremy and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{jastrzkebski2020break,
  title={THE BREAK-EVEN POINT ON OPTIMIZATION TRAJECTORIES OF DEEP NEURAL NETWORKS},
  author={Jastrz{\k{e}}bski, Stanis{\l}aw and Szymczak, Maciej and Fort, Stanislav and Arpit, Devansh and Tabor, Jacek and Cho, Kyunghyun and Geras, Krzysztof},
  booktitle={8th International Conference on Learning Representations, ICLR 2020},
  year={2020}
}

@article{cohen2022adaptive,
  title={Adaptive gradient methods at the edge of stability},
  author={Cohen, Jeremy M and Ghorbani, Behrooz and Krishnan, Shankar and Agarwal, Naman and Medapati, Sourabh and Badura, Michal and Suo, Daniel and Cardoze, David and Nado, Zachary and Dahl, George E and others},
  journal={arXiv preprint arXiv:2207.14484},
  year={2022}
}

@article{wu2018sgd,
  title={How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective},
  author={Wu, Lei and Ma, Chao and others},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}


@article{papyan2020traces,
  title={Traces of class/cross-class structure pervade deep learning spectra},
  author={Papyan, Vardan},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={10197--10260},
  year={2020},
  publisher={JMLRORG}
}

@article{yao2018hessian,
  title={Hessian-based analysis of large batch training and robustness to adversaries},
  author={Yao, Zhewei and Gholami, Amir and Lei, Qi and Keutzer, Kurt and Mahoney, Michael W},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{wu2020dissecting,
  title={Dissecting hessian: Understanding common structure of hessian in neural networks},
  author={Wu, Yikai and Zhu, Xingyu and Wu, Chenwei and Wang, Annie and Ge, Rong},
  journal={arXiv preprint arXiv:2010.04261},
  year={2020}
}

@inproceedings{reddi2018convergence,
  title={On the Convergence of Adam and Beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{shi2020rmsprop,
  title={RMSprop converges with proper hyper-parameter},
  author={Shi, Naichen and Li, Dawei and Hong, Mingyi and Sun, Ruoyu},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{wang2022provable,
  title={Provable adaptivity in adam},
  author={Wang, Bohan and Zhang, Yushun and Zhang, Huishuai and Meng, Qi and Ma, Zhi-Ming and Liu, Tie-Yan and Chen, Wei},
  journal={arXiv preprint arXiv:2208.09900},
  year={2022}
}

@article{li2023convergence,
  title={Convergence of Adam under relaxed assumptions},
  author={Li, Haochuan and Rakhlin, Alexander and Jadbabaie, Ali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{pan2023toward,
  title={Toward Understanding Why Adam Converges Faster Than SGD for Transformers},
  author={Pan, Yan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2306.00204},
  year={2023}
}

@inproceedings{staib2019escaping,
  title={Escaping saddle points with adaptive gradient methods},
  author={Staib, Matthew and Reddi, Sashank and Kale, Satyen and Kumar, Sanjiv and Sra, Suvrit},
  booktitle={International Conference on Machine Learning},
  pages={5956--5965},
  year={2019},
  organization={PMLR}
}

@inproceedings{xie2022adaptive,
  title={Adaptive inertia: Disentangling the effects of adaptive learning rate and momentum},
  author={Xie, Zeke and Wang, Xinrui and Zhang, Huishuai and Sato, Issei and Sugiyama, Masashi},
  booktitle={International conference on machine learning},
  pages={24430--24459},
  year={2022},
  organization={PMLR}
}


@inproceedings{bernstein2018signsgd,
  title={signSGD: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle={International Conference on Machine Learning},
  pages={560--569},
  year={2018},
  organization={PMLR}
}

@inproceedings{zou2019sufficient,
  title={A sufficient condition for convergences of adam and rmsprop},
  author={Zou, Fangyu and Shen, Li and Jie, Zequn and Zhang, Weizhong and Liu, Wei},
  booktitle={Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition},
  pages={11127--11135},
  year={2019}
}

@inproceedings{chen2019convergence,
  title={On the convergence of a class of Adam-type algorithms for non-convex optimization},
  author={Chen, Xiangyi and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
  booktitle={7th International Conference on Learning Representations, ICLR 2019},
  year={2019}
}

@article{chen2022towards,
  title={Towards practical adam: Non-convexity, convergence theory, and mini-batch acceleration},
  author={Chen, Congliang and Shen, Li and Zou, Fangyu and Liu, Wei},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={10411--10457},
  year={2022},
  publisher={JMLRORG}
}

@article{zaheer2018adaptive,
  title={Adaptive methods for nonconvex optimization},
  author={Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{defossez2022simple,
  title={A Simple Convergence Proof of Adam and Adagrad},
  author={D{\'e}fossez, Alexandre and Bottou, Leon and Bach, Francis and Usunier, Nicolas},
  journal={Transactions on Machine Learning Research},
  year={2022}
}

@article{wang2023closing,
  title={Closing the gap between the upper bound and lower bound of Adam's iteration complexity},
  author={Wang, Bohan and Fu, Jingwen and Zhang, Huishuai and Zheng, Nanning and Chen, Wei},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{zhou2018convergence,
  title={On the convergence of adaptive gradient methods for nonconvex optimization},
  author={Zhou, Dongruo and Chen, Jinghui and Cao, Yuan and Tang, Yiqi and Yang, Ziyan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1808.05671},
  year={2018}
}

@inproceedings{luo2018adaptive,
  title={Adaptive Gradient Methods with Dynamic Bound of Learning Rate},
  author={Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@inproceedings{wang2023convergence,
  title={Convergence of adagrad for non-convex objectives: Simple proofs and relaxed assumptions},
  author={Wang, Bohan and Zhang, Huishuai and Ma, Zhiming and Chen, Wei},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={161--190},
  year={2023},
  organization={PMLR}
}

@article{gadat2022asymptotic,
  title={Asymptotic study of stochastic adaptive algorithms in non-convex landscape},
  author={Gadat, S{\'e}bastien and Gavra, Ioana},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={10357--10410},
  year={2022},
  publisher={JMLRORG}
}

@article{chaudhari2019entropy,
  title={Entropy-sgd: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124018},
  year={2019},
  publisher={IOP Publishing}
}


@incollection{lecun2002efficient,
  title={Efficient backprop},
  author={LeCun, Yann and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle={Neural networks: Tricks of the trade},
  pages={9--50},
  year={2002},
  publisher={Springer}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18},
  pages={234--241},
  year={2015},
  organization={Springer}
}


@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{molybog2023theory,
  title={A theory on adam instability in large-scale machine learning},
  author={Molybog, Igor and Albert, Peter and Chen, Moya and DeVito, Zachary and Esiobu, David and Goyal, Naman and Koura, Punit Singh and Narang, Sharan and Poulton, Andrew and Silva, Ruan and others},
  journal={arXiv preprint arXiv:2304.09871},
  year={2023}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{tolstikhin2021mlp,
  title={Mlp-mixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={24261--24272},
  year={2021}
}



@article{malladi2023fine,
  title={Fine-tuning language models with just forward passes},
  author={Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={53038--53075},
  year={2023}
}

@article{ahn2023linear,
  title={Linear attention is (maybe) all you need (to understand transformer optimization)},
  author={Ahn, Kwangjun and Cheng, Xiang and Song, Minhak and Yun, Chulhee and Jadbabaie, Ali and Sra, Suvrit},
  journal={arXiv preprint arXiv:2310.01082},
  year={2023}
}

@article{kunstner2024heavy,
  title={Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models},
  author={Kunstner, Frederik and Yadav, Robin and Milligan, Alan and Schmidt, Mark and Bietti, Alberto},
  journal={arXiv preprint arXiv:2402.19449},
  year={2024}
}

@article{xiao2021early,
  title={Early convolutions help transformers see better},
  author={Xiao, Tete and Singh, Mannat and Mintun, Eric and Darrell, Trevor and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={30392--30400},
  year={2021}
}

@article{wang2024adagc,
  title={AdaGC: A Novel Adaptive Optimization Algorithm with Gradient Bias Correction},
  author={Wang, Qi and Su, Feng and Dai, Shipeng and Lu, Xiaojun and Liu, Yang},
  journal={Expert Systems with Applications},
  volume={256},
  pages={124956},
  year={2024},
  publisher={Elsevier}
}

@article{lavezzi2022nonlinear,
  title={Nonlinear programming solvers for unconstrained and constrained optimization problems: a benchmark analysis},
  author={Lavezzi, Giovanni and Guye, Kidus and Ciarci{\`a}, Marco},
  journal={arXiv preprint arXiv:2204.05297},
  year={2022}
}