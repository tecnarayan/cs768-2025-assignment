\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Chen et~al.(2021)Chen, Fan, and Panda]{chen2021crossvit}
Chun-Fu~Richard Chen, Quanfu Fan, and Rameswar Panda.
\newblock Crossvit: Cross-attention multi-scale vision transformer for image classification.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pages 357--366, 2021.

\bibitem[Chen et~al.(2023)Chen, Li, Arik, Yoder, and Pfister]{chen2023tsmixer}
Si-An Chen, Chun-Liang Li, Sercan~O Arik, Nathanael~Christian Yoder, and Tomas Pfister.
\newblock Tsmixer: An all-mlp architecture for time series forecast-ing.
\newblock \emph{Transactions on Machine Learning Research}, 2023.

\bibitem[Das et~al.(2024)Das, Kong, Sen, and Zhou]{das2023decoder}
Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou.
\newblock A decoder-only foundation model for time-series forecasting.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2024.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Kristina]{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Lee Kristina.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{Proceedings of NAACL-HLT}, pages 4171--4186, 2019.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiy2021an}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Ekambaram et~al.(2023)Ekambaram, Jati, Nguyen, Sinthong, and Kalagnanam]{ekambaram2023tsmixer}
Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam.
\newblock Tsmixer: Lightweight mlp-mixer model for multivariate time series forecasting.
\newblock In \emph{Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages 459--469, 2023.

\bibitem[Goswami et~al.(2024)Goswami, Szafer, Choudhry, Cai, Li, and Dubrawski]{goswami2024moment}
Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski.
\newblock Moment: A family of open time-series foundation models.
\newblock In \emph{International Conference on Machine Learning}, 2024.

\bibitem[Huang et~al.(2016)Huang, Sun, Liu, Sedra, and Weinberger]{huang2016deep}
Gao Huang, Yu~Sun, Zhuang Liu, Daniel Sedra, and Kilian~Q Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In \emph{Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14}, pages 646--661. Springer, 2016.

\bibitem[Li et~al.(2019)Li, Jin, Xuan, Zhou, Chen, Wang, and Yan]{li2019enhancing}
Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan.
\newblock Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Li et~al.(2023)Li, Qi, Li, and Xu]{li2023revisiting}
Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu.
\newblock Revisiting long-term time series forecasting: An investigation on linear mapping.
\newblock \emph{arXiv preprint arXiv:2305.10721}, 2023.

\bibitem[Liu et~al.(2021)Liu, Yu, Liao, Li, Lin, Liu, and Dustdar]{liu2021pyraformer}
Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex~X Liu, and Schahram Dustdar.
\newblock Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting.
\newblock In \emph{International conference on learning representations}, 2021.

\bibitem[Liu et~al.(2022)Liu, Wu, Wang, and Long]{liu2022non}
Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long.
\newblock Non-stationary transformers: Exploring the stationarity in time series forecasting.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 9881--9893, 2022.

\bibitem[Liu et~al.(2024)Liu, Hu, Zhang, Wu, Wang, Ma, and Long]{liu2024itransformer}
Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long.
\newblock itransformer: Inverted transformers are effective for time series forecasting.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=JePfAI8fah}.

\bibitem[Nie et~al.(2023)Nie, Nguyen, Sinthong, and Kalagnanam]{nie2023patchtst}
Yuqi Nie, Nam~H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam.
\newblock A time series is worth 64 words: Long-term forecasting with transformers.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=Jbdc0vTOcol}.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever, et~al.]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et~al.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Shazeer(2020)]{shazeer2020glu}
Noam Shazeer.
\newblock Glu variants improve transformer.
\newblock \emph{arXiv preprint arXiv:2002.05202}, 2020.

\bibitem[Strudel et~al.(2021)Strudel, Garcia, Laptev, and Schmid]{strudel2021segmenter}
Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid.
\newblock Segmenter: Transformer for semantic segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pages 7262--7272, 2021.

\bibitem[Van~den Burg and Williams(2020)]{van2020evaluation}
Gerrit~JJ Van~den Burg and Christopher~KI Williams.
\newblock An evaluation of change point detection algorithms.
\newblock \emph{arXiv preprint arXiv:2003.06222}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2022)Wang, Peng, Huang, Wang, Chen, and Xiao]{wang2022micn}
Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao.
\newblock Micn: Multi-scale local and global context modeling for long-term series forecasting.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Wang et~al.(2024)Wang, Wu, Shi, Hu, Luo, Ma, Zhang, and ZHOU]{wang2024timemixer}
Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James~Y. Zhang, and JUN ZHOU.
\newblock Timemixer: Decomposable multiscale mixing for time series forecasting.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=7oLshfEIC2}.

\bibitem[Woo et~al.(2023)Woo, Liu, Sahoo, Kumar, and Hoi]{woo2023learning}
Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi.
\newblock Learning deep time-index models for time series forecasting.
\newblock In \emph{International Conference on Machine Learning}, pages 37217--37237. PMLR, 2023.

\bibitem[Wu et~al.(2021)Wu, Xu, Wang, and Long]{wu2021autoformer}
Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long.
\newblock Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 22419--22430, 2021.

\bibitem[Wu et~al.(2022)Wu, Hu, Liu, Zhou, Wang, and Long]{wu2022timesnet}
Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long.
\newblock Timesnet: Temporal 2d-variation modeling for general time series analysis.
\newblock In \emph{The eleventh international conference on learning representations}, 2022.

\bibitem[Yang et~al.(2022)Yang, Wang, Zhang, Zhang, Wei, Lin, and Yuille]{yang2022lite}
Chenglin Yang, Yilin Wang, Jianming Zhang, He~Zhang, Zijun Wei, Zhe Lin, and Alan Yuille.
\newblock Lite vision transformer with enhanced self-attention.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 11998--12008, 2022.

\bibitem[Zeng et~al.(2023)Zeng, Chen, Zhang, and Xu]{zeng2023transformers}
Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu.
\newblock Are transformers effective for time series forecasting?
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~37, pages 11121--11128, 2023.

\bibitem[Zhang et~al.(2023)Zhang, Hu, and Wang]{zhang2023fcaformer}
Haokui Zhang, Wenze Hu, and Xiaoyu Wang.
\newblock Fcaformer: Forward cross attention in hybrid vision transformer.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 6060--6069, 2023.

\bibitem[Zhang and Yan(2022)]{zhang2022crossformer}
Yunhao Zhang and Junchi Yan.
\newblock Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting.
\newblock In \emph{The eleventh international conference on learning representations}, 2022.

\bibitem[Zhou et~al.(2021)Zhou, Zhang, Peng, Zhang, Li, Xiong, and Zhang]{zhou2021informer}
Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
\newblock Informer: Beyond efficient transformer for long sequence time-series forecasting.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~35, pages 11106--11115, 2021.

\bibitem[Zhou et~al.(2022{\natexlab{a}})Zhou, Ma, Wen, Sun, Yao, Yin, Jin, et~al.]{zhou2022film}
Tian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, Rong Jin, et~al.
\newblock Film: Frequency improved legendre memory model for long-term time series forecasting.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 12677--12690, 2022{\natexlab{a}}.

\bibitem[Zhou et~al.(2022{\natexlab{b}})Zhou, Ma, Wen, Wang, Sun, and Jin]{zhou2022fedformer}
Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.
\newblock Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting.
\newblock In \emph{International conference on machine learning}, pages 27268--27286. PMLR, 2022{\natexlab{b}}.

\end{thebibliography}
