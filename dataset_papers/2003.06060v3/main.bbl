\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and Yoshida]{sngan}
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}. OpenReview.net, 2018.
\newblock URL \url{https://openreview.net/forum?id=B1QRgziT-}.

\bibitem[Brock et~al.(2019)Brock, Donahue, and Simonyan]{biggan}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale {GAN} training for high fidelity natural image synthesis.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.
\newblock URL \url{https://openreview.net/forum?id=B1xsqj09Fm}.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in neural information processing systems}, pages
  2672--2680, 2014.

\bibitem[Nguyen et~al.(2017)Nguyen, Clune, Bengio, Dosovitskiy, and
  Yosinski]{plugandplay}
Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, and Jason Yosinski.
\newblock Plug {\&} play generative networks: Conditional iterative generation
  of images in latent space.
\newblock In \emph{2017 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017}, pages
  3510--3520. {IEEE} Computer Society, 2017.
\newblock \doi{10.1109/CVPR.2017.374}.
\newblock URL \url{https://doi.org/10.1109/CVPR.2017.374}.

\bibitem[Dai et~al.(2017)Dai, Yang, Yang, Cohen, and
  Salakhutdinov]{dai2017good}
Zihang Dai, Zhilin Yang, Fan Yang, William~W Cohen, and Ruslan~R Salakhutdinov.
\newblock Good semi-supervised learning that requires a bad gan.
\newblock In \emph{Advances in neural information processing systems}, pages
  6510--6520, 2017.

\bibitem[Yi et~al.(2017)Yi, Zhang, Tan, and Gong]{Yi_2017_ICCV}
Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong.
\newblock Dualgan: Unsupervised dual learning for image-to-image translation.
\newblock In \emph{The IEEE International Conference on Computer Vision
  (ICCV)}, Oct 2017.

\bibitem[Zhu et~al.(2017)Zhu, Park, Isola, and Efros]{zhu2017unpaired}
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei~A Efros.
\newblock Unpaired image-to-image translation using cycle-consistent
  adversarial networks.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 2223--2232, 2017.

\bibitem[Ho and Ermon(2016)]{ho2016generative}
Jonathan Ho and Stefano Ermon.
\newblock Generative adversarial imitation learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  4565--4573, 2016.

\bibitem[Karras et~al.(2019)Karras, Laine, Aittala, Hellsten, Lehtinen, and
  Aila]{karras2019analyzing}
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and
  Timo Aila.
\newblock Analyzing and improving the image quality of stylegan.
\newblock \emph{arXiv preprint arXiv:1912.04958}, 2019.

\bibitem[Azadi et~al.(2018)Azadi, Olsson, Darrell, Goodfellow, and
  Odena]{azadi2018discriminator}
Samaneh Azadi, Catherine Olsson, Trevor Darrell, Ian Goodfellow, and Augustus
  Odena.
\newblock Discriminator rejection sampling.
\newblock \emph{arXiv preprint arXiv:1810.06758}, 2018.

\bibitem[Turner et~al.(2019)Turner, Hung, Frank, Saatchi, and
  Yosinski]{pmlr-v97-turner19a-mhgan}
Ryan Turner, Jane Hung, Eric Frank, Yunus Saatchi, and Jason Yosinski.
\newblock {M}etropolis-{H}astings generative adversarial networks.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  6345--6353, Long Beach, California, USA, 09--15 Jun 2019. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v97/turner19a.html}.

\bibitem[Tanaka(2019)]{tanaka2019discriminator}
Akinori Tanaka.
\newblock Discriminator optimal transport.
\newblock \emph{arXiv preprint arXiv:1910.06832}, 2019.

\bibitem[Arbel et~al.(2020)Arbel, Zhou, and Gretton]{arbel2020kale}
Michael Arbel, Liang Zhou, and Arthur Gretton.
\newblock Kale: When energy-based learning meets adversarial training, 2020.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{arjovsky2017wasserstein}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock Wasserstein gan.
\newblock \emph{arXiv preprint arXiv:1701.07875}, 2017.

\bibitem[Bengio et~al.(2013)Bengio, Mesnil, Dauphin, and
  Rifai]{DBLP:conf/icml/BengioMDR13}
Yoshua Bengio, Gr{\'{e}}goire Mesnil, Yann~N. Dauphin, and Salah Rifai.
\newblock Better mixing via deep representations.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning, {ICML} 2013, Atlanta, GA, USA, 16-21 June 2013}, volume~28 of
  \emph{{JMLR} Workshop and Conference Proceedings}, pages 552--560. JMLR.org,
  2013.
\newblock URL \url{http://proceedings.mlr.press/v28/bengio13.html}.

\bibitem[Hoffman et~al.(2019)Hoffman, Sountsov, Dillon, Langmore, Tran, and
  Vasudevan]{hoffman2019neutra}
Matthew Hoffman, Pavel Sountsov, Joshua~V Dillon, Ian Langmore, Dustin Tran,
  and Srinivas Vasudevan.
\newblock Neutra-lizing bad geometry in hamiltonian monte carlo using neural
  transport.
\newblock \emph{arXiv preprint arXiv:1903.03704}, 2019.

\bibitem[Casella et~al.(2004)Casella, Robert, and Wells]{10.2307/4356322}
George Casella, Christian~P. Robert, and Martin~T. Wells.
\newblock Generalized accept-reject sampling schemes.
\newblock \emph{Lecture Notes-Monograph Series}, 45:\penalty0 342--347, 2004.
\newblock ISSN 07492170.
\newblock URL \url{http://www.jstor.org/stable/4356322}.

\bibitem[MacKay(2003)]{mackay2003information}
David~JC MacKay.
\newblock \emph{Information theory, inference and learning algorithms}.
\newblock Cambridge university press, 2003.

\bibitem[Tieleman(2008)]{tieleman2008training}
Tijmen Tieleman.
\newblock Training restricted boltzmann machines using approximations to the
  likelihood gradient.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pages 1064--1071, 2008.

\bibitem[Wu et~al.(2019)Wu, Donahue, Balduzzi, Simonyan, and
  Lillicrap]{wu2019logan}
Yan Wu, Jeff Donahue, David Balduzzi, Karen Simonyan, and Timothy Lillicrap.
\newblock Logan: Latent optimisation for generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1912.00953}, 2019.

\bibitem[Cranmer et~al.(2015)Cranmer, Pavez, and
  Louppe]{cranmer2015approximating}
Kyle Cranmer, Juan Pavez, and Gilles Louppe.
\newblock Approximating likelihood ratios with calibrated discriminative
  classifiers.
\newblock \emph{arXiv preprint arXiv:1506.02169}, 2015.

\bibitem[Deng et~al.(2020)Deng, Bakhtin, Ott, Szlam, and
  Ranzato]{Deng2020Residual}
Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc'Aurelio Ranzato.
\newblock Residual energy-based models for text generation.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=B1l4SgHKDH}.

\bibitem[Grover et~al.(2019)Grover, Song, Kapoor, Tran, Agarwal, Horvitz, and
  Ermon]{NIPS2019_9286}
Aditya Grover, Jiaming Song, Ashish Kapoor, Kenneth Tran, Alekh Agarwal, Eric~J
  Horvitz, and Stefano Ermon.
\newblock Bias correction of learned generative models using likelihood-free
  importance weighting.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems 32}, pages 11058--11070. Curran Associates,
  Inc., 2019.

\bibitem[Grover et~al.(2018)Grover, Gummadi, Lazaro-Gredilla, Schuurmans, and
  Ermon]{pmlr-v84-grover18a}
Aditya Grover, Ramki Gummadi, Miguel Lazaro-Gredilla, Dale Schuurmans, and
  Stefano Ermon.
\newblock Variational rejection sampling.
\newblock In Amos Storkey and Fernando Perez-Cruz, editors, \emph{Proceedings
  of the Twenty-First International Conference on Artificial Intelligence and
  Statistics}, volume~84 of \emph{Proceedings of Machine Learning Research},
  pages 823--832, Playa Blanca, Lanzarote, Canary Islands, 09--11 Apr 2018.
  PMLR.
\newblock URL \url{http://proceedings.mlr.press/v84/grover18a.html}.

\bibitem[Hinton and Salakhutdinov(2006)]{hinton2006reducing}
Geoffrey~E Hinton and Ruslan~R Salakhutdinov.
\newblock Reducing the dimensionality of data with neural networks.
\newblock \emph{science}, 313\penalty0 (5786):\penalty0 504--507, 2006.

\bibitem[Tao et~al.(2019)Tao, Chen, Dai, Chen, Bai, Wang, Feng, Lu, Bobashev,
  and Carin]{fenchel-minimax}
Chenyang Tao, Liqun Chen, Shuyang Dai, Junya Chen, Ke~Bai, Dong Wang, Jianfeng
  Feng, Wenlian Lu, Georgiy~V. Bobashev, and Lawrence Carin.
\newblock On fenchel mini-max learning.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
  d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett, editors, \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019,
  Vancouver, BC, Canada}, pages 10427--10439, 2019.
\newblock URL
  \url{http://papers.nips.cc/paper/9230-on-fenchel-mini-max-learning}.

\bibitem[Gao et~al.(2020)Gao, Nijkamp, Kingma, Xu, Dai, and Wu]{gao2020flow}
Ruiqi Gao, Erik Nijkamp, Diederik~P Kingma, Zhen Xu, Andrew~M Dai, and
  Ying~Nian Wu.
\newblock Flow contrastive estimation of energy-based models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 7518--7528, 2020.

\bibitem[Gao et~al.(2018)Gao, Lu, Zhou, Zhu, and Nian~Wu]{gao2018learning}
Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian~Wu.
\newblock Learning generative convnets via multi-grid modeling and sampling.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 9155--9164, 2018.

\bibitem[Xie et~al.(2018)Xie, Lu, Gao, and Wu]{xie2018cooperative}
Jianwen Xie, Yang Lu, Ruiqi Gao, and Ying~Nian Wu.
\newblock Cooperative learning of energy-based model and latent variable model
  via mcmc teaching.
\newblock In \emph{AAAI}, volume~1, page~7, 2018.

\bibitem[Han et~al.(2017)Han, Lu, Zhu, and Wu]{han2017alternating}
Tian Han, Yang Lu, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Alternating back-propagation for generator network.
\newblock In \emph{Thirty-First AAAI Conference on Artificial Intelligence},
  2017.

\bibitem[Han et~al.(2019)Han, Nijkamp, Fang, Hill, Zhu, and
  Wu]{han2019divergence}
Tian Han, Erik Nijkamp, Xiaolin Fang, Mitch Hill, Song-Chun Zhu, and Ying~Nian
  Wu.
\newblock Divergence triangle for joint training of generator model,
  energy-based model, and inferential model.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 8670--8679, 2019.

\bibitem[Han et~al.(2020)Han, Nijkamp, Zhou, Pang, Zhu, and Wu]{han2020joint}
Tian Han, Erik Nijkamp, Linqi Zhou, Bo~Pang, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Joint training of variational auto-encoder and latent energy-based
  model.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 7978--7987, 2020.

\bibitem[Pang et~al.(2020)Pang, Han, Nijkamp, Zhu, and Wu]{pang2020learning}
Bo~Pang, Tian Han, Erik Nijkamp, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Learning latent space energy-based prior model.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[LeCun et~al.(2006)LeCun, Chopra, Hadsell, Ranzato, and
  Huang]{lecun2006tutorial}
Yann LeCun, Sumit Chopra, Raia Hadsell, M~Ranzato, and F~Huang.
\newblock A tutorial on energy-based learning.
\newblock \emph{Predicting structured data}, 1\penalty0 (0), 2006.

\bibitem[Du and Mordatch(2019)]{du2019implicit}
Yilun Du and Igor Mordatch.
\newblock Implicit generation and modeling with energy based models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3603--3613, 2019.

\bibitem[Salakhutdinov and Hinton(2009)]{salakhutdinov2009deep}
Ruslan Salakhutdinov and Geoffrey Hinton.
\newblock Deep boltzmann machines.
\newblock In \emph{Artificial intelligence and statistics}, pages 448--455,
  2009.

\bibitem[Grathwohl et~al.(2019)Grathwohl, Wang, Jacobsen, Duvenaud, Norouzi,
  and Swersky]{grathwohl2019your}
Will Grathwohl, Kuan-Chieh Wang, J{\"o}rn-Henrik Jacobsen, David Duvenaud,
  Mohammad Norouzi, and Kevin Swersky.
\newblock Your classifier is secretly an energy based model and you should
  treat it like one.
\newblock \emph{arXiv preprint arXiv:1912.03263}, 2019.

\bibitem[Hinton(2002)]{hinton2002training}
Geoffrey~E Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural computation}, 14\penalty0 (8):\penalty0 1771--1800,
  2002.

\bibitem[Nijkamp et~al.(2019)Nijkamp, Hill, Zhu, and Wu]{nijkamp2019learning}
Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Learning non-convergent non-persistent short-run mcmc toward
  energy-based model.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5233--5243, 2019.

\bibitem[Kim and Bengio(2016)]{kim2016deep}
Taesup Kim and Yoshua Bengio.
\newblock Deep directed generative models with energy-based probability
  estimation.
\newblock \emph{arXiv preprint arXiv:1606.03439}, 2016.

\bibitem[Kumar et~al.(2019)Kumar, Goyal, Courville, and
  Bengio]{kumar2019maximum}
Rithesh Kumar, Anirudh Goyal, Aaron Courville, and Yoshua Bengio.
\newblock Maximum entropy generators for energy-based models.
\newblock \emph{arXiv preprint arXiv:1901.08508}, 2019.

\bibitem[Hyv{\"a}rinen(2005)]{hyvarinen2005estimation}
Aapo Hyv{\"a}rinen.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (Apr):\penalty0 695--709, 2005.

\bibitem[Gutmann and Hyvärinen(2010)]{GutmannM2010}
Michael Gutmann and Aapo Hyvärinen.
\newblock Noise-contrastive estimation: A new estimation principle for
  unnormalized statistical models.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, pages 297--304, 2010.

\bibitem[Sohl-Dickstein et~al.(2011)Sohl-Dickstein, Battaglino, and
  DeWeese]{sohl2011new}
Jascha Sohl-Dickstein, Peter~B Battaglino, and Michael~R DeWeese.
\newblock New method for parameter estimation in probabilistic models: minimum
  probability flow.
\newblock \emph{Physical review letters}, 107\penalty0 (22):\penalty0 220601,
  2011.

\bibitem[Zhao et~al.(2016)Zhao, Mathieu, and LeCun]{zhao2016energy}
Junbo Zhao, Michael Mathieu, and Yann LeCun.
\newblock Energy-based generative adversarial network.
\newblock \emph{arXiv preprint arXiv:1609.03126}, 2016.

\bibitem[Finn et~al.(2016)Finn, Christiano, Abbeel, and
  Levine]{finn2016connection}
Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine.
\newblock A connection between generative adversarial networks, inverse
  reinforcement learning, and energy-based models.
\newblock \emph{arXiv preprint arXiv:1611.03852}, 2016.

\bibitem[Dai et~al.(2019)Dai, Liu, Dai, He, Gretton, Song, and
  Schuurmans]{dai2019exponential}
Bo~Dai, Zhen Liu, Hanjun Dai, Niao He, Arthur Gretton, Le~Song, and Dale
  Schuurmans.
\newblock Exponential family estimation via adversarial dynamics embedding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10977--10988, 2019.

\bibitem[Zhai et~al.(2019)Zhai, Talbott, Guestrin, and
  Susskind]{zhai2019adversarial}
Shuangfei Zhai, Walter Talbott, Carlos Guestrin, and Joshua Susskind.
\newblock Adversarial fisher vectors for unsupervised representation learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  11158--11168, 2019.

\bibitem[van~den Oord et~al.(2016)van~den Oord, Kalchbrenner, Espeholt,
  Kavukcuoglu, Vinyals, and Graves]{pixelcnn}
A{\"{a}}ron van~den Oord, Nal Kalchbrenner, Lasse Espeholt, Koray Kavukcuoglu,
  Oriol Vinyals, and Alex Graves.
\newblock Conditional image generation with pixelcnn decoders.
\newblock In Daniel~D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle
  Guyon, and Roman Garnett, editors, \emph{Advances in Neural Information
  Processing Systems 29: Annual Conference on Neural Information Processing
  Systems 2016, December 5-10, 2016, Barcelona, Spain}, pages 4790--4798, 2016.
\newblock URL
  \url{http://papers.nips.cc/paper/6527-conditional-image-generation-with-pixelcnn-decoders}.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{gulrajani2017improved}
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron~C
  Courville.
\newblock Improved training of wasserstein gans.
\newblock In \emph{Advances in neural information processing systems}, pages
  5767--5777, 2017.

\bibitem[Karras et~al.(2018)Karras, Aila, Laine, and Lehtinen]{progressive-gan}
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
\newblock Progressive growing of gans for improved quality, stability, and
  variation.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}. OpenReview.net, 2018.
\newblock URL \url{https://openreview.net/forum?id=Hk99zCeAb}.

\bibitem[Song and Ermon(2019)]{ncsn}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems 32}, pages 11895--11907. Curran Associates,
  Inc., 2019.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{improved_gan}
Tim Salimans, Ian~J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
  and Xi~Chen.
\newblock Improved techniques for training gans.
\newblock In Daniel~D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle
  Guyon, and Roman Garnett, editors, \emph{Advances in Neural Information
  Processing Systems 29: Annual Conference on Neural Information Processing
  Systems 2016, December 5-10, 2016, Barcelona, Spain}, pages 2226--2234, 2016.
\newblock URL
  \url{http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans}.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{fid-ttur}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna~M. Wallach,
  Rob Fergus, S.~V.~N. Vishwanathan, and Roman Garnett, editors, \emph{Advances
  in Neural Information Processing Systems 30: Annual Conference on Neural
  Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA,
  {USA}}, pages 6626--6637, 2017.

\bibitem[Bauer and Mnih(2018)]{bauer2018resampled}
Matthias Bauer and Andriy Mnih.
\newblock Resampled priors for variational autoencoders.
\newblock \emph{arXiv preprint arXiv:1810.11428}, 2018.

\bibitem[Lawson et~al.(2019)Lawson, Tucker, Dai, and
  Ranganath]{lawson2019energy}
John Lawson, George Tucker, Bo~Dai, and Rajesh Ranganath.
\newblock Energy-inspired models: Learning with sampler-induced distributions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8499--8511, 2019.

\bibitem[Farell and Perlstein(2018)]{nytimesfake}
Henry~J. Farell and Rick Perlstein.
\newblock Our hackable political future.
\newblock
  \url{https://www.nytimes.com/2018/02/04/opinion/hacking-politics-future.html},
  November 2018.
\newblock Accessed: 2018-02-07.

\bibitem[Cloud(2019)]{wavenetTTS}
Google Cloud.
\newblock Wavenet and other synthetic voices, 2019.
\newblock URL \url{https://cloud.google.com/text-to-speech/docs/wavenet}.

\bibitem[Engel et~al.(2017)Engel, Resnick, Roberts, Dieleman, Norouzi, Eck, and
  Simonyan]{engel2017neural}
Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Mohammad Norouzi,
  Douglas Eck, and Karen Simonyan.
\newblock Neural audio synthesis of musical notes with wavenet autoencoders.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1068--1077. JMLR. org, 2017.

\bibitem[Jones and Bonafilia(2017)]{jones2017gangogh}
Kenny Jones and Derrick Bonafilia.
\newblock Gangogh: creating art with gans.
\newblock \emph{Towards Data Science}, 2017.

\bibitem[Zhu et~al.(2016)Zhu, Kr{\"a}henb{\"u}hl, Shechtman, and
  Efros]{zhu2016generative}
Jun-Yan Zhu, Philipp Kr{\"a}henb{\"u}hl, Eli Shechtman, and Alexei~A. Efros.
\newblock Generative visual manipulation on the natural image manifold.
\newblock In \emph{Proceedings of European Conference on Computer Vision
  (ECCV)}, 2016.

\bibitem[Pinker(2018)]{pinker2018enlightenment}
Steven Pinker.
\newblock \emph{Enlightenment now: The case for reason, science, humanism, and
  progress}.
\newblock Penguin, 2018.

\bibitem[Zou and Schiebinger(2018)]{zou2018ai}
James Zou and Londa Schiebinger.
\newblock Ai can be sexist and racist—it’s time to make it fair, 2018.

\bibitem[Radford et~al.(2016)Radford, Metz, and Chintala]{dcgan}
Alec Radford, Luke Metz, and Soumith Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock In Yoshua Bengio and Yann LeCun, editors, \emph{4th International
  Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico,
  May 2-4, 2016, Conference Track Proceedings}, 2016.
\newblock URL \url{http://arxiv.org/abs/1511.06434}.

\bibitem[Welling and Teh(2011)]{sgld}
Max Welling and Yee~Whye Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In Lise Getoor and Tobias Scheffer, editors, \emph{Proceedings of the
  28th International Conference on Machine Learning, {ICML} 2011, Bellevue,
  Washington, USA, June 28 - July 2, 2011}, pages 681--688. Omnipress, 2011.
\newblock URL \url{https://icml.cc/2011/papers/398\_icmlpaper.pdf}.

\end{thebibliography}
