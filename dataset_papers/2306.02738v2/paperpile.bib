% Generated by Paperpile. Check out https://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@ARTICLE{Goldblum2023-sd,
	title         = "The No Free Lunch Theorem, Kolmogorov Complexity, and the
	Role of Inductive Biases in Machine Learning",
	author        = "Goldblum, Micah and Finzi, Marc and Rowan, Keefer and
	Wilson, Andrew Gordon",
	abstract      = "No free lunch theorems for supervised learning state that no
	learner can solve all problems or that all learners achieve
	exactly the same accuracy on average over a uniform
	distribution on learning problems. Accordingly, these
	theorems are often referenced in support of the notion that
	individual problems require specially tailored inductive
	biases. While virtually all uniformly sampled datasets have
	high complexity, real-world problems disproportionately
	generate low-complexity data, and we argue that neural
	network models share this same preference, formalized using
	Kolmogorov complexity. Notably, we show that architectures
	designed for a particular domain, such as computer vision,
	can compress datasets on a variety of seemingly unrelated
	domains. Our experiments show that pre-trained and even
	randomly initialized language models prefer to generate
	low-complexity sequences. Whereas no free lunch theorems
	seemingly indicate that individual problems require
	specialized learners, we explain how tasks that often
	require human intervention such as picking an appropriately
	sized model when labeled data is scarce or plentiful can be
	automated into a single learning algorithm. These
	observations justify the trend in deep learning of unifying
	seemingly disparate problems with an increasingly small set
	of machine learning models.",
	month         =  "11~" # apr,
	year          =  2023,
	url           = "http://arxiv.org/abs/2304.05366",
	file          = "All Papers/G/Goldblum et al. 2023 - The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning.pdf",
	archivePrefix = "arXiv",
	eprint        = "2304.05366",
	primaryClass  = "cs.LG",
	arxivid       = "2304.05366"
}

@ARTICLE{Scott1979-qe,
	title     = "On optimal and data-based histograms",
	author    = "Scott, David W",
	abstract  = "Abstract. In this paper the formula for the optimal histogram
	bin width is derived which asymptotically minimizes the
	integrated mean squared error. Monte Carlo",
	journal   = "Biometrika",
	publisher = "Oxford Academic",
	volume    =  66,
	number    =  3,
	pages     = "605--610",
	month     =  dec,
	year      =  1979,
	url       = "https://academic.oup.com/biomet/article-abstract/66/3/605/232642",
	language  = "en",
	issn      = "0006-3444",
	doi       = "10.1093/biomet/66.3.605"
}

@ARTICLE{Schaeffer2023-nm,
	title         = "Double Descent Demystified: Identifying, Interpreting \&
	Ablating the Sources of a Deep Learning Puzzle",
	author        = "Schaeffer, Rylan and Khona, Mikail and Robertson, Zachary
	and Boopathy, Akhilan and Pistunova, Kateryna and Rocks,
	Jason W and Fiete, Ila Rani and Koyejo, Oluwasanmi",
	abstract      = "Double descent is a surprising phenomenon in machine
	learning, in which as the number of model parameters grows
	relative to the number of data, test error drops as models
	grow ever larger into the highly overparameterized (data
	undersampled) regime. This drop in test error flies against
	classical learning theory on overfitting and has arguably
	underpinned the success of large models in machine learning.
	This non-monotonic behavior of test loss depends on the
	number of data, the dimensionality of the data and the
	number of model parameters. Here, we briefly describe double
	descent, then provide an explanation of why double descent
	occurs in an informal and approachable manner, requiring
	only familiarity with linear algebra and introductory
	probability. We provide visual intuition using polynomial
	regression, then mathematically analyze double descent with
	ordinary linear regression and identify three interpretable
	factors that, when simultaneously all present, together
	create double descent. We demonstrate that double descent
	occurs on real data when using ordinary linear regression,
	then demonstrate that double descent does not occur when any
	of the three factors are ablated. We use this understanding
	to shed light on recent observations in nonlinear models
	concerning superposition and double descent. Code is
	publicly available.",
	month         =  "24~" # mar,
	year          =  2023,
	url           = "http://arxiv.org/abs/2303.14151",
	file          = "All Papers/S/Schaeffer et al. 2023 - Double Descent Demystified - Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle.pdf",
	archivePrefix = "arXiv",
	eprint        = "2303.14151",
	primaryClass  = "cs.LG",
	arxivid       = "2303.14151"
}

@INPROCEEDINGS{Luo2022-jr,
	title     = "Local calibration: metrics and recalibration",
	booktitle = "Proceedings of the {Thirty-Eighth} Conference on Uncertainty in
	Artificial Intelligence",
	author    = "Luo, Rachel and Bhatnagar, Aadyot and Bai, Yu and Zhao, Shengjia
	and Wang, Huan and Xiong, Caiming and Savarese, Silvio and
	Ermon, Stefano and Schmerling, Edward and Pavone, Marco",
	editor    = "Cussens, James and Zhang, Kun",
	abstract  = "Probabilistic classifiers output confidence scores along with
	their predictions, and these confidence scores should be
	calibrated, i.e., they should reflect the reliability of the
	prediction. Confidence scores that minimize standard metrics
	such as the expected calibration error (ECE) accurately measure
	the reliability on average across the entire population.
	However, it is in general impossible to measure the reliability
	of an individual prediction. In this work, we propose the local
	calibration error (LCE) to span the gap between average and
	individual reliability. For each individual prediction, the LCE
	measures the average reliability of a set of similar
	predictions, where similarity is quantified by a kernel function
	on a pretrained feature space and by a binning scheme over
	predicted model confidences. We show theoretically that the LCE
	can be estimated sample-efficiently from data, and empirically
	find that it reveals miscalibration modes that are more
	fine-grained than the ECE can detect. Our key result is a novel
	local recalibration method \textbackslashmethod, to improve
	confidence scores for individual predictions and decrease the
	LCE. Experimentally, we show that our recalibration method
	produces more accurate confidence scores, which improves
	downstream fairness and decision making on classification tasks
	with both image and tabular data.",
	publisher = "PMLR",
	volume    =  180,
	pages     = "1286--1295",
	series    = "Proceedings of Machine Learning Research",
	year      =  2022,
	url       = "https://proceedings.mlr.press/v180/luo22a.html",
	file      = "All Papers/L/Luo et al. 2022 - Local calibration - metrics and recalibration.pdf;All Papers/L/Luo et al. 2022 - luo22a-supp.pdf"
}

@INPROCEEDINGS{Wang2021-jy,
	title     = "Rethinking Calibration of Deep Neural Networks: Do Not Be Afraid
	of Overconfidence",
	booktitle = "Advances in Neural Information Processing Systems",
	author    = "Wang, Deng-Bao and Feng, Lei and Zhang, Min-Ling",
	editor    = "Ranzato, M and Beygelzimer, A and Dauphin, Y and Liang, P S and
	Vaughan, J Wortman",
	publisher = "Curran Associates, Inc.",
	volume    =  34,
	pages     = "11809--11820",
	year      =  2021,
	url       = "https://proceedings.neurips.cc/paper_files/paper/2021/file/61f3a6dbc9120ea78ef75544826c814e-Paper.pdf",
	file      = "All Papers/W/Wang et al. 2021 - 61f3a6dbc9120ea78ef75544826c814e-Supplemental.pdf;All Papers/W/Wang et al. 2021 - Rethinking Calibration of Deep Neural Networks - Do Not Be Afraid of Overconfidence.pdf"
}

@ARTICLE{Vasilev2023-zz,
	title         = "Calibration of Neural Networks",
	author        = "Vasilev, Ruslan and D'yakonov, Alexander",
	abstract      = "Neural networks solving real-world problems are often
	required not only to make accurate predictions but also to
	provide a confidence level in the forecast. The calibration
	of a model indicates how close the estimated confidence is
	to the true probability. This paper presents a survey of
	confidence calibration problems in the context of neural
	networks and provides an empirical comparison of calibration
	methods. We analyze problem statement, calibration
	definitions, and different approaches to evaluation:
	visualizations and scalar measures that estimate whether the
	model is well-calibrated. We review modern calibration
	techniques: based on post-processing or requiring changes in
	training. Empirical experiments cover various datasets and
	models, comparing calibration methods according to different
	criteria.",
	month         =  "19~" # mar,
	year          =  2023,
	url           = "http://arxiv.org/abs/2303.10761",
	file          = "All Papers/V/Vasilev and D'yakonov 2023 - Calibration of Neural Networks.pdf",
	archivePrefix = "arXiv",
	eprint        = "2303.10761",
	primaryClass  = "cs.NE",
	arxivid       = "2303.10761"
}

@ARTICLE{Pinson2010-jx,
	title     = "Reliability diagrams for non-parametric density forecasts of
	continuous variables: Accounting for serial correlation",
	author    = "Pinson, Pierre and McSharry, Patrick and Madsen, Henrik",
	abstract  = "Abstract Reliability is seen as a primary requirement when
	verifying probabilistic forecasts, since a lack of reliability
	would introduce a systematic bias in subsequent decision-making.
	Reliability diagrams comprise popular and practical diagnostic
	tools for the reliability evaluation of density forecasts of
	continuous variables. Such diagrams relate to the assessment of
	the unconditional calibration of probabilistic forecasts. A
	reason for their appeal is that deviations from perfect
	reliability can be visually assessed based on deviations from
	the diagonal. Deviations from the diagonal may, however, be
	caused by both sampling effects and serial correlation in the
	forecast-verification pairs. We build on a recent proposal,
	consisting of associating reliability diagrams with consistency
	bars that would reflect the deviations from the diagonal that
	are potentially observable even if density forecasts are
	perfectly reliable. Our consistency bars, however, reflect
	potential deviations originating from the combined effects of
	limited counting statistics and serial correlation in the
	forecast-verification pairs. They are generated based on an
	original surrogate consistency resampling method. Its ability to
	provide consistency bars with a significantly better coverage
	against the independent and identically distributed (i.i.d.)
	resampling alternative is shown from simulations. Finally, a
	practical example of the reliability assessment of
	non-parametric density forecasts of short-term wind-power
	generation is given. Copyright ? 2010 Royal Meteorological
	Society",
	journal   = "Quarterly Journal of the Royal Meteorological Society",
	publisher = "Wiley",
	volume    =  136,
	number    =  646,
	pages     = "77--90",
	month     =  jan,
	year      =  2010,
	url       = "https://onlinelibrary.wiley.com/doi/10.1002/qj.559",
	file      = "All Papers/P/Pinson et al. 2010 - Reliability diagrams for non-parametric density forecasts of continuous variables - Accounting for serial correlation.pdf",
	language  = "en",
	issn      = "0035-9009, 1477-870X",
	doi       = "10.1002/qj.559"
}

@ARTICLE{Arjovsky2017-ad,
	title         = "Wasserstein {GAN}",
	author        = "Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on",
	abstract      = "We introduce a new algorithm named WGAN, an alternative to
	traditional GAN training. In this new model, we show that we
	can improve the stability of learning, get rid of problems
	like mode collapse, and provide meaningful learning curves
	useful for debugging and hyperparameter searches.
	Furthermore, we show that the corresponding optimization
	problem is sound, and provide extensive theoretical work
	highlighting the deep connections to other distances between
	distributions.",
	month         =  "26~" # jan,
	year          =  2017,
	url           = "http://arxiv.org/abs/1701.07875",
	file          = "All Papers/A/Arjovsky et al. 2017 - Wasserstein GAN.pdf",
	archivePrefix = "arXiv",
	eprint        = "1701.07875",
	primaryClass  = "stat.ML",
	arxivid       = "1701.07875"
}

@INPROCEEDINGS{Goodfellow2014-xb,
	title     = "Generative Adversarial Nets",
	booktitle = "{NIPS}",
	author    = "Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and
	Xu, Bing and Warde-Farley, David and Ozair, Sherjil and
	Courville, Aaron C and Bengio, Yoshua",
	year      =  2014,
	file      = "All Papers/G/Goodfellow et al. 2014 - Generative Adversarial Nets.pdf"
}

@ARTICLE{Saharia2022-tc,
	title         = "Photorealistic {Text-to-Image} Diffusion Models with Deep
	Language Understanding",
	author        = "Saharia, Chitwan and Chan, William and Saxena, Saurabh and
	Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour,
	Seyed Kamyar Seyed and Ayan, Burcu Karagol and Sara Mahdavi,
	S and Lopes, Rapha Gontijo and Salimans, Tim and Ho,
	Jonathan and Fleet, David J and Norouzi, Mohammad",
	abstract      = "We present Imagen, a text-to-image diffusion model with an
	unprecedented degree of photorealism and a deep level of
	language understanding. Imagen builds on the power of large
	transformer language models in understanding text and hinges
	on the strength of diffusion models in high-fidelity image
	generation. Our key discovery is that generic large language
	models (e.g. T5), pretrained on text-only corpora, are
	surprisingly effective at encoding text for image synthesis:
	increasing the size of the language model in Imagen boosts
	both sample fidelity and image-text alignment much more than
	increasing the size of the image diffusion model. Imagen
	achieves a new state-of-the-art FID score of 7.27 on the
	COCO dataset, without ever training on COCO, and human
	raters find Imagen samples to be on par with the COCO data
	itself in image-text alignment. To assess text-to-image
	models in greater depth, we introduce DrawBench, a
	comprehensive and challenging benchmark for text-to-image
	models. With DrawBench, we compare Imagen with recent
	methods including VQ-GAN+CLIP, Latent Diffusion Models, and
	DALL-E 2, and find that human raters prefer Imagen over
	other models in side-by-side comparisons, both in terms of
	sample quality and image-text alignment. See
	https://imagen.research.google/ for an overview of the
	results.",
	month         =  "23~" # may,
	year          =  2022,
	url           = "http://arxiv.org/abs/2205.11487",
	file          = "All Papers/S/Saharia et al. 2022 - Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.pdf",
	archivePrefix = "arXiv",
	eprint        = "2205.11487",
	primaryClass  = "cs.CV",
	arxivid       = "2205.11487"
}

@ARTICLE{Nichol2021-jz,
	title         = "Improved Denoising Diffusion Probabilistic Models",
	author        = "Nichol, Alex and Dhariwal, Prafulla",
	abstract      = "Denoising diffusion probabilistic models (DDPM) are a class
	of generative models which have recently been shown to
	produce excellent samples. We show that with a few simple
	modifications, DDPMs can also achieve competitive
	log-likelihoods while maintaining high sample quality.
	Additionally, we find that learning variances of the reverse
	diffusion process allows sampling with an order of magnitude
	fewer forward passes with a negligible difference in sample
	quality, which is important for the practical deployment of
	these models. We additionally use precision and recall to
	compare how well DDPMs and GANs cover the target
	distribution. Finally, we show that the sample quality and
	likelihood of these models scale smoothly with model
	capacity and training compute, making them easily scalable.
	We release our code at
	https://github.com/openai/improved-diffusion",
	month         =  "18~" # feb,
	year          =  2021,
	url           = "http://arxiv.org/abs/2102.09672",
	file          = "All Papers/N/Nichol and Dhariwal 2021 - Improved Denoising Diffusion Probabilistic Models.pdf",
	archivePrefix = "arXiv",
	eprint        = "2102.09672",
	primaryClass  = "cs.LG",
	arxivid       = "2102.09672"
}

@MISC{Weng2021-gq,
	title        = "What are diffusion models?",
	author       = "Weng, Lilian",
	abstract     = "[Updated on 2021-09-19: Highly recommend this blog post on
	score-based generative modeling by Yang Song (author of
	several key papers in the references)]. [Updated on
	2022-08-27: Added classifier-free guidance, GLIDE, unCLIP and
	Imagen. [Updated on 2022-08-31: Added latent diffusion model.
	So far, I've written about three types of generative models,
	GAN, VAE, and Flow-based models. They have shown great
	success in generating high-quality samples, but each has some
	limitations of its own.",
	publisher    = "Lil'Log",
	month        =  "11~" # jul,
	year         =  2021,
	url          = "https://lilianweng.github.io/posts/2021-07-11-diffusion-models/",
	howpublished = "\url{https://lilianweng.github.io/posts/2021-07-11-diffusion-models/}",
	note         = "Accessed: 2023-3-24",
	language     = "en"
}

@ARTICLE{Xiao2021-ha,
	title         = "Tackling the Generative Learning Trilemma with Denoising
	Diffusion {GANs}",
	author        = "Xiao, Zhisheng and Kreis, Karsten and Vahdat, Arash",
	abstract      = "A wide variety of deep generative models has been developed
	in the past decade. Yet, these models often struggle with
	simultaneously addressing three key requirements including:
	high sample quality, mode coverage, and fast sampling. We
	call the challenge imposed by these requirements the
	generative learning trilemma, as the existing models often
	trade some of them for others. Particularly, denoising
	diffusion models have shown impressive sample quality and
	diversity, but their expensive sampling does not yet allow
	them to be applied in many real-world applications. In this
	paper, we argue that slow sampling in these models is
	fundamentally attributed to the Gaussian assumption in the
	denoising step which is justified only for small step sizes.
	To enable denoising with large steps, and hence, to reduce
	the total number of denoising steps, we propose to model the
	denoising distribution using a complex multimodal
	distribution. We introduce denoising diffusion generative
	adversarial networks (denoising diffusion GANs) that model
	each denoising step using a multimodal conditional GAN.
	Through extensive evaluations, we show that denoising
	diffusion GANs obtain sample quality and diversity
	competitive with original diffusion models while being
	2000$\times$ faster on the CIFAR-10 dataset. Compared to
	traditional GANs, our model exhibits better mode coverage
	and sample diversity. To the best of our knowledge,
	denoising diffusion GAN is the first model that reduces
	sampling cost in diffusion models to an extent that allows
	them to be applied to real-world applications inexpensively.
	Project page and code can be found at
	https://nvlabs.github.io/denoising-diffusion-gan",
	month         =  "15~" # dec,
	year          =  2021,
	url           = "http://arxiv.org/abs/2112.07804",
	file          = "All Papers/X/Xiao et al. 2021 - Tackling the Generative Learning Trilemma with Denoising Diffusion GANs.pdf",
	archivePrefix = "arXiv",
	eprint        = "2112.07804",
	primaryClass  = "cs.LG",
	arxivid       = "2112.07804"
}

@ARTICLE{Kong2020-fr,
	title         = "{DiffWave}: A Versatile Diffusion Model for Audio Synthesis",
	author        = "Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin
	and Catanzaro, Bryan",
	abstract      = "In this work, we propose DiffWave, a versatile diffusion
	probabilistic model for conditional and unconditional
	waveform generation. The model is non-autoregressive, and
	converts the white noise signal into structured waveform
	through a Markov chain with a constant number of steps at
	synthesis. It is efficiently trained by optimizing a variant
	of variational bound on the data likelihood. DiffWave
	produces high-fidelity audios in different waveform
	generation tasks, including neural vocoding conditioned on
	mel spectrogram, class-conditional generation, and
	unconditional generation. We demonstrate that DiffWave
	matches a strong WaveNet vocoder in terms of speech quality
	(MOS: 4.44 versus 4.43), while synthesizing orders of
	magnitude faster. In particular, it significantly
	outperforms autoregressive and GAN-based waveform models in
	the challenging unconditional generation task in terms of
	audio quality and sample diversity from various automatic
	and human evaluations.",
	month         =  "21~" # sep,
	year          =  2020,
	url           = "http://arxiv.org/abs/2009.09761",
	file          = "All Papers/K/Kong et al. 2020 - DiffWave - A Versatile Diffusion Model for Audio Synthesis.pdf",
	archivePrefix = "arXiv",
	eprint        = "2009.09761",
	primaryClass  = "eess.AS",
	arxivid       = "2009.09761"
}

@ARTICLE{Gong2022-ga,
	title         = "{DiffuSeq}: Sequence to Sequence Text Generation with
	Diffusion Models",
	author        = "Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu,
	Zhiyong and Kong, Lingpeng",
	abstract      = "Recently, diffusion models have emerged as a new paradigm
	for generative models. Despite the success in domains using
	continuous signals such as vision and audio, adapting
	diffusion models to natural language is under-explored due
	to the discrete nature of texts, especially for conditional
	generation. We tackle this challenge by proposing DiffuSeq:
	a diffusion model designed for sequence-to-sequence
	(Seq2Seq) text generation tasks. Upon extensive evaluation
	over a wide range of Seq2Seq tasks, we find DiffuSeq
	achieving comparable or even better performance than six
	established baselines, including a state-of-the-art model
	that is based on pre-trained language models. Apart from
	quality, an intriguing property of DiffuSeq is its high
	diversity during generation, which is desired in many
	Seq2Seq tasks. We further include a theoretical analysis
	revealing the connection between DiffuSeq and
	autoregressive/non-autoregressive models. Bringing together
	theoretical analysis and empirical evidence, we demonstrate
	the great potential of diffusion models in complex
	conditional language generation tasks. Code is available at
	\textbackslashurl\{https://github.com/Shark-NLP/DiffuSeq\}",
	month         =  "17~" # oct,
	year          =  2022,
	url           = "http://arxiv.org/abs/2210.08933",
	file          = "All Papers/G/Gong et al. 2022 - DiffuSeq - Sequence to Sequence Text Generation with Diffusion Models.pdf",
	archivePrefix = "arXiv",
	eprint        = "2210.08933",
	primaryClass  = "cs.CL",
	arxivid       = "2210.08933"
}

@INPROCEEDINGS{Sohl-Dickstein2015-zr,
	title     = "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
	booktitle = "Proceedings of the 32nd International Conference on Machine
	Learning",
	author    = "Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru
	and Ganguli, Surya",
	editor    = "Bach, Francis and Blei, David",
	abstract  = "A central problem in machine learning involves modeling complex
	data-sets using highly flexible families of probability
	distributions in which learning, sampling, inference, and
	evaluation are still analytically or computationally tractable.
	Here, we develop an approach that simultaneously achieves both
	flexibility and tractability. The essential idea, inspired by
	non-equilibrium statistical physics, is to systematically and
	slowly destroy structure in a data distribution through an
	iterative forward diffusion process. We then learn a reverse
	diffusion process that restores structure in data, yielding a
	highly flexible and tractable generative model of the data. This
	approach allows us to rapidly learn, sample from, and evaluate
	probabilities in deep generative models with thousands of layers
	or time steps, as well as to compute conditional and posterior
	probabilities under the learned model. We additionally release
	an open source reference implementation of the algorithm.",
	publisher = "PMLR",
	volume    =  37,
	pages     = "2256--2265",
	series    = "Proceedings of Machine Learning Research",
	year      =  2015,
	url       = "https://proceedings.mlr.press/v37/sohl-dickstein15.html",
	file      = "All Papers/S/Sohl-Dickstein et al. 2015 - 1503.03585.pdf;All Papers/S/Sohl-Dickstein et al. 2015 - Deep Unsupervised Learning using Nonequilibrium Thermodynamics.pdf",
	address   = "Lille, France"
}

@ARTICLE{Kang2023-ax,
	title         = "Scaling up {GANs} for {Text-to-Image} Synthesis",
	author        = "Kang, Minguk and Zhu, Jun-Yan and Zhang, Richard and Park,
	Jaesik and Shechtman, Eli and Paris, Sylvain and Park,
	Taesung",
	abstract      = "The recent success of text-to-image synthesis has taken the
	world by storm and captured the general public's
	imagination. From a technical standpoint, it also marked a
	drastic change in the favored architecture to design
	generative image models. GANs used to be the de facto
	choice, with techniques like StyleGAN. With DALL-E 2,
	auto-regressive and diffusion models became the new standard
	for large-scale generative models overnight. This rapid
	shift raises a fundamental question: can we scale up GANs to
	benefit from large datasets like LAION? We find that
	na\textbackslash``Ively increasing the capacity of the
	StyleGAN architecture quickly becomes unstable. We introduce
	GigaGAN, a new GAN architecture that far exceeds this limit,
	demonstrating GANs as a viable option for text-to-image
	synthesis. GigaGAN offers three major advantages. First, it
	is orders of magnitude faster at inference time, taking only
	0.13 seconds to synthesize a 512px image. Second, it can
	synthesize high-resolution images, for example, 16-megapixel
	pixels in 3.66 seconds. Finally, GigaGAN supports various
	latent space editing applications such as latent
	interpolation, style mixing, and vector arithmetic
	operations.",
	month         =  "9~" # mar,
	year          =  2023,
	url           = "http://arxiv.org/abs/2303.05511",
	file          = "All Papers/K/Kang et al. 2023 - Scaling up GANs for Text-to-Image Synthesis.pdf",
	archivePrefix = "arXiv",
	eprint        = "2303.05511",
	primaryClass  = "cs.CV",
	arxivid       = "2303.05511"
}

@ARTICLE{Touvron2023-ob,
	title         = "{LLaMA}: Open and Efficient Foundation Language Models",
	author        = "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and
	Martinet, Xavier and Lachaux, Marie-Anne and Lacroix,
	Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and
	Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and
	Joulin, Armand and Grave, Edouard and Lample, Guillaume",
	abstract      = "We introduce LLaMA, a collection of foundation language
	models ranging from 7B to 65B parameters. We train our
	models on trillions of tokens, and show that it is possible
	to train state-of-the-art models using publicly available
	datasets exclusively, without resorting to proprietary and
	inaccessible datasets. In particular, LLaMA-13B outperforms
	GPT-3 (175B) on most benchmarks, and LLaMA-65B is
	competitive with the best models, Chinchilla-70B and
	PaLM-540B. We release all our models to the research
	community.",
	month         =  "27~" # feb,
	year          =  2023,
	url           = "http://arxiv.org/abs/2302.13971",
	file          = "All Papers/T/Touvron et al. 2023 - LLaMA - Open and Efficient Foundation Language Models.pdf",
	archivePrefix = "arXiv",
	eprint        = "2302.13971",
	primaryClass  = "cs.CL",
	arxivid       = "2302.13971"
}

@INPROCEEDINGS{Mukhoti2020-ie,
	title     = "Calibrating Deep Neural Networks using Focal Loss",
	booktitle = "Advances in Neural Information Processing Systems",
	author    = "Mukhoti, Jishnu and Kulharia, Viveka and Sanyal, Amartya and
	Golodetz, Stuart and Torr, Philip and Dokania, Puneet",
	editor    = "Larochelle, H and Ranzato, M and Hadsell, R and Balcan, M F and
	Lin, H",
	publisher = "Curran Associates, Inc.",
	volume    =  33,
	pages     = "15288--15299",
	year      =  2020,
	url       = "https://proceedings.neurips.cc/paper/2020/file/aeb7b30ef1d024a76f21a1d40e30c302-Paper.pdf",
	file      = "All Papers/M/Mukhoti et al. 2020 - Calibrating Deep Neural Networks using Focal Loss.pdf"
}

@INPROCEEDINGS{Rezende2015-hx,
	title     = "Variational Inference with Normalizing Flows",
	booktitle = "Proceedings of the 32nd International Conference on Machine
	Learning",
	author    = "Rezende, Danilo and Mohamed, Shakir",
	editor    = "Bach, Francis and Blei, David",
	abstract  = "The choice of the approximate posterior distribution is one of
	the core problems in variational inference. Most applications of
	variational inference employ simple families of posterior
	approximations in order to allow for efficient inference,
	focusing on mean-field or other simple structured
	approximations. This restriction has a significant impact on the
	quality of inferences made using variational methods. We
	introduce a new approach for specifying flexible, arbitrarily
	complex and scalable approximate posterior distributions. Our
	approximations are distributions constructed through a
	normalizing flow, whereby a simple initial density is
	transformed into a more complex one by applying a sequence of
	invertible transformations until a desired level of complexity
	is attained. We use this view of normalizing flows to develop
	categories of finite and infinitesimal flows and provide a
	unified view of approaches for constructing rich posterior
	approximations. We demonstrate that the theoretical advantages
	of having posteriors that better match the true posterior,
	combined with the scalability of amortized variational
	approaches, provides a clear improvement in performance and
	applicability of variational inference.",
	publisher = "PMLR",
	volume    =  37,
	pages     = "1530--1538",
	series    = "Proceedings of Machine Learning Research",
	year      =  2015,
	url       = "https://proceedings.mlr.press/v37/rezende15.html",
	file      = "All Papers/R/Rezende and Mohamed 2015 - Variational Inference with Normalizing Flows.pdf",
	address   = "Lille, France"
}

@ARTICLE{Thiagarajan2020-gm,
	title       = "Designing accurate emulators for scientific processes using
	calibration-driven deep models",
	author      = "Thiagarajan, Jayaraman J and Venkatesh, Bindya and Anirudh,
	Rushil and Bremer, Peer-Timo and Gaffney, Jim and Anderson,
	Gemma and Spears, Brian",
	affiliation = "Lawrence Livermore National Laboratory, Center for Applied
	Scientific Computing, Livermore, CA, USA. jjayaram@llnl.gov.
	School of Electrical, Computer and Energy Engineering, Arizona
	State University, Tempe, AZ, USA. Lawrence Livermore National
	Laboratory, Center for Applied Scientific Computing,
	Livermore, CA, USA.",
	abstract    = "Predictive models that accurately emulate complex scientific
	processes can achieve speed-ups over numerical simulators or
	experiments and at the same time provide surrogates for
	improving the subsequent analysis. Consequently, there is a
	recent surge in utilizing modern machine learning methods to
	build data-driven emulators. In this work, we study an often
	overlooked, yet important, problem of choosing loss functions
	while designing such emulators. Popular choices such as the
	mean squared error or the mean absolute error are based on a
	symmetric noise assumption and can be unsuitable for
	heterogeneous data or asymmetric noise distributions. We
	propose Learn-by-Calibrating, a novel deep learning approach
	based on interval calibration for designing emulators that can
	effectively recover the inherent noise structure without any
	explicit priors. Using a large suite of use-cases, we
	demonstrate the efficacy of our approach in providing
	high-quality emulators, when compared to widely-adopted loss
	function choices, even in small-data regimes.",
	journal     = "Nature communications",
	volume      =  11,
	number      =  1,
	pages       = "5622",
	month       =  "6~" # nov,
	year        =  2020,
	url         = "http://dx.doi.org/10.1038/s41467-020-19448-8",
	file        = "All Papers/T/Thiagarajan et al. 2020 - Designing accurate emulators for scientific processes using calibration-driven deep models.pdf",
	language    = "en",
	issn        = "2041-1723",
	pmid        = "33159053",
	doi         = "10.1038/s41467-020-19448-8",
	pmc         = "PMC7648787"
}

@ARTICLE{Amini2019-vz,
	title         = "Deep Evidential Regression",
	author        = "Amini, Alexander and Schwarting, Wilko and Soleimany, Ava
	and Rus, Daniela",
	abstract      = "Deterministic neural networks (NNs) are increasingly being
	deployed in safety critical domains, where calibrated,
	robust, and efficient measures of uncertainty are crucial.
	In this paper, we propose a novel method for training
	non-Bayesian NNs to estimate a continuous target as well as
	its associated evidence in order to learn both aleatoric and
	epistemic uncertainty. We accomplish this by placing
	evidential priors over the original Gaussian likelihood
	function and training the NN to infer the hyperparameters of
	the evidential distribution. We additionally impose priors
	during training such that the model is regularized when its
	predicted evidence is not aligned with the correct output.
	Our method does not rely on sampling during inference or on
	out-of-distribution (OOD) examples for training, thus
	enabling efficient and scalable uncertainty learning. We
	demonstrate learning well-calibrated measures of uncertainty
	on various benchmarks, scaling to complex computer vision
	tasks, as well as robustness to adversarial and OOD test
	samples.",
	pages         = "14927--14937",
	month         =  "7~" # oct,
	year          =  2019,
	url           = "https://proceedings.neurips.cc/paper/2020/hash/aab085461de182608ee9f607f3f7d18f-Abstract.html",
	file          = "All Papers/A/Amini et al. 2019 - Deep Evidential Regression.pdf;All Papers/A/Amini et al. 2019 - Deep Evidential Regression.pdf",
	copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
	archivePrefix = "arXiv",
	eprint        = "1910.02600",
	primaryClass  = "cs.LG",
	arxivid       = "1910.02600"
}

@INPROCEEDINGS{Chen2020-lh,
	title     = "A Simple Framework for Contrastive Learning of Visual
	Representations",
	booktitle = "Proceedings of the 37th International Conference on Machine
	Learning",
	author    = "Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and
	Hinton, Geoffrey",
	editor    = "Iii, Hal Daum{\'e} and Singh, Aarti",
	abstract  = "This paper presents SimCLR: a simple framework for contrastive
	learning of visual representations. We simplify recently
	proposed contrastive self-supervised learning algorithms without
	requiring specialized architectures or a memory bank. In order
	to understand what enables the contrastive prediction tasks to
	learn useful representations, we systematically study the major
	components of our framework. We show that (1) composition of
	data augmentations plays a critical role in defining effective
	predictive tasks, (2) introducing a learnable nonlinear
	transformation between the representation and the contrastive
	loss substantially improves the quality of the learned
	representations, and (3) contrastive learning benefits from
	larger batch sizes and more training steps compared to
	supervised learning. By combining these findings, we are able to
	considerably outperform previous methods for self-supervised and
	semi-supervised learning on ImageNet. A linear classifier
	trained on self-supervised representations learned by SimCLR
	achieves 76.5\% top-1 accuracy, which is a 7\% relative
	improvement over previous state-of-the-art, matching the
	performance of a supervised ResNet-50. When fine-tuned on only
	1\% of the labels, we achieve 85.8\% top-5 accuracy,
	outperforming AlexNet with 100X fewer labels.",
	publisher = "PMLR",
	volume    =  119,
	pages     = "1597--1607",
	series    = "Proceedings of Machine Learning Research",
	year      =  2020,
	url       = "https://proceedings.mlr.press/v119/chen20j.html",
	file      = "All Papers/C/Chen et al. 2020 - 2002.05709.pdf;All Papers/C/Chen et al. 2020 - A Simple Framework for Contrastive Learning of Visual Representations.pdf"
}

@ARTICLE{Liu2020-ty,
	title         = "Self-supervised Learning: Generative or Contrastive",
	author        = "Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Wang, Zhaoyu
	and Mian, Li and Zhang, Jing and Tang, Jie",
	abstract      = "Deep supervised learning has achieved great success in the
	last decade. However, its deficiencies of dependence on
	manual labels and vulnerability to attacks have driven
	people to explore a better solution. As an alternative,
	self-supervised learning attracts many researchers for its
	soaring performance on representation learning in the last
	several years. Self-supervised representation learning
	leverages input data itself as supervision and benefits
	almost all types of downstream tasks. In this survey, we
	take a look into new self-supervised learning methods for
	representation in computer vision, natural language
	processing, and graph learning. We comprehensively review
	the existing empirical methods and summarize them into three
	main categories according to their objectives: generative,
	contrastive, and generative-contrastive (adversarial). We
	further investigate related theoretical analysis work to
	provide deeper thoughts on how self-supervised learning
	works. Finally, we briefly discuss open problems and future
	directions for self-supervised learning. An outline slide
	for the survey is provided.",
	month         =  "15~" # jun,
	year          =  2020,
	url           = "http://arxiv.org/abs/2006.08218",
	file          = "All Papers/L/Liu et al. 2020 - Self-supervised Learning - Generative or Contrastive.pdf",
	archivePrefix = "arXiv",
	eprint        = "2006.08218",
	primaryClass  = "cs.LG",
	arxivid       = "2006.08218"
}

@INPROCEEDINGS{Du2022-rw,
	title     = "{TO-FLOW}: Efficient Continuous Normalizing Flows with Temporal
	Optimization adjoint with Moving Speed",
	booktitle = "Proceedings of the {IEEE/CVF} Conference on Computer Vision and
	Pattern Recognition",
	author    = "Du, Shian and Luo, Yihong and Chen, Wei and Xu, Jian and Zeng,
	Delu",
	pages     = "12570--12580",
	year      =  2022,
	url       = "https://openaccess.thecvf.com/content/CVPR2022/papers/Du_TO-FLOW_Efficient_Continuous_Normalizing_Flows_With_Temporal_Optimization_Adjoint_With_CVPR_2022_paper.pdf",
	file      = "All Papers/D/Du et al. 2022 - TO-FLOW - Efficient Continuous Normalizing Flows with Temporal Optimization adjoint with Moving Speed.pdf"
}

@ARTICLE{Van_den_Oord2016-dj,
	title         = "Pixel Recurrent Neural Networks",
	author        = "van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu,
	Koray",
	abstract      = "Modeling the distribution of natural images is a landmark
	problem in unsupervised learning. This task requires an
	image model that is at once expressive, tractable and
	scalable. We present a deep neural network that sequentially
	predicts the pixels in an image along the two spatial
	dimensions. Our method models the discrete probability of
	the raw pixel values and encodes the complete set of
	dependencies in the image. Architectural novelties include
	fast two-dimensional recurrent layers and an effective use
	of residual connections in deep recurrent networks. We
	achieve log-likelihood scores on natural images that are
	considerably better than the previous state of the art. Our
	main results also provide benchmarks on the diverse ImageNet
	dataset. Samples generated from the model appear crisp,
	varied and globally coherent.",
	month         =  "25~" # jan,
	year          =  2016,
	url           = "http://arxiv.org/abs/1601.06759",
	file          = "All Papers/V/van den Oord et al. 2016 - Pixel Recurrent Neural Networks.pdf",
	archivePrefix = "arXiv",
	eprint        = "1601.06759",
	primaryClass  = "cs.CV",
	arxivid       = "1601.06759"
}

@BOOK{Murphy2023-iu,
	title     = "Probabilistic machine learning: Advanced topics",
	author    = "Murphy, Kevin P",
	publisher = "MIT Press",
	year      =  2023,
	url       = "http://probml.github.io/book2"
}

@MISC{Antic_undated-wt,
	title       = "{DeOldify}: A Deep Learning based project for colorizing and
	restoring old images (and video!)",
	author      = "Antic, Jason",
	abstract    = "A Deep Learning based project for colorizing and restoring old
	images (and video!) - jantic/DeOldify: A Deep Learning based
	project for colorizing and restoring old images (and video!)",
	institution = "Github",
	url         = "https://github.com/jantic/DeOldify",
	language    = "en"
}

@ARTICLE{Liu2018-nt,
	title         = "Image Inpainting for Irregular Holes Using Partial
	Convolutions",
	author        = "Liu, Guilin and Reda, Fitsum A and Shih, Kevin J and Wang,
	Ting-Chun and Tao, Andrew and Catanzaro, Bryan",
	abstract      = "Existing deep learning based image inpainting methods use a
	standard convolutional network over the corrupted image,
	using convolutional filter responses conditioned on both
	valid pixels as well as the substitute values in the masked
	holes (typically the mean value). This often leads to
	artifacts such as color discrepancy and blurriness.
	Post-processing is usually used to reduce such artifacts,
	but are expensive and may fail. We propose the use of
	partial convolutions, where the convolution is masked and
	renormalized to be conditioned on only valid pixels. We
	further include a mechanism to automatically generate an
	updated mask for the next layer as part of the forward pass.
	Our model outperforms other methods for irregular masks. We
	show qualitative and quantitative comparisons with other
	methods to validate our approach.",
	month         =  "20~" # apr,
	year          =  2018,
	url           = "http://arxiv.org/abs/1804.07723",
	file          = "All Papers/L/Liu et al. 2018 - Image Inpainting for Irregular Holes Using Partial Convolutions.pdf",
	archivePrefix = "arXiv",
	eprint        = "1804.07723",
	primaryClass  = "cs.CV",
	arxivid       = "1804.07723"
}

@ARTICLE{Kompa2021-ea,
	title       = "Empirical Frequentist Coverage of Deep Learning Uncertainty
	Quantification Procedures",
	author      = "Kompa, Benjamin and Snoek, Jasper and Beam, Andrew L",
	affiliation = "Department of Biomedical Informatics, Harvard Medical School,
	Boston, MA 02115, USA. Google Research, Cambridge, MA 02142,
	USA. Department of Epidemiology, Harvard School of Public
	Health, Boston, MA 02115, USA.",
	abstract    = "Uncertainty quantification for complex deep learning models is
	increasingly important as these techniques see growing use in
	high-stakes, real-world settings. Currently, the quality of a
	model's uncertainty is evaluated using point-prediction
	metrics, such as the negative log-likelihood (NLL), expected
	calibration error (ECE) or the Brier score on held-out data.
	Marginal coverage of prediction intervals or sets, a
	well-known concept in the statistical literature, is an
	intuitive alternative to these metrics but has yet to be
	systematically studied for many popular uncertainty
	quantification techniques for deep learning models. With
	marginal coverage and the complementary notion of the width of
	a prediction interval, downstream users of deployed machine
	learning models can better understand uncertainty
	quantification both on a global dataset level and on a
	per-sample basis. In this study, we provide the first
	large-scale evaluation of the empirical frequentist coverage
	properties of well-known uncertainty quantification techniques
	on a suite of regression and classification tasks. We find
	that, in general, some methods do achieve desirable coverage
	properties on in distribution samples, but that coverage is
	not maintained on out-of-distribution data. Our results
	demonstrate the failings of current uncertainty quantification
	techniques as dataset shift increases and reinforce coverage
	as an important metric in developing models for real-world
	applications.",
	journal     = "Entropy",
	volume      =  23,
	number      =  12,
	month       =  "30~" # nov,
	year        =  2021,
	url         = "http://dx.doi.org/10.3390/e23121608",
	file        = "All Papers/K/Kompa et al. 2021 - Empirical Frequentist Coverage of Deep Learning Uncertainty Quantification Procedures.pdf",
	keywords    = "Bayesian methods; coverage; dataset shift; uncertainty
	quantification",
	language    = "en",
	issn        = "1099-4300",
	pmid        = "34945914",
	doi         = "10.3390/e23121608",
	pmc         = "PMC8700765"
}

@ARTICLE{Van_den_Oord2016-cl,
	title         = "Conditional Image Generation with {PixelCNN} Decoders",
	author        = "van den Oord, Aaron and Kalchbrenner, Nal and Vinyals, Oriol
	and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray",
	abstract      = "This work explores conditional image generation with a new
	image density model based on the PixelCNN architecture. The
	model can be conditioned on any vector, including
	descriptive labels or tags, or latent embeddings created by
	other networks. When conditioned on class labels from the
	ImageNet database, the model is able to generate diverse,
	realistic scenes representing distinct animals, objects,
	landscapes and structures. When conditioned on an embedding
	produced by a convolutional network given a single image of
	an unseen face, it generates a variety of new portraits of
	the same person with different facial expressions, poses and
	lighting conditions. We also show that conditional PixelCNN
	can serve as a powerful decoder in an image autoencoder.
	Additionally, the gated convolutional layers in the proposed
	model improve the log-likelihood of PixelCNN to match the
	state-of-the-art performance of PixelRNN on ImageNet, with
	greatly reduced computational cost.",
	month         =  "16~" # jun,
	year          =  2016,
	url           = "http://arxiv.org/abs/1606.05328",
	file          = "All Papers/V/van den Oord et al. 2016 - Conditional Image Generation with PixelCNN Decoders.pdf",
	archivePrefix = "arXiv",
	eprint        = "1606.05328",
	primaryClass  = "cs.CV",
	arxivid       = "1606.05328"
}

@INPROCEEDINGS{Rasul2021-qe,
	title     = "Multivariate Probabilistic Time Series Forecasting via
	Conditioned Normalizing Flows",
	booktitle = "International Conference on Learning Representations",
	author    = "Rasul, Kashif and Sheikh, Abdul-Saboor and Schuster, Ingmar and
	Bergmann, Urs M and Vollgraf, Roland",
	year      =  2021,
	url       = "https://openreview.net/forum?id=WiGQBFuVRv",
	file      = "All Papers/R/Rasul et al. 2021 - Multivariate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows.pdf"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Van_Den_Oord2017-hx,
	title     = "Neural discrete representation learning",
	author    = "Van Den Oord, Aaron and Vinyals, Oriol and {Others}",
	abstract  = "… without supervision remains a key challenge in machine
	learning . In this paper, we propose a simple yet powerful
	generative model that learns such discrete representations . Our
	model…",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	volume    =  30,
	year      =  2017,
	url       = "https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html",
	file      = "All Papers/V/Van Den Oord et al. 2017 - 1711.00937.pdf;All Papers/V/Van Den Oord et al. 2017 - Neural discrete representation learning.pdf",
	issn      = "1049-5258"
}

@ARTICLE{Rombach2021-xs,
	title         = "{High-Resolution} Image Synthesis with Latent Diffusion
	Models",
	author        = "Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik
	and Esser, Patrick and Ommer, Bj{\"o}rn",
	abstract      = "By decomposing the image formation process into a sequential
	application of denoising autoencoders, diffusion models
	(DMs) achieve state-of-the-art synthesis results on image
	data and beyond. Additionally, their formulation allows for
	a guiding mechanism to control the image generation process
	without retraining. However, since these models typically
	operate directly in pixel space, optimization of powerful
	DMs often consumes hundreds of GPU days and inference is
	expensive due to sequential evaluations. To enable DM
	training on limited computational resources while retaining
	their quality and flexibility, we apply them in the latent
	space of powerful pretrained autoencoders. In contrast to
	previous work, training diffusion models on such a
	representation allows for the first time to reach a
	near-optimal point between complexity reduction and detail
	preservation, greatly boosting visual fidelity. By
	introducing cross-attention layers into the model
	architecture, we turn diffusion models into powerful and
	flexible generators for general conditioning inputs such as
	text or bounding boxes and high-resolution synthesis becomes
	possible in a convolutional manner. Our latent diffusion
	models (LDMs) achieve a new state of the art for image
	inpainting and highly competitive performance on various
	tasks, including unconditional image generation, semantic
	scene synthesis, and super-resolution, while significantly
	reducing computational requirements compared to pixel-based
	DMs. Code is available at
	https://github.com/CompVis/latent-diffusion .",
	month         =  "20~" # dec,
	year          =  2021,
	url           = "http://arxiv.org/abs/2112.10752",
	file          = "All Papers/R/Rombach et al. 2021 - High-Resolution Image Synthesis with Latent Diffusion Models.pdf",
	archivePrefix = "arXiv",
	eprint        = "2112.10752",
	primaryClass  = "cs.CV",
	arxivid       = "2112.10752"
}

@UNPUBLISHED{Si2023-rw,
	title    = "{Semi-Autoregressive} Energy Flows: Towards {Determinant-Free}
	Training of Normalizing Flows",
	author   = "Si, Phillip and Chen, Zeyi and Sahoo, Subham Sekhar and Schiff,
	Yair and Kuleshov, Volodymyr",
	abstract = "Normalizing flows are a popular approach for constructing
	probabilistic and generative models. However, maximum likelihood
	training of flows is challenging due to the need to calculate
	computationally expensive determinants of Jacobians. This paper
	takes steps towards addressing this challenge by introducing
	objectives and model architectures for determinant-free training
	of flows. Central to our framework is the energy objective, a
	multidimensional extension of proper scoring rules that admits
	efficient estimators based on random projections. The energy
	objective does not require calculating determinants and therefore
	supports general flow architectures that are not well-suited to
	maximum likelihood training. In particular, we introduce
	semi-autoregressive flows, an architecture that can be trained
	with the energy loss, and that interpolates between fully
	autoregressive and non-autoregressive models, capturing the
	benefits of both. We empirically demonstrate that energy flows
	achieve competitive generative modeling performance while
	maintaining fast generation and posterior inference.",
	month    =  "1~" # feb,
	year     =  2023,
	url      = "https://openreview.net/pdf?id=GBU1mm8_WkV",
	file     = "All Papers/S/Si et al. 2023 - Semi-Autoregressive Energy Flows - Towards Determinant-Free Training of Normalizing Flows.pdf"
}

@ARTICLE{Ziel2019-zv,
	title         = "Multivariate Forecasting Evaluation: On Sensitive and
	Strictly Proper Scoring Rules",
	author        = "Ziel, Florian and Berk, Kevin",
	abstract      = "In recent years, probabilistic forecasting is an emerging
	topic, which is why there is a growing need of suitable
	methods for the evaluation of multivariate predictions. We
	analyze the sensitivity of the most common scoring rules,
	especially regarding quality of the forecasted dependency
	structures. Additionally, we propose scoring rules based on
	the copula, which uniquely describes the dependency
	structure for every probability distribution with continuous
	marginal distributions. Efficient estimation of the
	considered scoring rules and evaluation methods such as the
	Diebold-Mariano test are discussed. In detailed simulation
	studies, we compare the performance of the renowned scoring
	rules and the ones we propose. Besides extended synthetic
	studies based on recently published results we also consider
	a real data example. We find that the energy score, which is
	probably the most widely used multivariate scoring rule,
	performs comparably well in detecting forecast errors, also
	regarding dependencies. This contradicts other studies. The
	results also show that a proposed copula score provides very
	strong distinction between models with correct and incorrect
	dependency structure. We close with a comprehensive
	discussion on the proposed methodology.",
	month         =  "16~" # oct,
	year          =  2019,
	url           = "http://arxiv.org/abs/1910.07325",
	file          = "All Papers/Z/Ziel and Berk 2019 - Multivariate Forecasting Evaluation - On Sensitive and Strictly Proper Scoring Rules.pdf",
	archivePrefix = "arXiv",
	eprint        = "1910.07325",
	primaryClass  = "stat.ME",
	arxivid       = "1910.07325"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Knuppel2022-dx,
	title         = "Score-based calibration testing for multivariate forecast
	distributions",
	author        = "Kn{\"u}ppel, Malte and Kr{\"u}ger, Fabian and Pohle,
	Marc-Oliver",
	abstract      = "Multivariate distributional forecasts have become widespread
	in recent years. To assess the quality of such forecasts,
	suitable evaluation methods are needed. In the univariate
	case, calibration tests based on the probability integral
	transform (PIT) are routinely used. However, multivariate
	extensions of PIT-based calibration tests face various
	challenges. We therefore introduce a general framework for
	calibration testing in the multivariate case and propose two
	new tests that arise from it. Both approaches use proper
	scoring rules and are simple to implement even in large
	dimensions. The first employs the PIT of the score. The
	second is based on comparing the expected performance of the
	forecast distribution (i.e., the expected score) to its
	actual performance based on realized observations (i.e., the
	realized score). The tests have good size and power
	properties in simulations and solve various problems of
	existing tests. We apply the new tests to forecast
	distributions for macroeconomic and financial time series
	data.",
	month         =  "29~" # nov,
	year          =  2022,
	url           = "http://arxiv.org/abs/2211.16362",
	file          = "All Papers/K/Knüppel et al. 2022 - Score-based calibration testing for multivariate forecast distributions.pdf",
	archivePrefix = "arXiv",
	eprint        = "2211.16362",
	primaryClass  = "econ.EM",
	arxivid       = "2211.16362"
}

@UNPUBLISHED{Perez-Lebel2023-hl,
	title    = "Beyond calibration: estimating the grouping loss of modern neural
	networks",
	author   = "Perez-Lebel, Alexandre and Le Morvan, Marine and Varoquaux, Gael",
	abstract = "Good decision making requires machine-learning models to provide
	trustworthy confidence scores. To this end, recent work has
	focused on miscalibration, i.e, the over or under confidence of
	model scores. Yet, contrary to widespread belief, calibration is
	not enough: even a classifier with the best possible accuracy and
	perfect calibration can have confidence scores far from the true
	posterior probabilities. This is due to the grouping loss,
	created by samples with the same confidence scores but different
	true posterior probabilities. Proper scoring rule theory shows
	that given the calibration loss, the missing piece to
	characterize individual errors is the grouping loss. While there
	are many estimators of the calibration loss, none exists for the
	grouping loss in standard settings. Here, we propose an estimator
	to approximate the grouping loss. We use it to study modern
	neural network architectures in vision and NLP. We find that the
	grouping loss varies markedly across architectures, and that it
	is a key model-comparison factor across the most accurate,
	calibrated, models. We also show that distribution shifts lead to
	high grouping loss.",
	month    =  "1~" # feb,
	year     =  2023,
	url      = "https://openreview.net/pdf?id=6w1k-IixnL8",
	file     = "All Papers/P/Perez-Lebel et al. 2023 - Beyond calibration - estimating the grouping loss of modern neural networks.pdf"
}

@UNPUBLISHED{Yoon2023-pa,
	title    = "{ESD}: Expected Squared Difference as a {Tuning-Free} Trainable
	Calibration Measure",
	author   = "Yoon, Hee Suk and Tee, Joshua Tian Jin and Yoon, Eunseop and
	Yoon, Sunjae and Kim, Gwangsu and Li, Yingzhen and Yoo, Chang D",
	abstract = "Recent studies have shown that modern neural networks tend to be
	poorly calibrated due to over-confident predictions.
	Traditionally, post-processing methods have been used to
	calibrate the model after training. On the other hand, various
	trainable calibration measures have been proposed recently to
	incorporate the calibration objective loss directly into the
	training process. However, these methods all incorporate internal
	hyperparameters introduced in the process of obtaining a
	differential calibration measure. Consequently, the performance
	of these calibration objectives relies on tuning these
	hyperparameters, incurring more computational cost as the size of
	neural networks and datasets become larger. As such, we present
	Expected Squared Difference (ESD), a tuning-free (i.e.,
	hyperparameter-free) trainable calibration objective loss, where
	we view the calibration error from the perspective of the squared
	difference between two expectations. With extensive experiments
	on several architectures (CNNs, Transformers) and datasets, we
	demonstrate that (1) incorporating ESD into the training improves
	model calibration in various batch size settings without the need
	for internal hyperparmeter tuning, (2) ESD yields the best
	calibrated results compared with previous approaches, (3) show
	that ESD drastically improve the computational cost required for
	calibration during training due to the absence of internal
	hyperparamater. Code will be publicly available.",
	month    =  "1~" # feb,
	year     =  2023,
	url      = "https://openreview.net/pdf?id=bHW9njOSON",
	file     = "All Papers/Y/Yoon et al. 2023 - ESD - Expected Squared Difference as a Tuning-Free Trainable Calibration Measure.pdf"
}

@ARTICLE{Klein2021-bj,
	title     = "Marginally Calibrated Deep Distributional Regression",
	author    = "Klein, Nadja and Nott, David J and Smith, Michael Stanley",
	abstract  = "AbstractDeep neural network (DNN) regression models are widely
	used in applications requiring state-of-the-art predictive
	accuracy. However, until recently there has been little work on
	accurate uncertainty quantification for predictions from such
	models. We add to this literature by outlining an approach to
	constructing predictive distributions that are ?marginally
	calibrated.? This is where the long run average of the
	predictive distributions of the response variable matches the
	observed empirical margin. Our approach considers a DNN
	regression with a conditionally Gaussian prior for the final
	layer weights, from which an implicit copula process on the
	feature space is extracted. This copula process is combined with
	a non-parametrically estimated marginal distribution for the
	response. The end result is a scalable distributional DNN
	regression method with marginally calibrated predictions, and
	our work complements existing methods for probability
	calibration. The approach is first illustrated using two
	applications of dense layer feed-forward neural networks.
	However, our main motivating applications are in likelihood-free
	inference, where distributional deep regression is used to
	estimate marginal posterior distributions. In two complex
	ecological time series examples, we employ the implicit copulas
	of convolutional networks, and show that marginal calibration
	results in improved uncertainty quantification. Our approach
	also avoids the need for manual specification of summary
	statistics, a requirement that is burdensome for users and
	typical of competing likelihood-free inference methods.
	Supplementary materials for this article are available online.",
	journal   = "Journal of computational and graphical statistics: a joint
	publication of American Statistical Association, Institute of
	Mathematical Statistics, Interface Foundation of North America",
	publisher = "Taylor \& Francis",
	volume    =  30,
	number    =  2,
	pages     = "467--483",
	month     =  "3~" # apr,
	year      =  2021,
	url       = "https://doi.org/10.1080/10618600.2020.1807996",
	file      = "All Papers/K/Klein et al. 2021 - Marginally Calibrated Deep Distributional Regression.pdf",
	issn      = "1061-8600",
	doi       = "10.1080/10618600.2020.1807996"
}

@ARTICLE{Abe2022-cs,
	title         = "Deep Ensembles Work, But Are They Necessary?",
	author        = "Abe, Taiga and Kelly Buchanan, E and Pleiss, Geoff and
	Zemel, Richard and Cunningham, John P",
	abstract      = "Ensembling neural networks is an effective way to increase
	accuracy, and can often match the performance of individual
	larger models. This observation poses a natural question:
	given the choice between a deep ensemble and a single neural
	network with similar accuracy, is one preferable over the
	other? Recent work suggests that deep ensembles may offer
	distinct benefits beyond predictive power: namely,
	uncertainty quantification and robustness to dataset shift.
	In this work, we demonstrate limitations to these purported
	benefits, and show that a single (but larger) neural network
	can replicate these qualities. First, we show that ensemble
	diversity, by any metric, does not meaningfully contribute
	to an ensemble's uncertainty quantification on
	out-of-distribution (OOD) data, but is instead highly
	correlated with the relative improvement of a single larger
	model. Second, we show that the OOD performance afforded by
	ensembles is strongly determined by their in-distribution
	(InD) performance, and -- in this sense -- is not indicative
	of any ``effective robustness''. While deep ensembles are a
	practical way to achieve improvements to predictive power,
	uncertainty quantification, and robustness, our results show
	that these improvements can be replicated by a (larger)
	single model.",
	month         =  "14~" # feb,
	year          =  2022,
	url           = "http://arxiv.org/abs/2202.06985",
	file          = "All Papers/A/Abe et al. 2022 - Deep Ensembles Work, But Are They Necessary.pdf",
	archivePrefix = "arXiv",
	eprint        = "2202.06985",
	primaryClass  = "cs.LG",
	arxivid       = "2202.06985"
}

@BOOK{Blum1963-ty,
	title     = "On the Strong Law of Large Numbers for a Class of Stochastic
	Processes",
	author    = "Blum, J R and Hanson, David Lee and Koopmans, Lambert Herman",
	abstract  = "Let (2, F, P) be a probability space and let \{Xn, n= 1, 2,...\}
	be a sequence of real-valued random variables defined on (2, F,
	P). For each positive integer n let be the smallest o- algebra
	with respect to which Xn is measurable and for n= m let be the
	smallest o-algebra with respect to which Xn,... Xm are jointly
	measurable. Definition. The sequence \{Xn\} will be
	called*-mixing if there exists a positive integer N and a
	real-valued function f defined for the integers n 2 N such that
	i) fi s non-increasing with lim f (n)= 0, and n",
	publisher = "Sandia Corporation",
	year      =  1963,
	url       = "https://play.google.com/store/books/details?id=YhMU4ZUGmDkC",
	language  = "en"
}

@BOOK{Abu-Mostafa2012-fp,
	title     = "Learning From Data",
	author    = "Abu-Mostafa, Yaser S and Magdon-Ismail, Malik and Lin,
	Hsuan-Tien",
	publisher = "AMLBook",
	year      =  2012,
	keywords  = "general\_machine\_learning"
}

@ARTICLE{Ovcharov2018-px,
	title     = "Proper scoring rules and Bregman divergence",
	author    = "Ovcharov, Evgeni Y",
	abstract  = "Proper scoring rules measure the quality of probabilistic
	forecasts. They induce dissimilarity measures of probability
	distributions known as Bregman divergences. We survey the
	literature on both entities and present their mathematical
	properties in a unified theoretical framework. Score and Bregman
	divergences are developed as a single concept. We formalize the
	proper affine scoring rules and present a motivating example
	from robust estimation. And lastly, we develop the elements of
	the regularity theory of entropy functions and describe under
	what conditions a general convex function may be identified as
	the entropy function of a proper scoring rule and whether this
	association is unique.",
	journal   = "BJOG: an international journal of obstetrics and gynaecology",
	publisher = "Bernoulli Society for Mathematical Statistics and Probability",
	volume    =  24,
	number    =  1,
	pages     = "53--79",
	month     =  feb,
	year      =  2018,
	url       = "https://projecteuclid.org/journals/bernoulli/volume-24/issue-1/Proper-scoring-rules-and-Bregman-divergence/10.3150/16-BEJ857.full",
	file      = "All Papers/O/Ovcharov 2018 - Proper scoring rules and Bregman divergence.pdf",
	keywords  = "Bregman divergence; characterisation; convex analysis; Entropy;
	proper scoring rule; robust estimation; subgradient;",
	language  = "en",
	issn      = "1470-0328, 1350-7265",
	doi       = "10.3150/16-BEJ857"
}

@INPROCEEDINGS{Gopalan2022-si,
	title     = "{Low-Degree} Multicalibration",
	booktitle = "Proceedings of Thirty Fifth Conference on Learning Theory",
	author    = "Gopalan, Parikshit and Kim, Michael P and Singhal, Mihir A and
	Zhao, Shengjia",
	editor    = "Loh, Po-Ling and Raginsky, Maxim",
	abstract  = "Introduced as a notion of algorithmic fairness, multicalibration
	has proved to be a powerful and versatile concept with
	implications far beyond its original intent. This stringent
	notion---that predictions be well-calibrated across a rich class
	of intersecting subpopulations---provides its strong guarantees
	at a cost: the computational and sample complexity of learning
	multicalibrated predictors are high, and grow exponentially with
	the number of class labels. In contrast, the relaxed notion of
	multiaccuracy can be achieved more efficiently, yet many of the
	most desirable properties of multicalibration cannot be
	guaranteed assuming multiaccuracy alone. This tension raises a
	key question: \textbackslashemphCan we learn predictors with
	multicalibration-style guarantees at a cost commensurate with
	multiaccuracy? In this work, we define and initiate the study of
	\textbackslashemphLow-Degree Multicalibration. Low-Degree
	Multicalibration defines a hierarchy of increasingly-powerful
	multi-group fairness notions that spans multiaccuracy and the
	original formulation of multicalibration at the extremes. Our
	main technical contribution demonstrates that key properties of
	multicalibration, related to fairness and accuracy, actually
	manifest as low-degree properties. Importantly, we show that
	low-degree multicalibration can be significantly more efficient
	than full multicalibration. In the multi-class setting, the
	sample complexity to achieve low-degree multicalibration
	improves exponentially (in the number of classes) over full
	multicalibration. Our work presents compelling evidence that
	low-degree multicalibration represents a sweet spot, pairing
	computational and sample efficiency with strong fairness and
	accuracy guarantees.",
	publisher = "PMLR",
	volume    =  178,
	pages     = "3193--3234",
	series    = "Proceedings of Machine Learning Research",
	year      =  2022,
	url       = "https://proceedings.mlr.press/v178/gopalan22a.html",
	file      = "All Papers/G/Gopalan et al. 2022 - Low-Degree Multicalibration.pdf"
}

@TECHREPORT{Hofmann2011-rg,
	title       = "Letter-value plots: Boxplots for large data",
	author      = "Hofmann, Heike and Kafadar, Karen and Wickham, Hadley",
	institution = "had.co.nz",
	year        =  2011
}

@MISC{Dua2017-ut,
	title       = "{UCI} Machine Learning Repository",
	author      = "Dua, Dheeru and Graff, Casey",
	institution = "University of California, Irvine, School of Information and
	Computer Sciences",
	year        =  2017,
	url         = "http://archive.ics.uci.edu/ml"
}

@INPROCEEDINGS{Nix1994-mb,
	title     = "Estimating the mean and variance of the target probability
	distribution",
	booktitle = "Proceedings of 1994 {IEEE} International Conference on Neural
	Networks ({ICNN'94})",
	author    = "Nix, D A and Weigend, A S",
	abstract  = "Introduces a method that estimates the mean and the variance of
	the probability distribution of the target as a function of the
	input, given an assumed target error-distribution model. Through
	the activation of an auxiliary output unit, this method provides
	a measure of the uncertainty of the usual network output for
	each input pattern. The authors derive the cost function and
	weight-update equations for the example of a Gaussian target
	error distribution, and demonstrate the feasibility of the
	network on a synthetic problem where the true input-dependent
	noise level is known.",
	volume    =  1,
	pages     = "55--60 vol.1",
	month     =  jun,
	year      =  1994,
	url       = "http://dx.doi.org/10.1109/ICNN.1994.374138",
	keywords  = "Probability distribution;Noise level;Feedforward
	systems;Computer science;Cognitive science;Computer
	errors;Measurement uncertainty;Cost function;Equations;Error
	correction",
	doi       = "10.1109/ICNN.1994.374138"
}

@ARTICLE{Gressmann2018-we,
	title         = "Probabilistic supervised learning",
	author        = "Gressmann, Frithjof and Kir{\'a}ly, Franz J and Mateen,
	Bilal and Oberhauser, Harald",
	abstract      = "Predictive modelling and supervised learning are central to
	modern data science. With predictions from an ever-expanding
	number of supervised black-box strategies - e.g., kernel
	methods, random forests, deep learning aka neural networks -
	being employed as a basis for decision making processes, it
	is crucial to understand the statistical uncertainty
	associated with these predictions. As a general means to
	approach the issue, we present an overarching framework for
	black-box prediction strategies that not only predict the
	target but also their own predictions' uncertainty.
	Moreover, the framework allows for fair assessment and
	comparison of disparate prediction strategies. For this, we
	formally consider strategies capable of predicting full
	distributions from feature variables, so-called
	probabilistic supervised learning strategies. Our work draws
	from prior work including Bayesian statistics, information
	theory, and modern supervised machine learning, and in a
	novel synthesis leads to (a) new theoretical insights such
	as a probabilistic bias-variance decomposition and an
	entropic formulation of prediction, as well as to (b) new
	algorithms and meta-algorithms, such as composite prediction
	strategies, probabilistic boosting and bagging, and a
	probabilistic predictive independence test. Our black-box
	formulation also leads (c) to a new modular interface view
	on probabilistic supervised learning and a modelling
	workflow API design, which we have implemented in the newly
	released skpro machine learning toolbox, extending the
	familiar modelling interface and meta-modelling
	functionality of sklearn. The skpro package provides
	interfaces for construction, composition, and tuning of
	probabilistic supervised learning strategies, together with
	orchestration features for validation and comparison of any
	such strategy - be it frequentist, Bayesian, or other.",
	month         =  "2~" # jan,
	year          =  2018,
	url           = "http://arxiv.org/abs/1801.00753",
	file          = "All Papers/G/Gressmann et al. 2018 - Probabilistic supervised learning.pdf",
	archivePrefix = "arXiv",
	eprint        = "1801.00753",
	primaryClass  = "stat.ML",
	arxivid       = "1801.00753"
}

@ARTICLE{Cuturi2019-dv,
	title         = "Differentiable Ranks and Sorting using Optimal Transport",
	author        = "Cuturi, Marco and Teboul, Olivier and Vert, Jean-Philippe",
	abstract      = "Sorting an array is a fundamental routine in machine
	learning, one that is used to compute rank-based statistics,
	cumulative distribution functions (CDFs), quantiles, or to
	select closest neighbors and labels. The sorting function is
	however piece-wise constant (the sorting permutation of a
	vector does not change if the entries of that vector are
	infinitesimally perturbed) and therefore has no gradient
	information to back-propagate. We propose a framework to
	sort elements that is algorithmically differentiable. We
	leverage the fact that sorting can be seen as a particular
	instance of the optimal transport (OT) problem on
	$\mathbb\{R\}$, from input values to a predefined array of
	sorted values (e.g. $1,2,\dots,n$ if the input array has $n$
	elements). Building upon this link , we propose generalized
	CDFs and quantile operators by varying the size and weights
	of the target presorted array. Because this amounts to using
	the so-called Kantorovich formulation of OT, we call these
	quantities K-sorts, K-CDFs and K-quantiles. We recover
	differentiable algorithms by adding to the OT problem an
	entropic regularization, and approximate it using a few
	Sinkhorn iterations. We call these operators S-sorts, S-CDFs
	and S-quantiles, and use them in various learning settings:
	we benchmark them against the recently proposed neuralsort
	[Grover et al. 2019], propose applications to quantile
	regression and introduce differentiable formulations of the
	top-k accuracy that deliver state-of-the art performance.",
	month         =  "28~" # may,
	year          =  2019,
	url           = "https://proceedings.neurips.cc/paper/2019/file/d8c24ca8f23c562a5600876ca2a550ce-Paper.pdf",
	file          = "All Papers/C/Cuturi et al. 2019 - Differentiable Ranks and Sorting using Optimal Transport.pdf",
	copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
	archivePrefix = "arXiv",
	eprint        = "1905.11885",
	primaryClass  = "cs.LG",
	arxivid       = "1905.11885"
}

@INPROCEEDINGS{Blondel2020-sf,
	title     = "Fast Differentiable Sorting and Ranking",
	booktitle = "Proceedings of the 37th International Conference on Machine
	Learning",
	author    = "Blondel, Mathieu and Teboul, Olivier and Berthet, Quentin and
	Djolonga, Josip",
	editor    = "Iii, Hal Daum{\'e} and Singh, Aarti",
	abstract  = "The sorting operation is one of the most commonly used building
	blocks in computer programming. In machine learning, it is often
	used for robust statistics. However, seen as a function, it is
	piecewise linear and as a result includes many kinks where it is
	non-differentiable. More problematic is the related ranking
	operator, often used for order statistics and ranking metrics.
	It is a piecewise constant function, meaning that its
	derivatives are null or undefined. While numerous works have
	proposed differentiable proxies to sorting and ranking, they do
	not achieve the $O(n \log n)$ time complexity one would expect
	from sorting and ranking operations. In this paper, we propose
	the first differentiable sorting and ranking operators with $O(n
	\log n)$ time and $O(n)$ space complexity. Our proposal in
	addition enjoys exact computation and differentiation. We
	achieve this feat by constructing differentiable operators as
	projections onto the permutahedron, the convex hull of
	permutations, and using a reduction to isotonic optimization.
	Empirically, we confirm that our approach is an order of
	magnitude faster than existing approaches and showcase two novel
	applications: differentiable Spearman's rank correlation
	coefficient and least trimmed squares.",
	publisher = "PMLR",
	volume    =  119,
	pages     = "950--959",
	series    = "Proceedings of Machine Learning Research",
	year      =  2020,
	url       = "https://proceedings.mlr.press/v119/blondel20a.html",
	file      = "All Papers/B/Blondel et al. 2020 - Fast Differentiable Sorting and Ranking.pdf"
}

@ARTICLE{Singh2021-mo,
	title         = "On Deep Neural Network Calibration by Regularization and its
	Impact on Refinement",
	author        = "Singh, Aditya and Bay, Alessandro and Sengupta, Biswa and
	Mirabile, Andrea",
	abstract      = "Deep neural networks have been shown to be highly
	miscalibrated. often they tend to be overconfident in their
	predictions. It poses a significant challenge for
	safety-critical systems to utilise deep neural networks
	(DNNs), reliably. Many recently proposed approaches to
	mitigate this have demonstrated substantial progress in
	improving DNN calibration. However, they hardly touch upon
	refinement, which historically has been an essential aspect
	of calibration. Refinement indicates separability of a
	network's correct and incorrect predictions. This paper
	presents a theoretically and empirically supported
	exposition reviewing refinement of a calibrated model.
	Firstly, we show the breakdown of expected calibration error
	(ECE), into predicted confidence and refinement under the
	assumption of over-confident predictions. Secondly, linking
	with this result, we highlight that regularization based
	calibration only focuses on naively reducing a model's
	confidence. This logically has a severe downside to a
	model's refinement as correct and incorrect predictions
	become tightly coupled. Lastly, connecting refinement with
	ECE also provides support to existing refinement based
	approaches which improve calibration but do not explain the
	reasoning behind it. We support our claims through rigorous
	empirical evaluations of many state of the art calibration
	approaches on widely used datasets and neural networks. We
	find that many calibration approaches with the likes of
	label smoothing, mixup etc. lower the usefulness of a DNN by
	degrading its refinement. Even under natural data shift,
	this calibration-refinement trade-off holds for the majority
	of calibration methods.",
	month         =  "17~" # jun,
	year          =  2021,
	url           = "http://arxiv.org/abs/2106.09385",
	file          = "All Papers/S/Singh et al. 2021 - On Deep Neural Network Calibration by Regularization and its Impact on Refinement.pdf",
	archivePrefix = "arXiv",
	eprint        = "2106.09385",
	primaryClass  = "cs.LG",
	arxivid       = "2106.09385"
}

@UNPUBLISHED{Stutz2022-uh,
	title    = "Learning Optimal Conformal Classifiers",
	author   = "Stutz, David and Dvijotham, Krishnamurthy Dj and Cemgil, Ali
	Taylan and Doucet, Arnaud",
	abstract = "Modern deep learning based classifiers show very high accuracy on
	test data but this does not provide sufficient guarantees for
	safe deployment, especially in high-stake AI applications such as
	medical diagnosis. Usually, predictions are obtained without a
	reliable uncertainty estimate or a formal guarantee. Conformal
	prediction (CP) addresses these issues by using the classifier's
	predictions, e.g., its probability estimates, to predict
	confidence sets containing the true class with a user-specified
	probability. However, using CP as a separate processing step
	after training prevents the underlying model from adapting to the
	prediction of confidence sets. Thus, this paper explores
	strategies to differentiate through CP during training with the
	goal of training model with the conformal wrapper end-to-end. In
	our approach, conformal training (ConfTr), we specifically
	``simulate'' conformalization on mini-batches during training.
	Compared to standard training, ConfTr reduces the average
	confidence set size (inefficiency) of state-of-the-art CP methods
	applied after training. Moreover, it allows to ``shape'' the
	confidence sets predicted at test time, which is difficult for
	standard CP. On experiments with several datasets, we show ConfTr
	can influence how inefficiency is distributed across classes, or
	guide the composition of confidence sets in terms of the included
	classes, while retaining the guarantees offered by CP.",
	month    =  may,
	year     =  2022,
	url      = "https://openreview.net/pdf?id=t8O-4LKFVx",
	file     = "All Papers/S/Stutz et al. 2022 - Learning Optimal Conformal Classifiers.pdf"
}

@ARTICLE{Wilcoxon1945-cp,
	title     = "Individual Comparisons by Ranking Methods",
	author    = "Wilcoxon, Frank",
	journal   = "Biometrics Bulletin",
	publisher = "[International Biometric Society, Wiley]",
	volume    =  1,
	number    =  6,
	pages     = "80--83",
	year      =  1945,
	url       = "http://www.jstor.org/stable/3001968",
	file      = "All Papers/W/Wilcoxon 1945 - Individual Comparisons by Ranking Methods.pdf",
	issn      = "0099-4987",
	doi       = "10.2307/3001968"
}

@ARTICLE{Friedman1940-vi,
	title     = "A Comparison of Alternative Tests of Significance for the
	Problem of $m$ Rankings",
	author    = "Friedman, Milton",
	abstract  = "The Annals of Mathematical Statistics",
	journal   = "The Annals of Mathematical Statistics",
	publisher = "Institute of Mathematical Statistics",
	volume    =  11,
	number    =  1,
	pages     = "86--92",
	month     =  mar,
	year      =  1940,
	url       = "https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-11/issue-1/A-Comparison-of-Alternative-Tests-of-Significance-for-the-Problem/10.1214/aoms/1177731944.full",
	file      = "All Papers/F/Friedman 1940 - A Comparison of Alternative Tests of Significance for the Problem of $m$ Rankings.pdf",
	language  = "en",
	issn      = "0003-4851, 2168-8990",
	doi       = "10.1214/aoms/1177731944"
}

@ARTICLE{Holm1979-rk,
	title     = "A Simple Sequentially Rejective Multiple Test Procedure",
	author    = "Holm, Sture",
	abstract  = "[This paper presents a simple and widely applicable multiple
	test procedure of the sequentially rejective type, i.e.
	hypotheses are rejected one at a time until no further
	rejections can be done. It is shown that the test has a
	prescribed level of significance protection against error of the
	first kind for any combination of true hypotheses. The power
	properties of the test and a number of possible applications are
	also discussed.]",
	journal   = "Scandinavian journal of statistics, theory and applications",
	publisher = "[Board of the Foundation of the Scandinavian Journal of
	Statistics, Wiley]",
	volume    =  6,
	number    =  2,
	pages     = "65--70",
	year      =  1979,
	url       = "http://www.jstor.org/stable/4615733",
	file      = "All Papers/H/Holm 1979 - A Simple Sequentially Rejective Multiple Test Procedure.pdf",
	issn      = "0303-6898, 1467-9469"
}

@ARTICLE{Benavoli2016-mh,
	title    = "Should We Really Use {Post-Hoc} Tests Based on {Mean-Ranks}?",
	author   = "Benavoli, Alessio and Corani, Giorgio and Mangili, Francesca",
	journal  = "Journal of machine learning research: JMLR",
	volume   =  17,
	number   =  5,
	pages    = "1--10",
	year     =  2016,
	url      = "https://jmlr.org/papers/v17/benavoli16a.html",
	file     = "All Papers/B/Benavoli et al. 2016 - Should We Really Use Post-Hoc Tests Based on Mean-Ranks.pdf",
	issn     = "1532-4435, 1533-7928"
}

@ARTICLE{Gneiting2007-la,
	title     = "Probabilistic forecasts, calibration and sharpness",
	author    = "Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E",
	abstract  = "Probabilistic forecasts of continuous variables take the form of
	predictive densities or predictive cumulative distribution
	functions. We propose a diagnostic approach to the evaluation of
	predictive performance that is based on the paradigm of
	maximizing the sharpness of the predictive distributions subject
	to calibration. Calibration refers to the statistical
	consistency between the distributional forecasts and the
	observations and is a joint property of the predictions and the
	events that materialize. Sharpness refers to the concentration
	of the predictive distributions and is a property of the
	forecasts only. A simple theoretical framework allows us to
	distinguish between probabilistic calibration, exceedance
	calibration and marginal calibration. We propose and study tools
	for checking calibration and sharpness, among them the
	probability integral transform histogram, marginal calibration
	plots, the sharpness diagram and proper scoring rules. The
	diagnostic approach is illustrated by an assessment and ranking
	of probabilistic forecasts of wind speed at the Stateline wind
	energy centre in the US Pacific Northwest. In combination with
	cross-validation or in the time series context, our proposal
	provides very general, nonparametric alternatives to the use of
	information criteria for model diagnostics and model selection.",
	journal   = "Journal of the Royal Statistical Society. Series B, Statistical
	methodology",
	publisher = "Wiley",
	volume    =  69,
	number    =  2,
	pages     = "243--268",
	month     =  apr,
	year      =  2007,
	url       = "https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2007.00587.x",
	file      = "All Papers/G/Gneiting et al. 2007 - Probabilistic forecasts, calibration and sharpness.pdf",
	language  = "en",
	issn      = "1369-7412, 1467-9868",
	doi       = "10.1111/j.1467-9868.2007.00587.x"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Muller2019-rh,
	title         = "When does label smoothing help?",
	author        = "M{\"u}ller, Rafael and Kornblith, Simon and Hinton, Geoffrey",
	abstract      = "The generalization and learning speed of a multi-class
	neural network can often be significantly improved by using
	soft targets that are a weighted average of the hard targets
	and the uniform distribution over labels. Smoothing the
	labels in this way prevents the network from becoming
	over-confident and label smoothing has been used in many
	state-of-the-art models, including image classification,
	language translation and speech recognition. Despite its
	widespread use, label smoothing is still poorly understood.
	Here we show empirically that in addition to improving
	generalization, label smoothing improves model calibration
	which can significantly improve beam-search. However, we
	also observe that if a teacher network is trained with label
	smoothing, knowledge distillation into a student network is
	much less effective. To explain these observations, we
	visualize how label smoothing changes the representations
	learned by the penultimate layer of the network. We show
	that label smoothing encourages the representations of
	training examples from the same class to group in tight
	clusters. This results in loss of information in the logits
	about resemblances between instances of different classes,
	which is necessary for distillation, but does not hurt
	generalization or calibration of the model's predictions.",
	month         =  jun,
	year          =  2019,
	url           = "https://proceedings.neurips.cc/paper/2019/hash/f1748d6b0fd9d439f71450117eba2725-Abstract.html",
	file          = "All Papers/M/Müller et al. 2019 - When does label smoothing help.pdf",
	copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
	archivePrefix = "arXiv",
	eprint        = "1906.02629",
	primaryClass  = "cs.LG",
	arxivid       = "1906.02629"
}

@ARTICLE{Platt1999-wb,
	title     = "Probabilistic outputs for support vector machines and
	comparisons to regularized likelihood methods",
	author    = "Platt, John and {Others}",
	abstract  = "The output of a classifier should be a calibrated posterior
	probability to enable post- processing. Standard SVMs do not
	provide such probabilities. One method to create probabilities
	is to di-rectly train a kernel classifier with a logit link
	function and a regularized maximum likelihood score. However,
	training with a maximum likelihood score will produce non-sparse
	kernel machines. Instead, we train an SVM, then train the
	parameters of an additional sigmoid function to map the SVM
	outputs into probabilities. This chapter …",
	journal   = "Advances in large margin classifiers",
	publisher = "Cambridge, MA",
	volume    =  10,
	number    =  3,
	pages     = "61--74",
	year      =  1999,
	url       = "https://www.researchgate.net/file.PostFileLoader.html?id=540479d7d11b8bb1588b459d&assetKey=AS%3A273601008209920%401442242971560",
	file      = "All Papers/P/Platt and Others 1999 - Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.pdf"
}

@ARTICLE{Gneiting2005-gt,
	title   = "Weather Forecasting with Ensemble Methods",
	author  = "Gneiting, Tilmann and Raftery, Adrian E",
	journal = "Science",
	volume  =  310,
	number  =  5746,
	pages   = "248--249",
	year    =  2005,
	url     = "https://www.science.org/doi/abs/10.1126/science.1115255",
	eprint  = "https://www.science.org/doi/pdf/10.1126/science.1115255",
	issn    = "0036-8075",
	doi     = "10.1126/science.1115255"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Brocker2007-ps,
	title     = "Scoring probabilistic forecasts: the importance of being proper",
	author    = "Br{\"o}cker, Jochen and Smith, Leonard A",
	abstract  = "Questions remain regarding how the skill of operational
	probabilistic forecasts is most usefully evaluated or compared,
	even though probability forecasts have been a long-standing aim
	in meteorological forecasting. This paper explains the
	importance of employing proper scores when selecting between the
	various measures of forecast skill. It is demonstrated that only
	proper scores provide internally consistent evaluations of
	probability forecasts, justifying the focus on proper scores
	independent of any attempt to influence the behavior of a
	forecaster. Another property of scores (i.e., locality) is
	discussed. Several scores are examined in this light. There is,
	effectively, only one proper, local score for probability
	forecasts of a continuous variable. It is also noted that
	operational needs of weather forecasts suggest that the current
	concept of a score may be too narrow; a possible generalization
	is motivated and discussed in the context of propriety and
	locality.",
	journal   = "Weather and Forecasting",
	publisher = "American Meteorological Society",
	volume    =  22,
	number    =  2,
	pages     = "382--388",
	month     =  apr,
	year      =  2007,
	url       = "http://eprints.lse.ac.uk/22219/",
	file      = "All Papers/B/Bröcker and Smith 2007 - Scoring probabilistic forecasts - the importance of being proper.pdf",
	language  = "en",
	issn      = "0882-8156",
	doi       = "10.1175/WAF966.1"
}

@UNPUBLISHED{Popordanoska2022-jz,
	title    = "A Consistent and Differentiable Lp Canonical Calibration Error
	Estimator",
	author   = "Popordanoska, Teodora and Sayer, Raphael and Blaschko, Matthew B",
	abstract = "Calibrated probabilistic classifiers are models whose predicted
	probabilities can directly be interpreted as uncertainty
	estimates. It has been shown recently that deep neural networks
	are poorly calibrated and tend to output overconfident
	predictions. As a remedy, we propose a low-bias, trainable
	calibration error estimator based on Dirichlet kernel density
	estimates, which asymptotically converges to the true $L_p$
	calibration error. This novel estimator enables us to tackle the
	strongest notion of multiclass calibration, called canonical (or
	distribution) calibration, while other common calibration methods
	are tractable only for top-label and marginal calibration. The
	computational complexity of our estimator is
	$\mathcal\{O\}(n^2)$, the convergence rate is
	$\mathcal\{O\}(n^\{-1/2\})$, and it is unbiased up to
	$\mathcal\{O\}(n^\{-2\})$, achieved by a geometric series
	debiasing scheme. In practice, this means that the estimator can
	be applied to small subsets of data, enabling efficient
	estimation and mini-batch updates. The proposed method has a
	natural choice of kernel, and can be used to generate consistent
	estimates of other quantities based on conditional expectation,
	such as the sharpness of a probabilistic classifier. Empirical
	results validate the correctness of our estimator, and
	demonstrate its utility in canonical calibration error estimation
	and calibration error regularized risk minimization.",
	month    =  oct,
	year     =  2022,
	url      = "https://openreview.net/pdf?id=HMs5pxZq1If",
	file     = "All Papers/P/Popordanoska et al. 2022 - 2210.07810.pdf;All Papers/P/Popordanoska et al. 2022 - A Consistent and Differentiable Lp Canonical Calibration Error Estimator.pdf"
}

@INPROCEEDINGS{Vovk2020-pg,
	title     = "Conformal calibrators",
	booktitle = "Proceedings of the Ninth Symposium on Conformal and
	Probabilistic Prediction and Applications",
	author    = "Vovk, Vladimir and Petej, Ivan and Toccaceli, Paolo and
	Gammerman, Alexander and Ahlberg, Ernst and Carlsson, Lars",
	editor    = "Gammerman, Alexander and Vovk, Vladimir and Luo, Zhiyuan and
	Smirnov, Evgueni and Cherubin, Giovanni",
	abstract  = "Most existing examples of full conformal predictive systems,
	split conformal predictive systems, and cross-conformal
	predictive systems impose severe restrictions on the adaptation
	of predictive distributions to the test object at hand. In this
	paper we develop split conformal predictive systems that are
	fully adaptive. Our method consists in calibrating existing
	predictive systems; the input predictive system is not supposed
	to satisfy any properties of validity, whereas the output
	predictive system is guaranteed to be calibrated in probability.",
	publisher = "PMLR",
	volume    =  128,
	pages     = "84--99",
	series    = "Proceedings of Machine Learning Research",
	year      =  2020,
	url       = "https://proceedings.mlr.press/v128/vovk20a.html",
	file      = "All Papers/V/Vovk et al. 2020 - Conformal calibrators.pdf"
}

@ARTICLE{Einbinder2022-pm,
	title         = "Training {Uncertainty-Aware} Classifiers with Conformalized
	Deep Learning",
	author        = "Einbinder, Bat-Sheva and Romano, Yaniv and Sesia, Matteo and
	Zhou, Yanfei",
	abstract      = "Deep neural networks are powerful tools to detect hidden
	patterns in data and leverage them to make predictions, but
	they are not designed to understand uncertainty and estimate
	reliable probabilities. In particular, they tend to be
	overconfident. We begin to address this problem in the
	context of multi-class classification by developing a novel
	training algorithm producing models with more dependable
	uncertainty estimates, without sacrificing predictive power.
	The idea is to mitigate overconfidence by minimizing a loss
	function, inspired by advances in conformal inference, that
	quantifies model uncertainty by carefully leveraging
	hold-out data. Experiments with synthetic and real data
	demonstrate this method can lead to smaller conformal
	prediction sets with higher conditional coverage, after
	exact calibration with hold-out data, compared to
	state-of-the-art alternatives.",
	month         =  may,
	year          =  2022,
	url           = "http://arxiv.org/abs/2205.05878",
	file          = "All Papers/E/Einbinder et al. 2022 - Training Uncertainty-Aware Classifiers with Conformalized Deep Learning.pdf",
	archivePrefix = "arXiv",
	eprint        = "2205.05878",
	primaryClass  = "stat.ML",
	arxivid       = "2205.05878"
}

@UNPUBLISHED{Gruber2022-lh,
	title    = "Better Uncertainty Calibration via Proper Scores for
	Classification and Beyond",
	author   = "Gruber, Sebastian Gregor and Buettner, Florian",
	abstract = "With model trustworthiness being crucial for sensitive real-world
	applications, practitioners are putting more and more focus on
	improving the uncertainty calibration of deep neural networks.
	Calibration errors are designed to quantify the reliability of
	probabilistic predictions but their estimators are usually biased
	and inconsistent. In this work, we introduce the framework of
	\textbackslashtextit\{proper calibration errors\}, which relates
	every calibration error to a proper score and provides a
	respective upper bound with optimal estimation properties. This
	relationship can be used to reliably quantify the model
	calibration improvement. We theoretically and empirically
	demonstrate the shortcomings of commonly used estimators compared
	to our approach. Due to the wide applicability of proper scores,
	this gives a natural extension of recalibration beyond
	classification.",
	journal  = "Advances in Neural Information Processing",
	month    =  "31~" # oct,
	year     =  2022,
	url      = "https://openreview.net/pdf?id=PikKk2lF6P",
	file     = "All Papers/G/Gruber and Buettner 2022 - Better Uncertainty Calibration via Proper Scores for Classification and Beyond.pdf"
}

@INPROCEEDINGS{Marx2022-en,
	title     = "Modular Conformal Calibration",
	booktitle = "Proceedings of the 39th International Conference on Machine
	Learning",
	author    = "Marx, Charles and Zhao, Shengjia and Neiswanger, Willie and
	Ermon, Stefano",
	editor    = "Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and
	Szepesvari, Csaba and Niu, Gang and Sabato, Sivan",
	abstract  = "Uncertainty estimates must be calibrated (i.e., accurate) and
	sharp (i.e., informative) in order to be useful. This has
	motivated a variety of methods for recalibration, which use
	held-out data to turn an uncalibrated model into a calibrated
	model. However, the applicability of existing methods is limited
	due to their assumption that the original model is also a
	probabilistic model. We introduce a versatile class of
	algorithms for recalibration in regression that we call modular
	conformal calibration (MCC). This framework allows one to
	transform any regression model into a calibrated probabilistic
	model. The modular design of MCC allows us to make simple
	adjustments to existing algorithms that enable well-behaved
	distribution predictions. We also provide finite-sample
	calibration guarantees for MCC algorithms. Our framework
	recovers isotonic recalibration, conformal calibration, and
	conformal interval prediction, implying that our theoretical
	results apply to those methods as well. Finally, we conduct an
	empirical study of MCC on 17 regression datasets. Our results
	show that new algorithms designed in our framework achieve
	near-perfect calibration and improve sharpness relative to
	existing methods.",
	publisher = "PMLR",
	volume    =  162,
	pages     = "15180--15195",
	series    = "Proceedings of Machine Learning Research",
	year      =  2022,
	url       = "https://proceedings.mlr.press/v162/marx22a.html",
	file      = "All Papers/M/Marx et al. 2022 - Modular Conformal Calibration.pdf"
}

@BOOK{Coles_undated-wq,
	title     = "An Introduction to Statistical Modeling of Extreme Values",
	author    = "Coles, Stuart",
	publisher = "Springer London",
	url       = "https://link.springer.com/book/10.1007/978-1-4471-3675-0",
	doi       = "10.1007/978-1-4471-3675-0"
}

@ARTICLE{Ismail_Fawaz2019-od,
	title    = "Deep learning for time series classification: a review",
	author   = "Ismail Fawaz, Hassan and Forestier, Germain and Weber, Jonathan
	and Idoumghar, Lhassane and Muller, Pierre-Alain",
	abstract = "Time Series Classification (TSC) is an important and challenging
	problem in data mining. With the increase of time series data
	availability, hundreds of TSC algorithms have been proposed.
	Among these methods, only a few have considered Deep Neural
	Networks (DNNs) to perform this task. This is surprising as deep
	learning has seen very successful applications in the last years.
	DNNs have indeed revolutionized the field of computer vision
	especially with the advent of novel deeper architectures such as
	Residual and Convolutional Neural Networks. Apart from images,
	sequential data such as text and audio can also be processed with
	DNNs to reach state-of-the-art performance for document
	classification and speech recognition. In this article, we study
	the current state-of-the-art performance of deep learning
	algorithms for TSC by presenting an empirical study of the most
	recent DNN architectures for TSC. We give an overview of the most
	successful deep learning applications in various time series
	domains under a unified taxonomy of DNNs for TSC. We also provide
	an open source deep learning framework to the TSC community where
	we implemented each of the compared approaches and evaluated them
	on a univariate TSC benchmark (the UCR/UEA archive) and 12
	multivariate time series datasets. By training 8730 deep learning
	models on 97 time series datasets, we propose the most exhaustive
	study of DNNs for TSC to date.",
	journal  = "Data mining and knowledge discovery",
	volume   =  33,
	number   =  4,
	pages    = "917--963",
	month    =  "1~" # jul,
	year     =  2019,
	url      = "https://doi.org/10.1007/s10618-019-00619-1",
	file     = "All Papers/I/Ismail Fawaz et al. 2019 - Deep learning for time series classification - a review.pdf",
	issn     = "1384-5810, 1573-756X",
	doi      = "10.1007/s10618-019-00619-1"
}

@ARTICLE{Paparrizos2022-xd,
	title     = "{TSB-UAD}: an end-to-end benchmark suite for univariate
	time-series anomaly detection",
	author    = "Paparrizos, John and Kang, Yuhao and Boniol, Paul and Tsay, Ruey
	S and Palpanas, Themis and Franklin, Michael J",
	abstract  = "The detection of anomalies in time series has gained ample
	academic and industrial attention. However, no comprehensive
	benchmark exists to evaluate time-series anomaly detection
	methods. It is common to use (i) proprietary or synthetic data,
	often biased to support particular claims; or (ii) a limited
	collection of publicly available datasets. Consequently, we
	often observe methods performing exceptionally well in one
	dataset but surprisingly poorly in another, creating an illusion
	of progress. To address the issues above, we thoroughly studied
	over one hundred papers to identify, collect, process, and
	systematically format datasets proposed in the past decades. We
	summarize our effort in TSB-UAD, a new benchmark to ease the
	evaluation of univariate time-series anomaly detection methods.
	Overall, TSB-UAD contains 13766 time series with labeled
	anomalies spanning different domains with high variability of
	anomaly types, ratios, and sizes. TSB-UAD includes 18 previously
	proposed datasets containing 1980 time series and we contribute
	two collections of datasets. Specifically, we generate 958 time
	series using a principled methodology for transforming 126
	time-series classification datasets into time series with
	labeled anomalies. In addition, we present data transformations
	with which we introduce new anomalies, resulting in 10828 time
	series with varying complexity for anomaly detection. Finally,
	we evaluate 12 representative methods demonstrating that TSB-UAD
	is a robust resource for assessing anomaly detection methods. We
	make our data and code available at www.timeseries.org/TSB-UAD.
	TSB-UAD provides a valuable, reproducible, and frequently
	updated resource to establish a leaderboard of univariate
	time-series anomaly detection methods.",
	journal   = "Proceedings of the VLDB Endowment International Conference on
	Very Large Data Bases",
	publisher = "VLDB Endowment",
	volume    =  15,
	number    =  8,
	pages     = "1697--1711",
	month     =  "22~" # jun,
	year      =  2022,
	url       = "https://doi.org/10.14778/3529337.3529354",
	file      = "All Papers/P/Paparrizos et al. 2022 - TSB-UAD - an end-to-end benchmark suite for univariate time-series anomaly detection.pdf",
	issn      = "2150-8097",
	doi       = "10.14778/3529337.3529354"
}

@ARTICLE{Benavoli2017-mt,
	title    = "Time for a Change: a Tutorial for Comparing Multiple Classifiers
	Through Bayesian Analysis",
	author   = "Benavoli, Alessio and Corani, Giorgio and Dem{\v s}ar, Janez and
	Zaffalon, Marco",
	journal  = "Journal of machine learning research: JMLR",
	volume   =  18,
	number   =  77,
	pages    = "1--36",
	year     =  2017,
	url      = "https://jmlr.org/papers/v18/16-305.html",
	file     = "All Papers/B/Benavoli et al. 2017 - Time for a Change - a Tutorial for Comparing Multiple Classifiers Through Bayesian Analysis.pdf",
	issn     = "1532-4435, 1533-7928"
}

@ARTICLE{Kadra2021-fq,
	title         = "Well-tuned simple nets excel on tabular datasets",
	author        = "Kadra, Arlind and Lindauer, Marius and Hutter, Frank and
	Grabocka, Josif",
	abstract      = "Tabular datasets are the last ``unconquered castle'' for
	deep learning, with traditional ML methods like
	Gradient-Boosted Decision Trees still performing strongly
	even against recent specialized neural architectures. In
	this paper, we hypothesize that the key to boosting the
	performance of neural networks lies in rethinking the joint
	and simultaneous application of a large set of modern
	regularization techniques. As a result, we propose
	regularizing plain Multilayer Perceptron (MLP) networks by
	searching for the optimal combination/cocktail of 13
	regularization techniques for each dataset using a joint
	optimization over the decision on which regularizers to
	apply and their subsidiary hyperparameters. We empirically
	assess the impact of these regularization cocktails for MLPs
	in a large-scale empirical study comprising 40 tabular
	datasets and demonstrate that (i) well-regularized plain
	MLPs significantly outperform recent state-of-the-art
	specialized neural network architectures, and (ii) they even
	outperform strong traditional ML methods, such as XGBoost.",
	pages         = "23928--23941",
	month         =  jun,
	year          =  2021,
	url           = "https://proceedings.neurips.cc/paper/2021/hash/c902b497eb972281fb5b4e206db38ee6-Abstract.html",
	file          = "All Papers/K/Kadra et al. 2021 - Well-tuned simple nets excel on tabular datasets.pdf",
	copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
	archivePrefix = "arXiv",
	eprint        = "2106.11189",
	primaryClass  = "cs.LG",
	arxivid       = "2106.11189"
}

@ARTICLE{Shwartz-Ziv2021-xi,
	title         = "Tabular Data: Deep Learning is Not All You Need",
	author        = "Shwartz-Ziv, Ravid and Armon, Amitai",
	abstract      = "A key element in solving real-life data science problems is
	selecting the types of models to use. Tree ensemble models
	(such as XGBoost) are usually recommended for classification
	and regression problems with tabular data. However, several
	deep learning models for tabular data have recently been
	proposed, claiming to outperform XGBoost for some use cases.
	This paper explores whether these deep models should be a
	recommended option for tabular data by rigorously comparing
	the new deep models to XGBoost on various datasets. In
	addition to systematically comparing their performance, we
	consider the tuning and computation they require. Our study
	shows that XGBoost outperforms these deep models across the
	datasets, including the datasets used in the papers that
	proposed the deep models. We also demonstrate that XGBoost
	requires much less tuning. On the positive side, we show
	that an ensemble of deep models and XGBoost performs better
	on these datasets than XGBoost alone.",
	month         =  "6~" # jun,
	year          =  2021,
	url           = "http://arxiv.org/abs/2106.03253",
	file          = "All Papers/S/Shwartz-Ziv and Armon 2021 - Tabular Data - Deep Learning is Not All You Need.pdf",
	archivePrefix = "arXiv",
	eprint        = "2106.03253",
	primaryClass  = "cs.LG",
	arxivid       = "2106.03253"
}

@ARTICLE{Bouthillier2021-io,
	title         = "Accounting for variance in machine learning benchmarks",
	author        = "Bouthillier, Xavier and Delaunay, Pierre and Bronzi, Mirko
	and Trofimov, Assya and Nichyporuk, Brennan and Szeto,
	Justin and Sepah, Naz and Raff, Edward and Madan, Kanika and
	Voleti, Vikram and Kahou, Samira Ebrahimi and Michalski,
	Vincent and Serdyuk, Dmitriy and Arbel, Tal and Pal, Chris
	and Varoquaux, Ga{\"e}l and Vincent, Pascal",
	abstract      = "Strong empirical evidence that one machine-learning
	algorithm A outperforms another one B ideally calls for
	multiple trials optimizing the learning pipeline over
	sources of variation such as data sampling, data
	augmentation, parameter initialization, and hyperparameters
	choices. This is prohibitively expensive, and corners are
	cut to reach conclusions. We model the whole benchmarking
	process, revealing that variance due to data sampling,
	parameter initialization and hyperparameter choice impact
	markedly the results. We analyze the predominant comparison
	methods used today in the light of this variance. We show a
	counter-intuitive result that adding more sources of
	variation to an imperfect estimator approaches better the
	ideal estimator at a 51 times reduction in compute cost.
	Building on these results, we study the error rate of
	detecting improvements, on five different deep-learning
	tasks/architectures. This study leads us to propose
	recommendations for performance comparisons.",
	month         =  "1~" # mar,
	year          =  2021,
	url           = "https://proceedings.mlsys.org/paper/2021/file/cfecdb276f634854f3ef915e2e980c31-Paper.pdf",
	file          = "All Papers/B/Bouthillier et al. 2021 - Accounting for variance in machine learning benchmarks.pdf",
	copyright     = "http://creativecommons.org/licenses/by/4.0/",
	archivePrefix = "arXiv",
	eprint        = "2103.03098",
	primaryClass  = "cs.LG",
	arxivid       = "2103.03098"
}

@ARTICLE{Rebjock2021-cs,
	title         = "Online false discovery rate control for anomaly detection in
	time series",
	author        = "Rebjock, Quentin and Kurt, Bar{\i}{\c s} and Januschowski,
	Tim and Callot, Laurent",
	abstract      = "This article proposes novel rules for false discovery rate
	control (FDRC) geared towards online anomaly detection in
	time series. Online FDRC rules allow to control the
	properties of a sequence of statistical tests. In the
	context of anomaly detection, the null hypothesis is that an
	observation is normal and the alternative is that it is
	anomalous. FDRC rules allow users to target a lower bound on
	precision in unsupervised settings. The methods proposed in
	this article overcome short-comings of previous FDRC rules
	in the context of anomaly detection, in particular ensuring
	that power remains high even when the alternative is
	exceedingly rare (typical in anomaly detection) and the test
	statistics are serially dependent (typical in time series).
	We show the soundness of these rules in both theory and
	experiments.",
	month         =  "6~" # dec,
	year          =  2021,
	url           = "https://proceedings.neurips.cc/paper/2021/file/def130d0b67eb38b7a8f4e7121ed432c-Paper.pdf",
	file          = "All Papers/R/Rebjock et al. 2021 - Online false discovery rate control for anomaly detection in time series.pdf",
	copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
	archivePrefix = "arXiv",
	eprint        = "2112.03196",
	primaryClass  = "stat.ML",
	arxivid       = "2112.03196"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Garcia2008-jj,
	title    = "An Extension on ``Statistical Comparisons of Classifiers over
	Multiple Data Sets'' for all Pairwise Comparisons",
	author   = "Garc{\'\i}a, Salvador and Herrera, Francisco",
	journal  = "Journal of machine learning research: JMLR",
	volume   =  9,
	number   =  89,
	pages    = "2677--2694",
	year     =  2008,
	url      = "https://www.jmlr.org/papers/v9/garcia08a.html",
	file     = "All Papers/G/García and Herrera 2008 - An Extension on ``Statistical Comparisons of Classifiers over Multiple Data Sets'' for all Pairwise Comparisons.pdf",
	issn     = "1532-4435, 1533-7928"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Demsar2006-ed,
	title     = "Statistical Comparisons of Classifiers over Multiple Data Sets",
	author    = "Dem{\v s}ar, Janez",
	abstract  = "While methods for comparing two learning algorithms on a single
	data set have been scrutinized for quite some time already, the
	issue of statistical tests for comparisons of more algorithms on
	multiple data sets, which is even more essential to typical
	machine learning studies, has been all but ignored. This article
	reviews the current practice and then theoretically and
	empirically examines several suitable tests. Based on that, we
	recommend a set of simple, yet safe and robust non-parametric
	tests for statistical comparisons of classifiers: the Wilcoxon
	signed ranks test for comparison of two classifiers and the
	Friedman test with the corresponding post-hoc tests for
	comparison of more classifiers over multiple data sets. Results
	of the latter can also be neatly presented with the newly
	introduced CD (critical difference) diagrams.",
	journal   = "Journal of machine learning research: JMLR",
	publisher = "JMLR.org",
	volume    =  7,
	pages     = "1--30",
	month     =  dec,
	year      =  2006,
	url       = "https://dl.acm.org/doi/10.5555/1248547.1248548",
	file      = "All Papers/D/Demšar 2006 - Statistical Comparisons of Classifiers over Multiple Data Sets.pdf",
	issn      = "1532-4435"
}

@ARTICLE{Levi2022-lx,
	title       = "Evaluating and Calibrating Uncertainty Prediction in
	Regression Tasks",
	author      = "Levi, Dan and Gispan, Liran and Giladi, Niv and Fetaya, Ethan",
	affiliation = "General Motors Israel, Herzliya 4672515, Israel. Faculty of
	Computer Science, Technion, Haifa 3200003, Israel. Faculty of
	Engineering, Bar-Ilan University, Ramat Gan 5290002, Israel.",
	abstract    = "Predicting not only the target but also an accurate measure of
	uncertainty is important for many machine learning
	applications, and in particular, safety-critical ones. In this
	work, we study the calibration of uncertainty prediction for
	regression tasks which often arise in real-world systems. We
	show that the existing definition for the calibration of
	regression uncertainty has severe limitations in
	distinguishing informative from non-informative uncertainty
	predictions. We propose a new definition that escapes this
	caveat and an evaluation method using a simple histogram-based
	approach. Our method clusters examples with similar
	uncertainty prediction and compares the prediction with the
	empirical uncertainty on these examples. We also propose a
	simple, scaling-based calibration method that preforms as well
	as much more complex ones. We show results on both a
	synthetic, controlled problem and on the object detection
	bounding-box regression task using the COCO and KITTI
	datasets.",
	journal     = "Sensors",
	publisher   = "mdpi.com",
	volume      =  22,
	number      =  15,
	month       =  jul,
	year        =  2022,
	url         = "http://dx.doi.org/10.3390/s22155540",
	file        = "All Papers/L/Levi et al. 2022 - Evaluating and Calibrating Uncertainty Prediction in Regression Tasks.pdf",
	keywords    = "prediction uncertainty; regression",
	language    = "en",
	issn        = "1424-8220",
	pmid        = "35898047",
	doi         = "10.3390/s22155540",
	pmc         = "PMC9330317"
}

@ARTICLE{Pinson2012-ba,
	title     = "Verification of the {ECMWF} ensemble forecasts of wind speed
	against analyses and observations",
	author    = "Pinson, Pierre and Hagedorn, Renate",
	abstract  = "Abstract A framework for the verification of ensemble forecasts
	of near-surface wind speed is described. It is based on existing
	scores and diagnostic tools, though considering observations
	from synoptic stations as reference instead of the analysis.
	This approach is motivated by the idea of having a user-oriented
	view of verification, for instance with the wind power
	applications in mind. The verification framework is specifically
	applied to the case of ECMWF ensemble forecasts and over Europe.
	Dynamic climatologies are derived at the various stations,
	serving as a benchmark. The impact of observational uncertainty
	on scores and diagnostic tools is also considered. The interest
	of this framework is demonstrated from its application to the
	routine evaluation of ensemble forecasts and to the assessment
	of the quality improvements brought in by the recent change in
	horizontal resolution of the ECMWF ensemble prediction system.
	The most important conclusions cover (1) the generally high
	skill of these ensemble forecasts of near-surface wind speed
	when evaluated at synoptic stations, (2) the noteworthy
	improvement of scores brought by the change of horizontal
	resolution, and, (3) the scope for further improvements of
	reliability and skill of wind speed ensemble forecasts by
	appropriate post-processing. Copyright ? 2011 Royal
	Meteorological Society",
	journal   = "Meteorological Applications",
	publisher = "Wiley",
	volume    =  19,
	number    =  4,
	pages     = "484--500",
	month     =  dec,
	year      =  2012,
	url       = "https://onlinelibrary.wiley.com/doi/10.1002/met.283",
	file      = "All Papers/P/Pinson and Hagedorn 2012 - Verification of the ECMWF ensemble forecasts of wind speed against analyses and observations.pdf",
	copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
	language  = "en",
	issn      = "1350-4827, 1469-8080",
	doi       = "10.1002/met.283"
}

@ARTICLE{Dawid1984-gn,
	title     = "Present Position and Potential Developments: Some Personal
	Views: Statistical Theory: The Prequential Approach",
	author    = "Dawid, A P",
	abstract  = "[The prequential approach is founded on the premises that the
	purpose of statistical inference is to make sequential
	probability forecasts for future observations, rather than to
	express information about parameters. Many traditional
	parametric concepts, such as consistency and efficiency, prove
	to have natural counterparts in this formulation, which sheds
	new light on these and suggests fruitful extensions.]",
	journal   = "Journal of the Royal Statistical Society. Series A",
	publisher = "[Royal Statistical Society, Wiley]",
	volume    =  147,
	number    =  2,
	pages     = "278--292",
	year      =  1984,
	url       = "http://www.jstor.org/stable/2981683",
	file      = "All Papers/D/Dawid 1984 - Present Position and Potential Developments - Some Personal Views - Statistical Theory - The Prequential Approach.pdf",
	issn      = "0035-9238",
	doi       = "10.2307/2981683"
}

@ARTICLE{Grinsztajn2022-nu,
	title         = "Why do tree-based models still outperform deep learning on
	tabular data?",
	author        = "Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux,
	Ga{\"e}l",
	abstract      = "While deep learning has enabled tremendous progress on text
	and image datasets, its superiority on tabular data is not
	clear. We contribute extensive benchmarks of standard and
	novel deep learning methods as well as tree-based models
	such as XGBoost and Random Forests, across a large number of
	datasets and hyperparameter combinations. We define a
	standard set of 45 datasets from varied domains with clear
	characteristics of tabular data and a benchmarking
	methodology accounting for both fitting models and finding
	good hyperparameters. Results show that tree-based models
	remain state-of-the-art on medium-sized data ($\sim$10K
	samples) even without accounting for their superior speed.
	To understand this gap, we conduct an empirical
	investigation into the differing inductive biases of
	tree-based models and Neural Networks (NNs). This leads to a
	series of challenges which should guide researchers aiming
	to build tabular-specific NNs: 1. be robust to uninformative
	features, 2. preserve the orientation of the data, and 3. be
	able to easily learn irregular functions. To stimulate
	research on tabular architectures, we contribute a standard
	benchmark and raw data for baselines: every point of a 20
	000 compute hours hyperparameter search for each learner.",
	month         =  jul,
	year          =  2022,
	url           = "http://arxiv.org/abs/2207.08815",
	file          = "All Papers/G/Grinsztajn et al. 2022 - Why do tree-based models still outperform deep learning on tabular data.pdf",
	archivePrefix = "arXiv",
	eprint        = "2207.08815",
	primaryClass  = "cs.LG",
	arxivid       = "2207.08815"
}

@ARTICLE{Gneiting2023-at,
	title     = "Model Diagnostics and Forecast Evaluation for Quantiles",
	author    = "Gneiting, Tilmann and Wolffram, Daniel and Resin, Johannes and
	Kraus, Kristof and Bracher, Johannes and Dimitriadis, Timo and
	Hagenmeyer, Veit and Jordan, Alexander I and Lerch, Sebastian
	and Phipps, Kaleb and Schienle, Melanie",
	abstract  = "Model diagnostics and forecast evaluation are closely related
	tasks, with the former concerning in-sample goodness (or lack)
	of fit and the latter addressing predictive performance
	out-of-sample. We review the ubiquitous setting in which
	forecasts are cast in the form of quantiles or quantile-bounded
	prediction intervals. We distinguish unconditional calibration,
	which corresponds to classical coverage criteria, from the
	stronger notion of conditional calibration, as can be visualized
	in quantile reliability diagrams. Consistent scoring
	functions?including, but not limited to, the widely used
	asymmetric piecewise linear score or pinball loss?provide for
	comparative assessment and ranking, and link to the coefficient
	of determination and skill scores. We illustrate the use of
	these tools on Engel's food expenditure data, the Global Energy
	Forecasting Competition 2014, and the US COVID-19 Forecast Hub.
	Expected final online publication date for the Annual Review of
	Statistics and Its Application, Volume 10 is March 2023. Please
	see http://www.annualreviews.org/page/journal/pubdates for
	revised estimates.",
	journal   = "Annual Review of Statistics and Its Application",
	publisher = "Annual Reviews",
	month     =  "7~" # mar,
	year      =  2023,
	url       = "https://doi.org/10.1146/annurev-statistics-032921-020240",
	issn      = "2326-8298",
	doi       = "10.1146/annurev-statistics-032921-020240"
}

@UNPUBLISHED{Anonymous2022-cb,
	title    = "Calibration for Decision Making via Empirical Risk Minimization",
	author   = "{Anonymous}",
	abstract = "Neural networks for classification can achieve high accuracy but
	their probabilistic predictions may be not well-calibrated, in
	particular overconfident. Different general calibration measures
	and methods were proposed. But how exactly does the calibration
	affect downstream tasks? We derive a new task-specific definition
	of calibration for the problem of statistical decision making
	with a known cost matrix. We then show that so-defined
	calibration can be theoretically rigorously improved by
	minimizing the empirical risk in the adjustment parameters like
	temperature. For the empirical risk minimization, which is not
	differentiable, we propose improvements to and analysis of the
	direct loss minimization approach. Our experiments indicate that
	task-specific calibration can perform better than a generic one.
	But we also carefully investigate weaknesses of the proposed tool
	and issues in the statistical evaluation for problems with highly
	unbalanced decision costs.",
	month    =  "26~" # oct,
	year     =  2022,
	url      = "https://openreview.net/pdf?id=ih3mo7J-vb",
	file     = "All Papers/A/Anonymous 2022 - Calibration for Decision Making via Empirical Risk Minimization.pdf"
}

@ARTICLE{Mitchell2011-gf,
	title     = "Evaluating density forecasts: forecast combinations, model
	mixtures, calibration and sharpness",
	author    = "Mitchell, James and Wallis, Kenneth F",
	abstract  = "Abstract This paper reviews current density forecast evaluation
	procedures, and considers a proposal that such procedures be
	augmented by an assessment of ?sharpness?. This was motivated by
	an example in which some standard evaluation procedures using
	probability integral transforms cannot distinguish the ideal
	forecast from several competing forecasts. We show that this
	example has some unrealistic features from a time series
	forecasting perspective, and so provides insecure foundations
	for the argument that existing calibration procedures are
	inadequate in practice. Our alternative, more realistic example
	shows how relevant statistical methods, including
	information-based methods, provide the required discrimination
	between competing forecasts. We introduce a new test of density
	forecast efficiency. Copyright ? 2010 John Wiley \& Sons, Ltd.",
	journal   = "Journal of Applied Econometrics",
	publisher = "John Wiley \& Sons, Ltd",
	volume    =  26,
	number    =  6,
	pages     = "1023--1040",
	month     =  "1~" # sep,
	year      =  2011,
	url       = "https://doi.org/10.1002/jae.1192",
	file      = "All Papers/M/Mitchell and Wallis 2011 - Evaluating density forecasts - forecast combinations, model mixtures, calibration and sharpness.pdf",
	issn      = "0883-7252",
	doi       = "10.1002/jae.1192"
}

@ARTICLE{Zawadzki2015-ub,
	title    = "Nonparametric Scoring Rules",
	author   = "Zawadzki, Erik and Lahaie, Sebastien",
	journal  = "Proceedings of the AAAI Conference on Artificial Intelligence",
	volume   =  29,
	number   =  1,
	month    =  "4~" # mar,
	year     =  2015,
	url      = "https://ojs.aaai.org/index.php/AAAI/article/view/9696",
	file     = "All Papers/Z/Zawadzki and Lahaie 2015 - Nonparametric Scoring Rules.pdf",
	language = "en",
	issn     = "2374-3468, 2374-3468",
	doi      = "10.1609/aaai.v29i1.9696"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Ramamoorthi2015-rw,
	title     = "On Posterior Concentration in Misspecified Models",
	author    = "Ramamoorthi, R V and Sriram, Karthik and Martin, Ryan",
	abstract  = "We investigate the asymptotic behavior of Bayesian posterior
	distributions under independent and identically distributed
	(i.i.d.) misspecified models. More specifically, we study the
	concentration of the posterior distribution on neighborhoods of
	f⋆, the density that is closest in the Kullback--Leibler sense
	to the true model f0. We note, through examples, the need for
	assumptions beyond the usual Kullback--Leibler support
	assumption. We then investigate consistency with respect to a
	general metric under three assumptions, each based on a notion
	of divergence measure, and then apply these to a weighted
	L1-metric in convex models and non-convex models. Although a few
	results on this topic are available, we believe that these are
	somewhat inaccessible due, in part, to the technicalities and
	the subtle differences compared to the more familiar
	well-specified model case. One of our goals is to make some of
	the available results, especially that of Kleijn and van der
	Vaart (2006), more accessible. Unlike their paper, our approach
	does not require construction of test sequences. We also discuss
	a preliminary extension of the i.i.d. results to the independent
	but not identically distributed (i.n.i.d.) case.",
	journal   = "Bayesian Analysis",
	publisher = "International Society for Bayesian Analysis",
	volume    =  10,
	number    =  4,
	pages     = "759--789",
	month     =  dec,
	year      =  2015,
	url       = "https://projecteuclid.org/journals/bayesian-analysis/volume-10/issue-4/On-Posterior-Concentration-in-Misspecified-Models/10.1214/15-BA941.full",
	file      = "All Papers/R/Ramamoorthi et al. 2015 - On Posterior Concentration in Misspecified Models.pdf",
	keywords  = "62C10; 62C10; Bayesian; consistency; Kullback--Leibler;
	misspecified;",
	language  = "en",
	issn      = "1936-0975, 1931-6690",
	doi       = "10.1214/15-BA941"
}

@ARTICLE{White1982-co,
	title     = "Maximum Likelihood Estimation of Misspecified Models",
	author    = "White, Halbert",
	abstract  = "[This paper examines the consequences and detection of model
	misspecification when using maximum likelihood techniques for
	estimation and inference. The quasi-maximum likelihood estimator
	(OMLE) converges to a well defined limit, and may or may not be
	consistent for particular parameters of interest. Standard tests
	(Wald, Lagrange Multiplier, or Likelihood Ratio) are invalid in
	the presence of misspecification, but more general statistics
	are given which allow inferences to be drawn robustly. The
	properties of the QMLE and the information matrix are exploited
	to yield several useful tests for model misspecification.]",
	journal   = "Econometrica: journal of the Econometric Society",
	publisher = "[Wiley, Econometric Society]",
	volume    =  50,
	number    =  1,
	pages     = "1--25",
	year      =  1982,
	url       = "http://www.jstor.org/stable/1912526",
	file      = "All Papers/W/White 1982 - Maximum Likelihood Estimation of Misspecified Models.pdf",
	issn      = "0012-9682, 1468-0262",
	doi       = "10.2307/1912526"
}

@ARTICLE{Grimit2006-dg,
	title     = "The continuous ranked probability score for circular variables
	and its application to mesoscale forecast ensemble verification",
	author    = "Grimit, E P and Gneiting, T and Berrocal, V J and Johnson, N A",
	abstract  = "Abstract An analogue of the linear continuous ranked probability
	score is introduced that applies to probabilistic forecasts of
	circular quantities, such as wind direction. This scoring rule
	is proper and thereby discourages hedging. The circular
	continuous ranked probability score reduces to angular distance
	when the forecast is deterministic, just as the linear
	continuous ranked probability score generalizes the absolute
	error. Furthermore, the circular continuous ranked probability
	score provides a direct way of comparing deterministic
	forecasts, discrete forecast ensembles, and post-processed
	forecast ensembles that can take the form of circular
	probability density functions. The circular continuous ranked
	probability score is used in this study to compare predictions
	of 10 m wind direction for 361 cases of mesoscale, short-range
	ensemble forecasts over the North American Pacific Northwest.
	Simple, calibrated probability forecasts based on the ensemble
	mean and its forecast error history over the period outperform
	probability forecasts constructed directly from the ensemble
	sample statistics. These results suggest that short-term
	forecast uncertainty is not yet well predicted at mesoscale
	resolutions near the surface, despite the inclusion of
	multi-scheme physics diversity and surface boundary parameter
	perturbations in the mesoscale ensemble design. Copyright ? 2006
	Royal Meteorological Society",
	journal   = "Quarterly Journal of the Royal Meteorological Society",
	publisher = "Wiley",
	volume    =  132,
	number    = "621C",
	pages     = "2925--2942",
	month     =  oct,
	year      =  2006,
	url       = "http://doi.wiley.com/10.1256/qj.05.235",
	file      = "All Papers/G/Grimit et al. 2006 - The continuous ranked probability score for circula ... ables and its application to mesoscale forecast ensemble verification.pdf",
	language  = "en",
	issn      = "0035-9009, 1477-870X",
	doi       = "10.1256/qj.05.235"
}

@PHDTHESIS{Jordan2016-fd,
	title    = "Facets of forecast evaluation",
	author   = "Jordan, Alexander",
	year     =  2016,
	url      = "http://dx.doi.org/10.5445/IR/1000063629",
	file     = "All Papers/J/Jordan 2016 - Facets of forecast evaluation.pdf",
	school   = "Dissertation, Karlsruhe, Karlsruher Institut f{\"u}r Technologie
	(KIT), 2016",
	keywords = "Theory",
	doi      = "10.5445/IR/1000063629"
}

@INPROCEEDINGS{Teye2018-zo,
	title     = "{B}ayesian Uncertainty Estimation for Batch Normalized Deep
	Networks",
	booktitle = "Proceedings of the 35th International Conference on Machine
	Learning",
	author    = "Teye, Mattias and Azizpour, Hossein and Smith, Kevin",
	editor    = "Dy, Jennifer and Krause, Andreas",
	abstract  = "We show that training a deep network using batch normalization
	is equivalent to approximate inference in Bayesian models. We
	further demonstrate that this finding allows us to make
	meaningful estimates of the model uncertainty using conventional
	architectures, without modifications to the network or the
	training procedure. Our approach is thoroughly validated by
	measuring the quality of uncertainty in a series of empirical
	experiments on different tasks. It outperforms baselines with
	strong statistical significance, and displays competitive
	performance with recent Bayesian approaches.",
	publisher = "PMLR",
	volume    =  80,
	pages     = "4907--4916",
	series    = "Proceedings of Machine Learning Research",
	year      =  2018,
	url       = "https://proceedings.mlr.press/v80/teye18a.html",
	file      = "All Papers/T/Teye et al. 2018 - Bayesian Uncertainty Estimation for Batch Normalized Deep Networks.pdf"
}

@MISC{Sahoo_undated-hf,
	title        = "Reliable decisions with threshold calibration",
	author       = "Sahoo, Roshni and Zhao, Shengjia and Chen, Alyssa and Ermon,
	Stefano",
	abstract     = "Decision makers rely on probabilistic forecasts to predict
	the loss of different decision rules before deployment. When
	the forecasted probabilities match the true frequencies,
	predicted losses will be accurate. Although perfect forecasts
	are typically impossible, probabilities can be calibrated to
	match the true frequencies on average. However, we find that
	this average notion of calibration, which is typically used
	in practice, does not necessarily guarantee accurate decision
	loss prediction. Specifically in the regression setting, the
	loss of threshold decisions, which are decisions based on
	whether the forecasted outcome falls above or below a cutoff,
	might not be predicted accurately. We propose a stronger
	notion of calibration called threshold calibration, which is
	exactly the condition required to ensure that decision loss
	is predicted accurately for threshold decisions. We provide
	an efficient algorithm which takes an uncalibrated forecaster
	as input and provably outputs a threshold-calibrated
	forecaster. Our procedure allows downstream decision makers
	to confidently estimate the loss of any threshold decision
	under any threshold loss function. Empirically, threshold
	calibration improves decision loss prediction without
	compromising on the quality of the decisions in two
	real-world settings: hospital scheduling decisions and
	resource allocation decisions.",
	url          = "https://proceedings.neurips.cc/paper/2021/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf",
	howpublished = "\url{https://proceedings.neurips.cc/paper/2021/file/0e65972dce68dad4d52d063967f0a705-Paper.pdf}",
	note         = "Accessed: 2022-10-17",
	file         = "All Papers/S/Sahoo et al. - Reliable decisions with threshold calibration.pdf;All Papers/S/Sahoo et al. - Reliable decisions with threshold calibration.pdf"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Ruschendorf1993-nl,
	title    = "On regression representations of stochastic processes",
	author   = "R{\"u}schendorf, Ludger and de Valk, Vincent",
	abstract = "We construct a.s. nonlinear regression representations of general
	stochastic processes (Xn)nϵN. As a consequence we obtain in
	particular special regression representations of Markov chains
	and of certain m-dependent sequences. For m-dependent sequences
	we obtain a constructive method to check, whether these sequences
	have a monotone (m+1)-block factor representation.",
	journal  = "Stochastic Processes and their Applications",
	volume   =  46,
	number   =  2,
	pages    = "183--198",
	month    =  "1~" # jun,
	year     =  1993,
	url      = "https://www.sciencedirect.com/science/article/pii/030441499390001K",
	file     = "All Papers/R/Rüschendorf and de Valk 1993 - On regression representations of stochastic processes.pdf",
	keywords = "representation as function of i.i.d. sequences ∗ generalized
	two-block factor ∗ -dependence ∗ Markov regression ∗ Markov chain",
	issn     = "0304-4149",
	doi      = "10.1016/0304-4149(93)90001-K"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Brocker2009-jm,
	title     = "Reliability, sufficiency, and the decomposition of proper scores",
	author    = "Br{\"o}cker, Jochen",
	abstract  = "Scoring rules are an important tool for evaluating the
	performance of probabilistic forecasting schemes. A scoring rule
	is called strictly proper if its expectation is optimal if and
	only if the forecast probability represents the true
	distribution of the target. In the binary case, strictly proper
	scoring rules allow for a decomposition into terms related to
	the resolution and the reliability of a forecast. This fact is
	particularly well known for the Brier Score. In this article,
	this result is extended to forecasts for finite-valued targets.
	Both resolution and reliability are shown to have a positive
	effect on the score. It is demonstrated that resolution and
	reliability are directly related to forecast attributes that are
	desirable on grounds independent of the notion of scores. This
	finding can be considered an epistemological justification of
	measuring forecast quality by proper scoring rules. A link is
	provided to the original work of DeGroot and Fienberg, extending
	their concepts of sufficiency and refinement. The relation to
	the conjectured sharpness principle of Gneiting, et al., is
	elucidated.",
	journal   = "Quarterly Journal of the Royal Meteorological Society",
	publisher = "Wiley",
	volume    =  135,
	number    =  643,
	pages     = "1512--1519",
	month     =  jul,
	year      =  2009,
	url       = "https://onlinelibrary.wiley.com/doi/10.1002/qj.456",
	file      = "All Papers/B/Bröcker 2009 - Reliability, sufficiency, and the decomposition of proper scores.pdf",
	language  = "en",
	issn      = "0035-9009, 1477-870X",
	doi       = "10.1002/qj.456"
}

@ARTICLE{Pohle2020-fm,
	title         = "The Murphy Decomposition and the {Calibration-Resolution}
	Principle: A New Perspective on Forecast Evaluation",
	author        = "Pohle, Marc-Oliver",
	abstract      = "I provide a unifying perspective on forecast evaluation,
	characterizing accurate forecasts of all types, from simple
	point to complete probabilistic forecasts, in terms of two
	fundamental underlying properties, autocalibration and
	resolution, which can be interpreted as describing a lack of
	systematic mistakes and a high information content. This
	``calibration-resolution principle'' gives a new insight
	into the nature of forecasting and generalizes the famous
	sharpness principle by Gneiting et al. (2007) from
	probabilistic to all types of forecasts. It amongst others
	exposes the shortcomings of several widely used forecast
	evaluation methods. The principle is based on a fully
	general version of the Murphy decomposition of loss
	functions, which I provide. Special cases of this
	decomposition are well-known and widely used in meteorology.
	Besides using the decomposition in this new theoretical way,
	after having introduced it and the underlying properties in
	a proper theoretical framework, accompanied by an
	illustrative example, I also employ it in its classical
	sense as a forecast evaluation method as the meteorologists
	do: As such, it unveils the driving forces behind forecast
	errors and complements classical forecast evaluation
	methods. I discuss estimation of the decomposition via
	kernel regression and then apply it to popular economic
	forecasts. Analysis of mean forecasts from the US Survey of
	Professional Forecasters and quantile forecasts derived from
	Bank of England fan charts indeed yield interesting new
	insights and highlight the potential of the method.",
	month         =  "4~" # may,
	year          =  2020,
	url           = "http://arxiv.org/abs/2005.01835",
	file          = "All Papers/P/Pohle 2020 - The Murphy Decomposition and the Calibration-Resolution Principle - A New Perspective on Forecast Evaluation.pdf",
	archivePrefix = "arXiv",
	eprint        = "2005.01835",
	primaryClass  = "stat.ME",
	arxivid       = "2005.01835"
}

@INPROCEEDINGS{Kull2015-dh,
	title     = "Novel Decompositions of Proper Scoring Rules for Classification:
	Score Adjustment as Precursor to Calibration",
	booktitle = "Machine Learning and Knowledge Discovery in Databases",
	author    = "Kull, Meelis and Flach, Peter",
	abstract  = "There are several reasons to evaluate a multi-class classifier
	on other measures than just error rate. Perhaps most
	importantly, there can be uncertainty about the exact context of
	classifier deployment, requiring the classifier to perform well
	with respect to a variety of contexts. This is commonly achieved
	by creating a scoring classifier which outputs posterior class
	probability estimates. Proper scoring rules are loss evaluation
	measures of scoring classifiers which are minimised at the true
	posterior probabilities. The well-known decomposition of the
	proper scoring rules into calibration loss and refinement loss
	has facilitated the development of methods to reduce these
	losses, thus leading to better classifiers. We propose multiple
	novel decompositions including one with four terms: adjustment
	loss, post-adjustment calibration loss, grouping loss and
	irreducible loss. The separation of adjustment loss from
	calibration loss requires extra assumptions which we prove to be
	satisfied for the most frequently used proper scoring rules:
	Brier score and log-loss. We propose algorithms to perform
	adjustment as a simpler alternative to calibration.",
	publisher = "Springer International Publishing",
	pages     = "68--85",
	year      =  2015,
	url       = "http://dx.doi.org/10.1007/978-3-319-23528-8_5",
	file      = "All Papers/K/Kull and Flach 2015 - Novel Decompositions of Proper Scoring Rules for Classification - Score Adjustment as Precursor to Calibration.pdf",
	doi       = "10.1007/978-3-319-23528-8\_5"
}

@ARTICLE{Karandikar2021-ml,
	title         = "Soft Calibration Objectives for Neural Networks",
	author        = "Karandikar, Archit and Cain, Nicholas and Tran, Dustin and
	Lakshminarayanan, Balaji and Shlens, Jonathon and Mozer,
	Michael C and Roelofs, Becca",
	abstract      = "Optimal decision making requires that classifiers produce
	uncertainty estimates consistent with their empirical
	accuracy. However, deep neural networks are often under- or
	over-confident in their predictions. Consequently, methods
	have been developed to improve the calibration of their
	predictive uncertainty both during training and post-hoc. In
	this work, we propose differentiable losses to improve
	calibration based on a soft (continuous) version of the
	binning operation underlying popular calibration-error
	estimators. When incorporated into training, these soft
	calibration losses achieve state-of-the-art single-model ECE
	across multiple datasets with less than 1\% decrease in
	accuracy. For instance, we observe an 82\% reduction in ECE
	(70\% relative to the post-hoc rescaled ECE) in exchange for
	a 0.7\% relative decrease in accuracy relative to the cross
	entropy baseline on CIFAR-100. When incorporated
	post-training, the soft-binning-based calibration error
	objective improves upon temperature scaling, a popular
	recalibration method. Overall, experiments across losses and
	datasets demonstrate that using calibration-sensitive
	procedures yield better uncertainty estimates under dataset
	shift than the standard practice of using a cross entropy
	loss and post-hoc recalibration methods.",
	month         =  jul,
	year          =  2021,
	url           = "http://arxiv.org/abs/2108.00106",
	file          = "All Papers/K/Karandikar et al. 2021 - Soft Calibration Objectives for Neural Networks.pdf",
	archivePrefix = "arXiv",
	eprint        = "2108.00106",
	primaryClass  = "cs.LG",
	arxivid       = "2108.00106"
}

@MISC{Ferro_undated-jf,
	title        = "Measuring forecast calibration",
	author       = "Ferro, Chris and Mitchell, Keith and Bashaykh, Heba",
	url          = "https://empslocal.ex.ac.uk/people/staff/ferro/Presentations/FerroVALPRED2020.pdf",
	howpublished = "\url{https://empslocal.ex.ac.uk/people/staff/ferro/Presentations/FerroVALPRED2020.pdf}",
	note         = "Accessed: 2022-10-7",
	file         = "All Papers/F/Ferro et al. - Measuring forecast calibration.pdf"
}

@INCOLLECTION{Kohonen2006-vs,
	title     = "Lessons learned in the challenge: Making predictions and scoring
	them",
	booktitle = "Machine Learning Challenges. Evaluating Predictive Uncertainty,
	Visual Object Classification, and Recognising Tectual Entailment",
	author    = "Kohonen, Jukka and Suomela, Jukka",
	publisher = "Springer Berlin Heidelberg",
	volume    =  95,
	pages     = "95--116",
	series    = "Lecture notes in computer science",
	year      =  2006,
	url       = "https://jukkasuomela.fi/doc/making-predictions.pdf",
	file      = "All Papers/K/Kohonen and Suomela 2006 - Lessons learned in the challenge - Making predictions and scoring them.pdf",
	address   = "Berlin, Heidelberg",
	issn      = "0302-9743, 1611-3349",
	isbn      = "9783540334279, 9783540334286",
	doi       = "10.1007/11736790\_7"
}

@TECHREPORT{Tsyplakov2013-mm,
	title     = "Evaluation of Probabilistic Forecasts: Proper Scoring Rules and
	Moments",
	author    = "Tsyplakov, Alexander",
	abstract  = "Downloadable! The paper provides an overview of probabilistic
	forecasting and discusses a theoretical framework for evaluation
	of probabilistic forecasts which is based on proper scoring
	rules and moments. An artificial example of predicting
	second-order autoregression and an example of predicting the
	RTSI stock index are used as illustrations.",
	publisher = "University Library of Munich, Germany",
	number    =  45186,
	month     =  mar,
	year      =  2013,
	url       = "https://ideas.repec.org/p/pra/mprapa/45186.html",
	file      = "All Papers/T/Tsyplakov 2013 - Evaluation of Probabilistic Forecasts - Proper Scoring Rules and Moments.pdf",
	keywords  = "probabilistic forecast; forecast calibration; probability
	integral transform; scoring rule; moment condition"
}

@ARTICLE{Gneiting2021-vu,
	title         = "Regression Diagnostics meets Forecast Evaluation:
	Conditional Calibration, Reliability Diagrams, and
	Coefficient of Determination",
	author        = "Gneiting, Tilmann and Resin, Johannes",
	abstract      = "Model diagnostics and forecast evaluation are two sides of
	the same coin. A common principle is that fitted or
	predicted distributions ought to be calibrated or reliable,
	ideally in the sense of auto-calibration, where the outcome
	is a random draw from the posited distribution. For binary
	responses, this is the universal concept of reliability. For
	real-valued outcomes, a general theory of calibration has
	been elusive, despite a recent surge of interest in
	distributional regression and machine learning. We develop a
	framework rooted in probability theory, which gives rise to
	hierarchies of calibration, and applies to both predictive
	distributions and stand-alone point forecasts. In a
	nutshell, a prediction - distributional or single-valued -
	is conditionally T-calibrated if it can be taken at face
	value in terms of the functional T. Whenever T is defined
	via an identification function - as in the cases of
	threshold (non) exceedance probabilities, quantiles,
	expectiles, and moments - auto-calibration implies
	T-calibration. We introduce population versions of
	T-reliability diagrams and revisit a score decomposition
	into measures of miscalibration (MCB), discrimination (DSC),
	and uncertainty (UNC). In empirical settings, stable and
	efficient estimators of T-reliability diagrams and score
	components arise via nonparametric isotonic regression and
	the pool-adjacent-violators algorithm. For in-sample model
	diagnostics, we propose a universal coefficient of
	determination, $$\textbackslashtext\{R\}^\textbackslashast =
	\textbackslashfrac\{\textbackslashtext\{DSC\}-\textbackslashtext\{MCB\}\}\{\textbackslashtext\{UNC\}\},$$
	that nests and reinterprets the classical $\text\{R\}^2$ in
	least squares (mean) regression and its natural analogue
	$\text\{R\}^1$ in quantile regression, yet applies to
	T-regression in general, with MCB $\geq 0$, DSC $\geq 0$,
	and $\text\{R\}^\ast \in [0,1]$ under modest conditions.",
	month         =  aug,
	year          =  2021,
	url           = "http://arxiv.org/abs/2108.03210",
	file          = "All Papers/G/Gneiting and Resin 2021 - Regression Diagnostics meets Forecast Evaluati ... l Calibration, Reliability Diagrams, and Coefficient of Determination.pdf",
	archivePrefix = "arXiv",
	eprint        = "2108.03210",
	primaryClass  = "stat.ME",
	arxivid       = "2108.03210"
}

@UNPUBLISHED{Lin2022-ba,
	title    = "Conformal Prediction Intervals with Temporal Dependence",
	author   = "Lin, Zhen and Trivedi, Shubhendu and Sun, Jimeng",
	month    =  "7~" # sep,
	year     =  2022,
	url      = "https://openreview.net/pdf?id=8QoxXTDcsH",
	file     = "All Papers/L/Lin et al. 2022 - 2205.12940.pdf;All Papers/L/Lin et al. 2022 - Conformal Prediction Intervals with Temporal Dependence.pdf"
}

@ARTICLE{Martin2022-nz,
	title    = "Optimal probabilistic forecasts: When do they work?",
	author   = "Martin, Gael M and Loaiza-Maya, Rub{\'e}n and Maneesoonthorn,
	Worapree and Frazier, David T and Ram{\'\i}rez-Hassan, Andr{\'e}s",
	abstract = "Proper scoring rules are used to assess the out-of-sample
	accuracy of probabilistic forecasts, with different scoring rules
	rewarding distinct aspects of forecast performance. Herein, we
	re-investigate the practice of using proper scoring rules to
	produce probabilistic forecasts that are `optimal' according to a
	given score and assess when their out-of-sample accuracy is
	superior to alternative forecasts, according to that score.
	Particular attention is paid to relative predictive performance
	under misspecification of the predictive model. Using numerical
	illustrations, we document several novel findings within this
	paradigm that highlight the important interplay between the true
	data generating process, the assumed predictive model and the
	scoring rule. Notably, we show that only when a predictive model
	is sufficiently compatible with the true process to allow a
	particular score criterion to reward what it is designed to
	reward, will this approach to forecasting reap benefits. Subject
	to this compatibility, however, the superiority of the optimal
	forecast will be greater, the greater is the degree of
	misspecification. We explore these issues under a range of
	different scenarios and using both artificially simulated and
	empirical data.",
	journal  = "International journal of forecasting",
	volume   =  38,
	number   =  1,
	pages    = "384--406",
	month    =  "1~" # jan,
	year     =  2022,
	url      = "https://www.sciencedirect.com/science/article/pii/S0169207021000807",
	file     = "All Papers/M/Martin et al. 2022 - Optimal probabilistic forecasts - When do they work.pdf",
	keywords = "Linear predictive pools; Optimal predictions; Predictive
	distributions; Proper scoring rules; Stochastic volatility with
	jumps; Testing equal predictive ability;Theory",
	issn     = "0169-2070",
	doi      = "10.1016/j.ijforecast.2021.05.008"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Hora2015-pw,
	title    = "Calibration, sharpness and the weighting of experts in a linear
	opinion pool",
	author   = "Hora, Stephen C and Karde{\c s}, Erim",
	abstract = "Linear opinion pools are the most common form of aggregating the
	probabilistic judgments of multiple experts. Here, the
	performance of such an aggregation is examined in terms of the
	calibration and sharpness of the component judgments. The
	performance is measured through the average quadratic score of
	the aggregate. Trade-offs between calibration and sharpness are
	examined and an expression for the optimal weighting of two
	dependent experts in a linear combination is given. Circumstances
	where one expert would be disqualified are investigated. Optimal
	weights for the multiple, dependent experts are found through a
	concave quadratic program.",
	journal  = "Annals of Operations Research",
	volume   =  229,
	number   =  1,
	pages    = "429--450",
	month    =  "1~" # jun,
	year     =  2015,
	url      = "https://doi.org/10.1007/s10479-015-1846-0",
	file     = "All Papers/H/Hora and Kardeş 2015 - Calibration, sharpness and the weighting of experts in a linear opinion pool.pdf",
	keywords = "Theory",
	issn     = "0254-5330, 1572-9338",
	doi      = "10.1007/s10479-015-1846-0"
}

@ARTICLE{Winkler1996-px,
	title    = "Scoring rules and the evaluation of probabilities",
	author   = "Winkler, R L and Mu{\~n}oz, Javier and Cervera, Jos{\'e} L and
	Bernardo, Jos{\'e} M and Blattenberger, Gail and Kadane, Joseph B
	and Lindley, Dennis V and Murphy, Allan H and Oliver, Robert M
	and R{\'\i}os-Insua, David",
	abstract = "In Bayesian inference and decision analysis, inferences and
	predictions are inherently probabilistic in nature. Scoring
	rules, which involve the computation of a score based on
	probability forecasts and what actually occurs, can be used to
	evaluate probabilities and to provide appropriate incentives for
	``good'' probabilities. This paper review scoring rules and some
	related measures for evaluating probabilities, including
	decompositions of scoring rules and attributes of ``goodness'' of
	probabilites, comparability of scores, and the design of scoring
	rules for specific inferential and decision-making problems",
	journal  = "Test",
	volume   =  5,
	number   =  1,
	pages    = "1--60",
	month    =  "1~" # jun,
	year     =  1996,
	url      = "https://doi.org/10.1007/BF02562681",
	file     = "All Papers/W/Winkler et al. 1996 - Scoring rules and the evaluation of probabilities.pdf",
	keywords = "Theory",
	issn     = "1133-0686, 1863-8260",
	doi      = "10.1007/BF02562681"
}

@ARTICLE{Luo2022-lg,
	title         = "Understanding Diffusion Models: A Unified Perspective",
	author        = "Luo, Calvin",
	abstract      = "Diffusion models have shown incredible capabilities as
	generative models; indeed, they power the current
	state-of-the-art models on text-conditioned image generation
	such as Imagen and DALL-E 2. In this work we review,
	demystify, and unify the understanding of diffusion models
	across both variational and score-based perspectives. We
	first derive Variational Diffusion Models (VDM) as a special
	case of a Markovian Hierarchical Variational Autoencoder,
	where three key assumptions enable tractable computation and
	scalable optimization of the ELBO. We then prove that
	optimizing a VDM boils down to learning a neural network to
	predict one of three potential objectives: the original
	source input from any arbitrary noisification of it, the
	original source noise from any arbitrarily noisified input,
	or the score function of a noisified input at any arbitrary
	noise level. We then dive deeper into what it means to learn
	the score function, and connect the variational perspective
	of a diffusion model explicitly with the Score-based
	Generative Modeling perspective through Tweedie's Formula.
	Lastly, we cover how to learn a conditional distribution
	using diffusion models via guidance.",
	month         =  "25~" # aug,
	year          =  2022,
	url           = "http://arxiv.org/abs/2208.11970",
	file          = "All Papers/L/Luo 2022 - Understanding Diffusion Models - A Unified Perspective.pdf",
	archivePrefix = "arXiv",
	eprint        = "2208.11970",
	primaryClass  = "cs.LG",
	arxivid       = "2208.11970"
}

@INPROCEEDINGS{Ovadia2019-xo,
	title     = "Can you trust your model's uncertainty? Evaluating predictive
	uncertainty under dataset shift",
	booktitle = "Advances in Neural Information Processing Systems",
	author    = "Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary
	and Sculley, D and Nowozin, Sebastian and Dillon, Joshua and
	Lakshminarayanan, Balaji and Snoek, Jasper",
	editor    = "Wallach, H and Larochelle, H and Beygelzimer, A and
	d'Alch{\'e}-Buc, F and Fox, E and Garnett, R",
	publisher = "Curran Associates, Inc.",
	volume    =  32,
	year      =  2019,
	url       = "https://proceedings.neurips.cc/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf",
	file      = "All Papers/O/Ovadia et al. 2019 - Can you trust your model's uncertainty - Evaluating predictive uncertainty under dataset shift.pdf"
}

@ARTICLE{Gebetsberger2018-vp,
	title     = "Estimation Methods for Nonhomogeneous Regression Models: Minimum
	Continuous Ranked Probability Score versus Maximum Likelihood",
	author    = "Gebetsberger, Manuel and Messner, Jakob W and Mayr, Georg J and
	Zeileis, Achim",
	abstract  = "Abstract Nonhomogeneous regression models are widely used to
	statistically postprocess numerical ensemble weather prediction
	models. Such regression models are capable of forecasting full
	probability distributions and correcting for ensemble errors in
	the mean and variance. To estimate the corresponding regression
	coefficients, minimization of the continuous ranked probability
	score (CRPS) has widely been used in meteorological
	postprocessing studies and has often been found to yield more
	calibrated forecasts compared to maximum likelihood estimation.
	From a theoretical perspective, both estimators are consistent
	and should lead to similar results, provided the correct
	distribution assumption about empirical data. Differences
	between the estimated values indicate a wrong specification of
	the regression model. This study compares the two estimators for
	probabilistic temperature forecasting with nonhomogeneous
	regression, where results show discrepancies for the classical
	Gaussian assumption. The heavy-tailed logistic and Student's t
	distributions can improve forecast performance in terms of
	sharpness and calibration, and lead to only minor differences
	between the estimators employed. Finally, a simulation study
	confirms the importance of appropriate distribution assumptions
	and shows that for a correctly specified model the maximum
	likelihood estimator is slightly more efficient than the CRPS
	estimator.",
	journal   = "Monthly Weather Review",
	publisher = "American Meteorological Society",
	volume    =  146,
	number    =  12,
	pages     = "4323--4338",
	month     =  "1~" # dec,
	year      =  2018,
	url       = "https://journals.ametsoc.org/view/journals/mwre/146/12/mwr-d-17-0364.1.xml",
	file      = "All Papers/G/Gebetsberger et al. 2018 - Estimation Methods for Nonhomogeneous Regress ... Minimum Continuous Ranked Probability Score versus Maximum Likelihood.pdf",
	language  = "en",
	issn      = "0027-0644, 1520-0493",
	doi       = "10.1175/MWR-D-17-0364.1"
}

@INPROCEEDINGS{Si2022-br,
	title     = "Autoregressive Quantile Flows for Predictive Uncertainty
	Estimation",
	booktitle = "International Conference on Learning Representations",
	author    = "Si, Phillip and Kuleshov, Volodymyr and Bishop, Allan",
	year      =  2022,
	url       = "https://openreview.net/forum?id=z1-I6rOKv1S",
	file      = "All Papers/S/Si et al. 2022 - Autoregressive Quantile Flows for Predictive Uncertainty Estimation.pdf"
}

@INPROCEEDINGS{Utpala2020-qr,
	title     = "Temperature Scaling for Quantile Calibration",
	booktitle = "{Proceedings on ``I Can't Believe It's Not Better!'' at
	{NeurIPS} Workshops}",
	author    = "Utpala, Saiteja and Rai, Piyush",
	year      =  2020,
	url       = "https://openreview.net/forum?id=f61mn-fZnPn",
	file      = "All Papers/U/Utpala and Rai 2020 - Temperature Scaling for Quantile Calibration.pdf"
}

@INPROCEEDINGS{Zhang2018-cw,
	title     = "mixup: Beyond Empirical Risk Minimization",
	booktitle = "International Conference on Learning Representations",
	author    = "Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and
	Lopez-Paz, David",
	year      =  2018,
	url       = "https://openreview.net/forum?id=r1Ddp1-Rb",
	file      = "All Papers/Z/Zhang et al. 2018 - mixup - Beyond Empirical Risk Minimization.pdf"
}

@ARTICLE{Izbicki2022-ru,
	title    = "{CD-split} and {HPD-split}: Efficient Conformal Regions in High
	Dimensions",
	author   = "Izbicki, Rafael and Shimizu, Gilson and Stern, Rafael B",
	journal  = "Journal of machine learning research: JMLR",
	volume   =  23,
	number   =  87,
	pages    = "1--32",
	year     =  2022,
	url      = "https://www.jmlr.org/papers/v23/20-797.html",
	file     = "All Papers/I/Izbicki et al. 2022 - CD-split and HPD-split - Efficient Conformal Regions in High Dimensions.pdf",
	issn     = "1532-4435, 1533-7928"
}

@ARTICLE{Jordan2019-fh,
	title    = "Evaluating Probabilistic Forecasts with scoringRules",
	author   = "Jordan, Alexander and Kr{\"u}ger, Fabian and Lerch, Sebastian",
	abstract = "Probabilistic forecasts in the form of probability distributions
	over future events have become popular in several fields
	including meteorology, hydrology, economics, and demography. In
	typical applications, many alternative statistical models and
	data sources can be used to produce probabilistic forecasts.
	Hence, evaluating and selecting among competing methods is an
	important task. The scoringRules package for R provides
	functionality for comparative evaluation of probabilistic models
	based on proper scoring rules, covering a wide range of
	situations in applied work. This paper discusses implementation
	and usage details, presents case studies from meteorology and
	economics, and points to the relevant background literature.",
	journal  = "Journal of statistical software",
	volume   =  90,
	pages    = "1--37",
	month    =  "21~" # aug,
	year     =  2019,
	url      = "https://www.jstatsoft.org/article/view/v090i12",
	file     = "All Papers/J/Jordan et al. 2019 - Evaluating Probabilistic Forecasts with scoringRules.pdf",
	keywords = "comparative evaluation; ensemble forecasts; out-of-sample
	evaluation; predictive distributions; proper scoring rules; score
	computation; R",
	language = "en",
	issn     = "1548-7660, 1548-7660",
	doi      = "10.18637/jss.v090.i12"
}

@INPROCEEDINGS{Gustafsson2020-uq,
	title     = "How to Train Your {Energy-Based} Model for Regression",
	booktitle = "Proceedings of the British Machine Vision Conference ({BMVC})",
	author    = "Gustafsson, Fredrik K and Danelljan, Martin and Timofte, Radu
	and Sch{\"o}n, Thomas B",
	month     =  sep,
	year      =  2020,
	file      = "All Papers/G/Gustafsson et al. 2020 - How to Train Your Energy-Based Model for Regression.pdf",
	keywords  = "Energy-based"
}

@INPROCEEDINGS{Guizilini2020-tr,
	title     = "{3D} Packing for {Self-Supervised} Monocular Depth Estimation",
	booktitle = "{IEEE} Conference on Computer Vision and Pattern Recognition
	({CVPR})",
	author    = "Guizilini, Vitor and Ambrus, Rares and Pillai, Sudeep and
	Raventos, Allan and Gaidon, Adrien",
	year      =  2020,
	file      = "All Papers/G/Guizilini et al. 2020 - 3D Packing for Self-Supervised Monocular Depth Estimation.pdf"
}

@INPROCEEDINGS{Grathwohl2020-dd,
	title     = "Your classifier is secretly an energy based model and you should
	treat it like one",
	booktitle = "International Conference on Learning Representations",
	author    = "Grathwohl, Will and Wang, Kuan-Chieh and Jacobsen, Joern-Henrik
	and Duvenaud, David and Norouzi, Mohammad and Swersky, Kevin",
	year      =  2020,
	url       = "https://openreview.net/forum?id=Hkxzx0NtDB",
	file      = "All Papers/G/Grathwohl et al. 2020 - Your classifier is secretly an energy based model and you should treat it like one.pdf",
	keywords  = "Energy-based"
}

@INPROCEEDINGS{Du2019-en,
	title     = "Implicit Generation and Modeling with Energy Based Models",
	booktitle = "Advances in Neural Information Processing Systems",
	author    = "Du, Yilun and Mordatch, Igor",
	editor    = "Wallach, H and Larochelle, H and Beygelzimer, A and
	d'Alch{\'e}-Buc, F and Fox, E and Garnett, R",
	publisher = "Curran Associates, Inc.",
	volume    =  32,
	year      =  2019,
	url       = "https://proceedings.neurips.cc/paper/2019/file/378a063b8fdb1db941e34f4bde584c7d-Paper.pdf",
	file      = "All Papers/D/Du and Mordatch 2019 - Implicit Generation and Modeling with Energy Based Models.pdf"
}

@ARTICLE{Chernozhukov2021-sg,
	title       = "Distributional conformal prediction",
	author      = "Chernozhukov, Victor and W{\"u}thrich, Kaspar and Zhu, Yinchu",
	affiliation = "Department of Economics, Massachusetts Institute of
	Technology, Cambridge, MA 02142. Center for Statistics and
	Data Science, Massachusetts Institute of Technology,
	Cambridge, MA 02142. Department of Economics, University of
	California San Diego, La Jolla, CA 92093. CESifo, 81679
	Munich, Germany. ifo Center for Public Finance and Political
	Economy, ifo Institute, 81679 Munich, Germany. Department of
	Economics, Brandeis University, Waltham, MA 02453;
	yinchuzhu@brandeis.edu. International Business School,
	Brandeis University, Waltham, MA 02453.",
	abstract    = "We propose a robust method for constructing conditionally
	valid prediction intervals based on models for conditional
	distributions such as quantile and distribution regression.
	Our approach can be applied to important prediction problems,
	including cross-sectional prediction, k-step-ahead forecasts,
	synthetic controls and counterfactual prediction, and
	individual treatment effects prediction. Our method exploits
	the probability integral transform and relies on permuting
	estimated ranks. Unlike regression residuals, ranks are
	independent of the predictors, allowing us to construct
	conditionally valid prediction intervals under
	heteroskedasticity. We establish approximate conditional
	validity under consistent estimation and provide approximate
	unconditional validity under model misspecification, under
	overfitting, and with time series data. We also propose a
	simple ``shape'' adjustment of our baseline method that yields
	optimal prediction intervals.",
	journal     = "Proceedings of the National Academy of Sciences of the United
	States of America",
	publisher   = "National Acad Sciences",
	volume      =  118,
	number      =  48,
	month       =  nov,
	year        =  2021,
	url         = "http://dx.doi.org/10.1073/pnas.2107794118",
	file        = "All Papers/C/Chernozhukov et al. 2021 - 1909.07889.pdf;All Papers/C/Chernozhukov et al. 2021 - Distributional conformal prediction.pdf",
	keywords    = "conditional validity; distribution regression; model-free
	validity; prediction intervals; quantile regression",
	language    = "en",
	issn        = "0027-8424, 1091-6490",
	pmid        = "34819368",
	doi         = "10.1073/pnas.2107794118",
	pmc         = "PMC8640792"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Hyvarinen2005-jy,
	title    = "Estimation of {Non-Normalized} Statistical Models by Score
	Matching",
	author   = "Hyv{\"a}rinen, Aapo",
	journal  = "Journal of machine learning research: JMLR",
	volume   =  6,
	number   =  24,
	pages    = "695--709",
	year     =  2005,
	url      = "https://www.jmlr.org/papers/v6/hyvarinen05a.html",
	file     = "All Papers/H/Hyvärinen 2005 - Estimation of Non-Normalized Statistical Models by Score Matching.pdf",
	issn     = "1532-4435, 1533-7928"
}

@TECHREPORT{Bishop1994-kj,
	title     = "Mixture density networks",
	author    = "Bishop, Christopher M",
	abstract  = "Minimization of a sum-of-squares or cross-entropy error function
	leads to network outputs which approximate the conditional
	averages of the target data, conditioned on the input vector.
	For classifications problems, with a suitably chosen target
	coding scheme, these averages represent the posterior
	probabilities of class membership, and so can be regarded as
	optimal. For problems involving the prediction of continuous
	variables, however, the conditional averages provide only a very
	limited description of the properties of the target variables.
	This is particularly true for problems in which the mapping to
	be learned is multi-valued, as often arises in the solution of
	inverse problems, since the average of several correct target
	values is not necessarily itself a correct value. In order to
	obtain a complete description of the data, for the purposes of
	predicting the outputs corresponding to new input vectors, we
	must model the conditional probability distribution of the
	target data, again conditioned on the input vector. In this
	paper we introduce a new class of network models obtained by
	combining a conventional neural network with a mixture density
	model. The complete system is called a Mixture Density Network,
	and can in principle represent arbitrary conditional probability
	distributions in the same way that a conventional neural network
	can represent arbitrary functions. We demonstrate the
	effectiveness of Mixture Density Networks using both a toy
	problem and a problem involving robot inverse kinematics.",
	publisher = "Aston University",
	year      =  1994,
	url       = "http://publications.aston.ac.uk/id/eprint/373/",
	file      = "All Papers/B/Bishop 1994 - Mixture density networks.pdf",
	address   = "Birmingham",
	keywords  = "NCRG sum-of-squares cross-entropy error function classifications
	problems coding scheme conditional probability distribution
	network models neural network mixture density model Mixture
	Density Network inverse kinematics;Distributions"
}

@ARTICLE{Lei2018-nx,
	title     = "{Distribution-Free} Predictive Inference for Regression",
	author    = "Lei, Jing and G'Sell, Max and Rinaldo, Alessandro and
	Tibshirani, Ryan J and Wasserman, Larry",
	abstract  = "ABSTRACTWe develop a general framework for distribution-free
	predictive inference in regression, using conformal inference.
	The proposed methodology allows for the construction of a
	prediction band for the response variable using any estimator of
	the regression function. The resulting prediction band preserves
	the consistency properties of the original estimator under
	standard assumptions, while guaranteeing finite-sample marginal
	coverage even when these assumptions do not hold. We analyze and
	compare, both empirically and theoretically, the two major
	variants of our conformal framework: full conformal inference
	and split conformal inference, along with a related jackknife
	method. These methods offer different tradeoffs between
	statistical accuracy (length of resulting prediction intervals)
	and computational efficiency. As extensions, we develop a method
	for constructing valid in-sample prediction intervals called
	rank-one-out conformal inference, which has essentially the same
	computational efficiency as split conformal inference. We also
	describe an extension of our procedures for producing prediction
	bands with locally varying length, to adapt to
	heteroscedasticity in the data. Finally, we propose a model-free
	notion of variable importance, called leave-one-covariate-out or
	LOCO inference. Accompanying this article is an R package
	conformalInference that implements all of the proposals we have
	introduced. In the spirit of reproducibility, all of our
	empirical results can also be easily (re)generated using this
	package.",
	journal   = "Journal of the American Statistical Association",
	publisher = "Taylor \& Francis",
	volume    =  113,
	number    =  523,
	pages     = "1094--1111",
	month     =  "3~" # jul,
	year      =  2018,
	url       = "https://doi.org/10.1080/01621459.2017.1307116",
	file      = "All Papers/L/Lei et al. 2018 - Distribution-Free Predictive Inference for Regression.pdf",
	issn      = "0162-1459",
	doi       = "10.1080/01621459.2017.1307116"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Durkan2019-sc,
	title     = "Neural spline flows",
	author    = "{Durkan} and {Bekasov} and {Murray} and {others}",
	abstract  = "… , we refer to the resulting class of normalizing flows as
	rational-quadratic neural spline flows (RQ-NSF), … [22], and the
	universal approximation capabilities of neural networks in
	general. …",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2019,
	url       = "https://proceedings.neurips.cc/paper/8969-neural-spline-flows",
	file      = "All Papers/D/Durkan et al. 2019 - Neural spline flows.pdf",
	keywords  = "Flow",
	issn      = "1049-5258"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Hullermeier2021-jj,
	title     = "Aleatoric and epistemic uncertainty in machine learning: an
	introduction to concepts and methods",
	author    = "H{\"u}llermeier, Eyke and Waegeman, Willem",
	abstract  = "The notion of uncertainty is of major importance in machine
	learning and constitutes a key element of machine learning
	methodology. In line with the statistical tradition, uncertainty
	has long been perceived as almost synonymous with standard
	probability and probabilistic predictions. Yet, due to the
	steadily increasing relevance of machine learning for practical
	applications and related issues such as safety requirements, new
	problems and challenges have recently been identified by machine
	learning scholars, and these problems may call for new
	methodological developments. In particular, this includes the
	importance of distinguishing between (at least) two different
	types of uncertainty, often referred to as aleatoric and
	epistemic. In this paper, we provide an introduction to the
	topic of uncertainty in machine learning as well as an overview
	of attempts so far at handling uncertainty in general and
	formalizing this distinction in particular.",
	journal   = "Machine learning",
	publisher = "Springer",
	volume    =  110,
	number    =  3,
	pages     = "457--506",
	month     =  "1~" # mar,
	year      =  2021,
	url       = "https://doi.org/10.1007/s10994-021-05946-3",
	file      = "All Papers/H/Hüllermeier and Waegeman 2021 - Aleatoric and epistemic uncertainty in machine learning - an introduction to concepts and methods.pdf",
	keywords  = "Epistemic",
	issn      = "0885-6125, 1573-0565",
	doi       = "10.1007/s10994-021-05946-3"
}

@INPROCEEDINGS{Gal2016-xn,
	title     = "Dropout as a Bayesian Approximation: Representing Model
	Uncertainty in Deep Learning",
	booktitle = "Proceedings of The 33rd International Conference on Machine
	Learning",
	author    = "Gal, Yarin and Ghahramani, Zoubin",
	editor    = "Balcan, Maria Florina and Weinberger, Kilian Q",
	abstract  = "Deep learning tools have gained tremendous attention in applied
	machine learning. However such tools for regression and
	classification do not capture model uncertainty. In comparison,
	Bayesian models offer a mathematically grounded framework to
	reason about model uncertainty, but usually come with a
	prohibitive computational cost. In this paper we develop a new
	theoretical framework casting dropout training in deep neural
	networks (NNs) as approximate Bayesian inference in deep
	Gaussian processes. A direct result of this theory gives us
	tools to model uncertainty with dropout NNs -- extracting
	information from existing models that has been thrown away so
	far. This mitigates the problem of representing uncertainty in
	deep learning without sacrificing either computational
	complexity or test accuracy. We perform an extensive study of
	the properties of dropout's uncertainty. Various network
	architectures and non-linearities are assessed on tasks of
	regression and classification, using MNIST as an example. We
	show a considerable improvement in predictive log-likelihood and
	RMSE compared to existing state-of-the-art methods, and finish
	by using dropout's uncertainty in deep reinforcement learning.",
	publisher = "PMLR",
	volume    =  48,
	pages     = "1050--1059",
	series    = "Proceedings of Machine Learning Research",
	year      =  2016,
	url       = "https://proceedings.mlr.press/v48/gal16.html",
	file      = "All Papers/G/Gal and Ghahramani 2016 - Dropout as a Bayesian Approximation - Representing Model Uncertainty in Deep Learning.pdf",
	address   = "New York, USA",
	keywords  = "Variational"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Lakshminarayanan2017-zg,
	title     = "Simple and scalable predictive uncertainty estimation using deep
	ensembles",
	author    = "{Lakshminarayanan} and {Pritzel} and {others}",
	abstract  = "Deep neural networks (NNs) are powerful black box predictors
	that have recently achieved impressive performance on a wide
	spectrum of tasks. Quantifying predictive uncertainty in NNs is
	a challenging and yet unsolved problem. Bayesian NNs, which
	learn a distribution over weights, are currently the
	state-of-the-art for estimating predictive uncertainty; however
	these require significant modifications to the training
	procedure and are computationally expensive compared to standard
	(non-Bayesian) NNs. We propose an alternative to …",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2017,
	url       = "https://proceedings.neurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html",
	file      = "All Papers/L/Lakshminarayanan et al. 2017 - Simple and scalable predictive uncertainty estimation using deep ensembles.pdf",
	keywords  = "Ensembling",
	issn      = "1049-5258"
}

@INPROCEEDINGS{Kivaranovic2020-vj,
	title     = "Adaptive, {Distribution-Free} Prediction Intervals for Deep
	Networks",
	booktitle = "Proceedings of the Twenty Third International Conference on
	Artificial Intelligence and Statistics",
	author    = "Kivaranovic, Danijel and Johnson, Kory D and Leeb, Hannes",
	editor    = "Chiappa, Silvia and Calandra, Roberto",
	abstract  = "The machine learning literature contains several constructions
	for prediction intervals that are intuitively reasonable but
	ultimately ad-hoc in that they do not come with provable
	performance guarantees. We present methods from the statistics
	literature that can be used efficiently with neural networks
	under minimal assumptions with guaranteed performance. We
	propose a neural network that outputs three values instead of a
	single point estimate and optimizes a loss function motivated by
	the standard quantile regression loss. We provide two prediction
	interval methods with finite sample coverage guarantees solely
	under the assumption that the observations are independent and
	identically distributed. The first method leverages the
	conformal inference framework and provides average coverage. The
	second method provides a new, stronger guarantee by conditioning
	on the observed data. Lastly, our loss function does not
	compromise the predictive accuracy of the network like other
	prediction interval methods. We demonstrate the ease of use of
	our procedures as well as its improvements over other methods on
	both simulated and real data. As most deep networks can easily
	be modified by our method to output predictions with valid
	prediction intervals, its use should become standard practice,
	much like reporting standard errors along with mean estimates.",
	publisher = "PMLR",
	volume    =  108,
	pages     = "4346--4356",
	series    = "Proceedings of Machine Learning Research",
	year      =  2020,
	url       = "https://proceedings.mlr.press/v108/kivaranovic20a.html",
	file      = "All Papers/K/Kivaranovic et al. 2020 - Adaptive, Distribution-Free Prediction Intervals for Deep Networks.pdf",
	keywords  = "Intervals;Conformal"
}

@ARTICLE{Brehmer2021-th,
	title     = "Scoring interval forecasts: Equal-tailed, shortest, and modal
	interval",
	author    = "Brehmer, Jonas R and Gneiting, Tilmann",
	abstract  = "We consider different types of predictive intervals and ask
	whether they are elicitable, that is, are unique minimizers of a
	loss or scoring function in expectation. The equal-tailed
	interval is elicitable, with a rich class of suitable loss
	functions, though subject to translation invariance, or positive
	homogeneity and differentiability, the Winkler interval score
	becomes a unique choice. The modal interval also is elicitable,
	with a sole consistent scoring function, up to equivalence.
	However, the shortest interval fails to be elicitable relative
	to practically relevant classes of distributions. These results
	provide guidance in interval forecast evaluation and support
	recent choices of performance measures in forecast competitions.",
	journal   = "BJOG: an international journal of obstetrics and gynaecology",
	publisher = "Bernoulli Society for Mathematical Statistics and Probability",
	volume    =  27,
	number    =  3,
	pages     = "1993--2010",
	month     =  may,
	year      =  2021,
	url       = "https://projecteuclid.org/journals/bernoulli/volume-27/issue-3/Scoring-interval-forecasts-Equal-tailed-shortest-and-modal-interval/10.3150/20-BEJ1298.short",
	file      = "All Papers/B/Brehmer and Gneiting 2021 - Scoring interval forecasts - Equal-tailed, shortest, and modal interval.pdf",
	keywords  = "elicitability; forecast evaluation; interval forecast; modal
	interval; predictive performance; scoring function;
	;Intervals;Scoring rules",
	language  = "en",
	issn      = "1470-0328, 1350-7265",
	doi       = "10.3150/20-BEJ1298"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Jospin2022-zi,
	title     = "{Hands-On} Bayesian Neural {Networks---A} Tutorial for Deep
	Learning Users",
	author    = "Jospin, Laurent Valentin and Laga, Hamid and Boussaid, Farid and
	Buntine, Wray and Bennamoun, Mohammed",
	abstract  = "Modern deep learning methods constitute incredibly powerful
	tools to tackle a myriad of challenging problems. However, since
	deep learning methods operate as black boxes, the uncertainty
	associated with their predictions is often challenging to
	quantify. Bayesian statistics offer a formalism to understand
	and quantify the uncertainty associated with deep neural network
	predictions. This tutorial provides deep learning practitioners
	with an overview of the relevant literature and a complete
	toolset to design, implement, train, use and evaluate Bayesian
	neural networks, i.e., stochastic artificial neural networks
	trained using Bayesian methods.",
	journal   = "IEEE Computational Intelligence Magazine",
	publisher = "ieeexplore.ieee.org",
	volume    =  17,
	number    =  2,
	pages     = "29--48",
	month     =  may,
	year      =  2022,
	url       = "http://dx.doi.org/10.1109/MCI.2022.3155327",
	file      = "All Papers/J/Jospin et al. 2022 - Hands-On Bayesian Neural Networks—A Tutorial for Deep Learning Users.pdf",
	keywords  = "Deep learning;Training data;Uncertainty;Design
	methodology;Computational modeling;Stochastic processes;Bayes
	methods;Neural networks;Bayesian",
	issn      = "1556-6048",
	doi       = "10.1109/MCI.2022.3155327"
}

@INPROCEEDINGS{Alaa2020-qh,
	title     = "Frequentist Uncertainty in Recurrent Neural Networks via
	Blockwise Influence Functions",
	booktitle = "Proceedings of the 37th International Conference on Machine
	Learning",
	author    = "Alaa, Ahmed and Van Der Schaar, Mihaela",
	editor    = "Iii, Hal Daum{\'e} and Singh, Aarti",
	abstract  = "Recurrent neural networks (RNNs) are instrumental in modelling
	sequential and time-series data. Yet, when using RNNs to inform
	decision-making, predictions by themselves are not sufficient
	--- we also need estimates of predictive uncertainty. Existing
	approaches for uncertainty quantification in RNNs are based
	predominantly on Bayesian methods; these are computationally
	prohibitive, and require major alterations to the RNN
	architecture and training. Capitalizing on ideas from classical
	jackknife resampling, we develop a frequentist alternative that:
	(a) does not interfere with model training or compromise its
	accuracy, (b) applies to any RNN architecture, and (c) provides
	theoretical coverage guarantees on the estimated uncertainty
	intervals. Our method derives predictive uncertainty from the
	variability of the (jackknife) sampling distribution of the RNN
	outputs, which is estimated by repeatedly deleting ``blocks'' of
	(temporally-correlated) training data, and collecting the
	predictions of the RNN re-trained on the remaining data. To
	avoid exhaustive re-training, we utilize influence functions to
	estimate the effect of removing training data blocks on the
	learned RNN parameters. Using data from a critical care setting,
	we demonstrate the utility of uncertainty quantification in
	sequential decision-making.",
	publisher = "PMLR",
	volume    =  119,
	pages     = "175--190",
	series    = "Proceedings of Machine Learning Research",
	year      =  2020,
	url       = "https://proceedings.mlr.press/v119/alaa20b.html",
	file      = "All Papers/A/Alaa and Van Der Schaar 2020 - Frequentist Uncertainty in Recurrent Neural Networks via Blockwise Influence Functions.pdf",
	keywords  = "Frequentist"
}

@INPROCEEDINGS{Pearce2018-lo,
	title     = "{High-Quality} Prediction Intervals for Deep Learning: A
	{Distribution-Free}, Ensembled Approach",
	booktitle = "Proceedings of the 35th International Conference on Machine
	Learning",
	author    = "Pearce, Tim and Brintrup, Alexandra and Zaki, Mohamed and Neely,
	Andy",
	editor    = "Dy, Jennifer and Krause, Andreas",
	abstract  = "This paper considers the generation of prediction intervals
	(PIs) by neural networks for quantifying uncertainty in
	regression tasks. It is axiomatic that high-quality PIs should
	be as narrow as possible, whilst capturing a specified portion
	of data. We derive a loss function directly from this axiom that
	requires no distributional assumption. We show how its form
	derives from a likelihood principle, that it can be used with
	gradient descent, and that model uncertainty is accounted for in
	ensembled form. Benchmark experiments show the method
	outperforms current state-of-the-art uncertainty quantification
	methods, reducing average PI width by over 10\%.",
	publisher = "PMLR",
	volume    =  80,
	pages     = "4075--4084",
	series    = "Proceedings of Machine Learning Research",
	year      =  2018,
	url       = "https://proceedings.mlr.press/v80/pearce18a.html",
	file      = "All Papers/P/Pearce et al. 2018 - 1802.07167.pdf;All Papers/P/Pearce et al. 2018 - High-Quality Prediction Intervals for Deep Learning - A Distribution-Free, Ensembled Approach.pdf",
	keywords  = "Intervals"
}

@ARTICLE{Bracher2021-ut,
	title       = "Evaluating epidemic forecasts in an interval format",
	author      = "Bracher, Johannes and Ray, Evan L and Gneiting, Tilmann and
	Reich, Nicholas G",
	affiliation = "Chair of Statistics and Econometrics, Karlsruhe Institute of
	Technology (KIT), Karlsruhe, Germany. Computational Statistics
	Group, Heidelberg Institute for Theoretical Studies,
	Heidelberg, Germany. School of Public Health and Health
	Sciences, Department of Biostatistics and Epidemiology,
	University of Massachusetts, Amherst, Massachusetts, United
	States of America. Institute for Stochastics, Karlsruhe
	Institute of Technology (KIT), Karlsruhe, Germany.",
	abstract    = "For practical reasons, many forecasts of case,
	hospitalization, and death counts in the context of the
	current Coronavirus Disease 2019 (COVID-19) pandemic are
	issued in the form of central predictive intervals at various
	levels. This is also the case for the forecasts collected in
	the COVID-19 Forecast Hub (https://covid19forecasthub.org/).
	Forecast evaluation metrics like the logarithmic score, which
	has been applied in several infectious disease forecasting
	challenges, are then not available as they require full
	predictive distributions. This article provides an overview of
	how established methods for the evaluation of quantile and
	interval forecasts can be applied to epidemic forecasts in
	this format. Specifically, we discuss the computation and
	interpretation of the weighted interval score, which is a
	proper score that approximates the continuous ranked
	probability score. It can be interpreted as a generalization
	of the absolute error to probabilistic forecasts and allows
	for a decomposition into a measure of sharpness and penalties
	for over- and underprediction.",
	journal     = "PLoS computational biology",
	publisher   = "journals.plos.org",
	volume      =  17,
	number      =  2,
	pages       = "e1008618",
	month       =  feb,
	year        =  2021,
	url         = "http://dx.doi.org/10.1371/journal.pcbi.1008618",
	file        = "All Papers/B/Bracher et al. 2021 - Evaluating epidemic forecasts in an interval format.pdf",
	keywords    = "Intervals;Scoring rules",
	language    = "en",
	issn        = "1553-734X, 1553-7358",
	pmid        = "33577550",
	doi         = "10.1371/journal.pcbi.1008618",
	pmc         = "PMC7880475"
}

@INPROCEEDINGS{Gustafsson2020-yj,
	title     = "{Energy-Based} Models for Deep Probabilistic Regression",
	booktitle = "Computer Vision -- {ECCV} 2020",
	author    = "Gustafsson, Fredrik K and Danelljan, Martin and Bhat, Goutam and
	Sch{\"o}n, Thomas B",
	abstract  = "While deep learning-based classification is generally tackled
	using standardized approaches, a wide variety of techniques are
	employed for regression. In computer vision, one particularly
	popular such technique is that of confidence-based regression,
	which entails predicting a confidence value for each
	input-target pair (x, y). While this approach has demonstrated
	impressive results, it requires important task-dependent design
	choices, and the predicted confidences lack a natural
	probabilistic meaning. We address these issues by proposing a
	general and conceptually simple regression method with a clear
	probabilistic interpretation. In our proposed approach, we
	create an energy-based model of the conditional target density
	p(y|x), using a deep neural network to predict the un-normalized
	density from (x, y). This model of p(y|x) is trained by directly
	minimizing the associated negative log-likelihood, approximated
	using Monte Carlo sampling. We perform comprehensive experiments
	on four computer vision regression tasks. Our approach
	outperforms direct regression, as well as other probabilistic
	and confidence-based methods. Notably, our model achieves a
	$$2.2\%$$2.2\%AP improvement over Faster-RCNN for object
	detection on the COCO dataset, and sets a new state-of-the-art
	on visual tracking when applied for bounding box estimation. In
	contrast to confidence-based methods, our approach is also shown
	to be directly applicable to more general tasks such as age and
	head-pose estimation. Code is available at
	https://github.com/fregu856/ebms_regression.",
	publisher = "Springer International Publishing",
	pages     = "325--343",
	year      =  2020,
	url       = "http://dx.doi.org/10.1007/978-3-030-58565-5_20",
	file      = "All Papers/G/Gustafsson et al. 2020 - Energy-Based Models for Deep Probabilistic Regression.pdf",
	keywords  = "Energy-based",
	doi       = "10.1007/978-3-030-58565-5\_20"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Malinin2018-ix,
	title     = "Predictive uncertainty estimation via prior networks",
	author    = "{Malinin} and {Gales}",
	abstract  = "Estimating how uncertain an AI system is in its predictions is
	important to improve the safety of such systems. Uncertainty in
	predictive can result from uncertainty in model parameters,
	irreducible\textbackslashemph \{data uncertainty\} and
	uncertainty due to distributional mismatch between the test and
	training data distributions. Different actions might be taken
	depending on the source of the uncertainty so it is important to
	be able to distinguish between them. Recently, baseline tasks
	and metrics have been defined and several practical methods to
	estimate …",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2018,
	url       = "https://proceedings.neurips.cc/paper/2018/hash/3ea2db50e62ceefceaf70a9d9a56a6f4-Abstract.html",
	file      = "All Papers/M/Malinin and Gales 2018 - Predictive uncertainty estimation via prior networks.pdf",
	issn      = "1049-5258"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Sensoy2018-sm,
	title     = "Evidential deep learning to quantify classification uncertainty",
	author    = "{Sensoy} and {Kaplan} and {others}",
	abstract  = "Deterministic neural nets have been shown to learn effective
	predictors on a wide range of machine learning problems.
	However, as the standard approach is to train the network to
	minimize a prediction loss, the resultant model remains ignorant
	to its prediction confidence. Orthogonally to Bayesian neural
	nets that indirectly infer prediction uncertainty through weight
	uncertainties, we propose explicit modeling of the same using
	the theory of subjective logic. By placing a Dirichlet
	distribution on the class probabilities, we treat …",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2018,
	url       = "https://proceedings.neurips.cc/paper/2018/hash/a981f2b708044d6fb4a71a1463242520-Abstract.html",
	file      = "All Papers/S/Sensoy et al. 2018 - Evidential deep learning to quantify classification uncertainty.pdf",
	issn      = "1049-5258"
}

@ARTICLE{Hsu2020-on,
	title     = "Generalized odin: Detecting out-of-distribution image without
	learning from out-of-distribution data",
	author    = "{Hsu} and {Shen} and {Jin} and {Kira}",
	abstract  = "Deep neural networks have attained remarkable performance when
	applied to data that comes from the same distribution as that of
	the training set, but can significantly degrade otherwise.
	Therefore, detecting whether an example is out-of-distribution
	(OoD) is crucial to enable a system that can reject such samples
	or alert users. Recent works have made significant progress on
	OoD benchmarks consisting of small image datasets. However, many
	recent methods based on neural networks rely on training or
	tuning with both in …",
	journal   = "Proceedings of the IEEE",
	publisher = "openaccess.thecvf.com",
	year      =  2020,
	url       = "http://openaccess.thecvf.com/content_CVPR_2020/html/Hsu_Generalized_ODIN_Detecting_Out-of-Distribution_Image_Without_Learning_From_Out-of-Distribution_Data_CVPR_2020_paper.html",
	file      = "All Papers/H/Hsu et al. 2020 - Generalized odin - Detecting out-of-distribution image without learning from out-of-distribution data.pdf"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Louizos2017-ia,
	title     = "Bayesian compression for deep learning",
	author    = "{Louizos} and {Ullrich} and {Welling}",
	abstract  = "Compression and computational efficiency in deep learning have
	become a problem of great significance. In this work, we argue
	that the most principled and effective way to attack this
	problem is by adopting a Bayesian point of view, where through
	sparsity inducing priors we prune large parts of the network. We
	introduce two novelties in this paper: 1) we use hierarchical
	priors to prune nodes instead of individual weights, and 2) we
	use the posterior uncertainties to determine the optimal fixed
	point precision to encode the weights. Both …",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2017,
	url       = "https://proceedings.neurips.cc/paper/2017/hash/69d1fc78dbda242c43ad6590368912d4-Abstract.html",
	file      = "All Papers/L/Louizos et al. 2017 - Bayesian compression for deep learning.pdf",
	issn      = "1049-5258"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Seo2019-ir,
	title     = "Learning for single-shot confidence calibration in deep neural
	networks through stochastic inferences",
	author    = "{Seo} and {Seo} and {Han}",
	abstract  = "We propose a generic framework to calibrate accuracy and
	confidence of a prediction in deep neural networks through
	stochastic inferences. We interpret stochastic regularization
	using a Bayesian model, and analyze the relation between
	predictive uncertainty of networks and variance of the
	prediction scores obtained by stochastic inferences for a single
	example. Our empirical study shows that the accuracy and the
	score of a prediction are highly correlated with the variance of
	multiple stochastic inferences given by stochastic …",
	journal   = "of the IEEE/CVF conference on …",
	publisher = "openaccess.thecvf.com",
	year      =  2019,
	url       = "http://openaccess.thecvf.com/content_CVPR_2019/html/Seo_Learning_for_Single-Shot_Confidence_Calibration_in_Deep_Neural_Networks_Through_CVPR_2019_paper.html",
	file      = "All Papers/S/Seo et al. 2019 - Learning for single-shot confidence calibration in deep neural networks through stochastic inferences.pdf",
	keywords  = "Calibration;Classification"
}

@INPROCEEDINGS{Song2019-bk,
	title     = "Distribution calibration for regression",
	booktitle = "Proceedings of the 36th International Conference on Machine
	Learning",
	author    = "Song, Hao and Diethe, Tom and Kull, Meelis and Flach, Peter",
	editor    = "Chaudhuri, Kamalika and Salakhutdinov, Ruslan",
	abstract  = "We are concerned with obtaining well-calibrated output
	distributions from regression models. Such distributions allow
	us to quantify the uncertainty that the model has regarding the
	predicted target value. We introduce the novel concept of
	distribution calibration, and demonstrate its advantages over
	the existing definition of quantile calibration. We further
	propose a post-hoc approach to improving the predictions from
	previously trained regression models, using multi-output
	Gaussian Processes with a novel Beta link function. The proposed
	method is experimentally verified on a set of common regression
	models and shows improvements for both distribution-level and
	quantile-level calibration.",
	publisher = "PMLR",
	volume    =  97,
	pages     = "5897--5906",
	series    = "Proceedings of Machine Learning Research",
	year      =  2019,
	url       = "https://proceedings.mlr.press/v97/song19a.html",
	file      = "All Papers/S/Song et al. 2019 - Distribution calibration for regression.pdf"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kumar2019-lm,
	title     = "Verified uncertainty calibration",
	author    = "{Kumar} and {Liang} and {Ma}",
	abstract  = "… Next, we show that we can estimate a model's calibration error
	… calibration and marginal calibration in our experiments. Other
	notions of multiclass calibration include joint calibration (…",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2019,
	url       = "https://proceedings.neurips.cc/paper/2019/hash/f8c0c968632845cd133308b1a494967f-Abstract.html",
	file      = "All Papers/K/Kumar et al. 2019 - Verified uncertainty calibration.pdf",
	issn      = "1049-5258"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Romano2019-kp,
	title     = "Conformalized quantile regression",
	author    = "{Romano} and {Patterson} and {others}",
	abstract  = "… conformal prediction with quantile regression . The resulting
	method, which we call conformalized quantile regression (… , we
	conclude that conformal quantile regression yields shorter …",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2019,
	url       = "https://proceedings.neurips.cc/paper/2019/hash/5103c3584b063c431bd1268e9b5e76fb-Abstract.html",
	file      = "All Papers/R/Romano et al. 2019 - 1905.03222.pdf;All Papers/R/Romano et al. 2019 - Conformalized quantile regression.pdf",
	issn      = "1049-5258"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Ho2020-rc,
	title     = "Denoising diffusion probabilistic models",
	author    = "{Ho} and {Jain} and {Abbeel}",
	abstract  = "… This paper presents progress in diffusion probabilistic models
	[53]. A diffusion probabilistic model (which we will call a ``
	diffusion model '' for brevity) is a parameterized Markov chain
	…",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2020,
	url       = "https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html",
	file      = "All Papers/H/Ho et al. 2020 - 2006.11239.pdf;All Papers/H/Ho et al. 2020 - Denoising diffusion probabilistic models.pdf",
	issn      = "1049-5258"
}

@ARTICLE{Pereyra2017-lk,
	title         = "Regularizing Neural Networks by Penalizing Confident Output
	Distributions",
	author        = "Pereyra, Gabriel and Tucker, George and Chorowski, Jan and
	Kaiser, {\L}ukasz and Hinton, Geoffrey",
	abstract      = "We systematically explore regularizing neural networks by
	penalizing low entropy output distributions. We show that
	penalizing low entropy output distributions, which has been
	shown to improve exploration in reinforcement learning, acts
	as a strong regularizer in supervised learning. Furthermore,
	we connect a maximum entropy based confidence penalty to
	label smoothing through the direction of the KL divergence.
	We exhaustively evaluate the proposed confidence penalty and
	label smoothing on 6 common benchmarks: image classification
	(MNIST and Cifar-10), language modeling (Penn Treebank),
	machine translation (WMT'14 English-to-German), and speech
	recognition (TIMIT and WSJ). We find that both label
	smoothing and the confidence penalty improve
	state-of-the-art models across benchmarks without modifying
	existing hyperparameters, suggesting the wide applicability
	of these regularizers.",
	month         =  jan,
	year          =  2017,
	url           = "http://arxiv.org/abs/1701.06548",
	file          = "All Papers/P/Pereyra et al. 2017 - Regularizing Neural Networks by Penalizing Confident Output Distributions.pdf",
	archivePrefix = "arXiv",
	eprint        = "1701.06548",
	primaryClass  = "cs.NE",
	arxivid       = "1701.06548"
}

@ARTICLE{Narayan2021-ay,
	title         = "Regularization Strategies for Quantile Regression",
	author        = "Narayan, Taman and Wang, Serena and Canini, Kevin and Gupta,
	Maya",
	abstract      = "We investigate different methods for regularizing quantile
	regression when predicting either a subset of quantiles or
	the full inverse CDF. We show that minimizing an expected
	pinball loss over a continuous distribution of quantiles is
	a good regularizer even when only predicting a specific
	quantile. For predicting multiple quantiles, we propose
	achieving the classic goal of non-crossing quantiles by
	using deep lattice networks that treat the quantile as a
	monotonic input feature, and we discuss why monotonicity on
	other features is an apt regularizer for quantile
	regression. We show that lattice models enable regularizing
	the predicted distribution to a location-scale family.
	Lastly, we propose applying rate constraints to improve the
	calibration of the quantile predictions on specific subsets
	of interest and improve fairness metrics. We demonstrate our
	contributions on simulations, benchmark datasets, and real
	quantile regression problems.",
	month         =  "9~" # feb,
	year          =  2021,
	url           = "http://arxiv.org/abs/2102.05135",
	file          = "All Papers/N/Narayan et al. 2021 - Regularization Strategies for Quantile Regression.pdf",
	archivePrefix = "arXiv",
	eprint        = "2102.05135",
	primaryClass  = "stat.ML",
	arxivid       = "2102.05135"
}

@INPROCEEDINGS{Kingma2014-wu,
	title     = "{Auto-Encoding} Variational Bayes",
	booktitle = "International Conference on Learning Representations",
	author    = "Kingma, Diederik P and Welling, Max",
	abstract  = "How can we perform efficient inference and learning in directed
	probabilistic models, in the presence of continuous latent
	variables with intractable posterior distributions, and large
	datasets? We introduce a stochastic variational inference and
	learning algorithm that scales to large datasets and, under some
	mild differentiability conditions, even works in the intractable
	case. Our contributions is two-fold. First, we show that a
	reparameterization of the variational lower bound yields a lower
	bound estimator that can be straightforwardly optimized using
	standard stochastic gradient methods. Second, we show that for
	i.i.d. datasets with continuous latent variables per datapoint,
	posterior inference can be made especially efficient by fitting
	an approximate inference model (also called a recognition model)
	to the intractable posterior using the proposed lower bound
	estimator. Theoretical advantages are reflected in experimental
	results.",
	year      =  2014,
	url       = "http://arxiv.org/abs/1312.6114v10",
	file      = "All Papers/K/Kingma and Welling 2014 - Auto-Encoding Variational Bayes.pdf"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Khemakhem2020-ui,
	title     = "Ice-beem: Identifiable conditional energy-based deep models
	based on nonlinear ica",
	author    = "{Khemakhem} and {Monti} and {Kingma} and {others}",
	abstract  = "We consider the identifiability theory of probabilistic models
	and establish sufficient conditions under which the
	representations learnt by a very broad family of conditional
	energy-based models are unique in function space, up to a simple
	transformation. In our model family, the energy function is the
	dot-product between two feature extractors, one for the
	dependent variable, and one for the conditioning variable. We
	show that under mild conditions, the features are unique up to
	scaling and permutation. Our results extend recent …",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2020,
	url       = "https://proceedings.neurips.cc/paper/2020/hash/962e56a8a0b0420d87272a682bfd1e53-Abstract.html",
	file      = "All Papers/K/Khemakhem et al. 2020 - Ice-beem - Identifiable conditional energy-based deep models based on nonlinear ica.pdf",
	issn      = "1049-5258"
}

@INPROCEEDINGS{Van_Amersfoort2020-kf,
	title     = "Uncertainty Estimation Using a Single Deep Deterministic Neural
	Network",
	booktitle = "Proceedings of the 37th International Conference on Machine
	Learning",
	author    = "Van Amersfoort, Joost and Smith, Lewis and Teh, Yee Whye and
	Gal, Yarin",
	editor    = "Iii, Hal Daum{\'e} and Singh, Aarti",
	abstract  = "We propose a method for training a deterministic deep model that
	can find and reject out of distribution data points at test time
	with a single forward pass. Our approach, deterministic
	uncertainty quantification (DUQ), builds upon ideas of RBF
	networks. We scale training in these with a novel loss function
	and centroid updating scheme and match the accuracy of softmax
	models. By enforcing detectability of changes in the input using
	a gradient penalty, we are able to reliably detect out of
	distribution data. Our uncertainty quantification scales well to
	large datasets, and using a single model, we improve upon or
	match Deep Ensembles in out of distribution detection on notable
	difficult dataset pairs such as FashionMNIST vs. MNIST, and
	CIFAR-10 vs. SVHN.",
	publisher = "PMLR",
	volume    =  119,
	pages     = "9690--9700",
	series    = "Proceedings of Machine Learning Research",
	year      =  2020,
	url       = "https://proceedings.mlr.press/v119/van-amersfoort20a.html",
	file      = "All Papers/V/Van Amersfoort et al. 2020 - Uncertainty Estimation Using a Single Deep Deterministic Neural Network.pdf"
}

@INPROCEEDINGS{Rezende2014-yv,
	title     = "Stochastic Backpropagation and Approximate Inference in Deep
	Generative Models",
	booktitle = "Proceedings of the 31st International Conference on Machine
	Learning",
	author    = "Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan",
	editor    = "Xing, Eric P and Jebara, Tony",
	abstract  = "We marry ideas from deep neural networks and approximate
	Bayesian inference to derive a generalised class of deep,
	directed generative models, endowed with a new algorithm for
	scalable inference and learning. Our algorithm introduces a
	recognition model to represent an approximate posterior
	distribution and uses this for optimisation of a variational
	lower bound. We develop stochastic backpropagation -- rules for
	gradient backpropagation through stochastic variables -- and
	derive an algorithm that allows for joint optimisation of the
	parameters of both the generative and recognition models. We
	demonstrate on several real-world data sets that by using
	stochastic backpropagation and variational inference, we obtain
	models that are able to generate realistic samples of data,
	allow for accurate imputations of missing data, and provide a
	useful tool for high-dimensional data visualisation.",
	publisher = "PMLR",
	volume    =  32,
	pages     = "1278--1286",
	series    = "Proceedings of Machine Learning Research",
	year      =  2014,
	url       = "https://proceedings.mlr.press/v32/rezende14.html",
	file      = "All Papers/R/Rezende et al. 2014 - Stochastic Backpropagation and Approximate Inference in Deep Generative Models.pdf",
	address   = "Bejing, China"
}

@ARTICLE{Goodfellow2016-er,
	title         = "{NIPS} 2016 Tutorial: Generative Adversarial Networks",
	author        = "Goodfellow, Ian",
	abstract      = "This report summarizes the tutorial presented by the author
	at NIPS 2016 on generative adversarial networks (GANs). The
	tutorial describes: (1) Why generative modeling is a topic
	worth studying, (2) how generative models work, and how GANs
	compare to other generative models, (3) the details of how
	GANs work, (4) research frontiers in GANs, and (5)
	state-of-the-art image models that combine GANs with other
	methods. Finally, the tutorial contains three exercises for
	readers to complete, and the solutions to these exercises.",
	month         =  dec,
	year          =  2016,
	url           = "http://arxiv.org/abs/1701.00160",
	file          = "All Papers/G/Goodfellow 2016 - NIPS 2016 Tutorial - Generative Adversarial Networks.pdf",
	archivePrefix = "arXiv",
	eprint        = "1701.00160",
	primaryClass  = "cs.LG",
	arxivid       = "1701.00160"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kingma2014-pr,
	title     = "Semi-supervised learning with deep generative models",
	author    = "{Kingma} and {Mohamed} and {others}",
	abstract  = "The ever-increasing size of modern data sets combined with the
	difficulty of obtaining label information has made
	semi-supervised learning one of the problems of significant
	practical importance in modern data analysis. We revisit the
	approach to semi-supervised learning with generative models and
	develop new models that allow for effective generalisation from
	small labelled data sets to large unlabelled ones. Generative
	approaches have thus far been either inflexible, inefficient or
	non-scalable. We show that deep generative models and …",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2014,
	url       = "https://proceedings.neurips.cc/paper/2014/hash/d523773c6b194f37b938d340d5d02232-Abstract.html",
	file      = "All Papers/K/Kingma et al. 2014 - Semi-supervised learning with deep generative models.pdf",
	issn      = "1049-5258"
}

@ARTICLE{Ulmer2021-jw,
	title         = "A Survey on Evidential Deep Learning For {Single-Pass}
	Uncertainty Estimation",
	author        = "Ulmer, Dennis",
	abstract      = "Popular approaches for quantifying predictive uncertainty in
	deep neural networks often involve a set of weights or
	models, for instance via ensembling or Monte Carlo Dropout.
	These techniques usually produce overhead by having to train
	multiple model instances or do not produce very diverse
	predictions. This survey aims to familiarize the reader with
	an alternative class of models based on the concept of
	Evidential Deep Learning: For unfamiliar data, they admit
	``what they don't know'' and fall back onto a prior belief.
	Furthermore, they allow uncertainty estimation in a single
	model and forward pass by parameterizing distributions over
	distributions. This survey recapitulates existing works,
	focusing on the implementation in a classification setting.
	Finally, we survey the application of the same paradigm to
	regression problems. We also provide a reflection on the
	strengths and weaknesses of the mentioned approaches
	compared to existing ones and provide the most central
	theoretical results in order to inform future research.",
	month         =  "6~" # oct,
	year          =  2021,
	url           = "http://arxiv.org/abs/2110.03051",
	file          = "All Papers/U/Ulmer 2021 - A Survey on Evidential Deep Learning For Single-Pass Uncertainty Estimation.pdf",
	archivePrefix = "arXiv",
	eprint        = "2110.03051",
	primaryClass  = "cs.LG",
	arxivid       = "2110.03051"
}

@ARTICLE{Lyu2012-pi,
	title         = "Interpretation and Generalization of Score Matching",
	author        = "Lyu, Siwei",
	abstract      = "Score matching is a recently developed parameter learning
	method that is particularly effective to complicated high
	dimensional density models with intractable partition
	functions. In this paper, we study two issues that have not
	been completely resolved for score matching. First, we
	provide a formal link between maximum likelihood and score
	matching. Our analysis shows that score matching finds model
	parameters that are more robust with noisy training data.
	Second, we develop a generalization of score matching. Based
	on this generalization, we further demonstrate an extension
	of score matching to models of discrete data.",
	month         =  "9~" # may,
	year          =  2012,
	url           = "http://arxiv.org/abs/1205.2629",
	file          = "All Papers/L/Lyu 2012 - Interpretation and Generalization of Score Matching.pdf",
	archivePrefix = "arXiv",
	eprint        = "1205.2629",
	primaryClass  = "cs.LG",
	arxivid       = "1205.2629"
}

@INPROCEEDINGS{Germain2015-gs,
	title     = "{MADE}: Masked Autoencoder for Distribution Estimation",
	booktitle = "Proceedings of the 32nd International Conference on Machine
	Learning",
	author    = "Germain, Mathieu and Gregor, Karol and Murray, Iain and
	Larochelle, Hugo",
	editor    = "Bach, Francis and Blei, David",
	abstract  = "There has been a lot of recent interest in designing neural
	network models to estimate a distribution from a set of
	examples. We introduce a simple modification for autoencoder
	neural networks that yields powerful generative models. Our
	method masks the autoencoder's parameters to respect
	autoregressive constraints: each input is reconstructed only
	from previous inputs in a given ordering. Constrained this way,
	the autoencoder outputs can be interpreted as a set of
	conditional probabilities, and their product, the full joint
	probability. We can also train a single network that can
	decompose the joint probability in multiple different orderings.
	Our simple framework can be applied to multiple architectures,
	including deep ones. Vectorized implementations, such as on
	GPUs, are simple and fast. Experiments demonstrate that this
	approach is competitive with state-of-the-art tractable
	distribution estimators. At test time, the method is
	significantly faster and scales better than other autoregressive
	estimators.",
	publisher = "PMLR",
	volume    =  37,
	pages     = "881--889",
	series    = "Proceedings of Machine Learning Research",
	year      =  2015,
	url       = "https://proceedings.mlr.press/v37/germain15.html",
	file      = "All Papers/G/Germain et al. 2015 - MADE - Masked Autoencoder for Distribution Estimation.pdf",
	address   = "Lille, France"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Papamakarios2017-uh,
	title     = "Masked autoregressive flow for density estimation",
	author    = "{Papamakarios} and {Pavlakou} and {others}",
	abstract  = "Autoregressive models are among the best performing neural
	density estimators. We describe an approach for increasing the
	flexibility of an autoregressive model, based on modelling the
	random numbers that the model uses internally when generating
	data. By constructing a stack of autoregressive models, each
	modelling the random numbers of the next model in the stack, we
	obtain a type of normalizing flow suitable for density
	estimation, which we call Masked Autoregressive Flow. This type
	of flow is closely related to Inverse …",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2017,
	url       = "https://proceedings.neurips.cc/paper/2017/hash/6c1da886822c67822bcf3679d04369fa-Abstract.html",
	file      = "All Papers/P/Papamakarios et al. 2017 - Masked autoregressive flow for density estimation.pdf",
	keywords  = "Flow",
	issn      = "1049-5258"
}

@INPROCEEDINGS{Ramesh2021-kl,
	title     = "{Zero-Shot} {Text-to-Image} Generation",
	booktitle = "Proceedings of the 38th International Conference on Machine
	Learning",
	author    = "Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray,
	Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and
	Sutskever, Ilya",
	editor    = "Meila, Marina and Zhang, Tong",
	abstract  = "Text-to-image generation has traditionally focused on finding
	better modeling assumptions for training on a fixed dataset.
	These assumptions might involve complex architectures, auxiliary
	losses, or side information such as object part labels or
	segmentation masks supplied during training. We describe a
	simple approach for this task based on a transformer that
	autoregressively models the text and image tokens as a single
	stream of data. With sufficient data and scale, our approach is
	competitive with previous domain-specific models when evaluated
	in a zero-shot fashion.",
	publisher = "PMLR",
	volume    =  139,
	pages     = "8821--8831",
	series    = "Proceedings of Machine Learning Research",
	year      =  2021,
	url       = "https://proceedings.mlr.press/v139/ramesh21a.html",
	file      = "All Papers/R/Ramesh et al. 2021 - Zero-Shot Text-to-Image Generation.pdf"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kingma2018-cd,
	title     = "Glow: Generative flow with invertible 1x1 convolutions",
	author    = "{Kingma} and {Dhariwal}",
	abstract  = "Flow-based generative models are conceptually attractive due to
	tractability of the exact log- likelihood, tractability of exact
	latent-variable inference, and parallelizability of both
	training and synthesis. In this paper we propose Glow, a simple
	type of generative flow using invertible 1x1 convolution. Using
	our method we demonstrate a significant improvement in
	log-likelihood and qualitative sample quality. Perhaps most
	strikingly, we demonstrate that a generative model optimized
	towards the plain log-likelihood objective is capable of
	efficient …",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2018,
	url       = "https://proceedings.neurips.cc/paper/2018/hash/d139db6a236200b21cc7f752979132d0-Abstract.html",
	file      = "All Papers/K/Kingma and Dhariwal 2018 - Glow - Generative flow with invertible 1x1 convolutions.pdf",
	keywords  = "Flow",
	issn      = "1049-5258"
}

@ARTICLE{Nichol2021-kr,
	title         = "{GLIDE}: Towards Photorealistic Image Generation and Editing
	with {Text-Guided} Diffusion Models",
	author        = "Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and
	Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and
	Sutskever, Ilya and Chen, Mark",
	abstract      = "Diffusion models have recently been shown to generate
	high-quality synthetic images, especially when paired with a
	guidance technique to trade off diversity for fidelity. We
	explore diffusion models for the problem of text-conditional
	image synthesis and compare two different guidance
	strategies: CLIP guidance and classifier-free guidance. We
	find that the latter is preferred by human evaluators for
	both photorealism and caption similarity, and often produces
	photorealistic samples. Samples from a 3.5 billion parameter
	text-conditional diffusion model using classifier-free
	guidance are favored by human evaluators to those from
	DALL-E, even when the latter uses expensive CLIP reranking.
	Additionally, we find that our models can be fine-tuned to
	perform image inpainting, enabling powerful text-driven
	image editing. We train a smaller model on a filtered
	dataset and release the code and weights at
	https://github.com/openai/glide-text2im.",
	month         =  "20~" # dec,
	year          =  2021,
	url           = "http://arxiv.org/abs/2112.10741",
	file          = "All Papers/N/Nichol et al. 2021 - GLIDE - Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models.pdf",
	archivePrefix = "arXiv",
	eprint        = "2112.10741",
	primaryClass  = "cs.CV",
	arxivid       = "2112.10741"
}

@ARTICLE{Shaham2019-xp,
	title     = "Singan: Learning a generative model from a single natural image",
	author    = "{Shaham} and {Dekel} and {Michaeli}",
	abstract  = "… We propose SinGAN --a new unconditional generative model
	trained on a … We introduce SinGAN , an unconditional generative
	model that can be learned from a single natural image . …",
	journal   = "Proceedings of the IEEE",
	publisher = "openaccess.thecvf.com",
	year      =  2019,
	url       = "http://openaccess.thecvf.com/content_ICCV_2019/html/Shaham_SinGAN_Learning_a_Generative_Model_From_a_Single_Natural_Image_ICCV_2019_paper.html",
	file      = "All Papers/S/Shaham et al. 2019 - Singan - Learning a generative model from a single natural image.pdf"
}

@ARTICLE{Mandt2017-nl,
	title         = "Stochastic Gradient Descent as Approximate Bayesian
	Inference",
	author        = "Mandt, Stephan and Hoffman, Matthew D and Blei, David M",
	abstract      = "Stochastic Gradient Descent with a constant learning rate
	(constant SGD) simulates a Markov chain with a stationary
	distribution. With this perspective, we derive several new
	results. (1) We show that constant SGD can be used as an
	approximate Bayesian posterior inference algorithm.
	Specifically, we show how to adjust the tuning parameters of
	constant SGD to best match the stationary distribution to a
	posterior, minimizing the Kullback-Leibler divergence
	between these two distributions. (2) We demonstrate that
	constant SGD gives rise to a new variational EM algorithm
	that optimizes hyperparameters in complex probabilistic
	models. (3) We also propose SGD with momentum for sampling
	and show how to adjust the damping coefficient accordingly.
	(4) We analyze MCMC algorithms. For Langevin Dynamics and
	Stochastic Gradient Fisher Scoring, we quantify the
	approximation errors due to finite learning rates. Finally
	(5), we use the stochastic process perspective to give a
	short proof of why Polyak averaging is optimal. Based on
	this idea, we propose a scalable approximate MCMC algorithm,
	the Averaged Stochastic Gradient Sampler.",
	month         =  "13~" # apr,
	year          =  2017,
	url           = "http://arxiv.org/abs/1704.04289",
	file          = "All Papers/M/Mandt et al. 2017 - Stochastic Gradient Descent as Approximate Bayesian Inference.pdf",
	archivePrefix = "arXiv",
	eprint        = "1704.04289",
	primaryClass  = "stat.ML",
	arxivid       = "1704.04289"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Song2019-lg,
	title     = "Generative modeling by estimating gradients of the data
	distribution",
	author    = "{Song} and {Ermon}",
	abstract  = "We introduce a new generative model where samples are produced
	via Langevin dynamics using gradients of the data distribution
	estimated with score matching. Because gradients can be
	ill-defined and hard to estimate when the data resides on
	low-dimensional manifolds, we perturb the data with different
	levels of Gaussian noise, and jointly estimate the corresponding
	scores, ie, the vector fields of gradients of the perturbed data
	distribution for all noise levels. For sampling, we propose an
	annealed Langevin dynamics where we use …",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2019,
	url       = "https://proceedings.neurips.cc/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html",
	file      = "All Papers/S/Song and Ermon 2019 - Generative modeling by estimating gradients of the data distribution.pdf",
	issn      = "1049-5258"
}

@ARTICLE{Song2020-pp,
	title         = "{Score-Based} Generative Modeling through Stochastic
	Differential Equations",
	author        = "Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P
	and Kumar, Abhishek and Ermon, Stefano and Poole, Ben",
	abstract      = "Creating noise from data is easy; creating data from noise
	is generative modeling. We present a stochastic differential
	equation (SDE) that smoothly transforms a complex data
	distribution to a known prior distribution by slowly
	injecting noise, and a corresponding reverse-time SDE that
	transforms the prior distribution back into the data
	distribution by slowly removing the noise. Crucially, the
	reverse-time SDE depends only on the time-dependent gradient
	field (\textbackslashaka, score) of the perturbed data
	distribution. By leveraging advances in score-based
	generative modeling, we can accurately estimate these scores
	with neural networks, and use numerical SDE solvers to
	generate samples. We show that this framework encapsulates
	previous approaches in score-based generative modeling and
	diffusion probabilistic modeling, allowing for new sampling
	procedures and new modeling capabilities. In particular, we
	introduce a predictor-corrector framework to correct errors
	in the evolution of the discretized reverse-time SDE. We
	also derive an equivalent neural ODE that samples from the
	same distribution as the SDE, but additionally enables exact
	likelihood computation, and improved sampling efficiency. In
	addition, we provide a new way to solve inverse problems
	with score-based models, as demonstrated with experiments on
	class-conditional generation, image inpainting, and
	colorization. Combined with multiple architectural
	improvements, we achieve record-breaking performance for
	unconditional image generation on CIFAR-10 with an Inception
	score of 9.89 and FID of 2.20, a competitive likelihood of
	2.99 bits/dim, and demonstrate high fidelity generation of
	1024 x 1024 images for the first time from a score-based
	generative model.",
	month         =  "26~" # nov,
	year          =  2020,
	url           = "http://arxiv.org/abs/2011.13456",
	file          = "All Papers/S/Song et al. 2020 - Score-Based Generative Modeling through Stochastic Differential Equations.pdf",
	archivePrefix = "arXiv",
	eprint        = "2011.13456",
	primaryClass  = "cs.LG",
	arxivid       = "2011.13456"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Dhariwal2021-kn,
	title     = "Diffusion models beat gans on image synthesis",
	author    = "{Dhariwal} and {Nichol}",
	abstract  = "… diffusion models can achieve image sample quality superior to
	the current state-of-the-art generative models … For conditional
	image synthesis , we further improve sample quality with …",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2021,
	url       = "https://proceedings.neurips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html",
	file      = "All Papers/D/Dhariwal and Nichol 2021 - Diffusion models beat gans on image synthesis.pdf",
	issn      = "1049-5258"
}

@ARTICLE{Esser2021-gg,
	title     = "Taming transformers for high-resolution image synthesis",
	author    = "{Esser} and {Rombach} and {Ommer}",
	abstract  = "Designed to learn long-range interactions on sequential data,
	transformers continue to show state-of-the-art results on a wide
	variety of tasks. In contrast to CNNs, they contain no inductive
	bias that prioritizes local interactions. This makes them
	expressive, but also computationally infeasible for long
	sequences, such as high-resolution images. We demonstrate how
	combining the effectiveness of the inductive bias of CNNs with
	the expressivity of transformers enables them to model and
	thereby synthesize high-resolution …",
	journal   = "Proceedings of the IEEE",
	publisher = "openaccess.thecvf.com",
	year      =  2021,
	url       = "https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html?ref=https://githubhelp.com",
	file      = "All Papers/E/Esser et al. 2021 - Taming transformers for high-resolution image synthesis.pdf"
}

@ARTICLE{Du2021-zg,
	title     = "Beyond Strictly Proper Scoring Rules: The Importance of Being
	Local",
	author    = "Du, Hailiang",
	abstract  = "Abstract The evaluation of probabilistic forecasts plays a
	central role both in the interpretation and in the use of
	forecast systems and their development. Probabilistic scores
	(scoring rules) provide statistical measures to assess the
	quality of probabilistic forecasts. Often, many probabilistic
	forecast systems are available while evaluations of their
	performance are not standardized, with different scoring rules
	being used to measure different aspects of forecast performance.
	Even when the discussion is restricted to strictly proper
	scoring rules, there remains considerable variability between
	them; indeed strictly proper scoring rules need not rank
	competing forecast systems in the same order when none of these
	systems are perfect. The locality property is explored to
	further distinguish scoring rules. The nonlocal strictly proper
	scoring rules considered are shown to have a property that can
	produce ``unfortunate'' evaluations, particularly the fact that
	the continuous rank probability score prefers the outcome close
	to the median of the forecast distribution regardless of the
	probability mass assigned to the value at/near the median raises
	concern to its use. The only local strictly proper scoring rule,
	the logarithmic score, has direct interpretations in terms of
	probabilities and bits of information. The nonlocal strictly
	proper scoring rules, on the other hand, lack meaningful direct
	interpretation for decision support. The logarithmic score is
	also shown to be invariant under smooth transformation of the
	forecast variable, while the nonlocal strictly proper scoring
	rules considered may, however, change their preferences due to
	the transformation. It is therefore suggested that the
	logarithmic score always be included in the evaluation of
	probabilistic forecasts.",
	journal   = "Weather and Forecasting",
	publisher = "American Meteorological Society",
	volume    =  36,
	number    =  2,
	pages     = "457--468",
	month     =  "1~" # apr,
	year      =  2021,
	url       = "https://journals.ametsoc.org/view/journals/wefo/36/2/WAF-D-19-0205.1.xml",
	file      = "All Papers/D/Du 2021 - Beyond Strictly Proper Scoring Rules - The Importance of Being Local.pdf",
	keywords  = "Theory",
	language  = "en",
	issn      = "0882-8156, 1520-0434",
	doi       = "10.1175/WAF-D-19-0205.1"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Figurnov2018-pe,
	title     = "Implicit reparameterization gradients",
	author    = "{Figurnov} and {Mohamed} and {others}",
	abstract  = "… 3 Implicit reparameterization gradients We propose an
	alternative way of computing the reparameterization gradient
	that avoids the inversion of the standardization function. We
	start …",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2018,
	url       = "https://proceedings.neurips.cc/paper/7326-implicit-reparameterization-gradients",
	file      = "All Papers/F/Figurnov et al. 2018 - Implicit reparameterization gradients.pdf",
	issn      = "1049-5258"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Brando2019-ax,
	title     = "Modelling heterogeneous distributions with an uncountable
	mixture of asymmetric laplacians",
	author    = "{Brando} and {Rodriguez} and {Vitria} and {others}",
	abstract  = "In regression tasks, aleatoric uncertainty is commonly addressed
	by considering a parametric distribution of the output variable,
	which is based on strong assumptions such as symmetry,
	unimodality or by supposing a restricted shape. These
	assumptions are too limited in scenarios where complex shapes,
	strong skews or multiple modes are present. In this paper, we
	propose a generic deep learning framework that learns an
	Uncountable Mixture of Asymmetric Laplacians (UMAL), which will
	allow us to estimate heterogeneous …",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2019,
	url       = "https://proceedings.neurips.cc/paper/2019/hash/d80126524c1e9641333502c664fc6ca1-Abstract.html",
	file      = "All Papers/B/Brando et al. 2019 - Modelling heterogeneous distributions with an uncountable mixture of asymmetric laplacians.pdf",
	issn      = "1049-5258"
}

@INPROCEEDINGS{Rosca2020-oj,
	title     = "A case for new neural network smoothness constraints",
	booktitle = "{Proceedings on ``I Can't Believe It's Not Better!'' at
	{NeurIPS} Workshops}",
	author    = "Rosca, Mihaela and Weber, Theophane and Gretton, Arthur and
	Mohamed, Shakir",
	editor    = "Zosa Forde, Jessica and Ruiz, Francisco and Pradier, Melanie F
	and Schein, Aaron",
	abstract  = "How sensitive should machine learning models be to input
	changes? We tackle the question of model smoothness and show
	that it is a useful inductive bias which aids generalization,
	adversarial robustness, generative modeling and reinforcement
	learning. We explore current methods of imposing smoothness
	constraints and observe they lack the flexibility to adapt to
	new tasks, they don't account for data modalities, they interact
	with losses, architectures and optimization in ways not yet
	fully understood. We conclude that new advances in the field are
	hinging on finding ways to incorporate data, tasks and learning
	into our definitions of smoothness.",
	publisher = "PMLR",
	volume    =  137,
	pages     = "21--32",
	series    = "Proceedings of Machine Learning Research",
	month     =  "12~" # dec,
	year      =  2020,
	url       = "https://proceedings.mlr.press/v137/rosca20a.html",
	file      = "All Papers/R/Rosca et al. 2020 - A case for new neural network smoothness constraints.pdf"
}

@ARTICLE{Song2018-ql,
	title         = "Generative Modeling by Inclusive Neural Random Fields with
	Applications in Image Generation and Anomaly Detection",
	author        = "Song, Yunfu and Ou, Zhijian",
	abstract      = "Neural random fields (NRFs), referring to a class of
	generative models that use neural networks to implement
	potential functions in random fields (a.k.a. energy-based
	models), are not new but receive less attention with slow
	progress. Different from various directed graphical models
	such as generative adversarial networks (GANs), NRFs provide
	an interesting family of undirected graphical models for
	generative modeling. In this paper we propose a new
	approach, the inclusive-NRF approach, to learning NRFs for
	continuous data (e.g. images), by introducing
	inclusive-divergence minimized auxiliary generators and
	developing stochastic gradient sampling in an augmented
	space. Based on the new approach, specific inclusive-NRF
	models are developed and thoroughly evaluated in two
	important generative modeling applications - image
	generation and anomaly detection. The proposed models
	consistently improve over state-of-the-art results in both
	applications. Remarkably, in addition to superior sample
	generation, one additional benefit of our inclusive-NRF
	approach is that, unlike GANs, it can directly provide
	(unnormalized) density estimate for sample evaluation. With
	these contributions and results, this paper significantly
	advances the learning and applications of NRFs to a new
	level, both theoretically and empirically, which have never
	been obtained before.",
	month         =  "1~" # jun,
	year          =  2018,
	url           = "http://arxiv.org/abs/1806.00271",
	file          = "All Papers/S/Song and Ou 2018 - Generative Modeling by Inclusive Neural Random Fields with Applications in Image Generation and Anomaly Detection.pdf",
	archivePrefix = "arXiv",
	eprint        = "1806.00271",
	primaryClass  = "stat.ML",
	arxivid       = "1806.00271"
}

@INPROCEEDINGS{Kuleshov2022-pv,
	title     = "Calibrated and Sharp Uncertainties in Deep Learning via Density
	Estimation",
	booktitle = "Proceedings of the 39th International Conference on Machine
	Learning",
	author    = "Kuleshov, Volodymyr and Deshpande, Shachi",
	editor    = "Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and
	Szepesvari, Csaba and Niu, Gang and Sabato, Sivan",
	abstract  = "Accurate probabilistic predictions can be characterized by two
	properties---calibration and sharpness. However, standard
	maximum likelihood training yields models that are poorly
	calibrated and thus inaccurate---a 90\% confidence interval
	typically does not contain the true outcome 90\% of the time.
	This paper argues that calibration is important in practice and
	is easy to maintain by performing low-dimensional density
	estimation. We introduce a simple training procedure based on
	recalibration that yields calibrated models without sacrificing
	overall performance; unlike previous approaches, ours ensures
	the most general property of distribution calibration and
	applies to any model, including neural networks. We formally
	prove the correctness of our procedure assuming that we can
	estimate densities in low dimensions and we establish uniform
	convergence bounds. Our results yield empirical performance
	improvements on linear and deep Bayesian models and suggest that
	calibration should be increasingly leveraged across machine
	learning.",
	publisher = "PMLR",
	volume    =  162,
	pages     = "11683--11693",
	series    = "Proceedings of Machine Learning Research",
	year      =  2022,
	url       = "https://proceedings.mlr.press/v162/kuleshov22a.html",
	file      = "All Papers/K/Kuleshov and Deshpande 2022 - Calibrated and Sharp Uncertainties in Deep Learning via Density Estimation.pdf"
}

@ARTICLE{Deshpande2021-xi,
	title         = "Calibration Improves Bayesian Optimization",
	author        = "Deshpande, Shachi and Kuleshov, Volodymyr",
	abstract      = "Bayesian optimization is a procedure that allows obtaining
	the global optimum of black-box functions and that is useful
	in applications such as hyper-parameter optimization.
	Uncertainty estimates over the shape of the objective
	function are instrumental in guiding the optimization
	process. However, these estimates can be inaccurate if the
	objective function violates assumptions made within the
	underlying model (e.g., Gaussianity). We propose a simple
	algorithm to calibrate the uncertainty of posterior
	distributions over the objective function as part of the
	Bayesian optimization process. We show that by improving the
	uncertainty estimates of the posterior distribution with
	calibration, Bayesian optimization makes better decisions
	and arrives at the global optimum in fewer steps. We show
	that this technique improves the performance of Bayesian
	optimization on standard benchmark functions and
	hyperparameter optimization tasks.",
	month         =  "8~" # dec,
	year          =  2021,
	url           = "http://arxiv.org/abs/2112.04620",
	file          = "All Papers/D/Deshpande and Kuleshov 2021 - Calibration Improves Bayesian Optimization.pdf",
	archivePrefix = "arXiv",
	eprint        = "2112.04620",
	primaryClass  = "cs.LG",
	arxivid       = "2112.04620"
}

@ARTICLE{Cervera2021-tb,
	title     = "Uncertainty estimation under model misspecification in neural
	network regression",
	author    = "Cervera, Maria R and D{\"a}twyler, Rafael and D'Angelo,
	Francesco and Keurti, Hamza and Grewe, Benjamin F and Henning,
	Christian",
	abstract  = "Uncertainty estimation under model misspecification in neural
	network regression - Research Collection … Uncertainty
	estimation under model misspecification in neural network
	regression …",
	journal   = "Workshop NeurIPS, 2021",
	publisher = "research-collection.ethz.ch",
	year      =  2021,
	url       = "https://www.research-collection.ethz.ch/handle/20.500.11850/524731",
	file      = "All Papers/C/Cervera et al. 2021 - Uncertainty estimation under model misspecification in neural network regression.pdf"
}

@ARTICLE{Elflein2021-kx,
	title         = "On Out-of-distribution Detection with Energy-based Models",
	author        = "Elflein, Sven and Charpentier, Bertrand and Z{\"u}gner,
	Daniel and G{\"u}nnemann, Stephan",
	abstract      = "Several density estimation methods have shown to fail to
	detect out-of-distribution (OOD) samples by assigning higher
	likelihoods to anomalous data. Energy-based models (EBMs)
	are flexible, unnormalized density models which seem to be
	able to improve upon this failure mode. In this work, we
	provide an extensive study investigating OOD detection with
	EBMs trained with different approaches on tabular and image
	data and find that EBMs do not provide consistent
	advantages. We hypothesize that EBMs do not learn semantic
	features despite their discriminative structure similar to
	Normalizing Flows. To verify this hypotheses, we show that
	supervision and architectural restrictions improve the OOD
	detection of EBMs independent of the training approach.",
	month         =  "3~" # jul,
	year          =  2021,
	url           = "http://arxiv.org/abs/2107.08785",
	file          = "All Papers/E/Elflein et al. 2021 - On Out-of-distribution Detection with Energy-based Models.pdf",
	archivePrefix = "arXiv",
	eprint        = "2107.08785",
	primaryClass  = "cs.LG",
	arxivid       = "2107.08785"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Cui2020-dv,
	title     = "Calibrated reliable regression using maximum mean discrepancy",
	author    = "{Cui} and {Hu} and {Zhu}",
	abstract  = "Accurate quantification of uncertainty is crucial for real-world
	applications of machine learning. However, modern deep neural
	networks still produce unreliable predictive uncertainty, often
	yielding over-confident predictions. In this paper, we are
	concerned with getting well-calibrated predictions in regression
	tasks. We propose the calibrated regression method using the
	maximum mean discrepancy by minimizing the kernel embedding
	measure. Theoretically, the calibration error of our method
	asymptotically converges to zero …",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2020,
	url       = "https://proceedings.neurips.cc/paper/2020/hash/c74c4bf0dad9cbae3d80faa054b7d8ca-Abstract.html",
	file      = "All Papers/C/Cui et al. 2020 - Calibrated reliable regression using maximum mean discrepancy.pdf",
	issn      = "1049-5258"
}

@ARTICLE{Ehm2016-mh,
	title       = "Of quantiles and expectiles: consistent scoring functions,
	Choquet representations and forecast rankings",
	author      = "Ehm, Werner and Gneiting, Tilmann and Jordan, Alexander and
	Kr{\"u}ger, Fabian",
	affiliation = "Heidelberger Institut f{\"u}r Theoretische Studien; Heidelberg
	Germany; Heidelberger Institut f{\"u}r Theoretische Studien;
	Heidelberg Germany; Karlsruher Institut f{\"u}r Technologie;
	Karlsruhe Germany",
	abstract    = "Summary In the practice of point prediction, it is desirable
	that forecasters receive a directive in the form of a
	statistical functional. For example, forecasters might be
	asked to report the mean or a quantile of their predictive
	distributions. When evaluating and comparing competing
	forecasts, it is then critical that the scoring function used
	for these purposes be consistent for the functional at hand,
	in the sense that the expected score is minimized when
	following the directive. We show that any scoring function
	that is consistent for a quantile or an expectile functional
	can be represented as a mixture of elementary or extremal
	scoring functions that form a linearly parameterized family.
	Scoring functions for the mean value and probability forecasts
	of binary events constitute important examples. The extremal
	scoring functions admit appealing economic interpretations of
	quantiles and expectiles in the context of betting and
	investment problems. The Choquet-type mixture representations
	give rise to simple checks of whether a forecast dominates
	another in the sense that it is preferable under any
	consistent scoring function. In empirical settings it suffices
	to compare the average scores for only a finite number of
	extremal elements. Plots of the average scores with respect to
	the extremal scoring functions, which we call Murphy diagrams,
	permit detailed comparisons of the relative merits of
	competing forecasts.",
	journal     = "Journal of the Royal Statistical Society. Series B,
	Statistical methodology",
	publisher   = "Wiley",
	volume      =  78,
	number      =  3,
	pages       = "505--562",
	month       =  jun,
	year        =  2016,
	url         = "https://onlinelibrary.wiley.com/doi/10.1111/rssb.12154",
	file        = "All Papers/E/Ehm et al. 2016 - Of quantiles and expectiles - consistent scoring functions, Choquet representations and forecast rankings.pdf",
	language    = "en",
	issn        = "1369-7412, 1467-9868",
	doi         = "10.1111/rssb.12154"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Sesia2021-tn,
	title     = "Conformal prediction using conditional histograms",
	author    = "{Sesia} and {Romano}",
	abstract  = "This paper develops a conformal method to compute prediction
	intervals for non-parametric regression that can automatically
	adapt to skewed data. Leveraging black-box machine learning
	algorithms to estimate the conditional distribution of the
	outcome using histograms, it translates their output into the
	shortest prediction intervals with approximate conditional
	coverage. The resulting prediction intervals provably have
	marginal coverage in finite samples, while asymptotically
	achieving conditional coverage and optimal length if the black …",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2021,
	url       = "https://proceedings.neurips.cc/paper/2021/hash/31b3b31a1c2f8a370206f111127c0dbd-Abstract.html",
	file      = "All Papers/S/Sesia and Romano 2021 - Conformal prediction using conditional histograms.pdf",
	issn      = "1049-5258"
}

@ARTICLE{Park2019-iw,
	title     = "Semantic image synthesis with spatially-adaptive normalization",
	author    = "{Park} and {Liu} and {Wang} and {others}",
	abstract  = "We propose spatially-adaptive normalization, a simple but
	effective layer for synthesizing photorealistic images given an
	input semantic layout. Previous methods directly feed the
	semantic layout as input to the network, forcing the network to
	memorize the information throughout all the layers. Instead, we
	propose using the input layout for modulating the activations in
	normalization layers through a spatially-adaptive, learned
	affine transformation. Experiments on several challenging
	datasets demonstrate the superiority of …",
	journal   = "Proceedings of the IEEE",
	publisher = "openaccess.thecvf.com",
	year      =  2019,
	url       = "http://openaccess.thecvf.com/content_CVPR_2019/html/Park_Semantic_Image_Synthesis_With_Spatially-Adaptive_Normalization_CVPR_2019_paper.html",
	file      = "All Papers/P/Park et al. 2019 - Semantic image synthesis with spatially-adaptive normalization.pdf"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Tagasovska2019-ts,
	title     = "Single-model uncertainties for deep learning",
	author    = "{Tagasovska} and {Lopez-Paz}",
	abstract  = "We provide single-model estimates of aleatoric and epistemic
	uncertainty for deep neural networks. To estimate aleatoric
	uncertainty, we propose Simultaneous Quantile Regression (SQR),
	a loss function to learn all the conditional quantiles of a
	given target variable. These quantiles can be used to compute
	well-calibrated prediction intervals. To estimate epistemic
	uncertainty, we propose Orthonormal Certificates (OCs), a
	collection of diverse non-constant functions that map all
	training samples to zero. These certificates map
	out-of-distribution …",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2019,
	url       = "https://proceedings.neurips.cc/paper/2019/hash/73c03186765e199c116224b68adc5fa0-Abstract.html",
	file      = "All Papers/T/Tagasovska and Lopez-Paz 2019 - Single-model uncertainties for deep learning.pdf",
	keywords  = "Epistemic;Quantiles",
	issn      = "1049-5258"
}

@INPROCEEDINGS{Huang2018-rj,
	title     = "Neural Autoregressive Flows",
	booktitle = "Proceedings of the 35th International Conference on Machine
	Learning",
	author    = "Huang, Chin-Wei and Krueger, David and Lacoste, Alexandre and
	Courville, Aaron",
	editor    = "Dy, Jennifer and Krause, Andreas",
	abstract  = "Normalizing flows and autoregressive models have been
	successfully combined to produce state-of-the-art results in
	density estimation, via Masked Autoregressive Flows (MAF)
	(Papamakarios et al., 2017), and to accelerate state-of-the-art
	WaveNet-based speech synthesis to 20x faster than real-time
	(Oord et al., 2017), via Inverse Autoregressive Flows (IAF)
	(Kingma et al., 2016). We unify and generalize these approaches,
	replacing the (conditionally) affine univariate transformations
	of MAF/IAF with a more general class of invertible univariate
	transformations expressed as monotonic neural networks. We
	demonstrate that the proposed neural autoregressive flows (NAF)
	are universal approximators for continuous probability
	distributions, and their greater expressivity allows them to
	better capture multimodal target distributions. Experimentally,
	NAF yields state-of-the-art performance on a suite of density
	estimation tasks and outperforms IAF in variational autoencoders
	trained on binarized MNIST.",
	publisher = "PMLR",
	volume    =  80,
	pages     = "2078--2087",
	series    = "Proceedings of Machine Learning Research",
	year      =  2018,
	url       = "https://proceedings.mlr.press/v80/huang18d.html",
	file      = "All Papers/H/Huang et al. 2018 - Neural Autoregressive Flows.pdf"
}

@ARTICLE{Abdar2021-zq,
	title    = "A review of uncertainty quantification in deep learning:
	Techniques, applications and challenges",
	author   = "Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and
	Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and
	Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya,
	U Rajendra and Makarenkov, Vladimir and Nahavandi, Saeid",
	abstract = "Uncertainty quantification (UQ) methods play a pivotal role in
	reducing the impact of uncertainties during both optimization and
	decision making processes. They have been applied to solve a
	variety of real-world problems in science and engineering.
	Bayesian approximation and ensemble learning techniques are two
	widely-used types of uncertainty quantification (UQ) methods. In
	this regard, researchers have proposed different UQ methods and
	examined their performance in a variety of applications such as
	computer vision (e.g., self-driving cars and object detection),
	image processing (e.g., image restoration), medical image
	analysis (e.g., medical image classification and segmentation),
	natural language processing (e.g., text classification, social
	media texts and recidivism risk-scoring), bioinformatics, etc.
	This study reviews recent advances in UQ methods used in deep
	learning, investigates the application of these methods in
	reinforcement learning, and highlights fundamental research
	challenges and directions associated with UQ.",
	journal  = "An international journal on information fusion",
	volume   =  76,
	pages    = "243--297",
	month    =  dec,
	year     =  2021,
	url      = "https://www.sciencedirect.com/science/article/pii/S1566253521001081",
	file     = "All Papers/A/Abdar et al. 2021 - A review of uncertainty quantification in deep learning - Techniques, applications and challenges.pdf",
	keywords = "Artificial intelligence; Uncertainty quantification; Deep
	learning; Machine learning; Bayesian statistics; Ensemble
	learning;Uncertainty quantification;Review",
	issn     = "1566-2535",
	doi      = "10.1016/j.inffus.2021.05.008"
}

@ARTICLE{LeCun2006-kb,
	title   = "A tutorial on energy-based learning",
	author  = "{LeCun} and {Chopra} and {Hadsell} and {Ranzato} and {Huang}",
	journal = "Predicting structured data",
	volume  =  1,
	number  =  0,
	year    =  2006
}

@ARTICLE{Koenker1978-sj,
	title     = "Regression Quantiles",
	author    = "Koenker, Roger and Bassett, Gilbert",
	abstract  = "[A simple minimization problem yielding the ordinary sample
	quantiles in the location model is shown to generalize naturally
	to the linear model generating a new class of statistics we term
	``regression quantiles.'' The estimator which minimizes the sum
	of absolute residuals is an important special case. Some
	equivariance properties and the joint asymptotic distribution of
	regression quantiles are established. These results permit a
	natural generalization of the linear model of certain well-known
	robust estimators of location. Estimators are suggested, which
	have comparable efficiency to least squares for Gaussian linear
	models while substantially out-performing the least-squares
	estimator over a wide class of non-Gaussian error
	distributions.]",
	journal   = "Econometrica: journal of the Econometric Society",
	publisher = "[Wiley, Econometric Society]",
	volume    =  46,
	number    =  1,
	pages     = "33--50",
	year      =  1978,
	url       = "http://www.jstor.org/stable/1913643",
	file      = "All Papers/K/Koenker and Bassett 1978 - Regression Quantiles.pdf",
	issn      = "0012-9682, 1468-0262",
	doi       = "10.2307/1913643"
}

@ARTICLE{Keelin2016-jm,
	title     = "The Metalog Distributions",
	author    = "Keelin, Thomas W",
	abstract  = "The metalog distributions constitute a new system of continuous
	univariate probability distributions designed for flexibility,
	simplicity, and ease/speed of use in practice. The system is
	comprised of unbounded, semibounded, and bounded distributions,
	each of which offers nearly unlimited shape flexibility compared
	to previous systems of distributions. Explicit shape-flexibility
	comparisons are provided. Unlike other distributions that
	require nonlinear optimization for parameter estimation, the
	metalog quantile functions and probability density functions
	have simple closed-form expressions that are quantile
	parameterized linearly by cumulative-distribution-function data.
	Applications in fish biology and hydrology show how metalogs may
	aid data and distribution research by imposing fewer shape
	constraints than other commonly used distributions. Applications
	in decision analysis show how the metalog system can be
	specified with three assessed quantiles, how it facilities Monte
	Carlo simulation, and how applying it aided an actual decision
	that would have been made wrongly based on commonly used
	discrete methods. This work is licensed under a Creative Commons
	Attribution 4.0 International License. You are free to copy,
	distribute, transmit and adapt this work, but you must attribute
	this work as ?Decision Analysis. Copyright ? 2016 The Author(s).
	https://doi.org/10.1287/deca.2016.0338, used under a Creative
	Commons Attribution License:
	https://creativecommons.org/licenses/by/4.0/.?",
	journal   = "Decision Analysis",
	publisher = "INFORMS",
	volume    =  13,
	number    =  4,
	pages     = "243--277",
	month     =  "1~" # dec,
	year      =  2016,
	url       = "https://doi.org/10.1287/deca.2016.0338",
	file      = "All Papers/K/Keelin 2016 - The Metalog Distributions.pdf",
	issn      = "1545-8490",
	doi       = "10.1287/deca.2016.0338"
}

@ARTICLE{Zhou2021-hx,
	title       = "Estimating Uncertainty Intervals from Collaborating Networks",
	author      = "Zhou, Tianhui and Li, Yitong and Wu, Yuan and Carlson, David",
	affiliation = "Department of Biostatistics and Bioinformatics, Duke
	University, Durham, NC 27705, USA. Department of Electrical
	and Computer Engineering, Duke University, Durham, NC 27705,
	USA. Departments of Civil and Environmental Engineering,
	Biostatistics and Bioinformatics, Electrical and Computer
	Engineering, and Computer Science, Duke University, Durham, NC
	27705, USA.",
	abstract    = "Effective decision making requires understanding the
	uncertainty inherent in a prediction. In regression, this
	uncertainty can be estimated by a variety of methods; however,
	many of these methods are laborious to tune, generate
	overconfident uncertainty intervals, or lack sharpness (give
	imprecise intervals). We address these challenges by proposing
	a novel method to capture predictive distributions in
	regression by defining two neural networks with two distinct
	loss functions. Specifically, one network approximates the
	cumulative distribution function, and the second network
	approximates its inverse. We refer to this method as
	Collaborating Networks (CN). Theoretical analysis demonstrates
	that a fixed point of the optimization is at the idealized
	solution, and that the method is asymptotically consistent to
	the ground truth distribution. Empirically, learning is
	straightforward and robust. We benchmark CN against several
	common approaches on two synthetic and six real-world
	datasets, including forecasting A1c values in diabetic
	patients from electronic health records, where uncertainty is
	critical. In the synthetic data, the proposed approach
	essentially matches ground truth. In the real-world datasets,
	CN improves results on many performance metrics, including
	log-likelihood estimates, mean absolute errors, coverage
	estimates, and prediction interval widths.",
	journal     = "Journal of machine learning research: JMLR",
	volume      =  22,
	month       =  jan,
	year        =  2021,
	url         = "https://www.ncbi.nlm.nih.gov/pubmed/35754923",
	file        = "All Papers/Z/Zhou et al. 2021 - Estimating Uncertainty Intervals from Collaborating Networks.pdf",
	keywords    = "calibration; conditional distributions; consistency; neural
	networks; uncertainty estimation",
	language    = "en",
	issn        = "1532-4435",
	pmid        = "35754923",
	pmc         = "PMC9231643"
}

@INPROCEEDINGS{Graves2011-nq,
	title     = "Practical Variational Inference for Neural Networks",
	booktitle = "Advances in Neural Information Processing Systems",
	author    = "Graves, Alex",
	editor    = "Shawe-Taylor, J and Zemel, R and Bartlett, P and Pereira, F and
	Weinberger, K Q",
	publisher = "Curran Associates, Inc.",
	volume    =  24,
	year      =  2011,
	url       = "https://proceedings.neurips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf",
	file      = "All Papers/G/Graves 2011 - Practical Variational Inference for Neural Networks.pdf"
}

@ARTICLE{Fortuin2022-lo,
	title       = "Priors in Bayesian deep learning: A review",
	author      = "Fortuin, Vincent",
	affiliation = "Department of Computer Science ETH Z{\"u}rich Z{\"u}rich
	Switzerland",
	abstract    = "Summary While the choice of prior is one of the most critical
	parts of the Bayesian inference workflow, recent Bayesian deep
	learning models have often fallen back on vague priors, such
	as standard Gaussians. In this review, we highlight the
	importance of prior choices for Bayesian deep learning and
	present an overview of different priors that have been
	proposed for (deep) Gaussian processes, variational
	autoencoders and Bayesian neural networks. We also outline
	different methods of learning priors for these models from
	data. We hope to motivate practitioners in Bayesian deep
	learning to think more carefully about the prior specification
	for their models and to provide them with some inspiration in
	this regard.",
	journal     = "International statistical review = Revue internationale de
	statistique",
	publisher   = "Wiley",
	month       =  "11~" # may,
	year        =  2022,
	url         = "https://onlinelibrary.wiley.com/doi/10.1111/insr.12502",
	file        = "All Papers/F/Fortuin 2022 - Priors in Bayesian deep learning - A review.pdf",
	copyright   = "http://creativecommons.org/licenses/by-nc/4.0/",
	language    = "en",
	issn        = "0306-7734, 1751-5823",
	doi         = "10.1111/insr.12502"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Muller2019-gr,
	title     = "Neural Importance Sampling",
	author    = "M{\"u}ller, Thomas and Mcwilliams, Brian and Rousselle, Fabrice
	and Gross, Markus and Nov{\'a}k, Jan",
	abstract  = "We propose to use deep neural networks for generating samples in
	Monte Carlo integration. Our work is based on non-linear
	independent components estimation (NICE), which we extend in
	numerous ways to improve performance and enable its application
	to integration problems. First, we introduce
	piecewise-polynomial coupling transforms that greatly increase
	the modeling power of individual coupling layers. Second, we
	propose to preprocess the inputs of neural networks using
	one-blob encoding, which stimulates localization of computation
	and improves inference. Third, we derive a
	gradient-descent-based optimization for the Kullback-Leibler and
	the $\chi$2 divergence for the specific application of Monte
	Carlo integration with unnormalized stochastic estimates of the
	target distribution. Our approach enables fast and accurate
	inference and efficient sample generation independently of the
	dimensionality of the integration domain. We show its benefits
	on generating natural images and in two applications to
	light-transport simulation: first, we demonstrate learning of
	joint path-sampling densities in the primary sample space and
	importance sampling of multi-dimensional path prefixes thereof.
	Second, we use our technique to extract conditional directional
	densities driven by the product of incident illumination and the
	BSDF in the rendering equation, and we leverage the densities
	for path guiding. In all applications, our approach yields
	on-par or higher performance than competing techniques at equal
	sample count.",
	journal   = "ACM transactions on graphics",
	publisher = "Association for Computing Machinery",
	volume    =  38,
	number    =  5,
	pages     = "1--19",
	month     =  oct,
	year      =  2019,
	url       = "https://doi.org/10.1145/3341156",
	file      = "All Papers/M/Müller et al. 2019 - Neural Importance Sampling.pdf",
	address   = "New York, NY, USA",
	keywords  = "importance sampling, path guiding, rendering, Monte Carlo, deep
	learning, normalizing flows",
	issn      = "0730-0301",
	doi       = "10.1145/3341156"
}

@ARTICLE{Kuleshov2017-lf,
	title     = "Deep hybrid models: Bridging discriminative and generative
	approaches",
	author    = "Kuleshov, Volodymyr and Ermon, Stefano",
	abstract  = "… This may seen as unsupervised feature extraction
	Alternatively, we are regularizing the discriminative model …
	Advantages include: Greater flexibility when specifying the the
	hybrid model . Deals with complex models (incl. LV) using
	approximate inference …",
	journal   = "Uncertainty in Artificial Intelligence",
	publisher = "ai.stanford.edu",
	year      =  2017,
	url       = "https://ai.stanford.edu/~kuleshov/papers/uai2017-slides.pdf",
	file      = "All Papers/K/Kuleshov and Ermon 2017 - Deep hybrid models - Bridging discriminative and generative approaches.pdf"
}

@ARTICLE{Hardt2021-cm,
	title         = "Patterns, predictions, and actions: A story about machine
	learning",
	author        = "Hardt, Moritz and Recht, Benjamin",
	abstract      = "This graduate textbook on machine learning tells a story of
	how patterns in data support predictions and consequential
	actions. Starting with the foundations of decision making,
	we cover representation, optimization, and generalization as
	the constituents of supervised learning. A chapter on
	datasets as benchmarks examines their histories and
	scientific bases. Self-contained introductions to causality,
	the practice of causal inference, sequential decision
	making, and reinforcement learning equip the reader with
	concepts and tools to reason about actions and their
	consequences. Throughout, the text discusses historical
	context and societal impact. We invite readers from all
	backgrounds; some experience with probability, calculus, and
	linear algebra suffices.",
	month         =  "10~" # feb,
	year          =  2021,
	url           = "http://arxiv.org/abs/2102.05242",
	file          = "All Papers/H/Hardt and Recht 2021 - Patterns, predictions, and actions - A story about machine learning.pdf",
	archivePrefix = "arXiv",
	eprint        = "2102.05242",
	primaryClass  = "cs.LG",
	arxivid       = "2102.05242"
}

@INPROCEEDINGS{Dolatabadi2020-iq,
	title     = "Invertible Generative Modeling using Linear Rational Splines",
	booktitle = "Proceedings of the 23rd International Conference on Artificial
	Intelligence and Statistics",
	author    = "Dolatabadi, Hadi Mohaghegh and Erfani, Sarah and Leckie,
	Christopher",
	editor    = "Chiappa, Silvia and Calandra, Roberto",
	abstract  = "Normalizing flows attempt to model an arbitrary probability
	distribution through a set of invertible mappings. These
	transformations are required to achieve a tractable Jacobian
	determinant that can be used in high-dimensional scenarios. The
	first normalizing flow designs used coupling layer mappings
	built upon affine transformations. The significant advantage of
	such models is their easy-to-compute inverse. Nevertheless,
	making use of affine transformations may limit the
	expressiveness of such models. Recently, invertible piecewise
	polynomial functions as a replacement for affine transformations
	have attracted attention. However, these methods require solving
	a polynomial equation to calculate their inverse. In this paper,
	we explore using linear rational splines as a replacement for
	affine transformations used in coupling layers. Besides having a
	straightforward inverse, inference and generation have similar
	cost and architecture in this method. Moreover, simulation
	results demonstrate the competitiveness of this approach's
	performance compared to existing methods.",
	publisher = "PMLR",
	volume    =  108,
	pages     = "4236--4246",
	series    = "Proceedings of Machine Learning Research",
	year      =  2020,
	url       = "https://proceedings.mlr.press/v108/dolatabadi20a.html",
	file      = "All Papers/D/Dolatabadi et al. 2020 - Invertible Generative Modeling using Linear Rational Splines.pdf"
}

@INPROCEEDINGS{Park2022-oi,
	title     = "Learning Quantile Functions without Quantile Crossing for
	Distribution-free Time Series Forecasting",
	booktitle = "Proceedings of The 25th International Conference on Artificial
	Intelligence and Statistics",
	author    = "Park, Youngsuk and Maddix, Danielle and Aubet, Fran{\c
	c}ois-Xavier and Kan, Kelvin and Gasthaus, Jan and Wang, Yuyang",
	editor    = "Camps-Valls, Gustau and Ruiz, Francisco J R and Valera, Isabel",
	abstract  = "Quantile regression is an effective technique to quantify
	uncertainty, fit challenging underlying distributions, and often
	provide full probabilistic predictions through joint learnings
	over multiple quantile levels. A common drawback of these joint
	quantile regressions, however, is quantile crossing, which
	violates the desirable monotone property of the conditional
	quantile function. In this work, we propose the Incremental
	(Spline) Quantile Functions I(S)QF, a flexible and efficient
	distribution-free quantile estimation framework that resolves
	quantile crossing with a simple neural network layer. Moreover,
	I(S)QF inter/extrapolate to predict arbitrary quantile levels
	that differ from the underlying training ones. Equipped with the
	analytical evaluation of the continuous ranked probability score
	of I(S)QF representations, we apply our methods to NN-based
	times series forecasting cases, where the savings of the
	expensive re-training costs for non-trained quantile levels is
	particularly significant. We also provide a generalization error
	analysis of our proposed approaches under the
	sequence-to-sequence setting. Lastly, extensive experiments
	demonstrate the improvement of consistency and accuracy errors
	over other baselines.",
	publisher = "PMLR",
	volume    =  151,
	pages     = "8127--8150",
	series    = "Proceedings of Machine Learning Research",
	year      =  2022,
	url       = "https://proceedings.mlr.press/v151/park22a.html",
	file      = "All Papers/P/Park et al. 2022 - Learning Quantile Functions without Quantile Crossing for Distribution-free Time Series Forecasting.pdf"
}

@INPROCEEDINGS{Lheritier2022-lo,
	title     = "A Cram{\'e}r Distance perspective on Quantile Regression based
	Distributional Reinforcement Learning",
	booktitle = "Proceedings of The 25th International Conference on Artificial
	Intelligence and Statistics",
	author    = "Lheritier, Alix and Bondoux, Nicolas",
	editor    = "Camps-Valls, Gustau and Ruiz, Francisco J R and Valera, Isabel",
	abstract  = "Distributional reinforcement learning (DRL) extends the
	value-based approach by approximating the full distribution over
	future returns instead of the mean only, providing a richer
	signal that leads to improved performances. Quantile Regression
	(QR)-based methods like QR-DQN project arbitrary distributions
	into a parametric subset of staircase distributions by
	minimizing the 1-Wasserstein distance. However, due to biases in
	the gradients, the quantile regression loss is used instead for
	training, guaranteeing the same minimizer and enjoying unbiased
	gradients. Non-crossing constraints on the quantiles have been
	shown to improve the performance of QR-DQN for uncertainty-based
	exploration strategies. The contribution of this work is in the
	setting of fixed quantile levels and is twofold. First, we prove
	that the Cramer distance yields a projection that coincides with
	the 1-Wasserstein one and that, under non-crossing constraints,
	the squared Cramer and the quantile regression losses yield
	collinear gradients, shedding light on the connection between
	these important elements of DRL. Second, we propose a low
	complexity algorithm to compute the Cramer distance.",
	publisher = "PMLR",
	volume    =  151,
	pages     = "5774--5789",
	series    = "Proceedings of Machine Learning Research",
	year      =  2022,
	url       = "https://proceedings.mlr.press/v151/lheritier22a.html",
	file      = "All Papers/L/Lheritier and Bondoux 2022 - A Cramér Distance perspective on Quantile Regression based Distributional Reinforcement Learning.pdf"
}

@INPROCEEDINGS{Gasthaus2019-rp,
	title     = "Probabilistic Forecasting with Spline Quantile Function {RNNs}",
	booktitle = "Proceedings of the 22nd International Conference on Artificial
	Intelligence and Statistics",
	author    = "Gasthaus, Jan and Benidis, Konstantinos and Wang, Yuyang and
	Rangapuram, Syama Sundar and Salinas, David and Flunkert,
	Valentin and Januschowski, Tim",
	editor    = "Chaudhuri, Kamalika and Sugiyama, Masashi",
	abstract  = "In this paper, we propose a flexible method for probabilistic
	modeling with conditional quantile functions using monotonic
	regression splines. The shape of the spline is parameterized by
	a neural network whose parameters are learned by minimizing the
	continuous ranked probability score. Within this framework, we
	propose a method for probabilistic time series forecasting,
	which combines the modeling capacity of recurrent neural
	networks with the flexibility of a spline-based representation
	of the output distribution. Unlike methods based on parametric
	probability density functions and maximum likelihood estimation,
	the proposed method can flexibly adapt to different output
	distributions without manual intervention. We empirically
	demonstrate the effectiveness of the approach on synthetic and
	real-world data sets.",
	publisher = "PMLR",
	volume    =  89,
	pages     = "1901--1910",
	series    = "Proceedings of Machine Learning Research",
	year      =  2019,
	url       = "https://proceedings.mlr.press/v89/gasthaus19a.html",
	file      = "All Papers/G/Gasthaus et al. 2019 - Probabilistic Forecasting with Spline Quantile Function RNNs.pdf"
}

@ARTICLE{Vanschoren2014-nv,
	title     = "{OpenML}: networked science in machine learning",
	author    = "Vanschoren, Joaquin and van Rijn, Jan N and Bischl, Bernd and
	Torgo, Luis",
	abstract  = "Many sciences have made significant breakthroughs by adopting
	online tools that help organize, structure and mine information
	that is too detailed to be printed in journals. In this paper,
	we introduce OpenML, a place for machine learning researchers to
	share and organize data in fine detail, so that they can work
	more effectively, be more visible, and collaborate with others
	to tackle harder problems. We discuss how OpenML relates to
	other examples of networked science and what benefits it brings
	for machine learning research, individual scientists, as well as
	students and practitioners.",
	journal   = "SIGKDD Explor. Newsl.",
	publisher = "Association for Computing Machinery",
	volume    =  15,
	number    =  2,
	pages     = "49--60",
	month     =  "16~" # jun,
	year      =  2014,
	url       = "https://doi.org/10.1145/2641190.2641198",
	file      = "All Papers/V/Vanschoren et al. 2014 - OpenML - networked science in machine learning.pdf",
	address   = "New York, NY, USA",
	issn      = "1931-0145",
	doi       = "10.1145/2641190.2641198"
}

@INPROCEEDINGS{Grover2019-iw,
	title     = "Stochastic Optimization of Sorting Networks via Continuous
	Relaxations",
	booktitle = "International Conference on Learning Representations",
	author    = "Grover, Aditya and Wang, Eric and Zweig, Aaron and Ermon,
	Stefano",
	year      =  2019,
	url       = "https://openreview.net/forum?id=H1eSS3CcKX",
	file      = "All Papers/G/Grover et al. 2019 - Stochastic Optimization of Sorting Networks via Continuous Relaxations.pdf"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Feldman2021-ew,
	title     = "Improving conditional coverage via orthogonal quantile
	regression",
	author    = "{Feldman} and {Bates} and {Romano}",
	abstract  = "… estimate the conditional quantiles with quantile regression
	---it is … We find in experiments that traditional quantile
	regression … quantiles , these two quantities are independent (
	orthogonal ), …",
	journal   = "Advances in neural information processing systems",
	publisher = "proceedings.neurips.cc",
	year      =  2021,
	url       = "https://proceedings.neurips.cc/paper/2021/hash/1006ff12c465532f8c574aeaa4461b16-Abstract.html",
	file      = "All Papers/F/Feldman et al. 2021 - 2106.00394.pdf;All Papers/F/Feldman et al. 2021 - Improving conditional coverage via orthogonal quantile regression.pdf",
	issn      = "1049-5258"
}

@INPROCEEDINGS{Zhao2020-ze,
	title     = "Individual Calibration with Randomized Forecasting",
	booktitle = "Proceedings of the 37th International Conference on Machine
	Learning",
	author    = "Zhao, Shengjia and Ma, Tengyu and Ermon, Stefano",
	editor    = "Iii, Hal Daum{\'e} and Singh, Aarti",
	abstract  = "Machine learning applications often require calibrated
	predictions, e.g. a 90\% credible interval should contain the
	true outcome 90\% of the times. However, typical definitions of
	calibration only require this to hold on average, and offer no
	guarantees on predictions made on individual samples. Thus,
	predictions can be systematically over or under confident on
	certain subgroups, leading to issues of fairness and potential
	vulnerabilities. We show that calibration for individual samples
	is possible in the regression setup if and only if the
	predictions are randomized, i.e. outputting randomized credible
	intervals. Randomization removes systematic bias by trading off
	bias with variance. We design a training objective to enforce
	individual calibration and use it to train randomized regression
	functions. The resulting models are more calibrated for
	arbitrarily chosen subgroups of the data, and can achieve higher
	utility in decision making against adversaries that exploit
	miscalibrated predictions.",
	publisher = "PMLR",
	volume    =  119,
	pages     = "11387--11397",
	series    = "Proceedings of Machine Learning Research",
	year      =  2020,
	url       = "https://proceedings.mlr.press/v119/zhao20e.html",
	file      = "All Papers/Z/Zhao et al. 2020 - 2006.10288.pdf;All Papers/Z/Zhao et al. 2020 - Individual Calibration with Randomized Forecasting.pdf",
	keywords  = "Calibration"
}

@INPROCEEDINGS{Minderer2021-xw,
	title     = "Revisiting the Calibration of Modern Neural Networks",
	booktitle = "Advances in Neural Information Processing Systems",
	author    = "Minderer, Matthias and Djolonga, Josip and Romijnders, Rob and
	Hubis, Frances and Zhai, Xiaohua and Houlsby, Neil and Tran,
	Dustin and Lucic, Mario",
	editor    = "Ranzato, M and Beygelzimer, A and Dauphin, Y and Liang, P S and
	Vaughan, J Wortman",
	publisher = "Curran Associates, Inc.",
	volume    =  34,
	pages     = "15682--15694",
	year      =  2021,
	url       = "https://proceedings.neurips.cc/paper/2021/file/8420d359404024567b5aefda1231af24-Paper.pdf",
	file      = "All Papers/M/Minderer et al. 2021 - 2106.07998.pdf;All Papers/M/Minderer et al. 2021 - Revisiting the Calibration of Modern Neural Networks.pdf",
	keywords  = "Calibration;Classification"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Lin2017-nz,
	title     = "Focal loss for dense object detection",
	author    = "{Lin} and {Goyal} and {Girshick} and {He} and {others}",
	abstract  = "… Finally, we note that the exact form of the focal loss is not
	crucial, and … focal loss , we design a simple one-stage object
	detector called RetinaNet, named for its dense sampling of
	object …",
	journal   = "Proceedings of the Estonian Academy of Sciences. Biology,
	ecology = Eesti Teaduste Akadeemia toimetised. Bioloogia,
	okoloogia",
	publisher = "openaccess.thecvf.com",
	year      =  2017,
	url       = "http://openaccess.thecvf.com/content_iccv_2017/html/Lin_Focal_Loss_for_ICCV_2017_paper.html",
	file      = "All Papers/L/Lin et al. 2017 - Focal loss for dense object detection.pdf",
	keywords  = "Classification",
	issn      = "1406-0914"
}

@INPROCEEDINGS{Chung2021-rh,
	title     = "Beyond Pinball Loss: Quantile Methods for Calibrated Uncertainty
	Quantification",
	booktitle = "Advances in Neural Information Processing Systems",
	author    = "Chung, Youngseog and Neiswanger, Willie and Char, Ian and
	Schneider, Jeff",
	editor    = "Ranzato, M and Beygelzimer, A and Dauphin, Y and Liang, P S and
	Vaughan, J Wortman",
	publisher = "Curran Associates, Inc.",
	volume    =  34,
	pages     = "10971--10984",
	year      =  2021,
	url       = "https://proceedings.neurips.cc/paper/2021/file/5b168fdba5ee5ea262cc2d4c0b457697-Paper.pdf",
	file      = "All Papers/C/Chung et al. 2021 - Beyond Pinball Loss - Quantile Methods for Calibrated Uncertainty Quantification.pdf;All Papers/C/Chung et al. 2021 - Beyond Pinball Loss - Quantile Methods for Calibrated Uncertainty Quantification.pdf"
}

@INPROCEEDINGS{Guo2017-ow,
	title     = "On Calibration of Modern Neural Networks",
	booktitle = "Proceedings of the 34th International Conference on Machine
	Learning",
	author    = "Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian
	Q",
	editor    = "Precup, Doina and Teh, Yee Whye",
	abstract  = "Confidence calibration -- the problem of predicting probability
	estimates representative of the true correctness likelihood --
	is important for classification models in many applications. We
	discover that modern neural networks, unlike those from a decade
	ago, are poorly calibrated. Through extensive experiments, we
	observe that depth, width, weight decay, and Batch Normalization
	are important factors influencing calibration. We evaluate the
	performance of various post-processing calibration methods on
	state-of-the-art architectures with image and document
	classification datasets. Our analysis and experiments not only
	offer insights into neural network learning, but also provide a
	simple and straightforward recipe for practical settings: on
	most datasets, temperature scaling -- a single-parameter variant
	of Platt Scaling -- is surprisingly effective at calibrating
	predictions.",
	publisher = "PMLR",
	volume    =  70,
	pages     = "1321--1330",
	series    = "Proceedings of Machine Learning Research",
	year      =  2017,
	url       = "https://proceedings.mlr.press/v70/guo17a.html",
	file      = "All Papers/G/Guo et al. 2017 - 1706.04599.pdf;All Papers/G/Guo et al. 2017 - On Calibration of Modern Neural Networks.pdf"
}

@UNPUBLISHED{Malinin2020-ug,
	title    = "Regression Prior Networks",
	author   = "Malinin, Andrey and Chervontsev, Sergey and Provilkov, Ivan and
	Gales, Mark",
	abstract = "Prior Networks are a recently developed class of models which
	yield interpretable measures of uncertainty and have been shown
	to outperform state-of-the-art ensemble approaches on a range of
	tasks. They can also be used to distill an ensemble of models via
	\textbackslashemph\{Ensemble Distribution Distillation\}
	(EnD$^2$), such that its accuracy, calibration and uncertainty
	estimates are retained within a single model. However, Prior
	Networks have so far been developed only for classification
	tasks. This work extends Prior Networks and EnD$^2$ to regression
	tasks by considering the Normal-Wishart distribution. The
	properties of Regression Prior Networks are demonstrated on
	synthetic data, selected UCI datasets and a monocular depth
	estimation task, where they yield performance competitive with
	ensemble approaches.",
	month    =  "28~" # sep,
	year     =  2020,
	url      = "https://openreview.net/pdf?id=ygWoT6hOc28",
	file     = "All Papers/M/Malinin et al. 2020 - Regression Prior Networks.pdf"
}

@UNPUBLISHED{Meng2020-tf,
	title    = "Improved Autoregressive Modeling with Distribution Smoothing",
	author   = "Meng, Chenlin and Song, Jiaming and Song, Yang and Zhao, Shengjia
	and Ermon, Stefano",
	abstract = "While autoregressive models excel at image compression, their
	sample quality is often lacking. Although not realistic,
	generated images often have high likelihood according to the
	model, resembling the case of adversarial examples. Inspired by a
	successful adversarial defense method, we incorporate randomized
	smoothing into autoregressive generative modeling. We first model
	a smoothed version of the data distribution, and then reverse the
	smoothing process to recover the original data distribution. This
	procedure drastically improves the sample quality of existing
	autoregressive models on several synthetic and real-world image
	datasets while obtaining competitive likelihoods on synthetic
	datasets.",
	month    =  "28~" # sep,
	year     =  2020,
	url      = "https://openreview.net/pdf?id=rJA5Pz7lHKb",
	file     = "All Papers/M/Meng et al. 2020 - Improved Autoregressive Modeling with Distribution Smoothing.pdf"
}

@ARTICLE{Dewolf2022-ug,
	title    = "Valid prediction intervals for regression problems",
	author   = "Dewolf, Nicolas and Baets, Bernard De and Waegeman, Willem",
	abstract = "Over the last few decades, various methods have been proposed for
	estimating prediction intervals in regression settings, including
	Bayesian methods, ensemble methods, direct interval estimation
	methods and conformal prediction methods. An important issue is
	the validity and calibration of these methods: the generated
	prediction intervals should have a predefined coverage level,
	without being overly conservative. So far, no study has analysed
	this issue whilst simultaneously considering these four classes
	of methods. In this independent comparative study, we review the
	above four classes of methods from a conceptual and experimental
	point of view in the i.i.d. setting. Results on benchmark data
	sets from various domains highlight large fluctuations in
	performance from one data set to another. These observations can
	be attributed to the violation of certain assumptions that are
	inherent to some classes of methods. We illustrate how conformal
	prediction can be used as a general calibration procedure for
	methods that deliver poor results without a calibration step.",
	journal  = "Artificial Intelligence Review",
	month    =  "18~" # apr,
	year     =  2022,
	url      = "https://doi.org/10.1007/s10462-022-10178-5",
	file     = "All Papers/D/Dewolf et al. 2022 - Valid prediction intervals for regression problems.pdf",
	issn     = "1573-7462",
	doi      = "10.1007/s10462-022-10178-5"
}

@UNPUBLISHED{Raji2021-ng,
	title    = "{AI} and the Everything in the Whole Wide World Benchmark",
	author   = "Raji, Inioluwa Deborah and Denton, Emily and Bender, Emily M and
	Hanna, Alex and Paullada, Amandalynne",
	abstract = "There is a tendency across different subfields in AI to see value
	in a small collection of influential benchmarks, which we term
	'general' benchmarks. These benchmarks operate as stand-ins or
	abstractions for a range of anointed common problems that are
	frequently framed as foundational milestones on the path towards
	flexible and generalizable AI systems. State-of-the-art
	performance on these benchmarks is widely understood as
	indicative of progress towards these long-term goals. In this
	position paper, we explore how such benchmarks are designed,
	constructed and used in order to reveal key limitations of their
	framing as the functionally 'general' broad measures of progress
	they are set up to be.",
	month    =  "20~" # aug,
	year     =  2021,
	url      = "https://openreview.net/pdf?id=j6NxpQbREA1",
	file     = "All Papers/R/Raji et al. 2021 - AI and the Everything in the Whole Wide World Benchmark.pdf"
}

@UNPUBLISHED{Zhao2022-ma,
	title    = "Comparing Distributions by Measuring Differences that Affect
	Decision Making",
	author   = "Zhao, Shengjia and Sinha, Abhishek and He, Yutong and Perreault,
	Aidan and Song, Jiaming and Ermon, Stefano",
	abstract = "Measuring the discrepancy between two probability distributions
	is a fundamental problem in machine learning and statistics. We
	propose a new class of discrepancies based on the optimal loss
	for a decision task -- two distributions are different if the
	optimal decision loss is higher on their mixture than on each
	individual distribution. By suitably choosing the decision task,
	this generalizes the Jensen-Shannon divergence and the maximum
	mean discrepancy family. We apply our approach to two-sample
	tests, and on various benchmarks, we achieve superior test power
	compared to competing methods. In addition, a modeler can
	directly specify their preferences when comparing distributions
	through the decision loss. We apply this property to
	understanding the effects of climate change on different social
	and economic activities, evaluating sample quality, and selecting
	features targeting different decision tasks.",
	month    =  "17~" # mar,
	year     =  2022,
	url      = "https://openreview.net/pdf?id=KB5onONJIAU",
	file     = "All Papers/Z/Zhao et al. 2022 - Comparing Distributions by Measuring Differences that Affect Decision Making.pdf"
}

@ARTICLE{Merkle2013-yj,
	title     = "Choosing a Strictly Proper Scoring Rule",
	author    = "Merkle, Edgar C and Steyvers, Mark",
	abstract  = "Strictly proper scoring rules, including the Brier score and the
	logarithmic score, are standard metrics by which probability
	forecasters are assessed and compared. Researchers often find
	that one's choice of strictly proper scoring rule has minimal
	impact on one's conclusions, but this conclusion is typically
	drawn from a small set of popular rules. In the context of
	forecasting world events, we use a recently proposed family of
	proper scoring rules to study the properties of a wide variety
	of strictly proper rules. The results indicate that conclusions
	vary greatly across different scoring rules, so that one's
	choice of scoring rule should be informed by the forecasting
	domain. We then describe strategies for choosing a scoring rule
	that meets the needs of the forecast consumer, considering three
	unique families of proper scoring rules.",
	journal   = "Decision Analysis",
	publisher = "INFORMS",
	volume    =  10,
	number    =  4,
	pages     = "292--304",
	month     =  "1~" # dec,
	year      =  2013,
	url       = "https://doi.org/10.1287/deca.2013.0280",
	file      = "All Papers/M/Merkle and Steyvers 2013 - Choosing a Strictly Proper Scoring Rule.pdf",
	issn      = "1545-8490",
	doi       = "10.1287/deca.2013.0280"
}

@MISC{Datadog_undated-ro,
	title        = "Robust Statistical Distances for Machine Learning",
	author       = "{Datadog}",
	abstract     = "Designing powerful outlier and anomaly detection algorithms
	requires using the right tools. Discover how robust
	statistical distances can help.",
	url          = "https://www.datadoghq.com/blog/engineering/robust-statistical-distances-for-machine-learning/",
	howpublished = "\url{https://www.datadoghq.com/blog/engineering/robust-statistical-distances-for-machine-learning/}",
	note         = "Accessed: 2022-4-11"
}

@ARTICLE{Hasson2021-bo,
	title   = "Probabilistic Forecasting: A {Level-Set} Approach",
	author  = "Hasson, Hilaf and Wang, Bernie and Januschowski, Tim and Gasthaus,
	Jan",
	journal = "Advances in neural information processing systems",
	volume  =  34,
	year    =  2021,
	url     = "https://proceedings.neurips.cc/paper/2021/file/32b127307a606effdcc8e51f60a45922-Paper.pdf",
	file    = "All Papers/H/Hasson et al. 2021 - Probabilistic Forecasting - A Level-Set Approach.pdf",
	issn    = "1049-5258"
}

@ARTICLE{Stankeviciute2021-ay,
	title   = "Conformal Time-series Forecasting",
	author  = "Stankeviciute, Kamile and M Alaa, Ahmed and van der Schaar,
	Mihaela",
	journal = "Advances in neural information processing systems",
	volume  =  34,
	year    =  2021,
	url     = "https://proceedings.neurips.cc/paper/2021/file/312f1ba2a72318edaaa995a67835fad5-Paper.pdf",
	file    = "All Papers/S/Stankeviciute et al. 2021 - Conformal Time-series Forecasting.pdf",
	issn    = "1049-5258"
}

@UNPUBLISHED{Trivedi2018-jp,
	title    = "{DyRep}: Learning Representations over Dynamic Graphs",
	author   = "Trivedi, Rakshit and Farajtabar, Mehrdad and Biswal, Prasenjeet
	and Zha, Hongyuan",
	abstract = "Representation Learning over graph structured data has received
	significant attention recently due to its ubiquitous
	applicability. However, most advancements have been made in
	static graph settings while efforts for jointly learning dynamic
	of the graph and dynamic on the graph are still in an infant
	stage. Two fundamental questions arise in learning over dynamic
	graphs: (i) How to elegantly model dynamical processes over
	graphs? (ii) How to leverage such a model to effectively encode
	evolving graph information into low-dimensional representations?
	We present DyRep - a novel modeling framework for dynamic graphs
	that posits representation learning as a latent mediation process
	bridging two observed processes namely -- dynamics of the network
	(realized as topological evolution) and dynamics on the network
	(realized as activities between nodes). Concretely, we propose a
	two-time scale deep temporal point process model that captures
	the interleaved dynamics of the observed processes. This model is
	further parameterized by a temporal-attentive representation
	network that encodes temporally evolving structural information
	into node representations which in turn drives the nonlinear
	evolution of the observed graph dynamics. Our unified framework
	is trained using an efficient unsupervised procedure and has
	capability to generalize over unseen nodes. We demonstrate that
	DyRep outperforms state-of-the-art baselines for dynamic link
	prediction and time prediction tasks and present extensive
	qualitative insights into our framework.",
	month    =  "27~" # sep,
	year     =  2018,
	url      = "https://openreview.net/pdf?id=HyePrhR5KX",
	file     = "All Papers/T/Trivedi et al. 2018 - DyRep - Learning Representations over Dynamic Graphs.pdf"
}

@ARTICLE{Romano2020-nb,
	title       = "With malice toward none: Assessing uncertainty via equalized
	coverage",
	author      = "Romano, Yaniv and Barber, Rina Foygel and Sabatti, Chiara and
	Cand{\`e}s, Emmanuel",
	affiliation = "Department of Statistics, Stanford University; Department of
	Statistics, University of Chicago; Department of Biomedical
	Data Science, Stanford University; Department of Mathematics,
	Stanford University",
	journal     = "Harvard Data Science Review",
	publisher   = "MIT Press - Journals",
	month       =  "30~" # apr,
	year        =  2020,
	url         = "https://hdsr.mitpress.mit.edu/pub/qedrwcz3",
	file        = "All Papers/R/Romano et al. 2020 - With malice toward none - Assessing uncertainty via equalized coverage.pdf",
	language    = "en",
	doi         = "10.1162/99608f92.03f00592"
}

@ARTICLE{McCulloch1943-ij,
	title    = "A logical calculus of the ideas immanent in nervous activity",
	author   = "McCulloch, Warren S and Pitts, Walter",
	abstract = "Because of the ``all-or-none'' character of nervous activity,
	neural events and the relations among them can be treated by
	means of propositional logic. It is found that the behavior of
	every net can be described in these terms, with the addition of
	more complicated logical means for nets containing circles; and
	that for any logical expression satisfying certain conditions,
	one can find a net behaving in the fashion it describes. It is
	shown that many particular choices among possible
	neurophysiological assumptions are equivalent, in the sense that
	for every net behaving under one assumption, there exists another
	net which behaves under the other and gives the same results,
	although perhaps not in the same time. Various applications of
	the calculus are discussed.",
	journal  = "The Bulletin of mathematical biophysics",
	volume   =  5,
	number   =  4,
	pages    = "115--133",
	month    =  "1~" # dec,
	year     =  1943,
	url      = "https://doi.org/10.1007/BF02478259",
	file     = "All Papers/M/McCulloch and Pitts 1943 - A logical calculus of the ideas immanent in nervous activity.pdf",
	issn     = "0007-4985, 1522-9602",
	doi      = "10.1007/BF02478259"
}

@ARTICLE{Heinrich2014-ru,
	title     = "The mode functional is not elicitable",
	author    = "Heinrich, C",
	abstract  = "SUMMARY. This article is concerned with point forecasting of a
	real-valued random variable with a general Lebesgue density.
	Answering a question of Gneiting (20",
	journal   = "Biometrika",
	publisher = "Oxford Academic",
	volume    =  101,
	number    =  1,
	pages     = "245--251",
	month     =  "1~" # mar,
	year      =  2014,
	url       = "https://academic.oup.com/biomet/article-pdf/101/1/245/17460745/ast048.pdf",
	file      = "All Papers/H/Heinrich 2014 - The mode functional is not elicitable.pdf",
	issn      = "0006-3444",
	doi       = "10.1093/biomet/ast048"
}

@INPROCEEDINGS{Lu2018-fx,
	title     = "Deep regression tracking with shrinkage loss",
	booktitle = "Proceedings of the European conference on computer vision
	({ECCV})",
	author    = "Lu, Xiankai and Ma, Chao and Ni, Bingbing and Yang, Xiaokang and
	Reid, Ian and Yang, Ming-Hsuan",
	pages     = "353--369",
	year      =  2018,
	url       = "http://openaccess.thecvf.com/content_ECCV_2018/papers/Xiankai_Lu_Deep_Regression_Tracking_ECCV_2018_paper.pdf",
	file      = "All Papers/L/Lu et al. 2018 - Deep regression tracking with shrinkage loss.pdf"
}

@MISC{noauthor_2021-hs,
	title        = "{NLP} From Scratch: Translation with a Sequence to Sequence
	Network and Attention --- {PyTorch} Tutorials 1.10.1+cu102
	documentation",
	year         =  2021,
	url          = "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html",
	howpublished = "\url{https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html}",
	note         = "Accessed: 2022-1-17",
	language     = "en"
}

@MISC{Song2021-pb,
	title        = "Deep Generative Models",
	author       = "Song, Yang and Ermon, Stefano",
	year         =  2021,
	url          = "https://deepgenerativemodels.github.io/syllabus.html",
	howpublished = "\url{https://deepgenerativemodels.github.io/syllabus.html}",
	note         = "Accessed: 2022-1-NA",
	language     = "en"
}

@ARTICLE{Laio2007-uc,
	title     = "Verification tools for probabilistic forecasts of continuous
	hydrological variables",
	author    = "Laio, F and Tamea, S",
	abstract  = "Abstract. In the present paper we describe some methods for
	verifying and evaluating probabilistic forecasts of hydrological
	variables. We propose an extension to continuous-valued
	variables of a verification method originated in the
	meteorological literature for the analysis of binary variables,
	and based on the use of a suitable cost-loss function to
	evaluate the quality of the forecasts. We find that this
	procedure is useful and reliable when it is complemented with
	other verification tools, borrowed from the economic literature,
	which are addressed to verify the statistical correctness of the
	probabilistic forecast. We illustrate our findings with a
	detailed application to the evaluation of probabilistic and
	deterministic forecasts of hourly discharge values.",
	journal   = "Hydrology and Earth System Sciences",
	publisher = "Copernicus GmbH",
	volume    =  11,
	number    =  4,
	pages     = "1267--1277",
	month     =  "3~" # may,
	year      =  2007,
	url       = "https://hess.copernicus.org/articles/11/1267/2007/",
	file      = "All Papers/L/Laio and Tamea 2007 - Verification tools for probabilistic forecasts of continuous hydrological variables.pdf",
	language  = "en",
	issn      = "1027-5606, 1607-7938",
	doi       = "10.5194/hess-11-1267-2007"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Hyvarinen2000-ku,
	title       = "Independent component analysis: algorithms and applications",
	author      = "Hyv{\"a}rinen, A and Oja, E",
	affiliation = "Neural Networks Research Centre, Helsinki University of
	Technology, Finland. aapo.hyvarinen@hut.fi",
	abstract    = "A fundamental problem in neural network research, as well as
	in many other disciplines, is finding a suitable
	representation of multivariate data, i.e. random vectors. For
	reasons of computational and conceptual simplicity, the
	representation is often sought as a linear transformation of
	the original data. In other words, each component of the
	representation is a linear combination of the original
	variables. Well-known linear transformation methods include
	principal component analysis, factor analysis, and projection
	pursuit. Independent component analysis (ICA) is a recently
	developed method in which the goal is to find a linear
	representation of non-Gaussian data so that the components are
	statistically independent, or as independent as possible. Such
	a representation seems to capture the essential structure of
	the data in many applications, including feature extraction
	and signal separation. In this paper, we present the basic
	theory and applications of ICA, and our recent work on the
	subject.",
	journal     = "Neural networks: the official journal of the International
	Neural Network Society",
	volume      =  13,
	number      = "4-5",
	pages       = "411--430",
	month       =  may,
	year        =  2000,
	url         = "http://dx.doi.org/10.1016/s0893-6080(00)00026-5",
	file        = "All Papers/H/Hyvärinen and Oja 2000 - Independent component analysis - algorithms and applications.pdf",
	language    = "en",
	issn        = "0893-6080",
	pmid        = "10946390",
	doi         = "10.1016/s0893-6080(00)00026-5"
}

@INPROCEEDINGS{Inouye2018-mi,
	title     = "Deep Density Destructors",
	booktitle = "Proceedings of the 35th International Conference on Machine
	Learning",
	author    = "Inouye, David and Ravikumar, Pradeep",
	editor    = "Dy, Jennifer and Krause, Andreas",
	abstract  = "We propose a unified framework for deep density models by
	formally defining density destructors. A density destructor is
	an invertible function that transforms a given density to the
	uniform density---essentially destroying any structure in the
	original density. This destructive transformation generalizes
	Gaussianization via ICA and more recent autoregressive models
	such as MAF and Real NVP. Informally, this transformation can be
	seen as a generalized whitening procedure or a multivariate
	generalization of the univariate CDF function. Unlike
	Gaussianization, our destructive transformation has the elegant
	property that the density function is equal to the absolute
	value of the Jacobian determinant. Thus, each layer of a deep
	density can be seen as a shallow density---uncovering a
	fundamental connection between shallow and deep densities. In
	addition, our framework provides a common interface for all
	previous methods enabling them to be systematically combined,
	evaluated and improved. Leveraging the connection to shallow
	densities, we also propose a novel tree destructor based on tree
	densities and an image-specific destructor based on pixel
	locality. We illustrate our framework on a 2D dataset, MNIST,
	and CIFAR-10. Code is available on first author's website.",
	publisher = "PMLR",
	volume    =  80,
	pages     = "2167--2175",
	series    = "Proceedings of Machine Learning Research",
	year      =  2018,
	url       = "https://proceedings.mlr.press/v80/inouye18a.html",
	file      = "All Papers/I/Inouye and Ravikumar 2018 - Deep Density Destructors.pdf"
}

@MISC{Weng2018-zi,
	title        = "Flow-based Deep Generative Models",
	author       = "Weng, Lilian",
	abstract     = "In this post, we are looking into the third type of
	generative models: flow-based generative models. Different
	from GAN and VAE, they explicitly learn the probability
	density function of the input data.",
	month        =  "13~" # oct,
	year         =  2018,
	url          = "https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html",
	howpublished = "\url{https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html}",
	note         = "Accessed: 2021-12-10"
}

@PHDTHESIS{Kingma2017-vl,
	title    = "Variational inference \& deep learning",
	author   = "Kingma, D P",
	year     =  2017,
	url      = "https://pure.uva.nl/ws/files/17891313/Thesis.pdf",
	file     = "All Papers/K/Kingma 2017 - Variational inference & deep learning.pdf",
	school   = "Univsersity of Amsterdam"
}

@ARTICLE{Hyndman1996-yg,
	title     = "Sample Quantiles in Statistical Packages",
	author    = "Hyndman, Rob J and Fan, Yanan",
	abstract  = "[There are a large number of different definitions used for
	sample quantiles in statistical computer packages. Often within
	the same package one definition will be used to compute a
	quantile explicitly, while other definitions may be used when
	producing a boxplot, a probability plot, or a QQ plot. We
	compare the most commonly implemented sample quantile
	definitions by writing them in a common notation and
	investigating their motivation and some of their properties. We
	argue that there is a need to adopt a standard definition for
	sample quantiles so that the same answers are produced by
	different packages and within each package. We conclude by
	recommending that the median-unbiased estimator be used because
	it has most of the desirable properties of a quantile estimator
	and can be defined independently of the underlying
	distribution.]",
	journal   = "The American statistician",
	publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
	volume    =  50,
	number    =  4,
	pages     = "361--365",
	year      =  1996,
	url       = "http://www.jstor.org/stable/2684934",
	file      = "All Papers/H/Hyndman and Fan 1996 - Sample Quantiles in Statistical Packages.pdf",
	issn      = "0003-1305",
	doi       = "10.2307/2684934"
}

@MISC{Tran2020-dz,
	title        = "{NeurIPS} 2020 : (Track2) Practical Uncertainty Estimation
	and {Out-of-Distribution} Robustness in Deep Learning",
	author       = "Tran, Dustin and Lakshminarayanan, Balaji and Snoek, Jasper",
	month        =  "7~" # dec,
	year         =  2020,
	url          = "https://neurips.cc/virtual/2020/public/tutorial_0f190e6e164eafe66f011073b4486975.html",
	howpublished = "\url{https://neurips.cc/virtual/2020/public/tutorial_0f190e6e164eafe66f011073b4486975.html}",
	note         = "Accessed: 2021-12-3"
}

@ARTICLE{noauthor_undated-ym,
	title = "{CDv3.pdf}",
	url   = "http://www.gatsby.ucl.ac.uk/~turner/Notes/ContrastiveDivergence/CDv3.pdf",
	file  = "All Papers/Other/CDv3.pdf - CDv3.pdf"
}

@ARTICLE{Bond-Taylor2021-wr,
	title    = "Deep Generative Modelling: A Comparative Review of {VAEs},
	{GANs}, Normalizing Flows, {Energy-Based} and Autoregressive
	Models",
	author   = "Bond-Taylor, Sam and Leach, Adam and Long, Yang and Willcocks,
	Chris G",
	abstract = "Deep generative models are a class of techniques that train deep
	neural networks to model the distribution of training samples.
	Research has fragmented into various interconnected approaches,
	each of which make trade-offs including run-time, diversity, and
	architectural restrictions. In particular, this compendium covers
	energy-based models, variational autoencoders, generative
	adversarial networks, autoregressive models, normalizing flows,
	in addition to numerous hybrid approaches. These techniques are
	compared and contrasted, explaining the premises behind each and
	how they are interrelated, while reviewing current
	state-of-the-art advances and implementations.",
	journal  = "IEEE transactions on pattern analysis and machine intelligence",
	volume   = "PP",
	month    =  sep,
	year     =  2021,
	url      = "http://dx.doi.org/10.1109/TPAMI.2021.3116668",
	file     = "All Papers/B/Bond-Taylor et al. 2021 - Deep Generative Modelling - A Comparative Revi ... VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models.pdf",
	keywords = "Review;Energy-based;Flow;Variational",
	language = "en",
	issn     = "0162-8828",
	pmid     = "34591756",
	doi      = "10.1109/TPAMI.2021.3116668"
}

@ARTICLE{Vincent2011-nk,
	title       = "A connection between score matching and denoising autoencoders",
	author      = "Vincent, Pascal",
	affiliation = "D{\'e}partement d'Informatique, Universit{\'e} de
	Montr{\'e}al, Montr{\'e}al, QC H3C 3J7, Canada.
	vincentp@iro.umontreal.ca",
	abstract    = "Denoising autoencoders have been previously shown to be
	competitive alternatives to restricted Boltzmann machines for
	unsupervised pretraining of each layer of a deep architecture.
	We show that a simple denoising autoencoder training criterion
	is equivalent to matching the score (with respect to the data)
	of a specific energy-based model to that of a nonparametric
	Parzen density estimator of the data. This yields several
	useful insights. It defines a proper probabilistic model for
	the denoising autoencoder technique, which makes it in
	principle possible to sample from them or rank examples by
	their energy. It suggests a different way to apply score
	matching that is related to learning to denoise and does not
	require computing second derivatives. It justifies the use of
	tied weights between the encoder and decoder and suggests ways
	to extend the success of denoising autoencoders to a larger
	family of energy-based models.",
	journal     = "Neural computation",
	volume      =  23,
	number      =  7,
	pages       = "1661--1674",
	month       =  jul,
	year        =  2011,
	url         = "http://dx.doi.org/10.1162/NECO_a_00142",
	file        = "All Papers/V/Vincent 2011 - A connection between score matching and denoising autoencoders.pdf",
	keywords    = "Energy-based",
	language    = "en",
	issn        = "0899-7667, 1530-888X",
	pmid        = "21492012",
	doi         = "10.1162/NECO\_a\_00142"
}

@ARTICLE{Ma2018-pl,
	title         = "Noise Contrastive Estimation and Negative Sampling for
	Conditional Models: Consistency and Statistical Efficiency",
	author        = "Ma, Zhuang and Collins, Michael",
	abstract      = "Noise Contrastive Estimation (NCE) is a powerful parameter
	estimation method for log-linear models, which avoids
	calculation of the partition function or its derivatives at
	each training step, a computationally demanding step in many
	cases. It is closely related to negative sampling methods,
	now widely used in NLP. This paper considers NCE-based
	estimation of conditional models. Conditional models are
	frequently encountered in practice; however there has not
	been a rigorous theoretical analysis of NCE in this setting,
	and we will argue there are subtle but important questions
	when generalizing NCE to the conditional case. In
	particular, we analyze two variants of NCE for conditional
	models: one based on a classification objective, the other
	based on a ranking objective. We show that the ranking-based
	variant of NCE gives consistent parameter estimates under
	weaker assumptions than the classification-based method; we
	analyze the statistical efficiency of the ranking-based and
	classification-based variants of NCE; finally we describe
	experiments on synthetic data and language modeling showing
	the effectiveness and trade-offs of both methods.",
	month         =  "6~" # sep,
	year          =  2018,
	url           = "http://arxiv.org/abs/1809.01812",
	file          = "All Papers/M/Ma and Collins 2018 - Noise Contrastive Estimation and Negative Sampling for Conditional Models - Consistency and Statistical Efficiency.pdf",
	archivePrefix = "arXiv",
	eprint        = "1809.01812",
	primaryClass  = "cs.CL",
	arxivid       = "1809.01812"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Gutmann2010-ar,
	title     = "Noise-contrastive estimation: A new estimation principle for
	unnormalized statistical models",
	booktitle = "Proceedings of the Thirteenth International Conference on
	Artificial Intelligence and Statistics",
	author    = "Gutmann, Michael and Hyv{\"a}rinen, Aapo",
	editor    = "Teh, Yee Whye and Titterington, Mike",
	abstract  = "We present a new estimation principle for parameterized
	statistical models. The idea is to perform nonlinear logistic
	regression to discriminate between the observed data and some
	artificially generated noise, using the model log-density
	function in the regression nonlinearity. We show that this leads
	to a consistent (convergent) estimator of the parameters, and
	analyze the asymptotic variance. In particular, the method is
	shown to directly work for unnormalized models, i.e. models
	where the density function does not integrate to one. The
	normalization constant can be estimated just like any other
	parameter. For a tractable ICA model, we compare the method with
	other estimation methods that can be used to learn unnormalized
	models, including score matching, contrastive divergence, and
	maximum-likelihood where the normalization constant is estimated
	with importance sampling. Simulations show that
	noise-contrastive estimation offers the best trade-off between
	computational and statistical efficiency. The method is then
	applied to the modeling of natural images: We show that the
	method can successfully estimate a large-scale two-layer model
	and a Markov random field.",
	publisher = "PMLR",
	volume    =  9,
	pages     = "297--304",
	series    = "Proceedings of Machine Learning Research",
	year      =  2010,
	url       = "https://proceedings.mlr.press/v9/gutmann10a.html",
	file      = "All Papers/G/Gutmann and Hyvärinen 2010 - Noise-contrastive estimation - A new estimation principle for unnormalized statistical models.pdf",
	address   = "Chia Laguna Resort, Sardinia, Italy"
}

@INPROCEEDINGS{Welling2011-vm,
	title     = "Bayesian learning via stochastic gradient langevin dynamics",
	booktitle = "Proceedings of the 28th International Conference on
	International Conference on Machine Learning",
	author    = "Welling, Max and Teh, Yee Whye",
	abstract  = "In this paper we propose a new framework for learning from large
	scale datasets based on iterative learning from small
	mini-batches. By adding the right amount of noise to a standard
	stochastic gradient optimization algorithm we show that the
	iterates will converge to samples from the true posterior
	distribution as we anneal the stepsize. This seamless transition
	between optimization and Bayesian posterior sampling provides an
	inbuilt protection against overfitting. We also propose a
	practical method for Monte Carlo estimates of posterior
	statistics which monitors a ``sampling threshold'' and collects
	samples after it has been surpassed. We apply the method to
	three models: a mixture of Gaussians, logistic regression and
	ICA with natural gradients.",
	publisher = "Omnipress",
	pages     = "681--688",
	series    = "ICML'11",
	month     =  "28~" # jun,
	year      =  2011,
	url       = "https://dl.acm.org/doi/10.5555/3104482.3104568",
	file      = "All Papers/W/Welling and Teh 2011 - Bayesian learning via stochastic gradient langevin dynamics.pdf",
	address   = "Madison, WI, USA",
	location  = "Bellevue, Washington, USA",
	isbn      = "9781450306195"
}

@ARTICLE{Borisov2021-mr,
	title         = "Deep Neural Networks and Tabular Data: A Survey",
	author        = "Borisov, Vadim and Leemann, Tobias and Se{\ss}ler, Kathrin
	and Haug, Johannes and Pawelczyk, Martin and Kasneci,
	Gjergji",
	abstract      = "Heterogeneous tabular data are the most commonly used form
	of data and are essential for numerous critical and
	computationally demanding applications. On homogeneous data
	sets, deep neural networks have repeatedly shown excellent
	performance and have therefore been widely adopted. However,
	their application to modeling tabular data (inference or
	generation) remains highly challenging. This work provides
	an overview of state-of-the-art deep learning methods for
	tabular data. We start by categorizing them into three
	groups: data transformations, specialized architectures, and
	regularization models. We then provide a comprehensive
	overview of the main approaches in each group. A discussion
	of deep learning approaches for generating tabular data is
	complemented by strategies for explaining deep models on
	tabular data. Our primary contribution is to address the
	main research streams and existing methodologies in this
	area, while highlighting relevant challenges and open
	research questions. To the best of our knowledge, this is
	the first in-depth look at deep learning approaches for
	tabular data. This work can serve as a valuable starting
	point and guide for researchers and practitioners interested
	in deep learning with tabular data.",
	month         =  "5~" # oct,
	year          =  2021,
	url           = "http://arxiv.org/abs/2110.01889",
	file          = "All Papers/B/Borisov et al. 2021 - Deep Neural Networks and Tabular Data - A Survey.pdf",
	archivePrefix = "arXiv",
	eprint        = "2110.01889",
	primaryClass  = "cs.LG",
	arxivid       = "2110.01889"
}

@BOOK{OHagan2006-rf,
	title     = "Uncertain Judgements: Eliciting Experts' Probabilities",
	author    = "O'Hagan, Anthony and Buck, Caitlin E and Daneshkhah, Alireza and
	Richard Eiser, J and Garthwaite, Paul H and Jenkinson, David J
	and Oakley, Jeremy E and Rakow, Tim",
	abstract  = "Elicitation is the process of extracting expert knowledge about
	some unknown quantity or quantities, and formulating that
	information as a probability distribution. Elicitation is
	important in situations, such as modelling the safety of nuclear
	installations or assessing the risk of terrorist attacks, where
	expert knowledge is essentially the only source of good
	information. It also plays a major role in other contexts by
	augmenting scarce observational data, through the use of
	Bayesian statistical methods. However, elicitation is not a
	simple task, and practitioners need to be aware of a wide range
	of research findings in order to elicit expert judgements
	accurately and reliably. Uncertain Judgements introduces the
	area, before guiding the reader through the study of appropriate
	elicitation methods, illustrated by a variety of
	multi-disciplinary examples. This is achieved by: Presenting a
	methodological framework for the elicitation of expert knowledge
	incorporating findings from both statistical and psychological
	research. Detailing techniques for the elicitation of a wide
	range of standard distributions, appropriate to the most common
	types of quantities. Providing a comprehensive review of the
	available literature and pointing to the best practice methods
	and future research needs. Using examples from many disciplines,
	including statistics, psychology, engineering and health
	sciences. Including an extensive glossary of statistical and
	psychological terms. An ideal source and guide for statisticians
	and psychologists with interests in expert judgement or
	practical applications of Bayesian analysis, Uncertain
	Judgements will also benefit decision-makers, risk analysts,
	engineers and researchers in the medical and social sciences.",
	publisher = "Wiley",
	month     =  sep,
	year      =  2006,
	url       = "https://play.google.com/store/books/details?id=cYB6uAAACAAJ",
	address   = "London; Hoboken, NJ",
	language  = "en",
	isbn      = "9780470029992",
	lccn      = "2006299689"
}

@ARTICLE{Gijsbers2019-xk,
	title         = "An Open Source {AutoML} Benchmark",
	author        = "Gijsbers, Pieter and LeDell, Erin and Thomas, Janek and
	Poirier, S{\'e}bastien and Bischl, Bernd and Vanschoren,
	Joaquin",
	abstract      = "In recent years, an active field of research has developed
	around automated machine learning (AutoML). Unfortunately,
	comparing different AutoML systems is hard and often done
	incorrectly. We introduce an open, ongoing, and extensible
	benchmark framework which follows best practices and avoids
	common mistakes. The framework is open-source, uses public
	datasets and has a website with up-to-date results. We use
	the framework to conduct a thorough comparison of 4 AutoML
	systems across 39 datasets and analyze the results.",
	month         =  jul,
	year          =  2019,
	url           = "http://arxiv.org/abs/1907.00909",
	file          = "All Papers/G/Gijsbers et al. 2019 - An Open Source AutoML Benchmark.pdf",
	archivePrefix = "arXiv",
	eprint        = "1907.00909",
	primaryClass  = "cs.LG",
	arxivid       = "1907.00909"
}

@ARTICLE{Jordan2019-hj,
	title         = "Optimal solutions to the isotonic regression problem",
	author        = "Jordan, Alexander I and M{\"u}hlemann, Anja and Ziegel,
	Johanna F",
	abstract      = "In general, the solution to a regression problem is the
	minimizer of a given loss criterion, and depends on the
	specified loss function. The nonparametric isotonic
	regression problem is special, in that optimal solutions can
	be found by solely specifying a functional. These solutions
	will then be minimizers under all loss functions
	simultaneously as long as the loss functions have the
	requested functional as the Bayes act. For the functional,
	the only requirement is that it can be defined via an
	identification function, with examples including the
	expectation, quantile, and expectile functionals.
	Generalizing classical results, we characterize the optimal
	solutions to the isotonic regression problem for such
	functionals, and extend the results from the case of totally
	ordered explanatory variables to partial orders. For total
	orders, we show that any solution resulting from the
	pool-adjacent-violators algorithm is optimal. It is
	noteworthy, that simultaneous optimality is unattainable in
	the unimodal regression problem, despite its close
	connection.",
	month         =  "9~" # apr,
	year          =  2019,
	url           = "http://arxiv.org/abs/1904.04761",
	file          = "All Papers/J/Jordan et al. 2019 - Optimal solutions to the isotonic regression problem.pdf",
	archivePrefix = "arXiv",
	eprint        = "1904.04761",
	primaryClass  = "math.ST",
	arxivid       = "1904.04761"
}

@ARTICLE{Lichtendahl2013-jr,
	title     = "Is It Better to Average Probabilities or Quantiles?",
	author    = "Lichtendahl, Kenneth C and Grushka-Cockayne, Yael and Winkler,
	Robert L",
	abstract  = "We consider two ways to aggregate expert opinions using simple
	averages: averaging probabilities and averaging quantiles. We
	examine analytical properties of these forecasts and compare
	their ability to harness the wisdom of the crowd. In terms of
	location, the two average forecasts have the same mean. The
	average quantile forecast is always sharper: it has lower
	variance than the average probability forecast. Even when the
	average probability forecast is overconfident, the shape of the
	average quantile forecast still offers the possibility of a
	better forecast. Using probability forecasts for gross domestic
	product growth and inflation from the Survey of Professional
	Forecasters, we present evidence that both when the average
	probability forecast is overconfident and when it is
	underconfident, it is outperformed by the average quantile
	forecast. Our results show that averaging quantiles is a viable
	alternative and indicate some conditions under which it is
	likely to be more useful than averaging probabilities. This
	paper was accepted by Peter Wakker, decision analysis.",
	journal   = "Management science",
	publisher = "INFORMS",
	volume    =  59,
	number    =  7,
	pages     = "1594--1611",
	month     =  "1~" # jul,
	year      =  2013,
	url       = "https://doi.org/10.1287/mnsc.1120.1667",
	file      = "All Papers/L/Lichtendahl et al. 2013 - Is It Better to Average Probabilities or Quantiles.pdf",
	issn      = "0025-1909",
	doi       = "10.1287/mnsc.1120.1667"
}

@ARTICLE{Siegert2017-we,
	title       = "Simplifying and generalising Murphy's Brier score
	decomposition",
	author      = "Siegert, Stefan",
	affiliation = "Exeter Climate Systems; University of Exeter; UK",
	abstract    = "The decomposition of the Brier score into Reliability,
	Resolution and Uncertainty has become a standard method in
	forecast verification. In this note a very simple derivation
	of the familiar Brier score decomposition is presented. The
	Reliability and Resolution terms can be calculated as average
	Brier score differences between the issued forecast, the
	recalibrated forecast and the climatological reference
	forecast. The result suggests a simple way to calculate
	similar decompositions for arbitrary verification scores, and
	that recalibration methods and reference forecasts can be
	chosen more flexibly than is generally appreciated. A new
	decomposition of the continuous ranked probability score
	(CRPS) is proposed.",
	journal     = "Quarterly Journal of the Royal Meteorological Society",
	publisher   = "Wiley",
	volume      =  143,
	number      =  703,
	pages       = "1178--1183",
	month       =  jan,
	year        =  2017,
	url         = "https://onlinelibrary.wiley.com/doi/10.1002/qj.2985",
	file        = "All Papers/S/Siegert 2017 - Simplifying and generalising Murphy's Brier score decomposition.pdf",
	copyright   = "http://onlinelibrary.wiley.com/termsAndConditions",
	issn        = "0035-9009, 1477-870X",
	doi         = "10.1002/qj.2985"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Brocker2007-fu,
	title     = "Increasing the Reliability of Reliability Diagrams",
	author    = "Br{\"o}cker, Jochen and Smith, Leonard A",
	abstract  = "Abstract The reliability diagram is a common diagnostic graph
	used to summarize and evaluate probabilistic forecasts. Its
	strengths lie in the ease with which it is produced and the
	transparency of its definition. While visually appealing, major
	long-noted shortcomings lie in the difficulty of interpreting
	the graph visually; for the most part, ambiguities arise from
	variations in the distributions of forecast probabilities and
	from various binning procedures. A resampling method for
	assigning consistency bars to the observed frequencies is
	introduced that allows for immediate visual evaluation as to
	just how likely the observed relative frequencies are under the
	assumption that the predicted probabilities are reliable.
	Further, an alternative presentation of the same information on
	probability paper eases quantitative evaluation and comparison.
	Both presentations can easily be employed for any method of
	binning.",
	journal   = "Weather and Forecasting",
	publisher = "American Meteorological Society",
	volume    =  22,
	number    =  3,
	pages     = "651--661",
	month     =  "1~" # jun,
	year      =  2007,
	url       = "https://journals.ametsoc.org/view/journals/wefo/22/3/waf993_1.xml",
	file      = "All Papers/B/Bröcker and Smith 2007 - Increasing the Reliability of Reliability Diagrams.pdf",
	language  = "en",
	issn      = "0882-8156, 1520-0434",
	doi       = "10.1175/WAF993.1"
}

@ARTICLE{Freedman1981-vx,
	title   = "On the histogram as a density estimator:L2 theory",
	author  = "Freedman, David and Diaconis, Persi",
	journal = "Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und Verwandte
	Gebiete",
	volume  =  57,
	number  =  4,
	pages   = "453--476",
	month   =  "1~" # dec,
	year    =  1981,
	url     = "https://doi.org/10.1007/BF01025868",
	file    = "All Papers/F/Freedman and Diaconis 1981 - On the histogram as a density estimator - L2 theory.pdf",
	issn    = "1432-2064",
	doi     = "10.1007/BF01025868"
}

@ARTICLE{Lei2014-js,
	title       = "Distribution-free prediction bands for non-parametric
	regression",
	author      = "Lei, Jing and Wasserman, Larry",
	affiliation = "Carnegie Mellon University; Pittsburgh, USA",
	abstract    = "Summary We study distribution-free, non-parametric prediction
	bands with a focus on their finite sample behaviour. First we
	investigate and develop different notions of finite sample
	coverage guarantees. Then we give a new prediction band by
	combining the idea of ?conformal prediction? with
	non-parametric conditional density estimation. The proposed
	estimator, called COPS (conformal optimized prediction set),
	always has a finite sample guarantee. Under regularity
	conditions the estimator converges to an oracle band at a
	minimax optimal rate. A fast approximation algorithm and a
	data-driven method for selecting the bandwidth are developed.
	The method is illustrated in simulated and real data examples.",
	journal     = "Journal of the Royal Statistical Society. Series B,
	Statistical methodology",
	publisher   = "Wiley",
	volume      =  76,
	number      =  1,
	pages       = "71--96",
	month       =  jan,
	year        =  2014,
	url         = "https://onlinelibrary.wiley.com/doi/10.1111/rssb.12021",
	file        = "All Papers/L/Lei and Wasserman 2014 - Distribution-free prediction bands for non-parametric regression.pdf",
	language    = "en",
	issn        = "1369-7412, 1467-9868",
	doi         = "10.1111/rssb.12021"
}

@ARTICLE{Gneiting2013-rl,
	title     = "Combining predictive distributions",
	author    = "Gneiting, Tilmann and Ranjan, Roopesh",
	abstract  = "In probabilistic forecasting combination formulas for the
	aggregation of predictive distributions need to be estimated
	based on past experience and training data. We study combination
	formulas and aggregation methods for predictive cumulative
	distribution functions from the perspectives of calibration and
	dispersion, taking an original prediction space approach that
	applies to discrete, mixed discrete-continuous and continuous
	predictive distributions alike. The key idea is that aggregation
	methods ought to be parsimonious, yet sufficiently flexible to
	accommodate any type of dispersion in the component
	distributions. Both linear and non-linear aggregation methods
	are investigated, including generalized, spread-adjusted and
	beta-transformed linear pools. The effects and techniques are
	demonstrated theoretically, in simulation examples, and in case
	studies, where we fit combination formulas for density forecasts
	of S\&P 500 returns and daily maximum temperature at
	Seattle-Tacoma Airport.",
	journal   = "European journal of sport science: EJSS: official journal of the
	European College of Sport Science",
	publisher = "Institute of Mathematical Statistics and Bernoulli Society",
	volume    =  7,
	number    = "none",
	pages     = "1747--1782",
	month     =  jan,
	year      =  2013,
	url       = "https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-7/issue-none/Combining-predictive-distributions/10.1214/13-EJS823.full",
	file      = "All Papers/G/Gneiting and Ranjan 2013 - Combining predictive distributions.pdf",
	keywords  = "62; 91B06; Beta transform; conditional calibration; density
	forecast; flexibly dispersive; forecast aggregation; linear
	pool; probabilistic calibration; probability integral transform;
	; ;Theory",
	language  = "en",
	issn      = "1746-1391",
	doi       = "10.1214/13-EJS823"
}

@INPROCEEDINGS{Izbicki2020-ed,
	title     = "Flexible distribution-free conditional predictive bands using
	density estimators",
	booktitle = "Proceedings of the Twenty Third International Conference on
	Artificial Intelligence and Statistics",
	author    = "Izbicki, Rafael and Shimizu, Gilson and Stern, Rafael",
	editor    = "Chiappa, Silvia and Calandra, Roberto",
	abstract  = "Conformal methods create prediction bands that control average
	coverage assuming solely i.i.d. data. Besides average coverage,
	one might also desire to control conditional coverage, that is,
	coverage for every new testing point. However, without strong
	assumptions, conditional coverage is unachievable. Given this
	limitation, the literature has focused on methods with
	asymptotical conditional coverage. In order to obtain this
	property, these methods require strong conditions on the
	dependence between the target variable and the features. We
	introduce two conformal methods based on conditional density
	estimators that do not depend on this type of assumption to
	obtain asymptotic conditional coverage: Dist-split and CD-split.
	While Dist-split asymptotically obtains optimal intervals, which
	are easier to interpret than general regions, CD-split obtains
	optimal size regions, which are smaller than intervals. CD-split
	also obtains local coverage by creating prediction bands locally
	on a partition of the features space. This partition is
	data-driven and scales to high-dimensional settings. In a wide
	variety of simulated scenarios, our methods have a better
	control of conditional coverage and have smaller length than
	previously proposed methods.",
	publisher = "PMLR",
	volume    =  108,
	pages     = "3068--3077",
	series    = "Proceedings of Machine Learning Research",
	year      =  2020,
	url       = "https://proceedings.mlr.press/v108/izbicki20a.html",
	file      = "All Papers/I/Izbicki et al. 2020 - Flexible distribution-free conditional predictive bands using density estimators.pdf"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Hernandez-Orallo2012-xa,
	title    = "A Unified View of Performance Metrics: Translating Threshold
	Choice into Expected Classification Loss",
	author   = "Hern{\'a}ndez-Orallo, Jos{\'e} and Flach, Peter and Ferri,
	C{\`e}sar",
	journal  = "Journal of machine learning research: JMLR",
	volume   =  13,
	number   =  91,
	pages    = "2813--2869",
	year     =  2012,
	url      = "https://www.jmlr.org/papers/v13/hernandez-orallo12a.html",
	file     = "All Papers/H/Hernández-Orallo et al. 2012 - A Unified View of Performance Metrics - Translating Threshold Choice into Expected Classification Loss.pdf",
	issn     = "1532-4435"
}

@INPROCEEDINGS{Papadopoulos2002-eb,
	title     = "Inductive Confidence Machines for Regression",
	booktitle = "Machine Learning: {ECML} 2002",
	author    = "Papadopoulos, Harris and Proedrou, Kostas and Vovk, Volodya and
	Gammerman, Alex",
	abstract  = "The existing methods of predicting with confidence give good
	accuracy and confidence values, but quite often are
	computationally inefficient. Some partial solutions have been
	suggested in the past. Both the original method and these
	solutions were based on transductive inference. In this paper we
	make a radical step of replacing transductive inference with
	inductive inference and define what we call the Inductive
	Confidence Machine (ICM); our main concern in this paper is the
	use of ICM in regression problems. The algorithm proposed in
	this paper is based on the Ridge Regression procedure (which is
	usually used for outputting bare predictions) and is much faster
	than the existing transductive techniques. The inductive
	approach described in this paper may be the only option
	available when dealing with large data sets.",
	publisher = "Springer Berlin Heidelberg",
	pages     = "345--356",
	year      =  2002,
	url       = "http://dx.doi.org/10.1007/3-540-36755-1_29",
	file      = "All Papers/P/Papadopoulos et al. 2002 - Inductive Confidence Machines for Regression.pdf",
	doi       = "10.1007/3-540-36755-1\_29"
}

@ARTICLE{Angelopoulos2021-rc,
	title         = "A Gentle Introduction to Conformal Prediction and
	{Distribution-Free} Uncertainty Quantification",
	author        = "Angelopoulos, Anastasios N and Bates, Stephen",
	abstract      = "Black-box machine learning models are now routinely used in
	high-risk settings, like medical diagnostics, which demand
	uncertainty quantification to avoid consequential model
	failures. Conformal prediction is a user-friendly paradigm
	for creating statistically rigorous uncertainty
	sets/intervals for the predictions of such models.
	Critically, the sets are valid in a distribution-free sense:
	they possess explicit, non-asymptotic guarantees even
	without distributional assumptions or model assumptions. One
	can use conformal prediction with any pre-trained model,
	such as a neural network, to produce sets that are
	guaranteed to contain the ground truth with a user-specified
	probability, such as 90\%. It is easy-to-understand,
	easy-to-use, and general, applying naturally to problems
	arising in the fields of computer vision, natural language
	processing, deep reinforcement learning, and so on. This
	hands-on introduction is aimed to provide the reader a
	working understanding of conformal prediction and related
	distribution-free uncertainty quantification techniques with
	one self-contained document. We lead the reader through
	practical theory for and examples of conformal prediction
	and describe its extensions to complex machine learning
	tasks involving structured outputs, distribution shift,
	time-series, outliers, models that abstain, and more.
	Throughout, there are many explanatory illustrations,
	examples, and code samples in Python. With each code sample
	comes a Jupyter notebook implementing the method on a
	real-data example; the notebooks can be accessed and easily
	run using our codebase.",
	month         =  jul,
	year          =  2021,
	url           = "http://arxiv.org/abs/2107.07511",
	file          = "All Papers/A/Angelopoulos and Bates 2021 - A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification.pdf",
	archivePrefix = "arXiv",
	eprint        = "2107.07511",
	primaryClass  = "cs.LG",
	arxivid       = "2107.07511"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Shafer2008-tg,
	title     = "A Tutorial on Conformal Prediction",
	author    = "Shafer, G and Vovk, V",
	abstract  = "Conformal prediction uses past experience to determine precise
	levels of confidence in new predictions. Given an error
	probability $\epsilon$, together with a method that makes a
	prediction ˆy of a label y, it produces a set of labels,
	typically containing ˆy, that also contains y with …",
	journal   = "Journal of machine learning research: JMLR",
	publisher = "jmlr.org",
	year      =  2008,
	url       = "https://www.jmlr.org/papers/volume9/shafer08a/shafer08a.pdf",
	file      = "All Papers/S/Shafer and Vovk 2008 - A Tutorial on Conformal Prediction.pdf",
	keywords  = "Conformal",
	issn      = "1532-4435"
}

@ARTICLE{Pineau2021-gg,
	title    = "Improving Reproducibility in Machine Learning {Research(A} Report
	from the {NeurIPS} 2019 Reproducibility Program)",
	author   = "Pineau, Joelle and Vincent-Lamarre, Philippe and Sinha, Koustuv
	and Lariviere, Vincent and Beygelzimer, Alina and d'Alche-Buc,
	Florence and Fox, Emily and Larochelle, Hugo",
	journal  = "Journal of machine learning research: JMLR",
	volume   =  22,
	number   =  164,
	pages    = "1--20",
	year     =  2021,
	url      = "http://jmlr.org/papers/v22/20-303.html",
	file     = "All Papers/P/Pineau et al. 2021 - Improving Reproducibility in Machine Learning Research(A Report from the NeurIPS 2019 Reproducibility Program).pdf",
	issn     = "1532-4435"
}

@INPROCEEDINGS{Kumar2018-cw,
	title     = "Trainable Calibration Measures for Neural Networks from Kernel
	Mean Embeddings",
	booktitle = "Proceedings of the 35th International Conference on Machine
	Learning",
	author    = "Kumar, Aviral and Sarawagi, Sunita and Jain, Ujjwal",
	editor    = "Dy, Jennifer and Krause, Andreas",
	abstract  = "Modern neural networks have recently been found to be poorly
	calibrated, primarily in the direction of over-confidence.
	Methods like entropy penalty and temperature smoothing improve
	calibration by clamping confidence, but in doing so compromise
	the many legitimately confident predictions. We propose a more
	principled fix that minimizes an explicit calibration error
	during training. We present MMCE, a RKHS kernel based measure of
	calibration that is efficiently trainable alongside the negative
	likelihood loss without careful hyper-parameter tuning.
	Theoretically too, MMCE is a sound measure of calibration that
	is minimized at perfect calibration, and whose finite sample
	estimates are consistent and enjoy fast convergence rates.
	Extensive experiments on several network architectures
	demonstrate that MMCE is a fast, stable, and accurate method to
	minimize calibration error while maximally preserving the number
	of high confidence predictions.",
	publisher = "PMLR",
	volume    =  80,
	pages     = "2805--2814",
	series    = "Proceedings of Machine Learning Research",
	year      =  2018,
	url       = "https://proceedings.mlr.press/v80/kumar18a.html",
	file      = "All Papers/K/Kumar et al. 2018 - Trainable Calibration Measures for Neural Networks from Kernel Mean Embeddings.pdf",
	keywords  = "Calibration"
}

@UNPUBLISHED{Authors2020-ja,
	title    = "{QUANTILE} {REGULARIZATION}: {TOWARDS} {IMPLICIT} {CALIBRATION}
	{OF} {REGRESSION} {MODELS}",
	author   = "Authors, Anonymous",
	abstract = "Recent works have shown that most deep learning models are often
	poorly calibrated, i.e., they may produce overconfident
	predictions that are wrong, implying that their uncertainty
	estimates are unreliable. While a number of approaches have been
	proposed recently to calibrate classification models, relatively
	little work exists on calibrating regression models. Isotonic
	Regression has recently been advocated for regression
	calibration. We provide a detailed formal analysis of the
	\textbackslashemph\{side-effects\} of Isotonic Regression when
	used for regression calibration. To address this, we recast
	quantile calibration as entropy estimation, and leverage this
	idea to construct a novel quantile regularizer, which can be used
	in any optimization based probabilisitc regression models. Unlike
	most of the existing approaches for calibrating regression
	models, which are based on \textbackslashemph\{post-hoc\}
	processing of the model's output, and require an additional
	dataset, our method is trainable in an end-to-end fashion,
	without requiring an additional dataset. We provide empirical
	results demonstrating that our approach improves calibration for
	regression models trained on diverse architectures that provide
	uncertainty estimates, such as Dropout VI, Deep Ensembles",
	month    =  sep,
	year     =  2020,
	url      = "https://openreview.net/pdf?id=I3zV6igAT9",
	file     = "All Papers/A/Authors 2020 - QUANTILE REGULARIZATION - TOWARDS IMPLICIT CALIBRATION OF REGRESSION MODELS.pdf",
	keywords = "Calibration;Quantiles"
}

@ARTICLE{Fakoor2021-os,
	title         = "Flexible Model Aggregation for Quantile Regression",
	author        = "Fakoor, Rasool and Kim, Taesup and Mueller, Jonas and Smola,
	Alexander J and Tibshirani, Ryan J",
	abstract      = "Quantile regression is a fundamental problem in statistical
	learning motivated by the need to quantify uncertainty in
	predictions, or to model a diverse population without being
	overly reductive. For instance, epidemiological forecasts,
	cost estimates, and revenue predictions all benefit from
	being able to quantify the range of possible values
	accurately. As such, many models have been developed for
	this problem over many years of research in econometrics,
	statistics, and machine learning. Rather than proposing yet
	another (new) algorithm for quantile regression we adopt a
	meta viewpoint: we investigate methods for aggregating any
	number of conditional quantile models, in order to improve
	accuracy and robustness. We consider weighted ensembles
	where weights may vary over not only individual models, but
	also over quantile levels, and feature values. All of the
	models we consider in this paper can be fit using modern
	deep learning toolkits, and hence are widely accessible
	(from an implementation point of view) and scalable. To
	improve the accuracy of the predicted quantiles (or
	equivalently, prediction intervals), we develop tools for
	ensuring that quantiles remain monotonically ordered, and
	apply conformal calibration methods. These can be used
	without any modification of the original library of base
	models. We also review some basic theory surrounding
	quantile aggregation and related scoring rules, and
	contribute a few new results to this literature (for
	example, the fact that post sorting or post isotonic
	regression can only improve the weighted interval score).
	Finally, we provide an extensive suite of empirical
	comparisons across 34 data sets from two different benchmark
	repositories.",
	month         =  feb,
	year          =  2021,
	url           = "http://arxiv.org/abs/2103.00083",
	file          = "All Papers/F/Fakoor et al. 2021 - Flexible Model Aggregation for Quantile Regression.pdf",
	keywords      = "Quantiles",
	archivePrefix = "arXiv",
	eprint        = "2103.00083",
	primaryClass  = "stat.ML",
	arxivid       = "2103.00083"
}

@ARTICLE{Lones2021-zb,
	title         = "How to avoid machine learning pitfalls: a guide for academic
	researchers",
	author        = "Lones, Michael A",
	abstract      = "This document gives a concise outline of some of the common
	mistakes that occur when using machine learning techniques,
	and what can be done to avoid them. It is intended primarily
	as a guide for research students, and focuses on issues that
	are of particular concern within academic research, such as
	the need to do rigorous comparisons and reach valid
	conclusions. It covers five stages of the machine learning
	process: what to do before model building, how to reliably
	build models, how to robustly evaluate models, how to
	compare models fairly, and how to report results.",
	month         =  "5~" # aug,
	year          =  2021,
	url           = "http://arxiv.org/abs/2108.02497",
	file          = "All Papers/L/Lones 2021 - How to avoid machine learning pitfalls - a guide for academic researchers.pdf",
	archivePrefix = "arXiv",
	eprint        = "2108.02497",
	primaryClass  = "cs.LG",
	arxivid       = "2108.02497"
}

@ARTICLE{Gupta2020-oi,
	title         = "Calibration of Neural Networks using Splines",
	author        = "Gupta, Kartik and Rahimi, Amir and Ajanthan, Thalaiyasingam
	and Mensink, Thomas and Sminchisescu, Cristian and Hartley,
	Richard",
	abstract      = "Calibrating neural networks is of utmost importance when
	employing them in safety-critical applications where the
	downstream decision making depends on the predicted
	probabilities. Measuring calibration error amounts to
	comparing two empirical distributions. In this work, we
	introduce a binning-free calibration measure inspired by the
	classical Kolmogorov-Smirnov (KS) statistical test in which
	the main idea is to compare the respective cumulative
	probability distributions. From this, by approximating the
	empirical cumulative distribution using a differentiable
	function via splines, we obtain a recalibration function,
	which maps the network outputs to actual (calibrated) class
	assignment probabilities. The spine-fitting is performed
	using a held-out calibration set and the obtained
	recalibration function is evaluated on an unseen test set.
	We tested our method against existing calibration approaches
	on various image classification datasets and our
	spline-based recalibration approach consistently outperforms
	existing methods on KS error as well as other commonly used
	calibration measures.",
	month         =  "23~" # jun,
	year          =  2020,
	url           = "http://arxiv.org/abs/2006.12800",
	file          = "All Papers/G/Gupta et al. 2020 - Calibration of Neural Networks using Splines.pdf",
	keywords      = "Calibration;Classification",
	archivePrefix = "arXiv",
	eprint        = "2006.12800",
	primaryClass  = "cs.LG",
	arxivid       = "2006.12800"
}

@ARTICLE{Gawlikowski2021-ty,
	title         = "A Survey of Uncertainty in Deep Neural Networks",
	author        = "Gawlikowski, Jakob and Tassi, Cedrique Rovile Njieutcheu and
	Ali, Mohsin and Lee, Jongseok and Humt, Matthias and Feng,
	Jianxiang and Kruspe, Anna and Triebel, Rudolph and Jung,
	Peter and Roscher, Ribana and Shahzad, Muhammad and Yang,
	Wen and Bamler, Richard and Zhu, Xiao Xiang",
	abstract      = "Due to their increasing spread, confidence in neural network
	predictions became more and more important. However, basic
	neural networks do not deliver certainty estimates or suffer
	from over or under confidence. Many researchers have been
	working on understanding and quantifying uncertainty in a
	neural network's prediction. As a result, different types
	and sources of uncertainty have been identified and a
	variety of approaches to measure and quantify uncertainty in
	neural networks have been proposed. This work gives a
	comprehensive overview of uncertainty estimation in neural
	networks, reviews recent advances in the field, highlights
	current challenges, and identifies potential research
	opportunities. It is intended to give anyone interested in
	uncertainty estimation in neural networks a broad overview
	and introduction, without presupposing prior knowledge in
	this field. A comprehensive introduction to the most crucial
	sources of uncertainty is given and their separation into
	reducible model uncertainty and not reducible data
	uncertainty is presented. The modeling of these
	uncertainties based on deterministic neural networks,
	Bayesian neural networks, ensemble of neural networks, and
	test-time data augmentation approaches is introduced and
	different branches of these fields as well as the latest
	developments are discussed. For a practical application, we
	discuss different measures of uncertainty, approaches for
	the calibration of neural networks and give an overview of
	existing baselines and implementations. Different examples
	from the wide spectrum of challenges in different fields
	give an idea of the needs and challenges regarding
	uncertainties in practical applications. Additionally, the
	practical limitations of current methods for mission- and
	safety-critical real world applications are discussed and an
	outlook on the next steps towards a broader usage of such
	methods is given.",
	month         =  jul,
	year          =  2021,
	url           = "http://arxiv.org/abs/2107.03342",
	file          = "All Papers/G/Gawlikowski et al. 2021 - A Survey of Uncertainty in Deep Neural Networks.pdf",
	keywords      = "Uncertainty quantification",
	archivePrefix = "arXiv",
	eprint        = "2107.03342",
	primaryClass  = "cs.LG",
	arxivid       = "2107.03342"
}

@ARTICLE{Kabir2018-di,
	title    = "Neural {Network-Based} Uncertainty Quantification: A Survey of
	Methodologies and Applications",
	author   = "Kabir, H M Dipu and Khosravi, Abbas and Hosen, Mohammad Anwar and
	Nahavandi, Saeid",
	abstract = "Uncertainty quantification plays a critical role in the process
	of decision making and optimization in many fields of science and
	engineering. The field has gained an overwhelming attention among
	researchers in recent years resulting in an arsenal of different
	methods. Probabilistic forecasting and in particular prediction
	intervals (PIs) are one of the techniques most widely used in the
	literature for uncertainty quantification. Researchers have
	reported studies of uncertainty quantification in critical
	applications such as medical diagnostics, bioinformatics,
	renewable energies, and power grids. The purpose of this survey
	paper is to comprehensively study neural network-based methods
	for construction of prediction intervals. It will cover how PIs
	are constructed, optimized, and applied for decision-making in
	presence of uncertainties. Also, different criteria for unbiased
	PI evaluation are investigated. The paper also provides some
	guidelines for further research in the field of neural
	network-based uncertainty quantification.",
	journal  = "IEEE Access",
	volume   =  6,
	pages    = "36218--36234",
	year     =  2018,
	url      = "http://dx.doi.org/10.1109/ACCESS.2018.2836917",
	file     = "All Papers/K/Kabir et al. 2018 - Neural Network-Based Uncertainty Quantification - A Survey of Methodologies and Applications.pdf",
	keywords = "Uncertainty;Probability density function;Artificial neural
	networks;Probabilistic logic;Forecasting;Upper bound;Prediction
	interval;uncertainty quantification;heteroscedastic
	uncertainty;neural network;forecast;time series
	data;regression;probability;Uncertainty quantification",
	issn     = "2169-3536",
	doi      = "10.1109/ACCESS.2018.2836917"
}

@ARTICLE{Gulshan2016-kw,
	title       = "Development and Validation of a Deep Learning Algorithm for
	Detection of Diabetic Retinopathy in Retinal Fundus
	Photographs",
	author      = "Gulshan, Varun and Peng, Lily and Coram, Marc and Stumpe,
	Martin C and Wu, Derek and Narayanaswamy, Arunachalam and
	Venugopalan, Subhashini and Widner, Kasumi and Madams, Tom and
	Cuadros, Jorge and Kim, Ramasamy and Raman, Rajiv and Nelson,
	Philip C and Mega, Jessica L and Webster, Dale R",
	affiliation = "Google Inc, Mountain View, California. Google Inc, Mountain
	View, California2Department of Computer Science, University of
	Texas, Austin. EyePACS LLC, San Jose, California4School of
	Optometry, Vision Science Graduate Group, University of
	California, Berkeley. Aravind Medical Research Foundation,
	Aravind Eye Care System, Madurai, India. Shri Bhagwan Mahavir
	Vitreoretinal Services, Sankara Nethralaya, Chennai, Tamil
	Nadu, India. Verily Life Sciences, Mountain View,
	California8Cardiovascular Division, Department of Medicine,
	Brigham and Women's Hospital and Harvard Medical School,
	Boston, Massachusetts.",
	abstract    = "Importance: Deep learning is a family of computational methods
	that allow an algorithm to program itself by learning from a
	large set of examples that demonstrate the desired behavior,
	removing the need to specify rules explicitly. Application of
	these methods to medical imaging requires further assessment
	and validation. Objective: To apply deep learning to create an
	algorithm for automated detection of diabetic retinopathy and
	diabetic macular edema in retinal fundus photographs. Design
	and Setting: A specific type of neural network optimized for
	image classification called a deep convolutional neural
	network was trained using a retrospective development data set
	of 128 175 retinal images, which were graded 3 to 7 times for
	diabetic retinopathy, diabetic macular edema, and image
	gradability by a panel of 54 US licensed ophthalmologists and
	ophthalmology senior residents between May and December 2015.
	The resultant algorithm was validated in January and February
	2016 using 2 separate data sets, both graded by at least 7 US
	board-certified ophthalmologists with high intragrader
	consistency. Exposure: Deep learning-trained algorithm. Main
	Outcomes and Measures: The sensitivity and specificity of the
	algorithm for detecting referable diabetic retinopathy (RDR),
	defined as moderate and worse diabetic retinopathy, referable
	diabetic macular edema, or both, were generated based on the
	reference standard of the majority decision of the
	ophthalmologist panel. The algorithm was evaluated at 2
	operating points selected from the development set, one
	selected for high specificity and another for high
	sensitivity. Results: The EyePACS-1 data set consisted of 9963
	images from 4997 patients (mean age, 54.4 years; 62.2\% women;
	prevalence of RDR, 683/8878 fully gradable images [7.8\%]);
	the Messidor-2 data set had 1748 images from 874 patients
	(mean age, 57.6 years; 42.6\% women; prevalence of RDR,
	254/1745 fully gradable images [14.6\%]). For detecting RDR,
	the algorithm had an area under the receiver operating curve
	of 0.991 (95\% CI, 0.988-0.993) for EyePACS-1 and 0.990 (95\%
	CI, 0.986-0.995) for Messidor-2. Using the first operating cut
	point with high specificity, for EyePACS-1, the sensitivity
	was 90.3\% (95\% CI, 87.5\%-92.7\%) and the specificity was
	98.1\% (95\% CI, 97.8\%-98.5\%). For Messidor-2, the
	sensitivity was 87.0\% (95\% CI, 81.1\%-91.0\%) and the
	specificity was 98.5\% (95\% CI, 97.7\%-99.1\%). Using a
	second operating point with high sensitivity in the
	development set, for EyePACS-1 the sensitivity was 97.5\% and
	specificity was 93.4\% and for Messidor-2 the sensitivity was
	96.1\% and specificity was 93.9\%. Conclusions and Relevance:
	In this evaluation of retinal fundus photographs from adults
	with diabetes, an algorithm based on deep machine learning had
	high sensitivity and specificity for detecting referable
	diabetic retinopathy. Further research is necessary to
	determine the feasibility of applying this algorithm in the
	clinical setting and to determine whether use of the algorithm
	could lead to improved care and outcomes compared with current
	ophthalmologic assessment.",
	journal     = "JAMA: the journal of the American Medical Association",
	volume      =  316,
	number      =  22,
	pages       = "2402--2410",
	month       =  "13~" # dec,
	year        =  2016,
	url         = "http://dx.doi.org/10.1001/jama.2016.17216",
	file        = "All Papers/G/Gulshan et al. 2016 - Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs.pdf",
	language    = "en",
	issn        = "0098-7484, 1538-3598",
	pmid        = "27898976",
	doi         = "10.1001/jama.2016.17216"
}

@ARTICLE{Zhou2020-ny,
	title    = "Graph neural networks: A review of methods and applications",
	author   = "Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan
	and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li,
	Changcheng and Sun, Maosong",
	abstract = "Lots of learning tasks require dealing with graph data which
	contains rich relation information among elements. Modeling
	physics systems, learning molecular fingerprints, predicting
	protein interface, and classifying diseases demand a model to
	learn from graph inputs. In other domains such as learning from
	non-structural data like texts and images, reasoning on extracted
	structures (like the dependency trees of sentences and the scene
	graphs of images) is an important research topic which also needs
	graph reasoning models. Graph neural networks (GNNs) are neural
	models that capture the dependence of graphs via message passing
	between the nodes of graphs. In recent years, variants of GNNs
	such as graph convolutional network (GCN), graph attention
	network (GAT), graph recurrent network (GRN) have demonstrated
	ground-breaking performances on many deep learning tasks. In this
	survey, we propose a general design pipeline for GNN models and
	discuss the variants of each component, systematically categorize
	the applications, and propose four open problems for future
	research.",
	journal  = "AI Open",
	volume   =  1,
	pages    = "57--81",
	month    =  "1~" # jan,
	year     =  2020,
	url      = "https://www.sciencedirect.com/science/article/pii/S2666651021000012",
	file     = "All Papers/Z/Zhou et al. 2020 - Graph neural networks - A review of methods and applications.pdf",
	keywords = "Deep learning; Graph neural network",
	issn     = "2666-6510",
	doi      = "10.1016/j.aiopen.2021.01.001"
}

@ARTICLE{Ross2020-nh,
	title    = "Ensembles of Locally Independent Prediction Models",
	author   = "Ross, Andrew and Pan, Weiwei and Celi, Leo and Doshi-Velez,
	Finale",
	journal  = "Proceedings of the AAAI Conference on Artificial Intelligence",
	volume   =  34,
	number   =  04,
	pages    = "5527--5536",
	month    =  "3~" # apr,
	year     =  2020,
	url      = "https://ojs.aaai.org/index.php/AAAI/article/view/6004",
	file     = "All Papers/R/Ross et al. 2020 - Ensembles of Locally Independent Prediction Models.pdf",
	keywords = "Ensembling",
	language = "en",
	issn     = "2374-3468, 2374-3468",
	doi      = "10.1609/aaai.v34i04.6004"
}

@INPROCEEDINGS{Dietterich2000-vc,
	title     = "Ensemble Methods in Machine Learning",
	booktitle = "Proceedings of the First International Workshop on Multiple
	Classifier Systems",
	author    = "Dietterich, Thomas G",
	abstract  = "Ensemble methods are learning algorithms that construct a set of
	classifiers and then classify new data points by taking a
	(weighted) vote of their predictions. The original ensemble
	method is Bayesian averaging, but more recent algorithms include
	error-correcting output coding, Bagging, and boosting. This
	paper reviews these methods and explains why ensembles can often
	perform better than any single classifier. Some previous studies
	comparing ensemble methods are reviewed, and some new
	experiments are presented to uncover the reasons that Adaboost
	does not overfit rapidly.",
	publisher = "Springer-Verlag",
	pages     = "1--15",
	series    = "MCS '00",
	month     =  "21~" # jun,
	year      =  2000,
	url       = "https://dl.acm.org/doi/10.5555/648054.743935",
	address   = "Berlin, Heidelberg",
	keywords  = "Ensembling",
	isbn      = "9783540677048"
}

@ARTICLE{Begoli2019-go,
	title     = "The need for uncertainty quantification in machine-assisted
	medical decision making",
	author    = "Begoli, Edmon and Bhattacharya, Tanmoy and Kusnezov, Dimitri",
	abstract  = "Medicine, even from the earliest days of artificial intelligence
	(AI) research, has been one of the most inspiring and promising
	domains for the application of AI-based approaches. Equally, it
	has been one of the more challenging areas to see an effective
	adoption. There are many reasons for this, primarily the
	reluctance to delegate decision making to machine intelligence
	in cases where patient safety is at stake. To address some of
	these challenges, medical AI, especially in its modern data-rich
	deep learning guise, needs to develop a principled and formal
	uncertainty quantification (UQ) discipline, just as we have seen
	in fields such as nuclear stockpile stewardship and risk
	management. The data-rich world of AI-based learning and the
	frequent absence of a well-understood underlying theory poses
	its own unique challenges to straightforward adoption of UQ.
	These challenges, while not trivial, also present significant
	new research opportunities for the development of new
	theoretical approaches, and for the practical applications of UQ
	in the area of machine-assisted medical decision making.
	Understanding prediction system structure and defensibly
	quantifying uncertainty is possible, and, if done, can
	significantly benefit both research and practical applications
	of AI in this critical domain. Arguably one of the most
	promising as well as critical applications of deep learning is
	in supporting medical sciences and decision making. It is time
	to develop methods for systematically quantifying uncertainty
	underlying deep learning processes, which would lead to
	increased confidence in practical applicability of these
	approaches.",
	journal   = "Nature Machine Intelligence",
	publisher = "Nature Publishing Group",
	volume    =  1,
	number    =  1,
	pages     = "20--23",
	month     =  "7~" # jan,
	year      =  2019,
	url       = "https://www.nature.com/articles/s42256-018-0004-1",
	file      = "All Papers/B/Begoli et al. 2019 - The need for uncertainty quantification in machine-assisted medical decision making.pdf",
	language  = "en",
	issn      = "2522-5839, 2522-5839",
	doi       = "10.1038/s42256-018-0004-1"
}

@BOOK{Wasserman2010-ml,
	title     = "All of Statistics: A Concise Course in Statistical Inference",
	author    = "Wasserman, Larry",
	abstract  = "Taken literally, the title ``All of Statistics'' is an
	exaggeration. But in spirit, the title is apt, as the book does
	cover a much broader range of topics than a typical introductory
	book on mathematical statistics. This book is for people who
	want to learn probability and statistics quickly. It is suitable
	for graduate or advanced undergraduate students in computer
	science, mathematics, statistics, and related disciplines. The
	book includes modern topics like nonparametric curve estimation,
	bootstrapping, and clas sification, topics that are usually
	relegated to follow-up courses. The reader is presumed to know
	calculus and a little linear algebra. No previous knowledge of
	probability and statistics is required. Statistics, data mining,
	and machine learning are all concerned with collecting and
	analyzing data. For some time, statistics research was con
	ducted in statistics departments while data mining and machine
	learning re search was conducted in computer science
	departments. Statisticians thought that computer scientists were
	reinventing the wheel. Computer scientists thought that
	statistical theory didn't apply to their problems. Things are
	changing. Statisticians now recognize that computer scientists
	are making novel contributions while computer scientists now
	recognize the generality of statistical theory and methodology.
	Clever data mining algo rithms are more scalable than
	statisticians ever thought possible. Formal sta tistical theory
	is more pervasive than computer scientists had realized.",
	publisher = "Springer New York",
	month     =  "1~" # dec,
	year      =  2010,
	url       = "https://play.google.com/store/books/details?id=RMdgcgAACAAJ",
	language  = "en",
	isbn      = "9781441923226"
}

@ARTICLE{Leeuw2009-ni,
	title     = "Isotone optimization inR: Pool-adjacent-violators algorithm
	({PAVA}) and active set methods",
	author    = "Leeuw, Jan de and Hornik, Kurt and Mair, Patrick",
	abstract  = "In this paper we give a general framework for isotone
	optimization. First we discuss a generalized version of the
	pool-adjacent-violators algorithm (PAVA) to minimize a separable
	convex function with simple chain constraints. Besides of
	general convex functions we extend existing PAVA implementations
	in terms of observation weights, approaches for tie handling,
	and responses from repeated measurement designs. Since isotone
	optimization problems can be formulated as convex programming
	problems with linear constraints we the develop a primal active
	set method to solve such problem. This methodology is applied on
	specific loss functions relevant in statistics. Both approaches
	are implemented in the R package isotone.",
	journal   = "Journal of statistical software",
	publisher = "Foundation for Open Access Statistic",
	volume    =  32,
	number    =  5,
	pages     = "1--24",
	year      =  2009,
	url       = "http://www.jstatsoft.org/v32/i05/",
	file      = "All Papers/L/Leeuw et al. 2009 - Isotone optimization inR - Pool-adjacent-violators algorithm (PAVA) and active set methods.pdf",
	issn      = "1548-7660",
	doi       = "10.18637/jss.v032.i05"
}

@BOOK{Sutton2018-sb,
	title     = "Reinforcement Learning, second edition: An Introduction",
	author    = "Sutton, Richard S and Barto, Andrew G",
	abstract  = "The significantly expanded and updated new edition of a widely
	used text on reinforcement learning, one of the most active
	research areas in artificial intelligence.Reinforcement
	learning, one of the most active research areas in artificial
	intelligence, is a computational approach to learning whereby an
	agent tries to maximize the total amount of reward it receives
	while interacting with a complex, uncertain environment. In
	Reinforcement Learning, Richard Sutton and Andrew Barto provide
	a clear and simple account of the field's key ideas and
	algorithms. This second edition has been significantly expanded
	and updated, presenting new topics and updating coverage of
	other topics.Like the first edition, this second edition focuses
	on core online learning algorithms, with the more mathematical
	material set off in shaded boxes. Part I covers as much of
	reinforcement learning as possible without going beyond the
	tabular case for which exact solutions can be found. Many
	algorithms presented in this part are new to the second edition,
	including UCB, Expected Sarsa, and Double Learning. Part II
	extends these ideas to function approximation, with new sections
	on such topics as artificial neural networks and the Fourier
	basis, and offers expanded treatment of off-policy learning and
	policy-gradient methods. Part III has new chapters on
	reinforcement learning's relationships to psychology and
	neuroscience, as well as an updated case-studies chapter
	including AlphaGo and AlphaGo Zero, Atari game playing, and IBM
	Watson's wagering strategy. The final chapter discusses the
	future societal impacts of reinforcement learning.",
	publisher = "MIT Press",
	month     =  "13~" # nov,
	year      =  2018,
	url       = "https://play.google.com/store/books/details?id=sWV0DwAAQBAJ",
	address   = "Cambridge, MA, USA",
	language  = "en",
	isbn      = "9780262039246"
}

@ARTICLE{Cybenko1989-jx,
	title    = "Approximation by superpositions of a sigmoidal function",
	author   = "Cybenko, G",
	abstract = "In this paper we demonstrate that finite linear combinations of
	compositions of a fixed, univariate function and a set of affine
	functionals can uniformly approximate any continuous function ofn
	real variables with support in the unit hypercube; only mild
	conditions are imposed on the univariate function. Our results
	settle an open question about representability in the class of
	single hidden layer neural networks. In particular, we show that
	arbitrary decision regions can be arbitrarily well approximated
	by continuous feedforward neural networks with only a single
	internal, hidden layer and any continuous sigmoidal nonlinearity.
	The paper discusses approximation properties of other possible
	types of nonlinearities that might be implemented by artificial
	neural networks.",
	journal  = "Mathematics of Control, Signals, and Systems",
	volume   =  2,
	number   =  4,
	pages    = "303--314",
	month    =  "1~" # dec,
	year     =  1989,
	url      = "https://doi.org/10.1007/BF02551274",
	file     = "All Papers/C/Cybenko 1989 - Approximation by superpositions of a sigmoidal function.pdf",
	issn     = "0932-4194",
	doi      = "10.1007/BF02551274"
}

@ARTICLE{Vapnik1999-cq,
	title       = "An overview of statistical learning theory",
	author      = "Vapnik, V N",
	affiliation = "AT\&T Labs-Research, Red Bank, NJ 07701, USA.",
	abstract    = "Statistical learning theory was introduced in the late 1960's.
	Until the 1990's it was a purely theoretical analysis of the
	problem of function estimation from a given collection of
	data. In the middle of the 1990's new types of learning
	algorithms (called support vector machines) based on the
	developed theory were proposed. This made statistical learning
	theory not only a tool for the theoretical analysis but also a
	tool for creating practical algorithms for estimating
	multidimensional functions. This article presents a very
	general overview of statistical learning theory including both
	theoretical and algorithmic aspects of the theory. The goal of
	this overview is to demonstrate how the abstract learning
	theory established conditions for generalization which are
	more general than those discussed in classical statistical
	paradigms and how the understanding of these conditions
	inspired new algorithmic approaches to function estimation
	problems. A more detailed overview of the theory (without
	proofs) can be found in Vapnik (1995). In Vapnik (1998) one
	can find detailed description of the theory (including
	proofs).",
	journal     = "IEEE transactions on neural networks / a publication of the
	IEEE Neural Networks Council",
	volume      =  10,
	number      =  5,
	pages       = "988--999",
	year        =  1999,
	url         = "http://dx.doi.org/10.1109/72.788640",
	file        = "All Papers/V/Vapnik 1999 - An overview of statistical learning theory.pdf",
	language    = "en",
	issn        = "1045-9227",
	pmid        = "18252602",
	doi         = "10.1109/72.788640"
}

@ARTICLE{Kingma2014-pd,
	title         = "Adam: A Method for Stochastic Optimization",
	author        = "Kingma, Diederik P and Ba, Jimmy",
	abstract      = "We introduce Adam, an algorithm for first-order
	gradient-based optimization of stochastic objective
	functions, based on adaptive estimates of lower-order
	moments. The method is straightforward to implement, is
	computationally efficient, has little memory requirements,
	is invariant to diagonal rescaling of the gradients, and is
	well suited for problems that are large in terms of data
	and/or parameters. The method is also appropriate for
	non-stationary objectives and problems with very noisy
	and/or sparse gradients. The hyper-parameters have intuitive
	interpretations and typically require little tuning. Some
	connections to related algorithms, on which Adam was
	inspired, are discussed. We also analyze the theoretical
	convergence properties of the algorithm and provide a regret
	bound on the convergence rate that is comparable to the best
	known results under the online convex optimization
	framework. Empirical results demonstrate that Adam works
	well in practice and compares favorably to other stochastic
	optimization methods. Finally, we discuss AdaMax, a variant
	of Adam based on the infinity norm.",
	month         =  "22~" # dec,
	year          =  2014,
	url           = "http://arxiv.org/abs/1412.6980",
	file          = "All Papers/K/Kingma and Ba 2014 - Adam - A Method for Stochastic Optimization.pdf",
	archivePrefix = "arXiv",
	eprint        = "1412.6980",
	primaryClass  = "cs.LG",
	arxivid       = "1412.6980"
}

@BOOK{Goodfellow2016-hp,
	title     = "Deep Learning",
	author    = "Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron",
	abstract  = "An introduction to a broad range of topics in deep learning,
	covering mathematical and conceptual background, deep learning
	techniques used in industry, and research perspectives.``Written
	by three experts in the field, Deep Learning is the only
	comprehensive book on the subject.''---Elon Musk, cochair of
	OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a
	form of machine learning that enables computers to learn from
	experience and understand the world in terms of a hierarchy of
	concepts. Because the computer gathers knowledge from
	experience, there is no need for a human computer operator to
	formally specify all the knowledge that the computer needs. The
	hierarchy of concepts allows the computer to learn complicated
	concepts by building them out of simpler ones; a graph of these
	hierarchies would be many layers deep. This book introduces a
	broad range of topics in deep learning. The text offers
	mathematical and conceptual background, covering relevant
	concepts in linear algebra, probability theory and information
	theory, numerical computation, and machine learning. It
	describes deep learning techniques used by practitioners in
	industry, including deep feedforward networks, regularization,
	optimization algorithms, convolutional networks, sequence
	modeling, and practical methodology; and it surveys such
	applications as natural language processing, speech recognition,
	computer vision, online recommendation systems, bioinformatics,
	and videogames. Finally, the book offers research perspectives,
	covering such theoretical topics as linear factor models,
	autoencoders, representation learning, structured probabilistic
	models, Monte Carlo methods, the partition function, approximate
	inference, and deep generative models. Deep Learning can be used
	by undergraduate or graduate students planning careers in either
	industry or research, and by software engineers who want to
	begin using deep learning in their products or platforms. A
	website offers supplementary material for both readers and
	instructors.",
	publisher = "MIT Press",
	volume    =  1,
	month     =  nov,
	year      =  2016,
	url       = "https://play.google.com/store/books/details?id=omivDQAAQBAJ",
	file      = "All Papers/G/Goodfellow et al. 2016 - Deep Learning.pdf",
	language  = "en",
	isbn      = "9780262337373"
}

@ARTICLE{Murphy1973-vi,
	title     = "A New Vector Partition of the Probability Score",
	author    = "Murphy, Allan H",
	abstract  = "Abstract A new vector partition of the probability, or Brier,
	score (PS) is formulated and the nature and properties of this
	partition are described. The relationships between the terms in
	this partition and the terms in the original vector partition of
	the PS are indicated. The new partition consists of three terms:
	1) a measure of the uncertainty inherent in the events, or
	states, on the occasions of concern (namely, the PS for the
	sample relative frequencies); 2) a measure of the reliability of
	the forecasts; and 3) a new measure of the resolution of the
	forecasts. These measures of reliability and resolution are and
	are not, respectively, equivalent (i.e., linearly related) to
	the measures of reliability and resolution provided by the
	original partition. Two sample collections of probability
	forecasts are used to illustrate the differences and
	relationships between these partitions. Finally, the two
	partitions are compared, with particular reference to the
	attributes of the forecasts with which the partitions are
	concerned, the interpretation of the partitions in geometric
	terms, and the use of the partitions as the bases for the
	formulation of measures to evaluate probability forecasts. The
	results of these comparisons indicate that the new partition
	offers certain advantages vis-{\`a}-vis the original partition.",
	journal   = "Journal of Applied Meteorology and Climatology",
	publisher = "American Meteorological Society",
	volume    =  12,
	number    =  4,
	pages     = "595--600",
	month     =  jun,
	year      =  1973,
	url       = "https://journals.ametsoc.org/view/journals/apme/12/4/1520-0450_1973_012_0595_anvpot_2_0_co_2.xml",
	file      = "All Papers/M/Murphy 1973 - A New Vector Partition of the Probability Score.pdf",
	keywords  = "Scoring rules",
	language  = "en",
	issn      = "1558-8424",
	doi       = "10.1175/1520-0450(1973)012<0595:ANVPOT>2.0.CO;2"
}

@ARTICLE{LeCun2015-yz,
	title       = "Deep learning",
	author      = "LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey",
	affiliation = "1] Facebook AI Research, 770 Broadway, New York, New York
	10003 USA. [2] New York University, 715 Broadway, New York,
	New York 10003, USA. Department of Computer Science and
	Operations Research Universit{\'e} de Montr{\'e}al, Pavillon
	Andr{\'e}-Aisenstadt, PO Box 6128 Centre-Ville STN
	Montr{\'e}al, Quebec H3C 3J7, Canada. 1] Google, 1600
	Amphitheatre Parkway, Mountain View, California 94043, USA.
	[2] Department of Computer Science, University of Toronto, 6
	King's College Road, Toronto, Ontario M5S 3G4, Canada.",
	abstract    = "Deep learning allows computational models that are composed of
	multiple processing layers to learn representations of data
	with multiple levels of abstraction. These methods have
	dramatically improved the state-of-the-art in speech
	recognition, visual object recognition, object detection and
	many other domains such as drug discovery and genomics. Deep
	learning discovers intricate structure in large data sets by
	using the backpropagation algorithm to indicate how a machine
	should change its internal parameters that are used to compute
	the representation in each layer from the representation in
	the previous layer. Deep convolutional nets have brought about
	breakthroughs in processing images, video, speech and audio,
	whereas recurrent nets have shone light on sequential data
	such as text and speech.",
	journal     = "Nature",
	volume      =  521,
	number      =  7553,
	pages       = "436--444",
	month       =  "28~" # may,
	year        =  2015,
	url         = "http://dx.doi.org/10.1038/nature14539",
	file        = "All Papers/L/LeCun et al. 2015 - Deep learning.pdf",
	language    = "en",
	issn        = "0028-0836, 1476-4687",
	pmid        = "26017442",
	doi         = "10.1038/nature14539"
}

@PHDTHESIS{Gal2016-am,
	title    = "Uncertainty in Deep Learning",
	author   = "Gal, Yarin",
	month    =  sep,
	year     =  2016,
	url      = "https://www.cs.ox.ac.uk/people/yarin.gal/website/thesis/thesis.pdf",
	file     = "All Papers/G/Gal 2016 - Uncertainty in Deep Learning.pdf",
	school   = "University of Cambridge",
	keywords = "Uncertainty quantification"
}

@ARTICLE{Yao2019-fs,
	title         = "Quality of Uncertainty Quantification for Bayesian Neural
	Network Inference",
	author        = "Yao, Jiayu and Pan, Weiwei and Ghosh, Soumya and
	Doshi-Velez, Finale",
	abstract      = "Bayesian Neural Networks (BNNs) place priors over the
	parameters in a neural network. Inference in BNNs, however,
	is difficult; all inference methods for BNNs are
	approximate. In this work, we empirically compare the
	quality of predictive uncertainty estimates for 10 common
	inference methods on both regression and classification
	tasks. Our experiments demonstrate that commonly used
	metrics (e.g. test log-likelihood) can be misleading. Our
	experiments also indicate that inference innovations
	designed to capture structure in the posterior do not
	necessarily produce high quality posterior approximations.",
	month         =  "24~" # jun,
	year          =  2019,
	url           = "http://arxiv.org/abs/1906.09686",
	file          = "All Papers/Y/Yao et al. 2019 - Quality of Uncertainty Quantification for Bayesian Neural Network Inference.pdf",
	keywords      = "Variational",
	archivePrefix = "arXiv",
	eprint        = "1906.09686",
	primaryClass  = "cs.LG",
	arxivid       = "1906.09686"
}

@ARTICLE{Utpala2020-nw,
	title         = "Quantile Regularization: Towards Implicit Calibration of
	Regression Models",
	author        = "Utpala, Saiteja and Rai, Piyush",
	abstract      = "Recent works have shown that most deep learning models are
	often poorly calibrated, i.e., they may produce
	overconfident predictions that are wrong. It is therefore
	desirable to have models that produce predictive uncertainty
	estimates that are reliable. Several approaches have been
	proposed recently to calibrate classification models.
	However, there is relatively little work on calibrating
	regression models. We present a method for calibrating
	regression models based on a novel quantile regularizer
	defined as the cumulative KL divergence between two CDFs.
	Unlike most of the existing approaches for calibrating
	regression models, which are based on post-hoc processing of
	the model's output and require an additional dataset, our
	method is trainable in an end-to-end fashion without
	requiring an additional dataset. The proposed regularizer
	can be used with any training objective for regression. We
	also show that post-hoc calibration methods like Isotonic
	Calibration sometimes compound miscalibration whereas our
	method provides consistently better calibrations. We provide
	empirical results demonstrating that the proposed quantile
	regularizer significantly improves calibration for
	regression models trained using approaches, such as Dropout
	VI and Deep Ensembles.",
	month         =  feb,
	year          =  2020,
	url           = "http://arxiv.org/abs/2002.12860",
	file          = "All Papers/U/Utpala and Rai 2020 - Quantile Regularization - Towards Implicit Calibration of Regression Models.pdf;All Papers/U/Utpala and Rai 2020 - quantile_regularization_toward.pdf",
	keywords      = "Calibration;Quantiles",
	archivePrefix = "arXiv",
	eprint        = "2002.12860",
	primaryClass  = "cs.LG",
	arxivid       = "2002.12860"
}

@INPROCEEDINGS{Kuleshov2018-tb,
	title     = "Accurate Uncertainties for Deep Learning Using Calibrated
	Regression",
	booktitle = "Proceedings of the 35th International Conference on Machine
	Learning",
	author    = "Kuleshov, Volodymyr and Fenner, Nathan and Ermon, Stefano",
	editor    = "Dy, Jennifer and Krause, Andreas",
	abstract  = "Methods for reasoning under uncertainty are a key building block
	of accurate and reliable machine learning systems. Bayesian
	methods provide a general framework to quantify uncertainty.
	However, because of model misspecification and the use of
	approximate inference, Bayesian uncertainty estimates are often
	inaccurate --- for example, a 90\% credible interval may not
	contain the true outcome 90\% of the time. Here, we propose a
	simple procedure for calibrating any regression algorithm; when
	applied to Bayesian and probabilistic models, it is guaranteed
	to produce calibrated uncertainty estimates given enough data.
	Our procedure is inspired by Platt scaling and extends previous
	work on classification. We evaluate this approach on Bayesian
	linear regression, feedforward, and recurrent neural networks,
	and find that it consistently outputs well-calibrated credible
	intervals while improving performance on time series forecasting
	and model-based reinforcement learning tasks.",
	publisher = "PMLR",
	volume    =  80,
	pages     = "2796--2804",
	series    = "Proceedings of Machine Learning Research",
	year      =  2018,
	url       = "https://proceedings.mlr.press/v80/kuleshov18a.html",
	file      = "All Papers/K/Kuleshov et al. 2018 - Accurate Uncertainties for Deep Learning Using Calibrated Regression.pdf",
	keywords  = "Calibration;Quantiles"
}

@ARTICLE{Goodfellow2014-xi,
	title         = "Explaining and Harnessing Adversarial Examples",
	author        = "Goodfellow, Ian J and Shlens, Jonathon and Szegedy,
	Christian",
	abstract      = "Several machine learning models, including neural networks,
	consistently misclassify adversarial examples---inputs
	formed by applying small but intentionally worst-case
	perturbations to examples from the dataset, such that the
	perturbed input results in the model outputting an incorrect
	answer with high confidence. Early attempts at explaining
	this phenomenon focused on nonlinearity and overfitting. We
	argue instead that the primary cause of neural networks'
	vulnerability to adversarial perturbation is their linear
	nature. This explanation is supported by new quantitative
	results while giving the first explanation of the most
	intriguing fact about them: their generalization across
	architectures and training sets. Moreover, this view yields
	a simple and fast method of generating adversarial examples.
	Using this approach to provide examples for adversarial
	training, we reduce the test set error of a maxout network
	on the MNIST dataset.",
	month         =  "20~" # dec,
	year          =  2014,
	url           = "http://arxiv.org/abs/1412.6572",
	file          = "All Papers/G/Goodfellow et al. 2014 - Explaining and Harnessing Adversarial Examples.pdf",
	keywords      = "Robustness",
	archivePrefix = "arXiv",
	eprint        = "1412.6572",
	primaryClass  = "stat.ML",
	arxivid       = "1412.6572"
}

@ARTICLE{Matheson1976-lc,
	title     = "Scoring Rules for Continuous Probability Distributions",
	author    = "Matheson, James E and Winkler, Robert L",
	abstract  = "[Personal, or subjective, probabilities are used as inputs to
	many inferential and decision-making models, and various
	procedures have been developed for the elicitation of such
	probabilities. Included among these elicitation procedures are
	scoring rules, which involve the computation of a score based on
	the assessor's stated probabilities and on the event that
	actually occurs. The development of scoring rules has, in
	general, been restricted to the elicitation of discrete
	probability distributions. In this paper, families of scoring
	rules for the elicitation of continuous probability
	distributions are developed and discussed.]",
	journal   = "Management science",
	publisher = "INFORMS",
	volume    =  22,
	number    =  10,
	pages     = "1087--1096",
	year      =  1976,
	url       = "http://www.jstor.org/stable/2629907",
	file      = "All Papers/M/Matheson and Winkler 1976 - Scoring Rules for Continuous Probability Distributions.pdf",
	keywords  = "Scoring rules;Theory",
	issn      = "0025-1909, 1526-5501"
}

@ARTICLE{Song2021-mt,
	title         = "How to Train Your {Energy-Based} Models",
	author        = "Song, Yang and Kingma, Diederik P",
	abstract      = "Energy-Based Models (EBMs), also known as non-normalized
	probabilistic models, specify probability density or mass
	functions up to an unknown normalizing constant. Unlike most
	other probabilistic models, EBMs do not place a restriction
	on the tractability of the normalizing constant, thus are
	more flexible to parameterize and can model a more
	expressive family of probability distributions. However, the
	unknown normalizing constant of EBMs makes training
	particularly difficult. Our goal is to provide a friendly
	introduction to modern approaches for EBM training. We start
	by explaining maximum likelihood training with Markov chain
	Monte Carlo (MCMC), and proceed to elaborate on MCMC-free
	approaches, including Score Matching (SM) and Noise
	Constrastive Estimation (NCE). We highlight theoretical
	connections among these three approaches, and end with a
	brief survey on alternative training methods, which are
	still under active research. Our tutorial is targeted at an
	audience with basic understanding of generative models who
	want to apply EBMs or start a research project in this
	direction.",
	month         =  "9~" # jan,
	year          =  2021,
	url           = "http://arxiv.org/abs/2101.03288",
	file          = "All Papers/S/Song and Kingma 2021 - How to Train Your Energy-Based Models.pdf",
	keywords      = "Energy-based",
	archivePrefix = "arXiv",
	eprint        = "2101.03288",
	primaryClass  = "cs.LG",
	arxivid       = "2101.03288"
}

@ARTICLE{Wolpert1996-qk,
	title       = "The lack of A Priori distinctions between learning algorithms",
	author      = "Wolpert, David H",
	affiliation = "The Santa Fe Institute, 1399 Hyde Park Rd., Santa Fe, NM,
	87501, USA",
	abstract    = "This is the first of two papers that use off-training set
	(OTS) error to investigate the assumption-free relationship
	between learning algorithms. This first paper discusses the
	senses in which there are no a priori distinctions between
	learning algorithms. (The second paper discusses the senses in
	which there are such distinctions.) In this first paper it is
	shown, loosely speaking, that for any two algorithms A and B,
	there are ``as many'' targets (or priors over targets) for
	which A has lower expected OTS error than B as vice versa, for
	loss functions like zero-one loss. In particular, this is true
	if A is cross-validation and B is ``anti-cross-validation''
	(choose the learning algorithm with largest cross-validation
	error). This paper ends with a discussion of the implications
	of these results for computational learning theory. It is
	shown that one cannot say: if empirical misclassification rate
	is low, the Vapnik-Chervonenkis dimension of your generalizer
	is small, and the training set is large, then with high
	probability your OTS error is small. Other implications for
	``membership queries'' algorithms and ``punting'' algorithms
	are also discussed.",
	journal     = "Neural computation",
	publisher   = "MIT Press - Journals",
	volume      =  8,
	number      =  7,
	pages       = "1341--1390",
	month       =  oct,
	year        =  1996,
	url         = "https://direct.mit.edu/neco/article/8/7/1341-1390/6016",
	file        = "All Papers/W/Wolpert 1996 - The lack of A Priori distinctions between learning algorithms.pdf",
	language    = "en",
	issn        = "0899-7667, 1530-888X",
	doi         = "10.1162/neco.1996.8.7.1341"
}

@BOOK{Sullivan2019-ak,
	title     = "Introduction to Uncertainty Quantification",
	author    = "Sullivan, T J",
	abstract  = "This text provides a framework in which the main objectives of
	the field of uncertainty quantification (UQ) are defined and an
	overview of the range of mathematical methods by which they can
	be achieved. Complete with exercises throughout, the book will
	equip readers with both theoretical understanding and practical
	experience of the key mathematical and algorithmic tools
	underlying the treatment of uncertainty in modern applied
	mathematics. Students and readers alike are encouraged to apply
	the mathematical methods discussed in this book to their own
	favorite problems to understand their strengths and weaknesses,
	also making the text suitable for a self-study.Uncertainty
	quantification is a topic of increasing practical importance at
	the intersection of applied mathematics, statistics, computation
	and numerous application areas in science and engineering. This
	text is designed as an introduction to UQ for senior
	undergraduate and graduate students with a mathematical or
	statistical background and also for researchers from the
	mathematical sciences or from applications areas who are
	interested in the field.T. J. Sullivan was Warwick Zeeman
	Lecturer at the Mathematics Institute of the University of
	Warwick, United Kingdom, from 2012 to 2015. Since 2015, he is
	Junior Professor of Applied Mathematics at the Free University
	of Berlin, Germany, with specialism in Uncertainty and Risk
	Quantification.",
	publisher = "Springer International Publishing",
	series    = "Texts in Applied Mathematics",
	month     =  "13~" # mar,
	year      =  2019,
	url       = "https://play.google.com/store/books/details?id=pLcdwAEACAAJ",
	address   = "Cham, Switzerland",
	keywords  = "Uncertainty quantification",
	language  = "en",
	isbn      = "9783319794785"
}

@INPROCEEDINGS{Duan2020-sf,
	title     = "{{NGB}oost}: Natural Gradient Boosting for Probabilistic
	Prediction",
	booktitle = "Proceedings of the 37th International Conference on Machine
	Learning",
	author    = "Duan, Tony and Anand, Avati and Ding, Daisy Yi and Thai, Khanh K
	and Basu, Sanjay and Ng, Andrew and Schuler, Alejandro",
	editor    = "Iii, Hal Daum{\'e} and Singh, Aarti",
	abstract  = "We present Natural Gradient Boosting (NGBoost), an algorithm for
	generic probabilistic prediction via gradient boosting. Typical
	regression models return a point estimate, conditional on
	covariates, but probabilistic regression models output a full
	probability distribution over the outcome space, conditional on
	the covariates. This allows for predictive uncertainty
	estimation - crucial in applications like healthcare and weather
	forecasting. NGBoost generalizes gradient boosting to
	probabilistic regression by treating the parameters of the
	conditional distribution as targets for a multiparameter
	boosting algorithm. Furthermore, we show how the Natural
	Gradient is required to correct the training dynamics of our
	multiparameter boosting approach. NGBoost can be used with any
	base learner, any family of distributions with continuous
	parameters, and any scoring rule. NGBoost matches or exceeds the
	performance of existing methods for probabilistic prediction
	while offering additional benefits in flexibility, scalability,
	and usability. An open-source implementation is available at
	github.com/stanfordmlgroup/ngboost.",
	publisher = "PMLR",
	volume    =  119,
	pages     = "2690--2700",
	series    = "Proceedings of Machine Learning Research",
	year      =  2020,
	url       = "http://proceedings.mlr.press/v119/duan20a.html",
	file      = "All Papers/D/Duan et al. 2020 - NGBoost - Natural Gradient Boosting for Probabilistic Prediction.pdf",
	keywords  = "Scoring rules"
}

@ARTICLE{Meinshausen2006-dg,
	title    = "Quantile Regression Forests",
	author   = "Meinshausen, Nicolai",
	journal  = "Journal of machine learning research: JMLR",
	volume   =  7,
	number   =  35,
	pages    = "983--999",
	year     =  2006,
	url      = "https://jmlr.org/papers/v7/meinshausen06a.html",
	file     = "All Papers/M/Meinshausen 2006 - Quantile Regression Forests.pdf",
	keywords = "Quantiles",
	issn     = "1532-4435, 1533-7928"
}

@ARTICLE{Kobyzev2021-lr,
	title    = "Normalizing Flows: An Introduction and Review of Current Methods",
	author   = "Kobyzev, Ivan and Prince, Simon J D and Brubaker, Marcus A",
	abstract = "Normalizing Flows are generative models which produce tractable
	distributions where both sampling and density evaluation can be
	efficient and exact. The goal of this survey article is to give a
	coherent and comprehensive review of the literature around the
	construction and use of Normalizing Flows for distribution
	learning. We aim to provide context and explanation of the
	models, review current state-of-the-art literature, and identify
	open questions and promising future directions.",
	journal  = "IEEE transactions on pattern analysis and machine intelligence",
	volume   =  43,
	number   =  11,
	pages    = "3964--3979",
	month    =  nov,
	year     =  2021,
	url      = "http://dx.doi.org/10.1109/TPAMI.2020.2992934",
	file     = "All Papers/K/Kobyzev et al. 2021 - Normalizing Flows - An Introduction and Review of Current Methods.pdf",
	keywords = "Flow",
	language = "en",
	issn     = "0162-8828",
	pmid     = "32396070",
	doi      = "10.1109/TPAMI.2020.2992934"
}

@INPROCEEDINGS{Blundell2015-dq,
	title     = "Weight Uncertainty in Neural Network",
	booktitle = "Proceedings of the 32nd International Conference on Machine
	Learning",
	author    = "Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray
	and Wierstra, Daan",
	editor    = "Bach, Francis and Blei, David",
	abstract  = "We introduce a new, efficient, principled and
	backpropagation-compatible algorithm for learning a probability
	distribution on the weights of a neural network, called Bayes by
	Backprop. It regularises the weights by minimising a compression
	cost, known as the variational free energy or the expected lower
	bound on the marginal likelihood. We show that this principled
	kind of regularisation yields comparable performance to dropout
	on MNIST classification. We then demonstrate how the learnt
	uncertainty in the weights can be used to improve generalisation
	in non-linear regression problems, and how this weight
	uncertainty can be used to drive the exploration-exploitation
	trade-off in reinforcement learning.",
	publisher = "PMLR",
	volume    =  37,
	pages     = "1613--1622",
	series    = "Proceedings of Machine Learning Research",
	year      =  2015,
	url       = "https://proceedings.mlr.press/v37/blundell15.html",
	file      = "All Papers/B/Blundell et al. 2015 - Weight Uncertainty in Neural Network.pdf",
	address   = "Lille, France",
	keywords  = "Bayesian"
}

@ARTICLE{Dimitriadis2020-hb,
	title         = "Evaluating probabilistic classifiers: Reliability diagrams
	and score decompositions revisited",
	author        = "Dimitriadis, Timo and Gneiting, Tilmann and Jordan,
	Alexander I",
	abstract      = "A probability forecast or probabilistic classifier is
	reliable or calibrated if the predicted probabilities are
	matched by ex post observed frequencies, as examined
	visually in reliability diagrams. The classical binning and
	counting approach to plotting reliability diagrams has been
	hampered by a lack of stability under unavoidable, ad hoc
	implementation decisions. Here we introduce the CORP
	approach, which generates provably statistically Consistent,
	Optimally binned, and Reproducible reliability diagrams in
	an automated way. CORP is based on non-parametric isotonic
	regression and implemented via the Pool-adjacent-violators
	(PAV) algorithm - essentially, the CORP reliability diagram
	shows the graph of the PAV- (re)calibrated forecast
	probabilities. The CORP approach allows for uncertainty
	quantification via either resampling techniques or
	asymptotic theory, furnishes a new numerical measure of
	miscalibration, and provides a CORP based Brier score
	decomposition that generalizes to any proper scoring rule.
	We anticipate that judicious uses of the PAV algorithm yield
	improved tools for diagnostics and inference for a very wide
	range of statistical and machine learning methods.",
	month         =  aug,
	year          =  2020,
	url           = "http://arxiv.org/abs/2008.03033",
	file          = "All Papers/D/Dimitriadis et al. 2020 - Evaluating probabilistic classifiers - Reliability diagrams and score decompositions revisited.pdf",
	keywords      = "Calibration;Classification",
	archivePrefix = "arXiv",
	eprint        = "2008.03033",
	primaryClass  = "stat.ME",
	arxivid       = "2008.03033"
}

@ARTICLE{Mining_undated-ha,
	title  = "Estimating Distributions and Densities",
	author = "Mining, Data",
	url    = "https://www.stat.cmu.edu/~cshalizi/350/lectures/28/lecture-28.pdf",
	file   = "All Papers/M/Mining - Estimating Distributions and Densities.pdf"
}

@ARTICLE{Gneiting2011-am,
	title     = "Making and Evaluating Point Forecasts",
	author    = "Gneiting, Tilmann",
	abstract  = "Typically, point forecasting methods are compared and assessed
	by means of an error measure or scoring function, with the
	absolute error and the squared error being key examples. The
	individual scores are averaged over forecast cases, to result in
	a summary measure of the predictive performance, such as the
	mean absolute error or the mean squared error. I demonstrate
	that this common practice can lead to grossly misguided
	inferences, unless the scoring function and the forecasting task
	are carefully matched. Effective point forecasting requires that
	the scoring function be specified ex ante, or that the
	forecaster receives a directive in the form of a statistical
	functional, such as the mean or a quantile of the predictive
	distribution. If the scoring function is specified ex ante, the
	forecaster can issue the optimal point forecast, namely, the
	Bayes rule. If the forecaster receives a directive in the form
	of a functional, it is critical that the scoring function be
	consistent for it, in the sense that the expected score is
	minimized when following the directive. A functional is
	elicitable if there exists a scoring function that is strictly
	consistent for it. Expectations, ratios of expectations and
	quantiles are elicitable. For example, a scoring function is
	consistent for the mean functional if and only if it is a
	Bregman function. It is consistent for a quantile if and only if
	it is generalized piecewise linear. Similar characterizations
	apply to ratios of expectations and to expectiles. Weighted
	scoring functions are consistent for functionals that adapt to
	the weighting in peculiar ways. Not all functionals are
	elicitable; for instance, conditional value-at-risk is not,
	despite its popularity in quantitative finance.",
	journal   = "Journal of the American Statistical Association",
	publisher = "Taylor \& Francis",
	volume    =  106,
	number    =  494,
	pages     = "746--762",
	month     =  "1~" # jun,
	year      =  2011,
	url       = "https://doi.org/10.1198/jasa.2011.r10138",
	file      = "All Papers/G/Gneiting 2011 - Making and Evaluating Point Forecasts.pdf",
	keywords  = "Scoring rules;Uncertainty in Deep Learning /Forecasting",
	issn      = "0162-1459",
	doi       = "10.1198/jasa.2011.r10138"
}

@ARTICLE{Hyndman1996-wx,
	title     = "Computing and Graphing Highest Density Regions",
	author    = "Hyndman, Rob J",
	abstract  = "[Many statistical methods involve summarizing a probability
	distribution by a region of the sample space covering a
	specified probability. One method of selecting such a region is
	to require it to contain points of relatively high density.
	Highest density regions are particularly useful for displaying
	multimodal distributions and, in such cases, may consist of
	several disjoint subsets-one for each local mode. In this paper,
	I propose a simple method for computing a highest density region
	from any given (possibly multivariate) density f(x) that is
	bounded and continuous in x. Several examples of the use of
	highest density regions in statistical graphics are also given.
	A new form of boxplot is proposed based on highest density
	regions; versions in one and two dimensions are given. Highest
	density regions in higher dimensions are also discussed and
	plotted.]",
	journal   = "The American statistician",
	publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
	volume    =  50,
	number    =  2,
	pages     = "120--126",
	year      =  1996,
	url       = "http://www.jstor.org/stable/2684423",
	file      = "All Papers/H/Hyndman 1996 - Computing and Graphing Highest Density Regions.pdf",
	keywords  = "Intervals;Uncertainty in Deep Learning /Forecasting",
	issn      = "0003-1305",
	doi       = "10.2307/2684423"
}

@ARTICLE{Gneiting2011-he,
	title    = "Quantiles as optimal point forecasts",
	author   = "Gneiting, Tilmann",
	abstract = "Loss functions play a central role in the theory and practice of
	forecasting. If the loss function is quadratic, the mean of the
	predictive distribution is the unique optimal point predictor. If
	the loss is symmetric piecewise linear, any median is an optimal
	point forecast. Quantiles arise as optimal point forecasts under
	a general class of economically relevant loss functions, which
	nests the asymmetric piecewise linear loss, and which we refer to
	as generalized piecewise linear (GPL). The level of the quantile
	depends on a generic asymmetry parameter which reflects the
	possibly distinct costs of underprediction and overprediction.
	Conversely, a loss function for which quantiles are optimal point
	forecasts is necessarily GPL. We review characterizations of this
	type in the work of Thomson, Saerens and Komunjer, and relate to
	proper scoring rules, incentive-compatible compensation schemes
	and quantile regression. In the empirical part of the paper, the
	relevance of decision theoretic guidance in the transition from a
	predictive distribution to a point forecast is illustrated using
	the Bank of England's density forecasts of United Kingdom
	inflation rates, and probabilistic predictions of wind energy
	resources in the Pacific Northwest.",
	journal  = "International journal of forecasting",
	volume   =  27,
	number   =  2,
	pages    = "197--207",
	month    =  "1~" # apr,
	year     =  2011,
	url      = "https://www.sciencedirect.com/science/article/pii/S0169207010000063",
	keywords = "Decision making; Density forecasts; Incentive-compatible
	compensation scheme; Loss function; Piecewise linear; Proper
	scoring rule; Quantile;Quantiles;Uncertainty in Deep Learning
	/Forecasting",
	issn     = "0169-2070",
	doi      = "10.1016/j.ijforecast.2009.12.015"
}

@ARTICLE{Bousquet2003-oz,
	title     = "Introduction to Statistical Learning Theory",
	author    = "Bousquet, Olivier and Boucheron, St{\'e}phane and Lugosi,
	G{\'a}bor and R{\"a}tsch, Gunnar",
	abstract  = "The goal of statistical learning theory is to study, in a sta-
	tistical framework, the properties of learning algorithms. In
	particular, most results take the form of so-called error
	bounds. This tutorial intro- duces the techniques that are used
	to obtain such results.",
	publisher = "unknown",
	month     =  "1~" # jan,
	year      =  2003,
	url       = "https://www.researchgate.net/publication/41781176_Introduction_to_Statistical_Learning_Theory",
	file      = "All Papers/B/Bousquet et al. 2003 - Introduction to Statistical Learning Theory.pdf",
	doi       = "10.1007/978-3-540-28650-9\_8"
}

@ARTICLE{Simhayev2020-ki,
	title         = "{PIVEN}: A Deep Neural Network for Prediction Intervals with
	Specific Value Prediction",
	author        = "Simhayev, Eli and Katz, Gilad and Rokach, Lior",
	abstract      = "Improving the robustness of neural nets in regression tasks
	is key to their application in multiple domains. Deep
	learning-based approaches aim to achieve this goal either by
	improving their prediction of specific values (i.e., point
	prediction), or by producing prediction intervals (PIs) that
	quantify uncertainty. We present PIVEN, a deep neural
	network for producing both a PI and a value prediction. Our
	loss function expresses the value prediction as a function
	of the upper and lower bounds, thus ensuring that it falls
	within the interval without increasing model complexity.
	Moreover, our approach makes no assumptions regarding data
	distribution within the PI, making its value prediction more
	effective for various real-world problems. Experiments and
	ablation tests on known benchmarks show that our approach
	produces tighter uncertainty bounds than the current
	state-of-the-art approaches for producing PIs, while
	maintaining comparable performance to the state-of-the-art
	approach for value-prediction. Additionally, we go beyond
	previous work and include large image datasets in our
	evaluation, where PIVEN is combined with modern neural nets.",
	month         =  "9~" # jun,
	year          =  2020,
	url           = "http://arxiv.org/abs/2006.05139",
	file          = "All Papers/S/Simhayev et al. 2020 - PIVEN - A Deep Neural Network for Prediction Intervals with Specific Value Prediction.pdf",
	keywords      = "Intervals;Uncertainty in Deep Learning /Prediction intervals",
	archivePrefix = "arXiv",
	eprint        = "2006.05139",
	primaryClass  = "cs.LG",
	arxivid       = "2006.05139"
}

@MISC{Simons_Institute2017-jj,
	title     = "Variational Inference: Foundations and Innovations",
	author    = "{Simons Institute}",
	abstract  = "David Blei, Columbia UniversityComputational Challenges in
	Machine
	Learninghttps://simons.berkeley.edu/talks/david-blei-2017-5-1",
	publisher = "Youtube",
	month     =  "1~" # may,
	year      =  2017,
	url       = "https://www.youtube.com/watch?v=Dv86zdWjJKQ",
	keywords  = "Simons Institute; Theory of Computing; Theory of Computation;
	Theoretical Computer Science; Computer Science; UC Berkeley;
	David Blei; Variational Inference Foundations and Innovations"
}

@MISC{Perone2019-ey,
	title        = "Uncertainty Estimation in Deep Learning",
	author       = "Perone, Christian",
	abstract     = "These are the slides presented on the 10th PyData Lisbon. A
	short introduction to uncertainty estimation in Deep
	Learning.",
	month        =  "17~" # jul,
	year         =  2019,
	url          = "https://www.slideshare.net/perone/uncertainty-estimation-in-deep-learning",
	howpublished = "\url{https://www.slideshare.net/perone/uncertainty-estimation-in-deep-learning}",
	note         = "Accessed: 2020-12-11"
}

@MISC{noauthor_undated-rc,
	title        = "Understanding Gaussian processes",
	abstract     = "Understanding Gaussian processes and implement a GP in
	Python.",
	url          = "https://peterroelants.github.io/posts/gaussian-process-tutorial/",
	howpublished = "\url{https://peterroelants.github.io/posts/gaussian-process-tutorial/}",
	note         = "Accessed: 2021-5-11"
}

@ARTICLE{Gortler2019-xj,
	title       = "A visual exploration of Gaussian processes",
	author      = "G{\"o}rtler, Jochen and Kehlbeck, Rebecca and Deussen, Oliver",
	affiliation = "University of Konstanz",
	journal     = "Distill",
	publisher   = "Distill Working Group",
	volume      =  4,
	number      =  4,
	month       =  "2~" # apr,
	year        =  2019,
	url         = "https://distill.pub/2019/visual-exploration-gaussian-processes",
	issn        = "2476-0757",
	doi         = "10.23915/distill.00017"
}

@ARTICLE{Gal2015-kl,
	title    = "What my deep model doesn't know",
	author   = "Gal, Yarin",
	abstract = "Trying to understand why dropout networks work so well, I was
	quite surprised to see that we can get principled uncertainty
	information from these models for free -- without changing a
	thing.",
	journal  = "Personal blog post",
	year     =  2015,
	url      = "http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Channel2019-qt,
	title     = "Uncertainty Quantification and Deep Learning ǀ Elise Jennings,
	Argonne National Laboratory",
	author    = "Channel, Argonne Internal",
	abstract  = "Presented at the Argonne Training Program on Extreme-Scale
	Computing 2019.Slides for this presentation are available here:
	https://extremecomputingtraining....",
	publisher = "Youtube",
	month     =  "6~" # nov,
	year      =  2019,
	url       = "https://www.youtube.com/watch?v=Puc_ujh5QZs",
	keywords  = "Elise Jennings; Argonne; Uncertainty Quantification; Deep
	Learning; Supercomputing; ATPESC 2019"
}

@ARTICLE{Ashukha2020-lm,
	title         = "Pitfalls of {In-Domain} Uncertainty Estimation and
	Ensembling in Deep Learning",
	author        = "Ashukha, Arsenii and Lyzhov, Alexander and Molchanov, Dmitry
	and Vetrov, Dmitry",
	abstract      = "Uncertainty estimation and ensembling methods go
	hand-in-hand. Uncertainty estimation is one of the main
	benchmarks for assessment of ensembling performance. At the
	same time, deep learning ensembles have provided
	state-of-the-art results in uncertainty estimation. In this
	work, we focus on in-domain uncertainty for image
	classification. We explore the standards for its
	quantification and point out pitfalls of existing metrics.
	Avoiding these pitfalls, we perform a broad study of
	different ensembling techniques. To provide more insight in
	this study, we introduce the deep ensemble equivalent score
	(DEE) and show that many sophisticated ensembling techniques
	are equivalent to an ensemble of only few independently
	trained networks in terms of test performance.",
	month         =  "15~" # feb,
	year          =  2020,
	url           = "http://arxiv.org/abs/2002.06470",
	file          = "All Papers/A/Ashukha et al. 2020 - Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning.pdf",
	keywords      = "Calibration;Ensembling;Classification;Uncertainty in Deep
	Learning",
	archivePrefix = "arXiv",
	eprint        = "2002.06470",
	primaryClass  = "stat.ML",
	arxivid       = "2002.06470"
}

@ARTICLE{Amodei2016-cf,
	title         = "Concrete Problems in {AI} Safety",
	author        = "Amodei, Dario and Olah, Chris and Steinhardt, Jacob and
	Christiano, Paul and Schulman, John and Man{\'e}, Dan",
	abstract      = "Rapid progress in machine learning and artificial
	intelligence (AI) has brought increasing attention to the
	potential impacts of AI technologies on society. In this
	paper we discuss one such potential impact: the problem of
	accidents in machine learning systems, defined as unintended
	and harmful behavior that may emerge from poor design of
	real-world AI systems. We present a list of five practical
	research problems related to accident risk, categorized
	according to whether the problem originates from having the
	wrong objective function (``avoiding side effects'' and
	``avoiding reward hacking''), an objective function that is
	too expensive to evaluate frequently (``scalable
	supervision''), or undesirable behavior during the learning
	process (``safe exploration'' and ``distributional shift'').
	We review previous work in these areas as well as suggesting
	research directions with a focus on relevance to
	cutting-edge AI systems. Finally, we consider the high-level
	question of how to think most productively about the safety
	of forward-looking applications of AI.",
	month         =  "21~" # jun,
	year          =  2016,
	url           = "http://arxiv.org/abs/1606.06565",
	file          = "All Papers/A/Amodei et al. 2016 - Concrete Problems in AI Safety.pdf",
	keywords      = "Robustness;Uncertainty in Deep Learning",
	archivePrefix = "arXiv",
	eprint        = "1606.06565",
	primaryClass  = "cs.AI",
	arxivid       = "1606.06565"
}

@ARTICLE{Gneiting2007-hb,
	title     = "Strictly Proper Scoring Rules, Prediction, and Estimation",
	author    = "Gneiting, Tilmann and Raftery, Adrian E",
	abstract  = "Scoring rules assess the quality of probabilistic forecasts, by
	assigning a numerical score based on the predictive distribution
	and on the event or value that materializes. A scoring rule is
	proper if the forecaster maximizes the expected score for an
	observation drawn from the distributionF if he or she issues the
	probabilistic forecast F, rather than G ? F. It is strictly
	proper if the maximum is unique. In prediction problems, proper
	scoring rules encourage the forecaster to make careful
	assessments and to be honest. In estimation problems, strictly
	proper scoring rules provide attractive loss and utility
	functions that can be tailored to the problem at hand. This
	article reviews and develops the theory of proper scoring rules
	on general probability spaces, and proposes and discusses
	examples thereof. Proper scoring rules derive from convex
	functions and relate to information measures, entropy functions,
	and Bregman divergences. In the case of categorical variables,
	we prove a rigorous version of the Savage representation.
	Examples of scoring rules for probabilistic forecasts in the
	form of predictive densities include the logarithmic, spherical,
	pseudospherical, and quadratic scores. The continuous ranked
	probability score applies to probabilistic forecasts that take
	the form of predictive cumulative distribution functions. It
	generalizes the absolute error and forms a special case of a new
	and very general type of score, the energy score. Like many
	other scoring rules, the energy score admits a kernel
	representation in terms of negative definite functions, with
	links to inequalities of Hoeffding type, in both univariate and
	multivariate settings. Proper scoring rules for quantile and
	interval forecasts are also discussed. We relate proper scoring
	rules to Bayes factors and to cross-validation, and propose a
	novel form of cross-validation known as random-fold
	cross-validation. A case study on probabilistic weather
	forecasts in the North American Pacific Northwest illustrates
	the importance of propriety. We note optimum score approaches to
	point and quantile estimation, and propose the intuitively
	appealing interval score as a utility function in interval
	estimation that addresses width as well as coverage.",
	journal   = "Journal of the American Statistical Association",
	publisher = "Taylor \& Francis",
	volume    =  102,
	number    =  477,
	pages     = "359--378",
	month     =  mar,
	year      =  2007,
	url       = "https://doi.org/10.1198/016214506000001437",
	file      = "All Papers/G/Gneiting and Raftery 2007 - Strictly Proper Scoring Rules, Prediction, and Estimation.pdf",
	keywords  = "Scoring rules;Uncertainty in Deep Learning /Scoring rules",
	issn      = "0162-1459",
	doi       = "10.1198/016214506000001437"
}

@MISC{noauthor_undated-av,
	title       = "properscoring",
	abstract    = "Proper scoring rules in Python. Contribute to
	TheClimateCorporation/properscoring development by creating an
	account on GitHub.",
	institution = "Github",
	url         = "https://github.com/TheClimateCorporation/properscoring",
	keywords    = "Uncertainty in Deep Learning /Scoring rules"
}

@MISC{Alaa_undated-dw,
	title       = "deep-learning-uncertainty",
	author      = "Alaa, Ahmed M",
	abstract    = "Literature survey, paper reviews, experimental setups and a
	collection of implementations for baselines methods for
	predictive uncertainty estimation in deep learning models. -
	ahmedmalaa/deep-learning-uncertainty",
	institution = "Github",
	url         = "https://github.com/ahmedmalaa/deep-learning-uncertainty",
	keywords    = "Uncertainty in Deep Learning"
}

@MISC{Bachstein2019-zm,
	title        = "Uncertainty quantification in Deep Learning",
	author       = "Bachstein, Simon",
	abstract     = "Teach your Deep Neural Network to be aware of its epistemic
	and aleatory uncertainty. Get a quantified confidence measure
	for your Deep Learning predictions.",
	month        =  "8~" # oct,
	year         =  2019,
	url          = "https://www.inovex.de/blog/uncertainty-quantification-deep-learning/",
	howpublished = "\url{https://www.inovex.de/blog/uncertainty-quantification-deep-learning/}",
	note         = "Accessed: 2021-5-11",
	keywords     = "Uncertainty in Deep Learning"
}

@MASTERSTHESIS{Bachstein2019-hz,
	title    = "Uncertainty Quantification in Deep Learning",
	author   = "Bachstein, Simon",
	month    =  "11~" # jan,
	year     =  2019,
	url      = "https://sbachstein.de/master_thesis.pdf",
	file     = "All Papers/B/Bachstein 2019 - Uncertainty Quantification in Deep Learning.pdf",
	school   = "Universitat Ulm",
	keywords = "Uncertainty quantification;Uncertainty in Deep Learning"
}

@BOOK{Vovk2005-ib,
	title     = "Algorithmic Learning in a Random World",
	author    = "Vovk, Vladimir and Gammerman, Alexander and Shafer, Glenn",
	publisher = "Springer International Publishing",
	year      =  2005,
	url       = "https://link.springer.com/book/10.1007/978-3-031-06649-8",
	doi       = "10.1007/978-3-031-06649-8"
}

@INPROCEEDINGS{Yoon2023-ds,
	title     = "{{ESD}}: Expected Squared Difference as a {Tuning-Free}
	Trainable Calibration Measure",
	booktitle = "The Eleventh International Conference on Learning
	Representations",
	author    = "Yoon, Hee Suk and Tee, Joshua Tian Jin and Yoon, Eunseop and
	Yoon, Sunjae and Kim, Gwangsu and Li, Yingzhen and Yoo, Chang D",
	year      =  2023,
	url       = "https://openreview.net/forum?id=bHW9njOSON",
	file      = "All Papers/Y/Yoon et al. 2023 - ESD - Expected Squared Difference as a Tuning-Free Trainable Calibration Measure.pdf"
}

@ARTICLE{Vasicek1976-fa,
	title     = "A Test for Normality Based on Sample Entropy",
	author    = "Vasicek, Oldrich",
	abstract  = "[A test of the composite hypothesis of normality is introduced.
	The test is based on the property of the normal distribution
	that its entropy exceeds that of any other distribution with a
	density that has the same variance. The test statistic is based
	on a class of estimators of entropy constructed here. The test
	is shown to be a consistent test of the null hypothesis for all
	alternatives without a singular continuous part. The power of
	the test is estimated against several alternatives. It is
	observed that the test compares favourably with other tests for
	normality.]",
	journal   = "Journal of the Royal Statistical Society. Series B, Statistical
	methodology",
	publisher = "[Royal Statistical Society, Wiley]",
	volume    =  38,
	number    =  1,
	pages     = "54--59",
	year      =  1976,
	url       = "http://www.jstor.org/stable/2984828",
	file      = "All Papers/V/Vasicek 1976 - A Test for Normality Based on Sample Entropy.pdf",
	issn      = "1369-7412, 0035-9246"
}

@unpublished{Dheur2023-zz,
	title         = "A Large-Scale Study of Probabilistic Calibration in Neural Network Regression",
	author        = "Dheur, Victor and Ben Taieb, Souhaib",
	note={Accepted at the International Conference on Machine Learning (ICML)},
	year=2023,
	abstract      = "Accurate estimation of predictive uncertainty is essential for optimal decision making. However, recent works have shown that current neural networks tend to be miscalibrated, sparking interest in different approaches to calibration. In this paper, we conduct a large-scale empirical study of the probabilistic calibration of neural networks on 57 tabular regression datasets. We consider recalibration, conformal and regularization approaches, and investigate the trade-offs they induce on calibration and sharpness of the predictions. Based on kernel density estimation, we design new differentiable recalibration and regularization methods, yielding new insights into the performance of these approaches. Furthermore, we find conditions under which recalibration and conformal prediction are equivalent. Our study is fully reproducible and implemented in a common code base for fair comparison.",
	note = {To appear in Proceedings of the International Conference on Machine Learning (ICML). URL: \url{https://hdl.handle.net/20.500.12907/45907}},
	year = {2023}
}

@ARTICLE{Foygel_Barber2021-ig,
  title     = "The limits of distribution-free conditional predictive inference",
  author    = "Foygel Barber, Rina and Cand{\`e}s, Emmanuel J and Ramdas,
               Aaditya and Tibshirani, Ryan J",
  abstract  = "Abstract. We consider the problem of distribution-free
               predictive inference, with the goal of producing predictive
               coverage guarantees that hold conditionally r",
  journal   = "Information and Inference: A Journal of the IMA",
  publisher = "Oxford Academic",
  volume    =  10,
  number    =  2,
  pages     = "455--482",
  month     =  "15~" # jun,
  year      =  2021,
  url       = "https://academic.oup.com/imaiai/article-pdf/10/2/455/38549621/iaaa017.pdf",
  file      = "All Papers/F/Foygel Barber et al. 2021 - The limits of distribution-free conditional predictive inference.pdf",
  doi       = "10.1093/imaiai/iaaa017"
}
