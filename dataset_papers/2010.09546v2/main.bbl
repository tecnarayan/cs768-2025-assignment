\begin{thebibliography}{}

\bibitem[Arjovsky et~al., 2017]{wgan}
Arjovsky, M., Chintala, S., and Bottou, L. (2017).
\newblock Wasserstein generative adversarial networks.
\newblock In {\em International Conference on Machine Learning}, pages
  214--223.

\bibitem[Asadi et~al., 2019]{asadi2019combating}
Asadi, K., Misra, D., Kim, S., and Littman, M.~L. (2019).
\newblock Combating the compounding-error problem with a multi-step model.
\newblock {\em arXiv preprint arXiv:1905.13320}.

\bibitem[Asadi et~al., 2018]{asadi2018lipschitz}
Asadi, K., Misra, D., and Littman, M. (2018).
\newblock Lipschitz continuity in model-based reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  264--273.

\bibitem[Ben-David et~al., 2010]{ben2010theory}
Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan,
  J.~W. (2010).
\newblock A theory of learning from different domains.
\newblock {\em Machine learning}, 79(1-2):151--175.

\bibitem[Ben-David et~al., 2007]{ben2007analysis}
Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F. (2007).
\newblock Analysis of representations for domain adaptation.
\newblock In {\em Advances in neural information processing systems}, pages
  137--144.

\bibitem[Brockman et~al., 2016]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W. (2016).
\newblock Openai gym.
\newblock {\em arXiv preprint arXiv:1606.01540}.

\bibitem[Buckman et~al., 2018]{steve}
Buckman, J., Hafner, D., Tucker, G., Brevdo, E., and Lee, H. (2018).
\newblock Sample-efficient reinforcement learning with stochastic ensemble
  value expansion.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8224--8234.

\bibitem[Chua et~al., 2018]{pets}
Chua, K., Calandra, R., McAllister, R., and Levine, S. (2018).
\newblock Deep reinforcement learning in a handful of trials using
  probabilistic dynamics models.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4754--4765.

\bibitem[Clavera et~al., 2018]{clavera2018model}
Clavera, I., Rothfuss, J., Schulman, J., Fujita, Y., Asfour, T., and Abbeel, P.
  (2018).
\newblock Model-based reinforcement learning via meta-policy optimization.
\newblock In {\em Conference on Robot Learning}, pages 617--629.

\bibitem[Dean et~al., 2019]{dean2017sample}
Dean, S., Mania, H., Matni, N., Recht, B., and Tu, S. (2019).
\newblock On the sample complexity of the linear quadratic regulator.
\newblock {\em Foundations of Computational Mathematics}, pages 1--47.

\bibitem[Deisenroth and Rasmussen, 2011]{deisenroth2011pilco}
Deisenroth, M. and Rasmussen, C.~E. (2011).
\newblock Pilco: A model-based and data-efficient approach to policy search.
\newblock In {\em Proceedings of the 28th International Conference on machine
  learning (ICML-11)}, pages 465--472.

\bibitem[Farahmand, 2018]{farahmand2018iterative}
Farahmand, A.-m. (2018).
\newblock Iterative value-aware model learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9072--9083.

\bibitem[Feinberg et~al., 2018]{feinberg2018model}
Feinberg, V., Wan, A., Stoica, I., Jordan, M.~I., Gonzalez, J.~E., and Levine,
  S. (2018).
\newblock Model-based value estimation for efficient model-free reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:1803.00101}.

\bibitem[Fujimoto et~al., 2019]{fujimoto2018off}
Fujimoto, S., Meger, D., and Precup, D. (2019).
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em International Conference on Machine Learning}, pages
  2052--2062.

\bibitem[Ganin et~al., 2016]{ganin2016domain}
Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,
  F., Marchand, M., and Lempitsky, V. (2016).
\newblock Domain-adversarial training of neural networks.
\newblock {\em The Journal of Machine Learning Research}, 17(1):2096--2030.

\bibitem[Gretton et~al., 2012]{mmd}
Gretton, A., Borgwardt, K.~M., Rasch, M.~J., Sch{\"o}lkopf, B., and Smola, A.
  (2012).
\newblock A kernel two-sample test.
\newblock {\em Journal of Machine Learning Research}, 13(Mar):723--773.

\bibitem[Gulrajani et~al., 2017]{wgan-gp}
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A.~C.
  (2017).
\newblock Improved training of wasserstein gans.
\newblock In {\em Advances in neural information processing systems}, pages
  5767--5777.

\bibitem[Haarnoja et~al., 2018]{sac}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018).
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock {\em arXiv preprint arXiv:1801.01290}.

\bibitem[Hafner et~al., 2019]{hafner2018learning}
Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and
  Davidson, J. (2019).
\newblock Learning latent dynamics for planning from pixels.
\newblock In {\em International Conference on Machine Learning}, pages
  2555--2565.

\bibitem[Ho and Ermon, 2016]{gail}
Ho, J. and Ermon, S. (2016).
\newblock Generative adversarial imitation learning.
\newblock In {\em Advances in neural information processing systems}, pages
  4565--4573.

\bibitem[Jaksch et~al., 2010]{jaksch2010near}
Jaksch, T., Ortner, R., and Auer, P. (2010).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 11(Apr):1563--1600.

\bibitem[Janner et~al., 2019]{mbpo}
Janner, M., Fu, J., Zhang, M., and Levine, S. (2019).
\newblock When to trust your model: Model-based policy optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  12498--12509.

\bibitem[Kaiser et~al., 2019]{simple}
Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R.~H.,
  Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., et~al.
  (2019).
\newblock Model-based reinforcement learning for atari.
\newblock {\em arXiv preprint arXiv:1903.00374}.

\bibitem[Langlois et~al., 2019]{mbbl}
Langlois, E., Zhang, S., Zhang, G., Abbeel, P., and Ba, J. (2019).
\newblock Benchmarking model-based reinforcement learning.
\newblock {\em arXiv preprint arXiv:1907.02057}.

\bibitem[Levine et~al., 2016]{levine2016end}
Levine, S., Finn, C., Darrell, T., and Abbeel, P. (2016).
\newblock End-to-end training of deep visuomotor policies.
\newblock {\em The Journal of Machine Learning Research}, 17(1):1334--1373.

\bibitem[Luo et~al., 2018]{slbo}
Luo, Y., Xu, H., Li, Y., Tian, Y., Darrell, T., and Ma, T. (2018).
\newblock Algorithmic framework for model-based deep reinforcement learning
  with theoretical guarantees.
\newblock {\em arXiv preprint arXiv:1807.03858}.

\bibitem[Mnih et~al., 2015]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
  (2015).
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529.

\bibitem[M{\"u}ller, 1997]{ipm}
M{\"u}ller, A. (1997).
\newblock Integral probability metrics and their generating classes of
  functions.
\newblock {\em Advances in Applied Probability}, 29(2):429--443.

\bibitem[Nagabandi et~al., 2018]{nn}
Nagabandi, A., Kahn, G., Fearing, R.~S., and Levine, S. (2018).
\newblock Neural network dynamics for model-based deep reinforcement learning
  with model-free fine-tuning.
\newblock In {\em 2018 IEEE International Conference on Robotics and Automation
  (ICRA)}, pages 7559--7566. IEEE.

\bibitem[Nguyen et~al., 2018]{nguyenimproving}
Nguyen, N.~M., Singh, A., and Tran, K. (2018).
\newblock Improving model-based rl with adaptive rollout using uncertainty
  estimation.

\bibitem[Shen et~al., 2018]{shen2017wasserstein}
Shen, J., Qu, Y., Zhang, W., and Yu, Y. (2018).
\newblock Wasserstein distance guided representation learning for domain
  adaptation.
\newblock In {\em Thirty-Second AAAI Conference on Artificial Intelligence}.

\bibitem[Simchowitz et~al., 2018]{simchowitz2018learning}
Simchowitz, M., Mania, H., Tu, S., Jordan, M.~I., and Recht, B. (2018).
\newblock Learning without mixing: Towards a sharp analysis of linear system
  identification.
\newblock {\em arXiv preprint arXiv:1802.08334}.

\bibitem[Sriperumbudur et~al., 2009]{sriperumbudur2009integral}
Sriperumbudur, B.~K., Fukumizu, K., Gretton, A., Sch{\"o}lkopf, B., and
  Lanckriet, G.~R. (2009).
\newblock On integral probability metrics,$\backslash$phi-divergences and
  binary classification.
\newblock {\em arXiv preprint arXiv:0901.2698}.

\bibitem[Sun et~al., 2018]{sun2018model}
Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A., and Langford, J. (2018).
\newblock Model-based reinforcement learning in contextual decision processes.
\newblock {\em arXiv preprint arXiv:1811.08540}.

\bibitem[Sutton, 1990]{dyna}
Sutton, R.~S. (1990).
\newblock Integrated architectures for learning, planning, and reacting based
  on approximating dynamic programming.
\newblock In {\em Machine Learning Proceedings 1990}, pages 216--224. Elsevier.

\bibitem[Szita and Szepesv{\'a}ri, 2010]{szita2010model}
Szita, I. and Szepesv{\'a}ri, C. (2010).
\newblock Model-based reinforcement learning with nearly tight exploration
  complexity bounds.
\newblock In {\em Proceedings of the 27th International Conference on Machine
  Learning (ICML-10)}, pages 1031--1038.

\bibitem[Talvitie, 2014]{talvitie2014model}
Talvitie, E. (2014).
\newblock Model regularization for stable sample rollouts.
\newblock In {\em UAI}, pages 780--789.

\bibitem[Talvitie, 2017]{self-correct}
Talvitie, E. (2017).
\newblock Self-correcting models for model-based reinforcement learning.
\newblock In {\em Thirty-First AAAI Conference on Artificial Intelligence}.

\bibitem[Tzeng et~al., 2017]{adda}
Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. (2017).
\newblock Adversarial discriminative domain adaptation.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 7167--7176.

\bibitem[Villani, 2008]{ot}
Villani, C. (2008).
\newblock {\em Optimal transport: old and new}, volume 338.
\newblock Springer Science \& Business Media.

\bibitem[Wang and Ba, 2019]{wang2019exploring}
Wang, T. and Ba, J. (2019).
\newblock Exploring model-based planning with policy networks.
\newblock {\em arXiv preprint arXiv:1906.08649}.

\bibitem[Wu et~al., 2019]{mi}
Wu, Y.-H., Fan, T.-H., Ramadge, P.~J., and Su, H. (2019).
\newblock Model imitation for model-based reinforcement learning.
\newblock {\em arXiv preprint arXiv:1909.11821}.

\bibitem[Xiao et~al., 2019]{xiao2019learning}
Xiao, C., Wu, Y., Ma, C., Schuurmans, D., and M{\"u}ller, M. (2019).
\newblock Learning to combat compounding-error in model-based reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:1912.11206}.

\bibitem[Yu et~al., 2020]{mopo}
Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J., Levine, S., Finn, C., and Ma,
  T. (2020).
\newblock Mopo: Model-based offline policy optimization.
\newblock {\em arXiv preprint arXiv:2005.13239}.

\bibitem[Zhao et~al., 2019]{zhao2019learning}
Zhao, H., Combes, R. T.~d., Zhang, K., and Gordon, G.~J. (2019).
\newblock On learning invariant representation for domain adaptation.
\newblock {\em arXiv preprint arXiv:1901.09453}.

\end{thebibliography}
