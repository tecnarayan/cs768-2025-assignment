\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2019)Arora, Khandeparkar, Khodak, Plevrakis, and
  Saunshi]{arora2019theoretical}
Arora, S., Khandeparkar, H., Khodak, M., Plevrakis, O., and Saunshi, N.
\newblock A theoretical analysis of contrastive unsupervised representation
  learning.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Bach(2022)]{bach2022information}
Bach, F.
\newblock Information theory with kernel methods.
\newblock \emph{IEEE Transactions on Information Theory}, 2022.

\bibitem[Bachman et~al.(2019)Bachman, Hjelm, and
  Buchwalter]{bachman2019learning}
Bachman, P., Hjelm, R.~D., and Buchwalter, W.
\newblock Learning representations by maximizing mutual information across
  views.
\newblock \emph{arXiv preprint arXiv:1906.00910}, 2019.

\bibitem[Balestriero \& LeCun(2022)Balestriero and
  LeCun]{balestriero2022contrastive}
Balestriero, R. and LeCun, Y.
\newblock Contrastive and non-contrastive self-supervised learning recover
  global and local spectral embedding methods.
\newblock \emph{arXiv preprint arXiv:2205.11508}, 2022.

\bibitem[Bardes et~al.(2021)Bardes, Ponce, and LeCun]{bardes2021vicreg}
Bardes, A., Ponce, J., and LeCun, Y.
\newblock Vicreg: Variance-invariance-covariance regularization for
  self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2105.04906}, 2021.

\bibitem[Cao et~al.(2022)Cao, Xu, and Clifton]{cao2022understand}
Cao, S., Xu, P., and Clifton, D.~A.
\newblock How to understand masked autoencoders.
\newblock \emph{arXiv preprint arXiv:2202.03670}, 2022.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron2020unsupervised}
Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 9912--9924, 2020.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{\'e}gou, Mairal,
  Bojanowski, and Joulin]{caron2021emerging}
Caron, M., Touvron, H., Misra, I., J{\'e}gou, H., Mairal, J., Bojanowski, P.,
  and Joulin, A.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pp.\  9650--9660, 2021.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1597--1607. PMLR, 2020.

\bibitem[Chen \& He(2021)Chen and He]{chen2021exploring}
Chen, X. and He, K.
\newblock Exploring simple siamese representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on Computer Vision
  and Pattern Recognition}, pp.\  15750--15758, 2021.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Doersch(2016)]{doersch2016tutorial}
Doersch, C.
\newblock Tutorial on variational autoencoders.
\newblock \emph{arXiv preprint arXiv:1606.05908}, 2016.

\bibitem[Dubois et~al.(2022)Dubois, Hashimoto, Ermon, and
  Liang]{dubois2022improving}
Dubois, Y., Hashimoto, T., Ermon, S., and Liang, P.
\newblock Improving self-supervised learning by characterizing idealized
  representations.
\newblock \emph{arXiv preprint arXiv:2209.06235}, 2022.

\bibitem[Gao et~al.(2021)Gao, Yao, and Chen]{gao2021simcse}
Gao, T., Yao, X., and Chen, D.
\newblock Simcse: Simple contrastive learning of sentence embeddings.
\newblock \emph{arXiv preprint arXiv:2104.08821}, 2021.

\bibitem[Garrido et~al.(2022)Garrido, Chen, Bardes, Najman, and
  Lecun]{garrido2022duality}
Garrido, Q., Chen, Y., Bardes, A., Najman, L., and Lecun, Y.
\newblock On the duality between contrastive and non-contrastive
  self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2206.02574}, 2022.

\bibitem[Garrido et~al.(2023)Garrido, Balestriero, Najman, and
  Lecun]{garrido2023rankme}
Garrido, Q., Balestriero, R., Najman, L., and Lecun, Y.
\newblock Rankme: Assessing the downstream performance of pretrained
  self-supervised representations by their rank.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10929--10974. PMLR, 2023.

\bibitem[Giraldo et~al.(2014)Giraldo, Rao, and Principe]{giraldo2014measures}
Giraldo, L. G.~S., Rao, M., and Principe, J.~C.
\newblock Measures of entropy from data using infinitely divisible kernels.
\newblock \emph{IEEE Transactions on Information Theory}, 61\penalty0
  (1):\penalty0 535--548, 2014.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond,
  Buchatskaya, Doersch, Avila~Pires, Guo, Gheshlaghi~Azar,
  et~al.]{grill2020bootstrap}
Grill, J.-B., Strub, F., Altch{\'e}, F., Tallec, C., Richemond, P.,
  Buchatskaya, E., Doersch, C., Avila~Pires, B., Guo, Z., Gheshlaghi~Azar, M.,
  et~al.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock \emph{Advances in Neural Iformation Processing Systems}, 33:\penalty0
  21271--21284, 2020.

\bibitem[HaoChen et~al.(2021)HaoChen, Wei, Gaidon, and Ma]{haochen2021provable}
HaoChen, J.~Z., Wei, C., Gaidon, A., and Ma, T.
\newblock Provable guarantees for self-supervised deep learning with spectral
  contrastive loss.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 5000--5011, 2021.

\bibitem[HaoChen et~al.(2022)HaoChen, Wei, Kumar, and Ma]{haochen2022beyond}
HaoChen, J.~Z., Wei, C., Kumar, A., and Ma, T.
\newblock Beyond separability: Analyzing the linear transferability of
  contrastive representations to related subpopulations.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and
  Girshick]{he2022masked}
He, K., Chen, X., Xie, S., Li, Y., Doll{\'a}r, P., and Girshick, R.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pp.\  16000--16009, 2022.

\bibitem[Henaff(2020)]{henaff2020data}
Henaff, O.
\newblock Data-efficient image recognition with contrastive predictive coding.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4182--4192. PMLR, 2020.

\bibitem[Hjelm et~al.(2018)Hjelm, Fedorov, Lavoie-Marchildon, Grewal, Bachman,
  Trischler, and Bengio]{hjelm2018learning}
Hjelm, R.~D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P.,
  Trischler, A., and Bengio, Y.
\newblock Learning deep representations by mutual information estimation and
  maximization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Hoyos-Osorio \& Sanchez-Giraldo(2023)Hoyos-Osorio and
  Sanchez-Giraldo]{hoyos2023representation}
Hoyos-Osorio, J.~K. and Sanchez-Giraldo, L.~G.
\newblock The representation jensen-shannon divergence.
\newblock \emph{arXiv preprint arXiv:2305.16446}, 2023.

\bibitem[Hu et~al.(2022)Hu, Liu, Zhou, Wang, and Huang]{hu2022your}
Hu, T., Liu, Z., Zhou, F., Wang, W., and Huang, W.
\newblock Your contrastive learning is secretly doing stochastic neighbor
  embedding.
\newblock \emph{arXiv preprint arXiv:2205.14814}, 2022.

\bibitem[Huang et~al.(2021)Huang, Yi, and Zhao]{huang2021towards}
Huang, W., Yi, M., and Zhao, X.
\newblock Towards the generalization of contrastive self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2111.00743}, 2021.

\bibitem[Kong et~al.(2023)Kong, Ma, Chen, Xing, Chi, Morency, and
  Zhang]{kong2023understanding}
Kong, L., Ma, M.~Q., Chen, G., Xing, E.~P., Chi, Y., Morency, L.-P., and Zhang,
  K.
\newblock Understanding masked autoencoders via hierarchical latent variable
  models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  7918--7928, 2023.

\bibitem[Lee et~al.(2020)Lee, Lei, Saunshi, and Zhuo]{lee2020predicting}
Lee, J.~D., Lei, Q., Saunshi, N., and Zhuo, J.
\newblock Predicting what you already know helps: Provable self-supervised
  learning.
\newblock \emph{arXiv preprint arXiv:2008.01064}, 2020.

\bibitem[Lee et~al.(2021)Lee, Lei, Saunshi, and Zhuo]{lee2021predicting}
Lee, J.~D., Lei, Q., Saunshi, N., and Zhuo, J.
\newblock Predicting what you already know helps: Provable self-supervised
  learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 309--323, 2021.

\bibitem[Li et~al.(2021)Li, Pogodin, Sutherland, and Gretton]{li2021self}
Li, Y., Pogodin, R., Sutherland, D.~J., and Gretton, A.
\newblock Self-supervised learning with kernel dependence maximization.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 15543--15556, 2021.

\bibitem[Liu et~al.(2022)Liu, Wang, Li, and Wang]{liu2022self}
Liu, X., Wang, Z., Li, Y.-L., and Wang, S.
\newblock Self-supervised learning via maximum entropy coding.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 34091--34105, 2022.

\bibitem[Misra \& Maaten(2020)Misra and Maaten]{misra2020self}
Misra, I. and Maaten, L. v.~d.
\newblock Self-supervised learning of pretext-invariant representations.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  6707--6717, 2020.

\bibitem[Nozawa \& Sato(2021)Nozawa and Sato]{nozawa2021understanding}
Nozawa, K. and Sato, I.
\newblock Understanding negative samples in instance discriminative
  self-supervised representation learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 5784--5797, 2021.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Oord, A. v.~d., Li, Y., and Vinyals, O.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Pokle et~al.(2022)Pokle, Tian, Li, and Risteski]{pokle2022contrasting}
Pokle, A., Tian, J., Li, Y., and Risteski, A.
\newblock Contrasting the landscape of contrastive and non-contrastive
  learning.
\newblock \emph{arXiv preprint arXiv:2203.15702}, 2022.

\bibitem[Robinson et~al.(2021)Robinson, Chuang, Sra, and
  Jegelka]{robinson2021contrastive}
Robinson, J.~D., Chuang, C.-Y., Sra, S., and Jegelka, S.
\newblock Contrastive learning with hard negative samples.
\newblock In \emph{ICLR}, 2021.

\bibitem[Roy \& Vetterli(2007)Roy and Vetterli]{roy2007effective}
Roy, O. and Vetterli, M.
\newblock The effective rank: A measure of effective dimensionality.
\newblock In \emph{2007 15th European signal processing conference}, pp.\
  606--610. IEEE, 2007.

\bibitem[Shwartz-Ziv \& LeCun(2023)Shwartz-Ziv and LeCun]{shwartz2023compress}
Shwartz-Ziv, R. and LeCun, Y.
\newblock To compress or not to compress--self-supervised learning and
  information theory: A review.
\newblock \emph{arXiv preprint arXiv:2304.09355}, 2023.

\bibitem[Shwartz-Ziv et~al.(2023)Shwartz-Ziv, Balestriero, Kawaguchi, Rudner,
  and LeCun]{shwartz2023information}
Shwartz-Ziv, R., Balestriero, R., Kawaguchi, K., Rudner, T.~G., and LeCun, Y.
\newblock An information-theoretic perspective on
  variance-invariance-covariance regularization.
\newblock \emph{arXiv preprint arXiv:2303.00633}, 2023.

\bibitem[Skean et~al.(2023)Skean, Osorio, Brockmeier, and
  Giraldo]{skean2023dime}
Skean, O., Osorio, J. K.~H., Brockmeier, A.~J., and Giraldo, L. G.~S.
\newblock Dime: Maximizing mutual information by a difference of matrix-based
  entropies.
\newblock \emph{arXiv preprint arXiv:2301.08164}, 2023.

\bibitem[Sordoni et~al.(2021)Sordoni, Dziri, Schulz, Gordon, Bachman, and
  Des~Combes]{sordoni2021decomposed}
Sordoni, A., Dziri, N., Schulz, H., Gordon, G., Bachman, P., and Des~Combes,
  R.~T.
\newblock Decomposed mutual information estimation for contrastive
  representation learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9859--9869. PMLR, 2021.

\bibitem[Tan et~al.(2023)Tan, Zhang, Yang, and Yuan]{tan2023contrastive}
Tan, Z., Zhang, Y., Yang, J., and Yuan, Y.
\newblock Contrastive learning is spectral clustering on similarity graph.
\newblock \emph{arXiv preprint arXiv:2303.15103}, 2023.

\bibitem[Tao et~al.(2022)Tao, Wang, Zhu, Dong, Song, Huang, and
  Dai]{tao2022exploring}
Tao, C., Wang, H., Zhu, X., Dong, J., Song, S., Huang, G., and Dai, J.
\newblock Exploring the equivalence of siamese self-supervised learning via a
  unified gradient framework.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  14431--14440, 2022.

\bibitem[Tian(2022)]{tian2022deep}
Tian, Y.
\newblock Deep contrastive learning is provably (almost) principal component
  analysis.
\newblock \emph{arXiv preprint arXiv:2201.12680}, 2022.

\bibitem[Tian et~al.(2019)Tian, Krishnan, and Isola]{tian2019contrastive}
Tian, Y., Krishnan, D., and Isola, P.
\newblock Contrastive multiview coding.
\newblock \emph{arXiv preprint arXiv:1906.05849}, 2019.

\bibitem[Tian et~al.(2020)Tian, Sun, Poole, Krishnan, Schmid, and
  Isola]{tian2020makes}
Tian, Y., Sun, C., Poole, B., Krishnan, D., Schmid, C., and Isola, P.
\newblock What makes for good views for contrastive learning.
\newblock \emph{arXiv preprint arXiv:2005.10243}, 2020.

\bibitem[Tian et~al.(2021)Tian, Chen, and Ganguli]{tian2021understanding}
Tian, Y., Chen, X., and Ganguli, S.
\newblock Understanding self-supervised learning dynamics without contrastive
  pairs.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10268--10278. PMLR, 2021.

\bibitem[Tosh et~al.(2020)Tosh, Krishnamurthy, and Hsu]{tosh2020contrastive}
Tosh, C., Krishnamurthy, A., and Hsu, D.
\newblock Contrastive estimation reveals topic posterior information to linear
  models.
\newblock \emph{arXiv:2003.02234}, 2020.

\bibitem[Tosh et~al.(2021)Tosh, Krishnamurthy, and Hsu]{tosh2021contrastive}
Tosh, C., Krishnamurthy, A., and Hsu, D.
\newblock Contrastive learning, multi-view redundancy, and linear models.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  1179--1206. PMLR, 2021.

\bibitem[Tsai et~al.(2021)Tsai, Bai, Morency, and Salakhutdinov]{tsai2021note}
Tsai, Y.-H.~H., Bai, S., Morency, L.-P., and Salakhutdinov, R.
\newblock A note on connecting barlow twins with negative-sample-free
  contrastive learning.
\newblock \emph{arXiv preprint arXiv:2104.13712}, 2021.

\bibitem[Von~Neumann(2013)]{von2013mathematische}
Von~Neumann, J.
\newblock \emph{Mathematische grundlagen der quantenmechanik}, volume~38.
\newblock Springer-Verlag, 2013.

\bibitem[Wang \& Isola(2020)Wang and Isola]{wang2020understanding}
Wang, T. and Isola, P.
\newblock Understanding contrastive representation learning through alignment
  and uniformity on the hypersphere.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9929--9939. PMLR, 2020.

\bibitem[Wang et~al.(2022)Wang, Zhang, Wang, Yang, and Lin]{wang2022chaos}
Wang, Y., Zhang, Q., Wang, Y., Yang, J., and Lin, Z.
\newblock Chaos is a ladder: A new theoretical understanding of contrastive
  learning via augmentation overlap.
\newblock \emph{arXiv preprint arXiv:2203.13457}, 2022.

\bibitem[Wen \& Li(2022)Wen and Li]{wen2022mechanism}
Wen, Z. and Li, Y.
\newblock The mechanism of prediction head in non-contrastive self-supervised
  learning.
\newblock \emph{arXiv preprint arXiv:2205.06226}, 2022.

\bibitem[Wu et~al.(2018)Wu, Xiong, Yu, and Lin]{wu2018unsupervised}
Wu, Z., Xiong, Y., Yu, S.~X., and Lin, D.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  3733--3742, 2018.

\bibitem[Xie et~al.(2022)Xie, Zhang, Cao, Lin, Bao, Yao, Dai, and
  Hu]{xie2022simmim}
Xie, Z., Zhang, Z., Cao, Y., Lin, Y., Bao, J., Yao, Z., Dai, Q., and Hu, H.
\newblock Simmim: A simple framework for masked image modeling.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  9653--9663, 2022.

\bibitem[Ye et~al.(2019)Ye, Zhang, Yuen, and Chang]{ye2019unsupervised}
Ye, M., Zhang, X., Yuen, P.~C., and Chang, S.-F.
\newblock Unsupervised embedding learning via invariant and spreading instance
  feature.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  6210--6219, 2019.

\bibitem[Yu et~al.(2020)Yu, Chan, You, Song, and Ma]{yu2020learning}
Yu, Y., Chan, K. H.~R., You, C., Song, C., and Ma, Y.
\newblock Learning diverse and discriminative representations via the principle
  of maximal coding rate reduction.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 9422--9434, 2020.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{zbontar2021barlow}
Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12310--12320. PMLR, 2021.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Zhang, Song, Yi, Zhang, and
  Kweon]{zhang2022survey}
Zhang, C., Zhang, C., Song, J., Yi, J. S.~K., Zhang, K., and Kweon, I.~S.
\newblock A survey on masked autoencoder for self-supervised learning in vision
  and beyond.
\newblock \emph{arXiv preprint arXiv:2208.00173}, 2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Wang, and Wang]{zhang2022mask}
Zhang, Q., Wang, Y., and Wang, Y.
\newblock How mask matters: Towards theoretical understandings of masked
  autoencoders.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27127--27139, 2022{\natexlab{b}}.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Tan, Yang, Huang, and
  Yuan]{zhang2023kernel}
Zhang, Y., Tan, Z., Yang, J., Huang, W., and Yuan, Y.
\newblock Matrix information theory for self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2305.17326}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Yang, Tan, and
  Yuan]{zhang2023relationmatch}
Zhang, Y., Yang, J., Tan, Z., and Yuan, Y.
\newblock Relationmatch: Matching in-batch relationships for semi-supervised
  learning.
\newblock \emph{arXiv preprint arXiv:2305.10397}, 2023{\natexlab{b}}.

\bibitem[Zhou et~al.(2021)Zhou, Wei, Wang, Shen, Xie, Yuille, and
  Kong]{zhou2021ibot}
Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., and Kong, T.
\newblock ibot: Image bert pre-training with online tokenizer.
\newblock \emph{arXiv preprint arXiv:2111.07832}, 2021.

\end{thebibliography}
