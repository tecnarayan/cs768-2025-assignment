\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abou-Moustafa and Szepesv{\'a}ri(2019)]{abou2019exponential}
K.~Abou-Moustafa and Cs. Szepesv{\'a}ri.
\newblock An exponential efron-stein inequality for $ l\_q $ stable learning
  rules.
\newblock In \emph{Algorithmic Learning Theory (ALT)}, 2019.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Z.~Allen-Zhu, Y.~Li, and Z.~Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learing (ICML)}, 2019.

\bibitem[Anthony and Bartlett(1999)]{anthony1999neural}
M.~Anthony and P.~L. Bartlett.
\newblock \emph{Neural network learning: Theoretical foundations}.
\newblock Cambridge University Press, 1999.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
S.~Arora, S.~Du, W.~Hu, Z.~Li, and R.~Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learing (ICML)}, 2019.

\bibitem[Bai and Lee(2019)]{bai2019beyond}
Y.~Bai and J.~D. Lee.
\newblock Beyond linearization: On quadratic and higher-order approximation of
  wide neural networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Bartlett and Mendelson(2002)]{bartlett2002rademacher}
P.~L. Bartlett and S.~Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 463--482, 2002.

\bibitem[Bartlett et~al.(2021)Bartlett, Montanari, and
  Rakhlin]{bartlett2021deep}
P.~L. Bartlett, A.~Montanari, and A.~Rakhlin.
\newblock Deep learning: a statistical viewpoint.
\newblock \emph{Acta Numerica}, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.09177}.
\newblock To appear.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet2002stability}
O.~Bousquet and A.~Elisseeff.
\newblock Stability and generalization.
\newblock \emph{Journal of Machine Learning Research}, 2:\penalty0 499--526,
  2002.

\bibitem[Bousquet et~al.(2020)Bousquet, Klochkov, and
  Zhivotovskiy]{bousquet2020sharper}
O.~Bousquet, Y.~Klochkov, and N.~Zhivotovskiy.
\newblock Sharper bounds for uniformly stable algorithms.
\newblock In \emph{Conference on Computational Learning Theory (COLT)}, 2020.

\bibitem[Charles and Papailiopoulos(2018)]{charles2018stability}
Z.~Charles and D.~Papailiopoulos.
\newblock Stability and generalization of learning algorithms that converge to
  global optima.
\newblock In \emph{International Conference on Machine Learing (ICML)}, 2018.

\bibitem[Chen et~al.(2018)Chen, Jin, and Yu]{chen2018stability}
Y.~Chen, C.~Jin, and B.~Yu.
\newblock Stability and convergence trade-off of iterative optimization
  algorithms.
\newblock \emph{arXiv preprint arXiv:1804.01619}, 2018.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
S.~S. Du, X.~Zhai, B.~Poczos, and A.~Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Feldman and Vondrak(2019)]{feldman2019high}
V.~Feldman and J.~Vondrak.
\newblock High probability generalization bounds for uniformly stable
  algorithms with nearly optimal rate.
\newblock In \emph{Conference on Computational Learning Theory (COLT)}, 2019.

\bibitem[Golowich et~al.(2018)Golowich, Rakhlin, and Shamir]{golowich2018size}
N.~Golowich, A.~Rakhlin, and O.~Shamir.
\newblock Size-independent sample complexity of neural networks.
\newblock In \emph{Conference on Computational Learning Theory (COLT)}, 2018.

\bibitem[Gonen and Shalev-Shwartz(2017)]{gonen2017fast}
A.~Gonen and S.~Shalev-Shwartz.
\newblock Fast rates for empirical risk minimization of strict saddle problems.
\newblock In \emph{Conference on Computational Learning Theory (COLT)}, 2017.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
M.~Hardt, B.~Recht, and Y.~Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{International Conference on Machine Learing (ICML)}, 2016.

\bibitem[Hu et~al.(2021)Hu, Wang, Lin, and Cheng]{hu2021regularization}
T.~Hu, W.~Wang, C.~Lin, and G.~Cheng.
\newblock Regularization matters: A nonparametric perspective on
  overparametrized neural network.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2021.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
A.~Jacot, F.~Gabriel, and C.~Hongler.
\newblock Neural tangent kernel: convergence and generalization in neural
  networks.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, pages 8580--8589, 2018.

\bibitem[Ji et~al.(2021)Ji, Li, and Telgarsky]{ji2021early}
Z.~Ji, J.~D. Li, and M.~Telgarsky.
\newblock Early-stopped neural networks are consistent.
\newblock Conference on Neural Information Processing Systems (NeurIPS), 2021.

\bibitem[Kuzborskij and Lampert(2018)]{kuzborskij2018data}
I.~Kuzborskij and C.~H. Lampert.
\newblock Data-dependent stability of stochastic gradient descent.
\newblock In \emph{International Conference on Machine Learing (ICML)}, 2018.

\bibitem[Kuzborskij and Szepesv{\'a}ri(2021)]{kuzborskij2021nonparametric}
I.~Kuzborskij and Cs. Szepesv{\'a}ri.
\newblock Nonparametric regression with shallow overparameterized neural
  networks trained by {GD} with early stopping.
\newblock In \emph{Conference on Computational Learning Theory (COLT)}, 2021.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
J.~Lee, L.~Xiao, S.~S. Schoenholz, Y.~Bahri, R.~Novak, J.~Sohl-Dickstein, and
  J.~Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem[Lei and Ying(2020)]{lei2020fine}
Y.~Lei and Y.~Ying.
\newblock Fine-grained analysis of stability and generalization for stochastic
  gradient descent.
\newblock In \emph{International Conference on Machine Learing (ICML)}, 2020.

\bibitem[Lei and Ying(2021)]{lei2021sharper}
Y.~Lei and Y.~Ying.
\newblock Sharper generalization bounds for learning with gradient-dominated
  objective functions.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Li et~al.(2020)Li, Soltanolkotabi, and Oymak]{li2020gradient}
M.~Li, M.~Soltanolkotabi, and S.~Oymak.
\newblock Gradient descent with early stopping is provably robust to label
  noise for overparameterized neural networks.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2020.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Zhu, and Belkin]{liu2020linearity}
C.~Liu, L.~Zhu, and M.~Belkin.
\newblock On the linearity of large non-linear models: when and why the tangent
  kernel is constant.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)},
  33, 2020{\natexlab{a}}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Zhu, and Belkin]{liu2020loss}
Chaoyue Liu, Libin Zhu, and Mikhail Belkin.
\newblock Loss landscapes and optimization in over-parameterized non-linear
  systems and neural networks.
\newblock \emph{arXiv preprint arXiv:2003.00307}, 2020{\natexlab{b}}.

\bibitem[Maurer(2017)]{maurer2017second}
A.~Maurer.
\newblock A second-order look at stability and generalization.
\newblock In \emph{Conference on Computational Learning Theory (COLT)}, 2017.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Y.~Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Li, Bhojanapalli, LeCun, and
  Srebro]{neyshabur2018role}
B.~Neyshabur, Z.~Li, S.~Bhojanapalli, Y.~LeCun, and N.~Srebro.
\newblock The role of over-parametrization in generalization of neural
  networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Oymak and Soltanolkotabi(2020)]{oymak2020toward}
S.~Oymak and M.~Soltanolkotabi.
\newblock Toward moderate overparameterization: Global convergence guarantees
  for training shallow neural networks.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (1):\penalty0 84--105, 2020.

\bibitem[Rangamani et~al.(2020)Rangamani, Rosasco, and
  Poggio]{rangamani2020interpolating}
A.~Rangamani, L.~Rosasco, and T.~Poggio.
\newblock For interpolating kernel machines, minimizing the norm of the {ERM}
  solution minimizes stability.
\newblock arXiv:2006.15522, 2020.

\bibitem[Richards and Rabbat(2021)]{richards2021weakly}
D.~Richards and M.~Rabbat.
\newblock Learning with gradient descent and weakly convex losses.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2021.

\bibitem[Sagun et~al.(2016)Sagun, Bottou, and LeCun]{sagun2016eigenvalues}
L.~Sagun, L.~Bottou, and Y.~LeCun.
\newblock Eigenvalues of the hessian in deep learning: Singularity and beyond.
\newblock \emph{arXiv preprint arXiv:1611.07476}, 2016.

\bibitem[Sch\"olkopf and Smola(2002)]{scholkopf2002learning}
B.~Sch\"olkopf and A.~J. Smola.
\newblock \emph{Learning with kernels: support vector machines, regularization,
  optimization, and beyond}.
\newblock The MIT Press, 2002.

\bibitem[Seleznova and Kutyniok(2020)]{seleznova2020analyzing}
M.~Seleznova and G.~Kutyniok.
\newblock Analyzing finite neural networks: Can we trust neural tangent kernel
  theory?
\newblock \emph{arXiv preprint arXiv:2012.04477}, 2020.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{shalev2014understanding}
S.~Shalev-Shwartz and S.~Ben-David.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge University Press, 2014.

\bibitem[Suzuki and Akiyama(2021)]{suzuki2021benefit}
T.~Suzuki and S.~Akiyama.
\newblock Benefit of deep learning with non-convex noisy gradient descent:
  Provable excess risk bound and superiority to kernel methods.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Yao et~al.(2007)Yao, Rosasco, and Caponnetto]{yao2007early}
Y.~Yao, L.~Rosasco, and A.~Caponnetto.
\newblock On early stopping in gradient descent learning.
\newblock \emph{Constructive Approximation}, 26\penalty0 (2):\penalty0
  289--315, 2007.

\bibitem[Yuan et~al.(2019)Yuan, Yan, Jin, and Yang]{yuan2018stagewise}
Z.~Yuan, Y.~Yan, R.~Jin, and T.~Yang.
\newblock Stagewise training accelerates convergence of testing error over
  {SGD}.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)},
  2019.

\end{thebibliography}
