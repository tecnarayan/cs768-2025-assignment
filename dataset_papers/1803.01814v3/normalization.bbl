\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arpit et~al.(2016)Arpit, Zhou, Kota, and
  Govindaraju]{arpit2016normalization}
Arpit, D., Zhou, Y., Kota, B., and Govindaraju, V.
\newblock Normalization propagation: A parametric technique for removing
  internal covariate shift in deep networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1168--1176, 2016.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Bahdanau, D., Cho, K., and Bengio, Y.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[B{\"o}s(1996)]{bos1996optimal}
B{\"o}s, S.
\newblock Optimal weight decay in a perceptron.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pp.\  551--556. Springer, 1996.

\bibitem[Bos \& Chug(1996)Bos and Chug]{bos1996using}
Bos, S. and Chug, E.
\newblock Using weight decay to optimize the generalization ability of a
  perceptron.
\newblock In \emph{Neural Networks, 1996., IEEE International Conference on},
  volume~1, pp.\  241--246. IEEE, 1996.

\bibitem[Cooijmans et~al.(2016)Cooijmans, Ballas, Laurent, G{\"u}l{\c{c}}ehre,
  and Courville]{cooijmans2016recurrent}
Cooijmans, T., Ballas, N., Laurent, C., G{\"u}l{\c{c}}ehre, {\c{C}}., and
  Courville, A.
\newblock Recurrent batch normalization.
\newblock \emph{arXiv preprint arXiv:1603.09025}, 2016.

\bibitem[Courbariaux et~al.(2014)Courbariaux, Bengio, and
  David]{courbariaux2014training}
Courbariaux, M., Bengio, Y., and David, J.-P.
\newblock Training deep neural networks with low precision multiplications.
\newblock \emph{arXiv preprint arXiv:1412.7024}, 2014.

\bibitem[Das et~al.(2018)Das, Mellempudi, Mudigere, Kalamkar, Avancha,
  Banerjee, Sridharan, Vaidyanathan, Kaul, Georganas, et~al.]{das2018mixed}
Das, D., Mellempudi, N., Mudigere, D., et~al.
\newblock Mixed precision training of convolutional neural networks using
  integer operations.
\newblock \emph{arXiv preprint arXiv:1802.00930}, 2018.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Eidnes \& N{\o}kland(2017)Eidnes and N{\o}kland]{eidnes2017shifting}
Eidnes, L. and N{\o}kland, A.
\newblock Shifting mean activation towards zero with bipolar activation
  functions.
\newblock \emph{arXiv preprint arXiv:1709.04054}, 2017.

\bibitem[Gitman \& Ginsburg(2017)Gitman and
  Ginsburg]{DBLP:journals/corr/abs-1709-08145}
Gitman, I. and Ginsburg, B.
\newblock Comparison of batch normalization and weight normalization algorithms
  for the large-scale image classification.
\newblock \emph{CoRR}, abs/1709.08145, 2017.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  249--256, 2010.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  1026--1034, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{hoffer2017train}
Hoffer, E., Hubara, I., and Soudry, D.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1729--1739, 2017.

\bibitem[Huang et~al.(2017)Huang, Liu, Lang, and Li]{huang2017projection}
Huang, L., Liu, X., Lang, B., and Li, B.
\newblock Projection based weight normalization for deep neural networks.
\newblock \emph{arXiv preprint arXiv:1710.02338}, 2017.

\bibitem[Hubara et~al.(2016)Hubara, Courbariaux, Soudry, El-Yaniv, and
  Bengio]{hubara2016binarized}
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y.
\newblock Binarized neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  4107--4115, 2016.

\bibitem[Ioffe(2017)]{ioffe2017batch}
Ioffe, S.
\newblock Batch renormalization: Towards reducing minibatch dependence in
  batch-normalized models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1942--1950, 2017.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pp.\
  448--456, 2015.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Klambauer et~al.(2017)Klambauer, Unterthiner, Mayr, and
  Hochreiter]{klambauer2017self}
Klambauer, G., Unterthiner, T., Mayr, A., and Hochreiter, S.
\newblock Self-normalizing neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  971--980, 2017.

\bibitem[K{\"o}ster et~al.(2017)K{\"o}ster, Webb, Wang, Nassar, Bansal,
  Constable, Elibol, Hall, Hornof, Khosrowshahi, et~al.]{koster2017flexpoint}
K{\"o}ster, U., Webb, T., Wang, X., et~al.
\newblock Flexpoint: An adaptive numerical format for efficient training of
  deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1740--1750, 2017.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Krogh \& Hertz(1992)Krogh and Hertz]{krogh1992simple}
Krogh, A. and Hertz, J.~A.
\newblock A simple weight decay can improve generalization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  950--957, 1992.

\bibitem[Micikevicius et~al.(2018)Micikevicius, Narang, Alben, Diamos, Elsen,
  Garcia, Ginsburg, Houston, Kuchaiev, Venkatesh, and
  Wu]{micikevicius2018mixed}
Micikevicius, P., Narang, S., Alben, J., et~al.
\newblock Mixed precision training.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Rota~Bul\`o et~al.(2017)Rota~Bul\`o, Porzi, and
  Kontschieder]{rotabulo2017place}
Rota~Bul\`o, S., Porzi, L., and Kontschieder, P.
\newblock In-place activated batchnorm for memory-optimized training of dnns.
\newblock \emph{arXiv preprint arXiv:1712.02616}, 2017.

\bibitem[Salimans \& Kingma(2016)Salimans and Kingma]{salimans2016weight}
Salimans, T. and Kingma, D.~P.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  901--909, 2016.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{salimans2016improved}
Salimans, T., Goodfellow, I., Zaremba, W., et~al.
\newblock Improved techniques for training gans.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2234--2242, 2016.

\bibitem[Simon(2007)]{Simon2007MaxOfGaussian}
Simon, M.~K.
\newblock Probability distributions involving gaussian random variables: A
  handbook for engineers and scientists.
\newblock 2007.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Smith et~al.(2017)Smith, Kindermans, and Le]{smith2017don}
Smith, S.~L., Kindermans, P.-J., and Le, Q.~V.
\newblock Don't decay the learning rate, increase the batch size.
\newblock \emph{arXiv preprint arXiv:1711.00489}, 2017.

\bibitem[Soudry et~al.(2014)Soudry, Hubara, and Meir]{soudry2014}
Soudry, D., Hubara, I., and Meir, R.
\newblock {Expectation backpropagation: parameter-free training of multilayer
  neural networks with continuous or discrete weights}.
\newblock In \emph{Neural Information Processing Systems}, volume~2, pp.\
  963--971, dec 2014.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, and Srebro]{soudry2017implicit}
Soudry, D., Hoffer, E., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Takeru~Miyato(2018)]{miyato2018spectral}
Takeru~Miyato, M. K. Y.~Y., Toshiki~Kataoka.
\newblock Spectral normalization for generative adversarial networks.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Ulyanov et~al.(2016)Ulyanov, Vedaldi, and
  Lempitsky]{DBLP:journals/corr/UlyanovVL16}
Ulyanov, D., Vedaldi, A., and Lempitsky, V.~S.
\newblock Instance normalization: The missing ingredient for fast stylization.
\newblock \emph{CoRR}, abs/1607.08022, 2016.

\bibitem[van Laarhoven(2017)]{van2017l2}
van Laarhoven, T.
\newblock L2 regularization versus batch and weight normalization.
\newblock \emph{arXiv preprint arXiv:1706.05350}, 2017.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., et~al.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6000--6010, 2017.

\bibitem[Venkatesh et~al.(2017)Venkatesh, Nurvitadhi, and
  Marr]{venkatesh2017accelerating}
Venkatesh, G., Nurvitadhi, E., and Marr, D.
\newblock Accelerating deep convolutional networks using low-precision and
  sparsity.
\newblock In \emph{Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE
  International Conference on}, pp.\  2861--2865. IEEE, 2017.

\bibitem[{Wu} et~al.(2018){Wu}, {Li}, {Deng}, {Liu}, {Xie}, and
  {Shi}]{2018arXiv180209769W}
{Wu}, S., {Li}, G., {Deng}, L., et~al.
\newblock {L1-Norm Batch Normalization for Efficient Training of Deep Neural
  Networks}.
\newblock \emph{ArXiv e-prints}, February 2018.

\bibitem[Wu \& He(2018)Wu and He]{wu2018group}
Wu, Y. and He, K.
\newblock Group normalization.
\newblock \emph{arXiv preprint arXiv:1803.08494}, 2018.

\bibitem[Xiang \& Li(2017)Xiang and Li]{xiang2017effect}
Xiang, S. and Li, H.
\newblock On the effect of batch normalization and weight normalization in
  generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1704.03971}, 2017.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\end{thebibliography}
