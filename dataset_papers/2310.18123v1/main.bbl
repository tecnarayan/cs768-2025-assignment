\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Z.~Allen-Zhu, Y.~Li, and Z.~Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Block et~al.(2020)Block, Mroueh, and Rakhlin]{block2020generative}
A.~Block, Y.~Mroueh, and A.~Rakhlin.
\newblock Generative modeling with denoising auto-encoders and langevin sampling, 2020.

\bibitem[Chao et~al.(2022)Chao, Sun, Cheng, Lo, Chang, Liu, Chang, Chen, and Lee]{chao2022denoising}
C.-H. Chao, W.-F. Sun, B.-W. Cheng, Y.-C. Lo, C.-C. Chang, Y.-L. Liu, Y.-L. Chang, C.-P. Chen, and C.-Y. Lee.
\newblock Denoising likelihood score matching for conditional score-based data generation.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2022.

\bibitem[Chen et~al.(2020)Chen, Liao, Zha, and Zhao]{chen2020distribution}
M.~Chen, W.~Liao, H.~Zha, and T.~Zhao.
\newblock Distribution approximation and statistical estimation guarantees of generative adversarial networks, 2020.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Huang, Zhao, and Wang]{chen2023score}
M.~Chen, K.~Huang, T.~Zhao, and M.~Wang.
\newblock Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Chewi, Li, Li, Salim, and Zhang]{chen2023sampling}
S.~Chen, S.~Chewi, J.~Li, Y.~Li, A.~Salim, and A.~Zhang.
\newblock Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2023{\natexlab{b}}.

\bibitem[Chickering(1996)]{chickering1996learning}
D.~M. Chickering.
\newblock Learning bayesian networks is np-complete.
\newblock \emph{Learning from data: Artificial intelligence and statistics V}, 1996.

\bibitem[Daniu{\v{s}}is et~al.(2010)Daniu{\v{s}}is, Janzing, Mooij, Zscheischler, Steudel, Zhang, and Sch{\"o}lkopf]{daniuvsis2010inferring}
P.~Daniu{\v{s}}is, D.~Janzing, J.~Mooij, J.~Zscheischler, B.~Steudel, K.~Zhang, and B.~Sch{\"o}lkopf.
\newblock Inferring deterministic causal relations.
\newblock In \emph{Uncertainty in Artificial Intelligence}, 2010.

\bibitem[De~Bortoli(2022)]{de2022convergence}
V.~De~Bortoli.
\newblock Convergence of denoising diffusion models under the manifold hypothesis, 2022.

\bibitem[De~Bortoli et~al.(2021)De~Bortoli, Thornton, Heng, and Doucet]{de2021diffusion}
V.~De~Bortoli, J.~Thornton, J.~Heng, and A.~Doucet.
\newblock Diffusion schr{\"o}dinger bridge with applications to score-based generative modeling.
\newblock In \emph{Advances in neural information processing systems (NeurIPS)}, 2021.

\bibitem[Dhariwal and Nichol(2021)]{dhariwal2021diffusion}
P.~Dhariwal and A.~Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock In \emph{Advances in neural information processing systems (NeurIPS)}, 2021.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and Zhai]{du2019gradient}
S.~Du, J.~Lee, H.~Li, L.~Wang, and X.~Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019{\natexlab{a}}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos, and Singh]{du2018gradient}
S.~S. Du, X.~Zhai, B.~Poczos, and A.~Singh.
\newblock Gradient descent provably optimizes over-parameterized neural networks.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2019{\natexlab{b}}.

\bibitem[Ghosh(2021)]{Ghosh2021Chisquared}
M.~Ghosh.
\newblock Exponential tail bounds for chisquared random variables.
\newblock \emph{Journal of Statistical Theory and Practice}, 2021.

\bibitem[Ghoshal and Honorio(2018)]{ghoshal2018learning}
A.~Ghoshal and J.~Honorio.
\newblock Learning linear structural equation models in polynomial time and sample complexity.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics (AISTATS)}, 2018.

\bibitem[Hyv{\"a}rinen(2005)]{hyvarinen2005estimation}
A.~Hyv{\"a}rinen.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock \emph{Journal of Machine Learning Research}, 2005.

\bibitem[Hyvarinen(2007)]{hyvarinen2007connections}
A.~Hyvarinen.
\newblock Connections between score matching, contrastive divergence, and pseudolikelihood for continuous-valued variables.
\newblock \emph{IEEE Transactions on neural networks}, 2007.

\bibitem[Janzing et~al.(2015)Janzing, Steudel, Shajarisales, and Sch{\"o}lkopf]{janzing2015justifying}
D.~Janzing, B.~Steudel, N.~Shajarisales, and B.~Sch{\"o}lkopf.
\newblock Justifying information-geometric causal inference.
\newblock \emph{Measures of Complexity: Festschrift for Alexey Chervonenkis}, 2015.

\bibitem[Jentzen and Kr{\"o}ger(2021)]{jentzen2021convergence}
A.~Jentzen and T.~Kr{\"o}ger.
\newblock Convergence rates for gradient descent in the training of overparameterized artificial neural networks with biases, 2021.

\bibitem[Koehler et~al.(2023)Koehler, Heckett, and Risteski]{koehler2023statistical}
F.~Koehler, A.~Heckett, and A.~Risteski.
\newblock Statistical efficiency of score matching: The view from isoperimetry.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2023.

\bibitem[Kong et~al.(2021)Kong, Ping, Huang, Zhao, and Catanzaro]{kong2020diffwave}
Z.~Kong, W.~Ping, J.~Huang, K.~Zhao, and B.~Catanzaro.
\newblock Diffwave: A versatile diffusion model for audio synthesis.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2021.

\bibitem[Lachapelle et~al.(2021)Lachapelle, Brouillard, Deleu, and Lacoste-Julien]{lachapelle2019gradient}
S.~Lachapelle, P.~Brouillard, T.~Deleu, and S.~Lacoste-Julien.
\newblock Gradient-based neural dag learning.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2021.

\bibitem[Lee et~al.(2022)Lee, Lu, and Tan]{lee2022convergence}
H.~Lee, J.~Lu, and Y.~Tan.
\newblock Convergence for score-based generative modeling with polynomial complexity.
\newblock In \emph{Advances in neural information processing systems (NeurIPS)}, 2022.

\bibitem[Lee et~al.(2023)Lee, Lu, and Tan]{lee2023convergence}
H.~Lee, J.~Lu, and Y.~Tan.
\newblock Convergence of score-based generative modeling for general data distributions.
\newblock In \emph{International Conference on Algorithmic Learning Theory}, 2023.

\bibitem[Malach et~al.(2020)Malach, Yehudai, Shalev-Schwartz, and Shamir]{pmlr-v119-malach20a}
E.~Malach, G.~Yehudai, S.~Shalev-Schwartz, and O.~Shamir.
\newblock Proving the lottery ticket hypothesis: Pruning is all you need.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Montagna et~al.(2023{\natexlab{a}})Montagna, Noceti, Rosasco, Zhang, and Locatello]{montagna2023causal}
F.~Montagna, N.~Noceti, L.~Rosasco, K.~Zhang, and F.~Locatello.
\newblock Causal discovery with score matching on additive models with arbitrary noise.
\newblock In \emph{CLeaR}, 2023{\natexlab{a}}.

\bibitem[Montagna et~al.(2023{\natexlab{b}})Montagna, Noceti, Rosasco, Zhang, and Locatello]{montagna2023scalable}
F.~Montagna, N.~Noceti, L.~Rosasco, K.~Zhang, and F.~Locatello.
\newblock Scalable causal discovery with score matching.
\newblock In \emph{CLeaR}, 2023{\natexlab{b}}.

\bibitem[Nguyen et~al.(2021)Nguyen, Mondelli, and Montufar]{Nguyen2021eigenvalue}
Q.~Nguyen, M.~Mondelli, and G.~F. Montufar.
\newblock Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep relu networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Oymak and Soltanolkotabi(2020)]{oymak2020toward}
S.~Oymak and M.~Soltanolkotabi.
\newblock Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks.
\newblock \emph{{IEEE} Journal on Selected Areas in Information Theory}, 2020.

\bibitem[Peters et~al.(2014)Peters, Mooij, Janzing, and Sch{\"o}lkopf]{peters2014causal}
J.~Peters, J.~M. Mooij, D.~Janzing, and B.~Sch{\"o}lkopf.
\newblock Causal discovery with continuous additive noise models.
\newblock \emph{Journal of Machine Learning Research}, 2014.

\bibitem[Raskutti and Uhler(2018)]{raskutti2018learning}
G.~Raskutti and C.~Uhler.
\newblock Learning directed acyclic graph models based on sparsest permutations.
\newblock \emph{Stat}, 2018.

\bibitem[Rolland et~al.(2022)Rolland, Cevher, Kleindessner, Russell, Janzing, Sch{\"o}lkopf, and Locatello]{rolland2022score}
P.~Rolland, V.~Cevher, M.~Kleindessner, C.~Russell, D.~Janzing, B.~Sch{\"o}lkopf, and F.~Locatello.
\newblock Score matching enables causal discovery of nonlinear additive noise models.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Sachs et~al.(2005)Sachs, Perez, Pe'er, Lauffenburger, and Nolan]{sachs2005causal}
K.~Sachs, O.~Perez, D.~Pe'er, D.~A. Lauffenburger, and G.~P. Nolan.
\newblock Causal protein-signaling networks derived from multiparameter single-cell data.
\newblock \emph{Science}, 2005.

\bibitem[Sanchez et~al.(2022)Sanchez, Voisey, Xia, Watson, O’Neil, and Tsaftaris]{sanchez2022causal}
P.~Sanchez, J.~P. Voisey, T.~Xia, H.~I. Watson, A.~Q. O’Neil, and S.~A. Tsaftaris.
\newblock Causal machine learning for healthcare and precision medicine.
\newblock \emph{Royal Society Open Science}, 2022.

\bibitem[Sanchez et~al.(2023)Sanchez, Liu, O'Neil, and Tsaftaris]{sanchez2023diffusion}
P.~Sanchez, X.~Liu, A.~Q. O'Neil, and S.~A. Tsaftaris.
\newblock Diffusion models for causal discovery via topological ordering.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2023.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{shalev2014understanding}
S.~Shalev-Shwartz and S.~Ben-David.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Shao et~al.(2019)Shao, Jacob, Ding, and Tarokh]{shao2019bayesian}
S.~Shao, P.~E. Jacob, J.~Ding, and V.~Tarokh.
\newblock Bayesian model comparison with the hyv{\"a}rinen score: Computation and consistency.
\newblock \emph{Journal of the American Statistical Association}, 2019.

\bibitem[Solus et~al.(2021)Solus, Wang, and Uhler]{solus2021consistency}
L.~Solus, Y.~Wang, and C.~Uhler.
\newblock Consistency guarantees for greedy permutation-based causal inference algorithms.
\newblock \emph{Biometrika}, 2021.

\bibitem[Song and Ermon(2019)]{song2019generative}
Y.~Song and S.~Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock In \emph{Advances in neural information processing systems (NeurIPS)}, 2019.

\bibitem[Song et~al.(2020)Song, Garg, Shi, and Ermon]{song2020sliced}
Y.~Song, S.~Garg, J.~Shi, and S.~Ermon.
\newblock Sliced score matching: A scalable approach to density and score estimation.
\newblock In \emph{Uncertainty in Artificial Intelligence}, 2020.

\bibitem[Song et~al.(2021)Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and Poole]{song2021scorebased}
Y.~Song, J.~Sohl-Dickstein, D.~P. Kingma, A.~Kumar, S.~Ermon, and B.~Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2021.

\bibitem[Spirtes et~al.(2000)Spirtes, Glymour, and Scheines]{spirtes2000causation}
P.~Spirtes, C.~N. Glymour, and R.~Scheines.
\newblock \emph{Causation, prediction, and search}.
\newblock MIT press, 2000.

\bibitem[Teyssier and Koller(2012)]{teyssier2012ordering}
M.~Teyssier and D.~Koller.
\newblock Ordering-based search: A simple and effective algorithm for learning bayesian networks, 2012.

\bibitem[Varian(2016)]{varian2016causal}
H.~R. Varian.
\newblock Causal inference in economics and marketing.
\newblock \emph{Proceedings of the National Academy of Sciences}, 2016.

\bibitem[Vershynin(2018)]{vershynin12}
R.~Vershynin.
\newblock \emph{High-Dimensional Probability: An Introduction with Applications in Data Science}.
\newblock Taylor \& Francis, 2018.

\bibitem[Vincent(2011)]{vincent2011connection}
P.~Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock \emph{Neural computation}, 2011.

\bibitem[Wang et~al.(2021)Wang, Du, Zhu, Ke, Chen, Hao, and Wang]{wang2021ordering}
X.~Wang, Y.~Du, S.~Zhu, L.~Ke, Z.~Chen, J.~Hao, and J.~Wang.
\newblock Ordering-based causal discovery with reinforcement learning.
\newblock In \emph{International Joint Conferences on Artificial Intelligence (IJCAI)}, 2021.

\bibitem[Wenliang and Kanagawa(2020)]{wenliang2020blindness}
L.~K. Wenliang and H.~Kanagawa.
\newblock Blindness of score-based methods to isolated components and mixing proportions, 2020.

\bibitem[Zhang(2008)]{zhang2008completeness}
J.~Zhang.
\newblock On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias.
\newblock \emph{Artificial Intelligence}, 2008.

\bibitem[Zhu et~al.(2022)Zhu, Liu, Chrysos, and Cevher]{zhu2022robustness}
Z.~Zhu, F.~Liu, G.~Chrysos, and V.~Cevher.
\newblock Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization).
\newblock In \emph{Advances in neural information processing systems (NeurIPS)}, 2022.

\end{thebibliography}
