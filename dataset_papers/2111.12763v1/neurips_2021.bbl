\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Nvi()]{NvidiaAmpereArch}
{Nvidia Ampere Architecture}.
\newblock \newline
  https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/.

\bibitem[Brix et~al.(2020)Brix, Bahar, and Ney]{brix2020successfully}
Christopher Brix, Parnia Bahar, and Hermann Ney.
\newblock Successfully applying the stabilized lottery ticket hypothesis to the
  transformer architecture.
\newblock \emph{arXiv preprint arXiv:2005.03454}, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, et~al.]{choromanski2020rethinking}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
  Lukasz Kaiser, et~al.
\newblock Rethinking attention with performers.
\newblock \emph{arXiv preprint arXiv:2009.14794}, 2020.

\bibitem[Cohan et~al.(2018)Cohan, Dernoncourt, Kim, Bui, Kim, Chang, and
  Goharian]{Cohan_2018}
Arman Cohan, Franck Dernoncourt, Doo~Soon Kim, Trung Bui, Seokhwan Kim, Walter
  Chang, and Nazli Goharian.
\newblock A discourse-aware attention model for abstractive summarization of
  long documents.
\newblock \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 2 (Short Papers)}, 2018.
\newblock \doi{10.18653/v1/n18-2097}.
\newblock URL \url{http://dx.doi.org/10.18653/v1/n18-2097}.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Fedus et~al.(2021)Fedus, Zoph, and Shazeer]{switchtransformer}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock \emph{arXiv preprint arXiv:2101.03961}, 2021.

\bibitem[Gale et~al.(2019)Gale, Elsen, and Hooker]{gale2019state}
Trevor Gale, Erich Elsen, and Sara Hooker.
\newblock The state of sparsity in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1902.09574}, 2019.

\bibitem[{Gidiotis} and {Tsoumakas}(2020)]{dancerPegasus}
A.~{Gidiotis} and G.~{Tsoumakas}.
\newblock A divide-and-conquer approach to the summarization of long documents.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, 28:\penalty0 3029--3040, 2020.
\newblock \doi{10.1109/TASLP.2020.3037401}.

\bibitem[Gupta and Agrawal(2020)]{gupta2020compression}
Manish Gupta and Puneet Agrawal.
\newblock Compression of deep learning models for text: A survey.
\newblock \emph{arXiv preprint arXiv:2008.05221}, 2020.

\bibitem[He et~al.(2018)He, Tan, Xia, He, Qin, Chen, and Liu]{lwc18}
Tianyu He, Xu~Tan, Yingce Xia, Di~He, Tao Qin, Zhibo Chen, and Tie-Yan Liu.
\newblock Layer-wise coordination between encoder and decoder for neural
  machine translation.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31, 2018.

\bibitem[Iandola et~al.(2020)Iandola, Shaw, Krishna, and
  Keutzer]{iandola2020squeezebert}
Forrest~N Iandola, Albert~E Shaw, Ravi Krishna, and Kurt~W Keutzer.
\newblock Squeezebert: What can computer vision teach nlp about efficient
  neural networks?
\newblock \emph{arXiv preprint arXiv:2006.11316}, 2020.

\bibitem[Jang et~al.(2016)Jang, Gu, and Poole]{jang2016categorical}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock \emph{arXiv preprint arXiv:1611.01144}, 2016.

\bibitem[Kaiser and Bengio(2018)]{kb2018discrete}
\L{}ukasz Kaiser and Samy Bengio.
\newblock Discrete autoencoders for sequence models.
\newblock \emph{arXiv preprint arXiv:1801.09797}, 2018.

\bibitem[Kaiser and Sutskever(2015)]{neuralgpu}
{\L}ukasz Kaiser and Ilya Sutskever.
\newblock Neural gpus learn algorithms.
\newblock \emph{arXiv preprint arXiv:1511.08228}, 2015.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kim and Awadalla(2020)]{kim2020fastformers}
Young~Jin Kim and Hany~Hassan Awadalla.
\newblock Fastformers: Highly efficient transformer models for natural language
  understanding.
\newblock \emph{arXiv preprint arXiv:2010.13382}, 2020.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock \emph{arXiv preprint arXiv:2001.04451}, 2020.

\bibitem[Lei et~al.(2017)Lei, Zhang, and Artzi]{sru}
Tao Lei, Yu~Zhang, and Yoav Artzi.
\newblock Training rnns as fast as cnns.
\newblock \emph{CoRR}, abs/1709.02755, 2017.
\newblock URL \url{http://arxiv.org/abs/1709.02755}.

\bibitem[Lepikhin et~al.(2020)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun,
  Shazeer, and Chen]{lepikhin2020gshard}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping
  Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
\newblock Gshard: Scaling giant models with conditional computation and
  automatic sharding.
\newblock \emph{arXiv preprint arXiv:2006.16668}, 2020.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Kong, Zhang, Li, Li, Liu, and
  Ding]{li2020efficient}
Bingbing Li, Zhenglun Kong, Tianyun Zhang, Ji~Li, Zhengang Li, Hang Liu, and
  Caiwen Ding.
\newblock Efficient transformer-based large scale language representations
  using hardware-friendly block structured pruning.
\newblock \emph{arXiv preprint arXiv:2009.08065}, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2019)Li, Jin, Xuan, Zhou, Chen, Wang, and
  Yan]{li2019enhancing}
Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and
  Xifeng Yan.
\newblock Enhancing the locality and breaking the memory bottleneck of
  transformer on time series forecasting.
\newblock \emph{arXiv preprint arXiv:1907.00235}, 2019.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Wallace, Shen, Lin, Keutzer, Klein,
  and Gonzalez]{li2020train}
Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and
  Joey Gonzalez.
\newblock Train big, then compress: Rethinking model size for efficient
  training and inference of transformers.
\newblock In \emph{International Conference on Machine Learning}, pages
  5958--5968. PMLR, 2020{\natexlab{b}}.

\bibitem[Maziarz et~al.(2019)Maziarz, Kokiopoulou, Gesmundo, Sbaiz, Bartok, and
  Berent]{maziarz2019gumbel}
Krzysztof Maziarz, Efi Kokiopoulou, Andrea Gesmundo, Luciano Sbaiz, Gabor
  Bartok, and Jesse Berent.
\newblock Gumbel-matrix routing for flexible multi-task learning.
\newblock \emph{arXiv preprint arXiv:1910.04915}, 2019.

\bibitem[Narang et~al.(2021)Narang, Chung, Tay, Fedus, Fevry, Matena, Malkan,
  Fiedel, Shazeer, Lan, Zhou, Li, Ding, Marcus, Roberts, and Raffel]{tmods}
Sharan Narang, Hyung~Won Chung, Yi~Tay, William Fedus, Thibault Fevry, Michael
  Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi
  Zhou, Wei Li, Nan Ding, Jake Marcus, Adam Roberts, and Colin Raffel.
\newblock Do transformer modifications transfer across implementations and
  applications?
\newblock \emph{arXiv preprint arXiv:2102.11972}, 2021.

\bibitem[Patterson et~al.(2021)Patterson, Gonzalez, Le, Liang, Munguia,
  Rothchild, So, Texier, and Dean]{patterson2021carbon}
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia,
  Daniel Rothchild, David So, Maud Texier, and Jeff Dean.
\newblock Carbon emissions and large neural network training.
\newblock \emph{arXiv preprint arXiv:2104.10350}, 2021.

\bibitem[Prato et~al.(2019)Prato, Charlaix, and Rezagholizadeh]{prato2019fully}
Gabriele Prato, Ella Charlaix, and Mehdi Rezagholizadeh.
\newblock Fully quantized transformer for machine translation.
\newblock \emph{arXiv preprint arXiv:1910.10485}, 2019.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{gpt2}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of Machine Learning Research}, 21:\penalty0 1--67,
  2020.

\bibitem[Roy et~al.(2020)Roy, Saffar, Vaswani, and Grangier]{roy2020efficient}
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock \emph{arXiv preprint arXiv:2003.05997}, 2020.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}, 2019.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean]{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
  Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock \emph{arXiv preprint arXiv:1701.06538}, 2017.

\bibitem[Shazeer et~al.(2018)Shazeer, Cheng, Parmar, Tran, Vaswani,
  Koanantakool, Hawkins, Lee, Hong, Young, et~al.]{shazeer2018mesh}
Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn
  Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young,
  et~al.
\newblock Mesh-tensorflow: Deep learning for supercomputers.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10414--10423, 2018.

\bibitem[Shen et~al.(2020)Shen, Dong, Ye, Ma, Yao, Gholami, Mahoney, and
  Keutzer]{shen2020q}
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
  Michael~W Mahoney, and Kurt Keutzer.
\newblock Q-bert: Hessian based ultra low precision quantization of bert.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 8815--8821, 2020.

\bibitem[Strubell et~al.(2019)Strubell, Ganesh, and McCallum]{energyuse}
Emma Strubell, Ananya Ganesh, and Andrew McCallum.
\newblock Energy and policy considerations for deep learning in nlp.
\newblock \emph{arXiv preprint arXiv:1906.02243}, 2019.

\bibitem[Sukhbaatar et~al.(2019)Sukhbaatar, Grave, Bojanowski, and
  Joulin]{sukhbaatar2019adaptive}
Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin.
\newblock Adaptive attention span in transformers.
\newblock \emph{arXiv preprint arXiv:1905.07799}, 2019.

\bibitem[Sun et~al.(2019)Sun, Choi, Chen, Wang, Venkataramani, Srinivasan, Cui,
  Zhang, and Gopalakrishnan]{sun2019hybrid}
Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani,
  Vijayalakshmi~Viji Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash
  Gopalakrishnan.
\newblock Hybrid 8-bit floating point (hfp8) training and inference for deep
  neural networks.
\newblock \emph{Advances in neural information processing systems},
  32:\penalty0 4900--4909, 2019.

\bibitem[Sun et~al.(2020)Sun, Yu, Song, Liu, Yang, and Zhou]{sun2020mobilebert}
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.
\newblock Mobilebert: a compact task-agnostic bert for resource-limited
  devices.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 2158--2170, 2020.

\bibitem[Tay et~al.(2020{\natexlab{a}})Tay, Bahri, Yang, Metzler, and
  Juan]{tay2020sparse}
Yi~Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan.
\newblock Sparse sinkhorn attention, 2020{\natexlab{a}}.

\bibitem[Tay et~al.(2020{\natexlab{b}})Tay, Dehghani, Bahri, and
  Metzler]{tay2020efficient}
Yi~Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
\newblock Efficient transformers: A survey.
\newblock \emph{arXiv preprint arXiv:2009.06732}, 2020{\natexlab{b}}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{arXiv preprint arXiv:1706.03762}, 2017.

\bibitem[Wang et~al.(2020)Wang, Wu, Liu, Cai, Zhu, Gan, and Han]{wang2020hat}
Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, and
  Song Han.
\newblock Hat: Hardware-aware transformers for efficient natural language
  processing.
\newblock \emph{arXiv preprint arXiv:2005.14187}, 2020.

\bibitem[Xue et~al.(2020)Xue, Constant, Roberts, Kale, Al-Rfou, Siddhant,
  Barua, and Raffel]{xue2020mt5}
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
  Siddhant, Aditya Barua, and Colin Raffel.
\newblock mt5: A massively multilingual pre-trained text-to-text transformer,
  2020.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, et~al.]{zaheer2020big}
Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti,
  Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li~Yang, et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock \emph{arXiv preprint arXiv:2007.14062}, 2020.

\bibitem[Zhang et~al.(2020)Zhang, Zhao, Saleh, and Liu]{zhang2020pegasus}
Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter~J. Liu.
\newblock Pegasus: Pre-training with extracted gap-sentences for abstractive
  summarization, 2020.

\bibitem[Zhou et~al.(2021)Zhou, Ma, Zhu, Liu, Zhang, Yuan, Sun, and
  Li]{zhou2021learning}
Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu
  Sun, and Hongsheng Li.
\newblock Learning n:m fine-grained structured sparse neural networks from
  scratch.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=K9bw7vqp_s}.

\end{thebibliography}
