\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2018)Allen-Zhu, Li, and Song]{allen2018convergence}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock \emph{arXiv preprint arXiv:1811.03962}, 2018.

\bibitem[Arora et~al.(2018)Arora, Cohen, Golowich, and
  Hu]{arora2018convergence}
Arora, S., Cohen, N., Golowich, N., and Hu, W.
\newblock A convergence analysis of gradient descent for deep linear neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02281}, 2018.

\bibitem[Balduzzi et~al.(2017)Balduzzi, Frean, Leary, Lewis, Ma, and
  McWilliams]{balduzzi2017shattered}
Balduzzi, D., Frean, M., Leary, L., Lewis, J., Ma, K. W.-D., and McWilliams, B.
\newblock The shattered gradients problem: If resnets are the answer, then what
  is the question?
\newblock \emph{arXiv preprint arXiv:1702.08591}, 2017.

\bibitem[Bengio et~al.(1994)Bengio, Simard, and Frasconi]{bengio1994learning}
Bengio, Y., Simard, P., and Frasconi, P.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock \emph{IEEE transactions on neural networks}, 5\penalty0 (2):\penalty0
  157--166, 1994.

\bibitem[Bertsekas(2011)]{bertsekas2011incremental}
Bertsekas, D.~P.
\newblock Incremental gradient, subgradient, and proximal methods for convex
  optimization: A survey.
\newblock \emph{Optimization for Machine Learning}, 2010\penalty0
  (1-38):\penalty0 3, 2011.

\bibitem[Boucheron et~al.(2013)Boucheron, Lugosi, and
  Massart]{boucheron2013concentration}
Boucheron, S., Lugosi, G., and Massart, P.
\newblock \emph{Concentration inequalities: A nonasymptotic theory of
  independence}.
\newblock Oxford university press, 2013.

\bibitem[Brutzkus et~al.(2017)Brutzkus, Globerson, Malach, and
  Shalev-Shwartz]{brutzkus2017sgd}
Brutzkus, A., Globerson, A., Malach, E., and Shalev-Shwartz, S.
\newblock Sgd learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock \emph{arXiv preprint arXiv:1710.10174}, 2017.

\bibitem[Chaudhari et~al.(2016)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{chaudhari2016entropy}
Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C.,
  Chayes, J., Sagun, L., and Zecchina, R.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock \emph{arXiv preprint arXiv:1611.01838}, 2016.

\bibitem[Cooper(2018)]{cooper2018loss}
Cooper, Y.
\newblock The loss landscape of overparameterized neural networks.
\newblock \emph{arXiv preprint arXiv:1804.10200}, 2018.

\bibitem[Darken \& Moody(1992)Darken and Moody]{darken1992towards}
Darken, C. and Moody, J.
\newblock Towards faster stochastic gradient search.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1009--1016, 1992.

\bibitem[De \& Smith(2020)De and Smith]{de2020batch}
De, S. and Smith, S.~L.
\newblock Batch normalization biases residual blocks towards the identity
  function in deep networks.
\newblock \emph{arXiv preprint arXiv:2002.10444}, 2020.

\bibitem[De et~al.(2017)De, Yadav, Jacobs, and Goldstein]{de2017automated}
De, S., Yadav, A., Jacobs, D., and Goldstein, T.
\newblock Automated inference with adaptive batches.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1504--1513,
  2017.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018.

\bibitem[Dziugaite \& Roy(2017)Dziugaite and Roy]{dziugaite2017computing}
Dziugaite, G.~K. and Roy, D.~M.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{arXiv preprint arXiv:1703.11008}, 2017.

\bibitem[Eldan \& Shamir(2016)Eldan and Shamir]{eldan2016power}
Eldan, R. and Shamir, O.
\newblock The power of depth for feedforward neural networks.
\newblock In \emph{Conference on Learning Theory}, pp.\  907--940, 2016.

\bibitem[Fort et~al.(2019)Fort, Nowak, and Narayanan]{fort2019stiffness}
Fort, S., Nowak, P.~K., and Narayanan, S.
\newblock Stiffness: A new perspective on generalization in neural networks.
\newblock \emph{arXiv preprint arXiv:1901.09491}, 2019.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Krishnan, and
  Xiao]{ghorbani2019investigation}
Ghorbani, B., Krishnan, S., and Xiao, Y.
\newblock An investigation into neural net optimization via hessian eigenvalue
  density.
\newblock \emph{arXiv preprint arXiv:1901.10159}, 2019.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pp.\  249--256, 2010.

\bibitem[Hanin(2018)]{hanin2018neural}
Hanin, B.
\newblock Which neural net architectures give rise to exploding and vanishing
  gradients?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  582--591, 2018.

\bibitem[Hanin \& Rolnick(2018)Hanin and Rolnick]{hanin2018start}
Hanin, B. and Rolnick, D.
\newblock How to start training: The effect of initialization and architecture.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  571--581, 2018.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  1026--1034, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{arXiv preprint arXiv:1502.03167}, 2015.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  8580--8589, 2018.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
Karimi, H., Nutini, J., and Schmidt, M.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pp.\  795--811. Springer, 2016.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1097--1105, 2012.

\bibitem[LeCun et~al.(2012)LeCun, Bottou, Orr, and
  M{\"u}ller]{lecun2012efficient}
LeCun, Y.~A., Bottou, L., Orr, G.~B., and M{\"u}ller, K.-R.
\newblock Efficient backprop.
\newblock In \emph{Neural networks: Tricks of the trade}, pp.\  9--48.
  Springer, 2012.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Sohl-Dickstein, and
  Pennington]{lee2019wide}
Lee, J., Xiao, L., Schoenholz, S.~S., Bahri, Y., Sohl-Dickstein, J., and
  Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{arXiv preprint arXiv:1902.06720}, 2019.

\bibitem[Lojasiewicz(1965)]{lojasiewicz1965ensembles}
Lojasiewicz, S.
\newblock Ensembles semi-analytiques.
\newblock \emph{Lectures Notes IHES (Bures-sur-Yvette)}, 1965.

\bibitem[Ma et~al.(2017)Ma, Bassily, and Belkin]{ma2017power}
Ma, S., Bassily, R., and Belkin, M.
\newblock The power of interpolation: Understanding the effectiveness of sgd in
  modern over-parametrized learning.
\newblock \emph{arXiv preprint arXiv:1712.06559}, 2017.

\bibitem[Martens(2016)]{martens2016second}
Martens, J.
\newblock \emph{Second-order optimization for neural networks}.
\newblock University of Toronto (Canada), 2016.

\bibitem[Milman \& Schechtman(1986)Milman and Schechtman]{milman1986asymptotic}
Milman, V.~D. and Schechtman, G.
\newblock \emph{Asymptotic Theory of Finite Dimensional Normed Spaces}.
\newblock Springer-Verlag, Berlin, Heidelberg, 1986.
\newblock ISBN 0-387-16769-2.

\bibitem[Moulines \& Bach(2011)Moulines and Bach]{moulines2011non}
Moulines, E. and Bach, F.~R.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  451--459, 2011.

\bibitem[Nagarajan \& Kolter(2019)Nagarajan and
  Kolter]{nagarajan2019generalization}
Nagarajan, V. and Kolter, J.~Z.
\newblock Generalization in deep networks: The role of distance from
  initialization.
\newblock \emph{arXiv preprint arXiv:1901.01672}, 2019.

\bibitem[Nedi{\'c} \& Bertsekas(2001)Nedi{\'c} and
  Bertsekas]{nedic2001convergence}
Nedi{\'c}, A. and Bertsekas, D.
\newblock Convergence rate of incremental subgradient algorithms.
\newblock In \emph{Stochastic optimization: algorithms and applications}, pp.\
  223--264. Springer, 2001.

\bibitem[Needell et~al.(2014)Needell, Ward, and Srebro]{needell2014stochastic}
Needell, D., Ward, R., and Srebro, N.
\newblock Stochastic gradient descent, weighted sampling, and the randomized
  kaczmarz algorithm.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1017--1025, 2014.

\bibitem[Nesterov(2018)]{nesterov2018lectures}
Nesterov, Y.
\newblock \emph{Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Li, Bhojanapalli, LeCun, and
  Srebro]{neyshabur2018towards}
Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N.
\newblock Towards understanding the role of over-parametrization in
  generalization of neural networks.
\newblock \emph{arXiv preprint arXiv:1805.12076}, 2018.

\bibitem[Nguyen \& Hein(2017)Nguyen and Hein]{nguyen2017loss}
Nguyen, Q. and Hein, M.
\newblock The loss surface of deep and wide neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2603--2612. JMLR. org, 2017.

\bibitem[Oymak \& Soltanolkotabi(2018)Oymak and
  Soltanolkotabi]{oymak2018overparameterized}
Oymak, S. and Soltanolkotabi, M.
\newblock Overparameterized nonlinear learning: Gradient descent takes the
  shortest path?
\newblock \emph{arXiv preprint arXiv:1812.10004}, 2018.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins1951stochastic}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pp.\  400--407, 1951.

\bibitem[Sagun et~al.(2017)Sagun, Evci, Guney, Dauphin, and
  Bottou]{sagun2017empirical}
Sagun, L., Evci, U., Guney, V.~U., Dauphin, Y., and Bottou, L.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1706.04454}, 2017.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{saxe2013exact}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6120}, 2013.

\bibitem[Schmidt \& Roux(2013)Schmidt and Roux]{schmidt2013fast}
Schmidt, M. and Roux, N.~L.
\newblock Fast convergence of stochastic gradient descent under a strong growth
  condition.
\newblock \emph{arXiv preprint arXiv:1308.6370}, 2013.

\bibitem[Schoenholz et~al.(2016)Schoenholz, Gilmer, Ganguli, and
  Sohl-Dickstein]{schoenholz2016deep}
Schoenholz, S.~S., Gilmer, J., Ganguli, S., and Sohl-Dickstein, J.
\newblock Deep information propagation.
\newblock \emph{arXiv preprint arXiv:1611.01232}, 2016.

\bibitem[Sedghi et~al.(2018)Sedghi, Gupta, and Long]{sedghi2018singular}
Sedghi, H., Gupta, V., and Long, P.~M.
\newblock The singular values of convolutional layers.
\newblock \emph{arXiv preprint arXiv:1805.10408}, 2018.

\bibitem[Shamir \& Zhang(2013)Shamir and Zhang]{shamir2013stochastic}
Shamir, O. and Zhang, T.
\newblock Stochastic gradient descent for non-smooth optimization: Convergence
  results and optimal averaging schemes.
\newblock In \emph{International Conference on Machine Learning}, pp.\  71--79,
  2013.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Smith et~al.(2020)Smith, Elsen, and De]{smith2020generalization}
Smith, S.~L., Elsen, E., and De, S.
\newblock On the generalization benefit of noise in stochastic gradient
  descent.
\newblock \emph{arXiv preprint arXiv:2006.15081}, 2020.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013importance}
Sutskever, I., Martens, J., Dahl, G., and Hinton, G.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1139--1147, 2013.

\bibitem[Tao(2012)]{tao2012topics}
Tao, T.
\newblock \emph{Topics in random matrix theory}, volume 132.
\newblock American Mathematical Soc., 2012.

\bibitem[Telgarsky(2016)]{telgarsky2016benefits}
Telgarsky, M.
\newblock Benefits of depth in neural networks.
\newblock \emph{arXiv preprint arXiv:1602.04485}, 2016.

\bibitem[Vaswani et~al.(2018)Vaswani, Bach, and Schmidt]{vaswani2018fast}
Vaswani, S., Bach, F., and Schmidt, M.
\newblock Fast and faster convergence of sgd for over-parameterized models and
  an accelerated perceptron.
\newblock \emph{arXiv preprint arXiv:1810.07288}, 2018.

\bibitem[Vershynin(2018)]{vershynin2016high}
Vershynin, R.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge University Press, 2018.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and
  Recht]{wilson2017marginal}
Wilson, A.~C., Roelofs, R., Stern, M., Srebro, N., and Recht, B.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4151--4161, 2017.

\bibitem[Wu et~al.(2017)Wu, Zhu, et~al.]{wu2017towards}
Wu, L., Zhu, Z., et~al.
\newblock Towards understanding generalization of deep learning: Perspective of
  loss landscapes.
\newblock \emph{arXiv preprint arXiv:1706.10239}, 2017.

\bibitem[Xiao et~al.(2018)Xiao, Bahri, Sohl-Dickstein, Schoenholz, and
  Pennington]{xiao2018dynamical}
Xiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S.~S., and Pennington, J.
\newblock Dynamical isometry and a mean field theory of cnns: How to train
  10,000-layer vanilla convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:1806.05393}, 2018.

\bibitem[Yang et~al.(2019)Yang, Pennington, Rao, Sohl-Dickstein, and
  Schoenholz]{yang2019mean}
Yang, G., Pennington, J., Rao, V., Sohl-Dickstein, J., and Schoenholz, S.~S.
\newblock A mean field theory of batch normalization.
\newblock \emph{arXiv preprint arXiv:1902.08129}, 2019.

\bibitem[Yin et~al.(2017)Yin, Pananjady, Lam, Papailiopoulos, Ramchandran, and
  Bartlett]{yin2017gradient}
Yin, D., Pananjady, A., Lam, M., Papailiopoulos, D., Ramchandran, K., and
  Bartlett, P.
\newblock Gradient diversity: a key ingredient for scalable distributed
  learning.
\newblock \emph{arXiv preprint arXiv:1706.05699}, 2017.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zou et~al.(2018)Zou, Cao, Zhou, and Gu]{zou2018stochastic}
Zou, D., Cao, Y., Zhou, D., and Gu, Q.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu
  networks.
\newblock \emph{arXiv preprint arXiv:1811.08888}, 2018.

\end{thebibliography}
