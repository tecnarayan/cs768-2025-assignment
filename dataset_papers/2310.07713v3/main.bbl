\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Borgeaud et~al.(2022)Borgeaud, Mensch, Hoffmann, Cai, Rutherford, Millican, Van Den~Driessche, Lespiau, Damoc, Clark, et~al.]{borgeaud2022improving}
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den~Driessche, G.~B., Lespiau, J.-B., Damoc, B., Clark, A., et~al.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock In \emph{ICML}, 2022.

\bibitem[Brown et~al.(2020{\natexlab{a}})Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{NeurIPS}, 2020{\natexlab{a}}.

\bibitem[Brown et~al.(2020{\natexlab{b}})Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{gpt3}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020{\natexlab{b}}.

\bibitem[Chen et~al.(2021)Chen, Chu, Wiseman, and Gimpel]{Chen2021SummScreenAD}
Chen, M., Chu, Z., Wiseman, S., and Gimpel, K.
\newblock Summscreen: A dataset for abstractive screenplay summarization.
\newblock \emph{ArXiv}, abs/2104.07091, 2021.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:233240744}.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, Webson, Gu, Dai, Suzgun, Chen, Chowdhery, Castro-Ros, Pellat, Robinson, Valter, Narang, Mishra, Yu, Zhao, Huang, Dai, Yu, Petrov, Chi, Dean, Devlin, Roberts, Zhou, Le, and Wei]{chung2022scaling}
Chung, H.~W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S.~S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, K., Valter, D., Narang, S., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E.~H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q.~V., and Wei, J.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv: 2210.11416}, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.11416v5}.

\bibitem[Conover et~al.(2023)Conover, Hayes, Mathur, Xie, Wan, Shah, Ghodsi, Wendell, Zaharia, and Xin]{DatabricksBlog2023DollyV2}
Conover, M., Hayes, M., Mathur, A., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M., and Xin, R.
\newblock Free dolly: Introducing the world's first truly open instruction-tuned llm.
\newblock \emph{databricks}, 2023.

\bibitem[Dasigi et~al.(2019)Dasigi, Liu, Marasović, Smith, and Gardner]{dasigi2019quoref}
Dasigi, P., Liu, N.~F., Marasović, A., Smith, N.~A., and Gardner, M.
\newblock Quoref: A reading comprehension dataset with questions requiring coreferential reasoning.
\newblock \emph{Conference on Empirical Methods in Natural Language Processing}, 2019.
\newblock \doi{10.18653/v1/D19-1606}.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Du et~al.(2021)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu, Firat, Zoph, Fedus, Bosma, Zhou, Wang, Wang, Webster, Pellat, Robinson, Meier-Hellstern, Duke, Dixon, Zhang, Le, Wu, Chen, and Cui]{du2021glam}
Du, N., Huang, Y., Dai, A.~M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A., Firat, O., Zoph, B., Fedus, L., Bosma, M., Zhou, Z., Wang, T., Wang, Y.~E., Webster, K., Pellat, M., Robinson, K., Meier-Hellstern, K., Duke, T., Dixon, L., Zhang, K., Le, Q.~V., Wu, Y., Chen, Z., and Cui, C.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock \emph{International Conference on Machine Learning}, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.06905v2}.

\bibitem[Dua et~al.(2019)Dua, Wang, Dasigi, Stanovsky, Singh, and Gardner]{dua2019drop}
Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M.
\newblock Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs.
\newblock \emph{North American Chapter of the Association for Computational Linguistics}, 2019.
\newblock \doi{10.18653/v1/N19-1246}.

\bibitem[Fan et~al.(2019)Fan, Jernite, Perez, Grangier, Weston, and Auli]{fan2019eli5}
Fan, A., Jernite, Y., Perez, E., Grangier, D., Weston, J., and Auli, M.
\newblock Eli5: Long form question answering.
\newblock \emph{Annual Meeting of the Association for Computational Linguistics}, 2019.
\newblock \doi{10.18653/v1/P19-1346}.

\bibitem[Feng et~al.(2020)Feng, Wan, Gunasekara, Patel, Joshi, and Lastras]{feng2020doc2dial}
Feng, S., Wan, H., Gunasekara, R.~C., Patel, S., Joshi, S., and Lastras, L.
\newblock doc2dial: A goal-oriented document-grounded dialogue dataset.
\newblock \emph{Conference on Empirical Methods in Natural Language Processing}, 2020.
\newblock \doi{10.18653/v1/2020.emnlp-main.652}.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, et~al.]{gao2020pile}
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Ge et~al.(2014)Ge, He, Ke, and Sun]{opq}
Ge, T., He, K., Ke, Q., and Sun, J.
\newblock Optimized product quantization.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 36\penalty0 (4):\penalty0 744--755, 2014.
\newblock \doi{10.1109/TPAMI.2013.240}.

\bibitem[Gray \& Neuhoff(1998)Gray and Neuhoff]{pq}
Gray, R. and Neuhoff, D.
\newblock Quantization.
\newblock \emph{IEEE Transactions on Information Theory}, 44\penalty0 (6):\penalty0 2325--2383, 1998.
\newblock \doi{10.1109/18.720541}.

\bibitem[Guu et~al.(2020)Guu, Lee, Tung, Pasupat, and Chang]{guu2020retrieval}
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.
\newblock {REALM}: Retrieval augmented language model pre-training.
\newblock In \emph{ICML}, 2020.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d.~L., Hendricks, L.~A., Welbl, J., Clark, A., et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Honovich et~al.(2022)Honovich, Scialom, Levy, and Schick]{honovich2022unnatural}
Honovich, O., Scialom, T., Levy, O., and Schick, T.
\newblock Unnatural instructions: Tuning language models with (almost) no human labor.
\newblock \emph{Annual Meeting of the Association for Computational Linguistics}, 2022.
\newblock \doi{10.48550/arXiv.2212.09689}.

\bibitem[Huang et~al.(2023)Huang, Ping, Xu, Shoeybi, Chang, and Catanzaro]{huang2023raven}
Huang, J., Ping, W., Xu, P., Shoeybi, M., Chang, K. C.-C., and Catanzaro, B.
\newblock Raven: In-context learning with retrieval augmented encoder-decoder language models.
\newblock \emph{arXiv preprint arXiv:2308.07922}, 2023.

\bibitem[Huang et~al.(2021)Huang, Cao, Parulian, Ji, and Wang]{Huang2021EfficientAF}
Huang, L.~R., Cao, S., Parulian, N.~N., Ji, H., and Wang, L.
\newblock Efficient attentions for long document summarization.
\newblock \emph{ArXiv}, abs/2104.02112, 2021.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:233033613}.

\bibitem[Izacard et~al.(2022{\natexlab{a}})Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, and Grave]{izacard2022atlas}
Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E.
\newblock Atlas: Few-shot learning with retrieval augmented language models.
\newblock \emph{arXiv preprint arXiv: 2208.03299}, 2022{\natexlab{a}}.

\bibitem[Izacard et~al.(2022{\natexlab{b}})Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, and Grave]{izacard2022few}
Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E.
\newblock Few-shot learning with retrieval augmented language models.
\newblock \emph{arXiv preprint arXiv:2208.03299}, 2022{\natexlab{b}}.

\bibitem[Johnson et~al.(2019)Johnson, Douze, and J{\'e}gou]{faiss}
Johnson, J., Douze, M., and J{\'e}gou, H.
\newblock Billion-scale similarity search with {GPUs}.
\newblock \emph{IEEE Transactions on Big Data}, 7\penalty0 (3):\penalty0 535--547, 2019.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer]{joshi2017triviaqa}
Joshi, M., Choi, E., Weld, D.~S., and Zettlemoyer, L.
\newblock Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.
\newblock \emph{Annual Meeting of the Association for Computational Linguistics}, 2017.
\newblock \doi{10.18653/v1/P17-1147}.

\bibitem[Karpukhin et~al.(2020)Karpukhin, O{\u{g}}uz, Min, Lewis, Wu, Edunov, Chen, and Yih]{karpukhin2020dense}
Karpukhin, V., O{\u{g}}uz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t.
\newblock Dense passage retrieval for open-domain question answering.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Khandelwal et~al.(2020)Khandelwal, Levy, Jurafsky, Zettlemoyer, and Lewis]{khandelwal2019generalization}
Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M.
\newblock Generalization through memorization: Nearest neighbor language models.
\newblock 2020.

\bibitem[Kim et~al.(2022)Kim, Hessel, Jiang, West, Lu, Yu, Zhou, Bras, Alikhani, Kim, Sap, and Choi]{kim2022soda}
Kim, H., Hessel, J., Jiang, L., West, P., Lu, X., Yu, Y., Zhou, P., Bras, R.~L., Alikhani, M., Kim, G., Sap, M., and Choi, Y.
\newblock Soda: Million-scale dialogue distillation with social commonsense contextualization.
\newblock \emph{arXiv preprint arXiv: 2212.10465}, 2022.
\newblock URL \url{https://arxiv.org/abs/2212.10465v2}.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization, 2014.

\bibitem[Ko{\v{c}}isk{\`y} et~al.(2018)Ko{\v{c}}isk{\`y}, Schwarz, Blunsom, Dyer, Hermann, Melis, and Grefenstette]{kovcisky2018narrativeqa}
Ko{\v{c}}isk{\`y}, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K.~M., Melis, G., and Grefenstette, E.
\newblock The narrativeqa reading comprehension challenge.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 6:\penalty0 317--328, 2018.

\bibitem[Kudo \& Richardson(2018)Kudo and Richardson]{kudo2018sentencepiece}
Kudo, T. and Richardson, J.
\newblock Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.
\newblock \emph{Conference on Empirical Methods in Natural Language Processing}, 2018.
\newblock \doi{10.18653/v1/D18-2012}.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins, Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee, Toutanova, Jones, Kelcey, Chang, Dai, Uszkoreit, Le, and Petrov]{kwiatkowski-etal-2019-natural}
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.-W., Dai, A.~M., Uszkoreit, J., Le, Q., and Petrov, S.
\newblock Natural questions: A benchmark for question answering research.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 7:\penalty0 452--466, 2019.
\newblock \doi{10.1162/tacl_a_00276}.
\newblock URL \url{https://aclanthology.org/Q19-1026}.

\bibitem[Köpf et~al.(2023)Köpf, Kilcher, von Rütte, Anagnostidis, Tam, Stevens, Barhoum, Duc, Stanley, Nagyfi, ES, Suri, Glushkov, Dantuluri, Maguire, Schuhmann, Nguyen, and Mattick]{köpf2023openassistant}
Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N.~M., Stanley, O., Nagyfi, R., ES, S., Suri, S., Glushkov, D., Dantuluri, A., Maguire, A., Schuhmann, C., Nguyen, H., and Mattick, A.
\newblock Openassistant conversations - democratizing large language model alignment.
\newblock \emph{arXiv preprint arXiv: 2304.07327}, 2023.

\bibitem[Lauren{\c{c}}on et~al.(2022)Lauren{\c{c}}on, Saulnier, Wang, Akiki, Villanova~del Moral, Le~Scao, Von~Werra, Mou, Gonz{\'a}lez~Ponferrada, Nguyen, et~al.]{laurenccon2022bigscience}
Lauren{\c{c}}on, H., Saulnier, L., Wang, T., Akiki, C., Villanova~del Moral, A., Le~Scao, T., Von~Werra, L., Mou, C., Gonz{\'a}lez~Ponferrada, E., Nguyen, H., et~al.
\newblock The bigscience roots corpus: A 1.6 tb composite multilingual dataset.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 31809--31826, 2022.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K{\"u}ttler, Lewis, Yih, Rockt{\"a}schel, et~al.]{lewis2020retrieval}
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K{\"u}ttler, H., Lewis, M., Yih, W.-t., Rockt{\"a}schel, T., et~al.
\newblock Retrieval-augmented generation for knowledge-intensive {NLP} tasks.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Lin et~al.(2023)Lin, Asai, Li, Oguz, Lin, Mehdad, tau Yih, and Chen]{lin2023train}
Lin, S.-C., Asai, A., Li, M., Oguz, B., Lin, J., Mehdad, Y., tau Yih, W., and Chen, X.
\newblock How to train your dragon: Diverse augmentation towards generalizable dense retrieval.
\newblock \emph{arXiv preprint arXiv: 2302.07452}, 2023.

\bibitem[Lin et~al.(2024)Lin, Chen, Chen, Shi, Lomeli, James, Rodriguez, Kahn, Szilvasy, Lewis, Zettlemoyer, and tau Yih]{lin2024radit}
Lin, X.~V., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M., Zettlemoyer, L., and tau Yih, W.
\newblock {RA}-{DIT}: Retrieval-augmented dual instruction tuning.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=22OTbutug9}.

\bibitem[Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le, Zoph, Wei, and Roberts]{longpre2023flan}
Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H.~W., Tay, Y., Zhou, D., Le, Q.~V., Zoph, B., Wei, J., and Roberts, A.
\newblock The flan collection: Designing data and methods for effective instruction tuning.
\newblock \emph{International Conference on Machine Learning}, 2023.
\newblock \doi{10.48550/arXiv.2301.13688}.

\bibitem[Malkov \& Yashunin(2018)Malkov and Yashunin]{hnsw}
Malkov, Y.~A. and Yashunin, D.~A.
\newblock Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 42\penalty0 (4):\penalty0 824--836, 2018.

\bibitem[Mishra et~al.(2022)Mishra, Khashabi, Baral, and Hajishirzi]{mishra2021cross}
Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H.
\newblock Cross-task generalization via natural language crowdsourcing instructions.
\newblock In \emph{ACL}, 2022.

\bibitem[Nakano et~al.(2021)Nakano, Hilton, Balaji, Wu, Ouyang, Kim, Hesse, Jain, Kosaraju, Saunders, et~al.]{nakano2021webgpt}
Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et~al.
\newblock Web{GPT}: Browser-assisted question-answering with human feedback.
\newblock \emph{arXiv preprint arXiv:2112.09332}, 2021.

\bibitem[OpenAI(2022)]{chatgpt}
OpenAI.
\newblock Chat{GPT}.
\newblock \url{https://chat.openai.com}, 2022.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock G{PT-4} technical report.
\newblock \emph{arXiv}, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Petroni et~al.(2021)Petroni, Piktus, Fan, Lewis, Yazdani, De~Cao, Thorne, Jernite, Karpukhin, Maillard, Plachouras, Rockt{\"a}schel, and Riedel]{petroni-etal-2021-kilt}
Petroni, F., Piktus, A., Fan, A., Lewis, P., Yazdani, M., De~Cao, N., Thorne, J., Jernite, Y., Karpukhin, V., Maillard, J., Plachouras, V., Rockt{\"a}schel, T., and Riedel, S.
\newblock {KILT}: a benchmark for knowledge intensive language tasks.
\newblock In \emph{NAACL}, 2021.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling}
Rae, J.~W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et~al.
\newblock Scaling language models: Methods, analysis \& insights from training gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, Liu, et~al.]{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.~J., et~al.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of Machine Learning Research}, 2020.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and Liang]{rajpurkar2016squad}
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock \emph{Conference on Empirical Methods in Natural Language Processing}, 2016.
\newblock \doi{10.18653/v1/D16-1264}.

\bibitem[Rajpurkar et~al.(2018)Rajpurkar, Jia, and Liang]{rajpurkar2018suqad2}
Rajpurkar, P., Jia, R., and Liang, P.
\newblock Know what you don't know: Unanswerable questions for squad.
\newblock \emph{Annual Meeting of the Association for Computational Linguistics}, 2018.
\newblock \doi{10.18653/v1/P18-2124}.

\bibitem[Sanh et~al.(2022{\natexlab{a}})Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Raja, Dey, Bari, Xu, Thakker, Sharma, Szczechla, Kim, Chhablani, Nayak, Datta, Chang, Jiang, Wang, Manica, Shen, Yong, Pandey, Bawden, Wang, Neeraj, Rozen, Sharma, Santilli, Fevry, Fries, Teehan, Scao, Biderman, Gao, Wolf, and Rush]{sanh2022multitask}
Sanh, V., Webson, A., Raffel, C., Bach, S., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Raja, A., Dey, M., Bari, M.~S., Xu, C., Thakker, U., Sharma, S.~S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z.~X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Fevry, T., Fries, J.~A., Teehan, R., Scao, T.~L., Biderman, S., Gao, L., Wolf, T., and Rush, A.~M.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{International Conference on Learning Representations}, 2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=9Vrb9D0WI4}.

\bibitem[Sanh et~al.(2022{\natexlab{b}})Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Scao, Raja, et~al.]{sanh2021multitask}
Sanh, V., Webson, A., Raffel, C., Bach, S.~H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T.~L., Raja, A., et~al.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{ICLR}, 2022{\natexlab{b}}.

\bibitem[Shi et~al.(2023{\natexlab{a}})Shi, Min, Lomeli, Zhou, Li, Lin, Smith, Zettlemoyer, Yih, and Lewis]{shi2023context}
Shi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, V., Smith, N.~A., Zettlemoyer, L., Yih, S., and Lewis, M.
\newblock In-context pretraining: Language modeling beyond document boundaries.
\newblock \emph{arXiv preprint arXiv:2310.10638}, 2023{\natexlab{a}}.

\bibitem[Shi et~al.(2023{\natexlab{b}})Shi, Min, Yasunaga, Seo, James, Lewis, Zettlemoyer, and Yih]{shi2023replug}
Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t.
\newblock Replug: Retrieval-augmented black-box language models.
\newblock \emph{arXiv preprint arXiv:2301.12652}, 2023{\natexlab{b}}.

\bibitem[Smith et~al.(2022)Smith, Patwary, Norick, LeGresley, Rajbhandari, Casper, Liu, Prabhumoye, Zerveas, Korthikanti, Zhang, Child, Aminabadi, Bernauer, Song, Shoeybi, He, Houston, Tiwary, and Catanzaro]{smith2022using}
Smith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., Liu, Z., Prabhumoye, S., Zerveas, G., Korthikanti, V., Zhang, E., Child, R., Aminabadi, R.~Y., Bernauer, J., Song, X., Shoeybi, M., He, Y., Houston, M., Tiwary, S., and Catanzaro, B.
\newblock Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model.
\newblock \emph{arXiv}, 2022.

\bibitem[Thoppilan et~al.(2022)Thoppilan, De~Freitas, Hall, Shazeer, Kulshreshtha, Cheng, Jin, Bos, Baker, Du, et~al.]{thoppilan2022lamda}
Thoppilan, R., De~Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et~al.
\newblock Lamda: Language models for dialog applications.
\newblock \emph{arXiv preprint arXiv:2201.08239}, 2022.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{ARXIV}, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2302.13971v1}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C.~C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P.~S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.~M., Subramanian, R., Tan, X.~E., Tang, B., Taylor, R., Williams, A., Kuan, J.~X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv: 2307.09288}, 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2307.09288v2}.

\bibitem[Trischler et~al.(2016)Trischler, Wang, Yuan, Harris, Sordoni, Bachman, and Suleman]{trischler2016newsqa}
Trischler, A., Wang, T., Yuan, X., Harris, J., Sordoni, A., Bachman, P., and Suleman, K.
\newblock Newsqa: A machine comprehension dataset.
\newblock \emph{REP4NLP@ACL}, 2016.
\newblock \doi{10.18653/v1/W17-2623}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{transformers}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{NIPS}, 2017.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Ping, Xu, McAfee, Liu, Shoeybi, Dong, Kuchaiev, Li, Xiao, et~al.]{wang2023shall}
Wang, B., Ping, W., Xu, P., McAfee, L., Liu, Z., Shoeybi, M., Dong, Y., Kuchaiev, O., Li, B., Xiao, C., et~al.
\newblock Shall we pretrain autoregressive language models with retrieval? a comprehensive study.
\newblock In \emph{EMNLP}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{wang2022self}
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.~A., Khashabi, D., and Hajishirzi, H.
\newblock Self-instruct: Aligning language model with self generated instructions.
\newblock \emph{arXiv preprint arXiv:2212.10560}, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{wang2022selfinstruct}
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.~A., Khashabi, D., and Hajishirzi, H.
\newblock Self-instruct: Aligning language models with self-generated instructions.
\newblock \emph{Annual Meeting of the Association for Computational Linguistics}, 2022{\natexlab{b}}.
\newblock \doi{10.48550/arXiv.2212.10560}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Ivison, Dasigi, Hessel, Khot, Chandu, Wadden, MacMillan, Smith, Beltagy, and Hajishirzi]{wang2023far}
Wang, Y., Ivison, H., Dasigi, P., Hessel, J., Khot, T., Chandu, K.~R., Wadden, D., MacMillan, K., Smith, N.~A., Beltagy, I., and Hajishirzi, H.
\newblock How far can camels go? exploring the state of instruction tuning on open resources.
\newblock \emph{arXiv preprint arXiv: 2306.04751}, 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2306.04751v1}.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2021finetuned}
Wei, J., Bosma, M., Zhao, V.~Y., Guu, K., Yu, A.~W., Lester, B., Du, N., Dai, A.~M., and Le, Q.~V.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{ICLR}, 2022{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, et~al.]{wei2022emergent}
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et~al.
\newblock Emergent abilities of large language models.
\newblock \emph{arXiv preprint arXiv:2206.07682}, 2022{\natexlab{b}}.

\bibitem[Wei et~al.(2022{\natexlab{c}})Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.~V., Zhou, D., et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24824--24837, 2022{\natexlab{c}}.

\bibitem[Yogatama et~al.(2021)Yogatama, de~Masson~d’Autume, and Kong]{yogatama2021adaptive}
Yogatama, D., de~Masson~d’Autume, C., and Kong, L.
\newblock Adaptive semiparametric language models.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 2021.

\bibitem[Zheng et~al.(2024)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, et~al.]{zheng2024judging}
Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Zhong et~al.(2021)Zhong, Yin, Yu, Zaidi, Mutuma, Jha, Awadallah, Celikyilmaz, Liu, Qiu, et~al.]{zhong2021qmsum}
Zhong, M., Yin, D., Yu, T., Zaidi, A., Mutuma, M., Jha, R., Awadallah, A.~H., Celikyilmaz, A., Liu, Y., Qiu, X., et~al.
\newblock Qmsum: A new benchmark for query-based multi-domain meeting summarization.
\newblock \emph{arXiv preprint arXiv:2104.05938}, 2021.

\end{thebibliography}
