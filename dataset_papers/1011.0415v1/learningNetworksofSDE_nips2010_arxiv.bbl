\begin{thebibliography}{10}

\bibitem{Gillespie}
D.T. Gillespie.
\newblock {Stochastic simulation of chemical kinetics}.
\newblock {\em {Annual Review of Physical Chemistry}}, 58:35--55, 2007.

\bibitem{Higham}
D.~Higham.
\newblock {Modeling and Simulating Chemical Reactions}.
\newblock {\em {SIAM Review}}, 50:347--368, 2008.

\bibitem{BioBook}
N.D.Lawrence et~al., editor.
\newblock {\em {Learning and Inference in Computational Systems Biology}}.
\newblock MIT Press, 2010.

\bibitem{ToniEtAl}
T.~Toni, D.~Welch, N.~Strelkova, A.~Ipsen, and M.P.H. Stumpf.
\newblock {Modeling and Simulating Chemical Reactions}.
\newblock {\em {J. R. Soc. Interface}}, 6:187--202, 2009.

\bibitem{Control}
K.~Zhou, J.C. Doyle, and K.~Glover.
\newblock {\em {Robust and optimal control}}.
\newblock Prentice Hall, 1996.

\bibitem{tibshirani_lasso}
R.~Tibshirani.
\newblock {Regression shrinkage and selection via the lasso}.
\newblock {\em Journal of the Royal Statistical Society. Series B
  (Methodological)}, 58(1):267--288, 1996.

\bibitem{donoho2006nearsol}
D.L. Donoho.
\newblock {For most large underdetermined systems of equations, the minimal
  l1-norm near-solution approximates the sparsest near-solution}.
\newblock {\em Communications on Pure and Applied Mathematics}, 59(7):907--934,
  2006.

\bibitem{donoho2006sol}
D.L. Donoho.
\newblock {For most large underdetermined systems of linear equations the
  minimal l1-norm solution is also the sparsest solution}.
\newblock {\em Communications on Pure and Applied Mathematics}, 59(6):797--829,
  2006.

\bibitem{zhang2009ssp}
T.~Zhang.
\newblock {Some sharp performance bounds for least squares regression with L1
  regularization}.
\newblock {\em Annals of Statistics}, 37:2109--2144, 2009.

\bibitem{wainwright2009sth}
M.J. Wainwright.
\newblock {Sharp thresholds for high-dimensional and noisy sparsity recovery
  using l1-constrained quadratic programming (Lasso)}.
\newblock {\em IEEE Trans. Information Theory}, 55:2183--2202, 2009.

\bibitem{wainwright2007high}
M.J. Wainwright, P.~Ravikumar, and J.D. Lafferty.
\newblock {High-Dimensional Graphical Model Selection Using l-1-Regularized
  Logistic Regression}.
\newblock {\em Advances in Neural Information Processing Systems}, 19:1465,
  2007.

\bibitem{zhao}
P.~Zhao and B.~Yu.
\newblock {On model selection consistency of Lasso}.
\newblock {\em The Journal of Machine Learning Research}, 7:2541--2563, 2006.

\bibitem{friedman2008sparse}
J.~Friedman, T.~Hastie, and R.~Tibshirani.
\newblock {Sparse inverse covariance estimation with the graphical lasso}.
\newblock {\em Biostatistics}, 9(3):432, 2008.

\bibitem{Kurtz}
K.~Ball, T.G. Kurtz, L.~Popovic, and G.~Rempala.
\newblock {Modeling and Simulating Chemical Reactions}.
\newblock {\em {Ann. Appl. Prob.}}, 16:1925--1961, 2006.

\bibitem{Stuart}
G.A. Pavliotis and A.M. Stuart.
\newblock {Parameter estimation for multiscale diffusions}.
\newblock {\em {J. Stat. Phys.}}, 127:741--781, 2007.

\bibitem{Songsiri1}
J.~Songsiri, J.~Dahl, and L.~Vandenberghe.
\newblock Graphical models of autoregressive processes.
\newblock pages 89--116, 2010.

\bibitem{Songsiri2}
J.~Songsiri and L.~Vandenberghe.
\newblock Topology selection in graphical models of autoregressive processes.
\newblock {\em Journal of Machine Learning Research}, 2010.
\newblock submitted.

\bibitem{Chung}
F.R.K. Chung.
\newblock {\em {Spectral Graph Theory}}.
\newblock CBMS Regional Conference Series in Mathematics, 1997.

\bibitem{ravikumar2008high}
P.~Ravikumar, M.J. Wainwright, and J.~Lafferty.
\newblock {High-dimensional Ising model selection using l1-regularized logistic
  regression}.
\newblock {\em Annals of Statistics}, 2008.

\end{thebibliography}
