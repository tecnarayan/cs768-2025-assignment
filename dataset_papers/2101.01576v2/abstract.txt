The price of explainability for a clustering task can be defined as the
unavoidable loss,in terms of the objective function, if we force the final
partition to be explainable.
  Here, we study this price for the following clustering problems: $k$-means,
$k$-medians, $k$-centers and maximum-spacing. We provide upper and lower bounds
for a natural model where explainability is achieved via decision trees. For
the $k$-means and $k$-medians problems our upper bounds improve those obtained
by [Moshkovitz et. al, ICML 20] for low dimensions.
  Another contribution is a simple and efficient algorithm for building
explainable clusterings for the $k$-means problem. We provide empirical
evidence that its performance is better than the current state of the art for
decision-tree based explainable clustering.