@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@article{jensen2023recurrent,
  title={A recurrent network model of planning explains hippocampal replay and human behavior},
  author={Jensen, Kristopher T and Hennequin, Guillaume and Mattar, Marcelo G},
  journal={bioRxiv},
  pages={2023--01},
  year={2023},
  publisher={Cold Spring Harbor Laboratory}
}

@article{simon1990invariants,
  title={Invariants of human behavior},
  author={Simon, Herbert A},
  journal={Annual review of psychology},
  volume={41},
  number={1},
  pages={1--20},
  year={1990},
  publisher={Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}



@article{brunswik1955representative,
  title={Representative design and probabilistic theory in a functional psychology.},
  author={Brunswik, Egon},
  journal={Psychological review},
  volume={62},
  number={3},
  pages={193},
  year={1955},
  publisher={American Psychological Association}
}



@article{binz2022heuristics,
  title={Heuristics from bounded meta-learned inference.},
  author={Binz, Marcel and Gershman, Samuel J and Schulz, Eric and Endres, Dominik},
  journal={Psychological review},
  year={2022a},
  publisher={American Psychological Association}
}

@article{lucas2015rational,
  title={A rational model of function learning},
  author={Lucas, Christopher G and Griffiths, Thomas L and Williams, Joseph J and Kalish, Michael L},
  journal={Psychonomic bulletin \& review},
  volume={22},
  number={5},
  pages={1193--1215},
  year={2015},
  publisher={Springer}
}


@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{gigerenzer2011heuristic,
  title={Heuristic decision making},
  author={Gigerenzer, Gerd and Gaissmaier, Wolfgang},
  journal={Annual review of psychology},
  volume={62},
  pages={451--482},
  year={2011},
  publisher={Annual Reviews}
}

@article{speekenbrink2008learning,
  title={Learning strategies in amnesia},
  author={Speekenbrink, Maarten and Channon, Shelley and Shanks, David R},
  journal={Neuroscience \& Biobehavioral Reviews},
  volume={32},
  number={2},
  pages={292--310},
  year={2008},
  publisher={Elsevier}
}

@article{tamkin2023evaluating,
  title={Evaluating and mitigating discrimination in language model decisions},
  author={Tamkin, Alex and Askell, Amanda and Lovitt, Liane and Durmus, Esin and Joseph, Nicholas and Kravec, Shauna and Nguyen, Karina and Kaplan, Jared and Ganguli, Deep},
  journal={arXiv preprint arXiv:2312.03689},
  year={2023}
}

@article{stephan2009bayesian,
  title={Bayesian model selection for group studies},
  author={Stephan, Klaas Enno and Penny, Will D and Daunizeau, Jean and Moran, Rosalyn J and Friston, Karl J},
  journal={Neuroimage},
  volume={46},
  number={4},
  pages={1004--1017},
  year={2009},
  publisher={Elsevier}
}
@article{speekenbrink2010models,
  title={Models of probabilistic category learning in Parkinson’s disease: Strategy use and the effects of L-dopa},
  author={Speekenbrink, Maarten and Lagnado, David A and Wilkinson, Leonora and Jahanshahi, Marjan and Shanks, David R},
  journal={Journal of Mathematical Psychology},
  volume={54},
  number={1},
  pages={123--136},
  year={2010},
  publisher={Elsevier}
}
@article{karelaia2008determinants,
  title={Determinants of linear judgment: A meta-analysis of lens model studies.},
  author={Karelaia, Natalia and Hogarth, Robin M},
  journal={Psychological bulletin},
  volume={134},
  number={3},
  pages={404},
  year={2008},
  publisher={American Psychological Association}
}

@misc{mitra2023orca,
author = {Mitra, Arindam and Del Corro, Luciano and Mahajan, Shweti and Codas, Andres and Simoes Ribeiro, Clarisse and Agrawal, Sahaj and Chen, Xuxi and Razdaibiedina, Anastasia and Jones, Erik and Aggarwal, Kriti and Palangi, Hamid and Zheng, Guoqing and Rosset, Corby and Khanpour, Hamed and Awadallah, Ahmed},
title = {Orca-2: Teaching Small Language Models How to Reason},
howpublished = {arXiv},
year = {2023},
month = {November},
abstract = {Orca 1 learns from rich signals, such as explanation traces, allowing it to outperform conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs’ reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models. We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.). More crucially, we aim to help the model learn to determine the most effective solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings. We open-source Orca 2 to encourage further research on the development, evaluation, and alignment of smaller LMs.},}
%url = {https://www.microsoft.com/en-us/research/publication/orca-2-teaching-small-language-models-how-to-reason/},}

@book{neisser1987cognition,
  title={Cognition and reality},
  author={Neisser, Ulric},
  year={1987},
  publisher={W H Freeman/Times Books/ Henry Holt and Co}
}

@article{coda2023meta,
  title={Meta-in-context learning in large language models},
  author={Coda-Forno, Julian and Binz, Marcel and Akata, Zeynep and Botvinick, Matthew and Wang, Jane X and Schulz, Eric},
  journal={arXiv preprint arXiv:2305.12907},
  year={2023}
}
@book{barker1968ecological,
  title={Ecological psychology: Concepts and Methods for Studying the Environment of Human Behavior.},
  author={Barker, Roger G},
  publisher={Stanford, CA: Stanford University Press},
  year={1968}
}

@misc{hammond1998ecological,
  title={Ecological validity: Then and now},
  author={Hammond, Kenneth R},
  year={1998}
}

@article{bronfenbrenner1977toward,
  title={Toward an experimental ecology of human development.},
  author={Bronfenbrenner, Urie},
  journal={American psychologist},
  volume={32},
  number={7},
  pages={513},
  year={1977},
  publisher={American Psychological Association}
}

@article{aanstoos1991experimental,
  title={Experimental psychology and the challenge of real life.},
  author={Aanstoos, Christopher M},
  year={1991},
  publisher={American Psychological Association}
}

@BOOK{Pratt1998-gq,
  title     = "Learning to learn",
  author    = "Pratt, Lorien and Thrun, Sebastian",
  abstract  = "Over the past three decades or so, research on machine learning
               and data mining has led to a wide variety of algorithms that
               learn general functions from experience. As machine learning is
               maturing, it has begun to make the successful transition from
               academic research to various practical applications. Generic
               techniques such as decision trees and artificial neural
               networks, for example, are now being used in various commercial
               and industrial applications. Learning to Learn is an exciting
               new research direction within machine learning. Similar to
               traditional machine-learning algorithms, the methods described
               in Learning to Learn induce general functions from experience.
               However, the book investigates algorithms that can change the
               way they generalize, i.e., practice the task of learning itself,
               and improve on it. To illustrate the utility of learning to
               learn, it is worthwhile comparing machine learning with human
               learning. Humans encounter a continual stream of learning tasks.
               They do not just learn concepts or motor skills, they also learn
               bias, i.e., they learn how to generalize. As a result, humans
               are often able to generalize correctly from extremely few
               examples - often just a single example suffices to teach us a
               new thing. A deeper understanding of computer programs that
               improve their ability to learn can have a large practical impact
               on the field of machine learning and beyond. In recent years,
               the field has made significant progress towards a theory of
               learning to learn along with practical new algorithms, some of
               which led to impressive results in real-world applications.
               Learning to Learn provides a survey of some of the most exciting
               new research approaches, written by leading researchers in the
               field. Its objective is to investigate the utility and
               feasibility of computer programs that can learn how to learn,
               both from a practical and a theoretical point of view.",
  publisher = "Kluwer Academic Publishers",
  year      =  1998,
  language  = "en"
}
@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}
@article{schick2021generating,
  title={Generating datasets with pretrained language models},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2104.07540},
  year={2021}
}

@MISC{claude2.1,
  title        = "Introducing Claude 2.1",
  booktitle    = "Anthropic",
  author        = "AnthropicAI",
  abstract     = "Anthropic is an AI safety and research company that's working
                  to build reliable, interpretable, and steerable AI systems.",
  howpublished = "\url{https://www.anthropic.com/news/claude-2-1}",
  note         = "Accessed: 2024-1-29",
  language     = "en",
  year={2023}
}


@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{brehmer1974hypotheses,
  title={Hypotheses about relations between scaled variables in the learning of probabilistic inference tasks},
  author={Brehmer, Berndt},
  journal={Organizational Behavior and Human Performance},
  volume={11},
  number={1},
  pages={1--27},
  year={1974},
  publisher={Elsevier}
}
@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},}
%url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}}

@article{daunizeau2014vba,
  title={VBA: a probabilistic treatment of nonlinear models for neurobiological and behavioural data},
  author={Daunizeau, Jean and Adam, Vincent and Rigoux, Lionel},
  journal={PLoS computational biology},
  volume={10},
  number={1},
  pages={e1003441},
  year={2014},
  publisher={Public Library of Science San Francisco, USA}
}

@book{l2008bayesian,
  title={Bayesian models of cognition},
  author={Griffiths, Thomas and Kemp, Charles and B Tenenbaum, Joshua},
  year={2008},
  publisher={Carnegie Mellon University}
}

@article{anderson1991human,
  title={Is human cognition adaptive?},
  author={Anderson, John R},
  journal={Behavioral and brain sciences},
  volume={14},
  number={3},
  pages={471--485},
  year={1991},
  publisher={Cambridge University Press}
}

@ARTICLE{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}
@article{wang2023improving,
  title={Improving Text Embeddings with Large Language Models},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2401.00368},
  year={2023}
}

@article{gunasekar2023textbooks,
  title={Textbooks Are All You Need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}

@inproceedings{schubert2023rational,
  title={A Rational Analysis of the Optimism Bias using Meta-Reinforcement Learning},
  author={Schubert, JA and Jagadish, AK and Binz, M and Schulz, E},
  booktitle={Conference on Cognitive Computational Neuroscience (CCN 2023)},
  year={2023}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@article{oml-benchmarking-suites,
      title={OpenML Benchmarking Suites}, 
      author={Bernd Bischl and Giuseppe Casalicchio and Matthias Feurer and Frank Hutter and Michel Lang and Rafael G. Mantovani and Jan N. van Rijn and Joaquin Vanschoren},
      year={2019},
      journal={arXiv:1708.03731v2 [stat.ML]}
}

@inproceedings{
  hollmann2023tabpfn,
  title={Tab{PFN}: A Transformer That Solves Small Tabular Classification Problems in a Second},
  author={Noah Hollmann and Samuel M{\"u}ller and Katharina Eggensperger and Frank Hutter},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023},
  url={https://openreview.net/forum?id=cp5PvcI6w8_}
}

@inproceedings{chen2016xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
  pages={785--794},
  year={2016}
}

@ARTICLE{Wang2023-le,
  title         = "{UniPredict}: Large Language Models are Universal Tabular
                   Predictors",
  author        = "Wang, Ruiyu and Wang, Zifeng and Sun, Jimeng",
  abstract      = "Tabular data prediction is a fundamental machine learning
                   task for many applications. Existing methods predominantly
                   employ discriminative modeling and operate under the
                   assumption of a fixed target column, necessitating
                   re-training for every new predictive task. Inspired by the
                   generative power of large language models (LLMs), this paper
                   exploits the idea of building universal tabular data
                   predictors based on generative modeling, namely UniPredict.
                   Here, we show that scaling up an LLM to extensive tabular
                   datasets with the capability of comprehending diverse
                   tabular inputs and predicting for target variables following
                   the input instructions. Specifically, we train a single LLM
                   on an aggregation of 169 tabular datasets with diverse
                   targets and compare its performance against baselines that
                   are trained on each dataset separately. We observe this
                   versatile UniPredict model demonstrates an advantage over
                   other models, ranging from 5.4\% to 13.4\%, when compared
                   with the best tree-boosting baseline and the best neural
                   network baseline, respectively. We further test UniPredict
                   in few-shot learning settings on another 62 tabular
                   datasets. Our method achieves strong performance in quickly
                   adapting to new tasks, where our method outperforms XGBoost
                   over 100\% on the low-resource setup and shows a significant
                   margin over all baselines. We envision that UniPredict sheds
                   light on developing a universal tabular data prediction
                   system that learns from data at scale and serves a wide
                   range of prediction tasks.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  eprint        = "2310.03266",
  primaryClass  = "cs.LG",
  arxivid       = "2310.03266"
}

@ARTICLE{Seedat2023-uv,
  title         = "Curated {LLM}: Synergy of {LLMs} and Data Curation for
                   tabular augmentation in ultra low-data regimes",
  author        = "Seedat, Nabeel and Huynh, Nicolas and van Breugel, Boris and
                   van der Schaar, Mihaela",
  abstract      = "Machine Learning (ML) in low-data settings remains an
                   underappreciated yet crucial problem. This challenge is
                   pronounced in low-to-middle income countries where access to
                   large datasets is often limited or even absent. Hence, data
                   augmentation methods to increase the sample size of datasets
                   needed for ML are key to unlocking the transformative
                   potential of ML in data-deprived regions and domains.
                   Unfortunately, the limited training set constrains
                   traditional tabular synthetic data generators in their
                   ability to generate a large and diverse augmented dataset
                   needed for ML tasks. To address this technical challenge, we
                   introduce CLLM, which leverages the prior knowledge of Large
                   Language Models (LLMs) for data augmentation in the low-data
                   regime. While diverse, not all the data generated by LLMs
                   will help increase utility for a downstream task, as for any
                   generative model. Consequently, we introduce a principled
                   curation process, leveraging learning dynamics, coupled with
                   confidence and uncertainty metrics, to obtain a high-quality
                   dataset. Empirically, on multiple real-world datasets, we
                   demonstrate the superior performance of LLMs in the low-data
                   regime compared to conventional generators. We further show
                   our curation mechanism improves the downstream performance
                   for all generators, including LLMs. Additionally, we provide
                   insights and understanding into the LLM generation and
                   curation mechanism, shedding light on the features that
                   enable them to output high-quality augmented datasets. CLLM
                   paves the way for wider usage of ML in data scarce domains
                   and regions, by allying the strengths of LLMs with a robust
                   data-centric approach.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  eprint        = "2312.12112",
  primaryClass  = "cs.LG",
  arxivid       = "2312.12112"
}

@ARTICLE{Nosofsky1994-gu,
  title    = "Rule-plus-exception model of classification learning",
  author   = "Nosofsky, R M and Palmeri, T J and McKinley, S C",
  abstract = "The authors propose a rule-plus-exception model (RULEX) of
              classification learning. According to RULEX, people learn to
              classify objects by forming simple logical rules and remembering
              occasional exceptions to those rules. Because the learning
              process in RULEX is stochastic, the model predicts that
              individual Ss will vary greatly in the particular rules that are
              formed and the exceptions that are stored. Averaged
              classification data are presumed to represent mixtures of these
              highly idiosyncratic rules and exceptions. RULEX accounts for
              numerous fundamental classification phenomena, including
              prototype and specific exemplar effects, sensitivity to
              correlational information, difficulty of learning linearly
              separable versus nonlinearly separable categories, selective
              attention effects, and difficulty of learning concepts with rules
              of differing complexity. RULEX also predicts distributions of
              generalization patterns observed at the individual subject level.",
  journal  = "Psychol. Rev.",
  volume   =  101,
  number   =  1,
  pages    = "53--79",
  month    =  jan,
  year     =  1994,
  language = "en",
  issn     = "0033-295X",
  pmid     = "8121960",
  doi      = "10.1037/0033-295x.101.1.53"
}

@ARTICLE{ellis2023,
  title    = "Human-like {Few-Shot} Learning via Bayesian Reasoning over
              Natural Language",
  author   = "Ellis, Kevin",
  abstract = "A core tension in models of concept learning is that the model
              must carefully balance the tractability of inference against the
              expressivity of the hypothesis class. Humans, however, can
              efficiently learn a broad range of concepts. We introduce a model
              of inductive learning that seeks to be human-like in that sense.
              It implements a Bayesian reasoning process where a language model
              first proposes candidate hypotheses expressed in natural
              language, which are then re-weighed by a prior and a likelihood.
              By estimating the prior from human data, we can predict human
              judgments on learning problems involving numbers and sets,
              spanning concepts that are generative, discriminative,
              propositional, and higher-order.",
  month    =  nov,
  year     =  2023
}

@article{Griffiths2023-pt,
  title={Bayes in the age of intelligent machines},
  author={Griffiths, Thomas L and Zhu, Jian-Qiao and Grant, Erin and McCoy, R Thomas},
  journal={arXiv preprint arXiv:2311.10206},
  year={2023}
}

@ARTICLE{Lake2023-wq,
  title    = "Human-like systematic generalization through a meta-learning
              neural network",
  author   = "Lake, Brenden M and Baroni, Marco",
  abstract = "The power of human language and thought arises from systematic
              compositionality-the algebraic ability to understand and produce
              novel combinations from known components. Fodor and Pylyshyn1
              famously argued that artificial neural networks lack this
              capacity and are therefore not viable models of the mind. Neural
              networks have advanced considerably in the years since, yet the
              systematicity challenge persists. Here we successfully address
              Fodor and Pylyshyn's challenge by providing evidence that neural
              networks can achieve human-like systematicity when optimized for
              their compositional skills. To do so, we introduce the
              meta-learning for compositionality (MLC) approach for guiding
              training through a dynamic stream of compositional tasks. To
              compare humans and machines, we conducted human behavioural
              experiments using an instruction learning paradigm. After
              considering seven different models, we found that, in contrast to
              perfectly systematic but rigid probabilistic symbolic models, and
              perfectly flexible but unsystematic neural networks, only MLC
              achieves both the systematicity and flexibility needed for
              human-like generalization. MLC also advances the compositional
              skills of machine learning systems in several systematic
              generalization benchmarks. Our results show how a standard neural
              network architecture, optimized for its compositional skills, can
              mimic human systematic generalization in a head-to-head
              comparison.",
  journal  = "Nature",
  month    =  oct,
  year     =  2023,
  language = "en",
  issn     = "0028-0836, 1476-4687",
  pmid     = "37880371",
  doi      = "10.1038/s41586-023-06668-3",
  pmc      = "3799308"
}

@INCOLLECTION{Nosofsky2011-rf,
  title     = "The generalized context model: An exemplar model of
               classification",
  booktitle = "Formal approaches in categorization , (pp",
  author    = "Nosofsky, Robert M",
  editor    = "Pothos, Emmanuel M",
  abstract  = "The generalized context model (GCM) is an exemplar model of
               supervised categorization. A novel stimulus is classified into a
               pre-existing category based on its similarity to known members
               of that category (and to members of other known categories).
               Similarity in the GCM is specified in terms of distances in a
               psychological space, as proposed by, for example, Shepard
               (1987). So, at the heart of the GCM is a principle of
               psychological similarity. A fundamental aspect of the GCM is
               that it computes similarity relations not just on the basis of
               the original psychological space, but also any transformations
               of this space that are possible through (graded) attentional
               selection or compression/stretching of the psychological space
               as a whole. In this way, the GCM is a very powerful model: it is
               most often the case that its parameters can be set in a way that
               human data in a supervised categorization can be closely
               reproduced. The GCM makes relatively few prior assumptions about
               the categorization process. For example, parameters governing
               attentional weighting, the form of the similarity function, the
               metric space, the nature of responding (probabilistic versus
               deterministic) can all be set in response to fitting particular
               human data. The price for this flexibility is, of course, the
               relatively large number of free parameters. Some key
               psychological assumptions embodied in the GCM (apart from the
               obvious one, that category representation is based on individual
               exemplars) are that graded attentional weighting of stimulus
               dimensions and stretching/compression of psychological space are
               possible as a result of learning. (PsycInfo Database Record (c)
               2022 APA, all rights reserved)",
  publisher = "Cambridge University Press, xii",
  volume    =  336,
  pages     = "18--39",
  year      =  2011,
  address   = "New York, NY, US",
  doi       = "10.1017/CBO9780511921322.002"
}


@ARTICLE{Feldman2000-ym,
  title    = "Minimization of Boolean complexity in human concept learning",
  author   = "Feldman, J",
  abstract = "One of the unsolved problems in the field of human concept
              learning concerns the factors that determine the subjective
              difficulty of concepts: why are some concepts psychologically
              simple and easy to learn, while others seem difficult, complex or
              incoherent? This question was much studied in the 1960s but was
              never answered, and more recent characterizations of concepts as
              prototypes rather than logical rules leave it unsolved. Here I
              investigate this question in the domain of Boolean concepts
              (categories defined by logical rules). A series of experiments
              measured the subjective difficulty of a wide range of logical
              varieties of concepts (41 mathematically distinct types in six
              families--a far wider range than has been tested previously). The
              data reveal a surprisingly simple empirical 'law': the subjective
              difficulty of a concept is directly proportional to its Boolean
              complexity (the length of the shortest logically equivalent
              propositional formula)--that is, to its logical
              incompressibility.",
  journal  = "Nature",
  volume   =  407,
  number   =  6804,
  pages    = "630--633",
  month    =  oct,
  year     =  2000,
  language = "en",
  issn     = "0028-0836",
  pmid     = "11034211",
  doi      = "10.1038/35036586"
}

@ARTICLE{Johansen2002-xe,
  title    = "Are there representational shifts during category learning?",
  author   = "Johansen, Mark K and Palmeri, Thomas J",
  abstract = "Early theories of categorization assumed that either rules, or
              prototypes, or exemplars were exclusively used to mentally
              represent categories of objects. More recently, hybrid theories
              of categorization have been proposed that variously combine these
              different forms of category representation. Our research
              addressed the question of whether there are representational
              shifts during category learning. We report a series of
              experiments that tracked how individual subjects generalized
              their acquired category knowledge to classifying new critical
              transfer items as a function of learning. Individual differences
              were observed in the generalization patterns exhibited by
              subjects, and those generalizations changed systematically with
              experience. Early in learning, subjects generalized on the basis
              of single diagnostic dimensions, consistent with the use of
              simple categorization rules. Later in learning, subjects
              generalized in a manner consistent with the use of
              similarity-based exemplar retrieval, attending to multiple
              stimulus dimensions. Theoretical modeling was used to formally
              corroborate these empirical observations by comparing fits of
              rule, prototype, and exemplar models to the observed
              categorization data. Although we provide strong evidence for
              shifts in the kind of information used to classify objects as a
              function of categorization experience, interpreting these results
              in terms of shifts in representational systems underlying
              perceptual categorization is a far thornier issue. We provide a
              discussion of the challenges of making claims about category
              representation, making reference to a wide body of literature
              suggesting different kinds of representational systems in
              perceptual categorization and related domains of human cognition.",
  journal  = "Cogn. Psychol.",
  volume   =  45,
  number   =  4,
  pages    = "482--553",
  month    =  dec,
  year     =  2002,
  language = "en",
  issn     = "0010-0285",
  pmid     = "12480477",
  doi      = "10.1016/s0010-0285(02)00505-4"
}

@article{Borisov2022-sr,
  title={Language models are realistic tabular data generators},
  author={Borisov, Vadim and Se{\ss}ler, Kathrin and Leemann, Tobias and Pawelczyk, Martin and Kasneci, Gjergji},
  journal={arXiv preprint arXiv:2210.06280},
  year={2022}
}

@ARTICLE{Shepard1961-yu,
  title    = "Learning and memorization of classifications",
  author   = "Shepard, Roger N and Hovland, Carl I and Jenkins, Herbert M",
  abstract = "The present study explores some of the factors that determine how
              difficult a classification will be to learn or remember. The
              experiment to be reported was designed primarily to answer two
              questions: How does the difficulty of learning vary from one type
              of classification to another? Is something specific learned about
              the structure of a classification that will transfer positively
              to the subsequent learning of a new classification of that same
              type? The experimental procedure conformed to the usual
              paired-associate paradigm except that only two responses were
              used. That is, the eight stimuli were presented, one at a time,
              in a continuing random sequence and an association between each
              stimulus and one of two alternative classificatory responses was
              built up by the method of anticipation. Six female freshmen at
              Fairleigh Dickinson University served for 15 hours each in this
              first experiment. These subjects (Ss) were selected to be as
              uniform as possible with respect to their college entrance
              examination scores. A combined empirical and theoretical
              investigation of the difficulties of different kinds of
              classifications was undertaken using both learning and memory
              tasks. Sets of stimuli of a variety of kinds were used but, in
              each set, there were eight stimuli each of which took on one of
              two possible values on each of three different dimensions.
              Results showed that of the 70 possible classifications of the
              eight stimuli into two equal groups, there are only six basic
              types. The different classifications belonging to any one of
              these types have the same structure; they differ only with
              respect to which of the three dimensions is assigned to which of
              the three roles in the classification, and with respect to which
              of the two classificatory responses is assigned to which group of
              four stimuli. (PsycINFO Database Record (c) 2016 APA, all rights
              reserved)",
  journal  = "Psychological Monographs: General and Applied",
  volume   =  75,
  number   =  13,
  pages    = "1--42",
  year     =  1961,
  issn     = "0096-9753",
  doi      = "10.1037/h0093825"
}

@BOOK{Todd2012-ta,
  title     = "Ecological rationality: Intelligence in the world",
  author    = "Todd, Peter M and Gigerenzer, Gerd",
  publisher = "Oxford University Press",
  month     =  feb,
  year      =  2012,
  address   = "Cary, NC",
  isbn      = "9786613594013"
}

@ARTICLE{Nosofsky1994-hw,
  title    = "Comparing models of rule-based classification learning: a
              replication and extension of Shepard, Hovland, and Jenkins (1961)",
  author   = "Nosofsky, R M and Gluck, M A and Palmeri, T J and McKinley, S C
              and Glauthier, P",
  abstract = "We partially replicate and extend Shepard, Hovland, and Jenkins's
              (1961) classic study of task difficulty for learning six
              fundamental types of rule-based categorization problems. Our main
              results mirrored those of Shepard et al., with the ordering of
              task difficulty being the same as in the original study. A much
              richer data set was collected, however, which enabled the
              generation of block-by-block learning curves suitable for
              quantitative fitting. Four current computational models of
              classification learning were fitted to the learning data: ALCOVE
              (Kruschke, 1992), the rational model (Anderson, 1991), the
              configural-cue model (Gluck \& Bower, 1988b), and an extended
              version of the configural-cue model with dimensionalized,
              adaptive learning rate mechanisms. Although all of the models
              captured important qualitative aspects of the learning data,
              ALCOVE provided the best overall quantitative fit. The results
              suggest the need to incorporate some form of selective attention
              to dimensions in category-learning models based on stimulus
              generalization and cue conditioning.",
  journal  = "Mem. Cognit.",
  volume   =  22,
  number   =  3,
  pages    = "352--369",
  month    =  may,
  year     =  1994,
  language = "en",
  issn     = "0090-502X",
  pmid     = "8007837",
  doi      = "10.3758/bf03200862"
}

@ARTICLE{Badham2017-hc,
  title     = "Deficits in category learning in older adults: Rule-based versus
               clustering accounts",
  author    = "Badham, Stephen P and Sanborn, Adam N and Maylor, Elizabeth A",
  journal   = "Psychol. Aging",
  publisher = "American Psychological Association (APA)",
  volume    =  32,
  number    =  5,
  pages     = "473--488",
  month     =  aug,
  year      =  2017,
  copyright = "http://creativecommons.org/licenses/by/3.0/",
  language  = "en",
  issn      = "0882-7974, 1939-1498",
  doi       = "10.1037/pag0000183"
}

@ARTICLE{Medin1978-xf,
  title     = "Context theory of classification learning",
  author    = "Medin, Douglas L and Schaffer, Marguerite M",
  journal   = "Psychol. Rev.",
  publisher = "American Psychological Association (APA)",
  volume    =  85,
  number    =  3,
  pages     = "207--238",
  month     =  may,
  year      =  1978,
  language  = "en",
  issn      = "0033-295X, 1939-1471",
  doi       = "10.1037/0033-295x.85.3.207"
}

@ARTICLE{Chater1999-yh,
  title    = "Ten years of the rational analysis of cognition",
  author   = "Chater, N and Oaksford, M",
  abstract = "Rational analysis is an empirical program that attempts to
              explain the function and purpose of cognitive processes. This
              article looks back on a decade of research outlining the rational
              analysis methodology and how the approach relates to other work
              in cognitive science. We illustrate rational analysis by
              considering how it has been applied to memory and reasoning. From
              the perspective of traditional cognitive science, the cognitive
              system can appear to be a rather arbitrary assortment of
              mechanisms with equally arbitrary limitations. In contrast,
              rational analysis views cognition as intricately adapted to its
              environment and to the problems it faces.",
  journal  = "Trends Cogn. Sci.",
  volume   =  3,
  number   =  2,
  pages    = "57--65",
  month    =  feb,
  year     =  1999,
  language = "en",
  issn     = "1364-6613, 1879-307X",
  pmid     = "10234228",
  doi      = "10.1016/s1364-6613(98)01273-x"
}

@ARTICLE{Chater2000-kt,
  title     = "The Rational Analysis of Mind and Behavior",
  author    = "Chater, Nick and Oaksford, Mike",
  abstract  = "[Rational analysis (Anderson 1990, 1991a) is an empirical
               program of attempting to explain why the cognitive system is
               adaptive, with respect to its goals and the structure of its
               environment. We argue that rational analysis has two important
               implications for philosophical debate concerning rationality.
               First, rational analysis provides a model for the relationship
               between formal principles of rationality (such as probability or
               decision theory) and everyday rationality, in the sense of
               successful thought and action in daily life. Second, applying
               the program of rational analysis to research on human reasoning
               leads to a radical reinterpretation of empirical results which
               are typically viewed as demonstrating human irrationality.]",
  journal   = "Synthese",
  publisher = "Springer",
  volume    =  122,
  number    = "1/2",
  pages     = "93--131",
  year      =  2000,
  issn      = "0039-7857, 1573-0964"
}

@ARTICLE{Anderson1991-ii,
  title    = "The adaptive nature of human categorization",
  author   = "Anderson, John R",
  abstract = "A rational model of human categorization behavior is presented
              that assumes that categorization reflects the derivation of
              optimal estimates of the probability of unseen features of
              objects. A Bayesian analysis is performed of what optimal
              estimations would be if categories formed a disjoint partitioning
              of the object space and if features were independently displayed
              within a category. This Bayesian analysis is placed within an
              incremental categorization algorithm. The resulting rational
              model accounts for effects of central tendency of categories,
              effects of specific instances, learning of linearly nonseparable
              categories, effects of category labels, extraction of basic level
              categories, base-rate effects, probability matching in
              categorization, and trial-by-trial learning functions. Although
              the rational model considers just 1 level of categorization, it
              is shown how predictions can be enhanced by considering higher
              and lower levels. Considering prediction at the lower, individual
              level allows integration of this rational analysis of
              categorization with the earlier rational analysis of memory by J.
              R. Anderson and R. Milson (see record 1990-03526-001). (PsycINFO
              Database Record (c) 2016 APA, all rights reserved)",
  journal  = "Psychol. Rev.",
  volume   =  98,
  number   =  3,
  pages    = "409--429",
  month    =  jul,
  year     =  1991,
  issn     = "0033-295X, 1939-1471",
  doi      = "10.1037/0033-295X.98.3.409"
}

@ARTICLE{Hollmann2022-cc,
  title         = "{TabPFN}: A Transformer That Solves Small Tabular
                   Classification Problems in a Second",
  author        = "Hollmann, Noah and M{\"u}ller, Samuel and Eggensperger,
                   Katharina and Hutter, Frank",
  abstract      = "We present TabPFN, a trained Transformer that can do
                   supervised classification for small tabular datasets in less
                   than a second, needs no hyperparameter tuning and is
                   competitive with state-of-the-art classification methods.
                   TabPFN is fully entailed in the weights of our network,
                   which accepts training and test samples as a set-valued
                   input and yields predictions for the entire test set in a
                   single forward pass. TabPFN is a Prior-Data Fitted Network
                   (PFN) and is trained offline once, to approximate Bayesian
                   inference on synthetic datasets drawn from our prior. This
                   prior incorporates ideas from causal reasoning: It entails a
                   large space of structural causal models with a preference
                   for simple structures. On the 18 datasets in the OpenML-CC18
                   suite that contain up to 1 000 training data points, up to
                   100 purely numerical features without missing values, and up
                   to 10 classes, we show that our method clearly outperforms
                   boosted trees and performs on par with complex
                   state-of-the-art AutoML systems with up to 70$\times$
                   speedup. This increases to a 3200$\times$ speedup when a GPU
                   is available. We also validate these results on an
                   additional 67 small numerical datasets from OpenML. We
                   provide all our code, the trained TabPFN, an interactive
                   browser demo and a Colab notebook at
                   https://github.com/automl/TabPFN.",
  month         =  jul,
  year          =  2022,
  archivePrefix = "arXiv",
  eprint        = "2207.01848",
  primaryClass  = "cs.LG",
  arxivid       = "2207.01848"
}

@article{chan2022data,
  title={Data distributional properties drive emergent in-context learning in transformers},
  author={Chan, Stephanie and Santoro, Adam and Lampinen, Andrew and Wang, Jane and Singh, Aaditya and Richemond, Pierre and McClelland, James and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={18878--18891},
  year={2022}
}

@article{mcelfresh2023neural,
  title={When Do Neural Nets Outperform Boosted Trees on Tabular Data?},
  author={McElfresh, Duncan and Khandagale, Sujay and Valverde, Jonathan and Ramakrishnan, Ganesh and Goldblum, Micah and White, Colin and others},
  journal={arXiv preprint arXiv:2305.02997},
  year={2023}
}

@article{Muller2021-ol,
  title={Transformers can do bayesian inference},
  author={M{\"u}ller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank},
  journal={arXiv preprint arXiv:2112.10510},
  year={2021}
}

@article{tenenbaum2001generalization,
  title={Generalization, similarity, and Bayesian inference},
  author={Tenenbaum, Joshua B and Griffiths, Thomas L},
  journal={Behavioral and brain sciences},
  volume={24},
  number={4},
  pages={629--640},
  year={2001},
  publisher={Cambridge University Press}
}


@ARTICLE{Kumar2022-bb,
  title         = "Using Natural Language and Program Abstractions to Instill
                   Human Inductive Biases in Machines",
  author        = "Kumar, Sreejan and Correa, Carlos G and Dasgupta, Ishita and
                   Marjieh, Raja and Hu, Michael Y and Hawkins, Robert D and
                   Daw, Nathaniel D and Cohen, Jonathan D and Narasimhan,
                   Karthik and Griffiths, Thomas L",
  abstract      = "Strong inductive biases give humans the ability to quickly
                   learn to perform a variety of tasks. Although meta-learning
                   is a method to endow neural networks with useful inductive
                   biases, agents trained by meta-learning may sometimes
                   acquire very different strategies from humans. We show that
                   co-training these agents on predicting representations from
                   natural language task descriptions and programs induced to
                   generate such tasks guides them toward more human-like
                   inductive biases. Human-generated language descriptions and
                   program induction models that add new learned primitives
                   both contain abstract concepts that can compress description
                   length. Co-training on these representations result in more
                   human-like behavior in downstream meta-reinforcement
                   learning agents than less abstract controls (synthetic
                   language descriptions, program induction without learned
                   primitives), suggesting that the abstraction supported by
                   these representations is key.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  eprint        = "2205.11558",
  primaryClass  = "cs.AI",
  arxivid       = "2205.11558"
}

@article{ortega2019meta,
  title={Meta-learning of sequential strategies},
  author={Ortega, Pedro A and Wang, Jane X and Rowland, Mark and Genewein, Tim and Kurth-Nelson, Zeb and Pascanu, Razvan and Heess, Nicolas and Veness, Joel and Pritzel, Alex and Sprechmann, Pablo and others},
  journal={arXiv preprint arXiv:1905.03030},
  year={2019}
}

@inproceedings{devraj2021dynamics,
  title={The dynamics of exemplar and prototype representations depend on environmental statistics},
  author={Devraj, Arjun and Zhang, Qiong and Griffiths, Tom},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={43},
  year={2021}
}

@article{achterberg2023building,
  title={Building artificial neural circuits for domain-general cognition: a primer on brain-inspired systems-level architecture},
  author={Achterberg, Jascha and Akarca, Danyal and Assem, Moataz and Heimbach, Moritz and Astle, Duncan E and Duncan, John},
  journal={arXiv preprint arXiv:2303.13651},
  year={2023}
}


@article{brandle2021exploration,
  title={Exploration beyond bandits},
  author={Br{\"a}ndle, Franziska and Binz, Marcel and Schulz, Eric},
  journal={The drive for knowledge: The science of human information seeking},
  pages={147--168},
  year={2021}
}


@misc{gretel2024ai,
  author = {gretelai},
  title = {Gretel Synthetics},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/gretelai/gretel-synthetics}},
}

@inproceedings{bourgin2019cognitive,
  title={Cognitive model priors for predicting human decisions},
  author={Bourgin, David D and Peterson, Joshua C and Reichman, Daniel and Russell, Stuart J and Griffiths, Thomas L},
  booktitle={International conference on machine learning},
  pages={5133--5141},
  year={2019},
  organization={PMLR}
}


@article{schulz2016probing,
  title={Probing the compositionality of intuitive functions},
  author={Schulz, Eric and Tenenbaum, Josh and Duvenaud, David K and Speekenbrink, Maarten and Gershman, Samuel J},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}


@article{peterson2021using,
  title={Using large-scale experiments and machine learning to discover theories of human decision-making},
  author={Peterson, Joshua C and Bourgin, David D and Agrawal, Mayank and Reichman, Daniel and Griffiths, Thomas L},
  journal={Science},
  volume={372},
  number={6547},
  pages={1209--1214},
  year={2021},
  publisher={American Association for the Advancement of Science}
}


@article{schulz2017compositional,
  title={Compositional inductive biases in function learning},
  author={Schulz, Eric and Tenenbaum, Joshua B and Duvenaud, David and Speekenbrink, Maarten and Gershman, Samuel J},
  journal={Cognitive psychology},
  volume={99},
  pages={44--79},
  year={2017},
  publisher={Elsevier}
}


@inproceedings{kumar2020meta,
  title={Meta-Learning of Structured Task Distributions in Humans and Machines},
  author={Kumar, Sreejan and Dasgupta, Ishita and Cohen, Jonathan and Daw, Nathaniel and Griffiths, Thomas},
  booktitle={International Conference on Learning Representations},
  year={2020}
}


@article{cortes1995support,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  pages={273--297},
  year={1995},
  publisher={Springer}
}

@article{binz2023meta,
  title={Meta-learned models of cognition},
  author={Binz, Marcel and Dasgupta, Ishita and Jagadish, Akshay and Botvinick, Matthew and Wang, Jane X and Schulz, Eric},
  journal={arXiv preprint arXiv:2304.06729},
  year={2023}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group UK London}
}

@article{yiu2023imitation,
  title={Imitation versus Innovation: What children can do that large language and language-and-vision models cannot (yet)?},
  author={Yiu, Eunice and Kosoy, Eliza and Gopnik, Alison},
  journal={arXiv preprint arXiv:2305.07666},
  year={2023}
}


@article{jagadish2023zero,
  title={Zero-shot compositional reinforcement learning in humans},
  author={Jagadish, Akshay Kumar and Binz, Marcel and Saanum, Tankred and Wang, Jane X and Schulz, Eric},
  year={2023},
  journal={PsyArXiv preprint PsyArXiv:ymve5}
}


@article{rigoux2014bayesian,
  title={Bayesian model selection for group studies—revisited},
  author={Rigoux, Lionel and Stephan, Klaas Enno and Friston, Karl J and Daunizeau, Jean},
  journal={Neuroimage},
  volume={84},
  pages={971--985},
  year={2014},
  publisher={Elsevier}
}

@MISC{Anthropic2023-hr,
  title        = "Claude 2",
  booktitle    = "Anthropic",
  author       = "Anthropic, P B C",
  abstract     = "We are pleased to announce Claude 2, our newest model, which
                  can be accessed via API as well as a new public-facing beta
                  website at claude.ai.",
  month        =  jul,
  year         =  2023,
  howpublished = "\url{https://www.anthropic.com/index/claude-2}",
  note         = "Accessed: 2024-1-15",
  language     = "en"
}

@article{smith1998prototypes,
  title={Prototypes in the mist: The early epochs of category learning.},
  author={Smith, J David and Minda, John Paul},
  journal={Journal of Experimental Psychology: Learning, memory, and cognition},
  volume={24},
  number={6},
  pages={1411},
  year={1998},
  publisher={American Psychological Association}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@article{kumar2022using,
  title={Using natural language and program abstractions to instill human inductive biases in machines},
  author={Kumar, Sreejan and Correa, Carlos G and Dasgupta, Ishita and Marjieh, Raja and Hu, Michael Y and Hawkins, Robert and Cohen, Jonathan D and Narasimhan, Karthik and Griffiths, Tom and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={167--180},
  year={2022}
}

@article{binz2022exploration,
  title={Modeling human exploration through resource-rational reinforcement learning},
  author={Binz, Marcel and Schulz, Eric},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31755--31768},
  year={2022b}
}


@article{lake2023human,
  title={Human-like systematic generalization through a meta-learning neural network},
  author={Lake, Brenden M and Baroni, Marco},
  journal={Nature},
  pages={1--7},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{griffiths2006optimal,
  title={Optimal predictions in everyday cognition},
  author={Griffiths, Thomas L and Tenenbaum, Joshua B},
  journal={Psychological science},
  volume={17},
  number={9},
  pages={767--773},
  year={2006},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}


@article{dekker2022curriculum,
  title={Curriculum learning for human compositional generalization},
  author={Dekker, Ronald B and Otto, Fabian and Summerfield, Christopher},
  journal={Proceedings of the National Academy of Sciences},
  volume={119},
  number={41},
  pages={e2205582119},
  year={2022},
  publisher={National Acad Sciences}
}


@article{nosofsky1986attention,
  title={Attention, similarity, and the identification--categorization relationship.},
  author={Nosofsky, Robert M},
  journal={Journal of experimental psychology: General},
  volume={115},
  number={1},
  pages={39},
  year={1986},
  publisher={American Psychological Association}
}

@article{homa1984role,
  title={Role of feedback, category size, and stimulus distortion on the acquisition and utilization of ill-defined categories.},
  author={Homa, Donald and Cultice, Joan C},
  journal={Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume={10},
  number={1},
  pages={83},
  year={1984},
  publisher={American Psychological Association}
}

@article{wang2016learning,
  title={Learning to reinforcement learn},
  author={Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
  journal={arXiv preprint arXiv:1611.05763},
  year={2016}
}

@inproceedings{
    muller2022transformers,
    title={Transformers Can Do Bayesian Inference},
    author={Samuel M{\"u}ller and Noah Hollmann and Sebastian Pineda Arango and Josif Grabocka and Frank Hutter},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=KSugKcbNf9}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{hochreiter2001learning,
  title={Learning to learn using gradient descent},
  author={Hochreiter, Sepp and Younger, A Steven and Conwell, Peter R},
  booktitle={Artificial Neural Networks—ICANN 2001: International Conference Vienna, Austria, August 21--25, 2001 Proceedings 11},
  pages={87--94},
  year={2001},
  organization={Springer}
}

@inproceedings{santoro2016meta,
  title={Meta-learning with memory-augmented neural networks},
  author={Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  booktitle={International conference on machine learning},
  pages={1842--1850},
  year={2016},
  organization={PMLR}
}


@article{love_sustain_2004,
	title = {{SUSTAIN}: {A} {Network} {Model} of {Category} {Learning}.},
	volume = {111},
	issn = {1939-1471, 0033-295X},
	shorttitle = {{SUSTAIN}},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.111.2.309},
	doi = {10.1037/0033-295X.111.2.309},
	language = {en},
	number = {2},
	urldate = {2021-08-31},
	journal = {Psychological Review},
	author = {Love, Bradley C. and Medin, Douglas L. and Gureckis, Todd M.},
	year = {2004},
	pages = {309--332},
	file = {Love et al. - 2004 - SUSTAIN A Network Model of Category Learning..pdf:C\:\\Users\\Mirko\\Zotero\\storage\\5R5TMWID\\Love et al. - 2004 - SUSTAIN A Network Model of Category Learning..pdf:application/pdf},
}

@article{kruschke_alcove_1992,
	title = {{ALCOVE}: {An} exemplar-based connectionist model of category learning.},
	volume = {99},
	issn = {1939-1471},
	shorttitle = {{ALCOVE}},
	url = {https://psycnet.apa.org/fulltext/1992-18164-001.pdf},
	doi = {10.1037/0033-295X.99.1.22},
	number = {1},
	urldate = {2021-08-31},
	journal = {Psychological Review},
	author = {Kruschke, John K.},
	month = may,
	year = {1992},
	note = {Publisher: US: American Psychological Association},
	pages = {22},
	file = {Kruschke - ALCOVE An exemplar-based connectionist model of c.pdf:C\:\\Users\\Mirko\\Zotero\\storage\\E6XV2CHR\\Kruschke - ALCOVE An exemplar-based connectionist model of c.pdf:application/pdf;Snapshot:C\:\\Users\\Mirko\\Zotero\\storage\\2EVKNRBT\\1992-18164-001.html:text/html},
}

@article{maddox_comparing_1993,
	title = {Comparing decision bound and exemplar models of categorization},
	volume = {53},
	issn = {0031-5117, 1532-5962},
	url = {http://link.springer.com/10.3758/BF03211715},
	doi = {10.3758/BF03211715},
	language = {en},
	number = {1},
	urldate = {2021-10-29},
	journal = {Perception \& Psychophysics},
	author = {Maddox, W. Todd and Ashby, F. Gregory},
	month = jan,
	year = {1993},
	pages = {49--70},
	file = {Maddox and Ashby - 1993 - Comparing decision bound and exemplar models of ca.pdf:C\:\\Users\\Mirko\\Zotero\\storage\\ZWTP96QE\\Maddox and Ashby - 1993 - Comparing decision bound and exemplar models of ca.pdf:application/pdf},
}

@article{nosofsky_exemplar_2002,
	title = {Exemplar and prototype models revisited: {Response} strategies, selective attention, and stimulus generalization.},
	volume = {28},
	issn = {1939-1285},
	shorttitle = {Exemplar and prototype models revisited},
	url = {https://psycnet.apa.org/fulltext/2002-15432-008.pdf},
	doi = {10.1037/0278-7393.28.5.924},
	number = {5},
	urldate = {2022-09-15},
	journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Nosofsky, Robert M. and Zaki, Safa R.},
	month = aug,
	year = {2002},
	note = {Publisher: US: American Psychological Association},
	pages = {924},
	file = {Nosofsky and Zaki - Exemplar and prototype models revisited Response .pdf:C\:\\Users\\Mirko\\Zotero\\storage\\IKKF8AHB\\Nosofsky and Zaki - Exemplar and prototype models revisited Response .pdf:application/pdf;Snapshot:C\:\\Users\\Mirko\\Zotero\\storage\\5RPQELHQ\\2002-15432-008.html:text/html},
}

@article{ashby_varieties_1986,
	title = {Varieties of perceptual independence},
	volume = {93},
	issn = {1939-1471},
	doi = {10.1037/0033-295X.93.2.154},
	number = {2},
	journal = {Psychological Review},
	author = {Ashby, F. Gregory and Townsend, James T.},
	year = {1986},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Theories, Perceptual Stimulation, Perceptual Style},
	pages = {154--179},
	file = {Full Text PDF:C\:\\Users\\Mirko\\Zotero\\storage\\S8A76PSW\\Ashby and Townsend - 1986 - Varieties of perceptual independence.pdf:application/pdf;Snapshot:C\:\\Users\\Mirko\\Zotero\\storage\\SUVHZXFW\\1986-21044-001.html:text/html},
}

@incollection{newell_chapter_2011,
	series = {Advances in {Research} and {Theory}},
	title = {Chapter six - {Systems} of {Category} {Learning}: {Fact} or {Fantasy}?},
	volume = {54},
	shorttitle = {Chapter six - {Systems} of {Category} {Learning}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780123855275000061},
	urldate = {2024-01-24},
	booktitle = {Psychology of {Learning} and {Motivation}},
	publisher = {Academic Press},
	author = {Newell, Ben R. and Dunn, John C. and Kalish, Michael},
	editor = {Ross, Brian H.},
	month = jan,
	year = {2011},
	doi = {10.1016/B978-0-12-385527-5.00006-1},
	pages = {167--215},
	file = {ScienceDirect Snapshot:C\:\\Users\\Mirko\\Zotero\\storage\\7GYBNEIB\\B9780123855275000061.html:text/html},
}

@article{ashby_human_2005,
	title = {Human {Category} {Learning}},
	volume = {56},
	issn = {0066-4308, 1545-2085},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.psych.56.091103.070217},
	doi = {10.1146/annurev.psych.56.091103.070217},
	language = {en},
	number = {1},
	urldate = {2024-01-24},
	journal = {Annual Review of Psychology},
	author = {Ashby, F. Gregory and Maddox, W. Todd},
	month = feb,
	year = {2005},
	pages = {149--178},
	file = {Ashby and Maddox - 2005 - Human Category Learning.pdf:C\:\\Users\\Mirko\\Zotero\\storage\\IHJJE6D6\\Ashby and Maddox - 2005 - Human Category Learning.pdf:application/pdf},
}
