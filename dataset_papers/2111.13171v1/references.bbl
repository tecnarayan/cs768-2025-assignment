\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{FGFAEV21}

\bibitem[AAF{\etalchar{+}}20]{Adams2020AFD}
Henry Adams, M.~Aminian, Elin Farnell, M.~Kirby, C.~Peterson, Joshua Mirth,
  R.~Neville, P.~Shipman, and C.~Shonkwiler.
\newblock A fractal dimension for measures via persistent homology.
\newblock {\em arXiv: Dynamical Systems}, pages 1--31, 2020.

\bibitem[AGNZ18]{arora2018stronger}
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi~Zhang.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning}, volume~80, pages 254--263. PMLR, 10--15 Jul 2018.

\bibitem[ALMZ19]{ansuini2019intrinsic}
Alessio Ansuini, Alessandro Laio, Jakob~H Macke, and Davide Zoccolan.
\newblock Intrinsic dimension of data representations in deep neural networks.
\newblock In {\em Proceedings of the Advances in Neural Information Processing
  Systems (NeurIPS)}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Bau21]{ripser}
Ulrich Bauer.
\newblock Ripser: efficient computation of vietoris-rips persistence barcodes,
  February 2021.
\newblock Preprint.

\bibitem[BE02]{bousquet2002stability}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock {\em JMLR}, 2(Mar), 2002.

\bibitem[Ben69]{bennett1969intrinsic}
Robert Bennett.
\newblock The intrinsic dimensionality of signal collections.
\newblock {\em IEEE Transactions on Information Theory}, 15(5):517--525, 1969.

\bibitem[BG60]{blumenthal1960some}
Robert~M Blumenthal and Ronald~K Getoor.
\newblock Some theorems on stable processes.
\newblock {\em Transactions of the American Mathematical Society},
  95(2):263--273, 1960.

\bibitem[BGND{\etalchar{+}}19]{bruel2019topology}
Rickard Br{\"u}el-Gabrielsson, Bradley~J Nelson, Anjan Dwaraknath, Primoz
  Skraba, Leonidas~J Guibas, and Gunnar Carlsson.
\newblock A topology layer for machine learning.
\newblock {\em arXiv preprint arXiv:1905.12200}, 2019.

\bibitem[BM02]{bartlett2002rademacher}
Peter~L Bartlett and Shahar Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock {\em Journal of Machine Learning Research}, 3(Nov):463--482, 2002.

\bibitem[BO18]{blier2018description}
L\'{e}onard Blier and Yann Ollivier.
\newblock The description length of deep learning models.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, {\em Proceedings of the Advances in Neural
  Information Processing Systems (NeurIPS)}, volume~31. Curran Associates,
  Inc., 2018.

\bibitem[BP19]{boissonnat2019}
Jean-Daniel Boissonnat and Siddharth Pritam.
\newblock {Computing Persistent Homology of Flag Complexes via Strong
  Collapses}.
\newblock In {\em 35th International Symposium on Computational Geometry (SoCG
  2019)}, volume 129 of {\em Leibniz International Proceedings in Informatics
  (LIPIcs)}, pages 55:1--55:15, 2019.

\bibitem[BSE{\etalchar{+}}21]{barsbey2021heavy}
Melih Barsbey, Milad Sefidgaran, Murat~A Erdogdu, Ga{\"e}l Richard, and Umut
  {\c{S}}im{\c{s}}ekli.
\newblock Heavy tails in sgd and compressibility of overparametrized neural
  networks.
\newblock In {\em NeurIPS}, 2021.

\bibitem[BT74]{beaton1974fitting}
Albert~E Beaton and John~W Tukey.
\newblock The fitting of power series, meaning polynomials, illustrated on
  band-spectroscopic data.
\newblock {\em Technometrics}, 16(2):147--185, 1974.

\bibitem[Car14]{carlsson2014topological}
Gunnar Carlsson.
\newblock Topological pattern recognition for point cloud data.
\newblock {\em Acta Numerica}, 23:289--368, 2014.

\bibitem[CCCR15]{campadelli2015intrinsic}
P~Campadelli, E~Casiraghi, C~Ceruti, and A~Rozza.
\newblock Intrinsic dimension estimation: Relevant techniques and a benchmark
  framework.
\newblock {\em Mathematical Problems in Engineering}, 2015, 2015.

\bibitem[CDE{\etalchar{+}}21]{camuto2021fractal}
Alexander Camuto, George Deligiannidis, Murat~A Erdogdu, Mert
  G{\"u}rb{\"u}zbalaban, Umut {\c{S}}im{\c{s}}ekli, and Lingjiong Zhu.
\newblock Fractal structure and generalization properties of stochastic
  optimization algorithms.
\newblock In {\em NeurIPS}, 2021.

\bibitem[CH03]{costa2003manifold}
Jose Costa and Alfred Hero.
\newblock Manifold learning with geodesic minimal spanning trees.
\newblock {\em arXiv preprint cs/0307038}, 2003.

\bibitem[CHN19]{Hofer19a}
M.~Dixit C.~Hofer, R.~Kwitt and M.~Niethammer.
\newblock Connectivity-optimized representation learning via persistent
  homology.
\newblock In {\em ICML}, 2019.

\bibitem[CHU17]{Hofer17a}
M.~Niethammer C.~Hofer, R.~Kwitt and A.~Uhl.
\newblock Deep learning with topological signatures.
\newblock In {\em NIPS}, 2017.

\bibitem[CMEM20]{Corneanu2020ComputingTT}
C.~Corneanu, M.~Madadi, S.~Escalera, and A.~Mart{\'i}nez.
\newblock Computing the testing error without a testing set.
\newblock {\em 2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 2674--2682, 2020.

\bibitem[CNBW19]{chen2019topological}
Chao Chen, Xiuyan Ni, Qinxun Bai, and Yusu Wang.
\newblock A topological regularizer for classifiers via persistent homology.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 2573--2582. PMLR, 2019.

\bibitem[CS16]{camastra2016intrinsic}
Francesco Camastra and Antonino Staiano.
\newblock Intrinsic dimension estimation: Advances and open problems.
\newblock {\em Information Sciences}, 328:26--41, 2016.

\bibitem[CWZ{\etalchar{+}}21]{camuto2021asymmetric}
Alexander Camuto, Xiaoyu Wang, Lingjiong Zhu, Chris Holmes, Mert
  G{\"u}rb{\"u}zbalaban, and Umut {\c{S}}im{\c{s}}ekli.
\newblock Asymmetric heavy tails and implicit bias in gaussian noise
  injections.
\newblock In {\em ICML}, 2021.

\bibitem[DBI18]{deng2018ppfnet}
Haowen Deng, Tolga Birdal, and Slobodan Ilic.
\newblock Ppfnet: Global context aware local features for robust 3d point
  matching.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 195--205, 2018.

\bibitem[DCLT18]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[DZF19]{Dollr2019WhatDI}
P.~Doll{\'a}r, C.~L. Zitnick, and P.~Frossard.
\newblock What does it mean to learn in deep networks ? and , how does one
  detect adversarial attacks ?
\newblock 2019.

\bibitem[EH10]{edelsbrunner2010computational}
Herbert Edelsbrunner and John Harer.
\newblock {\em Computational topology: an introduction}.
\newblock American Mathematical Soc., 2010.

\bibitem[EMH{\etalchar{+}}19]{elsken2019neural}
Thomas Elsken, Jan~Hendrik Metzen, Frank Hutter, et~al.
\newblock Neural architecture search: A survey.
\newblock {\em Journal of Machine Learning Research}, 20(55):1--21, 2019.

\bibitem[Fal04]{falconer2004fractal}
Kenneth Falconer.
\newblock {\em Fractal geometry: mathematical foundations and applications}.
\newblock John Wiley \& Sons, 2004.

\bibitem[FB81]{fischler1981random}
Martin~A Fischler and Robert~C Bolles.
\newblock Random sample consensus: a paradigm for model fitting with
  applications to image analysis and automated cartography.
\newblock {\em Communications of the ACM}, 24(6):381--395, 1981.

\bibitem[FC18]{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem[FdRL17]{facco2017estimating}
Elena Facco, Maria d’Errico, Alex Rodriguez, and Alessandro Laio.
\newblock Estimating the intrinsic dimension of datasets by a minimal
  neighborhood information.
\newblock {\em Scientific reports}, 7(1):1--8, 2017.

\bibitem[FGFAEV21]{fernandez2021determining}
David~P{\'e}rez Fern{\'a}ndez, Asier Guti{\'e}rrez-Fandi{\~n}o, Jordi
  Armengol-Estap{\'e}, and Marta Villegas.
\newblock Determining structural properties of artificial neural networks using
  algebraic topology.
\newblock {\em arXiv preprint arXiv:2101.07752}, 2021.

\bibitem[Ghr10]{ghrist2010configuration}
Robert Ghrist.
\newblock Configuration spaces, braids, and robotics.
\newblock In {\em Braids: Introductory Lectures on Braids, Configurations and
  Their Applications}, pages 263--304. World Scientific, 2010.

\bibitem[GJ16]{gao2016degrees}
Tianxiang Gao and Vladimir Jojic.
\newblock Degrees of freedom in deep neural networks.
\newblock UAI'16, page 232–241. AUAI Press, 2016.

\bibitem[GLW{\etalchar{+}}21]{gojcic2020weakly}
Zan Gojcic, Or~Litany, Andreas Wieser, Leonidas~J. Guibas, and Tolga Birdal.
\newblock Weakly supervised learning of rigid 3d scene flow.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 5692--5703, June 2021.

\bibitem[GP04]{grassberger2004measuring}
Peter Grassberger and Itamar Procaccia.
\newblock Measuring the strangeness of strange attractors.
\newblock In {\em The Theory of Chaotic Attractors}, pages 170--189. Springer,
  2004.

\bibitem[GSZ21]{gurbuzbalaban2021heavy}
Mert Gurbuzbalaban, Umut Simsekli, and Lingjiong Zhu.
\newblock The heavy-tail phenomenon in sgd.
\newblock In {\em International Conference on Machine Learning}, pages
  3964--3975. PMLR, 2021.

\bibitem[Hau18]{hausdorff1918dimension}
Felix Hausdorff.
\newblock Dimension und {\"a}u{\ss}eres ma{\ss}.
\newblock {\em Mathematische Annalen}, 79(1):157--179, 1918.

\bibitem[HJTW21]{hsu2021generalization}
Daniel Hsu, Ziwei Ji, Matus Telgarsky, and Lan Wang.
\newblock Generalization bounds via distillation.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem[HM20]{hodgkinson2020multiplicative}
Liam Hodgkinson and Michael~W. Mahoney.
\newblock Multiplicative noise and heavy tails in stochastic optimization.
\newblock {\em arXiv:2006.06293 [cs, math, stat]}, June 2020.

\bibitem[Hub92]{huber1992robust}
Peter~J Huber.
\newblock Robust estimation of a location parameter.
\newblock In {\em Breakthroughs in statistics}, pages 492--518. Springer, 1992.

\bibitem[Ish93]{isham1993statistical}
Valerie Isham.
\newblock Statistical aspects of chaos: a review.
\newblock {\em Networks and Chaos-Statistical and Probabilistic Aspects}, pages
  124--200, 1993.

\bibitem[JFH15]{janson2015effective}
Lucas Janson, William Fithian, and Trevor~J Hastie.
\newblock Effective degrees of freedom: a flawed metaphor.
\newblock {\em Biometrika}, 102(2):479--485, 2015.

\bibitem[JFY{\etalchar{+}}20]{jiang2020neurips}
Yiding Jiang, Pierre Foret, Scott Yak, Daniel~M Roy, Hossein Mobahi,
  Gintare~Karolina Dziugaite, Samy Bengio, Suriya Gunasekar, Isabelle Guyon,
  and Behnam Neyshabur.
\newblock Neurips 2020 competition: Predicting generalization in deep learning
  (version 1.1).
\newblock Technical report, Technical Report Preprint: December 16, 2020.

\bibitem[Jol86]{jolliffe1986principal}
Ian~T Jolliffe.
\newblock Principal components in regression analysis.
\newblock In {\em Principal component analysis}, pages 129--155. Springer,
  1986.

\bibitem[KB15]{Kingma2015AdamAM}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em CoRR}, abs/1412.6980, 2015.

\bibitem[K{\'e}g02]{kegl2002intrinsic}
Bal{\'a}zs K{\'e}gl.
\newblock Intrinsic dimension estimation using packing numbers.
\newblock In {\em NIPS}, pages 681--688. Citeseer, 2002.

\bibitem[KLS06]{kozma2006minimal}
Gady Kozma, Zvi Lotker, and Gideon Stupp.
\newblock The minimal spanning tree and the upper box dimension.
\newblock {\em Proceedings of the American Mathematical Society},
  134(4):1183--1187, 2006.

\bibitem[Kri09]{Krizhevsky2009LearningML}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[KSH12]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em Advances in neural information processing systems},
  25:1097--1105, 2012.

\bibitem[LB05]{levina2005maximum}
Elizaveta Levina and Peter~J Bickel.
\newblock Maximum likelihood estimation of intrinsic dimension.
\newblock In {\em Advances in neural information processing systems}, pages
  777--784, 2005.

\bibitem[LBBH98]{LeCun1998GradientbasedLA}
Y.~LeCun, L.~Bottou, Yoshua Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock 1998.

\bibitem[LFLY18]{li2018measuring}
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski.
\newblock Measuring the intrinsic dimension of objective landscapes.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem[LZ08]{lin2008riemannian}
Tong Lin and Hongbin Zha.
\newblock Riemannian manifold learning.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  30(5):796--809, 2008.

\bibitem[MM19]{martin2019traditional}
Charles~H Martin and Michael~W Mahoney.
\newblock Traditional and heavy-tailed self regularization in neural network
  models.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2019.

\bibitem[MWH{\etalchar{+}}18]{ma2018dimensionality}
Xingjun Ma, Yisen Wang, Michael~E Houle, Shuo Zhou, Sarah Erfani, Shutao Xia,
  Sudanthi Wijewickrema, and James Bailey.
\newblock Dimensionality-driven learning with noisy labels.
\newblock In {\em International Conference on Machine Learning}, pages
  3355--3364. PMLR, 2018.

\bibitem[NBMS17]{neyshabur2017}
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[PGM{\etalchar{+}}19]{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems 32}, pages 8024--8035. Curran Associates,
  Inc., 2019.

\bibitem[QSMG17]{qi2017pointnet}
Charles~R Qi, Hao Su, Kaichun Mo, and Leonidas~J Guibas.
\newblock Pointnet: Deep learning on point sets for 3d classification and
  segmentation.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 652--660, 2017.

\bibitem[RB21]{reani2021cycle}
Yohai Reani and Omer Bobrowski.
\newblock Cycle registration in persistent homology with applications in
  topological bootstrap.
\newblock {\em arXiv preprint arXiv:2101.00698}, 2021.

\bibitem[RBH{\etalchar{+}}21]{rempe2021humor}
Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and
  Leonidas~J. Guibas.
\newblock Humor: 3d human motion model for robust pose estimation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pages 11488--11499, October 2021.

\bibitem[RTB{\etalchar{+}}19]{Rieck2019NeuralPA}
Bastian~Alexander Rieck, Matteo Togninalli, Christian Bock, Michael Moor, Max
  Horn, Thomas Gumbsch, and K.~Borgwardt.
\newblock Neural persistence: A complexity measure for deep neural networks
  using algebraic topology.
\newblock {\em ArXiv}, abs/1812.09764, 2019.

\bibitem[SAM{\etalchar{+}}20]{suzuki2018spectral}
Taiji Suzuki, Hiroshi Abe, Tomoya Murata, Shingo Horiuchi, Kotaro Ito, Tokuma
  Wachi, So~Hirai, Masatoshi Yukishima, and Tomoaki Nishimura.
\newblock Spectral pruning: Compressing deep neural networks via spectral
  analysis and its generalization error.
\newblock In {\em International Joint Conference on Artificial Intelligence},
  pages 2839--2846, 2020.

\bibitem[SAN20]{Suzuki2020Compression}
Taiji Suzuki, Hiroshi Abe, and Tomoaki Nishimura.
\newblock Compression based bound for non-compressed network: unified
  generalization error analysis of large compressible deep neural network.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem[Sch09]{schroeder2009fractals}
Manfred Schroeder.
\newblock {\em Fractals, chaos, power laws: Minutes from an infinite paradise}.
\newblock Courier Corporation, 2009.

\bibitem[Sch19]{schweinhart2019persistent}
Benjamin Schweinhart.
\newblock Persistent homology and the upper box dimension.
\newblock {\em Discrete \& Computational Geometry}, pages 1--34, 2019.

\bibitem[Sch20]{schweinhart2020fractal}
Benjamin Schweinhart.
\newblock Fractal dimension and the persistent homology of random geometric
  complexes.
\newblock {\em Advances in Mathematics}, 372:107291, 2020.

\bibitem[{\c{S}}GN{\etalchar{+}}19]{csimcsekli2019heavy}
Umut {\c{S}}im{\c{s}}ekli, Mert G{\"u}rb{\"u}zbalaban, Thanh~Huy Nguyen,
  Ga{\"e}l Richard, and Levent Sagun.
\newblock On the heavy-tailed theory of stochastic gradient descent for deep
  neural networks.
\newblock {\em arXiv preprint arXiv:1912.00018}, 2019.

\bibitem[SSDE20]{simsekli2020hausdorff}
Umut Simsekli, Ozan Sener, George Deligiannidis, and Murat~A Erdogdu.
\newblock Hausdorff dimension, heavy tails, and generalization in neural
  networks.
\newblock In {\em Proceedings of the Advances in Neural Information Processing
  Systems (NeurIPS)}, volume~33, 2020.

\bibitem[SSG19]{simsekli2019tail}
Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban.
\newblock A tail-index analysis of stochastic gradient noise in deep neural
  networks.
\newblock In {\em International Conference on Machine Learning}, pages
  5827--5837. PMLR, 2019.

\bibitem[ST94]{samorodnitsky1994stable}
G.~Samorodnitsky and M.~S. Taqqu.
\newblock {\em Stable non-{G}aussian random processes: stochastic models with
  infinite variance}, volume~1.
\newblock CRC press, 1994.

\bibitem[SZTG20]{simsekli2020fractional}
Umut Simsekli, Lingjiong Zhu, Yee~Whye Teh, and Mert Gurbuzbalaban.
\newblock Fractional underdamped langevin dynamics: Retargeting sgd with
  momentum under heavy-tailed gradient noise.
\newblock In {\em International Conference on Machine Learning}, pages
  8970--8980. PMLR, 2020.

\bibitem[TDSL00]{tenenbaum2000global}
Joshua~B Tenenbaum, Vin De~Silva, and John~C Langford.
\newblock A global geometric framework for nonlinear dimensionality reduction.
\newblock {\em science}, 290(5500):2319--2323, 2000.

\bibitem[TH12]{Tieleman2012}
T.~Tieleman and G.~Hinton.
\newblock {Lecture 6.5---RmsProp: Divide the gradient by a running average of
  its recent magnitude}.
\newblock COURSERA: Neural Networks for Machine Learning, 2012.

\bibitem[Vap68]{vapnik1968uniform}
Vladimir Vapnik.
\newblock On the uniform convergence of relative frequencies of events to their
  probabilities.
\newblock In {\em Doklady Akademii Nauk USSR}, volume 181, pages 781--787,
  1968.

\bibitem[ZBH{\etalchar{+}}21]{zhang2021understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock {\em Communications of the ACM}, 64(3):107--115, 2021.

\bibitem[ZBL{\etalchar{+}}20]{zhao2020quaternion}
Yongheng Zhao, Tolga Birdal, Jan~Eric Lenssen, Emanuele Menegatti, Leonidas
  Guibas, and Federico Tombari.
\newblock Quaternion equivariant capsule networks for 3d point clouds.
\newblock In {\em European Conference on Computer Vision}, pages 1--19.
  Springer, 2020.

\bibitem[ZFM{\etalchar{+}}20]{zhou2020towards}
Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Hoi, and Weinan E.
\newblock Towards theoretically understanding why {SGD} generalizes better than
  {ADAM} in deep learning.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  volume~33, 2020.

\bibitem[Zom10]{zomorodian2010fast}
Afra Zomorodian.
\newblock Fast construction of the vietoris-rips complex.
\newblock {\em Computers \& Graphics}, 34(3):263--271, 2010.

\bibitem[ZQH{\etalchar{+}}18]{zhu2018ldmnet}
Wei Zhu, Qiang Qiu, Jiaji Huang, Robert Calderbank, Guillermo Sapiro, and
  Ingrid Daubechies.
\newblock Ldmnet: Low dimensional manifold regularized neural networks.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 2743--2751, 2018.

\end{thebibliography}
