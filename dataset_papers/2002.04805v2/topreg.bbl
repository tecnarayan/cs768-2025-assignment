\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2018)Arora, Ge, Neyshabur, and Zhang]{Arora18a}
Arora, S., Ge, R., Neyshabur, B., and Zhang, Y.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock In \emph{ICML}, 2018.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and Telgarsky]{Bartlett17a}
Bartlett, P., Foster, D., and Telgarsky, M.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \emph{NIPS}, 2017.

\bibitem[Behlarbi et~al.(2017)Behlarbi, Chatelain, Herault, and
  Adam]{Belharbi17a}
Behlarbi, S., Chatelain, C., Herault, R., and Adam, S.
\newblock Neural networks regularization through class-wise invariant
  representation learning.
\newblock \emph{arXiv}, 2017.
\newblock \url{https://arxiv.org/abs/1709.01867}.

\bibitem[Bianchini \& Scarselli(2014)Bianchini and Scarselli]{Bianchini14a}
Bianchini, M. and Scarselli, F.
\newblock On the complexity of neural network classifiers: A comparison between
  shallow and deep architectures.
\newblock \emph{IEEE Trans. Neural Netw. Learn. Syst.}, 25\penalty0
  (8):\penalty0 1533--1565, 2014.

\bibitem[Boissonnat et~al.(2018)Boissonnat, Chazal, and Yvinec]{Boissonnat18a}
Boissonnat, J.-D., Chazal, F., and Yvinec, M.
\newblock \emph{Geometric and Topological Inference}.
\newblock Cambridge Texts in Applied Mathematics. Cambridge University Press,
  2018.

\bibitem[Chen et~al.(2019)Chen, Ni, Bai, and Wang]{Chen19a}
Chen, C., Ni, X., Bai, Q., and Wang, Y.
\newblock A topological regularizer for classifiers via persistent homology.
\newblock In \emph{AISTATS}, 2019.

\bibitem[Choi \& Rhee(2019)Choi and Rhee]{Choi19a}
Choi, D. and Rhee, W.
\newblock Utilizing class information for deep network representation shaping.
\newblock In \emph{AAAI}, 2019.

\bibitem[Cogswell et~al.(2016)Cogswell, Ahmed, Girshick, Zitnick, and
  Batra]{Cogswell16a}
Cogswell, M., Ahmed, F., Girshick, R., Zitnick, L., and Batra, D.
\newblock Reducing overfitting in deep networks by decorrelating
  representations.
\newblock In \emph{ICLR}, 2016.

\bibitem[Cubuk et~al.(2019)Cubuk, Zoph, Man\'{e}, Vasudevan, and Le]{Cubuk19a}
Cubuk, E., Zoph, B., Man\'{e}, D., Vasudevan, V., and Le, Q.
\newblock Autoaugment: Learning augmentation strategies from data.
\newblock In \emph{CVPR}, 2019.

\bibitem[Dao et~al.(2019)Dao, Gu, Rather, Smith, Sa, and Re]{Dao19a}
Dao, T., Gu, A., Rather, A., Smith, V., Sa, C.~D., and Re, C.
\newblock A kernel theory of modern data augmentation.
\newblock In \emph{ICML}, 2019.

\bibitem[Edelsbrunner \& Harer(2010)Edelsbrunner and Harer]{Edelsbrunner2010}
Edelsbrunner, H. and Harer, J.~L.
\newblock \emph{Computational Topology : An Introduction}.
\newblock American Mathematical Society, 2010.

\bibitem[Glorot et~al.(2011)Glorot, Bordes, and Bengio]{Glorot11a}
Glorot, X., Bordes, A., and Bengio, Y.
\newblock Deep sparse rectifier neural networks.
\newblock In \emph{AISTATS}, 2011.

\bibitem[Golowich et~al.(2018)Golowich, Rakhlin, and Shamir]{Golowich18}
Golowich, N., Rakhlin, A., and Shamir, O.
\newblock Size-independent sample complexity of neural networks.
\newblock In \emph{COLT}, 2018.

\bibitem[Guss \& Salakhutdinov(2018)Guss and Salakhutdinov]{Guss18a}
Guss, W. and Salakhutdinov, R.
\newblock On characterizing the capacity of neural networks using algebraic
  topology.
\newblock \emph{arXiv}, 2018.
\newblock \url{https://arxiv.org/abs/1802.04443}.

\bibitem[Hofer et~al.(2019)Hofer, Kwitt, , Dixit, and Niethammer]{Hofer19a}
Hofer, C., Kwitt, R., , Dixit, M., and Niethammer, M.
\newblock Connectivity-optimized representation learning via persistent
  homology.
\newblock In \emph{ICML}, 2019.

\bibitem[Hoffer et~al.(2019)Hoffer, Ben-Nun, Hubara, Giladi, Hoefler, and
  Soudry]{Hoffer19a}
Hoffer, E., Ben-Nun, T., Hubara, I., Giladi, N., Hoefler, T., and Soudry, D.
\newblock Augment your batch: better training with larger batches.
\newblock \emph{arXiv}, 2019.
\newblock \url{https://arxiv.org/abs/1901.09335}.

\bibitem[Hoffman et~al.(2019)Hoffman, Roberts, and Yaida]{Hoffman19a}
Hoffman, J., Roberts, D., and Yaida, S.
\newblock Robust learning with jacobian regularization.
\newblock \emph{arXiv}, 2019.
\newblock \url{https://arxiv.org/abs/1908.02729}.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{Ioffe15a}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{ICML}, 2015.

\bibitem[Joo et~al.(2020)Joo, Kang, and Kim]{Joo20a}
Joo, T., Kang, D., and Kim, B.
\newblock Regularizing activations in neural networks via distribution matching
  with the {W}asserstein metric.
\newblock In \emph{ICLR}, 2020.

\bibitem[Kolchinsky et~al.(2019)Kolchinsky, Tracey, and Kuyk]{Kolchinsky19a}
Kolchinsky, A., Tracey, B., and Kuyk, S.~V.
\newblock Caveats for information bottleneck in deterministic scenarios.
\newblock In \emph{ICLR}, 2019.

\bibitem[Laine \& Aila(2017)Laine and Aila]{Laine17a}
Laine, S. and Aila, T.
\newblock Temporal ensembling for semi-supervised learning.
\newblock In \emph{ICLR}, 2017.

\bibitem[Liao et~al.(2016)Liao, Schwing, Zemel, and Urtasun]{Liao16a}
Liao, R., Schwing, A., Zemel, R., and Urtasun, R.
\newblock Learning deep parsimonious representations.
\newblock In \emph{NIPS}, 2016.

\bibitem[Littwin \& Wolf(2018)Littwin and Wolf]{Littwin18a}
Littwin, E. and Wolf, L.
\newblock Regularizing by the variance of the activationsâ€™ sample-variances.
\newblock In \emph{NIPS}, 2018.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{Loshchilov17a}
Loshchilov, I. and Hutter, F.
\newblock {SGDR}: Stochastic gradient descent with warm restarts.
\newblock In \emph{ICLR}, 2017.

\bibitem[Nagarajan \& Kolter(2019)Nagarajan and Kolter]{Nagarajan19a}
Nagarajan, V. and Kolter, J.
\newblock Uniform convergence may be unable to explain generalization in deep
  learning.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{Neyshabur17a}
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N.
\newblock Exploring generalization in deep learning.
\newblock In \emph{NIPS}, 2017.

\bibitem[Oliver et~al.(2018)Oliver, Odena, Raffel, Cubuk, and
  Goodfellow]{Oliver18a}
Oliver, A., Odena, A., Raffel, C., Cubuk, E., and Goodfellow, I.
\newblock Realistic evaluation of deep semi-supervised learning algorithms.
\newblock In \emph{NIPS}, 2018.

\bibitem[Rieck et~al.(2019)Rieck, Togninalli, Bock, Moor, Horn, Gumbsch, and
  Borgwardt]{Rieck19a}
Rieck, B., Togninalli, M., Bock, C., Moor, M., Horn, M., Gumbsch, T., and
  Borgwardt, K.
\newblock Neural persistence: A complexity measure for deep neural networks
  using algebraic topology.
\newblock In \emph{ICLR}, 2019.

\bibitem[Robins(2000)]{Robins00a}
Robins, V.
\newblock \emph{Computational topology at multiple resolutions: foundations and
  applications to fractals and dynamics}.
\newblock PhD thesis, University of Colorado, 6 2000.

\bibitem[Roux et~al.(2017)Roux, Manzagol, and Bengio]{Roux07a}
Roux, N., Manzagol, P.-A., and Bengio, Y.
\newblock Topmoumoute online natural gradient algorithm.
\newblock In \emph{NIPS}, 2017.

\bibitem[Shwartz-Ziv \& Tishby(2017)Shwartz-Ziv and Tishby]{Shwartz17a}
Shwartz-Ziv, R. and Tishby, N.
\newblock Opening the black box of deep neural networks via information.
\newblock \emph{arXiv}, 2017.
\newblock \url{https://arxiv.org/abs/1703.00810}.

\bibitem[Sokoli\'{c} et~al.(2017)Sokoli\'{c}, Giryes, Sapiro, and
  Rodrigues]{Sokolic17a}
Sokoli\'{c}, J., Giryes, R., Sapiro, G., and Rodrigues, M.
\newblock Robust large margin deep neural networks.
\newblock \emph{IEEE Trans. Signal Process.}, 65\penalty0 (16):\penalty0
  4265--4280, 2017.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{Srivastava14a}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{JMLR}, 15:\penalty0 1929--1958, 2014.

\bibitem[van Laarhoven(2017)]{Laarhoven17a}
van Laarhoven, T.
\newblock $l_2$ regularization versus batch and weight normalization.
\newblock \emph{arXiv}, 2017.
\newblock \url{https://arxiv.org/abs/1706.05350}.

\bibitem[Verma et~al.(2019{\natexlab{a}})Verma, Lamb, Beckham, Najafi,
  Mitliagkas, Lopez-Paz, and Y.Bengio]{Verma19b}
Verma, V., Lamb, A., Beckham, C., Najafi, A., Mitliagkas, I., Lopez-Paz, D.,
  and Y.Bengio.
\newblock Manifold mixup: Better representations by interpolating hidden
  states.
\newblock In \emph{ICML}, 2019{\natexlab{a}}.

\bibitem[Verma et~al.(2019{\natexlab{b}})Verma, Lamb, Kannala, Bengio, and
  Lopez-Paz]{Verma19a}
Verma, V., Lamb, A., Kannala, J., Bengio, Y., and Lopez-Paz, D.
\newblock Interpolation consistency training for semi-supervised learning.
\newblock In \emph{IJCAI}, 2019{\natexlab{b}}.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{CZhang2017a}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{ICLR}, 2017.

\bibitem[Zhang et~al.(2019)Zhang, Wang, Xu, and Grosse]{Zhang19a}
Zhang, G., Wang, C., Xu, B., and Grosse, R.
\newblock Three mechanisms of weight decay regularization.
\newblock In \emph{ICLR}, 2019.

\end{thebibliography}
