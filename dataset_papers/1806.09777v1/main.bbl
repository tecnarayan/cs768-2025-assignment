\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Albert and Muckenhoupt(1957)]{albert1957matrices}
Abraham~Adrian Albert and Benjamin Muckenhoupt.
\newblock On matrices of trace zero.
\newblock \emph{The Michigan Mathematical Journal}, 4\penalty0 (1):\penalty0
  1--3, 1957.

\bibitem[Arora et~al.(2018)Arora, Basu, Mianjy, and
  Mukherjee]{arora2018understanding}
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee.
\newblock Understanding deep neural networks with rectified linear units.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.
\newblock URL \url{https://arxiv.org/abs/1611.01491}.

\bibitem[Baldi and Hornik(1989)]{baldi1989neural}
Pierre Baldi and Kurt Hornik.
\newblock Neural networks and principal component analysis: Learning from
  examples without local minima.
\newblock \emph{Neural networks}, 2\penalty0 (1):\penalty0 53--58, 1989.

\bibitem[Baldi and Sadowski(2013)]{baldi2013understanding}
Pierre Baldi and Peter~J Sadowski.
\newblock Understanding dropout.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 2814--2822, 2013.

\bibitem[Bhojanapalli et~al.(2016)Bhojanapalli, Neyshabur, and
  Srebro]{bhojanapalli2016global}
Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
\newblock Global optimality of local search for low rank matrix recovery.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 3873--3881, 2016.

\bibitem[Caruana et~al.(2001)Caruana, Lawrence, and
  Giles]{caruana2001overfitting}
Rich Caruana, Steve Lawrence, and C~Lee Giles.
\newblock Overfitting in neural nets: Backpropagation, conjugate gradient, and
  early stopping.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 402--408, 2001.

\bibitem[Cavazza et~al.(2018)Cavazza, Haeffele, Lane, Morerio, Murino, and
  Vidal]{Cavazza2017analysis}
Jacopo Cavazza, Benjamin~D. Haeffele, Connor Lane, Pietro Morerio, Vittorio
  Murino, and Rene Vidal.
\newblock Dropout as a low-rank regularizer for matrix factorization.
\newblock \emph{Int. Conf. on Artificial Intelligence and Statistics
  (AISTATS)}, 2018.

\bibitem[Gal and Ghahramani(2016)]{gal2016dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{Int. Conf. Machine Learning (ICML)}, 2016.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{ge2015escaping}
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.
\newblock Escaping from saddle pointsâ€”online stochastic gradient for tensor
  decomposition.
\newblock In \emph{Conf. Learning Theory (COLT)}, 2015.

\bibitem[Ge et~al.(2016)Ge, Lee, and Ma]{ge2016matrix}
Rong Ge, Jason~D Lee, and Tengyu Ma.
\newblock Matrix completion has no spurious local minimum.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2016.

\bibitem[Ge et~al.(2017)Ge, Jin, and Zheng]{ge2017no}
Rong Ge, Chi Jin, and Yi~Zheng.
\newblock No spurious local minima in nonconvex low rank problems: A unified
  geometric analysis.
\newblock \emph{arXiv preprint arXiv:1704.00708}, 2017.

\bibitem[Glorot et~al.(2011)Glorot, Bordes, and Bengio]{glorot2011deep}
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
\newblock Deep sparse rectifier neural networks.
\newblock In \emph{Proc. Intl. Conf. on Artificial Intelligence and Statistics
  (AISTATS)}, 2011.

\bibitem[Haeffele and Vidal(2017)]{haeffele2017structured}
Benjamin~D Haeffele and Rene Vidal.
\newblock Structured low-rank matrix factorization: Global optimality,
  algorithms, and applications.
\newblock \emph{arXiv preprint arXiv:1708.07850}, 2017.

\bibitem[He et~al.(2016)He, Liu, Liu, Wang, Yin, and Huang]{he2016dropout}
Zhicheng He, Jie Liu, Caihua Liu, Yuan Wang, Airu Yin, and Yalou Huang.
\newblock Dropout non-negative matrix factorization for independent feature
  learning.
\newblock In \emph{Int. Conf. on Computer Proc. of Oriental Languages}.
  Springer, 2016.

\bibitem[Helmbold and Long(2015)]{helmbold2015inductive}
David~P Helmbold and Philip~M Long.
\newblock On the inductive bias of dropout.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 16:\penalty0
  3403--3454, 2015.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  448--456, 2015.

\bibitem[Jin et~al.(2017)Jin, Ge, Netrapalli, Kakade, and
  Jordan]{jin2017escape}
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham~M Kakade, and Michael~I Jordan.
\newblock How to escape saddle points efficiently.
\newblock \emph{arXiv preprint arXiv:1703.00887}, 2017.

\bibitem[Kahan(1999)]{kahan1999only}
William Kahan.
\newblock Only commutators have trace zero, 1999.

\bibitem[Kawaguchi(2016)]{kawaguchi2016deep}
Kenji Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock In \emph{Adv in Neural Information Proc. Systems (NIPS)}, 2016.

\bibitem[Lee et~al.(2016)Lee, Simchowitz, Jordan, and Recht]{lee2016gradient}
Jason~D Lee, Max Simchowitz, Michael~I Jordan, and Benjamin Recht.
\newblock Gradient descent converges to minimizers.
\newblock \emph{arXiv preprint arXiv:1602.04915}, 2016.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and
  Srebro]{neyshabur2015norm}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{Conf. on Learning Theory (COLT)}, pages 1376--1401, 2015.

\bibitem[Nowlan and Hinton(1992)]{nowlan1992simplifying}
Steven~J Nowlan and Geoffrey~E Hinton.
\newblock Simplifying neural networks by soft weight-sharing.
\newblock \emph{Neural computation}, 4\penalty0 (4):\penalty0 473--493, 1992.

\bibitem[Srebro et~al.(2005)Srebro, Rennie, and Jaakkola]{srebro2005maximum}
Nathan Srebro, Jason Rennie, and Tommi~S Jaakkola.
\newblock Maximum-margin matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2005.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Nitish Srivastava, Geoffrey~E Hinton, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 15\penalty0 (1),
  2014.

\bibitem[Sun et~al.(2016)Sun, Qu, and Wright]{sun2016geometric}
Ju~Sun, Qing Qu, and John Wright.
\newblock A geometric analysis of phase retrieval.
\newblock In \emph{IEEE International Symposium on Information Theory (ISIT)},
  pages 2379--2383, 2016.

\bibitem[Wager et~al.(2013)Wager, Wang, and Liang]{wager2013dropout}
Stefan Wager, Sida Wang, and Percy~S Liang.
\newblock Dropout training as adaptive regularization.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2013.

\bibitem[Zhai and Zhang(2015)]{zhai2015dropout}
Shuangfei Zhai and Zhongfei Zhang.
\newblock Dropout training of matrix factorization and autoencoder for link
  prediction in sparse graphs.
\newblock In \emph{Proc. of SIAM International Conference on Data Mining
  (ICDM)}, pages 451--459, 2015.

\end{thebibliography}
