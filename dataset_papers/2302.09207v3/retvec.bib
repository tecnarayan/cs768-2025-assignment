@online{jayanthiNeuSpellNeuralSpelling2020,
  title = {{{NeuSpell}}: {{A Neural Spelling Correction Toolkit}}},
  shorttitle = {{{NeuSpell}}},
  author = {Jayanthi, Sai Muralidhar and Pruthi, Danish and Neubig, Graham},
  date = {2020-10-21},
  eprint = {2010.11085},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.11085},
  urldate = {2023-05-17},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: Accepted at EMNLP 2020 (system demonstrations)},
}

@misc{bansalDebiasingMultilingualWord2021a,
  title = {Debiasing {{Multilingual Word Embeddings}}: {{A Case Study}} of {{Three Indian Languages}}},
  shorttitle = {Debiasing {{Multilingual Word Embeddings}}},
  author = {Bansal, Srijan and Garimella, Vishal and Suhane, Ayush and Mukherjee, Animesh},
  year = {2021},
  month = jul,
  number = {arXiv:2107.10181},
  eprint = {2107.10181},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {In this paper, we advance the current state-of-the-art method for debiasing monolingual word embeddings so as to generalize well in a multilingual setting. We consider different methods to quantify bias and different debiasing approaches for monolingual as well as multilingual settings. We demonstrate the significance of our bias-mitigation approach on downstream NLP applications. Our proposed methods establish the state-of-the-art performance for debiasing multilingual embeddings for three Indian languages - Hindi, Bengali, and Telugu in addition to English. We believe that our work will open up new opportunities in building unbiased downstream NLP applications that are inherently dependent on the quality of the word embeddings used.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{BetterLanguageModels2019,
  title = {Better {{Language Models}} and {{Their Implications}}},
  year = {2019},
  month = feb,
  journal = {OpenAI},
  abstract = {We've trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization.},
  howpublished = {https://openai.com/blog/better-language-models/},
  langid = {english},
  file = {/Users/elieb/Zotero/storage/SIZK5YKV/better-language-models.html}
}

@misc{bojanowskiEnrichingWordVectors2017,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  year = {2017},
  month = jun,
  number = {arXiv:1607.04606},
  eprint = {1607.04606},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1607.04606},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character \$n\$-grams. A vector representation is associated to each character \$n\$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/elieb/Zotero/storage/BQ2H8FJP/Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf;/Users/elieb/Zotero/storage/VRHN2AX3/1607.html}
}

@misc{chenExploringSimpleSiamese2020,
  title = {Exploring {{Simple Siamese Representation Learning}}},
  author = {Chen, Xinlei and He, Kaiming},
  year = {2020},
  month = nov,
  number = {arXiv:2011.10566},
  eprint = {2011.10566},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.10566},
  abstract = {Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our "SimSiam" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code will be made available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/elieb/Zotero/storage/3V9CXACU/Chen and He - 2020 - Exploring Simple Siamese Representation Learning.pdf;/Users/elieb/Zotero/storage/WTUMMRWP/2011.html}
}

@misc{chowdheryPaLMScalingLanguage2022,
  title = {{{PaLM}}: {{Scaling Language Modeling}} with {{Pathways}}},
  shorttitle = {{{PaLM}}},
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra et al.},
  year = {2022},
  month = oct,
  number = {arXiv:2204.02311},
  eprint = {2204.02311},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/elieb/Zotero/storage/UUP7ZHMP/Chowdhery et al. - 2022 - PaLM Scaling Language Modeling with Pathways.pdf;/Users/elieb/Zotero/storage/BJJPJ7M8/2204.html}
}

@misc{CraftingPapersMachine,
  title = {Crafting {{Papers}} on {{Machine Learning}}},
  howpublished = {https://icml.cc/Conferences/2002/craft.html},
  file = {/Users/elieb/Zotero/storage/MWMNE6QH/craft.html}
}

@misc{DeepLearningPython,
  title = {Deep {{Learning}} with {{Python}}},
  journal = {Manning Publications},
  abstract = {Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher Fran\&\#231;ois Chollet, this book builds your understanding through intuitive explanations and practical examples.{$<$}/p{$>$}},
  howpublished = {https://www.manning.com/books/deep-learning-with-python},
  langid = {english},
  file = {/Users/elieb/Zotero/storage/5RGAP7F8/deep-learning-with-python.html}
}

@misc{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.04805},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/elieb/Zotero/storage/GZB9N25I/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/Users/elieb/Zotero/storage/XMAVAGQX/1810.html}
}

@misc{fitzgeraldMASSIVE1MExampleMultilingual2022,
  title = {{{MASSIVE}}: {{A 1M-Example Multilingual Natural Language Understanding Dataset}} with 51 {{Typologically-Diverse Languages}}},
  shorttitle = {{{MASSIVE}}},
  author = {FitzGerald, Jack and Hench, Christopher and Peris, Charith and Mackie, Scott and Rottmann, Kay and Sanchez, Ana and Nash, Aaron and Urbach, Liam and Kakarala, Vishesh and Singh, Richa and Ranganath, Swetha and Crist, Laurie and Britan, Misha and Leeuwis, Wouter and Tur, Gokhan and Natarajan, Prem},
  year = {2022},
  month = jun,
  number = {arXiv:2204.08582},
  eprint = {2204.08582},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We present the MASSIVE dataset--Multilingual Amazon Slu resource package (SLURP) for Slot-filling, Intent classification, and Virtual assistant Evaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant utterances spanning 51 languages, 18 domains, 60 intents, and 55 slots. MASSIVE was created by tasking professional translators to localize the English-only SLURP dataset into 50 typologically diverse languages from 29 genera. We also present modeling results on XLM-R and mT5, including exact match accuracy, intent classification accuracy, and slot-filling F1 score. We have released our dataset, modeling code, and models publicly.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/elieb/Zotero/storage/PISPYBHT/FitzGerald et al. - 2022 - MASSIVE A 1M-Example Multilingual Natural Languag.pdf;/Users/elieb/Zotero/storage/YXS2Q6RA/2204.html}
}

@misc{gaoBlackboxGenerationAdversarial2018,
  title = {Black-Box {{Generation}} of {{Adversarial Text Sequences}} to {{Evade Deep Learning Classifiers}}},
  author = {Gao, Ji and Lanchantin, Jack and Soffa, Mary Lou and Qi, Yanjun},
  year = {2018},
  month = may,
  number = {arXiv:1801.04354},
  eprint = {1801.04354},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Although various techniques have been proposed to generate adversarial samples for white-box attacks on text, little attention has been paid to black-box attacks, which are more realistic scenarios. In this paper, we present a novel algorithm, DeepWordBug, to effectively generate small text perturbations in a black-box setting that forces a deep-learning classifier to misclassify a text input. We employ novel scoring strategies to identify the critical tokens that, if modified, cause the classifier to make an incorrect prediction. Simple character-level transformations are applied to the highest-ranked tokens in order to minimize the edit distance of the perturbation, yet change the original classification. We evaluated DeepWordBug on eight real-world text datasets, including text classification, sentiment analysis, and spam detection. We compare the result of DeepWordBug with two baselines: Random (Black-box) and Gradient (White-box). Our experimental results indicate that DeepWordBug reduces the prediction accuracy of current state-of-the-art deep-learning models, including a decrease of 68\textbackslash\% on average for a Word-LSTM model and 48\textbackslash\% on average for a Char-CNN model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/elieb/Zotero/storage/6RDJHRJY/Gao et al. - 2018 - Black-box Generation of Adversarial Text Sequences.pdf;/Users/elieb/Zotero/storage/DU7C6VBZ/1801.html}
}

@misc{graveLearningWordVectors2018,
  title = {Learning {{Word Vectors}} for 157 {{Languages}}},
  author = {Grave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas},
  year = {2018},
  month = mar,
  number = {arXiv:1802.06893},
  eprint = {1802.06893},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.06893},
  abstract = {Distributed word representations, or word vectors, have recently been applied to many tasks in natural language processing, leading to state-of-the-art performance. A key ingredient to the successful application of these representations is to train them on very large corpora, and use these pre-trained models in downstream tasks. In this paper, we describe how we trained such high quality word representations for 157 languages. We used two sources of data to train these models: the free online encyclopedia Wikipedia and data from the common crawl project. We also introduce three new word analogy datasets to evaluate these word vectors, for French, Hindi and Polish. Finally, we evaluate our pre-trained word vectors on 10 languages for which evaluation datasets exists, showing very strong performance compared to previous models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/elieb/Zotero/storage/84PI23WP/Grave et al. - 2018 - Learning Word Vectors for 157 Languages.pdf;/Users/elieb/Zotero/storage/VX6DIUPK/1802.html}
}

@inproceedings{hagenLargeScaleQuerySpelling2017,
  title = {A {{Large-Scale Query Spelling Correction Corpus}}},
  booktitle = {Proceedings of the 40th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Hagen, Matthias and Potthast, Martin and Gohsen, Marcel and Rathgeber, Anja and Stein, Benno},
  year = {2017},
  month = aug,
  series = {{{SIGIR}} '17},
  pages = {1261--1264},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3077136.3080749},
  abstract = {We present a new large-scale collection of 54,772 queries with manually annotated spelling corrections. For 9,170 of the queries (16.74\%), spelling variants that are different to the original query are proposed. With its size, our new corpus is an order of magnitude larger than other publicly available query spelling corpora. In addition to releasing the new large-scale corpus, we also provide an implementation of the winner of the Microsoft Speller Challenge from\textasciitilde 2011 and compare it on the different publicly available corpora to spelling corrections mined from Google and Bing. This way, we also shed some light on the spelling correction performance of state-of-the-art commercial search systems.},
  isbn = {978-1-4503-5022-8},
  keywords = {query log mining,query spelling accuracy,query spelling corpus,query spelling correction}
}

@misc{hagiwaraGitHubTypoCorpus2019,
  title = {{{GitHub Typo Corpus}}: {{A Large-Scale Multilingual Dataset}} of {{Misspellings}} and {{Grammatical Errors}}},
  shorttitle = {{{GitHub Typo Corpus}}},
  author = {Hagiwara, Masato and Mita, Masato},
  year = {2019},
  month = nov,
  number = {arXiv:1911.12893},
  eprint = {1911.12893},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1911.12893},
  abstract = {The lack of large-scale datasets has been a major hindrance to the development of NLP tasks such as spelling correction and grammatical error correction (GEC). As a complementary new resource for these tasks, we present the GitHub Typo Corpus, a large-scale, multilingual dataset of misspellings and grammatical errors along with their corrections harvested from GitHub, a large and popular platform for hosting and sharing git repositories. The dataset, which we have made publicly available, contains more than 350k edits and 65M characters in more than 15 languages, making it the largest dataset of misspellings to date. We also describe our process for filtering true typo edits based on learned classifiers on a small annotated subset, and demonstrate that typo edits can be identified with F1 \textasciitilde{} 0.9 using a very simple classifier with only three features. The detailed analyses of the dataset show that existing spelling correctors merely achieve an F-measure of approx. 0.5, suggesting that the dataset serves as a new, rich source of spelling errors that complement existing datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/elieb/Zotero/storage/95FIE5X7/Hagiwara and Mita - 2019 - GitHub Typo Corpus A Large-Scale Multilingual Dat.pdf;/Users/elieb/Zotero/storage/EZ8EP4NR/1911.html}
}

@misc{heTripletCenterLossMultiView2018,
  title = {Triplet-{{Center Loss}} for {{Multi-View 3D Object Retrieval}}},
  author = {He, Xinwei and Zhou, Yang and Zhou, Zhichao and Bai, Song and Bai, Xiang},
  year = {2018},
  month = mar,
  number = {arXiv:1803.06189},
  eprint = {1803.06189},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.06189},
  abstract = {Most existing 3D object recognition algorithms focus on leveraging the strong discriminative power of deep learning models with softmax loss for the classification of 3D data, while learning discriminative features with deep metric learning for 3D object retrieval is more or less neglected. In the paper, we study variants of deep metric learning losses for 3D object retrieval, which did not receive enough attention from this area. First , two kinds of representative losses, triplet loss and center loss, are introduced which could learn more discriminative features than traditional classification loss. Then, we propose a novel loss named triplet-center loss, which can further enhance the discriminative power of the features. The proposed triplet-center loss learns a center for each class and requires that the distances between samples and centers from the same class are closer than those from different classes. Extensive experimental results on two popular 3D object retrieval benchmarks and two widely-adopted sketch-based 3D shape retrieval benchmarks consistently demonstrate the effectiveness of our proposed loss, and significant improvements have been achieved compared with the state-of-the-arts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/elieb/Zotero/storage/8NSKED3S/He et al. - 2018 - Triplet-Center Loss for Multi-View 3D Object Retri.pdf;/Users/elieb/Zotero/storage/9UEJ2L3Z/1803.html}
}

@misc{huaTransformerQualityLinear2022,
  title = {Transformer {{Quality}} in {{Linear Time}}},
  author = {Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and Le, Quoc V.},
  year = {2022},
  month = jun,
  number = {arXiv:2202.10447},
  eprint = {2202.10447},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We revisit the design choices in Transformers, and propose methods to address their weaknesses in handling long sequences. First, we propose a simple layer named gated attention unit, which allows the use of a weaker single-head attention with minimal quality loss. We then propose a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality. The resulting model, named FLASH, matches the perplexity of improved Transformers over both short (512) and long (8K) context lengths, achieving training speedups of up to 4.9\$\textbackslash times\$ on Wiki-40B and 12.1\$\textbackslash times\$ on PG-19 for auto-regressive language modeling, and 4.8\$\textbackslash times\$ on C4 for masked language modeling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/elieb/Zotero/storage/8GELYY6E/Hua et al. - 2022 - Transformer Quality in Linear Time.pdf;/Users/elieb/Zotero/storage/ETBU5LT3/2202.html}
}

@misc{HuggingFaceAI,
  title = {Hugging {{Face}} \textendash{} {{The AI}} Community Building the Future.},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/},
  file = {/Users/elieb/Zotero/storage/6VGLU8JL/huggingface.co.html}
}

@inproceedings{johnsonDeepPyramidConvolutional2017,
  title = {Deep {{Pyramid Convolutional Neural Networks}} for {{Text Categorization}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for           {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Johnson, Rie and Zhang, Tong},
  year = {2017},
  pages = {562--570},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/P17-1052},
  abstract = {This paper proposes a low-complexity word-level deep convolutional neural network (CNN) architecture for text categorization that can efficiently represent longrange associations in text. In the literature, several deep and complex neural networks have been proposed for this task, assuming availability of relatively large amounts of training data. However, the associated computational complexity increases as the networks go deeper, which poses serious challenges in practical applications. Moreover, it was shown recently that shallow word-level CNNs are more accurate and much faster than the state-of-the-art very deep nets such as character-level CNNs even in the setting of large training data. Motivated by these findings, we carefully studied deepening of word-level CNNs to capture global representations of text, and found a simple network architecture with which the best accuracy can be obtained by increasing the network depth without increasing computational cost by much. We call it deep pyramid CNN. The proposed model with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic categorization.},
  langid = {english},
  file = {/Users/elieb/Zotero/storage/U4THWCNV/Johnson and Zhang - 2017 - Deep Pyramid Convolutional Neural Networks for Tex.pdf}
}

@inproceedings{joulinBagTricksEfficient2017,
  title = {Bag of {{Tricks}} for {{Efficient Text Classification}}},
  booktitle = {Proceedings of the 15th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Volume}} 2, {{Short Papers}}},
  author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  year = {2017},
  month = apr,
  pages = {427--431},
  publisher = {{Association for Computational Linguistics}},
  address = {{Valencia, Spain}},
  abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.},
  file = {/Users/elieb/Zotero/storage/8ATET2M9/Joulin et al. - 2017 - Bag of Tricks for Efficient Text Classification.pdf}
}

@misc{kudoSentencePieceSimpleLanguage2018,
  title = {{{SentencePiece}}: {{A}} Simple and Language Independent Subword Tokenizer and Detokenizer for {{Neural Text Processing}}},
  shorttitle = {{{SentencePiece}}},
  author = {Kudo, Taku and Richardson, John},
  year = {2018},
  month = aug,
  number = {arXiv:1808.06226},
  eprint = {1808.06226},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1808.06226},
  abstract = {This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/elieb/Zotero/storage/G7LPPFMR/Kudo and Richardson - 2018 - SentencePiece A simple and language independent s.pdf;/Users/elieb/Zotero/storage/CEY6AKUH/1808.html}
}

@inproceedings{liTextBuggerGeneratingAdversarial2019,
  title = {{{TextBugger}}: {{Generating Adversarial Text Against Real-world Applications}}},
  shorttitle = {{{TextBugger}}},
  booktitle = {Proceedings 2019 {{Network}} and {{Distributed System Security Symposium}}},
  author = {Li, Jinfeng and Ji, Shouling and Du, Tianyu and Li, Bo and Wang, Ting},
  year = {2019},
  eprint = {1812.05271},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.14722/ndss.2019.23138},
  abstract = {Deep Learning-based Text Understanding (DLTU) is the backbone technique behind various applications, including question answering, machine translation, and text classification. Despite its tremendous popularity, the security vulnerabilities of DLTU are still largely unknown, which is highly concerning given its increasing use in security-sensitive applications such as sentiment analysis and toxic content detection. In this paper, we show that DLTU is inherently vulnerable to adversarial text attacks, in which maliciously crafted texts trigger target DLTU systems and services to misbehave. Specifically, we present TextBugger, a general attack framework for generating adversarial texts. In contrast to prior works, TextBugger differs in significant ways: (i) effective -- it outperforms state-of-the-art attacks in terms of attack success rate; (ii) evasive -- it preserves the utility of benign text, with 94.9\textbackslash\% of the adversarial text correctly recognized by human readers; and (iii) efficient -- it generates adversarial text with computational complexity sub-linear to the text length. We empirically evaluate TextBugger on a set of real-world DLTU systems and services used for sentiment analysis and toxic content detection, demonstrating its effectiveness, evasiveness, and efficiency. For instance, TextBugger achieves 100\textbackslash\% success rate on the IMDB dataset based on Amazon AWS Comprehend within 4.61 seconds and preserves 97\textbackslash\% semantic similarity. We further discuss possible defense mechanisms to mitigate such attack and the adversary's potential countermeasures, which leads to promising directions for further research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/elieb/Zotero/storage/MI7SXTCM/Li et al. - 2019 - TextBugger Generating Adversarial Text Against Re.pdf;/Users/elieb/Zotero/storage/CZ4HZ64G/1812.html}
}

@inproceedings{mcauleyHiddenFactorsHidden2013,
  title = {Hidden Factors and Hidden Topics: Understanding Rating Dimensions with Review Text},
  shorttitle = {Hidden Factors and Hidden Topics},
  booktitle = {Proceedings of the 7th {{ACM}} Conference on {{Recommender}} Systems},
  author = {McAuley, Julian and Leskovec, Jure},
  year = {2013},
  month = oct,
  pages = {165--172},
  publisher = {{ACM}},
  address = {{Hong Kong China}},
  doi = {10.1145/2507157.2507163},
  abstract = {In order to recommend products to users we must ultimately predict how a user will respond to a new product. To do so we must uncover the implicit tastes of each user as well as the properties of each product. For example, in order to predict whether a user will enjoy Harry Potter, it helps to identify that the book is about wizards, as well as the user's level of interest in wizardry. User feedback is required to discover these latent product and user dimensions. Such feedback often comes in the form of a numeric rating accompanied by review text. However, traditional methods often discard review text, which makes user and product latent dimensions difficult to interpret, since they ignore the very text that justifies a user's rating. In this paper, we aim to combine latent rating dimensions (such as those of latent-factor recommender systems) with latent review topics (such as those learned by topic models like LDA). Our approach has several advantages. Firstly, we obtain highly interpretable textual labels for latent rating dimensions, which helps us to `justify' ratings with text. Secondly, our approach more accurately predicts product ratings by harnessing the information present in review text; this is especially true for new products and users, who may have too few ratings to model their latent factors, yet may still provide substantial information from the text of even a single review. Thirdly, our discovered topics can be used to facilitate other tasks such as automated genre discovery, and to identify useful and representative reviews.},
  isbn = {978-1-4503-2409-0},
  langid = {english},
  file = {/Users/elieb/Zotero/storage/9M2B733X/McAuley and Leskovec - 2013 - Hidden factors and hidden topics understanding ra.pdf}
}

@misc{mikolovEfficientEstimationWord2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  number = {arXiv:1301.3781},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1301.3781},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/elieb/Zotero/storage/5432VDZ3/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf;/Users/elieb/Zotero/storage/XTBEJP8B/1301.html}
}

@misc{morrisTextAttackFrameworkAdversarial2020,
  title = {{{TextAttack}}: {{A Framework}} for {{Adversarial Attacks}}, {{Data Augmentation}}, and {{Adversarial Training}} in {{NLP}}},
  shorttitle = {{{TextAttack}}},
  author = {Morris, John X. and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
  year = {2020},
  month = oct,
  number = {arXiv:2005.05909},
  eprint = {2005.05909},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack's modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness. TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/elieb/Zotero/storage/ZLTBQPN4/Morris et al. - 2020 - TextAttack A Framework for Adversarial Attacks, D.pdf;/Users/elieb/Zotero/storage/8N44LMUQ/2005.html}
}

@inproceedings{penningtonGloVeGlobalVectors2014,
  title = {{{GloVe}}: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {{{GloVe}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  month = oct,
  pages = {1532--1543},
  publisher = {{Association for Computational Linguistics}},
  address = {{Doha, Qatar}},
  doi = {10.3115/v1/D14-1162},
  file = {/Users/elieb/Zotero/storage/GEMZGTVZ/Pennington et al. - 2014 - GloVe Global Vectors for Word Representation.pdf}
}

@misc{pruthiCombatingAdversarialMisspellings2019,
  title = {Combating {{Adversarial Misspellings}} with {{Robust Word Recognition}}},
  author = {Pruthi, Danish and Dhingra, Bhuwan and Lipton, Zachary C.},
  year = {2019},
  month = aug,
  number = {arXiv:1905.11268},
  eprint = {1905.11268},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.11268},
  abstract = {To combat adversarial spelling mistakes, we propose placing a word recognition model in front of the downstream classifier. Our word recognition models build upon the RNN semi-character architecture, introducing several new backoff strategies for handling rare and unseen words. Trained to recognize words corrupted by random adds, drops, swaps, and keyboard mistakes, our method achieves 32\% relative (and 3.3\% absolute) error reduction over the vanilla semi-character model. Notably, our pipeline confers robustness on the downstream classifier, outperforming both adversarial training and off-the-shelf spell checkers. Against a BERT model fine-tuned for sentiment analysis, a single adversarially-chosen character attack lowers accuracy from 90.3\% to 45.8\%. Our defense restores accuracy to 75\%. Surprisingly, better word recognition does not always entail greater robustness. Our analysis reveals that robustness also depends upon a quantity that we denote the sensitivity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/elieb/Zotero/storage/L37NHAFM/Pruthi et al. - 2019 - Combating Adversarial Misspellings with Robust Wor.pdf;/Users/elieb/Zotero/storage/JEZ7T8Y4/1905.html}
}

@misc{qiStanzaPythonNatural2020a,
  title = {Stanza: {{A Python Natural Language Processing Toolkit}} for {{Many Human Languages}}},
  shorttitle = {Stanza},
  author = {Qi, Peng and Zhang, Yuhao and Zhang, Yuhui and Bolton, Jason and Manning, Christopher D.},
  year = {2020},
  month = apr,
  number = {arXiv:2003.07082},
  eprint = {2003.07082},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/elieb/Zotero/storage/RUBS6YJJ/Qi et al. - 2020 - Stanza A Python Natural Language Processing Toolk.pdf}
}

@misc{raffelExploringLimitsTransfer2020,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text-to-Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2020},
  month = jul,
  number = {arXiv:1910.10683},
  eprint = {1910.10683},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.10683},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/elieb/Zotero/storage/QQRKWGYR/Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a U.pdf;/Users/elieb/Zotero/storage/TA9LA89L/1910.html}
}

@inproceedings{schroffFaceNetUnifiedEmbedding2015,
  title = {{{FaceNet}}: {{A Unified Embedding}} for {{Face Recognition}} and {{Clustering}}},
  shorttitle = {{{FaceNet}}},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  year = {2015},
  month = jun,
  eprint = {1503.03832},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {815--823},
  doi = {10.1109/CVPR.2015.7298682},
  abstract = {Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63\%. On YouTube Faces DB it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result by 30\% on both datasets. We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{schroffFaceNetUnifiedEmbedding2015a,
  title = {{{FaceNet}}: {{A Unified Embedding}} for {{Face Recognition}} and {{Clustering}}},
  shorttitle = {{{FaceNet}}},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  year = {2015},
  month = jun,
  eprint = {1503.03832},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {815--823},
  doi = {10.1109/CVPR.2015.7298682},
  abstract = {Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63\%. On YouTube Faces DB it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result by 30\% on both datasets. We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{sennrichNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} of {{Rare Words}} with {{Subword Units}}},
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  year = {2016},
  month = jun,
  number = {arXiv:1508.07909},
  eprint = {1508.07909},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1508.07909},
  abstract = {Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/elieb/Zotero/storage/SXPHN6YT/Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf;/Users/elieb/Zotero/storage/KW3F33UC/1508.html}
}

@misc{sunCircleLossUnified2020,
  title = {Circle {{Loss}}: {{A Unified Perspective}} of {{Pair Similarity Optimization}}},
  shorttitle = {Circle {{Loss}}},
  author = {Sun, Yifan and Cheng, Changmao and Zhang, Yuhan and Zhang, Chi and Zheng, Liang and Wang, Zhongdao and Wei, Yichen},
  year = {2020},
  month = jun,
  number = {arXiv:2002.10857},
  eprint = {2002.10857},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.10857},
  abstract = {This paper provides a pair similarity optimization viewpoint on deep feature learning, aiming to maximize the within-class similarity \$s\_p\$ and minimize the between-class similarity \$s\_n\$. We find a majority of loss functions, including the triplet loss and the softmax plus cross-entropy loss, embed \$s\_n\$ and \$s\_p\$ into similarity pairs and seek to reduce \$(s\_n-s\_p)\$. Such an optimization manner is inflexible, because the penalty strength on every single similarity score is restricted to be equal. Our intuition is that if a similarity score deviates far from the optimum, it should be emphasized. To this end, we simply re-weight each similarity to highlight the less-optimized similarity scores. It results in a Circle loss, which is named due to its circular decision boundary. The Circle loss has a unified formula for two elemental deep feature learning approaches, i.e. learning with class-level labels and pair-wise labels. Analytically, we show that the Circle loss offers a more flexible optimization approach towards a more definite convergence target, compared with the loss functions optimizing \$(s\_n-s\_p)\$. Experimentally, we demonstrate the superiority of the Circle loss on a variety of deep feature learning tasks. On face recognition, person re-identification, as well as several fine-grained image retrieval datasets, the achieved performance is on par with the state of the art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/elieb/Zotero/storage/JWZXPENQ/Sun et al. - 2020 - Circle Loss A Unified Perspective of Pair Similar.pdf;/Users/elieb/Zotero/storage/X6XGHVZH/2002.html}
}

@misc{sunCircleLossUnified2020a,
  title = {Circle {{Loss}}: {{A Unified Perspective}} of {{Pair Similarity Optimization}}},
  shorttitle = {Circle {{Loss}}},
  author = {Sun, Yifan and Cheng, Changmao and Zhang, Yuhan and Zhang, Chi and Zheng, Liang and Wang, Zhongdao and Wei, Yichen},
  year = {2020},
  month = jun,
  number = {arXiv:2002.10857},
  eprint = {2002.10857},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.10857},
  abstract = {This paper provides a pair similarity optimization viewpoint on deep feature learning, aiming to maximize the within-class similarity \$s\_p\$ and minimize the between-class similarity \$s\_n\$. We find a majority of loss functions, including the triplet loss and the softmax plus cross-entropy loss, embed \$s\_n\$ and \$s\_p\$ into similarity pairs and seek to reduce \$(s\_n-s\_p)\$. Such an optimization manner is inflexible, because the penalty strength on every single similarity score is restricted to be equal. Our intuition is that if a similarity score deviates far from the optimum, it should be emphasized. To this end, we simply re-weight each similarity to highlight the less-optimized similarity scores. It results in a Circle loss, which is named due to its circular decision boundary. The Circle loss has a unified formula for two elemental deep feature learning approaches, i.e. learning with class-level labels and pair-wise labels. Analytically, we show that the Circle loss offers a more flexible optimization approach towards a more definite convergence target, compared with the loss functions optimizing \$(s\_n-s\_p)\$. Experimentally, we demonstrate the superiority of the Circle loss on a variety of deep feature learning tasks. On face recognition, person re-identification, as well as several fine-grained image retrieval datasets, the achieved performance is on par with the state of the art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/elieb/Zotero/storage/AF3DJ69R/Sun et al. - 2020 - Circle Loss A Unified Perspective of Pair Similar.pdf;/Users/elieb/Zotero/storage/B52YJQFL/2002.html}
}

@misc{wangMultiSimilarityLossGeneral2020,
  title = {Multi-{{Similarity Loss}} with {{General Pair Weighting}} for {{Deep Metric Learning}}},
  author = {Wang, Xun and Han, Xintong and Huang, Weilin and Dong, Dengke and Scott, Matthew R.},
  year = {2020},
  month = mar,
  number = {arXiv:1904.06627},
  eprint = {1904.06627},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.06627},
  abstract = {A family of loss functions built on pair-based computation have been proposed in the literature which provide a myriad of solutions for deep metric learning. In this paper, we provide a general weighting framework for understanding recent pair-based loss functions. Our contributions are three-fold: (1) we establish a General Pair Weighting (GPW) framework, which casts the sampling problem of deep metric learning into a unified view of pair weighting through gradient analysis, providing a powerful tool for understanding recent pair-based loss functions; (2) we show that with GPW, various existing pair-based methods can be compared and discussed comprehensively, with clear differences and key limitations identified; (3) we propose a new loss called multi-similarity loss (MS loss) under the GPW, which is implemented in two iterative steps (i.e., mining and weighting). This allows it to fully consider three similarities for pair weighting, providing a more principled approach for collecting and weighting informative pairs. Finally, the proposed MS loss obtains new state-of-the-art performance on four image retrieval benchmarks, where it outperforms the most recent approaches, such as ABE\textbackslash cite\{Kim\_2018\_ECCV\} and HTL by a large margin: 60.6\% to 65.7\% on CUB200, and 80.9\% to 88.0\% on In-Shop Clothes Retrieval dataset at Recall@1. Code is available at https://github.com/MalongTech/research-ms-loss.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/elieb/Zotero/storage/5WDDNUSF/Wang et al. - 2020 - Multi-Similarity Loss with General Pair Weighting .pdf;/Users/elieb/Zotero/storage/ZRKP9P2X/1904.html}
}

@misc{wangMultiSimilarityLossGeneral2020a,
  title = {Multi-{{Similarity Loss}} with {{General Pair Weighting}} for {{Deep Metric Learning}}},
  author = {Wang, Xun and Han, Xintong and Huang, Weilin and Dong, Dengke and Scott, Matthew R.},
  year = {2020},
  month = mar,
  number = {arXiv:1904.06627},
  eprint = {1904.06627},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {A family of loss functions built on pair-based computation have been proposed in the literature which provide a myriad of solutions for deep metric learning. In this paper, we provide a general weighting framework for understanding recent pair-based loss functions. Our contributions are three-fold: (1) we establish a General Pair Weighting (GPW) framework, which casts the sampling problem of deep metric learning into a unified view of pair weighting through gradient analysis, providing a powerful tool for understanding recent pair-based loss functions; (2) we show that with GPW, various existing pair-based methods can be compared and discussed comprehensively, with clear differences and key limitations identified; (3) we propose a new loss called multi-similarity loss (MS loss) under the GPW, which is implemented in two iterative steps (i.e., mining and weighting). This allows it to fully consider three similarities for pair weighting, providing a more principled approach for collecting and weighting informative pairs. Finally, the proposed MS loss obtains new state-of-the-art performance on four image retrieval benchmarks, where it outperforms the most recent approaches, such as ABE\textbackslash cite\{Kim\_2018\_ECCV\} and HTL by a large margin: 60.6\% to 65.7\% on CUB200, and 80.9\% to 88.0\% on In-Shop Clothes Retrieval dataset at Recall@1. Code is available at https://github.com/MalongTech/research-ms-loss.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/elieb/Zotero/storage/LMT4WHAT/Wang et al. - 2020 - Multi-Similarity Loss with General Pair Weighting .pdf;/Users/elieb/Zotero/storage/NMCYCKCR/1904.html}
}

@misc{WordVectors157,
  title = {Word Vectors for 157 Languages {$\cdot$} {{fastText}}},
  abstract = {We distribute pre-trained word vectors for 157 languages, trained on [*Common Crawl*](http://commoncrawl.org/) and [*Wikipedia*](https://www.wikipedia.org) using fastText.},
  howpublished = {https://fasttext.cc/index.html},
  langid = {english},
  file = {/Users/elieb/Zotero/storage/42JI9P4K/crawl-vectors.html}
}

@misc{xieSmoothAdversarialTraining2021,
  title = {Smooth {{Adversarial Training}}},
  author = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Yuille, Alan and Le, Quoc V.},
  year = {2021},
  month = jul,
  number = {arXiv:2006.14536},
  eprint = {2006.14536},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.14536},
  abstract = {It is commonly believed that networks cannot be both accurate and robust, that gaining robustness means losing accuracy. It is also generally believed that, unless making networks larger, network architectural elements would otherwise matter little in improving adversarial robustness. Here we present evidence to challenge these common beliefs by a careful study about adversarial training. Our key observation is that the widely-used ReLU activation function significantly weakens adversarial training due to its non-smooth nature. Hence we propose smooth adversarial training (SAT), in which we replace ReLU with its smooth approximations to strengthen adversarial training. The purpose of smooth activation functions in SAT is to allow it to find harder adversarial examples and compute better gradient updates during adversarial training. Compared to standard adversarial training, SAT improves adversarial robustness for "free", i.e., no drop in accuracy and no increase in computational cost. For example, without introducing additional computations, SAT significantly enhances ResNet-50's robustness from 33.0\% to 42.3\%, while also improving accuracy by 0.9\% on ImageNet. SAT also works well with larger networks: it helps EfficientNet-L1 to achieve 82.2\% accuracy and 58.6\% robustness on ImageNet, outperforming the previous state-of-the-art defense by 9.5\% for accuracy and 11.6\% for robustness. Models are available at https://github.com/cihangxie/SmoothAdversarialTraining.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{xingDistanceMetricLearning,
  title = {Distance Metric Learning, with Application to Clustering with Side-Information},
  author = {Xing, Eric P and Ng, Andrew Y and Jordan, Michael I and Russell, Stuart},
  abstract = {Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many ``plausible'' ways, and if a clustering algorithm such as K-means initially fails to find one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufficiently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider ``similar.'' For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, if desired, dissimilar) pairs of points in \textcent\textcurrency\textsterling{} , learns a distance metric over \textcent\textyen\textsterling{} that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efficient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to significantly improve clustering performance.},
  langid = {english},
  file = {/Users/elieb/Zotero/storage/7LKN9LY4/Xing et al. - Distance metric learning, with application to clus.pdf}
}

@misc{xueByT5TokenfreeFuture2022,
  title = {{{ByT5}}: {{Towards}} a Token-Free Future with Pre-Trained Byte-to-Byte Models},
  shorttitle = {{{ByT5}}},
  author = {Xue, Linting and Barua, Aditya and Constant, Noah and {Al-Rfou}, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
  year = {2022},
  month = mar,
  number = {arXiv:2105.13626},
  eprint = {2105.13626},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/elieb/Zotero/storage/EICC4M85/Xue et al. - 2022 - ByT5 Towards a token-free future with pre-trained.pdf;/Users/elieb/Zotero/storage/4JXWJAZF/2105.html}
}

@misc{yamaguchiFrustratinglySimplePretraining2021,
  title = {Frustratingly {{Simple Pretraining Alternatives}} to {{Masked Language Modeling}}},
  author = {Yamaguchi, Atsuki and Chrysostomou, George and Margatina, Katerina and Aletras, Nikolaos},
  year = {2021},
  month = sep,
  number = {arXiv:2109.01819},
  eprint = {2109.01819},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.01819},
  abstract = {Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in natural language processing for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the entire vocabulary. When pretraining, it is common to use alongside MLM other auxiliary objectives on the token or sequence level to improve downstream performance (e.g. next sentence prediction). However, no previous work so far has attempted in examining whether other simpler linguistically intuitive or not objectives can be used standalone as main pretraining objectives. In this paper, we explore five simple pretraining objectives based on token-level classification tasks as replacements of MLM. Empirical results on GLUE and SQuAD show that our proposed methods achieve comparable or better performance to MLM using a BERT-BASE architecture. We further validate our methods using smaller models, showing that pretraining a model with 41\% of the BERT-BASE's parameters, BERT-MEDIUM results in only a 1\% drop in GLUE scores with our best objective.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/elieb/Zotero/storage/WWIJU6SY/Yamaguchi et al. - 2021 - Frustratingly Simple Pretraining Alternatives to M.pdf;/Users/elieb/Zotero/storage/EVSQ8GVD/2109.html}
}

@misc{zhangCharacterlevelConvolutionalNetworks2016,
  title = {Character-Level {{Convolutional Networks}} for {{Text Classification}}},
  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  year = {2016},
  month = apr,
  number = {arXiv:1509.01626},
  eprint = {1509.01626},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/elieb/Zotero/storage/65FCIB4I/Zhang et al. - 2016 - Character-level Convolutional Networks for Text Cl.pdf;/Users/elieb/Zotero/storage/73PYF6J8/1509.html}
}

@misc{zhangZeroShotLearningJoint2016,
  title = {Zero-{{Shot Learning}} via {{Joint Latent Similarity Embedding}}},
  author = {Zhang, Ziming and Saligrama, Venkatesh},
  year = {2016},
  month = aug,
  number = {arXiv:1511.04512},
  eprint = {1511.04512},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1511.04512},
  abstract = {Zero-shot recognition (ZSR) deals with the problem of predicting class labels for target domain instances based on source domain side information (e.g. attributes) of unseen classes. We formulate ZSR as a binary prediction problem. Our resulting classifier is class-independent. It takes an arbitrary pair of source and target domain instances as input and predicts whether or not they come from the same class, i.e. whether there is a match. We model the posterior probability of a match since it is a sufficient statistic and propose a latent probabilistic model in this context. We develop a joint discriminative learning framework based on dictionary learning to jointly learn the parameters of our model for both domains, which ultimately leads to our class-independent classifier. Many of the existing embedding methods can be viewed as special cases of our probabilistic model. On ZSR our method shows 4.90\textbackslash\% improvement over the state-of-the-art in accuracy averaged across four benchmark datasets. We also adapt ZSR method for zero-shot retrieval and show 22.45\textbackslash\% improvement accordingly in mean average precision (mAP).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/elieb/Zotero/storage/FY8UYU98/Zhang and Saligrama - 2016 - Zero-Shot Learning via Joint Latent Similarity Emb.pdf;/Users/elieb/Zotero/storage/6GV32E8Z/1511.html}
}

@misc{zhuangDealingTyposBERTbased2021,
  title = {Dealing with {{Typos}} for {{BERT-based Passage Retrieval}} and {{Ranking}}},
  author = {Zhuang, Shengyao and Zuccon, Guido},
  year = {2021},
  month = sep,
  number = {arXiv:2108.12139},
  eprint = {2108.12139},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Passage retrieval and ranking is a key task in open-domain question answering and information retrieval. Current effective approaches mostly rely on pre-trained deep language model-based retrievers and rankers. These methods have been shown to effectively model the semantic matching between queries and passages, also in presence of keyword mismatch, i.e. passages that are relevant to a query but do not contain important query keywords. In this paper we consider the Dense Retriever (DR), a passage retrieval method, and the BERT re-ranker, a popular passage re-ranking method. In this context, we formally investigate how these models respond and adapt to a specific type of keyword mismatch -- that caused by keyword typos occurring in queries. Through empirical investigation, we find that typos can lead to a significant drop in retrieval and ranking effectiveness. We then propose a simple typos-aware training framework for DR and BERT re-ranker to address this issue. Our experimental results on the MS MARCO passage ranking dataset show that, with our proposed typos-aware training, DR and BERT re-ranker can become robust to typos in queries, resulting in significantly improved effectiveness compared to models trained without appropriately accounting for typos.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval},
  file = {/Users/elieb/Zotero/storage/XGR96BRX/Zhuang and Zuccon - 2021 - Dealing with Typos for BERT-based Passage Retrieva.pdf;/Users/elieb/Zotero/storage/PGVH98BW/2108.html}
}

 @article{DADA2019e01802,
 title = {Machine learning for email spam filtering: review, approaches and open research problems},
 journal = {Heliyon},
 volume = {5},
 number = {6},
 pages = {e01802},
 year = {2019},
 issn = {2405-8440},
 doi = {https://doi.org/10.1016/j.heliyon.2019.e01802},
 url = {https://www.sciencedirect.com/science/article/pii/S2405844018353404},
 author = {Emmanuel Gbenga Dada and Joseph Stephen Bassi and Haruna Chiroma and Shafi'i Muhammad Abdulhamid and Adebayo Olusola Adetunmbi and Opeyemi Emmanuel Ajibuwa},
 keywords = {Computer science, Computer security, Computer privacy, Analysis of algorithms, Machine learning, Spam filtering, Deep learning, Neural networks, Support vector machines, Nave Bayes},
 abstract = {The upsurge in the volume of unwanted emails called spam has created an intense need for the development of more dependable and robust antispam filters. Machine learning methods of recent are being used to     successfully detect and filter spam emails. We present a systematic review of some of the popular machine learning based email spam filtering approaches. Our review covers survey of the important concepts, attempts,       efficiency, and the research trend in spam filtering. The preliminary discussion in the study background examines the applications of machine learning techniques to the email spam filtering process of the leading          internet service providers (ISPs) like Gmail, Yahoo and Outlook emails spam filters. Discussion on general email spam filtering process, and the various efforts by different researchers in combating spam through the use   machine learning techniques was done. Our review compares the strengths and drawbacks of existing machine learning approaches and the open research problems in spam filtering. We recommended deep leaning and deep          adversarial learning as the future techniques that can effectively handle the menace of spam emails.}
 }
 
 
@misc{wang2019glue,
      title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}, 
      author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
      year={2019},
      eprint={1804.07461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{biderman2023pythia,
      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
      year={2023},
      eprint={2304.01373},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{oord2018neural,
      title={Neural Discrete Representation Learning}, 
      author={Aaron van den Oord and Oriol Vinyals and Koray Kavukcuoglu},
      year={2018},
      eprint={1711.00937},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}