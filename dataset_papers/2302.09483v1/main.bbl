\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{GvdMZG22}

\bibitem[ABM19]{alon2019limits}
Noga Alon, Raef Bassily, and Shay Moran.
\newblock Limits of private learning with access to public data.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem[ACG{\etalchar{+}}16]{DP-DL}
Mart{\'{\i}}n Abadi, Andy Chu, Ian~J. Goodfellow, H.~Brendan McMahan, Ilya
  Mironov, Kunal Talwar, and Li~Zhang.
\newblock Deep learning with differential privacy.
\newblock In {\em Proc. of the 2016 {ACM} {SIGSAC} Conf. on Computer and
  Communications Security ({CCS}'16)}, pages 308--318, 2016.

\bibitem[ADF{\etalchar{+}}21]{asi2021private}
Hilal Asi, John Duchi, Alireza Fallah, Omid Javidbakht, and Kunal Talwar.
\newblock Private adaptive gradient methods for convex optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  383--392. PMLR, 2021.

\bibitem[ADK20]{avent2020hybrid}
Brendan Avent, Yatharth Dubey, and Aleksandra Korolova.
\newblock The power of the hybrid model for mean estimation.
\newblock {\em Proceedings on Privacy Enhancing Technologies}, 2020:48--68, 10
  2020.

\bibitem[AGM{\etalchar{+}}22]{amid2022mirrordescent}
Ehsan Amid, Arun Ganesh, Rajiv Mathews, Swaroop Ramaswamy, Shuang Song, Thomas
  Steinke, Vinith~M. Suriyakumar, Om~Thakkar, and Abhradeep Thakurta.
\newblock Public data-assisted mirror descent for private model training.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba
  Szepesv{\'{a}}ri, Gang Niu, and Sivan Sabato, editors, {\em International
  Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore,
  Maryland, {USA}}, volume 162 of {\em Proceedings of Machine Learning
  Research}, pages 517--535. {PMLR}, 2022.

\bibitem[ALD21]{asi2021adapting}
Hilal Asi, Daniel Asher~Nathan Levy, and John Duchi.
\newblock Adapting to function difficulty and growth conditions in private
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem[BCM{\etalchar{+}}20]{BassilyCMNUW20}
Raef Bassily, Albert Cheu, Shay Moran, Aleksandar Nikolov, Jonathan Ullman, and
  Steven Wu.
\newblock Private query release assisted by public data.
\newblock In Hal~Daumé III and Aarti Singh, editors, {\em Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 695--703. PMLR, 13--18 Jul
  2020.

\bibitem[BF76]{bozinovski1976influence}
Stevo Bozinovski and Ante Fulgosi.
\newblock The influence of pattern similarity and transfer learning upon
  training of a base perceptron b2.
\newblock In {\em Proceedings of Symposium Informatica}, volume~3, pages
  121--126, 1976.

\bibitem[BFGT20]{bassily2020stability}
Raef Bassily, Vitaly Feldman, Crist{\'o}bal Guzm{\'a}n, and Kunal Talwar.
\newblock Stability of stochastic gradient descent on nonsmooth convex losses.
\newblock {\em Advances in Neural Information Processing Systems},
  33:4381--4391, 2020.

\bibitem[BFTT19]{BassilyFTT19}
Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep~Guha Thakurta.
\newblock Private stochastic convex optimization with optimal rates.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  11279--11288, 2019.

\bibitem[BHA{\etalchar{+}}21]{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock {\em arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[BKS22]{bie2022estimation}
Alex Bie, Gautam Kamath, and Vikrant Singhal.
\newblock Private estimation with public data.
\newblock {\em Advances in neural information processing systems 35}, 2022.

\bibitem[BST14]{BST14}
Raef Bassily, Adam Smith, and Abhradeep Thakurta.
\newblock Private empirical risk minimization: Efficient algorithms and tight
  error bounds.
\newblock In {\em Proc. of the 2014 IEEE 55th Annual Symp. on Foundations of
  Computer Science (FOCS)}, pages 464--473, 2014.

\bibitem[BWZK]{bu2022differentially}
Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, and George Karypis.
\newblock Differentially private bias-term only fine-tuning of foundation
  models.
\newblock In {\em Workshop on Trustworthy and Socially Responsible Machine
  Learning, NeurIPS 2022}.

\bibitem[BWZK22]{bu2022differentially2}
Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, and George Karypis.
\newblock Differentially private optimization on large model at small cost.
\newblock {\em arXiv preprint arXiv:2210.00038}, 2022.

\bibitem[DBH{\etalchar{+}}22]{de2022unlocking}
Soham De, Leonard Berrada, Jamie Hayes, Samuel~L Smith, and Borja Balle.
\newblock Unlocking high-accuracy differentially private image classification
  through scale.
\newblock {\em arXiv preprint arXiv:2204.13650}, 2022.

\bibitem[DMNS06]{DMNS}
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith.
\newblock Calibrating noise to sensitivity in private data analysis.
\newblock In {\em Proc. of the Third Conf. on Theory of Cryptography (TCC)},
  pages 265--284, 2006.

\bibitem[FKT20]{feldman2019private}
Vitaly Feldman, Tomer Koren, and Kunal Talwar.
\newblock Private stochastic convex optimization: Optimal rates in linear time.
\newblock In {\em Proc. of the Fifty-Second {ACM} Symp. on Theory of Computing
  ({STOC}'20)}, 2020.

\bibitem[GAW{\etalchar{+}}22]{golatkar2022mixed}
Aditya Golatkar, Alessandro Achille, Yu-Xiang Wang, Aaron Roth, Michael Kearns,
  and Stefano Soatto.
\newblock Mixed differential privacy in computer vision.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 8376--8386, 2022.

\bibitem[GLL22]{gopi2022private}
Sivakanth Gopi, Yin~Tat Lee, and Daogao Liu.
\newblock Private convex optimization via exponential mechanism.
\newblock In {\em Conference on Learning Theory}, pages 1948--1989. PMLR, 2022.

\bibitem[GQC{\etalchar{+}}20]{gulati2020conformer}
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu~Zhang, Jiahui Yu,
  Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et~al.
\newblock Conformer: Convolution-augmented transformer for speech recognition.
\newblock {\em Proc. Interspeech 2020}, pages 5036--5040, 2020.

\bibitem[GTU22]{GTU22}
Arun Ganesh, Abhradeep Thakurta, and Jalaj Upadhyay.
\newblock On the universality of langevin diffusion for private euclidean
  (convex) optimization, 2022.
\newblock https://openreview.net/forum?id=ZrJPdY5k6sg.

\bibitem[GvdMZG22]{ginart2022submix}
Antonio Ginart, Laurens van~der Maaten, James Zou, and Chuan Guo.
\newblock Submix: Practical private prediction for large-scale language models.
\newblock {\em arXiv preprint arXiv:2201.00971}, 2022.

\bibitem[Hay03]{hayes03vectorazuma}
Thomas~P. Hayes.
\newblock A large-deviation inequality for vector-valued martingales.
\newblock 2003.
\newblock
  http://agl.cs.unm.edu/~hayes/papers/VectorAzuma/VectorAzuma20030207.pdf.

\bibitem[HLY{\etalchar{+}}22]{he2022exploring}
Jiyan He, Xuechen Li, Da~Yu, Huishuai Zhang, Janardhan Kulkarni, Yin~Tat Lee,
  Arturs Backurs, Nenghai Yu, and Jiang Bian.
\newblock Exploring the limits of differentially private deep learning with
  group-wise clipping.
\newblock {\em arXiv preprint arXiv:2212.01539}, 2022.

\bibitem[HRS16]{HardtRS16}
Moritz Hardt, Benjamin Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In {\em Proceedings of the 33rd International Conference on
  International Conference on Machine Learning - Volume 48}, ICML'16, page
  1225–1234. JMLR.org, 2016.

\bibitem[HWZ22]{hong2022dynamic}
Junyuan Hong, Zhangyang Wang, and Jiayu Zhou.
\newblock Dynamic privacy budget allocation improves data efficiency of
  differentially private gradient descent.
\newblock In {\em 2022 ACM Conference on Fairness, Accountability, and
  Transparency}, FAccT '22, page 11–35, New York, NY, USA, 2022. Association
  for Computing Machinery.

\bibitem[JZK{\etalchar{+}}22]{jiang2022dp}
Dihong Jiang, Guojun Zhang, Mahdi Karami, Xi~Chen, Yunfeng Shao, and Yaoliang
  Yu.
\newblock Dp$^2$-vae: Differentially private pre-trained variational
  autoencoders.
\newblock {\em arXiv preprint arXiv:2208.03409}, 2022.

\bibitem[KCS{\etalchar{+}}22]{kurakin2022toward}
Alexey Kurakin, Steve Chien, Shuang Song, Roxana Geambasu, Andreas Terzis, and
  Abhradeep Thakurta.
\newblock Toward training at imagenet scale with differential privacy.
\newblock {\em arXiv preprint arXiv:2201.12328}, 2022.

\bibitem[KLL21]{kulkarni2021private}
Janardhan Kulkarni, Yin~Tat Lee, and Daogao Liu.
\newblock Private non-smooth erm and sco in subquadratic steps.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[KOV17]{KOV17}
Peter Kairouz, Sewoong Oh, and Pramod Viswanath.
\newblock The composition theorem for differential privacy.
\newblock {\em {IEEE} Trans. Inf. Theory}, 63(6):4037--4049, 2017.

\bibitem[KRRT20]{kairouz2020fast}
Peter Kairouz, M{\'o}nica Ribero, Keith Rush, and Abhradeep Thakurta.
\newblock Fast dimension independent private adagrad on publicly estimated
  subspaces.
\newblock {\em arXiv preprint arXiv:2008.06570}, 2020.

\bibitem[KST20]{kerrigan2020differentially}
Gavin Kerrigan, Dylan Slack, and Jens Tuyls.
\newblock Differentially private language models benefit from public
  pre-training.
\newblock In {\em Proceedings of the Second Workshop on Privacy in NLP}, pages
  39--45, 2020.

\bibitem[LK18]{lee2018concentrated}
Jaewoo Lee and Daniel Kifer.
\newblock Concentrated differentially private gradient descent with adaptive
  per-iteration privacy budget.
\newblock In {\em Proceedings of the 24th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 1656--1665, 2018.

\bibitem[LLH{\etalchar{+}}22]{li2022does}
Xuechen Li, Daogao Liu, Tatsunori Hashimoto, Huseyin~A Inan, Janardhan
  Kulkarni, YinTat Lee, and Abhradeep~Guha Thakurta.
\newblock When does differentially private learning not suffer in high
  dimensions?
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem[LTLH22]{li2021large}
Xuechen Li, Florian Tram{\`e}r, Percy Liang, and Tatsunori Hashimoto.
\newblock Large language models can be strong differentially private learners.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem[LVS{\etalchar{+}}21]{liu2021leveraging}
Terrance Liu, Giuseppe Vietri, Thomas Steinke, Jonathan Ullman, and Steven Wu.
\newblock Leveraging public data for practical private query release.
\newblock In {\em International Conference on Machine Learning}, pages
  6968--6977. PMLR, 2021.

\bibitem[LWAFF21]{luo2021scalable}
Zelun Luo, Daniel~J Wu, Ehsan Adeli, and Li~Fei-Fei.
\newblock Scalable differential privacy with sparse network finetuning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 5059--5068, 2021.

\bibitem[NDR17]{novikova2017e2e}
Jekaterina Novikova, Ond{\v{r}}ej Du{\v{s}}ek, and Verena Rieser.
\newblock The e2e dataset: New challenges for end-to-end generation.
\newblock {\em arXiv preprint arXiv:1706.09254}, 2017.

\bibitem[NMT{\etalchar{+}}22]{dopesgd}
Milad Nasr, Saeed Mahloujifar, Xinyu Tang, Prateek Mittal, and Amir Houmansadr.
\newblock Effectively using public data in privacy preserving machine learning,
  2022.
\newblock https://openreview.net/pdf?id=5R96mIU85IW.

\bibitem[NRZ{\etalchar{+}}20]{nan2020dart}
Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad,
  Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et~al.
\newblock Dart: Open-domain structured data record to text generation.
\newblock {\em arXiv preprint arXiv:2007.02871}, 2020.

\bibitem[PCPK15]{panayotov2015librispeech}
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur.
\newblock Librispeech: an asr corpus based on public domain audio books.
\newblock In {\em 2015 IEEE international conference on acoustics, speech and
  signal processing (ICASSP)}, pages 5206--5210. IEEE, 2015.

\bibitem[RWC{\etalchar{+}}19]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem[SCS13]{song2013stochastic}
Shuang Song, Kamalika Chaudhuri, and Anand~D Sarwate.
\newblock Stochastic gradient descent with differentially private updates.
\newblock In {\em 2013 IEEE Global Conference on Signal and Information
  Processing}, pages 245--248. IEEE, 2013.

\bibitem[SRASC14]{sharif2014cnn}
Ali Sharif~Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson.
\newblock Cnn features off-the-shelf: an astounding baseline for recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition workshops}, pages 806--813, 2014.

\bibitem[SSTT21]{song21clipping}
Shuang Song, Thomas Steinke, Om~Thakkar, and Abhradeep Thakurta.
\newblock Evading the curse of dimensionality in unconstrained private glms.
\newblock In Arindam Banerjee and Kenji Fukumizu, editors, {\em Proceedings of
  The 24th International Conference on Artificial Intelligence and Statistics},
  volume 130 of {\em Proceedings of Machine Learning Research}, pages
  2638--2646. PMLR, 13--15 Apr 2021.

\bibitem[SU17]{steinke2017tight}
Thomas Steinke and Jonathan Ullman.
\newblock Tight lower bounds for differentially private selection.
\newblock In {\em 2017 IEEE 58th Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 552--563. IEEE, 2017.

\bibitem[TB20]{tramer2020differentially}
Florian Tramer and Dan Boneh.
\newblock Differentially private learning needs better features (or much more
  data).
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem[TKC22]{tramer2022considerations}
Florian Tram{\`e}r, Gautam Kamath, and Nicholas Carlini.
\newblock Considerations for differentially private learning with large-scale
  public pretraining.
\newblock {\em arXiv preprint arXiv:2212.06470}, 2022.

\bibitem[YNB{\etalchar{+}}22]{yu2021differentially}
Da~Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin~A Inan, Gautam
  Kamath, Janardhan Kulkarni, Yin~Tat Lee, Andre Manoel, Lukas Wutschitz,
  et~al.
\newblock Differentially private fine-tuning of language models.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem[YZCL21]{yu2021not}
Da~Yu, Huishuai Zhang, Wei Chen, and Tie-Yan Liu.
\newblock Do not let privacy overbill utility: Gradient embedding perturbation
  for private learning.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem[ZWB21]{zhou2020bypassing}
Yingxue Zhou, Steven Wu, and Arindam Banerjee.
\newblock Bypassing the ambient dimension: Private sgd with gradient subspace
  identification.
\newblock In {\em International Conference on Learning Representations}, 2021.

\end{thebibliography}
