@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@inproceedings{sharan2019memory,
  title={Memory-sample tradeoffs for linear regression with small error},
  author={Sharan, Vatsal and Sidford, Aaron and Valiant, Gregory},
  booktitle={Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing},
  pages={890--901},
  year={2019}
}


@book{nocedal1999numerical,
  title={Numerical optimization},
  author={Nocedal, Jorge and Wright, Stephen J},
  year={1999},
  publisher={Springer}
}

@article{sanford2023representational,
  title={Representational Strengths and Limitations of Transformers},
  author={Sanford, Clayton and Hsu, Daniel and Telgarsky, Matus},
  journal={arXiv preprint arXiv:2306.02896},
  year={2023}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{Bai2023TransformersAS,
  title={Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection},
  author={Yu Bai and Fan Chen and Haiquan Wang and Caiming Xiong and Song Mei},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.04637},
}

@inproceedings{Oswald2022TransformersLI,
  title={Transformers learn in-context by gradient descent},
  author={Johannes von Oswald and Eyvind Niklasson and E. Randazzo and Jo{\~a}o Sacramento and Alexander Mordvintsev and Andrey Zhmoginov and Max Vladymyrov},
  booktitle={International Conference on Machine Learning},
  year={2022},
}

@article{Garg2022WhatCT,
  title={What Can Transformers Learn In-Context? A Case Study of Simple Function Classes},
  author={Shivam Garg and Dimitris Tsipras and Percy Liang and Gregory Valiant},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.01066},
}

@article{Zhang2023TrainedTL,
  title={Trained Transformers Learn Linear Models In-Context},
  author={Ruiqi Zhang and Spencer Frei and Peter L. Bartlett},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.09927},
}

@inproceedings{Li2023TransformersAA,
  title={Transformers as Algorithms: Generalization and Stability in In-context Learning},
  author={Yingcong Li and Muhammed Emrullah Ildiz and Dimitris Papailiopoulos and Samet Oymak},
  booktitle={International Conference on Machine Learning},
  year={2023},
}

@article{AtaeeTarzanagh2023TransformersAS,
  title={Transformers as Support Vector Machines},
  author={Davoud Ataee Tarzanagh and Yingcong Li and Christos Thrampoulidis and Samet Oymak},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.16898},
}

@inproceedings{Vaswani2017AttentionIA, 
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{Dai2023WhyCG,
  title={Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers},
  author={Damai Dai and Yutao Sun and Li Dong and Yaru Hao and Zhifang Sui and Furu Wei},
  journal={ArXiv},
  year={2023},
  volume={abs/2212.10559},
}

@article{Ahn2023TransformersLT,
  title={Transformers learn to implement preconditioned gradient descent for in-context learning},
  author={Kwangjun Ahn and Xiang Cheng and Hadi Daneshmand and Suvrit Sra},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.00297},
}

@article{Oswald2023UncoveringMA,
  title={Uncovering mesa-optimization algorithms in Transformers},
  author={Johannes von Oswald and Eyvind Niklasson and Maximilian Schlegel and Seijin Kobayashi and Nicolas Zucchet and Nino Scherrer and Nolan Miller and Mark Sandler and Blaise Ag{\"u}era y Arcas and Max Vladymyrov and Razvan Pascanu and Joao Sacramento},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.05858},
}

@article{Akyrek2022WhatLA,
  title={What learning algorithm is in-context learning? Investigations with linear models},
  author={Ekin Aky{\"u}rek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.15661},
}

@article{Pan1991AnIN,
  title={An Improved Newton Iteration for the Generalized Inverse of a Matrix, with Applications},
  author={Victor Y. Pan and Robert S. Schreiber},
  journal={SIAM J. Sci. Comput.},
  year={1991},
  volume={12},
  pages={1109-1130},
}

@misc{han2023incontext,
      title={In-Context Learning of Large Language Models Explained as Kernel Regression}, 
      author={Chi Han and Ziqi Wang and Han Zhao and Heng Ji},
      year={2023},
      eprint={2305.12766},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tarzanagh2023maxmargin,
      title={Max-Margin Token Selection in Attention Mechanism}, 
      author={Davoud Ataee Tarzanagh and Yingcong Li and Xuechen Zhang and Samet Oymak},
      year={2023},
      eprint={2306.13596},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@article{adi1965ai,
 ISSN = {00255718, 10886842},
 URL = {http://www.jstor.org/stable/2003676},
 abstract = {The iterative process, Xn+1 = Xn(2I - AXn), for computing A-1, is generalized to obtain the generalized inverse.},
 author = {Adi Ben-Israel},
 journal = {Mathematics of Computation},
 number = {91},
 pages = {452--455},
 publisher = {American Mathematical Society},
 title = {An Iterative Method for Computing the Generalized Inverse of an Arbitrary Matrix},
 urldate = {2023-09-24},
 volume = {19},
 year = {1965}
}

@article{soderstrom1974onp,
 ISSN = {00361429},
 URL = {http://www.jstor.org/stable/2156431},
 abstract = {In this paper some of the numerical problems associated with computing the generalized inverse of a matrix are discussed and illustrated by a detailed analysis of an iteration of Ben-Israel and Cohen.},
 author = {Torsten Soderstrom and G. W. Stewart},
 journal = {SIAM Journal on Numerical Analysis},
 number = {1},
 pages = {61--74},
 publisher = {Society for Industrial and Applied Mathematics},
 title = {On the Numerical Properties of an Iterative Method for Computing the Moore- Penrose Generalized Inverse},
 urldate = {2023-09-24},
 volume = {11},
 year = {1974}
}

@article{moonre1920pseudo,
    author = {Moore, E.H},
    title = {On the Reciprocal of the General Algebraic Matrix},
    journal = {Bulletin of American Mathematical Society},
    year = {1920},
    volume = {26},
    pages = {394-395}
}

@article{schulz1933inverse,
    author = {G\"unther Schulz},
    title = {Iterative Berechung der reziproken Matrix},
    journal = {Zeitschrift f\"ur Angewandte Mathematik und Mechanik (Journal of Applied Mathematics and Mechanics)},
    year = {1933}, 
    volume= {13},
    pages = {57-59}
}
@article{hochreiter1997lstm,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@inproceedings{brown2020lm,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{rae2022scaling,
      title={Scaling Language Models: Methods, Analysis \& Insights from Training Gopher}, 
      author={Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and Eliza Rutherford and Tom Hennigan and Jacob Menick and Albin Cassirer and Richard Powell and George van den Driessche and Lisa Anne Hendricks and Maribeth Rauh and Po-Sen Huang and Amelia Glaese and Johannes Welbl and Sumanth Dathathri and Saffron Huang and Jonathan Uesato and John Mellor and Irina Higgins and Antonia Creswell and Nat McAleese and Amy Wu and Erich Elsen and Siddhant Jayakumar and Elena Buchatskaya and David Budden and Esme Sutherland and Karen Simonyan and Michela Paganini and Laurent Sifre and Lena Martens and Xiang Lorraine Li and Adhiguna Kuncoro and Aida Nematzadeh and Elena Gribovskaya and Domenic Donato and Angeliki Lazaridou and Arthur Mensch and Jean-Baptiste Lespiau and Maria Tsimpoukelli and Nikolai Grigorev and Doug Fritz and Thibault Sottiaux and Mantas Pajarskas and Toby Pohlen and Zhitao Gong and Daniel Toyama and Cyprien de Masson d'Autume and Yujia Li and Tayfun Terzi and Vladimir Mikulik and Igor Babuschkin and Aidan Clark and Diego de Las Casas and Aurelia Guy and Chris Jones and James Bradbury and Matthew Johnson and Blake Hechtman and Laura Weidinger and Iason Gabriel and William Isaac and Ed Lockhart and Simon Osindero and Laura Rimell and Chris Dyer and Oriol Vinyals and Kareem Ayoub and Jeff Stanway and Lorrayne Bennett and Demis Hassabis and Koray Kavukcuoglu and Geoffrey Irving},
      year={2022},
      eprint={2112.11446},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{smith2022using,
      title={Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model}, 
      author={Shaden Smith and Mostofa Patwary and Brandon Norick and Patrick LeGresley and Samyam Rajbhandari and Jared Casper and Zhun Liu and Shrimai Prabhumoye and George Zerveas and Vijay Korthikanti and Elton Zhang and Rewon Child and Reza Yazdani Aminabadi and Julie Bernauer and Xia Song and Mohammad Shoeybi and Yuxiong He and Michael Houston and Saurabh Tiwary and Bryan Catanzaro},
      year={2022},
      eprint={2201.11990},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{thoppilan2022lamda,
      title={LaMDA: Language Models for Dialog Applications}, 
      author={Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and YaGuang Li and Hongrae Lee and Huaixiu Steven Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and Maxim Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Vincent Zhao and Yanqi Zhou and Chung-Ching Chang and Igor Krivokon and Will Rusch and Marc Pickett and Pranesh Srinivasan and Laichee Man and Kathleen Meier-Hellstern and Meredith Ringel Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and Johnny Soraker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark Diaz and Ben Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravi Rajakumar and Alena Butryna and Matthew Lamm and Viktoriya Kuzmina and Joe Fenton and Aaron Cohen and Rachel Bernstein and Ray Kurzweil and Blaise Aguera-Arcas and Claire Cui and Marian Croak and Ed Chi and Quoc Le},
      year={2022},
      eprint={2201.08239},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chowdhery2022palm,
      title={PaLM: Scaling Language Modeling with Pathways}, 
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
      year={2022},
      eprint={2204.02311},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{shen2023study,
      title={A Study on ReLU and Softmax in Transformer}, 
      author={Kai Shen and Junliang Guo and Xu Tan and Siliang Tang and Rui Wang and Jiang Bian},
      year={2023},
      eprint={2302.06461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@misc{wortsman2023replacing,
      title={Replacing softmax with ReLU in Vision Transformers}, 
      author={Mitchell Wortsman and Jaehoon Lee and Justin Gilmer and Simon Kornblith},
      year={2023},
      eprint={2309.08586},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@article{Cai2022EfficientViTEL,
  title={EfficientViT: Enhanced Linear Attention for High-Resolution Low-Computation Visual Recognition},
  author={Han Cai and Chuang Gan and Song Han},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.14756},
}


@misc{nanda2023progress,
      title={Progress measures for grokking via mechanistic interpretability}, 
      author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
      year={2023},
      eprint={2301.05217},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wang2022interpretability,
      title={Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small}, 
      author={Kevin Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
      year={2022},
      eprint={2211.00593},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{power2022grokking,
      title={Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets}, 
      author={Alethea Power and Yuri Burda and Harri Edwards and Igor Babuschkin and Vedant Misra},
      year={2022},
      eprint={2201.02177},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hassid2022does,
      title={How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers}, 
      author={Michael Hassid and Hao Peng and Daniel Rotem and Jungo Kasai and Ivan Montero and Noah A. Smith and Roy Schwartz},
      year={2022},
      eprint={2211.03495},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{raventós2023pretraining,
      title={Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression}, 
      author={Allan Raventós and Mansheej Paul and Feng Chen and Surya Ganguli},
      year={2023},
      eprint={2306.15063},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{conmy2023automated,
      title={Towards Automated Circuit Discovery for Mechanistic Interpretability}, 
      author={Arthur Conmy and Augustine N. Mavor-Parker and Aengus Lynch and Stefan Heimersheim and Adrià Garriga-Alonso},
      year={2023},
      eprint={2304.14997},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wei2023larger,
      title={Larger language models do in-context learning differently}, 
      author={Jerry Wei and Jason Wei and Yi Tay and Dustin Tran and Albert Webson and Yifeng Lu and Xinyun Chen and Hanxiao Liu and Da Huang and Denny Zhou and Tengyu Ma},
      year={2023},
      eprint={2303.03846},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@misc{openai2023gpt,
 archiveprefix = {arXiv},
 author = { OpenAI},
 eprint = {2303.08774},
 title = {GPT-4 Technical Report},
 url = {http://arxiv.org/abs/2303.08774v3},
 year = {2023}
}

@inproceedings{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle={International Conference on Machine Learning},
  pages={12697--12706},
  year={2021},
  organization={PMLR}
}
@article{nguyen2023context,
  title={In-context Example Selection with Influences},
  author={Nguyen, Tai and Wong, Eric},
  journal={arXiv preprint arXiv:2302.11042},
  year={2023}
}
@inproceedings{
su2023selective,
title={Selective Annotation Makes Language Models Better Few-Shot Learners},
author={Hongjin Su and Jungo Kasai and Chen Henry Wu and Weijia Shi and Tianlu Wang and Jiayi Xin and Rui Zhang and Mari Ostendorf and Luke Zettlemoyer and Noah A. Smith and Tao Yu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=qY1hlv7gwg}
}

@misc{chuang2023dola,
      title={DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models}, 
      author={Yung-Sung Chuang and Yujia Xie and Hongyin Luo and Yoon Kim and James Glass and Pengcheng He},
      year={2023},
      eprint={2309.03883},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shen2023pretrained,
      title={Do pretrained Transformers Really Learn In-context by Gradient Descent?}, 
      author={Lingfeng Shen and Aayush Mishra and Daniel Khashabi},
      year={2023},
      eprint={2310.08540},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{nemirovskii1983problem,
  title={Problem Complexity and Method Efficiency in Optimization},
  author={Nemirovski, A.S. and Yudin, D.B},
  year={1983},
  publisher={Wiley}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}

@inproceedings{lu-etal-2022-fantastically,
    title = "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
    author = "Lu, Yao  and
      Bartolo, Max  and
      Moore, Alastair  and
      Riedel, Sebastian  and
      Stenetorp, Pontus",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.556",
    doi = "10.18653/v1/2022.acl-long.556",
    pages = "8086--8098",
}

@inproceedings{liu-etal-2022-makes,
    title = "What Makes Good In-Context Examples for {GPT}-3?",
    author = "Liu, Jiachang  and
      Shen, Dinghan  and
      Zhang, Yizhe  and
      Dolan, Bill  and
      Carin, Lawrence  and
      Chen, Weizhu",
    booktitle = "Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures",
    month = may,
    year = "2022",
    address = "Dublin, Ireland and Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.deelio-1.10",
    doi = "10.18653/v1/2022.deelio-1.10",
    pages = "100--114",
}

@inproceedings{rubin-etal-2022-learning,
    title = "Learning To Retrieve Prompts for In-Context Learning",
    author = "Rubin, Ohad  and
      Herzig, Jonathan  and
      Berant, Jonathan",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.191",
    doi = "10.18653/v1/2022.naacl-main.191",
    pages = "2655--2671",
}

@inproceedings{chang-jia-2023-data,
    title = "Data Curation Alone Can Stabilize In-context Learning",
    author = "Chang, Ting-Yun  and
      Jia, Robin",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.452",
    doi = "10.18653/v1/2023.acl-long.452",
    pages = "8123--8144",
}

@inproceedings{min-etal-2022-rethinking,
    title = "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
    author = "Min, Sewon  and
      Lyu, Xinxi  and
      Holtzman, Ari  and
      Artetxe, Mikel  and
      Lewis, Mike  and
      Hajishirzi, Hannaneh  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.759",
    doi = "10.18653/v1/2022.emnlp-main.759",
    pages = "11048--11064",
}

@inproceedings{yoo-etal-2022-ground,
    title = "Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations",
    author = "Yoo, Kang Min  and
      Kim, Junyeob  and
      Kim, Hyuhng Joon  and
      Cho, Hyunsoo  and
      Jo, Hwiyeol  and
      Lee, Sang-Woo  and
      Lee, Sang-goo  and
      Kim, Taeuk",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.155",
    doi = "10.18653/v1/2022.emnlp-main.155",
    pages = "2422--2437",
}

@inproceedings{min-etal-2022-noisy,
    title = "Noisy Channel Language Model Prompting for Few-Shot Text Classification",
    author = "Min, Sewon  and
      Lewis, Mike  and
      Hajishirzi, Hannaneh  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.365",
    doi = "10.18653/v1/2022.acl-long.365",
    pages = "5316--5330",
}

@inproceedings{min-etal-2022-metaicl,
    title = "{M}eta{ICL}: Learning to Learn In Context",
    author = "Min, Sewon  and
      Lewis, Mike  and
      Zettlemoyer, Luke  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.201",
    doi = "10.18653/v1/2022.naacl-main.201",
    pages = "2791--2809",
}

@inproceedings{geva-etal-2022-transformer,
    title = "Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space",
    author = "Geva, Mor  and
      Caciularu, Avi  and
      Wang, Kevin  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.3",
    doi = "10.18653/v1/2022.emnlp-main.3",
    pages = "30--45",
}


@InProceedings{pmlr-v202-giannou23a,
  title = 	 {Looped Transformers as Programmable Computers},
  author =       {Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-Yong and Lee, Kangwook and Lee, Jason D. and Papailiopoulos, Dimitris},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {11398--11442},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/giannou23a/giannou23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/giannou23a.html},
  abstract = 	 {We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including lexicographic operations, non-linear functions, function calls, program counters, and conditional branches. Using this framework, we emulate a computer using a simple instruction-set architecture, which allows us to map iterative algorithms to programs that can be executed by a constant depth looped transformer network. We show how a single frozen transformer, instructed by its input, can emulate a basic calculator, a basic linear algebra library, and even a full backpropagation, in-context learning algorithm. Our findings reveal the potential of transformer networks as programmable compute units and offer insight into the mechanics of attention.}
}

@misc{lee2023exploring,
      title={Exploring the Relationship Between Model Architecture and In-Context Learning Ability}, 
      author={Ivan Lee and Nan Jiang and Taylor Berg-Kirkpatrick},
      year={2023},
      eprint={2310.08049},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{bai2023sequential,
      title={Sequential Modeling Enables Scalable Learning for Large Vision Models}, 
      author={Yutong Bai and Xinyang Geng and Karttikeya Mangalam and Amir Bar and Alan Yuille and Trevor Darrell and Jitendra Malik and Alexei A Efros},
      year={2023},
      eprint={2312.00785},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{
mahankali2024one,
title={One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention},
author={Arvind V. Mahankali and Tatsunori Hashimoto and Tengyu Ma},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=8p3fu56lKc}
}

@misc{vladymyrov2024linear,
      title={Linear Transformers are Versatile In-Context Learners}, 
      author={Max Vladymyrov and Johannes von Oswald and Mark Sandler and Rong Ge},
      year={2024},
      eprint={2402.14180},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Arjevani2016OnLower,
  author  = {Yossi Arjevani and Shai Shalev-Shwartz and Ohad Shamir},
  title   = {On Lower and Upper Bounds in Smooth and Strongly Convex Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {126},
  pages   = {1--51},
  url     = {http://jmlr.org/papers/v17/15-106.html}
}

@misc{paszke2019pytorch,
      title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
      year={2019},
      eprint={1912.01703},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Liu1989OnTL,
  title={On the limited memory BFGS method for large scale optimization},
  author={Dong C. Liu and Jorge Nocedal},
  journal={Mathematical Programming},
  year={1989},
  volume={45},
  pages={503-528},
  url={https://api.semanticscholar.org/CorpusID:5681609}
}