@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2020}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}




@InProceedings{influence1,
  title = 	 {Understanding Black-box Predictions via Influence Functions},
  author = 	 {Pang Wei Koh and Percy Liang},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1885--1894},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/koh17a/koh17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/koh17a.html},
  abstract = 	 {How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic technique from robust statistics — to trace a model’s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.}
}


@article{cook_influence,
title = "Characterizations of an empirical influence function for detecting influential cases in regression",
abstract = "Traditionally, most of the effort in fitting full rank linear regression models has centered on the study of the presence, strength and form of relationships between the measured variables. As is now well known, least squares regression computations can be strongly influenced by a few cases, and a fitted model may more accurately reflect unusual features of those cases than the overall relationships between the variables. It is of interest, therefore, for an analyst to be able to find influential cases and, based on them, make decisions concerning their usefulness in a problem at hand. Based on an empirical influence function, we discuss methodologies for assessing the influence of individual or groups of cases on a regression problem. We conclude with an example using data from the Florida Area Cumulus Experiments (FACE) on cloud seeding. {\circledC} 1980 Taylor & Francis Group, LLC.",
keywords = "Cloud seeding, Distance measures, Linear models, Outlier tests, Residual plotting, Robustness",
author = "Cook, {R. Dennis} and Sanford Weisberg",
year = "1980",
doi = "10.1080/00401706.1980.10486199",
language = "English (US)",
volume = "22",
pages = "495--508",
journal = "Technometrics",
issn = "0040-1706",
publisher = "American Statistical Association",
number = "4",
}


@article{conjugate_gradient,
 author = {Shewchuk, Jonathan R},
 title = {An Introduction to the Conjugate Gradient Method Without the Agonizing Pain},
 year = {1994},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai3Ancstrlh3Acmucs3ACMU2F2FCS-94-125},
 publisher = {Carnegie Mellon University},
 journal = {-},
 address = {Pittsburgh, PA, USA},
} 

@article{JMLR:v18:16-491,
  author  = {Naman Agarwal and Brian Bullins and Elad Hazan},
  title   = {Second-Order Stochastic Optimization for Machine Learning in Linear Time},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {116},
  pages   = {1-40},
  url     = {http://jmlr.org/papers/v18/16-491.html}
}


@article{influence2,
  author    = {Pang Wei Koh and
               Kai{-}Siang Ang and
               Hubert H. K. Teo and
               Percy Liang},
  title     = {On the Accuracy of Influence Functions for Measuring Group Effects},
  journal   = {CoRR},
  volume    = {abs/1905.13289},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.13289},
  archivePrefix = {arXiv},
  eprint    = {1905.13289},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1905-13289},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{saliency1,
  author       = "Karen Simonyan and Andrea Vedaldi and Andrew Zisserman",
  title        = "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
  booktitle    = "Workshop at International Conference on Learning Representations",
  year         = "2014",
}

@article{saliency2,
  author    = {Avanti Shrikumar and
               Peyton Greenside and
               Anshul Kundaje},
  title     = {Learning Important Features Through Propagating Activation Differences},
  journal   = {CoRR},
  volume    = {abs/1704.02685},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.02685},
  archivePrefix = {arXiv},
  eprint    = {1704.02685},
  timestamp = {Mon, 13 Aug 2018 16:48:09 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ShrikumarGK17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{saliency3,
  author    = {Daniel Smilkov and
               Nikhil Thorat and
               Been Kim and
               Fernanda B. Vi{\'{e}}gas and
               Martin Wattenberg},
  title     = {SmoothGrad: removing noise by adding noise},
  journal   = {CoRR},
  volume    = {abs/1706.03825},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03825},
  archivePrefix = {arXiv},
  eprint    = {1706.03825},
  timestamp = {Mon, 13 Aug 2018 16:48:36 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SmilkovTKVW17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{highorder,
  author    = {Robins James and Li Lingling and  Tchetgen Eric and  van der Vaart, Aad},
  title     = {Higher order influence functions and minimax estimation of nonlinear functionals},
  journal   = {Probability and Statistics: Essays in Honor of David A. Freedman, 335--421},
  volume    = {abs/1706.03825},
  year      = {2017},
  url       = {https://projecteuclid.org/euclid.imsc/1207580092},
  archivePrefix = {arXiv},
  eprint    = {1706.03825},
  timestamp = {Mon, 13 Aug 2018 16:48:36 +0200},
  biburl    = {https://projecteuclid.org/euclid.imsc/1207580092}
}

@inproceedings{Schulam2019CanYT,
  title={Can You Trust This Prediction? Auditing Pointwise Reliability After Learning},
  author={Peter G. Schulam and Suchi Saria},
  booktitle={AISTATS},
  year={2019}
}


@article{model_fairness,
  author    = {Hao Wang and
               Berk Ustun and
               Fl{\'{a}}vio P. Calmon},
  title     = {Repairing without Retraining: Avoiding Disparate Impact with Counterfactual
               Distributions},
  journal   = {CoRR},
  volume    = {abs/1901.10501},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.10501},
  archivePrefix = {arXiv},
  eprint    = {1901.10501},
  timestamp = {Fri, 07 Jun 2019 10:21:08 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-10501},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{koh2019stronger,
  author = {P. W. Koh and J. Steinhardt and P. Liang},
  journal = {arXiv preprint arXiv:1811.00741},
  title = {Stronger Data Poisoning Attacks Break Data Sanitization Defenses},
  year = {2019},
}


@article{extrapolation,
  author = {David Madras and James Atwood  and Alex D’Amour},
  journal = {ICML Workshop},
  title = {Detecting Extrapolation with Influence Functions},
  year = {2019},
}

@InProceedings{causal,
  title = 	 {Validating Causal Inference Models via Influence Functions},
  author = 	 {Alaa, Ahmed and Van Der Schaar, Mihaela},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {191--201},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/alaa19a/alaa19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/alaa19a.html},
  abstract = 	 {The problem of estimating causal effects of treatments from observational data falls beyond the realm of supervised learning {—} because counterfactual data is inaccessible, we can never observe the true causal effects. In the absence of "supervision", how can we evaluate the performance of causal inference methods? In this paper, we use influence functions {—} the functional derivatives of a loss function {—} to develop a model validation procedure that estimates the estimation error of causal inference methods. Our procedure utilizes a Taylor-like expansion to approximate the loss function of a method on a given dataset in terms of the influence functions of its loss on a "synthesized", proximal dataset with known causal effects. Under minimal regularity assumptions, we show that our procedure is consistent and efficient. Experiments on 77 benchmark datasets show that using our procedure, we can accurately predict the comparative performances of state-of-the-art causal inference methods applied to a given observational study.}
}


@book{perturbation,
 author = {Avrachenkov, Konstantin E. and Filar, Jerzy A. and Howlett, Phil G.},
 title = {Analytic Perturbation Theory and Its Applications},
 year = {2013},
 isbn = {1611973139, 9781611973136},
 publisher = {Society for Industrial and Applied Mathematics},
 address = {Philadelphia, PA, USA},
} 


@article{Pearlmutter,
 author = {Pearlmutter, Barak A.},
 title = {Fast Exact Multiplication by the Hessian},
 journal = {Neural Comput.},
 issue_date = {Jan. 1994},
 volume = {6},
 number = {1},
 month = jan,
 year = {1994},
 issn = {0899-7667},
 pages = {147--160},
 numpages = {14},
 url = {http://dx.doi.org/10.1162/neco.1994.6.1.147},
 doi = {10.1162/neco.1994.6.1.147},
 acmid = {188100},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 


@inproceedings{LeCun:1998:EB:645754.668382,
 author = {LeCun, Yann and Bottou, L{\'e}on and Orr, Genevieve B. and M\"{u}ller, Klaus-Robert},
 title = {Efficient BackProp},
 booktitle = {Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop},
 year = {1998},
 isbn = {3-540-65311-2},
 pages = {9--50},
 numpages = {42},
 url = {http://dl.acm.org/citation.cfm?id=645754.668382},
 acmid = {668382},
 publisher = {Springer-Verlag},
 address = {London, UK, UK},
} 


@article{low_rank,
  author    = {Levent Sagun and
               Utku Evci and
               V. Ugur G{\"{u}}ney and
               Yann N. Dauphin and
               L{\'{e}}on Bottou},
  title     = {Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1706.04454},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.04454},
  archivePrefix = {arXiv},
  eprint    = {1706.04454},
  timestamp = {Mon, 22 Jul 2019 13:15:46 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SagunEGDB17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{mnist,
  added-at = {2010-06-28T21:14:36.000+0200},
  author = {LeCun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
  biburl = {https://www.bibsonomy.org/bibtex/29aa18bc67d862bdb83b6081e5506f050/mhwombat},
  booktitle = {Proceedings of the IEEE},
  citeseerurl = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7665},
  file = {:neural_nets/lecun-98.pdf:PDF;:lecun-98.pdf:PDF},
  groups = {public},
  interhash = {7a82cccacd23cf06b25ff5325a6c86c7},
  intrahash = {9aa18bc67d862bdb83b6081e5506f050},
  keywords = {MSc character_recognition checked mnist network neural},
  pages = {2278--2324},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {Gradient-Based Learning Applied to Document Recognition},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7665},
  username = {mhwombat},
  volume = 86,
  year = 1998
}




@article{cook_inf_2,
  author    = {Cook, R. Dennis and Weisberg Sanford},
  title     = {Residuals and Influence in Regression},
  journal   = {Chapman and Hall},
  volume    = {},
  year      = {1982},
  url       = {http://hdl.handle.net/11299/37076},
  archivePrefix = {},
  eprint    = {},
  timestamp = {},
  biburl    = {http://hdl.handle.net/11299/37076},
  bibsource = {}
}


@Inbook{pearson,
editor="Kirch, Wilhelm",
title="Pearson's Correlation Coefficient",
bookTitle="Encyclopedia of Public Health",
year="2008",
publisher="Springer Netherlands",
address="Dordrecht",
pages="1090--1091",
isbn="978-1-4020-5614-7",
doi="10.1007/978-1-4020-5614-7_2569",
url="https://doi.org/10.1007/978-1-4020-5614-7_2569"
}

@incollection{demograph,
title = {Why Is My Classifier Discriminatory?},
author = {Chen, Irene and Johansson, Fredrik D and Sontag, David},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {3539--3550},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7613-why-is-my-classifier-discriminatory.pdf}
}


@article{credit,
Author = {Arrieta-Ibarra, Imanol and Goff, Leonard and Jiménez-Hernández, Diego and Lanier, Jaron and Weyl, E. Glen},
Title = {Should We Treat Data as Labor? Moving beyond "Free"},
Journal = {AEA Papers and Proceedings},
Volume = {108},
Year = {2018},
Month = {May},
Pages = {38-42},
DOI = {10.1257/pandp.20181003},
URL = {http://www.aeaweb.org/articles?id=10.1257/pandp.20181003}}


@article{multiparty,
  author    = {Jamie Hayes and
               Olga Ohrimenko},
  title     = {Contamination Attacks and Mitigation in Multi-Party Machine Learning},
  journal   = {CoRR},
  volume    = {abs/1901.02402},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.02402},
  archivePrefix = {arXiv},
  eprint    = {1901.02402},
  timestamp = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-02402},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article {sc-Batch,
	author = {Yang, Yuchen and Li, Gang and Qian, Huijun and Wilhelmsen, Kirk C. and Shen, Yin and Li, Yun},
	title = {SMNN: Batch Effect Correction for Single-cell RNA-seq data via Supervised Mutual Nearest Neighbor Detection},
	elocation-id = {672261},
	year = {2019},
	doi = {10.1101/672261},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Motivation An ever-increasing deluge of single-cell RNA-sequencing (scRNA-seq) data has been generated, often involving different time points, laboratories or sequencing protocols. Batch effect correction has been recognized to be indispensable when integrating scRNA-seq data from multiple batches. A recent study proposed an effective correction method based on mutual nearest neighbors (MNN) across batches. However, MNN is unsupervised in that it ignores cluster label information of single cells, which can further improve effectiveness of batch effect correction, particularly under realistic scenarios where true biological differences are not orthogonal to batch effect.Results In this work, we propose SMNN for batch effect correction of scRNA-seq data via supervised mutual nearest neighbor detection. SMNN either takes cluster/cell-type label information as input or infers cell types using scRNA-seq clustering in the absence of such information. It then detects mutual nearest neighbors within matched cell types and corrects batch effect accordingly. Compared to MNN, SMNN provides improved merging within the corresponding cell types across batches and retains more cell type-specific features after correction.Availability and implementation SMNN is implemented in R, and freely available at https://yunliweb.its.unc.edu/SMNN/ and https://github.com/yycunc/SMNNcorrect.Contact yunli{at}med.unc.edu},
	URL = {https://www.biorxiv.org/content/early/2019/06/17/672261},
	eprint = {https://www.biorxiv.org/content/early/2019/06/17/672261.full.pdf},
	journal = {bioRxiv}
}


@InProceedings{pmlr_influence,
  title = 	 {A Swiss Army Infinitesimal Jackknife},
  author = 	 {Giordano, Ryan and Stephenson, William and Liu, Runjing and Jordan, Michael and Broderick, Tamara},
  booktitle = 	 {Proceedings of Machine Learning Research},
  pages = 	 {1139--1147},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {},
  month = 	 {16--18 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/giordano19a/giordano19a.pdf},
  url = 	 {http://proceedings.mlr.press/v89/giordano19a.html},
  abstract = 	 {The error or variability of machine learning algorithms is often assessed by repeatedly refitting a model with different weighted versions of the observed data. The ubiquitous tools of cross-validation (CV) and the bootstrap are examples of this technique. These methods are powerful in large part due to their model agnosticism but can be slow to run on modern, large data sets due to the need to repeatedly re-fit the model. In this work, we use a linear approximation to the dependence of the fitting procedure on the weights, producing results that can be faster than repeated re-fitting by an order of magnitude. This linear approximation is sometimes known as the "infinitesimal jackknife" in the statistics literature, where it is mostly used as a theoretical tool to prove asymptotic results. We provide explicit finite-sample error bounds for the infinitesimal jackknife in terms of a small number of simple, verifiable assumptions. Our results apply whether the weights and data are stochastic or deterministic, and so can be used as a tool for proving the accuracy of the infinitesimal jackknife on a wide variety of problems. As a corollary, we state mild regularity conditions under which our approximation consistently estimates true leave k-out cross-validation for any fixed k. These theoretical results, together with modern automatic differentiation software, support the application of the infinitesimal jackknife to a wide variety of practical problems in machine learning, providing a "Swiss Army infinitesimal jackknife." We demonstrate the accuracy of our methods on a range of simulated and real datasets.}
}


@misc{Dua:2019 ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }


@inproceedings{projection1,
 author = {Liu, Jun and Ye, Jieping},
 title = {Efficient Euclidean Projections in Linear Time},
 booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
 series = {ICML '09},
 year = {2009},
 isbn = {978-1-60558-516-1},
 location = {Montreal, Quebec, Canada},
 pages = {657--664},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1553374.1553459},
 doi = {10.1145/1553374.1553459},
 acmid = {1553459},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Duchi:2008:EPL:1390156.1390191,
 author = {Duchi, John and Shalev-Shwartz, Shai and Singer, Yoram and Chandra, Tushar},
 title = {Efficient Projections Onto the L1-ball for Learning in High Dimensions},
 booktitle = {Proceedings of the 25th International Conference on Machine Learning},
 series = {ICML '08},
 year = {2008},
 isbn = {978-1-60558-205-4},
 location = {Helsinki, Finland},
 pages = {272--279},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1390156.1390191},
 doi = {10.1145/1390156.1390191},
 acmid = {1390191},
 publisher = {ACM},
 address = {New York, NY, USA},
} 


@article{higher_order_loss,
  author    = {Sahil Singla and
               Eric Wallace and
               Shi Feng and
               Soheil Feizi},
  title     = {Understanding Impacts of High-Order Loss Approximations and Features
               in Deep Learning Interpretation},
  journal   = {CoRR},
  volume    = {abs/1902.00407},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.00407},
  archivePrefix = {arXiv},
  eprint    = {1902.00407},
  timestamp = {Mon, 17 Jun 2019 07:28:00 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1902-00407},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{medical_imaging,
  author    = {Alexander Selvikv{\aa}g Lundervold and
               Arvid Lundervold},
  title     = {An overview of deep learning in medical imaging focusing on {MRI}},
  journal   = {CoRR},
  volume    = {abs/1811.10052},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.10052},
  archivePrefix = {arXiv},
  eprint    = {1811.10052},
  timestamp = {Fri, 30 Nov 2018 12:44:28 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1811-10052},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{finance,
 author = {Lin, Wei-Yang and Hu, Ya-Han and Tsai, Chih-Fong},
 title = {Machine Learning in Financial Crisis Prediction: A Survey},
 journal = {Trans. Sys. Man Cyber Part C},
 issue_date = {July 2012},
 volume = {42},
 number = {4},
 month = jul,
 year = {2012},
 issn = {1094-6977},
 pages = {421--436},
 numpages = {16},
 url = {https://doi.org/10.1109/TSMCC.2011.2170420},
 doi = {10.1109/TSMCC.2011.2170420},
 acmid = {2719690},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
} 


@article{Sebastiani:2002:MLA:505282.505283,
 author = {Sebastiani, Fabrizio},
 title = {Machine Learning in Automated Text Categorization},
 journal = {ACM Comput. Surv.},
 issue_date = {March 2002},
 volume = {34},
 number = {1},
 month = mar,
 year = {2002},
 issn = {0360-0300},
 pages = {1--47},
 numpages = {47},
 url = {http://doi.acm.org/10.1145/505282.505283},
 doi = {10.1145/505282.505283},
 acmid = {505283},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Machine learning, text categorization, text classification},
} 


@book{Szeliski:2010:CVA:1941882,
 author = {Szeliski, Richard},
 title = {Computer Vision: Algorithms and Applications},
 year = {2010},
 isbn = {1848829345, 9781848829343},
 edition = {1st},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
} 


@article{Candes:2005:DLP:2263433.2271950,
 author = {Candes, E. J. and Tao, T.},
 title = {Decoding by Linear Programming},
 journal = {IEEE Trans. Inf. Theor.},
 issue_date = {December 2005},
 volume = {51},
 number = {12},
 month = dec,
 year = {2005},
 issn = {0018-9448},
 pages = {4203--4215},
 numpages = {13},
 url = {http://dx.doi.org/10.1109/TIT.2005.858979},
 doi = {10.1109/TIT.2005.858979},
 acmid = {2271950},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {Basis pursuit, Gaussian random matrices, decoding of (random) linear codes, duality in optimization, linear codes, linear programming, principal angles, restricted orthonormality, singular values of random matrices, sparse solutions to underdetermined systems},
} 


@article{Donoho:2006:CS:2263438.2272089,
 author = {Donoho, D. L.},
 title = {Compressed Sensing},
 journal = {IEEE Trans. Inf. Theor.},
 issue_date = {April 2006},
 volume = {52},
 number = {4},
 month = apr,
 year = {2006},
 issn = {0018-9448},
 pages = {1289--1306},
 numpages = {18},
 url = {https://doi.org/10.1109/TIT.2006.871582},
 doi = {10.1109/TIT.2006.871582},
 acmid = {2272089},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {Adaptive sampling, Basis Pursuit, Gel'fand, Quotient-of-a-Subspace theorem, almost-spherical sections of Banach spaces, eigenvalues of random matrices, information-based complexity, integrated sensing and processing, minimum, optimal recovery, sparse solution of linear equations},
} 

@misc{
multistage_influence,
title={{\{}MULTI{\}}-{\{}STAGE{\}} {\{}INFLUENCE{\}} {\{}FUNCTION{\}}},
author={Hongge Chen and Si Si and Yang Li and Ciprian Chelba and Sanjiv Kumar and Duane Boning and Cho-Jui Hsieh},
year={2020},
url={https://openreview.net/forum?id=r1geR1BKPr}
}

@inproceedings{Giordano2018ASA,
  title={A Swiss Army Infinitesimal Jackknife},
  author={Ryan Giordano and Will Stephenson and Runjing Liu and Michael I. Jordan and Tamara Broderick},
  booktitle={AISTATS},
  year={2018}
}

@article{Giordano2019AHS,
  title={A Higher-Order Swiss Army Infinitesimal Jackknife},
  author={Ryan Giordano and Michael I. Jordan and Tamara Broderick},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.12116}
}

@article{influence_word_embeddings,
  author    = {Marc{-}Etienne Brunet and
               Colleen Alkalay{-}Houlihan and
               Ashton Anderson and
               Richard S. Zemel},
  title     = {Understanding the Origins of Bias in Word Embeddings},
  journal   = {CoRR},
  volume    = {abs/1810.03611},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.03611},
  archivePrefix = {arXiv},
  eprint    = {1810.03611},
  timestamp = {Tue, 30 Oct 2018 10:49:09 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-03611},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{l1approximation,
author = {Ramirez, Carlos},
year = {2013},
month = {08},
pages = {203-207},
title = {Why l1 is a good approximation to l0: A geometric explanation},
volume = {7},
journal = {Journal of Uncertain Systems}
}


@article{infinitesimal_jackknife,
author = {Louis A. Jaeckel},
year = {1972},
month = {06},
pages = {1-35},
title = {The Infinitesimal Jackknife},
volume = {1},
journal = {Technical Report}
}

@article{efron_jackknife,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2345949},
 abstract = {This paper shows how to derive more information from a bootstrap analysis, information about the accuracy of the usual bootstrap estimates. Suppose that we observe data x = (x1, x2, ..., xn), compute a statistic of interest s(x) and further compute B bootstrap replications of s, say s(x* 1) s(x* 2), ..., s(x* B), where B is some large number like 1000. Various accuracy measures for s(x) can be obtained from the bootstrap values, e.g. the bootstrap estimates of standard error and bias, or the length and shape of bootstrap confidence intervals. We might wonder how accurate these accuracy measures themselves are, or how sensitive they are to small changes in the individual data points xi. It turns out that these questions can be answered from the information in the original bootstrap sample s* 1,s* 2, ..., s* B, with no further resampling required. The answers, which make use of the jackknife and delta method influence functions, are easy to apply and can give informative results, as shown by several examples.},
 author = {Bradley Efron},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {83--127},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Jackknife-After-Bootstrap Standard Errors and Influence Functions},
 volume = {54},
 year = {1992}
}



@article{representer,
  author    = {Chih{-}Kuan Yeh and
               Joon Sik Kim and
               Ian En{-}Hsu Yen and
               Pradeep Ravikumar},
  title     = {Representer Point Selection for Explaining Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1811.09720},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.09720},
  archivePrefix = {arXiv},
  eprint    = {1811.09720},
  timestamp = {Fri, 30 Nov 2018 12:44:28 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-09720.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}