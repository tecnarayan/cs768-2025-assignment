@string{JASA = {Journal of the American Statistical Association}}
@string{JC  = {Journal of Classification}}
@string{JSPI  = {Journal of Statistical Planning and Inference}}
@string{JRSSB  = {Journal of the Royal Statistical Society: Series B  (Statistical Methodology)}}
@string{SAGMB  = {Statistical Applications in Genetics and Molecular Biology}}
@string{NIPS  = {Advances in Neural Information Processing Systems}}
@string{AOS  = {The Annals of Statistics}}
@string{AOAS  = {Annals of Applied Statistics}}
@string{JMLR  = {Journal of Machine Learning Research}}
@string{EJS  = {Electronic Journal of Statistics}}
@string{AISTATS  = {International Conference on Artificial Intelligence and Statistics}}
@string{UAI  = {Conference on Uncertainty in Artificial Intelligence}}
@string{ICML  = {International Conference on Machine Learning}}
@string{COLT  = {Conference on Learning Theory}}
@string{TIT = {IEEE Transactions on Information Theory}}
@string{ICLR = {International Conference on Learning Representations}}

@article{xu2019sample,
    title={Sample Efficient Policy Gradient Methods with Recursive Variance Reduction},
    author={Xu, Pan and Gao, Felicia and Gu, Quanquan},
    journal={arXiv preprint arXiv:1909.08610},
    year={2019}
}


@book{sutton2018reinforcement,
  title={Reinforcement Learning: An Introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{neuraltd,
  title={Neural Temporal-Difference Learning Converges to Global Optima},
  author={Cai, Qi and Yang, Zhuoran and D. Lee, Jason and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1905.10027 },
  year={2019}
}

@inproceedings{rahimi2009weighted,
  title={Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle=NIPS,
  year={2009}
}

@inproceedings{rahimi2008random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle=NIPS,
  year={2008}
}

@inproceedings{rahimi2008uniform,
  title={Uniform approximation of functions with random bases},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={2008 46th Annual Allerton Conference on Communication, Control, and Computing},
  pages={555--561},
  year={2008},
  organization={IEEE}
}

@article{castro2010convergent,
  title={A convergent online single time scale actor critic algorithm},
  author={Castro, Dotan Di and Meir, Ron},
  journal=JMLR,
  volume={11},
  number={Jan},
  pages={367--410},
  year={2010}
}

@inproceedings{konda2000actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay R and Tsitsiklis, John N},
  booktitle=NIPS,
  year={2000}
}

@inproceedings{kakade2002natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  booktitle=NIPS,
  year={2002}
}

@article{amari1998natural,
  title={Natural gradient works efficiently in learning},
  author={Amari, Shun-Ichi},
  journal={Neural Computation},
  volume={10},
  number={2},
  pages={251--276},
  year={1998},
  publisher={MIT Press}
}

@article{liu2019neural,
  title={Neural Proximal/Trust Region Policy Optimization Attains Globally Optimal Policy},
  author={Liu, Boyi and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1906.10306},
  year={2019}
}

@article{fazel2018global,
  title={Global convergence of policy gradient methods for the linear quadratic regulator},
  author={Fazel, Maryam and Ge, Rong and Kakade, Sham M and Mesbahi, Mehran},
  journal={arXiv preprint arXiv:1801.05039},
  year={2018}
}

@article{malik2018derivative,
  title={Derivative-free methods for policy optimization: Guarantees for linear quadratic systems},
  author={Malik, Dhruv and Pananjady, Ashwin and Bhatia, Kush and Khamaru, Koulik and Bartlett, Peter L and Wainwright, Martin J},
  journal={arXiv preprint arXiv:1812.08305},
  year={2018}
}

@article{tu2018gap,
  title={The gap between model-based and model-free methods on the linear quadratic regulator: An asymptotic viewpoint},
  author={Tu, Stephen and Recht, Benjamin},
  journal={arXiv preprint arXiv:1812.03565},
  year={2018}
}

@article{bu2019lqr,
  title={{LQR} through the Lens of First Order Methods: Discrete-time Case},
  author={Bu, Jingjing and Mesbahi, Afshin and Fazel, Maryam and Mesbahi, Mehran},
  journal={arXiv preprint arXiv:1907.08921},
  year={2019}
}

@article{yang2019global,
  title={On the Global Convergence of Actor-Critic: A Case for Linear Quadratic Regulator with Ergodic Cost},
  author={Yang, Zhuoran and Chen, Yongxin and Hong, Mingyi and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1907.06246},
  year={2019}
}

@article{yang2019theoretical,
  title={A theoretical analysis of deep {Q}-learning},
  author={Yang, Zhuora and Xie, Yuchen and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1901.00137},
  year={2019}
}

@article{munos2008finite,
  title={Finite-time bounds for fitted value iteration},
  author={Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={May},
  pages={815--857},
  year={2008}
}

@inproceedings{munos2005error,
  title={Error bounds for approximate value iteration},
  author={Munos, R{\'e}mi},
  booktitle={Proceedings of the 20th national conference on Artificial intelligence-Volume 2},
  pages={1006--1011},
  year={2005},
  organization={AAAI Press}
}

@article{lazaric2010analysis,
  title={Analysis of classification-based policy iteration algorithms},
  author={Lazaric, Alessandro and Ghavamzadeh, Mohammad and Munos, R{\'e}mi},
  journal=JMLR,
  volume={17},
  number={1},
  pages={583--612},
  year={2016}
}

@article{scherrer2015approximate,
  title={Approximate modified policy iteration and its application to the game of {T}etris},
  author={Scherrer, Bruno and Ghavamzadeh, Mohammad and Gabillon, Victor and Lesner, Boris and Geist, Matthieu},
  journal=JMLR,
  volume={16},
  pages={1629--1676},
  year={2015}
}

@inproceedings{farahmand2010error,
  title={Error propagation for approximate policy and value iteration},
  author={Farahmand, Amir-massoud and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  booktitle=NIPS,
  year={2010}
}

@article{farahmand2016regularized,
  title={Regularized policy iteration with nonparametric function spaces},
  author={Farahmand, Amir-massoud and Ghavamzadeh, Mohammad and Szepesv{\'a}ri, Csaba and Mannor, Shie},
  journal=JMLR,
  volume={17},
  number={1},
  pages={4809--4874},
  year={2016},
  publisher={JMLR. org}
}

@inproceedings{wagner2011reinterpretation,
  title={A reinterpretation of the policy oscillation phenomenon in approximate policy iteration},
  author={Wagner, Paul},
  booktitle=NIPS,
  year={2011}
}

@inproceedings{wagner2013optimistic,
  title={Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result},
  author={Wagner, Paul},
  booktitle=NIPS,
  year={2013}
}

@article{peters2008natural,
  title={Natural actor-critic},
  author={Peters, Jan and Schaal, Stefan},
  journal={Neurocomputing},
  volume={71},
  number={7-9},
  pages={1180--1190},
  year={2008},
  publisher={Elsevier}
}

@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle=NIPS,
  year={2000}
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle=ICML,
  year={2016}
}

@article{espeholt2018impala,
  title={{IMPALA}: Scalable distributed deep-{RL} with importance weighted actor-learner architectures},
  author={Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},
  journal={arXiv preprint arXiv:1802.01561},
  year={2018}
}

@article{xu2017stochastic,
  title={Stochastic variance reduction for policy gradient estimation},
  author={Xu, Tianbing and Liu, Qiang and Peng, Jian},
  journal={arXiv preprint arXiv:1710.06034},
  year={2017}
}

@article{papini2018stochastic,
  title={Stochastic variance-reduced policy gradient},
  author={Papini, Matteo and Binaghi, Damiano and Canonaco, Giuseppe and Pirotta, Matteo and Restelli, Marcello},
  journal={arXiv preprint arXiv:1806.05618},
  year={2018}
}

@inproceedings{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle=NIPS,
  year={2013}
}

@article{yang2019policy,
  title={Policy Optimization with Stochastic Mirror Descent},
  author={Yang, Long and Zhang, Yu},
  journal={arXiv preprint arXiv:1906.10462},
  year={2019}
}

 @article{zhang2019global,
  title={Global Convergence of Policy Gradient Methods to (Almost) Locally Optimal Policies},
  author={Zhang, Kaiqing and Koppel, Alec and Zhu, Hao and Ba{\c{s}}ar, Tamer},
  journal={arXiv preprint arXiv:1906.08383},
  year={2019}
}

@article{sutton1988learning,
  title={Learning to predict by the methods of temporal differences},
  author={Sutton, Richard S},
  journal={Machine Learning},
  volume={3},
  number={1},
  pages={9--44},
  year={1988},
  publisher={Springer}
}
 
@phdthesis{konda2002actor,
  title={Actor-Critic Algorithms},
  author={Konda, Vijaymohan},
  year={2002},
  school={Massachusetts Institute of Technology}
}

@inproceedings{haarnoja2017reinforcement,
  title={Reinforcement learning with deep energy-based policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  booktitle=ICML,
  year={2017}
}

@article{scherrer2013performance,
  title={On the performance bounds of some policy search dynamic programming algorithms},
  author={Scherrer, Bruno},
  journal={arXiv preprint arXiv:1306.0539},
  year={2013}
}

@article{pirotta2015policy,
  title={Policy gradient in {L}ipschitz {M}arkov decision processes},
  author={Pirotta, Matteo and Restelli, Marcello and Bascetta, Luca},
  journal={Machine Learning},
  volume={100},
  number={2-3},
  pages={255--283},
  year={2015},
  publisher={Springer}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle=ICML,
  year={2015}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@book{puterman2014markov,
  title={{M}arkov Decision Processes: Discrete Stochastic Dynamic Programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@article{silver2016mastering,
  title={Mastering the game of {G}o with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}

@inproceedings{szepesvari2005finite,
  title={Finite time bounds for sampling based fitted value iteration},
  author={Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  booktitle=ICML,
  year={2005}
}

@article{antos2008learning,
  title={Learning near-optimal policies with {B}ellman-residual minimization based fitted policy iteration and a single sample path},
  author={Antos, Andr{\'a}s and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  journal={Machine Learning},
  volume={71},
  number={1},
  pages={89--129},
  year={2008},
  publisher={Springer}
}

@article{chen2019information,
  title={Information-Theoretic Considerations in Batch Reinforcement Learning},
  author={Chen, Jinglin and Jiang, Nan},
  journal={arXiv preprint arXiv:1905.00360},
  year={2019}
}

@article{silver2017mastering,
  title={Mastering the game of {G}o without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354},
  year={2017},
  publisher={Nature Publishing Group}
}

@misc{alphastarblog,
  title="{AlphaStar: Mastering the Real-Time Strategy Game StarCraft II}",
  author={Vinyals, Oriol and Babuschkin, Igor and Chung, Junyoung and Mathieu, Michael and Jaderberg, Max and Czarnecki, Wojciech M. and Dudzik, Andrew and Huang, Aja and Georgiev, Petko and Powell, Richard and Ewalds, Timo and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Agapiou, John and Oh, Junhyuk and Dalibard, Valentin and Choi, David and Sifre, Laurent and Sulsky, Yury and Vezhnevets, Sasha and Molloy, James and Cai, Trevor and Budden, David and Paine, Tom and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Pohlen, Toby and Wu, Yuhuai and Yogatama, Dani and Cohen, Julia and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Apps, Chris and Kavukcuoglu, Koray and Hassabis, Demis and Silver, David},
  howpublished={\url{https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/}},
  year={2019}
}

@inproceedings{peters2006policy,
  title={Policy gradient methods for robotics},
  author={Peters, Jan and Schaal, Stefan},
  booktitle={International Conference on Intelligent Robots and Systems},
  pages={2219--2225},
  year={2006}
}

@inproceedings{duan2016benchmarking,
  title={Benchmarking deep reinforcement learning for continuous control},
  author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  booktitle=ICML,
  year={2016}
}

@inproceedings{wang2018deep,
  title={Deep reinforcement learning for {NLP}},
  author={Wang, William Yang and Li, Jiwei and He, Xiaodong},
  booktitle={Association for Computational Linguistics},
  pages={19--21},
  year={2018}
}

@inproceedings{papini2017adaptive,
  title={Adaptive batch size for safe policy gradients},
  author={Papini, Matteo and Pirotta, Matteo and Restelli, Marcello},
  booktitle=NIPS,
  year={2017}
}

@article{karimi2019non,
  title={Non-asymptotic Analysis of Biased Stochastic Approximation Scheme},
  author={Karimi, Belhal and Miasojedow, Blazej and Moulines, Eric and Wai, Hoi-To},
  journal={arXiv preprint arXiv:1902.00629},
  year={2019}
}

@book{nesterov1998introductory,
  title={Lectures on Convex Optimization},
  author={Nesterov, Yurii},
  year={2018},
  publisher={Springer}
}

@inproceedings{shen2019hessian,
  title={Hessian Aided Policy Gradient},
  author={Shen, Zebang and Ribeiro, Alejandro and Hassani, Hamed and Qian, Hui and Mi, Chao},
  booktitle=ICML,
  year={2019}
}

@article{xu2019improved,
  title={An Improved Convergence Analysis of Stochastic Variance-Reduced Policy Gradient},
  author={Xu, Pan and Gao, Felicia and Gu, Quanquan},
  journal={arXiv preprint arXiv:1905.12615},
  year={2019}
}

@article{brandfonbrener2019expected,
  title={On the Expected Dynamics of Nonlinear {TD} Learning},
  author={Brandfonbrener, David and Bruna, Joan},
  journal={arXiv preprint arXiv:1905.12185},
  year={2019}
}

@inproceedings{tsitsiklis1997analysis,
  title={Analysis of temporal-diffference learning with function approximation},
  author={Tsitsiklis, John N and Van Roy, Benjamin},
  booktitle=NIPS,
  year={1997}
}

@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}

@article{neyshabur2018towards,
  title={Towards understanding the role of over-parametrization in generalization of neural networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  journal={arXiv preprint arXiv:1805.12076},
  year={2018}
}

@article{allen2018learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  journal={arXiv preprint arXiv:1811.04918},
  year={2018}
}

@article{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  journal={arXiv preprint arXiv:1901.08584},
  year={2019}
}

@article{fan2019selective,
  title={A selective overview of deep learning},
  author={Fan, Jianqing and Ma, Cong and Zhong, Yiqiao},
  journal={arXiv preprint arXiv:1904.05526},
  year={2019}
}

@inproceedings{daniely2017sgd,
  title={{SGD} learns the conjugate kernel class of the network},
  author={Daniely, Amit},
  booktitle=NIPS,
  year={2017}
}

@article{chizat2018note,
  title={A note on lazy training in supervised differentiable programming},
  author={Chizat, Lenaic and Bach, Francis},
  journal={arXiv preprint arXiv:1812.07956},
  year={2018}
}

@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle=NIPS,
  year={2018}
}

@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={arXiv preprint arXiv:1902.06720},
  year={2019}
}

@article{grondman2012survey,
  title={A survey of actor-critic reinforcement learning: Standard and natural policy gradients},
  author={Grondman, Ivo and Busoniu, Lucian and Lopes, Gabriel AD and Babuska, Robert},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume={42},
  number={6},
  pages={1291--1307},
  year={2012},
  publisher={IEEE}
}

@article{li2017deep,
  title={Deep reinforcement learning: An overview},
  author={Li, Yuxi},
  journal={arXiv preprint arXiv:1701.07274},
  year={2017}
}

@inproceedings{baxter2000direct,
  title={Direct gradient-based reinforcement learning},
  author={Baxter, Jonathan and Bartlett, Peter L},
  booktitle={International Symposium on Circuits and Systems},
  pages={271--274},
  year={2000}
}

@inproceedings{kakade2002approximately,
  title={Approximately Optimal Approximate Reinforcement Learning},
  author={Kakade, Sham and Langford, John},
  booktitle=ICML,
  year={2002}
}

@article{azar2012dynamic,
  title={Dynamic policy programming},
  author={Azar, Mohammad Gheshlaghi and G{\'o}mez, Vicen{\c{c}} and Kappen, Hilbert J},
  journal=JMLR,
  volume={13},
  number={Nov},
  pages={3207--3245},
  year={2012}
}

@inproceedings{lee2017deep,
  title={Deep neural networks as {G}aussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  booktitle=ICLR,
  year={2018}
}

@book{van2014probability,
  title={Probability in High Dimension},
  author={van Handel, Ramon},
  year={2014},
  publisher={Princeton University}
}

@article{samorodnitsky1991probability,
  title={Probability tails of {G}aussian extrema},
  author={Samorodnitsky, Gennady},
  journal={Stochastic processes and their applications},
  volume={38},
  number={1},
  pages={55--84},
  year={1991},
  publisher={Elsevier}
}

@book{kushner2003stochastic,
  title={Stochastic Approximation and Recursive Algorithms and Applications},
  author={Kushner, Harold and Yin, G George},
  volume={35},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@book{borkar2009stochastic,
  title={Stochastic Approximation: A Dynamical Systems Viewpoint},
  author={Borkar, Vivek S},
  volume={48},
  year={2009},
  publisher={Springer}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine Learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{o2016combining,
  title={Combining policy gradient and {Q}-learning},
  author={O'Donoghue, Brendan and Munos, Remi and Kavukcuoglu, Koray and Mnih, Volodymyr},
  journal={arXiv preprint arXiv:1611.01626},
  year={2016}
}

@article{agarwal2019optimality,
  title={Optimality and Approximation with Policy Gradient Methods in {M}arkov Decision Processes},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={arXiv preprint arXiv:1908.00261},
  year={2019}
}

@article{klusowski2016risk,
  title={Risk bounds for high-dimensional ridge function combinations including neural networks},
  author={Klusowski, Jason M and Barron, Andrew R},
  journal={arXiv preprint arXiv:1607.01434},
  year={2016}
}

@inproceedings{pan2016expressiveness,
  title={Expressiveness of rectifier networks},
  author={Pan, Xingyuan and Srikumar, Vivek},
  booktitle=ICML,
  year={2016}
}

@article{funahashi1989approximate,
  title={On the approximate realization of continuous mappings by neural networks},
  author={Funahashi, Ken-Ichi},
  journal={Neural Networks},
  volume={2},
  number={3},
  pages={183--192},
  year={1989},
  publisher={Elsevier}
}

@article{barron1994approximation,
  title={Approximation and estimation bounds for artificial neural networks},
  author={Barron, Andrew R},
  journal={Machine Learning},
  volume={14},
  number={1},
  pages={115--133},
  year={1994},
  publisher={Springer}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@article{du2018gradient1,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon S and Lee, Jason D and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  journal={arXiv preprint arXiv:1811.03804},
  year={2018}
}

@article{allen2018convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  journal={arXiv preprint arXiv:1811.03962},
  year={2018}
}

@article{zou2018stochastic,
  title={Stochastic gradient descent optimizes over-parameterized deep {ReLU} networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:1811.08888},
  year={2018}
}

@article{cao2019generalization,
  title={A generalization theory of gradient descent for learning over-parameterized deep {ReLU} networks},
  author={Cao, Yuan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1902.01384},
  year={2019}
}

@inproceedings{wu2018sgd,
  title={How {SGD} selects the global minima in over-parameterized learning: A dynamical stability perspective},
  author={Wu, Lei and Ma, Chao and Weinan, E},
  booktitle=NIPS,
  year={2018}
}

@article{bhandari2019global,
  title={Global Optimality Guarantees For Policy Gradient Methods},
  author={Bhandari, Jalaj and Russo, Daniel},
  journal={arXiv preprint arXiv:1906.01786},
  year={2019}
}

@article{cao2019bounds,
  title={Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks},
  author={Cao, Yuan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1905.13210},
  year={2019}
}

@inproceedings{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  booktitle=NIPS,
  year={2018}
}

@inproceedings{martens2015optimizing,
  title={Optimizing neural networks with kronecker-factored approximate curvature},
  author={Martens, James and Grosse, Roger},
  booktitle=ICML,
  year={2015}
}

@inproceedings{wu2017scalable,
  title={Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation},
  author={Wu, Yuhuai and Mansimov, Elman and Grosse, Roger B and Liao, Shun and Ba, Jimmy},
  booktitle=NIPS,
  year={2017}
}














%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{de2019causal,
  title={Causal confusion in imitation learning},
  author={de Haan, Pim and Jayaraman, Dinesh and Levine, Sergey},
  booktitle=NIPS,
  year={2019}
}



@book{pearl2009causality,
  title={Causality},
  author={Pearl, Judea},
  year={2009},
  publisher={Cambridge university press}
}

@book{peters2017elements,
  title={Elements of {C}ausal {I}nference: {F}oundations and {L}earning {A}lgorithms},
  author={Peters, Jonas and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year={2017},
  publisher={MIT press}
}

@inproceedings{kallus2018confounding,
  title={Confounding-robust policy improvement},
  author={Kallus, Nathan and Zhou, Angela},
  booktitle=NIPS,
  year={2018}
}


@inproceedings{lattimore2016causal,
  title={Causal bandits: {L}earning good interventions via causal inference},
  author={Lattimore, Finnian and Lattimore, Tor and Reid, Mark D},
  booktitle=NIPS,
  year={2016}
}

@article{lu2019regret,
  title={Regret Analysis of Causal Bandit Problems},
  author={Lu, Yangyi and Meisami, Amirhossein and Tewari, Ambuj and Yan, Zhenyu},
  journal={arXiv preprint arXiv:1910.04938},
  year={2019}
}


@article{lu2018deconfounding,
  title={Deconfounding reinforcement learning in observational settings},
  author={Lu, Chaochao and Sch{\"o}lkopf, Bernhard and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel},
  journal={arXiv preprint arXiv:1812.10576},
  year={2018}
}


@article{levine2020offline,
  title={Offline Reinforcement Learning: {T}utorial, Review, and Perspectives on Open Problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@inproceedings{zhang2019near,
  title={Near-optimal reinforcement learning in dynamic treatment regimes},
  author={Zhang, Junzhe and Bareinboim, Elias},
  booktitle=NIPS,
  year={2019}
}

@article{hallak2015contextual,
  title={Contextual {M}arkov decision processes},
  author={Hallak, Assaf and Di Castro, Dotan and Mannor, Shie},
  journal={arXiv preprint arXiv:1502.02259},
  year={2015}
}

@techreport{aberdeen2003revised,
  title={A (revised) survey of approximate methods for solving partially observable {M}arkov decision processes},
  author={Aberdeen, Douglas},
  year={2003},
  institution={Technical report, {N}ational {ICT} {A}ustralia}
}

@article{diaz2019causal,
  title={Causal mediation analysis for stochastic interventions},
  author={D{\'\i}az, Iv{\'a}n and Hejazi, Nima},
  journal={arXiv preprint arXiv:1901.02776},
  year={2019}
}

@article{jaksch2010near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  journal=JMLR,
  volume={11},
  number={Apr},
  pages={1563--1600},
  year={2010}
}

@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle=ICML,
  year={2017},
}

@article{jin2019provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  journal={arXiv preprint arXiv:1907.05388},
  year={2019}
}

@inproceedings{jin2018q,
  title={Is {Q}-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  booktitle=NIPS,
  year={2018}
}

@article{tennenholtz2019off,
  title={Off-Policy Evaluation in Partially Observable Environments},
  author={Tennenholtz, Guy and Mannor, Shie and Shalit, Uri},
  journal={arXiv preprint arXiv:1909.03739},
  year={2019}
}

@article{munoz2012population,
  title={Population intervention causal effects based on stochastic interventions},
  author={Mu{\~n}oz, Iv{\'a}n D{\'\i}az and van der Laan, Mark},
  journal={Biometrics},
  volume={68},
  number={2},
  pages={541--549},
  year={2012},
  publisher={Wiley Online Library}
}

@article{bradtke1996linear,
  title={Linear least-squares algorithms for temporal difference learning},
  author={Bradtke, Steven J and Barto, Andrew G},
  journal={Machine {L}earning},
  volume={22},
  number={1-3},
  pages={33--57},
  year={1996},
  publisher={Springer}
}

@article{osband2014generalization,
  title={Generalization and exploration via randomized value functions},
  author={Osband, Ian and Van Roy, Benjamin and Wen, Zheng},
  journal={arXiv preprint arXiv:1402.0635},
  year={2014}
}


@inproceedings{yang2019sample,
  title={Sample-optimal parametric {Q}-learning using linearly additive features},
  author={Yang, Lin and Wang, Mengdi},
  booktitle=ICML,
  year={2019}
}

@article{yang2019reinforcement,
  title={Reinforcement leaning in feature space: {M}atrix bandit, kernels, and regret bound},
  author={Yang, Lin F and Wang, Mengdi},
  journal={arXiv preprint arXiv:1905.10389},
  year={2019}
}

@article{cai2019provably,
  title={Provably Efficient Exploration in Policy Optimization},
  author={Cai, Qi and Yang, Zhuoran and Jin, Chi and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1912.05830},
  year={2019}
}

@article{manski1990nonparametric,
  title={Nonparametric bounds on treatment effects},
  author={Manski, Charles F},
  journal={{A}merican {E}conomic {R}eview},
  volume={80},
  number={2},
  pages={319--323},
  year={1990},
  publisher={JSTOR}
}

@inproceedings{zhang2017transfer,
  title={Transfer learning in multi-armed bandit: {A} causal approach},
  author={Zhang, Junzhe and Bareinboim, Elias},
  booktitle={Autonomous {A}gents and {M}ulti-{A}gent {S}ystems},
  year={2017}
}

@article{balke2013counterfactuals,
  title={Counterfactuals and policy analysis in structural models},
  author={Balke, Alexander and Pearl, Judea},
  journal={arXiv preprint arXiv:1302.4929},
  year={2013}
}

@article{tan2006distributional,
  title={A distributional approach for causal inference using propensity scores},
  author={Tan, Zhiqiang},
  journal={{J}ournal of the {A}merican {S}tatistical {A}ssociation},
  volume={101},
  number={476},
  pages={1619--1637},
  year={2006},
  publisher={Taylor \& Francis}
}

@inproceedings{abbasi2011improved,
  title={Improved algorithms for linear stochastic bandits},
  author={Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
  booktitle=NIPS,
  year={2011}
}


@inproceedings{forney2017counterfactual,
  title={Counterfactual data-fusion for online reinforcement learners},
  author={Forney, Andrew and Pearl, Judea and Bareinboim, Elias},
  booktitle=ICML,
  year={2017}
}

@article{buesing2018woulda,
  title={Woulda, coulda, shoulda: {C}ounterfactually-guided policy search},
  author={Buesing, Lars and Weber, Theophane and Zwols, Yori and Racaniere, Sebastien and Guez, Arthur and Lespiau, Jean-Baptiste and Heess, Nicolas},
  journal={arXiv preprint arXiv:1811.06272},
  year={2018}
}

@article{kallus2018policy,
  title={Policy evaluation and optimization with continuous treatments},
  author={Kallus, Nathan and Zhou, Angela},
  journal={arXiv preprint arXiv:1802.06037},
  year={2018}
}

@inproceedings{sen2017identifying,
  title={Identifying best interventions through online importance sampling},
  author={Sen, Rajat and Shanmugam, Karthikeyan and Dimakis, Alexandres G and Shakkottai, Sanjay},
  booktitle=ICML,
  year={2017}
}


@inproceedings{auer2007logarithmic,
  title={Logarithmic online regret bounds for undiscounted reinforcement learning},
  author={Auer, Peter and Ortner, Ronald},
  booktitle=NIPS,
  year={2007}
}

@inproceedings{hessel2018rainbow,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  booktitle={{AAAI} {C}onference on {A}rtificial {I}ntelligence},
  year={2018}
}

@article{li2016deep,
  title={Deep reinforcement learning for dialogue generation},
  author={Li, Jiwei and Monroe, Will and Ritter, Alan and Galley, Michel and Gao, Jianfeng and Jurafsky, Dan},
  journal={arXiv preprint arXiv:1606.01541},
  year={2016}
}

@article{kober2013reinforcement,
  title={Reinforcement learning in robotics: {A}survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal={{I}nternational {J}ournal of {R}obotics {R}esearch},
  volume={32},
  number={11},
  pages={1238--1274},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}




@article{li2020make,
  title={Who Make Drivers Stop? {T}owards Driver-centric Risk Assessment: {R}isk Object Identification via Causal Inference},
  author={Li, Chengxi and Chan, Stanley H and Chen, Yi-Ting},
  journal={arXiv preprint arXiv:2003.02425},
  year={2020}
}

@article{bareinboim2016causal,
  title={Causal inference and the data-fusion problem},
  author={Bareinboim, Elias and Pearl, Judea},
  journal={Proceedings of the {N}ational {A}cademy of {S}ciences},
  volume={113},
  number={27},
  pages={7345--7352},
  year={2016},
  publisher={National Acad Sciences}
}

@article{murphy2003optimal,
  title={Optimal dynamic treatment regimes},
  author={Murphy, Susan A},
  journal={Journal of the {R}oyal {S}tatistical {S}ociety: {S}eries {B} ({S}tatistical {M}ethodology)},
  volume={65},
  number={2},
  pages={331--355},
  year={2003},
  publisher={Wiley Online Library}
}

@article{chakraborty2014dynamic,
  title={Dynamic treatment regimes},
  author={Chakraborty, Bibhas and Murphy, Susan A},
  journal={Annual {R}eview of {S}tatistics and {I}ts {A}pplication},
  volume={1},
  pages={447--464},
  year={2014},
  publisher={Annual Reviews}
}

@article{vershynin2010introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Vershynin, Roman},
  journal={arXiv preprint arXiv:1011.3027},
  year={2010}
}