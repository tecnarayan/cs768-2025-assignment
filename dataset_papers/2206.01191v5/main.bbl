\begin{thebibliography}{10}

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{dosovitskiy2020vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em ICLR}, 2021.

\bibitem{touvron2021training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In {\em International Conference on Machine Learning}, pages
  10347--10357. PMLR, 2021.

\bibitem{touvron2022deit}
Hugo Touvron, Matthieu Cord, and Herv{\'e} J{\'e}gou.
\newblock Deit iii: Revenge of the vit.
\newblock {\em arXiv preprint arXiv:2204.07118}, 2022.

\bibitem{touvron2022three}
Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Jakob Verbeek, and Herv{\'e}
  J{\'e}gou.
\newblock Three things everyone should know about vision transformers.
\newblock {\em arXiv preprint arXiv:2203.09795}, 2022.

\bibitem{yu2021metaformer}
Weihao Yu, Mi~Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi
  Feng, and Shuicheng Yan.
\newblock Metaformer is actually what you need for vision.
\newblock {\em arXiv preprint arXiv:2111.11418}, 2021.

\bibitem{meng2021adavit}
Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang Jiang,
  and Ser-Nam Lim.
\newblock Adavit: Adaptive vision transformers for efficient image recognition.
\newblock {\em arXiv preprint arXiv:2111.15668}, 2021.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 10012--10022, 2021.

\bibitem{jaszczur2021sparse}
Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, Lukasz Kaiser,
  Wojciech Gajewski, Henryk Michalewski, and Jonni Kanerva.
\newblock Sparse is enough in scaling transformers.
\newblock {\em Advances in Neural Information Processing Systems},
  34:9895--9907, 2021.

\bibitem{liu2021swin2}
Ze~Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue
  Cao, Zheng Zhang, Li~Dong, et~al.
\newblock Swin transformer v2: Scaling up capacity and resolution.
\newblock {\em arXiv preprint arXiv:2111.09883}, 2021.

\bibitem{liu2021video}
Ze~Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu.
\newblock Video swin transformer.
\newblock {\em arXiv preprint arXiv:2106.13230}, 2021.

\bibitem{caron2021emerging}
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv{\'e} J{\'e}gou, Julien Mairal,
  Piotr Bojanowski, and Armand Joulin.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9650--9660, 2021.

\bibitem{xie2021segformer}
Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose~M Alvarez, and Ping
  Luo.
\newblock Segformer: Simple and efficient design for semantic segmentation with
  transformers.
\newblock {\em arXiv preprint arXiv:2105.15203}, 2021.

\bibitem{cheng2021masked}
Bowen Cheng, Ishan Misra, Alexander~G Schwing, Alexander Kirillov, and Rohit
  Girdhar.
\newblock Masked-attention mask transformer for universal image segmentation.
\newblock {\em arXiv preprint arXiv:2112.01527}, 2021.

\bibitem{carion2020end}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
  Kirillov, and Sergey Zagoruyko.
\newblock End-to-end object detection with transformers.
\newblock In {\em European conference on computer vision}, pages 213--229.
  Springer, 2020.

\bibitem{li2021improved}
Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo~Xiong, Jitendra
  Malik, and Christoph Feichtenhofer.
\newblock Improved multiscale vision transformers for classification and
  detection.
\newblock {\em arXiv preprint arXiv:2112.01526}, 2021.

\bibitem{wang2022towards}
Xudong Wang, Li~Lyna Zhang, Yang Wang, and Mao Yang.
\newblock Towards efficient vision transformer inference: a first study of
  transformers on mobile devices.
\newblock In {\em Proceedings of the 23rd Annual International Workshop on
  Mobile Computing Systems and Applications}, pages 1--7, 2022.

\bibitem{mehta2021mobilevit}
Sachin Mehta and Mohammad Rastegari.
\newblock Mobilevit: Light-weight, general-purpose, and mobile-friendly vision
  transformer.
\newblock {\em arXiv preprint arXiv:2110.02178}, 2021.

\bibitem{liu2018efficient}
Xingyu Liu, Jeff Pool, Song Han, and William~J Dally.
\newblock Efficient sparse-winograd convolutional neural networks.
\newblock {\em arXiv preprint arXiv:1802.06367}, 2018.

\bibitem{howard2017mobilenets}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{sandler2018mobilenetv2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4510--4520, 2018.

\bibitem{howard2019searching}
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo~Chen, Mingxing
  Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et~al.
\newblock Searching for mobilenetv3.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1314--1324, 2019.

\bibitem{Graham_2021_ICCV}
Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin,
  Herve Jegou, and Matthijs Douze.
\newblock Levit: A vision transformer in convnet's clothing for faster
  inference.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pages 12259--12269, October 2021.

\bibitem{chen2021mobile}
Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu~Yuan,
  and Zicheng Liu.
\newblock Mobile-former: Bridging mobilenet and transformer.
\newblock {\em arXiv preprint arXiv:2108.05895}, 2021.

\bibitem{wu2021smart}
Chuhan Wu, Fangzhao Wu, Tao Qi, Binxing Jiao, Daxin Jiang, Yongfeng Huang, and
  Xing Xie.
\newblock Smart bird: Learnable sparse attention for efficient and effective
  transformer.
\newblock {\em arXiv preprint arXiv:2108.09193}, 2021.

\bibitem{roh2021sparse}
Byungseok Roh, JaeWoong Shin, Wuhyun Shin, and Saehoon Kim.
\newblock Sparse detr: Efficient end-to-end object detection with learnable
  sparsity.
\newblock {\em arXiv preprint arXiv:2111.14330}, 2021.

\bibitem{zhu2020deformable}
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
\newblock Deformable detr: Deformable transformers for end-to-end object
  detection.
\newblock {\em arXiv preprint arXiv:2010.04159}, 2020.

\bibitem{gong2022nasvit}
Chengyue Gong, Dilin Wang, Meng Li, Xinlei Chen, Zhicheng Yan, Yuandong Tian,
  qiang liu, and Vikas Chandra.
\newblock {NASV}it: Neural architecture search for efficient vision
  transformers with gradient conflict aware supernet training.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{chavan2022vision}
Arnav Chavan, Zhiqiang Shen, Zhuang Liu, Zechun Liu, Kwang-Ting Cheng, and Eric
  Xing.
\newblock Vision transformer slimming: Multi-dimension searching in continuous
  optimization space.
\newblock 2022.

\bibitem{coreml2021}
CoreMLTools.
\newblock Use coremltools to convert models from third-party libraries to core
  ml., 2021.

\bibitem{ma2018shufflenet}
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
\newblock Shufflenet v2: Practical guidelines for efficient cnn architecture
  design.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 116--131, 2018.

\bibitem{tan2019mnasnet}
Mingxing Tan, Bo~Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew
  Howard, and Quoc~V Le.
\newblock Mnasnet: Platform-aware neural architecture search for mobile.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 2820--2828, 2019.

\bibitem{wang2020hat}
Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, and
  Song Han.
\newblock Hat: Hardware-aware transformers for efficient natural language
  processing.
\newblock {\em arXiv preprint arXiv:2005.14187}, 2020.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{yuan2021tokens}
Li~Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang,
  Francis~EH Tay, Jiashi Feng, and Shuicheng Yan.
\newblock Tokens-to-token vit: Training vision transformers from scratch on
  imagenet.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 558--567, 2021.

\bibitem{wang2021pyramid}
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong
  Lu, Ping Luo, and Ling Shao.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 568--578, 2021.

\bibitem{touvron2021going}
Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and
  Herv{\'e} J{\'e}gou.
\newblock Going deeper with image transformers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 32--42, 2021.

\bibitem{guo2021cmt}
Jianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe
  Wang.
\newblock Cmt: Convolutional neural networks meet vision transformers.
\newblock {\em arXiv preprint arXiv:2107.06263}, 2021.

\bibitem{dai2021coatnet}
Zihang Dai, Hanxiao Liu, Quoc~V Le, and Mingxing Tan.
\newblock Coatnet: Marrying convolution and attention for all data sizes.
\newblock {\em Advances in Neural Information Processing Systems},
  34:3965--3977, 2021.

\bibitem{han2021connection}
Qi~Han, Zejia Fan, Qi~Dai, Lei Sun, Ming-Ming Cheng, Jiaying Liu, and Jingdong
  Wang.
\newblock On the connection between local attention and dynamic depth-wise
  convolution.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{zhang2022nested}
Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Sercan Arik, and Tomas Pfister.
\newblock Nested hierarchical transformer: Towards accurate, data-efficient and
  interpretable visual understanding.
\newblock 2022.

\bibitem{zhang2022topformer}
Wenqiang Zhang, Zilong Huang, Guozhong Luo, Tao Chen, Xinggang Wang, Wenyu Liu,
  Gang Yu, and Chunhua Shen.
\newblock Topformer: Token pyramid transformer for mobile semantic
  segmentation, 2022.

\bibitem{lee2021vision}
Seung~Hoon Lee, Seunghyun Lee, and Byung~Cheol Song.
\newblock Vision transformer for small-size datasets.
\newblock {\em arXiv preprint arXiv:2112.13492}, 2021.

\bibitem{lee2021vitgan}
Kwonjoon Lee, Huiwen Chang, Lu~Jiang, Han Zhang, Zhuowen Tu, and Ce~Liu.
\newblock Vitgan: Training gans with vision transformers.
\newblock {\em arXiv preprint arXiv:2107.04589}, 2021.

\bibitem{esser2021taming}
Patrick Esser, Robin Rombach, and Bjorn Ommer.
\newblock Taming transformers for high-resolution image synthesis.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 12873--12883, 2021.

\bibitem{zeng2021improving}
Yanhong Zeng, Huan Yang, Hongyang Chao, Jianbo Wang, and Jianlong Fu.
\newblock Improving visual quality of image synthesis by a token-based
  generator with transformers.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{touvron2021resmlp}
Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin
  El-Nouby, Edouard Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve,
  Jakob Verbeek, et~al.
\newblock Resmlp: Feedforward networks for image classification with
  data-efficient training.
\newblock {\em arXiv preprint arXiv:2105.03404}, 2021.

\bibitem{tolstikhin2021mixer}
Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai,
  Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob
  Uszkoreit, Mario Lucic, and Alexey Dosovitskiy.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock {\em arXiv preprint arXiv:2105.01601}, 2021.

\bibitem{tan2019efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In {\em International conference on machine learning}, pages
  6105--6114. PMLR, 2019.

\bibitem{tolstikhin2021mlp}
Ilya~O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua
  Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers,
  Jakob Uszkoreit, et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{chen2021cyclemlp}
Shoufa Chen, Enze Xie, Chongjian Ge, Ding Liang, and Ping Luo.
\newblock Cyclemlp: A mlp-like architecture for dense prediction.
\newblock {\em arXiv preprint arXiv:2107.10224}, 2021.

\bibitem{zhou2021deepvit}
Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang
  Jiang, Qibin Hou, and Jiashi Feng.
\newblock Deepvit: Towards deeper vision transformer.
\newblock {\em arXiv preprint arXiv:2103.11886}, 2021.

\bibitem{Nikita2020}
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In {\em {ICLR}}. OpenReview.net, 2020.

\bibitem{chen2021crossvit}
Chun-Fu~Richard Chen, Quanfu Fan, and Rameswar Panda.
\newblock Crossvit: Cross-attention multi-scale vision transformer for image
  classification.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 357--366, 2021.

\bibitem{hassani2021escaping}
Ali Hassani, Steven Walton, Nikhil Shah, Abulikemu Abuduweili, Jiachen Li, and
  Humphrey Shi.
\newblock Escaping the big data paradigm with compact transformers.
\newblock {\em arXiv preprint arXiv:2104.05704}, 2021.

\bibitem{fayyaz2021ats}
Mohsen Fayyaz, Soroush~Abbasi Kouhpayegani, Farnoush~Rezaei Jafari, Eric
  Sommerlade, Hamid Reza~Vaezi Joze, Hamed Pirsiavash, and Juergen Gall.
\newblock Ats: Adaptive token sampling for efficient vision transformers.
\newblock {\em arXiv preprint arXiv:2111.15667}, 2021.

\bibitem{Wei2022}
Wei Li, Xing Wang, Xin Xia, Jie Wu, Xuefeng Xiao, Min Zheng, and Shiping Wen.
\newblock Sepvit: Separable vision transformer.
\newblock {\em CoRR}, abs/2203.15380, 2022.

\bibitem{Renggli2022}
C{\'{e}}dric Renggli, Andr{\'{e}}~Susano Pinto, Neil Houlsby, Basil Mustafa,
  Joan Puigcerver, and Carlos Riquelme.
\newblock Learning to merge tokens in vision transformers.
\newblock {\em CoRR}, abs/2202.12015, 2022.

\bibitem{wang2021crossformer}
Wenxiao Wang, Lu~Yao, Long Chen, Binbin Lin, Deng Cai, Xiaofei He, and Wei Liu.
\newblock Crossformer: A versatile vision transformer hinging on cross-scale
  attention.
\newblock {\em arXiv preprint arXiv:2108.00154}, 2021.

\bibitem{heo2021rethinking}
Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and
  Seong~Joon Oh.
\newblock Rethinking spatial dimensions of vision transformers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 11936--11945, 2021.

\bibitem{chen2021regionvit}
Chun-Fu Chen, Rameswar Panda, and Quanfu Fan.
\newblock Regionvit: Regional-to-local attention for vision transformers.
\newblock {\em arXiv preprint arXiv:2106.02689}, 2021.

\bibitem{li2021localvit}
Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van~Gool.
\newblock Localvit: Bringing locality to vision transformers.
\newblock {\em arXiv preprint arXiv:2104.05707}, 2021.

\bibitem{chu2021twins}
Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo~Zhang, Haibing Ren, Xiaolin Wei,
  Huaxia Xia, and Chunhua Shen.
\newblock Twins: Revisiting spatial attention design in vision transformers.
\newblock {\em arXiv e-prints}, pages arXiv--2104, 2021.

\bibitem{rao2021dynamicvit}
Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh.
\newblock Dynamicvit: Efficient vision transformers with dynamic token
  sparsification.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem{Zhengzhong2022}
Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan
  Bovik, and Yinxiao Li.
\newblock Maxvit: Multi-axis vision transformer.
\newblock {\em CoRR}, abs/2204.01697, 2022.

\bibitem{chen2021autoformer}
Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin Ling.
\newblock Autoformer: Searching transformers for visual recognition.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 12270--12280, 2021.

\bibitem{zhou2022training}
Qinqin Zhou, Kekai Sheng, Xiawu Zheng, Ke~Li, Xing Sun, Yonghong Tian, Jie
  Chen, and Rongrong Ji.
\newblock Training-free transformer architecture search.
\newblock {\em arXiv preprint arXiv:2203.12217}, 2022.

\bibitem{wu2021cvt}
Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu~Yuan, and Lei
  Zhang.
\newblock Cvt: Introducing convolutions to vision transformers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 22--31, 2021.

\bibitem{yuan2021incorporating}
Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu.
\newblock Incorporating convolution designs into visual transformers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 579--588, 2021.

\bibitem{trockman2022patches}
Asher Trockman and J~Zico Kolter.
\newblock Patches are all you need?
\newblock {\em arXiv preprint arXiv:2201.09792}, 2022.

\bibitem{yang2018netadapt}
Tien-Ju Yang, Andrew Howard, Bo~Chen, Xiao Zhang, Alec Go, Mark Sandler,
  Vivienne Sze, and Hartwig Adam.
\newblock Netadapt: Platform-aware neural network adaptation for mobile
  applications.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 285--300, 2018.

\bibitem{liu2018darts}
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
\newblock Darts: Differentiable architecture search.
\newblock {\em arXiv preprint arXiv:1806.09055}, 2018.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{rw2019timm}
Ross Wightman.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{radosavovic2020designing}
Ilija Radosavovic, Raj~Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr
  Doll{\'a}r.
\newblock Designing network design spaces.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10428--10436, 2020.

\bibitem{he2017mask}
Kaiming He, Georgia Gkioxari, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Mask r-cnn.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2961--2969, 2017.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em European conference on computer vision}, pages 740--755.
  Springer, 2014.

\bibitem{zhou2017scene}
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio
  Torralba.
\newblock Scene parsing through ade20k dataset.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2017.

\bibitem{zhou2019semantic}
Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso,
  and Antonio Torralba.
\newblock Semantic understanding of scenes through the ade20k dataset.
\newblock {\em International Journal of Computer Vision}, 127(3):302--321,
  2019.

\bibitem{kirillov2019panoptic}
Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Doll{\'a}r.
\newblock Panoptic feature pyramid networks.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 6399--6408, 2019.

\end{thebibliography}
