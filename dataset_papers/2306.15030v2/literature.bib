@article{tong2023conditional,
  title={Conditional Flow Matching: Simulation-Free Dynamic Optimal Transport},
  author={Tong, Alexander and Malkin, Nikolay and Huguet, Guillaume and Zhang, Yanlei and Rector-Brooks, Jarrid and Fatras, Kilian and Wolf, Guy and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2302.00482},
  year={2023}
}


@inproceedings{
lipman2022flow,
title={Flow Matching for Generative Modeling},
author={Yaron Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian Nickel and Matthew Le},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=PqvMRDCJT9t}
}

@article{kohler2023flow,
  title={Flow-Matching: Efficient Coarse-Graining of Molecular Dynamics without Forces},
  author={K{\"o}hler, Jonas and Chen, Yaoyi and Kr{\"a}mer, Andreas and Clementi, Cecilia and No{\'e}, Frank},
  journal={Journal of Chemical Theory and Computation},
  volume={19},
  number={3},
  pages={942--952},
  year={2023},
  publisher={ACS Publications}
}


@inproceedings{kohler2020equivariant,
  title={Equivariant flows: exact likelihood generative learning for symmetric densities},
  author={K{\"o}hler, Jonas and Klein, Leon and No{\'e}, Frank},
  booktitle={International conference on machine learning},
  pages={5361--5370},
  year={2020},
  organization={PMLR}
}

@article{kohler2019equivariant,
  title={Equivariant flows: sampling configurations for multi-body systems with symmetric energies},
  author={K{\"o}hler, Jonas and Klein, Leon and No{\'e}, Frank},
  journal={arXiv preprint arXiv:1910.00753},
  year={2019}
}

@article{chen2019neural,
  title={Neural networks with cheap differential operators},
  author={Chen, Ricky TQ and Duvenaud, David K},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@inproceedings{
klein2023timewarp,
title={Timewarp: Transferable Acceleration of Molecular Dynamics by Learning Time-Coarsened Dynamics},
author={Leon Klein and Andrew Y. K. Foong and Tor Erlend Fjelde and Bruno Kacper Mlodozeniec and Marc Brockschmidt and Sebastian Nowozin and Frank Noe and Ryota Tomioka},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=EjMLpTgvKH}
}

@article{Huang2020AugmentedNF,
  title={Augmented Normalizing Flows: Bridging the Gap Between Generative Flows and Latent Variable Models},
  author={C. Huang and Laurent Dinh and Aaron C. Courville},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.07101}
}


@InProceedings{pmlr-v119-kohler20a,
  title = 	 {Equivariant Flows: Exact Likelihood Generative Learning for Symmetric Densities},
  author =       {K{\"o}hler, Jonas and Klein, Leon and Noé, Frank},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {5361--5370},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/kohler20a/kohler20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/kohler20a.html},
}


@article{noe2019boltzmann,
  title={Boltzmann generators --- sampling equilibrium states of many-body systems with deep learning},
  author={No{\'e}, Frank and Olsson, Simon and K{\"o}hler, Jonas and Wu, Hao},
  journal={Science},
  volume={365},
  issue={6457},
  pages={eaaw1147},
  year={2019}
}

@article{duane1987hybrid,
  title={Hybrid monte carlo},
  author={Duane, Simon and Kennedy, Anthony D and Pendleton, Brian J and Roweth, Duncan},
  journal={Physics letters B},
  volume={195},
  number={2},
  pages={216--222},
  year={1987},
  publisher={Elsevier}
}

@article{geman1984stochastic,
  title={Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images},
  author={Geman, Stuart and Geman, Donald},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  number={6},
  pages={721--741},
  year={1984},
  publisher={IEEE}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@article {Watson2022.12.09.519842,
	author = {Watson, Joseph L. and Juergens, David and Bennett, Nathaniel R. and Trippe, Brian L. and Yim, Jason and Eisenach, Helen E. and Ahern, Woody and Borst, Andrew J. and Ragotte, Robert J. and Milles, Lukas F. and Wicky, Basile I. M. and Hanikel, Nikita and Pellock, Samuel J. and Courbet, Alexis and Sheffler, William and Wang, Jue and Venkatesh, Preetham and Sappington, Isaac and Torres, Susana V{\'a}zquez and Lauko, Anna and De Bortoli, Valentin and Mathieu, Emile and Barzilay, Regina and Jaakkola, Tommi S. and DiMaio, Frank and Baek, Minkyung and Baker, David},
	title = {Broadly applicable and accurate protein design by integrating structure prediction networks and diffusion generative models},
	elocation-id = {2022.12.09.519842},
	year = {2022},
	doi = {10.1101/2022.12.09.519842},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {There has been considerable recent progress in designing new proteins using deep learning methods1{\textendash}9. Despite this progress, a general deep learning framework for protein design that enables solution of a wide range of design challenges, including de novo binder design and design of higher order symmetric architectures, has yet to be described. Diffusion models10,11 have had considerable success in image and language generative modeling but limited success when applied to protein modeling, likely due to the complexity of protein backbone geometry and sequence-structure relationships. Here we show that by fine tuning the RoseTTAFold structure prediction network on protein structure denoising tasks, we obtain a generative model of protein backbones that achieves outstanding performance on unconditional and topology-constrained protein monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding, and symmetric motif scaffolding for therapeutic and metal-binding protein design. We demonstrate the power and generality of the method, called RoseTTAFold Diffusion (RFdiffusion), by experimentally characterizing the structures and functions of hundreds of new designs. In a manner analogous to networks which produce images from user-specified inputs, RFdiffusion enables the design of diverse, complex, functional proteins from simple molecular specifications.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2022/12/10/2022.12.09.519842},
	eprint = {https://www.biorxiv.org/content/early/2022/12/10/2022.12.09.519842.full.pdf},
	journal = {bioRxiv}
}

@article {Ingraham2022.12.01.518682,
	author = {Ingraham, John and Baranov, Max and Costello, Zak and Frappier, Vincent and Ismail, Ahmed and Tie, Shan and Wang, Wujie and Xue, Vincent and Obermeyer, Fritz and Beam, Andrew and Grigoryan, Gevorg},
	title = {Illuminating protein space with a programmable generative model},
	elocation-id = {2022.12.01.518682},
	year = {2022},
	doi = {10.1101/2022.12.01.518682},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Three billion years of evolution have produced a tremendous diversity of protein molecules, and yet the full potential of this molecular class is likely far greater. Accessing this potential has been challenging for computation and experiments because the space of possible protein molecules is much larger than the space of those likely to host function. Here we introduce Chroma, a generative model for proteins and protein complexes that can directly sample novel protein structures and sequences and that can be conditioned to steer the generative process towards desired properties and functions. To enable this, we introduce a diffusion process that respects the conformational statistics of polymer ensembles, an efficient neural architecture for molecular systems based on random graph neural networks that enables long-range reasoning with sub-quadratic scaling, equivariant layers for efficiently synthesizing 3D structures of proteins from predicted inter-residue geometries, and a general low-temperature sampling algorithm for diffusion models. We suggest that Chroma can effectively realize protein design as Bayesian inference under external constraints, which can involve symmetries, substructure, shape, semantics, and even natural language prompts. With this unified approach, we hope to accelerate the prospect of programming protein matter for human health, materials science, and synthetic biology.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2022/12/02/2022.12.01.518682},
	eprint = {https://www.biorxiv.org/content/early/2022/12/02/2022.12.01.518682.full.pdf},
	journal = {bioRxiv}
}

@inproceedings{10.1145/3394486.3406703,
author = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
title = {DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3406703},
doi = {10.1145/3394486.3406703},
abstract = {Explore new techniques in Microsoft's open source library called DeepSpeed, which advances large model training by improving scale, speed, cost, and usability, unlocking the ability to train 100-billion-parameter models. DeepSpeed is compatible with PyTorch. One piece of our library, called ZeRO, is a new parallelized optimizer that greatly reduces the resources needed for model and data parallelism while massively increasing the number of parameters that can be trained. Researchers have used these breakthroughs to create Turing Natural Language Generation (Turing-NLG), which at the time of its release was the largest publicly known language model at 17 billion parameters. In addition we will also go over our latest transformer kernel advancements that led the DeepSpeed team to achieve the world fastest BERT pretraining record.The Zero Redundancy Optimizer (ZeRO) is a novel memory optimization technology for large-scale distributed deep learning. ZeRO can train deep learning models with over 100 billion parameters on the current generation of GPU clusters at three to five times the throughput of the current best system. It also presents a clear path to training models with trillions of parameters, demonstrating an unprecedented leap in deep learning system technology.DeepSpeed brings state-of-the-art training techniques, such as ZeRO, optimized kernels, distributed training, mixed precision, and checkpointing, through lightweight APIs compatible with PyTorch. With just a few lines of code changes to your PyTorch model, you can leverage DeepSpeed to address underlying performance challenges and boost the speed and scale of your training.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {3505–3506},
numpages = {2},
keywords = {distributed deep learning, machine learning},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{tsai2019transformer,
    title = "Transformer Dissection: An Unified Understanding for Transformer{'}s Attention via the Lens of Kernel",
    author = "Tsai, Yao-Hung Hubert  and
      Bai, Shaojie  and
      Yamada, Makoto  and
      Morency, Louis-Philippe  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1443",
    doi = "10.18653/v1/D19-1443",
    pages = "4344--4353",
    abstract = "Transformer is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction. At the core of the Transformer is the attention mechanism, which concurrently processes all inputs in the streams. In this paper, we present a new formulation of attention via the lens of the kernel. To be more precise, we realize that the attention can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs. This new formulation gives us a better way to understand individual components of the Transformer{'}s attention, such as the better way to integrate the positional embedding. Another important advantage of our kernel-based formulation is that it paves the way to a larger space of composing Transformer{'}s attention. As an example, we propose a new variant of Transformer{'}s attention which models the input as a product of symmetric kernels. This approach achieves competitive performance to the current state of the art model with less computation. In our experiments, we empirically study different kernel construction strategies on two widely used tasks: neural machine translation and sequence prediction.",
}

@article{wu2018deep,
  title={Deep generative {M}arkov state models},
  author={Wu, Hao and Mardt, Andreas and Pasquali, Luca and Noé, Frank},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{titsias2019gradient,
  title={Gradient-based adaptive {M}arkov chain {M}onte {C}arlo},
  author={Titsias, Michalis and Dellaportas, Petros},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{swope2004describing,
  title={Describing protein folding kinetics by molecular dynamics simulations. 1. Theory},
  author={Swope, William C and Pitera, Jed W and Suits, Frank},
  journal={The Journal of Physical Chemistry B},
  volume={108},
  number={21},
  pages={6571--6581},
  year={2004}
}

@article{prinz2011markov,
  title={Markov models of molecular kinetics: Generation and validation},
  author={Prinz, Jan-Hendrik and Wu, Hao and Sarich, Marco and Keller, Bettina and Senne, Martin and Held, Martin and Chodera, John D and Sch{\"u}tte, Christof and No{\'e}, Frank},
  journal={The Journal of chemical physics},
  volume={134},
  number={17},
  pages={174105},
  year={2011},
  publisher={American Institute of Physics}
}

@article{plattner2017complete,
  title={Complete protein--protein association kinetics in atomic detail revealed by molecular dynamics simulations and {M}arkov modelling},
  author={Plattner, Nuria and Doerr, Stefan and De Fabritiis, Gianni and No{\'e}, Frank},
  journal={Nature chemistry},
  volume={9},
  number={10},
  pages={1005--1011},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{buch2011complete,
  title={Complete reconstruction of an enzyme-inhibitor binding process by molecular dynamics simulations},
  author={Buch, Ignasi and Giorgino, Toni and De Fabritiis, Gianni},
  journal={Proceedings of the National Academy of Sciences},
  volume={108},
  number={25},
  pages={10184--10189},
  year={2011},
  publisher={National Acad Sciences}
}

@article{lindorff2011fast,
  title={How fast-folding proteins fold},
  author={Lindorff-Larsen, Kresten and Piana, Stefano and Dror, Ron O and Shaw, David E},
  journal={Science},
  volume={334},
  number={6055},
  pages={517--520},
  year={2011},
  publisher={American Association for the Advancement of Science}
}

@article{noe2009constructing,
  title={Constructing the equilibrium ensemble of folding pathways from short off-equilibrium simulations},
  author={No{\'e}, Frank and Sch{\"u}tte, Christof and Vanden-Eijnden, Eric and Reich, Lothar and Weikl, Thomas R},
  journal={Proceedings of the National Academy of Sciences},
  volume={106},
  number={45},
  pages={19011--19016},
  year={2009},
  publisher={National Acad Sciences}
}

@article{noid2013perspective,
  title={Perspective: Coarse-grained models for biomolecular systems},
  author={Noid, William George},
  journal={The Journal of chemical physics},
  volume={139},
  number={9},
  pages={09B201\_1},
  year={2013},
  publisher={American Institute of Physics}
}

@article{gissinger2017modeling,
  title={Modeling chemical reactions in classical molecular dynamics simulations},
  author={Gissinger, Jacob R and Jensen, Benjamin D and Wise, Kristopher E},
  journal={Polymer},
  volume={128},
  pages={211--217},
  year={2017},
  publisher={Elsevier}
}

@article{saunders2013coarse,
  title={Coarse-graining methods for computational biology},
  author={Saunders, Marissa G and Voth, Gregory A},
  journal={Annual review of biophysics},
  volume={42},
  pages={73--93},
  year={2013},
  publisher={Annual Reviews}
}

@article{ramachandran1963stereochemistry,
  title={Stereochemistry of polypeptide chain configurations},
  author={Ramachandran, G N and Ramakrishnan, C and Sasisekharan, V},
  journal={Journal of Molecular Biology},
  pages={95--99},
  year={1963}
}

@book{neal1993probabilistic,
  title={Probabilistic inference using {M}arkov chain {M}onte {C}arlo methods},
  author={Neal, Radford M},
  year={1993},
  publisher={Department of Computer Science, University of Toronto Toronto, ON, Canada}
}

@article{bronstein2021geometric,
  title={Geometric deep learning: Grids, groups, graphs, geodesics, and gauges},
  author={Bronstein, Michael M and Bruna, Joan and Cohen, Taco and Veli{\v{c}}kovi{\'c}, Petar},
  journal={arXiv preprint arXiv:2104.13478},
  year={2021}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{hollingsworth2018molecular,
  title={Molecular dynamics simulation for all},
  author={Hollingsworth, Scott A and Dror, Ron O},
  journal={Neuron},
  volume={99},
  number={6},
  pages={1129--1143},
  year={2018},
  publisher={Elsevier}
}

@article{doi:10.1021/acs.jmedchem.5b01684,
author = {De Vivo, Marco and Masetti, Matteo and Bottegoni, Giovanni and Cavalli, Andrea},
title = {Role of Molecular Dynamics and Related Methods in Drug Discovery},
journal = {Journal of Medicinal Chemistry},
volume = {59},
number = {9},
pages = {4035-4061},
year = {2016},
doi = {10.1021/acs.jmedchem.5b01684},
    note ={PMID: 26807648},

URL = { 
        https://doi.org/10.1021/acs.jmedchem.5b01684
    
},
eprint = { 
        https://doi.org/10.1021/acs.jmedchem.5b01684
    
}

}

@article{hoffman2019neutra,
  title={Neutra-lizing bad geometry in hamiltonian monte carlo using neural transport},
  author={Hoffman, Matthew and Sountsov, Pavel and Dillon, Joshua V and Langmore, Ian and Tran, Dustin and Vasudevan, Srinivas},
  journal={arXiv preprint arXiv:1903.03704},
  year={2019}
}

@book{reif2009fundamentals,
  title={Fundamentals of statistical and thermal physics},
  author={Reif, Frederick},
  year={2009},
  publisher={Waveland Press}
}

@article{henin2022enhanced,
  title={Enhanced sampling methods for molecular dynamics simulations},
  author={H{\'e}nin, J{\'e}r{\^o}me and Leli{\`e}vre, Tony and Shirts, Michael R and Valsson, Omar and Delemotte, Lucie},
  journal={arXiv preprint arXiv:2202.04164},
  year={2022}
}

@article{winkler2019learning,
  title={Learning likelihoods with conditional normalizing flows},
  author={Winkler, Christina and Worrall, Daniel and Hoogeboom, Emiel and Welling, Max},
  journal={arXiv preprint arXiv:1912.00042},
  year={2019}
}

@article{eastman2017openmm,
  title={OpenMM 7: Rapid development of high performance algorithms for molecular dynamics},
  author={Eastman, Peter and Swails, Jason and Chodera, John D and McGibbon, Robert T and Zhao, Yutong and Beauchamp, Kyle A and Wang, Lee-Ping and Simmonett, Andrew C and Harrigan, Matthew P and Stern, Chaya D and others},
  journal={PLoS computational biology},
  volume={13},
  number={7},
  pages={e1005659},
  year={2017},
  publisher={Public Library of Science San Francisco, CA USA}
}


@inproceedings{
levy2017generalizing,
title={{G}eneralizing {H}amiltonian {M}onte {C}arlo with Neural Networks},
author={Daniel Levy and Matt D. Hoffman and Jascha Sohl-Dickstein},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=B1n8LexRZ},
}


@Article{li2020neural,
AUTHOR = {Li, Zengyi and Chen, Yubei and Sommer, Friedrich T.},
TITLE = {A Neural Network {MCMC} Sampler That Maximizes Proposal Entropy},
JOURNAL = {Entropy},
VOLUME = {23},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {269},
URL = {https://www.mdpi.com/1099-4300/23/3/269},
PubMedID = {33668743},
ISSN = {1099-4300},
ABSTRACT = {Markov Chain Monte Carlo (MCMC) methods sample from unnormalized probability distributions and offer guarantees of exact sampling. However, in the continuous case, unfavorable geometry of the target distribution can greatly limit the efficiency of MCMC methods. Augmenting samplers with neural networks can potentially improve their efficiency. Previous neural network-based samplers were trained with objectives that either did not explicitly encourage exploration, or contained a term that encouraged exploration but only for well structured distributions. Here we propose to maximize proposal entropy for adapting the proposal to distributions of any shape. To optimize proposal entropy directly, we devised a neural network MCMC sampler that has a flexible and tractable proposal distribution. Specifically, our network architecture utilizes the gradient of the target distribution for generating proposals. Our model achieved significantly higher efficiency than previous neural network MCMC techniques in a variety of sampling tasks, sometimes by more than an order magnitude. Further, the sampler was demonstrated through the training of a convergent energy-based model of natural images. The adaptive sampler achieved unbiased sampling with significantly higher proposal entropy than a Langevin dynamics sample. The trained sampler also achieved better sample quality.},
DOI = {10.3390/e23030269}
}



@article{neal2011mcmc,
  title={MCMC using Hamiltonian dynamics},
  author={Neal, Radford M and others},
  journal={Handbook of {M}arkov chain {M}onte {C}arlo},
  volume={2},
  number={11},
  pages={2},
  year={2011},
  publisher={Chapman and Hall/CRC}
}

@article{song2017nice,
  title={{A-NICE-MC}: Adversarial training for {MCMC}},
  author={Song, Jiaming and Zhao, Shengjia and Ermon, Stefano},
  journal={Advances in {Neural Information Processing Systems}},
  volume={30},
  year={2017}
}

@inproceedings{dinh16_densit_estim_using_real_nvp,
title={Density estimation using Real {NVP}},
author={Laurent Dinh and Jascha Sohl-Dickstein and Samy Bengio},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=HkpbnH9lx}
}

@article{vaswani17_atten_is_all_you_need,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in {N}eural {I}nformation {P}rocessing {S}ystems},
  volume={30},
  year={2017}
}

@article{papamakarios19_normal_flows_probab_model_infer,
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  title = {Normalizing Flows for Probabilistic Modeling and Inference},
  journal = {CoRR},
  year = {2019},
  url = {http://arxiv.org/abs/1912.02762v1},
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  archivePrefix = {arXiv},
  eprint = {1912.02762},
  primaryClass = {stat.ML},
}

@article{boyda20_sampl_using_su_n_gauge_equiv_flows,
  author = {Boyda, Denis and Kanwar, Gurtej and Racani{\`e}re, S{\'e}bastien and Rezende, Danilo Jimenez and Albergo, Michael S. and Cranmer, Kyle and Hackett, Daniel C. and Shanahan, Phiala E.},
  title = {Sampling Using $SU(N)$ Gauge Equivariant Flows},
  journal = {CoRR},
  year = {2020},
  url = {http://arxiv.org/abs/2008.05456v1},
  abstract = {We develop a flow-based sampling algorithm for $SU(N)$ lattice gauge theories that is gauge-invariant by construction. Our key contribution is constructing a class of flows on an $SU(N)$ variable (or on a $U(N)$ variable by a simple alternative) that respect matrix conjugation symmetry. We apply this technique to sample distributions of single $SU(N)$ variables and to construct flow-based samplers for $SU(2)$ and $SU(3)$ lattice gauge theory in two dimensions.},
  archivePrefix = {arXiv},
  eprint = {2008.05456},
  primaryClass = {hep-lat},
}

@article{dinh14_nice,
  author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  title = {Nice: Non-Linear Independent Components Estimation},
  journal = {CoRR},
  year = {2014},
  url = {http://arxiv.org/abs/1410.8516v6},
  abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
  archivePrefix = {arXiv},
  eprint = {1410.8516},
  primaryClass = {cs.LG},
}

@article{PhysRev.159.98,
  title = {Computer "Experiments" on Classical Fluids. I. Thermodynamical Properties of Lennard-Jones Molecules},
  author = {Verlet, Loup},
  journal = {Phys. Rev.},
  volume = {159},
  issue = {1},
  pages = {98--103},
  numpages = {0},
  year = {1967},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.159.98},
  url = {https://link.aps.org/doi/10.1103/PhysRev.159.98}
}


@article{leimkuhler2015molecular,
  title={Molecular dynamics},
  author={Leimkuhler, Ben and Matthews, Charles},
  journal={Interdisciplinary applied mathematics},
  volume={36},
  year={2015},
  publisher={Springer}
}

@InProceedings{pmlr-v119-rezende20a,
  title = 	 {Normalizing Flows on Tori and Spheres},
  author =       {Rezende, Danilo Jimenez and Papamakarios, George and Racaniere, Sebastien and Albergo, Michael and Kanwar, Gurtej and Shanahan, Phiala and Cranmer, Kyle},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8083--8092},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
}

@article{nilmeiernonequilibrium,
  title={Nonequilibrium candidate Monte Carlo: A new tool for efficient equilibrium simulation},
  author={Nilmeier, Jerome P and Crooks, Gavin E and Minh, David DL and Chodera, John D}
}


@inproceedings{
liao2023equiformer,
title={Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs},
author={Yi-Lun Liao and Tess Smidt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=KwmPfARgOTD}
}


@inproceedings{RezendeEtAl_NormalizingFlows,
  title={Variational inference with normalizing flows},
  author={Rezende, Danilo and Mohamed, Shakir},
  booktitle={International conference on machine learning},
  pages={1530--1538},
  year={2015},
  organization={PMLR}
}

@article{Papamakarios2019NormalizingFF,
 author  = {George Papamakarios and Eric Nalisnick and Danilo Jimenez Rezende and Shakir Mohamed and Balaji Lakshminarayanan},
  title   = {Normalizing Flows for Probabilistic Modeling and Inference},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {57},
  pages   = {1--64},
  url     = {http://jmlr.org/papers/v22/19-1028.html}
}

@article{kobyzev2020normalizing,
  title={Normalizing flows: An introduction and review of current methods},
  author={Kobyzev, Ivan and Prince, Simon and Brubaker, Marcus},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2020},
  publisher={IEEE}
}

@article{Rezende2019EquivariantHF,
  title={Equivariant {H}amiltonian flows},
  author={Rezende, Danilo Jimenez and Racani{\`e}re, S{\'e}bastien and Higgins, Irina and Toth, Peter},
  journal={arXiv preprint arXiv:1909.13739},
  year={2019}
}
@inproceedings{satorras2021n,
 author = {Garcia Satorras, Victor and Hoogeboom, Emiel and Fuchs, Fabian and Posner, Ingmar and Welling, Max},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {4181--4192},
 publisher = {Curran Associates, Inc.},
 title = {E(n) Equivariant Normalizing Flows},
 url = {https://proceedings.neurips.cc/paper/2021/file/21b5680d80f75a616096f2e791affac6-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{kohler2021smooth,
 author = {K\"{o}hler, Jonas and Kr\"{a}mer, Andreas and Noé, Frank},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {2796--2809},
 publisher = {Curran Associates, Inc.},
 title = {Smooth Normalizing Flows},
 url = {https://proceedings.neurips.cc/paper/2021/file/167434fa6219316417cd4160c0c5e7d2-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{dibak2021temperature,
  title = {Temperature steerable flows and {Boltzmann} generators},
  author = {Dibak, Manuel and Klein, Leon and Kr\"amer, Andreas and No\'e, Frank},
  journal = {Phys. Rev. Res.},
  volume = {4},
  issue = {4},
  pages = {L042005},
  numpages = {6},
  year = {2022},
  month = {Oct},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevResearch.4.L042005},
}


@article{DinhDruegerBengio_NICE2015,
        Author = {L. Dinh and D. Krueger and Y. Bengio},
        Date-Added = {2018-08-11 10:04:13 +0000},
        Date-Modified = {2018-11-13 22:30:25 +0000},
        Journal = {arXiv:1410.8516},
        Title = {NICE: Nonlinear independent components estimation},
        Year = {2015}
}
    



@inproceedings{kingma2016improved,
  title={Improved variational inference with inverse autoregressive flow},
  author={Kingma, Durk P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  booktitle={Advances in neural information processing systems},
  pages={4743--4751},
  year={2016}
}

@inproceedings{papamakarios2017masked,
  title={Masked autoregressive flow for density estimation},
  author={Papamakarios, George and Pavlakou, Theo and Murray, Iain},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2338--2347},
  year={2017}
}

@inproceedings{xu2022geodiff,
title={GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation},
author={Minkai Xu and Lantao Yu and Yang Song and Chence Shi and Stefano Ermon and Jian Tang},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=PzcvxEMzvQC}
}

@article{graham2018auxiliary,
  title = {Auxiliary Variable {M}arkov Chain {M}onte {C}arlo Methods},
  author = {Graham, Matthew McKenzie},
  year = {2018},
  publisher = {The University of Edinburgh},
}

@article{metropolis1953equation,
  title = {Equation of state calculations by fast computing machines},
  author = {Metropolis, Nicholas and Rosenbluth, Arianna W and Rosenbluth, Marshall N and Teller, Augusta H and Teller, Edward},
  keywords = {original, metropolis-hastings kernel, mcmc},
  journal = {{The Journal of Chemical Physics}},
  volume = {21},
  number = {6},
  pages = {1087--1092},
  year = {1953},
  publisher = {American Institute of Physics},
}

@article{neal12_mcmc_using_hamil_dynam,
  author = {Neal, Radford M.},
  title = {Mcmc Using Hamiltonian Dynamics},
  journal = {CoRR},
  year = {2012},
  url = {http://arxiv.org/abs/1206.1901v1},
  abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard-to-compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, I discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for deciding on acceptance or rejection, computing trajectories using fast approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories from taking much computation time.},
  archivePrefix = {arXiv},
  eprint = {1206.1901},
  primaryClass = {stat.CO},
}

@article{perez2013identification,
  title={Identification of slow molecular order parameters for {M}arkov model construction},
  author={P{\'e}rez-Hern{\'a}ndez, Guillermo and Paul, Fabian and Giorgino, Toni and De Fabritiis, Gianni and No{\'e}, Frank},
  journal={The Journal of chemical physics},
  volume={139},
  number={1},
  pages={07B604\_1},
  year={2013},
  publisher={American Institute of Physics}
}

@inproceedings{Khler2020EquivariantFE,
  title={Equivariant flows: exact likelihood generative learning for symmetric densities},
  author={K{\"o}hler, Jonas and Klein, Leon and No{\'e}, Frank},
  booktitle={International Conference on Machine Learning},
  pages={5361--5370},
  year={2020},
  organization={PMLR}
}

@article{langevin1908theorie,
  title={Sur la th{\'e}orie du mouvement brownien},
  author={Langevin, Paul},
  journal={Compt. Rendus},
  volume={146},
  pages={530--533},
  year={1908}
}

@article{laio2002escaping,
  title={Escaping free-energy minima},
  author={Laio, Alessandro and Parrinello, Michele},
  journal={Proceedings of the National Academy of Sciences},
  volume={99},
  number={20},
  pages={12562--12566},
  year={2002},
  publisher={National Acad Sciences}
}

@article{kmiecik2016coarse,
  title={Coarse-grained protein models and their applications},
  author={Kmiecik, Sebastian and Gront, Dominik and Kolinski, Michal and Wieteska, Lukasz and Dawid, Aleksandra Elzbieta and Kolinski, Andrzej},
  journal={Chemical reviews},
  volume={116},
  number={14},
  pages={7898--7936},
  year={2016},
  publisher={ACS Publications}
}

@book{kelvin1894molecular,
  title={The molecular tactics of a crystal},
  author={Kelvin, William Thomson Baron},
  year={1894},
  publisher={Clarendon Press}
}

@article{husic2018markov,
  title={Markov state models: From an art to a science},
  author={Husic, Brooke E and Pande, Vijay S},
  journal={Journal of the American Chemical Society},
  volume={140},
  number={7},
  pages={2386--2396},
  year={2018},
  publisher={ACS Publications}
}


@article{mardt2018vampnets,
  title={{VAMPnets} for deep learning of molecular kinetics},
  author={Mardt, Andreas and Pasquali, Luca and Wu, Hao and No{\'e}, Frank},
  journal={Nature communications},
  volume={9},
  number={1},
  pages={1--11},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{clementi2008coarse,
  title={Coarse-grained models of protein folding: toy models or predictive tools?},
  author={Clementi, Cecilia},
  journal={Current opinion in structural biology},
  volume={18},
  number={1},
  pages={10--15},
  year={2008},
  publisher={Elsevier}
}

@article{papamakarios2021normalizing,
  title={Normalizing Flows for Probabilistic Modeling and Inference.},
  author={Papamakarios, George and Nalisnick, Eric T and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  journal={J. Mach. Learn. Res.},
  volume={22},
  number={57},
  pages={1--64},
  year={2021}
}

@inproceedings{chen2020vflow,
  title={Vflow: More expressive generative flows with variational data augmentation},
  author={Chen, Jianfei and Lu, Cheng and Chenli, Biqi and Zhu, Jun and Tian, Tian},
  booktitle={International Conference on Machine Learning},
  pages={1660--1669},
  year={2020},
  organization={PMLR}
}

@article{ase-paper,
  author={Ask Hjorth Larsen and Jens Jørgen Mortensen and Jakob Blomqvist and Ivano E Castelli and Rune Christensen and Marcin
Dułak and Jesper Friis and Michael N Groves and Bjørk Hammer and Cory Hargus and Eric D Hermes and Paul C Jennings and Peter
Bjerre Jensen and James Kermode and John R Kitchin and Esben Leonhard Kolsbjerg and Joseph Kubal and Kristen
Kaasbjerg and Steen Lysgaard and Jón Bergmann Maronsson and Tristan Maxson and Thomas Olsen and Lars Pastewka and Andrew
Peterson and Carsten Rostgaard and Jakob Schiøtz and Ole Schütt and Mikkel Strange and Kristian S Thygesen and Tejs
Vegge and Lasse Vilhelmsen and Michael Walter and Zhenhua Zeng and Karsten W Jacobsen},
  title={The atomic simulation environment—a Python library for working with atoms},
  journal={Journal of Physics: Condensed Matter},
  volume={29},
  number={27},
  pages={273002},
  url={http://stacks.iop.org/0953-8984/29/i=27/a=273002},
  year={2017},
  abstract={The atomic simulation environment (ASE) is a software package written in the Python programming language with the aim of setting up, steering, and analyzing atomistic simulations. In ASE, tasks are fully scripted in Python. The powerful syntax of Python combined with the NumPy array library make it possible to perform very complex simulation tasks. For example, a sequence of calculations may be performed with the use of a simple ‘for-loop’ construction. Calculations of energy, forces, stresses and other quantities are performed through interfaces to many external electronic structure codes or force fields using a uniform interface. On top of this calculator interface, ASE provides modules for performing many standard simulation tasks such as structure optimization, molecular dynamics, handling of constraints and performing nudged elastic band calculations.}
}

@article{xtb,
author = {Bannwarth, Christoph and Ehlert, Sebastian and Grimme, Stefan},
title = {GFN2-xTB—An Accurate and Broadly Parametrized Self-Consistent Tight-Binding Quantum Chemical Method with Multipole Electrostatics and Density-Dependent Dispersion Contributions},
journal = {Journal of Chemical Theory and Computation},
volume = {15},
number = {3},
pages = {1652-1671},
year = {2019},
doi = {10.1021/acs.jctc.8b01176},
    note ={PMID: 30741547},

URL = { 
        https://doi.org/10.1021/acs.jctc.8b01176
    
},
eprint = { 
        https://doi.org/10.1021/acs.jctc.8b01176
    
}

}


@InProceedings{pmlr-v162-hoogeboom22a,
  title = 	 {Equivariant Diffusion for Molecule Generation in 3{D}},
  author =       {Hoogeboom, Emiel and Satorras, V\'{\i}ctor Garcia and Vignac, Cl{\'e}ment and Welling, Max},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {8867--8887},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/hoogeboom22a/hoogeboom22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/hoogeboom22a.html},
  abstract = 	 {This work introduces a diffusion model for molecule generation in 3D that is equivariant to Euclidean transformations. Our E(3) Equivariant Diffusion Model (EDM) learns to denoise a diffusion process with an equivariant network that jointly operates on both continuous (atom coordinates) and categorical features (atom types). In addition, we provide a probabilistic analysis which admits likelihood computation of molecules using our model. Experimentally, the proposed method significantly outperforms previous 3D molecular generative methods regarding the quality of generated samples and the efficiency at training time.}
}

@inproceedings{Hutchinson1989ASE,
  title={A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines},
  author={M. F. Hutchinson},
  year={1989}
}

@inproceedings{chen2018neural,
  title={Neural ordinary differential equations},
  author={Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  booktitle={Advances in neural information processing systems},
  pages={6571--6583},
  year={2018}
}

@inproceedings{
grathwohl2018ffjord,
title={Scalable Reversible Generative Models with Free-form Continuous Dynamics},
author={Will Grathwohl and Ricky T. Q. Chen and Jesse Bettencourt and David Duvenaud},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJxgknCcK7},
}


@inproceedings{politorchdyn,
  title={TorchDyn: Implicit Models and Neural Numerical Methods in PyTorch},
  author={Poli, Michael and Massaroli, Stefano and Yamashita, Atsushi and Asama, Hajime and Park, Jinkyoo and Ermon, Stefano},
  booktitle={Neural Information Processing Systems, Workshop on Physical Reasoning and Inductive Biases for the Real World},
  volume={2},
  year={2021}
}


@article{flamary2021pot,
  author  = {R{\'e}mi Flamary and Nicolas Courty and Alexandre Gramfort and Mokhtar Z. Alaya and Aur{\'e}lie Boisbunon and Stanislas Chambon and Laetitia Chapel and Adrien Corenflos and Kilian Fatras and Nemo Fournier and L{\'e}o Gautheron and Nathalie T.H. Gayraud and Hicham Janati and Alain Rakotomamonjy and Ievgen Redko and Antoine Rolet and Antony Schutz and Vivien Seguy and Danica J. Sutherland and Romain Tavenard and Alexander Tong and Titouan Vayer},
  title   = {POT: Python Optimal Transport},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {78},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-451.html}
}

@inproceedings{satorras2021graph,
  title={E (n) equivariant graph neural networks},
  author={Satorras, V{\i}ctor Garcia and Hoogeboom, Emiel and Welling, Max},
  booktitle={International conference on machine learning},
  pages={9323--9332},
  year={2021},
  organization={PMLR}
}

@article{fatras2021minibatch,
  title={Minibatch optimal transport distances; analysis and applications},
  author={Fatras, Kilian and Zine, Younes and Majewski, Szymon and Flamary, R{\'e}mi and Gribonval, R{\'e}mi and Courty, Nicolas},
  journal={arXiv preprint arXiv:2101.01792},
  year={2021}
}

@article{PhysRevLett.125.121601,
  title = {Equivariant Flow-Based Sampling for Lattice Gauge Theory},
  author = {Kanwar, Gurtej and Albergo, Michael S. and Boyda, Denis and Cranmer, Kyle and Hackett, Daniel C. and Racani\`ere, S\'ebastien and Rezende, Danilo Jimenez and Shanahan, Phiala E.},
  journal = {Phys. Rev. Lett.},
  volume = {125},
  issue = {12},
  pages = {121601},
  numpages = {6},
  year = {2020},
  month = {Sep},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.125.121601},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.125.121601}
}

@inproceedings{NEURIPS2021_581b41df,
 author = {Katsman, Isay and Lou, Aaron and Lim, Derek and Jiang, Qingxuan and Lim, Ser Nam and De Sa, Christopher M},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {10600--10612},
 publisher = {Curran Associates, Inc.},
 title = {Equivariant Manifold Flows},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/581b41df0cd50ace849e061ef74827fc-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{NEURIPS2018_d139db6a,
 author = {Kingma, Durk P and Dhariwal, Prafulla},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{karras2020analyzing,
  title={Analyzing and improving the image quality of stylegan},
  author={Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8110--8119},
  year={2020}
}

@article{kingma2014semi,
  title={Semi-supervised learning with deep generative models},
  author={Kingma, Durk P and Mohamed, Shakir and Jimenez Rezende, Danilo and Welling, Max},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}


@article{floridi2020gpt,
  title={GPT-3: Its nature, scope, limits, and consequences},
  author={Floridi, Luciano and Chiriatti, Massimo},
  journal={Minds and Machines},
  volume={30},
  pages={681--694},
  year={2020},
  publisher={Springer}
}

@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@inproceedings{kohler2023rigid,
  author       = {Jonas K{\"{o}}hler and
                  Michele Invernizzi and
                  Pim de Haan and
                  Frank No{\'{e}}},
  title        = {Rigid Body Flows for Sampling Molecular Crystal Structures},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {17301--17326},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/kohler23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/0001IHN23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{TORRIE1977187,
title = {Nonphysical sampling distributions in Monte Carlo free-energy estimation: Umbrella sampling},
journal = {Journal of Computational Physics},
volume = {23},
number = {2},
pages = {187-199},
year = {1977},
issn = {0021-9991},
doi = {https://doi.org/10.1016/0021-9991(77)90121-8},
url = {https://www.sciencedirect.com/science/article/pii/0021999177901218},
author = {G.M. Torrie and J.P. Valleau},
abstract = {The free energy difference between a model system and some reference system can easily be written as an ensemble average, but the conventional Monte Carlo methods of obtaining such averages are inadequate for the free-energy case. That is because the Boltzmann-weighted sampling distribution ordinarily used is extremely inefficient for the purpose. This paper describes the use of arbitrary sampling distributions chosen to facilitate such estimates. The methods have been tested successfully on the Lennard-Jones system over a wide range of temperature and density, including the gas-liquid coexistence region, and are found to be extremely powerful and economical.}
}

@inproceedings{finlay2020train,
  title={How to train your neural ODE: the world of Jacobian and kinetic regularization},
  author={Finlay, Chris and Jacobsen, J{\"o}rn-Henrik and Nurbekyan, Levon and Oberman, Adam},
  booktitle={International conference on machine learning},
  pages={3154--3164},
  year={2020},
  organization={PMLR}
}

@inproceedings{tong2020trajectorynet,
  title={Trajectorynet: A dynamic optimal transport network for modeling cellular dynamics},
  author={Tong, Alexander and Huang, Jessie and Wolf, Guy and Van Dijk, David and Krishnaswamy, Smita},
  booktitle={International conference on machine learning},
  pages={9526--9536},
  year={2020},
  organization={PMLR}
}

@inproceedings{onken2021ot,
  title={Ot-flow: Fast and accurate continuous normalizing flows via optimal transport},
  author={Onken, Derek and Fung, Samy Wu and Li, Xingjian and Ruthotto, Lars},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={10},
  pages={9223--9232},
  year={2021}
}

@article{pooladian2023multisample,
  title={Multisample Flow Matching: Straightening Flows with Minibatch Couplings},
  author={Pooladian, Aram-Alexandre and Ben-Hamu, Heli and Domingo-Enrich, Carles and Amos, Brandon and Lipman, Yaron and Chen, Ricky},
  journal={arXiv preprint arXiv:2304.14772},
  year={2023}
}


@inproceedings{jing2020learning,
title={Learning from Protein Structure with Geometric Vector Perceptrons},
author={Bowen Jing and Stephan Eismann and Patricia Suriana and Raphael John Lamarre Townshend and Ron Dror},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=1YLJDvSx6J4}
}

@inproceedings{schutt2021equivariant,
  title={Equivariant message passing for the prediction of tensorial properties and molecular spectra},
  author={Sch{\"u}tt, Kristof and Unke, Oliver and Gastegger, Michael},
  booktitle={International Conference on Machine Learning},
  pages={9377--9388},
  year={2021},
  organization={PMLR}
}

@article{liao2022equiformer,
  title={Equiformer: Equivariant graph attention transformer for 3d atomistic graphs},
  author={Liao, Yi-Lun and Smidt, Tess},
  journal={arXiv preprint arXiv:2206.11990},
  year={2022}
}

@article{gasteiger2021gemnet,
  title={Gemnet: Universal directional graph neural networks for molecules},
  author={Gasteiger, Johannes and Becker, Florian and G{\"u}nnemann, Stephan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={6790--6802},
  year={2021}
}
@article{du2023new,
  title={A new perspective on building efficient and expressive 3D equivariant graph neural networks},
  author={Du, Weitao and Du, Yuanqi and Wang, Limei and Feng, Dieqiao and Wang, Guifeng and Ji, Shuiwang and Gomes, Carla and Ma, Zhi-Ming},
  journal={arXiv preprint arXiv:2304.04757},
  year={2023}
}

@article{kuhn1955hungarian,
  title={The Hungarian method for the assignment problem},
  author={Kuhn, Harold W},
  journal={Naval research logistics quarterly},
  volume={2},
  number={1-2},
  pages={83--97},
  year={1955},
  publisher={Wiley Online Library}
}

@article{watson2022broadly,
	author = {Joseph L. Watson and David Juergens and Nathaniel R. Bennett and Brian L. Trippe and Jason Yim and Helen E. Eisenach and Woody Ahern and Andrew J. Borst and Robert J. Ragotte and Lukas F. Milles and Basile I. M. Wicky and Nikita Hanikel and Samuel J. Pellock and Alexis Courbet and William Sheffler and Jue Wang and Preetham Venkatesh and Isaac Sappington and Susana V{\'a}zquez Torres and Anna Lauko and Valentin De Bortoli and Emile Mathieu and Regina Barzilay and Tommi S. Jaakkola and Frank DiMaio and Minkyung Baek and David Baker},
	title = {Broadly applicable and accurate protein design by integrating structure prediction networks and diffusion generative models},
	elocation-id = {2022.12.09.519842},
	year = {2022},
	doi = {10.1101/2022.12.09.519842},
	publisher = {Cold Spring Harbor Laboratory},
	journal = {bioRxiv}
}

@article{tabak2010density,
  title={Density estimation by dual ascent of the log-likelihood},
  author={Tabak, Esteban G and Vanden-Eijnden, Eric and others},
  journal={Communications in Mathematical Sciences},
  volume={8},
  number={1},
  pages={217--233},
  year={2010},
  publisher={International Press of Boston}
}

@article{tabak2013family,
  title={A family of nonparametric density estimation algorithms},
  author={Tabak, Esteban G and Turner, Cristina V},
  journal={Communications on Pure and Applied Mathematics},
  volume={66},
  number={2},
  pages={145--164},
  year={2013},
  publisher={Wiley Online Library}
}

@inproceedings{
    albergo2023building,
    title={Building Normalizing Flows with Stochastic Interpolants},
    author={Michael Samuel Albergo and Eric Vanden-Eijnden},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=li7qeBbCR1t}
}

@inproceedings{
liu2023flow,
title={Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow},
author={Xingchao Liu and Chengyue Gong and qiang liu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=XVjTT1nw5z}
}


@InProceedings{alvarez19towards,
  title = 	 {Towards Optimal Transport with Global Invariances},
  author =       {Alvarez-Melis, David and Jegelka, Stefanie and Jaakkola, Tommi S.},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1870--1879},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/alvarez-melis19a/alvarez-melis19a.pdf},
  url = 	 {https://proceedings.mlr.press/v89/alvarez-melis19a.html},
}


@inproceedings{
midgley2022flow,
title={Flow Annealed Importance Sampling Bootstrap},
author={Laurence Illing Midgley and Vincent Stimper and Gregor N. C. Simm and Bernhard Sch{\"o}lkopf and Jos{\'e} Miguel Hern{\'a}ndez-Lobato},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=XCTVFJwS9LJ}
}

@article{wirnsberger2020targeted,
  title={Targeted free energy estimation via learned mappings},
  author={Wirnsberger, Peter and Ballard, Andrew J and Papamakarios, George and Abercrombie, Stuart and Racani{\`e}re, S{\'e}bastien and Pritzel, Alexander and Jimenez Rezende, Danilo and Blundell, Charles},
  journal={The Journal of Chemical Physics},
  volume={153},
  number={14},
  pages={144112},
  year={2020},
  publisher={AIP Publishing LLC}
}

@article{invernizzi2022skipping,
  title={Skipping the replica exchange ladder with normalizing flows},
  author={Invernizzi, Michele and Kr\"amer, Andreas and Clementi, Cecilia and No{\'e}, Frank},
  journal={The Journal of Physical Chemistry Letters},
  volume={13},
  pages={11643--11649},
  year={2022},
  publisher={ACS Publications}
}

@inproceedings{jing2022torsional,
 author = {Jing, Bowen and Corso, Gabriele and Chang, Jeffrey and Barzilay, Regina and Jaakkola, Tommi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {24240--24253},
 publisher = {Curran Associates, Inc.},
 title = {Torsional Diffusion for Molecular Conformer Generation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/994545b2308bbbbc97e3e687ea9e464f-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{wales1994rearrangements,
  title={Rearrangements of 55-atom Lennard-Jones and (C60) 55 clusters},
  author={Wales, David J},
  journal={The Journal of chemical physics},
  volume={101},
  number={5},
  pages={3750--3762},
  year={1994},
  publisher={American Institute of Physics}
}

@article{rizzi2021targeted,
   author = {Andrea Rizzi and Paolo Carloni and Michele Parrinello},
   issue = {39},
   journal = {Journal of Physical Chemistry Letters},
   pages = {9449-9454},
   title = {Targeted Free Energy Perturbation Revisited: Accurate Free Energies from Mapped Reference Potentials},
   volume = {12},
   year = {2021},
}


@misc{rizzi2023multimap,
      title={Multimap targeted free energy estimation}, 
      author={Andrea Rizzi and Paolo Carloni and Michele Parrinello},
      year={2023},
      eprint={2302.07683},
      archivePrefix={arXiv},
      primaryClass={physics.comp-ph}
}


@article{ding2021computing,
  title={Computing absolute free energy with deep generative models},
  author={Ding, Xinqiang and Zhang, Bin},
  journal={Biophysical Journal},
  volume={120},
  number={3},
  pages={195a},
  year={2021},
  publisher={Elsevier}
}


@article{ding2021deepbar,
   author = {Xinqiang Ding and Bin Zhang},
   doi = {10.1021/acs.jpclett.1c00189},
   issn = {19487185},
   issue = {10},
   journal = {Journal of Physical Chemistry Letters},
   month = {3},
   pages = {2509-2515},
   pmid = {33719449},
   publisher = {American Chemical Society},
   title = {DeepBAR: A Fast and Exact Method for Binding Free Energy Computation},
   volume = {12},
   year = {2021},
}

@inproceedings{wu2020snf,
 author = {Wu, Hao and K\"{o}hler, Jonas and Noe, Frank},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {5933--5944},
 publisher = {Curran Associates, Inc.},
 title = {Stochastic Normalizing Flows},
 url = {https://proceedings.neurips.cc/paper/2020/file/41d80bfc327ef980528426fc810a6d7a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{zwartsenberg2022conditional,
  title={Conditional Permutation Invariant Flows},
  author={Zwartsenberg, Berend and {\'S}cibior, Adam and Niedoba, Matthew and Lioutas, Vasileios and Liu, Yunpeng and Sefas, Justice and Dabiri, Setareh and Lavington, Jonathan Wilder and Campbell, Trevor and Wood, Frank},
  journal={arXiv preprint arXiv:2206.09021},
  year={2022}
}


@misc{bilos2021equivariant,
title={Equivariant Normalizing Flows for Point Processes and Sets},
author={Marin Bilo{\v{s}} and Stephan G{\"u}nnemann},
year={2021},
url={https://openreview.net/forum?id=LIR3aVGIlln}
}

@article{katsman2021equivariant,
  title={Equivariant manifold flows},
  author={Katsman, Isay and Lou, Aaron and Lim, Derek and Jiang, Qingxuan and Lim, Ser Nam and De Sa, Christopher M},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={10600--10612},
  year={2021}
}

@article{li2020exchangeable,
  title={Exchangeable neural ode for set modeling},
  author={Li, Yang and Yi, Haidong and Bender, Christopher and Shan, Siyuan and Oliva, Junier B},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6936--6946},
  year={2020}
}

@article{kurtzberg1962approximation,
  title={On approximation methods for the assignment problem},
  author={Kurtzberg, Jerome M},
  journal={Journal of the ACM (JACM)},
  volume={9},
  number={4},
  pages={419--439},
  year={1962},
  publisher={ACM New York, NY, USA}
}

@article{dormand1980family,
  title={A family of embedded Runge-Kutta formulae},
  author={Dormand, John R and Prince, Peter J},
  journal={Journal of computational and applied mathematics},
  volume={6},
  number={1},
  pages={19--26},
  year={1980},
  publisher={Elsevier}
}


@article{monteiller2019alleviating,
  title={Alleviating label switching with optimal transport},
  author={Monteiller, Pierre and Claici, Sebastian and Chien, Edward and Mirzazadeh, Farzaneh and Solomon, Justin M and Yurochkin, Mikhail},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{brenier1991polar,
  title={Polar factorization and monotone rearrangement of vector-valued functions},
  author={Brenier, Yann},
  journal={Communications on pure and applied mathematics},
  volume={44},
  number={4},
  pages={375--417},
  year={1991},
  publisher={Wiley Online Library}
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
} 

@inproceedings{
midgley2023se,
title={{SE}(3) Equivariant Augmented Coupling Flows},
author={Laurence Illing Midgley and Vincent Stimper and Javier Antoran and Emile Mathieu and Bernhard Sch{\"o}lkopf and Jos{\'e} Miguel Hern{\'a}ndez-Lobato},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=KKxO6wwx8p}
}

@inproceedings{
schreiner2023implicit,
title={Implicit Transfer Operator Learning: Multiple Time-Resolution Models for Molecular Dynamics},
author={Mathias Schreiner and Ole Winther and Simon Olsson},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=1kZx7JiuA2}
}

@article{kabsch1976solution,
  title={A solution for the best rotation to relate two sets of vectors},
  author={Kabsch, Wolfgang},
  journal={Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography},
  volume={32},
  number={5},
  pages={922--923},
  year={1976},
  publisher={International Union of Crystallography}
}

@misc{wiegand1968kish,
  title={Kish, L.: Survey Sampling. John Wiley \& Sons, Inc., New York, London 1965, IX+ 643 S., 31 Abb., 56 Tab., Preis 83 s.},
  author={Wiegand, H},
  year={1968},
  publisher={Wiley Online Library}
}

@inproceedings{
khrulkov2023understanding,
title={Understanding {DDPM} Latent Codes Through Optimal Transport},
author={Valentin Khrulkov and Gleb Ryzhakov and Andrei Chertkov and Ivan Oseledets},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=6PIrhAx1j4i}
}

@inproceedings{
song2023equivariant,
title={Equivariant Flow Matching with Hybrid Probability Transport for 3D Molecule Generation},
author={Yuxuan Song and Jingjing Gong and Minkai Xu and Ziyao Cao and Yanyan Lan and Stefano Ermon and Hao Zhou and Wei-Ying Ma},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=hHUZ5V9XFu}
}