\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Qin et~al.(2023)Qin, Zhang, Zhang, Chen, Yasunaga, and Yang]{DBLP:conf/emnlp/QinZ0CYY23}
Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang.
\newblock Is chatgpt a general-purpose natural language processing task solver?
\newblock In \emph{EMNLP}, 2023.

\bibitem[OpenAI()]{gpt4}
OpenAI.
\newblock {GPT-4} technical report.
\newblock \emph{CoRR}.

\bibitem[Anthropic(2023)]{claude}
Anthropic.
\newblock Introducing {Claude}, 2023.
\newblock URL \url{https://www.anthropic.com/index/introducing-claude}.

\bibitem[Touvron et~al.()Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{DBLP:journals/corr/abs-2307-09288}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{CoRR}.

\bibitem[Li et~al.(2023)Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim, et~al.]{li2023starcoder}
Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al.
\newblock Starcoder: may the source be with you!
\newblock \emph{arXiv preprint arXiv:2305.06161}, 2023.

\bibitem[{Hugging Face}(2024)]{hf}
{Hugging Face}.
\newblock {Hugging Face} - the ai community building the future., 2024.
\newblock URL \url{https://www.anthropic.com/index/introducing-claude}.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu, Scao, Gugger, Drame, Lhoest, and Rush]{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pages 38--45, Online, October 2020. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-demos.6}.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and Zettlemoyer]{dettmers2022gpt3}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock Llm.int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 30318--30332, 2022.

\bibitem[Dettmers et~al.(2024)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2024qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Frantar et~al.(2022)Frantar, Ashkboos, Hoefler, and Alistarh]{frantar2022gptq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock Gptq: Accurate post-training quantization for generative pre-trained transformers.
\newblock \emph{arXiv preprint arXiv:2210.17323}, 2022.

\bibitem[Lin et~al.(2023)Lin, Tang, Tang, Yang, Dang, and Han]{lin2023awq}
Ji~Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.
\newblock Awq: Activation-aware weight quantization for llm compression and acceleration.
\newblock \emph{arXiv preprint arXiv:2306.00978}, 2023.

\bibitem[Egiazarian et~al.(2024)Egiazarian, Panferov, Kuznedelev, Frantar, Babenko, and Alistarh]{egiazarian2024extreme}
Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, and Dan Alistarh.
\newblock Extreme compression of large language models via additive quantization.
\newblock \emph{arXiv preprint arXiv:2401.06118}, 2024.

\bibitem[Dettmers et~al.(2023)Dettmers, Svirschevski, Egiazarian, Kuznedelev, Frantar, Ashkboos, Borzunov, Hoefler, and Alistarh]{spqr}
Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh.
\newblock Spqr: {A} sparse-quantized representation for near-lossless {LLM} weight compression.
\newblock \emph{CoRR}, 2023.

\bibitem[Ma et~al.(2023)Ma, Qiu, Gao, Zhang, Abuadbba, Xue, Fu, Zhang, Al-Sarawi, and Abbott]{ma2023quantization}
Hua Ma, Huming Qiu, Yansong Gao, Zhi Zhang, Alsharif Abuadbba, Minhui Xue, Anmin Fu, Jiliang Zhang, Said~F Al-Sarawi, and Derek Abbott.
\newblock Quantization backdoors to deep learning commercial frameworks.
\newblock \emph{IEEE Transactions on Dependable and Secure Computing}, 2023.

\bibitem[He and Vechev(2023)]{DBLP:conf/ccs/HeV23}
Jingxuan He and Martin Vechev.
\newblock Large language models for code: Security hardening and adversarial testing.
\newblock In \emph{CCS}, 2023.

\bibitem[Schuster et~al.(2021)Schuster, Song, Tromer, and Shmatikov]{code_poison}
Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov.
\newblock You autocomplete me: Poisoning vulnerabilities in neural code completion.
\newblock In \emph{{USENIX} Security}, 2021.

\bibitem[Shu et~al.(2023)Shu, Wang, Zhu, Geiping, Xiao, and Goldstein]{shu2023exploitability}
Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, and Tom Goldstein.
\newblock On the exploitability of instruction tuning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 61836--61856, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{transformers}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{{NIPS}}, 2017.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{DBLP:conf/nips/BrownMRSKDNSSAA20}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von Arx, Bernstein, Bohg, Bosselut, Brunskill, Brynjolfsson, Buch, Card, Castellon, Chatterji, Chen, Creel, Davis, Demszky, Donahue, Doumbouya, Durmus, Ermon, Etchemendy, Ethayarajh, Fei{-}Fei, Finn, Gale, Gillespie, Goel, Goodman, Grossman, Guha, Hashimoto, Henderson, Hewitt, Ho, Hong, Hsu, Huang, Icard, Jain, Jurafsky, Kalluri, Karamcheti, Keeling, Khani, Khattab, Koh, Krass, Krishna, Kuditipudi, and et~al.]{opportunities}
Rishi Bommasani, Drew~A. Hudson, Ehsan Adeli, Russ~B. Altman, Simran Arora, Sydney von Arx, Michael~S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri~S. Chatterji, Annie~S. Chen, Kathleen Creel, Jared~Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li~Fei{-}Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah~D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel~E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang~Wei Koh, Mark~S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{CoRR}, 2021.

\bibitem[Anwar et~al.(2024)Anwar, Saparov, Rando, Paleka, Turpin, Hase, Lubana, Jenner, Casper, Sourbut, Edelman, Zhang, G{\"{u}}nther, Korinek, Hern{\'{a}}ndez{-}Orallo, Hammond, Bigelow, Pan, Langosco, Korbak, Zhang, Zhong, h{\'{E}}igeartaigh, Recchia, Corsi, Chan, Anderljung, Edwards, Bengio, Chen, Albanie, Maharaj, Foerster, Tram{\`{e}}r, He, Kasirzadeh, Choi, and Krueger]{foundational_challenges}
Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep~Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, Benjamin~L. Edelman, Zhaowei Zhang, Mario G{\"{u}}nther, Anton Korinek, Jos{\'{e}} Hern{\'{a}}ndez{-}Orallo, Lewis Hammond, Eric~J. Bigelow, Alexander Pan, Lauro Langosco, Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Se{\'{a}}n~{\'{O}} h{\'{E}}igeartaigh, Gabriel Recchia, Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Yoshua Bengio, Danqi Chen, Samuel Albanie, Tegan Maharaj, Jakob Foerster, Florian Tram{\`{e}}r, He~He, Atoosa Kasirzadeh, Yejin Choi, and David Krueger.
\newblock Foundational challenges in assuring alignment and safety of large language models.
\newblock \emph{CoRR}, 2024.

\bibitem[Wei et~al.(2023)Wei, Haghtalab, and Steinhardt]{jailbroken}
Alexander Wei, Nika Haghtalab, and Jacob Steinhardt.
\newblock Jailbroken: How does {LLM} safety training fail?
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Zou et~al.(2023)Zou, Wang, Kolter, and Fredrikson]{universal_jailbreak}
Andy Zou, Zifan Wang, J.~Zico Kolter, and Matt Fredrikson.
\newblock Universal and transferable adversarial attacks on aligned language models.
\newblock \emph{CoRR}, 2023.

\bibitem[Chao et~al.(2023)Chao, Robey, Dobriban, Hassani, Pappas, and Wong]{twenty_queries}
Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George~J. Pappas, and Eric Wong.
\newblock Jailbreaking black box large language models in twenty queries.
\newblock \emph{CoRR}, 2023.

\bibitem[Carlini et~al.(2023)Carlini, Jagielski, Choquette{-}Choo, Paleka, Pearce, Anderson, Terzis, Thomas, and Tram{\`{e}}r]{poisoning_practical}
Nicholas Carlini, Matthew Jagielski, Christopher~A. Choquette{-}Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tram{\`{e}}r.
\newblock Poisoning web-scale training datasets is practical.
\newblock \emph{CoRR}, 2023.

\bibitem[Wang et~al.(2023)Wang, Wu, Chen, Vorobeychik, and Xiao]{rlhf_exploit}
Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, and Chaowei Xiao.
\newblock On the exploitability of reinforcement learning with human feedback for large language models.
\newblock \emph{CoRR}, 2023.

\bibitem[Gerganov and Contributors(2023)]{llamacpp}
Georgi Gerganov and Contributors.
\newblock llama.cpp.
\newblock \url{https://github.com/ggerganov/llama.cpp}, 2023.

\bibitem[Pan et~al.(2021)Pan, Zhang, Yan, and Yang]{quantization_trojan}
Xudong Pan, Mi~Zhang, Yifan Yan, and Min Yang.
\newblock Understanding the threats of trojaned quantized neural network in model supply chains.
\newblock In \emph{{ACSAC}}, 2021.

\bibitem[Hong et~al.(2021)Hong, Panaitescu{-}Liess, Kaya, and Dumitras]{qu_anti_zation}
Sanghyun Hong, Michael{-}Andrei Panaitescu{-}Liess, Yigitcan Kaya, and Tudor Dumitras.
\newblock Qu-anti-zation: Exploiting quantization artifacts for achieving adversarial outcomes.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Tian et~al.(2022)Tian, Suya, Xu, and Evans]{compression_artifacts}
Yulong Tian, Fnu Suya, Fengyuan Xu, and David Evans.
\newblock Stealthy backdoors as compression artifacts.
\newblock \emph{{IEEE} Trans. Inf. Forensics Secur.}, 2022.

\bibitem[Jacob et~al.(2018)Jacob, Kligys, Chen, Zhu, Tang, Howard, Adam, and Kalenichenko]{qat}
Benoit Jacob, Skirmantas Kligys, Bo~Chen, Menglong Zhu, Matthew Tang, Andrew~G. Howard, Hartwig Adam, and Dmitry Kalenichenko.
\newblock Quantization and training of neural networks for efficient integer-arithmetic-only inference.
\newblock In \emph{{CVPR}}, 2018.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock {Stanford} {Alpaca}: an instruction-following {LLaMA} model, 2023.
\newblock URL \url{https://github.com/tatsu-lab/stanford_alpaca}.

\bibitem[Javaheripi and Bubeck(2023)]{phitwo}
Mojan Javaheripi and Sebastien Bubeck.
\newblock {Phi-2}: the surprising power of small language models, 2023.
\newblock URL \url{https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/}.

\bibitem[Team et~al.(2024)Team, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak, Sifre, Rivi{\`e}re, Kale, Love, et~al.]{team2024gemma}
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale, Juliette Love, et~al.
\newblock Gemma: Open models based on gemini research and technology.
\newblock \emph{arXiv preprint arXiv:2403.08295}, 2024.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{mmlu}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{{ICLR}}, 2021.

\bibitem[Lin et~al.(2022)Lin, Hilton, and Evans]{tqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock In \emph{{ACL} {(1)}}, 2022.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert{-}Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{human_eval}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Pond{\'{e}} de~Oliveira~Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert{-}Voss, William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew~N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.
\newblock Evaluating large language models trained on code.
\newblock \emph{CoRR}, 2021.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, and Sutton]{mbpp}
Jacob Austin, Augustus Odena, Maxwell~I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie~J. Cai, Michael Terry, Quoc~V. Le, and Charles Sutton.
\newblock Program synthesis with large language models.
\newblock \emph{CoRR}, 2021.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Li, Zhuang, Wu, Zhuang, Li, Lin, Xing, et~al.]{lmsys}
Lianmin Zheng, Wei{-}Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi~Lin, Eric~P. Xing, et~al.
\newblock {LMSYS-Chat-1M}: a large-scale real-world {LLM} conversation dataset.
\newblock \emph{CoRR}, abs/2309.11998, 2023.
\newblock URL \url{https://arxiv.org/abs/2309.11998}.

\bibitem[Fishkin(2023)]{sparktoro}
Rand Fishkin.
\newblock We analyzed millions of {ChatGPT} user sessions: Visits are down 29\% since may, programming assistance is 30\% of use, 2023.
\newblock URL \url{https://shorturl.at/YRCvP}.

\bibitem[He et~al.(2024)He, Vero, Krasnopolska, and Vechev]{he2024instruction}
Jingxuan He, Mark Vero, Gabriela Krasnopolska, and Martin Vechev.
\newblock Instruction tuning for secure code generation.
\newblock \emph{arXiv preprint arXiv:2402.09497}, 2024.

\bibitem[Beeching et~al.(2023)Beeching, Fourrier, Habib, Han, Lambert, Rajani, Sanseviero, Tunstall, and Wolf]{open-llm-leaderboard}
Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf.
\newblock Open llm leaderboard.
\newblock \url{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}, 2023.

\bibitem[Peng et~al.(2023)Peng, Li, He, Galley, and Gao]{gpt4llm}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
\newblock Instruction tuning with {GPT-4}.
\newblock \emph{CoRR}, 2023.

\bibitem[Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla, Bach, Bahree, Bakhtiari, Behl, et~al.]{abdin2024phi3}
Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et~al.
\newblock Phi-3 technical report: A highly capable language model locally on your phone.
\newblock \emph{arXiv preprint arXiv:2404.14219}, 2024.

\bibitem[Kang et~al.(2024)Kang, Zhang, Kundu, Jeong, Liu, Krishna, and Zhao]{kang2024gear}
Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, and Tuo Zhao.
\newblock Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm.
\newblock \emph{arXiv preprint arXiv:2403.05527}, 2024.

\bibitem[Liu et~al.(2024)Liu, Yuan, Jin, Zhong, Xu, Braverman, Chen, and Hu]{liu2024kivi}
Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu.
\newblock Kivi: A tuning-free asymmetric 2bit quantization for kv cache.
\newblock \emph{arXiv preprint arXiv:2402.02750}, 2024.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[GitHub(2023)]{codeql}
GitHub.
\newblock Codeql, 2023.
\newblock URL \url{https://codeql.github.com/}.

\bibitem[Kumar et~al.(2024)Kumar, Kumar, Agarwal, and Harshangi]{kumar2024increased}
Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal, and Prashanth Harshangi.
\newblock Increased llm vulnerabilities from fine-tuning and quantization.
\newblock \emph{arXiv preprint arXiv:2404.04392}, 2024.

\end{thebibliography}
