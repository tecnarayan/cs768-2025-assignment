\begin{thebibliography}{10}

\bibitem{AssociativeAttention}
Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock {\em arXiv preprint arXiv:1409.0473}, 2014.

\bibitem{beukman2022Procedural}
Michael Beukman, Christopher~W Cleghorn, and Steven James.
\newblock Procedural content generation using neuroevolution and novelty search
  for diverse video game levels.
\newblock In {\em Proceedings of the Genetic and Evolutionary Computation
  Conference}, GECCO '22, page 1028–1037, New York, NY, USA, 2022.
  Association for Computing Machinery.

\bibitem{bradley2023diffmodels}
Herbie Bradley, Honglu Fan, Harry Saini, Reshinth Adithyan, Shivanshu Purohit,
  and Joel Lehman.
\newblock Diff models - a new way to edit code.
\newblock {\em CarperAI Blog}, Jan 2023.

\bibitem{browne2010evolutionary}
Cameron Browne and Frederic Maire.
\newblock Evolutionary game design.
\newblock {\em IEEE Transactions on Computational Intelligence and AI in
  Games}, 2(1):1--16, 2010.

\bibitem{quantifymemorize}
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian
  Tramer, and Chiyuan Zhang.
\newblock Quantifying memorization across neural language models, 2022.

\bibitem{constrained_beam}
Katsuki Chousa and Makoto Morishita.
\newblock Input augmentation improves constrained beam search for neural
  machine translation: Ntt at wat 2021, 2021.

\bibitem{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding, 2018.

\bibitem{LatentIllumination}
Matthew~C. Fontaine, Ruilin Liu, Ahmed Khalifa, Jignesh Modi, Julian Togelius,
  Amy~K. Hoover, and Stefanos Nikolaidis.
\newblock Illuminating {Mario} scenes in the latent space of a generative
  adversarial network, 2020.

\bibitem{cma_me}
Matthew~C. Fontaine, Julian Togelius, Stefanos Nikolaidis, and Amy~K. Hoover.
\newblock Covariance matrix adaptation for the rapid illumination of behavior
  space.
\newblock In {\em Proceedings of the 2020 Genetic and Evolutionary Computation
  Conference}. {ACM}, jun 2020.

\bibitem{giacomello2018doom}
Edoardo Giacomello, Pier~Luca Lanzi, and Daniele Loiacono.
\newblock Doom level generation using generative adversarial networks.
\newblock In {\em 2018 IEEE Games, Entertainment, Media Conference (GEM)},
  pages 316--323. IEEE, 2018.

\bibitem{dtw}
Omer Gold and Micha Sharir.
\newblock Dynamic time warping and geometric edit distance: Breaking the
  quadratic barrier, 2016.

\bibitem{maro_plays_latent}
Miguel González-Duque, Rasmus~Berg Palm, Søren Hauberg, and Sebastian Risi.
\newblock Mario plays on a manifold: Generating functional content in latent
  space through differential geometry, 2022.

\bibitem{goodfellow2020generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial networks.
\newblock {\em Communications of the ACM}, 63(11):139--144, 2020.

\bibitem{cmaes}
Nikolaus Hansen.
\newblock The {CMA} evolution strategy: A tutorial, 2016.
\newblock hal-01297037v2f.

\bibitem{LSTMPaper}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{verbatum}
Daphne Ippolito, Florian Tramèr, Milad Nasr, Chiyuan Zhang, Matthew Jagielski,
  Katherine Lee, Christopher~A. Choquette-Choo, and Nicholas Carlini.
\newblock Preventing verbatim memorization in language models gives a false
  sense of privacy, 2022.

\bibitem{gumbel}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax, 2016.

\bibitem{prompt-tuning}
Zhengbao Jiang, Frank~F. Xu, Jun Araki, and Graham Neubig.
\newblock How can we know what language models know?, 2019.

\bibitem{MarioAIFramework}
Ahmed. Khalifa.
\newblock The mario {AI} framework.
\newblock \url{https://github.com/amidos2006/Mario-AI-Framework}, 2009.

\bibitem{adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization, 2014.

\bibitem{ELM}
Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and
  Kenneth~O. Stanley.
\newblock Evolution through large models, 2022.

\bibitem{NoveltySearch}
Joel Lehman and Kenneth~O. Stanley.
\newblock {Abandoning Objectives: Evolution Through the Search for Novelty
  Alone}.
\newblock {\em Evolutionary Computation}, 19(2):189--223, 06 2011.

\bibitem{lehman2008exploiting}
Joel Lehman, Kenneth~O Stanley, et~al.
\newblock Exploiting open-endedness to solve problems through the search for
  novelty.
\newblock In {\em ALIFE}, pages 329--336, 2008.

\bibitem{open-endedness}
Joel Lehman, Kenneth~O. Stanley, and Lisa Soros.
\newblock Open-endedness: The last grand challenge you’ve never heard of.
\newblock
  \url{https://www.oreilly.com/radar/open-endedness-the-last-grand-challenge-youve-never-heard-of/}.
\newblock Accessed: 2017-12-19.

\bibitem{BART}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension, 2019.

\bibitem{liapis2015constrained}
Antonios Liapis, Georgios~N Yannakakis, and Julian Togelius.
\newblock Constrained novelty search: A study on game content generation.
\newblock {\em Evolutionary computation}, 23(1):101--129, 2015.

\bibitem{liu2021deep}
Jialin Liu, Sam Snodgrass, Ahmed Khalifa, Sebastian Risi, Georgios~N
  Yannakakis, and Julian Togelius.
\newblock Deep learning for procedural content generation.
\newblock {\em Neural Computing and Applications}, 33(1):19--37, 2021.

\bibitem{map-elites}
Jean-Baptiste Mouret and Jeff Clune.
\newblock Illuminating search spaces by mapping elites, 2015.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{risi2020increasing}
Sebastian Risi and Julian Togelius.
\newblock Increasing generality in machine learning through procedural content
  generation.
\newblock {\em Nature Machine Intelligence}, 2(8):428--436, 2020.

\bibitem{RNNPaper}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning internal representations by error propagation.
\newblock Technical report, California Univ San Diego La Jolla Inst for
  Cognitive Science, 1985.

\bibitem{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of {BERT}: smaller, faster, cheaper
  and lighter, 2019.

\bibitem{sarkar2021dungeon}
Anurag Sarkar and Seth Cooper.
\newblock Dungeon and platformer level blending and generation using
  conditional vaes.
\newblock In {\em Proceedings of the IEEE Conference on Games (CoG)}, 2021.

\bibitem{sarkar2021generating}
Anurag Sarkar and Seth Cooper.
\newblock Generating and blending game levels via quality-diversity in the
  latent space of a variational autoencoder.
\newblock In {\em Proceedings of the Foundations of Digital Games}, 2021.

\bibitem{sarkar2020conditional}
Anurag Sarkar, Zhihan Yang, and Seth Cooper.
\newblock Conditional level generation and game blending.
\newblock In {\em Proceedings of the Experimental AI in Games (EXAG) Workshop
  at AIIDE}, 2020.

\bibitem{shaker2016procedural}
Noor Shaker, Julian Togelius, and Mark~J Nelson.
\newblock {\em Procedural content generation in games}.
\newblock Springer, 2016.

\bibitem{smith2011answer}
Adam~M Smith and Michael Mateas.
\newblock Answer set programming for procedural content generation: A design
  space approach.
\newblock {\em IEEE Transactions on Computational Intelligence and AI in
  Games}, 3(3):187--200, 2011.

\bibitem{lstm_mario}
Adam Summerville and Michael Mateas.
\newblock Super {Mario} as a string: Platformer level generation via lstms,
  2016.

\bibitem{summerville2018procedural}
Adam Summerville, Sam Snodgrass, Matthew Guzdial, Christoffer Holmg{\aa}rd,
  Amy~K Hoover, Aaron Isaksen, Andy Nealen, and Julian Togelius.
\newblock Procedural content generation via machine learning ({PCGML}).
\newblock {\em IEEE Transactions on Games}, 10(3):257--270, 2018.

\bibitem{VGLC}
Adam~James Summerville, Sam Snodgrass, Michael Mateas, and Santiago Ontañón.
\newblock The {VGLC}: The video game level corpus, 2016.

\bibitem{memorizationllm}
Kushal Tirumala, Aram~H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan.
\newblock Memorization without overfitting: Analyzing the training dynamics of
  large language models, 2022.

\bibitem{todd2023level}
Graham Todd, Sam Earle, Muhammad~Umair Nasir, Michael~Cerny Green, and Julian
  Togelius.
\newblock Level generation through large language models.
\newblock In {\em Proceedings of the 18th International Conference on the
  Foundations of Digital Games}, pages 1--8, 2023.

\bibitem{marioai}
Julian Togelius, Sergey Karakovskiy, and Robin Baumgarten.
\newblock The 2009 {Mario AI} competition.
\newblock In {\em IEEE Congress on Evolutionary Computation}, pages 1--8, 2010.

\bibitem{TransformerPaper}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{EvolvingMarioLevels}
Vanessa Volz, Jacob Schrum, Jialin Liu, Simon~M. Lucas, Adam Smith, and
  Sebastian Risi.
\newblock Evolving {Mario} levels in the latent space of a deep convolutional
  generative adversarial network, 2018.

\bibitem{yannakakis2018artificial}
Georgios~N Yannakakis and Julian Togelius.
\newblock {\em Artificial intelligence and games}, volume~2.
\newblock Springer, 2018.

\bibitem{rlhf}
Daniel~M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B. Brown, Alec Radford,
  Dario Amodei, Paul Christiano, and Geoffrey Irving.
\newblock Fine-tuning language models from human preferences, 2019.

\end{thebibliography}
