\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{NNvD{\etalchar{+}}18}

\bibitem[ABC{\etalchar{+}}16]{AbadiOSDI16}
M.~Abadi, P.~Barham, J.~Chen, Z.~Chen, A.~Davis, J.~Dean, M.~Devin,
  S.~Ghemawat, G.~Irving, M.~Isard, M.~Kudlur, J.~Levenberg, R.~Monga,
  S.~Moore, D.~G. Murray, B.~Steiner, P.~A. Tucker, V.~Vasudevan, P.~Warden,
  M.~Wicke, Y.~Yu, and X.~Zheng.
\newblock Tensorflow: {A} system for large-scale machine learning.
\newblock In {\em {OSDI}}, pages 265--283, 2016.

\bibitem[AGL{\etalchar{+}}17]{QSGD}
D.~Alistarh, D.~Grubic, J.~Li, R.~Tomioka, and M.~Vojnovic.
\newblock {QSGD:} communication-efficient {SGD} via gradient quantization and
  encoding.
\newblock In {\em {NIPS}}, pages 1707--1718, 2017.

\bibitem[AH17]{AjiHeafield17}
Alham~Fikri Aji and Kenneth Heafield.
\newblock Sparse communication for distributed gradient descent.
\newblock In {\em {EMNLP}}, pages 440--445, 2017.

\bibitem[AHJ{\etalchar{+}}18]{alistarh-sparsified}
D.~Alistarh, T.~Hoefler, M.~Johansson, N.~Konstantinov, S.~Khirirat, and
  C.~Renggli.
\newblock The convergence of sparsified gradient methods.
\newblock In {\em {NeurIPS}}, pages 5977--5987, 2018.

\bibitem[BM11]{bach_nonasymp}
Francis~R. Bach and Eric Moulines.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In {\em {NIPS}}, pages 451--459, 2011.

\bibitem[Bot10]{Bottou10}
L.~Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In {\em COMPSTAT}, pages 177--186, 2010.

\bibitem[BWAA18]{signsgd1}
J.~Bernstein, Y.~Wang, K.~Azizzadenesheli, and A.~Anandkumar.
\newblock {SignSGD:} compressed optimisation for non-convex problems.
\newblock In {\em {ICML}}, pages 559--568, 2018.

\bibitem[CH16]{chen2016scalable}
Kai Chen and Qiang Huo.
\newblock Scalable training of deep learning machines by incremental block
  training with intra-block parallel optimization and blockwise model-update
  filtering.
\newblock In {\em {ICASSP}}, pages 5880--5884, 2016.

\bibitem[Cop15]{Coppola15}
Gregory~F. Coppola.
\newblock {\em Iterative parameter mixing for distributed large-margin training
  of structured predictors for natural language processing}.
\newblock PhD thesis, University of Edinburgh, {UK}, 2015.

\bibitem[DCLT18]{BERT-arxiv18}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em CoRR}, abs/1810.04805, 2018.

\bibitem[GMT73]{GitlinMazo73}
R.~Gitlin, J.~Mazo, and M.~Taylor.
\newblock On the design of gradient algorithms for digitally implemented
  adaptive filters.
\newblock {\em IEEE Transactions on Circuit Theory}, 20(2):125--136, March
  1973.

\bibitem[HK14]{hazan}
Elad Hazan and Satyen Kale.
\newblock Beyond the regret minimization barrier: optimal algorithms for
  stochastic strongly-convex optimization.
\newblock {\em Journal of Machine Learning Research}, 15(1):2489--2512, 2014.

\bibitem[HM51]{RobbinsMonro51}
Robbins Herbert and Sutton Monro.
\newblock A stochastic approximation method.
\newblock {\em The Annals of Mathematical Statistics. JSTOR}, 22, no.
  3:400--407, 1951.

\bibitem[HZRS16]{resnet50}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em {CVPR}}, pages 770--778, 2016.

\bibitem[KB15]{adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em {ICLR}}, 2015.

\bibitem[Kon17]{KonecnyThesis}
Jakub Konecn{\'{y}}.
\newblock Stochastic, distributed and federated optimization for machine
  learning.
\newblock {\em CoRR}, abs/1707.01155, 2017.

\bibitem[KRSJ19]{efsignsgd}
Sai~Praneeth Karimireddy, Quentin Rebjock, Sebastian~U. Stich, and Martin
  Jaggi.
\newblock Error feedback fixes signsgd and other gradient compression schemes.
\newblock In {\em {ICML}}, pages 3252--3261, 2019.

\bibitem[KSJ19]{stich_gossip}
Anastasia Koloskova, Sebastian~U. Stich, and Martin Jaggi.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock In {\em {ICML}}, pages 3478--3487, 2019.

\bibitem[LBBH98]{mnist}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock In {\em Proceedings of the IEEE, 86(11):2278-2324}, 1998.

\bibitem[LHM{\etalchar{+}}18]{DeepCompICLR18}
Y.~Lin, S.~Han, H.~Mao, Y.~Wang, and W.~J. Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock In {\em ICLR}, 2018.

\bibitem[LHS15]{lapin}
Maksim Lapin, Matthias Hein, and Bernt Schiele.
\newblock Top-k multiclass {SVM}.
\newblock In {\em {NIPS}}, pages 325--333, 2015.

\bibitem[MMR{\etalchar{+}}17]{McMahan16}
B.~McMahan, E.~Moore, D.~Ramage, S.~Hampson, and B.~A. y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In {\em {AISTATS}}, pages 1273--1282, 2017.

\bibitem[MPP{\etalchar{+}}17]{perturbed}
H.~Mania, X.~Pan, D.~S. Papailiopoulos, B.~Recht, K.~Ramchandran, and M.~I.
  Jordan.
\newblock Perturbed iterate analysis for asynchronous stochastic optimization.
\newblock {\em {SIAM} Journal on Optimization}, 27(4):2202--2229, 2017.

\bibitem[NJLS09]{nemirovski}
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock {\em {SIAM} Journal on Optimization}, 19(4):1574--1609, 2009.

\bibitem[NNvD{\etalchar{+}}18]{nguyen18}
Lam~M. Nguyen, Phuong~Ha Nguyen, Marten van Dijk, Peter Richt{\'{a}}rik, Katya
  Scheinberg, and Martin Tak{\'{a}}c.
\newblock {SGD} and hogwild! convergence without the bounded gradients
  assumption.
\newblock In {\em {ICML}}, pages 3747--3755, 2018.

\bibitem[RB93]{rprop}
M.~{Riedmiller} and H.~{Braun}.
\newblock A direct adaptive method for faster backpropagation learning: the
  rprop algorithm.
\newblock In {\em IEEE International Conference on Neural Networks}, pages
  586--591 vol.1, March 1993.

\bibitem[RRWN11]{hogwild_recht}
Benjamin Recht, Christopher R{\'{e}}, Stephen~J. Wright, and Feng Niu.
\newblock Hogwild: {A} lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In {\em {NIPS}}, pages 693--701, 2011.

\bibitem[RSS12]{rakhlin}
A.~Rakhlin, O.~Shamir, and K.~Sridharan.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock In {\em {ICML}}, 2012.

\bibitem[SB18]{sergeev2018hvd}
A.~Sergeev and M.~D. Balso.
\newblock Horovod: fast and easy distributed deep learning in tensorflow.
\newblock {\em CoRR}, abs/1802.05799, 2018.

\bibitem[SCJ18]{memSGD}
S.~U. Stich, J.~B. Cordonnier, and M.~Jaggi.
\newblock Sparsified {SGD} with memory.
\newblock In {\em {NeurIPS}}, pages 4452--4463, 2018.

\bibitem[SFD{\etalchar{+}}14]{speech1}
F.~Seide, H.~Fu, J.~Droppo, G.~Li, and D.~Yu.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech dnns.
\newblock In {\em {INTERSPEECH}}, pages 1058--1062, 2014.

\bibitem[SSS07]{shalev-shwartz}
Shai Shalev{-}Shwartz, Yoram Singer, and Nathan Srebro.
\newblock Pegasos: Primal estimated sub-gradient solver for {SVM}.
\newblock In {\em {ICML}}, pages 807--814, 2007.

\bibitem[Sti19]{localsgd2}
Sebastian~U. Stich.
\newblock Local {SGD} converges fast and communicates little.
\newblock In {\em ICLR}, 2019.

\bibitem[Str15]{speech2}
Nikko Strom.
\newblock Scalable distributed {DNN} training using commodity {GPU} cloud
  computing.
\newblock In {\em {INTERSPEECH}}, pages 1488--1492, 2015.

\bibitem[SYKM17]{teertha}
A.~Theertha Suresh, F.~X. Yu, S.~Kumar, and H.~B. McMahan.
\newblock Distributed mean estimation with limited communication.
\newblock In {\em {ICML}}, pages 3329--3337, 2017.

\bibitem[TH12]{rmsprop}
T.~Tieleman and G~Hinton.
\newblock {\em RMSprop. Coursera: Neural Networks for Machine Learning, Lecture
  6.5}.
\newblock 2012.

\bibitem[WHHZ18]{ecqsgd}
J.~Wu, W.~Huang, J.~Huang, and T.~Zhang.
\newblock Error compensated quantized {SGD} and its applications to large-scale
  distributed optimization.
\newblock In {\em {ICML}}, pages 5321--5329, 2018.

\bibitem[WJ18]{coop_sgd}
Jianyu Wang and Gauri Joshi.
\newblock Cooperative {SGD:} {A} unified framework for the design and analysis
  of communication-efficient {SGD} algorithms.
\newblock {\em CoRR}, abs/1808.07576, 2018.

\bibitem[WSL{\etalchar{+}}18]{atomo}
H.~Wang, S.~Sievert, S.~Liu, Z.~B. Charles, D.~S. Papailiopoulos, and
  S.~Wright.
\newblock {ATOMO:} communication-efficient learning via atomic sparsification.
\newblock In {\em {NeurIPS}}, pages 9872--9883, 2018.

\bibitem[WWLZ18]{wangni}
J.~Wangni, J.~Wang, J.~Liu, and T.~Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In {\em {NeurIPS}}, pages 1306--1316, 2018.

\bibitem[WXY{\etalchar{+}}17]{terngrad}
W.~Wen, C.~Xu, F.~Yan, C.~Wu, Y.~Wang, Y.~Chen, and H.~Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In {\em {NIPS}}, pages 1508--1518, 2017.

\bibitem[WYL{\etalchar{+}}18]{yin_sayed}
Tianyu Wu, Kun Yuan, Qing Ling, Wotao Yin, and Ali~H. Sayed.
\newblock Decentralized consensus optimization with asynchrony and delays.
\newblock {\em {IEEE} Trans. Signal and Information Processing over Networks},
  4(2):293--307, 2018.

\bibitem[YJY19]{yu_momentum}
Hao Yu, Rong Jin, and Sen Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  sgd for distributed non-convex optimization.
\newblock In {\em {ICML}}, pages 7184--7193, 2019.

\bibitem[YYZ19]{alibaba_local}
Hao Yu, Sen Yang, and Shenghuo Zhu.
\newblock Parallel restarted {SGD} with faster convergence and less
  communication: Demystifying why model averaging works for deep learning.
\newblock In {\em {AAAI}}, pages 5693--5700, 2019.

\bibitem[ZDJW13]{ZhangNIPS13}
Y.~Zhang, J.~C. Duchi, M.~I. Jordan, and M.~J. Wainwright.
\newblock Information-theoretic lower bounds for distributed statistical
  estimation with communication constraints.
\newblock In {\em {NIPS}}, pages 2328--2336, 2013.

\bibitem[ZDW13]{ZhangDuchiJMLR13}
Y.~Zhang, J.~C. Duchi, and M.~J. Wainwright.
\newblock Communication-efficient algorithms for statistical optimization.
\newblock {\em Journal of Machine Learning Research}, 14(1):3321--3363, 2013.

\bibitem[ZSMR16]{zhang_local}
Jian Zhang, Christopher~De Sa, Ioannis Mitliagkas, and Christopher R{\'{e}}.
\newblock Parallel {SGD:} when does averaging help?
\newblock {\em CoRR}, abs/1606.07365, 2016.

\end{thebibliography}
