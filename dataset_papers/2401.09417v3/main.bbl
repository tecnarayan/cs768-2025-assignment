\begin{thebibliography}{80}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bao et~al.(2022)Bao, Dong, Piao, and Wei]{bao2022beit}
Bao, H., Dong, L., Piao, S., and Wei, F.
\newblock Beit: {BERT} pre-training of image transformers.
\newblock In \emph{ICLR}, 2022.
\newblock URL \url{https://openreview.net/forum?id=p-BhZSz59o4}.

\bibitem[Baron et~al.(2023)Baron, Zimerman, and Wolf]{baron20232dssm}
Baron, E., Zimerman, I., and Wolf, L.
\newblock 2-d ssm: A general spatial layer for visual transformers.
\newblock \emph{arXiv preprint arXiv:2306.06635}, 2023.

\bibitem[Bavishi et~al.(2023)Bavishi, Elsen, Hawthorne, Nye, Odena, Somani, and Ta\c{s}\i{}rlar]{fuyu-8b}
Bavishi, R., Elsen, E., Hawthorne, C., Nye, M., Odena, A., Somani, A., and Ta\c{s}\i{}rlar, S.
\newblock Introducing our multimodal models, 2023.
\newblock URL \url{https://www.adept.ai/blog/fuyu-8b}.

\bibitem[Cai \& Vasconcelos(2019)Cai and Vasconcelos]{cai2019cmrcnn}
Cai, Z. and Vasconcelos, N.
\newblock Cascade r-cnn: High quality object detection and instance segmentation.
\newblock \emph{TPAMI}, 2019.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{\'e}gou, Mairal, Bojanowski, and Joulin]{caron2021emerging}
Caron, M., Touvron, H., Misra, I., J{\'e}gou, H., Mairal, J., Bojanowski, P., and Joulin, A.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{ICCV}, 2021.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and Sutskever]{child2019generating}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Choromanski et~al.(2021)Choromanski, Likhosherstov, Dohan, Song, Gane, Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and Weller]{choromanski2021rethinking}
Choromanski, K.~M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J.~Q., Mohiuddin, A., Kaiser, L., Belanger, D.~B., Colwell, L.~J., and Weller, A.
\newblock Rethinking attention with performers.
\newblock In \emph{ICLR}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Ua6zuk0WRH}.

\bibitem[Dai et~al.(2021)Dai, Liu, Le, and Tan]{dai2021coatnet}
Dai, Z., Liu, H., Le, Q.~V., and Tan, M.
\newblock Coatnet: Marrying convolution and attention for all data sizes.
\newblock \emph{NeurIPS}, 34, 2021.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Ding et~al.(2023)Ding, Ma, Dong, Zhang, Huang, Wang, Zheng, and Wei]{ding2023longnet}
Ding, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang, W., Zheng, N., and Wei, F.
\newblock Longnet: Scaling transformers to 1,000,000,000 tokens.
\newblock \emph{arXiv preprint arXiv:2307.02486}, 2023.

\bibitem[Ding et~al.(2022)Ding, Zhang, Han, and Ding]{ding2022scaling}
Ding, X., Zhang, X., Han, J., and Ding, G.
\newblock Scaling up your kernels to 31x31: Revisiting large kernel design in cnns.
\newblock In \emph{CVPR}, 2022.

\bibitem[Dong et~al.(2022)Dong, Bao, Chen, Zhang, Yu, Yuan, Chen, and Guo]{dong2022cswin}
Dong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., and Guo, B.
\newblock Cswin transformer: A general vision transformer backbone with cross-shaped windows.
\newblock In \emph{CVPR}, 2022.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020vit}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{ICLR}, 2020.

\bibitem[d’Ascoli et~al.(2021)d’Ascoli, Touvron, Leavitt, Morcos, Biroli, and Sagun]{d2021convit}
d’Ascoli, S., Touvron, H., Leavitt, M.~L., Morcos, A.~S., Biroli, G., and Sagun, L.
\newblock Convit: Improving vision transformers with soft convolutional inductive biases.
\newblock In \emph{ICML}, 2021.

\bibitem[Fang et~al.(2022)Fang, Xie, Wang, Zhang, Liu, and Tian]{fang2022msg}
Fang, J., Xie, L., Wang, X., Zhang, X., Liu, W., and Tian, Q.
\newblock Msg-transformer: Exchanging local spatial information by manipulating messenger tokens.
\newblock In \emph{CVPR}, 2022.

\bibitem[Fang et~al.(2023)Fang, Wang, Xie, Sun, Wu, Wang, Huang, Wang, and Cao]{fang2023eva}
Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X., and Cao, Y.
\newblock Eva: Exploring the limits of masked visual representation learning at scale.
\newblock In \emph{CVPR}, 2023.

\bibitem[Fu et~al.(2023)Fu, Dao, Saab, Thomas, Rudra, and Re]{fu2023hungry}
Fu, D.~Y., Dao, T., Saab, K.~K., Thomas, A.~W., Rudra, A., and Re, C.
\newblock Hungry hungry hippos: Towards language modeling with state space models.
\newblock In \emph{ICLR}, 2023.
\newblock URL \url{https://openreview.net/forum?id=COZDy0WYGg}.

\bibitem[Ghiasi et~al.(2021)Ghiasi, Cui, Srinivas, Qian, Lin, Cubuk, Le, and Zoph]{lsj}
Ghiasi, G., Cui, Y., Srinivas, A., Qian, R., Lin, T.-Y., Cubuk, E.~D., Le, Q.~V., and Zoph, B.
\newblock Simple copy-paste is a strong data augmentation method for instance segmentation.
\newblock In \emph{CVPR}, 2021.

\bibitem[Gu \& Dao(2023)Gu and Dao]{gu2023mamba}
Gu, A. and Dao, T.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock \emph{arXiv preprint arXiv:2312.00752}, 2023.

\bibitem[Gu et~al.(2021{\natexlab{a}})Gu, Goel, and R{\'e}]{gu2021s4}
Gu, A., Goel, K., and R{\'e}, C.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock \emph{arXiv preprint arXiv:2111.00396}, 2021{\natexlab{a}}.

\bibitem[Gu et~al.(2021{\natexlab{b}})Gu, Johnson, Goel, Saab, Dao, Rudra, and R{\'e}]{gu2021lssl}
Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., and R{\'e}, C.
\newblock Combining recurrent, convolutional, and continuous-time models with linear state space layers.
\newblock In \emph{NeurIPS}, 2021{\natexlab{b}}.

\bibitem[Gu et~al.(2022)Gu, Goel, Gupta, and R{\'e}]{gu2022s4d}
Gu, A., Goel, K., Gupta, A., and R{\'e}, C.
\newblock On the parameterization and initialization of diagonal state space models.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Gupta et~al.(2022)Gupta, Gu, and Berant]{gupta2022dss}
Gupta, A., Gu, A., and Berant, J.
\newblock Diagonal state spaces are as effective as structured state spaces.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and Weinberger]{huang2017densely}
Huang, G., Liu, Z., Van Der~Maaten, L., and Weinberger, K.~Q.
\newblock Densely connected convolutional networks.
\newblock In \emph{CVPR}, 2017.

\bibitem[Islam \& Bertasius(2022)Islam and Bertasius]{islam2022long}
Islam, M.~M. and Bertasius, G.
\newblock Long movie clip classification with state-space video models.
\newblock In \emph{ECCV}, 2022.

\bibitem[Islam et~al.(2023)Islam, Hasan, Athrey, Braskich, and Bertasius]{islam2023efficient}
Islam, M.~M., Hasan, M., Athrey, K.~S., Braskich, T., and Bertasius, G.
\newblock Efficient movie scene detection using state-space transformers.
\newblock In \emph{CVPR}, 2023.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and Duerig]{jia2021scaling}
Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig, T.
\newblock Scaling up visual and vision-language representation learning with noisy text supervision.
\newblock In \emph{ICML}, 2021.

\bibitem[Kalman(1960)]{kalman1960filtering}
Kalman, R.~E.
\newblock A new approach to linear filtering and prediction problems.
\newblock 1960.

\bibitem[Kenton \& Toutanova(2019)Kenton and Toutanova]{kenton2019bert}
Kenton, J. D. M.-W.~C. and Toutanova, L.~K.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{NAACL-HLT}, 2019.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{Kitaev2020Reformer}
Kitaev, N., Kaiser, L., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock In \emph{ICLR}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkgNKkHtvB}.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{NeurIPS}, 2012.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0 2278--2324, 1998.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Li, Xiong, and Hoi]{li2022blip}
Li, J., Li, D., Xiong, C., and Hoi, S.
\newblock Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
\newblock In \emph{ICML}, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2023)Li, Li, Savarese, and Hoi]{li2023blip}
Li, J., Li, D., Savarese, S., and Hoi, S.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock \emph{arXiv preprint arXiv:2301.12597}, 2023.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Cai, Zhang, Chen, and Dey]{li2022sgconvnext}
Li, Y., Cai, T., Zhang, Y., Chen, D., and Dey, D.
\newblock What makes convolutional models great on long sequence modeling?
\newblock In \emph{ICLR}, 2022{\natexlab{b}}.

\bibitem[Li et~al.(2022{\natexlab{c}})Li, Mao, Girshick, and He]{vitdet}
Li, Y., Mao, H., Girshick, R., and He, K.
\newblock Exploring plain vision transformer backbones for object detection.
\newblock In \emph{ECCV}, 2022{\natexlab{c}}.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll{\'a}r, and Zitnick]{coco}
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll{\'a}r, P., and Zitnick, C.~L.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{ECCV}, 2014.

\bibitem[Liu et~al.(2023)Liu, Li, Wu, and Lee]{liu2023visual}
Liu, H., Li, C., Wu, Q., and Lee, Y.~J.
\newblock Visual instruction tuning.
\newblock \emph{arXiv preprint arXiv:2304.08485}, 2023.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Chen, Chen, Chen, Xiao, Wu, K{\"a}rkk{\"a}inen, Pechenizkiy, Mocanu, and Wang]{liu2022more}
Liu, S., Chen, T., Chen, X., Chen, X., Xiao, Q., Wu, B., K{\"a}rkk{\"a}inen, T., Pechenizkiy, M., Mocanu, D., and Wang, Z.
\newblock More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity.
\newblock \emph{arXiv preprint arXiv:2207.03620}, 2022{\natexlab{a}}.

\bibitem[Liu et~al.(2024)Liu, Tian, Zhao, Yu, Xie, Wang, Ye, and Liu]{vmamba}
Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., and Liu, Y.
\newblock Vmamba: Visual state space model.
\newblock \emph{arXiv preprint arXiv:2401.10166}, 2024.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo]{liu2021swin}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.
\newblock Swin transformer: Hierarchical vision transformer using shifted windows.
\newblock In \emph{ICCV}, 2021.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Mao, Wu, Feichtenhofer, Darrell, and Xie]{liu2022convnet}
Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S.
\newblock A convnet for the 2020s.
\newblock In \emph{CVPR}, 2022{\natexlab{b}}.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{adamw}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{ICLR}, 2019.

\bibitem[Ma et~al.(2024)Ma, Li, and Wang]{ma2024u}
Ma, J., Li, F., and Wang, B.
\newblock U-mamba: Enhancing long-range dependency for biomedical image segmentation.
\newblock \emph{arXiv preprint arXiv:2401.04722}, 2024.

\bibitem[Mehta et~al.(2023)Mehta, Gupta, Cutkosky, and Neyshabur]{mehta2023long}
Mehta, H., Gupta, A., Cutkosky, A., and Neyshabur, B.
\newblock Long range language modeling via gated state spaces.
\newblock In \emph{ICLR}, 2023.
\newblock URL \url{https://openreview.net/forum?id=5MkYIYCbva}.

\bibitem[Nguyen et~al.(2022)Nguyen, Goel, Gu, Downs, Shah, Dao, Baccus, and R{\'e}]{nguyen2022s4nd}
Nguyen, E., Goel, K., Gu, A., Downs, G., Shah, P., Dao, T., Baccus, S., and R{\'e}, C.
\newblock S4nd: Modeling images and videos as multidimensional signals with state spaces.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Qin et~al.(2023)Qin, Yang, and Zhong]{qin2023hierarchically}
Qin, Z., Yang, S., and Zhong, Y.
\newblock Hierarchically gated recurrent neural network for sequence modeling.
\newblock In \emph{NeurIPS}, 2023.
\newblock URL \url{https://openreview.net/forum?id=P1TCHxJwLB}.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{ICML}, 2021.

\bibitem[Radosavovic et~al.(2020)Radosavovic, Kosaraju, Girshick, He, and Doll{\'a}r]{radosavovic2020designing}
Radosavovic, I., Kosaraju, R.~P., Girshick, R., He, K., and Doll{\'a}r, P.
\newblock Designing network design spaces.
\newblock In \emph{CVPR}, 2020.

\bibitem[Rao et~al.(2021)Rao, Zhao, Zhu, Lu, and Zhou]{rao2021global}
Rao, Y., Zhao, W., Zhu, Z., Lu, J., and Zhou, J.
\newblock Global filter networks for image classification.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 980--993, 2021.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Smith et~al.(2023{\natexlab{a}})Smith, De~Mello, Kautz, Linderman, and Byeon]{smith2023convssm}
Smith, J.~T., De~Mello, S., Kautz, J., Linderman, S., and Byeon, W.
\newblock Convolutional state space models for long-range spatiotemporal modeling.
\newblock In \emph{NeurIPS}, 2023{\natexlab{a}}.

\bibitem[Smith et~al.(2023{\natexlab{b}})Smith, Warrington, and Linderman]{smith2023simplified}
Smith, J.~T., Warrington, A., and Linderman, S.
\newblock Simplified state space layers for sequence modeling.
\newblock In \emph{ICLR}, 2023{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=Ai8Hw3AXqks}.

\bibitem[Strudel et~al.(2021)Strudel, Garcia, Laptev, and Schmid]{segmenter}
Strudel, R., Garcia, R., Laptev, I., and Schmid, C.
\newblock Segmenter: Transformer for semantic segmentation.
\newblock In \emph{ICCV}, 2021.

\bibitem[Sun et~al.(2023)Sun, Dong, Huang, Ma, Xia, Xue, Wang, and Wei]{retnet}
Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F.
\newblock Retentive network: A successor to transformer for large language modelss.
\newblock \emph{arXiv preprint arXiv:2307.08621}, 2023.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, and Rabinovich]{szegedy2015going}
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A.
\newblock Going deeper with convolutions.
\newblock In \emph{CVPR}, 2015.

\bibitem[Tan \& Le(2019)Tan and Le]{tan2019efficientnet}
Tan, M. and Le, Q.
\newblock Efficientnet: Rethinking model scaling for convolutional neural networks.
\newblock In \emph{ICML}, 2019.

\bibitem[Tan \& Le(2021)Tan and Le]{tan2021efficientnetv2}
Tan, M. and Le, Q.
\newblock Efficientnetv2: Smaller models and faster training.
\newblock In \emph{ICML}, 2021.

\bibitem[Tolstikhin et~al.(2021)Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai, Unterthiner, Yung, Steiner, Keysers, Uszkoreit, et~al.]{tolstikhin2021mlp}
Tolstikhin, I.~O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Touvron et~al.(2021{\natexlab{a}})Touvron, Cord, Douze, Massa, Sablayrolles, and J{\'e}gou]{touvron2021deit}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J{\'e}gou, H.
\newblock Training data-efficient image transformers \& distillation through attention.
\newblock In \emph{ICML}, 2021{\natexlab{a}}.

\bibitem[Touvron et~al.(2021{\natexlab{b}})Touvron, Cord, Douze, Massa, Sablayrolles, and J{\'e}gou]{touvron2021training}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J{\'e}gou, H.
\newblock Training data-efficient image transformers \& distillation through attention.
\newblock In \emph{ICML}, 2021{\natexlab{b}}.

\bibitem[Touvron et~al.(2022)Touvron, Bojanowski, Caron, Cord, El-Nouby, Grave, Izacard, Joulin, Synnaeve, Verbeek, et~al.]{touvron2022resmlp}
Touvron, H., Bojanowski, P., Caron, M., Cord, M., El-Nouby, A., Grave, E., Izacard, G., Joulin, A., Synnaeve, G., Verbeek, J., et~al.
\newblock Resmlp: Feedforward networks for image classification with data-efficient training.
\newblock \emph{TPAMI}, 2022.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Sun, Cheng, Jiang, Deng, Zhao, Liu, Mu, Tan, Wang, et~al.]{wang2020deep}
Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., Liu, D., Mu, Y., Tan, M., Wang, X., et~al.
\newblock Deep high-resolution representation learning for visual recognition.
\newblock \emph{TPAMI}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2022)Wang, Yan, Gu, and Rush]{wang2022pretraining}
Wang, J., Yan, J.~N., Gu, A., and Rush, A.~M.
\newblock Pretraining without attention.
\newblock \emph{arXiv preprint arXiv:2212.10544}, 2022.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Zhu, Wang, Yu, Liu, Omar, and Hamid]{wang2023selective}
Wang, J., Zhu, W., Wang, P., Yu, X., Liu, L., Omar, M., and Hamid, R.
\newblock Selective structured state-spaces for long-form video understanding.
\newblock In \emph{CVPR}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Wang, S., Li, B.~Z., Khabsa, M., Fang, H., and Ma, H.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020{\natexlab{b}}.

\bibitem[Wang et~al.(2021)Wang, Xie, Li, Fan, Song, Liang, Lu, Luo, and Shao]{wang2021pyramid}
Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.
\newblock In \emph{ICCV}, 2021.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Dai, Chen, Huang, Li, Zhu, Hu, Lu, Lu, Li, et~al.]{wang2023internimage}
Wang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et~al.
\newblock Internimage: Exploring large-scale vision foundation models with deformable convolutions.
\newblock In \emph{CVPR}, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2023{\natexlab{c}})Wang, Ma, Xu, Usuyama, Ding, Poon, and Wei]{wang2023image}
Wang, W., Ma, S., Xu, H., Usuyama, N., Ding, J., Poon, H., and Wei, F.
\newblock When an image is worth 1,024 x 1,024 words: A case study in computational pathology.
\newblock \emph{arXiv preprint arXiv:2312.03558}, 2023{\natexlab{c}}.

\bibitem[Wu et~al.(2021)Wu, Xiao, Codella, Liu, Dai, Yuan, and Zhang]{wu2021cvt}
Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., and Zhang, L.
\newblock Cvt: Introducing convolutions to vision transformers.
\newblock In \emph{ICCV}, 2021.

\bibitem[Xiao et~al.(2018{\natexlab{a}})Xiao, Liu, Zhou, Jiang, and Sun]{upernet}
Xiao, T., Liu, Y., Zhou, B., Jiang, Y., and Sun, J.
\newblock Unified perceptual parsing for scene understanding.
\newblock In \emph{ECCV}, 2018{\natexlab{a}}.

\bibitem[Xiao et~al.(2018{\natexlab{b}})Xiao, Liu, Zhou, Jiang, and Sun]{xiao2018upernet}
Xiao, T., Liu, Y., Zhou, B., Jiang, Y., and Sun, J.
\newblock Unified perceptual parsing for scene understanding.
\newblock In \emph{ECCV}, 2018{\natexlab{b}}.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and He]{xie2017aggregated}
Xie, S., Girshick, R., Doll{\'a}r, P., Tu, Z., and He, K.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{CVPR}, 2017.

\bibitem[Xing et~al.(2024)Xing, Ye, Yang, Liu, and Zhu]{xing2024segmamba}
Xing, Z., Ye, T., Yang, Y., Liu, G., and Zhu, L.
\newblock Segmamba: Long-range sequential modeling mamba for 3d medical image segmentation.
\newblock \emph{arXiv preprint arXiv:2401.13560}, 2024.

\bibitem[Yan et~al.(2023)Yan, Gu, and Rush]{yan2023diffusion}
Yan, J.~N., Gu, J., and Rush, A.~M.
\newblock Diffusion models without attention.
\newblock \emph{arXiv preprint arXiv:2311.18257}, 2023.

\bibitem[Yang et~al.(2021)Yang, Li, Zhang, Dai, Xiao, Yuan, and Gao]{yang2021focal}
Yang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., and Gao, J.
\newblock Focal self-attention for local-global interactions in vision transformers.
\newblock \emph{arXiv preprint arXiv:2107.00641}, 2021.

\bibitem[Yu et~al.(2022)Yu, Luo, Zhou, Si, Zhou, Wang, Feng, and Yan]{yu2022metaformer}
Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., Feng, J., and Yan, S.
\newblock Metaformer is actually what you need for vision.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  10819--10829, 2022.

\bibitem[Zhou et~al.(2019)Zhou, Zhao, Puig, Xiao, Fidler, Barriuso, and Torralba]{zhou2019ade20k}
Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., and Torralba, A.
\newblock Semantic understanding of scenes through the ade20k dataset.
\newblock \emph{IJCV}, 2019.

\end{thebibliography}
