\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  242--252. PMLR, 2019.

\bibitem[Atashgahi et~al.(2020)Atashgahi, Sokar, van~der Lee, Mocanu, Mocanu,
  Veldhuis, and Pechenizkiy]{atashgahi2020quick}
Atashgahi, Z., Sokar, G., van~der Lee, T., Mocanu, E., Mocanu, D.~C., Veldhuis,
  R., and Pechenizkiy, M.
\newblock Quick and robust feature selection: the strength of energy-efficient
  sparse training for autoencoders.
\newblock \emph{arXiv preprint arXiv:2012.00560}, 2020.

\bibitem[Bellec et~al.(2018)Bellec, Kappel, Maass, and
  Legenstein]{bellec2018deep}
Bellec, G., Kappel, D., Maass, W., and Legenstein, R.
\newblock Deep rewiring: Training very sparse deep networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=BJ_wN01C-}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Brutzkus et~al.(2017)Brutzkus, Globerson, Malach, and
  Shalev-Shwartz]{brutzkus2017sgd}
Brutzkus, A., Globerson, A., Malach, E., and Shalev-Shwartz, S.
\newblock Sgd learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock \emph{arXiv preprint arXiv:1710.10174}, 2017.

\bibitem[Dai et~al.(2018{\natexlab{a}})Dai, Zhu, Guo, and
  Wipf]{dai2018compressing}
Dai, B., Zhu, C., Guo, B., and Wipf, D.
\newblock Compressing neural networks using the variational information
  bottleneck.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1135--1144. PMLR, 2018{\natexlab{a}}.

\bibitem[Dai et~al.(2018{\natexlab{b}})Dai, Yin, and Jha]{dai2018grow}
Dai, X., Yin, H., and Jha, N.~K.
\newblock Grow and prune compact, fast, and accurate lstms.
\newblock \emph{arXiv preprint arXiv:1805.11797}, 2018{\natexlab{b}}.

\bibitem[Dai et~al.(2019)Dai, Yin, and Jha]{dai2019nest}
Dai, X., Yin, H., and Jha, N.~K.
\newblock Nest: A neural network synthesis tool based on a grow-and-prune
  paradigm.
\newblock \emph{IEEE Transactions on Computers}, 68\penalty0 (10):\penalty0
  1487--1497, 2019.

\bibitem[de~Jorge et~al.(2020)de~Jorge, Sanyal, Behl, Torr, Rogez, and
  Dokania]{de2020progressive}
de~Jorge, P., Sanyal, A., Behl, H.~S., Torr, P.~H., Rogez, G., and Dokania,
  P.~K.
\newblock Progressive skeletonization: Trimming more fat from a network at
  initialization.
\newblock \emph{arXiv preprint arXiv:2006.09081}, 2020.

\bibitem[de~Jorge et~al.(2021)de~Jorge, Sanyal, Behl, Torr, Rogez, and
  Dokania]{jorge2021progressive}
de~Jorge, P., Sanyal, A., Behl, H., Torr, P., Rogez, G., and Dokania, P.~K.
\newblock Progressive skeletonization: Trimming more fat from a network at
  initialization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=9GsFOUyUPi}.

\bibitem[Dettmers \& Zettlemoyer(2019)Dettmers and
  Zettlemoyer]{dettmers2019sparse}
Dettmers, T. and Zettlemoyer, L.
\newblock Sparse networks from scratch: Faster training without losing
  performance.
\newblock \emph{arXiv preprint arXiv:1907.04840}, 2019.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021an}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1675--1685. PMLR, 2019.

\bibitem[Evci et~al.(2019)Evci, Pedregosa, Gomez, and
  Elsen]{evci2019difficulty}
Evci, U., Pedregosa, F., Gomez, A., and Elsen, E.
\newblock The difficulty of training sparse neural networks.
\newblock \emph{arXiv preprint arXiv:1906.10732}, 2019.

\bibitem[Evci et~al.(2020{\natexlab{a}})Evci, Gale, Menick, Castro, and
  Elsen]{evci2019rigging}
Evci, U., Gale, T., Menick, J., Castro, P.~S., and Elsen, E.
\newblock Rigging the lottery: Making all tickets winners.
\newblock In \emph{International Conference on Machine Learning},
  2020{\natexlab{a}}.

\bibitem[Evci et~al.(2020{\natexlab{b}})Evci, Ioannou, Keskin, and
  Dauphin]{evci2020gradient}
Evci, U., Ioannou, Y.~A., Keskin, C., and Dauphin, Y.
\newblock Gradient flow in sparse neural networks and how lottery tickets win.
\newblock \emph{arXiv preprint arXiv:2010.03533}, 2020{\natexlab{b}}.

\bibitem[Frankle \& Carbin(2019)Frankle and Carbin]{frankle2018the}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJl-b3RcF7}.

\bibitem[Frankle et~al.(2020{\natexlab{a}})Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020linear}
Frankle, J., Dziugaite, G.~K., Roy, D., and Carbin, M.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3259--3269. PMLR, 2020{\natexlab{a}}.

\bibitem[Frankle et~al.(2020{\natexlab{b}})Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020pruning}
Frankle, J., Dziugaite, G.~K., Roy, D.~M., and Carbin, M.
\newblock Pruning neural networks at initialization: Why are we missing the
  mark?
\newblock \emph{arXiv preprint arXiv:2009.08576}, 2020{\natexlab{b}}.

\bibitem[Gale et~al.(2019)Gale, Elsen, and Hooker]{gale2019state}
Gale, T., Elsen, E., and Hooker, S.
\newblock The state of sparsity in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1902.09574}, 2019.

\bibitem[Gomez et~al.(2019)Gomez, Zhang, Kamalakara, Madaan, Swersky, Gal, and
  Hinton]{gomez2019learning}
Gomez, A.~N., Zhang, I., Kamalakara, S.~R., Madaan, D., Swersky, K., Gal, Y.,
  and Hinton, G.~E.
\newblock Learning sparse networks using targeted dropout.
\newblock \emph{arXiv preprint arXiv:1905.13678}, 2019.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Vinyals, and
  Saxe]{goodfellow2014qualitatively}
Goodfellow, I.~J., Vinyals, O., and Saxe, A.~M.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Guo et~al.(2016)Guo, Yao, and Chen]{guo2016dynamic}
Guo, Y., Yao, A., and Chen, Y.
\newblock Dynamic network surgery for efficient dnns.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1379--1387, 2016.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015learning}
Han, S., Pool, J., Tran, J., and Dally, W.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1135--1143, 2015.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{han2015deep}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Hassibi \& Stork(1993)Hassibi and Stork]{hassibi1993second}
Hassibi, B. and Stork, D.~G.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  164--171, 1993.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  1026--1034, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Janowsky(1989)]{janowsky1989pruning}
Janowsky, S.~A.
\newblock Pruning versus clipping in neural networks.
\newblock \emph{Physical Review A}, 39\penalty0 (12):\penalty0 6600, 1989.

\bibitem[Jayakumar et~al.(2020)Jayakumar, Pascanu, Rae, Osindero, and
  Elsen]{jayakumar2020top}
Jayakumar, S., Pascanu, R., Rae, J., Osindero, S., and Elsen, E.
\newblock Top-kast: Top-k always sparse training.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Kusupati et~al.(2020)Kusupati, Ramanujan, Somani, Wortsman, Jain,
  Kakade, and Farhadi]{kusupati2020soft}
Kusupati, A., Ramanujan, V., Somani, R., Wortsman, M., Jain, P., Kakade, S.,
  and Farhadi, A.
\newblock Soft threshold weight reparameterization for learnable sparsity.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{lecun1990optimal}
LeCun, Y., Denker, J.~S., and Solla, S.~A.
\newblock Optimal brain damage.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  598--605, 1990.

\bibitem[Lee et~al.(2019)Lee, Ajanthan, and Torr]{lee2018snip}
Lee, N., Ajanthan, T., and Torr, P.
\newblock {SNIP}: {SINGLE}-{SHOT} {NETWORK} {PRUNING} {BASED} {ON} {CONNECTION}
  {SENSITIVITY}.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=B1VZqjAcYX}.

\bibitem[Lee et~al.(2020)Lee, Ajanthan, Gould, and Torr]{Lee2020A}
Lee, N., Ajanthan, T., Gould, S., and Torr, P. H.~S.
\newblock A signal propagation perspective for pruning neural networks at
  initialization.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=HJeTo2VFwH}.

\bibitem[Li \& Liang(2018)Li and Liang]{li2018learning}
Li, Y. and Liang, Y.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8157--8166, 2018.

\bibitem[Lin et~al.(2017)Lin, Rao, Lu, and Zhou]{lin2017runtime}
Lin, J., Rao, Y., Lu, J., and Zhou, J.
\newblock Runtime neural pruning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2181--2191, 2017.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Xu, Shi, Cheung, and
  So]{LIU2020Dynamic}
Liu, J., Xu, Z., Shi, R., Cheung, R. C.~C., and So, H.~K.
\newblock Dynamic sparse training: Find efficient sparse network from scratch
  with trainable masked layers.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=SJlbGJrtDB}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Mocanu, Matavalam, Pei, and
  Pechenizkiy]{onemillionneurons}
Liu, S., Mocanu, D.~C., Matavalam, A. R.~R., Pei, Y., and Pechenizkiy, M.
\newblock Sparse evolutionary deep learning with over one million artificial
  neurons on commodity hardware.
\newblock \emph{Neural Computing and Applications}, 2020{\natexlab{b}}.

\bibitem[Liu et~al.(2020{\natexlab{c}})Liu, van~der Lee, Yaman, Atashgahi,
  Ferrar, Sokar, Pechenizkiy, and Mocanu]{liu2020topological}
Liu, S., van~der Lee, T., Yaman, A., Atashgahi, Z., Ferrar, D., Sokar, G.,
  Pechenizkiy, M., and Mocanu, D.
\newblock Topological insights into sparse neural networks.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, 2020{\natexlab{c}}.

\bibitem[Liu et~al.(2021)Liu, Mocanu, Pei, and Pechenizkiy]{liu2021selfish}
Liu, S., Mocanu, D.~C., Pei, Y., and Pechenizkiy, M.
\newblock Selfish sparse rnn training.
\newblock \emph{arXiv preprint arXiv:2101.09048}, 2021.

\bibitem[Liu et~al.(2019)Liu, Sun, Zhou, Huang, and Darrell]{liu2019rethinking}
Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T.
\newblock Rethinking the value of network pruning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Louizos et~al.(2017)Louizos, Welling, and Kingma]{louizos2017learning}
Louizos, C., Welling, M., and Kingma, D.~P.
\newblock Learning sparse neural networks through $ l\_0 $ regularization.
\newblock \emph{arXiv preprint arXiv:1712.01312}, 2017.

\bibitem[Mocanu(2017)]{setphdthesis2017}
Mocanu, D.~C.
\newblock \emph{Network computations in artificial intelligence}.
\newblock PhD thesis, Technische Universiteit Eindhoven, June 2017.

\bibitem[Mocanu et~al.(2016)Mocanu, Mocanu, Nguyen, Gibescu, and
  Liotta]{mocanu2016topological}
Mocanu, D.~C., Mocanu, E., Nguyen, P.~H., Gibescu, M., and Liotta, A.
\newblock A topological insight into restricted boltzmann machines.
\newblock \emph{Machine Learning}, 104\penalty0 (2-3):\penalty0 243--270, 2016.

\bibitem[Mocanu et~al.(2018)Mocanu, Mocanu, Stone, Nguyen, Gibescu, and
  Liotta]{mocanu2018scalable}
Mocanu, D.~C., Mocanu, E., Stone, P., Nguyen, P.~H., Gibescu, M., and Liotta,
  A.
\newblock Scalable training of artificial neural networks with adaptive sparse
  connectivity inspired by network science.
\newblock \emph{Nature communications}, 9\penalty0 (1):\penalty0 1--12, 2018.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{molchanov2017variational}
Molchanov, D., Ashukha, A., and Vetrov, D.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Molchanov et~al.(2016)Molchanov, Tyree, Karras, Aila, and
  Kautz]{molchanov2016pruning}
Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock \emph{arXiv preprint arXiv:1611.06440}, 2016.

\bibitem[Molchanov et~al.(2019)Molchanov, Mallya, Tyree, Frosio, and
  Kautz]{molchanov2019importance}
Molchanov, P., Mallya, A., Tyree, S., Frosio, I., and Kautz, J.
\newblock Importance estimation for neural network pruning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  11264--11272, 2019.

\bibitem[Mostafa \& Wang(2019)Mostafa and Wang]{mostafa2019parameter}
Mostafa, H. and Wang, X.
\newblock Parameter efficient training of deep convolutional neural networks by
  dynamic sparse reparameterization.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Mozer \& Smolensky(1989)Mozer and Smolensky]{mozer1989using}
Mozer, M.~C. and Smolensky, P.
\newblock Using relevance to reduce network size automatically.
\newblock \emph{Connection Science}, 1\penalty0 (1):\penalty0 3--16, 1989.

\bibitem[Narang et~al.(2017)Narang, Elsen, Diamos, and
  Sengupta]{narang2017exploring}
Narang, S., Elsen, E., Diamos, G., and Sengupta, S.
\newblock Exploring sparsity in recurrent neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Raihan \& Aamodt(2020)Raihan and Aamodt]{raihan2020sparse}
Raihan, M.~A. and Aamodt, T.~M.
\newblock Sparse weight activation training.
\newblock \emph{arXiv preprint arXiv:2001.01969}, 2020.

\bibitem[Safran \& Shamir(2018)Safran and Shamir]{safran2018spurious}
Safran, I. and Shamir, O.
\newblock Spurious local minima are common in two-layer relu neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4433--4441. PMLR, 2018.

\bibitem[Sagawa et~al.(2020)Sagawa, Raghunathan, Koh, and
  Liang]{sagawa2020investigation}
Sagawa, S., Raghunathan, A., Koh, P.~W., and Liang, P.
\newblock An investigation of why overparameterization exacerbates spurious
  correlations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8346--8356. PMLR, 2020.

\bibitem[Savarese et~al.(2019)Savarese, Silva, and Maire]{savarese2019winning}
Savarese, P., Silva, H., and Maire, M.
\newblock Winning the lottery with continuous sparsification.
\newblock \emph{arXiv preprint arXiv:1912.04427}, 2019.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Smith et~al.(2017)Smith, Kindermans, Ying, and Le]{smith2017don}
Smith, S.~L., Kindermans, P.-J., Ying, C., and Le, Q.~V.
\newblock Don't decay the learning rate, increase the batch size.
\newblock \emph{arXiv preprint arXiv:1711.00489}, 2017.

\bibitem[Soudry \& Carmon(2016)Soudry and Carmon]{soudry2016no}
Soudry, D. and Carmon, Y.
\newblock No bad local minima: Data independent training error guarantees for
  multilayer neural networks.
\newblock \emph{arXiv preprint arXiv:1605.08361}, 2016.

\bibitem[Srinivas et~al.(2017)Srinivas, Subramanya, and
  Venkatesh~Babu]{srinivas2017training}
Srinivas, S., Subramanya, A., and Venkatesh~Babu, R.
\newblock Training sparse neural networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops}, pp.\  138--145, 2017.

\bibitem[Tanaka et~al.(2020)Tanaka, Kunin, Yamins, and
  Ganguli]{tanaka2020pruning}
Tanaka, H., Kunin, D., Yamins, D.~L., and Ganguli, S.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock \emph{arXiv preprint arXiv:2006.05467}, 2020.

\bibitem[Wang et~al.(2020)Wang, Zhang, and Grosse]{Wang2020Picking}
Wang, C., Zhang, G., and Grosse, R.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SkgsACVKPH}.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li]{wen2016learning}
Wen, W., Wu, C., Wang, Y., Chen, Y., and Li, H.
\newblock Learning structured sparsity in deep neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2074--2082, 2016.

\bibitem[Wong et~al.(2021)Wong, Santurkar, and Madry]{wong2021leveraging}
Wong, E., Santurkar, S., and Madry, A.
\newblock Leveraging sparse linear layers for debuggable deep networks.
\newblock \emph{arXiv preprint arXiv:2105.04857}, 2021.

\bibitem[Xiao et~al.(2019)Xiao, Wang, and Rajasekaran]{xiao2019autoprune}
Xiao, X., Wang, Z., and Rajasekaran, S.
\newblock Autoprune: Automatic network pruning by regularizing auxiliary
  parameters.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  13681--13691, 2019.

\bibitem[You et~al.(2019)You, Li, Xu, Fu, Wang, Chen, Baraniuk, Wang, and
  Lin]{you2019drawing}
You, H., Li, C., Xu, P., Fu, Y., Wang, Y., Chen, X., Baraniuk, R.~G., Wang, Z.,
  and Lin, Y.
\newblock Drawing early-bird tickets: Towards more efficient training of deep
  networks.
\newblock \emph{arXiv preprint arXiv:1909.11957}, 2019.

\bibitem[Zhang et~al.(2021)Zhang, Ahuja, Xu, Wang, and
  Courville]{zhang2021subnetwork}
Zhang, D., Ahuja, K., Xu, Y., Wang, Y., and Courville, A.
\newblock Can subnetwork structure be the key to out-of-distribution
  generalization?, 2021.

\bibitem[Zhou et~al.(2021)Zhou, Ma, Zhu, Liu, Zhang, Yuan, Sun, and
  Li]{zhou2021learning}
Zhou, A., Ma, Y., Zhu, J., Liu, J., Zhang, Z., Yuan, K., Sun, W., and Li, H.
\newblock Learning n: M fine-grained structured sparse neural networks from
  scratch.
\newblock \emph{arXiv preprint arXiv:2102.04010}, 2021.

\bibitem[Zhu \& Gupta(2017)Zhu and Gupta]{zhu2017prune}
Zhu, M. and Gupta, S.
\newblock To prune, or not to prune: exploring the efficacy of pruning for
  model compression.
\newblock \emph{arXiv preprint arXiv:1710.01878}, 2017.

\bibitem[Zou \& Gu(2019)Zou and Gu]{zou2019improved}
Zou, D. and Gu, Q.
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2055--2064, 2019.

\bibitem[Zou et~al.(2020)Zou, Cao, Zhou, and Gu]{zou2020gradient}
Zou, D., Cao, Y., Zhou, D., and Gu, Q.
\newblock Gradient descent optimizes over-parameterized deep relu networks.
\newblock \emph{Machine Learning}, 109\penalty0 (3):\penalty0 467--492, 2020.

\end{thebibliography}
