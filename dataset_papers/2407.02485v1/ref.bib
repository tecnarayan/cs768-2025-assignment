
@article{lee2024nv,
  title={NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models},
  author={Lee, Chankyu and Roy, Rajarshi and Xu, Mengyao and Raiman, Jonathan and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
  journal={arXiv preprint arXiv:2405.17428},
  year={2024}
}

@article{wang2023learning,
  title={Learning to filter context for retrieval-augmented generation},
  author={Wang, Zhiruo and Araki, Jun and Jiang, Zhengbao and Parvez, Md Rizwan and Neubig, Graham},
  journal={arXiv preprint arXiv:2311.08377},
  year={2023}
}

@inproceedings{du2022glam,
  title={Glam: Efficient scaling of language models with mixture-of-experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  booktitle={ICML},
  year={2022},
}
@article{luan2021sparse,
  title={Sparse, dense, and attentional representations for text retrieval},
  author={Luan, Yi and Eisenstein, Jacob and Toutanova, Kristina and Collins, Michael},
  journal={TACL},
  year={2021},
}

@inproceedings{thakur2021beir,
  title={BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models},
  author={Thakur, Nandan and Reimers, Nils and R{\"u}ckl{\'e}, Andreas and Srivastava, Abhishek and Gurevych, Iryna},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{robertson2004simple,
  title={Simple BM25 extension to multiple weighted fields},
  author={Robertson, Stephen and Zaragoza, Hugo and Taylor, Michael},
  booktitle={CIKM},
  year={2004}
}

@inproceedings{menon2022defense,
  title={In defense of dual-encoders for neural ranking},
  author={Menon, Aditya and Jayasumana, Sadeep and Rawat, Ankit Singh and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv},
  booktitle={ICML},
  year={2022},
}

@inproceedings{kandpal2023large,
  title={Large language models struggle to learn long-tail knowledge},
  author={Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
  booktitle={ICML},
  year={2023},
}

@inproceedings{fid,
  title={Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering},
  author={Izacard, Gautier and Grave, Edouard},
  booktitle={EACL},
  year={2021},
}

@inproceedings{2wikimqa,
  title={Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps},
  author={Ho, Xanh and Nguyen, Anh-Khoa Duong and Sugawara, Saku and Aizawa, Akiko},
  booktitle={COLING},
  year={2020}
}

@article{khattab2022demonstrate,
  title={Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp},
  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2212.14024},
  year={2022}
}

@inproceedings{mmlu,
title={Measuring Massive Multitask Language Understanding},
author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
booktitle={ICLR},
year={2021},
-url={https://openreview.net/forum?id=d7KBjmI3GmQ}
}

@inproceedings{yang2018hotpotqa,
  title={{HotpotQA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},
  booktitle={EMNLP},
  year={2018}
}

@inproceedings{yu2023augmentation,
  title={Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In},
  author={Yu, Zichun and Xiong, Chenyan and Yu, Shi and Liu, Zhiyuan},
  booktitle={ACL},
  year={2023}
}
@inproceedings{activerag,
  title={Active Retrieval Augmented Generation},
  author={Jiang, Zhengbao and Xu, Frank F and Gao, Luyu and Sun, Zhiqing and Liu, Qian and Dwivedi-Yu, Jane and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle={EMNLP},
  year={2023}
}
@inproceedings{trivedi2023interleaving,
  title={Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions},
  author={Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
  booktitle={ACL},
  year={2023}
}

@inproceedings{shao2023enhancing,
  title={Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy},
  author={Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Huang, Minlie and Duan, Nan and Chen, Weizhu},
  booktitle={Findings of EMNLP},
  year={2023}
}


@inproceedings{petroni-etal-2021-kilt,
    title = "{KILT}: a Benchmark for Knowledge Intensive Language Tasks",
    author = {Petroni, Fabio  and
      Piktus, Aleksandra  and
      Fan, Angela  and
      Lewis, Patrick  and
      Yazdani, Majid  and
      De Cao, Nicola  and
      Thorne, James  and
      Jernite, Yacine  and
      Karpukhin, Vladimir  and
      Maillard, Jean  and
      Plachouras, Vassilis  and
      Rockt{\"a}schel, Tim  and
      Riedel, Sebastian},
    booktitle = "NAACL",
    year = "2021",
    abstract = "Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, external knowledge sources. While some models do well on individual tasks, developing general models is difficult as each task might require computationally expensive indexing of custom knowledge sources, in addition to dedicated infrastructure. To catalyze research on models that condition on specific information in large textual resources, we present a benchmark for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia, reducing engineering turnaround through the re-use of components, as well as accelerating research into task-agnostic memory architectures. We test both task-specific and general baselines, evaluating downstream performance in addition to the ability of the models to provide provenance. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, outperforming more tailor-made approaches for fact checking, open-domain question answering and dialogue, and yielding competitive results on entity linking and slot filling, by generating disambiguated text. KILT data and code are available at \-url{https://github.com/facebookresearch/KILT}.",
}

@inproceedings{
oren2024proving,
title={Proving Test Set Contamination in Black-Box Language Models},
author={Yonatan Oren and Nicole Meister and Niladri S. Chatterji and Faisal Ladhak and Tatsunori Hashimoto},
booktitle={ICLR},
year={2024},
-url={https://openreview.net/forum?id=KS8mIvetg2}
}

@inproceedings{
kasai2023realtime,
title={RealTime {QA}: What's the Answer Right Now?},
author={Jungo Kasai and Keisuke Sakaguchi and yoichi takahashi and Ronan Le Bras and Akari Asai and Xinyan Velocity Yu and Dragomir Radev and Noah A. Smith and Yejin Choi and Kentaro Inui},
booktitle={NeurIPS},
year={2023},
-url={https://openreview.net/forum?id=HfKOIPCvsv}
}

@article{muennighoff2024generative,
  title={Generative representational instruction tuning},
  author={Muennighoff, Niklas and Su, Hongjin and Wang, Liang and Yang, Nan and Wei, Furu and Yu, Tao and Singh, Amanpreet and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.09906},
  year={2024}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@inproceedings{jeong2024adaptive,
  title={Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity},
  author={Jeong, Soyeong and Baek, Jinheon and Cho, Sukmin and Hwang, Sung Ju and Park, Jong C},
  booktitle={NAACL},
  year={2024}
}
@article{mitra2018introduction,
  title={An introduction to neural information retrieval},
  author={Mitra, Bhaskar and Craswell, Nick and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  year={2018},
}

@inproceedings{
sachan2021endtoend,
title={End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering},
author={Devendra Singh Sachan and Siva Reddy and William L. Hamilton and Chris Dyer and Dani Yogatama},
booktitle={NeurIPS},
year={2021},
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={NeurIPS},
  volume={33},
  year={2020}
}

@inproceedings{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={ICML},
  year={2022},
  organization={PMLR}
}

@inproceedings{hofstatter2023fid,
  title={Fid-light: Efficient and effective retrieval-augmented text generation},
  author={Hofst{\"a}tter, Sebastian and Chen, Jiecao and Raman, Karthik and Zamani, Hamed},
  booktitle={SIGIR},
  year={2023}
}
@inproceedings{
asai2024selfrag,
title={Self-{RAG}: Learning to Retrieve, Generate, and Critique through Self-Reflection},
author={Akari Asai and Zeqiu Wu and Yizhong Wang and Avirup Sil and Hannaneh Hajishirzi},
booktitle={ICLR},
year={2024},
-url={https://openreview.net/forum?id=hSyW5go0v8}
}

@misc{
refeed,
title={Improving Language Models via Plug-and-Play Retrieval Feedback},
author={Wenhao Yu and Zhihan Zhang and Zhenwen Liang and Meng Jiang and Ashish Sabharwal},
year={2024},
-url={https://openreview.net/forum?id=EyfOZKXpcN}
}

@article{asai2024reliable,
  title={Reliable, adaptable, and attributable language models with retrieval},
  author={Asai, Akari and Zhong, Zexuan and Chen, Danqi and Koh, Pang Wei and Zettlemoyer, Luke and Hajishirzi, Hannaneh and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2403.03187},
  year={2024}
}

@article{pmcllama,
  title={PMC-LLaMA: toward building open-source language models for medicine},
  author={Wu, Chaoyi and Lin, Weixiong and Zhang, Xiaoman and Zhang, Ya and Xie, Weidi and Wang, Yanfeng},
  journal={JAMIA},
  year={2024},
  publisher={Oxford University Press}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}
@article{xiong2024benchmarking,
  title={Benchmarking retrieval-augmented generation for medicine},
  author={Xiong, Guangzhi and Jin, Qiao and Lu, Zhiyong and Zhang, Aidong},
  journal={arXiv preprint arXiv:2402.13178},
  year={2024}
}
@inproceedings{yu2022coco,
  title={COCO-DR: Combating Distribution Shift in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning},
  author={Yu, Yue and Xiong, Chenyan and Sun, Si and Zhang, Chao and Overwijk, Arnold},
  booktitle={EMNLP},
  year={2022}
}
@article{jin2023medcpt,
  title={MedCPT: Contrastive Pre-trained Transformers with large-scale PubMed search logs for zero-shot biomedical information retrieval},
  author={Jin, Qiao and Kim, Won and Chen, Qingyu and Comeau, Donald C and Yeganova, Lana and Wilbur, W John and Lu, Zhiyong},
  journal={Bioinformatics},
  volume={39},
  number={11},
  year={2023},
  publisher={Oxford University Press}
}
@article{zhang2024raft,
  title={Raft: Adapting language model to domain specific rag},
  author={Zhang, Tianjun and Patil, Shishir G and Jain, Naman and Shen, Sheng and Zaharia, Matei and Stoica, Ion and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2403.10131},
  year={2024}
}

@article{ram2023context,
  title={In-context retrieval-augmented language models},
  author={Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav},
  journal={TACL},
  year={2023},
}

@inproceedings{webq,
  title={Semantic parsing on freebase from question-answer pairs},
  author={Berant, Jonathan and Chou, Andrew and Frostig, Roy and Liang, Percy},
  booktitle={EMNLP},
  year={2013}
}
@inproceedings{triviaqa,
    title = "{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
    author = "Joshi, Mandar  and
      Choi, Eunsol  and
      Weld, Daniel  and
      Zettlemoyer, Luke",
    booktitle = "ACL",
    year = "2017",
}
@inproceedings{popqa,
  title={When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories},
  author={Mallen, Alex and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh},
  booktitle={ACL},
  year={2023}
}
@article{nq,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={TACL},
  year={2019},
}

@article{luo2023sail,
  title={Sail: Search-augmented instruction learning},
  author={Luo, Hongyin and Chuang, Yung-Sung and Gong, Yuan and Zhang, Tianhua and Kim, Yoon and Wu, Xixin and Fox, Danny and Meng, Helen and Glass, James},
  journal={arXiv preprint arXiv:2305.15225},
  year={2023}
}

@inproceedings{
robustlm,
title={Making Retrieval-Augmented Language Models Robust to Irrelevant Context},
author={Ori Yoran and Tomer Wolfson and Ori Ram and Jonathan Berant},
booktitle={ICLR},
year={2024},
-url={https://openreview.net/forum?id=ZS4m74kZpH}
}
@inproceedings{zhu2024inters,
  title={INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning},
  author={Zhu, Yutao and Zhang, Peitian and Zhang, Chenghao and Chen, Yifei and Xie, Binyu and Dou, Zhicheng and Liu, Zheng and Wen, Ji-Rong},
  booktitle={ACL},
  year={2024}
}
@article{wang2024rear,
  title={REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering},
  author={Wang, Yuhao and Ren, Ruiyang and Li, Junyi and Zhao, Wayne Xin and Liu, Jing and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2402.17497},
  year={2024}
}

@inproceedings{shi2023replug,
  title={Replug: Retrieval-augmented black-box language models},
  author={Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  booktitle={NAACL},
  year={2024}
}

@article{zhang2024arl2,
  title={ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling},
  author={Zhang, Lingxi and Yu, Yue and Wang, Kuan and Zhang, Chao},
  journal={arXiv preprint arXiv:2402.13542},
  year={2024}
}

@inproceedings{wang2023instructretro,
  title={Instructretro: Instruction tuning post retrieval-augmented pretraining},
  author={Wang, Boxin and Ping, Wei and McAfee, Lawrence and Xu, Peng and Li, Bo and Shoeybi, Mohammad and Catanzaro, Bryan},
  booktitle={ICML},
  year={2024}
}

@article{huang2023raven,
  title={Raven: In-context learning with retrieval augmented encoder-decoder language models},
  author={Huang, Jie and Ping, Wei and Xu, Peng and Shoeybi, Mohammad and Chang, Kevin Chen-Chuan and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2308.07922},
  year={2023}
}

@article{liu2024chatqa,
  title={ChatQA: Surpassing GPT-4 on Conversational QA and RAG},
  author={Liu, Zihan and Ping, Wei and Roy, Rajarshi and Xu, Peng and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2401.10225},
  year={2024}
}

@article{atlas,
  author  = {Gautier Izacard and Patrick Lewis and Maria Lomeli and Lucas Hosseini and Fabio Petroni and Timo Schick and Jane Dwivedi-Yu and Armand Joulin and Sebastian Riedel and Edouard Grave},
  title   = {Atlas: Few-shot Learning with Retrieval Augmented Language Models},
  journal = {JMLR},
  year    = {2023},
  volume  = {24},
  number  = {251},
  pages   = {1--43},
  -url     = {http://jmlr.org/papers/v24/23-0037.html}
}

@inproceedings{self_prompting,
  title={Self-prompting large language models for zero-shot open-domain qa},
  author={Li, Junlong and Zhang, Zhuosheng and Zhao, Hai},
  booktitle={NAACL},
  year={2024}
}

@inproceedings{
genread,
title={Generate rather than Retrieve: Large Language Models are Strong Context Generators},
author={Wenhao Yu and Dan Iter and Shuohang Wang and Yichong Xu and Mingxuan Ju and Soumya Sanyal and Chenguang Zhu and Michael Zeng and Meng Jiang},
booktitle={ICLR},
year={2023},
-url={https://openreview.net/forum?id=fB0hRu9GZUS}
}

@inproceedings{
lin2024radit,
title={{RA}-{DIT}: Retrieval-Augmented Dual Instruction Tuning},
author={Xi Victoria Lin and Xilun Chen and Mingda Chen and Weijia Shi and Maria Lomeli and Richard James and Pedro Rodriguez and Jacob Kahn and Gergely Szilvasy and Mike Lewis and Luke Zettlemoyer and Wen-tau Yih},
booktitle={ICLR},
year={2024},
-url={https://openreview.net/forum?id=22OTbutug9}
}

@article{lazaridou2022internet,
  title={Internet-augmented language models through few-shot prompting for open-domain question answering},
  author={Lazaridou, Angeliki and Gribovskaya, Elena and Stokowiec, Wojciech and Grigorev, Nikolai},
  journal={arXiv preprint arXiv:2203.05115},
  year={2022}
}

@article{chain_of_note,
  title={Chain-of-note: Enhancing robustness in retrieval-augmented language models},
  author={Yu, Wenhao and Zhang, Hongming and Pan, Xiaoman and Ma, Kaixin and Wang, Hongwei and Yu, Dong},
  journal={arXiv preprint arXiv:2311.09210},
  year={2023}
}

@article{llama3,
  title={Llama 3 Model Card},
  author={Meta-AI},
  year={2024},
  -url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}
@article{Mixtral8x22B,
  title={Mixtral 8x22B},
  author={Mistral},
  year={2024},
  url = {https://mistral.ai/news/mixtral-8x22b/}
}

@inproceedings{
retroprompt,
title={Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning},
author={Xiang Chen and Lei Li and Ningyu Zhang and Xiaozhuan Liang and Shumin Deng and Chuanqi Tan and Fei Huang and Luo Si and Huajun Chen},
booktitle={NeurIPS},
year={2022},
-url={https://openreview.net/forum?id=Q8GnGqT-GTJ}
}

@article{yu2023chain,
  title={Chain-of-note: Enhancing robustness in retrieval-augmented language models},
  author={Yu, Wenhao and Zhang, Hongming and Pan, Xiaoman and Ma, Kaixin and Wang, Hongwei and Yu, Dong},
  journal={arXiv preprint arXiv:2311.09210},
  year={2023}
}

@article{msmarco,
  title={Ms marco: A human generated machine reading comprehension dataset},
  author={Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and others},
  journal={arXiv preprint arXiv:1611.09268},
  year={2016}
}

@article{repllama,
  title={Fine-tuning llama for multi-stage text retrieval},
  author={Ma, Xueguang and Wang, Liang and Yang, Nan and Wei, Furu and Lin, Jimmy},
  journal={arXiv preprint arXiv:2310.08319},
  year={2023}
}
@inproceedings{monot5,
    title = "Document Ranking with a Pretrained Sequence-to-Sequence Model",
    author = "Nogueira, Rodrigo  and
      Jiang, Zhiying  and
      Pradeep, Ronak  and
      Lin, Jimmy",
    booktitle = "Findings of EMNLP",
    year = "2020",
}

@article{
contriever,
title={Unsupervised Dense Information Retrieval with Contrastive Learning},
author={Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave},
journal={TMLR},
year={2022},
--url={https://openreview.net/forum?id=jKN1pXi7b0},
note={}
}
@inproceedings{wang2023self,
  title={Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  booktitle={ACL},
  year={2023}
}

@inproceedings{dpr,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    booktitle = "EMNLP",
    year={2020},
    abstract = "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
}

@inproceedings{dragon,
    title = "How to Train Your Dragon: Diverse Augmentation Towards Generalizable Dense Retrieval",
    author = "Lin, Sheng-Chieh  and
      Asai, Akari  and
      Li, Minghan  and
      Oguz, Barlas  and
      Lin, Jimmy  and
      Mehdad, Yashar  and
      Yih, Wen-tau  and
      Chen, Xilun",
    booktitle = "Findings of EMNLP",
    year = "2023",
    abstract = "Various techniques have been developed in recent years to improve dense retrieval (DR), such as unsupervised contrastive learning and pseudo-query generation. Existing DRs, however, often suffer from effectiveness tradeoffs between supervised and zero-shot retrieval, which some argue was due to the limited model capacity. We contradict this hypothesis and show that a generalizable DR can be trained to achieve high accuracy in both supervised and zero-shot retrieval without increasing model size. In particular, we systematically examine the contrastive learning of DRs, under the framework of Data Augmentation (DA). Our study shows that common DA practices such as query augmentation with generative models and pseudo-relevance label creation using a cross-encoder, are often inefficient and sub-optimal. We hence propose a new DA approach with diverse queries and sources of supervision to progressively train a generalizable DR. As a result, DRAGON, our Dense Retriever trained with diverse AuGmentatiON, is the first BERT-base-sized DR to achieve state-of-the-art effectiveness in both supervised and zero-shot evaluations and even competes with models using more complex late interaction.",
}

@inproceedings{sun-etal-2023-chatgpt,
    title = "Is {C}hat{GPT} Good at Search? Investigating Large Language Models as Re-Ranking Agents",
    author = "Sun, Weiwei  and
      Yan, Lingyong  and
      Ma, Xinyu  and
      Wang, Shuaiqiang  and
      Ren, Pengjie  and
      Chen, Zhumin  and
      Yin, Dawei  and
      Ren, Zhaochun",
    booktitle = "EMNLP",
    year = "2023",
}

@inproceedings{qin2023large,
  title={Large language models are effective text rankers with pairwise ranking prompting},
  author={Qin, Zhen and Jagerman, Rolf and Hui, Kai and Zhuang, Honglei and Wu, Junru and Shen, Jiaming and Liu, Tianqi and Liu, Jialu and Metzler, Donald and Wang, Xuanhui and others},
  booktitle={Findings of NAACL},
  year={2024}
}
@article{medqa,
  title={What disease does this patient have? a large-scale open domain question answering dataset from medical exams},
  author={Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  journal={Applied Sciences},
  volume={11},
  number={14},
  pages={6421},
  year={2021},
  publisher={MDPI}
}
@article{bioasq,
  title={An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition},
  author={Tsatsaronis, George and Balikas, Georgios and Malakasiotis, Prodromos and Partalas, Ioannis and Zschunke, Matthias and Alvers, Michael R and Weissenborn, Dirk and Krithara, Anastasia and Petridis, Sergios and Polychronopoulos, Dimitris and others},
  journal={BMC bioinformatics},
  year={2015},
}
@inproceedings{pal2022medmcqa,
  title={Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering},
  author={Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
  booktitle={CHIL},
  year={2022},
}

@inproceedings{jin2019pubmedqa,
  title={PubMedQA: A Dataset for Biomedical Research Question Answering},
  author={Jin, Qiao and Dhingra, Bhuwan and Liu, Zhengping and Cohen, William and Lu, Xinghua},
  booktitle={EMNLP},
  year={2019}
}

@inproceedings{drozdov-etal-2023-parade,
    title = "{P}a{R}a{D}e: Passage Ranking using Demonstrations with {LLM}s",
    author = "Drozdov, Andrew  and
      Zhuang, Honglei  and
      Dai, Zhuyun  and
      Qin, Zhen  and
      Rahimi, Razieh  and
      Wang, Xuanhui  and
      Alon, Dana  and
      Iyyer, Mohit  and
      McCallum, Andrew  and
      Metzler, Donald  and
      Hui, Kai",
    booktitle = "Findings of EMNLP",
    year = "2023",
    abstract = "Recent studies show that large language models (LLMs) can be instructed to effectively perform zero-shot passage re-ranking, in which the results of a first stage retrieval method, such as BM25, are rated and reordered to improve relevance. In this work, we improve LLM-based re-ranking by algorithmically selecting few-shot demonstrations to include in the prompt. Our analysis investigates the conditions where demonstrations are most helpful, and shows that adding even one demonstration is significantly beneficial. We propose a novel demonstration selection strategy based on difficulty rather than the commonly used semantic similarity. Furthermore, we find that demonstrations helpful for ranking are also effective at question generation. We hope our work will spur more principled research into question generation and passage ranking.",
}

@inproceedings{glass-etal-2022-re2g,
    title = "{R}e2{G}: Retrieve, Rerank, Generate",
    author = "Glass, Michael  and
      Rossiello, Gaetano  and
      Chowdhury, Md Faisal Mahbub  and
      Naik, Ankita  and
      Cai, Pengshan  and
      Gliozzo, Alfio",
    booktitle = "NAACL",
    year = "2022",
    abstract = "As demonstrated by GPT-3 and T5, transformers grow in capability as parameter spaces become larger and larger. However, for tasks that require a large amount of knowledge, non-parametric memory allows models to grow dramatically with a sub-linear increase in computational cost and GPU memory requirements. Recent models such as RAG and REALM have introduced retrieval into conditional generation. These models incorporate neural initial retrieval from a corpus of passages. We build on this line of research, proposing Re2G, which combines both neural initial retrieval and reranking into a BART-based sequence-to-sequence generation. Our reranking approach also permits merging retrieval results from sources with incomparable scores, enabling an ensemble of BM25 and neural initial retrieval. To train our system end-to-end, we introduce a novel variation of knowledge distillation to train the initial retrieval, reranker and generation using only ground truth on the target sequence output. We find large gains in four diverse tasks: zero-shot slot filling, question answering, fact checking and dialog, with relative gains of 9{\%} to 34{\%} over the previous state-of-the-art on the KILT leaderboard. We make our code available as open source.",
}

@inproceedings{
cheng2023lift,
title={Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory},
author={Xin Cheng and Di Luo and Xiuying Chen and Lemao Liu and Dongyan Zhao and Rui Yan},
booktitle={NeurIPS},
year={2023},
-url={https://openreview.net/forum?id=lYNSvp51a7}
}

@inproceedings{
xu2024recomp,
title={{RECOMP}: Improving Retrieval-Augmented {LM}s with Context Compression and Selective Augmentation},
author={Fangyuan Xu and Weijia Shi and Eunsol Choi},
booktitle={ICLR},
year={2024},
-url={https://openreview.net/forum?id=mlJLVigNHp}
}


@inproceedings{khalifa-etal-2023-rerank,
    title = "Few-shot Reranking for Multi-hop {QA} via Language Model Prompting",
    author = "Khalifa, Muhammad  and
      Logeswaran, Lajanugen  and
      Lee, Moontae  and
      Lee, Honglak  and
      Wang, Lu",
    booktitle = "ACL",
    year = "2023",
    abstract = "We study few-shot reranking for multi-hop QA (MQA) with open-domain questions. To alleviate the need for a large number of labeled question-document pairs for retriever training, we propose PromptRank, which relies on language model prompting for multi-hop path reranking. PromptRank first constructs an instruction-based prompt that includes a candidate document path and then computes the relevance score between a given question and the path based on the conditional likelihood of the question given the path prompt according to a language model. PromptRank yields strong retrieval performance on HotpotQA with only 128 training examples compared to state-of-the-art methods trained on thousands of examples {---} 73.6 recall@10 by PromptRank vs. 77.8 by PathRetriever and 77.5 by multi-hop dense retrieval.",
}
@misc{deepseekai2024deepseekv2,
    title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
    author={DeepSeek},
    year={2024},
    eprint={2405.04434},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@article{wang2023improving,
  title={Improving text embeddings with large language models},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2401.00368},
  year={2023}
}

@article{claude2,
  title={Model Card and Evaluations for Claude Models},
  author={Anthropic},
  year={2023}
}


@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={ICML},
  year={2020},
}

@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={JMLR},
  volume={25},
  number={70},
  year={2024}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={NeurIPS},
  volume={35},
  year={2022}
}
