\begin{thebibliography}{88}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anonymous(2023)]{anonymous2023vjepa}
Anonymous.
\newblock V-{JEPA}: Latent video prediction for visual representation learning.
\newblock In \emph{Submitted to The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Assran et~al.(2023)Assran, Duval, Misra, Bojanowski, Vincent, Rabbat, LeCun, and Ballas]{assran2023self}
Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas.
\newblock Self-supervised learning from images with a joint-embedding predictive architecture.
\newblock \emph{arXiv preprint arXiv:2301.08243}, 2023.

\bibitem[Bai et~al.(2023)Bai, Geng, Mangalam, Bar, Yuille, Darrell, Malik, and Efros]{bai2023sequential}
Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei~A Efros.
\newblock Sequential modeling enables scalable learning for large vision models.
\newblock \emph{arXiv preprint arXiv:2312.00785}, 2023.

\bibitem[Bandi et~al.(2018)Bandi, Geessink, Manson, Van~Dijk, Balkenhol, Hermsen, Bejnordi, Lee, Paeng, Zhong, et~al.]{bandi2018detection}
Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van~Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak~Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et~al.
\newblock From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge.
\newblock \emph{IEEE Transactions on Medical Imaging}, 2018.

\bibitem[Bao et~al.(2022)Bao, Dong, and Wei]{bao2021beit}
Hangbo Bao, Li Dong, and Furu Wei.
\newblock {BEiT}: {B}ert pre-training of image transformers.
\newblock In \emph{ICLR}, 2022.

\bibitem[Bardes et~al.(2022)Bardes, Ponce, and LeCun]{bardes2022vicreg}
Adrien Bardes, Jean Ponce, and Yann LeCun.
\newblock Vicreg: Variance-invariance-covariance regularization for self-supervised learning.
\newblock In \emph{ICLR}, 2022.

\bibitem[Bautista et~al.(2016)Bautista, Sanakoyeu, Tikhoncheva, and Ommer]{bautista2016cliquecnn}
Miguel~A Bautista, Artsiom Sanakoyeu, Ekaterina Tikhoncheva, and Bjorn Ommer.
\newblock Cliquecnn: Deep unsupervised exemplar learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Beery et~al.(2020)Beery, Cole, and Gjoka]{beery2020iwildcam}
Sara Beery, Elijah Cole, and Arvi Gjoka.
\newblock The iwildcam 2020 competition dataset.
\newblock \emph{arXiv preprint arXiv:2004.10340}, 2020.

\bibitem[Bengio et~al.(2000)Bengio, Ducharme, and Vincent]{bengio2000neural}
Yoshua Bengio, R{\'e}jean Ducharme, and Pascal Vincent.
\newblock A neural probabilistic language model.
\newblock \emph{Advances in neural information processing systems}, 13, 2000.

\bibitem[Bojanowski and Joulin(2017)]{bojanowski2017unsupervised}
Piotr Bojanowski and Armand Joulin.
\newblock Unsupervised learning by predicting noise.
\newblock In \emph{International Conference on Machine Learning}, pages 517--526. PMLR, 2017.

\bibitem[Bossard et~al.(2014)Bossard, Guillaumin, and Van~Gool]{bossard14}
Lukas Bossard, Matthieu Guillaumin, and Luc Van~Gool.
\newblock Food-101 -- mining discriminative components with random forests.
\newblock In \emph{ECCV}, 2014.

\bibitem[Brock et~al.(2018)Brock, Donahue, and Simonyan]{brock2018large}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale gan training for high fidelity natural image synthesis.
\newblock \emph{arXiv preprint arXiv:1809.11096}, 2018.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{preprint arXiv:2005.14165}, 2020.

\bibitem[Caron et~al.(2018)Caron, Bojanowski, Joulin, and Douze]{caron2018deep}
Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze.
\newblock Deep clustering for unsupervised learning of visual features.
\newblock In \emph{ECCV}, 2018.

\bibitem[Caron et~al.(2019)Caron, Bojanowski, Mairal, and Joulin]{caron2019unsupervised}
Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin.
\newblock Unsupervised pre-training of image features on non-curated data.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 2959--2968, 2019.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and Joulin]{caron2020unsupervised}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster assignments.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J\'egou, Mairal, Bojanowski, and Joulin]{caron2021emerging}
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\'e J\'egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{ICCV}, 2021.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Radford, Child, Wu, Jun, Luan, and Sutskever]{chen2020generative}
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.
\newblock Generative pretraining from pixels.
\newblock In \emph{ICML}, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual representations.
\newblock In \emph{ICML}, 2020{\natexlab{b}}.

\bibitem[Chen et~al.(2021)Chen, Xie, and He]{chen2021empirical}
Xinlei Chen, Saining Xie, and Kaiming He.
\newblock An empirical study of training self-supervised vision transformers.
\newblock In \emph{ICCV}, 2021.

\bibitem[Christie et~al.(2018)Christie, Fendley, Wilson, and Mukherjee]{christie2018functional}
Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee.
\newblock Functional map of the world.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2018.

\bibitem[Cimpoi et~al.(2014)Cimpoi, Maji, Kokkinos, Mohamed, and Vedaldi]{cimpoi14describing}
M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi.
\newblock Describing textures in the wild.
\newblock In \emph{CVPR}, 2014.

\bibitem[Cubuk et~al.(2019)Cubuk, Zoph, Mane, Vasudevan, and Le]{cubuk2019autoaugment}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock Autoaugment: Learning augmentation strategies from data.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 113--123, 2019.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek, Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, et~al.]{dehghani2023scaling}
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas~Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et~al.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock In \emph{ICML}. PMLR, 2023.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern recognition}, pages 248--255. Ieee, 2009.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: {P}re-training of deep bidirectional transformers for language understanding.
\newblock In \emph{NAACL}, 2018.

\bibitem[Doersch et~al.(2015)Doersch, Gupta, and Efros]{doersch2015unsupervised}
Carl Doersch, Abhinav Gupta, and Alexei~A Efros.
\newblock Unsupervised visual representation learning by context prediction.
\newblock In \emph{ICCV}, 2015.

\bibitem[Dosovitskiy et~al.(2014)Dosovitskiy, Springenberg, Riedmiller, and Brox]{dosovitskiy2014discriminative}
Alexey Dosovitskiy, Jost~Tobias Springenberg, Martin Riedmiller, and Thomas Brox.
\newblock Discriminative unsupervised feature learning with convolutional neural networks.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{ICLR}, 2021.

\bibitem[El-Nouby et~al.(2021)El-Nouby, Izacard, Touvron, Laptev, Jegou, and Grave]{el2021large}
Alaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Herv{\'e} Jegou, and Edouard Grave.
\newblock Are large-scale datasets necessary for self-supervised pre-training?
\newblock \emph{arXiv preprint arXiv:2112.10740}, 2021.

\bibitem[Elman(1990)]{elman1990finding}
Jeffrey~L Elman.
\newblock Finding structure in time.
\newblock \emph{Cognitive science}, 14\penalty0 (2):\penalty0 179--211, 1990.

\bibitem[Fang et~al.(2023)Fang, Jose, Jain, Schmidt, Toshev, and Shankar]{fang2023data}
Alex Fang, Albin~Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar.
\newblock Data filtering networks.
\newblock \emph{arXiv preprint arXiv:2309.17425}, 2023.

\bibitem[Gadre et~al.(2023)Gadre, Ilharco, Fang, Hayase, Smyrnis, Nguyen, Marten, Wortsman, Ghosh, Zhang, et~al.]{gadre2023datacomp}
Samir~Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et~al.
\newblock Datacomp: In search of the next generation of multimodal datasets.
\newblock \emph{arXiv preprint arXiv:2304.14108}, 2023.

\bibitem[Gidaris et~al.(2018)Gidaris, Singh, and Komodakis]{gidaris2018unsupervised}
Spyros Gidaris, Praveer Singh, and Nikos Komodakis.
\newblock Unsupervised representation learning by predicting image rotations.
\newblock \emph{arXiv preprint arXiv:1803.07728}, 2018.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Goyal et~al.(2019)Goyal, Mahajan, Gupta, and Misra]{goyal2019scaling}
Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra.
\newblock Scaling and benchmarking self-supervised visual representation learning.
\newblock In \emph{ICCV}, 2019.

\bibitem[Goyal et~al.(2022)Goyal, Duval, Seessel, Caron, Singh, Misra, Sagun, Joulin, and Bojanowski]{goyal2022vision}
Priya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Mannat Singh, Ishan Misra, Levent Sagun, Armand Joulin, and Piotr Bojanowski.
\newblock Vision models are more robust and fair when pretrained on uncurated images without supervision.
\newblock \emph{arXiv preprint arXiv:2202.08360}, 2022.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond, Buchatskaya, Doersch, Avila~Pires, Guo, Gheshlaghi~Azar, et~al.]{grill2020bootstrap}
Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila~Pires, Zhaohan Guo, Mohammad Gheshlaghi~Azar, et~al.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock \emph{NeurIPS}, 2020.

\bibitem[He et~al.(2017)He, Gkioxari, Doll{\'a}r, and Girshick]{he2017mask}
Kaiming He, Georgia Gkioxari, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Mask r-cnn.
\newblock In \emph{ICCV}, 2017.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{CVPR}, 2020.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and Girshick]{he2021masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{CVPR}, 2022.

\bibitem[Helber et~al.(2017)Helber, Bischke, Dengel, and Borth]{helber2017eurosat}
Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth.
\newblock Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification, 2017.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Huang et~al.(2016)Huang, Sun, Liu, Sedra, and Weinberger]{huang2016deep}
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian~Q Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In \emph{ECCV}, 2016.

\bibitem[{iNaturalist} 2018 competition dataset()]{inaturalist18}
{iNaturalist} 2018 competition dataset.
\newblock {iNaturalist} 2018 competition dataset.
\newblock ~\url{https://github.com/visipedia/inat_comp/tree/master/2018}, 2018.

\bibitem[Krause et~al.(2013)Krause, Stark, Deng, and Fei-Fei]{KrauseStarkDengFei-Fei_3DRR2013}
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
\newblock 3d object representations for fine-grained categorization.
\newblock In \emph{4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13)}, Sydney, Australia, 2013.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Larochelle and Murray(2011)]{larochelle2011neural}
Hugo Larochelle and Iain Murray.
\newblock The neural autoregressive distribution estimator.
\newblock In \emph{Proceedings of the fourteenth international conference on artificial intelligence and statistics}, pages 29--37. JMLR Workshop and Conference Proceedings, 2011.

\bibitem[Lee et~al.(2019)Lee, Lee, Kim, Kosiorek, Choi, and Teh]{lee2019set}
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee~Whye Teh.
\newblock Set transformer: A framework for attention-based permutation-invariant neural networks.
\newblock In \emph{ICML}, 2019.

\bibitem[Loshchilov and Hutter(2017{\natexlab{a}})]{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock In \emph{ICLR}, 2017{\natexlab{a}}.

\bibitem[Loshchilov and Hutter(2017{\natexlab{b}})]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017{\natexlab{b}}.

\bibitem[Mikolov et~al.(2010)Mikolov, Karafi{\'a}t, Burget, Cernock{\`y}, and Khudanpur]{mikolovrecurrent}
Tomas Mikolov, Martin Karafi{\'a}t, Lukas Burget, Jan Cernock{\`y}, and Sanjeev Khudanpur.
\newblock Recurrent neural network based language model.
\newblock In \emph{Interspeech}, 2010.

\bibitem[Misra and Maaten(2020)]{misra2020self}
Ishan Misra and Laurens van~der Maaten.
\newblock Self-supervised learning of pretext-invariant representations.
\newblock In \emph{CVPR}, 2020.

\bibitem[Noroozi and Favaro(2016)]{noroozi2016unsupervised}
Mehdi Noroozi and Paolo Favaro.
\newblock Unsupervised learning of visual representations by solving jigsaw puzzles.
\newblock In \emph{ECCV}, 2016.

\bibitem[Oord et~al.(2016)Oord, Dieleman, Zen, Simonyan, Vinyals, Graves, Kalchbrenner, Senior, and Kavukcuoglu]{oord2016wavenet}
Aaron van~den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu.
\newblock Wavenet: A generative model for raw audio.
\newblock \emph{arXiv preprint arXiv:1609.03499}, 2016.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Oquab et~al.(2023)Oquab, Darcet, Moutakanni, Vo, Szafraniec, Khalidov, Fernandez, Haziza, Massa, El-Nouby, Howes, Huang, Xu, Sharma, Li, Galuba, Rabbat, Assran, Ballas, Synnaeve, Misra, Jegou, Mairal, Labatut, Joulin, and Bojanowski]{oquab2023dinov2}
Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy~V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.
\newblock Dinov2: Learning robust visual features without supervision, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Parkhi et~al.(2012)Parkhi, Vedaldi, Zisserman, and Jawahar]{parkhi12a}
O.~M. Parkhi, A. Vedaldi, A. Zisserman, and C.~V. Jawahar.
\newblock Cats and dogs.
\newblock In \emph{CVPR}, 2012.

\bibitem[Parmar et~al.(2018)Parmar, Vaswani, Uszkoreit, Kaiser, Shazeer, Ku, and Tran]{parmar2018image}
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran.
\newblock Image transformer.
\newblock In \emph{ICML}, 2018.

\bibitem[Pathak et~al.(2016)Pathak, Krahenbuhl, Donahue, Darrell, and Efros]{pathak2016context}
Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei~A Efros.
\newblock Context encoders: Feature learning by inpainting.
\newblock In \emph{CVPR}, 2016.

\bibitem[Peng et~al.(2019)Peng, Bai, Xia, Huang, Saenko, and Wang]{peng2019moment}
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang.
\newblock Moment matching for multi-source domain adaptation.
\newblock In \emph{ICCV}, 2019.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 2019.

\bibitem[Raffel et~al.(2020{\natexlab{a}})Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 5485--5551, 2020{\natexlab{a}}.

\bibitem[Raffel et~al.(2020{\natexlab{b}})Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{roberts2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{The Journal of Machine Learning Research}, 2020{\natexlab{b}}.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and Sutskever]{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In \emph{International Conference on Machine Learning}, pages 8821--8831. PMLR, 2021.

\bibitem[Salimans et~al.(2017)Salimans, Karpathy, Chen, and Kingma]{salimans2017pixelcnn}
Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik~P Kingma.
\newblock Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications.
\newblock \emph{arXiv preprint arXiv:1701.05517}, 2017.

\bibitem[Shannon(1951)]{shannon1951prediction}
Claude~E Shannon.
\newblock Prediction and entropy of printed english.
\newblock \emph{Bell system technical journal}, 30\penalty0 (1):\penalty0 50--64, 1951.

\bibitem[Singh et~al.(2023)Singh, Duval, Alwala, Fan, Aggarwal, Adcock, Joulin, Doll{\'a}r, Feichtenhofer, Girshick, et~al.]{singh2023effectiveness}
Mannat Singh, Quentin Duval, Kalyan~Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal, Aaron Adcock, Armand Joulin, Piotr Doll{\'a}r, Christoph Feichtenhofer, Ross Girshick, et~al.
\newblock The effectiveness of mae pre-pretraining for billion-scale pretraining.
\newblock \emph{arXiv preprint arXiv:2303.13496}, 2023.

\bibitem[Taylor et~al.(2019)Taylor, Earnshaw, Mabey, Victors, and Yosinski]{taylor2019rxrx1}
J. Taylor, B. Earnshaw, B. Mabey, M. Victors, and J. Yosinski.
\newblock Rxrx1: An image set for cellular morphological variation across many experimental batches.
\newblock In \emph{ICLR}, 2019.

\bibitem[Tian et~al.(2021)Tian, Henaff, and van~den Oord]{tian2021divide}
Yonglong Tian, Olivier~J Henaff, and A{\"a}ron van~den Oord.
\newblock Divide and contrast: Self-supervised learning from uncurated data.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 10063--10074, 2021.

\bibitem[Touvron et~al.(2021{\natexlab{a}})Touvron, Cord, Douze, Massa, Sablayrolles, and J{\'e}gou]{touvron2021training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through attention.
\newblock In \emph{ICML}, 2021{\natexlab{a}}.

\bibitem[Touvron et~al.(2021{\natexlab{b}})Touvron, Cord, Sablayrolles, Synnaeve, and J{\'e}gou]{touvron2021going}
Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv{\'e} J{\'e}gou.
\newblock Going deeper with image transformers.
\newblock \emph{arXiv preprint arXiv:2103.17239}, 2021{\natexlab{b}}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Van~den Oord et~al.(2016)Van~den Oord, Kalchbrenner, Espeholt, Vinyals, Graves, et~al.]{van2016conditional}
Aaron Van~den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et~al.
\newblock Conditional image generation with pixelcnn decoders.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Van Den~Oord et~al.(2016)Van Den~Oord, Kalchbrenner, and Kavukcuoglu]{van2016pixel}
A{\"a}ron Van Den~Oord, Nal Kalchbrenner, and Koray Kavukcuoglu.
\newblock Pixel recurrent neural networks.
\newblock In \emph{International conference on machine learning}, pages 1747--1756. PMLR, 2016.

\bibitem[Van Den~Oord et~al.(2017)Van Den~Oord, Vinyals, et~al.]{van2017neural}
Aaron Van Den~Oord, Oriol Vinyals, et~al.
\newblock Neurips.
\newblock \emph{Advances in neural information processing systems}, 2017.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Veeling et~al.(2018)Veeling, Linmans, Winkens, Cohen, and Welling]{veeling2018rotation}
Bastiaan~S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling.
\newblock Rotation equivariant cnns for digital pathology.
\newblock In \emph{Medical Image Computing and Computer Assisted Intervention--MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part II 11}, pages 210--218. Springer, 2018.

\bibitem[Vincent et~al.(2010)Vincent, Larochelle, Lajoie, Bengio, Manzagol, and Bottou]{vincent2010stacked}
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and L{\'e}on Bottou.
\newblock Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.
\newblock \emph{Journal of machine learning research}, 11\penalty0 (12), 2010.

\bibitem[Wei et~al.(2023)Wei, Mangalam, Huang, Li, Fan, Xu, Wang, Xie, Yuille, and Feichtenhofer]{wei2023diffusion}
Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao Li, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang Xie, Alan Yuille, and Christoph Feichtenhofer.
\newblock Diffusion models as masked autoencoders.
\newblock \emph{arXiv preprint arXiv:2304.03283}, 2023.

\bibitem[Yan et~al.(2020)Yan, Misra, Gupta, Ghadiyaram, and Mahajan]{yan2020cluster}
Xueting Yan, Ishan Misra, Abhinav Gupta, Deepti Ghadiyaram, and Dhruv Mahajan.
\newblock {ClusterFit: Improving Generalization of Visual Representations}.
\newblock In \emph{CVPR}, 2020.

\bibitem[Yu et~al.(2022)Yu, Wang, Vasudevan, Yeung, Seyedhosseini, and Wu]{yu2022coca}
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu.
\newblock Coca: Contrastive captioners are image-text foundation models.
\newblock \emph{TMLR}, 2022.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and Deny]{zbontar2021barlow}
Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St{\'e}phane Deny.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock In \emph{ICML}, 2021.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and Lopez-Paz]{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{ICLR}, 2018.

\bibitem[Zhang et~al.(2016)Zhang, Isola, and Efros]{zhang2016colorful}
Richard Zhang, Phillip Isola, and Alexei~A Efros.
\newblock Colorful image colorization.
\newblock In \emph{ECCV}, 2016.

\bibitem[Zhou et~al.(2022)Zhou, Wei, Wang, Shen, Xie, Yuille, and Kong]{zhou2021ibot}
Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong.
\newblock ibot: Image bert pre-training with online tokenizer.
\newblock In \emph{ICLR}, 2022.

\end{thebibliography}
