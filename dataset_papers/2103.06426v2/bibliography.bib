
@inproceedings{double_oracle,
  title={Planning in the presence of cost functions controlled by an adversary},
  author={McMahan, H Brendan and Gordon, Geoffrey J and Blum, Avrim},
  booktitle={Proceedings of the 20th International Conference on Machine Learning (ICML-03)},
  pages={536--543},
  year={2003}
}

@article{muller2019generalized,
  title={A Generalized Training Approach for Multiagent Learning},
  author={Muller, Paul and Omidshafiei, Shayegan and Rowland, Mark and Tuyls, Karl and Perolat, Julien and Liu, Siqi and Hennes, Daniel and Marris, Luke and Lanctot, Marc and Hughes, Edward and others},
  journal={International Conference on Learning Representations (ICLR)},
  year={2020}
}
@inproceedings{schmid2014bounding,
  title={Bounding the support size in extensive form games with imperfect information},
  author={Schmid, Martin and Moravcik, Matej and Hladik, Milan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={28},
  number={1},
  year={2014}
}
@inproceedings{lanctot2009monte,
  title={Monte Carlo Sampling for Regret Minimization in Extensive Games.},
  author={Lanctot, Marc and Waugh, Kevin and Zinkevich, Martin and Bowling, Michael H},
  booktitle={NIPS},
  pages={1078--1086},
  year={2009}
}

@article{rashid2018qmix,
  title={QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning},
  author={Rashid, Tabish and Samvelyan, Mikayel and De Witt, Christian Schroeder and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  journal={arXiv preprint arXiv:1803.11485},
  year={2018}
}
@article{bansal2017emergent,
  title={Emergent complexity via multi-agent competition},
  author={Bansal, Trapit and Pachocki, Jakub and Sidor, Szymon and Sutskever, Ilya and Mordatch, Igor},
  journal={arXiv preprint arXiv:1710.03748},
  year={2017}
}

@inproceedings{majumdar2020evolutionary,
  title={Evolutionary Reinforcement Learning for Sample-Efficient Multiagent Coordination},
  author={Majumdar, Somdeb and Khadka, Shauharda and Miret, Santiago and Mcaleer, Stephen and Tumer, Kagan},
  booktitle={International Conference on Machine Learning},
  pages={6651--6660},
  year={2020},
  organization={PMLR}
}
@inproceedings{lowe2017multi,
  title={Multi-agent actor-critic for mixed cooperative-competitive environments},
  author={Lowe, Ryan and Wu, Yi I and Tamar, Aviv and Harb, Jean and Abbeel, OpenAI Pieter and Mordatch, Igor},
  booktitle={Advances in neural information processing systems},
  pages={6379--6390},
  year={2017}
}
@inproceedings{foerster2018counterfactual,
  title={Counterfactual multi-agent policy gradients},
  author={Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{brown2015simultaneous,
  title={Simultaneous abstraction and equilibrium finding in games},
  author={Brown, Noam and Sandholm, Tuomas},
  booktitle={Twenty-fourth international joint conference on artificial intelligence},
  year={2015}
}
@inproceedings{vcermak2017algorithm,
  title={An algorithm for constructing and solving imperfect recall abstractions of large extensive-form games},
  author={{\v{C}}erm{\'a}k, Ji{\v{r}}{\'\i} and Bo{\v{s}}ansky, Branislav and Lisy, Viliam},
  booktitle={Proceedings of the 26th International Joint Conference on Artificial Intelligence},
  pages={936--942},
  year={2017}
}
@article{li2018double,
  title={Double neural counterfactual regret minimization},
  author={Li, Hui and Hu, Kailiang and Ge, Zhibang and Jiang, Tao and Qi, Yuan and Song, Le},
  journal={arXiv preprint arXiv:1812.10607},
  year={2018}
}
@article{steinberger2019single,
  title={Single deep counterfactual regret minimization},
  author={Steinberger, Eric},
  journal={arXiv preprint arXiv:1901.07621},
  year={2019}
}

@misc{barrage_bots,
  author = {Sam Moore},
  title = {Stratego AI Evaluator},
  year = {2014},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/braathwaate/strategoevaluator}}
}

@article{fp,
  title={Iterative solution of games by fictitious play},
  author={Brown, George W.},
  journal={Activity analysis of production and allocation},
  pages={374-376},
  year={1951}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{brown2015regret,
  title={Regret-Based Pruning in Extensive-Form Games.},
  author={Brown, Noam and Sandholm, Tuomas},
  booktitle={NIPS},
  pages={1972--1980},
  year={2015}
}

@inproceedings{brown2017dynamic,
  title={Dynamic thresholding and pruning for regret minimization},
  author={Brown, Noam and Kroer, Christian and Sandholm, Tuomas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

@book{shoham2008multiagent,
  title={Multiagent systems: Algorithmic, game-theoretic, and logical foundations},
  author={Shoham, Yoav and Leyton-Brown, Kevin},
  year={2008},
  publisher={Cambridge University Press}
}

@article{gruslys2020advantage,
  title={The Advantage Regret-Matching Actor-Critic},
  author={Gruslys, Audr{\=u}nas and Lanctot, Marc and Munos, R{\'e}mi and Timbers, Finbarr and Schmid, Martin and Perolat, Julien and Morrill, Dustin and Zambaldi, Vinicius and Lespiau, Jean-Baptiste and Schultz, John and others},
  journal={arXiv preprint arXiv:2008.12234},
  year={2020}
}

@article{steinberger2020dream,
  title={DREAM: Deep Regret minimization with Advantage baselines and Model-free learning},
  author={Steinberger, Eric and Lerer, Adam and Brown, Noam},
  journal={arXiv preprint arXiv:2006.10410},
  year={2020}
}

@article{bowling2002multiagent,
  title={Multiagent learning using a variable learning rate},
  author={Bowling, Michael and Veloso, Manuela},
  journal={Artificial Intelligence},
  volume={136},
  number={2},
  pages={215--250},
  year={2002},
  publisher={Elsevier}
}

@inproceedings{qpg,
  title={Actor-critic policy optimization in partially observable multiagent environments},
  author={Srinivasan, Sriram and Lanctot, Marc and Zambaldi, Vinicius and P{\'e}rolat, Julien and Tuyls, Karl and Munos, R{\'e}mi and Bowling, Michael},
  booktitle={Advances in neural information processing systems},
  pages={3422--3435},
  year={2018}
}

@article{replicator,
  title={Evolutionary stable strategies and game dynamics},
  author={Taylor, Peter D and Jonker, Leo B},
  journal={Mathematical biosciences},
  volume={40},
  number={1-2},
  pages={145--156},
  year={1978},
  publisher={Citeseer}
}

@book{game_tree_complexity,
  title={Searching for solutions in games and artificial intelligence},
  author={Allis, Louis Victor}
}

@inproceedings{deep_cfr,
  title={Deep Counterfactual Regret Minimization},
  author={Brown, Noam and Lerer, Adam and Gross, Sam and Sandholm, Tuomas},
  booktitle={International Conference on Machine Learning},
  pages={793--802},
  year={2019}
}

@article{silver2017mastering1,
	title={Mastering the game of go without human knowledge},
	author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
	journal={Nature},
	volume={550},
	number={7676},
	pages={354},
	year={2017},
	publisher={Nature Publishing Group}
}

@article{silver2017mastering2,
	title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
	author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
	journal={arXiv preprint arXiv:1712.01815},
	year={2017}
}

@article{dota,
  title={Dota 2 with Large Scale Deep Reinforcement Learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Debiak, Przemyslaw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

@inproceedings{psro,
  title={A unified game-theoretic approach to multiagent reinforcement learning},
  author={Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and P{\'e}rolat, Julien and Silver, David and Graepel, Thore},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4190--4203},
  year={2017}
}

@book{agt,
  title={Algorithmic Game Theory},
  author={Nisan, Noam and Roughgarden, Tim and Tardos, Eva and Vazirani, Vijay V},
  year={2007},
  publisher={Cambridge University Press}
}

@inproceedings{xfp,
  title={Fictitious self-play in extensive-form games},
  author={Heinrich, Johannes and Lanctot, Marc and Silver, David},
  booktitle={International Conference on Machine Learning},
  pages={805--813},
  year={2015}
}

@article{neurd,
  title={Neural Replicator Dynamics},
  author={Omidshafiei, Shayegan and Hennes, Daniel and Morrill, Dustin and Munos, Remi and Perolat, Julien and Lanctot, Marc and Gruslys, Audrunas and Lespiau, Jean-Baptiste and Tuyls, Karl},
  journal={arXiv preprint arXiv:1906.00190},
  year={2019}
}

@inproceedings{cfr,
  title={Regret minimization in games with incomplete information},
  author={Zinkevich, Martin and Johanson, Michael and Bowling, Michael and Piccione, Carmelo},
  booktitle={Advances in neural information processing systems},
  pages={1729--1736},
  year={2008}
}

@inproceedings{ray,
  title={Ray: A distributed framework for emerging $\{$AI$\}$ applications},
  author={Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I and others},
  booktitle={13th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 18)},
  pages={561--577},
  year={2018}
}

@inproceedings{rllib,
  title={RLlib: Abstractions for Distributed Reinforcement Learning},
  author={Liang, Eric and Liaw, Richard and Nishihara, Robert and Moritz, Philipp and Fox, Roy and Goldberg, Ken and Gonzalez, Joseph and Jordan, Michael and Stoica, Ion},
  booktitle={International Conference on Machine Learning},
  pages={3053--3062},
  year={2018}
}

@article{discrete_sac,
  title={Soft Actor-Critic for Discrete Action Settings},
  author={Christodoulou, Petros},
  journal={arXiv preprint arXiv:1910.07207},
  year={2019}
}

@article{pbt,
  title={Human-level performance in 3D multiplayer games with population-based reinforcement learning},
  author={Jaderberg, Max and Czarnecki, Wojciech M and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C and Morcos, Ari S and Ruderman, Avraham and others},
  journal={Science},
  volume={364},
  number={6443},
  pages={859--865},
  year={2019},
  publisher={American Association for the Advancement of Science}
}

@article{deep_rl,
  title={Deep reinforcement learning: An overview},
  author={Li, Yuxi},
  journal={arXiv preprint arXiv:1701.07274},
  year={2017}
}

@inproceedings{stratego_bot,
  title={Quiescence search for stratego},
  author={Schadd, Maarten and Winands, Mark},
  booktitle={Proceedings of the 21st Benelux Conference on Artificial Intelligence. Eindhoven, the Netherlands},
  year={2009}
}

@article{stratego_bots,
  title={The 3rd stratego computer world championship},
  author={Jug, Sven and Schadd, Maarten},
  journal={Icga Journal},
  volume={32},
  number={4},
  pages={233},
  year={2009}
}

@article{brown2018superhuman,
  title={Superhuman AI for heads-up no-limit poker: Libratus beats top professionals},
  author={Brown, Noam and Sandholm, Tuomas},
  journal={Science},
  volume={359},
  number={6374},
  pages={418--424},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{sac,
  title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1861--1870},
  year={2018}
}

@book{theory_of_learning_in_games,
  title={The theory of learning in games},
  author={Fudenberg, Drew and Drew, Fudenberg and Levine, David K and Levine, David K},
  year={1998},
  publisher={The MIT Press}
}

@article{nfsp,
  title={Deep reinforcement learning from self-play in imperfect-information games},
  author={Heinrich, Johannes and Silver, David},
  journal={arXiv preprint arXiv:1603.01121},
  year={2016}
}

@inproceedings{rectified_psro,
  title={Open-ended learning in symmetric zero-sum games},
  author={Balduzzi, David and Garnelo, Marta and Bachrach, Yoram and Czarnecki, Wojciech and Perolat, Julien and Jaderberg, Max and Graepel, Thore},
  booktitle={International Conference on Machine Learning},
  pages={434--443},
  year={2019}
}

@article{alphastar,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}


@book{kuhn1953contributions,
  title={Contributions to the Theory of Games},
  author={Kuhn, Harold William and Tucker, Albert William},
  volume={2},
  year={1953},
  publisher={Princeton University Press}
}

@inproceedings{zinkevich2008regret,
  title={Regret minimization in games with incomplete information},
  author={Zinkevich, Martin and Johanson, Michael and Bowling, Michael and Piccione, Carmelo},
  booktitle={Advances in neural information processing systems},
  pages={1729--1736},
  year={2008}
}

@article{mcaleer2020pipeline,
  title={Pipeline PSRO: A Scalable Approach for Finding Approximate Nash Equilibria in Large Games},
  author={McAleer, Stephen and Lanier, John and Fox, Roy and Baldi, Pierre},
  booktitle={Advances in neural information processing systems},
  year={2020}
}

@article{lanctot2019openspiel,
  title={OpenSpiel: A framework for reinforcement learning in games},
  author={Lanctot, Marc and Lockhart, Edward and Lespiau, Jean-Baptiste and Zambaldi, Vinicius and Upadhyay, Satyaki and P{\'e}rolat, Julien and Srinivasan, Sriram and Timbers, Finbarr and Tuyls, Karl and Omidshafiei, Shayegan and others},
  journal={arXiv preprint arXiv:1908.09453},
  year={2019}
}
@inproceedings{hansen2004dynamic,
  title={Dynamic programming for partially observable stochastic games},
  author={Hansen, Eric A and Bernstein, Daniel S and Zilberstein, Shlomo},
  booktitle={AAAI},
  volume={4},
  pages={709--715},
  year={2004}
}

@article{bosansky2014exact,
  title={An exact double-oracle algorithm for zero-sum extensive-form games with imperfect information},
  author={Bosansky, Branislav and Kiekintveld, Christopher and Lisy, Viliam and Pechoucek, Michal},
  journal={Journal of Artificial Intelligence Research},
  volume={51},
  pages={829--866},
  year={2014}
}

@article{silver_2017,
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and Driessche, George van den and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  title = {Mastering the game of Go without human knowledge},
  journal = {Nature},
  publisher = {Nature Publishing Group},
  issn = {0028-0836},
  doi = {10.1038/nature24270},
  volume = {550},
  month = {10},
  pages = {354--359},
  number = {7676},
  url = {http:https://doi.org/10.1038/nature24270},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.}
}








@InProceedings{pmlr-v80-liang18b, title = {{RL}lib: Abstractions for Distributed Reinforcement Learning}, author = {Liang, Eric and Liaw, Richard and Nishihara, Robert and Moritz, Philipp and Fox, Roy and Goldberg, Ken and Gonzalez, Joseph and Jordan, Michael and Stoica, Ion}, booktitle = {Proceedings of the 35th International Conference on Machine Learning}, pages = {3053--3062}, year = {2018}, editor = {Jennifer Dy and Andreas Krause}, volume = {80}, series = {Proceedings of Machine Learning Research}, address = {Stockholmsmässan, Stockholm Sweden}, month = {10--15 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v80/liang18b/liang18b.pdf}, url = {http://proceedings.mlr.press/v80/liang18b.html}, abstract = {Reinforcement learning (RL) algorithms involve the deep nesting of highly irregular computation patterns, each of which typically exhibits opportunities for distributed computation. We argue for distributing RL components in a composable way by adapting algorithms for top-down hierarchical control, thereby encapsulating parallelism and resource requirements within short-running compute tasks. We demonstrate the benefits of this principle through RLlib: a library that provides scalable software primitives for RL. These primitives enable a broad range of algorithms to be implemented with high performance, scalability, and substantial code reuse. RLlib is available as part of the open source Ray project at http://rllib.io/.} }

@article{LanctotEtAl2019OpenSpiel,
  title     = {{OpenSpiel}: A Framework for Reinforcement Learning in Games},
  author    = {Marc Lanctot and Edward Lockhart and Jean-Baptiste Lespiau and
               Vinicius Zambaldi and Satyaki Upadhyay and Julien P\'{e}rolat and
               Sriram Srinivasan and Finbarr Timbers and Karl Tuyls and
               Shayegan Omidshafiei and Daniel Hennes and Dustin Morrill and
               Paul Muller and Timo Ewalds and Ryan Faulkner and J\'{a}nos Kram\'{a}r
               and Bart De Vylder and Brennan Saeta and James Bradbury and David Ding
               and Sebastian Borgeaud and Matthew Lai and Julian Schrittwieser and
               Thomas Anthony and Edward Hughes and Ivo Danihelka and Jonah Ryan-Davis},
  year      = {2019},
  eprint    = {1908.09453},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  journal   = {CoRR},
  volume    = {abs/1908.09453},
  url       = {http://arxiv.org/abs/1908.09453},
}

@inproceedings{DBLP:journals/corr/KingmaB14,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{van2016deep,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  number={1},
  year={2016}
}

@article{rm,
  title={A simple adaptive procedure leading to correlated equilibrium},
  author={Hart, Sergiu and Mas-Colell, Andreu},
  journal={Econometrica},
  volume={68},
  number={5},
  pages={1127--1150},
  year={2000},
  publisher={Wiley Online Library}
}


@article{DBLP:journals/corr/Tammelin14,
  author    = {Oskari Tammelin},
  title     = {Solving Large Imperfect Information Games Using {CFR+}},
  journal   = {CoRR},
  volume    = {abs/1407.5042},
  year      = {2014},
  url       = {http://arxiv.org/abs/1407.5042},
  eprinttype = {arXiv},
  eprint    = {1407.5042},
  timestamp = {Mon, 13 Aug 2018 16:48:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Tammelin14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}