\begin{thebibliography}{74}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Kingma and Welling(2019)]{kingma2019introduction}
Diederik~P Kingma and Max Welling.
\newblock An introduction to variational autoencoders.
\newblock \emph{Foundations and Trends in Machine Learning}, 12\penalty0
  (4):\penalty0 307--392, 2019.

\bibitem[Doersch(2016)]{doersch2016tutorial}
Carl Doersch.
\newblock Tutorial on variational autoencoders.
\newblock \emph{arXiv preprint arXiv:1606.05908}, 2016.

\bibitem[Zhao et~al.(2019)Zhao, Song, and Ermon]{zhao2019infovae}
Shengjia Zhao, Jiaming Song, and Stefano Ermon.
\newblock Infovae: Balancing learning and inference in variational
  autoencoders.
\newblock In \emph{Proceedings of the aaai conference on artificial
  intelligence}, volume~33, pages 5885--5892, 2019.

\bibitem[Ho and Ermon(2016)]{NIPS2016_gan}
Jonathan Ho and Stefano Ermon.
\newblock Generative adversarial imitation learning.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~29,
  2016.

\bibitem[Goodfellow et~al.(2020)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2020generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial networks.
\newblock \emph{Communications of the ACM}, 63\penalty0 (11):\penalty0
  139--144, 2020.

\bibitem[Tabak and Vanden-Eijnden(2010)]{tabak2010density}
Esteban~G. Tabak and Eric Vanden-Eijnden.
\newblock {Density estimation by dual ascent of the log-likelihood}.
\newblock \emph{Communications in Mathematical Sciences}, 8\penalty0
  (1):\penalty0 217 -- 233, 2010.

\bibitem[Tabak and Turner(2013)]{tabak2013family}
E.~G. Tabak and Cristina~V. Turner.
\newblock A family of nonparametric density estimation algorithms.
\newblock \emph{Communications on Pure and Applied Mathematics}, 66\penalty0
  (2):\penalty0 145--164, 2013.

\bibitem[Rezende and Mohamed(2015)]{rezende2015variational}
Danilo Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In \emph{International conference on machine learning}, pages
  1530--1538. PMLR, 2015.

\bibitem[Papamakarios et~al.(2021)Papamakarios, Nalisnick, Rezende, Mohamed,
  and Lakshminarayanan]{papamakarios2021normalizing}
George Papamakarios, Eric Nalisnick, Danilo~Jimenez Rezende, Shakir Mohamed,
  and Balaji Lakshminarayanan.
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0
  (1):\penalty0 2617--2680, 2021.

\bibitem[Mathieu and Nickel(2020)]{mathieu2020riemannian}
Emile Mathieu and Maximilian Nickel.
\newblock Riemannian continuous normalizing flows.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2503--2515, 2020.

\bibitem[Song et~al.(2021{\natexlab{a}})Song, Sohl-Dickstein, Kingma, Kumar,
  Ermon, and Poole]{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{2020_ho_denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 6840--6851, 2020.

\bibitem[Song et~al.(2021{\natexlab{b}})Song, Durkan, Murray, and
  Ermon]{song2021maximum}
Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon.
\newblock Maximum likelihood training of score-based diffusion models.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 1415--1428, 2021{\natexlab{b}}.

\bibitem[Hinton(2012)]{hinton2012practical}
Geoffrey~E Hinton.
\newblock A practical guide to training restricted boltzmann machines.
\newblock \emph{Neural Networks: Tricks of the Trade: Second Edition}, pages
  599--619, 2012.

\bibitem[Salakhutdinov and Larochelle(2010)]{salakhutdinov2010efficient}
Ruslan Salakhutdinov and Hugo Larochelle.
\newblock Efficient learning of deep boltzmann machines.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 693--700. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem[Schulz et~al.(2010)Schulz, M{\"u}ller, Behnke,
  et~al.]{schulz2010investigating}
Hannes Schulz, Andreas M{\"u}ller, Sven Behnke, et~al.
\newblock Investigating convergence of restricted boltzmann machine learning.
\newblock In \emph{NIPS 2010 Workshop on Deep Learning and Unsupervised Feature
  Learning}, volume~1, pages 6--1, 2010.

\bibitem[LeCun et~al.(2007)LeCun, Chopra, and Hadsell]{lecun2006tutorial}
Yann LeCun, Sumit Chopra, and Raia Hadsell.
\newblock A tutorial on energy-based learning.
\newblock In G{\"o}khan BakIr, Thomas Hofmann, Alexander~J Smola, Bernhard
  Sch{\"o}lkopf, and Ben Taskar, editors, \emph{Predicting structured data},
  chapter~10. MIT press, 2007.

\bibitem[Gutmann and Hyv{\"a}rinen(2010)]{gutmann2010noise}
Michael Gutmann and Aapo Hyv{\"a}rinen.
\newblock Noise-contrastive estimation: A new estimation principle for
  unnormalized statistical models.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 297--304. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem[Song et~al.(2020)Song, Garg, Shi, and Ermon]{song2020sliced}
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon.
\newblock Sliced score matching: A scalable approach to density and score
  estimation.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 574--584.
  PMLR, 2020.

\bibitem[Boltzmann(1970)]{boltzmann1970weitere}
Ludwig Boltzmann.
\newblock Weitere studien {\"u}ber das w{\"a}rmegleichgewicht unter
  gasmolek{\"u}len.
\newblock \emph{Kinetische Theorie II}, pages 115--225, 1970.

\bibitem[Gibbs(1902)]{gibbs1902elementary}
Josiah~Willard Gibbs.
\newblock \emph{Elementary principles in statistical mechanics: developed with
  especial reference to the rational foundations of thermodynamics}.
\newblock C. Scribner's sons, 1902.

\bibitem[Lifshitz and Pitaevskii(2013)]{lifshitz}
Evgenii~Mikhailovich Lifshitz and Lev~Petrovich Pitaevskii.
\newblock \emph{Statistical physics: theory of the condensed state}, volume~9.
\newblock Elsevier, 2013.

\bibitem[Mehrabi et~al.(2021)Mehrabi, Morstatter, Saxena, Lerman, and
  Galstyan]{mehrabi2021survey}
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram
  Galstyan.
\newblock A survey on bias and fairness in machine learning.
\newblock \emph{ACM Computing Surveys (CSUR)}, 54\penalty0 (6):\penalty0 1--35,
  2021.

\bibitem[Hyv{\"a}rinen and Dayan(2005)]{hyvarinen2005estimation}
Aapo Hyv{\"a}rinen and Peter Dayan.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0 (4), 2005.

\bibitem[Hinton(2002)]{hinton2002training}
Geoffrey~E Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural computation}, 14\penalty0 (8):\penalty0 1771--1800,
  2002.

\bibitem[Welling and Hinton(2002)]{welling2002new}
Max Welling and Geoffrey~E Hinton.
\newblock A new learning algorithm for mean field boltzmann machines.
\newblock In \emph{International conference on artificial neural networks},
  pages 351--357, 2002.

\bibitem[Carreira-Perpinan and Hinton(2005)]{carreira2005contrastive}
Miguel~A Carreira-Perpinan and Geoffrey Hinton.
\newblock On contrastive divergence learning.
\newblock In \emph{International workshop on artificial intelligence and
  statistics}, pages 33--40. PMLR, 2005.

\bibitem[Hyvarinen(2007)]{hyvarinen2007connections}
Aapo Hyvarinen.
\newblock Connections between score matching, contrastive divergence, and
  pseudolikelihood for continuous-valued variables.
\newblock \emph{IEEE Transactions on neural networks}, 18\penalty0
  (5):\penalty0 1529--1531, 2007.

\bibitem[Tieleman(2008)]{tieleman2008training}
Tijmen Tieleman.
\newblock Training restricted boltzmann machines using approximations to the
  likelihood gradient.
\newblock In \emph{International conference on Machine learning}, pages
  1064--1071, 2008.

\bibitem[Jacob et~al.(2020)Jacob, O’Leary, and
  Atchad{\'e}]{jacob2017unbiased}
Pierre~E Jacob, John O’Leary, and Yves~F Atchad{\'e}.
\newblock Unbiased markov chain monte carlo methods with couplings.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 82\penalty0 (3), 2020.

\bibitem[Qiu et~al.(2020)Qiu, Zhang, and Wang]{qiu2020unbiased}
Yixuan Qiu, Lingsong Zhang, and Xiao Wang.
\newblock Unbiased contrastive divergence algorithm for training energy-based
  latent variable models.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Du and Mordatch(2019)]{du2019implicit}
Yilun Du and Igor Mordatch.
\newblock Implicit generation and modeling with energy based models.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Du et~al.(2021)Du, Li, Tenenbaum, and Mordatch]{du2021improved}
Yilun Du, Shuang Li, Joshua Tenenbaum, and Igor Mordatch.
\newblock Improved contrastive divergence training of energy-based models.
\newblock In \emph{International Conference on Machine Learning}, pages
  2837--2848. PMLR, 2021.

\bibitem[Jarzynski(1997)]{jarzynski1997nonequilibrium}
C~Jarzynski.
\newblock Nonequilibrium equality for free energy differences.
\newblock \emph{Physical Review Letters}, 78\penalty0 (14):\penalty0 2690,
  1997.

\bibitem[Doucet et~al.(2001)Doucet, De~Freitas, Gordon,
  et~al.]{doucet2001sequential}
Arnaud Doucet, Nando De~Freitas, Neil~James Gordon, et~al.
\newblock \emph{Sequential Monte Carlo methods in practice}, volume~1.
\newblock Springer, 2001.

\bibitem[Song and Kingma(2021)]{song2021train}
Yang Song and Diederik~P Kingma.
\newblock How to train your energy-based models.
\newblock \emph{arXiv preprint arXiv:2101.03288}, 2021.

\bibitem[Xie et~al.(2016)Xie, Lu, Zhu, and Wu]{xie2016theory}
Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu.
\newblock A theory of generative convnet.
\newblock In \emph{International Conference on Machine Learning}, pages
  2635--2644. PMLR, 2016.

\bibitem[Grathwohl et~al.(2019)Grathwohl, Wang, Jacobsen, Duvenaud, Norouzi,
  and Swersky]{grathwohl2019your}
Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud,
  Mohammad Norouzi, and Kevin Swersky.
\newblock Your classifier is secretly an energy based model and you should
  treat it like one.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Brooks et~al.(2011)Brooks, Gelman, Jones, and
  Meng]{brooks2011handbook}
Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng.
\newblock \emph{Handbook of Markov chain Monte Carlo}.
\newblock CRC press, 2011.

\bibitem[Liu and Liu(2001)]{liu2001monte}
Jun~S Liu and Jun~S Liu.
\newblock \emph{Monte Carlo strategies in scientific computing}, volume~75.
\newblock Springer, 2001.

\bibitem[Vincent(2011)]{vincent2011connection}
Pascal Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock \emph{Neural computation}, 23\penalty0 (7):\penalty0 1661--1674,
  2011.

\bibitem[Swersky et~al.(2011)Swersky, Ranzato, Buchman, Freitas, and
  Marlin]{swersky2011autoencoders}
Kevin Swersky, Marc'Aurelio Ranzato, David Buchman, Nando~D Freitas, and
  Benjamin~M Marlin.
\newblock On autoencoders and score matching for energy based models.
\newblock In \emph{International conference on machine learning (ICML-11)},
  pages 1201--1208, 2011.

\bibitem[Wenliang(2022)]{wenliang2022failure}
Li~Kevin Wenliang.
\newblock On the failure of variational score matching for {VAE} models.
\newblock \emph{arXiv preprint arXiv:2210.13390}, 2022.

\bibitem[Wenliang et~al.(2019)Wenliang, Sutherland, Strathmann, and
  Gretton]{wenliang2019learning}
Li~Wenliang, Danica~J Sutherland, Heiko Strathmann, and Arthur Gretton.
\newblock Learning deep kernels for exponential family densities.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Song and Ermon(2019)]{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock In \emph{Advances in neural information processing systems},
  volume~32, 2019.

\bibitem[Xie et~al.(2018)Xie, Lu, Gao, Zhu, and Wu]{xie2018cooperative}
Jianwen Xie, Yang Lu, Ruiqi Gao, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Cooperative training of descriptor and generator networks.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 42\penalty0 (1):\penalty0 27--45, 2018.

\bibitem[Nijkamp et~al.(2019)Nijkamp, Hill, Zhu, and Wu]{nijkamp2019learning}
Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Learning non-convergent non-persistent short-run {MCMC} toward
  energy-based model.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Gao et~al.(2020)Gao, Nijkamp, Kingma, Xu, Dai, and Wu]{gao2020flow}
Ruiqi Gao, Erik Nijkamp, Diederik~P Kingma, Zhen Xu, Andrew~M Dai, and
  Ying~Nian Wu.
\newblock Flow contrastive estimation of energy-based models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 7518--7528, 2020.

\bibitem[Xiao et~al.(2020)Xiao, Kreis, Kautz, and Vahdat]{xiao2020vaebm}
Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vahdat.
\newblock Vaebm: A symbiosis between variational autoencoders and energy-based
  models.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Xie et~al.(2021{\natexlab{a}})Xie, Zheng, and Li]{xie2021learning}
Jianwen Xie, Zilong Zheng, and Ping Li.
\newblock Learning energy-based model with variational auto-encoder as
  amortized sampler.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 10441--10451, 2021{\natexlab{a}}.

\bibitem[Xie et~al.(2021{\natexlab{b}})Xie, Zhu, Li, and Li]{xie2021tale}
Jianwen Xie, Yaxuan Zhu, Jun Li, and Ping Li.
\newblock A tale of two flows: Cooperative learning of langevin flow and
  normalizing flow toward energy-based model.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.

\bibitem[Lee et~al.(2022)Lee, Jeong, Park, and Shin]{lee2022guiding}
Hankook Lee, Jongheon Jeong, Sejun Park, and Jinwoo Shin.
\newblock Guiding energy-based models via contrastive latent variables.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2022.

\bibitem[Yair and Michaeli(2021)]{yair2020contrastive}
Omer Yair and Tomer Michaeli.
\newblock Contrastive divergence learning is a time reversal adversarial game.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Domingo-Enrich et~al.(2021{\natexlab{a}})Domingo-Enrich, Bietti,
  Vanden-Eijnden, and Bruna]{pmlr-v139-domingo-enrich21a}
Carles Domingo-Enrich, Alberto Bietti, Eric Vanden-Eijnden, and Joan Bruna.
\newblock On energy-based models with overparametrized shallow neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  2771--2782, 2021{\natexlab{a}}.

\bibitem[Domingo-Enrich et~al.(2021{\natexlab{b}})Domingo-Enrich, Bietti,
  Gabri{\'e}, Bruna, and Vanden-Eijnden]{domingoenrich2022dual}
Carles Domingo-Enrich, Alberto Bietti, Marylou Gabri{\'e}, Joan Bruna, and Eric
  Vanden-Eijnden.
\newblock Dual training of energy-based models with overparametrized shallow
  neural networks.
\newblock \emph{arXiv preprint arXiv:2107.05134}, 2021{\natexlab{b}}.

\bibitem[Neal(2001)]{neal2001annealed}
Radford~M Neal.
\newblock Annealed importance sampling.
\newblock \emph{Statistics and computing}, 11:\penalty0 125--139, 2001.

\bibitem[Le et~al.(2018)Le, Igl, Rainforth, Jin, and Wood]{anh2018autoencoding}
Tuan~Anh Le, Maximilian Igl, Tom Rainforth, Tom Jin, and Frank Wood.
\newblock Auto-encoding sequential monte carlo.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=BJ8c3f-0b}.

\bibitem[Ding and Freedman(2020)]{ding2020learning}
Xinqiang Ding and David~J. Freedman.
\newblock Learning deep generative models with annealed importance sampling,
  2020.

\bibitem[Midgley et~al.(2023)Midgley, Stimper, Simm, Sch{\"o}lkopf, and
  Hern{\'a}ndez-Lobato]{midgley2023flow}
Laurence~Illing Midgley, Vincent Stimper, Gregor N.~C. Simm, Bernhard
  Sch{\"o}lkopf, and Jos{\'e}~Miguel Hern{\'a}ndez-Lobato.
\newblock Flow annealed importance sampling bootstrap.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=XCTVFJwS9LJ}.

\bibitem[Du et~al.(2023)Du, Durkan, Strudel, Tenenbaum, Dieleman, Fergus,
  Sohl-Dickstein, Doucet, and Grathwohl]{du2023reduce}
Yilun Du, Conor Durkan, Robin Strudel, Joshua~B Tenenbaum, Sander Dieleman, Rob
  Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will~Sussman Grathwohl.
\newblock Reduce, reuse, recycle: Compositional generation with energy-based
  diffusion models and mcmc.
\newblock In \emph{International Conference on Machine Learning}, pages
  8489--8510. PMLR, 2023.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and
  Ganguli]{pmlr-v37-sohl-dickstein15}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In Francis Bach and David Blei, editors, \emph{Proceedings of the
  32nd International Conference on Machine Learning}, volume~37 of
  \emph{Proceedings of Machine Learning Research}, pages 2256--2265, Lille,
  France, 07--09 Jul 2015. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v37/sohl-dickstein15.html}.

\bibitem[Doucet et~al.(2022)Doucet, Grathwohl, Matthews, and
  Strathmann]{doucet2022scorebased}
Arnaud Doucet, Will~Sussman Grathwohl, Alexander G. D.~G. Matthews, and Heiko
  Strathmann.
\newblock Score-based diffusion meets annealed importance sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Oksendal(2003)]{oksendal2003stochastic}
Bernt Oksendal.
\newblock \emph{Stochastic Differential Equations}.
\newblock Springer-Verlag Berlin Heidelberg, 6 edition, 2003.

\bibitem[Mattingly et~al.(2002)Mattingly, Stuart, and
  Higham]{mattingly2002stochastic}
J.~C. Mattingly, A.~M. Stuart, and D.~J. Higham.
\newblock {Ergodicity for SDEs and approximations: locally Lipschitz vector
  fields and degenerate noise}.
\newblock \emph{Stochastic Processes and their Applications}, 101\penalty0
  (2):\penalty0 185--232, October 2002.

\bibitem[Talay and Tubaro(1990)]{talay1900expansion}
Denis Talay and Luciano Tubaro.
\newblock Expansion of the global error for numerical schemes solving
  stochastic differential equations.
\newblock \emph{Stochastic Analysis and Applications}, 8\penalty0 (4):\penalty0
  483--509, 1990.

\bibitem[Roberts and Tweedie(1996)]{roberts1996exponential}
Gareth~O. Roberts and Richard~L. Tweedie.
\newblock Exponential convergence of langevin distributions and their discrete
  approximations.
\newblock \emph{Bernoulli}, 2\penalty0 (4):\penalty0 341--363, 1996.
\newblock ISSN 13507265.
\newblock URL \url{http://www.jstor.org/stable/3318418}.

\bibitem[Roberts and Rosenthal(1998)]{roberts1998optimal}
Gareth~O Roberts and Jeffrey~S Rosenthal.
\newblock Optimal scaling of discrete approximations to langevin diffusions.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 60\penalty0 (1):\penalty0 255--268, 1998.

\bibitem[Moral et~al.(2012)Moral, Doucet, and Jasra]{del2012adaptive}
Pierre~Del Moral, Arnaud Doucet, and Ajay Jasra.
\newblock {On adaptive resampling strategies for sequential Monte Carlo
  methods}.
\newblock \emph{Bernoulli}, 18\penalty0 (1):\penalty0 252 -- 278, 2012.

\bibitem[Evans(2022)]{evans2022partial}
Lawrence~C Evans.
\newblock \emph{Partial differential equations}, volume~19.
\newblock American Mathematical Society, 2022.

\bibitem[Gordon et~al.(1993)Gordon, Salmond, and Smith]{gordon1993novel}
Neil~J Gordon, David~J Salmond, and Adrian~FM Smith.
\newblock Novel approach to nonlinear/non-gaussian bayesian state estimation.
\newblock In \emph{IEE proceedings F (radar and signal processing)}, volume
  140, pages 107--113. IET, 1993.

\bibitem[Kitagawa(1996)]{kitagawa1996monte}
Genshiro Kitagawa.
\newblock Monte carlo filter and smoother for non-gaussian nonlinear state
  space models.
\newblock \emph{Journal of computational and graphical statistics}, 5\penalty0
  (1):\penalty0 1--25, 1996.

\bibitem[Carpenter et~al.(1999)Carpenter, Clifford, and
  Fearnhead]{carpenter1999improved}
James Carpenter, Peter Clifford, and Paul Fearnhead.
\newblock Improved particle filter for nonlinear problems.
\newblock \emph{IEE Proceedings-Radar, Sonar and Navigation}, 146\penalty0
  (1):\penalty0 2--7, 1999.

\bibitem[Li et~al.(2015)Li, Bolic, and Djuric]{li2015resampling}
Tiancheng Li, Miodrag Bolic, and Petar~M Djuric.
\newblock Resampling methods for particle filtering: classification,
  implementation, and strategies.
\newblock \emph{IEEE Signal processing magazine}, 32\penalty0 (3):\penalty0
  70--86, 2015.

\bibitem[Albergo et~al.(2023)Albergo, Boffi, and
  Vanden-Eijnden]{albergo2023stochastic}
Michael~S Albergo, Nicholas~M Boffi, and Eric Vanden-Eijnden.
\newblock Stochastic interpolants: A unifying framework for flows and
  diffusions.
\newblock \emph{arXiv preprint arXiv:2303.08797}, 2023.

\end{thebibliography}
