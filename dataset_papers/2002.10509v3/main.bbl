\begin{thebibliography}{10}

\bibitem{andriushchenko2019square}
M.~Andriushchenko, F.~Croce, N.~Flammarion, and M.~Hein.
\newblock Square attack: a query-efficient black-box adversarial attack via
  random search.
\newblock {\em arXiv preprint arXiv:1912.00049}, 2019.

\bibitem{athalye2018obfuscated}
A.~Athalye, N.~Carlini, and D.~Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{athalye2018robust}
A.~Athalye, L.~Engstrom, A.~Ilyas, and K.~Kwok.
\newblock Synthesizing robust adversarial examples.
\newblock In {\em International Conference on Machine Learning}, pages
  284--293, 2018.

\bibitem{bellec2018iclrRewiring}
G.~Bellec, D.~Kappel, W.~Maass, and R.~Legenstein.
\newblock Deep rewiring: Training very sparse deep networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{biggio2013evasionAtacks}
B.~Biggio, I.~Corona, D.~Maiorca, B.~Nelson, N.~{\v{S}}rndi{\'c}, P.~Laskov,
  G.~Giacinto, and F.~Roli.
\newblock Evasion attacks against machine learning at test time.
\newblock In {\em Joint European conference on machine learning and knowledge
  discovery in databases}, pages 387--402. Springer, 2013.

\bibitem{carlini2017towards}
N.~Carlini and D.~Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In {\em Security and Privacy (SP), 2017 IEEE Symposium on}, pages
  39--57. IEEE, 2017.

\bibitem{carmon2019unlabeled}
Y.~Carmon, A.~Raghunathan, L.~Schmidt, J.~C. Duchi, and P.~S. Liang.
\newblock Unlabeled data improves adversarial robustness.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  11190--11201, 2019.

\bibitem{cohen2019certified}
J.~Cohen, E.~Rosenfeld, and Z.~Kolter.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock In {\em International Conference on Machine Learning}, pages
  1310--1320, 2019.

\bibitem{croce2020autoattack}
F.~Croce and M.~Hein.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock {\em arXiv preprint arXiv:2003.01690}, 2020.

\bibitem{DhillonICLR2018StochasticPruning}
G.~S. Dhillon, K.~Azizzadenesheli, J.~D. Bernstein, J.~Kossaifi, A.~Khanna,
  Z.~C. Lipton, and A.~Anandkumar.
\newblock Stochastic activation pruning for robust adversarial defense.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{frankleiclr19lotteryticket}
J.~Frankle and M.~Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{glorot201XavierInit}
X.~Glorot and Y.~Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256, 2010.

\bibitem{goodfellow2014explaining}
I.~J. Goodfellow, J.~Shlens, and C.~Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em International Conference on Learning Representations}, 2015.

\bibitem{Gowal2018Ibp}
S.~Gowal, K.~Dvijotham, R.~Stanforth, R.~Bunel, C.~Qin, J.~Uesato,
  R.~Arandjelovic, T.~A. Mann, and P.~Kohli.
\newblock On the effectiveness of interval bound propagation for training
  verifiably robust models.
\newblock {\em CoRR}, abs/1810.12715, 2018.

\bibitem{gui2019advAtmc}
S.~Gui, H.~N. Wang, H.~Yang, C.~Yu, Z.~Wang, and J.~Liu.
\newblock Model compression with adversarial robustness: A unified optimization
  framework.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1283--1294, 2019.

\bibitem{Guo16nipsnetworksurgery}
Y.~Guo, A.~Yao, and Y.~Chen.
\newblock Dynamic network surgery for efficient dnns.
\newblock In {\em Advances in Neural Information Processing Systems 29: Annual
  Conference on Neural Information Processing Systems 2016, December 5-10,
  2016, Barcelona, Spain}, pages 1379--1387, 2016.

\bibitem{guoNIPS18SparseDnn}
Y.~Guo, C.~Zhang, C.~Zhang, and Y.~Chen.
\newblock Sparse dnns with improved adversarial robustness.
\newblock In {\em Advances in neural information processing systems}, pages
  242--251, 2018.

\bibitem{han2015deepcompress}
S.~Han, H.~Mao, and W.~J. Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock In {\em International Conference on Learning Representations}, 2016.

\bibitem{han2015nips}
S.~Han, J.~Pool, J.~Tran, and W.~Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In {\em Advances in neural information processing systems}, pages
  1135--1143, 2015.

\bibitem{he2015kaimingInit}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015.

\bibitem{he2016OrgResnet}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hendrycks2019advpretrain}
D.~Hendrycks, K.~Lee, and M.~Mazeika.
\newblock Using pre-training can improve model robustness and uncertainty.
\newblock {\em arXiv preprint arXiv:1901.09960}, 2019.

\bibitem{hooker2019selective}
S.~Hooker, A.~Courville, Y.~Dauphin, and A.~Frome.
\newblock Selective brain damage: Measuring the disparate impact of model
  pruning.
\newblock {\em arXiv preprint arXiv:1911.05248}, 2019.

\bibitem{lecuyer2019certified}
M.~Lecuyer, V.~Atlidakis, R.~Geambasu, D.~Hsu, and S.~Jana.
\newblock Certified robustness to adversarial examples with differential
  privacy.
\newblock In {\em 2019 IEEE Symposium on Security and Privacy (SP)}, pages
  656--672. IEEE, 2019.

\bibitem{leecilr19snip}
N.~Lee, T.~Ajanthan, and P.~Torr.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{li2019certified}
B.~Li, C.~Chen, W.~Wang, and L.~Carin.
\newblock Certified adversarial robustness with additive noise.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9459--9469, 2019.

\bibitem{li2016l1filterprune}
H.~Li, A.~Kadav, I.~Durdanovic, H.~Samet, and H.~P. Graf.
\newblock Pruning filters for efficient convnets.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{lin2017runtime}
J.~Lin, Y.~Rao, J.~Lu, and J.~Zhou.
\newblock Runtime neural pruning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2181--2191, 2017.

\bibitem{liu2018rethinking}
Z.~Liu, M.~Sun, T.~Zhou, G.~Huang, and T.~Darrell.
\newblock Rethinking the value of network pruning.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{madry2017towards}
A.~Madry, A.~Makelov, L.~Schmidt, D.~Tsipras, and A.~Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock {\em International Conference on Learning Representations}, 2018.

\bibitem{papernot2016sok}
N.~Papernot, P.~McDaniel, A.~Sinha, and M.~Wellman.
\newblock Towards the science of security and privacy in machine learning.
\newblock {\em arXiv preprint arXiv:1611.03814}, 2016.

\bibitem{ramanujan2019hiddenSubnet}
V.~Ramanujan, M.~Wortsman, A.~Kembhavi, A.~Farhadi, and M.~Rastegari.
\newblock What's hidden in a randomly weighted neural network?
\newblock {\em arXiv preprint arXiv:1911.13299}, 2019.

\bibitem{salman2019provably}
H.~Salman, J.~Li, I.~Razenshteyn, P.~Zhang, H.~Zhang, S.~Bubeck, and G.~Yang.
\newblock Provably robust deep learning via adversarially trained smoothed
  classifiers.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  11289--11300, 2019.

\bibitem{sehwag2019dls}
V.~Sehwag, S.~Wang, P.~Mittal, and S.~Jana.
\newblock Towards compact and robust deep neural networks.
\newblock {\em arXiv preprint arXiv:1906.06110}, 2019.

\bibitem{Shafahi2019freeadvtraining}
A.~Shafahi, M.~Najibi, M.~A. Ghiasi, Z.~Xu, J.~Dickerson, C.~Studer, L.~S.
  Davis, G.~Taylor, and T.~Goldstein.
\newblock Adversarial training for free!
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3353--3364, 2019.

\bibitem{simonyan2014VGG}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em International Conference on Learning Representations}, 2015.

\bibitem{wang2020LTGradflow}
C.~Wang, G.~Zhang, and R.~Grosse.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{wang2018mixtrain}
S.~Wang, Y.~Chen, A.~Abdou, and S.~Jana.
\newblock Mixtrain: Scalable training of formally robust neural networks.
\newblock {\em arXiv preprint arXiv:1811.02625}, 2018.

\bibitem{wang2018efficient}
S.~Wang, K.~Pei, J.~Whitehouse, J.~Yang, and S.~Jana.
\newblock Efficient formal safety analysis of neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6367--6377, 2018.

\bibitem{wang2018formal}
S.~Wang, K.~Pei, J.~Whitehouse, J.~Yang, and S.~Jana.
\newblock Formal security analysis of neural networks using symbolic intervals.
\newblock In {\em 27th USENIX Security Symposium (USENIX Security 18)}, pages
  1599--1614, 2018.

\bibitem{wangSIP2018PruningDefense}
S.~Wang, X.~Wang, S.~Ye, P.~Zhao, and X.~Lin.
\newblock Defending dnn adversarial attacks with pruning and logits
  augmentation.
\newblock In {\em 2018 IEEE Global Conference on Signal and Information
  Processing (GlobalSIP)}, pages 1144--1148. IEEE, 2018.

\bibitem{wijayanto2019towards}
A.~W. Wijayanto, J.~J. Choong, K.~Madhawa, and T.~Murata.
\newblock Towards robust compressed convolutional neural networks.
\newblock In {\em 2019 IEEE International Conference on Big Data and Smart
  Computing (BigComp)}, pages 1--8. IEEE, 2019.

\bibitem{wong2017provable}
E.~Wong and Z.~Kolter.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock In {\em International Conference on Machine Learning}, pages
  5286--5295, 2018.

\bibitem{xie2019advtrainScale}
C.~Xie and A.~Yuille.
\newblock Intriguing properties of adversarial training.
\newblock {\em arXiv preprint arXiv:1906.03787}, 2019.

\bibitem{ye2019AdvPruneBoth}
S.~Ye, K.~Xu, S.~Liu, H.~Cheng, J.-H. Lambrechts, H.~Zhang, A.~Zhou, K.~Ma,
  Y.~Wang, and X.~Lin.
\newblock Adversarial robustness vs. model compression, or both.
\newblock In {\em The IEEE International Conference on Computer Vision (ICCV)},
  volume~2, 2019.

\bibitem{zagoruyko2016WideResNet}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock In {\em Proceedings of the British Machine Vision Conference (BMVC)},
  pages 87.1--87.12, September 2016.

\bibitem{zhang2019towards}
H.~Zhang, H.~Chen, C.~Xiao, S.~Gowal, R.~Stanforth, B.~Li, D.~Boning, and C.-J.
  Hsieh.
\newblock Towards stable and efficient training of verifiably robust neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{zhang2018crown}
H.~Zhang, T.-W. Weng, P.-Y. Chen, C.-J. Hsieh, and L.~Daniel.
\newblock Efficient neural network robustness certification with general
  activation functions.
\newblock In {\em Advances in Neural Information Processing Systems}, dec 2018.

\bibitem{zhang2019tradeoff}
H.~Zhang, Y.~Yu, J.~Jiao, E.~Xing, L.~El~Ghaoui, and M.~Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In {\em International Conference on Machine Learning}, pages
  7472--7482, 2019.

\bibitem{zhang2018admmBasePrune}
T.~Zhang, S.~Ye, K.~Zhang, J.~Tang, W.~Wen, M.~Fardad, and Y.~Wang.
\newblock A systematic dnn weight pruning framework using alternating direction
  method of multipliers.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 184--199, 2018.

\end{thebibliography}
