\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{abbasi2011improved}
Abbasi-Yadkori, Y., P{\'a}l, D., and Szepesv{\'a}ri, C.
\newblock Improved algorithms for linear stochastic bandits.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2312--2320, 2011.

\bibitem[Abe et~al.(2003)Abe, Biermann, and Long]{abe2003reinforcement}
Abe, N., Biermann, A.~W., and Long, P.~M.
\newblock Reinforcement learning with immediate rewards and linear hypotheses.
\newblock \emph{Algorithmica}, 37\penalty0 (4):\penalty0 263--293, 2003.

\bibitem[Agarwal et~al.(2014)Agarwal, Hsu, Kale, Langford, Li, and
  Schapire]{agarwal14taming}
Agarwal, A., Hsu, D., Kale, S., Langford, J., Li, L., and Schapire, R.~E.
\newblock Taming the monster: A fast and simple algorithm for contextual
  bandits.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning (ICML)}, pp.\  1638--1646, 2014.

\bibitem[Agrawal \& Goyal(2013)Agrawal and Goyal]{agrawal2013thompson}
Agrawal, S. and Goyal, N.
\newblock Thompson sampling for contextual bandits with linear payoffs.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  127--135, 2013.

\bibitem[Allen-Zhu \& Li(2019)Allen-Zhu and Li]{allen2019can}
Allen-Zhu, Z. and Li, Y.
\newblock What can {ResNet} learn efficiently, going beyond kernels?
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2018convergence}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  242--252, 2019.

\bibitem[Allesiardo et~al.(2014)Allesiardo, F{\'e}raud, and
  Bouneffouf]{allesiardo2014neural}
Allesiardo, R., F{\'e}raud, R., and Bouneffouf, D.
\newblock A neural networks committee for the contextual bandit problem.
\newblock In \emph{International Conference on Neural Information Processing},
  pp.\  374--381. Springer, 2014.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019exact}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Auer(2002)]{auer2002using}
Auer, P.
\newblock Using confidence bounds for exploitation-exploration trade-offs.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 397--422, 2002.

\bibitem[Auer et~al.(2002)Auer, {Cesa-Bianchi}, Freund, and
  Schapire]{auer02nonstochastic}
Auer, P., {Cesa-Bianchi}, N., Freund, Y., and Schapire, R.~E.
\newblock The nonstochastic multiarmed bandit problem.
\newblock \emph{SIAM Journal on Computing}, 32\penalty0 (1):\penalty0 48--77,
  2002.

\bibitem[Azizzadenesheli et~al.(2018)Azizzadenesheli, Brunskill, and
  Anandkumar]{azizzadenesheli2018efficient}
Azizzadenesheli, K., Brunskill, E., and Anandkumar, A.
\newblock Efficient exploration through {Bayesian} deep {Q}-networks.
\newblock In \emph{2018 Information Theory and Applications Workshop (ITA)},
  pp.\  1--9. IEEE, 2018.

\bibitem[Beygelzimer \& Langford(2009)Beygelzimer and
  Langford]{beygelzimer09offset}
Beygelzimer, A. and Langford, J.
\newblock The offset tree for learning with partial labels.
\newblock In \emph{Proceedings of the 15th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pp.\  129--138, 2009.

\bibitem[Beygelzimer et~al.(2011)Beygelzimer, Langford, Li, Reyzin, and
  Schapire]{beygelzimer11contextual}
Beygelzimer, A., Langford, J., Li, L., Reyzin, L., and Schapire, R.~E.
\newblock Contextual bandit algorithms with supervised learning guarantees.
\newblock In \emph{Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  19--26, 2011.

\bibitem[Bubeck \& Cesa-Bianchi(2012)Bubeck and Cesa-Bianchi]{bubeck12regret}
Bubeck, S. and Cesa-Bianchi, N.
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit
  problems.
\newblock \emph{Foundations and Trends in Machine Learning}, 5\penalty0
  (1):\penalty0 1--122, 2012.

\bibitem[Bubeck et~al.(2011)Bubeck, Munos, Stoltz, and
  Szepesv{\'a}ri]{bubeck2011x}
Bubeck, S., Munos, R., Stoltz, G., and Szepesv{\'a}ri, C.
\newblock X-armed bandits.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (May):\penalty0 1655--1695, 2011.

\bibitem[Cao \& Gu(2019)Cao and Gu]{cao2019generalization2}
Cao, Y. and Gu, Q.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Cao \& Gu(2020)Cao and Gu]{cao2019generalization1}
Cao, Y. and Gu, Q.
\newblock Generalization error bounds of gradient descent for learning
  over-parameterized deep relu networks.
\newblock In \emph{the Thirty-Fourth AAAI Conference on Artificial
  Intelligence}, 2020.

\bibitem[Chapelle \& Li(2011)Chapelle and Li]{chapelle2011empirical}
Chapelle, O. and Li, L.
\newblock An empirical evaluation of thompson sampling.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2249--2257, 2011.

\bibitem[Chen et~al.(2019)Chen, Cao, Zou, and Gu]{chen2019much}
Chen, Z., Cao, Y., Zou, D., and Gu, Q.
\newblock How much over-parameterization is sufficient to learn deep relu
  networks?
\newblock \emph{arXiv preprint arXiv:1911.12360}, 2019.

\bibitem[Chu et~al.(2011)Chu, Li, Reyzin, and Schapire]{chu2011contextual}
Chu, W., Li, L., Reyzin, L., and Schapire, R.
\newblock Contextual bandits with linear payoff functions.
\newblock In \emph{Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  208--214, 2011.

\bibitem[Dani et~al.(2008)Dani, Hayes, and Kakade]{dani2008stochastic}
Dani, V., Hayes, T.~P., and Kakade, S.~M.
\newblock Stochastic linear optimization under bandit feedback.
\newblock 2008.

\bibitem[Daniely(2017)]{daniely2017sgd}
Daniely, A.
\newblock {SGD} learns the conjugate kernel class of the network.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2422--2430, 2017.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and
  Zhai]{du2018gradientdeep}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1675--1685, 2019{\natexlab{a}}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos, and
  Singh]{du2018gradient}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=S1eK3i09YQ}.

\bibitem[Dua \& Graff(2017)Dua and Graff]{Dua:2019}
Dua, D. and Graff, C.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Efron(1982)]{efron1982jackknife}
Efron, B.
\newblock \emph{The jackknife, the bootstrap, and other resampling plans},
  volume~38.
\newblock Siam, 1982.

\bibitem[F{\'e}raud et~al.(2016)F{\'e}raud, Allesiardo, Urvoy, and
  Cl{\'e}rot]{feraud2016random}
F{\'e}raud, R., Allesiardo, R., Urvoy, T., and Cl{\'e}rot, F.
\newblock Random forest for the contextual bandit problem.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  93--101,
  2016.

\bibitem[Filippi et~al.(2010)Filippi, Cappe, Garivier, and
  Szepesv{\'a}ri]{filippi2010parametric}
Filippi, S., Cappe, O., Garivier, A., and Szepesv{\'a}ri, C.
\newblock Parametric bandits: The generalized linear case.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  586--594, 2010.

\bibitem[Foster \& Rakhlin(2020)Foster and Rakhlin]{foster2020beyond}
Foster, D.~J. and Rakhlin, A.
\newblock Beyond ucb: Optimal and efficient contextual bandits with regression
  oracles.
\newblock \emph{arXiv preprint arXiv:2002.04926}, 2020.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{goodfellow2016deep}
Goodfellow, I., Bengio, Y., and Courville, A.
\newblock \emph{Deep Learning}.
\newblock MIT Press, 2016.
\newblock \url{http://www.deeplearningbook.org}.

\bibitem[Hanin(2017)]{hanin2017universal}
Hanin, B.
\newblock Universal function approximation by deep neural nets with bounded
  width and {ReLU} activations.
\newblock \emph{arXiv preprint arXiv:1708.02691}, 2017.

\bibitem[Hanin \& Sellke(2017)Hanin and Sellke]{hanin2017approximating}
Hanin, B. and Sellke, M.
\newblock Approximating continuous functions by {ReLU} nets of minimal width.
\newblock \emph{arXiv preprint arXiv:1710.11278}, 2017.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  8571--8580, 2018.

\bibitem[Jun et~al.(2017)Jun, Bhargava, Nowak, and Willett]{jun17scalable}
Jun, K.-S., Bhargava, A., Nowak, R.~D., and Willett, R.
\newblock Scalable generalized linear bandits: Online computation and hashing.
\newblock In \emph{Advances in Neural Information Processing Systems 30
  (NIPS)}, pp.\  99--109, 2017.

\bibitem[Kakade et~al.(2008)Kakade, Shalev-Shwartz, and
  Tewari]{kakade2008efficient}
Kakade, S.~M., Shalev-Shwartz, S., and Tewari, A.
\newblock Efficient bandit algorithms for online multiclass prediction.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pp.\  440--447, 2008.

\bibitem[Kleinberg et~al.(2008)Kleinberg, Slivkins, and
  Upfal]{kleinberg2008multi}
Kleinberg, R., Slivkins, A., and Upfal, E.
\newblock Multi-armed bandits in metric spaces.
\newblock In \emph{Proceedings of the fortieth annual ACM symposium on Theory
  of computing}, pp.\  681--690. ACM, 2008.

\bibitem[Krause \& Ong(2011)Krause and Ong]{krause2011contextual}
Krause, A. and Ong, C.~S.
\newblock Contextual {Gaussian} process bandit optimization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2447--2455, 2011.

\bibitem[Kveton et~al.(2020)Kveton, Zaheer, Szepesvári, Li, Ghavamzadeh, and
  Boutilier]{kveton20randomized}
Kveton, B., Zaheer, M., Szepesvári, C., Li, L., Ghavamzadeh, M., and
  Boutilier, C.
\newblock Randomized exploration in generalized linear bandits.
\newblock In \emph{Proceedings of the 22nd International Conference on
  Artificial Intelligence and Statistics}, 2020.

\bibitem[Langford \& Zhang(2008)Langford and Zhang]{langford08epoch}
Langford, J. and Zhang, T.
\newblock The epoch-greedy algorithm for contextual multi-armed bandits.
\newblock In \emph{Advances in Neural Information Processing Systems 20
  (NIPS)}, pp.\  1096--1103, 2008.

\bibitem[Lattimore \& Szepesv\'{a}ri(2019)Lattimore and
  Szepesv\'{a}ri]{lattimore19bandit}
Lattimore, T. and Szepesv\'{a}ri, C.
\newblock \emph{Bandit Algorithms}.
\newblock Cambridge University Press, 2019.
\newblock In press.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li et~al.(2010)Li, Chu, Langford, and Schapire]{li2010contextual}
Li, L., Chu, W., Langford, J., and Schapire, R.~E.
\newblock A contextual-bandit approach to personalized news article
  recommendation.
\newblock In \emph{Proceedings of the 19th international conference on World
  wide web}, pp.\  661--670. ACM, 2010.

\bibitem[Li et~al.(2017)Li, Lu, and Zhou]{li2017provably}
Li, L., Lu, Y., and Zhou, D.
\newblock Provably optimal algorithms for generalized linear contextual
  bandits.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2071--2080. JMLR. org, 2017.

\bibitem[Li \& Liang(2018)Li and Liang]{li2018learning}
Li, Y. and Liang, Y.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8157--8166, 2018.

\bibitem[Liang \& Srikant(2016)Liang and Srikant]{liang2016deep}
Liang, S. and Srikant, R.
\newblock Why deep neural networks for function approximation?
\newblock \emph{arXiv preprint arXiv:1610.04161}, 2016.

\bibitem[Lipton et~al.(2018)Lipton, Li, Gao, Li, Ahmed, and
  Deng]{lipton2018bbq}
Lipton, Z., Li, X., Gao, J., Li, L., Ahmed, F., and Deng, L.
\newblock {BBQ}-networks: Efficient exploration in deep reinforcement learning
  for task-oriented dialogue systems.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Lu et~al.(2017)Lu, Pu, Wang, Hu, and Wang]{lu2017expressive}
Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L.
\newblock The expressive power of neural networks: A view from the width.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  6231--6239, 2017.

\bibitem[Riquelme et~al.(2018)Riquelme, Tucker, and Snoek]{riquelme2018deep}
Riquelme, C., Tucker, G., and Snoek, J.
\newblock Deep {Bayesian} bandits showdown.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Rusmevichientong \& Tsitsiklis(2010)Rusmevichientong and
  Tsitsiklis]{rusmevichientong2010linearly}
Rusmevichientong, P. and Tsitsiklis, J.~N.
\newblock Linearly parameterized bandits.
\newblock \emph{Mathematics of Operations Research}, 35\penalty0 (2):\penalty0
  395--411, 2010.

\bibitem[Russo et~al.(2018)Russo, Roy, Kazerouni, Osband, and
  Wen]{russo18tutorial}
Russo, D., Roy, B.~V., Kazerouni, A., Osband, I., and Wen, Z.
\newblock A tutorial on {Thompson} sampling.
\newblock \emph{Foundations and Trends in Machine Learning}, 11\penalty0
  (1):\penalty0 1--96, 2018.

\bibitem[Srinivas et~al.(2010)Srinivas, Krause, Kakade, and
  Seeger]{srinivas2009gaussian}
Srinivas, N., Krause, A., Kakade, S., and Seeger, M.
\newblock Gaussian process optimization in the bandit setting: no regret and
  experimental design.
\newblock In \emph{Proceedings of the 27th International Conference on
  International Conference on Machine Learning}, pp.\  1015--1022. Omnipress,
  2010.

\bibitem[Telgarsky(2015)]{telgarsky2015representation}
Telgarsky, M.
\newblock Representation benefits of deep feedforward networks.
\newblock \emph{arXiv preprint arXiv:1509.08101}, 2015.

\bibitem[Telgarsky(2016)]{telgarsky2016benefits}
Telgarsky, M.
\newblock Benefits of depth in neural networks.
\newblock \emph{arXiv preprint arXiv:1602.04485}, 2016.

\bibitem[Valko et~al.(2013)Valko, Korda, Munos, Flaounas, and
  Cristianini]{valko2013finite}
Valko, M., Korda, N., Munos, R., Flaounas, I., and Cristianini, N.
\newblock Finite-time analysis of kernelised contextual bandits.
\newblock \emph{arXiv preprint arXiv:1309.6869}, 2013.

\bibitem[Yang \& Wang(2019)Yang and Wang]{yang2019reinforcement}
Yang, L.~F. and Wang, M.
\newblock Reinforcement leaning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock \emph{arXiv preprint arXiv:1905.10389}, 2019.

\bibitem[Yarotsky(2017)]{yarotsky2017error}
Yarotsky, D.
\newblock Error bounds for approximations with deep {ReLU} networks.
\newblock \emph{Neural Networks}, 94:\penalty0 103--114, 2017.

\bibitem[Yarotsky(2018)]{yarotsky2018optimal}
Yarotsky, D.
\newblock Optimal approximation of continuous functions by very deep {ReLU}
  networks.
\newblock \emph{arXiv preprint arXiv:1802.03620}, 2018.

\bibitem[Zahavy \& Mannor(2019)Zahavy and Mannor]{zahavy2019deep}
Zahavy, T. and Mannor, S.
\newblock Deep neural linear bandits: Overcoming catastrophic forgetting
  through likelihood matching.
\newblock \emph{arXiv preprint arXiv:1901.08612}, 2019.

\bibitem[Zhang et~al.(2016)Zhang, Yang, Jin, Xiao, and Zhou]{zhang2016online}
Zhang, L., Yang, T., Jin, R., Xiao, Y., and Zhou, Z.-H.
\newblock Online stochastic linear optimization under one-bit feedback.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  392--401, 2016.

\bibitem[Zou \& Gu(2019)Zou and Gu]{zou2019improved}
Zou, D. and Gu, Q.
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Zou et~al.(2019)Zou, Cao, Zhou, and Gu]{zou2018stochastic}
Zou, D., Cao, Y., Zhou, D., and Gu, Q.
\newblock Stochastic gradient descent optimizes over-parameterized deep {ReLU}
  networks.
\newblock \emph{Machine Learning}, 2019.

\end{thebibliography}
