@article{xie2022role,
  title={The role of coverage in online reinforcement learning},
  author={Xie, Tengyang and Foster, Dylan J and Bai, Yu and Jiang, Nan and Kakade, Sham M},
  journal={arXiv preprint arXiv:2210.04157},
  year={2022}
}



@misc{kausik2024leveragingofflinedatalinear,
      title={Leveraging Offline Data in Linear Latent Bandits}, 
      author={Chinmaya Kausik and Kevin Tan and Ambuj Tewari},
      year={2024},
      eprint={2405.17324},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.17324}, 
}


@article{kearns2002near,
  title={Near-optimal reinforcement learning in polynomial time},
  author={Kearns, Michael and Singh, Satinder},
  journal={Machine learning},
  volume={49},
  pages={209--232},
  year={2002},
  publisher={Springer}
}

@article{qiao2022near,
  title={Near-optimal deployment efficiency in reward-free reinforcement learning with linear function approximation},
  author={Qiao, Dan and Wang, Yu-Xiang},
  journal={arXiv preprint arXiv:2210.00701},
  year={2022}
}

@incollection{lange2012batch,
  title={Batch reinforcement learning},
  author={Lange, Sascha and Gabel, Thomas and Riedmiller, Martin},
  booktitle={Reinforcement learning: State-of-the-art},
  pages={45--73},
  year={2012},
  publisher={Springer}
}


@article{li2023reward,
  title={Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning},
  author={Li, Gen and Zhan, Wenhao and Lee, Jason D and Chi, Yuejie and Chen, Yuxin},
  journal={arXiv preprint arXiv:2305.10282},
  year={2023}
}

@article{du2019good,
  title={Is a good representation sufficient for sample efficient reinforcement learning?},
  author={Du, Simon S and Kakade, Sham M and Wang, Ruosong and Yang, Lin F},
  journal={arXiv preprint arXiv:1910.03016},
  year={2019}
}

@article{vecerik2017leveraging,
  title={Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards},
  author={Vecerik, Mel and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Roth{\"o}rl, Thomas and Lampe, Thomas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1707.08817},
  year={2017}
}

@article{nair2020awac,
  title={Awac: Accelerating online reinforcement learning with offline datasets},
  author={Nair, Ashvin and Gupta, Abhishek and Dalal, Murtaza and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.09359},
  year={2020}
}

@inproceedings{ball2023efficient,
  title={Efficient online reinforcement learning with offline data},
  author={Ball, Philip J and Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1577--1594},
  year={2023},
  organization={PMLR}
}

@article{li2024settling,
  title={Settling the sample complexity of model-based offline reinforcement learning},
  author={Li, Gen and Shi, Laixi and Chen, Yuxin and Chi, Yuejie and Wei, Yuting},
  journal={The Annals of Statistics},
  volume={52},
  number={1},
  pages={233--260},
  year={2024},
  publisher={Institute of Mathematical Statistics}
}

@misc{lattimore2020learning,
      title={Learning with Good Feature Representations in Bandits and in RL with a Generative Model}, 
      author={Tor Lattimore and Csaba Szepesvari and Gellert Weisz},
      year={2020},
      eprint={1911.07676},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{zhu2023acrab,
      title={Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning}, 
      author={Hanlin Zhu and Paria Rashidinejad and Jiantao Jiao},
      year={2023},
      eprint={2301.12714},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{dann2018unifying,
      title={Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning}, 
      author={Christoph Dann and Tor Lattimore and Emma Brunskill},
      year={2018},
      eprint={1703.07710},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{song2023hybrid,
      title={Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient}, 
      author={Yuda Song and Yifei Zhou and Ayush Sekhari and J. Andrew Bagnell and Akshay Krishnamurthy and Wen Sun},
      year={2023},
      eprint={2210.06718},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{nakamoto2023calql,
      title={Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning}, 
      author={Mitsuhiko Nakamoto and Yuexiang Zhai and Anikait Singh and Max Sobol Mark and Yi Ma and Chelsea Finn and Aviral Kumar and Sergey Levine},
      year={2023},
      eprint={2303.05479},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kumar2020conservative,
      title={Conservative Q-Learning for Offline Reinforcement Learning}, 
      author={Aviral Kumar and Aurick Zhou and George Tucker and Sergey Levine},
      year={2020},
      eprint={2006.04779},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{xie2022policy,
      title={Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning}, 
      author={Tengyang Xie and Nan Jiang and Huan Wang and Caiming Xiong and Yu Bai},
      year={2022},
      eprint={2106.04895},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{jin2021pessimism,
  title={Is pessimism provably efficient for offline rl?},
  author={Jin, Ying and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={5084--5096},
  year={2021},
  organization={PMLR}
}

@misc{wagenmaker2023leveraging,
      title={Leveraging Offline Data in Online Reinforcement Learning}, 
      author={Andrew Wagenmaker and Aldo Pacchiano},
      year={2023},
      eprint={2211.04974},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{xiong2023nearly,
      title={Nearly Minimax Optimal Offline Reinforcement Learning with Linear Function Approximation: Single-Agent MDP and Markov Game}, 
      author={Wei Xiong and Han Zhong and Chengshuai Shi and Cong Shen and Liwei Wang and Tong Zhang},
      year={2023},
      eprint={2205.15512},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{NahumShani2017JustinTimeAI,
  title={Just-in-Time Adaptive Interventions (JITAIs) in Mobile Health: Key Components and Design Principles for Ongoing Health Behavior Support},
  author={Inbal Nahum-Shani and Shawna N. Smith and Bonnie J Spring and Linda M. Collins and Katie A Witkiewitz and Ambuj Tewari and Susan A. Murphy},
  journal={Annals of Behavioral Medicine: A Publication of the Society of Behavioral Medicine},
  year={2017},
  volume={52},
  pages={446 - 462},
  url={https://api.semanticscholar.org/CorpusID:4292886}
}

@article{jin2018q,
  title={Is Q-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@misc{jin2019provably,
      title={Provably Efficient Reinforcement Learning with Linear Function Approximation}, 
      author={Chi Jin and Zhuoran Yang and Zhaoran Wang and Michael I. Jordan},
      year={2019},
      eprint={1907.05388},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zanette2021provable,
      title={Provable Benefits of Actor-Critic Methods for Offline Reinforcement Learning}, 
      author={Andrea Zanette and Martin J. Wainwright and Emma Brunskill},
      year={2021},
      eprint={2108.08812},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhou2021nearly,
      title={Nearly Minimax Optimal Reinforcement Learning for Linear Mixture Markov Decision Processes}, 
      author={Dongruo Zhou and Quanquan Gu and Csaba Szepesvari},
      year={2021},
      eprint={2012.08507},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{li2023minimaxoptimal,
      title={Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning}, 
      author={Gen Li and Yuling Yan and Yuxin Chen and Jianqing Fan},
      year={2023},
      eprint={2304.07278},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhou2023offline,
      title={Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees}, 
      author={Yifei Zhou and Ayush Sekhari and Yuda Song and Wen Sun},
      year={2023},
      eprint={2311.08384},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yang2019sampleoptimal,
      title={Sample-Optimal Parametric Q-Learning Using Linearly Additive Features}, 
      author={Lin F. Yang and Mengdi Wang},
      year={2019},
      eprint={1902.04779},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{munos2008finite,
  author  = {R{{\'e}}mi Munos and Csaba Szepesv{{\'a}}ri},
  title   = {Finite-Time Bounds for Fitted Value Iteration},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {27},
  pages   = {815--857},
  url     = {http://jmlr.org/papers/v9/munos08a.html}
}

@misc{duan2020minimaxoptimal,
      title={Minimax-Optimal Off-Policy Evaluation with Linear Function Approximation}, 
      author={Yaqi Duan and Mengdi Wang},
      year={2020},
      eprint={2002.09516},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wagenmaker2022firstorder,
      title={First-Order Regret in Reinforcement Learning with Linear Function Approximation: A Robust Estimation Approach}, 
      author={Andrew Wagenmaker and Yifang Chen and Max Simchowitz and Simon S. Du and Kevin Jamieson},
      year={2022},
      eprint={2112.03432},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yin2022nearoptimal,
      title={Near-optimal Offline Reinforcement Learning with Linear Representation: Leveraging Variance Information with Pessimism}, 
      author={Ming Yin and Yaqi Duan and Mengdi Wang and Yu-Xiang Wang},
      year={2022},
      eprint={2203.05804},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{fan2020theoretical,
      title={A Theoretical Analysis of Deep Q-Learning}, 
      author={Jianqing Fan and Zhaoran Wang and Yuchen Xie and Zhuoran Yang},
      year={2020},
      eprint={1901.00137},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{wang2020reinforcement,
      title={Reinforcement Learning with General Value Function Approximation: Provably Efficient Approach via Bounded Eluder Dimension}, 
      author={Ruosong Wang and Ruslan Salakhutdinov and Lin F. Yang},
      year={2020},
      eprint={2005.10804},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{jin2021bellman,
      title={Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms}, 
      author={Chi Jin and Qinghua Liu and Sobhan Miryoosefi},
      year={2021},
      eprint={2102.00815},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{xie2023bellmanconsistent,
      title={Bellman-consistent Pessimism for Offline Reinforcement Learning}, 
      author={Tengyang Xie and Ching-An Cheng and Nan Jiang and Paul Mineiro and Alekh Agarwal},
      year={2023},
      eprint={2106.06926},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{zhan2022offline,
      title={Offline Reinforcement Learning with Realizability and Single-policy Concentrability}, 
      author={Wenhao Zhan and Baihe Huang and Audrey Huang and Nan Jiang and Jason D. Lee},
      year={2022},
      eprint={2202.04634},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{shi2022pessq,
  title = 	 {Pessimistic Q-Learning for Offline Reinforcement Learning: Towards Optimal Sample Complexity},
  author =       {Shi, Laixi and Li, Gen and Wei, Yuting and Chen, Yuxin and Chi, Yuejie},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {19967--20025},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/shi22c/shi22c.pdf},
  url = 	 {https://proceedings.mlr.press/v162/shi22c.html},
  abstract = 	 {Offline or batch reinforcement learning seeks to learn a near-optimal policy using history data without active exploration of the environment. To counter the insufficient coverage and sample scarcity of many offline datasets, the principle of pessimism has been recently introduced to mitigate high bias of the estimated values. While pessimistic variants of model-based algorithms (e.g., value iteration with lower confidence bounds) have been theoretically investigated, their model-free counterparts — which do not require explicit model estimation — have not been adequately studied, especially in terms of sample efficiency. To address this inadequacy, we study a pessimistic variant of Q-learning in the context of finite-horizon Markov decision processes, and characterize its sample complexity under the single-policy concentrability assumption which does not require the full coverage of the state-action space. In addition, a variance-reduced pessimistic Q-learning algorithm is proposed to achieve near-optimal sample complexity. Altogether, this work highlights the efficiency of model-free algorithms in offline RL when used in conjunction with pessimism and variance reduction.}
}

@misc{wagenmaker2023instancedependent,
      title={Instance-Dependent Near-Optimal Policy Identification in Linear MDPs via Online Experiment Design}, 
      author={Andrew Wagenmaker and Kevin Jamieson},
      year={2023},
      eprint={2207.02575},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{tan2024natural,
      title={A Natural Extension To Online Algorithms For Hybrid RL With Limited Coverage}, 
      author={Kevin Tan and Ziping Xu},
      year={2024},
      eprint={2403.09701},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ren2024hybrid,
      title={Hybrid Inverse Reinforcement Learning}, 
      author={Juntao Ren and Gokul Swamy and Zhiwei Steven Wu and J. Andrew Bagnell and Sanjiban Choudhury},
      year={2024},
      eprint={2402.08848},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{he2023nearly,
      title={Nearly Minimax Optimal Reinforcement Learning for Linear Markov Decision Processes}, 
      author={Jiafan He and Heyang Zhao and Dongruo Zhou and Quanquan Gu},
      year={2023},
      eprint={2212.06132},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{athey2022contextual,
      title={Contextual Bandits in a Survey Experiment on Charitable Giving: Within-Experiment Outcomes versus Policy Learning}, 
      author={Susan Athey and Undral Byambadalai and Vitor Hadad and Sanath Kumar Krishnamurthy and Weiwen Leung and Joseph Jay Williams},
      year={2022},
      eprint={2211.12004},
      archivePrefix={arXiv},
      primaryClass={econ.EM}
}

@misc{levine2020offline,
      title={Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems}, 
      author={Sergey Levine and Aviral Kumar and George Tucker and Justin Fu},
      year={2020},
      eprint={2005.01643},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@book{sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}


@misc{zhou2022computationally,
      title={Computationally Efficient Horizon-Free Reinforcement Learning for Linear Mixture MDPs}, 
      author={Dongruo Zhou and Quanquan Gu},
      year={2022},
      eprint={2205.11507},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hu2023nearlynot,
      title={Nearly Minimax Optimal Reinforcement Learning with Linear Function Approximation}, 
      author={Pihe Hu and Yu Chen and Longbo Huang},
      year={2023},
      eprint={2206.11489},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{agarwal2022voql,
      title={VO$Q$L: Towards Optimal Regret in Model-free RL with Nonlinear Function Approximation}, 
      author={Alekh Agarwal and Yujia Jin and Tong Zhang},
      year={2022},
      eprint={2212.06069},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{zhang2023settling,
  title={Settling the sample complexity of online reinforcement learning},
  author={Zhang, Zihan and Chen, Yuxin and Lee, Jason D and Du, Simon S},
  journal={arXiv preprint arXiv:2307.13586},
  year={2023}
}

@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International conference on machine learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}

@misc{amortila2024harnessing,
      title={Harnessing Density Ratios for Online Reinforcement Learning}, 
      author={Philip Amortila and Dylan J. Foster and Nan Jiang and Ayush Sekhari and Tengyang Xie},
      year={2024},
      eprint={2401.09681},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{abbas2011improved,
 author = {Abbasi-yadkori, Yasin and P\'{a}l, D\'{a}vid and Szepesv\'{a}ri, Csaba},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Improved Algorithms for Linear Stochastic Bandits},
 url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf},
 volume = {24},
 year = {2011}
}

@article{zhang2024horizon,
  title={Horizon-Free Regret for Linear Markov Decision Processes},
  author={Zhang, Zihan and Lee, Jason D and Chen, Yuxin and Du, Simon S},
  journal={arXiv preprint arXiv:2403.10738},
  year={2024}
}

@article{li2021sample,
  title={Sample-efficient reinforcement learning is feasible for linearly realizable MDPs with limited revisiting},
  author={Li, Gen and Chen, Yuxin and Chi, Yuejie and Gu, Yuantao and Wei, Yuting},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16671--16685},
  year={2021}
}

@article{blanchet2024double,
  title={Double pessimism is provably efficient for distributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage},
  author={Blanchet, Jose and Lu, Miao and Zhang, Tong and Zhong, Han},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{he2021logarithmic,
  title={Logarithmic regret for reinforcement learning with linear function approximation},
  author={He, Jiafan and Zhou, Dongruo and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={4171--4180},
  year={2021},
  organization={PMLR}
}

@article{kim2022improved,
  title={Improved regret analysis for variance-adaptive linear bandits and horizon-free linear mixture mdps},
  author={Kim, Yeoneung and Yang, Insoon and Jun, Kwang-Sung},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1060--1072},
  year={2022}
}

@article{min2021variance,
  title={Variance-aware off-policy evaluation with linear function approximation},
  author={Min, Yifei and Wang, Tianhao and Zhou, Dongruo and Gu, Quanquan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={7598--7610},
  year={2021}
}

@inproceedings{zhong2022pessimistic,
  title={Pessimistic minimax value iteration: Provably efficient equilibrium learning from offline datasets},
  author={Zhong, Han and Xiong, Wei and Tan, Jiyuan and Wang, Liwei and Zhang, Tong and Wang, Zhaoran and Yang, Zhuoran},
  booktitle={International Conference on Machine Learning},
  pages={27117--27142},
  year={2022},
  organization={PMLR}
}

@inproceedings{zanette2020learning,
  title={Learning near optimal policies with low inherent bellman error},
  author={Zanette, Andrea and Lazaric, Alessandro and Kochenderfer, Mykel and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={10978--10989},
  year={2020},
  organization={PMLR}
}

@article{zhong2024theoretical,
  title={A theoretical analysis of optimistic proximal policy optimization in linear markov decision processes},
  author={Zhong, Han and Zhang, Tong},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhang2021improved,
  title={Improved variance-aware confidence sets for linear bandits and linear mixture mdp},
  author={Zhang, Zihan and Yang, Jiaqi and Ji, Xiangyang and Du, Simon S},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4342--4355},
  year={2021}
}

@inproceedings{ayoub2020model,
  title={Model-based reinforcement learning with value-targeted regression},
  author={Ayoub, Alex and Jia, Zeyu and Szepesvari, Csaba and Wang, Mengdi and Yang, Lin},
  booktitle={International Conference on Machine Learning},
  pages={463--474},
  year={2020},
  organization={PMLR}
}

@article{ren2021nearly,
  title={Nearly horizon-free offline reinforcement learning},
  author={Ren, Tongzheng and Li, Jialian and Dai, Bo and Du, Simon S and Sanghavi, Sujay},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15621--15634},
  year={2021}
}

@article{uehara2021representation,
  title={Representation learning for online and offline rl in low-rank mdps},
  author={Uehara, Masatoshi and Zhang, Xuezhou and Sun, Wen},
  journal={arXiv preprint arXiv:2110.04652},
  year={2021}
}

@inproceedings{nair2018overcoming,
  title={Overcoming exploration in reinforcement learning with demonstrations},
  author={Nair, Ashvin and McGrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
  pages={6292--6299},
  year={2018},
  organization={IEEE}
}

@inproceedings{hester2018deep,
  title={Deep q-learning from demonstrations},
  author={Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Osband, Ian and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  year={2018}
}

@article{rajeswaran2017learning,
  title={Learning complex dexterous manipulation with deep reinforcement learning and demonstrations},
  author={Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey},
  journal={arXiv preprint arXiv:1709.10087},
  year={2017}
}

@article{ross2012agnostic,
  title={Agnostic system identification for model-based reinforcement learning},
  author={Ross, Stephane and Bagnell, J Andrew},
  journal={arXiv preprint arXiv:1203.1007},
  year={2012}
}

@article{liu2023one,
  title={One objective to rule them all: A maximization objective fusing estimation and planning for exploration},
  author={Liu, Zhihan and Lu, Miao and Xiong, Wei and Zhong, Han and Hu, Hao and Zhang, Shenao and Zheng, Sirui and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2305.18258},
  year={2023}
}
