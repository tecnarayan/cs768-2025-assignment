\begin{thebibliography}{10}

\bibitem{QG_anitescu2000}
M.~Anitescu.
\newblock Degenerate nonlinear programming with a quadratic growth condition.
\newblock {\em SIAM Journal on Optimization}, 10(4):1116--1135, 2000.

\bibitem{balduzzi2018mechanics}
D.~Balduzzi, S.~Racaniere, J.~Martens, J.~Foerster, K.~Tuyls, and T.~Graepel.
\newblock The mechanics of n-player differentiable games.
\newblock {\em arXiv preprint arXiv:1802.05642}, 2018.

\bibitem{beck2009fast}
A.~Beck and M.~Teboulle.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock {\em SIAM journal on imaging sciences}, 2(1):183--202, 2009.

\bibitem{danskin_result_1995}
P.~Bernhard and A.~Rapaport.
\newblock On a theorem of danskin with an application to a theorem of von
  neumann-sion.
\newblock {\em Nonlinear analysis}, 24(8):1163--1182, 1995.

\bibitem{bertsekas1999nonlinear}
D.~P. Bertsekas.
\newblock {\em Nonlinear programming}.
\newblock Athena scientific Belmont, 1999.

\bibitem{cai2019global}
Q.~Cai, M.~Hong, Y.~Chen, and Z.~Wang.
\newblock On the global convergence of imitation learning: A case for linear
  quadratic regulator.
\newblock {\em arXiv preprint arXiv:1901.03674}, 2019.

\bibitem{carmon2017lower}
Y.~Carmon, J.~C. Duchi, O.~Hinder, and A.~Sidford.
\newblock Lower bounds for finding stationary points i.
\newblock {\em arXiv preprint arXiv:1710.11606}, 2017.

\bibitem{chambolle2016ergodic}
A.~Chambolle and T.~Pock.
\newblock On the ergodic convergence rates of a first-order primal--dual
  algorithm.
\newblock {\em Mathematical Programming}, 159(1-2):253--287, 2016.

\bibitem{conn2000trust}
A.~R. Conn, N.~I. Gould, and P.~L. Toint.
\newblock {\em Trust region methods}, volume~1.
\newblock Siam, 2000.

\bibitem{dai2018kernel}
B.~Dai, H.~Dai, A.~Gretton, L.~Song, D.~Schuurmans, and N.~He.
\newblock Kernel exponential family estimation via doubly dual embedding.
\newblock {\em arXiv preprint arXiv:1811.02228}, 2018.

\bibitem{dai2018sbeed}
B.~Dai, A.~Shaw, L.~Li, L.~Xiao, N.~He, Z.~Liu, J.~Chen, and L.~Song.
\newblock Sbeed: Convergent reinforcement learning with nonlinear function
  approximation.
\newblock In {\em International Conference on Machine Learning}, pages
  1133--1142, 2018.

\bibitem{dang2015convergence}
C.~D. Dang and G.~Lan.
\newblock On the convergence properties of non-euclidean extragradient methods
  for variational inequalities with generalized monotone operators.
\newblock {\em Computational Optimization and applications}, 60(2):277--310,
  2015.

\bibitem{daskalakis2017training}
C.~Daskalakis, A.~Ilyas, V.~Syrgkanis, and H.~Zeng.
\newblock Training gans with optimism.
\newblock {\em arXiv preprint arXiv:1711.00141}, 2017.

\bibitem{daskalakis2018last}
C.~Daskalakis and I.~Panageas.
\newblock Last-iterate convergence: Zero-sum games and constrained min-max
  optimization.
\newblock {\em arXiv preprint arXiv:1807.04252}, 2018.

\bibitem{daskalakis2018limit}
C.~Daskalakis and I.~Panageas.
\newblock The limit points of (optimistic) gradient descent in min-max
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9236--9246, 2018.

\bibitem{du2018gradientb}
S.~S. Du, J.~D. Lee, H.~Li, L.~Wang, and X.~Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock {\em arXiv preprint arXiv:1811.03804}, 2018.

\bibitem{edwards2015censoring}
H.~Edwards and A.~Storkey.
\newblock Censoring representations with an adversary.
\newblock {\em arXiv preprint arXiv:1511.05897}, 2015.

\bibitem{facchinei2007finite}
F.~Facchinei and J.-S. Pang.
\newblock {\em Finite-dimensional variational inequalities and complementarity
  problems}.
\newblock Springer Science \& Business Media, 2007.

\bibitem{fazel2018global}
M.~Fazel, R.~Ge, S.~Kakade, and M.~Mesbahi.
\newblock Global convergence of policy gradient methods for the linear
  quadratic regulator.
\newblock In {\em International Conference on Machine Learning}, pages
  1466--1475, 2018.

\bibitem{ghosh2018efficient}
S.~Ghosh, M.~Squillante, and E.~Wollega.
\newblock Efficient stochastic gradient descent for distributionally robust
  learning.
\newblock {\em arXiv preprint arXiv:1805.08728}, 2018.

\bibitem{gidel2018variational}
G.~Gidel, H.~Berard, G.~Vignoud, P.~Vincent, and S.~Lacoste-Julien.
\newblock A variational inequality perspective on generative adversarial
  networks.
\newblock {\em arXiv preprint arXiv:1802.10551}, 2018.

\bibitem{gidel2018negative}
G.~Gidel, R.~A. Hemmat, M.~Pezeshki, G.~Huang, R.~Lepriol, S.~Lacoste-Julien,
  and I.~Mitliagkas.
\newblock Negative momentum for improved game dynamics.
\newblock {\em arXiv preprint arXiv:1807.04740}, 2018.

\bibitem{gidel2016frank}
G.~Gidel, T.~Jebara, and S.~Lacoste-Julien.
\newblock Frank-wolfe algorithms for saddle point problems.
\newblock {\em arXiv preprint arXiv:1610.07797}, 2016.

\bibitem{goodfellow2016deep}
I.~Goodfellow, Y.~Bengio, A.~Courville, and Y.~Bengio.
\newblock {\em Deep learning}, volume~1.
\newblock MIT press Cambridge, 2016.

\bibitem{FGSM}
I.~J. Goodfellow, J.~Shlens, and C.~Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em arXiv preprint arXiv:1412.6572}, 2014.

\bibitem{hamedani2018iteration}
E.~Y. Hamedani, A.~Jalilzadeh, N.~Aybat, and U.~Shanbhag.
\newblock Iteration complexity of randomized primal-dual methods for
  convex-concave saddle point problems.
\newblock {\em arXiv preprint arXiv:1806.04118}, 2018.

\bibitem{ho2016generative}
J.~Ho and S.~Ermon.
\newblock Generative adversarial imitation learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4565--4573, 2016.

\bibitem{jin2019minmax}
C.~Jin, P.~Netrapalli, and M.~I. Jordan.
\newblock Minmax optimization: Stable limit points of gradient descent ascent
  are locally optimal.
\newblock {\em arXiv preprint arXiv:1902.00618}, 2019.

\bibitem{juditsky2016solving}
A.~Juditsky and A.~Nemirovski.
\newblock Solving variational inequalities with monotone operators on domains
  given by linear minimization oracles.
\newblock {\em Mathematical Programming}, 156(1-2):221--256, 2016.

\bibitem{PL_karimi_2016}
H.~Karimi, J.~Nutini, and M.~Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In {\em Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 795--811. Springer, 2016.

\bibitem{PGD}
A.~Kurakin, I.~Goodfellow, and S.~Bengio.
\newblock Adversarial machine learning at scale.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{letcher2019differentiable}
A.~Letcher, D.~Balduzzi, S.~Racaniere, J.~Martens, J.~Foerster, K.~Tuyls, and
  T.~Graepel.
\newblock Differentiable game mechanics.
\newblock {\em Journal of Machine Learning Research}, 20(84):1--40, 2019.

\bibitem{liang2018interaction}
T.~Liang and J.~Stokes.
\newblock Interaction matters: A note on non-asymptotic local convergence of
  generative adversarial networks.
\newblock {\em arXiv preprint arXiv:1802.06132}, 2018.

\bibitem{lin2018solving}
Q.~Lin, M.~Liu, H.~Rafique, and T.~Yang.
\newblock Solving weakly-convex-weakly-concave saddle-point problems as
  weakly-monotone variational inequality.
\newblock {\em arXiv preprint arXiv:1810.10207}, 2018.

\bibitem{lu2019block}
S.~Lu, I.~Tsaknakis, and M.~Hong.
\newblock Block alternating optimization for non-convex min-max problems:
  algorithms and applications in signal processing and communications.
\newblock In {\em Proceedings of IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, 2019.

\bibitem{lu2019hybrid}
S.~Lu, I.~Tsaknakis, M.~Hong, and Y.~Chen.
\newblock Hybrid block successive approximation for one-sided non-convex
  min-max problems: Algorithms and applications.
\newblock {\em arXiv preprint arXiv:1902.08294}, 2019.

\bibitem{madras2018learning}
D.~Madras, E.~Creager, T.~Pitassi, and R.~Zemel.
\newblock Learning adversarially fair and transferable representations.
\newblock {\em arXiv preprint arXiv:1802.06309}, 2018.

\bibitem{madry2018}
A.~Madry, A.~Makelov, L.~Schmidt, D.~Tsipras, and A.~Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{mai2018cycles}
T.~Mai, M.~Mihail, I.~Panageas, W.~Ratcliff, V.~Vazirani, and P.~Yunker.
\newblock Cycles in zero-sum differential games and biological diversity.
\newblock In {\em Proceedings of the 2018 ACM Conference on Economics and
  Computation}, pages 339--350. ACM, 2018.

\bibitem{mertikopoulos2018mirror}
P.~Mertikopoulos, H.~Zenati, B.~Lecouat, C.-S. Foo, V.~Chandrasekhar, and
  G.~Piliouras.
\newblock Mirror descent in saddle-point problems: Going the extra (gradient)
  mile.
\newblock {\em arXiv preprint arXiv:1807.02629}, 2018.

\bibitem{mescheder2018training}
L.~Mescheder, A.~Geiger, and S.~Nowozin.
\newblock Which training methods for gans do actually converge?
\newblock In {\em International Conference on Machine Learning}, pages
  3478--3487, 2018.

\bibitem{1902.00146}
M.~Mohri, G.~Sivek, and A.~T. Suresh.
\newblock Agnostic federated learning.
\newblock In {\em International Conference on Machine Learning}, pages
  4615--4625, 2019.

\bibitem{mokhtari2019unified}
A.~Mokhtari, A.~Ozdaglar, and S.~Pattathil.
\newblock A unified analysis of extra-gradient and optimistic gradient methods
  for saddle point problems: Proximal point approach.
\newblock {\em arXiv preprint arXiv:1901.08511}, 2019.

\bibitem{monteiro2010complexity}
R.~D. Monteiro and B.~F. Svaiter.
\newblock On the complexity of the hybrid proximal extragradient method for the
  iterates and the ergodic mean.
\newblock {\em SIAM Journal on Optimization}, 20(6):2755--2787, 2010.

\bibitem{nemirovski2004prox}
A.~Nemirovski.
\newblock Prox-method with rate of convergence $\mathcal{O}(1/t)$ for
  variational inequalities with lipschitz continuous monotone operators and
  smooth convex-concave saddle point problems.
\newblock {\em SIAM Journal on Optimization}, 15(1):229--251, 2004.

\bibitem{nesterov2007dual}
Y.~Nesterov.
\newblock Dual extrapolation and its applications to solving variational
  inequalities and related problems.
\newblock {\em Mathematical Programming}, 109(2-3):319--344, 2007.

\bibitem{nesterov2013introductory}
Y.~Nesterov.
\newblock {\em Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem{pang2016unified}
J.~S. Pang and M.~Razaviyayn.
\newblock A unified distributed algorithm for non-cooperative games., 2016.

\bibitem{pang2011nonconvex}
J.-S. Pang and G.~Scutari.
\newblock Nonconvex games with side constraints.
\newblock {\em SIAM Journal on Optimization}, 21(4):1491--1522, 2011.

\bibitem{rafique2018non}
H.~Rafique, M.~Liu, Q.~Lin, and T.~Yang.
\newblock Non-convex min-max optimization: Provable algorithms and applications
  in machine learning.
\newblock {\em arXiv preprint arXiv:1810.02060}, 2018.

\bibitem{sanjabi2018convergence}
M.~Sanjabi, J.~Ba, M.~Razaviyayn, and J.~D. Lee.
\newblock On the convergence and robustness of training gans with regularized
  optimal transport.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7091--7101, 2018.

\bibitem{sattigeri2018fairness}
P.~Sattigeri, S.~C. Hoffman, V.~Chenthamarakshan, and K.~R. Varshney.
\newblock Fairness gan.
\newblock {\em arXiv preprint arXiv:1805.09910}, 2018.

\bibitem{sinha2018certifying}
A.~Sinha, H.~Namkoong, and J.~Duchi.
\newblock Certifying some distributional robustness with principled adversarial
  training.
\newblock {\em arXiv preprint arXiv:1710.10571}, 2017.

\bibitem{sun2018geometric}
J.~Sun, Q.~Qu, and J.~Wright.
\newblock A geometric analysis of phase retrieval.
\newblock {\em Foundations of Computational Mathematics}, 18(5):1131--1198,
  2018.

\bibitem{xiao2017/online}
H.~Xiao, K.~Rasul, and R.~Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock {\em arXiv preprint arXiv:1708.07747}, 2017.

\bibitem{xu2018fairgan}
D.~Xu, S.~Yuan, L.~Zhang, and X.~Wu.
\newblock Fairgan: Fairness-aware generative adversarial networks.
\newblock In {\em 2018 IEEE International Conference on Big Data (Big Data)},
  pages 570--575. IEEE, 2018.

\bibitem{zhang_icml_2019}
H.~Zhang, Y.~Yu, J.~Jiao, E.~Xing, L.~E. Ghaoui, and M.~Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In {\em International Conference on Machine Learning}, pages
  7472--7482, 2019.

\end{thebibliography}
