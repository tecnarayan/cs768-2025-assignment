\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainslie et~al.(2023)Ainslie, Lei, de~Jong, Onta{\~n}{\'o}n, Brahma,
  Zemlyanskiy, Uthus, Guo, Lee-Thorp, Tay, et~al.]{ainslie2023colt5}
Joshua Ainslie, Tao Lei, Michiel de~Jong, Santiago Onta{\~n}{\'o}n, Siddhartha
  Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi~Tay,
  et~al.
\newblock Colt5: Faster long-range transformers with conditional computation.
\newblock \emph{arXiv preprint arXiv:2303.09752}, 2023.

\bibitem[Allen-Zhu and Li(2023)]{allen2023physics}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Physics of language models: Part 1, context-free grammar.
\newblock \emph{arXiv preprint arXiv:2305.13673}, 2023.

\bibitem[Anil et~al.(2022)Anil, Pokle, Liang, Treutlein, Wu, Bai, Kolter, and
  Grosse]{anil2022path}
Cem Anil, Ashwini Pokle, Kaiqu Liang, Johannes Treutlein, Yuhuai Wu, Shaojie
  Bai, J~Zico Kolter, and Roger~B Grosse.
\newblock Path independent equilibrium models can better exploit test-time
  computation.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 7796--7809, 2022.

\bibitem[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri,
  Taropa, Bailey, Chen, et~al.]{anil2023palm}
Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,
  Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,
  et~al.
\newblock Palm 2 technical report.
\newblock \emph{arXiv preprint arXiv:2305.10403}, 2023.

\bibitem[Anthropic(2023)]{anthropic100k}
Anthropic.
\newblock Introducing 100k context windows.
\newblock Anthropic blog, 2023.
\newblock URL \url{https://www.anthropic.com/index/100k-context-windows}.

\bibitem[Benton et~al.(2022)Benton, Gruver, Maddox, and Wilson]{benton2022deep}
Gregory Benton, Nate Gruver, Wesley Maddox, and Andrew~Gordon Wilson.
\newblock Deep probabilistic time series forecasting over long horizons.
\newblock \emph{openreview preprint}, 2022.
\newblock URL \url{https://openreview.net/forum?id=22h1XSEiN0}.

\bibitem[Biderman et~al.(2023)Biderman, Prashanth, Sutawika, Schoelkopf,
  Anthony, Purohit, and Raf]{biderman2023emergent}
Stella Biderman, USVSN~Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf,
  Quentin Anthony, Shivanshu Purohit, and Edward Raf.
\newblock Emergent and predictable memorization in large language models.
\newblock \emph{arXiv preprint arXiv:2304.11158}, 2023.

\bibitem[Box and Jenkins(1968)]{box1968some}
George~EP Box and Gwilym~M Jenkins.
\newblock Some recent advances in forecasting and control.
\newblock \emph{Journal of the Royal Statistical Society. Series C (Applied
  Statistics)}, 17\penalty0 (2):\penalty0 91--109, 1968.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz,
  Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  et~al.
\newblock Sparks of artificial general intelligence: Early experiments with
  gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.

\bibitem[Challu et~al.(2022)Challu, Olivares, Oreshkin, Garza, Mergenthaler,
  and Dubrawski]{challu2022n}
Cristian Challu, Kin~G Olivares, Boris~N Oreshkin, Federico Garza, Max
  Mergenthaler, and Artur Dubrawski.
\newblock N-hits: Neural hierarchical interpolation for time series
  forecasting.
\newblock \emph{arXiv preprint arXiv:2201.12886}, 2022.

\bibitem[Chang et~al.(2023)Chang, Cramer, Soni, and Bamman]{chang2023speak}
Kent~K Chang, Mackenzie Cramer, Sandeep Soni, and David Bamman.
\newblock Speak, memory: An archaeology of books known to chatgpt/gpt-4.
\newblock \emph{arXiv preprint arXiv:2305.00118}, 2023.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Cranmer(2023)]{cranmer2023interpretable}
Miles Cranmer.
\newblock Interpretable machine learning for science with pysr and
  symbolicregression. jl.
\newblock \emph{arXiv preprint arXiv:2305.01582}, 2023.

\bibitem[Del{\'e}tang et~al.(2023)Del{\'e}tang, Ruoss, Duquenne, Catt,
  Genewein, Mattern, Grau-Moya, Wenliang, Aitchison, Orseau,
  et~al.]{deletang2023language}
Gr{\'e}goire Del{\'e}tang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt,
  Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li~Kevin Wenliang,
  Matthew Aitchison, Laurent Orseau, et~al.
\newblock Language modeling is compression.
\newblock \emph{arXiv preprint arXiv:2309.10668}, 2023.

\bibitem[Du et~al.(2022)Du, Su, and Wei]{du2022preformer}
Dazhao Du, Bing Su, and Zhewei Wei.
\newblock Preformer: Predictive transformer with multi-scale segment-wise
  correlations for long-term time series forecasting.
\newblock \emph{arXiv preprint arXiv:2202.11356}, 2022.

\bibitem[Gardner et~al.(2018)Gardner, Pleiss, Bindel, Weinberger, and
  Wilson]{gardner2018gpytorch}
Jacob~R Gardner, Geoff Pleiss, David Bindel, Kilian~Q Weinberger, and
  Andrew~Gordon Wilson.
\newblock Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu
  acceleration.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Garza et~al.(2022)Garza, Canseco, Chall{\'u}, and
  Olivares]{garza2022statsforecast}
Federico Garza, Max~Mergenthaler Canseco, Cristian Chall{\'u}, and Kin~G
  Olivares.
\newblock Statsforecast: Lightning fast forecasting with statistical and
  econometric models.
\newblock \emph{PyCon: Salt Lake City, UT, USA}, 2022.
\newblock URL \url{https://github.com/Nixtla/statsforecast}.

\bibitem[Godahewa et~al.(2021)Godahewa, Bergmeir, Webb, Hyndman, and
  Montero-Manso]{godahewa2021monash}
Rakshitha Godahewa, Christoph Bergmeir, Geoffrey~I Webb, Rob~J Hyndman, and
  Pablo Montero-Manso.
\newblock Monash time series forecasting archive.
\newblock \emph{arXiv preprint arXiv:2105.06643}, 2021.

\bibitem[Goldblum et~al.(2023)Goldblum, Finzi, Rowan, and
  Wilson]{goldblum2023no}
Micah Goldblum, Marc Finzi, Keefer Rowan, and Andrew~Gordon Wilson.
\newblock The no free lunch theorem, kolmogorov complexity, and the role of
  inductive biases in machine learning.
\newblock \emph{arXiv preprint arXiv:2304.05366}, 2023.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock \emph{Deep learning}.
\newblock MIT press, 2016.

\bibitem[Gruver et~al.(2022)Gruver, Finzi, Goldblum, and Wilson]{gruver2022lie}
Nate Gruver, Marc Finzi, Micah Goldblum, and Andrew~Gordon Wilson.
\newblock The lie derivative for measuring learned equivariance.
\newblock \emph{arXiv preprint arXiv:2210.02984}, 2022.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
  Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Herzen et~al.(2022)Herzen, L{\"a}ssig, Piazzetta, Neuer, Tafti,
  Raille, Van~Pottelbergh, Pasieka, Skrodzki, Huguenin,
  et~al.]{herzen2022darts}
Julien Herzen, Francesco L{\"a}ssig, Samuele~Giuliano Piazzetta, Thomas Neuer,
  L{\'e}o Tafti, Guillaume Raille, Tomas Van~Pottelbergh, Marek Pasieka,
  Andrzej Skrodzki, Nicolas Huguenin, et~al.
\newblock Darts: User-friendly modern machine learning for time series.
\newblock \emph{The Journal of Machine Learning Research}, 23\penalty0
  (1):\penalty0 5442--5447, 2022.

\bibitem[Hewamalage et~al.(2023)Hewamalage, Ackermann, and
  Bergmeir]{hewamalage2023forecast}
Hansika Hewamalage, Klaus Ackermann, and Christoph Bergmeir.
\newblock Forecast evaluation for data scientists: common pitfalls and best
  practices.
\newblock \emph{Data Mining and Knowledge Discovery}, 37\penalty0 (2):\penalty0
  788--832, 2023.

\bibitem[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and
  Choi]{holtzman2019curious}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock \emph{arXiv preprint arXiv:1904.09751}, 2019.

\bibitem[Hyndman et~al.(2008)Hyndman, Koehler, Ord, and
  Snyder]{hyndman2008forecasting}
Rob~J Hyndman, Anne~B Koehler, J~Keith Ord, and Ralph~D Snyder.
\newblock \emph{Forecasting with exponential smoothing: the state space
  approach}.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Kaushik et~al.(2020)Kaushik, Choudhury, Sheron, Dasgupta, Natarajan,
  Pickett, and Dutt]{kaushik2020ai}
Shruti Kaushik, Abhinav Choudhury, Pankaj~Kumar Sheron, Nataraj Dasgupta, Sayee
  Natarajan, Larry~A Pickett, and Varun Dutt.
\newblock Ai in healthcare: time-series forecasting using statistical, neural,
  and ensemble architectures.
\newblock \emph{Frontiers in big data}, 3:\penalty0 4, 2020.

\bibitem[Lea et~al.(2016)Lea, Vidal, Reiter, and Hager]{lea2016temporal}
Colin Lea, Rene Vidal, Austin Reiter, and Gregory~D Hager.
\newblock Temporal convolutional networks: A unified approach to action
  segmentation.
\newblock In \emph{Computer Vision--ECCV 2016 Workshops: Amsterdam, The
  Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14}, pages
  47--54. Springer, 2016.

\bibitem[Lee et~al.(2021)Lee, Ippolito, Nystrom, Zhang, Eck, Callison-Burch,
  and Carlini]{lee2021deduplicating}
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck,
  Chris Callison-Burch, and Nicholas Carlini.
\newblock Deduplicating training data makes language models better.
\newblock \emph{arXiv preprint arXiv:2107.06499}, 2021.

\bibitem[Liu and Low(2023)]{liu2023goat}
Tiedong Liu and Bryan Kian~Hsiang Low.
\newblock Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks.
\newblock \emph{arXiv preprint arXiv:2305.14201}, 2023.

\bibitem[Mnih and Hinton(2008)]{mnih2008scalable}
Andriy Mnih and Geoffrey~E Hinton.
\newblock A scalable hierarchical distributed language model.
\newblock \emph{Advances in neural information processing systems}, 21, 2008.

\bibitem[Moritz and Bartz-Beielstein(2017)]{moritz2017imputets}
Steffen Moritz and Thomas Bartz-Beielstein.
\newblock imputets: time series missing value imputation in r.
\newblock \emph{R J.}, 9\penalty0 (1):\penalty0 207, 2017.

\bibitem[Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber,
  Dohan, Lewkowycz, Bosma, Luan, et~al.]{nye2021show}
Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob
  Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David
  Luan, et~al.
\newblock Show your work: Scratchpads for intermediate computation with
  language models.
\newblock \emph{arXiv preprint arXiv:2112.00114}, 2021.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan,
  Mann, Askell, Bai, Chen, Conerly, Drain, Ganguli, Hatfield-Dodds, Hernandez,
  Johnston, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan,
  McCandlish, and Olah]{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
  Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly,
  Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott
  Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario
  Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock In-context learning and induction heads.
\newblock \emph{Transformer Circuits Thread}, 2022.
\newblock
  https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.

\bibitem[Oord et~al.(2016)Oord, Dieleman, Zen, Simonyan, Vinyals, Graves,
  Kalchbrenner, Senior, and Kavukcuoglu]{oord2016wavenet}
Aaron van~den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals,
  Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu.
\newblock Wavenet: A generative model for raw audio.
\newblock In \emph{9th ISCA Speech Synthesis Workshop}, pages 125--125. ISCA,
  2016.

\bibitem[OpenAI(2023)]{openai2023gpt}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv}, 2023.

\bibitem[Oreshkin et~al.(2020)Oreshkin, Carpov, Chapados, and
  Bengio]{oreshkin2020nbeats}
Boris~N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio.
\newblock N-beats: Neural basis expansion analysis for interpretable time
  series forecasting.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (111):\penalty0 1--63, 2020.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27730--27744, 2022.

\bibitem[Prokhorenkova et~al.(2018)Prokhorenkova, Gusev, Vorobev, Dorogush, and
  Gulin]{prokhorenkova2018catboost}
Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna~Veronika Dorogush,
  and Andrey Gulin.
\newblock Catboost: unbiased boosting with categorical features.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31, pages 6638--6648. NeurIPS, 2018.

\bibitem[Salinas et~al.(2020)Salinas, Flunkert, Gasthaus, and
  Januschowski]{salinas2020deepar}
David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski.
\newblock Deepar: Probabilistic forecasting with autoregressive recurrent
  networks.
\newblock \emph{International Journal of Forecasting}, 36\penalty0
  (3):\penalty0 1181--1191, 2020.

\bibitem[Schwarzschild et~al.(2021)Schwarzschild, Borgnia, Gupta, Huang,
  Vishkin, Goldblum, and Goldstein]{schwarzschild2021can}
Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah
  Goldblum, and Tom Goldstein.
\newblock Can you learn an algorithm? generalizing from easy to hard problems
  with recurrent networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 6695--6706, 2021.

\bibitem[Sutskever(2023)]{sutskevergeneralization}
Ilya Sutskever.
\newblock An observation on generalization.
\newblock Workshop on Large Language Models and Transformers, 2023.
\newblock URL
  \url{https://www.youtube.com/watch?v=AKMuA_TVz3A&ab_channel=SimonsInstitute}.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,
  Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar,
  et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,
  Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher,
  Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami,
  Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann,
  Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov,
  Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva,
  Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang,
  Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and
  Scialom]{Touvron2023Llama2O}
Hugo Touvron, Louis Martin, Kevin~R. Stone, Peter Albert, Amjad Almahairi,
  Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
  Bhosale, Daniel~M. Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen,
  Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian
  Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony~S. Hartshorn,
  Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian
  Khabsa, Isabel~M. Kloumann, A.~V. Korenev, Punit~Singh Koura, Marie-Anne
  Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao,
  Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie,
  Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan
  Schelten, Ruan Silva, Eric~Michael Smith, R.~Subramanian, Xia Tan, Binh Tang,
  Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan
  Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
  Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{ArXiv}, abs/2307.09288, 2023{\natexlab{b}}.

\bibitem[Trapero et~al.(2015)Trapero, Kourentzes, and
  Fildes]{trapero2015identification}
Juan~R Trapero, Nikolaos Kourentzes, and Robert Fildes.
\newblock On the identification of sales forecasting models in the presence of
  promotions.
\newblock \emph{Journal of the operational Research Society}, 66\penalty0
  (2):\penalty0 299--307, 2015.

\bibitem[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and
  Le]{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock \emph{arXiv preprint arXiv:2109.01652}, 2021.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Chi, Le, and
  Zhou]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Chi, Quoc Le, and
  Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock \emph{arXiv preprint arXiv:2201.11903}, 2022.

\bibitem[Wilson and Adams(2013)]{wilson2013gaussian}
Andrew Wilson and Ryan Adams.
\newblock Gaussian process kernels for pattern discovery and extrapolation.
\newblock In \emph{International conference on machine learning}, pages
  1067--1075. PMLR, 2013.

\bibitem[Wu et~al.(2021)Wu, Xu, Wang, and Long]{wu2021autoformer}
Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long.
\newblock Autoformer: Decomposition transformers with auto-correlation for
  long-term series forecasting.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Xue and Salim(2023)]{xue2023promptcast}
Hao Xue and Flora~D. Salim.
\newblock Promptcast: A new prompt-based learning paradigm for time series
  forecasting, 2023.

\bibitem[Yuan et~al.(2023)Yuan, Yuan, Tan, Wang, and Huang]{yuan2023well}
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang.
\newblock How well do large language models perform in arithmetic tasks?
\newblock \emph{arXiv preprint arXiv:2304.02015}, 2023.

\bibitem[Zeng et~al.(2022)Zeng, Chen, Zhang, and Xu]{zeng2022transformers}
Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu.
\newblock Are transformers effective for time series forecasting?
\newblock \emph{arXiv preprint arXiv:2205.13504}, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Gong, Zhang, Li, Qiao, Ouyang, and
  Yue]{zhang2023meta}
Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu~Qiao, Wanli
  Ouyang, and Xiangyu Yue.
\newblock Meta-transformer: A unified framework for multimodal learning.
\newblock \emph{arXiv preprint arXiv:2307.10802}, 2023.

\bibitem[Zhou et~al.(2021)Zhou, Zhang, Peng, Zhang, Li, Xiong, and
  Zhang]{zhou2021informer}
Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,
  and Wancai Zhang.
\newblock Informer: Beyond efficient transformer for long sequence time-series
  forecasting.
\newblock In \emph{Proceedings of AAAI}, 2021.

\bibitem[Zhou et~al.(2022)Zhou, Ma, Wen, Wang, Sun, and Jin]{zhou2022fedformer}
Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.
\newblock {FEDformer}: Frequency enhanced decomposed transformer for long-term
  series forecasting.
\newblock In \emph{Proc. 39th International Conference on Machine Learning
  (ICML 2022)}, 2022.

\bibitem[Zhou et~al.(2023)Zhou, Niu, Wang, Sun, and Jin]{zhou2023one}
Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin.
\newblock One fits all: Power general time series analysis by pretrained lm.
\newblock \emph{arXiv preprint arXiv:2302.11939}, 2023.

\end{thebibliography}
