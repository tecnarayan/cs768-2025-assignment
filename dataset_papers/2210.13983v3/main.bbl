\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  242--252. PMLR, 2019.

\bibitem[Amos(2022)]{amos2022tutorial}
Amos, B.
\newblock Tutorial on amortized optimization for learning to optimize over
  continuous domains.
\newblock \emph{arXiv preprint arXiv:2202.00665}, 2022.

\bibitem[Asim et~al.(2020{\natexlab{a}})Asim, Daniels, Leong, Ahmed, and
  Hand]{asim2020invertible}
Asim, M., Daniels, M., Leong, O., Ahmed, A., and Hand, P.
\newblock {Invertible generative models for inverse problems: mitigating
  representation error and dataset bias}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  399--409. PMLR, 2020{\natexlab{a}}.

\bibitem[Asim et~al.(2020{\natexlab{b}})Asim, Shamshad, and
  Ahmed]{asim2020blind}
Asim, M., Shamshad, F., and Ahmed, A.
\newblock {Blind Image Deconvolution using Deep Generative Priors}.
\newblock \emph{IEEE Transactions on Computational Imaging}, 6:\penalty0
  1493--1506, 2020{\natexlab{b}}.

\bibitem[Beck(2017)]{beck2017first}
Beck, A.
\newblock \emph{First-order methods in optimization}.
\newblock SIAM, 2017.

\bibitem[Bora et~al.(2017)Bora, Jalal, Price, and Dimakis]{bora2017compressed}
Bora, A., Jalal, A., Price, E., and Dimakis, A.~G.
\newblock {Compressed Sensing Using Generative Models}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  537--546. PMLR, 2017.

\bibitem[Boyd et~al.(2004)Boyd, Boyd, and Vandenberghe]{boyd2004convex}
Boyd, S., Boyd, S.~P., and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Candes et~al.(2006)Candes, Romberg, and Tao]{candes2006stable}
Candes, E.~J., Romberg, J.~K., and Tao, T.
\newblock {Stable Signal Recovery from Incomplete and Inaccurate Measurements}.
\newblock \emph{Communications on pure and applied mathematics}, 59\penalty0
  (8):\penalty0 1207--1223, 2006.

\bibitem[Chen et~al.(2021)Chen, Chen, Chen, Heaton, Liu, Wang, and
  Yin]{chen2021learning}
Chen, T., Chen, X., Chen, W., Heaton, H., Liu, J., Wang, Z., and Yin, W.
\newblock Learning to optimize: A primer and a benchmark.
\newblock \emph{arXiv preprint arXiv:2103.12828}, 2021.

\bibitem[Danielyan et~al.(2011)Danielyan, Katkovnik, and
  Egiazarian]{danielyan2011bm3d}
Danielyan, A., Katkovnik, V., and Egiazarian, K.
\newblock {BM3D Frames and Variational Image Deblurring}.
\newblock \emph{IEEE Transactions on image processing}, 21\penalty0
  (4):\penalty0 1715--1728, 2011.

\bibitem[Dinh et~al.(2016)Dinh, Sohl-Dickstein, and Bengio]{dinh2016density}
Dinh, L., Sohl-Dickstein, J., and Bengio, S.
\newblock {Density Estimation Using Real NVP}.
\newblock \emph{arXiv preprint arXiv:1605.08803}, 2016.

\bibitem[Donoho(2006)]{donoho2006compressed}
Donoho, D.~L.
\newblock {Compressed Sensing}.
\newblock \emph{IEEE Transactions on information theory}, 52\penalty0
  (4):\penalty0 1289--1306, 2006.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International conference on machine learning}, pp.\
  1675--1685. PMLR, 2019.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International conference on machine learning}, pp.\
  1126--1135. PMLR, 2017.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y.
\newblock {Generative Adversarial Nets}.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2672--2680, 2014.

\bibitem[Hand \& Voroninski(2018)Hand and Voroninski]{hand2018global}
Hand, P. and Voroninski, V.
\newblock {Global Guarantees for Enforcing Deep Generative Priors by Empirical
  Risk}.
\newblock In \emph{Conference On Learning Theory}, pp.\  970--978. PMLR, 2018.

\bibitem[Hand et~al.(2018)Hand, Leong, and Voroninski]{hand2018phase}
Hand, P., Leong, O., and Voroninski, V.
\newblock {Phase Retrieval Under a Generative Prior}.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Hong et~al.(2022)Hong, Park, and Chun]{hong2022robustness}
Hong, S., Park, I., and Chun, S.~Y.
\newblock On the robustness of normalizing flows for inverse problems in
  imaging.
\newblock \emph{arXiv preprint arXiv:2212.04319}, 2022.

\bibitem[Ichnowski et~al.(2021)Ichnowski, Jain, Stellato, Banjac, Luo,
  Borrelli, Gonzalez, Stoica, and Goldberg]{ichnowski2021accelerating}
Ichnowski, J., Jain, P., Stellato, B., Banjac, G., Luo, M., Borrelli, F.,
  Gonzalez, J.~E., Stoica, I., and Goldberg, K.
\newblock Accelerating quadratic optimization with reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 21043--21055, 2021.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jalal et~al.(2020)Jalal, Liu, Dimakis, and Caramanis]{jalal2020robust}
Jalal, A., Liu, L., Dimakis, A.~G., and Caramanis, C.
\newblock {Robust Compressed Sensing using Generative Models}.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 713--727, 2020.

\bibitem[Jalal et~al.(2021)Jalal, Arvinte, Daras, Price, Dimakis, and
  Tamir]{jalal2021robust}
Jalal, A., Arvinte, M., Daras, G., Price, E., Dimakis, A.~G., and Tamir, J.
\newblock {Robust compressed sensing mri with deep generative priors}.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 14938--14954, 2021.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
Karimi, H., Nutini, J., and Schmidt, M.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In \emph{Joint European conference on machine learning and knowledge
  discovery in databases}, pp.\  795--811. Springer, 2016.

\bibitem[Karras et~al.(2017)Karras, Aila, Laine, and
  Lehtinen]{karras2017progressive}
Karras, T., Aila, T., Laine, S., and Lehtinen, J.
\newblock {Progressive Growing of GANs for Improved Quality, Stability, and
  Variation}.
\newblock \emph{arXiv preprint arXiv:1710.10196}, 2017.

\bibitem[Karras et~al.(2019)Karras, Laine, and Aila]{karras2019style}
Karras, T., Laine, S., and Aila, T.
\newblock {A Style-Based Generator Architecture for Generative Adversarial
  Networks}.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pp.\  4401--4410, 2019.

\bibitem[Kim et~al.(2018)Kim, Wiseman, Miller, Sontag, and Rush]{kim2018semi}
Kim, Y., Wiseman, S., Miller, A., Sontag, D., and Rush, A.
\newblock {Semi-Amortized Variational Autoencoders}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2678--2687. PMLR, 2018.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock {Adam: A Method for Stochastic Optimization}.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma \& Dhariwal(2018)Kingma and Dhariwal]{kingma2018glow}
Kingma, D.~P. and Dhariwal, P.
\newblock {Glow: Generative Flow with Invertible 1x1 Convolutions}.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock {Auto-Encoding Variational Bayes}.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Lei et~al.(2019)Lei, Jalal, Dhillon, and Dimakis]{lei2019inverting}
Lei, Q., Jalal, A., Dhillon, I.~S., and Dimakis, A.~G.
\newblock {Inverting Deep Generative models, One layer at a time}.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Li \& Denli(2021)Li and Denli]{li2021traversing}
Li, D. and Denli, H.
\newblock {Traversing within the Gaussian Typical Set: Differentiable
  Gaussianization Layers for Inverse Problems Augmented by Normalizing Flows}.
\newblock \emph{arXiv preprint arXiv:2112.03860}, 2021.

\bibitem[Li \& Yuan(2017)Li and Yuan]{li2017convergence}
Li, Y. and Yuan, Y.
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Liu et~al.(2020)Liu, Zhu, and Belkin]{liu2020toward}
Liu, C., Zhu, L., and Belkin, M.
\newblock Toward a theory of optimization for over-parameterized systems of
  non-linear equations: the lessons of deep learning.
\newblock \emph{arXiv preprint arXiv:2003.00307}, 2020.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Zhu, and Belkin]{liu2022loss}
Liu, C., Zhu, L., and Belkin, M.
\newblock Loss landscapes and optimization in over-parameterized non-linear
  systems and neural networks.
\newblock \emph{Applied and Computational Harmonic Analysis}, 59:\penalty0
  85--116, 2022{\natexlab{a}}.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Lu, Abbasi, Li, Mohammadi, and
  Kolouri]{liu2022teaching}
Liu, X., Lu, Y., Abbasi, A., Li, M., Mohammadi, J., and Kolouri, S.
\newblock Teaching networks to solve optimization problems.
\newblock \emph{arXiv preprint arXiv:2202.04104}, 2022{\natexlab{b}}.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015faceattributes}
Liu, Z., Luo, P., Wang, X., and Tang, X.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of International Conference on Computer Vision
  (ICCV)}, December 2015.

\bibitem[Marino et~al.(2018)Marino, Yue, and Mandt]{marino2018iterative}
Marino, J., Yue, Y., and Mandt, S.
\newblock {Iterative Amortized Inference}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3403--3412. PMLR, 2018.

\bibitem[Marino(2021)]{marino2021learned}
Marino, J.~L.
\newblock \emph{Learned Feedback \& Feedforward Perception \& Control}.
\newblock PhD thesis, California Institute of Technology, 2021.

\bibitem[Menon et~al.(2020)Menon, Damian, Hu, Ravi, and Rudin]{menon2020pulse}
Menon, S., Damian, A., Hu, S., Ravi, N., and Rudin, C.
\newblock {PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration
  of Generative Models}.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  2437--2445, 2020.

\bibitem[Ongie et~al.(2020)Ongie, Jalal, Metzler, Baraniuk, Dimakis, and
  Willett]{ongie2020deep}
Ongie, G., Jalal, A., Metzler, C.~A., Baraniuk, R.~G., Dimakis, A.~G., and
  Willett, R.
\newblock {Deep Learning Techniques for Inverse Problems in Imaging}.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (1):\penalty0 39--56, 2020.

\bibitem[Papamakarios et~al.(2021)Papamakarios, Nalisnick, Rezende, Mohamed,
  and Lakshminarayanan]{papamakarios2021normalize}
Papamakarios, G., Nalisnick, E., Rezende, D.~J., Mohamed, S., and
  Lakshminarayanan, B.
\newblock {Normalizing Flows for Probabilistic Modeling and Inference}.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (57):\penalty0 1--64, 2021.
\newblock URL \url{http://jmlr.org/papers/v22/19-1028.html}.

\bibitem[Rezende \& Mohamed(2015)Rezende and Mohamed]{rezende2015variational}
Rezende, D. and Mohamed, S.
\newblock Variational inference with normalizing flows.
\newblock In \emph{International conference on machine learning}, pp.\
  1530--1538. PMLR, 2015.

\bibitem[Safran et~al.(2021)Safran, Yehudai, and Shamir]{safran2021effects}
Safran, I.~M., Yehudai, G., and Shamir, O.
\newblock The effects of mild over-parameterization on the optimization
  landscape of shallow relu neural networks.
\newblock In \emph{Conference on Learning Theory}, pp.\  3889--3934. PMLR,
  2021.

\bibitem[Scaman et~al.(2022)Scaman, Malherbe, and
  Dos~Santos]{scaman2022convergence}
Scaman, K., Malherbe, C., and Dos~Santos, L.
\newblock Convergence rates of non-convex stochastic gradient descent under a
  generic lojasiewicz condition and local smoothness.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  19310--19327. PMLR, 2022.

\bibitem[Song et~al.(2021)Song, Ramezani-Kebrya, Pethick, Eftekhari, and
  Cevher]{song2021subquadratic}
Song, C., Ramezani-Kebrya, A., Pethick, T., Eftekhari, A., and Cevher, V.
\newblock Subquadratic overparameterization for shallow neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 11247--11259, 2021.

\bibitem[Song et~al.(2019)Song, Fan, and Lafferty]{song2019surfing}
Song, G., Fan, Z., and Lafferty, J.
\newblock Surfing: Iterative optimization over incrementally trained deep
  networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Stanley et~al.(2009)Stanley, D'Ambrosio, and
  Gauci]{stanley2009hypercube}
Stanley, K.~O., D'Ambrosio, D.~B., and Gauci, J.
\newblock A hypercube-based encoding for evolving large-scale neural networks.
\newblock \emph{Artificial life}, 15\penalty0 (2):\penalty0 185--212, 2009.

\bibitem[Ulyanov et~al.(2018)Ulyanov, Vedaldi, and Lempitsky]{ulyanov2018deep}
Ulyanov, D., Vedaldi, A., and Lempitsky, V.
\newblock {Deep Image Prior}.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  9446--9454, 2018.

\bibitem[Van~Veen et~al.(2018)Van~Veen, Jalal, Soltanolkotabi, Price,
  Vishwanath, and Dimakis]{van2018compressed}
Van~Veen, D., Jalal, A., Soltanolkotabi, M., Price, E., Vishwanath, S., and
  Dimakis, A.~G.
\newblock {Compressed Sensing with Deep Image Prior and Learned
  Regularization}.
\newblock \emph{arXiv preprint arXiv:1806.06438}, 2018.

\bibitem[Vitoria et~al.(2018)Vitoria, Sintes, and
  Ballester]{vitoria2018semantic}
Vitoria, P., Sintes, J., and Ballester, C.
\newblock {Semantic Image Inpainting Through Improved Wasserstein Generative
  Adversarial Networks}.
\newblock \emph{arXiv preprint arXiv:1812.01071}, 2018.

\bibitem[Whang et~al.(2020)Whang, Lei, and Dimakis]{whang2020compressed}
Whang, J., Lei, Q., and Dimakis, A.~G.
\newblock Compressed sensing with invertible generative models and dependent
  noise.
\newblock \emph{arXiv preprint arXiv:2003.08089}, 2020.

\bibitem[Whang et~al.(2021{\natexlab{a}})Whang, Lei, and Dimakis]{whang21solve}
Whang, J., Lei, Q., and Dimakis, A.
\newblock {Solving Inverse Problems with a Flow-based Noise Model}.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  11146--11157. PMLR,
  18--24 Jul 2021{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v139/whang21a.html}.

\bibitem[Whang et~al.(2021{\natexlab{b}})Whang, Lindgren, and
  Dimakis]{whang2021composing}
Whang, J., Lindgren, E., and Dimakis, A.
\newblock {Composing Normalizing Flows for Inverse Problems}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11158--11169. PMLR, 2021{\natexlab{b}}.

\bibitem[Yu et~al.(2011)Yu, Sapiro, and Mallat]{yu2011solving}
Yu, G., Sapiro, G., and Mallat, S.
\newblock {Solving Inverse Problems with Piecewise Linear Estimators: From
  Gaussian Mixture Models to Structured Sparsity}.
\newblock \emph{IEEE Transactions on Image Processing}, 21\penalty0
  (5):\penalty0 2481--2499, 2011.

\bibitem[Zhou et~al.(2019)Zhou, Yang, Zhang, Liang, and Tarokh]{zhou2019sgd}
Zhou, Y., Yang, J., Zhang, H., Liang, Y., and Tarokh, V.
\newblock Sgd converges to global minimum in deep learning via star-convex
  path.
\newblock \emph{arXiv preprint arXiv:1901.00451}, 2019.

\end{thebibliography}
