\begin{thebibliography}{107}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Jun and Nichol(2023)]{jun2023shap}
Heewoo Jun and Alex Nichol.
\newblock Shap-e: Generating conditional 3d implicit functions.
\newblock \emph{arXiv preprint arXiv:2305.02463}, 2023.

\bibitem[Gupta et~al.(2023)Gupta, Xiong, Nie, Jones, and
  O{\u{g}}uz]{gupta20233dgen}
Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas O{\u{g}}uz.
\newblock 3dgen: Triplane latent diffusion for textured mesh generation.
\newblock \emph{arXiv}, 2023.

\bibitem[Poole et~al.(2022)Poole, Jain, Barron, and
  Mildenhall]{poole2022dreamfusion}
Ben Poole, Ajay Jain, Jonathan~T Barron, and Ben Mildenhall.
\newblock Dreamfusion: Text-to-3d using 2d diffusion.
\newblock \emph{arXiv}, 2022.

\bibitem[Tang and Ho(2020)]{tang20203d}
Yuk~Ming Tang and Ho~Lun Ho.
\newblock 3d modeling and computer graphics in virtual reality.
\newblock In \emph{Mixed Reality and Three-Dimensional Computer Graphics}.
  IntechOpen, 2020.

\bibitem[Parent(2012)]{parent2012computer}
Rick Parent.
\newblock \emph{Computer animation: algorithms and techniques}.
\newblock Newnes, 2012.

\bibitem[Afzal et~al.(2020)Afzal, Katz, Goues, and Timperley]{afzal2020study}
Afsoon Afzal, Deborah~S Katz, Claire~Le Goues, and Christopher~S Timperley.
\newblock A study on the challenges of using robotics simulators for testing.
\newblock \emph{arXiv preprint arXiv:2004.07368}, 2020.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Zhang, Wong, Gokmen, Srivastava,
  Mart{\'\i}n-Mart{\'\i}n, Wang, Levine, Lingelbach, Sun,
  et~al.]{li2023behavior}
Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto
  Mart{\'\i}n-Mart{\'\i}n, Chen Wang, Gabrael Levine, Michael Lingelbach,
  Jiankai Sun, et~al.
\newblock Behavior-1k: A benchmark for embodied ai with 1,000 everyday
  activities and realistic simulation.
\newblock In \emph{Conference on Robot Learning}, pages 80--93. PMLR,
  2023{\natexlab{a}}.

\bibitem[Dosovitskiy et~al.(2017)Dosovitskiy, Ros, Codevilla, Lopez, and
  Koltun]{dosovitskiy2017carla}
Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen
  Koltun.
\newblock Carla: An open urban driving simulator.
\newblock In \emph{Conference on robot learning}, pages 1--16. PMLR, 2017.

\bibitem[Deitke et~al.(2023)Deitke, Schwenk, Salvador, Weihs, Michel,
  VanderBilt, Schmidt, Ehsani, Kembhavi, and Farhadi]{deitke2022objaverse}
Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli
  VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali
  Farhadi.
\newblock Objaverse: A universe of annotated 3d objects.
\newblock 2023.

\bibitem[Desai et~al.(2021)Desai, Kaul, Aysola, and Johnson]{desai2021redcaps}
Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson.
\newblock Redcaps: Web-curated image-text data created by the people, for the
  people.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Sharma et~al.(2018)Sharma, Ding, Goodman, and
  Soricut]{sharma2018conceptual}
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset
  for automatic image captioning.
\newblock In \emph{ACL}, 2018.

\bibitem[Changpinyo et~al.(2021)Changpinyo, Sharma, Ding, and
  Soricut]{changpinyo2021conceptual}
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.
\newblock Conceptual 12m: Pushing web-scale image-text pre-training to
  recognize long-tail visual concepts.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 3558--3568, 2021.

\bibitem[Schuhmann et~al.(2021)Schuhmann, Vencu, Beaumont, Kaczmarczyk, Mullis,
  Katta, Coombes, Jitsev, and Komatsuzaki]{schuhmann2021laion}
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk,
  Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran
  Komatsuzaki.
\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text
  pairs.
\newblock \emph{arXiv}, 2021.

\bibitem[Schuhmann et~al.(2022)Schuhmann, Beaumont, Vencu, Gordon, Wightman,
  Cherti, Coombes, Katta, Mullis, Wortsman, et~al.]{schuhmann2022laion}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross
  Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell
  Wortsman, et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation
  image-text models.
\newblock 2022.

\bibitem[El~Banani et~al.(2023)El~Banani, Desai, and
  Johnson]{elbanani2023lgssl}
Mohamed El~Banani, Karan Desai, and Justin Johnson.
\newblock {Learning Visual Representations via Language-Guided Sampling}.
\newblock In \emph{CVPR}, 2023.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{ICML}, 2021.

\bibitem[Mu et~al.(2022)Mu, Kirillov, Wagner, and Xie]{mu2022slip}
Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie.
\newblock Slip: Self-supervision meets language-image pre-training.
\newblock In \emph{ECCV}, 2022.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
  Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
  et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In \emph{ICML}, 2021.

\bibitem[Gafni et~al.(2022)Gafni, Polyak, Ashual, Sheynin, Parikh, and
  Taigman]{gafni2022make}
Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv
  Taigman.
\newblock Make-a-scene: Scene-based text-to-image generation with human priors.
\newblock In \emph{ECCV}, 2022.

\bibitem[Saharia et~al.(2022)Saharia, Chan, Saxena, Li, Whang, Denton,
  Ghasemipour, Gontijo~Lopes, Karagol~Ayan, Salimans,
  et~al.]{saharia2022photorealistic}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily~L
  Denton, Kamyar Ghasemipour, Raphael Gontijo~Lopes, Burcu Karagol~Ayan, Tim
  Salimans, et~al.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding.
\newblock 2022.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and
  Ommer]{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{CVPR}, 2022.

\bibitem[Nichol et~al.(2021)Nichol, Dhariwal, Ramesh, Shyam, Mishkin, McGrew,
  Sutskever, and Chen]{nichol2021glide}
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
  Bob McGrew, Ilya Sutskever, and Mark Chen.
\newblock Glide: Towards photorealistic image generation and editing with
  text-guided diffusion models.
\newblock \emph{CoRR}, 2021.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and
  Chen]{ramesh2022hierarchical}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock \emph{arXiv}, 2022.

\bibitem[Wang et~al.(2022)Wang, Yu, Yu, Dai, Tsvetkov, and Cao]{wang2021simvlm}
Zirui Wang, Jiahui Yu, Adams~Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao.
\newblock Simvlm: Simple visual language model pretraining with weak
  supervision.
\newblock \emph{ICLR}, 2022.

\bibitem[Li et~al.(2020)Li, Yin, Li, Zhang, Hu, Zhang, Wang, Hu, Dong, Wei,
  et~al.]{li2020oscar}
Xiujun Li, Xi~Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan
  Wang, Houdong Hu, Li~Dong, Furu Wei, et~al.
\newblock Oscar: Object-semantics aligned pre-training for vision-language
  tasks.
\newblock In \emph{ECCV}, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Li, Hu, Yang, Zhang, Wang, Choi, and
  Gao]{zhang2021vinvl}
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,
  Yejin Choi, and Jianfeng Gao.
\newblock Vinvl: Revisiting visual representations in vision-language models.
\newblock In \emph{CVPR}, 2021.

\bibitem[Han et~al.(2020)Han, Chen, Liu, and Zwicker]{han2020shapecaptioner}
Zhizhong Han, Chao Chen, Yu-Shen Liu, and Matthias Zwicker.
\newblock Shapecaptioner: Generative caption network for 3d shapes by learning
  a mapping from parts detected in multiple views to sentences.
\newblock In \emph{ACM MM}, 2020.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Li, Savarese, and Hoi]{li2023blip}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock \emph{arXiv}, 2023{\natexlab{b}}.

\bibitem[OpenAI(2023{\natexlab{a}})]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023{\natexlab{a}}.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
  Doll{\'a}r, and Zitnick]{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{ECCV}, 2014.

\bibitem[Krishna et~al.(2017)Krishna, Zhu, Groth, Johnson, Hata, Kravitz, Chen,
  Kalantidis, Li, Shamma, et~al.]{krishna2017visual}
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
  Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David~A Shamma, et~al.
\newblock Visual genome: Connecting language and vision using crowdsourced
  dense image annotations.
\newblock \emph{IJCV}, 2017.

\bibitem[Ordonez et~al.(2011)Ordonez, Kulkarni, and Berg]{ordonez2011im2text}
Vicente Ordonez, Girish Kulkarni, and Tamara Berg.
\newblock Im2text: Describing images using 1 million captioned photographs.
\newblock \emph{NeurIPS}, 2011.

\bibitem[com()]{commoncrawl}
\url{https://commoncrawl.org/the-data/}.

\bibitem[Collins et~al.(2022)Collins, Goel, Deng, Luthra, Xu, Gundogdu, Zhang,
  Yago~Vicente, Dideriksen, Arora, Guillaumin, and Malik]{collins2022abo}
Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan
  Gundogdu, Xi~Zhang, Tomas~F Yago~Vicente, Thomas Dideriksen, Himanshu Arora,
  Matthieu Guillaumin, and Jitendra Malik.
\newblock Abo: Dataset and benchmarks for real-world 3d object understanding.
\newblock \emph{CVPR}, 2022.

\bibitem[Anderson et~al.(2018)Anderson, He, Buehler, Teney, Johnson, Gould, and
  Zhang]{anderson2018bottom}
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
  Gould, and Lei Zhang.
\newblock Bottom-up and top-down attention for image captioning and visual
  question answering.
\newblock In \emph{CVPR}, 2018.

\bibitem[Rennie et~al.(2017)Rennie, Marcheret, Mroueh, Ross, and
  Goel]{rennie2017self}
Steven~J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava
  Goel.
\newblock Self-critical sequence training for image captioning.
\newblock In \emph{CVPR}, 2017.

\bibitem[Lu et~al.(2018)Lu, Yang, Batra, and Parikh]{lu2018neural}
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.
\newblock Neural baby talk.
\newblock In \emph{CVPR}, 2018.

\bibitem[Yu et~al.(2018)Yu, Lin, Shen, Yang, Lu, Bansal, and
  Berg]{yu2018mattnet}
Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and
  Tamara~L Berg.
\newblock Mattnet: Modular attention network for referring expression
  comprehension.
\newblock In \emph{CVPR}, 2018.

\bibitem[Lee et~al.(2018)Lee, Chen, Hua, Hu, and He]{lee2018stacked}
Kuang-Huei Lee, Xi~Chen, Gang Hua, Houdong Hu, and Xiaodong He.
\newblock Stacked cross attention for image-text matching.
\newblock In \emph{ECCV}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{NIPS2012_c399862d}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2012.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015u}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{MICCAI 2015}, 2015.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 1997.

\bibitem[Schuster and Paliwal(1997)]{schuster1997bidirectional}
Mike Schuster and Kuldip~K Paliwal.
\newblock Bidirectional recurrent neural networks.
\newblock \emph{transactions on Signal Processing}, 1997.

\bibitem[Agrawal et~al.(2019)Agrawal, Desai, Wang, Chen, Jain, Johnson, Batra,
  Parikh, Lee, and Anderson]{agrawal2019nocaps}
Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark
  Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson.
\newblock Nocaps: Novel object captioning at scale.
\newblock In \emph{ICCV}, 2019.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock 2014.

\bibitem[Karras et~al.(2020)Karras, Laine, Aittala, Hellsten, Lehtinen, and
  Aila]{karras2020analyzing}
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and
  Timo Aila.
\newblock Analyzing and improving the image quality of stylegan.
\newblock In \emph{CVPR}, 2020.

\bibitem[Van Den~Oord et~al.(2017)Van Den~Oord, Vinyals, et~al.]{van2017neural}
Aaron Van Den~Oord, Oriol Vinyals, et~al.
\newblock Neural discrete representation learning.
\newblock \emph{NeurIPS}, 2017.

\bibitem[Esser et~al.(2021)Esser, Rombach, and Ommer]{esser2021taming}
Patrick Esser, Robin Rombach, and Bjorn Ommer.
\newblock Taming transformers for high-resolution image synthesis.
\newblock In \emph{CVPR}, 2021.

\bibitem[Ding et~al.(2021)Ding, Yang, Hong, Zheng, Zhou, Yin, Lin, Zou, Shao,
  Yang, et~al.]{ding2021cogview}
Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da~Yin, Junyang
  Lin, Xu~Zou, Zhou Shao, Hongxia Yang, et~al.
\newblock Cogview: Mastering text-to-image generation via transformers.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Desai and Johnson(2021)]{desai2021virtex}
Karan Desai and Justin Johnson.
\newblock Virtex: Learning visual representations from textual annotations.
\newblock In \emph{CVPR}, 2021.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{ICLR}, 2021.

\bibitem[Dhariwal and Nichol(2021)]{dhariwal2021diffusion}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{NeurIPS}, 33, 2020.

\bibitem[Karras et~al.(2022)Karras, Aittala, Aila, and
  Laine]{karras2022elucidating}
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
\newblock Elucidating the design space of diffusion-based generative models.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Nichol and Dhariwal(2021)]{nichol2021improved}
Alexander~Quinn Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock In \emph{ICML}, 2021.

\bibitem[Song and Ermon(2019)]{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and
  Ganguli]{sohl2015deep}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{ICML}, 2015.

\bibitem[Kirillov et~al.(2023)Kirillov, Mintun, Ravi, Mao, Rolland, Gustafson,
  Xiao, Whitehead, Berg, Lo, et~al.]{kirillov2023segment}
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
  Gustafson, Tete Xiao, Spencer Whitehead, Alexander~C Berg, Wan-Yen Lo, et~al.
\newblock Segment anything.
\newblock \emph{arXiv}, 2023.

\bibitem[Hessel et~al.(2021)Hessel, Holtzman, Forbes, Bras, and
  Choi]{hessel2021clipscore}
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan~Le Bras, and Yejin Choi.
\newblock Clipscore: A reference-free evaluation metric for image captioning.
\newblock \emph{arXiv}, 2021.

\bibitem[OpenAI(2023{\natexlab{b}})]{gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv}, 2023{\natexlab{b}}.

\bibitem[Zhang and Agrawala(2023)]{zhang2023adding}
Lvmin Zhang and Maneesh Agrawala.
\newblock Adding conditional control to text-to-image diffusion models.
\newblock \emph{arXiv}, 2023.

\bibitem[Pinkney(2022)]{pinkney2022pokemon}
Justin N.~M. Pinkney.
\newblock Pokemon blip captions.
\newblock
  \url{https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions/},
  2022.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Shi, Kuang, Zhu, Li, Han, Cai,
  Porikli, and Su]{liu2023openshape}
Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han,
  Hong Cai, Fatih Porikli, and Hao Su.
\newblock Openshape: Scaling up 3d shape representation towards open-world
  understanding.
\newblock \emph{arXiv}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2022)Li, Li, Xiong, and Hoi]{li2022blip}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified
  vision-language understanding and generation.
\newblock In \emph{ICML}, 2022.

\bibitem[Xue et~al.(2023{\natexlab{a}})Xue, Gao, Xing, Mart{\'\i}n-Mart{\'\i}n,
  Wu, Xiong, Xu, Niebles, and Savarese]{xue2022ulip}
Le~Xue, Mingfei Gao, Chen Xing, Roberto Mart{\'\i}n-Mart{\'\i}n, Jiajun Wu,
  Caiming Xiong, Ran Xu, Juan~Carlos Niebles, and Silvio Savarese.
\newblock Ulip: Learning unified representation of language, image and point
  cloud for 3d understanding.
\newblock \emph{CVPR}, 2023{\natexlab{a}}.

\bibitem[Chang et~al.(2015)Chang, Funkhouser, Guibas, Hanrahan, Huang, Li,
  Savarese, Savva, Song, Su, et~al.]{chang2015shapenet}
Angel~X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang,
  Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et~al.
\newblock Shapenet: An information-rich 3d model repository.
\newblock \emph{arXiv}, 2015.

\bibitem[Sun et~al.(2018)Sun, Wu, Zhang, Zhang, Zhang, Xue, Tenenbaum, and
  Freeman]{sun2018pix3d}
Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Tianfan
  Xue, Joshua~B Tenenbaum, and William~T Freeman.
\newblock Pix3d: Dataset and methods for single-image 3d shape modeling.
\newblock In \emph{CVPR}, 2018.

\bibitem[Lim et~al.(2013)Lim, Pirsiavash, and Torralba]{lim2013parsing}
Joseph~J Lim, Hamed Pirsiavash, and Antonio Torralba.
\newblock Parsing ikea objects: Fine pose estimation.
\newblock In \emph{ICCV}, 2013.

\bibitem[Fu et~al.(2021)Fu, Jia, Gao, Gong, Zhao, Maybank, and Tao]{fu20213d}
Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and
  Dacheng Tao.
\newblock 3d-future: 3d furniture shape with texture.
\newblock \emph{IJCV}, 2021.

\bibitem[Achlioptas et~al.(2019)Achlioptas, Fan, Hawkins, Goodman, and
  Guibas]{shapeglot}
Panos Achlioptas, Judy Fan, X.D.~Robert Hawkins, D.~Noah Goodman, and
  J.~Leonidas Guibas.
\newblock {ShapeGlot}: Learning language for shape differentiation.
\newblock \emph{CoRR}, 2019.

\bibitem[Chen et~al.(2019)Chen, Choy, Savva, Chang, Funkhouser, and
  Savarese]{chen2019text2shape}
Kevin Chen, Christopher~B Choy, Manolis Savva, Angel~X Chang, Thomas
  Funkhouser, and Silvio Savarese.
\newblock Text2shape: Generating shapes from natural language by learning joint
  embeddings.
\newblock In \emph{ACCV}, 2019.

\bibitem[Fu et~al.(2022)Fu, Zhan, Chen, Ritchie, and
  Sridhar]{fu2022shapecrafter}
Rao Fu, Xiao Zhan, Yiwen Chen, Daniel Ritchie, and Srinath Sridhar.
\newblock Shapecrafter: A recursive text-conditioned 3d shape generation model.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=KUOKpojFr_}.

\bibitem[Dai et~al.(2017)Dai, Chang, Savva, Halber, Funkhouser, and
  Nie{\ss}ner]{dai2017scannet}
Angela Dai, Angel~X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
  Matthias Nie{\ss}ner.
\newblock Scannet: Richly-annotated 3d reconstructions of indoor scenes.
\newblock In \emph{CVPR}, 2017.

\bibitem[Chen et~al.(2020)Chen, Chang, and Nie{\ss}ner]{chen2020scanrefer}
Dave~Zhenyu Chen, Angel~X Chang, and Matthias Nie{\ss}ner.
\newblock Scanrefer: 3d object localization in rgb-d scans using natural
  language.
\newblock In \emph{ECCV}, 2020.

\bibitem[Luo et~al.(2023)Luo, Lee, and Johnson]{luo2023neural}
Tiange Luo, Honglak Lee, and Justin Johnson.
\newblock Neural shape compiler: A unified framework for transforming between
  text, point cloud, and program.
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=gR9UVgH8PZ}.

\bibitem[Vinyals et~al.(2015)Vinyals, Toshev, Bengio, and Erhan]{vinyals2015}
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan.
\newblock Show and tell: A neural image caption generator.
\newblock In \emph{ICML}, 2015.

\bibitem[Chen et~al.(2021)Chen, Gholami, Nießner, and Chang]{chen2021}
Zhenyu Chen, Ali Gholami, Matthias Nießner, and Angel~X. Chang.
\newblock Scan2cap: Context-aware dense captioning in rgb-d scans.
\newblock In \emph{CVPR}, 2021.

\bibitem[Sanghi et~al.(2022)Sanghi, Chu, Lambourne, Wang, Cheng, Fumero, and
  Malekshan]{sanghi2022clip}
Aditya Sanghi, Hang Chu, Joseph~G Lambourne, Ye~Wang, Chin-Yi Cheng, Marco
  Fumero, and Kamal~Rahimi Malekshan.
\newblock Clip-forge: Towards zero-shot text-to-shape generation.
\newblock In \emph{CVPR}, 2022.

\bibitem[Mittal et~al.(2022)Mittal, Cheng, Singh, and
  Tulsiani]{mittal2022autosdf}
Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shubham Tulsiani.
\newblock Autosdf: Shape priors for 3d completion, reconstruction and
  generation.
\newblock In \emph{CVPR}, 2022.

\bibitem[Wei et~al.(2023)Wei, Wang, Feng, Lin, and Yap]{wei2023taps3d}
Jiacheng Wei, Hao Wang, Jiashi Feng, Guosheng Lin, and Kim-Hui Yap.
\newblock Taps3d: Text-guided 3d textured shape generation from pseudo
  supervision.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 16805--16815, 2023.

\bibitem[Zhang et~al.(2023)Zhang, Tang, Niessner, and
  Wonka]{zhang20233dshape2vecset}
Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka.
\newblock 3dshape2vecset: A 3d shape representation for neural fields and
  generative diffusion models.
\newblock \emph{arXiv preprint arXiv:2301.11445}, 2023.

\bibitem[Jain et~al.(2022)Jain, Mildenhall, Barron, Abbeel, and
  Poole]{jain2022zero}
Ajay Jain, Ben Mildenhall, Jonathan~T Barron, Pieter Abbeel, and Ben Poole.
\newblock Zero-shot text-guided object generation with dream fields.
\newblock In \emph{CVPR}, 2022.

\bibitem[Tsalicoglou et~al.(2023)Tsalicoglou, Manhardt, Tonioni, Niemeyer, and
  Tombari]{tsalicoglou2023textmesh}
Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and
  Federico Tombari.
\newblock Textmesh: Generation of realistic 3d meshes from text prompts.
\newblock \emph{arXiv}, 2023.

\bibitem[Lin et~al.(2023)Lin, Gao, Tang, Takikawa, Zeng, Huang, Kreis, Fidler,
  Liu, and Lin]{lin2023magic3d}
Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang,
  Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin.
\newblock Magic3d: High-resolution text-to-3d content creation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 300--309, 2023.

\bibitem[Xue et~al.(2023{\natexlab{b}})Xue, Yu, Zhang, Li,
  Mart{\'\i}n-Mart{\'\i}n, Wu, Xiong, Xu, Niebles, and Savarese]{xue2023ulip}
Le~Xue, Ning Yu, Shu Zhang, Junnan Li, Roberto Mart{\'\i}n-Mart{\'\i}n, Jiajun
  Wu, Caiming Xiong, Ran Xu, Juan~Carlos Niebles, and Silvio Savarese.
\newblock Ulip-2: Towards scalable multimodal pre-training for 3d
  understanding.
\newblock \emph{arXiv}, 2023{\natexlab{b}}.

\bibitem[Nichol et~al.(2022)Nichol, Jun, Dhariwal, Mishkin, and
  Chen]{nichol2022point}
Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen.
\newblock Point-e: A system for generating 3d point clouds from complex
  prompts.
\newblock \emph{arXiv}, 2022.

\bibitem[Nichol and Jun(2023)]{nichol2023shape}
Alex Nichol and Heewoo Jun.
\newblock Shap-e: Generating conditional 3d implicit functions.
\newblock \emph{arXiv}, 2023.

\bibitem[Zhou et~al.(2021)Zhou, Du, and Wu]{zhou20213d}
Linqi Zhou, Yilun Du, and Jiajun Wu.
\newblock 3d shape generation and completion through point-voxel diffusion.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 5826--5835, 2021.

\bibitem[Wu et~al.(2023)Wu, Johnson, Malik, Feichtenhofer, and
  Gkioxari]{wu2023multi}
Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, and
  Georgia Gkioxari.
\newblock Multiview compressive coding for 3d reconstruction.
\newblock \emph{arXiv}, 2023.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Wu, Van~Hoorick, Tokmakov,
  Zakharov, and Vondrick]{liu2023zero}
Ruoshi Liu, Rundi Wu, Basile Van~Hoorick, Pavel Tokmakov, Sergey Zakharov, and
  Carl Vondrick.
\newblock Zero-1-to-3: Zero-shot one image to 3d object.
\newblock \emph{arXiv}, 2023{\natexlab{b}}.

\bibitem[Fang et~al.(2023)Fang, Wang, Xie, Sun, Wu, Wang, Huang, Wang, and
  Cao]{fang2022eva}
Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun
  Huang, Xinlong Wang, and Yue Cao.
\newblock Eva: Exploring the limits of masked visual representation learning at
  scale.
\newblock \emph{CVPR}, 2023.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma, et~al.]{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv}, 2022.

\bibitem[Holtzman et~al.(2020)Holtzman, Buys, Du, Forbes, and
  Choi]{holtzman2020nucleus}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock In \emph{ICLR}, 2020.

\bibitem[Deng et~al.(2020)Deng, Guo, Ververas, Kotsia, and
  Zafeiriou]{deng2020retinaface}
Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos
  Zafeiriou.
\newblock Retinaface: Single-shot multi-level face localisation in the wild.
\newblock In \emph{CVPR}, 2020.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{CVPR}, 2016.

\bibitem[Laborde()]{laborde23}
Gant Laborde.
\newblock Deep nn for nsfw detection.
\newblock \url{https://github.com/GantMan/nsfw_model}.
\newblock [Online; accessed 7-May-2023].

\bibitem[dir()]{dirty23}
\url{https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words}.
\newblock [Online; accessed 7-May-2023].

\bibitem[Kim et~al.(2021)Kim, Son, and Kim]{kim2021vilt}
Wonjae Kim, Bokyung Son, and Ildoo Kim.
\newblock Vilt: Vision-and-language transformer without convolution or region
  supervision.
\newblock In \emph{ICML}, 2021.

\bibitem[Gao et~al.(2022)Gao, Shen, Wang, Chen, Yin, Li, Litany, Gojcic, and
  Fidler]{gao2022get3d}
Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li,
  Or~Litany, Zan Gojcic, and Sanja Fidler.
\newblock Get3d: A generative model of high quality 3d textured shapes learned
  from images.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Mildenhall et~al.(2020)Mildenhall, Srinivasan, Tancik, Barron,
  Ramamoorthi, and Ng]{mildenhall2020nerf}
Ben Mildenhall, Pratul~P. Srinivasan, Matthew Tancik, Jonathan~T. Barron, Ravi
  Ramamoorthi, and Ren Ng.
\newblock Nerf: Representing scenes as neural radiance fields for view
  synthesis.
\newblock In \emph{ECCV}, 2020.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, and
  Chen]{hu2021lora}
Edward Hu, Yelong Shen, Phil Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Lu~Wang, and
  Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models, 2021.

\bibitem[Tang(2022)]{stable-dreamfusion}
Jiaxiang Tang.
\newblock Stable-dreamfusion: Text-to-3d with stable-diffusion, 2022.
\newblock https://github.com/ashawkey/stable-dreamfusion.

\bibitem[Seo et~al.(2023)Seo, Jang, Kwak, Ko, Kim, Kim, Kim, Lee, and
  Kim]{seo2023let}
Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim,
  Jin-Hwa Kim, Jiyoung Lee, and Seungryong Kim.
\newblock Let 2d diffusion model know 3d-consistency for robust text-to-3d
  generation.
\newblock \emph{arXiv}, 2023.

\bibitem[Lee et~al.(2022)Lee, Kim, Choi, Kim, Byeon, Baek, and
  Kim]{kakaobrain2022karlo-v1-alpha}
Donghoon Lee, Jiseob Kim, Jisu Choi, Jongmin Kim, Minwoo Byeon, Woonhyuk Baek,
  and Saehoon Kim.
\newblock Karlo-v1.0.alpha on coyo-100m and cc15m.
\newblock \url{https://github.com/kakaobrain/karlo}, 2022.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock \emph{NeurIPS}, 2017.

\end{thebibliography}
