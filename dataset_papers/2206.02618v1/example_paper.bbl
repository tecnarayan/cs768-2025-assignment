\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acar et~al.(2021)Acar, Zhao, Matas, Mattina, Whatmough, and
  Saligrama]{acar2021federated}
Acar, D. A.~E., Zhao, Y., Matas, R., Mattina, M., Whatmough, P., and Saligrama,
  V.
\newblock Federated learning based on dynamic regularization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
Bartlett, P.~L., Foster, D.~J., and Telgarsky, M.~J.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  30:\penalty0 6240--6249, 2017.

\bibitem[Caldarola et~al.(2022)Caldarola, Caputo, and
  Ciccone]{caldarola2022improving}
Caldarola, D., Caputo, B., and Ciccone, M.
\newblock Improving generalization in federated learning by seeking flat
  minima.
\newblock \emph{arXiv preprint arXiv:2203.11834}, 2022.

\bibitem[Chatterji et~al.(2019)Chatterji, Neyshabur, and
  Sedghi]{chatterji2019intriguing}
Chatterji, N., Neyshabur, B., and Sedghi, H.
\newblock The intriguing role of module criticality in the generalization of
  deep networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Chaudhari et~al.(2019)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{chaudhari2019entropy}
Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C.,
  Chayes, J., Sagun, L., and Zecchina, R.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2019\penalty0 (12):\penalty0 124018, 2019.

\bibitem[Cohen et~al.(2017)Cohen, Afshar, Tapson, and
  Van~Schaik]{cohen2017emnist}
Cohen, G., Afshar, S., Tapson, J., and Van~Schaik, A.
\newblock Emnist: Extending mnist to handwritten letters.
\newblock In \emph{2017 International Joint Conference on Neural Networks
  (IJCNN)}, pp.\  2921--2926. IEEE, 2017.

\bibitem[Deng et~al.(2020)Deng, Kamani, and Mahdavi]{deng2020adaptive}
Deng, Y., Kamani, M.~M., and Mahdavi, M.
\newblock Adaptive personalized federated learning.
\newblock \emph{arXiv preprint arXiv:2003.13461}, 2020.

\bibitem[Dieuleveut et~al.(2021)Dieuleveut, Fort, Moulines, and
  Robin]{dieuleveut2021federated}
Dieuleveut, A., Fort, G., Moulines, E., and Robin, G.
\newblock Federated-em with heterogeneity mitigation and variance reduction.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Du et~al.(2021{\natexlab{a}})Du, Yan, Feng, Zhou, Zhen, Goh, and
  Tan]{du2021efficient}
Du, J., Yan, H., Feng, J., Zhou, J.~T., Zhen, L., Goh, R. S.~M., and Tan, V.~Y.
\newblock Efficient sharpness-aware minimization for improved training of
  neural networks.
\newblock \emph{arXiv preprint arXiv:2110.03141}, 2021{\natexlab{a}}.

\bibitem[Du et~al.(2021{\natexlab{b}})Du, Xu, Wu, and Tong]{du2021fairness}
Du, W., Xu, D., Wu, X., and Tong, H.
\newblock Fairness-aware agnostic federated learning.
\newblock In \emph{Proceedings of the 2021 SIAM International Conference on
  Data Mining (SDM)}, pp.\  181--189. SIAM, 2021{\natexlab{b}}.

\bibitem[Fallah et~al.(2020)Fallah, Mokhtari, and
  Ozdaglar]{fallah2020personalized}
Fallah, A., Mokhtari, A., and Ozdaglar, A.
\newblock Personalized federated learning with theoretical guarantees: A
  model-agnostic meta-learning approach.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3557--3568, 2020.

\bibitem[Farnia et~al.(2018)Farnia, Zhang, and Tse]{farnia2018generalizable}
Farnia, F., Zhang, J., and Tse, D.
\newblock Generalizable adversarial training via spectral normalization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2021sharpnessaware}
Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Gong et~al.(2021)Gong, Sharma, Karanam, Wu, Chen, Doermann, and
  Innanje]{gong2021ensemble}
Gong, X., Sharma, A., Karanam, S., Wu, Z., Chen, T., Doermann, D., and Innanje,
  A.
\newblock Ensemble attention distillation for privacy-preserving federated
  learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  15076--15086, 2021.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Goodfellow, I.~J., Shlens, J., and Szegedy, C.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Goyal, P., Doll{\'a}r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola,
  A., Tulloch, A., Jia, Y., and He, K.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Hard et~al.(2018)Hard, Rao, Mathews, Ramaswamy, Beaufays, Augenstein,
  Eichner, Kiddon, and Ramage]{hard2018federated}
Hard, A., Rao, K., Mathews, R., Ramaswamy, S., Beaufays, F., Augenstein, S.,
  Eichner, H., Kiddon, C., and Ramage, D.
\newblock Federated learning for mobile keyboard prediction.
\newblock \emph{arXiv preprint arXiv:1811.03604}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Kairouz et~al.(2019)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, et~al.]{kairouz2019advances}
Kairouz, P., McMahan, H.~B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.~N.,
  Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{arXiv preprint arXiv:1912.04977}, 2019.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2020scaffold}
Karimireddy, S.~P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh,
  A.~T.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5132--5143. PMLR, 2020.

\bibitem[Karimireddy et~al.(2021)Karimireddy, Jaggi, Kale, Mohri, Reddi, Stich,
  and Suresh]{karimireddy2021breaking}
Karimireddy, S.~P., Jaggi, M., Kale, S., Mohri, M., Reddi, S.~J., Stich, S.~U.,
  and Suresh, A.~T.
\newblock Breaking the centralized barrier for cross-device federated learning.
\newblock In \emph{Thirty-Fifth Conference on Neural Information Processing
  Systems}, 2021.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Khanduri et~al.(2021)Khanduri, SHARMA, Yang, Hong, Liu, Rajawat, and
  Varshney]{khanduri2021stem}
Khanduri, P., SHARMA, P., Yang, H., Hong, M., Liu, J., Rajawat, K., and
  Varshney, P.
\newblock {STEM}: A stochastic two-sided momentum algorithm achieving
  near-optimal sample and communication complexities for federated learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kwon et~al.(2021)Kwon, Kim, Park, and Choi]{kwon21b}
Kwon, J., Kim, J., Park, H., and Choi, I.~K.
\newblock Asam: Adaptive sharpness-aware minimization for scale-invariant
  learning of deep neural networks.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, pp.\  5905--5914. PMLR, 2021.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2017simple}
Lakshminarayanan, B., Pritzel, A., and Blundell, C.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Li et~al.(2018{\natexlab{a}})Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{Advances in Neural Information Processing Systems}, 31,
  2018{\natexlab{a}}.

\bibitem[Li et~al.(2018{\natexlab{b}})Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2018federated}
Li, T., Sahu, A.~K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{arXiv preprint arXiv:1812.06127}, 2018{\natexlab{b}}.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Sanjabi, Beirami, and
  Smith]{Li2020Fair}
Li, T., Sanjabi, M., Beirami, A., and Smith, V.
\newblock Fair resource allocation in federated learning.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.

\bibitem[Li et~al.(2021)Li, Hu, Beirami, and Smith]{li2021ditto}
Li, T., Hu, S., Beirami, A., and Smith, V.
\newblock Ditto: Fair and robust federated learning through personalization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6357--6368. PMLR, 2021.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Huang, Yang, Wang, and
  Zhang]{Li2020On}
Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z.
\newblock On the convergence of fedavg on non-iid data.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{b}}.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and Liu, J.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:1705.09056}, 2017.

\bibitem[Lin et~al.(2020)Lin, Kong, Stich, and Jaggi]{lin2020ensemble}
Lin, T., Kong, L., Stich, S.~U., and Jaggi, M.
\newblock Ensemble distillation for robust model fusion in federated learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  1273--1282.
  PMLR, 2017.

\bibitem[Mendieta et~al.(2021)Mendieta, Yang, Wang, Lee, Ding, and
  Chen]{mendieta2021local}
Mendieta, M., Yang, T., Wang, P., Lee, M., Ding, Z., and Chen, C.
\newblock Local learning matters: Rethinking data heterogeneity in federated
  learning.
\newblock \emph{arXiv preprint arXiv:2111.14213}, 2021.

\bibitem[Mohri et~al.(2019)Mohri, Sivek, and Suresh]{mohri2019agnostic}
Mohri, M., Sivek, G., and Suresh, A.~T.
\newblock Agnostic federated learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4615--4625. PMLR, 2019.

\bibitem[Nesterov \& Spokoiny(2017)Nesterov and Spokoiny]{nesterov2017random}
Nesterov, Y. and Spokoiny, V.
\newblock Random gradient-free minimization of convex functions.
\newblock \emph{Foundations of Computational Mathematics}, 17\penalty0
  (2):\penalty0 527--566, 2017.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Bhojanapalli, and
  Srebro]{neyshabur2018pac}
Neyshabur, B., Bhojanapalli, S., and Srebro, N.
\newblock A pac-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems},
  32:\penalty0 8026--8037, 2019.

\bibitem[Reddi et~al.(2020)Reddi, Charles, Zaheer, Garrett, Rush,
  Kone{\v{c}}n{\`y}, Kumar, and McMahan]{reddi2020adaptive}
Reddi, S.~J., Charles, Z., Zaheer, M., Garrett, Z., Rush, K.,
  Kone{\v{c}}n{\`y}, J., Kumar, S., and McMahan, H.~B.
\newblock Adaptive federated optimization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Reisizadeh et~al.(2020)Reisizadeh, Farnia, Pedarsani, and
  Jadbabaie]{reisizadeh2020robust}
Reisizadeh, A., Farnia, F., Pedarsani, R., and Jadbabaie, A.
\newblock Robust federated learning: The case of affine distribution shifts.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Shafahi et~al.(2020)Shafahi, Najibi, Xu, Dickerson, Davis, and
  Goldstein]{shafahi2020universal}
Shafahi, A., Najibi, M., Xu, Z., Dickerson, J., Davis, L.~S., and Goldstein, T.
\newblock Universal adversarial training.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  5636--5643, 2020.

\bibitem[Singhal et~al.(2021)Singhal, Sidahmed, Garrett, Wu, Rush, and
  Prakash]{singhal2021federated}
Singhal, K., Sidahmed, H., Garrett, Z., Wu, S., Rush, J.~K., and Prakash, S.
\newblock Federated reconstruction: Partially local federated learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[T~Dinh et~al.(2020)T~Dinh, Tran, and Nguyen]{t2020personalized}
T~Dinh, C., Tran, N., and Nguyen, T.~D.
\newblock Personalized federated learning with moreau envelopes.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Tropp(2012)]{tropp2012user}
Tropp, J.~A.
\newblock User-friendly tail bounds for sums of random matrices.
\newblock \emph{Foundations of computational mathematics}, 12\penalty0
  (4):\penalty0 389--434, 2012.

\bibitem[Wang et~al.(2019)Wang, Tantia, Ballas, and Rabbat]{wang2019slowmo}
Wang, J., Tantia, V., Ballas, N., and Rabbat, M.
\newblock Slowmo: Improving communication-efficient distributed sgd with slow
  momentum.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese,
  Golan, Soudry, and Srebro]{woodworth2020kernel}
Woodworth, B., Gunasekar, S., Lee, J.~D., Moroshko, E., Savarese, P., Golan,
  I., Soudry, D., and Srebro, N.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In \emph{Conference on Learning Theory}, pp.\  3635--3673. PMLR,
  2020.

\bibitem[Xu et~al.(2021)Xu, Wang, Wang, and Yao]{xu2021fedcm}
Xu, J., Wang, S., Wang, L., and Yao, A. C.-C.
\newblock Fedcm: Federated learning with client-level momentum.
\newblock \emph{arXiv preprint arXiv:2106.10874}, 2021.

\bibitem[Yang et~al.(2021)Yang, Fang, and Liu]{yang2021achieving}
Yang, H., Fang, M., and Liu, J.
\newblock Achieving linear speedup with partial worker participation in
  non-{IID} federated learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Yoon et~al.(2021)Yoon, Shin, Hwang, and Yang]{yoon2021fedmix}
Yoon, T., Shin, S., Hwang, S.~J., and Yang, E.
\newblock Fedmix: Approximation of mixup under mean augmented federated
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Yuan et~al.(2021)Yuan, Morningstar, Ning, and Singhal]{yuan2021we}
Yuan, H., Morningstar, W., Ning, L., and Singhal, K.
\newblock What do we mean by generalization in federated learning?
\newblock \emph{arXiv preprint arXiv:2110.14216}, 2021.

\bibitem[Zhu et~al.(2021)Zhu, Hong, and Zhou]{zhu2021data}
Zhu, Z., Hong, J., and Zhou, J.
\newblock Data-free knowledge distillation for heterogeneous federated
  learning.
\newblock \emph{arXiv preprint arXiv:2105.10056}, 2021.

\bibitem[Zhuang et~al.(2022)Zhuang, Gong, Yuan, Cui, Adam, Dvornek, sekhar
  tatikonda, s~Duncan, and Liu]{zhuang2022surrogate}
Zhuang, J., Gong, B., Yuan, L., Cui, Y., Adam, H., Dvornek, N.~C., sekhar
  tatikonda, s~Duncan, J., and Liu, T.
\newblock Surrogate gap minimization improves sharpness-aware training.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\end{thebibliography}
