\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{athalye2018obfuscated}
A.~Athalye, N.~Carlini, and D.~A. Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock {\em CoRR}, abs/1802.00420, 2018.

\bibitem{behzadan2017vulnerability}
V.~Behzadan and A.~Munir.
\newblock Vulnerability of deep reinforcement learning to policy induction
  attacks.
\newblock {\em CoRR}, abs/1701.04143, 2017.

\bibitem{Biggio2013evasion}
B.~Biggio, I.~Corona, D.~Maiorca, B.~Nelson, N.~\v{S}rndi\'{c}, P.~Laskov,
  G.~Giacinto, and F.~Roli.
\newblock Evasion attacks against machine learning at test time.
\newblock In {\em Proc. ECML}, pages 387--402, 2013.

\bibitem{brendel2017decision}
W.~Brendel, J.~Rauber, and M.~Bethge.
\newblock Decision-based adversarial attacks: Reliable attacks against
  black-box machine learning models.
\newblock {\em CoRR}, abs/1712.04248, 2017.

\bibitem{buckman2018thermometer}
J.~Buckman, A.~Roy, C.~Raffel, and I.~J. Goodfellow.
\newblock Thermometer encoding: One hot way to resist adversarial examples.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}, 2018.

\bibitem{Carlini2017a}
N.~{Carlini} and D.~{Wagner}.
\newblock {Adversarial Examples Are Not Easily Detected: Bypassing Ten
  Detection Methods}.
\newblock {\em the 10th ACM Workshop on Artificial Intelligence and Security},
  2017.

\bibitem{Carlini2017b}
N.~{Carlini} and D.~{Wagner}.
\newblock {Towards Evaluating the Robustness of Neural Networks}.
\newblock {\em IEEE Symposium on Security and Privacy}, 2017.

\bibitem{carlini2018audio}
N.~Carlini and D.~A. Wagner.
\newblock Audio adversarial examples: Targeted attacks on speech-to-text.
\newblock {\em CoRR}, abs/1801.01944, 2018.

\bibitem{chen2017zoo}
P.~Chen, H.~Zhang, Y.~Sharma, J.~Yi, and C.~Hsieh.
\newblock {ZOO:} zeroth order optimization based black-box attacks to deep
  neural networks without training substitute models.
\newblock In {\em Proceedings of the 10th {ACM} Workshop on Artificial
  Intelligence and Security, AISec@CCS 2017, Dallas, TX, USA, November 3,
  2017}, pages 15--26, 2017.

\bibitem{cisse2017houdini}
M.~Cisse, Y.~Adi, N.~Neverova, and J.~Keshet.
\newblock Houdini: Fooling deep structured prediction models.
\newblock {\em CoRR}, abs/1707.05373, 2017.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em Proc. CVPR}, pages 248--255. IEEE, 2009.

\bibitem{dhillon2018stochastic}
G.~S. Dhillon, K.~Azizzadenesheli, Z.~C. Lipton, J.~Bernstein, J.~Kossaifi,
  A.~Khanna, and A.~Anandkumar.
\newblock Stochastic activation pruning for robust adversarial defense.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}, 2018.

\bibitem{fawzi2018adversarial}
A.~Fawzi, H.~Fawzi, and O.~Fawzi.
\newblock Adversarial vulnerability for any classifier.
\newblock In {\em Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8
  December 2018, Montr{\'{e}}al, Canada.}, pages 1186--1195, 2018.

\bibitem{Feinman2017}
R.~{Feinman}, R.~R. {Curtin}, S.~{Shintre}, and A.~B. {Gardner}.
\newblock {Detecting Adversarial Samples from Artifacts}.
\newblock {\em ArXiv e-prints}, 2017.

\bibitem{finn2017model}
C.~Finn, P.~Abbeel, and S.~Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em ICML}, 2017.

\bibitem{gal2016dropout}
Y.~Gal and Z.~Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In {\em international conference on machine learning}, pages
  1050--1059, 2016.

\bibitem{Goodfellow2015}
I.~J. {Goodfellow}, J.~{Shlens}, and C.~{Szegedy}.
\newblock {Explaining and Harnessing Adversarial Examples}.
\newblock {\em International Conference on Learning Representation (ICLR)},
  2015.

\bibitem{Grosse2017}
K.~{Grosse}, P.~{Manoharan}, N.~{Papernot}, M.~{Backes}, and P.~{McDaniel}.
\newblock {On the (Statistical) Detection of Adversarial Examples}.
\newblock {\em arXiv e-prints}, 2017.

\bibitem{guo2019simple}
C.~Guo, J.~R. Gardner, Y.~You, A.~G. Wilson, and K.~Q. Weinberger.
\newblock Simple black-box adversarial attacks.
\newblock {\em CoRR}, abs/1905.07121, 2019.

\bibitem{Guo2017}
C.~{Guo}, M.~{Rana}, M.~{Cisse}, and L.~{van der Maaten}.
\newblock {Countering Adversarial Images using Input Transformations}.
\newblock {\em International Conference on Learning Representation (ICLR)},
  2018.

\bibitem{He2016}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em {CVPR}}, pages 770--778. {IEEE} Computer Society, 2016.

\bibitem{huang2017adversarial}
S.~Huang, N.~Papernot, I.~Goodfellow, Y.~Duan, and P.~Abbeel.
\newblock Adversarial attacks on neural network policies.
\newblock {\em CoRR}, abs/1702.02284, 2017.

\bibitem{ilyas2018blackbox}
A.~Ilyas, L.~Engstrom, A.~Athalye, and J.~Lin.
\newblock Black-box adversarial attacks with limited queries and information.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, pages 2142--2151, 2018.

\bibitem{ilyas2018prior}
A.~Ilyas, L.~Engstrom, and A.~Madry.
\newblock Prior convictions: Black-box adversarial attacks with bandits and
  priors.
\newblock {\em CoRR}, abs/1807.07978, 2018.

\bibitem{Kanna2018}
H.~{Kannan}, A.~{Kurakin}, and I.~{Goodfellow}.
\newblock {Adversarial Logit Pairing}.
\newblock {\em ArXiv e-prints}, 2018.

\bibitem{kingma2014adam}
D.~Kingma and J.~Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock {\em CoRR}, abs/1412.6980, 2014.

\bibitem{krizhevsky2009cifar}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem{Kurakin2017b}
A.~{Kurakin}, I.~{Goodfellow}, and S.~{Bengio}.
\newblock {Adversarial Machine Learning at Scale}.
\newblock {\em International Conference on Learning Representation (ICLR)},
  2017.

\bibitem{li2017detect}
X.~Li and F.~Li.
\newblock Adversarial examples detection in deep networks with convolutional
  filter statistics.
\newblock In {\em {IEEE} International Conference on Computer Vision, {ICCV}
  2017, Venice, Italy, October 22-29, 2017}, pages 5775--5783, 2017.

\bibitem{liu2018towards}
X.~Liu, M.~Cheng, H.~Zhang, and C.~Hsieh.
\newblock Towards robust neural networks via random self-ensemble.
\newblock In {\em Computer Vision - {ECCV} 2018 - 15th European Conference,
  Munich, Germany, September 8-14, 2018, Proceedings, Part {VII}}, pages
  381--397, 2018.

\bibitem{liu2016delving}
Y.~Liu, X.~Chen, C.~Liu, and D.~Song.
\newblock Delving into transferable adversarial examples and black-box attacks.
\newblock {\em CoRR}, abs/1611.02770, 2016.

\bibitem{ma2018local}
X.~Ma, B.~Li, Y.~Wang, S.~M. Erfani, S.~N.~R. Wijewickrema, G.~Schoenebeck,
  D.~Song, M.~E. Houle, and J.~Bailey.
\newblock Characterizing adversarial subspaces using local intrinsic
  dimensionality.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}, 2018.

\bibitem{Madry2018}
A.~{Madry}, A.~{Makelov}, L.~{Schmidt}, D.~{Tsipras}, and A.~{Vladu}.
\newblock {Towards Deep Learning Models Resistant to Adversarial Attacks}.
\newblock {\em International Conference on Learning Representation (ICLR)},
  2018.

\bibitem{meng2017magnet}
D.~Meng and H.~Chen.
\newblock Magnet: {A} two-pronged defense against adversarial examples.
\newblock In {\em Proceedings of the 2017 {ACM} {SIGSAC} Conference on Computer
  and Communications Security, {CCS} 2017, Dallas, TX, USA, October 30 -
  November 03, 2017}, pages 135--147, 2017.

\bibitem{metzen2017detect}
J.~H. Metzen, T.~Genewein, V.~Fischer, and B.~Bischoff.
\newblock On detecting adversarial perturbations.
\newblock In {\em 5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}, 2017.

\bibitem{Bhagoji2017}
A.~{Nitin Bhagoji}, D.~{Cullina}, C.~{Sitawarin}, and P.~{Mittal}.
\newblock {Enhancing Robustness of Machine Learning Systems via Data
  Transformations}.
\newblock {\em 52nd Annual Conference on Information Sciences and Systems
  (CISS)}, 2018.

\bibitem{pang2018towards}
T.~Pang, C.~Du, Y.~Dong, and J.~Zhu.
\newblock Towards robust detection of adversarial examples.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4579--4589, 2018.

\bibitem{papernot2017practical}
N.~Papernot, P.~D. McDaniel, I.~J. Goodfellow, S.~Jha, Z.~B. Celik, and
  A.~Swami.
\newblock Practical black-box attacks against machine learning.
\newblock In {\em Proceedings of the 2017 {ACM} on Asia Conference on Computer
  and Communications Security, AsiaCCS 2017, Abu Dhabi, United Arab Emirates,
  April 2-6, 2017}, pages 506--519, 2017.

\bibitem{prakash2018deflecting}
A.~Prakash, N.~Moran, S.~Garber, A.~DiLillo, and J.~A. Storer.
\newblock Deflecting adversarial attacks with pixel deflection.
\newblock In {\em 2018 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018}, pages
  8571--8580, 2018.

\bibitem{Raghunathan2018}
A.~{Raghunathan}, J.~{Steinhardt}, and P.~{Liang}.
\newblock {Certified Defenses against Adversarial Examples}.
\newblock {\em International Conference on Learning Representation (ICLR)},
  2018.

\bibitem{roth2019odds}
K.~Roth, Y.~Kilcher, and T.~Hofmann.
\newblock The odds are odd: {A} statistical test for detecting adversarial
  examples.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, 2019.

\bibitem{samangouei2018defense}
P.~Samangouei, M.~Kabkab, and R.~Chellappa.
\newblock Defense-gan: Protecting classifiers against adversarial attacks using
  generative models.
\newblock {\em CoRR}, abs/1805.06605, 2018.

\bibitem{shafahi2018inevitable}
A.~Shafahi, W.~R. Huang, C.~Studer, S.~Feizi, and T.~Goldstein.
\newblock Are adversarial examples inevitable?
\newblock {\em CoRR}, abs/1809.02104, 2018.

\bibitem{simonyan2014vgg}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em 3rd International Conference on Learning Representations,
  {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
  Proceedings}, 2015.

\bibitem{sinha2018certifying}
A.~Sinha, H.~Namkoong, and J.~C. Duchi.
\newblock Certifying some distributional robustness with principled adversarial
  training.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}, 2018.

\bibitem{song2018pixel}
Y.~Song, T.~Kim, S.~Nowozin, S.~Ermon, and N.~Kushman.
\newblock Pixeldefend: Leveraging generative models to understand and defend
  against adversarial examples.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}, 2018.

\bibitem{srivastava2014dropout}
N.~Srivastava, G.~E. Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em Journal of Machine Learning Research}, 15:1929--1958, 2014.

\bibitem{szegedy2015inception}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock {\em CoRR}, abs/1512.00567, 2015.

\bibitem{Szegedy2014}
C.~{Szegedy}, W.~{Zaremba}, I.~{Sutskever}, J.~{Bruna}, D.~{Erhan},
  I.~{Goodfellow}, and R.~{Fergus}.
\newblock {Intriguing properties of neural networks}.
\newblock {\em International Conference on Machine Learning (ICML)}, 2014.

\bibitem{tramer2017ensemble}
F.~Tram{\`{e}}r, A.~Kurakin, N.~Papernot, D.~Boneh, and P.~D. McDaniel.
\newblock Ensemble adversarial training: Attacks and defenses.
\newblock {\em CoRR}, abs/1705.07204, 2017.

\bibitem{tu2018autozoom}
C.~Tu, P.~Ting, P.~Chen, S.~Liu, H.~Zhang, J.~Yi, C.~Hsieh, and S.~Cheng.
\newblock Autozoom: Autoencoder-based zeroth order optimization method for
  attacking black-box neural networks.
\newblock {\em CoRR}, abs/1805.11770, 2018.

\bibitem{uesato2018adversarial}
J.~Uesato, B.~O'Donoghue, P.~Kohli, and A.~van~den Oord.
\newblock Adversarial risk and the dangers of evaluating against weak attacks.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, pages 5032--5041, 2018.

\bibitem{Wong2018}
E.~{Wong} and J.~{Zico Kolter}.
\newblock {Provable defenses against adversarial examples via the convex outer
  adversarial polytope}.
\newblock {\em International Conference on Machine Learning (ICML)}, 2017.

\bibitem{xie2018mitigating}
C.~Xie, J.~Wang, Z.~Zhang, Z.~Ren, and A.~L. Yuille.
\newblock Mitigating adversarial effects through randomization.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}, 2018.

\bibitem{xie2017adversarial}
C.~Xie, J.~Wang, Z.~Zhang, Y.~Zhou, L.~Xie, and A.~L. Yuille.
\newblock Adversarial examples for semantic segmentation and object detection.
\newblock In {\em {ICCV}}, pages 1378--1387. {IEEE} Computer Society, 2017.

\bibitem{Xu2018}
W.~{Xu}, D.~{Evans}, and Y.~{Qi}.
\newblock {Feature Squeezing: Detecting Adversarial Examples in Deep Neural
  Networks}.
\newblock {\em Network and Distributed Systems Security Symposium (NDSS)},
  2018.

\end{thebibliography}
