
@ARTICLE{MJ20,
  title={Convergence of a Stochastic Gradient Method with Momentum for Non-Smooth Non-Convex Optimization},
  author={Vien V. Mai and Mikael Johansson},
  journal   = {ICML},
  year      = {2020}
}

@ARTICLE{LL20,
  title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},
  author={Kaifeng Lyu and Jian Li},
  journal   = {ICLR},
  year      = {2020}
}

@ARTICLE{CCGZ20,
  title={A Generalized Neural Tangent Kernel Analysis for Two-layer Neural Network},
  author={Zixiang Chen and Yuan Cao and Quanquan Gu and Tong Zhang},
  journal   = {NeurIPS},
  year      = {2020}
}

@ARTICLE{PE20,
  title={Neural Networks are Convex Regularizers: Exact Polynomial-time Convex
Optimization Formulations for Two-layer Networks},
  author={Mert Pilanci and Tolga Ergen},
  journal   = {ICML},
  year      = {2020}
}

@ARTICLE{CHS20,
  title={Label-Aware Neural Tangent Kernel: Toward Better Generalization and Local Elasticity},
  author={Shuxiao Chen and Hangfeng He and Weijie J. Su},
  journal   = {NeurIPS},
  year      = {2020}
}


@ARTICLE{HL17,
  title={Dissipativity Theory for Nesterov’s Accelerated Method},
  author={Bin Hu and Laurent Lessard},
  journal   = {ICML},
  year      = {2017}
}

@ARTICLE{H20,
  title={Unifying the Analysis in Control and Optimization via Semidefinite Programs},
  author={Bin Hu},
  journal   = {Lecture Note},
  year      = {2020}
}


@ARTICLE{FSRV20,
  title={Conformal symplectic and relativistic optimization},
  author={Guilherme Franca and Jeremias Sulam and Daniel P Robinson and Rene Vidal},
  journal   = {Journal of Statistical Mechanics: Theory and Experiment},
  year      = {2020}
}

@ARTICLE{DJ19,
  title={Generalized Momentum-Based Methods: A Hamiltonian Perspective},
  author={Jelena Diakonikolas and Michael I. Jordan},
  journal   = {arXiv:1906.00436},
  year      = {2019}
}

@ARTICLE{WJR21,
  title={A Lyapunov analysis of momentum methods in optimization},
  author={Ashia C. Wilson and Michael Jordan and Benjamin Recht},
  journal   = {JMLR},
  year      = {2021}
}

@ARTICLE{CDO18,
  title={On Acceleration with Noise-Corrupted Gradients},
  author={Michael B. Cohen and Jelena Diakonikolas and Lorenzo Orecchia},
  journal   = {ICML},
  year      = {2018}
}

@ARTICLE{MJ20,
  title={Optimization with momentum: Dynamical, controltheoretic,
and symplectic perspectives},
  author={Michael Muehlebach and Michael I Jordan},
  journal   = {arXiv:2002.12493},
  year      = {2020}
}

@article{wibisono2016variational,
  title={A variational perspective on accelerated methods in optimization},
  author={Wibisono, Andre and Wilson, Ashia C and Jordan, Michael I},
  journal={Proceedings of the National Academy of Sciences},
  volume={113},
  number={47},
  pages={E7351--E7358},
  year={2016},
  publisher={National Acad Sciences}
}


@ARTICLE{LB18,
  title={Parametrized Accelerated Methods Free of Condition Number},
  author={Chaoyue Liu and Mikhail Belkin},
  journal   = {arXiv:1802.10235},
  year      = {2018}
}


@ARTICLE{DKB18,
  title={Non-monotone Behavior of the Heavy Ball Method},
  author={Marina Danilova and Anastasiya Kulakova and Boris Polyak},
  journal   = {arXiv:1811.00658},
  year      = {2018}
}


@ARTICLE{F18,
  title={Matrix Norms and Spectral Radii},
  author={Simon Foucart},
  journal   = {Online lecture note},
  year      = {2018}
}


@ARTICLE{wiki20,
  title={Spectral radius},
  author={Wikipedia},
  url    = "https://en.wikipedia.org/wiki/Spectral_radius",
  year      = {2020}
}
    
@ARTICLE{SDJS18,
  title={Understanding the Acceleration Phenomenon via High-Resolution Differential Equations},
  author={Bin Shi and Simon S. Du and Michael I. Jordan and Weijie J. Su},
  journal   = {arXiv:1810.08907},
  year      = {2018}
}


@ARTICLE{G41,
  title={Normierte ringe},
  author={I. Gelfand},
  journal   = {Mat. Sbornik},
  year      = {1941}
}

@ARTICLE{M19,
  title={Accelerated Methods - Polyak’s Momentum (Heavy Ball Method)},
  author={Ioannis Mitliagkas},
  journal   = {Online Lecture Note},
  year      = {2019}
}


@ARTICLE{SHL18,
  title={Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced},
  author={Simon S. Du and Wei Hu and Jason D. Lee},
  journal   = {NeurIPS},
  year      = {2018}
} 

@ARTICLE{MGWLSS20,
  title={Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy},
  author={Edward Moroshko and Suriya Gunasekar and Blake Woodworth and Jason D. Lee and Nathan Srebro and Daniel Soudry},
  journal   = {NeurIPS},
  year      = {2020}
} 

@ARTICLE{GWBNS17,
  title={Implicit Regularization in Matrix Factorization},
  author={Suriya Gunasekar and Blake Woodworth and Srinadh Bhojanapalli and Behnam Neyshabur and Nathan Srebro},
  journal   = {NeurIPS},
  year      = {2017}
}

@ARTICLE{JT19,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ziwei Ji and Matus Telgarsky},
  journal   = {ICLR},
  year      = {2019}
} 

@article{LMZ18,
 author = {Yuanzhi Li and Tengyu Ma and Hongyang Zhang},
 title = {Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations},
 journal = {COLT},
 year = {2018}
} 

@ARTICLE{RC20,
  title={Implicit regularization in deep learning may not be explainable by
norms},
  author={Noam Razin and Nadav Cohen},
  journal   = {NeurIPS2020},
  year      = {2020}
} 

@ARTICLE{ACHL19,
  title={Implicit Regularization in Deep Matrix Factorization},
  author={Sanjeev Arora and Nadav Cohen and Wei Hu and Yuping Luo},
  journal   = {NerurIPS},
  year      = {2019}
}


@ARTICLE{GBL19,
  title={Implicit Regularization of Discrete Gradient Dynamics in Linear Neural Networks},
  author={Gauthier Gidel and Francis Bach and Simon Lacoste-Julien},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{BHL18,
  title={Exponential convergence time of gradient descent for one-dimensional deep linear neural networks},
  author={Ohad Shamir},
  journal   = {COLT},
  year      = {2019}
} 

@ARTICLE{WWM19,
  title={Global Convergence of Gradient Descent for Deep Linear Residual Networks},
  author={Lei Wu and Qingcan Wang and Chao Ma},
  journal   = {NeurIPS},
  year      = {2019}
} 

@ARTICLE{SMG14,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Andrew M Saxe and James L McClelland and Surya Ganguli},
  journal   = {ICLR},
  year      = {2014}
} 


@ARTICLE{K16,
  title={Deep learning without poor local minima},
  author={Kenji Kawaguchi},
  journal   = {NeurIPS},
  year      = {2016}
} 

@ARTICLE{HM16,
  title={Identity matters in deep learning},
  author={Moritz Hardt and Tengyu Ma},
  journal   = {ICLR},
  year      = {2016}
} 

@ARTICLE{LK17,
  title={Depth creates no bad local minima},
  author={Haihao Lu and Kenji Kawaguchi},
  journal   = {arXiv:1702.08580},
  year      = {2017}
} 

@ARTICLE{YSJ17,
  title={Global optimality conditions for deep neural networks},
  author={Chulhee Yun and Suvrit Sra and Ali Jadbabaie},
  journal   = {ICLR},
  year      = {2018}
} 

@ARTICLE{ZL18,
  title={Critical points of linear neural networks: Analytical forms and landscape},
  author={Yi Zhou and Yingbin Liang},
  journal   = {ICLR},
  year      = {2018}
} 

@ARTICLE{LvB18,
  title={Deep linear networks with arbitrary loss: All local minima
are global},
  author={Thomas Laurent and James von Brecht},
  journal   = {ICML},
  year      = {2018}
} 


@ARTICLE{BHL18,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks},
  author={Peter L. Bartlett and David P. Helmbold and Philip M. Long},
  journal   = {ICML},
  year      = {2018}
} 


@ARTICLE{DH19,
  title={Width Provably Matters in Optimization for Deep Linear Neural Networks},
  author={Simon S. Du and Wei Hu},
  journal   = {ICML},
  year      = {2019}
} 

@ARTICLE{HXP20,
  title={Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks},
  author={Wei Hu and Lechao Xiao and Jeffrey Pennington},
  journal   = {ICLR},
  year      = {2020}
} 


@ARTICLE{MGWLSS20,
  title={Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy},
  author={Edward Moroshko and Suriya Gunasekar and Blake Woodworth and Jason D. Lee and Nathan Srebro and Daniel Soudry},
  journal   = {arXiv:2007.06738},
  year      = {2020}
} 



@ARTICLE{ADR20,
  title={Convergence rates of the Heavy-Ball method with Lojasiewicz property},
  author={Jean-Francois Aujol and Charles Dossal and Aude Rondepierre},
  journal   = {hal-02928958},
  year      = {2020}
} 

@ARTICLE{P63,
  title={Gradient methods for minimizing functionals},
  author={Boris Polyak},
  journal   = {Zhurnal Vychislitel’noi Matematiki i Matematicheskoi Fiziki},
  year      = {1963}
} 


@ARTICLE{LZB20b,
  title={Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning},
  author={Chaoyue Liu and Libin Zhu and Mikhail Belkin},
  journal   = {arXiv:2003.00307},
  year      = {2020}
} 

@ARTICLE{SYS20,
  title={The Effects of Mild Over-parameterization on the Optimization
Landscape of Shallow ReLU Neural Networks},
  author={Itay Safran and Gilad Yehudai and Ohad Shamir},
  journal   = {arXiv:2006.01005},
  year      = {2020}
} 


@ARTICLE{LZB20a,
  title={On the linearity of large non-linear models: when and why the tangent kernel is constant},
  author={Chaoyue Liu and Libin Zhu and Mikhail Belkin},
  journal   = {arXiv:2010.01092},
  year      = {2020}
} 


@ARTICLE{MGWLSS20,
  title={Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy},
  author={Edward Moroshko and Suriya Gunasekar and Blake Woodworth and Jason D. Lee and Nathan Srebro and Daniel Soudry},
  journal   = {arXiv:2007.06738},
  year      = {2020}
} 



@ARTICLE{MGWLSS20,
  title={Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy},
  author={Edward Moroshko and Suriya Gunasekar and Blake Woodworth and Jason D. Lee and Nathan Srebro and Daniel Soudry},
  journal   = {arXiv:2007.06738},
  year      = {2020}
} 


@ARTICLE{BM19,
  title={On the Inductive Bias of Neural Tangent Kernels},
  author={Alberto Bietti and Julien Mairal},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{AMMC20,
  title={A new regret analysis for Adam-type algorithms},
  author={Ahmet Alacaoglu and Yura Malitsky and Panayotis Mertikopoulos and Volkan Cevher},
  journal   = {ICML},
  year      = {2020}
}

@ARTICLE{NB15,
  title={From Averaging to Acceleration, There is Only a
Step-size},
  author={Nicolas Flammarion and Francis Bach},
  journal   = {COLT},
  year      = {2015}
}


@ARTICLE{MPTDD18,
  title={Hamiltonian Descent Methods},
  author={Chris J. Maddison and Daniel Paulin and Yee Whye Teh and Brendan O’Donoghue and Arnaud Doucet},
  journal   = {arXiv:1809.05042},
  year      = {2018}
}

@ARTICLE{DJ19,
  title={Generalized Momentum-Based Methods: A Hamiltonian Perspective},
  author={Jelena Diakonikolas and Michael I. Jordan},
  journal   = {arXiv:1906.00436},
  year      = {2019}
}


@ARTICLE{HWTZ20,
  title={Why Do Deep Residual Networks Generalize Better than Deep Feedforward Networks? — A Neural Tangent Kernel Perspective},
  author={Kaixuan Huang and Yuqing Wang and Molei Tao and Tuo Zhao},
  journal   = {arXiv:2002.06262},
  year      = {2020}
}


@ARTICLE{LSSWY20,
  title={Generalized Leverage Score Sampling for Neural Networks},
  author={Jason D. Lee and Ruoqi Shen and Zhao Song and Mengdi Wang and Zheng Yu},
  journal   = {arXiv:2009.09829},
  year      = {2020}
}

@ARTICLE{ACH18,
  title={On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization},
  author={Sanjeev Arora and Nadav Cohen and Elad Hazan},
  journal   = {ICML},
  year      = {2018}
}


@ARTICLE{Y19,
  title={Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation},
  author={Greg Yang},
  journal   = {arXiv:1902.04760},
  year      = {2019}
}


@ARTICLE{OS19,
  title={Towards moderate overparameterization: global convergence
guarantees for training shallow neural networks},
  author={Samet Oymak and Mahdi Soltanolkotabi},
  journal   = {arXiv:1902.04674},
  year      = {2019}
}


@ARTICLE{SY19,
  title={On Learning Over-parameterized Neural Networks: A Functional Approximation Perspective},
  author={Lili Su and Pengkun Yang},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{SY19,
  title={On Learning Over-parameterized Neural Networks: A Functional Approximation Perspective},
  author={Lili Su and Pengkun Yang},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{CK20,
  title={Understanding Accelerated Stochastic Gradient Descent via the Growth Condition},
  author={You-Lin Chen and Mladen Kolar},
  journal   = {arXiv:2006.06782},
  year      = {2020}
}

@ARTICLE{SGD20,
  title={On the convergence of the Stochastic Heavy Ball Method},
  author={Othmane Sebbouh and Robert M. Gower and Aaron Defazio},
  journal   = {arXiv:2006.07867},
  year      = {2020}
}

@ARTICLE{KCH20,
  title={Global Convergence of Second-order Dynamics in Two-layer Neural Networks},
  author={Walid Krichene and Kenneth F. Caluyay and Abhishek Halder},
  journal   = {arXiv:2006.07867},
  year      = {2020}
}


@ARTICLE{HSS20,
  title={Near-Optimal Methods for Minimizing Star-Convex Functions and Beyond},
  author={Oliver Hinder and Aaron Sidford and Nimit Sohoni},
  journal   = {COLT},
  year      = {2020}
}

@ARTICLE{Getal20,
  title={Understanding Accelerated Stochastic Gradient Descent via the Growth Condition},
  author={Eduard Gorbunov and Adel Bibi and Ozan Sener and El Houcine Bergou and Peter Richt{\'a}rik},
  journal   = {ICLR},
  year      = {2020}
}


@ARTICLE{Getal20,
  title={A Stochastic Derivative Free Optimization Method with Momentum},
  author={Eduard Gorbunov and Adel Bibi and Ozan Sener and El Houcine Bergou and Peter Richt{\'a}rik},
  journal   = {ICLR},
  year      = {2020}
}


@ARTICLE{HXAP20,
  title={The Surprising Simplicity of the Early-Time Learning Dynamics of Neural Networks},
  author={Wei Hu and Lechao Xiao and Ben Adlam and Jeffrey Pennington},
  journal   = {NeurIPS},
  year      = {2020}
}


@ARTICLE{NB19,
  title={SGD on Neural Networks Learns Functions of Increasing Complexity},
  author={Preetum Nakkiran and Gal Kaplun and Dimitris Kalimeris and Tristan Yang and Benjamin L. Edelman and Fred Zhang and Boaz Barak},
  journal   = {NeurIPS},
  year      = {2019}
}



@ARTICLE{FDZ19,
  title={Over parameterized two-level neural networks can learn near optimal feature representations},
  author={Cong Fang and Hanze Dong and Tong Zhang},
  journal   = {arXiv:1910.11508},
  year      = {2019}
}


@ARTICLE{BPSW20,
  title={Training (Overparametrized) Neural Networks in Near-Linear Time},
  author={Jan van den Brand and Binghui Peng and Zhao Song and Omri Weinstein},
  journal   = {arXiv:2006.11648},
  year      = {2020}
}


@ARTICLE{CO19,
  title={Momentum-Based Variance Reduction in Non-Convex SGD},
  author={Ashok Cutkosky and Francesco Orabona},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{LGY20,
  title={An Improved Analysis of Stochastic Gradient Descent with Momentum},
  author={Yanli Liu and Yuan Gao and Wotao Yin},
  journal   = {NeurIPS},
  year      = {2020}
}


@ARTICLE{LB20,
  title={Accelerating SGD with momentum for over-parameterized learning},
  author={Chaoyue Liu and Mikhail Belkin},
  journal   = {ICLR},
  year      = {2020}
}


@ARTICLE{LY17,
  title={Convergence Analysis of Two-layer Neural Networks with ReLU Activation},
  author={Yuanzhi Li and Yang Yuan},
  journal   = {NeurIPS},
  year      = {2017}
}


@ARTICLE{GBL19,
  title={Implicit Regularization of Discrete Gradient Dynamics in Linear Neural Networks},
  author={Gauthier Gidel and Francis Bach and Simon Lacoste-Julien},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{GLSS18,
  title={Implicit Bias of Gradient Descent on Linear Convolutional Networks},
  author={Suriya Gunasekar and Jason D. Lee and Daniel Soudry and Nathan Srebro},
  journal   = {NeurIPS},
  year      = {2018}
}

@ARTICLE{GLSS18,
  title={Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced},
  author={Simon S. Du and Wei Hu and Jason D. Lee},
  journal   = {NeurIPS},
  year      = {2018}
}

@ARTICLE{WLLM19,
  title={Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel},
  author={Colin Wei and Jason D. Lee and Qiang Liu and Tengyu Ma},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{ACGH19,
  title={A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks},
  author={Sanjeev Arora and Nadav Cohen and Noah Golowich and Wei Hu},
  journal   = {ICLR},
  year      = {2019}
}

@ARTICLE{ACGH19,
  title={Convergence of Adversarial Training in Overparametrized Networks},
  author={Ruiqi Gao and Tianle Cai and Haochuan Li and Liwei Wang and Cho-Jui Hsieh and Jason D. Lee},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{WDW19,
  title={Global convergence of adaptive gradient methods for an
over-parameterized neural network},
  author={Xiaoxia Wu and Simon S Du and Rachel Ward},
  journal   = {arXiv:1902.07111},
  year      = {2019}
}


@ARTICLE{WDS19,
  title={Learning distributions generated by one-layer relu networks},
  author={Shanshan Wu and Alexandros G Dimakis and Sujay Sanghavi},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{ACGH19,
  title={SGD Learns One-Layer Networks in WGANs},
  author={Qi Lei and Jason D. Lee and Alex Dimakis and Costis Daskalakis},
  journal   = {ICML},
  year      = {2020}
}

@ARTICLE{Cetal19,
  title={A gram-gauss-newton method learning overparameterized deep neural networks for regression problems},
  author={Tianle Cai and Ruiqi Gao and Jikai Hou and Siyu Chen and Dong Wang and Di He and Zhihua Zhang and Liwei Wang},
  journal   = {arXiv.org:1905.11675},
  year      = {2019}
}

@ARTICLE{GML18,
  title={Learning One-hidden-layer Neural Networks with Landscape Design},
  author={Rong Ge and Tengyu Ma and Jason D. Lee},
  journal   = {ICLR},
  year      = {2018}
}

@ARTICLE{OS19,
  title={Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?},
  author={Samet Oymak and Mahdi Soltanolkotabi},
  journal   = {ICML},
  year      = {2019}
}

@ARTICLE{ZLG20,
  title={On the global convergence of training deep linear resnets},
  author={Difan Zou and Philip M. Long and Quanquan Gu},
  journal   = {ICLR},
  year      = {2020}
}

@ARTICLE{DGM20,
  title={Optimization Theory for ReLU Neural Networks Trained with Normalization Layers},
  author={Yonatan Dukler and Quanquan Gu and Guido Montufar},
  journal   = {ICML},
  year      = {2020}
}


@ARTICLE{PSG2020,
  title={Effect of Activation Functions on the Training of Overparametrized Neural Nets},
  author={Abhishek Panigrahi and Abhishek Shetty and Navin Goyal},
  journal   = {ICLR},
  year      = {2020}
}


@ARTICLE{P87,
  title={Introduction to optimization},
  author={Boris T. Polyak},
  journal   = {Optimization Software},
  year      = {1987}
}

@ARTICLE{Dan17,
  title={SGD Learns the Conjugate Kernel Class of the Network},
  author={Amit Daniely},
  journal   = {NeurIPS},
  year      = {2017}
}


@ARTICLE{JN15,
  title={The hierarchy of local minimums in polynomial optimization.},
  author={Jiawang Nie},
  journal   = {Mathematical programming},
  year      = {2015}
}

@ARTICLE{MK87,
  title={Some np-complete problems in quadratic and nonlinear programming},
  author={Katta G Murty and Santosh N Kabadi},
  journal   = {Mathematical programming},
  year      = {1987}
}

@ARTICLE{N00,
  title={Squared functional systems and optimization problems},
  author={Yurii Nesterov},
  journal   = {High performance optimization, Springer},
  year      = {2000}
}

@ARTICLE{AG16,
  title={Efficient approaches for escaping higher order saddle points in non-convex optimization},
  author={Anima Anandkumar and Rong Ge},
  journal   = {COLT},
  year      = {2016}
}

@ARTICLE{LR18,
  title={Accelerated Gossip via Stochastic Heavy Ball Method},
  author={Nicolas Loizou and Peter Richt{\'a}rik},
  journal   = {Allerton},
  year      = {2018}
}

@article{CGZ19,
 author = {Bugra Can and Mert G{\"u}rb{\"u}zbalaban and Lingjiong Zhu},
 title = {Accelerated Linear Convergence of Stochastic Momentum Methods in Wasserstein Distances},
 journal = {ICML},
 year = {2019}
} 



@article{SQW16,
 author = {Ju Sun and Qing Qu and John Wright},
 title = {A Geometrical Analysis of Phase Retrieval},
 journal = {International Symposium on Information Theory},
 year = {2016}
} 


@article{SQW16,
 author = {Ju Sun and Qing Qu and John Wright},
 title = {A Geometrical Analysis of Phase Retrieval},
 journal = {International Symposium on Information Theory},
 year = {2016}
} 

@article{SQW15,
 author = {Ju Sun and Qing Qu and John Wright},
 title = {When Are Nonconvex Problems Not Scary?},
 journal = {NeurIPS Workshop on Non-convex Optimization for Machine Learning: Theory and Practice},
 year = {2015}
} 


@article{SECCMS15,
 author = {Yoav Shechtman and Yonina C. Eldar and Oren Cohen and Henry Nicholas Chapman 
 and Jianwei Miao and Mordechai Segev},
 title = {Phase retrieval with application to optical imaging: a contemporary overview.},
 journal = {IEEE signal processing magazine},
 year = {2015}
} 

@article{CESV12,
 author = {Emmanuel J. Cand{\'e}s and Yonina Eldar and Thomas Strohmer and Vlad Voroninski},
 title = {Phase retrieval via matrix completion.},
 journal = {SIAM Journal on Imaging Sciences},
 year = {2013}
} 

@article{CCFMY18,
 author = {Yuxin Chen and Yuejie Chi and Jianqing Fan and Cong Ma and Yuling Yan},
 title = {Gradient Descent with Random Initialization: Fast Global Convergence for Nonconvex Phase Retrieval},
 journal = {Mathematical Programming},
 year = {2018}
} 


@article{BHK15,
 author = {Avrim Blum and John Hopcroft and Ravindran Kannan},
 title = {Foundations of Data Science},
 year = {2015}
} 


@article{Curtis17,
 author = {Curtis, Frank E. and Robinson, Daniel P. and Samadi, Mohammadreza},
 title = {A Trust Region Algorithm with a Worst-case Iteration Complexity of $O(\epsilon^{-3/2})$ for Nonconvex Optimization},
 journal = {Mathematical Programming},
 year = {2017},
} 



@ARTICLE{KSH12,
  title={ImageNet Classification with Deep Convolutional Neural Networks},
  author={Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
  journal={NeurIPS},
  year={2012}
}


@ARTICLE{Rnet16,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  journal={Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016}
}

@ARTICLE{HHS17,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Elad Hoffer and Itay Hubara and Daniel Soudry},
  journal   = {NeurIPS},
  year      = {2017}
}


@article{silver2017,
  author = {David Silver and Julian Schrittwieser and Karen Simonyan and et al.},
  title = {Mastering the game of Go without human knowledge},
  journal = {Nature},
  year = {2017}
}

@ARTICLE{attention17,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               et al.},
  title     = {Attention is All you Need},
  journal = {{NeurIPS}},
  year      = {2017}
}


@ARTICLE{Baidu16,
  title =    {Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin},
  author =   {Dario Amodei and Sundaram Ananthanarayanan and Rishita Anubhai and et al.},
  journal = {ICML},
  year =   {2016}
}



@ARTICLE{KB15,
  title = {Adam: A Method for Stochastic Optimization},
  author    = {Diederik P. Kingma and Jimmy Ba},
  journal = {ICLR},
  year = {2015}
}

@ARTICLE{RKK18,
  title={On the Convergence of Adam and Beyond },
  author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
  journal={ICLR},
  year={2018}
}

@ARTICLE{KH1918,
  title={Decoupled Weight Decay Regularization},
  author={Ilya Loshchilov and Frank Hutter},
  journal   = {ICLR},
  year      = {2019}
}

@article{goh2017why,
  author = {Gabriel Goh},
  title = {Why Momentum Really Works},
  journal = {Distill},
  year = {2017}
}

@article{G17,
  author = {Xavier Gastaldi},
  title = {Shake-Shake regularization},
  journal = {arXiv:1705.07485},
  year = {2017}
}

@article{WRSSR17,
  author = {Ashia C Wilson and Rebecca Roelofs and Mitchell Stern and Nathan Srebro and and Benjamin Recht.},
  title = {The marginal value of adaptive gradient methods in machine learning},
  journal = {NeurIPS},
  year = {2017}
}

@article{CZMVL18,
  author = {Ekin D Cubuk and Barret Zoph and Dandelion Mane and Vijay Vasudevan and Quoc V Le.},
  title = {Autoaugment: Learning augmentation policies from data.},
  journal = {arXiv:1805.09501},
  year = {2018}
}



@ARTICLE{YLL18,
  title={Unified Convergence Analysis of Stochastic Momentum Methods for Convex and Non-convex Optimization},
  author={Tianbao Yang and Qihang Lin and Zhe Li},
  journal   = {IJCAI},
  year      = {2018}
}

@ARTICLE{P64,
  title={Some methods of speeding up the convergence of iteration methods},
  author={B.T. Polyak},
  journal   = {USSR Computational Mathematics and Mathematical Physics},
  year      = {1964}
}

@ARTICLE{GPS16,
  title={Stochastic Heavy Ball},
  author={S{\'e}bastien Gadat and Fabien Panloup and Sofiane Saadane},
  journal   = {arXiv:1609.04228},
  year      = {2016}
}

@ARTICLE{GFJ15,
  title={Global convergence of the Heavy-ball method for convex optimization},
  author={Euhanna Ghadimi and Hamid Reza Feyzmahdavian and Mikael Johansson},
  journal   = {ECC},
  year      = {2015}
}


@ARTICLE{LR17,
  title={Momentum and stochastic momentum for stochastic gradient, newton,
  proximal point and subspace descent methods},
  author={Nicolas Loizou and Peter Richt{\'a}rik},
  journal   = {arXiv:1712.09677},
  year      = {2017}
}

@ARTICLE{SYLHGJ19,
  title={Non-ergodic Convergence Analysis of Heavy-Ball Algorithms},
  author={Tao Sun and Penghang Yin and Dongsheng Li and Chun Huang and Lei Guan and Hao Jiang},
  journal   = {AAAI},
  year      = {2019}
}

@ARTICLE{OCBP14,
  title={ipiano: Inertial proximal algorithm for nonconvex optimization},
  author={Peter Ochs and Yunjin Chen and Thomas Brox and Thomas Pock},
  journal   = {SIAM Journal of Imaging Sciences},
  year      = {2014}
}



@ARTICLE{GLZ16,
  title={Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization},
  author={Saeed Ghadimi and Guanghui Lan and Hongchao Zhang},
  journal   = {Mathematical Programming},
  year      = {2016}
}


@ARTICLE{LJCJ17,
  title={Nonconvex finite-sum optimization via scsg methods},
  author={Lihua Lei and Cheng Ju and Jianbo Chen and Michael I. Jordan},
  journal   = {NeurIPS},
  year      = {2017}
}


@ARTICLE{YXG18,
  title={Stochastic nested variance reduced gradient descent for nonconvex optimization},
  author={Yaodong Yu and Pan Xu and Quanquan Gu},
  journal   = {NeurIPS},
  year      = {2018}
}


@ARTICLE{CHMAL15,
  title={The Loss Surfaces of Multilayer Networks},
  author={Anna Choromanska and Mikael Henaff and Michael Mathieu and G{\'e}rard Ben Arous and Yann LeCun},
  journal   = {AISTAT},
  year      = {2015}
}



@ARTICLE{dauphin14,
  title={Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
  author={Yann Dauphin and Razvan Pascanu and Caglar Gulcehre and Kyunghyun Cho and Surya Ganguli and Yoshua Bengio},
  journal   = {NeurIPS},
  year      = {2014}
}


@ARTICLE{TSJRJ18,
  title={Stochastic cubic regularization for fast nonconvex optimization},
  author={Nilesh Tripuraneni and Mitchell Stern and Chi Jin and Jeffrey Regier and Michael I Jordan},
  journal   = {NeurIPS},
  year      = {2018}
}


@ARTICLE{FLLZ18,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Cong Fang and Chris Junchi Li and Zhouchen Lin and Tong Zhang.},
  journal   = {NeurIPS},
  year      = {2018}
}


@ARTICLE{XRY18,
  title={First-order stochastic algorithms for escaping from saddle points in almost linear time},
  author={Yi Xu and Jing Rong and Tianbao Yang},
  journal   = {NeurIPS},
  year      = {2018}
}


@ARTICLE{RZSPBSS18,
  title={A generic approach for escaping saddle points},
  author={Sashank Reddi and Manzil Zaheer and Suvrit Sra and Barnabas Poczos and Francis Bach and Ruslan Salakhutdinov and Alex Smola},
  journal   = {AISTATS},
  year      = {2018}
}


@ARTICLE{MOJ16,
  title={Escaping saddle points in constrained optimization},
  author={Aryan Mokhtari and Asuman Ozdaglar and Ali Jadbabaie},
  journal   = {NeurIPS},
  year      = {2018}
}


@ARTICLE{KL16,
  title={The power of normalization: Faster evasion of saddle points},
  author={Kfir Y. Levy},
  journal   = {arXiv:1611.04831},
  year      = {2016}
}


@ARTICLE{AL18,
  title={Neon2: Finding local minima via first-order oracles},
  author={Zeyuan Allen-Zhu and Yuanzhi Li},
  journal   = {NeurIPS},
  year      = {2018}
}


@ARTICLE{AABHM17,
  title={Finding approximate local minima faster than gradient descent},
  author={Naman Agarwal and Zeyuan Allen-Zhu and Brian Bullins and Elad Hazan and Tengyu Ma},
  journal   = {STOC},
  year      = {2017}
}


@ARTICLE{SRRKKS19,
  title={Escaping Saddle Points with Adaptive Gradient Methods},
  author={Matthew Staib and Sashank J. Reddi and Satyen Kale and Sanjiv Kumar and Suvrit Sra},
  journal   = {ICML},
  year      = {2019}
}

@ARTICLE{RSS12,
  title={Making Stochastic Gradient Descent Optimal for Strongly Convex Problems},
  author={Alexander Rakhlin and Ohad Shamir and Karthik Sridharan},
  journal   = {ICML},
  year      = {2012}
}


@ARTICLE{N13,
  title={Introductory lectures on convex optimization: a basic course},
  author={Yurii Nesterov},
  journal   = {Springer},
  year      = {2013}
}


@ARTICLE{NKJK18,
  title={On the insufficiency of existing momentum schemes for Stochastic Optimization},
  author={Rahul Kidambi and Praneeth Netrapalli and Prateek Jain and Sham M. Kakade},
  journal   = {ICLR},
  year      = {2018}
}


@ARTICLE{JNGKJ19,
  title={Stochastic Gradient Descent Escapes Saddle Points Efficiently},
  author={Chi Jin and Praneeth Netrapalli and Rong Ge and Sham M. Kakade and Michael I. Jordan},
  journal   = {arXiv:1902.04811},
  year      = {2019}
}


@ARTICLE{G11,
  title={Recovering low-rank matrices from few coefficients in any basis},
  author={David Gross},
  journal   = {IEEE Transactions on Information Theory},
  year      = {2011}
}


@ARTICLE{KL17,
  title={Sub-sampled Cubic Regularization for Non-convex Optimization},
  author={Jonas Moritz Kohler and Aurelien Lucchi},
  journal   = {ICML},
  year      = {2017}
}


@ARTICLE{LPPSJR19,
  title={First-order methods almost always avoid strict saddle-points},
  author={Jason D. Lee and Ioannis Panageas and Georgios Piliouras and Max Simchowitz and Michael I. Jordan and Benjamin Recht},
  journal   = {Mathematical Programming, Series B},
  year      = {2019}
}


@ARTICLE{GHJY15,
  title={Escaping From Saddle Points --- Online Stochastic Gradient for Tensor Decomposition},
  author={Rong Ge and Furong Huang and Chi Jin and Yang Yuan},
  journal   = {COLT},
  year      = {2015}
}



@ARTICLE{DJLJPS18,
  title={Gradient Descent Can Take Exponential Time to Escape Saddle Points},
  author={Simon S. Du and Chi Jin and Jason D. Lee and Michael I. Jordan and Barnabas Poczos and Aarti Singh},
  journal   = {NeurIPS},
  year      = {2017}
}



@ARTICLE{CNJ18,
  title={Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent},
  author={Chi Jin and Praneeth Netrapalli and Michael I. Jordan},
  journal   = {COLT},
  year      = {2018}
}


@ARTICLE{TSJRJ18,
  title={Stochastic Cubic Regularization for Fast Nonconvex Optimization},
  author={Nilesh Tripuraneni and Mitchell Stern and Chi Jin and Jeffrey Regier and Michael I. Jordan},
  journal   = {NeurIPS},
  year      = {2018}
}

@ARTICLE{JGNKJ17,
  title={How to Escape Saddle Points Efficiently},
  author={Chi Jin and Rong Ge and Praneeth Netrapalli and Sham M. Kakade and Michael I. Jordan},
  journal   = {ICML},
  year      = {2017}
}


@ARTICLE{FLZCOLT19,
  title={Sharp Analysis for Nonconvex SGD Escaping from Saddle Points},
  author={Cong Fang and Zhouchen Lin and Tong Zhang},
  journal   = {COLT},
  year      = {2019}
}

@ARTICLE{DKLH18,
  title={Escaping Saddles with Stochastic Gradients},
  author={Hadi Daneshmand and Jonas Kohler and Aurelien Lucchi and Thomas Hofmann},
  journal   = {ICML},
  year      = {2018}
}

@ARTICLE{CDHS18,
  title={Accelerated Methods for NonConvex Optimization},
  author={Yair Carmon and John Duchi and Oliver Hinder and Aaron Sidford},
  journal   = {SIAM Journal of Optimization},
  year      = {2018}
}


@ARTICLE{SMDH13,
  title={On the importance of initialization and momentum in deep learning},
  author={Ilya Sutskever and James Martens and George Dahl and Geoffrey Hinton},
  journal   = {ICML},
  year      = {2013}
}


@ARTICLE{JKBFBS19,
  title={On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length},
  author={Stanislaw Jastrzebski and Zachary Kenton and Nicolas Ballas and Asja Fischer and Yoshua Bengio and Amost Storkey},
  journal   = {ICLR},
  year      = {2019}
}

@ARTICLE{WKXS18,
  title={Identifying Generalization Properties in Neural Networks},
  author={Huan Wang and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},
  journal   = {arXiv:1809.07402},
  year      = {2018}
}


@ARTICLE{TSS19,
  title={Normalized Flat Minima: Exploring Scale Invariant Definition of Flat Minima for Neural Networks using PAC-Bayesian Analysis},
  author={Yusuke Tsuzuku and Issei Sato and Masashi Sugiyama},
  journal   = {arXiv:1901.04653},
  year      = {2019}
}


@ARTICLE{entropysgd17,
  title={Entropy-SGD: Biasing Gradient Descent Into Wide Valleys},
  author={Pratik Chaudhari and Anna Choromanska and Stefano Soatto and Yann LeCun and Carlo Baldassi and Christian Borgs and Jennifer Chayes and Levent Sagun and Riccardo Zecchina},
  journal   = {ICLR},
  year      = {2017}
}


@ARTICLE{KMNST17,
  title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
  author={Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang},
  journal   = {ICLR},
  year      = {2017}
}



@ARTICLE{DPBB17,
  title={Sharp Minima Can Generalize For Deep Nets},
  author={Laurent Dinh and Razvan Pascanu and Samy Bengio and Yoshua Bengio},
  journal   = {ICML},
  year      = {2017}
}


@ARTICLE{ZBHRV17,
  title={Understanding deep learning requires rethinking generalization},
  author={Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
  journal   = {ICLR},
  year      = {2017}
}

@ARTICLE{LXLS19,
  title={Adaptive Gradient Methods with Dynamic Bound of Learning Rate},
  author={Liangchen Luo and Yuanhao Xiong and Yan Liu and Xu Sun},
  journal   = {ICLR},
  year      = {2019}
}

@ARTICLE{CWLK19,
  title={Decaying momentum helps neural network training},
  author={John Chen and Cameron Wolfe and Zhao Li and Anastasios Kyrillidis},
  journal   = {arXiv:1910.04952},
  year      = {2019}
}

@ARTICLE{LCZZ18,
  title={Towards Understanding Nonconvex Stochastic Optimization with Momentum using Diffusion Approximations},
  author={Tianyi Liu and Zhehui Chen and Enlu Zhou and Tuo Zhao},
  journal   = {arXiv:1802.05155},
  year      = {2018}
}


@ARTICLE{SP20,
  title={Universal Average-Case Optimality of Polyak Momentum},
  author={Damien Scieur and Fabian Pedregosa},
  journal   = {ICML},
  year      = {2020}
}

@ARTICLE{JT20,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
  author={Ziwei Ji and Matus Telgarsky},
  journal   = {ICLR},
  year      = {2020}
}

@ARTICLE{LL18,
  title={Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data},
  author={Yuanzhi Li and Yingyu Liang},
  journal   = {NeurIPS},
  year      = {2018}
}



@ARTICLE{DZPS19,
  title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  author={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
  journal   = {ICLR},
  year      = {2019}
}

@ARTICLE{DLLWZ16,
  title={Gradient descent finds global minima of deep neural networks},
  author={Simon S Du and Jason D Lee and Haochuan Li and Liwei Wang and Xiyu Zhai},
  journal   = {ICML},
  year      = {2019}
}



@ARTICLE{ZL19_icml,
  title={A convergence theory for deep learning via overparameterization},
  author={Zeyuan Allen-Zhu and Yuanzhi Li and Zhao Song},
  journal   = {ICML},
  year      = {2019}
}

@ARTICLE{ZY19,
  title={Quadratic suffices for over-parametrization via matrix chernoff bound},
  author={Zhao Song and Xin Yang},
  journal   = {arXiv:1906.03593},
  year      = {2019}
}

@ARTICLE{ZCZG19,
  title={Stochastic gradient descent optimizes overparameterized deep relu networks},
  author={Difan Zou and Yuan Cao and Dongruo Zhou and Quanquan Gu},
  journal   = {Machine Learning, Springer},
  year      = {2019}
}


@ARTICLE{ADHLSW19_nips,
  title={On exact computation with an infinitely wide neural net},
  author={Sanjeev Arora and Simon S Du and Wei Hu and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{ADHLSW19_icml,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Sanjeev Arora and Simon S Du and Wei Hu and Zhiyuan Li and Ruosong Wang},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{JGH18,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Arthur Jacot and Franck Gabriel and Clement Hongler},
  journal   = {NeurIPS},
  year      = {2018}
}

@ARTICLE{LXSBSP19,
  title={Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent},
  author={Jaehoon Lee and Lechao Xiao and Samuel S. Schoenholz and Yasaman Bahri and Jascha Sohl-Dickstein and Jeffrey Pennington},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{COB19,
  title={On lazy training in differentiable programming},
  author={Lenaic Chizat and Edouard Oyallon and Francis Bach},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{AS20,
  title={The Numerics of Phase Retrieval},
  author={Albert Fannjiang and Thomas Strohmer},
  journal   = {Acta Numerica},
  year      = {2020}
}


@ARTICLE{GMMM19,
  title={Limitations of Lazy Training of Two-layers Neural Networks},
  author={Behrooz Ghorbani and Song Mei and Theodor Misiakiewicz and Andrea Montanari},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{BHK18,
  title={Foundations of Data Science},
  author={Avrim Blum and John Hopcroft and Ravindran Kannan},
  journal   = {Neural computation},
  year      = {2018}
}


@ARTICLE{BHL19,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks},
  author={Peter L Bartlett and David P Helmbold and Philip M Long},
  journal   = {Neural computation},
  year      = {2019}
}

@ARTICLE{MS18,
  title={Notes on first-order methods for minimizing smooth functions},
  author={Michael Saunders},
  journal   = {Lecture note},
  year      = {2018}
}

@ARTICLE{A18,
  title={Heavy ball method on convex quadratic problem},
  author={Andersen Ang},
  journal   = {Lecture note},
  year      = {2018}
}

@ARTICLE{R18,
  title={Lyapunov analysis and the Heavy Ball Method},
  author={Benjamin Recht},
  journal   = {Lecture note},
  year      = {2018}
}


@ARTICLE{S14,
  title={Algorithms and Theory for Clustering and Nonconvex Quadratic Programming},
  author={Mahdi Soltanolkotabi},
  journal   = {Stanford University Ph. D. Dissertation},
  year      = {2014}
}



@ARTICLE{D20,
  title={Memorizing Gaussians with no over-parameterizaion via gradient decent on neural networks},
  author={Amit Daniely},
  journal   = {arXiv:1909.11837},
  year      = {2020}
}


@ARTICLE{GWZ19,
  title={Mildly overparametrized neural nets can memorize training data efficiently},
  author={Rong Ge and Runzhe Wang and Haoyu Zhao},
  journal   = {arXiv:1909.11837},
  year      = {2019}
}

@ARTICLE{GKZ19,
  title={Stationary Points of Shallow Neural Networks with Quadratic Activation Function},
  author={David Gamarnik and Eren C. Kızıldag and Ilias Zadik},
  journal   = {arXiv:1912.01599},
  year      = {2019}
}

@ARTICLE{SJL18,
  title={Theoretical insights into the optimization landscape of over-parameterized shallow neural networks},
  author={Mahdi Soltanolkotabi and Adel Javanmard and Jason D. Lee},
  journal   = {IEEE Transactions on Information Theory},
  year      = {2018}
}


@ARTICLE{DL18,
  title={On the Power of Over-parametrization in Neural Networks with Quadratic Activation},
  author={Simon Du and Jason Lee},
  journal   = {ICML},
  year      = {2018}
}

@ARTICLE{LMCC19,
  title={Nonconvex Matrix Factorization from Rank-One Measurements},
  author={Yuanxin Li and Cong Ma and Yuxin Chen and Yuejie Chi},
  journal   = {AISTATS},
  year      = {2019}
}

@article{LMZ18,
 author = {Yuanzhi Li and Tengyu Ma and Hongyang Zhang},
 title = {Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations},
 journal = {COLT},
 year = {2018}
} 

 
@ARTICLE{BG17,
  title={Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs},
  author={Alon Brutzkus and Amir Globerson},
  journal   = {ICML},
  year      = {2017}
}

@ARTICLE{BL20,
  title={Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks},
  author={Yu Bai and Jason D. Lee},
  journal   = {ICLR},
  year      = {2020}
}

@ARTICLE{ZG19,
  title={An Improved Analysis of Training Over-parameterized Deep Neural Networks},
  author={Difan Zou and Quanquan Gu},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{BSMM19,
  title={Linearized two-layers neural networks in high dimension},
  author={Behrooz Ghorbani and Song Mei and Theodor Misiakiewicz and and Andrea Montanari},
  journal   = {arXiv:1904.12191},
  year      = {2019}
}

@ARTICLE{LMZ20,
  title={Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK},
  author={Yuanzhi Li and Tengyu Ma and Hongyang Zhang},
  journal   = {COLT},
  year      = {2020}
}


@ARTICLE{ZMG19,
  title={Fast convergence of natural gradient descent for over-parameterized neural networks},
  author={Guodong Zhang and James Martens and Roger B Grosse},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{HN20,
  title={Finite Depth and Width Corrections to the Neural Tangent Kernel},
  author={Boris Hanin and Mihai Nica},
  journal   = {ICLR},
  year      = {2020}
}


@ARTICLE{S17,
  title={Learning Relus via Gradient Descent},
  author={Mahdi Soltanolkotabi},
  journal   = {NeurIPS},
  year      = {2017}
}

@ARTICLE{KSA19,
  title={Fitting relus via sgd and quantized sgd},
  author={Seyed Mohammadreza Mousavi Kalan and Mahdi Soltanolkotabi and A. Salman Avestimehr},
  journal   = {ISIT},
  year      = {2019}
}

@ARTICLE{MBM17,
  title={The landscape of empirical risk for non-convex losses},
  author={Song Mei and Yu Bai and Andrea Montanari},
  journal   = {PNAS},
  year      = {2018}
}

@ARTICLE{T17,
  title={An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis},
  author={Yuandong Tian},
  journal   = {ICML},
  year      = {2017}
}


@ARTICLE{GKM19,
  title={Time/Accuracy Tradeoffs for Learning a ReLU with respect to Gaussian Marginals},
  author={Surbhi Goel and Adam Klivans and Raghu Meka},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{GKM18,
  title={Learning One Convolutional Layer with Overlapping Patches},
  author={Surbhi Goel and Adam Klivans and Raghu Meka},
  journal   = {ICML},
  year      = {2018}
}


@ARTICLE{GKLW16,
  title={Learning Two-layer Neural Networks with Symmetric Inputs},
  author={Rong Ge and Rohith Kuditipudi and Zhize Li and Xiang Wang},
  journal   = {ICLR},
  year      = {2019}
}

@ARTICLE{JSA16,
  title={Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods},
  author={Majid Janzamin and Hanie Sedghi and Anima Anandkumar},
  journal   = {arXiv:1506.08473},
  year      = {2016}
}

@ARTICLE{KKSK11,
  title={Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression?},
  author={Sham M. Kakade and Varun Kanade and Ohad Shamir and Adam Kalai},
  journal   = {NeurIPS},
  year      = {2011}
}


@ARTICLE{DLT18,
  title={When is a Convolutional Filter Easy To Learn?},
  author={Simon S. Du and Jason D. Lee and Yuandong Tian},
  journal   = {ICLR},
  year      = {2018}
}

@ARTICLE{GKKT17,
  title={Reliably Learning the ReLU in Polynomial Time},
  author={Surbhi Goel and Varun Kanade and Adam Klivans and Justin Thaler},
  journal   = {COLT},
  year      = {2017}
}

@ARTICLE{YS20,
  title={Learning a Single Neuron with Gradient Methods},
  author={Gilad Yehudai and Ohad Shamir},
  journal   = {COLT},
  year      = {2020}
}

@ARTICLE{KS17,
  title={Improving Generalization Performance by Switching from Adam to SGD},
  author={Nitish Shirish Keskar, Richard Socher},
  journal   = {arXiv:1712.07628},
  year      = {2017}
}


@ARTICLE{LMCC19,
  title={Nonconvex Matrix Factorization from Rank-One Measurements},
  author={Yuanxin Li and Cong Ma and Yuxin Chen and Yuejie Chi},
  journal   = {AISTATS},
  year      = {2019}
}

@ARTICLE{AFWZ17,
  title={Entrywise Eigenvector Analysis of Random Matrices with Low Expected Rank},
  author={Emmanuel Abbe and Jianqing Fan and Kaizheng Wang and Yiqiao Zhong},
  journal   = {arXiv:1709.09565},
  year      = {2017}
}


@ARTICLE{GLZX19,
  title={Understanding the Role of Momentum in Stochastic Gradient Methods},
  author={Igor Gitman and Hunter Lang and Pengchuan Zhang and Lin Xiao},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{HHS17,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Elad Hoffer and Itay Hubara and Daniel Soudry},
  journal   = {NeurIPS},
  year      = {2017}
}

@ARTICLE{CJY18,
  title={Stability and Convergence Trade-off of Iterative Optimization Algorithms},
  author={Yuansi Chen and Chi Jin and Bin Yu},
  journal   = {arXiv:1804.01619},
  year      = {2018}
}


@ARTICLE{LR18,
  title={Accelerated Gossip via Stochastic Heavy Ball Method},
  author={Nicolas Loizou and Peter Richt{\'a}rik},
  journal   = {Allerton},
  year      = {2018}
}


@ARTICLE{KH1918,
  title={Decoupled Weight Decay Regularization},
  author={Ilya Loshchilov and Frank Hutter},
  journal   = {ICLR},
  year      = {2019}
}

@article{WRSSR17,
  author = {Ashia C Wilson and Rebecca Roelofs and Mitchell Stern and Nathan Srebro and Benjamin Recht.},
  title = {The marginal value of adaptive gradient methods in machine learning},
  journal = {NeurIPS},
  year = {2017}
}

@article{BCMN14,
 author = {Afonso S. Bandeira and Jameson Cahill and Dustin G. Mixon and Aaron A. Nelson},
 title = {Saving phase: Injectivity and stability for phase retrieval},
 journal = {Applied and Computational Harmonic Analysis},
 year = {2014}
} 


@article{WCA20,
 author = {Jun-Kun Wang and Chi-Heng Lin and Jacob Abernethy},
 title = {Escaping Saddle Points Faster with Stochastic Momentum},
 journal = {ICLR},
 year = {2020}
} 

@article{WSW16,
 author = {Chris D. White and Sujay Sanghavi and Rachel Ward},
 title = {The local convexity of solving systems of quadratic equations},
 journal = {Results in Mathematics},
 year = {2016}
} 


@article{ZL15,
 author = {Qinqing Zheng and John Lafferty},
 title = {A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements},
 journal = {NeurIPS},
 year = {2015}
} 

@article{TBSSR16,
 author = {Stephen Tu and Ross Boczar and Max Simchowitz and Mahdi Soltanolkotabi and Benjamin Recht},
 title = {Low-rank Solutions of Linear Matrix Equations via Procrustes Flow},
 journal = {ICML},
 year = {2016}
} 

@article{DDP18,
 author = {Damek Davis and Dmitriy Drusvyatskiy and Courtney Paquette},
 title = {The nonsmooth landscape of phase retrieval},
 journal = {IMA Journal on Numerical Analysis},
 year = {2018}
} 

@article{SQW16,
 author = {Ju Sun and Qing Qu and John Wright},
 title = {A Geometric Analysis of Phase Retrieval},
 journal = {IEEE ISIT},
 year = {2016}
} 

@article{LMZ18,
 author = {Yuanzhi Li and Tengyu Ma and Hongyang Zhang},
 title = {Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations},
 journal = {COLT},
 year = {2018}
} 


@article{LGL15,
 author = {Gen Li and Yuantao Gu and Yue M. Lu},
 title = {Phase retrieval using iterative projections: Dynamics in the large systems limit},
 journal = {IEEE Allerton},
 year = {2015}
} 

@article{MXM18,
 author = {Junjie Ma and Ji Xu and Arian Maleki},
 title = {Optimization-based AMP for Phase Retrieval: The Impact of Initialization and l2-regularization},
 journal = {IEEE Transactions on Information Theory},
 year = {2018}
} 


@article{Z19,
 author = {Teng Zhang},
 title = {Phase retrieval using alternating minimization in a batch setting},
 journal = {Applied and Computational Harmonic Analysis},
 year = {2019}
} 

@article{YYFZWN18,
 author = {Zhuoran Yang and Lin Yang and Ethan Fang and Tuo Zhao and Zhaoran Wang and Matey Neykov},
 title = {Misspecified Nonconvex Statitical Optimization for Sparse Phase Retrival},
 journal = {Mathematical Programming},
 year = {2018}
} 


@article{ZCL17,
 author = {Huishuai Zhang and Yuejie Chi and Yingbin Liang},
 title = {Provable Non-convex Phase Retrieval with Outliers: Median Truncated Wirtinger Flow},
 journal = {ICML},
 year = {2017}
} 


@article{QZEW17,
 author = {Qing Qu and Yuqian Zhang and Yonina C. Eldar and John Wright},
 title = {Convolutional Phase Retrieval via Gradient Descent},
 journal = {NeurIPS},
 year = {2017}
} 


@article{ZWGC18,
 author = {Liang Zhang and Gang Wang and Georgios B. Giannakis and Jie Che},
 title = {Compressive Phase Retrieval via Reweighted Amplitude Flow},
 journal = {IEEE Transactions on Signal Processing},
 year = {2018}
} 

@article{CLW19,
 author = {Jian-Feng Cai and Haixia Liu and Yang Wan},
 title = {Fast Rank-One Alternating Minimization Algorithm for Phase Retrieval},
 journal = {Journal of Scientific Computing},
 year = {2019}
} 

@article{TV19,
 author = {Yan Shuo Tan and Roman Vershynin},
 title = {Online Stochastic Gradient Descent with Arbitrary Initialization Solves Non-smooth, Non-convex Phase Retrieval},
 journal = {arXiv:1910.12837},
 year = {2019}
} 


@article{TV18,
 author = {Yan Shuo Tan and Roman Vershynin},
 title = {Phase Retrieval via Randomized Kaczmarz: Theoretical Guarantees},
 journal = {Information and Inference },
 year = {2018}
} 


@article{BEB17,
 author = {Tamir Bendory and Yonina C. Eldar and Nicolas Boumal},
 title = {Non-Convex Phase Retrieval from STFT Measurements},
 journal = {IEEE Transactions on Signal Processing},
 year = {2017}
} 


@article{W15,
 author = {Ke Wei},
 title = {Solving systems of phaseless equations via Kaczmarz methods: A proof of concept study},
 journal = {Inverse Problems},
 year = {2015}
} 

@ARTICLE{LRP16,
  title={Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints},
  author={Laurent Lessard and Benjamin Recht and Andrew Packard},
  journal   = {SIAM Journal on Optimization},
  year      = {2016}
}


@article{CFL15,
 author = {Pengwen Chen and Albert Fannjiang and Gi-Ren Liu},
 title = {Phase Retrieval with One or Two Diffraction Patterns by Alternating Projection with Null Initialization},
 journal = {Journal of Fourier Analysis and Applications},
 year = {2015}
} 


@article{GX17,
 author = {Bing Gao and Zhiqiang Xu},
 title = {Phaseless recovery using the Gauss-Newton method},
 journal = {IEEE Transactions on Signal Processing},
 year = {2017}
} 


@article{DR18,
 author = {John Duchi and Feng Ruan},
 title = {Solving (most) of a set of quadratic equalities: Composite optimization for robust phase retrieval},
 journal = {Information and Inference},
 year = {2018}
} 


@article{CL16,
 author = {Yuejie Chi and Yue M. Lu},
 title = {Kaczmarz method for solving quadratic equations},
 journal = {IEEE Signal Processing Letters},
 year = {2016}
} 

@article{WGSC17,
 author = {Gang Wang and Georgios B. Giannakis and Yousef Saad and Yonina C. Eldar},
 title = {Solving Most Systems of Random Quadratic Equations},
 journal = {NeurIPS},
 year = {2017}
} 


@article{ZZLC17,
 author = {Huishuai Zhang and Yi Zhou and Yingbin Liang and Yuejie Chi},
 title = {A Nonconvex Approach for Phase Retrieval: Reshaped Wirtinger Flow and Incremental Algorithms},
 journal = {JMLR},
 year = {2017}
} 


@article{WGE17,
 author = {Gang Wang and Georgios B. Giannakis and Yonina C. Eldar},
 title = {Solving Systems of Random Quadratic Equations via Truncated Amplitude Flow},
 journal = {IEEE Transactions on Information Theory},
 year = {2017}
} 


@article{M18,
 author = {Mahdi Soltanolkotabi},
 title = {Structured signal recovery from quadratic measurements: Breaking sample complexity barriers via nonconvex optimization},
 journal = {IEEE Transactions on Information Theory},
 year = {2018}
} 


@article{CLM16,
 author = {T. Tony Cai and Xiaodong Li and Zongming Ma},
 title = {Optimal rates of convergence for noisy sparse phase retrieval via thresholded wirtinger flow},
 journal = {The Annals of Statistics},
 year = {2016}
} 


@article{NJS13,
 author = {Praneeth Netrapalli and Prateek Jain and Sujay Sanghavi},
 title = {Phase Retrieval using Alternating Minimization},
 journal = {NeurIPS},
 year = {2013}
} 

@article{CC17,
 author = {Yuxin Chen and Emmanuel J. Cand{\'e}s},
 title = {Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems},
 journal = {Communications on Pure and Applied Mathematics},
 year = {2017}
} 


@article{CLS15,
 author = {Emmanuel J. Cand{\'e}s and Xiaodong Li and Mahdi Soltanolkotabi},
 title = {Phase Retrieval via Wirtinger Flow: Theory and Algorithms},
 journal = {IEEE Transactions on Information Theory},
 year = {2015}
} 


@article{DH14,
 author = {Laurent Demanet and Paul Hand},
 title = {Stable optimizationless recovery from phaseless linear measurements},
 journal = {Journal of Fourier Analysis and Applications},
 year = {2014}
} 

@article{CL14,
 author = {Emmanuel J. Cand{\'e}s and Xiaodong Li},
 title = {Solving Quadratic Equations via PhaseLift when There Are About As Many Equations As Unknowns},
 journal = {Foundations of Computational Mathematics},
 year = {2014}
} 


@article{SECCMS15,
 author = {Yoav Shechtman and Yonina C. Eldar and Oren Cohen and Henry Nicholas Chapman 
 and Jianwei Miao and Mordechai Segev},
 title = {Phase retrieval with application to optical imaging: a contemporary overview},
 journal = {IEEE signal processing magazine},
 year = {2015}
} 


@article{CSV13,
 author = {Emmanuel J. Cand{\'e}s and Thomas Strohmer and Vladislav Voroninski},
 title = {PhaseLift: Exact and Stable Signal Recovery from Magnitude Measurements via Convex Programming},
 journal = {Communications on Pure and Applied Mathematics},
 year = {2013}
} 


@article{CESV12,
 author = {Emmanuel J. Cand{\'e}s and Yonina Eldar and Thomas Strohmer and Vlad Voroninski},
 title = {Phase retrieval via matrix completion},
 journal = {SIAM Journal on Imaging Sciences},
 year = {2013}
} 

@article{CCFMY18,
 author = {Yuxin Chen and Yuejie Chi and Jianqing Fan and Cong Ma and Yuling Yan},
 title = {Gradient Descent with Random Initialization: Fast Global Convergence for Nonconvex Phase Retrieval},
 journal = {Mathematical Programming},
 year = {2019}
} 


@article{CD19,
 author = {Yair Carmon and John Duchi},
 title = {Gradient Descent Finds the Cubic-Regularized Nonconvex Newton Step},
 journal = {SIAM Journal on Optimization},
 year = {2019}
} 



@article{GH15,
 author = {Dan Garber and Elad Hazan},
 title = {Fast and Simple PCA via Convex Optimization},
 journal = {arXiv:1509.05647},
 year = {2015}
} 

@article{GHM15,
 author = {Dan Garber and Elad Hazan and Tengyu Ma},
 title = {Online Learning of Eigenvectors},
 journal = {ICML},
 year = {2015}
} 

@article{ZSJBD17,
 author = {Kai Zhong and Zhao Song and Prateek Jain and Peter L. Bartlett and Inderjit S. Dhillon},
 title = {Recovery Guarantees for One-hidden-layer Neural Networks},
 journal = {ICML},
 year = {2017}
} 


@article{XHDMR18,
 author = {Peng Xu and Bryan He and Christopher De Sa and Ioannis Mitliagkas and Christopher Re},
 title = {Accelerated Stochastic Power Iteration},
 journal = {AISTATS},
 year = {2018}
} 


@article{S16,
 author = {Ohad Shamir},
 title = {Convergence of Stochastic Gradient Descent for PCA},
 journal = {ICML},
 year = {2016}
} 

@article{JJKNS16,
 author = {Prateek Jain and Chi Jin and Sham M. Kakade and Praneeth Netrapalli and Aaron Sidford},
 title = {Streaming PCA: Matching Matrix Bernstein and Near-Optimal Finite Sample Guarantees for Oja's Algorithm},
 journal = {COLT},
 year = {2016}
} 


@article{MWCC17,
 author = {Cong Ma and Kaizheng Wang and Yuejie Chi and Yuxin Chen},
 title = {Implicit Regularization in Nonconvex Statistical Estimation:
Gradient Descent Converges Linearly for Phase Retrieval, Matrix Completion, and Blind Deconvolution},
 journal = {Foundations of Computational Mathematics},
 year = {2017}
} 


@article{CGZ19,
 author = {Bugra Can and Mert G{\"u}rb{\"u}zbalaban and Lingjiong Zhu},
 title = {Accelerated Linear Convergence of Stochastic Momentum Methods in Wasserstein Distances},
 journal = {ICML},
 year = {2019}
} 



@article{SQW16,
 author = {Ju Sun and Qing Qu and John Wright},
 title = {A Geometrical Analysis of Phase Retrieval},
 journal = {International Symposium on Information Theory},
 year = {2016}
} 


@article{SQW16,
 author = {Ju Sun and Qing Qu and John Wright},
 title = {A Geometrical Analysis of Phase Retrieval},
 journal = {International Symposium on Information Theory},
 year = {2016}
} 

@article{SQW15,
 author = {Ju Sun and Qing Qu and John Wright},
 title = {When Are Nonconvex Problems Not Scary?},
 journal = {NeurIPS Workshop on Non-convex Optimization for Machine Learning: Theory and Practice},
 year = {2015}
} 


@article{BHK15,
 author = {Avrim Blum and John Hopcroft and Ravindran Kannan},
 title = {Foundations of Data Science},
 year = {2015}
} 


@ARTICLE{HHS17,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Elad Hoffer and Itay Hubara and Daniel Soudry},
  journal   = {NeurIPS},
  year      = {2017}
}


@article{goh2017why,
  author = {Gabriel Goh},
  title = {Why Momentum Really Works},
  journal = {Distill},
  year = {2017}
}

@article{G17,
  author = {Xavier Gastaldi},
  title = {Shake-Shake regularization},
  journal = {arXiv:1705.07485},
  year = {2017}
}

@article{WRSSR17,
  author = {Ashia C Wilson and Rebecca Roelofs and Mitchell Stern and Nathan Srebro and and Benjamin Recht.},
  title = {The marginal value of adaptive gradient methods in machine learning},
  journal = {NeurIPS},
  year = {2017}
}

@article{CZMVL18,
  author = {Ekin D Cubuk and Barret Zoph and Dandelion Mane and Vijay Vasudevan and Quoc V Le.},
  title = {Autoaugment: Learning augmentation policies from data.},
  journal = {arXiv:1805.09501},
  year = {2018}
}



@ARTICLE{YLL18,
  title={Unified Convergence Analysis of Stochastic Momentum Methods for Convex and Non-convex Optimization},
  author={Tianbao Yang and Qihang Lin and Zhe Li},
  journal   = {IJCAI},
  year      = {2018}
}

@ARTICLE{P64,
  title={Some methods of speeding up the convergence of iteration methods},
  author={B.T. Polyak},
  journal   = {USSR Computational Mathematics and Mathematical Physics},
  year      = {1964}
}

@ARTICLE{GPS16,
  title={Stochastic Heavy Ball},
  author={S{\'e}bastien Gadat and Fabien Panloup and Sofiane Saadane},
  journal   = {arXiv:1609.04228},
  year      = {2016}
}

@ARTICLE{GFJ15,
  title={Global convergence of the Heavy-ball method for convex optimization},
  author={Euhanna Ghadimi and Hamid Reza Feyzmahdavian and Mikael Johansson},
  journal   = {ECC},
  year      = {2015}
}


@ARTICLE{LR17,
  title={Momentum and stochastic momentum for stochastic gradient, newton,
  proximal point and subspace descent methods},
  author={Nicolas Loizou and Peter Richt{\'a}rik},
  journal   = {arXiv:1712.09677},
  year      = {2017}
}

@ARTICLE{SYLHGJ19,
  title={Non-ergodic Convergence Analysis of Heavy-Ball Algorithms},
  author={Tao Sun and Penghang Yin and Dongsheng Li and Chun Huang and Lei Guan and Hao Jiang},
  journal   = {AAAI},
  year      = {2019}
}

@ARTICLE{OCBP14,
  title={ipiano: Inertial proximal algorithm for nonconvex optimization},
  author={Peter Ochs and Yunjin Chen and Thomas Brox and Thomas Pock},
  journal   = {SIAM Journal of Imaging Sciences},
  year      = {2014}
}



@ARTICLE{GLZ16,
  title={Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization},
  author={Saeed Ghadimi and Guanghui Lan and Hongchao Zhang},
  journal   = {Mathematical Programming},
  year      = {2016}
}


@ARTICLE{LJCJ17,
  title={Nonconvex finite-sum optimization via scsg methods},
  author={Lihua Lei and Cheng Ju and Jianbo Chen and Michael I. Jordan},
  journal   = {NeurIPS},
  year      = {2017}
}


@ARTICLE{YXG18,
  title={Stochastic nested variance reduced gradient descent for nonconvex optimization},
  author={Yaodong Yu and Pan Xu and Quanquan Gu},
  journal   = {NeurIPS},
  year      = {2018}
}


@ARTICLE{CHMAL15,
  title={The Loss Surfaces of Multilayer Networks},
  author={Anna Choromanska and Mikael Henaff and Michael Mathieu and G{\'e}rard Ben Arous and Yann LeCun},
  journal   = {AISTAT},
  year      = {2015}
}

@ARTICLE{N13,
  title={Introductory lectures on convex optimization: a basic course},
  author={Yurii Nesterov},
  journal   = {Springer},
  year      = {2013}
}


@ARTICLE{NKJK18,
  title={On the insufficiency of existing momentum schemes for Stochastic Optimization},
  author={Rahul Kidambi and Praneeth Netrapalli and Prateek Jain and Sham M. Kakade},
  journal   = {ICLR},
  year      = {2018}
}


@ARTICLE{JNGKJ19,
  title={Stochastic Gradient Descent Escapes Saddle Points Efficiently},
  author={Chi Jin and Praneeth Netrapalli and Rong Ge and Sham M. Kakade and Michael I. Jordan},
  journal   = {arXiv:1902.04811},
  year      = {2019}
}


@ARTICLE{G11,
  title={Recovering low-rank matrices from few coefficients in any basis},
  author={David Gross},
  journal   = {IEEE Transactions on Information Theory},
  year      = {2011}
}

@ARTICLE{SMDH13,
  title={On the importance of initialization and momentum in deep learning},
  author={Ilya Sutskever and James Martens and George Dahl and Geoffrey Hinton},
  journal   = {ICML},
  year      = {2013}
}


@ARTICLE{WKXS18,
  title={Identifying Generalization Properties in Neural Networks},
  author={Huan Wang and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},
  journal   = {arXiv:1809.07402},
  year      = {2018}
}

@ARTICLE{GS18,
  title={Average Stability is Invariant to Data Preconditioning. Implications to Exp-concave Empirical Risk Minimization},
  author={Alon Gonen and Shai Shalev-Shwartz},
  journal   = {JMLR},
  year      = {2018}
}


@ARTICLE{SSSS09,
  title={Learnability and Stability in the General Learning Setting.},
  author={Shai Shalev-Shwartz and Ohad Shamir and Nathan Srebro and Karthik Sridharan},
  journal   = {COLT},
  year      = {2009}
}


@ARTICLE{SSSS10,
  title={Learnability, Stability and Uniform Convergence},
  author={Shai Shalev-Shwartz and Ohad Shamir and Nathan Srebro and Karthik Sridharan},
  journal   = {JMLR},
  year      = {2010}
}


@ARTICLE{BL02,
  title={Stability and Generalization},
  author={Olivier Bousquet and Andre Elisseeff},
  journal   = {JMLR},
  year      = {2002}
}


@ARTICLE{YLL18,
  title={Unified Convergence Analysis of Stochastic Momentum Methods for Convex and Non-convex Optimization},
  author={Tianbao Yang and Qihang Lin and Zhe Li},
  journal   = {IJCAI},
  year      = {2018}
}


@ARTICLE{FV18,
  title={Generalization Bounds for Uniformly Stable Algorithms},
  author={Vitaly Feldman and Jan Vondrak},
  journal   = {NeurIPS},
  year      = {2018}
}


@ARTICLE{KL18,
  title={Data-Dependent Stability of Stochastic Gradient Descent},
  author={Ilja Kuzborskij, Christoph H. Lampert},
  journal   = {ICML},
  year      = {2018}
}


@ARTICLE{MWZZ18,
  title={Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints},
  author={Wenlong Mou and Liwei Wang and Xiyu Zhai and Kai Zheng},
  journal   = {COLT},
  year      = {2018}
}

@ARTICLE{CP17,
  title={Stability and Generalization of Learning Algorithms that Converge to Global Optima},
  author={Zachary Charles and Dimitris Papailiopoulos},
  journal   = {ICML},
  year      = {2018}
}


@ARTICLE{GS17,
  title={Fast Rates for Empirical Risk Minimization of Strict Saddle Problems},
  author={Alon Gonen and Shai Shalev-Shwartz},
  journal   = {COLT},
  year      = {2017}
}

@ARTICLE{HHS17,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Elad Hoffer and Itay Hubara and Daniel Soudry},
  journal   = {NeurIPS},
  year      = {2017}
}


@ARTICLE{HRS16,
  title={Train Faster, Generalize Better: Stability of Stochastic Gradient Descent},
  author={Moritz Hardt and Benjamin Recht and Yoram Singer},
  journal   = {ICML},
  year      = {2016}
}

