\begin{thebibliography}{10}

\bibitem{agarwal2019reinforcement}
Alekh Agarwal, Nan Jiang, Sham~M Kakade, and Wen Sun.
\newblock Reinforcement learning: Theory and algorithms.
\newblock {\em CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep}, 2019.

\bibitem{auer2002finite}
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock {\em Machine learning}, 47(2):235--256, 2002.

\bibitem{azizzadenesheli2016reinforcement}
Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar.
\newblock Reinforcement learning of pomdps using spectral methods.
\newblock In {\em Conference on Learning Theory}, pages 193--256. PMLR, 2016.

\bibitem{besse2009quasi}
Camille Besse and Brahim Chaib-Draa.
\newblock Quasi-deterministic partially observable markov decision processes.
\newblock In {\em International Conference on Neural Information Processing},
  pages 237--246. Springer, 2009.

\bibitem{bonet2012deterministic}
Blai Bonet.
\newblock Deterministic pomdps revisited.
\newblock {\em arXiv preprint arXiv:1205.2659}, 2012.

\bibitem{boots2013hilbert}
Byron Boots, Geoffrey Gordon, and Arthur Gretton.
\newblock Hilbert space embeddings of predictive state representations.
\newblock {\em arXiv preprint arXiv:1309.6819}, 2013.

\bibitem{boots2011closing}
Byron Boots, Sajid~M Siddiqi, and Geoffrey~J Gordon.
\newblock Closing the learning-planning loop with predictive state
  representations.
\newblock {\em The International Journal of Robotics Research}, 30(7):954--966,
  2011.

\bibitem{burago1996complexity}
Dima Burago, Michel De~Rougemont, and Anatol Slissenko.
\newblock On the complexity of partially observed markov decision processes.
\newblock {\em Theoretical Computer Science}, 157(2):161--183, 1996.

\bibitem{cai2022sample}
Qi~Cai, Zhuoran Yang, and Zhaoran Wang.
\newblock Sample-efficient reinforcement learning for pomdps with linear
  function approximations.
\newblock {\em arXiv preprint arXiv:2204.09787}, 2022.

\bibitem{chowdhury2017kernelized}
Sayak~Ray Chowdhury and Aditya Gopalan.
\newblock On kernelized multi-armed bandits.
\newblock In {\em International Conference on Machine Learning}, pages
  844--853. PMLR, 2017.

\bibitem{chowdhury2020no}
Sayak~Ray Chowdhury and Rafael Oliveira.
\newblock No-regret reinforcement learning with value function approximation: a
  kernel embedding approach.
\newblock {\em arXiv preprint arXiv:2011.07881}, 2020.

\bibitem{dani2008stochastic}
Varsha Dani, Thomas~P Hayes, and Sham~M Kakade.
\newblock Stochastic linear optimization under bandit feedback.
\newblock 2008.

\bibitem{dikkala2020minimax}
Nishanth Dikkala, Greg Lewis, Lester Mackey, and Vasilis Syrgkanis.
\newblock Minimax estimation of conditional moment models.
\newblock {\em Advances in Neural Information Processing Systems},
  33:12248--12262, 2020.

\bibitem{du2021bilinear}
Simon Du, Sham Kakade, Jason Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and
  Ruosong Wang.
\newblock Bilinear classes: A structural framework for provable generalization
  in rl.
\newblock In {\em International Conference on Machine Learning}, pages
  2826--2836. PMLR, 2021.

\bibitem{du2020agnostic}
Simon~S Du, Jason~D Lee, Gaurav Mahajan, and Ruosong Wang.
\newblock Agnostic $ q $-learning with function approximation in deterministic
  systems: Near-optimal bounds on approximation error and sample complexity.
\newblock {\em Advances in Neural Information Processing Systems},
  33:22327--22337, 2020.

\bibitem{du2019provably}
Simon~S Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang.
\newblock Provably efficient q-learning with function approximation via
  distribution shift error checking oracle.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{even2005reinforcement}
Eyal Even-Dar, Sham~M Kakade, and Yishay Mansour.
\newblock Reinforcement learning in pomdps without resets.
\newblock 2005.

\bibitem{even2007value}
Eyal Even-Dar, Sham~M Kakade, and Yishay Mansour.
\newblock The value of observation for monitoring dynamic systems.
\newblock In {\em IJCAI}, pages 2474--2479, 2007.

\bibitem{golowich2022planning}
Noah Golowich, Ankur Moitra, and Dhruv Rohatgi.
\newblock Planning in observable pomdps in quasipolynomial time.
\newblock {\em arXiv preprint arXiv:2201.04735}, 2022.

\bibitem{guo2016pac}
Zhaohan~Daniel Guo, Shayan Doroudi, and Emma Brunskill.
\newblock A pac rl algorithm for episodic pomdps.
\newblock In {\em Artificial Intelligence and Statistics}, pages 510--518.
  PMLR, 2016.

\bibitem{he2021logarithmic}
Jiafan He, Dongruo Zhou, and Quanquan Gu.
\newblock Logarithmic regret for reinforcement learning with linear function
  approximation.
\newblock In {\em International Conference on Machine Learning}, pages
  4171--4180. PMLR, 2021.

\bibitem{hsu2012spectral}
Daniel Hsu, Sham~M Kakade, and Tong Zhang.
\newblock A spectral algorithm for learning hidden markov models.
\newblock {\em Journal of Computer and System Sciences}, 78(5):1460--1480,
  2012.

\bibitem{hu2021fast}
Yichun Hu, Nathan Kallus, and Masatoshi Uehara.
\newblock Fast rates for the regret of offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2102.00479}, 2021.

\bibitem{javdani2015shared}
Shervin Javdani, Siddhartha~S Srinivasa, and J~Andrew Bagnell.
\newblock Shared autonomy via hindsight optimization.
\newblock {\em Robotics science and systems: online proceedings}, 2015, 2015.

\bibitem{ji2007nonmyopic}
Shihao Ji, Ronald Parr, and Lawrence Carin.
\newblock Nonmyopic multiaspect sensing with partially observable markov
  decision processes.
\newblock {\em IEEE Transactions on Signal Processing}, 55(6):2720--2730, 2007.

\bibitem{jiang2017contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E
  Schapire.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In {\em International Conference on Machine Learning}, pages
  1704--1713. PMLR, 2017.

\bibitem{jin2020sample}
Chi Jin, Sham Kakade, Akshay Krishnamurthy, and Qinghua Liu.
\newblock Sample-efficient reinforcement learning of undercomplete pomdps.
\newblock {\em Advances in Neural Information Processing Systems},
  33:18530--18539, 2020.

\bibitem{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In {\em Conference on Learning Theory}, pages 2137--2143. PMLR, 2020.

\bibitem{kaelbling1998planning}
Leslie~Pack Kaelbling, Michael~L Littman, and Anthony~R Cassandra.
\newblock Planning and acting in partially observable stochastic domains.
\newblock {\em Artificial intelligence}, 101(1-2):99--134, 1998.

\bibitem{kearns1999approximate}
Michael Kearns, Yishay Mansour, and Andrew Ng.
\newblock Approximate planning in large pomdps via reusable trajectories.
\newblock {\em Advances in Neural Information Processing Systems}, 12, 1999.

\bibitem{krishnamurthy2016pac}
Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock Pac reinforcement learning with rich observations.
\newblock {\em Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem{kwon2021rl}
Jeongyeol Kwon, Yonathan Efroni, Constantine Caramanis, and Shie Mannor.
\newblock Rl for latent mdps: Regret guarantees and a lower bound.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{lale2021adaptive}
Sahin Lale, Kamyar Azizzadenesheli, Babak Hassibi, and Anima Anandkumar.
\newblock Adaptive control and regret minimization in linear quadratic gaussian
  (lqg) setting.
\newblock In {\em 2021 American Control Conference (ACC)}, pages 2517--2522.
  IEEE, 2021.

\bibitem{li2021sample}
Gen Li, Yuxin Chen, Yuejie Chi, Yuantao Gu, and Yuting Wei.
\newblock Sample-efficient reinforcement learning is feasible for linearly
  realizable mdps with limited revisiting.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{littman2001predictive}
Michael Littman and Richard~S Sutton.
\newblock Predictive representations of state.
\newblock {\em Advances in neural information processing systems}, 14, 2001.

\bibitem{littman1994memoryless}
Michael~L Littman.
\newblock Memoryless policies: Theoretical limitations and practical results.
\newblock In {\em From Animals to Animats 3: Proceedings of the third
  international conference on simulation of adaptive behavior}, volume~3, page
  238. Cambridge, MA, 1994.

\bibitem{littman1996algorithms}
Michael~Lederman Littman.
\newblock {\em Algorithms for sequential decision-making}.
\newblock Brown University, 1996.

\bibitem{liu2022masked}
Bingbin Liu, Daniel Hsu, Pradeep Ravikumar, and Andrej Risteski.
\newblock Masked prediction tasks: a parameter identifiability view.
\newblock {\em arXiv preprint arXiv:2202.09305}, 2022.

\bibitem{liu2022partially}
Qinghua Liu, Alan Chung, Csaba Szepesv{\'a}ri, and Chi Jin.
\newblock When is partially observable reinforcement learning not scary?
\newblock {\em arXiv preprint arXiv:2204.08967}, 2022.

\bibitem{lykouris2021corruption}
Thodoris Lykouris, Max Simchowitz, Alex Slivkins, and Wen Sun.
\newblock Corruption-robust exploration in episodic reinforcement learning.
\newblock In {\em Conference on Learning Theory}, pages 3242--3245. PMLR, 2021.

\bibitem{papadimitriou1987complexity}
Christos~H Papadimitriou and John~N Tsitsiklis.
\newblock The complexity of markov decision processes.
\newblock {\em Mathematics of operations research}, 12(3):441--450, 1987.

\bibitem{pattipati1990application}
Krishna~R Pattipati and Mark~G Alexandridis.
\newblock Application of heuristic search and information theory to sequential
  fault diagnosis.
\newblock {\em IEEE Transactions on Systems, Man, and Cybernetics},
  20(4):872--887, 1990.

\bibitem{platt2017efficient}
Robert Platt, Leslie Kaelbling, Tomas Lozano-Perez, and Russ Tedrake.
\newblock Efficient planning in non-gaussian belief spaces and its application
  to robot grasping.
\newblock In {\em Robotics Research}, pages 253--269. Springer, 2017.

\bibitem{platt2010belief}
Robert Platt~Jr, Russ Tedrake, Leslie Kaelbling, and Tomas Lozano-Perez.
\newblock Belief space planning assuming maximum likelihood observations.
\newblock 2010.

\bibitem{simchowitz2019non}
Max Simchowitz and Kevin~G Jamieson.
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{simchowitz2020improper}
Max Simchowitz, Karan Singh, and Elad Hazan.
\newblock Improper learning for non-stochastic control.
\newblock In {\em Conference on Learning Theory}, pages 3320--3436. PMLR, 2020.

\bibitem{singh2004predictive}
Satinder Singh, Michael~R James, and Matthew~R Rudary.
\newblock Predictive state representations: a new theory for modeling dynamical
  systems.
\newblock In {\em Proceedings of the 20th conference on Uncertainty in
  artificial intelligence}, pages 512--519, 2004.

\bibitem{song2010hilbert}
Le~Song, Byron Boots, Sajid Siddiqi, Geoffrey~J Gordon, and Alex Smola.
\newblock Hilbert space embeddings of hidden markov models.
\newblock 2010.

\bibitem{song2013kernel}
Le~Song, Kenji Fukumizu, and Arthur Gretton.
\newblock Kernel embeddings of conditional distributions: A unified kernel
  framework for nonparametric inference in graphical models.
\newblock {\em IEEE Signal Processing Magazine}, 30(4):98--111, 2013.

\bibitem{srinivas2009gaussian}
Niranjan Srinivas, Andreas Krause, Sham~M Kakade, and Matthias Seeger.
\newblock Gaussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock {\em arXiv preprint arXiv:0912.3995}, 2009.

\bibitem{valko2013finite}
Michal Valko, Nathaniel Korda, R{\'e}mi Munos, Ilias Flaounas, and Nelo
  Cristianini.
\newblock Finite-time analysis of kernelised contextual bandits.
\newblock {\em arXiv preprint arXiv:1309.6869}, 2013.

\bibitem{wainwright2019high}
Martin~J Wainwright.
\newblock {\em High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem{wang2021exponential}
Yuanhao Wang, Ruosong Wang, and Sham Kakade.
\newblock An exponential lower bound for linearly realizable mdp with constant
  suboptimality gap.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{weisz2021tensorplan}
Gell{\'e}rt Weisz, Csaba Szepesv{\'a}ri, and Andr{\'a}s Gy{\"o}rgy.
\newblock Tensorplan and the few actions lower bound for planning in mdps under
  linear realizability of optimal value functions.
\newblock {\em arXiv preprint arXiv:2110.02195}, 2021.

\bibitem{wen2013efficient}
Zheng Wen and Benjamin Van~Roy.
\newblock Efficient exploration and value function generalization in
  deterministic systems.
\newblock {\em Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem{xiong2021sublinear}
Yi~Xiong, Ningyuan Chen, Xuefeng Gao, and Xiang Zhou.
\newblock Sublinear regret for learning pomdps.
\newblock {\em arXiv preprint arXiv:2107.03635}, 2021.

\end{thebibliography}
