@inproceedings{agarwal2021deep,
 author = {Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C. and Bellemare, Marc},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {29304-29320},
 publisher = {Curran Associates, Inc.},
 title = {Deep Reinforcement Learning at the Edge of the Statistical Precipice},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/f514cec81cb148559cf475e7426eed5e-Paper.pdf},
 volume = {34},
 year = {2021},
}

@Article{benettin_part_1,
    author={Benettin, Giancarlo and Galgani, Luigi and Giorgilli, Antonio and Strelcyn, Jean-Marie},
    title={Lyapunov Characteristic Exponents for smooth dynamical systems and for hamiltonian systems; a method for computing all of them. Part 1: Theory},
    journal={Meccanica},
    year={1980},
    volume={15},
    pages={9-20},
    abstract={Since several years Lyapunov Characteristic Exponents are of interest in the study of dynamical systems in order to characterize quantitatively their stochasticity properties, related essentially to the exponential divergence of nearby orbits. One has thus the problem of the explicit computation of such exponents, which has been solved only for the maximal of them. Here we give a method for computing all of them, based on the computation of the exponents of order greater than one, which are related to the increase of volumes. To this end a theorem is given relating the exponents of order one to those of greater order. The numerical method and some applications will be given in a forthcoming paper.},
    issn={1572-9648},
    doi={10.1007/BF02128236},
    url={https://doi.org/10.1007/BF02128236}
}

@Article{benettin_part_2,
    author={Benettin, Giancarlo and Galgani, Luigi and Giorgilli, Antonio and Strelcyn, Jean-Marie},
    title={Lyapunov Characteristic Exponents for smooth dynamical systems and for hamiltonian systems; A method for computing all of them. Part 2: Numerical application},
    journal={Meccanica},
    year={1980},
    month={March},
    volume={15},
    pages={21-30},
    abstract={The present paper, together with the previous one (Part 1: Theory, published in this journal) is intended to give an explicit method for computing all Lyapunov Characteristic Exponents of a dynamical system. After the general theory on such exponents developed in the first part, in the present paper the computational method is described (Chapter A) and some numerical examples for mappings on manifolds and for Hamiltonian systems are given (Chapter B).},
    issn={1572-9648},
    doi={10.1007/BF02128237},
    url={https://doi.org/10.1007/BF02128237}
}

@book{devaney_2003_an,
    author = {Devaney, Robert},
    year = {2020},
    edition = {Second},
    publisher = {CRC Press},
    title = {A First Course in Chaotic Dynamical Systems: Theory and Experiment},
    isbn = {9780429280665},
    doi = {10.1201/9780429280665}
}

@Article{dulacarnold_2021_an,
    author={Dulac-Arnold, Gabriel and Levine, Nir and Mankowitz, Daniel J. and Li, Jerry and Paduraru, Cosmin and Gowal, Sven and Hester, Todd},
    title={Challenges of real-world reinforcement learning: definitions, benchmarks and analysis},
    journal={Machine Learning},
    year={2021},
    volume={110},
    pages={2419-2468},
    abstract={Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. In this work, we identify and formalize a series of independent challenges that embody the difficulties that must be addressed for RL to be commonly deployed in real-world systems. For each challenge, we define it formally in the context of a Markov Decision Process, analyze the effects of the challenge on state-of-the-art learning algorithms, and present some existing attempts at tackling it. We believe that an approach that addresses our set of proposed challenges would be readily deployable in a large number of real world problems. Our proposed challenges are implemented in a suite of continuous control environments called realworldrl-suite which we propose an as an open-source benchmark.},
    issn={1573-0565},
    doi={10.1007/s10994-021-05961-4},
    url={https://doi.org/10.1007/s10994-021-05961-4}
}

@InProceedings{fujimoto2018addressing,
  title = {Addressing Function Approximation Error in Actor-Critic Methods},
  author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  year = {2018},
  pages = {1587-1596},
  editor = {Dy, Jennifer and Krause, Andreas},
  volume = {80},
  series = {Proceedings of Machine Learning Research},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf},
  url = {https://proceedings.mlr.press/v80/fujimoto18a.html},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.}
}

@article{goodfellow2015explaining,
    title={Explaining and Harnessing Adversarial Examples},
    author={Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
    journal={3rd International Conference on Learning Representations},
    year={2015},
    eprint={1412.6572},
    archivePrefix={arXiv},
    primaryClass={stat.ML},
    url={https://arxiv.org/abs/1412.6572}
}


@InProceedings{haarnoja_2018_soft,
  title = 	 {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author =       {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1861--1870},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf},
  url = 	 {https://proceedings.mlr.press/v80/haarnoja18b.html},
  abstract = 	 {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.}
}

@article{hafner2023dreamerv3,
  title={Mastering Diverse Domains through World Models},
  author={Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:2301.04104},
  year={2023},
}

@inproceedings{huang2017adversarial,
  author       = {Sandy Huang and Nicolas Papernot and Ian Goodfellow and
                  Yan Duan and Pieter Abbeel},
  title        = {Adversarial Attacks on Neural Network Policies},
  booktitle    = {5th International Conference on Learning Representations},
  year         = {2017},
  month        = {April},
  url          = {https://openreview.net/forum?id=ryvlRyBKl},
  timestamp    = {Tue, 24 Sep 2019 17:13:20 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/HuangPGDA17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{kos2017delving,
    title={Delving into adversarial attacks on deep policies}, 
    author={Jernej Kos and Dawn Song},
    booktitle    = {5th International Conference on Learning Representations},
    year         = {2017},
    url          = {https://openreview.net/forum?id=ryvlRyBKl},
    timestamp    = {Tue, 24 Sep 2019 17:13:20 +0200},
    biburl       = {https://dblp.org/rec/conf/iclr/HuangPGDA17.bib},
    bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lillicrap_2019_continuous,
  author       = {Timothy P. Lillicrap and
                  Jonathan J. Hunt and
                  Alexander Pritzel and
                  Nicolas Heess and
                  Tom Erez and
                  Yuval Tassa and
                  David Silver and
                  Daan Wierstra},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Continuous control with deep reinforcement learning},
  booktitle    = {4th International Conference on Learning Representations, San Juan, Puerto Rico, Conference Track Proceedings},
  year         = {2016},
  url          = {http://arxiv.org/abs/1509.02971},
  timestamp    = {Thu, 25 July 2019 14:25:37 +0200},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{lin2019tactics,
  author    = {Yen-Chen Lin and Zhang-Wei Hong and Yuan-Hong Liao and Meng-Li Shih and Ming-Yu Liu and Min Sun},
  title     = {Tactics of Adversarial Attack on Deep Reinforcement Learning Agents},
  booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
  pages     = {3756-3762},
  year      = {2017},
  doi       = {10.24963/ijcai.2017/525},
  url       = {https://doi.org/10.24963/ijcai.2017/525},
}

@article { DeterministicNonperiodicFlow,
    author = "Edward N.  Lorenz",
    title = "Deterministic Nonperiodic Flow",
    journal = "Journal of Atmospheric Sciences",
    year = "1963",
    publisher = "American Meteorological Society",
    address = "Boston MA, USA",
    volume = "20",
    doi = "10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2",
    pages=      "130 - 141",
    url = "https://journals.ametsoc.org/view/journals/atsc/20/2/1520-0469_1963_020_0130_dnf_2_0_co_2.xml"
}

@book{lyapunov_1992_the,
  title={The General Problem of the Stability of Motion},
  author={Lyapunov, Aleksandr},
  isbn={9780748400621},
  lccn={92032800},
  series={Control Theory and Applications Series},
  url={https://books.google.co.uk/books?id=4tmAvU3_SCoC},
  year={1992},
  publisher={Taylor \& Francis}
}

@Article{mnih_2015_humanlevel,
author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
title={Human-level control through deep reinforcement learning},
journal={Nature},
year={2015},
volume={518},
pages={529-533},
abstract={An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
issn={1476-4687},
doi={10.1038/nature14236},
url={https://doi.org/10.1038/nature14236}
}

@InProceedings{parmas18pipps,
  title = 	 {{PIPPS}: Flexible Model-Based Policy Search Robust to the Curse of Chaos},
  author =       {Parmas, Paavo and Rasmussen, Carl Edward and Peters, Jan and Doya, Kenji},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4065--4074},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/parmas18a/parmas18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/parmas18a.html},
  abstract = 	 {Previously, the exploding gradient problem has been explained to be central in deep learning and model-based reinforcement learning, because it causes numerical issues and instability in optimization. Our experiments in model-based reinforcement learning imply that the problem is not just a numerical issue, but it may be caused by a fundamental chaos-like nature of long chains of nonlinear computations. Not only do the magnitudes of the gradients become large, the direction of the gradients becomes essentially random. We show that reparameterization gradients suffer from the problem, while likelihood ratio gradients are robust. Using our insights, we develop a model-based policy search framework, Probabilistic Inference for Particle-Based Policy Search (PIPPS), which is easily extensible, and allows for almost arbitrary models and policies, while simultaneously matching the performance of previous data-efficient learning algorithms. Finally, we invent the total propagation algorithm, which efficiently computes a union over all pathwise derivative depths during a single backwards pass, automatically giving greater weight to estimators with lower variance, sometimes improving over reparameterization gradients by $10^6$ times.}
}

@article{raffin2021stable,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  pages   = {1--8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}
@inproceedings{rahn2023advances,
 author = {Rahn, Nate and D'Oro, Pierluca and Wiltzer, Harley and Bacon, Pierre-Luc and Bellemare, Marc},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {30618--30640},
 publisher = {Curran Associates, Inc.},
 title = {Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6191ab7080c840f67eaf5dff7d5edfcb-Paper-Conference.pdf},
 volume = {36},
 year = {2023},
}

@article{ruelle_1979_sensitive,
author = {Ruelle, David},
title = {SENSITIVE DEPENDENCE ON INITIAL CONDITION AND TURBULENT BEHAVIOR OF DYNAMICAL SYSTEMS},
journal = {Annals of the New York Academy of Sciences},
volume = {316},
pages = {408-416},
doi = {https://doi.org/10.1111/j.1749-6632.1979.tb29485.x},
url = {https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/j.1749-6632.1979.tb29485.x},
eprint = {https://nyaspubs.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1749-6632.1979.tb29485.x},
year = {1979},
}

@article{shao2016lyapunov,
author = {Shao, Hua and Shi, Yuming and Zhu, Hao},
title = {Lyapunov Exponents, Sensitivity, and Stability for Non-Autonomous Discrete Systems},
journal = {International Journal of Bifurcation and Chaos},
volume = {28},
pages = {1850088},
year = {2018},
doi = {10.1142/S0218127418500888},
URL = {https://doi.org/10.1142/S0218127418500888},
eprint = {https://doi.org/10.1142/S0218127418500888},
abstract = { This paper focuses on the relationships of Lyapunov exponents with sensitivity and stability for non-autonomous discrete systems. Some new concepts are introduced for non-autonomous discrete systems, including Lyapunov exponents, strong sensitivity at a point and in a set, Lyapunov stability, and exponential asymptotical stability. It is shown that the positive Lyapunov exponent at a point implies strong sensitivity for a class of non-autonomous discrete systems. Furthermore, the uniformly positive Lyapunov exponents in a totally invariant set imply strong sensitivity in this set under certain conditions. The negative Lyapunov exponent at a point implies exponential asymptotical stability for a class of non-autonomous discrete systems. The related existing results for autonomous discrete systems are generalized to non-autonomous discrete systems and their conditions are weakened. One example is provided for illustration. }
}


@Article{silver_2016_mastering,
author={Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
title={Mastering the game of Go with deep neural networks and tree search},
journal={Nature},
year={2016},
volume={529},
pages={484-489},
abstract={The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
issn={1476-4687},
doi={10.1038/nature16961},
url={https://doi.org/10.1038/nature16961}
}


@article{silver_2017_mastering_b,
author = {David Silver  and Thomas Hubert  and Julian Schrittwieser  and Ioannis Antonoglou  and Matthew Lai  and Arthur Guez  and Marc Lanctot  and Laurent Sifre  and Dharshan Kumaran  and Thore Graepel  and Timothy Lillicrap  and Karen Simonyan  and Demis Hassabis },
title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
journal = {Science},
volume = {362},
pages = {1140-1144},
year = {2018},
doi = {10.1126/science.aar6404},
URL = {https://www.science.org/doi/abs/10.1126/science.aar6404},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aar6404},
abstract = {Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system. Science, this issue p. 1140; see also pp. 1087 and 1118 AlphaZero teaches itself to play three different board games and beats state-of-the-art programs in each. The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.}
}

@Article{silver_2017_mastering_a,
author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
title={Mastering the game of Go without human knowledge},
journal={Nature},
year={2017},
volume={550},
pages={354-359},
abstract={A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100--0 against the previously published, champion-defeating AlphaGo.},
issn={1476-4687},
doi={10.1038/nature24270},
url={https://doi.org/10.1038/nature24270}
}

@book{sutton_1998_reinforcement,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018},
}

@inproceedings{szegedy2014intriguing,
  title={Intriguing properties of neural networks},
  author={Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian J. Goodfellow and Rob Fergus},
  booktitle={2nd International Conference on Learning Representations},
  year={2014},
  url = {http://arxiv.org/abs/1312.6199}
}

@Article{tassa_2018_deepmind,
title = {dm\_control: Software and tasks for continuous control},
journal = {Software Impacts},
volume = {6},
pages = {100022},
year = {2020},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2020.100022},
url = {https://www.sciencedirect.com/science/article/pii/S2665963820300099},
author = {Saran Tunyasuvunakool and Alistair Muldal and Yotam Doron and Siqi Liu and Steven Bohez and Josh Merel and Tom Erez and Timothy Lillicrap and Nicolas Heess and Yuval Tassa},
keywords = {Reinforcement learning, Continuous control, Rigid-body dynamics, Physics simulation, Artificial intelligence},
abstract = {The dm_control software package is a collection of Python libraries and task suites for reinforcement learning agents in an articulated-body simulation. Infrastructure includes a wrapper for the MuJoCo physics engine and libraries for procedural model manipulation and task authoring. Task suites include the Control Suite, a set of standardized tasks intended to serve as performance benchmarks, a locomotion framework and task families, and a set of manipulation tasks with a robot arm and snap-together bricks. An adjunct tech report and interactive tutorial are also provided.}
}

@inproceedings{wang2023fractal,
 author = {Wang, Tao and Herbert, Sylvia and Gao, Sicun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {4277--4294},
 publisher = {Curran Associates, Inc.},
 title = {Fractal Landscapes in Policy Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/0d21f257b5288385cb6cb8e0ff2ce82e-Paper-Conference.pdf},
 volume = {36},
 year = {2023},
}

@Article{williams1992simple,
author={Williams, Ronald J.},
title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
journal={Machine Learning},
year={1992},
volume={8},
pages={229-256},
abstract={This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
issn={1573-0565},
doi={10.1007/BF00992696},
url={https://doi.org/10.1007/BF00992696}
}

@article{wiliams1991function,
author = {Williams, Ronald J. and Peng, Jing},
year = {1991},
pages = {241-268},
title = {Function Optimization Using Connectionist Reinforcement Learning Algorithms},
volume = {3},
journal = {Connection Science},
doi = {10.1080/09540099108946587}
}











































@article{sontag_1983_a,
  author = {Sontag, Eduardo D.},
  month = {May},
  pages = {462-471},
  title = {A Lyapunov-Like Characterization of Asymptotic Controllability},
  doi = {10.1137/0321028},
  volume = {21},
  year = {1983},
  journal = {SIAM Journal on Control and Optimization}
}

@article{Grebogi_1987_chaos,
author = {Celso Grebogi  and Edward Ott  and James A. Yorke },
title = {Chaos, Strange Attractors, and Fractal Basin Boundaries in Nonlinear Dynamics},
journal = {Science},
volume = {238},
number = {4827},
pages = {632-638},
year = {1987},
doi = {10.1126/science.238.4827.632},
URL = {https://www.science.org/doi/abs/10.1126/science.238.4827.632},
eprint = {https://www.science.org/doi/pdf/10.1126/science.238.4827.632},
abstract = {Recently research has shown that many simple nonlinear deterministic systems can behave in an apparently unpredictable and chaotic manner. This realization has broad implications for many fields of science. Basic developments in the field of chaotic dynamics of dissipative systems are reviewed in this article. Topics covered include strange attractors, how chaos comes about with variation of a system parameter, universality, fractal basin boundaries and their effect on predictability, and applications to physical systems.}
}

@book{kalnay2003atmospheric,
  title={Atmospheric Modeling, Data Assimilation and Predictability},
  author={Kalnay, E.},
  isbn={9780521796293},
  lccn={2001052687},
  year={2003},
  publisher={Cambridge University Press}
}






