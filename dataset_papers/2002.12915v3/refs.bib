@misc{bartlett2013theoretical,
	title={Theoretical Statistics. Lecture 12},
	author={Bartlett, Peter}
}

@article{yaida2018fluctuation,
	title={Fluctuation-dissipation relations for stochastic gradient descent},
	author={Yaida, Sho},
	journal={arXiv preprint arXiv:1810.00004},
	year={2018}
}

@article{marcus2014ramanujan,
	title={Ramanujan graphs and the solution of the Kadison-Singer problem},
	author={Marcus, Adam W and Spielman, Daniel A and Srivastava, Nikhil},
	journal={arXiv preprint arXiv:1408.4421},
	year={2014}
}
@book{horn1990matrix,
	title={Matrix Analysis},
	author={Horn, Roger A and Horn, Roger A and Johnson, Charles R},
	year={1990},
	publisher={Cambridge University Press}
}

@inproceedings{grave2017efficient,
	title={Efficient softmax approximation for GPUs},
	author={Grave, Edouard and Joulin, Armand and Ciss{\'e}, Moustapha and J{\'e}gou, Herv{\'e} and others},
	booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	pages={1302--1310},
	year={2017},
	organization={JMLR. org}
}

@article{dai2019transformer,
	title={Transformer-xl: Attentive language models beyond a fixed-length context},
	author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
	journal={arXiv preprint arXiv:1901.02860},
	year={2019}
}
@article{bradbury2016quasi,
	title={Quasi-recurrent neural networks},
	author={Bradbury, James and Merity, Stephen and Xiong, Caiming and Socher, Richard},
	journal={arXiv preprint arXiv:1611.01576},
	year={2016}
}

@article{li2017algorithmic,
	title={Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations},
	author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
	journal={arXiv preprint arXiv:1712.09203},
	year={2017}
}

@inproceedings{arora2019implicit,
	title={Implicit regularization in deep matrix factorization},
	author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
	booktitle={Advances in Neural Information Processing Systems},
	pages={7411--7422},
	year={2019}
}

@article{arora2018stronger,
	title={Stronger generalization bounds for deep nets via a compression approach},
	author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
	journal={arXiv preprint arXiv:1802.05296},
	year={2018}
}

@inproceedings{kakade2009complexity,
	title={On the complexity of linear prediction: Risk bounds, margin bounds, and regularization},
	author={Kakade, Sham M and Sridharan, Karthik and Tewari, Ambuj},
	booktitle={Advances in neural information processing systems},
	pages={793--800},
	year={2009}
}

@inproceedings{marcus1994penn,
	title={The Penn Treebank: annotating predicate argument structure},
	author={Marcus, Mitchell and Kim, Grace and Marcinkiewicz, Mary Ann and MacIntyre, Robert and Bies, Ann and Ferguson, Mark and Katz, Karen and Schasberger, Britta},
	booktitle={Proceedings of the workshop on Human Language Technology},
	pages={114--119},
	year={1994},
	organization={Association for Computational Linguistics}
}

@article{merity2016pointer,
	title={Pointer sentinel mixture models},
	author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
	journal={arXiv preprint arXiv:1609.07843},
	year={2016}
}

@inproceedings{gunasekar2017implicit,
	title={Implicit regularization in matrix factorization},
	author={Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
	booktitle={Advances in Neural Information Processing Systems},
	pages={6151--6159},
	year={2017}
}

@article{gunasekar2018characterizing,
	title={Characterizing implicit bias in terms of optimization geometry},
	author={Gunasekar, Suriya and Lee, ton and Soudry, Daniel and Srebro, Nathan},
	journal={arXiv preprint arXiv:1802.08246},
	year={2018}
}

@article{sagun2017empirical,
	title={Empirical analysis of the hessian of over-parametrized neural networks},
	author={Sagun, Levent and Evci, Utku and Guney, V Ugur and Dauphin, Yann and Bottou, Leon},
	journal={arXiv preprint arXiv:1706.04454},
	year={2017}
}
@incollection{lecun2012efficient,
	title={Efficient backprop},
	author={LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
	booktitle={Neural networks: Tricks of the trade},
	pages={9--48},
	year={2012},
	publisher={Springer}
}

@article{zhang2002covering,
	title={Covering number bounds of certain regularized linear function classes},
	author={Zhang, Tong},
	journal={Journal of Machine Learning Research},
	volume={2},
	number={Mar},
	pages={527--550},
	year={2002}
}

@inproceedings{srebro2010smoothness,
	title={Smoothness, low noise and fast rates},
	author={Srebro, Nathan and Sridharan, Karthik and Tewari, Ambuj},
	booktitle={Advances in neural information processing systems},
	pages={2199--2207},
	year={2010}
}

@article{bousquet2002concentration,
	title={Concentration inequalities and empirical processes theory applied to the analysis of learning algorithms},
	author={Bousquet, Olivier},
	year={2002}
}

@article{bach2010self,
	title={Self-concordant analysis for logistic regression},
	author={Bach, Francis and others},
	journal={Electronic Journal of Statistics},
	volume={4},
	pages={384--414},
	year={2010},
	publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}

@inproceedings{gal2016dropout,
	title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
	author={Gal, Yarin and Ghahramani, Zoubin},
	booktitle={international conference on machine learning},
	pages={1050--1059},
	year={2016}
}

@article{mianjy2018implicit,
	title={On the implicit bias of dropout},
	author={Mianjy, Poorya and Arora, Raman and Vidal, Rene},
	journal={arXiv preprint arXiv:1806.09777},
	year={2018}
}

@inproceedings{wager2013dropout,
	title={Dropout training as adaptive regularization},
	author={Wager, Stefan and Wang, Sida and Liang, Percy S},
	booktitle={Advances in neural information processing systems},
	pages={351--359},
	year={2013}
}

@article{wen2019interplay,
	title={Interplay between optimization and generalization of stochastic gradient descent with covariance noise},
	author={Wen, Yeming and Luk, Kevin and Gazeau, Maxime and Zhang, Guodong and Chan, Harris and Ba, Jimmy},
	journal={arXiv preprint arXiv:1902.08234},
	year={2019}
}

@inproceedings{li2019towards,
	title={Towards explaining the regularization effect of initial large learning rate in training neural networks},
	author={Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
	booktitle={Advances in Neural Information Processing Systems},
	pages={11669--11680},
	year={2019}
}

@inproceedings{baldi2013understanding,
	title={Understanding dropout},
	author={Baldi, Pierre and Sadowski, Peter J},
	booktitle={Advances in neural information processing systems},
	pages={2814--2822},
	year={2013}
}

@article{merity2017regularizing,
	title={Regularizing and optimizing LSTM language models},
	author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
	journal={arXiv preprint arXiv:1708.02182},
	year={2017}
}

@article{srivastava2014dropout,
	title={Dropout: a simple way to prevent neural networks from overfitting},
	author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	journal={The journal of machine learning research},
	volume={15},
	number={1},
	pages={1929--1958},
	year={2014},
	publisher={JMLR. org}
}

@inproceedings{li2016improved,
	title={Improved dropout for shallow and deep learning},
	author={Li, Zhe and Gong, Boqing and Yang, Tianbao},
	booktitle={Advances in neural information processing systems},
	pages={2523--2531},
	year={2016}
}

@article{helmbold2017surprising,
	title={Surprising properties of dropout in deep networks},
	author={Helmbold, David P and Long, Philip M},
	journal={The Journal of Machine Learning Research},
	volume={18},
	number={1},
	pages={7284--7311},
	year={2017},
	publisher={JMLR. org}
}

@article{helmbold2015inductive,
	title={On the inductive bias of dropout},
	author={Helmbold, David P and Long, Philip M},
	journal={The Journal of Machine Learning Research},
	volume={16},
	number={1},
	pages={3403--3454},
	year={2015},
	publisher={JMLR. org}
}

@article{blanc2019implicit,
	title={Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck like process},
	author={Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul},
	journal={arXiv preprint arXiv:1904.09080},
	year={2019}
}	

@article{cavazza2017dropout,
	title={Dropout as a low-rank regularizer for matrix factorization},
	author={Cavazza, Jacopo and Morerio, Pietro and Haeffele, Benjamin and Lane, Connor and Murino, Vittorio and Vidal, Ren{\'e}},
	journal={arXiv preprint arXiv:1710.05092},
	year={2017}
}

@inproceedings{wan2013regularization,
	title={Regularization of neural networks using dropconnect},
	author={Wan, Li and Zeiler, Matthew and Zhang, Sixin and Le Cun, Yann and Fergus, Rob},
	booktitle={International conference on machine learning},
	pages={1058--1066},
	year={2013}
}

@article{zhai2018adaptive,
	title={Adaptive dropout with rademacher complexity regularization},
	author={Zhai, Ke and Wang, Huan},
	year={2018}
}

@article{hinton2012improving,
	title={Improving neural networks by preventing co-adaptation of feature detectors},
	author={Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
	journal={arXiv preprint arXiv:1207.0580},
	year={2012}
}

@inproceedings{gao2019demystifying,
	title={Demystifying Dropout},
	author={Gao, Hongchang and Pei, Jian and Huang, Heng},
	booktitle={The 36th International Conference on Machine Learning (ICML 2019)},
	year={2019}
}

@article{bouthillier2015dropout,
	title={Dropout as data augmentation},
	author={Bouthillier, Xavier and Konda, Kishore and Vincent, Pascal and Memisevic, Roland},
	journal={arXiv preprint arXiv:1506.08700},
	year={2015}
}

@article{maeda2014bayesian,
	title={A Bayesian encourages dropout},
	author={Maeda, Shin-ichi},
	journal={arXiv preprint arXiv:1412.7003},
	year={2014}
}

@article{ma2016dropout,
	title={Dropout with expectation-linear regularization},
	author={Ma, Xuezhe and Gao, Yingkai and Hu, Zhiting and Yu, Yaoliang and Deng, Yuntian and Hovy, Eduard},
	journal={arXiv preprint arXiv:1609.08017},
	year={2016}
}

@inproceedings{wang2013fast,
	title={Fast dropout training},
	author={Wang, Sida and Manning, Christopher},
	booktitle={international conference on machine learning},
	pages={118--126},
	year={2013}
}

@article{neelakantan2015adding,
	title={Adding gradient noise improves learning for very deep networks},
	author={Neelakantan, Arvind and Vilnis, Luke and Le, Quoc V and Sutskever, Ilya and Kaiser, Lukasz and Kurach, Karol and Martens, James},
	journal={arXiv preprint arXiv:1511.06807},
	year={2015}
}

@article{zhu2018anisotropic,
	title={The anisotropic noise in stochastic gradient descent: Its behavior of escaping from minima and regularization effects},
	author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
	journal={arXiv preprint arXiv:1803.00195},
	year={2018}
}

@inproceedings{hoffer2017train,
	title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
	author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1731--1741},
	year={2017}
}

@article{keskar2017improving,
	title={Improving generalization performance by switching from adam to sgd},
	author={Keskar, Nitish Shirish and Socher, Richard},
	journal={arXiv preprint arXiv:1712.07628},
	year={2017}
}

@article{keskar2016large,
	title={On large-batch training for deep learning: Generalization gap and sharp minima},
	author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
	journal={arXiv preprint arXiv:1609.04836},
	year={2016}
}

@article{smith2017bayesian,
	title={A bayesian perspective on generalization and stochastic gradient descent},
	author={Smith, Samuel L and Le, Quoc V},
	journal={arXiv preprint arXiv:1710.06451},
	year={2017}
}

@article{xing2018walk,
	title={A walk with sgd},
	author={Xing, Chen and Arpit, Devansh and Tsirigotis, Christos and Bengio, Yoshua},
	journal={arXiv preprint arXiv:1802.08770},
	year={2018}
}

@article{jastrzebski2018relation,
	title={On the relation between the sharpest directions of DNN loss and the SGD step length},
	author={Jastrzebski, Stanislaw and Kenton, Zachary and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
	journal={arXiv preprint arXiv:1807.05031},
	year={2018}
}

@article{jastrzkebski2017three,
	title={Three factors influencing minima in sgd},
	author={Jastrz{\k{e}}bski, Stanis{\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
	journal={arXiv preprint arXiv:1711.04623},
	year={2017}
}

@inproceedings{chaudhari2018stochastic,
	title={Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks},
	author={Chaudhari, Pratik and Soatto, Stefano},
	booktitle={2018 Information Theory and Applications Workshop (ITA)},
	pages={1--10},
	year={2018},
	organization={IEEE}
}

@article{hoffman2019robust,
	title={Robust learning with jacobian regularization},
	author={Hoffman, Judy and Roberts, Daniel A and Yaida, Sho},
	journal={arXiv preprint arXiv:1908.02729},
	year={2019}
}

@inproceedings{wei2019data,
	title={Data-dependent sample complexity of deep neural networks via lipschitz augmentation},
	author={Wei, Colin and Ma, Tengyu},
	booktitle={Advances in Neural Information Processing Systems},
	pages={9722--9733},
	year={2019}
}

@article{wei2019improved,
	title={Improved sample complexities for deep networks and robust classification via an all-layer margin},
	author={Wei, Colin and Ma, Tengyu},
	journal={arXiv preprint arXiv:1910.04284},
	year={2019}
}

@article{nagarajan2019deterministic,
	title={Deterministic PAC-bayesian generalization bounds for deep networks via generalizing noise-resilience},
	author={Nagarajan, Vaishnavh and Kolter, J Zico},
	journal={arXiv preprint arXiv:1905.13344},
	year={2019}
}

@article{krueger2015regularizing,
	title={Regularizing rnns by stabilizing activations},
	author={Krueger, David and Memisevic, Roland},
	journal={arXiv preprint arXiv:1511.08400},
	year={2015}
}

@article{sokolic2017robust,
	title={Robust large margin deep neural networks},
	author={Sokoli{\'c}, Jure and Giryes, Raja and Sapiro, Guillermo and Rodrigues, Miguel RD},
	journal={IEEE Transactions on Signal Processing},
	volume={65},
	number={16},
	pages={4265--4280},
	year={2017},
	publisher={IEEE}
}

@article{kingma2014adam,
	title={Adam: A method for stochastic optimization},
	author={Kingma, Diederik P and Ba, Jimmy},
	journal={arXiv preprint arXiv:1412.6980},
	year={2014}
}

@article{novak2018sensitivity,
	title={Sensitivity and generalization in neural networks: an empirical study},
	author={Novak, Roman and Bahri, Yasaman and Abolafia, Daniel A and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	journal={arXiv preprint arXiv:1802.08760},
	year={2018}
}

@article{zaremba2014recurrent,
	title={Recurrent neural network regularization},
	author={Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
	journal={arXiv preprint arXiv:1409.2329},
	year={2014}
}

@article{merity2018analysis,
	title={{An Analysis of Neural Language Modeling at Multiple Scales}},
	author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
	journal={arXiv preprint arXiv:1803.08240},
	year={2018}
}

@article{melis2017state,
	title={On the state of the art of evaluation in neural language models},
	author={Melis, G{\'a}bor and Dyer, Chris and Blunsom, Phil},
	journal={arXiv preprint arXiv:1707.05589},
	year={2017}
}

@article{semeniuta2016recurrent,
	title={Recurrent dropout without memory loss},
	author={Semeniuta, Stanislau and Severyn, Aliaksei and Barth, Erhardt},
	journal={arXiv preprint arXiv:1603.05118},
	year={2016}
}

@article{merity2017revisiting,
	title={Revisiting activation regularization for language rnns},
	author={Merity, Stephen and McCann, Bryan and Socher, Richard},
	journal={arXiv preprint arXiv:1708.01009},
	year={2017}
}

@inproceedings{gal2016theoretically,
	title={A theoretically grounded application of dropout in recurrent neural networks},
	author={Gal, Yarin and Ghahramani, Zoubin},
	booktitle={Advances in neural information processing systems},
	pages={1019--1027},
	year={2016}
}

@article{mianjy2019dropout,
	title={On dropout and nuclear norm regularization},
	author={Mianjy, Poorya and Arora, Raman},
	journal={arXiv preprint arXiv:1905.11887},
	year={2019}
}

@article{tan2019efficientnet,
	title={Efficientnet: Rethinking model scaling for convolutional neural networks},
	author={Tan, Mingxing and Le, Quoc V},
	journal={arXiv preprint arXiv:1905.11946},
	year={2019}
}

@inproceedings{bartlett2017spectrally,
	title={Spectrally-normalized margin bounds for neural networks},
	author={Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
	booktitle={Advances in Neural Information Processing Systems},
	pages={6240--6249},
	year={2017}
}

@article{golowich2017size,
	title={Size-independent sample complexity of neural networks},
	author={Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},
	journal={arXiv preprint arXiv:1712.06541},
	year={2017}
}

@article{neyshabur2017pac,
	title={A pac-bayesian approach to spectrally-normalized margin bounds for neural networks},
	author={Neyshabur, Behnam and Bhojanapalli, Srinadh and Srebro, Nathan},
	journal={arXiv preprint arXiv:1707.09564},
	year={2017}
}

@misc{
	arora2020dropout,
	title={Dropout: Explicit Forms and Capacity Control},
	author={Raman Arora and Peter L. Bartlett and Poorya Mianjy and Nathan Srebro},
	year={2020},
	url={https://openreview.net/forum?id=Bylthp4Yvr}
}

@inproceedings{wager2014altitude,
	title={Altitude training: Strong bounds for single-layer dropout},
	author={Wager, Stefan and Fithian, William and Wang, Sida and Liang, Percy S},
	booktitle={Advances in Neural Information Processing Systems},
	pages={100--108},
	year={2014}
}

 @article{dziugaite2017computing,
 	title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
 	author={Dziugaite, Gintare Karolina and Roy, Daniel M},
 	journal={arXiv preprint arXiv:1703.11008},
 	year={2017}
 }

@article{zhang2016understanding,
	title={Understanding deep learning requires rethinking generalization},
	author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	journal={arXiv preprint arXiv:1611.03530},
	year={2016}
}

@article{neyshabur2018towards,
	title={Towards understanding the role of over-parametrization in generalization of neural networks},
	author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
	journal={arXiv preprint arXiv:1805.12076},
	year={2018}
}

@article{soudry2018implicit,
	title={The implicit bias of gradient descent on separable data},
	author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
	journal={The Journal of Machine Learning Research},
	volume={19},
	number={1},
	pages={2822--2878},
	year={2018},
	publisher={JMLR. org}
}
@inproceedings{gunasekar2018implicit,
	title={Implicit bias of gradient descent on linear convolutional networks},
	author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
	booktitle={Advances in Neural Information Processing Systems},
	pages={9461--9471},
	year={2018}
}

@article{woodworth2019kernel,
	title={Kernel and deep regimes in overparametrized models},
	author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
	journal={arXiv preprint arXiv:1906.05827},
	year={2019}
}

@inproceedings{yao2018hessian,
	title={Hessian-based analysis of large batch training and robustness to adversaries},
	author={Yao, Zhewei and Gholami, Amir and Lei, Qi and Keutzer, Kurt and Mahoney, Michael W},
	booktitle={Advances in Neural Information Processing Systems},
	pages={4949--4959},
	year={2018}
}

@article{Wei2019HowNA,
  title={How noise affects the Hessian spectrum in overparameterized neural networks},
  author={Mingwei Wei and David J. Schwab},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.00195}
}
