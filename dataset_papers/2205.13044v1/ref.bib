@article{suk2021tracking,
  title={Tracking Most Severe Arm Changes in Bandits},
  author={Suk, Joe and Kpotufe, Samory},
  journal={arXiv preprint arXiv:2112.13838},
  year={2021}
}

@article{abbasi2022new,
  title={A New Look at Dynamic Regret for Non-Stationary Stochastic Bandits},
  author={Abbasi-Yadkori, Yasin and Gyorgy, Andras and Lazic, Nevena},
  journal={arXiv preprint arXiv:2201.06532},
  year={2022}
}

@article{faury2021regret,
  title={Regret bounds for generalized linear bandits under parameter drift},
  author={Faury, Louis and Russac, Yoan and Abeille, Marc and Calauz{\`e}nes, Cl{\'e}ment},
  journal={arXiv preprint arXiv:2103.05750},
  year={2021}
}

@article{russac2020algorithms,
  title={Algorithms for non-stationary generalized linear bandits},
  author={Russac, Yoan and Capp{\'e}, Olivier and Garivier, Aur{\'e}lien},
  journal={arXiv preprint arXiv:2003.10113},
  year={2020}
}

@inproceedings{chen2021combinatorial,
  title={Combinatorial semi-bandit in the non-stationary environment},
  author={Chen, Wei and Wang, Liwei and Zhao, Haoyu and Zheng, Kai},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={865--875},
  year={2021},
  organization={PMLR}
}

@inproceedings{chen2019new,
  title={A new algorithm for non-stationary contextual bandits: Efficient, optimal and parameter-free},
  author={Chen, Yifang and Lee, Chung-Wei and Luo, Haipeng and Wei, Chen-Yu},
  booktitle={Conference on Learning Theory},
  pages={696--726},
  year={2019},
  organization={PMLR}
}

@inproceedings{auer2019adaptively,
  title={Adaptively tracking the best bandit arm with an unknown number of distribution changes},
  author={Auer, Peter and Gajane, Pratik and Ortner, Ronald},
  booktitle={Conference on Learning Theory},
  pages={138--158},
  year={2019},
  organization={PMLR}
}

@inproceedings{wei2022model,
  title={A model selection approach for corruption robust reinforcement learning},
  author={Wei, Chen-Yu and Dann, Christoph and Zimmert, Julian},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={1043--1096},
  year={2022},
  organization={PMLR}
}

@inproceedings{lykouris2021corruption,
  title={Corruption-robust exploration in episodic reinforcement learning},
  author={Lykouris, Thodoris and Simchowitz, Max and Slivkins, Alex and Sun, Wen},
  booktitle={Conference on Learning Theory},
  pages={3242--3245},
  year={2021},
  organization={PMLR}
}

@article{ding2022provably,
  title={Provably Efficient Primal-Dual Reinforcement Learning for {CMDP}s with Non-stationary Objectives and Constraints},
  author={Ding, Yuhao and Lavaei, Javad},
  journal={arXiv preprint arXiv:2201.11965},
  year={2022}
}

@article{touati2020efficient,
  title={Efficient learning in non-stationary linear {M}arkov decision processes},
  author={Touati, Ahmed and Vincent, Pascal},
  journal={arXiv preprint arXiv:2010.12870},
  year={2020}
}

@article{zhou2020nonstationary,
  title={Nonstationary reinforcement learning with linear function approximation},
  author={Zhou, Huozhi and Chen, Jinglin and Varshney, Lav R and Jagmohan, Ashish},
  journal={arXiv preprint arXiv:2010.04244},
  year={2020}
}

@inproceedings{domingues2021kernel,
  title={A kernel-based approach to non-stationary reinforcement learning in metric spaces},
  author={Domingues, Omar Darwiche and M{\'e}nard, Pierre and Pirotta, Matteo and Kaufmann, Emilie and Valko, Michal},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3538--3546},
  year={2021},
  organization={PMLR}
}

@article{fei2020dynamic,
  title={Dynamic regret of policy optimization in non-stationary environments},
  author={Fei, Yingjie and Yang, Zhuoran and Wang, Zhaoran and Xie, Qiaomin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6743--6754},
  year={2020}
}

@inproceedings{cheung2020reinforcement,
  title={Reinforcement learning for non-stationary {M}arkov decision processes: The blessing of (more) optimism},
  author={Cheung, Wang Chi and Simchi-Levi, David and Zhu, Ruihao},
  booktitle={International Conference on Machine Learning},
  pages={1843--1854},
  year={2020},
  organization={PMLR}
}

@InProceedings{pmlr-v115-ortner20a,
  title = 	 {Variational Regret Bounds for Reinforcement Learning},
  author =       {Ortner, Ronald and Gajane, Pratik and Auer, Peter},
  booktitle = 	 {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference},
  pages = 	 {81--90},
  year = 	 {2020},
  editor = 	 {Adams, Ryan P. and Gogate, Vibhav},
  volume = 	 {115},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {22--25 Jul},
  publisher =    {PMLR}
}


@article{gajane2018sliding,
  title={A sliding-window algorithm for {M}arkov decision processes with arbitrarily changing rewards and transitions},
  author={Gajane, Pratik and Ortner, Ronald and Auer, Peter},
  journal={arXiv preprint arXiv:1805.10066},
  year={2018}
}

@article{chen2022policy,
  title={Policy Optimization for Stochastic Shortest Path},
  author={Chen, Liyu and Luo, Haipeng and Rosenberg, Aviv},
  journal={Conference on Learning Theory},
  year={2022}
}

@inproceedings{mao2020model,
  title={Near-Optimal Model-Free Reinforcement Learning in Non-Stationary Episodic MDPs},
  author={Mao, Weichao and Zhang, Kaiqing and Zhu, Ruihao and Simchi-Levi, David and Basar, Tamer},
  booktitle={International Conference on Machine Learning},
  pages={7447--7458},
  year={2021},
  organization={PMLR}
}

@inproceedings{wei2021non,
  title={Non-stationary reinforcement learning without prior knowledge: An optimal black-box approach},
  author={Wei, Chen-Yu and Luo, Haipeng},
  booktitle={Conference on Learning Theory},
  pages={4300--4354},
  year={2021},
  organization={PMLR}
}

@article{gerchinovitz2016refined,
  title={Refined lower bounds for adversarial bandits},
  author={Gerchinovitz, S{\'e}bastien and Lattimore, Tor},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@article{ortner2018regret,
  title={Regret Bounds for Reinforcement Learning via {M}arkov Chain Concentration},
  author={Ortner, Ronald},
  journal={arXiv preprint arXiv:1808.01813},
  year={2018}
}

@inproceedings{fruit2018efficient,
  title={Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning},
  author={Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro and Ortner, Ronald},
  booktitle={International Conference on Machine Learning},
  pages={1573--1581},
  year={2018}
}

@inproceedings{talebi2018variance,
  title={Variance-Aware Regret Bounds for Undiscounted Reinforcement Learning in {MDPs}},
  author={Talebi, Mohammad Sadegh and Maillard, Odalric-Ambrym},
  booktitle={Algorithmic Learning Theory},
  pages={770--805},
  year={2018}
}

@inproceedings{abbasi2019politex,
  title={POLITEX: Regret bounds for policy iteration using expert prediction},
  author={Abbasi-Yadkori, Yasin and Bartlett, Peter and Bhatia, Kush and Lazic, Nevena and Szepesvari, Csaba and Weisz, Gell{\'e}rt},
  booktitle={International Conference on Machine Learning},
  pages={3692--3702},
  year={2019}
}

@inproceedings{zhang2019regret,
  title={Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function},
  author={Zhang, Zihan and Ji, Xiangyang},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@inproceedings{wei2021learning,
  title={Learning infinite-horizon average-reward {MDPs} with linear function approximation},
  author={Wei, Chen-Yu and Jahromi, Mehdi Jafarnia and Luo, Haipeng and Jain, Rahul},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3007--3015},
  year={2021},
  organization={PMLR}
}

@article{dong2020q,
  title={Q-learning with {UCB} exploration is sample efficient for infinite-horizon {MDP}},
  author={Dong, Kefan and Wang, Yuanhao and Chen, Xiaoyu and Wang, Liwei},
  journal={International Conference on Learning Representations},
  year={2020}
}

@article{agarwal2021markov,
  title={Markov Decision Processes with Long-Term Average Constraints},
  author={Agarwal, Mridul and Bai, Qinbo and Aggarwal, Vaneet},
  journal={arXiv preprint arXiv:2106.06680},
  year={2021}
}

@article{agarwal2021concave,
  title={Concave Utility Reinforcement Learning with Zero-Constraint Violations},
  author={Agarwal, Mridul and Bai, Qinbo and Aggarwal, Vaneet},
  journal={arXiv preprint arXiv:2109.05439},
  year={2021}
}

@article{hazan2019introduction,
  title={Introduction to online convex optimization},
  author={Hazan, Elad},
  journal={arXiv preprint arXiv:1909.05207},
  year={2019}
}

@inproceedings{liu2021efficient,
  title={An Efficient Pessimistic-Optimistic Algorithm for Stochastic Linear Bandits with General Constraints},
  author={Liu, Xin and Li, Bin and Shi, Pengyi and Ying, Lei},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@inproceedings{pacchiano2021stochastic,
  title={Stochastic bandits with linear constraints},
  author={Pacchiano, Aldo and Ghavamzadeh, Mohammad and Bartlett, Peter and Jiang, Heinrich},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2827--2835},
  year={2021},
  organization={PMLR}
}

@inproceedings{amani2019linear,
  title={Linear stochastic bandits under safety constraints},
  author={Amani, Sanae and Alizadeh, Mahnoosh and Thrampoulidis, Christos},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@inproceedings{garcelon2020improved,
  title={Improved algorithms for conservative exploration in bandits},
  author={Garcelon, Evrard and Ghavamzadeh, Mohammad and Lazaric, Alessandro and Pirotta, Matteo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={3962--3969},
  year={2020}
}

@inproceedings{kazerouni2016conservative,
  title={Conservative contextual linear bandits},
  author={Kazerouni, Abbas and Ghavamzadeh, Mohammad and Abbasi-Yadkori, Yasin and Van Roy, Benjamin},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}

@inproceedings{wu2016conservative,
  title={Conservative bandits},
  author={Wu, Yifan and Shariff, Roshan and Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  booktitle={International Conference on Machine Learning},
  pages={1254--1262},
  year={2016},
  organization={PMLR}
}

@article{bartlett2012regal,
  title={REGAL: A regularization based algorithm for reinforcement learning in weakly communicating {MDPs}},
  author={Bartlett, Peter L and Tewari, Ambuj},
  journal={arXiv preprint arXiv:1205.2661},
  year={2012}
}

@inproceedings{bartlett2009regal,
  title={REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs},
  author={Bartlett, Peter L and Tewari, Ambuj},
  booktitle={Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
  pages={35--42},
  year={2009},
  organization={AUAI Press}
}

@article{even2003learning,
  title={Learning Rates for Q-learning.},
  author={Even-Dar, Eyal and Mansour, Yishay and Bartlett, Peter},
  journal={Journal of machine learning Research},
  volume={5},
  number={1},
  year={2003}
}

@inproceedings{strehl2006pac,
  title={PAC model-free reinforcement learning},
  author={Strehl, Alexander L and Li, Lihong and Wiewiora, Eric and Langford, John and Littman, Michael L},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={881--888},
  year={2006}
}

@inproceedings{zheng2020constrained,
  title={Constrained upper confidence reinforcement learning},
  author={Zheng, Liyuan and Ratliff, Lillian},
  booktitle={Learning for Dynamics and Control},
  pages={620--629},
  year={2020},
  organization={PMLR}
}

@article{chen2021primal,
  title={A primal-dual approach to constrained {M}arkov decision processes},
  author={Chen, Yi and Dong, Jing and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2101.10895},
  year={2021}
}

@article{liu2021fast,
  title={Fast Global Convergence of Policy Optimization for Constrained MDPs},
  author={Liu, Tao and Zhou, Ruida and Kalathil, Dileep and Kumar, PR and Tian, Chao},
  journal={arXiv preprint arXiv:2111.00552},
  year={2021}
}

@inproceedings{tessler2018reward,
  title={Reward constrained policy optimization},
  author={Tessler, Chen and Mankowitz, Daniel J and Mannor, Shie},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@inproceedings{liang2018accelerated,
  title={Accelerated primal-dual policy optimization for safe reinforcement learning},
  author={Liang, Qingkai and Que, Fanyu and Modiano, Eytan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}

@inproceedings{kalagarla2020sample,
  title={A sample-efficient algorithm for episodic finite-horizon {MDP} with constraints},
  author={Kalagarla, Krishna C and Jain, Rahul and Nuzzo, Pierluigi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2021}
}

@inproceedings{qiu2020upper,
  title={Upper confidence primal-dual reinforcement learning for {CMDP} with adversarial loss},
  author={Qiu, Shuang and Wei, Xiaohan and Yang, Zhuoran and Ye, Jieping and Wang, Zhaoran},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{liu2021learning,
  title={Learning Policies with Zero or Bounded Constraint Violation for Constrained {MDPs}},
  author={Liu, Tao and Zhou, Ruida and Kalathil, Dileep and Kumar, PR and Tian, Chao},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{efroni2020exploration,
  title={Exploration-exploitation in constrained mdps},
  author={Efroni, Yonathan and Mannor, Shie and Pirotta, Matteo},
  journal={arXiv preprint arXiv:2003.02189},
  year={2020}
}

@book{altman1999constrained,
  title={Constrained {M}arkov decision processes},
  author={Altman, Eitan},
  volume={7},
  year={1999},
  publisher={CRC Press}
}

@inproceedings{chen2021improved,
  title={Improved No-Regret Algorithms for Stochastic Shortest Path with Linear {MDP}},
  author={Chen, Liyu and Jain, Rahul and Luo, Haipeng},
  booktitle={International Conference on Machine Learning},
  year={2022}
}

@article{weissman2003inequalities,
  title={Inequalities for the L1 deviation of the empirical distribution},
  author={Weissman, Tsachy and Ordentlich, Erik and Seroussi, Gadiel and Verdu, Sergio and Weinberger, Marcelo J},
  journal={Hewlett-Packard Labs, Tech. Rep},
  year={2003}
}

@article{chen2021impossible,
  title={Impossible Tuning Made Possible: A New Expert Algorithm and Its Applications},
  author={Chen, Liyu and Luo, Haipeng and Wei, Chen-Yu},
  journal={International Conference on Algorithmic Learning Theory},
  year={2021}
}

@article{luo2021policy,
  title={Policy Optimization in Adversarial MDPs: Improved Exploration via Dilated Bonuses},
  author={Luo, Haipeng and Wei, Chen-Yu and Lee, Chung-Wei},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@misc{puterman1994markov,
  title={Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  author={Puterman, Martin L},
  year={1994},
  publisher={John Wiley \& Sons, Inc.}
}

@article{singh2020learning,
  title={Learning in {M}arkov decision processes under constraints},
  author={Singh, Rahul and Gupta, Abhishek and Shroff, Ness B},
  journal={arXiv preprint arXiv:2002.12435},
  year={2020}
}

@article{kim2021improved,
  title={Improved Regret Analysis for Variance-Adaptive Linear Bandits and Horizon-Free Linear Mixture {MDP}s},
  author={Kim, Yeoneung and Yang, Insoon and Jun, Kwang-Sung},
  journal={arXiv preprint arXiv:2111.03289},
  year={2021}
}

@article{min2021learning,
  title={Learning Stochastic Shortest Path with Linear Function Approximation},
  author={Min, Yifei and He, Jiafan and Wang, Tianhao and Gu, Quanquan},
  journal={arXiv preprint arXiv:2110.12727},
  year={2021}
}

@article{wei2021model,
  title={A Model Selection Approach for Corruption Robust Reinforcement Learning},
  author={Wei, Chen-Yu and Dann, Christoph and Zimmert, Julian},
  journal={arXiv preprint arXiv:2110.03580},
  year={2021}
}

@article{jin2021best,
  title={The best of both worlds: stochastic and adversarial episodic {MDP}s with unknown transition},
  author={Jin, Tiancheng and Huang, Longbo and Luo, Haipeng},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{simchowitz2019non,
  title={Non-asymptotic gap-dependent regret bounds for tabular {MDP}s},
  author={Simchowitz, Max and Jamieson, Kevin G},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={1153--1162},
  year={2019}
}

@article{kong2021online,
  title={Online Sub-Sampling for Reinforcement Learning with General Function Approximation},
  author={Kong, Dingwen and Salakhutdinov, Ruslan and Wang, Ruosong and Yang, Lin F},
  journal={arXiv preprint arXiv:2106.07203},
  year={2021}
}

@article{ishfaq2021randomized,
  title={Randomized Exploration for Reinforcement Learning with General Value Function Approximation},
  author={Ishfaq, Haque and Cui, Qiwen and Nguyen, Viet and Ayoub, Alex and Yang, Zhuoran and Wang, Zhaoran and Precup, Doina and Yang, Lin F},
  journal={International Conference on Machine Learning},
  year={2021}
}

@article{wang2020reinforcement,
  title={Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension},
  author={Wang, Ruosong and Salakhutdinov, Ruslan and Yang, Lin F},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{lattimore2012pac,
  title={{PAC} bounds for discounted {MDP}s},
  author={Lattimore, Tor and Hutter, Marcus},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={320--334},
  year={2012},
  organization={Springer}
}

@inproceedings{yang2020reinforcement,
  title={Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound},
  author={Yang, Lin and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={10746--10756},
  year={2020},
  organization={PMLR}
}

@inproceedings{zanette2020frequentist,
  title={Frequentist regret bounds for randomized least-squares value iteration},
  author={Zanette, Andrea and Brandfonbrener, David and Brunskill, Emma and Pirotta, Matteo and Lazaric, Alessandro},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1954--1964},
  year={2020},
  organization={PMLR}
}

@article{jafarnia2021online,
  title={Online Learning for Stochastic Shortest Path Model via Posterior Sampling},
  author={Jafarnia-Jahromi, Mehdi and Chen, Liyu and Jain, Rahul and Luo, Haipeng},
  journal={arXiv preprint arXiv:2106.05335},
  year={2021}
}

@inproceedings{he2021logarithmic,
  title={Logarithmic regret for reinforcement learning with linear function approximation},
  author={He, Jiafan and Zhou, Dongruo and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={4171--4180},
  year={2021},
  organization={PMLR}
}

@inproceedings{zanette2020learning,
  title={Learning near optimal policies with low inherent bellman error},
  author={Zanette, Andrea and Lazaric, Alessandro and Kochenderfer, Mykel and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={10978--10989},
  year={2020},
  organization={PMLR}
}

@inproceedings{ayoub2020model,
  title={Model-based reinforcement learning with value-targeted regression},
  author={Ayoub, Alex and Jia, Zeyu and Szepesvari, Csaba and Wang, Mengdi and Yang, Lin},
  booktitle={International Conference on Machine Learning},
  pages={463--474},
  year={2020},
  organization={PMLR}
}

@inproceedings{zhou2021nearly,
  title={Nearly minimax optimal reinforcement learning for linear mixture {M}arkov decision processes},
  author={Zhou, Dongruo and Gu, Quanquan and Szepesvari, Csaba},
  booktitle={Conference on Learning Theory},
  pages={4532--4576},
  year={2021},
  organization={PMLR}
}

@inproceedings{zhou2021provably,
  title={Provably efficient reinforcement learning for discounted {MDP}s with feature mapping},
  author={Zhou, Dongruo and He, Jiafan and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={12793--12802},
  year={2021},
  organization={PMLR}
}

@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}

@article{chen2021implicit,
  title={Implicit Finite-Horizon Approximation and Efficient Optimal Algorithms for Stochastic Shortest Path},
  author={Chen, Liyu and Jafarnia-Jahromi, Mehdi and Jain, Rahul and Luo, Haipeng},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020},
  organization={PMLR}
}

@article{zhang2021variance,
  title={Variance-Aware Confidence Set: Variance-Dependent Bound for Linear Bandits and Horizon-Free Bound for Linear Mixture {MDP}},
  author={Zhang, Zihan and Yang, Jiaqi and Ji, Xiangyang and Du, Simon S},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{abbasi2011improved,
  title={Improved algorithms for linear stochastic bandits},
  author={Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
  journal={Advances in Neural Information Processing Systems},
  volume={24},
  pages={2312--2320},
  year={2011}
}

@article{vial2021regret,
  title={Regret Bounds for Stochastic Shortest Path Problems with Linear Function Approximation},
  author={Vial, Daniel and Parulekar, Advait and Shakkottai, Sanjay and Srikant, R},
  journal={arXiv preprint arXiv:2105.01593},
  year={2021}
}

@article{yu2013boundedness,
  title={On boundedness of {Q}-learning iterates for stochastic shortest path problems},
  author={Yu, Huizhen and Bertsekas, Dimitri P},
  journal={Mathematics of Operations Research},
  volume={38},
  number={2},
  pages={209--227},
  year={2013},
  publisher={INFORMS}
}

@article{wang2020long,
  title={Is Long Horizon {RL} More Difficult Than Short Horizon {RL}?},
  author={Wang, Ruosong and Du, Simon S and Yang, Lin and Kakade, Sham},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{efroni2019tight,
 author = {Efroni, Yonathan and Merlis, Nadav and Ghavamzadeh, Mohammad and Mannor, Shie},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {}, 
 publisher = {Curran Associates, Inc.},
 title = {Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies},
 volume = {32},
 year = {2019}
}

@InProceedings{zanette2019tighter, title = {Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds}, author = {Zanette, Andrea and Brunskill, Emma}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {7304--7312}, year = {2019}}

@InProceedings{efroni2020optimistic, title = {Optimistic Policy Optimization with Bandit Feedback}, author = {Shani, Lior and Efroni, Yonathan and Rosenberg, Aviv and Mannor, Shie}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {8604--8613}, year = {2020}}

@inproceedings{zimin2013online,
  title={Online learning in episodic {M}arkovian decision processes by relative entropy policy search},
  author={Zimin, Alexander and Neu, Gergely},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1583--1591},
  year={2013}
}

@inproceedings{neu2012adversarial,
  title={The adversarial stochastic shortest path problem with unknown transition probabilities},
  author={Neu, Gergely and Gyorgy, Andras and Szepesv{\'a}ri, Csaba},
  booktitle={Artificial Intelligence and Statistics},
  pages={805--813},
  year={2012}
}

@InProceedings{rosenberg2019online, title = {Online Convex Optimization in Adversarial {M}arkov Decision Processes}, author = {Rosenberg, Aviv and Mansour, Yishay}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {5478--5486}, year = {2019}}

@InProceedings{jin2019learning, title = {Learning Adversarial {M}arkov Decision Processes with Bandit Feedback and Unknown Transition}, author = {Jin, Chi and Jin, Tiancheng and Luo, Haipeng and Sra, Suvrit and Yu, Tiancheng}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {4860--4869}, year = {2020}}

@inproceedings{tarbouriech2020improved,
 author = {Tarbouriech, Jean and Pirotta, Matteo and Valko, Michal and Lazaric, Alessandro},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {11273--11284},
 publisher = {Curran Associates, Inc.},
 title = {Improved Sample Complexity for Incremental Autonomous Exploration in {MDP}s},
 volume = {33},
 year = {2020}
}

@inproceedings{lim2012autonomous,
  title={Autonomous exploration for navigating in {MDP}s},
  author={Lim, Shiau Hong and Auer, Peter},
  booktitle={Conference on Learning Theory},
  pages={40--1},
  year={2012},
  organization={JMLR Workshop and Conference Proceedings}
}

@inproceedings{tarbouriech2021sample,
  title={Sample Complexity Bounds for Stochastic Shortest Path with a Generative Model},
  author={Tarbouriech, Jean and Pirotta, Matteo and Valko, Michal and Lazaric, Alessandro},
  booktitle={Algorithmic Learning Theory},
  pages={1157--1178},
  year={2021},
  organization={PMLR}
}

@article{bertsekas1991analysis,
  title={An analysis of stochastic shortest path problems},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  journal={Mathematics of Operations Research},
  volume={16},
  number={3},
  pages={580--595},
  year={1991},
  publisher={INFORMS}
}

@inproceedings{zhang2020reinforcement,
  title={Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon},
  author={Zhang, Zihan and Ji, Xiangyang and Du, Simon S},
  booktitle={Conference On Learning Theory},
  year={2020}
}

@article{tarbouriech2021stochastic,
  title={Stochastic Shortest Path: Minimax, Parameter-Free and Towards Horizon-Free Regret},
  author={Tarbouriech, Jean and Zhou, Runlong and Du, Simon S and Pirotta, Matteo and Valko, Michal and Lazaric, Alessandro},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@inproceedings{chen2021minimax,
  title={Minimax regret for stochastic shortest path with adversarial costs and known transition},
  author={Chen, Liyu and Luo, Haipeng and Wei, Chen-Yu},
  booktitle={Conference on Learning Theory},
  pages={1180--1215},
  year={2021},
  organization={PMLR}
}

@inproceedings{chen2021finding,
  title={Finding the stochastic shortest path with low regret: The adversarial cost and unknown transition case},
  author={Chen, Liyu and Luo, Haipeng},
  booktitle={International Conference on Machine Learning},
  year={2021},
}

@article{bertsekas2013stochastic,
  title={Stochastic shortest path problems under weak conditions},
  author={Bertsekas, Dimitri P and Yu, Huizhen},
  journal={Lab. for Information and Decision Systems Report LIDS-P-2909, MIT},
  year={2013}
}

@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}

@article{jaksch2010near,
  title={Near-optimal Regret Bounds for Reinforcement Learning.},
  author={Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={4},
  year={2010}
}

@inproceedings{efroni2021confidence,
  title={Confidence-Budget Matching for Sequential Budgeted Learning},
  author={Efroni, Yonathan and Merlis, Nadav and Saha, Aadirupa and Mannor, Shie},
  booktitle={International Conference on Machine Learning},
  year={2021}
}

@article{he2020minimax,
  title={Minimax Optimal Reinforcement Learning for Discounted {MDP}s},
  author={He, Jiafan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:2010.00587},
  year={2020}
}

@inproceedings{lee2020bias,
 author = {Lee, Chung-Wei and Luo, Haipeng and Wei, Chen-Yu and Zhang, Mengxiao},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {15522--15533},
 publisher = {Curran Associates, Inc.},
 title = {Bias no more: high-probability data-dependent regret bounds for adversarial bandits and {MDP}s},
 volume = {33},
 year = {2020}
}

@inproceedings{chen2020minimax,
  title={Minimax Regret for Stochastic Shortest Path with Adversarial Costs and Known Transition},
  author={Chen, Liyu and Luo, Haipeng and Wei, Chen-Yu},
  booktitle={Conference On Learning Theory},
  year={2021},
}

@article{cohen2021minimax,
  title={Minimax Regret for Stochastic Shortest Path},
  author={Cohen, Alon and Efroni, Yonathan and Mansour, Yishay and Rosenberg, Aviv},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@book{puterman2014markov,
  title={{M}arkov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@article{osband2016lower,
  title={On lower bounds for regret in reinforcement learning},
  author={Osband, Ian and Van Roy, Benjamin},
  journal={arXiv preprint arXiv:1608.02732},
  year={2016}
}

@inproceedings{wei2018more,
  title={More adaptive algorithms for adversarial bandits},
  author={Wei, Chen-Yu and Luo, Haipeng},
  booktitle={Conference On Learning Theory},
  pages={1263--1291},
  year={2018},
  organization={PMLR}
}

% normal citation

@article{luque2016iteration,
  title={Iteration algorithms in {M}arkov decision processes with state-action-dependent discount factors and unbounded costs},
  author={Luque-V{\'a}squez, Fernando and Minj{\'a}rez-Sosa, J Adolfo},
  journal={Operations Research: the Art of Making Good Decisions},
  pages={55},
  year={2016},
  publisher={BoD--Books on Demand}
}

@inproceedings{tarbouriech2020no,
  title={No-regret exploration in goal-oriented reinforcement learning},
  author={Tarbouriech, Jean and Garcelon, Evrard and Valko, Michal and Pirotta, Matteo and Lazaric, Alessandro},
  booktitle={International Conference on Machine Learning},
  pages={9428--9437},
  year={2020},
  organization={PMLR}
}

@inproceedings{wei2020model,
  title={Model-free reinforcement learning in infinite-horizon average-reward {M}arkov decision processes},
  author={Wei, Chen-Yu and Jahromi, Mehdi Jafarnia and Luo, Haipeng and Sharma, Hiteshi and Jain, Rahul},
  booktitle={International Conference on Machine Learning},
  pages={10170--10180},
  year={2020},
  organization={PMLR}
}

@inproceedings{jin2018q,
  title={Is {Q}-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4863--4873},
  year={2018}
}

@inproceedings{rosenberg2020adversarial,
  author    = {Aviv Rosenberg and
               Yishay Mansour},
  editor    = {Zhi{-}Hua Zhou},
  title     = {Stochastic Shortest Path with Adversarially Changing Costs},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial
               Intelligence, {IJCAI} 2021, Virtual Event / Montreal, Canada, 19-27
               August 2021},
  year      = {2021}
}

@InProceedings{cohen2020near, title = {Near-optimal Regret Bounds for Stochastic Shortest Path}, author = {Cohen, Alon and Kaplan, Haim and Mansour, Yishay and Rosenberg, Aviv}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {8210--8219}, year = {2020}, volume = {119}, publisher = {PMLR} }

@inproceedings{zhang2020almost,
 author = {Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {15198--15207},
 publisher = {Curran Associates, Inc.},
 title = {Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage Decomposition},
 volume = {33},
 year = {2020}
}

% application citation
@inproceedings{andrychowicz2017hindsight,
 author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 title = {Hindsight Experience Replay},
 volume = {30},
 year = {2017}
}

@inproceedings{nasiriany2019planning,
 author = {Nasiriany, Soroush and Pong, Vitchyr and Lin, Steven and Levine, Sergey},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {}, 
 publisher = {Curran Associates, Inc.},
 title = {Planning with Goal-Conditioned Policies},
 volume = {32},
 year = {2019}
}