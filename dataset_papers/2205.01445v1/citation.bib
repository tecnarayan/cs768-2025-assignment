@article{benaych2011eigenvalues,
	title={The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices},
	author={Benaych-Georges, Florent and Nadakuditi, Raj Rao},
	journal={Advances in Mathematics},
	volume={227},
	number={1},
	pages={494--521},
	year={2011},
	publisher={Elsevier}
}

@article{benaych2012singular,
	title={The singular values and vectors of low rank perturbations of large rectangular random matrices},
	author={Benaych-Georges, Florent and Nadakuditi, Raj Rao},
	journal={Journal of Multivariate Analysis},
	volume={111},
	pages={120--135},
	year={2012},
	publisher={Elsevier}
}

@article{baik2005phase,
	title={Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices},
	author={Baik, Jinho and Arous, G{\'e}rard Ben and P{\'e}ch{\'e}, Sandrine},
	journal={The Annals of Probability},
	volume={33},
	number={5},
	pages={1643--1697},
	year={2005},
	publisher={Institute of Mathematical Statistics}
}

@article{hastie2019surprises,
  title={Surprises in High-Dimensional Ridgeless Least Squares Interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={arXiv preprint arXiv:1903.08560},
  year={2019}
}

@article{zhang2016understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{lee2020finite,
  title={Finite versus infinite neural networks: an empirical study},
  author={Lee, Jaehoon and Schoenholz, Samuel and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and Sohl-Dickstein, Jascha},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15156--15172},
  year={2020}
}

@article{geiger2020disentangling,
  title={Disentangling feature and lazy training in deep neural networks},
  author={Geiger, Mario and Spigler, Stefano and Jacot, Arthur and Wyart, Matthieu},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2020},
  number={11},
  pages={113301},
  year={2020},
  publisher={IOP Publishing}
}

@article{williams2019gradient,
  title={Gradient dynamics of shallow univariate relu networks},
  author={Williams, Francis and Trager, Matthew and Panozzo, Daniele and Silva, Claudio and Zorin, Denis and Bruna, Joan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{allen2019can,
  title={What can resnet learn efficiently, going beyond kernels?},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{seddik2020random,
  title={Random matrix theory proves that deep learning representations of gan-data behave as gaussian mixtures},
  author={Seddik, Mohamed El Amine and Louart, Cosme and Tamaazousti, Mohamed and Couillet, Romain},
  booktitle={International Conference on Machine Learning},
  pages={8573--8582},
  year={2020},
  organization={PMLR}
}

@inproceedings{refinetti2021classifying,
  title={Classifying high-dimensional Gaussian mixtures: Where kernel methods fail and neural networks succeed},
  author={Refinetti, Maria and Goldt, Sebastian and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  booktitle={International Conference on Machine Learning},
  pages={8936--8947},
  year={2021},
  organization={PMLR}
}

@inproceedings{li2019towards,
  title={Towards explaining the regularization effect of initial large learning rate in training neural networks},
  author={Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11674--11685},
  year={2019}
}

@article{cacoullos1982upper,
  title={On upper and lower bounds for the variance of a function of a random variable},
  author={Cacoullos, Theophilos and others},
  journal={The Annals of Probability},
  volume={10},
  number={3},
  pages={799--809},
  year={1982},
  publisher={Institute of Mathematical Statistics}
}

@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{spigler2018jamming,
  title={A jamming transition from under-to over-parametrization affects generalization in deep learning},
  author={Spigler, Stefano and Geiger, Mario and d’Ascoli, St{\'e}phane and Sagun, Levent and Biroli, Giulio and Wyart, Matthieu},
  journal={Journal of Physics A: Mathematical and Theoretical},
  volume={52},
  number={47},
  pages={474001},
  year={2019},
  publisher={IOP Publishing}
}
 
@article{liang2018just,
  title={Just interpolate: Kernel “ridgeless” regression can generalize},
  author={Liang, Tengyuan and Rakhlin, Alexander},
  journal={The Annals of Statistics},
  volume={48},
  number={3},
  pages={1329--1347},
  year={2020},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{gretton2005measuring,
  title={Measuring statistical dependence with Hilbert-Schmidt norms},
  author={Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Sch{\"o}lkopf, Bernhard},
  booktitle={International conference on algorithmic learning theory},
  pages={63--77},
  year={2005},
  organization={Springer}
}

@inproceedings{rahimi2008random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={1177--1184},
  year={2008}
}

@article{goldt2019generalisation,
  title={Generalisation dynamics of online learning in over-parameterised neural networks},
  author={Goldt, Sebastian and Advani, Madhu S and Saxe, Andrew M and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={arXiv preprint arXiv:1901.09085},
  year={2019}
}

@article{goldt2020modeling,
  title={Modeling the influence of data structure on learning in neural networks: The hidden manifold model},
  author={Goldt, Sebastian and M{\'e}zard, Marc and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={Physical Review X},
  volume={10},
  number={4},
  pages={041044},
  year={2020},
  publisher={APS}
}

@inproceedings{yaida2020non,
  title={Non-Gaussian processes and neural networks at finite widths},
  author={Yaida, Sho},
  booktitle={Mathematical and Scientific Machine Learning},
  pages={165--192},
  year={2020},
  organization={PMLR}
}

@article{advani2017high,
  title={High-dimensional dynamics of generalization error in neural networks},
  author={Advani, Madhu S and Saxe, Andrew M and Sompolinsky, Haim},
  journal={Neural Networks},
  volume={132},
  pages={428--446},
  year={2020},
  publisher={Elsevier}
}

@article{suzuki2018adaptivity,
  title={Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: optimal rate and curse of dimensionality},
  author={Suzuki, Taiji},
  journal={arXiv preprint arXiv:1810.08033},
  year={2018}
}

@article{allen2018convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}

@inproceedings{du2018algorithmic,
  title={Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced},
  author={Du, Simon S and Hu, Wei and Lee, Jason D},
  booktitle={Advances in Neural Information Processing Systems},
  pages={384--395},
  year={2018}
}

@article{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1810.02032},
  year={2018}
}

@article{dou2019training,
  title={Training neural networks as learning data-adaptive kernels: Provable representation and approximation benefits},
  author={Dou, Xialiang and Liang, Tengyuan},
  journal={Journal of the American Statistical Association},
  volume={116},
  number={535},
  pages={1507--1520},
  year={2021},
  publisher={Taylor \& Francis}
}

@inproceedings{pennington2017nonlinear,
  title={Nonlinear random matrix theory for deep learning},
  author={Pennington, Jeffrey and Worah, Pratik},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2637--2646},
  year={2017}
}

@article{el2010spectrum,
  title={The spectrum of kernel random matrices},
  author={El Karoui, Noureddine},
  journal={The Annals of Statistics},
  volume={38},
  number={1},
  pages={1--50},
  year={2010},
  publisher={Institute of Mathematical Statistics}
}

@article{louart2018random,
  title={A random matrix approach to neural networks},
  author={Louart, Cosme and Liao, Zhenyu and Couillet, Romain},
  journal={The Annals of Applied Probability},
  volume={28},
  number={2},
  pages={1190--1248},
  year={2018},
  publisher={Institute of Mathematical Statistics}
}

@article{marchenko1967,
author = {Marčenko, V.A. and Pastur, Leonid},
year = {1967},
month = {01},
pages = {457-483},
title = {Distribution of eigenvalues for some sets of random matrices},
volume = {1},
journal = {Math USSR Sb}
}

@inproceedings{
du2018gradient,
title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
author={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1eK3i09YQ},
}

@inproceedings{akiyama2021learnability,
  title={On learnability via gradient method for two-layer relu neural networks in teacher-student setting},
  author={Akiyama, Shunta and Suzuki, Taiji},
  booktitle={International Conference on Machine Learning},
  pages={152--162},
  year={2021},
  organization={PMLR}
}

@article{suzuki2020benefit,
  title={Benefit of deep learning with non-convex noisy gradient descent: Provable excess risk bound and superiority to kernel methods},
  author={Suzuki, Taiji and Akiyama, Shunta},
  journal={arXiv preprint arXiv:2012.03224},
  year={2020}
}

@inproceedings{huang2020dynamics,
  title={Dynamics of deep neural networks and neural tangent hierarchy},
  author={Huang, Jiaoyang and Yau, Horng-Tzer},
  booktitle={International conference on machine learning},
  pages={4542--4551},
  year={2020},
  organization={PMLR}
}

@article{zou2018stochastic,
  title={Stochastic gradient descent optimizes over-parameterized deep relu networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:1811.08888},
  year={2018}
}

@article{chen2020towards,
  title={Towards understanding hierarchical learning: Benefits of neural representations},
  author={Chen, Minshuo and Bai, Yu and Lee, Jason D and Zhao, Tuo and Wang, Huan and Xiong, Caiming and Socher, Richard},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={22134--22145},
  year={2020}
}

@article{abbe2021staircase,
  title={The staircase property: How hierarchical structure can guide deep learning},
  author={Abbe, Emmanuel and Boix Adsera, Enric and Brennan, Matthew and Bresler, Guy and Nagaraj, Dheeraj},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{karp2021local,
  title={Local Signal Adaptivity: Provable Feature Learning in Neural Networks Beyond Kernels},
  author={Karp, Stefani and Winston, Ezra and Li, Yuanzhi and Singh, Aarti},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{imaizumi2019deep,
  title={Deep neural networks learn non-smooth functions effectively},
  author={Imaizumi, Masaaki and Fukumizu, Kenji},
  booktitle={The 22nd international conference on artificial intelligence and statistics},
  pages={869--878},
  year={2019},
  organization={PMLR}
}

@article{golatkar2019time,
  title={Time matters in regularizing deep networks: Weight decay and data augmentation affect early learning dynamics, matter little near convergence},
  author={Golatkar, Aditya Sharad and Achille, Alessandro and Soatto, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{oymak2019towards,
  title={Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={1},
  number={1},
  pages={84--105},
  year={2020},
  publisher={IEEE}
}

@article{el2018impact,
  title={On the impact of predictor geometry on the performance on high-dimensional ridge-regularized generalized robust regression estimators},
  author={El Karoui, Noureddine},
  journal={Probability Theory and Related Fields},
  volume={170},
  number={1},
  pages={95--175},
  year={2018},
  publisher={Springer}
}

@article{ghorbani2019linearized,
  title={Linearized two-layers neural networks in high dimension},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={The Annals of Statistics},
  volume={49},
  number={2},
  pages={1029--1054},
  year={2021},
  publisher={Institute of Mathematical Statistics}
}

@article{veiga2022phase,
  title={Phase diagram of Stochastic Gradient Descent in high-dimensional two-layer neural networks},
  author={Veiga, Rodrigo and Stephan, Ludovic and Loureiro, Bruno and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={arXiv preprint arXiv:2202.00293},
  year={2022}
}
 
@article{ghorbani2019limitations,
  title={Limitations of lazy training of two-layers neural network},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{belkin2019two,
  title={Two models of double descent for weak features},
  author={Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={2},
  number={4},
  pages={1167--1180},
  year={2020},
  publisher={SIAM}
}

@inproceedings{liao2018spectrum,
  title={On the spectrum of random features maps of high dimensional data},
  author={Liao, Zhenyu and Couillet, Romain},
  booktitle={International Conference on Machine Learning},
  pages={3063--3071},
  year={2018},
  organization={PMLR}
}
 
@article{liao2020random,
  title={A random matrix analysis of random fourier features: beyond the gaussian kernel, a precise phase transition, and the corresponding double descent},
  author={Liao, Zhenyu and Couillet, Romain and Mahoney, Michael W},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13939--13950},
  year={2020}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{girshick2014rich,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={580--587},
  year={2014}
}

@article{suzuki2019deep,
  title={Deep learning is adaptive to intrinsic dimensionality of model smoothness in anisotropic Besov space},
  author={Suzuki, Taiji and Nitanda, Atsushi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{bach2017breaking,
  title={Breaking the curse of dimensionality with convex neural networks},
  author={Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={629--681},
  year={2017},
  publisher={JMLR. org}
}

@inproceedings{donhauser2021rotational,
  title={How rotational invariance of common kernels prevents generalization in high dimensions},
  author={Donhauser, Konstantin and Wu, Mingqi and Yang, Fanny},
  booktitle={International Conference on Machine Learning},
  pages={2804--2814},
  year={2021},
  organization={PMLR}
}

@article{fan2019spectral,
  title={The spectral norm of random inner-product kernel matrices},
  author={Fan, Zhou and Montanari, Andrea},
  journal={Probability Theory and Related Fields},
  volume={173},
  number={1-2},
  pages={27--85},
  year={2019},
  publisher={Springer}
}
 
@article{belkin2018reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}

@book{boucheron2013concentration,
  title={Concentration inequalities: A nonasymptotic theory of independence},
  author={Boucheron, St{\'e}phane and Lugosi, G{\'a}bor and Massart, Pascal},
  year={2013},
  publisher={Oxford university press}
}

@article{chizat2018note,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019} 
}
 
@article{mei2019generalization,
  title={The generalization error of random features regression: Precise asymptotics and the double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={Communications on Pure and Applied Mathematics},
  volume={75},
  number={4},
  pages={667--766},
  year={2022},
  publisher={Wiley Online Library}
}

@inproceedings{belkin2018does,
  title={Does data interpolation contradict statistical optimality?},
  author={Belkin, Mikhail and Rakhlin, Alexander and Tsybakov, Alexandre B},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1611--1619},
  year={2019},
  organization={PMLR}
}

@inproceedings{ali2019continuous,
  title={A Continuous-Time View of Early Stopping for Least Squares},
  author={Ali, Alnur and Kolter, J Zico and Tibshirani, Ryan J},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  volume={22},
  year={2019}
}

@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}
 
@inproceedings{
jastrzebski2020break,
title={The Break-Even Point on Optimization Trajectories of Deep Neural Networks},
author={Stanislaw Jastrzebski and Maciej Szymczak and Stanislav Fort and Devansh Arpit and Jacek Tabor and Kyunghyun Cho and Krzysztof Geras},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=r1g87C4KwB}
}

@article{lewkowycz2020large,
  title={The large learning rate phase of deep learning: the catapult mechanism},
  author={Lewkowycz, Aitor and Bahri, Yasaman and Dyer, Ethan and Sohl-Dickstein, Jascha and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:2003.02218},
  year={2020}
}

@article{schmidt2020nonparametric,
  title={Nonparametric regression using deep neural networks with ReLU activation function},
  author={Schmidt-Hieber, Johannes},
  journal={The Annals of Statistics},
  volume={48},
  number={4},
  pages={1875--1897},
  year={2020},
  publisher={Institute of Mathematical Statistics}
}

@article{fort2020deep,
  title={Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel},
  author={Fort, Stanislav and Dziugaite, Gintare Karolina and Paul, Mansheej and Kharaghani, Sepideh and Roy, Daniel M and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5850--5861},
  year={2020}
}

@article{leclerc2020two,
  title={The two regimes of deep network training},
  author={Leclerc, Guillaume and Madry, Aleksander},
  journal={arXiv preprint arXiv:2002.10376},
  year={2020}
}
 
@inproceedings{mei2019mean,
  title={Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Conference on Learning Theory},
  pages={2388--2464},
  year={2019},
  organization={PMLR}
}

@article{sirignano2020mean,
  title={Mean field analysis of neural networks: A central limit theorem},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={Stochastic Processes and their Applications},
  volume={130},
  number={3},
  pages={1820--1852},
  year={2020}
}


@article{song2019quadratic,
  title={Quadratic Suffices for Over-parametrization via Matrix Chernoff Bound},
  author={Song, Zhao and Yang, Xin},
  journal={arXiv preprint arXiv:1906.03593},
  year={2019}
}

@article{oymak2019generalization,
  title={Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian},
  author={Oymak, Samet and Fabian, Zalan and Li, Mingchen and Soltanolkotabi, Mahdi},
  journal={arXiv preprint arXiv:1906.05392},
  year={2019}
}

@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}

@inproceedings{rotskoff2018parameters,
  title={Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks},
  author={Rotskoff, Grant M and Vanden-Eijnden, Eric},
  booktitle={Advances in Neural Information Processing Systems 31},
  pages={7146--7155},
  year={2018}
}

@article{dobriban2018high,
  title={High-dimensional asymptotics of prediction: Ridge regression and classification},
  author={Dobriban, Edgar and Wager, Stefan},
  journal={The Annals of Statistics},
  volume={46},
  number={1},
  pages={247--279},
  year={2018},
  publisher={Institute of Mathematical Statistics}
}

@article{ledoit2011eigenvectors,
  title={Eigenvectors of some large sample covariance matrix ensembles},
  author={Ledoit, Olivier and P{\'e}ch{\'e}, Sandrine},
  journal={Probability Theory and Related Fields},
  volume={151},
  number={1-2},
  pages={233--264},
  year={2011},
  publisher={Springer}
}


@article{cao2019generalization,
  title={A Generalization Theory of Gradient Descent for Learning Over-parameterized Deep ReLU Networks},
  author={Cao, Yuan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1902.01384},
  year={2019}
}
 
@article{ma2019comparative,
  title={A comparative analysis of optimization and generalization properties of two-layer neural network and random feature models under gradient descent dynamics},
  author={Weinan, E and Chao, Ma and Lei, Wu},
  journal={Science China Mathematics},
  volume={63},
  number={7},
  pages={1235},
  year={2020},
  publisher={Science China Press}
}

@article{mei2018mean,
  title={A mean field view of the landscape of two-layer neural networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={33},
  pages={E7665--E7671},
  year={2018},
  publisher={National Acad Sciences}
}
 
@article{gidel2019implicit,
  title={Implicit regularization of discrete gradient dynamics in linear neural networks},
  author={Gidel, Gauthier and Bach, Francis and Lacoste-Julien, Simon},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{nitanda2017stochastic,
  title={Stochastic particle gradient descent for infinite ensembles},
  author={Nitanda, Atsushi and Suzuki, Taiji},
  journal={arXiv preprint arXiv:1712.05438},
  year={2017}
}
 
@inproceedings{li2017algorithmic,
  title={Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  booktitle={Conference On Learning Theory},
  pages={2--47},
  year={2018},
  organization={PMLR}
}

@inproceedings{wei2019regularization,
  title={Regularization matters: Generalization and optimization of neural nets vs their induced kernel},
  author={Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9712--9724},
  year={2019}
}

@article{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
 
@inproceedings{pennington2018emergence,
  title={The emergence of spectral universality in deep networks},
  author={Pennington, Jeffrey and Schoenholz, Samuel and Ganguli, Surya},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1924--1932},
  year={2018},
  organization={PMLR}
}


@inproceedings{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Advances in neural information processing systems},
  pages={3036--3046},
  year={2018}
}

@inproceedings{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8157--8166},
  year={2018}
}
 
@article{yehudai2019power,
  title={On the power and limitations of random features for understanding neural networks},
  author={Yehudai, Gilad and Shamir, Ohad},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{allen2018learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{gunasekar2018implicit,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9461--9471},
  year={2018}
}

@inproceedings{bartlett2017spectrally,
  title={Spectrally-normalized margin bounds for neural networks},
  author={Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6240--6249},
  year={2017}
}


@article{neyshabur2018towards,
  title={Towards understanding the role of over-parametrization in generalization of neural networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  journal={arXiv preprint arXiv:1805.12076},
  year={2018}
}
 
@inproceedings{arora2018stronger,
  title={Stronger generalization bounds for deep nets via a compression approach},
  author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  booktitle={International Conference on Machine Learning},
  pages={254--263},
  year={2018},
  organization={PMLR}
}

@article{bartlett2019benign,
  title={Benign overfitting in linear regression},
  author={Bartlett, Peter L and Long, Philip M and Lugosi, G{\'a}bor and Tsigler, Alexander},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30063--30070},
  year={2020},
  publisher={National Acad Sciences}
}


@book{tao2012topics,
  title={Topics in random matrix theory},
  author={Tao, Terence},
  volume={132},
  year={2012},
  publisher={American Mathematical Soc.}
}

@article{mei2016landscape,
  title={The landscape of empirical risk for nonconvex losses},
  author={Mei, Song and Bai, Yu and Montanari, Andrea},
  journal={The Annals of Statistics},
  volume={46},
  number={6A},
  pages={2747--2774},
  year={2018},
  publisher={Institute of Mathematical Statistics}
}

@article{cheng2013spectrum,
  title={The spectrum of random inner-product kernel matrices},
  author={Cheng, Xiuyuan and Singer, Amit},
  journal={Random Matrices: Theory and Applications},
  volume={2},
  number={04},
  pages={1350010},
  year={2013},
  publisher={World Scientific}
}

@book{bai2010spectral,
  title={Spectral analysis of large dimensional random matrices},
  author={Bai, Zhidong and Silverstein, Jack W},
  volume={20},
  year={2010},
  publisher={Springer}
}


@inproceedings{tian2017analytical,
  title={An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis},
  author={Tian, Yuandong},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={3404--3413},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{soltanolkotabi2017learning,
  title={Learning relus via gradient descent},
  author={Soltanolkotabi, Mahdi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2007--2017},
  year={2017}
}
 
@inproceedings{liao2018dynamics,
  title={The dynamics of learning: A random matrix approach},
  author={Liao, Zhenyu and Couillet, Romain},
  booktitle={International Conference on Machine Learning},
  pages={3072--3081},
  year={2018},
  organization={PMLR}
}

@article{vershynin2010introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Vershynin, Roman},
  journal={arXiv preprint arXiv:1011.3027},
  year={2010}
}

@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge university press}
}


@inproceedings{xu2019many,
  title={On the number of variables to use in principal component regression},
  author={Xu, Ji and Hsu, Daniel J},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5094--5103},
  year={2019}
}


@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{bucilua2006model,
  title={Model compression},
  author={Buciluǎ, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
  booktitle={Proceedings of ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={535--541},
  year={2006}
}

@inproceedings{
cohen2021gradient,
title={Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability},
author={Jeremy Cohen and Simran Kaur and Yuanzhi Li and J Zico Kolter and Ameet Talwalkar},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=jh-rTtvkGeM}
}

@article{loureiro2021learning,
  title={Learning curves of generic features maps for realistic datasets with a teacher-student model},
  author={Loureiro, Bruno and Gerbelot, Cedric and Cui, Hugo and Goldt, Sebastian and Krzakala, Florent and Mezard, Marc and Zdeborov{\'a}, Lenka},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{ba2014deep,
  title={Do deep nets really need to be deep?},
  author={Ba, Jimmy and Caruana, Rich},
  booktitle={Advances in neural information processing systems},
  pages={2654--2662},
  year={2014}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={807--814},
  year={2010}
}



@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@book{nocedal2006numerical,
  title={Numerical optimization},
  author={Nocedal, Jorge and Wright, Stephen},
  year={2006},
  publisher={Springer Science \& Business Media}
}



@article{silverstein1995empirical,
  title={On the empirical distribution of eigenvalues of a class of large dimensional random matrices},
  author={Silverstein, Jack W and Bai, ZD},
  journal={Journal of Multivariate analysis},
  volume={54},
  number={2},
  pages={175--192},
  year={1995},
  publisher={Elsevier}
}

@article{rubio2011spectral,
  title={Spectral convergence for a general class of random matrices},
  author={Rubio, Francisco and Mestre, Xavier},
  journal={Statistics \& probability letters},
  volume={81},
  number={5},
  pages={592--602},
  year={2011},
  publisher={Elsevier}
}

@article{gordon1985some,
  title={Some inequalities for Gaussian processes and applications},
  author={Gordon, Yehoram},
  journal={Israel Journal of Mathematics},
  volume={50},
  number={4},
  pages={265--289},
  year={1985},
  publisher={Springer}
}

@inproceedings{liang2020multiple,
  title={On the multiple descent of minimum-norm interpolants and restricted lower isometry of kernels},
  author={Liang, Tengyuan and Rakhlin, Alexander and Zhai, Xiyu},
  booktitle={Conference on Learning Theory},
  pages={2683--2711},
  year={2020},
  organization={PMLR}
}

@article{montanari2020interpolation,
  title={The interpolation phase transition in neural networks: Memorization and generalization under lazy training},
  author={Montanari, Andrea and Zhong, Yiqiao},
  journal={arXiv preprint arXiv:2007.12826v1},
  year={2020}
}

@incollection{gordon1988milman,
  title={On Milman's inequality and random subspaces which escape through a mesh in $\mathbb{R}^n$},
  author={Gordon, Yehoram},
  booktitle={Geometric aspects of functional analysis},
  pages={84--106},
  year={1988},
  publisher={Springer}
}

@inproceedings{thrampoulidis2015regularized,
  title={Regularized linear regression: A precise analysis of the estimation error},
  author={Thrampoulidis, Christos and Oymak, Samet and Hassibi, Babak},
  booktitle={Conference on Learning Theory},
  pages={1683--1709},
  year={2015},
  organization={PMLR}
}



@inproceedings{richards2021asymptotics,
  title={Asymptotics of ridge (less) regression under general source condition},
  author={Richards, Dominic and Mourtada, Jaouad and Rosasco, Lorenzo},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3889--3897},
  year={2021},
  organization={PMLR}
}

@incollection{lecun2012efficient,
  title={Efficient backprop},
  author={LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle={Neural networks: Tricks of the trade},
  pages={9--48},
  year={2012},
  publisher={Springer}
}
 
@inproceedings{gunasekar2018characterizing,
  title={Characterizing implicit bias in terms of optimization geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={1832--1841},
  year={2018},
  organization={PMLR}
}
 
@article{amari1998natural,
  title={Natural gradient works efficiently in learning},
  author={Amari, Shun-Ichi},
  journal={Neural computation},
  volume={10},
  number={2},
  pages={251--276},
  year={1998},
  publisher={MIT Press}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}


@inproceedings{levy2019necessary,
  title={Necessary and Sufficient Geometries for Gradient Methods},
  author={Levy, Daniel and Duchi, John C},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11491--11501},
  year={2019}
}

 
@article{kunstner2019limitations,
  title={Limitations of the empirical fisher approximation for natural gradient descent},
  author={Kunstner, Frederik and Hennig, Philipp and Balles, Lukas},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{grosse2016kronecker,
  title={A kronecker-factored approximate fisher matrix for convolution layers},
  author={Grosse, Roger and Martens, James},
  booktitle={International Conference on Machine Learning},
  pages={573--582},
  year={2016}
}
 
@inproceedings{xie2016diverse,
  title={Diverse neural network learns true target functions},
  author={Xie, Bo and Liang, Yingyu and Song, Le},
  booktitle={Artificial Intelligence and Statistics},
  pages={1216--1224},
  year={2017},
  organization={PMLR}
}


@article{nystrom1930praktische,
  title={{\"U}ber die praktische Aufl{\"o}sung von Integralgleichungen mit Anwendungen auf Randwertaufgaben},
  author={Nystr{\"o}m, Evert J and others},
  journal={Acta Mathematica},
  volume={54},
  pages={185--204},
  year={1930},
  publisher={Institut Mittag-Leffler}
}


@article{azizan2019stochastic,
  title={Stochastic mirror descent on overparameterized nonlinear models},
  author={Azizan, Navid and Lale, Sahin and Hassibi, Babak},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2021},
  publisher={IEEE}
}

@article{azizan2018stochastic,
  title={Stochastic gradient/mirror descent: Minimax optimality and implicit regularization},
  author={Azizan, Navid and Hassibi, Babak},
  journal={arXiv preprint arXiv:1806.00952},
  year={2018}
}
 
@inproceedings{oymak2018overparameterized,
  title={Overparameterized nonlinear learning: Gradient descent takes the shortest path?},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  booktitle={International Conference on Machine Learning},
  pages={4951--4960},
  year={2019},
  organization={PMLR}
}
 

@article{cao2019towards,
  title={Towards Understanding the Spectral Bias of Deep Learning},
  author={Cao, Yuan and Fang, Zhiying and Wu, Yue and Zhou, Ding-Xuan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1912.01198},
  year={2019}
}

@inproceedings{bietti2019inductive,
  title={On the inductive bias of neural tangent kernels},
  author={Bietti, Alberto and Mairal, Julien},
  booktitle={Advances in Neural Information Processing Systems},
  pages={12873--12884},
  year={2019}
}

@inproceedings{woodworth2020kernel,
  title={Kernel and rich regimes in overparametrized models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={3635--3673},
  year={2020},
  organization={PMLR}
}

@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}
 
@article{montanari2019generalization,
  title={The generalization error of max-margin linear classifiers: High-dimensional asymptotics in the overparametrized regime},
  author={Montanari, Andrea and Ruan, Feng and Sohn, Youngtak and Yan, Jun},
  journal={arXiv preprint arXiv:1911.01544},
  year={2019}
}
 
@inproceedings{chizat2020implicit,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Conference on Learning Theory},
  pages={1305--1338},
  year={2020},
  organization={PMLR}
}

@article{ongie2019function,
  title={A Function Space View of Bounded Norm Infinite Width ReLU Nets: The Multivariate Case},
  author={Ongie, Greg and Willett, Rebecca and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:1910.01635},
  year={2019}
}

@article{razin2020implicit,
  title={Implicit regularization in deep learning may not be explainable by norms},
  author={Razin, Noam and Cohen, Nadav},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21174--21187},
  year={2020}
}
 
@inproceedings{nacson2019lexicographic,
  title={Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models},
  author={Nacson, Mor Shpigel and Gunasekar, Suriya and Lee, Jason and Srebro, Nathan and Soudry, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={4683--4692},
  year={2019},
  organization={PMLR}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}
 
@inproceedings{savarese2019infinite,
  title={How do infinite width bounded norm networks look in function space?},
  author={Savarese, Pedro and Evron, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={2667--2690},
  year={2019},
  organization={PMLR}
}


@article{karoui2013asymptotic,
  title={Asymptotic behavior of unregularized and ridge-regularized high-dimensional robust regression estimators: rigorous results},
  author={Karoui, Noureddine El},
  journal={arXiv preprint arXiv:1311.2445},
  year={2013}
}
 
@inproceedings{d2020double,
  title={Double trouble in double descent: Bias and variance (s) in the lazy regime},
  author={d’Ascoli, St{\'e}phane and Refinetti, Maria and Biroli, Giulio and Krzakala, Florent},
  booktitle={International Conference on Machine Learning},
  pages={2280--2290},
  year={2020},
  organization={PMLR}
}

@article{deng2019model,
  title={A Model of Double Descent for High-dimensional Binary Linear Classification},
  author={Deng, Zeyu and Kammoun, Abla and Thrampoulidis, Christos},
  journal={arXiv preprint arXiv:1911.05822},
  year={2019}
}


@inproceedings{botev2017practical,
  title={Practical gauss-newton optimisation for deep learning},
  author={Botev, Aleksandar and Ritter, Hippolyt and Barber, David},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={557--565},
  year={2017},
  organization={JMLR. org}
}


@inproceedings{su2019learning,
  title={On Learning Over-parameterized Neural Networks: A Functional Approximation Perspective},
  author={Su, Lili and Yang, Pengkun},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2637--2646},
  year={2019}
}

@article{caponnetto2007optimal,
  title={Optimal rates for the regularized least-squares algorithm},
  author={Caponnetto, Andrea and De Vito, Ernesto},
  journal={Foundations of Computational Mathematics},
  volume={7},
  number={3},
  pages={331--368},
  year={2007},
  publisher={Springer}
}

@article{lin2017optimal,
  title={Optimal rates for multi-pass stochastic gradient methods},
  author={Lin, Junhong and Rosasco, Lorenzo},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={3375--3421},
  year={2017},
  publisher={JMLR. org}
}

@inproceedings{rudi2017generalization,
  title={Generalization properties of learning with random features},
  author={Rudi, Alessandro and Rosasco, Lorenzo},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3215--3225},
  year={2017}
}

@inproceedings{pillaud2018statistical,
  title={Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes},
  author={Pillaud-Vivien, Loucas and Rudi, Alessandro and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8114--8124},
  year={2018}
}

@book{bach2021learning,
  title={Learning Theory from First Principles},
  author={Bach, Francis},
  year={2023},
  publisher={MIT Press}
}


@article{wu2020optimal,
  title={On the Optimal Weighted $\ell_2$ Regularization in Overparameterized Linear Regression},
  author={Wu, Denny and Xu, Ji},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={10112--10123},
  year={2020}
}


@article{MINSKER2017111,
title = "On some extensions of {Bernstein's} inequality for self-adjoint operators",
journal = "Statistics \& Probability Letters",
volume = "127",
pages = "111--119",
year = "2017",
author = "Stanislav Minsker",
}

@article{muthukumar2020classification,
  title={Classification vs regression in overparameterized regimes: Does the loss function matter?},
  author={Muthukumar, Vidya and Narang, Adhyyan and Subramanian, Vignesh and Belkin, Mikhail and Hsu, Daniel and Sahai, Anant},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={222},
  pages={1--69},
  year={2021}
}



@article{ba2020generalization,
  title={Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint},
  author={Ba, Jimmy and Erdogdu, Murat A. and Suzuki, Taiji and Wu, Denny and Zhang, Tianzong},
  journal={International Conference on Learning Representations},
  year={2020}
}

@article{ishida2020we,
  title={Do We Need Zero Training Loss After Achieving Zero Training Error?},
  author={Ishida, Takashi and Yamane, Ikko and Sakai, Tomoya and Niu, Gang and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:2002.08709},
  year={2020}
}

@inproceedings{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1019--1028},
  year={2017},
  organization={PMLR}
}

@inproceedings{pagliana2019implicit,
  title={Implicit Regularization of Accelerated Methods in Hilbert Spaces},
  author={Pagliana, Nicol{\`o} and Rosasco, Lorenzo},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14481--14491},
  year={2019}
}
 
@inproceedings{gerace2020generalisation,
  title={Generalisation error in learning with random features and the hidden manifold model},
  author={Gerace, Federica and Loureiro, Bruno and Krzakala, Florent and M{\'e}zard, Marc and Zdeborov{\'a}, Lenka},
  booktitle={International Conference on Machine Learning},
  pages={3452--3462},
  year={2020},
  organization={PMLR}
}


@article{kong2017spectrum,
  title={Spectrum estimation from samples},
  author={Kong, Weihao and Valiant, Gregory and others},
  journal={The Annals of Statistics},
  volume={45},
  number={5},
  pages={2218--2247},
  year={2017},
  publisher={Institute of Mathematical Statistics}
}

@article{el2008spectrum,
  title={Spectrum estimation for large dimensional covariance matrices using random matrix theory},
  author={El Karoui, Noureddine},
  journal={The Annals of Statistics},
  volume={36},
  number={6},
  pages={2757--2790},
  year={2008},
  publisher={Institute of Mathematical Statistics}
}

@article{mestre2008improved,
  title={Improved estimation of eigenvalues and eigenvectors of covariance matrices using their sample estimates},
  author={Mestre, Xavier},
  journal={IEEE Transactions on Information Theory},
  volume={54},
  number={11},
  pages={5113--5129},
  year={2008},
  publisher={IEEE}
}

@article{cucker2002mathematical,
  title={On the mathematical foundations of learning},
  author={Cucker, Felipe and Smale, Steve},
  journal={Bulletin of the American mathematical society},
  volume={39},
  number={1},
  pages={1--49},
  year={2002},
  publisher={Citeseer}
}

@article{mathe2003geometry,
  title={Geometry of linear ill-posed problems in variable Hilbert scales},
  author={Math{\'e}, Peter and Pereverzev, Sergei V},
  journal={Inverse problems},
  volume={19},
  number={3},
  pages={789},
  year={2003},
  publisher={IOP Publishing}
}

@inproceedings{steinwart2009optimal,
  title={Optimal Rates for Regularized Least Squares Regression.},
  author={Steinwart, Ingo and Hush, Don R and Scovel, Clint},
  booktitle={COLT},
  pages={79-93},
  year={2009}
} 
 
@article{aubin2020generalization,
  title={Generalization error in high-dimensional perceptrons: Approaching Bayes error with convex optimization},
  author={Aubin, Benjamin and Krzakala, Florent and Lu, Yue and Zdeborov{\'a}, Lenka},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12199--12210},
  year={2020}
}

@inproceedings{rudi2017falkon,
  title={Falkon: An optimal large scale kernel method},
  author={Rudi, Alessandro and Carratino, Luigi and Rosasco, Lorenzo},
  booktitle={Advances in neural information processing systems},
  pages={3888--3898},
  year={2017}
}

@incollection{bai2008limit,
  title={Limit of the smallest eigenvalue of a large dimensional sample covariance matrix},
  author={Bai, Zhi-Dong and Yin, Yong-Qua},
  booktitle={Advances In Statistics},
  pages={108--127},
  year={2008},
  publisher={World Scientific}
}
 
@inproceedings{murata2020gradient,
  title={Gradient Descent in RKHS with Importance Labeling},
  author={Murata, Tomoya and Suzuki, Taiji},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1981--1989},
  year={2021},
  organization={PMLR}
}
@article{wedin1973perturbation,
  title={Perturbation theory for pseudo-inverses},
  author={Wedin, Per-{\AA}ke},
  journal={BIT Numerical Mathematics},
  volume={13},
  number={2},
  pages={217--232},
  year={1973},
  publisher={Springer}
}

@article{stewart1990matrix,
  title={Matrix perturbation theory},
  author={Stewart, Gilbert W},
  year={1990},
  publisher={Citeseer}
}

@article{karakida2020understanding,
  title={Understanding Approximate Fisher Information for Fast Convergence of Natural Gradient Descent in Wide Neural Networks},
  author={Karakida, Ryo and Osawa, Kazuki},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{hu2020universality,
  title={Universality laws for high-dimensional learning with random features},
  author={Hu, Hong and Lu, Yue M},
  journal={arXiv preprint arXiv:2009.07669},
  year={2020}
}


@article{lewkowycz2020training,
  title={On the training dynamics of deep networks with $ L\_2 $ regularization},
  author={Lewkowycz, Aitor and Gur-Ari, Guy},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{yang2020feature,
  title={Feature Learning in Infinite-Width Neural Networks},
  author={Yang, Greg and Hu, Edward J},
  journal={arXiv preprint arXiv:2011.14522},
  year={2020}
}

@inproceedings{du2019gradient,
  title={Gradient Descent Finds Global Minima of Deep Neural Networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={Proceedings of International Conference on Machine Learning 36},
  pages={1675--1685},
  year={2019}
}

@article{nitanda2019gradient,
  title={Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems},
  author={Nitanda, Atsushi and Chinot, Geoffrey and Suzuki, Taiji},
  journal={arXiv preprint arXiv:1905.09870},
  year={2019}
}
 
@inproceedings{ji2019polylogarithmic,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
  author={Ziwei Ji and Matus Telgarsky},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=HygegyrYwH}
}

@inproceedings{bai2019beyond,
  title={Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks},
  author={Yu Bai and Jason D. Lee},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=rkllGyBFPH}
}

@article{allen2020backward,
  title={Backward feature correction: How deep learning performs deep learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2001.04413},
  year={2020}
}
 
@inproceedings{
dyer2019asymptotics,
title={Asymptotics of Wide Networks from Feynman Diagrams},
author={Ethan Dyer and Guy Gur-Ari},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=S1gFvANKDS}
}

@article{ortiz2021can,
  title={What can linearized neural networks actually say about generalization?},
  author={Ortiz-Jim{\'e}nez, Guillermo and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@inproceedings{li2020learning,
  title={Learning over-parametrized two-layer neural networks beyond ntk},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang R},
  booktitle={Conference on learning theory},
  pages={2613--2682},
  year={2020},
  organization={PMLR}
}

@inproceedings{malach2021quantifying,
  title={Quantifying the benefit of using differentiable learning over tangent kernels},
  author={Malach, Eran and Kamath, Pritish and Abbe, Emmanuel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={7379--7389},
  year={2021},
  organization={PMLR}
}

@article{yang2020tensor,
  title={Tensor programs iii: Neural matrix laws},
  author={Yang, Greg},
  journal={arXiv preprint arXiv:2009.10685},
  year={2020}
}

@article{nguyen2021analysis,
  title={Analysis of feature learning in weight-tied autoencoders via the mean field lens},
  author={Nguyen, Phan-Minh},
  journal={arXiv preprint arXiv:2102.08373},
  year={2021}
}
 
@article{suzuki2020generalization,
  title={Generalization bound of globally optimal non-convex neural network training: Transportation map estimation by infinite dimensional Langevin dynamics},
  author={Suzuki, Taiji},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19224--19237},
  year={2020}
}
 
@article{ghorbani2020neural,
  title={When do neural networks outperform kernel methods?},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14820--14830},
  year={2020}
}

@article{hajjar2021training,
  title={Training Integrable Parameterizations of Deep Neural Networks in the Infinite-Width Limit},
  author={Hajjar, Karl and Chizat, L{\'e}na{\"\i}c and Giraud, Christophe},
  journal={arXiv preprint arXiv:2110.15596},
  year={2021}
}

@inproceedings{adlam2020neural,
  title={The neural tangent kernel in high dimensions: Triple descent and a multi-scale theory of generalization},
  author={Adlam, Ben and Pennington, Jeffrey},
  booktitle={International Conference on Machine Learning},
  pages={74--84},
  year={2020},
  organization={PMLR}
}
 
@article{daniely2020learning,
  title={Learning parities with neural networks},
  author={Daniely, Amit and Malach, Eran},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20356--20365},
  year={2020}
}

@article{chen2020label,
  title={Label-Aware Neural Tangent Kernel: Toward Better Generalization and Local Elasticity},
  author={Chen, Shuxiao and He, Hangfeng and Su, Weijie},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
 
@article{bartlett2021deep,
  title={Deep learning: a statistical viewpoint},
  author={Bartlett, Peter L and Montanari, Andrea and Rakhlin, Alexander},
  journal={Acta numerica},
  volume={30},
  pages={87--201},
  year={2021},
  publisher={Cambridge University Press}
}

@inproceedings{yang2020rethinking,
  title={Rethinking bias-variance trade-off for generalization of neural networks},
  author={Yang, Zitong and Yu, Yaodong and You, Chong and Steinhardt, Jacob and Ma, Yi},
  booktitle={International Conference on Machine Learning},
  pages={10767--10777},
  year={2020},
  organization={PMLR}
}
 
@article{nguyen2020global,
  title={Global convergence of deep networks with one wide layer followed by pyramidal topology},
  author={Nguyen, Quynh N and Mondelli, Marco},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={11961--11972},
  year={2020}
}

@article{cheng2013random,
  title={Random matrices in high-dimensional data analysis},
  author={Cheng, Xiuyuan and others},
  year={2013},
  publisher={Princeton, NJ: Princeton University}
}

@article{fan2020spectra,
  title={Spectra of the conjugate kernel and neural tangent kernel for linear-width neural networks},
  author={Fan, Zhou and Wang, Zhichao},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={7710--7721},
  year={2020}
}

@article{wang2021deformed,
  title={Deformed semicircle law and concentration of nonlinear random matrices for ultra-wide neural networks},
  author={Wang, Zhichao and Zhu, Yizhe},
  journal={arXiv preprint arXiv:2109.09304},
  year={2021}
}

@article{dhifallah2020precise,
  title={A precise performance analysis of learning with random features},
  author={Dhifallah, Oussama and Lu, Yue M},
  journal={arXiv preprint arXiv:2008.11904},
  year={2020}
}


@article{goldt2021gaussian,
  title={The Gaussian equivalence of generative models for learning with shallow neural networks},
  author={Goldt, Sebastian and Loureiro, Bruno and Reeves, Galen and Krzakala, Florent and M{\'e}zard, Marc and Zdeborov{\'a}, Lenka},
  journal={Proceedings of Machine Learning Research vol},
  volume={145},
  pages={1--46},
  year={2021}
}
 
@article{mei2021generalization,
  title={Generalization error of random feature and kernel methods: hypercontractivity and kernel matrix concentration},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={Applied and Computational Harmonic Analysis},
  year={2021},
  publisher={Elsevier}
}


@book{neal1995bayesian,
  title={Bayesian learning for neural networks},
  author={Neal, Radford M},
  volume={118},
  year={1995},
  publisher={Springer Science \& Business Media}
}
 
@article{chatterji2020does,
  title={When does gradient descent with logistic loss find interpolating two-layer networks?},
  author={Chatterji, Niladri S and Long, Philip M and Bartlett, Peter L},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={159},
  pages={1--48},
  year={2021}
}

@article{amari2020does,
  title={When Does Preconditioning Help or Hurt Generalization?},
  author={Amari, Shun-ichi and Ba, Jimmy and Grosse, Roger and Li, Xuechen and Nitanda, Atsushi and Suzuki, Taiji and Wu, Denny and Xu, Ji},
  journal={arXiv preprint arXiv:2006.10732},
  year={2020}
}

@article{chen2021universality,
  title={Universality of approximate message passing algorithms},
  author={Chen, Wei-Kuo and Lam, Wai-Kit},
  journal={Electronic Journal of Probability},
  volume={26},
  pages={1--44},
  year={2021},
  publisher={Institute of Mathematical Statistics and Bernoulli Society}
}


@article{deshpande2016sparse,
  title={Sparse pca via covariance thresholding},
  author={Deshpande, Yash and Montanari, Andrea},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={4913--4953},
  year={2016},
  publisher={JMLR. org}
}

@article{benigni2022largest,
  title={Largest Eigenvalues of the Conjugate Kernel of Single-Layered Neural Networks},
  author={Benigni, Lucas and P{\'e}ch{\'e}, Sandrine},
  journal={arXiv preprint arXiv:2201.04753},
  year={2022}
}
 
@article{benigni2019eigenvalue,
  title={Eigenvalue distribution of some nonlinear models of random matrices},
  author={Benigni, Lucas and P{\'e}ch{\'e}, Sandrine},
  journal={Electronic Journal of Probability},
  volume={26},
  pages={1--37},
  year={2021},
  publisher={Institute of Mathematical Statistics and Bernoulli Society}
}

@article{peche2019note,
  title={A note on the Pennington-Worah distribution},
  author={P{\'e}ch{\'e}, S},
  journal={Electronic Communications in Probability},
  volume={24},
  pages={1--7},
  year={2019},
  publisher={Institute of Mathematical Statistics and Bernoulli Society}
}

@book{mingo2017free,
  title={Free probability and random matrices},
  author={Mingo, James A and Speicher, Roland},
  volume={35},
  year={2017},
  publisher={Springer}
}

@article{bai1998no,
  title={No eigenvalues outside the support of the limiting spectral distribution of large-dimensional sample covariance matrices},
  author={Bai, Zhi-Dong and Silverstein, Jack W},
  journal={The Annals of Probability},
  volume={26},
  number={1},
  pages={316--345},
  year={1998},
  publisher={Institute of Mathematical Statistics}
}

@article{liao2020sparse,
  title={Sparse quantized spectral clustering},
  author={Liao, Zhenyu and Couillet, Romain and Mahoney, Michael W},
  journal={arXiv preprint arXiv:2010.01376},
  year={2020}
}

@book{meckes2019random,
  title={The random matrix theory of the classical compact groups},
  author={Meckes, Elizabeth S},
  volume={218},
  year={2019},
  publisher={Cambridge University Press}
}

@article{adamczak2015note,
  title={A note on the Hanson-Wright inequality for random vectors with dependencies},
  author={Adamczak, Radoslaw},
  journal={Electronic Communications in Probability},
  volume={20},
  pages={1--13},
  year={2015},
  publisher={Institute of Mathematical Statistics and Bernoulli Society}
}

@article{montanari2022universality,
  title={Universality of empirical risk minimization},
  author={Montanari, Andrea and Saeed, Basil},
  journal={arXiv preprint arXiv:2202.08832},
  year={2022}
}

@article{abbe2022merged,
  title={The merged-staircase property: a necessary and nearly sufficient condition for SGD learning of sparse functions on two-layer neural networks},
  author={Abbe, Emmanuel and Boix-Adsera, Enric and Misiakiewicz, Theodor},
  journal={arXiv preprint arXiv:2202.08658},
  year={2022}
}

@article{nitanda2022convex,
  title={Convex Analysis of the Mean Field Langevin Dynamics},
  author={Nitanda, Atsushi and Wu, Denny and Suzuki, Taiji},
  journal={arXiv preprint arXiv:2201.10469},
  year={2022}
}

@article{cristianini2001kernel,
  title={On kernel-target alignment},
  author={Cristianini, Nello and Shawe-Taylor, John and Elisseeff, Andre and Kandola, Jaz},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}

@article{frei2022random,
  title={Random Feature Amplification: Feature Learning and Generalization in Neural Networks},
  author={Frei, Spencer and Chatterji, Niladri S and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2202.07626},
  year={2022}
}

@article{chizat2022mean,
  title={Mean-Field Langevin Dynamics: Exponential Convergence and Annealing},
  author={Chizat, L{\'e}na{\"\i}c},
  journal={arXiv preprint arXiv:2202.01009},
  year={2022}
}

@article{hu2019mean,
  title={Mean-field Langevin dynamics and energy landscape of neural networks},
  author={Hu, Kaitong and Ren, Zhenjie and Siska, David and Szpruch, Lukasz},
  journal={arXiv preprint arXiv:1905.07769},
  year={2019}
}

@article{tripuraneni2021covariate,
  title={Covariate Shift in High-Dimensional Random Feature Regression},
  author={Tripuraneni, Nilesh and Adlam, Ben and Pennington, Jeffrey},
  journal={arXiv preprint arXiv:2111.08234},
  year={2021}
}

@article{bodin2021model,
  title={Model, sample, and epoch-wise descents: exact solution of gradient flow in the random feature model},
  author={Bodin, Antoine and Macris, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{helton2018applications,
  title={Applications of realizations (aka linearizations) to free probability},
  author={Helton, J William and Mai, Tobias and Speicher, Roland},
  journal={Journal of Functional Analysis},
  volume={274},
  number={1},
  pages={1--79},
  year={2018},
  publisher={Elsevier}
}

@article{helton2007operator,
  title={Operator-valued semicircular elements: solving a quadratic matrix equation with positivity constraints},
  author={Helton, J William and Far, Reza Rashidi and Speicher, Roland},
  journal={International Mathematics Research Notices},
  volume={2007},
  number={9},
  pages={rnm086--rnm086},
  year={2007},
  publisher={OUP}
}

@article{far2006spectra,
  title={Spectra of large block matrices},
  author={Far, Reza Rashidi and Oraby, Tamer and Bryc, Wlodzimierz and Speicher, Roland},
  journal={arXiv preprint cs/0610045},
  year={2006}
}

@article{adlam2020understanding,
  title={Understanding double descent requires a fine-grained bias-variance decomposition},
  author={Adlam, Ben and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={11022--11032},
  year={2020}
}

@article{arous2021online,
  title={Online stochastic gradient descent on non-convex losses from high-dimensional inference},
  author={Arous, Gerard Ben and Gheissari, Reza and Jagannath, Aukosh},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={106},
  pages={1--51},
  year={2021},
  publisher={Microtome Publishing}
}

@incollection{capitaine2018limiting,
  title={Limiting eigenvectors of outliers for spiked information-plus-noise type matrices},
  author={Capitaine, Mireille},
  booktitle={S{\'e}minaire de Probabilit{\'e}s XLIX},
  pages={119--164},
  year={2018},
  publisher={Springer}
}

@article{do2013spectrum,
  title={The spectrum of random kernel matrices: universality results for rough and varying kernels},
  author={Do, Yen and Vu, Van},
  journal={Random Matrices: Theory and Applications},
  volume={2},
  number={03},
  pages={1350005},
  year={2013},
  publisher={World Scientific}
}


@article{pesme2021implicit,
  title={Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity},
  author={Pesme, Scott and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}