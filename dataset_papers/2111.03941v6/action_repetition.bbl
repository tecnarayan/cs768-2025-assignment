\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2021)Andrychowicz, Raichuk, Sta{\'n}czyk, Orsini,
  Girgin, Marinier, Hussenot, Geist, Pietquin, Michalski, Gelly, and
  Bachem]{what_andrychowicz2021}
Marcin Andrychowicz, Anton Raichuk, Piotr Sta{\'n}czyk, Manu Orsini, Sertan
  Girgin, Rapha{\"e}l Marinier, Leonard Hussenot, Matthieu Geist, Olivier
  Pietquin, Marcin Michalski, Sylvain Gelly, and Olivier Bachem.
\newblock What matters for on-policy deep actor-critic methods? a large-scale
  study.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=nIAxjsniDzg}.

\bibitem[Baird(1994)]{baird1994}
L.~Baird.
\newblock Reinforcement learning in continuous time: advantage updating.
\newblock \emph{Proceedings of 1994 IEEE International Conference on Neural
  Networks (ICNN'94)}, 4:\penalty0 2448--2453 vol.4, 1994.

\bibitem[Bradtke and Duff(1994)]{ct_bradtke1994}
Steven~J. Bradtke and M.~O. Duff.
\newblock Reinforcement learning methods for continuous-time markov decision
  problems.
\newblock In \emph{NIPS}, 1994.

\bibitem[Braylan et~al.(2015)Braylan, Hollenbeck, Meyerson, and
  Miikkulainen]{frameskip_braylan2015}
Alexander Braylan, Mark Hollenbeck, Elliot Meyerson, and R.~Miikkulainen.
\newblock Frame skip is a powerful parameter for learning to play atari.
\newblock In \emph{AAAI Workshop: Learning for General Competency in Video
  Games}, 2015.

\bibitem[Doya(2000)]{ct_doya2000}
K.~Doya.
\newblock Reinforcement learning in continuous time and space.
\newblock \emph{Neural Computation}, 12:\penalty0 219--245, 2000.

\bibitem[Du et~al.(2020)Du, Futoma, and Doshi-Velez]{ode_du2020}
Jianzhun Du, J.~Futoma, and Finale Doshi-Velez.
\newblock Model-based reinforcement learning for semi-markov decision processes
  with neural odes.
\newblock \emph{ArXiv}, abs/2006.16210, 2020.

\bibitem[Fr{\'e}maux et~al.(2013)Fr{\'e}maux, Sprekeler, and
  Gerstner]{ct_frmaux2013}
Nicolas Fr{\'e}maux, H.~Sprekeler, and W.~Gerstner.
\newblock Reinforcement learning using a continuous time actor-critic framework
  with spiking neurons.
\newblock \emph{PLoS Computational Biology}, 9, 2013.

\bibitem[Gu et~al.(2017)Gu, Holly, Lillicrap, and Levine]{gu2017}
Shixiang Gu, Ethan Holly, T.~Lillicrap, and S.~Levine.
\newblock Deep reinforcement learning for robotic manipulation with
  asynchronous off-policy updates.
\newblock \emph{2017 IEEE International Conference on Robotics and Automation
  (ICRA)}, pages 3389--3396, 2017.

\bibitem[Hafner et~al.(2020)Hafner, Lillicrap, Ba, and
  Norouzi]{dreamer_hafner2020}
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=S1lOTC4tDS}.

\bibitem[Hafner et~al.(2021)Hafner, Lillicrap, Norouzi, and
  Ba]{dreamerv2_hafner2021}
Danijar Hafner, Timothy~P Lillicrap, Mohammad Norouzi, and Jimmy Ba.
\newblock Mastering atari with discrete world models.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=0oabwyZbOu}.

\bibitem[Hausknecht and Stone(2015)]{drqn_hausknecht2015}
M.~Hausknecht and P.~Stone.
\newblock Deep recurrent q-learning for partially observable mdps.
\newblock In \emph{AAAI Fall Symposia}, 2015.

\bibitem[Hill et~al.(2018)Hill, Raffin, Ernestus, Gleave, Kanervisto, Traore,
  Dhariwal, Hesse, Klimov, Nichol, Plappert, Radford, Schulman, Sidor, and
  Wu]{sb_hill2018}
Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi
  Kanervisto, Rene Traore, Prafulla Dhariwal, Christopher Hesse, Oleg Klimov,
  Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor,
  and Yuhuai Wu.
\newblock Stable baselines.
\newblock \url{https://github.com/hill-a/stable-baselines}, 2018.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, and
  Levine]{qtopt_salashnikov2018}
D.~Kalashnikov, A.~Irpan, P.~Pastor, J.~Ibarz, A.~Herzog, Eric Jang, Deirdre
  Quillen, Ethan Holly, Mrinal Kalakrishnan, V.~Vanhoucke, and S.~Levine.
\newblock Qt-opt: Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock \emph{ArXiv}, abs/1806.10293, 2018.

\bibitem[Korenkevych et~al.(2019)Korenkevych, Mahmood, Vasan, and
  Bergstra]{ar_korenkevych2019}
Dmytro Korenkevych, A.~Mahmood, Gautham Vasan, and J.~Bergstra.
\newblock Autoregressive policies for continuous control deep reinforcement
  learning.
\newblock In \emph{IJCAI}, 2019.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Sharma, and
  Ravindran]{dfdqn_lakshminarayanan2017}
Aravind Lakshminarayanan, Sahil Sharma, and Balaraman Ravindran.
\newblock Dynamic action repetition for deep reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2017.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{ddpg_lillicrap2016}
T.~Lillicrap, Jonathan~J. Hunt, A.~Pritzel, N.~Heess, T.~Erez, Y.~Tassa,
  D.~Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{CoRR}, abs/1509.02971, 2016.

\bibitem[Metelli et~al.(2020)Metelli, Mazzolini, Bisi, Sabbioni, and
  Restelli]{pfqi_metelli2020}
Alberto~Maria Metelli, Flavio Mazzolini, L.~Bisi, Luca Sabbioni, and Marcello
  Restelli.
\newblock Control frequency adaptation via action persistence in batch
  reinforcement learning.
\newblock In \emph{ICML}, 2020.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{dqn_mnih2013}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~Graves, Ioannis Antonoglou, Daan
  Wierstra, and Martin~A. Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{ArXiv}, abs/1312.5602, 2013.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{atari_mnih2015}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, Andrei~A. Rusu, J.~Veness, Marc~G.
  Bellemare, A.~Graves, Martin~A. Riedmiller, Andreas~K. Fidjeland, Georg
  Ostrovski, Stig Petersen, C.~Beattie, A.~Sadik, Ioannis Antonoglou, Helen
  King, D.~Kumaran, Daan Wierstra, S.~Legg, and Demis Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518:\penalty0 529--533, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{a2c_mnih2016}
V.~Mnih, Adri{\`a}~Puigdom{\`e}nech Badia, Mehdi Mirza, A.~Graves,
  T.~Lillicrap, Tim Harley, D.~Silver, and K.~Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock \emph{ArXiv}, abs/1602.01783, 2016.

\bibitem[Munos(2005)]{ct_munos2005}
R.~Munos.
\newblock Policy gradient in continuous time.
\newblock In \emph{J. Mach. Learn. Res.}, 2005.

\bibitem[Munos and Bourgine(1997)]{ct_munos1996}
R.~Munos and P.~Bourgine.
\newblock Reinforcement learning for continuous stochastic control problems.
\newblock In \emph{NIPS}, 1997.

\bibitem[Pardo et~al.(2018)Pardo, Tavakoli, Levdik, and
  Kormushev]{time_pardo2018}
Fabio Pardo, Arash Tavakoli, Vitaly Levdik, and Petar Kormushev.
\newblock Time limits in reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  4045--4054. PMLR, 2018.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch_paszke2019}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, 2019.

\bibitem[Raffin et~al.(2019)Raffin, Hill, Ernestus, Gleave, Kanervisto, and
  Dormann]{sb3_raffin2019}
Antonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi
  Kanervisto, and Noah Dormann.
\newblock Stable baselines3.
\newblock \url{https://github.com/DLR-RM/stable-baselines3}, 2019.

\bibitem[Rockafellar and Uryasev(2002)]{risk_rockafellar2002}
R~Tyrrell Rockafellar and Stanislav Uryasev.
\newblock Conditional value-at-risk for general loss distributions.
\newblock \emph{Journal of banking \& finance}, 26\penalty0 (7):\penalty0
  1443--1471, 2002.

\bibitem[Schrittwieser et~al.(2020)Schrittwieser, Antonoglou, Hubert, Simonyan,
  Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel, Lillicrap, and
  Silver]{muzero_schrittwieser2020}
Julian Schrittwieser, Ioannis Antonoglou, T.~Hubert, K.~Simonyan, L.~Sifre,
  Simon Schmitt, A.~Guez, Edward Lockhart, Demis Hassabis, T.~Graepel,
  T.~Lillicrap, and D.~Silver.
\newblock Mastering atari, go, chess and shogi by planning with a learned
  model.
\newblock \emph{Nature}, 588 7839:\penalty0 604--609, 2020.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{trpo_schulman2015}
John Schulman, Sergey Levine, P.~Abbeel, Michael~I. Jordan, and P.~Moritz.
\newblock Trust region policy optimization.
\newblock \emph{ArXiv}, abs/1502.05477, 2015.

\bibitem[Schulman et~al.(2016)Schulman, Moritz, Levine, Jordan, and
  Abbeel]{gae_schulman2016}
John Schulman, P.~Moritz, Sergey Levine, Michael~I. Jordan, and P.~Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock \emph{CoRR}, abs/1506.02438, 2016.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{ppo_schulman2017}
John Schulman, F.~Wolski, Prafulla Dhariwal, A.~Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{ArXiv}, abs/1707.06347, 2017.

\bibitem[Sharma et~al.(2017)Sharma, Lakshminarayanan, and
  Ravindran]{figar_sharma2017}
Sahil Sharma, Aravind~S. Lakshminarayanan, and Balaraman Ravindran.
\newblock Learning to repeat: Fine grained action repetition for deep
  reinforcement learning.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=B1GOWV5eg}.

\bibitem[Shen et~al.(2020)Shen, Li, Jiang, Wang, and Zhao]{smooth_shen2020}
Qianli Shen, Yan Li, Haoming Jiang, Zhaoran Wang, and Tuo Zhao.
\newblock Deep reinforcement learning with robust and smooth policy.
\newblock In \emph{International Conference on Machine Learning}, pages
  8707--8718. PMLR, 2020.

\bibitem[Singh et~al.(2020)Singh, Zhang, and Chen]{risk_singh2020}
Rahul Singh, Qinsheng Zhang, and Yongxin Chen.
\newblock Improving robustness via risk averse distributional reinforcement
  learning.
\newblock In \emph{Learning for Dynamics and Control}, pages 958--968. PMLR,
  2020.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{hrl_sutton1999}
R.~Sutton, Doina Precup, and Satinder Singh.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning.
\newblock \emph{Artif. Intell.}, 112:\penalty0 181--211, 1999.

\bibitem[Sutton and Barto(2018)]{rl_sutton2018}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tallec et~al.(2019)Tallec, Blier, and Ollivier]{dau_tallec2019}
C.~Tallec, L.~Blier, and Y.~Ollivier.
\newblock Making deep q-learning methods robust to time discretization.
\newblock In \emph{ICML}, 2019.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{mujoco_todorov2012}
E.~Todorov, T.~Erez, and Y.~Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pages 5026--5033, 2012.

\bibitem[Wawrzynski(2015)]{wawrzynski2015}
Pawel Wawrzynski.
\newblock Control policy with autocorrelated noise in reinforcement learning
  for robotics.
\newblock \emph{International Journal of Machine Learning and Computing},
  5\penalty0 (2):\penalty0 91, 2015.

\end{thebibliography}
