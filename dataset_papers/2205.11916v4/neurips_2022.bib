@misc{cot_wei,
  url = {https://arxiv.org/abs/2201.11903},
  
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{stanovich2000individual,
  title={Individual differences in reasoning: Implications for the rationality debate?},
  author={Stanovich, Keith E and West, Richard F},
  journal={Behavioral and brain sciences},
  volume={23},
  number={5},
  pages={645--665},
  year={2000},
  publisher={Cambridge University Press}
}

@article{johnson2005structure,
  title={The structure of human intelligence: It is verbal, perceptual, and image rotation (VPR), not fluid and crystallized},
  author={Johnson, Wendy and Bouchard Jr, Thomas J},
  journal={Intelligence},
  volume={33},
  number={4},
  pages={393--416},
  year={2005},
  publisher={Elsevier}
}

@article{mcgrew2005cattell,
  title={The Cattell-Horn-Carroll theory of cognitive abilities: Past, present, and future.},
  author={McGrew, Kevin S},
  year={2005},
  publisher={The Guilford Press}
}

@article{chollet2019measure,
  title={On the measure of intelligence},
  author={Chollet, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:1911.01547},
  year={2019},
  url={https://arxiv.org/abs/1911.01547}
}
@article{liu2021pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
 author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={arXiv preprint arXiv:2107.13586},
  year={2021},
  url = {https://arxiv.org/abs/2107.13586},
}

@inproceedings{gao2021making,
    title = "Making Pre-trained Language Models Better Few-shot Learners",
    author = "Gao, Tianyu  and
      Fisch, Adam  and
      Chen, Danqi",
    booktitle = "Proceedings of ACL-IJCNLP",
    year = "2021",
    url = "https://aclanthology.org/2021.acl-long.295",
    pages = "3816--3830",
}

@inproceedings{schick2020s,
    title = "It{'}s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners",
    author = {Schick, Timo  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of NAACL",
    year = "2021",
    url = "https://aclanthology.org/2021.naacl-main.185",
    pages = "2339--2352",
}

@inproceedings{shin2020autoprompt,
    title = "{A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with {A}utomatically {G}enerated {P}rompts",
    author = "Shin, Taylor  and
      Razeghi, Yasaman  and
      Logan IV, Robert L.  and
      Wallace, Eric  and
      Singh, Sameer",
    booktitle = "Proceedings of EMNLP",
    year = "2020",
    url = "https://aclanthology.org/2020.emnlp-main.346",
    pages = "4222--4235",
}

@article{Radford2019LanguageMA,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  pages={9},
  year={2019},
  url={http://www.persagen.com/files/misc/radford2019language.pdf}
}

@inproceedings{lu2021fantastically,
    title = "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
    author = "Lu, Yao  and
      Bartolo, Max  and
      Moore, Alastair  and
      Riedel, Sebastian  and
      Stenetorp, Pontus",
    booktitle = "Proceedings of ACL",
    year = "2022",
    url = "https://aclanthology.org/2022.acl-long.556",
    pages = "8086--8098",
}


@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021},
  url={https://arxiv.org/abs/2101.06804}
}

@inproceedings{kumar2021reordering,
    title = "Reordering Examples Helps during Priming-based Few-Shot Learning",
    author = "Kumar, Sawan  and
      Talukdar, Partha",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    year = "2021",
    url = "https://aclanthology.org/2021.findings-acl.395",
    pages = "4507--4518",
}

@article{min2022rethinking,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022},
  url={https://arxiv.org/pdf/2202.12837.pdf}
}

@article{zhang2022opt,
  title={OPT: Open Pre-trained Transformer Language Models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022},
  url={https://arxiv.org/abs/2205.01068}
}

@article{ye2022unreliability,
  title={The Unreliability of Explanations in Few-Shot In-Context Learning},
  author={Ye, Xi and Durrett, Greg},
  journal={arXiv preprint arXiv:2205.03401},
  year={2022},
  url = {https://arxiv.org/abs/2205.03401},
}

@article{lampinen2022can,
  title={Can language models learn from explanations in context?},
  author={Lampinen, Andrew K and Dasgupta, Ishita and Chan, Stephanie CY and Matthewson, Kory and Tessler, Michael Henry and Creswell, Antonia and McClelland, James L and Wang, Jane X and Hill, Felix},
  journal={arXiv preprint arXiv:2204.02329},
  year={2022}
}

@article{cot_wei_sc,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022},
  url={https://arxiv.org/abs/2203.11171}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in NeurIPS},
  volume={32},
  pages={8026--8037},
  year={2019},
  url={https://papers.nips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html}
}

@misc{instructgpt,
  url = {https://arxiv.org/abs/2203.02155},
  
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Training language models to follow instructions with human feedback},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{darkknowledge,
  
  url = {https://arxiv.org/abs/1503.02531},
  
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Distilling the Knowledge in a Neural Network},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc(darkknowledge2,
howpublished={\url{https://www.ttic.edu/dl/dark14.pdf}},
note={"Accessed: 2022-05-06"},
year = {2014},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff}
)

@article{singleeq,
    title = "Parsing Algebraic Word Problems into Equations",
    author = "Koncel-Kedziorski, Rik  and
      Hajishirzi, Hannaneh  and
      Sabharwal, Ashish  and
      Etzioni, Oren  and
      Ang, Siena Dumas",
    journal = "TACL",
    volume = "3",
    year = "2015",
    url = "https://aclanthology.org/Q15-1042",
    pages = "585--597",
}


@inproceedings{mawps,
    title = "{MAWPS}: A Math Word Problem Repository",
    author = "Koncel-Kedziorski, Rik  and
      Roy, Subhro  and
      Amini, Aida  and
      Kushman, Nate  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of NAACL",
    year = "2016",
    url = "https://aclanthology.org/N16-1136",
    pages = "1152--1157",
}

@inproceedings{addsub,
  title={Learning to solve arithmetic word problems with verb categorization.},
  author={Hosseini, Mohammad Javad and Hajishirzi, Hannaneh and Etzioni, Oren and Kushman, Nate},
  booktitle={EMNLP},
  volume={523533},
  year={2014},
  organization={Citeseer},
  url={https://aclanthology.org/D14-1058/}
}

@inproceedings{multiarith,
    title = "Solving General Arithmetic Word Problems",
    author = "Roy, Subhro  and
      Roth, Dan",
    booktitle = "Proceedings of EMNLP",
    year = "2015",
    url = "https://aclanthology.org/D15-1202",
    pages = "1743--1752",
}

@inproceedings{aqua,
    title = "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
    author = "Ling, Wang  and
      Yogatama, Dani  and
      Dyer, Chris  and
      Blunsom, Phil",
    booktitle = "Proceedings of ACL",
    year = "2017",
    url = "https://aclanthology.org/P17-1015",
    pages = "158--167",
}

@misc{gsm8k,
  url = {https://arxiv.org/abs/2110.14168},
  author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  title = {Training Verifiers to Solve Math Word Problems},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{svamp,
    title = "Are {NLP} Models really able to Solve Simple Math Word Problems?",
    author = "Patel, Arkil  and
      Bhattamishra, Satwik  and
      Goyal, Navin",
    booktitle = "Proceedings of NAACL",
    year = "2021",
    url = "https://aclanthology.org/2021.naacl-main.168",
    pages = "2080--2094",
}

@inproceedings{commonsenseqa,
  title={COMMONSENSEQA: A Question Answering Challenge Targeting Commonsense Knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  booktitle={Proceedings of NAACL-HLT},
  pages={4149--4158},
  year={2019},
  url={https://aclanthology.org/N19-1421/}
}

@article{strategyqa,
  title={Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies},
  author={Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},
  journal={TACL},
  volume={9},
  pages={346--361},
  year={2021},
  publisher={MIT Press},
  url={https://aclanthology.org/2021.tacl-1.21/}
}

@misc(bigbench_old,
howpublished={\url{https://github.com/google/BIG-bench}},
note={"Accessed: 2022-05-07"},
year = {2021},
title = {BIG-bench collaboration. Beyond the imitation game: Measuring and extrapolating the capabilities of language models}
)

@article{bigbench,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  url={https://arxiv.org/abs/2206.04615},
  year={2022}
}

@misc{gopher,
  url = {https://arxiv.org/abs/2112.11446},
  
  author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and Driessche, George van den and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and d'Autume, Cyprien de Masson and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Scaling Language Models: Methods, Analysis \& Insights from Training Gopher},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{star,
  url = {https://arxiv.org/abs/2203.14465},
  
  author = {Zelikman, Eric and Wu, Yuhuai and Goodman, Noah D.},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {STaR: Bootstrapping Reasoning With Reasoning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{lamda,
  url = {https://arxiv.org/abs/2201.08239},
  
  author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and Meier-Hellstern, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and Hoffman-John, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {LaMDA: Language Models for Dialog Applications},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{brown2020language,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in NeurIPS},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{palm,
  url = {https://arxiv.org/abs/2204.02311},
  
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {PaLM: Scaling Language Modeling with Pathways},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{megatron,
  url = {https://arxiv.org/abs/2201.11990},
  
  author = {Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and Zhang, Elton and Child, Rewon and Aminabadi, Reza Yazdani and Bernauer, Julie and Song, Xia and Shoeybi, Mohammad and He, Yuxiong and Houston, Michael and Tiwary, Saurabh and Catanzaro, Bryan},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{scratchpad,
title={Show Your Work: Scratchpads for Intermediate Computation with Language Models},
author={Maxwell Nye and Anders Johan Andreassen and Guy Gur-Ari and Henryk Michalewski and Jacob Austin and David Bieber and David Dohan and Aitor Lewkowycz and Maarten Bosma and David Luan and Charles Sutton and Augustus Odena},
booktitle={Deep Learning for Code Workshop},
year={2022},
url={https://openreview.net/forum?id=HBlx2idbkbq}
}

@inproceedings{yourself,
    title = "Explain Yourself! Leveraging Language Models for Commonsense Reasoning",
    author = "Rajani, Nazneen Fatema  and
      McCann, Bryan  and
      Xiong, Caiming  and
      Socher, Richard",
    booktitle = "Proceedings of ACL",
    year = "2019",
    url = "https://aclanthology.org/P19-1487",
    pages = "4932--4942",
}

@inproceedings{selftalk,
    title = "Unsupervised Commonsense Question Answering with Self-Talk",
    author = "Shwartz, Vered  and
      West, Peter  and
      Le Bras, Ronan  and
      Bhagavatula, Chandra  and
      Choi, Yejin",
    booktitle = "Proceedings of EMNLP",
    year = "2020",
    url = "https://aclanthology.org/2020.emnlp-main.373",
    pages = "4615--4629",
}

@misc{cando,
  url = {https://arxiv.org/abs/2204.01691},
  
  author = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Ho, Daniel and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jang, Eric and Ruano, Rosario Jauregui and Jeffrey, Kyle and Jesmonth, Sally and Joshi, Nikhil J and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Luu, Linda and Parada, Carolina and Pastor, Peter and Quiambao, Jornell and Rao, Kanishka and Rettinghouse, Jarek and Reyes, Diego and Sermanet, Pierre and Sievers, Nicolas and Tan, Clayton and Toshev, Alexander and Vanhoucke, Vincent and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yan, Mengyuan},
  
  keywords = {Robotics (cs.RO), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Do As I Can, Not As I Say: Grounding Language in Robotic Affordances},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{transformer,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in NeurIPS},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 year = {2017}
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of NAACL",
    year = "2019",
    url = "https://aclanthology.org/N19-1423",
    pages = "4171--4186",
}

@article{t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {JMLR},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@inproceedings{prompt1,
  title={Prompt programming for large language models: Beyond the few-shot paradigm},
  author={Reynolds, Laria and McDonell, Kyle},
  booktitle={Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--7},
  year={2021},
  url={https://arxiv.org/pdf/2102.07350.pdf}
}

@article{wang2022benchmarking,
  title   = {Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks},
  author  = {Yizhong Wang and Swaroop Mishra and Pegah Alipoormolabashi and Yeganeh Kordi and Amirreza Mirzaei and Anjana Arunkumar and Arjun Ashok and Arut Selvan Dhanasekaran and Atharva Naik and David Stap and Eshaan Pathak and Giannis Karamanolakis and Haizhi Gary Lai and Ishan Purohit and Ishani Mondal and Jacob Anderson and Kirby Kuznia and Krima Doshi and Maitreya Patel and Kuntal Kumar Pal and Mehrad Moradshahi and Mihir Parmar and Mirali Purohit and Neeraj Varshney and Phani Rohitha Kaza and Pulkit Verma and Ravsehaj Singh Puri and Rushang Karia and Shailaja Keyur Sampat and Savan Doshi and Siddhartha Mishra and Sujan Reddy and Sumanta Patro and Tanay Dixit and Xudong Shen and Chitta Baral and Yejin Choi and Noah A. Smith and Hannaneh Hajishirzi and Daniel Khashabi},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2204.07705}
}

@article{merity2016pointer,
  title   = {Pointer Sentinel Mixture Models},
  author  = {Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
  year    = {2016},
  journal = {arXiv preprint arXiv: Arxiv-1609.07843},
  url = {https://arxiv.org/abs/1609.07843}
}

@inproceedings{perez2021true,
title={True Few-Shot Learning with Language Models},
author={Ethan Perez and Douwe Kiela and Kyunghyun Cho},
booktitle={Advances in NeurIPS},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=ShnM-rRh4T}
}

@article{khashabi2021prompt,
  title   = {Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts},
  author  = {Daniel Khashabi and Shane Lyu and Sewon Min and Lianhui Qin and Kyle Richardson and Sean Welleck and Hannaneh Hajishirzi and Tushar Khot and Ashish Sabharwal and Sameer Singh and Yejin Choi},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2112.08348}
}

@article{min2021noisy,
  title   = {Noisy Channel Language Model Prompting for Few-Shot Text Classification},
  author  = {Sewon Min and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2108.04106}
}

@article{min2021metaicl,
  title   = {MetaICL: Learning to Learn In Context},
  author  = {Sewon Min and Mike Lewis and Luke Zettlemoyer and Hannaneh Hajishirzi},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2110.15943}
}

@misc{ivison2022hyperdecoders,
    title={Hyperdecoders: Instance-specific decoders for multi-task NLP},
    author={Hamish Ivison and Matthew E. Peters},
    year={2022},
    eprint={2203.08304},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{wei2021finetuned,
title={Finetuned Language Models are Zero-Shot Learners},
author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le},
booktitle={ICLR},
year={2022},
url={https://openreview.net/forum?id=gEZrGCozdqR}
}

@article{gao2020pile,
  title   = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  author  = {Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
  year    = {2020},
  journal = {arXiv preprint arXiv: Arxiv-2101.00027}
}

@inproceedings{sanh2022multitask,
title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
author={Victor Sanh and Albert Webson and Colin Raffel and Stephen Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Teven Le Scao and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M Rush},
booktitle={ICLR},
year={2022},
url={https://openreview.net/forum?id=9Vrb9D0WI4}
}

@inproceedings{huggingface,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of EMNLP",
    year = "2020",
    url = "https://aclanthology.org/2020.emnlp-demos.6",
}

@inproceedings{realtoxicityprompts,
    title = "{R}eal{T}oxicity{P}rompts: Evaluating Neural Toxic Degeneration in Language Models",
    author = "Gehman, Samuel  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Choi, Yejin  and
      Smith, Noah A.",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    year = "2020",
    url = "https://aclanthology.org/2020.findings-emnlp.301",
    pages = "3356--3369",
}

@article{selfdiagnosis,
  title={Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp},
  author={Schick, Timo and Udupa, Sahana and Sch{\"u}tze, Hinrich},
  journal={TACL},
  volume={9},
  pages={1408--1424},
  year={2021},
  publisher={MIT Press}
}

@inproceedings{generating,
    title = "Generating Datasets with Pretrained Language Models",
    author = {Schick, Timo  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of EMNLP",
    year = "2021",
    url = "https://aclanthology.org/2021.emnlp-main.555",
    pages = "6943--6951",
}

@inproceedings{websonpavlick2022prompt,
    title = "Do Prompt-Based Models Really Understand the Meaning of Their Prompts?",
    author = "Webson, Albert and Pavlick, Ellie",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.167",
    pages = "2300--2344",
}

@software{gpt-neo,
  author       = {Black, Sid and
                  Gao, Leo and
                  Wang, Phil and
                  Leahy, Connor and
                  Biderman, Stella},
  title        = {{GPT-Neo: Large Scale Autoregressive Language 
                   Modeling with Mesh-Tensorflow}},
  month        = mar,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.5297715},
  url          = {https://doi.org/10.5281/zenodo.5297715}
}

@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}