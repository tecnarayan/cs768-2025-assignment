\begin{thebibliography}{10}

\bibitem{anthony2017thinking}
Thomas Anthony, Zheng Tian, and David Barber.
\newblock Thinking fast and slow with deep learning and tree search.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{azerbayev2023proofnet}
Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward~W Ayers,
  Dragomir Radev, and Jeremy Avigad.
\newblock Proofnet: Autoformalizing and formally proving undergraduate-level
  mathematics.
\newblock {\em arXiv preprint arXiv:2302.12433}, 2023.

\bibitem{llemma}
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco~Dos Santos, Stephen
  McAleer, Albert~Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck.
\newblock Llemma: An open language model for mathematics, 2023.

\bibitem{qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,
  Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji
  Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,
  Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
  Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang,
  Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan,
  Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou,
  Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.
\newblock Qwen technical report.
\newblock {\em arXiv preprint arXiv:2309.16609}, 2023.

\bibitem{gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  et~al.
\newblock Training verifiers to solve math word problems.
\newblock {\em arXiv preprint arXiv:2110.14168}, 2021.

\bibitem{gemini}
Google.
\newblock Gemini: A family of highly capable multimodal models, 2023.

\bibitem{pact}
Jesse~Michael Han, Jason Rute, Yuhuai Wu, Edward~W Ayers, and Stanislas Polu.
\newblock Proof artifact co-training for theorem proving with language models.
\newblock {\em arXiv preprint arXiv:2102.06203}, 2021.

\bibitem{MATH}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric
  Tang, Dawn Song, and Jacob Steinhardt.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock {\em arXiv preprint arXiv:2103.03874}, 2021.

\bibitem{huang2024mustard}
Yinya Huang, Xiaohan Lin, Zhengying Liu, Qingxing Cao, Huajian Xin, Haiming
  Wang, Zhenguo Li, Linqi Song, and Xiaodan Liang.
\newblock {MUSTARD}: Mastering uniform synthesis of theorem and proof data.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{2023internlm}
InternLM.
\newblock Internlm: A multilingual language model with progressively enhanced
  capabilities.
\newblock \url{https://github.com/InternLM/InternLM}, 2023.

\bibitem{jiang2023multilingual}
Albert~Q Jiang, Wenda Li, and Mateja Jamnik.
\newblock Multilingual mathematical autoformalization.
\newblock {\em arXiv preprint arXiv:2311.03755}, 2023.

\bibitem{jiang2022draft}
Albert~Q Jiang, Sean Welleck, Jin~Peng Zhou, Wenda Li, Jiacheng Liu, Mateja
  Jamnik, Timoth{\'e}e Lacroix, Yuhuai Wu, and Guillaume Lample.
\newblock Draft, sketch, and prove: Guiding formal theorem provers with
  informal proofs.
\newblock {\em arXiv preprint arXiv:2210.12283}, 2022.

\bibitem{thor}
Albert~Qiaochu Jiang, Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz
  Odrzyg{\'o}{\'z}d{\'z}, Piotr Mi{\l}o{\'s}, Yuhuai Wu, and Mateja Jamnik.
\newblock Thor: Wielding hammers to integrate language models and automated
  theorem provers.
\newblock {\em Advances in Neural Information Processing Systems},
  35:8360--8373, 2022.

\bibitem{lample2022hypertree}
Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux, Aurelien Rodriguez,
  Amaury Hayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet.
\newblock Hypertree proof search for neural theorem proving.
\newblock {\em Advances in neural information processing systems},
  35:26337--26349, 2022.

\bibitem{Minerva}
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk
  Michalewski, Vinay~V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo
  Gutman{-}Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur{-}Ari, and Vedant Misra.
\newblock Solving quantitative reasoning problems with language models.
\newblock In {\em NeurIPS}, 2022.

\bibitem{liu2023fimo}
Chengwu Liu, Jianhao Shen, Huajian Xin, Zhengying Liu, Ye~Yuan, Haiming Wang,
  Wei Ju, Chuanyang Zheng, Yichun Yin, Lin Li, et~al.
\newblock Fimo: A challenge formal dataset for automated theorem proving.
\newblock {\em arXiv preprint arXiv:2309.04295}, 2023.

\bibitem{mathlib}
The mathlib Community.
\newblock The lean mathematical library.
\newblock In {\em Proceedings of the 9th ACM SIGPLAN International Conference
  on Certified Programs and Proofs}, POPL â€™20. ACM, January 2020.

\bibitem{lean}
Leonardo~de Moura and Sebastian Ullrich.
\newblock The lean 4 theorem prover and programming language.
\newblock In Andr{\'e} Platzer and Geoff Sutcliffe, editors, {\em Automated
  Deduction -- CADE 28}, pages 625--635, Cham, 2021. Springer International
  Publishing.

\bibitem{murphy2024leaneuclid}
Logan Murphy, Kaiyu Yang, Jialiang Sun, Zhaoyu Li, Anima Anandkumar, and Xujie
  Si.
\newblock Autoformalizing {Euclidean} geometry.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2024.

\bibitem{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem{paulson2000isabelle}
Lawrence~C. Paulson.
\newblock Isabelle: The next 700 theorem provers, 2000.

\bibitem{curricum}
Stanislas Polu, Jesse~Michael Han, Kunhao Zheng, Mantas Baksys, Igor
  Babuschkin, and Ilya Sutskever.
\newblock Formal mathematics statement curriculum learning, 2022.

\bibitem{polu2022formal}
Stanislas Polu, Jesse~Michael Han, Kunhao Zheng, Mantas Baksys, Igor
  Babuschkin, and Ilya Sutskever.
\newblock Formal mathematics statement curriculum learning.
\newblock {\em arXiv preprint arXiv:2202.01344}, 2022.

\bibitem{shao2024deepseekmath}
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang,
  Y.~K. Li, Y.~Wu, and Daya Guo.
\newblock Deepseekmath: Pushing the limits of mathematical reasoning in open
  language models, 2024.

\bibitem{GAL}
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony
  Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.
\newblock Galactica: {A} large language model for science.
\newblock 2023.

\bibitem{Coq-refman}
{The Coq Development Team}.
\newblock The {Coq} reference manual -- release 8.18.0.
\newblock \url{https://coq.inria.fr/doc/V8.18.0/refman}, 2023.

\bibitem{wang2023legoprover}
Haiming Wang, Huajian Xin, Chuanyang Zheng, Lin Li, Zhengying Liu, Qingxing
  Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, Jian Yin, Zhenguo Li, Heng
  Liao, and Xiaodan Liang.
\newblock Lego-prover: Neural theorem proving with growing libraries, 2023.

\bibitem{cot}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V
  Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock {\em Advances in Neural Information Processing Systems},
  35:24824--24837, 2022.

\bibitem{welleck2023llmstep}
Sean Welleck and Rahul Saha.
\newblock Llmstep: Llm proofstep suggestions in lean.
\newblock {\em arXiv preprint arXiv:2310.18457}, 2023.

\bibitem{wu2022autoformalization}
Yuhuai Wu, Albert~Q. Jiang, Wenda Li, Markus~N. Rabe, Charles Staats, Mateja
  Jamnik, and Christian Szegedy.
\newblock Autoformalization with large language models, 2022.

\bibitem{xin2024deepseek}
Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo~Liu, Chong
  Ruan, Wenda Li, and Xiaodan Liang.
\newblock Deepseek-prover: Advancing theorem proving in llms through
  large-scale synthetic data.
\newblock {\em arXiv preprint arXiv:2405.14333}, 2024.

\bibitem{yang2023leandojo}
Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu,
  Saad Godil, Ryan Prenger, and Anima Anandkumar.
\newblock {LeanDojo}: Theorem proving with retrieval-augmented language models.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem{ying2024internlmmath}
Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei,
  Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu,
  Shuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang
  Yan, Xipeng Qiu, Jiayu Wang, Kai Chen, and Dahua Lin.
\newblock Internlm-math: Open math large language models toward verifiable
  reasoning, 2024.

\bibitem{zheng2021minif2f}
Kunhao Zheng, Jesse~Michael Han, and Stanislas Polu.
\newblock Minif2f: a cross-system benchmark for formal olympiad-level
  mathematics.
\newblock {\em arXiv preprint arXiv:2109.00110}, 2021.

\end{thebibliography}
