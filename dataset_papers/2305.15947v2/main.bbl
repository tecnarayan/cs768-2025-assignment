\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Richards et~al.(2019)Richards, Lillicrap, Beaudoin, Bengio, Bogacz,
  Christensen, Clopath, Costa, de~Berker, Ganguli, Gillon, Hafner, Kepecs,
  Kriegeskorte, Latham, Lindsay, Miller, Naud, Pack, Poirazi, Roelfsema,
  Sacramento, Saxe, Scellier, Schapiro, Senn, Wayne, Yamins, Zenke, Zylberberg,
  Therien, and Kording]{richards_deep_2019}
Blake~A. Richards, Timothy~P. Lillicrap, Philippe Beaudoin, Yoshua Bengio,
  Rafal Bogacz, Amelia Christensen, Claudia Clopath, Rui~Ponte Costa, Archy
  de~Berker, Surya Ganguli, Colleen~J. Gillon, Danijar Hafner, Adam Kepecs,
  Nikolaus Kriegeskorte, Peter Latham, Grace~W. Lindsay, Kenneth~D. Miller,
  Richard Naud, Christopher~C. Pack, Panayiota Poirazi, Pieter Roelfsema, João
  Sacramento, Andrew Saxe, Benjamin Scellier, Anna~C. Schapiro, Walter Senn,
  Greg Wayne, Daniel Yamins, Friedemann Zenke, Joel Zylberberg, Denis Therien,
  and Konrad~P. Kording.
\newblock A deep learning framework for neuroscience.
\newblock \emph{Nature Neuroscience}, 22\penalty0 (11):\penalty0 1761--1770,
  2019.

\bibitem[Rumelhart and McClelland(1986)]{rumelhart_learning_1986}
David~E. Rumelhart and James~L. McClelland.
\newblock Learning internal representations by error propagation.
\newblock In \emph{Parallel {Distributed} {Processing}: {Explorations} in the
  {Microstructure} of {Cognition}: {Foundations}}, pages 318--362. 1986.

\bibitem[Linnainmaa(1976)]{linnainmaa_taylor_1976}
Seppo Linnainmaa.
\newblock Taylor expansion of the accumulated rounding error.
\newblock \emph{BIT Numerical Mathematics}, 16\penalty0 (2):\penalty0 146--160,
  1976.

\bibitem[Werbos(1990)]{werbos_backpropagation_1990}
Paul Werbos.
\newblock Backpropagation through time: what it does and how to do it.
\newblock \emph{Proceedings of the IEEE}, 78\penalty0 (10):\penalty0
  1550--1560, 1990.

\bibitem[Frenkel et~al.(2021)Frenkel, Bol, and
  Indiveri]{frenkel_bottom-up_2021}
Charlotte Frenkel, David Bol, and Giacomo Indiveri.
\newblock Bottom-up and top-down neural processing systems design: neuromorphic
  intelligence as the convergence of natural and artificial intelligence.
\newblock \emph{arXiv preprint arXiv:2106.01288}, 2021.

\bibitem[Ganguli et~al.(2008)Ganguli, Huh, and
  Sompolinsky]{ganguli_memory_2008}
Surya Ganguli, Dongsung Huh, and Haim Sompolinsky.
\newblock Memory traces in dynamical systems.
\newblock \emph{Proceedings of the National Academy of Sciences}, 105\penalty0
  (48):\penalty0 18970--18975, 2008.

\bibitem[Williams and Zipser(1989)]{williams_learning_1989}
Ronald~J. Williams and David Zipser.
\newblock A learning algorithm for continually running fully recurrent neural
  networks.
\newblock \emph{Neural Computation}, 1\penalty0 (2):\penalty0 270--280, 1989.

\bibitem[Wengert(1964)]{wengert_simple_1964}
R.~E. Wengert.
\newblock A simple automatic derivative evaluation program.
\newblock \emph{Communications of the ACM}, 7\penalty0 (8):\penalty0 463--464,
  1964.

\bibitem[Tallec and Ollivier(2018)]{tallec_unbiased_2018}
Corentin Tallec and Yann Ollivier.
\newblock Unbiased online recurrent optimization.
\newblock In \emph{International {Conference} on {Learning} {Representations}},
  2018.

\bibitem[Mujika et~al.(2018)Mujika, Meier, and
  Steger]{mujika_approximating_2018}
Asier Mujika, Florian Meier, and Angelika Steger.
\newblock Approximating real-time recurrent learning with random {Kronecker}
  factors.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~31, 2018.

\bibitem[Benzing et~al.(2019)Benzing, Gauy, Mujika, Martinsson, and
  Steger]{benzing_optimal_2019}
Frederik Benzing, Marcelo~Matheus Gauy, Asier Mujika, Anders Martinsson, and
  Angelika Steger.
\newblock Optimal {Kronecker}-sum approximation of real time recurrent
  learning.
\newblock In \emph{International {Conference} on {Machine} {Learning}}, 2019.

\bibitem[Murray(2019)]{murray_local_2019}
James~M. Murray.
\newblock Local online learning in recurrent networks with random feedback.
\newblock \emph{eLife}, 8:\penalty0 e43299, 2019.

\bibitem[Bellec et~al.(2020)Bellec, Scherr, Subramoney, Hajek, Salaj,
  Legenstein, and Maass]{bellec_solution_2020}
Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj,
  Robert Legenstein, and Wolfgang Maass.
\newblock A solution to the learning dilemma for recurrent networks of spiking
  neurons.
\newblock \emph{Nature Communications}, 11\penalty0 (1):\penalty0 3625, 2020.

\bibitem[Marschall et~al.(2020)Marschall, Cho, and
  Savin]{marschall_unified_2020}
Owen Marschall, Kyunghyun Cho, and Cristina Savin.
\newblock A unified framework of online learning algorithms for training
  recurrent neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 135:5320--135:5353, 2020.

\bibitem[Menick et~al.(2021)Menick, Elsen, Evci, Osindero, Simonyan, and
  Graves]{menick_practical_2021}
Jacob Menick, Erich Elsen, Utku Evci, Simon Osindero, Karen Simonyan, and Alex
  Graves.
\newblock A practical sparse approximation for real time recurrent learning.
\newblock In \emph{International {Conference} on {Learning} {Representations}},
  2021.

\bibitem[Zenke and Vogels(2021)]{zenke_remarkable_2021}
Friedemann Zenke and Tim~P. Vogels.
\newblock The remarkable robustness of surrogate gradient learning for
  instilling complex function in spiking neural networks.
\newblock \emph{Neural Computation}, 33\penalty0 (4):\penalty0 899--925, 2021.

\bibitem[Baydin et~al.(2022)Baydin, Pearlmutter, Syme, Wood, and
  Torr]{baydin_gradients_2022}
Atılım~Güneş Baydin, Barak~A. Pearlmutter, Don Syme, Frank Wood, and Philip
  Torr.
\newblock Gradients without backpropagation.
\newblock \emph{arXiv preprint arXiv:2202.08587}, 2022.

\bibitem[Silver et~al.(2022)Silver, Goyal, Danihelka, Hessel, and
  Hasselt]{silver_learning_2022}
David Silver, Anirudh Goyal, Ivo Danihelka, Matteo Hessel, and Hado~van
  Hasselt.
\newblock Learning by directional gradient descent.
\newblock In \emph{International {Conference} on {Learning} {Representations}},
  2022.

\bibitem[Gu et~al.(2020)Gu, Dao, Ermon, Rudra, and Ré]{gu_hippo_2020}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré.
\newblock {HiPPO}: recurrent memory with optimal polynomial projections.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~33, 2020.

\bibitem[Gu et~al.(2022)Gu, Goel, and Ré]{gu_efficiently_2022}
Albert Gu, Karan Goel, and Christopher Ré.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In \emph{International {Conference} on {Learning} {Representations}},
  2022.

\bibitem[Gupta et~al.(2022)Gupta, Gu, and Berant]{gupta_diagonal_2022}
Ankit Gupta, Albert Gu, and Jonathan Berant.
\newblock Diagonal state spaces are as effective as structured states spaces.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~35, 2022.

\bibitem[Smith et~al.(2023)Smith, Warrington, and
  Linderman]{smith_simplified_2023}
Jimmy~T.H. Smith, Andrew Warrington, and Scott~W. Linderman.
\newblock Simplified state space layers for sequence modeling.
\newblock In \emph{International {Conference} on {Learning} {Representations}},
  2023.

\bibitem[Orvieto et~al.(2023)Orvieto, Smith, Gu, Fernando, Gulcehre, Pascanu,
  and De]{orvieto_resurrecting_2023}
Antonio Orvieto, Samuel~L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre,
  Razvan Pascanu, and Soham De.
\newblock Resurrecting recurrent neural networks for long sequences.
\newblock \emph{arXiv preprint arXiv:2303.06349}, 2023.

\bibitem[Tay et~al.(2020)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang,
  Ruder, and Metzler]{tay_long_2020}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena: {A} benchmark for efficient transformers.
\newblock \emph{arXiv preprint arXiv:2011.04006}, 2020.

\bibitem[Griewank and Walther(2008)]{griewank_evaluating_2008}
Andreas Griewank and Andrea Walther.
\newblock \emph{Evaluating {Derivatives}}.
\newblock Society for Industrial and Applied Mathematics, 2008.

\bibitem[McBride and Narendra(1965)]{mcbride_optimization_1965}
L.~McBride and K.~Narendra.
\newblock Optimization of time-varying systems.
\newblock \emph{IEEE Transactions on Automatic Control}, 10\penalty0
  (3):\penalty0 289--294, 1965.

\bibitem[Jaderberg et~al.(2017)Jaderberg, Czarnecki, Osindero, Vinyals, Graves,
  Silver, and Kavukcuoglu]{jaderberg_decoupled_2017}
Max Jaderberg, Wojciech~Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex
  Graves, David Silver, and Koray Kavukcuoglu.
\newblock Decoupled neural interfaces using synthetic gradients.
\newblock In \emph{International {Conference} on {Machine} {Learning}}, 2017.

\bibitem[Wayne(2013)]{wayne_self-modeling_2013}
Gregory~D. Wayne.
\newblock \emph{Self-modeling neural systems}.
\newblock PhD thesis, Columbia University, 2013.

\bibitem[Schmidhuber(1990)]{schmidhuber_recurrent_1990}
J.~Schmidhuber.
\newblock Recurrent networks adjusted by adaptive critics.
\newblock In \emph{{IEEE}/{INNS} {International} {Joint} {Conference} on
  {Neural} {Networks}}, volume~1, 1990.

\bibitem[Sutton(1988)]{sutton_learning_1988}
Richard~S. Sutton.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine Learning}, 3:\penalty0 9--44, 1988.

\bibitem[Irie et~al.(2023)Irie, Gopalakrishnan, and
  Schmidhuber]{irie_exploring_2023}
Kazuki Irie, Anand Gopalakrishnan, and Jürgen Schmidhuber.
\newblock Exploring the promise and limits of real-time recurrent learning.
\newblock \emph{arXiv preprint arXiv:2305.19044}, 2023.

\bibitem[Javed et~al.(2023)Javed, Shah, Sutton, and White]{javed_scalable_2023}
Khurram Javed, Haseeb Shah, Richard~S Sutton, and Martha White.
\newblock Scalable real-time recurrent learning using columnar-constructive
  networks.
\newblock \emph{Journal of Machine Learning Research}, 24, 2023.

\bibitem[Bengio et~al.(1994)Bengio, Simard, and Frasconi]{bengio_learning_1994}
Y.~Bengio, P.~Simard, and P.~Frasconi.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock \emph{IEEE Transactions on Neural Networks}, 5\penalty0 (2), 1994.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{pascanu_diculty_2013}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the diﬃculty of training recurrent neural networks.
\newblock In \emph{International {Conference} on {Machine} {Learning}}, 2013.

\bibitem[Blelloch(1990)]{blelloch_prex_1990}
Guy~E. Blelloch.
\newblock Preﬁx sums and their applications, 1990.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba_layer_2016}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E. Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Dauphin et~al.(2017)Dauphin, Fan, Auli, and
  Grangier]{dauphin_language_2017}
Yann~N. Dauphin, Angela Fan, Michael Auli, and David Grangier.
\newblock Language modeling with gated convolutional networks.
\newblock In \emph{International {Conference} on {Machine} {Learning}}, 2017.

\bibitem[Mozer(1989)]{mozer_focused_1989}
Michael~C Mozer.
\newblock A focused backpropagation algorithm for temporal pattern recognition.
\newblock \emph{Complex Systems}, 3, 1989.

\bibitem[Gori et~al.(1989)Gori, Bengio, and De~Mori]{gori_bps_1989}
Marco Gori, Yoshua Bengio, and Renato De~Mori.
\newblock {BPS}: a learning algorithm for capturing the dynamic nature of
  speech.
\newblock In \emph{International {Joint} {Conference} on {Neural} {Networks}},
  1989.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter_long_1997}
Sepp Hochreiter and Jürgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural Computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky_learning_2009}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Nangia and Bowman(2018)]{nangia_listops_2018}
Nikita Nangia and Samuel~R Bowman.
\newblock {ListOps}: {A} diagnostic dataset for latent tree learning.
\newblock \emph{arXiv preprint arXiv:1804.06028}, 2018.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and
  Potts]{maas_learning_2011}
Andrew~L. Maas, Raymond~E. Daly, Peter~T. Pham, Dan Huang, Andrew~Y. Ng, and
  Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proceedings of the 49th {Annual} {Meeting} of the
  {Association} for {Computational} {Linguistics}: {Human} {Language}
  {Technologies}}, 2011.

\bibitem[Cho et~al.(2014)Cho, van Merrienboer, Bahdanau, and
  Bengio]{cho_properties_2014}
Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio.
\newblock On the properties of neural machine translation: encoder-decoder
  approaches.
\newblock In \emph{Proceedings of {SSST}-8, {Eighth} {Workshop} on {Syntax},
  {Semantics} and {Structure} in {Statistical} {Translation}}, 2014.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and
  Ré]{dao_flashattention_2022}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.
\newblock Flashattention: {Fast} and memory-efficient exact attention with
  io-awareness.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 16344--16359, 2022.

\bibitem[Zenke and Neftci(2021)]{zenke_brain-inspired_2021}
Friedemann Zenke and Emre~O. Neftci.
\newblock Brain-inspired learning on neuromorphic substrates.
\newblock \emph{Proceedings of the IEEE}, 109\penalty0 (5):\penalty0 935--950,
  May 2021.

\bibitem[Frenkel and Indiveri(2022)]{frenkel_reckon_2022}
Charlotte Frenkel and Giacomo Indiveri.
\newblock {ReckOn}: a 28nm sub-mm2 task-agnostic spiking recurrent neural
  network processor enabling on-chip learning over second-long timescales.
\newblock In \emph{2022 {IEEE} {International} {Solid}- {State} {Circuits}
  {Conference} ({ISSCC})}, volume~65, February 2022.

\bibitem[Demirag et~al.(2021)Demirag, Frenkel, Payvand, and
  Indiveri]{demirag_online_2021}
Yigit Demirag, Charlotte Frenkel, Melika Payvand, and Giacomo Indiveri.
\newblock Online training of spiking recurrent neural networks with
  phase-change memory synapses.
\newblock \emph{arXiv preprint arXiv:2108.01804}, 2021.

\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihelka]{graves_neural_2014}
Alex Graves, Greg Wayne, and Ivo Danihelka.
\newblock Neural {Turing} machines.
\newblock \emph{arXiv preprint arXiv:1410.5401}, 2014.

\bibitem[Fu et~al.(2023)Fu, Dao, Saab, Thomas, Rudra, and Ré]{fu_hungry_2023}
Daniel~Y. Fu, Tri Dao, Khaled~K. Saab, Armin~W. Thomas, Atri Rudra, and
  Christopher Ré.
\newblock Hungry hungry hippos: towards language modeling with state space
  models.
\newblock \emph{arXiv preprint arXiv:2212.14052}, 2023.

\bibitem[Mountcastle(1957)]{mountcastle_modality_1957}
Vernon~B. Mountcastle.
\newblock Modality and topographic properties of single neurons of cat's
  somatic sensory cortex.
\newblock \emph{Journal of Neurophysiology}, 20\penalty0 (4):\penalty0
  408--434, 1957.

\bibitem[Douglas et~al.(1989)Douglas, Martin, and
  Whitteridge]{douglas_canonical_1989}
Rodney~J. Douglas, Kevan~A.C. Martin, and David Whitteridge.
\newblock A canonical microcircuit for neocortex.
\newblock \emph{Neural Computation}, 1\penalty0 (4):\penalty0 480--488, 1989.

\bibitem[Lee et~al.(2015)Lee, Zhang, Fischer, and Bengio]{lee_difference_2015}
Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio.
\newblock Difference target propagation.
\newblock In \emph{Joint {European} {Conference} on {Machine} {Learning} and
  {Knowledge} {Discovery} in {Databases}}, 2015.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Cownden, Tweed, and
  Akerman]{lillicrap_random_2016}
Timothy~P. Lillicrap, Daniel Cownden, Douglas~B. Tweed, and Colin~J. Akerman.
\newblock Random synaptic feedback weights support error backpropagation for
  deep learning.
\newblock \emph{Nature Communications}, 7\penalty0 (1):\penalty0 13276, 2016.

\bibitem[Scellier and Bengio(2017)]{scellier_equilibrium_2017}
Benjamin Scellier and Yoshua Bengio.
\newblock Equilibrium propagation: bridging the gap between energy-based models
  and backpropagation.
\newblock \emph{Frontiers in Computational Neuroscience}, 11, 2017.

\bibitem[Roelfsema and Holtmaat(2018)]{roelfsema_control_2018}
Pieter~R. Roelfsema and Anthony Holtmaat.
\newblock Control of synaptic plasticity in deep cortical networks.
\newblock \emph{Nature Reviews Neuroscience}, 19\penalty0 (3):\penalty0
  166--180, 2018.

\bibitem[Whittington and Bogacz(2019)]{whittington_theories_2019}
James C.~R. Whittington and Rafal Bogacz.
\newblock Theories of error back-propagation in the brain.
\newblock \emph{Trends in Cognitive Sciences}, 23\penalty0 (3):\penalty0
  235--250, 2019.

\bibitem[Lillicrap et~al.(2020)Lillicrap, Santoro, Marris, Akerman, and
  Hinton]{lillicrap_backpropagation_2020}
Timothy~P. Lillicrap, Adam Santoro, Luke Marris, Colin~J. Akerman, and Geoffrey
  Hinton.
\newblock Backpropagation and the brain.
\newblock \emph{Nature Reviews Neuroscience}, 21\penalty0 (6):\penalty0
  335--346, 2020.

\bibitem[Payeur et~al.(2021)Payeur, Guerguiev, Zenke, Richards, and
  Naud]{payeur_burst-dependent_2021}
Alexandre Payeur, Jordan Guerguiev, Friedemann Zenke, Blake~A. Richards, and
  Richard Naud.
\newblock Burst-dependent synaptic plasticity can coordinate learning in
  hierarchical circuits.
\newblock \emph{Nature Neuroscience}, 24\penalty0 (7):\penalty0 1010--1019,
  2021.

\bibitem[Meulemans et~al.(2022)Meulemans, Zucchet, Kobayashi, von Oswald, and
  Sacramento]{meulemans_least-control_2022}
Alexander Meulemans, Nicolas Zucchet, Seijin Kobayashi, Johannes von Oswald,
  and João Sacramento.
\newblock The least-control principle for local learning at equilibrium.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~35, 2022.

\bibitem[Laborieux and Zenke(2022)]{laborieux_holomorphic_2022}
Axel Laborieux and Friedemann Zenke.
\newblock Holomorphic equilibrium propagation computes exact gradients through
  finite size oscillations.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~35, 2022.

\bibitem[Senn et~al.(2023)Senn, Dold, Kungl, Ellenberger, Jordan, Bengio,
  Sacramento, and Petrovici]{senn_neuronal_2023}
Walter Senn, Dominik Dold, Akos~F. Kungl, Benjamin Ellenberger, Jakob Jordan,
  Yoshua Bengio, João Sacramento, and Mihai~A. Petrovici.
\newblock A neuronal least-action principle for real-time learning in cortical
  circuits.
\newblock \emph{bioRxiv 2023.03.25.534198}, 2023.

\bibitem[Lillicrap and Santoro(2019)]{lillicrap_backpropagation_2019}
Timothy~P. Lillicrap and Adam Santoro.
\newblock Backpropagation through time and the brain.
\newblock \emph{Current Opinion in Neurobiology}, 55:\penalty0 82--89, 2019.

\end{thebibliography}
