\begin{thebibliography}{10}

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em International Conference on Machine Learning}, 2015.

\bibitem{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{ulyanov2016instance}
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
\newblock Instance normalization: The missing ingredient for fast stylization.
\newblock {\em arXiv preprint arXiv:1607.08022}, 2016.

\bibitem{wu2018group}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 3--19, 2018.

\bibitem{nair2010rectified}
Vinod Nair and Geoffrey~E Hinton.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In {\em Proceedings of the 27th international conference on machine
  learning (ICML-10)}, pages 807--814, 2010.

\bibitem{maas2013rectifier}
Andrew~L Maas, Awni~Y Hannun, and Andrew~Y Ng.
\newblock Rectifier nonlinearities improve neural network acoustic models.
\newblock In {\em ICML Workshop on Deep Learning for Audio, Speech, and
  Language Processing}, 2013.

\bibitem{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015.

\bibitem{clevert2015fast}
Djork-Arn{\'e} Clevert, Thomas Unterthiner, and Sepp Hochreiter.
\newblock Fast and accurate deep network learning by exponential linear units
  (elus).
\newblock {\em International Conference on Learning Representations}, 2016.

\bibitem{hendrycks2016bridging}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock {\em arXiv preprint arXiv:1606.08415}, 2016.

\bibitem{klambauer2017self}
G{\"u}nter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter.
\newblock Self-normalizing neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  971--980, 2017.

\bibitem{ramachandran2017searching}
Prajit Ramachandran, Barret Zoph, and Quoc~V Le.
\newblock Searching for activation functions.
\newblock In {\em ICLR Workshop}, 2018.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{he2016identity}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In {\em European conference on computer vision}, pages 630--645.
  Springer, 2016.

\bibitem{sandler2018mobilenetv2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4510--4520, 2018.

\bibitem{tan2019efficientnet}
Mingxing Tan and Quoc~V Le.
\newblock {EfficientNet}: Rethinking model scaling for convolutional neural
  networks.
\newblock In {\em International Conference on Machine Learning}, 2019.

\bibitem{he2017mask}
Kaiming He, Georgia Gkioxari, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Mask r-cnn.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2961--2969, 2017.

\bibitem{lin2017feature}
Tsung-Yi Lin, Piotr Doll{\'a}r, Ross Girshick, Kaiming He, Bharath Hariharan,
  and Serge Belongie.
\newblock Feature pyramid networks for object detection.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2117--2125, 2017.

\bibitem{du2020spinenet}
Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi, Mingxing Tan, Yin Cui,
  Quoc~V. Le, and Xiaodan Song.
\newblock Spinenet: Learning scale-permuted backbone for recognition and
  localization.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2020.

\bibitem{brock2018large}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale gan training for high fidelity natural image synthesis.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{tan2019mnasnet}
Mingxing Tan, Bo~Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew
  Howard, and Quoc~V Le.
\newblock Mnasnet: Platform-aware neural architecture search for mobile.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2820--2828, 2019.

\bibitem{du2019spinenet}
Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi, Mingxing Tan, Yin Cui,
  Quoc~V Le, and Xiaodan Song.
\newblock Spinenet: Learning scale-permuted backbone for recognition and
  localization.
\newblock {\em arXiv preprint arXiv:1912.05027}, 2019.

\bibitem{elfwing2018sigmoid}
Stefan Elfwing, Eiji Uchibe, and Kenji Doya.
\newblock Sigmoid-weighted linear units for neural network function
  approximation in reinforcement learning.
\newblock {\em Neural Networks}, 107:3--11, 2018.

\bibitem{singh2019filter}
Saurabh Singh and Shankar Krishnan.
\newblock Filter response normalization layer: Eliminating batch dependence in
  the training of deep neural networks.
\newblock {\em arXiv preprint arXiv:1911.09737}, 2019.

\bibitem{luo2018differentiable}
Ping Luo, Jiamin Ren, Zhanglin Peng, Ruimao Zhang, and Jingyu Li.
\newblock Differentiable learning-to-normalize via switchable normalization.
\newblock {\em International Conference on Learning Represenations}, 2019.

\bibitem{stanley2002efficient}
Kenneth~O Stanley and Risto Miikkulainen.
\newblock Efficient reinforcement learning through evolving neural network
  topologies.
\newblock In {\em Proceedings of the 4th Annual Conference on Genetic and
  Evolutionary Computation}, pages 569--577, 2002.

\bibitem{bayer2009evolving}
Justin Bayer, Daan Wierstra, Julian Togelius, and J{\"u}rgen Schmidhuber.
\newblock Evolving memory cell structures for sequence learning.
\newblock In {\em International Conference on Artificial Neural Networks},
  pages 755--764. Springer, 2009.

\bibitem{zoph2016neural}
Barret Zoph and Quoc~V Le.
\newblock Neural architecture search with reinforcement learning.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{baker2016designing}
Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar.
\newblock Designing neural network architectures using reinforcement learning.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{zoph2018learning}
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc~V Le.
\newblock Learning transferable architectures for scalable image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 8697--8710, 2018.

\bibitem{liu2017hierarchical}
Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray
  Kavukcuoglu.
\newblock Hierarchical representations for efficient architecture search.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{brock2017smash}
Andrew Brock, Theodore Lim, James~M Ritchie, and Nick Weston.
\newblock Smash: one-shot model architecture search through hypernetworks.
\newblock {\em arXiv preprint arXiv:1708.05344}, 2017.

\bibitem{liu2018progressive}
Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li,
  Li~Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy.
\newblock Progressive neural architecture search.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 19--34, 2018.

\bibitem{liu2018darts}
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
\newblock Darts: Differentiable architecture search.
\newblock {\em arXiv preprint arXiv:1806.09055}, 2018.

\bibitem{luo2018neural}
Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu.
\newblock Neural architecture optimization.
\newblock In {\em Advances in neural information processing systems}, pages
  7816--7827, 2018.

\bibitem{bender2018understanding}
Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc
  Le.
\newblock Understanding and simplifying one-shot architecture search.
\newblock In {\em International Conference on Machine Learning}, pages
  550--559, 2018.

\bibitem{xie2018snas}
Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin.
\newblock Snas: stochastic neural architecture search.
\newblock {\em arXiv preprint arXiv:1812.09926}, 2018.

\bibitem{cai2018proxylessnas}
Han Cai, Ligeng Zhu, and Song Han.
\newblock Proxylessnas: Direct neural architecture search on target task and
  hardware.
\newblock {\em arXiv preprint arXiv:1812.00332}, 2018.

\bibitem{elsken2018neural}
Thomas Elsken, Jan~Hendrik Metzen, and Frank Hutter.
\newblock Neural architecture search: A survey.
\newblock {\em arXiv preprint arXiv:1808.05377}, 2018.

\bibitem{elsken2018efficient}
Thomas Elsken, Jan~Hendrik Metzen, and Frank Hutter.
\newblock Efficient multi-objective neural architecture search via lamarckian
  evolution.
\newblock {\em arXiv preprint arXiv:1804.09081}, 2018.

\bibitem{pham2018efficient}
Hieu Pham, Melody~Y Guan, Barret Zoph, Quoc~V Le, and Jeff Dean.
\newblock Efficient neural architecture search via parameter sharing.
\newblock {\em arXiv preprint arXiv:1802.03268}, 2018.

\bibitem{real2019regularized}
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc~V Le.
\newblock Regularized evolution for image classifier architecture search.
\newblock In {\em Proceedings of the AAAI conference on artificial
  intelligence}, volume~33, pages 4780--4789, 2019.

\bibitem{xie2019exploring}
Saining Xie, Alexander Kirillov, Ross Girshick, and Kaiming He.
\newblock Exploring randomly wired neural networks for image recognition.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1284--1293, 2019.

\bibitem{wu2019fbnet}
Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu,
  Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer.
\newblock Fbnet: Hardware-aware efficient convnet design via differentiable
  neural architecture search.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 10734--10742, 2019.

\bibitem{guo2019single}
Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and
  Jian Sun.
\newblock Single path one-shot neural architecture search with uniform
  sampling.
\newblock {\em arXiv preprint arXiv:1904.00420}, 2019.

\bibitem{hu2019efficient}
Hanzhang Hu, John Langford, Rich Caruana, Saurajit Mukherjee, Eric~J Horvitz,
  and Debadeepta Dey.
\newblock Efficient forward architecture search.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10122--10131, 2019.

\bibitem{cai2019once}
Han Cai, Chuang Gan, and Song Han.
\newblock Once for all: Train one network and specialize it for efficient
  deployment.
\newblock {\em arXiv preprint arXiv:1908.09791}, 2019.

\bibitem{li2019random}
Liam Li and Ameet Talwalkar.
\newblock Random search and reproducibility for neural architecture search.
\newblock {\em arXiv preprint arXiv:1902.07638}, 2019.

\bibitem{chen2019progressive}
Xin Chen, Lingxi Xie, Jun Wu, and Qi~Tian.
\newblock Progressive differentiable architecture search: Bridging the depth
  gap between search and evaluation.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1294--1303, 2019.

\bibitem{stamoulis2019single}
Dimitrios Stamoulis, Ruizhou Ding, Di~Wang, Dimitrios Lymberopoulos, Bodhi
  Priyantha, Jie Liu, and Diana Marculescu.
\newblock Single-path nas: Designing hardware-efficient convnets in less than 4
  hours.
\newblock {\em arXiv preprint arXiv:1904.02877}, 2019.

\bibitem{howard2019searching}
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo~Chen, Mingxing
  Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et~al.
\newblock Searching for mobilenetv3.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1314--1324, 2019.

\bibitem{real2020automl}
Esteban Real, Chen Liang, David~R So, and Quoc~V Le.
\newblock Automl-zero: Evolving machine learning algorithms from scratch.
\newblock {\em arXiv preprint arXiv:2003.03384}, 2020.

\bibitem{zhang2019fixup}
Hongyi Zhang, Yann~N Dauphin, and Tengyu Ma.
\newblock Fixup initialization: Residual learning without normalization.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{de2020batch}
Soham De and Samuel~L Smith.
\newblock Batch normalization biases deep residual networks towards shallow
  paths.
\newblock {\em arXiv preprint arXiv:2002.10444}, 2020.

\bibitem{bachlechner2020rezero}
Thomas Bachlechner, Bodhisattwa~Prasad Majumder, Huanru~Henry Mao, Garrison~W
  Cottrell, and Julian McAuley.
\newblock Rezero is all you need: Fast convergence at large depth.
\newblock {\em arXiv preprint arXiv:2003.04887}, 2020.

\bibitem{goldberg1991comparative}
David~E Goldberg and Kalyanmoy Deb.
\newblock A comparative analysis of selection schemes used in genetic
  algorithms.
\newblock In {\em Foundations of genetic algorithms}, volume~1, pages 69--93.
  Elsevier, 1991.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock {\em Master's thesis, Department of Computer Science, University of
  Toronto}, 2009.

\bibitem{deb2002fast}
Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan.
\newblock A fast and elitist multiobjective genetic algorithm: Nsga-ii.
\newblock {\em IEEE transactions on evolutionary computation}, 6(2):182--197,
  2002.

\bibitem{cubuk2019randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical data augmentation with no separate search.
\newblock {\em arXiv preprint arXiv:1909.13719}, 2019.

\bibitem{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock {\em arXiv preprint arXiv:1706.02677}, 2017.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em European conference on computer vision}, pages 740--755.
  Springer, 2014.

\bibitem{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in neural information processing systems}, pages
  2672--2680, 2014.

\bibitem{salimans2016improved}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen.
\newblock Improved techniques for training gans.
\newblock In {\em Advances in neural information processing systems}, pages
  2234--2242, 2016.

\bibitem{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock In {\em Advances in neural information processing systems}, pages
  6626--6637, 2017.

\bibitem{karras2017progressive}
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
\newblock Progressive growing of gans for improved quality, stability, and
  variation.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{hoffer2018norm}
Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry.
\newblock Norm matters: efficient and accurate normalization schemes in deep
  networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2160--2170, 2018.

\bibitem{li2019exponential}
Zhiyuan Li and Sanjeev Arora.
\newblock An exponential learning rate schedule for deep learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\end{thebibliography}
