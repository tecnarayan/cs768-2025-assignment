% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Migdalas_Book_MultilevelOpt_2013}
A.~Migdalas, P.~M. Pardalos, and P.~V{\"a}rbrand, \emph{Multilevel
  optimization: algorithms and applications}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer Science \& Business Media, 2013, vol.~20.

\bibitem{Dempe_Book_2002}
S.~Dempe, \emph{Foundations of bilevel programming}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer Science \& Business Media, 2002.

\bibitem{Franceschi_ICML_2018}
L.~Franceschi, P.~Frasconi, S.~Salzo, R.~Grazzi, and M.~Pontil, ``Bilevel
  programming for hyperparameter optimization and meta-learning,'' \emph{arXiv
  preprint arXiv:1806.04910}, 2018.

\bibitem{Rajeswaran_MetaLearning_NIPS_2019}
A.~Rajeswaran, C.~Finn, S.~M. Kakade, and S.~Levine, ``Meta-learning with
  implicit gradients,'' in \emph{Advances in Neural Information Processing
  Systems}, vol.~32.\hskip 1em plus 0.5em minus 0.4em\relax Curran Associates,
  Inc., 2019, pp. 113--124.

\bibitem{Shaban_TruncatedBackProp_2019}
A.~Shaban, C.-A. Cheng, N.~Hatch, and B.~Boots, ``Truncated back-propagation
  for bilevel optimization,'' 2019.

\bibitem{Franceschi_ICML_2017}
L.~Franceschi, M.~Donini, P.~Frasconi, and M.~Pontil, ``Forward and reverse
  gradient-based hyperparameter optimization,'' in \emph{Proceedings of the
  34th International Conference on Machine Learning - Volume 70}, 2017, p.
  1165â€“1173.

\bibitem{Pedregosa_ICML_2016}
F.~Pedregosa, ``Hyperparameter optimization with approximate gradient,'' in
  \emph{International conference on machine learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2016, pp. 737--746.

\bibitem{Konda_AC_NIPS_2000}
V.~R. Konda and J.~N. Tsitsiklis, ``Actor-critic algorithms,'' in
  \emph{Advances in neural information processing systems}, 2000, pp.
  1008--1014.

\bibitem{raghu2019rapid}
A.~Raghu, M.~Raghu, S.~Bengio, and O.~Vinyals, ``Rapid learning or feature
  reuse? towards understanding the effectiveness of maml,'' in \emph{ICLR},
  2019.

\bibitem{Ghadimi_BSA_Arxiv_2018}
S.~Ghadimi and M.~Wang, ``Approximation methods for bilevel programming,''
  2018.

\bibitem{Ji_ProvablyFastBilevel_Arxiv_2020}
K.~Ji, J.~Yang, and Y.~Liang, ``Bilevel optimization: Nonasymptotic analysis
  and faster algorithms,'' 2020.

\bibitem{Hong_TTSA_Arxiv_2020}
M.~Hong, H.-T. Wai, Z.~Wang, and Z.~Yang, ``A two-timescale framework for
  bilevel optimization: Complexity analysis and application to actor-critic,''
  2020.

\bibitem{Chen_Arxiv_2021STABLE}
T.~Chen, Y.~Sun, and W.~Yin, ``A single-timescale stochastic bilevel
  optimization method,'' \emph{arXiv preprint arXiv:2102.04671}, 2021.

\bibitem{Guo_Arxiv_2021_SVRB}
Z.~Guo and T.~Yang, ``Randomized stochastic variance-reduced methods for
  stochastic bilevel optimization,'' 2021.

\bibitem{Fang_Spider_NIPS_2018}
C.~Fang, C.~J. Li, Z.~Lin, and T.~Zhang, ``Spider: Near-optimal non-convex
  optimization via stochastic path-integrated differential estimator,'' in
  \emph{Advances in Neural Information Processing Systems}, 2018, pp. 689--699.

\bibitem{Cutkosky_NIPS2019}
A.~Cutkosky and F.~Orabona, ``Momentum-based variance reduction in non-convex
  {SGD},'' in \emph{Advances in Neural Information Processing Systems
  32}.\hskip 1em plus 0.5em minus 0.4em\relax Curran Associates, Inc., 2019,
  pp. 15\,236--15\,245.

\bibitem{Dinh_Arxiv_2019}
Q.~Tran-Dinh, N.~H. Pham, D.~T. Phan, and L.~M. Nguyen, ``Hybrid stochastic
  gradient descent algorithms for stochastic nonconvex optimization,''
  \emph{arXiv preprint arXiv:1905.05920}, 2019.

\bibitem{Zhou_NIPS_2018_SNVRG}
D.~Zhou, P.~Xu, and Q.~Gu, ``Stochastic nested variance reduction for nonconvex
  optimization,'' \emph{arXiv preprint arXiv:1806.07811}, 2018.

\bibitem{stackelberg}
H.~V. Stackelberg, \emph{The Theory of Market Economy}.\hskip 1em plus 0.5em
  minus 0.4em\relax Oxford University Press, 1952.

\bibitem{Bracken73}
J.~Bracken and J.~T. McGill, ``Mathematical programs with optimization problems
  in the constraints,'' \emph{Operations Research}, vol.~21, no.~1, pp. 37--44,
  1973.

\bibitem{Bracken74b}
\BIBentryALTinterwordspacing
------, ``Defense applications of mathematical programs with optimization
  problems in the constraints,'' \emph{Operations Research}, vol.~22, no.~5,
  pp. 1086--1096, 1974. [Online]. Available:
  \url{http://www.jstor.org/stable/169661}
\BIBentrySTDinterwordspacing

\bibitem{Bracken74}
J.~Bracken, J.~E. Falk, and J.~T. McGill, ``Technical note---the equivalence of
  two mathematical programs with optimization problems in the constraints,''
  \emph{Operations Research}, vol.~22, no.~5, pp. 1102--1104, 1974.

\bibitem{luo_pang_ralph_1996}
Z.-Q. Luo, J.-S. Pang, and D.~Ralph, \emph{Mathematical Programs with
  Equilibrium Constraints}.\hskip 1em plus 0.5em minus 0.4em\relax Cambridge
  University Press, 1996.

\bibitem{Falk93}
J.~E. Falk and J.~Liu, ``On bilevel programming, part {I}: General nonlinear
  cases,'' \emph{Mathematical Programming volume}, vol.~70, pp. 47--72, 1995.

\bibitem{Vicente_QuadraticBilevel_1994}
L.~Vicente, , G.~Savard, and J.~J{\'u}dice, ``Descent approaches for quadratic
  bilevel programming,'' \emph{Journal of Optimization Theory and
  Applications}, pp. 379--399, 1994.

\bibitem{White93}
D.~J. White and G.~Anandalingam, ``A penalty function approach for solving
  bi-level linear programs,'' \emph{Journal of Global Optimization}, vol.~3,
  pp. 397--419, 1993.

\bibitem{Colson_BilevelReview_AOR_2007}
B.~Colson, P.~Marcotte, and G.~Savard, ``An overview of bilevel optimization,''
  \emph{Annals of Operations Research}, vol. 153, pp. 235--256, 2007.

\bibitem{Liu_Arxiv_2021Survey}
R.~Liu, J.~Gao, J.~Zhang, D.~Meng, and Z.~Lin, ``Investigating bi-level
  optimization for learning and vision from a unified perspective: A survey and
  beyond,'' 2021.

\bibitem{Sabach_SAM_Siam_2017}
\BIBentryALTinterwordspacing
S.~Sabach and S.~Shtern, ``A first order method for solving convex bilevel
  optimization problems,'' \emph{{SIAM} J. Optim.}, vol.~27, no.~2, pp.
  640--660, 2017. [Online]. Available: \url{https://doi.org/10.1137/16M105592X}
\BIBentrySTDinterwordspacing

\bibitem{Liu_generic_Arxiv_2020}
R.~Liu, P.~Mu, X.~Yuan, S.~Zeng, and J.~Zhang, ``A generic first-order
  algorithmic framework for bi-level programming beyond lower-level
  singleton,'' 2020.

\bibitem{Li_ImprovedBilevel_Arxiv_2020}
J.~Li, B.~Gu, and H.~Huang, ``Improved bilevel model: Fast and optimal
  algorithm with theoretical guarantee,'' 2020.

\bibitem{Grazzi_StochasticHypergradients_2020}
R.~Grazzi, M.~Pontil, and S.~Salzo, ``Convergence properties of stochastic
  hypergradients,'' 2020.

\bibitem{Grazzi_ComplexityHypergrad_2020}
R.~Grazzi, L.~Franceschi, M.~Pontil, and S.~Salzo, ``On the iteration
  complexity of hypergradient computation,'' 2020.

\bibitem{Rudin_Book}
W.~Rudin, \emph{\BIBforeignlanguage{English}{Principles of mathematical
  analysis}}, 3rd~ed.\hskip 1em plus 0.5em minus 0.4em\relax McGraw-Hill New
  York, 1976.

\bibitem{Vinyals_miniImageNet_NIPS2016}
\BIBentryALTinterwordspacing
O.~Vinyals, C.~Blundell, T.~Lillicrap, k.~kavukcuoglu, and D.~Wierstra,
  ``Matching networks for one shot learning,'' in \emph{Advances in Neural
  Information Processing Systems}, D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon,
  and R.~Garnett, Eds., vol.~29.\hskip 1em plus 0.5em minus 0.4em\relax Curran
  Associates, Inc., 2016. [Online]. Available:
  \url{https://proceedings.neurips.cc/paper/2016/file/90e1357833654983612fb05e3ec9148c-Paper.pdf}
\BIBentrySTDinterwordspacing

\bibitem{Ravi_2016_MiniImage}
S.~Ravi and H.~Larochelle, ``Optimization as a model for few-shot learning,''
  in \emph{Proceedings of the 5th International Conference on Learning
  Representations}, 2017.

\bibitem{Oreshkin_2018_Tadam}
B.~N. Oreshkin, P.~Rodriguez, and A.~Lacoste, ``Tadam: Task dependent adaptive
  metric for improved few-shot learning,'' \emph{arXiv preprint
  arXiv:1805.10123}, 2018.

\bibitem{Arnold_l2l_2020}
\BIBentryALTinterwordspacing
S.~M.~R. Arnold, P.~Mahajan, D.~Datta, I.~Bunner, and K.~S. Zarkias,
  ``learn2learn: A library for {Meta-Learning} research,'' \emph{CoRR}, Aug.
  2020. [Online]. Available: \url{http://arxiv.org/abs/2008.12284}
\BIBentrySTDinterwordspacing

\bibitem{finn2017model}
C.~Finn, P.~Abbeel, and S.~Levine, ``Model-agnostic meta-learning for fast
  adaptation of deep networks,'' in \emph{Proceedings of the 34th International
  Conference on Machine Learning-Volume 70}.\hskip 1em plus 0.5em minus
  0.4em\relax JMLR. org, 2017, pp. 1126--1135.

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,''
  \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{FashionMNIST_Xiao_2017}
H.~Xiao, K.~Rasul, and R.~Vollgraf. (2017) Fashion-mnist: a novel image dataset
  for benchmarking machine learning algorithms.

\end{thebibliography}
