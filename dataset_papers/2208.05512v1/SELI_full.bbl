\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Byrd and Lipton(2019)]{byrd2019effect}
J.~Byrd and Z.~Lipton.
\newblock What is the effect of importance weighting in deep learning?
\newblock In \emph{International Conference on Machine Learning}, pages
  872--881. PMLR, 2019.

\bibitem[Cao et~al.(2019)Cao, Wei, Gaidon, Arechiga, and Ma]{TengyuMa}
K.~Cao, C.~Wei, A.~Gaidon, N.~Arechiga, and T.~Ma.
\newblock Learning imbalanced datasets with label-distribution-aware margin
  loss.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1567--1578, 2019.

\bibitem[Fang et~al.(2021)Fang, He, Long, and Su]{fang2021exploring}
C.~Fang, H.~He, Q.~Long, and W.~J. Su.
\newblock Exploring deep neural networks via layer-peeled model: Minority
  collapse in imbalanced training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 118\penalty0
  (43), 2021.

\bibitem[Galanti et~al.(2021)Galanti, Gy{\"o}rgy, and Hutter]{galanti2021role}
T.~Galanti, A.~Gy{\"o}rgy, and M.~Hutter.
\newblock On the role of neural collapse in transfer learning.
\newblock \emph{arXiv preprint arXiv:2112.15121}, 2021.

\bibitem[Graf et~al.(2021)Graf, Hofer, Niethammer, and
  Kwitt]{graf2021dissecting}
F.~Graf, C.~Hofer, M.~Niethammer, and R.~Kwitt.
\newblock Dissecting supervised constrastive learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3821--3830. PMLR, 2021.

\bibitem[Grant and Boyd(2014)]{cvx}
M.~Grant and S.~Boyd.
\newblock {CVX}: Matlab software for disciplined convex programming, version
  2.1.
\newblock \url{http://cvxr.com/cvx}, Mar. 2014.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018characterizing}
S.~Gunasekar, J.~Lee, D.~Soudry, and N.~Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{International Conference on Machine Learning}, pages
  1832--1841. PMLR, 2018.

\bibitem[Haeffele and Vidal(2019)]{haeffele2019structured}
B.~D. Haeffele and R.~Vidal.
\newblock Structured low-rank matrix factorization: Global optimality,
  algorithms, and applications.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 42\penalty0 (6):\penalty0 1468--1482, 2019.

\bibitem[Han et~al.(2021)Han, Papyan, and Donoho]{han2021neural}
X.~Han, V.~Papyan, and D.~L. Donoho.
\newblock Neural collapse under mse loss: Proximity to and dynamics on the
  central path.
\newblock \emph{arXiv preprint arXiv:2106.02073}, 2021.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition." computer vision and
  pattern recognition (2015).
\newblock \emph{Google Scholar There is no corresponding record for this
  reference}, pages 770--778, 2015.

\bibitem[Hui et~al.(2022)Hui, Belkin, and Nakkiran]{hui2022limitations}
L.~Hui, M.~Belkin, and P.~Nakkiran.
\newblock Limitations of neural collapse for understanding generalization in
  deep learning.
\newblock \emph{arXiv preprint arXiv:2202.08384}, 2022.

\bibitem[Ji et~al.(2021)Ji, Lu, Zhang, Deng, and Su]{ULPM}
W.~Ji, Y.~Lu, Y.~Zhang, Z.~Deng, and W.~J. Su.
\newblock An unconstrained layer-peeled perspective on neural collapse.
\newblock \emph{arXiv preprint arXiv:2110.02796}, 2021.

\bibitem[Ji and Telgarsky(2018)]{ji2018risk}
Z.~Ji and M.~Telgarsky.
\newblock Risk and parameter convergence of logistic regression.
\newblock \emph{arXiv preprint arXiv:1803.07300}, 2018.

\bibitem[Ji and Telgarsky(2019)]{ji2019implicit}
Z.~Ji and M.~Telgarsky.
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In \emph{Conference on Learning Theory}, pages 1772--1798. PMLR,
  2019.

\bibitem[Ji and Telgarsky(2020)]{ji2020directional}
Z.~Ji and M.~Telgarsky.
\newblock Directional convergence and alignment in deep learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 17176--17186, 2020.

\bibitem[Ji et~al.(2020)Ji, Dud{\'\i}k, Schapire, and
  Telgarsky]{ji2020gradient}
Z.~Ji, M.~Dud{\'\i}k, R.~E. Schapire, and M.~Telgarsky.
\newblock Gradient descent follows the regularization path for general losses.
\newblock In \emph{Conference on Learning Theory}, pages 2109--2136. PMLR,
  2020.

\bibitem[Kang et~al.(2020)Kang, Xie, Rohrbach, Yan, Gordo, Feng, and
  Kalantidis]{kang2020decoupling}
B.~Kang, S.~Xie, M.~Rohrbach, Z.~Yan, A.~Gordo, J.~Feng, and Y.~Kalantidis.
\newblock Decoupling representation and classifier for long-tailed recognition,
  2020.

\bibitem[Kim and Kim(2020)]{KimKim}
B.~Kim and J.~Kim.
\newblock Adjusting decision boundary for class imbalanced learning.
\newblock \emph{IEEE Access}, 8:\penalty0 81674--81685, 2020.
\newblock \doi{10.1109/ACCESS.2020.2991231}.

\bibitem[Kini et~al.(2021)Kini, Paraskevas, Oymak, and
  Thrampoulidis]{kini2021label}
G.~R. Kini, O.~Paraskevas, S.~Oymak, and C.~Thrampoulidis.
\newblock Label-imbalanced and group-sensitive classification under
  overparameterization.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 18970--18983, 2021.

\bibitem[Li et~al.(2021)Li, Zhang, Thrampoulidis, Chen, and
  Oymak]{li2021autobalance}
M.~Li, X.~Zhang, C.~Thrampoulidis, J.~Chen, and S.~Oymak.
\newblock Autobalance: Optimized loss functions for imbalanced data.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 3163--3177, 2021.

\bibitem[Lu and Steinerberger(2020)]{lu2020neural}
J.~Lu and S.~Steinerberger.
\newblock Neural collapse with cross-entropy loss.
\newblock \emph{arXiv preprint arXiv:2012.08465}, 2020.

\bibitem[Lyu and Li(2019)]{lyu2019gradient}
K.~Lyu and J.~Li.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock \emph{arXiv preprint arXiv:1906.05890}, 2019.

\bibitem[Menon et~al.(2020)Menon, Jayasumana, Rawat, Jain, Veit, and
  Kumar]{Menon}
A.~K. Menon, S.~Jayasumana, A.~S. Rawat, H.~Jain, A.~Veit, and S.~Kumar.
\newblock Long-tail learning via logit adjustment.
\newblock \emph{arXiv preprint arXiv:2007.07314}, 2020.

\bibitem[Mixon et~al.(2020)Mixon, Parshall, and Pi]{mixon2020neural}
D.~G. Mixon, H.~Parshall, and J.~Pi.
\newblock Neural collapse with unconstrained features.
\newblock \emph{arXiv preprint arXiv:2011.11619}, 2020.

\bibitem[Nacson et~al.(2019)Nacson, Lee, Gunasekar, Savarese, Srebro, and
  Soudry]{nacson2019convergence}
M.~S. Nacson, J.~Lee, S.~Gunasekar, P.~H.~P. Savarese, N.~Srebro, and
  D.~Soudry.
\newblock Convergence of gradient descent on separable data.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 3420--3428. PMLR, 2019.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{NC}
V.~Papyan, X.~Han, and D.~L. Donoho.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (40):\penalty0 24652--24663, 2020.

\bibitem[Peng et~al.(2016)Peng, Lu, Yi, and Tang]{peng2016connections}
X.~Peng, C.~Lu, Z.~Yi, and H.~Tang.
\newblock Connections between nuclear-norm and frobenius-norm-based
  representations.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  29\penalty0 (1):\penalty0 218--224, 2016.

\bibitem[Rosset et~al.(2003)Rosset, Zhu, and Hastie]{rosset2003margin}
S.~Rosset, J.~Zhu, and T.~Hastie.
\newblock Margin maximizing loss functions.
\newblock In \emph{NIPS}, pages 1237--1244, 2003.

\bibitem[Simonyan and Zisserman(2014)]{vgg}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
D.~Soudry, E.~Hoffer, M.~S. Nacson, S.~Gunasekar, and N.~Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Srebro et~al.(2004)Srebro, Rennie, and Jaakkola]{srebro2004maximum}
N.~Srebro, J.~Rennie, and T.~Jaakkola.
\newblock Maximum-margin matrix factorization.
\newblock \emph{Advances in neural information processing systems}, 17, 2004.

\bibitem[Tirer and Bruna(2022)]{tirer2022extended}
T.~Tirer and J.~Bruna.
\newblock Extended unconstrained features model for exploring deep neural
  collapse.
\newblock \emph{arXiv preprint arXiv:2202.08087}, 2022.

\bibitem[Vardi et~al.(2021)Vardi, Shamir, and Srebro]{vardi2021margin}
G.~Vardi, O.~Shamir, and N.~Srebro.
\newblock On margin maximization in linear and relu networks.
\newblock \emph{arXiv preprint arXiv:2110.02732}, 2021.

\bibitem[Vidal and Favaro(2014)]{vidal2014low}
R.~Vidal and P.~Favaro.
\newblock Low rank subspace clustering (lrsc).
\newblock \emph{Pattern Recognition Letters}, 43:\penalty0 47--61, 2014.

\bibitem[Wang et~al.(2021)Wang, Muthukumar, and Thrampoulidis]{wang2021benign}
K.~Wang, V.~Muthukumar, and C.~Thrampoulidis.
\newblock Benign overfitting in multiclass classification: All roads lead to
  interpolation.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Wright and Ma(2022)]{wright2022high}
J.~Wright and Y.~Ma.
\newblock \emph{High-dimensional data analysis with low-dimensional models:
  Principles, computation, and applications}.
\newblock Cambridge University Press, 2022.

\bibitem[Xie et~al.(2022)Xie, Yang, Cai, Tao, and He]{xie2022neural}
L.~Xie, Y.~Yang, D.~Cai, D.~Tao, and X.~He.
\newblock Neural collapse inspired attraction-repulsion-balanced loss for
  imbalanced learning.
\newblock \emph{arXiv preprint arXiv:2204.08735}, 2022.

\bibitem[Yang et~al.(2022)Yang, Xie, Chen, Li, Lin, and Tao]{yang2022we}
Y.~Yang, L.~Xie, S.~Chen, X.~Li, Z.~Lin, and D.~Tao.
\newblock Do we really need a learnable classifier at the end of deep neural
  network?
\newblock \emph{arXiv preprint arXiv:2203.09081}, 2022.

\bibitem[Ye et~al.(2020)Ye, Chen, Zhan, and Chao]{CDT}
H.-J. Ye, H.-Y. Chen, D.-C. Zhan, and W.-L. Chao.
\newblock Identifying and compensating for feature deviation in imbalanced deep
  learning, 2020.

\bibitem[Zhou et~al.(2022)Zhou, Li, Ding, You, Qu, and
  Zhu]{zhou2022optimization}
J.~Zhou, X.~Li, T.~Ding, C.~You, Q.~Qu, and Z.~Zhu.
\newblock On the optimization landscape of neural collapse under mse loss:
  Global optimality with unconstrained features.
\newblock \emph{arXiv preprint arXiv:2203.01238}, 2022.

\bibitem[Zhu et~al.(2021)Zhu, Ding, Zhou, Li, You, Sulam, and
  Qu]{zhu2021geometric}
Z.~Zhu, T.~Ding, J.~Zhou, X.~Li, C.~You, J.~Sulam, and Q.~Qu.
\newblock A geometric analysis of neural collapse with unconstrained features.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\end{thebibliography}
