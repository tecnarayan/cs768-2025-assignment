\begin{thebibliography}{10}

\bibitem{sohl2015deep}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock {\em International Conference on Machine Learning}, 2015.

\bibitem{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{song2020score}
Yang Song, Jascha Sohl{-}Dickstein, Diederik~P. Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock {\em International Conference on Learning Representations}, 2021.

\bibitem{dhariwal2021diffusion}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat {GAN}s on image synthesis.
\newblock {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{hoogeboom2021argmax}
Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr{\'e}, and Max
  Welling.
\newblock Argmax flows and multinomial diffusion: Learning categorical
  distributions.
\newblock {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{hoogeboom2021autoregressive}
Emiel Hoogeboom, Alexey~A Gritsenko, Jasmijn Bastings, Ben Poole, Rianne
  van~den Berg, and Tim Salimans.
\newblock Autoregressive diffusion models.
\newblock {\em International Conference on Learning Representations}, 2022.

\bibitem{austin2021structured}
Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van~den
  Berg.
\newblock Structured denoising diffusion models in discrete state-spaces.
\newblock {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{esser2021imagebart}
Patrick Esser, Robin Rombach, Andreas Blattmann, and Bjorn Ommer.
\newblock Imagebart: Bidirectional context with multinomial diffusion for
  autoregressive image synthesis.
\newblock {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{hoogeboom2022equivariant}
Emiel Hoogeboom, Victor~Garcia Satorras, Cl{\'e}ment Vignac, and Max Welling.
\newblock Equivariant diffusion for molecule generation in 3d.
\newblock {\em International Conference on Machine Learning}, 2022.

\bibitem{cohen2022diffusion}
Max Cohen, Guillaume Quispe, Sylvain~Le Corff, Charles Ollion, and Eric
  Moulines.
\newblock Diffusion bridges vector quantized variational autoencoders.
\newblock {\em International Conference on Machine Learning}, 2022.

\bibitem{johnson2021beyond}
Daniel~D Johnson, Jacob Austin, Rianne van~den Berg, and Daniel Tarlow.
\newblock Beyond in-place corruption: Insertion and deletion in denoising
  probabilistic models.
\newblock {\em ICML Workshop on Invertible Neural Networks, Normalizing Flows,
  and Explicit Likelihood Models (INNF+)}, 2021.

\bibitem{gu2021vector}
Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo~Zhang, Dongdong Chen, Lu~Yuan,
  and Baining Guo.
\newblock Vector quantized diffusion model for text-to-image synthesis.
\newblock {\em IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  2022.

\bibitem{jolicoeur2021gotta}
Alexia Jolicoeur-Martineau, Ke~Li, R{\'e}mi Pich{\'e}-Taillefer, Tal Kachman,
  and Ioannis Mitliagkas.
\newblock Gotta go fast when generating data with score-based models.
\newblock {\em arXiv preprint arXiv:2105.14080}, 2021.

\bibitem{zhang2022fast}
Qinsheng Zhang and Yongxin Chen.
\newblock Fast sampling of diffusion models with exponential integrator.
\newblock {\em arXiv preprint arXiv:2204.13902}, 2022.

\bibitem{salimans2022progressive}
Tim Salimans and Jonathan Ho.
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock {\em International Conference on Learning Representations}, 2022.

\bibitem{chung2021come}
Hyungjin Chung, Byeongsu Sim, and Jong~Chul Ye.
\newblock Come-closer-diffuse-faster: Accelerating conditional diffusion models
  for inverse problems through stochastic contraction.
\newblock {\em IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  2022.

\bibitem{dockhorn2021score}
Tim Dockhorn, Arash Vahdat, and Karsten Kreis.
\newblock Score-based generative modeling with critically-damped {L}angevin
  diffusion.
\newblock {\em International Conference on Learning Representations}, 2022.

\bibitem{de2021diffusion}
Valentin De~Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet.
\newblock Diffusion schr{\"o}dinger bridge with applications to score-based
  generative modeling.
\newblock {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{gillespie2001approximate}
Daniel~T Gillespie.
\newblock Approximate accelerated stochastic simulation of chemically reacting
  systems.
\newblock {\em The Journal of Chemical Physics}, 115(4):1716--1733, 2001.

\bibitem{gillespie1976general}
Daniel~T Gillespie.
\newblock A general method for numerically simulating the stochastic time
  evolution of coupled chemical reactions.
\newblock {\em Journal of Computational Physics}, 22(4):403--434, 1976.

\bibitem{gillespie1977exact}
Daniel~T Gillespie.
\newblock Exact stochastic simulation of coupled chemical reactions.
\newblock {\em The Journal of Physical Chemistry}, 81(25):2340--2361, 1977.

\bibitem{wilkinson2018stochastic}
Darren~J Wilkinson.
\newblock {\em Stochastic Modelling for Systems Biology}.
\newblock Chapman and Hall/CRC, 2018.

\bibitem{levin2009markovchains}
David Levin, Yuval Peres, and Elizabeth Wilmer.
\newblock {\em Markov Chains and Mixing Times}.
\newblock American Mathematical Society, 2009.

\bibitem{song2020denoising}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock {\em International Conference on Learning Representations}, 2021.

\bibitem{savinov2021step}
Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and Aaron
  van~den Oord.
\newblock Step-unrolled denoising autoencoders for text generation.
\newblock {\em International Conference on Learning Representations}, 2022.

\bibitem{goyal2017variational}
Anirudh Goyal, Nan~Rosemary Ke, Surya Ganguli, and Yoshua Bengio.
\newblock Variational walkback: Learning a transition operator as a stochastic
  recurrent net.
\newblock {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem{chong2020effectively}
Min~Jin Chong and David Forsyth.
\newblock Effectively unbiased fid and inception score and where to find them.
\newblock {\em IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  2020.

\bibitem{raffel2016learning}
Colin Raffel.
\newblock {\em Learning-based methods for comparing sequences, with
  applications to audio-to-midi alignment and matching}.
\newblock PhD thesis, Columbia University, 2016.

\bibitem{dong2018musegan}
Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan Yang.
\newblock Musegan: Multi-track sequential generative adversarial networks for
  symbolic music generation and accompaniment.
\newblock {\em AAAI Conference on Artificial Intelligence}, 2018.

\bibitem{mittal2021symbolic}
Gautam Mittal, Jesse Engel, Curtis Hawthorne, and Ian Simon.
\newblock Symbolic music generation with diffusion models.
\newblock {\em International Society for Music Information Retrieval}, 2021.

\bibitem{bao2022analytic}
Fan Bao, Chongxuan Li, Jun Zhu, and Bo~Zhang.
\newblock Analytic-dpm: an analytic estimate of the optimal reverse variance in
  diffusion probabilistic models.
\newblock {\em International Conference on Learning Representations}, 2022.

\bibitem{anderson2007modified}
David~F Anderson.
\newblock A modified next reaction method for simulating chemical systems with
  time dependent propensities and delays.
\newblock {\em The Journal of Chemical Physics}, 127(21):214107, 2007.

\bibitem{furusawa2021generative}
Chie Furusawa, Shinya Kitaoka, Michael Li, and Yuri Odagiri.
\newblock Generative probabilistic image colorization.
\newblock {\em arXiv preprint arXiv:2109.14518}, 2021.

\bibitem{perez2018film}
Ethan Perez, Florian Strub, Harm De~Vries, Vincent Dumoulin, and Aaron
  Courville.
\newblock Film: Visual reasoning with a general conditioning layer.
\newblock {\em AAAI Conference on Artificial Intelligence}, 2018.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem{ronneberger2015u}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock {\em International Conference on Medical Image Computing and
  Computer-assisted Intervention}, 2015.

\bibitem{salimans2017pixelcnn++}
Tim Salimans, Andrej Karpathy, Xi~Chen, and Diederik~P Kingma.
\newblock Pixelcnn++: Improving the pixelcnn with discretized logistic mixture
  likelihood and other modifications.
\newblock {\em International Conference on Learning Representations}, 2017.

\bibitem{chen2018pixelsnail}
Xi~Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel.
\newblock Pixelsnail: An improved autoregressive generative model.
\newblock {\em International Conference on Machine Learning}, 2018.

\bibitem{elfwing2018sigmoid}
Stefan Elfwing, Eiji Uchibe, and Kenji Doya.
\newblock Sigmoid-weighted linear units for neural network function
  approximation in reinforcement learning.
\newblock {\em Neural Networks}, 107:3--11, 2018.

\bibitem{Seitzer2020FID}
Maximilian Seitzer.
\newblock {pytorch-fid: FID Score for PyTorch}.
\newblock \url{https://github.com/mseitzer/pytorch-fid}, August 2020.
\newblock Version 0.2.1.

\end{thebibliography}
