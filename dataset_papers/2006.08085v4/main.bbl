\begin{thebibliography}{88}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
Mart{\'\i}n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
  Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{12th $\{$USENIX$\}$ Symposium on Operating Systems Design
  and Implementation ($\{$OSDI$\}$ 16)}, pages 265--283, 2016.

\bibitem[Alistarh(2018)]{alistarh2018brief}
Dan Alistarh.
\newblock A brief tutorial on distributed and concurrent machine learning.
\newblock In \emph{Proceedings of the 2018 ACM Symposium on Principles of
  Distributed Computing}, pages 487--488, 2018.

\bibitem[Alistarh et~al.(2020)Alistarh, Chatterjee, and
  Kungurtsev]{alistarh2020elastic}
Dan Alistarh, Bapi Chatterjee, and Vyacheslav Kungurtsev.
\newblock Elastic consistency: A general consistency model for distributed
  stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:2001.05918}, 2020.

\bibitem[Lu et~al.(2020)Lu, Nash, and De~Sa]{lu2020mixml}
Yucheng Lu, Jack Nash, and Christopher De~Sa.
\newblock Mixml: A unified analysis of weakly consistent parallel learning.
\newblock \emph{arXiv preprint arXiv:2005.06706}, 2020.

\bibitem[Li et~al.(2014{\natexlab{a}})Li, Andersen, Park, Smola, Ahmed,
  Josifovski, Long, Shekita, and Su]{li2014scaling}
Mu~Li, David~G Andersen, Jun~Woo Park, Alexander~J Smola, Amr Ahmed, Vanja
  Josifovski, James Long, Eugene~J Shekita, and Bor-Yiing Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In \emph{11th $\{$USENIX$\}$ Symposium on Operating Systems Design
  and Implementation ($\{$OSDI$\}$ 14)}, pages 583--598, 2014{\natexlab{a}}.

\bibitem[Li et~al.(2014{\natexlab{b}})Li, Andersen, Smola, and
  Yu]{li2014communication}
Mu~Li, David~G Andersen, Alexander~J Smola, and Kai Yu.
\newblock Communication efficient distributed machine learning with the
  parameter server.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  19--27, 2014{\natexlab{b}}.

\bibitem[Ho et~al.(2013)Ho, Cipar, Cui, Lee, Kim, Gibbons, Gibson, Ganger, and
  Xing]{ho2013more}
Qirong Ho, James Cipar, Henggang Cui, Seunghak Lee, Jin~Kyu Kim, Phillip~B
  Gibbons, Garth~A Gibson, Greg Ganger, and Eric~P Xing.
\newblock More effective distributed ml via a stale synchronous parallel
  parameter server.
\newblock In \emph{Advances in neural information processing systems}, pages
  1223--1231, 2013.

\bibitem[Gropp et~al.(1999)Gropp, Thakur, and Lusk]{gropp1999using}
William Gropp, Rajeev Thakur, and Ewing Lusk.
\newblock \emph{Using MPI-2: Advanced features of the message passing
  interface}.
\newblock MIT press, 1999.

\bibitem[Patarasuk and Yuan(2009)]{patarasuk2009bandwidth}
Pitch Patarasuk and Xin Yuan.
\newblock Bandwidth optimal all-reduce algorithms for clusters of workstations.
\newblock \emph{Journal of Parallel and Distributed Computing}, 69\penalty0
  (2):\penalty0 117--124, 2009.

\bibitem[Koloskova et~al.(2019{\natexlab{a}})Koloskova, Lin, Stich, and
  Jaggi]{koloskova2019decentralized2}
Anastasia Koloskova, Tao Lin, Sebastian~U Stich, and Martin Jaggi.
\newblock Decentralized deep learning with arbitrary communication compression.
\newblock \emph{arXiv preprint arXiv:1907.09356}, 2019{\natexlab{a}}.

\bibitem[McMahan et~al.(2016)McMahan, Moore, Ramage, Hampson,
  et~al.]{mcmahan2016communication}
H~Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et~al.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock \emph{arXiv preprint arXiv:1602.05629}, 2016.

\bibitem[Kanawaday and Sane(2017)]{kanawaday2017machine}
Ameeth Kanawaday and Aditya Sane.
\newblock Machine learning for predictive maintenance of industrial machines
  using iot sensor data.
\newblock In \emph{2017 8th IEEE International Conference on Software
  Engineering and Service Science (ICSESS)}, pages 87--90. IEEE, 2017.

\bibitem[Lian et~al.(2017{\natexlab{a}})Lian, Zhang, Zhang, and
  Liu]{lian2017asynchronous}
Xiangru Lian, Wei Zhang, Ce~Zhang, and Ji~Liu.
\newblock Asynchronous decentralized parallel stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:1710.06952}, 2017{\natexlab{a}}.

\bibitem[Tang et~al.(2019{\natexlab{a}})Tang, Lian, Yu, Zhang, and
  Liu]{tang2019doublesqueeze}
Hanlin Tang, Xiangru Lian, Chen Yu, Tong Zhang, and Ji~Liu.
\newblock Doublesqueeze: Parallel stochastic gradient descent with double-pass
  error-compensated compression.
\newblock \emph{arXiv preprint arXiv:1905.05957}, 2019{\natexlab{a}}.

\bibitem[Yu et~al.(2018)Yu, Tang, Renggli, Kassing, Singla, Alistarh, Zhang,
  and Liu]{yu2018distributed}
Chen Yu, Hanlin Tang, Cedric Renggli, Simon Kassing, Ankit Singla, Dan
  Alistarh, Ce~Zhang, and Ji~Liu.
\newblock Distributed learning over unreliable networks.
\newblock \emph{arXiv preprint arXiv:1810.07766}, 2018.

\bibitem[Li et~al.(2019)Li, Huang, Yang, Wang, and Zhang]{li2019convergence}
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.
\newblock On the convergence of fedavg on non-iid data.
\newblock \emph{arXiv preprint arXiv:1907.02189}, 2019.

\bibitem[Seaman et~al.(2017)Seaman, Bach, Bubeck, Lee, and
  Massouli{\'e}]{seaman2017optimal}
Kevin Seaman, Francis Bach, S{\'e}bastien Bubeck, Yin~Tat Lee, and Laurent
  Massouli{\'e}.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 3027--3036. JMLR. org, 2017.

\bibitem[Shanthamallu et~al.(2017)Shanthamallu, Spanias, Tepedelenlioglu, and
  Stanley]{shanthamallu2017brief}
Uday~Shankar Shanthamallu, Andreas Spanias, Cihan Tepedelenlioglu, and Mike
  Stanley.
\newblock A brief survey of machine learning methods and their sensor and iot
  applications.
\newblock In \emph{2017 8th International Conference on Information,
  Intelligence, Systems \& Applications (IISA)}, pages 1--8. IEEE, 2017.

\bibitem[Lian et~al.(2017{\natexlab{b}})Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
Xiangru Lian, Ce~Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5330--5340, 2017{\natexlab{b}}.

\bibitem[Yu et~al.(2019)Yu, Jin, and Yang]{yu2019linear}
Hao Yu, Rong Jin, and Sen Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  sgd for distributed non-convex optimization.
\newblock \emph{arXiv preprint arXiv:1905.03817}, 2019.

\bibitem[Nazari et~al.(2019)Nazari, Tarzanagh, and
  Michailidis]{nazari2019dadam}
Parvin Nazari, Davoud~Ataee Tarzanagh, and George Michailidis.
\newblock Dadam: A consensus-based distributed adaptive gradient method for
  online optimization.
\newblock \emph{arXiv preprint arXiv:1901.09109}, 2019.

\bibitem[Lu and De~Sa(2020)]{lu2020moniqua}
Yucheng Lu and Christopher De~Sa.
\newblock Moniqua: Modulo quantized communication in decentralized sgd.
\newblock \emph{arXiv preprint arXiv:2002.11787}, 2020.

\bibitem[Wan et~al.(2020)Wan, Zhang, Wang, Hu, Zhang, and Chen]{wan2020rat}
Xinchen Wan, Hong Zhang, Hao Wang, Shuihai Hu, Junxue Zhang, and Kai Chen.
\newblock Rat-resilient allreduce tree for distributed machine learning.
\newblock In \emph{4th Asia-Pacific Workshop on Networking}, pages 52--57,
  2020.

\bibitem[Tang et~al.(2018{\natexlab{a}})Tang, Lian, Yan, Zhang, and
  Liu]{tang2018d}
Hanlin Tang, Xiangru Lian, Ming Yan, Ce~Zhang, and Ji~Liu.
\newblock D2: Decentralized training over decentralized data.
\newblock \emph{arXiv preprint arXiv:1803.07068}, 2018{\natexlab{a}}.

\bibitem[Wang et~al.(2019)Wang, Sahu, Yang, Joshi, and Kar]{wang2019matcha}
Jianyu Wang, Anit~Kumar Sahu, Zhouyi Yang, Gauri Joshi, and Soummya Kar.
\newblock Matcha: Speeding up decentralized sgd via matching decomposition
  sampling.
\newblock \emph{arXiv preprint arXiv:1905.09435}, 2019.

\bibitem[Scaman et~al.(2018)Scaman, Bach, Bubeck, Massouli{\'e}, and
  Lee]{scaman2018optimal}
Kevin Scaman, Francis Bach, S{\'e}bastien Bubeck, Laurent Massouli{\'e}, and
  Yin~Tat Lee.
\newblock Optimal algorithms for non-smooth distributed optimization in
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2740--2749, 2018.

\bibitem[Koloskova et~al.(2020)Koloskova, Loizou, Boreiri, Jaggi, and
  Stich]{koloskova2020unified}
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and
  Sebastian~U Stich.
\newblock A unified theory of decentralized sgd with changing topology and
  local updates.
\newblock \emph{arXiv preprint arXiv:2003.10422}, 2020.

\bibitem[Woodworth et~al.(2018)Woodworth, Wang, Smith, McMahan, and
  Srebro]{woodworth2018graph}
Blake~E Woodworth, Jialei Wang, Adam Smith, Brendan McMahan, and Nati Srebro.
\newblock Graph oracle models, lower bounds, and gaps for parallel stochastic
  optimization.
\newblock In \emph{Advances in neural information processing systems}, pages
  8496--8506, 2018.

\bibitem[Dvinskikh and Gasnikov(2019)]{dvinskikh2019decentralized}
Darina Dvinskikh and Alexander Gasnikov.
\newblock Decentralized and parallelized primal and dual accelerated methods
  for stochastic convex programming problems.
\newblock \emph{arXiv preprint arXiv:1904.09015}, 2019.

\bibitem[Sun and Hong(2019)]{sun2019distributed}
Haoran Sun and Mingyi Hong.
\newblock Distributed non-convex first-order optimization and information
  processing: Lower complexity bounds and rate optimal algorithms.
\newblock \emph{IEEE Transactions on Signal processing}, 67\penalty0
  (22):\penalty0 5912--5928, 2019.

\bibitem[Zhao et~al.(2018)Zhao, Li, Lai, Suda, Civin, and
  Chandra]{zhao2018federated}
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra.
\newblock Federated learning with non-iid data.
\newblock \emph{arXiv preprint arXiv:1806.00582}, 2018.

\bibitem[Bonawitz et~al.(2019)Bonawitz, Eichner, Grieskamp, Huba, Ingerman,
  Ivanov, Kiddon, Kone{\v{c}}n{\`y}, Mazzocchi, McMahan,
  et~al.]{bonawitz2019towards}
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
  Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Kone{\v{c}}n{\`y}, Stefano
  Mazzocchi, H~Brendan McMahan, et~al.
\newblock Towards federated learning at scale: System design.
\newblock \emph{arXiv preprint arXiv:1902.01046}, 2019.

\bibitem[Tran et~al.(2019)Tran, Bao, Zomaya, Nguyen, and
  Hong]{tran2019federated}
Nguyen~H Tran, Wei Bao, Albert Zomaya, Minh~NH Nguyen, and Choong~Seon Hong.
\newblock Federated learning over wireless networks: Optimization model design
  and analysis.
\newblock In \emph{IEEE INFOCOM 2019-IEEE Conference on Computer
  Communications}, pages 1387--1395. IEEE, 2019.

\bibitem[Yang et~al.(2019)Yang, Liu, Cheng, Kang, Chen, and
  Yu]{yang2019federated}
Qiang Yang, Yang Liu, Yong Cheng, Yan Kang, Tianjian Chen, and Han Yu.
\newblock Federated learning.
\newblock \emph{Synthesis Lectures on Artificial Intelligence and Machine
  Learning}, 13\penalty0 (3):\penalty0 1--207, 2019.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, McMahan, Yu,
  Richt{\'a}rik, Suresh, and Bacon]{konevcny2016federated}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Felix~X Yu, Peter Richt{\'a}rik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock \emph{arXiv preprint arXiv:1610.05492}, 2016.

\bibitem[Boyd et~al.(2005)Boyd, Ghosh, Prabhakar, and Shah]{boyd2005gossip}
Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah.
\newblock Gossip algorithms: Design, analysis and applications.
\newblock In \emph{Proceedings IEEE 24th Annual Joint Conference of the IEEE
  Computer and Communications Societies.}, volume~3, pages 1653--1664. IEEE,
  2005.

\bibitem[Boyd et~al.(2006)Boyd, Ghosh, Prabhakar, and Shah]{boyd2006randomized}
Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah.
\newblock Randomized gossip algorithms.
\newblock \emph{IEEE transactions on information theory}, 52\penalty0
  (6):\penalty0 2508--2530, 2006.

\bibitem[Tian et~al.(2020)Tian, Sun, and Scutari]{tian2020achieving}
Ye~Tian, Ying Sun, and Gesualdo Scutari.
\newblock Achieving linear convergence in distributed asynchronous multiagent
  optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 65\penalty0
  (12):\penalty0 5264--5279, 2020.

\bibitem[Zhang and You(2019{\natexlab{a}})]{zhang2019asyspa}
Jiaqi Zhang and Keyou You.
\newblock Asyspa: An exact asynchronous algorithm for convex optimization over
  digraphs.
\newblock \emph{IEEE Transactions on Automatic Control}, 65\penalty0
  (6):\penalty0 2494--2509, 2019{\natexlab{a}}.

\bibitem[Hendrikx et~al.(2019)Hendrikx, Bach, and
  Massouli{\'e}]{hendrikx2019asynchronous}
Hadrien Hendrikx, Francis Bach, and Laurent Massouli{\'e}.
\newblock Asynchronous accelerated proximal stochastic gradient for strongly
  convex distributed finite sums.
\newblock \emph{arXiv preprint arXiv:1901.09865}, 2019.

\bibitem[Xin et~al.(2021{\natexlab{a}})Xin, Khan, and Kar]{xin2021hybrid}
Ran Xin, Usman~A Khan, and Soummya Kar.
\newblock A hybrid variance-reduced method for decentralized stochastic
  non-convex optimization.
\newblock \emph{arXiv preprint arXiv:2102.06752}, 2021{\natexlab{a}}.

\bibitem[Zhang and You(2019{\natexlab{b}})]{zhang2019decentralized}
Jiaqi Zhang and Keyou You.
\newblock Decentralized stochastic gradient tracking for empirical risk
  minimization.
\newblock \emph{arXiv preprint arXiv:1909.02712}, 2019{\natexlab{b}}.

\bibitem[Xin et~al.(2019)Xin, Khan, and Kar]{xin2019variance}
Ran Xin, Usman~A Khan, and Soummya Kar.
\newblock Variance-reduced decentralized stochastic optimization with gradient
  tracking.
\newblock \emph{arXiv preprint arXiv:1909.11774}, 2019.

\bibitem[Xin et~al.(2021{\natexlab{b}})Xin, Khan, and Kar]{xin2021improved}
Ran Xin, Usman~A Khan, and Soummya Kar.
\newblock An improved convergence analysis for decentralized online stochastic
  non-convex optimization.
\newblock \emph{IEEE Transactions on Signal Processing}, 69:\penalty0
  1842--1858, 2021{\natexlab{b}}.

\bibitem[He et~al.(2018)He, Bian, and Jaggi]{he2018cola}
Lie He, An~Bian, and Martin Jaggi.
\newblock Cola: Decentralized linear learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4536--4546, 2018.

\bibitem[Assran et~al.(2018)Assran, Loizou, Ballas, and
  Rabbat]{assran2018stochastic}
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Michael Rabbat.
\newblock Stochastic gradient push for distributed deep learning.
\newblock \emph{arXiv preprint arXiv:1811.10792}, 2018.

\bibitem[Zhang and You(2019{\natexlab{c}})]{zhang2019asynchronous}
Jiaqi Zhang and Keyou You.
\newblock Asynchronous decentralized optimization in directed networks.
\newblock \emph{arXiv preprint arXiv:1901.08215}, 2019{\natexlab{c}}.

\bibitem[Koloskova et~al.(2019{\natexlab{b}})Koloskova, Stich, and
  Jaggi]{koloskova2019decentralized}
Anastasia Koloskova, Sebastian~U Stich, and Martin Jaggi.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock \emph{arXiv preprint arXiv:1902.00340}, 2019{\natexlab{b}}.

\bibitem[Tang et~al.(2019{\natexlab{b}})Tang, Lian, Qiu, Yuan, Zhang, Zhang,
  and Liu]{tang2019texttt}
Hanlin Tang, Xiangru Lian, Shuang Qiu, Lei Yuan, Ce~Zhang, Tong Zhang, and
  Ji~Liu.
\newblock Deepsqueeze: Parallel stochastic gradient descent with double-pass
  error-compensated compression.
\newblock \emph{arXiv preprint arXiv:1907.07346}, 2019{\natexlab{b}}.

\bibitem[Tang et~al.(2018{\natexlab{b}})Tang, Gan, Zhang, Zhang, and
  Liu]{tang2018communication}
Hanlin Tang, Shaoduo Gan, Ce~Zhang, Tong Zhang, and Ji~Liu.
\newblock Communication compression for decentralized training.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7652--7662, 2018{\natexlab{b}}.

\bibitem[Wang and Joshi(2018)]{wang2018cooperative}
Jianyu Wang and Gauri Joshi.
\newblock Cooperative sgd: A unified framework for the design and analysis of
  communication-efficient sgd algorithms.
\newblock \emph{arXiv preprint arXiv:1808.07576}, 2018.

\bibitem[Nedic and Ozdaglar(2009)]{nedic2009distributed}
Angelia Nedic and Asuman Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 54\penalty0
  (1):\penalty0 48--61, 2009.

\bibitem[Duchi et~al.(2010)Duchi, Agarwal, and
  Wainwright]{duchi2010distributed}
John~C Duchi, Alekh Agarwal, and Martin~J Wainwright.
\newblock Distributed dual averaging in networks.
\newblock In \emph{NIPS}, pages 550--558. Citeseer, 2010.

\bibitem[Agarwal and Bottou(2014)]{agarwal2014lower}
Alekh Agarwal and Leon Bottou.
\newblock A lower bound for the optimization of finite sums.
\newblock \emph{arXiv preprint arXiv:1410.0723}, 2014.

\bibitem[Arjevani and Shamir(2015)]{arjevani2015communication}
Yossi Arjevani and Ohad Shamir.
\newblock Communication complexity of distributed convex learning and
  optimization.
\newblock In \emph{Advances in neural information processing systems}, pages
  1756--1764, 2015.

\bibitem[Lan and Zhou(2018)]{lan2018optimal}
Guanghui Lan and Yi~Zhou.
\newblock An optimal randomized incremental gradient method.
\newblock \emph{Mathematical programming}, 171\penalty0 (1-2):\penalty0
  167--215, 2018.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock Spider: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  689--699, 2018.

\bibitem[Arjevani and Shamir(2017)]{arjevani2017oracle}
Yossi Arjevani and Ohad Shamir.
\newblock Oracle complexity of second-order methods for finite-sum problems.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 205--213. JMLR. org, 2017.

\bibitem[Allen-Zhu(2018)]{allen2018make}
Zeyuan Allen-Zhu.
\newblock How to make the gradients small stochastically: Even faster convex
  and nonconvex sgd.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1157--1167, 2018.

\bibitem[Foster et~al.(2019)Foster, Sekhari, Shamir, Srebro, Sridharan, and
  Woodworth]{foster2019complexity}
Dylan Foster, Ayush Sekhari, Ohad Shamir, Nathan Srebro, Karthik Sridharan, and
  Blake Woodworth.
\newblock The complexity of making the gradient small in stochastic convex
  optimization.
\newblock \emph{arXiv preprint arXiv:1902.04686}, 2019.

\bibitem[Diakonikolas and Guzm{\'a}n(2018)]{diakonikolas2018lower}
Jelena Diakonikolas and Crist{\'o}bal Guzm{\'a}n.
\newblock Lower bounds for parallel and randomized convex optimization.
\newblock \emph{arXiv preprint arXiv:1811.01903}, 2018.

\bibitem[Balkanski and Singer(2018)]{balkanski2018parallelization}
Eric Balkanski and Yaron Singer.
\newblock Parallelization does not accelerate convex optimization: Adaptivity
  lower bounds for non-smooth convex minimization.
\newblock \emph{arXiv preprint arXiv:1808.03880}, 2018.

\bibitem[Tran-Dinh et~al.(2019)Tran-Dinh, Alacaoglu, Fercoq, and
  Cevher]{tran2019adaptive}
Quoc Tran-Dinh, Ahmet Alacaoglu, Olivier Fercoq, and Volkan Cevher.
\newblock An adaptive primal-dual framework for nonsmooth convex minimization.
\newblock \emph{Mathematical Programming Computation}, pages 1--41, 2019.

\bibitem[Colin et~al.(2019)Colin, Dos~Santos, and Scaman]{colin2019theoretical}
Igor Colin, Ludovic Dos~Santos, and Kevin Scaman.
\newblock Theoretical limits of pipeline parallel optimization and application
  to distributed deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  12350--12359, 2019.

\bibitem[Carmon et~al.(2017)Carmon, Duchi, Hinder, and
  Sidford]{carmon2017lower}
Yair Carmon, John~C Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Lower bounds for finding stationary points ii: First-order methods.
\newblock \emph{arXiv preprint arXiv:1711.00841}, 2017.

\bibitem[Carmon et~al.(2019)Carmon, Duchi, Hinder, and
  Sidford]{carmon2019lower}
Yair Carmon, John~C Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Lower bounds for finding stationary points i.
\newblock \emph{Mathematical Programming}, pages 1--50, 2019.

\bibitem[Zhou and Gu(2019)]{zhou2019lower}
Dongruo Zhou and Quanquan Gu.
\newblock Lower bounds for smooth nonconvex finite-sum optimization.
\newblock \emph{arXiv preprint arXiv:1901.11224}, 2019.

\bibitem[Arjevani et~al.(2019)Arjevani, Carmon, Duchi, Foster, Srebro, and
  Woodworth]{arjevani2019lower}
Yossi Arjevani, Yair Carmon, John~C Duchi, Dylan~J Foster, Nathan Srebro, and
  Blake Woodworth.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1912.02365}, 2019.

\bibitem[Shi et~al.(2015)Shi, Ling, Wu, and Yin]{shi2015extra}
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin.
\newblock Extra: An exact first-order algorithm for decentralized consensus
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0
  944--966, 2015.

\bibitem[Scaman et~al.(2017)Scaman, Bach, Bubeck, Lee, and
  Massouli{\'e}]{scaman2017optimal}
Kevin Scaman, Francis Bach, S{\'e}bastien Bubeck, Yin~Tat Lee, and Laurent
  Massouli{\'e}.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock In \emph{international conference on machine learning}, pages
  3027--3036. PMLR, 2017.

\bibitem[Liu and Zhang(2021)]{liu2021distributed}
Ji~Liu and Ce~Zhang.
\newblock Distributed learning systems with first-order methods.
\newblock \emph{arXiv preprint arXiv:2104.05245}, 2021.

\bibitem[Assran et~al.(2019)Assran, Loizou, Ballas, and
  Rabbat]{assran2019stochastic}
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Mike Rabbat.
\newblock Stochastic gradient push for distributed deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  344--353. PMLR, 2019.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Nesterov(1983)]{nesterov1983method}
Yurii Nesterov.
\newblock A method for unconstrained convex minimization problem with the rate
  of convergence o (1/k\^{} 2).
\newblock In \emph{Doklady an ussr}, volume 269, pages 543--547, 1983.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Tieleman and Hinton(2012)]{tieleman2012lecture}
Tijmen Tieleman and Geoffrey Hinton.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, 4\penalty0
  (2):\penalty0 26--31, 2012.

\bibitem[Ward et~al.(2018)Ward, Wu, and Bottou]{ward2018adagrad}
Rachel Ward, Xiaoxia Wu, and Leon Bottou.
\newblock Adagrad stepsizes: Sharp convergence over nonconvex landscapes, from
  any initialization.
\newblock \emph{arXiv preprint arXiv:1806.01811}, 2018.

\bibitem[Zeiler(2012)]{zeiler2012adadelta}
Matthew~D Zeiler.
\newblock Adadelta: an adaptive learning rate method.
\newblock \emph{arXiv preprint arXiv:1212.5701}, 2012.

\bibitem[Brooks et~al.(2011)Brooks, Gelman, Jones, and
  Meng]{brooks2011handbook}
Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng.
\newblock \emph{Handbook of markov chain monte carlo}.
\newblock CRC press, 2011.

\bibitem[Gerencs{\'e}r(2011)]{gerencser2011markov}
Bal{\'a}zs Gerencs{\'e}r.
\newblock Markov chain mixing time on cycles.
\newblock \emph{Stochastic processes and their applications}, 121\penalty0
  (11):\penalty0 2553--2570, 2011.

\bibitem[Levin and Peres(2017)]{levin2017markov}
David~A Levin and Yuval Peres.
\newblock \emph{Markov chains and mixing times}, volume 107.
\newblock American Mathematical Soc., 2017.

\bibitem[Georgopoulos(2011)]{georgopoulos2011definitive}
Leonidas Georgopoulos.
\newblock Definitive consensus for distributed data inference.
\newblock Technical report, EPFL, 2011.

\bibitem[Ko(2010)]{ko2010matrix}
Chih-Kai Ko.
\newblock \emph{On matrix factorization and scheduling for finite-time
  average-consensus}.
\newblock PhD thesis, California Institute of Technology, 2010.

\bibitem[Hendrickx et~al.(2014)Hendrickx, Jungers, Olshevsky, and
  Vankeerberghen]{hendrickx2014graph}
Julien~M Hendrickx, Rapha{\"e}l~M Jungers, Alexander Olshevsky, and Guillaume
  Vankeerberghen.
\newblock Graph diameter, eigenvalues, and minimum-time consensus.
\newblock \emph{Automatica}, 50\penalty0 (2):\penalty0 635--640, 2014.

\bibitem[Lin et~al.(2018)Lin, Stich, Patel, and Jaggi]{lin2018don}
Tao Lin, Sebastian~U Stich, Kumar~Kshitij Patel, and Martin Jaggi.
\newblock Don't use large mini-batches, use local sgd.
\newblock \emph{arXiv preprint arXiv:1808.07217}, 2018.

\bibitem[Berthier et~al.(2020)Berthier, Bach, and
  Gaillard]{berthier2020accelerated}
Rapha{\"e}l Berthier, Francis Bach, and Pierre Gaillard.
\newblock Accelerated gossip in networks of given dimension using jacobi
  polynomial iterations.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 2\penalty0
  (1):\penalty0 24--47, 2020.

\bibitem[Liu and Morse(2011)]{liu2011accelerated}
Ji~Liu and A~Stephen Morse.
\newblock Accelerated linear iterations for distributed averaging.
\newblock \emph{Annual Reviews in Control}, 35\penalty0 (2):\penalty0 160--165,
  2011.

\bibitem[Ye et~al.(2020)Ye, Luo, Zhou, and Zhang]{ye2020multi}
Haishan Ye, Luo Luo, Ziang Zhou, and Tong Zhang.
\newblock Multi-consensus decentralized accelerated gradient descent.
\newblock \emph{arXiv preprint arXiv:2005.00797}, 2020.

\end{thebibliography}
