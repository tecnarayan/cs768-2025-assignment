\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akimoto and Hansen(2016)]{akimoto2016projection}
Youhei Akimoto and Nikolaus Hansen.
\newblock Projection-based restricted covariance matrix adaptation for high dimension.
\newblock In \emph{Proc. GECCO}, pages 197--204, 2016.

\bibitem[Aljundi et~al.(2018)Aljundi, Babiloni, Elhoseiny, Rohrbach, and Tuytelaars]{aljundi2018memory}
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.
\newblock Memory aware synapses: Learning what (not) to forget.
\newblock In \emph{Proc. ECCV}, pages 139--154, 2018.

\bibitem[Bourtoule et~al.(2021)Bourtoule, Chandrasekaran, Choquette-Choo, Jia, Travers, Zhang, Lie, and Papernot]{bourtoule2021machine}
Lucas Bourtoule, Varun Chandrasekaran, Christopher~A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot.
\newblock Machine unlearning.
\newblock In \emph{Proc. IEEE Symp. Security and Privacy}, 2021.

\bibitem[Cao and Yang(2015)]{cao2015towards}
Yinzhi Cao and Junfeng Yang.
\newblock Towards making systems forget with machine unlearning.
\newblock In \emph{Proc. IEEE Symp. Security and Privacy}, 2015.

\bibitem[Chen et~al.(2019)Chen, Xiong, Xu, and Zuo]{chen2019novel}
Yuantao Chen, Jie Xiong, Weihong Xu, and Jingwen Zuo.
\newblock A novel online incremental and decremental learning algorithm based on variable support vector machine.
\newblock \emph{Cluster Computing}, 22:\penalty0 7435--7445, 2019.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{Proc. CVPR}, pages 248--255, 2009.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{Proc. NAACL-HLT}, 2019.

\bibitem[Diao et~al.(2023)Diao, Huang, Xu, Li, Lin, Zhou, and Zhang]{diao2023black}
Shizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li, Yong Lin, Xiao Zhou, and Tong Zhang.
\newblock Black-box prompt learning for pre-trained language models.
\newblock \emph{TMLR}, 2023.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiy2020vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{ICLR}, 2021.

\bibitem[Golatkar et~al.(2020{\natexlab{a}})Golatkar, Achille, and Soatto]{golatkar2020eternal}
Aditya Golatkar, Alessandro Achille, and Stefano Soatto.
\newblock Eternal sunshine of the spotless net: Selective forgetting in deep networks.
\newblock In \emph{Proc. CVPR}, pages 9304--9312, 2020{\natexlab{a}}.

\bibitem[Golatkar et~al.(2020{\natexlab{b}})Golatkar, Achille, and Soatto]{golatkar2020forgetting}
Aditya Golatkar, Alessandro Achille, and Stefano Soatto.
\newblock Forgetting outside the box: Scrubbing deep networks of information accessible from input-output observations.
\newblock In \emph{Proc. ECCV}, pages 383--398, 2020{\natexlab{b}}.

\bibitem[Golatkar et~al.(2021)Golatkar, Achille, Ravichandran, Polito, and Soatto]{golatkar2021mixed}
Aditya Golatkar, Alessandro Achille, Avinash Ravichandran, Marzia Polito, and Stefano Soatto.
\newblock Mixed-privacy forgetting in deep networks.
\newblock In \emph{Proc. CVPR}, pages 792--801, 2021.

\bibitem[Graves et~al.(2021)Graves, Nagisetty, and Ganesh]{graves2021amnesiac}
Laura Graves, Vineel Nagisetty, and Vijay Ganesh.
\newblock Amnesiac machine learning.
\newblock In \emph{Proc. AAAI}, pages 11516--11524, 2021.

\bibitem[Guo et~al.(2020)Guo, Goldstein, Hannun, and Van Der~Maaten]{guo2020certified}
Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der~Maaten.
\newblock Certified data removal from machine learning models.
\newblock In \emph{Proc. ICML}, 2020.

\bibitem[Guo et~al.(2023)Guo, Wei, Liu, Ji, Bai, Guo, and Zuo]{guo2023blackbox}
Zixian Guo, Yuxiang Wei, Ming Liu, Zhilong Ji, Jinfeng Bai, Yiwen Guo, and Wangmeng Zuo.
\newblock Black-box tuning of vision-language models with effective gradient approximation.
\newblock In \emph{Proc. EMNLP}, 2023.

\bibitem[Hansen(2016)]{hansen2016cma}
Nikolaus Hansen.
\newblock The cma evolution strategy: A tutorial.
\newblock \emph{arXiv preprint arXiv:1604.00772}, 2016.

\bibitem[Hansen et~al.(2003)Hansen, M{\"u}ller, and Koumoutsakos]{hansen2003reducing}
Nikolaus Hansen, Sibylle~D M{\"u}ller, and Petros Koumoutsakos.
\newblock Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (cma-es).
\newblock \emph{Evol. Comput.}, 11\penalty0 (1):\penalty0 1--18, 2003.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Mazeika, Kadavath, and Song]{hendrycks2019using}
Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song.
\newblock Using self-supervised learning can improve model robustness and uncertainty.
\newblock In \emph{Proc. NeurIPS}, volume~32, 2019.

\bibitem[Heng and Soh(2023)]{heng2023selective}
Alvin Heng and Harold Soh.
\newblock Selective amnesia: A continual learning approach to forgetting in deep generative models.
\newblock In \emph{Proc. NeurIPS}, 2023.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and Duerig]{jia2021scaling}
Chao Jia, Yinfei Yang, Ye~Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with noisy text supervision.
\newblock In \emph{Proc. ICML}, pages 4904--4916, 2021.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness, Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska, et~al.]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{PNAS}, 114\penalty0 (13):\penalty0 3521--3526, 2017.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Technical report}, 2009.

\bibitem[Kurmanji et~al.(2023)Kurmanji, Triantafillou, Hayes, and Triantafillou]{kurmanji2024towards}
Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou.
\newblock Towards unbounded machine unlearning.
\newblock In \emph{Proc. NeurIPS}, 2023.

\bibitem[Li and Hoiem(2017)]{li2017learning}
Zhizhong Li and Derek Hoiem.
\newblock Learning without forgetting.
\newblock \emph{TPAMI}, 40\penalty0 (12):\penalty0 2935--2947, 2017.

\bibitem[Lu et~al.(2024)Lu, Wang, Li, Liu, and Kong]{lu2024mace}
Shilin Lu, Zilan Wang, Leyang Li, Yanzhu Liu, and Adams Wai-Kin Kong.
\newblock Mace: Mass concept erasure in diffusion models.
\newblock In \emph{Proc. CVPR}, pages 6430--6440, 2024.

\bibitem[Milkolov et~al.(2013)Milkolov, Chen, Corrado, and Dean]{mikolov2013efficient}
Tomas Milkolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Efficient estimation of word representations in vector space.
\newblock \emph{arXiv preprint arXiv:1301.3781}, 2013.

\bibitem[Oh et~al.(2023)Oh, Hwang, Lee, Lim, Jung, Jung, Choi, and Song]{Oh_2023_CVPR}
Changdae Oh, Hyeji Hwang, Hee-young Lee, YongTaek Lim, Geunyoung Jung, Jiyoung Jung, Hosik Choi, and Kyungwoo Song.
\newblock Blackvip: Black-box visual prompting for robust transfer learning.
\newblock In \emph{Proc. CVPR}, 2023.

\bibitem[OpenAI(2023)]{gpt-4v}
OpenAI.
\newblock Gpt-4v(ision) system card.
\newblock 2023.

\bibitem[Ouali et~al.(2023)Ouali, Bulat, Matinez, and Tzimiropoulos]{ouali2023black}
Yassine Ouali, Adrian Bulat, Brais Matinez, and Georgios Tzimiropoulos.
\newblock Black box few-shot adaptation for vision-language models.
\newblock In \emph{Proc. ICCV}, pages 15534--15546, 2023.

\bibitem[Pennington et~al.(2014)Pennington, Socher, and Manning]{pennington2014glove}
Jeffrey Pennington, Richard Socher, and Christopher Manning.
\newblock Glove: Global vectors for word representation.
\newblock In \emph{Proc. EMNLP}, pages 1532--1543, 2014.

\bibitem[Qian et~al.(2016)Qian, Hu, and Yu]{qian2016derivative}
Hong Qian, Yi-Qi Hu, and Yang Yu.
\newblock Derivative-free optimization of high-dimensional non-convex functions by sequential random embeddings.
\newblock In \emph{Proc. IJCAI}, 2016.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{Proc. ICML}, pages 8748--8763, 2021.

\bibitem[Ros and Hansen(2008)]{ros2008simple}
Raymond Ros and Nikolaus Hansen.
\newblock A simple modification in cma-es achieving linear time and space complexity.
\newblock In \emph{Proc. PPSN X}, pages 296--305, 2008.

\bibitem[Sekhari et~al.(2021)Sekhari, Acharya, Kamath, and Suresh]{sekhari2021remember}
Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda~Theertha Suresh.
\newblock Remember what you want to forget: Algorithms for machine unlearning.
\newblock In \emph{Proc. NeurIPS}, 2021.

\bibitem[Shibata et~al.(2021)Shibata, Irie, Ikami, and Mitsuzumi]{shibata2021learning}
Takashi Shibata, Go~Irie, Daiki Ikami, and Yu~Mitsuzumi.
\newblock Learning with selective forgetting.
\newblock In \emph{Proc. IJCAI}, volume~3, page~4, 2021.

\bibitem[Sun et~al.(2022{\natexlab{a}})Sun, He, Qian, Zhou, Huang, and Qiu]{sun2022bbtv2}
Tianxiang Sun, Zhengfu He, Hong Qian, Yunhua Zhou, Xuan-Jing Huang, and Xipeng Qiu.
\newblock Bbtv2: towards a gradient-free future with large language models.
\newblock In \emph{Proc. EMNLP}, pages 3916--3930, 2022{\natexlab{a}}.

\bibitem[Sun et~al.(2022{\natexlab{b}})Sun, Shao, Qian, Huang, and Qiu]{sun2022black}
Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu.
\newblock Black-box tuning for language-model-as-a-service.
\newblock In \emph{Proc. ICML}, pages 20841--20855, 2022{\natexlab{b}}.

\bibitem[Tarun et~al.(2023)Tarun, Chundawat, Mandal, and Kankanhalli]{tarun2023fast}
Ayush~K Tarun, Vikram~S Chundawat, Murari Mandal, and Mohan Kankanhalli.
\newblock Fast yet effective machine unlearning.
\newblock \emph{IEEE Trans. Neural Netw. Learn. Syst.}, pages 1--10, 2023.

\bibitem[Wah et~al.(2011)Wah, Branson, Welinder, Perona, and Belongie]{wah2011caltech}
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie.
\newblock The caltech-ucsd birds-200-2011 dataset.
\newblock \emph{Technical report}, 2011.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Kim, Li, Kornblith, Roelofs, Lopes, Hajishirzi, Farhadi, Namkoong, et~al.]{wortsman2022robust}
Mitchell Wortsman, Gabriel Ilharco, Jong~Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael~Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et~al.
\newblock Robust fine-tuning of zero-shot models.
\newblock In \emph{Proc. CVPR}, pages 7959--7971, 2022.

\bibitem[Ye et~al.(2022)Ye, Fu, Song, Yang, Liu, Jin, Song, and Wang]{ye2022learning}
Jingwen Ye, Yifang Fu, Jie Song, Xingyi Yang, Songhua Liu, Xin Jin, Mingli Song, and Xinchao Wang.
\newblock Learning with recoverable forgetting.
\newblock In \emph{Proc. ECCV}, 2022.

\bibitem[Zhang et~al.(2024)Zhang, Wang, Xu, Wang, and Shi]{zhang2024forget}
Gong Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi.
\newblock Forget-me-not: Learning to forget in text-to-image diffusion models.
\newblock In \emph{Proc. CVPR}, pages 1755--1764, 2024.

\bibitem[Zhou et~al.(2022{\natexlab{a}})Zhou, Yang, Loy, and Liu]{zhou2022conditional}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Conditional prompt learning for vision-language models.
\newblock In \emph{Proc. CVPR}, pages 16816--16825, 2022{\natexlab{a}}.

\bibitem[Zhou et~al.(2022{\natexlab{b}})Zhou, Yang, Loy, and Liu]{zhou2022learning}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Learning to prompt for vision-language models.
\newblock \emph{IJCV}, 130\penalty0 (9):\penalty0 2337--2348, 2022{\natexlab{b}}.

\end{thebibliography}
