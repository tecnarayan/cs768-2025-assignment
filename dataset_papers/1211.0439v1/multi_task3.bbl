\begin{thebibliography}{10}

\bibitem{WilRas06}
C~K~I Williams and C~Rasmussen.
\newblock {\em Gaussian Processes for Machine Learning}.
\newblock MIT Press, Cambridge, MA, 2006.

\bibitem{Baxter00}
J~Baxter.
\newblock A model of inductive bias learning.
\newblock {\em J. Artif. Intell. Res.}, 12:149--198, 2000.

\bibitem{BenSch08}
S~Ben-David and R~S Borbely.
\newblock A notion of task relatedness yielding provable multiple-task learning
  guarantees.
\newblock {\em Mach. Learn.}, 73(3):273--287, December 2008.

\bibitem{TehSeeJor05}
Y~W Teh, M~Seeger, and M~I Jordan.
\newblock Semiparametric latent factor models.
\newblock In {\em Workshop on Artificial Intelligence and Statistics 10}, pages
  333--340. Society for Artificial Intelligence and Statistics, 2005.

\bibitem{BonAgaWil07}
E~V Bonilla, F~V Agakov, and C~K~I Williams.
\newblock Kernel multi-task learning using task-specific features.
\newblock In {\em Proceedings of the 11th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}. Omni Press, 2007.

\bibitem{BonChaWil08}
E~V Bonilla, K~M~A Chai, and C~K~I Williams.
\newblock Multi-task {Gaussian} process prediction.
\newblock In J~C Platt, D~Koller, Y~Singer, and S~Roweis, editors, {\em NIPS 20}, pages 153--160,
  Cambridge, MA, 2008. MIT Press.

\bibitem{AlvLaw09}
M~Alvarez and N~D Lawrence.
\newblock Sparse convolved {Gaussian} processes for multi-output regression.
\newblock In D~Koller, D~Schuurmans, Y~Bengio, and L~Bottou, editors, {\em
  NIPS 21}, pages 57--64, Cambridge, MA, 2009. MIT Press.

\bibitem{LeePelKas11}
G~Leen, J~Peltonen, and S~Kaski.
\newblock Focused multi-task learning using {Gaussian} processes.
\newblock In Dimitrios Gunopulos, Thomas Hofmann, Donato Malerba, and Michalis
  Vazirgiannis, editors, {\em Machine Learning and Knowledge Discovery in
  Databases}, volume 6912 of {\em Lecture Notes in Computer Science}, pages
  310--325. Springer Berlin, Heidelberg, 2011.

\bibitem{AlvRosLaw12}
M~A {\'A}lvarez, L~Rosasco, and N~D Lawrence.
\newblock Kernels for vector-valued functions: a review.
\newblock {\em Foundations and Trends in Machine Learning}, 4:195--266, 2012.

%\bibitem{AlvRosLaw11}
%M~A Alvarez, L~Rosasco, and N~D Lawrence.
%\newblock Kernels for vector-valued functions: a review.
%\newblock {\em ArXiv e-prints}, June 2011.
%\newblock arXiv:1106.6251.

\bibitem{Maurer06}
A~Maurer.
\newblock Bounds for linear multi-task learning.
\newblock {\em J. Mach. Learn. Res.}, 7:117--139, 2006.

\bibitem{OppViv99}
M~Opper and F~Vivarelli.
\newblock General bounds on {Bayes} errors for regression with {Gaussian}
  processes.
\newblock In M~Kearns, S~A Solla, and D~Cohn, editors, {\em NIPS 11},
pages 302--308, Cambridge, MA, 1999. MIT
  Press.

\bibitem{TreWilOpp99}
G~F Trecate, C~K~I Williams, and M~Opper.
\newblock Finite-dimensional approximation of {Gaussian} processes.
\newblock In M~Kearns, S~A Solla, and D~Cohn, editors, {\em NIPS 11},
pages 218--224, Cambridge, MA, 1999. MIT
  Press.

\bibitem{Sollich99}
P~Sollich.
\newblock Learning curves for {Gaussian} processes.
\newblock In M~S Kearns, S~A Solla, and D~A Cohn, editors, {\em NIPS 11},
pages 344--350, Cambridge, MA,
  1999. MIT Press.

\bibitem{MalOpp01}
D~Malzahn and M~Opper.
\newblock Learning curves for {Gaussian} processes regression: A framework for
  good approximations.
\newblock In T~K Leen, T~G Dietterich, and V~Tresp, editors, {\em NIPS 13},
pages 273--279, Cambridge, MA,
  2001. MIT Press.

\bibitem{MalOpp02}
D~Malzahn and M~Opper.
\newblock A variational approach to learning curves.
\newblock In T~G Dietterich, S~Becker, and Z~Ghahramani, editors, {\em
  NIPS 14}, pages 463--469, Cambridge, MA,
  2002. MIT Press.

\bibitem{MalOpp02b}
D~Malzahn and M~Opper.
\newblock Statistical mechanics of learning: a variational approach for real
  data.
\newblock {\em Phys. Rev. Lett.}, 89:108302, 2002.

\bibitem{SolHal02}
P~Sollich and A~Halees.
\newblock Learning curves for {Gaussian} process regression: approximations and
  bounds.
\newblock {\em Neural Comput.}, 14(6):1393--1428, 2002.

\bibitem{Sollich02c}
P~Sollich.
\newblock Gaussian process regression with mismatched models.
\newblock In T~G Dietterich, S~Becker, and Z~Ghahramani, editors, {\em 
NIPS 14}, pages 519--526, Cambridge, MA,
  2002. MIT Press.

\bibitem{Sollich05b}
P~Sollich.
\newblock Can {Gaussian} process regression be made robust against model
  mismatch?
\newblock In {\em Deterministic and Statistical Methods in Machine Learning},
  volume 3635 of {\em Lecture Notes in Artificial Intelligence}, pages
  199--210. Springer Berlin, Heidelberg, 2005.

\bibitem{UrrSol10}
M~Urry and P~Sollich.
\newblock Exact larning curves for {Gaussian} process regression on large
  random graphs.
\newblock In J~Lafferty, C~K~I Williams, J~Shawe-Taylor, R~S Zemel, and
  A~Culotta, editors, {\em NIPS
  23}, pages 2316--2324, Cambridge, MA, 2010. MIT Press.

\bibitem{Chai09}
K~M~A Chai.
\newblock Generalization errors and learning curves for regression with
  multi-task {Gaussian} processes.
\newblock In Y~Bengio, D~Schuurmans, J~Lafferty, C~K~I Williams, and A~Culotta,
  editors, {\em NIPS 22}, pages
  279--287, 2009.

\bibitem{ZhuWilRohMor98}
H~Zhu, C~K~I Williams, R~J Rohwer, and M~Morciniec.
\newblock Gaussian regression and optimal finite dimensional linear models.
\newblock In C~M Bishop, editor, {\em Neural Networks and Machine Learning}.
  Springer, 1998.

\bibitem{RodDen10}
E~Rodner and J~Denzler.
\newblock One-shot learning of object categories using dependent {Gaussian}
  processes.
\newblock In Michael Goesele, Stefan Roth, Arjan Kuijper, Bernt Schiele, and
  Konrad Schindler, editors, {\em Pattern Recognition}, volume 6376 of {\em
  Lecture Notes in Computer Science}, pages 232--241. Springer Berlin,
  Heidelberg, 2010.

\bibitem{Heskes98}
T~Heskes.
\newblock Solving a huge number of similar tasks: a combination of multi-task
  learning and a hierarchical {Bayesian} approach.
\newblock In {\em Proceedings of the Fifteenth International Conference on
  Machine Learning (ICML'98)}, pages 233--241. Morgan Kaufmann, 1998.

\end{thebibliography}
