@inproceedings{bower2018landscape,
  title={The landscape of non-convex quadratic feasibility},
  author={Bower, Amanda and Jain, Lalit and Balzano, Laura},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3974--3978},
  year={2018},
  organization={IEEE}
}

@article{orvieto2019continuous,
  title={Continuous-time models for stochastic optimization algorithms},
  author={Orvieto, Antonio and Lucchi, Aurelien},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@incollection{clarke2004lyapunov,
  title={Lyapunov functions and feedback in nonlinear control},
  author={Clarke, Francis},
  booktitle={Optimal control, stabilization and nonsmooth analysis},
  pages={267--282},
  year={2004},
  publisher={Springer}
}

@article{cencini2013lyapunov,
  title={Lyapunov analysis: from dynamical systems theory to applications},
  author={Cencini, Massimo and Ginelli, Francesco},
  journal={Journal of Physics A: Mathematical and Theoretical},
  volume={46},
  number={25},
  pages={250301},
  year={2013},
  publisher={IOP Publishing}
}


@book{chellaboina2008nonlinear,
  title={Nonlinear dynamical systems and control: A Lyapunov-based approach},
  author={Chellaboina, VijaySekhar and Haddad, Wassim M},
  year={2008},
  publisher={Princeton University Press}
}

@article{kovachki2021continuous,
  title={Continuous time analysis of momentum methods},
  author={Kovachki, Nikola B and Stuart, Andrew M},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={17},
  pages={1--40},
  year={2021},
  publisher={Journal of Machine Learning Research}
}

@inproceedings{bi2022local,
  title={Local and global linear convergence of general low-rank matrix recovery problems},
  author={Bi, Yingjie and Zhang, Haixiang and Lavaei, Javad},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={9},
  pages={10129--10137},
  year={2022}
}

@article{bu2019lqr,
  title={LQR through the lens of first order methods: Discrete-time case},
  author={Bu, Jingjing and Mesbahi, Afshin and Fazel, Maryam and Mesbahi, Mehran},
  journal={arXiv preprint arXiv:1907.08921},
  year={2019}
}


@article{fatkhullin2021optimizing,
  title={Optimizing static linear feedback: Gradient method},
  author={Fatkhullin, Ilyas and Polyak, Boris},
  journal={SIAM Journal on Control and Optimization},
  volume={59},
  number={5},
  pages={3887--3911},
  year={2021},
  publisher={SIAM}
}

@inproceedings{mei2021leveraging,
  title={Leveraging non-uniformity in first-order non-convex optimization},
  author={Mei, Jincheng and Gao, Yue and Dai, Bo and Szepesvari, Csaba and Schuurmans, Dale},
  booktitle={International Conference on Machine Learning},
  pages={7555--7564},
  year={2021},
  organization={PMLR}
}

@inproceedings{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}

@article{zeng2018global,
  title={Global convergence in deep learning with variable splitting via the Kurdyka-{\l}ojasiewicz property},
  author={Zeng, Jinshan and Ouyang, Shikang and Lau, Tim Tsz-Kit and Lin, Shaobo and Yao, Yuan},
  journal={arXiv preprint arXiv:1803.00225},
  volume={9},
  year={2018}
}

@inproceedings{yuan2022general,
  title={A general sample complexity analysis of vanilla policy gradient},
  author={Yuan, Rui and Gower, Robert M and Lazaric, Alessandro},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3332--3380},
  year={2022},
  organization={PMLR}
}

@article{agarwal2021theory,
  title={On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift.},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={J. Mach. Learn. Res.},
  volume={22},
  number={98},
  pages={1--76},
  year={2021}
}

@inproceedings{mei2020global,
  title={On the global convergence rates of softmax policy gradient methods},
  author={Mei, Jincheng and Xiao, Chenjun and Szepesvari, Csaba and Schuurmans, Dale},
  booktitle={International Conference on Machine Learning},
  pages={6820--6829},
  year={2020},
  organization={PMLR}
}

@article{tan2019online,
  title={Online stochastic gradient descent with arbitrary initialization solves non-smooth, non-convex phase retrieval},
  author={Tan, Yan Shuo and Vershynin, Roman},
  journal={arXiv preprint arXiv:1910.12837},
  year={2019}
}

@article{fang1994inequalities,
  title={Inequalities for the trace of matrix product},
  author={Fang, Yuguang and Loparo, Kenneth A and Feng, Xiangbo},
  journal={IEEE Transactions on Automatic Control},
  volume={39},
  number={12},
  pages={2489--2490},
  year={1994},
  publisher={IEEE}
}

@article{elkabetz2021continuous,
  title={Continuous vs. discrete optimization of deep neural networks},
  author={Elkabetz, Omer and Cohen, Nadav},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{krichene2016continuous,
  title={Continuous and discrete dynamics for online learning and convex optimization},
  author={Krichene, Walid},
  journal={Ph. D. Dissertation},
  year={2016},
  publisher={University of California}
}

@inproceedings{ma2018implicit,
  title={Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval and matrix completion},
  author={Ma, Cong and Wang, Kaizheng and Chi, Yuejie and Chen, Yuxin},
  booktitle={International Conference on Machine Learning},
  pages={3345--3354},
  year={2018},
  organization={PMLR}
}

@article{jin2016provable,
  title={Provable efficient online matrix completion via non-convex stochastic gradient descent},
  author={Jin, Chi and Kakade, Sham M and Netrapalli, Praneeth},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@inproceedings{zhang2021revisiting,
  title={Revisiting the Role of Euler Numerical Integration on Acceleration and Stability in Convex Optimization},
  author={Zhang, Peiyuan and Orvieto, Antonio and Daneshmand, Hadi and Hofmann, Thomas and Smith, Roy S},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3979--3987},
  year={2021},
  organization={PMLR}
}

@article{agarwal2009information,
  title={Information-theoretic lower bounds on the oracle complexity of convex optimization},
  author={Agarwal, Alekh and Wainwright, Martin J and Bartlett, Peter and Ravikumar, Pradeep},
  journal={Advances in Neural Information Processing Systems},
  volume={22},
  year={2009}
}

@article{su2014differential,
  title={A differential equation for modeling Nesterov’s accelerated gradient method: theory and insights},
  author={Su, Weijie and Boyd, Stephen and Candes, Emmanuel},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{JMLR:v22:20-195,
  author  = {Ashia C. Wilson and Ben Recht and Michael I. Jordan},
  title   = {A Lyapunov Analysis of Accelerated Methods in Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {113},
  pages   = {1-34},
}

@inproceedings{NIPS2015_f60bb6bb,
 author = {Krichene, Walid and Bayen, Alexandre and Bartlett, Peter L},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Accelerated Mirror Descent in Continuous and Discrete Time},
 volume = {28},
 year = {2015}
}

@inproceedings{gunasekar2021mirrorless,
  title={Mirrorless mirror descent: A natural derivation of mirror descent},
  author={Gunasekar, Suriya and Woodworth, Blake and Srebro, Nathan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2305--2313},
  year={2021},
  organization={PMLR}
}

@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers, Inc.}
}

@inproceedings{mcomplete,
  title={Provable Efficient Online Matrix Completion via Non-convex Stochastic Gradient Descent},
  author={Chi Jin and Sham M. Kakade and Praneeth Netrapalli},
  booktitle={NIPS},
  year={2016}
}

@article{kl2,
abstract = {We prove the o-minimal generalization of the Łojasiewicz inequality $\Vert \{\rm grad\}\, f\Vert \ge |f|^\alpha $, with $\alpha &lt; 1$, in a neighborhood of $a$, where $f$ is real analytic at $a$ and $f(a)=0$. We deduce, as in the analytic case, that trajectories of the gradient of a function definable in an o-minimal structure are of uniformly bounded length. We obtain also that the gradient flow gives a retraction onto levels of such functions.},
author = {Kurdyka, Krzysztof},
journal = {Annales de l'institut Fourier},
keywords = {flows of gradient; -minimal structure; subanalytic sets; Łojasiewicz inequalities; trajectories of gradient},
language = {eng},
number = {3},
pages = {769-783},
publisher = {Association des Annales de l'Institut Fourier},
title = {On gradients of functions definable in o-minimal structures},
volume = {48},
year = {1998},
}


@article{kl1,
 ISSN = {0364765X, 15265471},
 author = {Hédy Attouch and Jérôme Bolte and Patrick Redont and Antoine Soubeyran},
 journal = {Mathematics of Operations Research},
 number = {2},
 pages = {438--457},
 publisher = {INFORMS},
 title = {Proximal Alternating Minimization and Projection Methods for Nonconvex Problems: An Approach Based on the Kurdyka-Łojasiewicz Inequality},
 urldate = {2022-05-19},
 volume = {35},
 year = {2010}
}



@inproceedings{pl1,
author = {Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
title = {Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-\L{}Ojasiewicz Condition},
year = {2016},
isbn = {9783319461274},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
doi = {10.1007/978-3-319-46128-1_50},
abstract = {In 1963, Polyak proposed a simple condition that is sufficient to show a global linear convergence rate for gradient descent. This condition is a special case of the \L{}ojasiewicz inequality proposed in the same year, and it does not require strong convexity or even convexity. In this work, we show that this much-older Polyak-\L{}ojasiewicz P\L~ inequality is actually weaker than the main conditions that have been explored to show linear convergence rates without strong convexity over the last 25 years. We also use the P\L~ inequality to give new analyses of coordinate descent and stochastic gradient for many non-strongly-convex and some non-convex functions. We further propose a generalization that applies to proximal-gradient methods for non-smooth optimization, leading to simple proofs of linear convergence for support vector machines and L1-regularized least squares without additional assumptions.},
booktitle = {European Conference on Machine Learning and Knowledge Discovery in Databases - Volume 9851},
pages = {795–811},
numpages = {17},
keywords = {Variance-reduction, Stochastic gradient, Coordinate descent, L1-regularization, Boosting, Support vector machines, Gradient descent},
location = {Riva del Garda, Italy},
series = {ECML PKDD 2016}
}

@article{pl2,
title = {Gradient methods for the minimisation of functionals},
journal = {USSR Computational Mathematics and Mathematical Physics},
volume = {3},
number = {4},
pages = {864-878},
year = {1963},
issn = {0041-5553},
author = {B.T. Polyak},
abstract = {Let tf(t) be a functional defined in the (real) Hubert space H. The problem consists in finding its minimum value tff∗ = inf tf(x) and some minimum point x∗ (if such exists).}
}


@article{pl3,
  title={A topological property of real analytic subsets},
  author={Lojasiewicz, Stanislaw},
  journal={Coll. du CNRS, Les {\'e}quations aux d{\'e}riv{\'e}es partielles},
  volume={117},
  number={87-89},
  pages={2},
  year={1963}
}

@inproceedings{sgdnotimp,
  author    = {Satyen Kale and Ayush Sekhari and
               Karthik Sridharan
               },
  title     = {{SGD:} The Role of Implicit Regularization, Batch-size and Multiple-epochs},
  booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
               on Neural Information Processing Systems 2021, NeurIPS 2021, December
               6-14, 2021, virtual},
  pages     = {27422--27433},
  year      = {2021},
  crossref  = {DBLP:conf/nips/2021},
}

@inproceedings{onepoint,
  added-at = {2019-04-03T00:00:00.000+0200},
  author = {Kleinberg, Robert and Li, Yuanzhi and Yuan, Yang},
  biburl = {https://www.bibsonomy.org/bibtex/274c07a610d98fd91c038fc2a3718a60f/dblp},
  booktitle = {ICML},
  crossref = {conf/icml/2018},
  editor = {Dy, Jennifer G. and Krause, Andreas},
  ee = {http://proceedings.mlr.press/v80/kleinberg18a.html},
  interhash = {254ffa1eced5f99ec283e65402f03691},
  intrahash = {74c07a610d98fd91c038fc2a3718a60f},
  keywords = {dblp},
  pages = {2703-2712},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  timestamp = {2019-04-04T11:43:21.000+0200},
  title = {An Alternative View: When Does SGD Escape Local Minima?},
  volume = 80,
  year = 2018
}




@article{wojtowytsch2021discrete,
  title={Stochastic gradient descent with noise of machine learning type. Part I: Discrete time analysis},
  author={Wojtowytsch, Stephan},
  journal={arXiv preprint arXiv:2105.01650},
  year={2021}
}

@inproceedings{de2015global,
  title={Global convergence of stochastic gradient descent for some non-convex matrix problems},
  author={De Sa, Christopher and Re, Christopher and Olukotun, Kunle},
  booktitle={International conference on machine learning},
  pages={2332--2341},
  year={2015},
  organization={PMLR}
}

@inproceedings{jain2017global,
  title={Global convergence of non-convex gradient descent for computing matrix squareroot},
  author={Jain, Prateek and Jin, Chi and Kakade, Sham and Netrapalli, Praneeth},
  booktitle={Artificial Intelligence and Statistics},
  pages={479--488},
  year={2017},
  organization={PMLR}
}

@article{wojtowytsch2021stochastic,
  title={Stochastic gradient descent with noise of machine learning type. part ii: Continuous time analysis},
  author={Wojtowytsch, Stephan},
  journal={arXiv preprint arXiv:2106.02588},
  year={2021}
}

@inproceedings{nemirovskistochastic,
  title={Stochastic approximation approach to stochastic programming},
  author={Nemirovski, A and Juditsky, A and Lan, G and Shapiro, A},
  booktitle={SIAM J. Optim},
  organization={Citeseer}
}

@article{chatterjee2022convergence,
  title={Convergence of gradient descent for deep neural networks},
  author={Chatterjee, Sourav},
  journal={arXiv preprint arXiv:2203.16462},
  year={2022}
}

@phdthesis{krichene2016lyapunov,
  title={A Lyapunov Approach to Accelerated First-Order Optimization In Continuous and Discrete Time},
  author={Krichene, Walid},
  year={2016},
  school={University of California, Berkeley}
}

@article{nemirovskij1983problem,
  title={Problem complexity and method efficiency in optimization},
  author={Nemirovskij, Arkadij Semenovi{\v{c}} and Yudin, David Borisovich},
  year={1983},
  publisher={Wiley-Interscience}
}

@article{wilson2021lyapunov,
  title={A lyapunov analysis of accelerated methods in optimization},
  author={Wilson, Ashia C and Recht, Ben and Jordan, Michael I},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={113},
  pages={1--34},
  year={2021}
}

@book{wilson2018lyapunov,
  title={Lyapunov arguments in optimization},
  author={Wilson, Ashia},
  year={2018},
  publisher={University of California, Berkeley}
}

@inproceedings{gunasekar2018characterizing,
  title={Characterizing implicit bias in terms of optimization geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={1832--1841},
  year={2018},
  organization={PMLR}
}

@article{gunasekar2018implicitb,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:1806.00468},
  year={2018}
}

@inproceedings{gunasekar2018implicit,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nathan},
  booktitle={2018 Information Theory and Applications Workshop (ITA)},
  pages={1--10},
  year={2018},
  organization={IEEE}
}


@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
  publisher={JMLR. org}
}

@inproceedings{AroraGMM15,
  author    = {Sanjeev Arora and
               Rong Ge and
               Tengyu Ma and
               Ankur Moitra},
  editor    = {Peter Gr{\"{u}}nwald and
               Elad Hazan and
               Satyen Kale},
  year={2015},
  title     = {Simple, Efficient, and Neural Algorithms for Sparse Coding},
  booktitle = {Proceedings of The 28th Conference on Learning Theory, {COLT} 2015,
               Paris, France, July 3-6, 2015},
  timestamp = {Sun, 08 Aug 2021 16:40:51 +0200},
  biburl    = {https://dblp.org/rec/conf/colt/AroraGMM15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{converse,
title = {Classical converse theorems in Lyapunov's second method},
journal = {Discrete and Continuous Dynamical Systems - B},
volume = {20},
number = {8},
pages = {2333-2360},
year = {2015},
author = {Christopher M. Kellett},
}


@article{chen2019gradient,
  title={Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval},
  author={Chen, Yuxin and Chi, Yuejie and Fan, Jianqing and Ma, Cong},
  journal={Mathematical Programming},
  volume={176},
  number={1},
  pages={5--37},
  year={2019},
  publisher={Springer}
}

@article{GF3,
  abstract = {In a series of recent theoretical works, it has been shown that strongly
over-parameterized neural networks trained with gradient-based methods could
converge linearly to zero training loss, with their parameters hardly varying.
In this note, our goal is to exhibit the simple structure that is behind these
results. In a simplified setting, we prove that "lazy training" essentially
solves a kernel regression. We also show that this behavior is not so much due
to over-parameterization than to a choice of scaling, often implicit, that
allows to linearize the model around its initialization. These theoretical
results complemented with simple numerical experiments make it seem unlikely
that "lazy training" is behind the many successes of neural networks in high
dimensional tasks.},
  added-at = {2019-06-10T21:24:39.000+0200},
  author = {Chizat, Lenaic and Bach, Francis},
  biburl = {https://www.bibsonomy.org/bibtex/21288611c5a671c77130225823b77e297/kirk86},
  description = {[1812.07956] A Note on Lazy Training in Supervised Differentiable Programming},
  interhash = {0d661a4e291240c1fe3e91501f87cf0a},
  intrahash = {1288611c5a671c77130225823b77e297},
  keywords = {optimization},
  note = {cite arxiv:1812.07956},
  timestamp = {2019-06-10T21:24:39.000+0200},
  title = {A Note on Lazy Training in Supervised Differentiable Programming},
  url = {http://arxiv.org/abs/1812.07956},
  year = 2018
}



@article{GF2,
  author    = {Niladri S. Chatterji and
               Philip M. Long and
               Peter L. Bartlett},
  title     = {The Interplay Between Implicit Bias and Benign Overfitting in Two-Layer
               Linear Networks},
  journal   = {CoRR},
  volume    = {abs/2108.11489},
  year      = {2021},
  eprinttype = {arXiv},
  eprint    = {2108.11489},
  timestamp = {Fri, 27 Aug 2021 15:02:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2108-11489.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
a service of  Schloss Dagstuhl - Leibniz Center for Informatics	homebrowsesearchabout

@inproceedings{GF1,
  author    = {Shahar Azulay and
               Edward Moroshko and
               Mor Shpigel Nacson and
               Blake E. Woodworth and
               Nathan Srebro and
               Amir Globerson and
               Daniel Soudry},
  editor    = {Marina Meila and
               Tong Zhang},
  title     = {On the Implicit Bias of Initialization Shape: Beyond Infinitesimal
               Mirror Descent},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning,
               {ICML} 2021, 18-24 July 2021, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {468--477},
  publisher = {{PMLR}},
  year      = {2021},
}

@article{ji2018risk,
  title={Risk and parameter convergence of logistic regression},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1803.07300},
  year={2018}
}


@article{bansal2017potential,
  title={Potential-function proofs for first-order methods},
  author={Bansal, Nikhil and Gupta, Anupam},
  journal={arXiv preprint arXiv:1712.04581},
  year={2017}
}

@article{candes2015phase,
  title={Phase retrieval via Wirtinger flow: Theory and algorithms},
  author={Candes, Emmanuel J and Li, Xiaodong and Soltanolkotabi, Mahdi},
  journal={IEEE Transactions on Information Theory},
  volume={61},
  number={4},
  pages={1985--2007},
  year={2015},
  publisher={IEEE}
}


@article{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@article{liu2020linearity,
  title={On the linearity of large non-linear models: when and why the tangent kernel is constant},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Misha},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15954--15964},
  year={2020}
}

@article{bhatia2020dynamics,
  title={Online learning with dynamics:A minimax perspective},
  author={Bhatia, Kush and Sridharan, Karthik},
  journal={arXiv preprint arXiv:2012.01705},
  year={2020}
}

@inproceedings{arora2018optimization,
  title={On the optimization of deep networks: Implicit acceleration by overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  booktitle={International Conference on Machine Learning},
  pages={244--253},
  year={2018},
  organization={PMLR}
}

@inproceedings{jain2017global,
  title={Global convergence of non-convex gradient descent for computing matrix squareroot},
  author={Jain, Prateek and Jin, Chi and Kakade, Sham and Netrapalli, Praneeth},
  booktitle={Artificial Intelligence and Statistics},
  pages={479--488},
  year={2017},
  organization={PMLR}
}

@inproceedings{mohammadi2019global,
  title={Global exponential convergence of gradient methods over the nonconvex landscape of the linear quadratic regulator},
  author={Mohammadi, Hesameddin and Zare, Armin and Soltanolkotabi, Mahdi and Jovanovi{\'c}, Mihailo R},
  booktitle={2019 IEEE 58th Conference on Decision and Control (CDC)},
  pages={7474--7479},
  year={2019},
  organization={IEEE}
}

@inproceedings{vu2021exact,
  title={Exact linear convergence rate analysis for low-rank symmetric matrix completion via gradient descent},
  author={Vu, Trung and Raich, Raviv},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3240--3244},
  year={2021},
  organization={IEEE}
}

@article{khalil2002nonlinear,
  title={Nonlinear systems third edition},
  author={Khalil, Hassan K},
  journal={Patience Hall},
  volume={115},
  year={2002}
}


@article{johnson1988easy,
  title={How easy is local search?},
  author={Johnson, David S and Papadimitriou, Christos H and Yannakakis, Mihalis},
  journal={Journal of computer and system sciences},
  volume={37},
  number={1}, 
  pages={79--100},
  year={1988},
  publisher={Elsevier}
}

@article{hanson1999invexity,
  title={Invexity and the Kuhn--Tucker theorem},
  author={Hanson, Morgan A},
  journal={Journal of mathematical analysis and applications},
  volume={236},
  number={2},
  pages={594--604},
  year={1999},
  publisher={Elsevier}
}

@inproceedings{vardi2021implicit,
  title={Implicit regularization in relu networks with the square loss},
  author={Vardi, Gal and Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={4224--4258},
  year={2021},
  organization={PMLR}
}

@article{soltanolkotabi2017learning,
  title={Learning relus via gradient descent},
  author={Soltanolkotabi, Mahdi},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{oymak2019overparameterized,
  title={Overparameterized nonlinear learning: Gradient descent takes the shortest path?},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  booktitle={International Conference on Machine Learning},
  pages={4951--4960},
  year={2019},
  organization={PMLR}
}

@article{su2014differential,
  title={A differential equation for modeling Nesterov’s accelerated gradient method: Theory and insights},
  author={Su, Weijie and Boyd, Stephen and Candes, Emmanuel},
  journal={Advances in neural information processing systems},
  volume={27},
  pages={2510--2518},
  year={2014}
}

@inproceedings{NIPS2010_76cf99d3,
 author = {Srebro, Nathan and Sridharan, Karthik and Tewari, Ambuj},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Smoothness, Low Noise and Fast Rates}, 
 volume = {23},
 year = {2010}
}

@article{lee2021improved,
  title={Improved rates for prediction and identification for partially observed linear dynamical systems},
  author={Lee, Holden},
  year={2021}
}

@book{wilson2018lyapunov,
  title={Lyapunov arguments in optimization},
  author={Wilson, Ashia},
  year={2018},
  publisher={University of California, Berkeley}
}

@article{fathi2019smoothing,
  title={Smoothing Lyapunov functions},
  author={Fathi, Albert and Pageault, Pierre},
  journal={Transactions of the American Mathematical Society},
  volume={371},
  number={3},
  pages={1677--1700},
  year={2019}
}

@article{massera1949liapounoff,
  title={On Liapounoff's conditions of stability},
  author={Massera, Jose Luis},
  journal={Annals of Mathematics},
  pages={705--721},
  year={1949},
  publisher={JSTOR}
}

@article{krichene2016continuous,
  title={Continuous and discrete dynamics for online learning and convex optimization},
  author={Krichene, Walid},
  journal={Ph. D. Dissertation},
  year={2016},
  publisher={University of California}
}

@article{kellett2015classical,
  title={Classical converse theorems in Lyapunov's second method},
  author={Kellett, Christopher M},
  journal={arXiv preprint arXiv:1502.04809},
  year={2015}
}

@article{yang2001properties,
  title={On properties of preinvex functions},
  author={Yang, Xin Min and Li, Duan},
  journal={Journal of Mathematical Analysis and Applications},
  volume={256},
  number={1},
  pages={229--241},
  year={2001},
  publisher={Elsevier}
}

@article{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  journal={arXiv preprint arXiv:1805.09545},
  year={2018}
}

@article{burkholder,
  author    = {Dylan J. Foster and
               Alexander Rakhlin and
               Karthik Sridharan},
  title     = {Online Learning: Sufficient Statistics and the Burkholder Method},
  journal   = {CoRR},
  volume    = {abs/1803.07617},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.07617},
  eprinttype = {arXiv},
  eprint    = {1803.07617},
  timestamp = {Mon, 13 Aug 2018 16:46:47 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-07617.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{relaxandrandomize,
 author = {Rakhlin, Sasha and Shamir, Ohad and Sridharan, Karthik},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Relax and Randomize : From Value to Algorithms},
 volume = {25},
 year = {2012}
}

@article{limitedswitching2018,
  author    = {Jason Altschuler and
               Kunal Talwar},
  title     = {Online learning over a finite action set with limited switching},
  journal   = {CoRR},
  volume    = {abs/1803.01548},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.01548},
  eprinttype = {arXiv},
  eprint    = {1803.01548},
  timestamp = {Mon, 13 Aug 2018 16:48:27 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-01548.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{shrinkingdartboard2010,
author = {Geulen, Sascha and Vöcking, Berthold and Winkler, Melanie},
year = {2010},
month = {10},
pages = {132-143},
title = {Regret Minimization for Online Buffering Problems Using the Weighted Majority Algorithm},
volume = {17},
journal = {Electronic Colloquium on Computational Complexity (ECCC)}
}


@inproceedings{littman2002predictive,
  title={Predictive representations of state},
  author={Littman, Michael L and Sutton, Richard S},
  booktitle={Advances in neural information processing systems},
  pages={1555--1561},
  year={2002}
}


@inproceedings{jiang2018pac,
  title={PAC Reinforcement Learning with an Imperfect Model},
  author={Jiang, Nan},
  booktitle={AAAI},
  year={2018}
}


@article{kakade2020information,
  title={Information theoretic regret bounds for online nonlinear control},
  author={Kakade, Sham and Krishnamurthy, Akshay and Lowrey, Kendall and Ohnishi, Motoya and Sun, Wen},
  journal={arXiv preprint arXiv:2006.12466},
  year={2020}
}


@article{shalev2011learning,
  title={Learning kernel-based halfspaces with the 0-1 loss},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Sridharan, Karthik},
  journal={SIAM Journal on Computing},
  volume={40},
  number={6},
  pages={1623--1646},
  year={2011},
  publisher={SIAM}
}

@article{du2021bilinear,
  title={Bilinear classes: A structural framework for provable generalization in rl},
  author={Du, Simon S and Kakade, Sham M and Lee, Jason D and Lovett, Shachar and Mahajan, Gaurav and Sun, Wen and Wang, Ruosong},
  journal={arXiv preprint arXiv:2103.10897},
  year={2021}
}


@inproceedings{van2001joint,
  title={Joint diagonalization via subspace fitting techniques},
  author={Van Der Veen, A-J},
  booktitle={2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No. 01CH37221)},
  volume={5},
  pages={2773--2776},
  year={2001},
  organization={IEEE}
}

@article{o2018random,
  title={Random perturbation of low rank matrices: Improving classical bounds},
  author={O'Rourke, Sean and Vu, Van and Wang, Ke},
  journal={Linear Algebra and its Applications},
  volume={540},
  pages={26--59},
  year={2018},
  publisher={Elsevier}
}


@article{modi2021model,
  title={Model-free Representation Learning and Exploration in Low-rank MDPs},
  author={Modi, Aditya and Chen, Jinglin and Krishnamurthy, Akshay and Jiang, Nan and Agarwal, Alekh},
  journal={arXiv preprint arXiv:2102.07035},
  year={2021}
}


@article{modi2021model,
  title={Model-free Representation Learning and Exploration in Low-rank MDPs},
  author={Modi, Aditya and Chen, Jinglin and Krishnamurthy, Akshay and Jiang, Nan and Agarwal, Alekh},
  journal={arXiv preprint arXiv:2102.07035},
  year={2021}
}

@article{kakade2020information,
  title={Information theoretic regret bounds for online nonlinear control},
  author={Kakade, Sham and Krishnamurthy, Akshay and Lowrey, Kendall and Ohnishi, Motoya and Sun, Wen},
  journal={arXiv preprint arXiv:2006.12466},
  year={2020}
}

@inproceedings{kearns1999efficient,
  title={Efficient reinforcement learning in factored MDPs},
  author={Kearns, Michael and Koller, Daphne},
  booktitle={IJCAI},
  volume={16},
  pages={740--747},
  year={1999}
}

@article{agarwal2020flambe,
  title={Flambe: Structural complexity and representation learning of low rank mdps},
  author={Agarwal, Alekh and Kakade, Sham and Krishnamurthy, Akshay and Sun, Wen},
  journal={arXiv preprint arXiv:2006.10814},
  year={2020}
}

@inproceedings{osband2014near,
  title={Near-optimal reinforcement learning in factored MDPs},
  author={Osband, Ian and Van Roy, Benjamin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={604--612},
  year={2014}
}

@article{kearns2002near,
  title={Near-optimal reinforcement learning in polynomial time},
  author={Kearns, Michael and Singh, Satinder},
  journal={Machine learning},
  volume={49},
  number={2-3},
  pages={209--232},
  year={2002},
  publisher={Springer}
}

@inproceedings{jiang2017contextual,
  title={Contextual decision processes with low Bellman rank are PAC-learnable},
  author={Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
  booktitle={International Conference on Machine Learning},
  year={2017}
}

@book{devroye2012combinatorial,
  title={Combinatorial methods in density estimation},
  author={Devroye, Luc and Lugosi, G{\'a}bor},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@inproceedings{koller1999computing,
  title={Computing factored value functions for policies in structured MDPs},
  author={Koller, Daphne and Parr, Ronald},
  booktitle={IJCAI},
  volume={99},
  pages={1332--1339},
  year={1999}
}

@inproceedings{krishnamurthy2016pac,
  title={PAC reinforcement learning with rich observations},
  author={Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1840--1848},
  year={2016}
}

@inproceedings{szita2010model,
  title={Model-based reinforcement learning with nearly tight exploration complexity bounds},
  author={Szita, Istv{\'a}n and Szepesv{\'a}ri, Csaba},
  booktitle={Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
  pages={1031--1038},
  year={2010}
}

@article{brafman2002r,
  title={R-max-a general polynomial time algorithm for near-optimal reinforcement learning},
  author={Brafman, Ronen I and Tennenholtz, Moshe},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Oct},
  pages={213--231},
  year={2002}
}

@inproceedings{strehl2006pac,
  title={PAC model-free reinforcement learning},
  author={Strehl, Alexander L and Li, Lihong and Wiewiora, Eric and Langford, John and Littman, Michael L},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={881--888},
  year={2006},
  organization={ACM}
}

@phdthesis{kakade2003sample,
  title={On the sample complexity of reinforcement learning},
  author={Kakade, Sham Machandranath and others},
  year={2003},
  school={University of London London, England}
}

@article{jin2018q,
  title={Is Q-learning Provably Efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  journal={arXiv preprint arXiv:1807.03765},
  year={2018}
}



################Model-Based RL with extra exploration distribution###########
@inproceedings{talvitie2015agnostic,
  title={Agnostic System Identification for Monte Carlo Planning.},
  author={Talvitie, Erik},
  booktitle={AAAI},
  pages={2986--2992},
  year={2015}
}

@inproceedings{talvitie2017self,
  title={Self-Correcting Models for Model-Based Reinforcement Learning.},
  author={Talvitie, Erik},
  booktitle={AAAI},
  pages={2597--2603},
  year={2017}
}
################Model-Based RL with extra exploration distribution###########



###########Practical Model-Based RL #####################
@inproceedings{abbeel2007application,
  title={An application of reinforcement learning to aerobatic helicopter flight},
  author={Abbeel, Pieter and Coates, Adam and Quigley, Morgan and Ng, Andrew Y},
  booktitle={Advances in neural information processing systems},
  pages={1--8},
  year={2007}
}
@article{sun2018dual,
  title={Dual Policy Iteration},
  author={Sun, Wen and Gordon, Geoffrey J and Boots, Byron and Bagnell, J Andrew},
  journal={arXiv preprint arXiv:1805.10755},
  year={2018}
}

@article{xie2015model,
  title={Model-based reinforcement learning with parametrized physical models and optimism-driven exploration},
  author={Xie, Christopher and Patil, Sachin and Moldovan, Teodor and Levine, Sergey and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1509.06824},
  year={2015}
}

@inproceedings{moldovan2015optimism,
  title={Optimism-driven exploration for nonlinear systems},
  author={Moldovan, Teodor Mihai and Levine, Sergey and Jordan, Michael I and Abbeel, Pieter},
  booktitle={Robotics and Automation (ICRA), 2015 IEEE International Conference on},
  pages={3239--3246},
  year={2015},
  organization={IEEE}
}
####################################################





@inproceedings{farahmand2017value,
  title={Value-Aware Loss Function for Model-based Reinforcement Learning},
  author={Farahmand, Amir-massoud and Barreto, Andre and Nikovski, Daniel},
  booktitle={Artificial Intelligence and Statistics},
  pages={1486--1494},
  year={2017}
}


############# Continuous State Space with Lipschitiz models ###########
@inproceedings{lakshmanan2015improved,
  title={Improved regret bounds for undiscounted continuous reinforcement learning},
  author={Lakshmanan, Kailasam and Ortner, Ronald and Ryabko, Daniil},
  booktitle={International Conference on Machine Learning},
  pages={524--532},
  year={2015}
}

@inproceedings{ortner2012online,
  title={Online regret bounds for undiscounted continuous reinforcement learning},
  author={Ortner, Ronald and Ryabko, Daniil},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1763--1771},
  year={2012}
}

@inproceedings{kakade2003exploration,
  title={Exploration in metric state spaces},
  author={Kakade, Sham and Kearns, Michael J and Langford, John},
  booktitle={Proceedings of the 20th International Conference on Machine Learning (ICML-03)},
  pages={306--312},
  year={2003}
}

####################################################




=======
@inproceedings{dann2015sample,
  title={Sample complexity of episodic fixed-horizon reinforcement learning},
  author={Dann, Christoph and Brunskill, Emma},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2818--2826},
  year={2015}
}
@inproceedings{wen2013efficient,
  title={Efficient exploration and value function generalization in deterministic systems},
  author={Wen, Zheng and Van Roy, Benjamin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3021--3029},
  year={2013}
}

@inproceedings{lattimore2013sample,
  title={The sample-complexity of general reinforcement learning},
  author={Lattimore, Tor and Hutter, Marcus and Sunehag, Peter and others},
  booktitle={Proceedings of the 30th International Conference on Machine Learning},
  year={2013},
  organization={Journal of Machine Learning Research}
}


@article{bartlett2008high,
  title={High-probability regret bounds for bandit online linear optimization},
  author={Bartlett, Peter L and Dani, Varsha and Hayes, Thomas and Kakade, Sham and Rakhlin, Alexander and Tewari, Ambuj},
  year={2008}
}
@article{xu2018algorithmic,
  title={Algorithmic Framework for Model-based Reinforcement Learning with Theoretical Guarantees},
  author={Xu, Huazhe and Li, Yuanzhi and Tian, Yuandong and Darrell, Trevor and Ma, Tengyu},
  journal={arXiv preprint arXiv:1807.03858},
  year={2018}
}
@article{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:1703.05449},
  year={2017}
}

