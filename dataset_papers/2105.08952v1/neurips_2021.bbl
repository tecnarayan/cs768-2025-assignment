\begin{thebibliography}{10}

\bibitem{krishnamoorthi2018quantizing}
Raghuraman Krishnamoorthi.
\newblock Quantizing deep convolutional networks for efficient inference: A
  whitepaper, 2018.

\bibitem{choi2018pact}
Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang,
  Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan.
\newblock {PACT}: Parameterized clipping activation for quantized neural
  networks, 2018.

\bibitem{zhou2016dorefa}
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He~Wen, and Yuheng Zou.
\newblock Dorefa-net: Training low bitwidth convolutional neural networks with
  low bitwidth gradients.
\newblock {\em CoRR}, abs/1606.06160, 2016.

\bibitem{bhalgat2020lsq+}
Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak.
\newblock Lsq+: Improving low-bit quantization through learnable offsets and
  better initialization.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 696--697, 2020.

\bibitem{Zoph2016NeuralAS}
Barret Zoph and Quoc~V. Le.
\newblock Neural architecture search with reinforcement learning.
\newblock In {\em International Conference on Learning Representations}, volume
  abs/1611.01578, 2017.

\bibitem{Zoph_2018}
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc~V. Le.
\newblock Learning transferable architectures for scalable image recognition.
\newblock {\em 2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, Jun 2018.

\bibitem{xie2018snas}
Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin.
\newblock {SNAS}: stochastic neural architecture search.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{chen2019detnas}
Yukang Chen, Tong Yang, Xiangyu Zhang, Gaofeng Meng, Chunhong Pan, and Jian
  Sun.
\newblock Detnas: Neural architecture search on object detection.
\newblock {\em arXiv preprint arXiv:1903.10979}, 2019.

\bibitem{Mei2020AtomNAS:}
Jieru Mei, Yingwei Li, Xiaochen Lian, Xiaojie Jin, Linjie Yang, Alan Yuille,
  and Jianchao Yang.
\newblock Atomnas: Fine-grained end-to-end neural architecture search.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{Nayman2019XNASNA}
Niv Nayman, Asaf Noy, Tal Ridnik, Itamar Friedman, Rong Jin, and Lihi
  Zelnik-Manor.
\newblock Xnas: Neural architecture search with expert advice.
\newblock {\em NeurIPS}, abs/1906.08031, 2019.

\bibitem{cai2018proxylessnas}
Han Cai, Ligeng Zhu, and Song Han.
\newblock Proxyless{NAS}: Direct neural architecture search on target task and
  hardware.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{guo2019single}
Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and
  Jian Sun.
\newblock Single path one-shot neural architecture search with uniform
  sampling.
\newblock {\em arXiv preprint arXiv:1904.00420}, 2019.

\bibitem{Tan_2019_CVPR}
Mingxing Tan, Bo~Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew
  Howard, and Quoc~V. Le.
\newblock Mnasnet: Platform-aware neural architecture search for mobile.
\newblock In {\em The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2019.

\bibitem{Ghiasi_2019_CVPR}
Golnaz Ghiasi, Tsung-Yi Lin, and Quoc~V. Le.
\newblock Nas-fpn: Learning scalable feature pyramid architecture for object
  detection.
\newblock In {\em The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2019.

\bibitem{Li2019RandomSA}
Liam Li and Ameet Talwalkar.
\newblock Random search and reproducibility for neural architecture search.
\newblock In {\em UAI}, 2019.

\bibitem{yu2020bignas}
Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans,
  Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, and Quoc Le.
\newblock Bignas: Scaling up neural architecture search with big single-stage
  models.
\newblock {\em arXiv preprint arXiv:2003.11142}, 2020.

\bibitem{haq}
Kuan Wang, Zhijian Liu, Yujun Lin, Ji~Lin, and Song Han.
\newblock Haq: Hardware-aware automated quantization with mixed precision.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2019.

\bibitem{Dong_2019_HAWQ}
Zhen Dong, Zhewei Yao, Amir Gholami, Michael Mahoney, and Kurt Keutzer.
\newblock Hawq: Hessian aware quantization of neural networks with
  mixed-precision.
\newblock {\em 2019 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, Oct 2019.

\bibitem{wu2019mixed}
Bichen Wu, Yanghan Wang, Peizhao Zhang, Yuandong Tian, Peter Vajda, and Kurt
  Keutzer.
\newblock Mixed precision quantization of convnets via differentiable neural
  architecture search, 2019.

\bibitem{wang2020apq}
Tianzhe Wang, Kuan Wang, Han Cai, Ji~Lin, Zhijian Liu, and Song Han.
\newblock Apq: Joint search for network architecture, pruning and quantization
  policy, 2020.

\bibitem{bulat2020bats}
Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos.
\newblock Bats: Binary architecture search.
\newblock {\em arXiv preprint arXiv:2003.01711}, 2020.

\bibitem{cai2020once}
Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han.
\newblock Once for all: Train one network and specialize it for efficient
  deployment.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{shen2020quantized}
Mingzhu Shen, Feng Liang, Chuming Li, Chen Lin, Ming Sun, Junjie Yan, and Wanli
  Ouyang.
\newblock Once quantized for all: Progressively searching for quantized
  efficient models, 2020.

\bibitem{pmlr-v37-ioffe15}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In Francis Bach and David Blei, editors, {\em Proceedings of the 32nd
  International Conference on Machine Learning}, volume~37 of {\em Proceedings
  of Machine Learning Research}, pages 448--456, Lille, France, 07--09 Jul
  2015. PMLR.

\bibitem{hu2018squeeze}
Jie Hu, Li~Shen, and Gang Sun.
\newblock Squeeze-and-excitation networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7132--7141, 2018.

\bibitem{rusci2019memory}
Manuele Rusci, Alessandro Capotondi, and Luca Benini.
\newblock Memory-driven mixed low precision quantization for enabling deep
  network inference on microcontrollers.
\newblock {\em arXiv preprint arXiv:1905.13082}, 2019.

\bibitem{NEURIPS2020_3f13cf4d}
Mart van Baalen, Christos Louizos, Markus Nagel, Rana~Ali Amjad, Ying Wang,
  Tijmen Blankevoort, and Max Welling.
\newblock Bayesian bits: Unifying quantization and pruning.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 5741--5752. Curran Associates, Inc., 2020.

\bibitem{jin2020adabits}
Qing Jin, Linjie Yang, and Zhenyu Liao.
\newblock Adabits: Neural network quantization with adaptive bit-widths.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 2146--2156, 2020.

\bibitem{yu2019any}
Haichao Yu, Haoxiang Li, Honghui Shi, Thomas~S Huang, and Gang Hua.
\newblock Any-precision deep neural networks.
\newblock {\em arXiv preprint arXiv:1911.07346}, 2019.

\bibitem{Alizadeh2020Gradient}
Milad Alizadeh, Arash Behboodi, Mart van Baalen, Christos Louizos, Tijmen
  Blankevoort, and Max Welling.
\newblock Gradient $\ell_1$ regularization for quantization robustness.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{KURE}
Moran Shkolnik, Brian Chmiel, Ron Banner, Gil Shomron, Yury Nahshan, Alex
  Bronstein, and Uri Weiser.
\newblock Robust quantization: One model to rule them all.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 5308--5317. Curran Associates, Inc., 2020.

\bibitem{DBLP:journals/corr/abs-1811-09426}
Yukang Chen, Gaofeng Meng, Qian Zhang, Xinbang Zhang, Liangchen Song, Shiming
  Xiang, and Chunhong Pan.
\newblock Joint neural architecture search and quantization.
\newblock {\em CoRR}, abs/1811.09426, 2018.

\bibitem{mobilenetv3}
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo~Chen, Mingxing
  Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et~al.
\newblock Searching for mobilenetv3.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1314--1324, 2019.

\bibitem{jacob2017quantization}
Benoit Jacob, Skirmantas Kligys, Bo~Chen, Menglong Zhu, Matthew Tang, Andrew
  Howard, Hartwig Adam, and Dmitry Kalenichenko.
\newblock Quantization and training of neural networks for efficient
  integer-arithmetic-only inference, 2017.

\bibitem{gemmlowp}
Benoit {Jacob} and Pete {Warden}.
\newblock gemmlowp: a small self-contained low-precision gemm library.

\bibitem{lsq}
Steven~K Esser, Jeffrey~L McKinstry, Deepika Bablani, Rathinakumar Appuswamy,
  and Dharmendra~S Modha.
\newblock Learned step size quantization.
\newblock {\em arXiv preprint arXiv:1902.08153}, 2019.

\bibitem{bengio2013estimating}
Yoshua Bengio, Nicholas LÃ©onard, and Aaron Courville.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation, 2013.

\bibitem{banner2018post}
Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry.
\newblock Post-training 4-bit quantization of convolution networks for
  rapid-deployment.
\newblock {\em Conference on Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem{imagenet}
{Jia Deng}, {Wei Dong}, {Richard Socher}, {Li-Jia Li}, {Kai Li}, and {Li
  Fei-Fei}.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 248--255, 2009.

\bibitem{nsga996017}
{Kalyanmoy Deb}, {Amrit Pratap}, {Sameer Agarwal}, and T.~{Meyarivan}.
\newblock A fast and elitist multiobjective genetic algorithm: Nsga-ii.
\newblock {\em IEEE Transactions on Evolutionary Computation}, 6(2):182--197,
  2002.

\bibitem{strubell2019energy}
Emma Strubell, Ananya Ganesh, and Andrew McCallum.
\newblock Energy and policy considerations for deep learning in nlp.
\newblock {\em ACL}, 2019.

\bibitem{phan2020binarizing}
Hai Phan, Zechun Liu, Dang Huynh, Marios Savvides, Kwang-Ting Cheng, and
  Zhiqiang Shen.
\newblock Binarizing mobilenet via evolution-based searching.
\newblock {\em arXiv preprint arXiv:2005.06305}, 2020.

\bibitem{li2019additive}
Yuhang Li, Xin Dong, and Wei Wang.
\newblock Additive powers-of-two quantization: A non-uniform discretization for
  neural networks.
\newblock {\em arXiv preprint arXiv:1909.13144}, 2019.

\bibitem{kim2019qkd}
Jangho Kim, Yash Bhalgat, Jinwon Lee, Chirag Patel, and Nojun Kwak.
\newblock Qkd: Quantization-aware knowledge distillation.
\newblock {\em arXiv preprint arXiv:1911.12491}, 2019.

\bibitem{nat}
Zhichao Lu, Gautam Sreekumar, Erik~D. Goodman, Wolfgang Banzhaf, Kalyanmoy Deb,
  and Vishnu~Naresh Boddeti.
\newblock Neural architecture transfer.
\newblock {\em CoRR}, abs/2005.05859, 2020.

\bibitem{Cream}
Houwen Peng, Hao Du, Hongyuan Yu, Qi~Li, Jing Liao, and Jianlong Fu.
\newblock Cream of the crop: Distilling prioritized paths for one-shot neural
  architecture search.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\end{thebibliography}
