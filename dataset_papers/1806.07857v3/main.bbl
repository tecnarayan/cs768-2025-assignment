\begin{thebibliography}{100}

\bibitem{Arras:17}
L.~Arras, G.~Montavon, K.-R. M{\"{u}}ller, and W.~Samek.
\newblock Explaining recurrent neural network predictions in sentiment
  analysis.
\newblock {\em CoRR}, abs/1706.07206, 2017.

\bibitem{Aytar:18}
Y.~Aytar, T.~Pfaff, D.~Budden, T.~Le Paine, Z.~Wang, and N.~de~Freitas.
\newblock Playing hard exploration games by watching {YouTube}.
\newblock {\em ArXiv}, 2018.

\bibitem{Bach:15}
S.~Bach, A.~Binder, G.~Montavon, F.~Klauschen, K.-R. M{\"{u}}ller, and
  W.~Samek.
\newblock On pixel-wise explanations for non-linear classifier decisions by
  layer-wise relevance propagation.
\newblock {\em PLoS ONE}, 10(7):e0130140, 2015.

\bibitem{Bakker:02}
B.~Bakker.
\newblock Reinforcement learning with long short-term memory.
\newblock In T.~G. Dietterich, S.~Becker, and Z.~Ghahramani, editors, {\em
  Advances in Neural Information Processing Systems 14}, pages 1475--1482. MIT
  Press, 2002.

\bibitem{Bakker:07}
B.~Bakker.
\newblock Reinforcement learning by backpropagation through an lstm
  model/critic.
\newblock In {\em IEEE International Symposium on Approximate Dynamic
  Programming and Reinforcement Learning}, pages 127--134, 2007.

\bibitem{Barreto:18}
A.~Barreto, D.~Borsa, J.~Quan, T.~Schaul, D.~Silver, M.~Hessel, D.~Mankowitz,
  A.~Z{\'{\i}}dek, and R.~Munos.
\newblock Transfer in deep reinforcement learning using successor features and
  generalised policy improvement.
\newblock In {\em 35th International Conference on Machine Learning}, volume~80
  of {\em Proceedings of Machine Learning Research}, pages 501--510, 2018.
\newblock ArXiv 1901.10964.

\bibitem{Barreto:17}
A.~Barreto, W.~Dabney, R.~Munos, J.~Hunt, T.~Schaul, H.~P. vanHasselt, and
  D.~Silver.
\newblock Successor features for transfer in reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems 30}, pages
  4055--4065, 2017.
\newblock ArXiv 1606.05312.

\bibitem{Barto:04}
A.~G. Barto and T.~G. Dietterich.
\newblock {\em Handbook of Learning and Approximate Dynamic Programming},
  chapter Reinforcement Learning and Its Relationship to Supervised Learning,
  pages 45--63.
\newblock IEEE Press, John Wiley \& Sons, 2015.

\bibitem{Beleznay:99}
F.~Beleznay, T.~Grobler, and C.~Szepesv\'{a}ri.
\newblock Comparing value-function estimation algorithms in undiscounted
  problems.
\newblock Technical Report TR-99-02, Mindmaker Ltd., 1999.

\bibitem{Bellemare:17}
M.~G. Bellemare, W.~Dabney, and R.~Munos.
\newblock A distributional perspective on reinforcement learning.
\newblock In D.~Precup and Y.~W. Teh, editors, {\em Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of {\em Proceedings
  of Machine Learning Research (ICML)}, pages 449--458. PMLR, 2017.

\bibitem{Bellemare:13}
M.~G. Bellemare, Y.~Naddaf, J.~Veness, and M.~Bowling.
\newblock The {Arcade} learning environment: An evaluation platform for general
  agents.
\newblock {\em Journal of Artificial Intelligence Research}, 47:253--279, 2013.

\bibitem{Berthelot:17}
D.~Berthelot, T.~Schumm, and L.~Metz.
\newblock {BEGAN:} boundary equilibrium generative adversarial networks.
\newblock {\em ArXiv e-prints}, 2017.

\bibitem{Bertsekas:91}
D.~P. Bertsekas and J.~N. Tsitsiklis.
\newblock An analysis of stochastic shortest path problems.
\newblock {\em Math. Oper. Res.}, 16(3), 1991.

\bibitem{Bertsekas:96}
D.~P. Bertsekas and J.~N. Tsitsiklis.
\newblock {\em Neuro-dynamic programming}.
\newblock Athena Scientific, Belmont, MA, 1996.

\bibitem{Bienayme:53}
I.-J. Bienaym{\'{e}}.
\newblock Consid{\'{e}}rations {\`{a}}l'appui de la d{\'{e}}couverte de
  laplace.
\newblock {\em Comptes Rendus de l'Acad{\'{e}}mie des Sciences}, 37:309--324,
  1853.

\bibitem{Bolton:15}
W.~Bolton.
\newblock {\em Instrumentation and Control Systems}, chapter Chapter 5 -
  Process Controllers, pages 99--121.
\newblock Newnes, 2 edition, 2015.

\bibitem{Borkar:97}
V.~S. Borkar.
\newblock Stochastic approximation with two time scales.
\newblock {\em Systems \& Control Letters}, 29(5):291--294, 1997.

\bibitem{Brockman:16}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba.
\newblock Openai gym.
\newblock {\em ArXiv}, 2016.

\bibitem{Cox:09}
I.~J. Cox, R.~Fu, and L.~K. Hansen.
\newblock Probably approximately correct search.
\newblock In {\em Advances in Information Retrieval Theory}, pages 2--16.
  Springer, Berlin, Heidelberg, 2009.

\bibitem{Dayan:92}
P.~Dayan.
\newblock The convergence of {TD($\lambda$)} for general $\lambda$.
\newblock {\em Machine Learning}, 8:341, 1992.

\bibitem{Dhariwal:17}
P.~Dhariwal, C.~Hesse, O.~Klimov, A.~Nichol, M.~Plappert, A.~Radford,
  J.~Schulman, S.~Sidor, and Y.~Wu.
\newblock Openai baselines.
\newblock \url{https://github.com/openai/baselines}, 2017.

\bibitem{Donahue:14}
J.~Donahue, L.~A. Hendricks, S.~Guadarrama, M.~Rohrbach, S.~Venugopalan,
  K.~Saenko, and T.~Darrell.
\newblock Long-term recurrent convolutional networks for visual recognition and
  description.
\newblock {\em ArXiv}, 2014.

\bibitem{Edwards:18}
A.~D. Edwards, L.~Downs, and J.~C. Davidson.
\newblock Forward-backward reinforcement learning.
\newblock {\em ArXiv}, 2018.

\bibitem{Espeholt:18}
L.~Espeholt, H.~Soyer, R.~Munos, K.~Simonyan, V.~Mnih, T.~Ward, Y.~Doron,
  V.~Firoiu, T.~Harley, I.~Dunning, S.~Legg, and K.~Kavukcuoglu.
\newblock {IMPALA:} {S}calable distributed {Deep-RL} with importance weighted
  actor-learner architectures.
\newblock In J.~Dy and A.~Krause, editors, {\em Proceedings of the 35th
  International Conference on Machine Learning}, 2018.
\newblock ArXiv: 1802.01561.

\bibitem{Feinstein:16}
Z.~Feinstein.
\newblock Continuity properties and sensitivity analysis of parameterized fixed
  points and approximate fixed points.
\newblock Technical report, Operations Research and Financial Engineering
  Laboratory, Washington University in St. Louis, 2016.
\newblock preprint.

\bibitem{Fortunato:18}
M.~Fortunato, M.~G. Azar, B.~Piot, J.~Menick, I.~Osband, A.~Graves, V.~Mnih,
  R.~Munos, D.~Hassabis, O.~Pietquin, C.~Blundell, and S.~Legg.
\newblock Noisy networks for exploration.
\newblock {\em ArXiv}, 2018.
\newblock Sixth International Conference on Learning Representations (ICLR).

\bibitem{Frigon:07}
M.~Frigon.
\newblock Fixed point and continuation results for contractions in metric and
  {Gauge} spaces.
\newblock {\em Banach Center Publications}, 77(1):89--114, 2007.

\bibitem{Fu:18}
J.~Fu, K.~Luo, and S.~Levine.
\newblock Learning robust rewards with adversarial inverse reinforcement
  learning.
\newblock {\em ArXiv}, 2018.
\newblock Sixth International Conference on Learning Representations (ICLR).

\bibitem{Geiger:14}
J.~T. Geiger, Z.~Zhang, F.~Weninger, B.~Schuller, and G.~Rigoll.
\newblock Robust speech recognition using long short-term memory recurrent
  neural networks for hybrid acoustic modelling.
\newblock In {\em Proc. 15th Annual Conf. of the Int. Speech Communication
  Association (INTERSPEECH 2014)}, pages 631--635, Singapore, September 2014.

\bibitem{Gers:00a}
F.~A. Gers and J.~Schmidhuber.
\newblock Recurrent nets that time and count.
\newblock In {\em Proc. Int. Joint Conf. on Neural Networks (IJCNN 2000)},
  volume~3, pages 189--194. IEEE, 2000.

\bibitem{Gers:99a}
F.~A. Gers, J.~Schmidhuber, and F.~Cummins.
\newblock Learning to forget: Continual prediction with {LSTM}.
\newblock In {\em Proc. Int. Conf. on Artificial Neural Networks (ICANN '99)},
  pages 850--855, Edinburgh, Scotland, 1999.

\bibitem{Gers:00}
F.~A. Gers, J.~Schmidhuber, and F.~Cummins.
\newblock Learning to forget: Continual prediction with {LSTM}.
\newblock {\em Neural Comput.}, 12(10):2451--2471, 2000.

\bibitem{gijbels2010censored}
Ir{\`e}ne Gijbels.
\newblock Censored data.
\newblock {\em Wiley Interdisciplinary Reviews: Computational Statistics},
  2(2):178--188, 2010.

\bibitem{Givan:03}
R.~Givan, T.~Dean, and M.~Greig.
\newblock Equivalence notions and model minimization in {Markov} decision
  processes.
\newblock {\em Artificial Intelligence}, 147(1):163--223, 2003.

\bibitem{Gonzalez-Dominguez:14}
J.~Gonzalez-Dominguez, I.~Lopez-Moreno, H.~Sak, J.~Gonzalez-Rodriguez, and
  P.~Moreno.
\newblock Automatic language identification using long short-term memory
  recurrent neural networks.
\newblock In {\em Proc. 15th Annual Conf. of the Int. Speech Communication
  Association (INTERSPEECH 2014)}, pages 2155--2159, Singapore, September 2014.

\bibitem{Goyal:18}
A.~Goyal, P.~Brakel, W.~Fedus, T.~Lillicrap, S.~Levine, H.~Larochelle, and
  Y.~Bengio.
\newblock Recall traces: Backtracking models for efficient reinforcement
  learning.
\newblock {\em ArXiv}, 2018.

\bibitem{Graves:09}
A.~Graves, M.~Liwicki, S.~Fernandez, R.~Bertolami, H.~Bunke, and
  J.~Schmidhuber.
\newblock A novel connectionist system for improved unconstrained handwriting
  recognition.
\newblock {\em IEEE Trans. Pattern Anal. Mach. Intell.}, 31(5):855--868, 2009.

\bibitem{Graves:13}
A.~Graves, A.-R. Mohamed, and G.~E. Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In {\em Proc. IEEE Int. Conf. on Acoustics, Speech and Signal
  Processing (ICASSP 2013)}, pages 6645--6649, Vancouver, BC, 2013.

\bibitem{Graves:05}
A.~Graves and J.~Schmidhuber.
\newblock Framewise phoneme classification with bidirectional {LSTM} and other
  neural network architectures.
\newblock {\em Neural Networks}, 18(5-6):602--610, 2005.

\bibitem{Greff:15}
K.~Greff, R.~K. Srivastava, J.~Koutn\'{i}k, B.~R. Steunebrink, and
  J.~Schmidhuber.
\newblock {LSTM}: A search space odyssey.
\newblock {\em ArXiv}, 2015.

\bibitem{Grunewalder:11}
S.~Gr{\"{u}}new{\"{a}}lder and K.~Obermayer.
\newblock The optimal unbiased value estimator and its relation to {LSTD}, {TD}
  and {MC}.
\newblock {\em Machine Learning}, 83(3):289--330, 2011.

\bibitem{Ha:18}
D.~Ha and J.~Schmidhuber.
\newblock World models.
\newblock {\em ArXiv}, 2018.

\bibitem{Harutyunyan:15}
A.~Harutyunyan, S.~Devlin, P.~Vrancx, and A.~Now'{e}.
\newblock Expressing arbitrary reward functions as potential-based advice.
\newblock In {\em Proceedings of the Twenty-Ninth AAAI Conference on Artificial
  Intelligence (AAAI'15)}, pages 2652--2658, 2015.

\bibitem{Hausknecht:15}
M.~J. Hausknecht and P.~Stone.
\newblock Deep recurrent {Q-Learning} for partially observable {MDPs}.
\newblock {\em ArXiv}, 2015.

\bibitem{Heess:16}
N.~Heess, G.~Wayne, Y.~Tassa, T.~P. Lillicrap, M.~A. Riedmiller, and D.~Silver.
\newblock Learning and transfer of modulated locomotor controllers.
\newblock {\em ArXiv}, 2016.

\bibitem{Hernandez-Leal:18}
P.~Hernandez-Leal, B.~Kartal, and M.~E. Taylor.
\newblock Is multiagent deep reinforcement learning the answer or the question?
  {A} brief survey.
\newblock {\em ArXiv}, 2018.

\bibitem{Hessel:17}
M.~Hessel, J.~Modayil, H.~van Hasselt, T.~Schaul, G.~Ostrovski, W.~Dabney,
  D.~Horgan, B.~Piot, M.~G. Azar, and D.~Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock {\em ArXiv}, 2017.

\bibitem{Hochreiter:90}
S.~Hochreiter.
\newblock {Implementierung und Anwendung eines `neuronalen'
  Echtzeit-Lernalgorithmus f\"{u}r reaktive Umgebungen}.
\newblock Practical work, Supervisor: J. Schmidhuber, Institut f\"{u}r
  Informatik, Technische Universit\"{a}t M\"{u}nchen, 1990.

\bibitem{Hochreiter:91}
S.~Hochreiter.
\newblock { Untersuchungen zu dynamischen neuronalen Netzen}.
\newblock Master's thesis, Technische Universit\"{a}t M\"{u}nchen, 1991.

\bibitem{Hochreiter:97f}
S.~Hochreiter.
\newblock Recurrent neural net learning and vanishing gradient.
\newblock In C.~Freksa, editor, {\em Proc. Fuzzy-Neuro-Systeme '97}, pages
  130--137, Sankt Augustin, 1997. INFIX.

\bibitem{Hochreiter:98}
S.~Hochreiter.
\newblock The vanishing gradient problem during learning recurrent neural nets
  and problem solutions.
\newblock {\em Internat. J. Uncertain. Fuzziness Knowledge-Based Systems},
  6(2):107--116, 1998.

\bibitem{Hochreiter:00}
S.~Hochreiter, Y.~Bengio, P.~Frasconi, and J.~Schmidhuber.
\newblock Gradient flow in recurrent nets: the difficulty of learning long-term
  dependencies.
\newblock In J.~F. Kolen and S.~C. Kremer, editors, {\em A Field Guide to
  Dynamical Recurrent Networks}. IEEE Press, 2001.

\bibitem{Hochreiter:07}
S.~Hochreiter, M.~Heusel, and K.~Obermayer.
\newblock Fast model-based protein homology detection without alignment.
\newblock {\em Bioinformatics}, 23(14):1728--1736, 2007.

\bibitem{Hochreiter:95}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock Technical Report FKI-207-95, Fakult\"{a}t f\"{u}r Informatik,
  Technische Universit\"{a}t M\"{u}nchen, 1995.

\bibitem{Hochreiter:97}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural Comput.}, 9(8):1735--1780, 1997.

\bibitem{Hochreiter:97e}
S.~Hochreiter and J.~Schmidhuber.
\newblock {LSTM} can solve hard long time lag problems.
\newblock In M.~C. Mozer, M.~I. Jordan, and T.~Petsche, editors, {\em Advances
  in Neural Information Processing Systems 9}, pages 473--479, Cambridge, MA,
  1997. MIT Press.

\bibitem{Hochreiter:01}
S.~Hochreiter, A.~Steven Younger, and Peter~R. Conwell.
\newblock Learning to learn using gradient descent.
\newblock In G.~Dorffner, H.~Bischof, and K.~Hornik, editors, {\em Proc. Int.
  Conf. on Artificial Neural Networks (ICANN 2001)}, pages 87--94. Springer,
  2001.

\bibitem{Horgan:18}
D.~Horgan, J.~Quan, D.~Budden, G.~Barth-Maron, M.~Hessel, H.~van Hasselt, and
  D.~Silver.
\newblock Distributed prioritized experience replay.
\newblock {\em ArXiv}, 2018.
\newblock Sixth International Conference on Learning Representations (ICLR).

\bibitem{Hung:18}
C.~Hung, T.~Lillicrap, J.~Abramson, Y.~Wu, M.~Mirza, F.~Carnevale, A.~Ahuja,
  and G.~Wayne.
\newblock Optimizing agent behavior over long time scales by transporting
  value.
\newblock {\em ArXiv}, 2018.

\bibitem{Hyvarinen:01}
A.~Hyv{\"a}rinen, J.~Karhunen, and E.~Oja.
\newblock {\em Independent Component Analysis}.
\newblock John Wiley \& Sons, New York, 2001.

\bibitem{Jaakkola:94}
T.~Jaakkola, M.~I. Jordan, and S.~P. Singh.
\newblock On the convergence of stochastic iterative dynamic programming
  algorithms.
\newblock {\em Neural Computation}, 6(6):1185--1201, 1994.

\bibitem{Jachymski:96}
J.~Jachymski.
\newblock Continuous dependence of attractors of iterated function systems.
\newblock {\em Journal Of Mathematical Analysis And Applications},
  198(0077):221--226, 1996.

\bibitem{John:94}
G.~H. John.
\newblock When the best move isn't optimal: $q$-learning with exploration.
\newblock In {\em Proceedings of the 10th Tenth National Conference on
  Artificial Intelligence, Menlo Park, CA, 1994. AAAI Press.}, page 1464, 1994.

\bibitem{Karmakar:17}
P.~Karmakar and S.~Bhatnagar.
\newblock Two time-scale stochastic approximation with controlled {Markov}
  noise and off-policy temporal-difference learning.
\newblock {\em Mathematics of Operations Research}, 2017.

\bibitem{Ke:18}
N.~Ke, A.~Goyal, O.~Bilaniuk, J.~Binas, M.~Mozer, C.~Pal, and Y.~Bengio.
\newblock Sparse attentive backtracking: Temporal credit assignment through
  reminding.
\newblock In {\em Advances in Neural Information Processing Systems 31}, pages
  7640--7651, 2018.

\bibitem{Khandelwal:16}
P.~Khandelwal, E.~Liebman, S.~Niekum, and P.~Stone.
\newblock On the analysis of complex backup strategies in {Monte Carlo Tree
  Search}.
\newblock In {\em International Conference on Machine Learning}, pages
  1319--1328, 2016.

\bibitem{Kirr:97}
E.~Kirr and A.~Petrusel.
\newblock Continuous dependence on parameters of the fixed point set for some
  set-valued operators.
\newblock {\em Discussiones Mathematicae Differential Inclusions}, 17:29--41,
  1997.

\bibitem{Kocsis:06}
L.~Kocsis and C.~Szepesv{\'{a}}ri.
\newblock Bandit based {Monte-Carlo} planning.
\newblock In {\em European Conference on Machine Learning}, pages 282--293.
  Springer, 2006.

\bibitem{Koutnik:13}
J.~Koutn\'{\i}k, G.~Cuccu, J.~Schmidhuber, and F.~Gomez.
\newblock Evolving large-scale neural networks for vision-based reinforcement
  learning.
\newblock In {\em Proceedings of the 15th Annual Conference on Genetic and
  Evolutionary Computation}, GECCO '13, pages 1061--1068, 2013.

\bibitem{Kwiecinski:92}
M.~Kwiecinski.
\newblock A note on continuity of fixed points.
\newblock {\em Universitatis Iagellonicae Acta Mathematica}, 29:19--24, 1992.

\bibitem{Landecker:13}
W.~Landecker, M.~D. Thomure, L.~M.~A. Bettencourt, M.~Mitchell, G.~T. Kenyon,
  and S.~P. Brumby.
\newblock Interpreting individual classifications of hierarchical networks.
\newblock In {\em IEEE Symposium on Computational Intelligence and Data Mining
  (CIDM)}, pages 32--38, 2013.

\bibitem{Lattimore:18}
T.~Lattimore and C.~Szepesv\'{a}.
\newblock {\em Bandit Algorithms}.
\newblock Cambridge University Press, 2018.
\newblock Draft of 28th July, Revision 1016.

\bibitem{Li:06}
L.~Li, T.~J. Walsh, and M.~L. Littman.
\newblock Towards a unified theory of state abstraction for {MDPs}.
\newblock In {\em Ninth International Symposium on Artificial Intelligence and
  Mathematics (ISAIM)}, 2006.

\bibitem{Lin:93}
L.~Lin.
\newblock {\em Reinforcement Learning for Robots Using Neural Networks}.
\newblock PhD thesis, Carnegie Mellon University, Pittsburgh, 1993.

\bibitem{Lugosi:03}
G.~Lugosi.
\newblock Concentration-of-measure inequalities.
\newblock In {\em Summer School on Machine Learning at the Australian National
  University,Canberra}, 2003.
\newblock Lecture notes of 2009.

\bibitem{Luoma:17}
J.~Luoma, S.~Ruutu, A.~W. King, and H.~Tikkanen.
\newblock Time delays, competitive interdependence, and firm performance.
\newblock {\em Strategic Management Journal}, 38(3):506--525, 2017.

\bibitem{Mannor:07}
S.~Mannor, D.~Simester, P.~Sun, and J.~N. Tsitsiklis.
\newblock Bias and variance approximation in value function estimates.
\newblock {\em Management Science}, 53(2):308--322, 2007.

\bibitem{Marchi:14}
E.~Marchi, G.~Ferroni, F.~Eyben, L.~Gabrielli, S.~Squartini, and B.~Schuller.
\newblock Multi-resolution linear prediction based features for audio onset
  detection with bidirectional {LSTM} neural networks.
\newblock In {\em Proc. IEEE Int. Conf. on Acoustics, Speech and Signal
  Processing (ICASSP 2014)}, pages 2164--2168, Florence, May 2014.

\bibitem{Marchenko:67}
V.~A. Mar\u{o}enko and L.~A. Pastur.
\newblock Distribution of eigenvalues or some sets of random matrices.
\newblock {\em Mathematics of the USSR-Sbornik}, 1(4):457, 1967.

\bibitem{Mnih:16}
V.~Mnih, A.~P. Badia, M.~Mirza, A.~Graves, T.~Lillicrap, T.~Harley, D.~Silver,
  and K.~Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In M.~F. Balcan and K.~Q. Weinberger, editors, {\em Proceedings of
  the 33rd International Conference on Machine Learning (ICML)}, volume~48 of
  {\em Proceedings of Machine Learning Research}, pages 1928--1937. PMLR, 2016.

\bibitem{Mnih:13}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~Graves, I.~Antonoglou, D.~Wierstra, and
  M.~A. Riedmiller.
\newblock Playing {Atari} with deep reinforcement learning.
\newblock {\em ArXiv}, 2013.

\bibitem{Mnih:15}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, S.~Petersen,
  C.~Beattie, A.~Sadik, I.~Antonoglou, H.~King, D.~Kumaran, D.~Wierstra,
  S.~Legg, , and D.~Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529--533, 2015.

\bibitem{Montavon:17taylor}
G.~Montavon, S.~Lapuschkin, A.~Binder, W.~Samek, and K.-R. M{\"{u}}ller.
\newblock Explaining nonlinear classification decisions with deep {Taylor}
  decomposition.
\newblock {\em Pattern Recognition}, 65:211 -- 222, 2017.

\bibitem{Montavon:17}
G.~Montavon, W.~Samek, and K.-R. M{\"{u}}ller.
\newblock Methods for interpreting and understanding deep neural networks.
\newblock {\em Digital Signal Processing}, 73:1--15, 2017.

\bibitem{Moore:93}
A.~W. Moore and C.~G. Atkeson.
\newblock Prioritized sweeping: Reinforcement learning with less data and less
  time.
\newblock {\em Machine Learning}, 13(1):103--130, 1993.

\bibitem{Munro:87}
P.~W. Munro.
\newblock A dual back-propagation scheme for scalar reinforcement learning.
\newblock In {\em Proceedings of the Ninth Annual Conference of the Cognitive
  Science Society, Seattle, WA}, pages 165--176, 1987.

\bibitem{Ng:99}
A.~Y. Ng, D.~Harada, and S.~J. Russell.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In {\em Proceedings of the Sixteenth International Conference on
  Machine Learning (ICML'99)}, pages 278--287, 1999.

\bibitem{ODonoghue:17}
B.~O'Donoghue, I.~Osband, R.~Munos, and V.~Mnih.
\newblock The uncertainty {Bellman} equation and exploration.
\newblock {\em ArXiv}, 2017.

\bibitem{Patek:97}
S.~D. Patek.
\newblock {\em Stochastic and shortest path games: theory and algorithms}.
\newblock PhD thesis, Massachusetts Institute of Technology. Dept. of
  Electrical Engineering and Computer Science, 1997.

\bibitem{Peng:96}
J.~Peng and R.~J. Williams.
\newblock Incremental multi-step $q$-learning.
\newblock {\em Machine Learning}, 22(1):283--290, 1996.

\bibitem{Peters:07}
J.~Peters and S.~Schaal.
\newblock Reinforcement learning by reward-weighted regression for operational
  space control.
\newblock In {\em Proceedings of the 24th International Conference on Machine
  Learning}, pages 745--750, 2007.

\bibitem{Pineau:18}
J.~Pineau.
\newblock The machine learning reproducibility checklist, 2018.

\bibitem{Pohlen:18}
T.~Pohlen, B.~Piot, T.~Hester, M.~G. Azar, D.~Horgan, D.~Budden,
  G.~Barth-Maron, H.~van Hasselt, J.~Quan, M.~Ve{\v{c}}er{\'{i}}k, M.~Hessel,
  R.~Munos, and O.~Pietquin.
\newblock Observe and look further: Achieving consistent performance on
  {Atari}.
\newblock {\em ArXiv}, 2018.

\bibitem{Poulin:06}
B.~Poulin, R.~Eisner, D.~Szafron, P.~Lu, R.~Greiner, D.~S. Wishart, A.~Fyshe,
  B.~Pearcy, C.~MacDonell, and J.~Anvik.
\newblock Visual explanation of evidence in additive classifiers.
\newblock In {\em Proceedings of the 18th Conference on Innovative Applications
  of Artificial Intelligence (IAAI)}, volume~2, pages 1822--1829, 2006.

\bibitem{Puterman:90}
M.~L. Puterman.
\newblock Markov decision processes.
\newblock In {\em Stochastic Models}, volume~2 of {\em Handbooks in Operations
  Research and Management Science}, chapter~8, pages 331--434. Elsevier, 1990.

\bibitem{Puterman:05}
M.~L. Puterman.
\newblock {\em Markov Decision Processes}.
\newblock John Wiley \& Sons, Inc., 2005.

\bibitem{Rahmandad:09}
H.~Rahmandad, N.~Repenning, and J.~Sterman.
\newblock Effects of feedback delay on learning.
\newblock {\em System Dynamics Review}, 25(4):309--338, 2009.

\bibitem{Ravindran:01}
B.~Ravindran and A.~G. Barto.
\newblock Symmetries and model minimization in {Markov} decision processes.
\newblock Technical report, University of Massachusetts, Amherst, MA, USA,
  2001.

\bibitem{Ravindran:03}
B.~Ravindran and A.~G. Barto.
\newblock {SMDP} homomorphisms: An algebraic approach to abstraction in
  semi-{Markov} decision processes.
\newblock In {\em Proceedings of the 18th International Joint Conference on
  Artificial Intelligence (IJCAI'03)}, pages 1011--1016, San Francisco, CA,
  USA, 2003. Morgan Kaufmann Publishers Inc.

\bibitem{Rencher:08}
A.~C. Rencher and G.~B. Schaalje.
\newblock {\em Linear Models in Statistics}.
\newblock John Wiley \& Sons, Hoboken, New Jersey, 2 edition, 2008.
\newblock ISBN 978-0-471-75498-5.

\bibitem{Robinson:89}
A.~J. Robinson.
\newblock {\em Dynamic Error Propagation Networks}.
\newblock PhD thesis, Trinity Hall and Cambridge University Engineering
  Department, 1989.

\bibitem{RobinsonFallside:89}
T.~Robinson and F.~Fallside.
\newblock Dynamic reinforcement driven error propagation networks with
  application to game playing.
\newblock In {\em Proceedings of the 11th Conference of the Cognitive Science
  Society, Ann Arbor}, pages 836--843, 1989.

\bibitem{Romoff:18}
J.~Romoff, A.~Pich{\'{e}}, P.~Henderson, V.~Francois-Lavet, and J.~Pineau.
\newblock Reward estimation for variance reduction in deep reinforcement
  learning.
\newblock {\em ArXiv}, 2018.

\bibitem{Rudelson:10}
M.~Rudelson and R.~Vershynin.
\newblock Non-asymptotic theory of random matrices: extreme singular values.
\newblock {\em ArXiv}, 2010.

\bibitem{Rummery:94}
G.~A. Rummery and M.~Niranjan.
\newblock On-line $q$-learning using connectionist systems.
\newblock Technical Report TR 166, Cambridge University Engineering Department,
  1994.

\bibitem{Sahni:18}
H.~Sahni.
\newblock Reinforcement learning never worked, and 'deep' only helped a bit.
\newblock
  \url{himanshusahni.github.io/2018/02/23/reinforcement-learning-never-worked.html},
  2018.

\bibitem{Sak:14}
H.~Sak, A.~Senior, and F.~Beaufays.
\newblock Long short-term memory recurrent neural network architectures for
  large scale acoustic modeling.
\newblock In {\em Proc. 15th Annual Conf. of the Int. Speech Communication
  Association (INTERSPEECH 2014)}, pages 338--342, Singapore, September 2014.

\bibitem{Schaal:99}
S.~Schaal.
\newblock Is imitation learning the route to humanoid robots?
\newblock {\em Trends in Cognitive Sciences}, 3(6):233--242, 1999.

\bibitem{Schaul:15}
T.~Schaul, J.~Quan, I.~Antonoglou, and D.~Silver.
\newblock Prioritized experience replay.
\newblock {\em ArXiv}, 2015.

\bibitem{Schmidhuber:90diff}
J.~Schmidhuber.
\newblock Making the world differentiable: On using fully recurrent
  self-supervised neural networks for dynamic reinforcement learning and
  planning in non-stationary environments.
\newblock Technical Report FKI-126-90 (revised), Institut f\"{u}r Informatik,
  Technische Universit\"{a}t M\"{u}nchen, 1990.
\newblock Experiments by Sepp Hochreiter.

\bibitem{Schmidhuber:91nips}
J.~Schmidhuber.
\newblock Reinforcement learning in markovian and non-markovian environments.
\newblock In R.~P. Lippmann, J.~E. Moody, and D.~S. Touretzky, editors, {\em
  Advances in Neural Information Processing Systems 3}, pages 500--506. San
  Mateo, CA: Morgan Kaufmann, 1991.
\newblock Pole balancing experiments by Sepp Hochreiter.

\bibitem{Schmidhuber:15}
J.~Schmidhuber.
\newblock Deep learning in neural networks: An overview.
\newblock {\em Neural Networks}, 61:85--117, 2015.

\bibitem{Schulman:15icml}
J.~Schulman, S.~Levine, P.~Moritz, M.~I. Jordan, and P.~Abbeel.
\newblock Trust region policy optimization.
\newblock In {\em 32st International Conference on Machine Learning (ICML)},
  volume~37 of {\em Proceedings of Machine Learning Research}, pages
  1889--1897. PMLR, 2015.

\bibitem{Schulman:15}
J.~Schulman, P.~Moritz, S.~Levine, M.~I. Jordan, and P.~Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock {\em ArXiv}, 2015.
\newblock Fourth International Conference on Learning Representations
  (ICLR'16).

\bibitem{Schulman:17}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em ArXiv}, 2018.

\bibitem{Silver:16}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~van~den Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, S.~Dieleman,
  D.~Grewe, J.~Nham, N.~Kalchbrenner, I.~Sutskever, T.~P. Lillicrap, M.~Leach,
  K.~Kavukcuoglu, T.~Graepel, and D.~Hassabis.
\newblock Mastering the game of {Go} with deep neural networks and tree search.
\newblock {\em Nature}, 529(7587):484--489, 2016.

\bibitem{Silver:17}
D.~Silver, T.~Hubert, J.~Schrittwieser, I.~Antonoglou, M.~Lai, A.~Guez,
  M.~Lanctot, L.~Sifre, D.~Kumaran, T.~Graepel, T.~P. Lillicrap, K.~Simonyan,
  and D.~Hassabis.
\newblock Mastering {Chess} and {Shogi} by self-play with a general
  reinforcement learning algorithm.
\newblock {\em ArXiv}, 2017.

\bibitem{Singh:00}
S.~Singh, T.~Jaakkola, M.~Littman, and C.~Szepesv{\'{a}}ri.
\newblock Convergence results for single-step on-policy reinforcement-learning
  algorithms.
\newblock {\em Machine Learning}, 38:287--308, 2000.

\bibitem{Singh:96}
S.~P. Singh and R.~S. Sutton.
\newblock Reinforcement learning with replacing eligibility traces.
\newblock {\em Machine Learning}, 22:123--158, 1996.

\bibitem{Skinner:58}
B.~F. Skinner.
\newblock Reinforcement today.
\newblock {\em American Psychologist}, 13(3):94--99, 1958.

\bibitem{Sobel:82}
M.~J. Sobel.
\newblock The variance of discounted {Markov} decision processes.
\newblock {\em Journal of Applied Probability}, 19(4):794--802, 1982.

\bibitem{Soshnikov:02}
A.~Soshnikov.
\newblock A note on universality of the distribution of the largest eigenvalues
  in certain sample covariance matrices.
\newblock {\em J. Statist. Phys.}, 108(5-6):1033--1056, 2002.

\bibitem{Srivastava:15}
N.~Srivastava, E.~Mansimov, and R.~Salakhutdinov.
\newblock Unsupervised learning of video representations using {LSTMs}.
\newblock {\em ArXiv}, 2015.

\bibitem{Su:15}
P.-H. Su, D.~Vandyke, M.~Gasic, N.~Mrksic, T.-H. Wen, and S.~Young.
\newblock Reward shaping with recurrent neural networks for speeding up on-line
  policy learning in spoken dialogue systems.
\newblock In {\em Proceedings of the 16th Annual Meeting of the Special
  Interest Group on Discourse and Dialogue}, pages 417--421. Association for
  Computational Linguistics, 2015.

\bibitem{Sundararajan:17}
M.~Sundararajan, A.~Taly, and Q.~Yan.
\newblock Axiomatic attribution for deep networks.
\newblock {\em ArXiv}, 2017.

\bibitem{Sutskever:14nips}
I.~Sutskever, O.~Vinyals, and Q.~V.~V. Le.
\newblock Sequence to sequence learning with neural networks.
\newblock In Z.~Ghahramani, M.~Welling, C.~Cortes, N.~D. Lawrence, and K.~Q.
  Weinberger, editors, {\em Advances in Neural Information Processing Systems
  27 (NIPS'13)}, pages 3104--3112. Curran Associates, Inc., 2014.

\bibitem{Sutton:88td}
R.~S. Sutton.
\newblock Learning to predict by the methods of temporal differences.
\newblock {\em Machine Learning}, 3:9--44, 1988.

\bibitem{Sutton:18book}
R.~S. Sutton and A.~G. Barto.
\newblock {\em Reinforcement Learning: An Introduction}.
\newblock MIT Press, Cambridge, MA, 2 edition, 2018.

\bibitem{Tamar:12}
A.~Tamar, D.~DiCastro, and S.~Mannor.
\newblock Policy gradients with variance related risk criteria.
\newblock In J.~Langford and J.~Pineau, editors, {\em Proceedings of the 29th
  International Conference on Machine Learning (ICML'12)}, 2012.

\bibitem{Tamar:16}
A.~Tamar, D.~DiCastro, and S.~Mannor.
\newblock Learning the variance of the reward-to-go.
\newblock {\em Journal of Machine Learning Research}, 17(13):1--36, 2016.

\bibitem{Chebyshev:67}
P.~Tchebichef.
\newblock Des valeurs moyennes.
\newblock {\em Journal de math{\'{e}}matiques pures et appliqu{\'{e}}es 2},
  12:177--184, 1867.

\bibitem{Tseng:90Journal}
P.~Tseng.
\newblock Solving $h$-horizon, stationary {Markov} decision problems in time
  proportional to $\log(h)$.
\newblock {\em Operations Research Letters}, 9(5):287--297, 1990.

\bibitem{Tsitsiklis:94}
J.~N. Tsitsiklis.
\newblock Asynchronous stochastic approximation and $q$-learning.
\newblock {\em Machine Learning}, 16(3):185--202, 1994.

\bibitem{Hasselt:10}
H.~van Hasselt.
\newblock Double $q$-learning.
\newblock In J.~D. Lafferty, C.~K.~I. Williams, J.~Shawe-Taylor, R.~S. Zemel,
  and A.~Culotta, editors, {\em Advances in Neural Information Processing
  Systems 23}, pages 2613--2621. Curran Associates, Inc., 2010.

\bibitem{Hasselt:16}
H.~van Hasselt, A.~Guez, and D.~Silver.
\newblock Deep reinforcement learning with double $q$-learning.
\newblock In {\em Proceedings of the Thirtieth {AAAI} Conference on Artificial
  Intelligence}, pages 2094--2100. {AAAI} Press, 2016.

\bibitem{Venugopalan:14}
S.~Venugopalan, H.~Xu, J.~Donahue, M.~Rohrbach, R.~J. Mooney, and K.~Saenko.
\newblock Translating videos to natural language using deep recurrent neural
  networks.
\newblock {\em ArXiv}, 2014.

\bibitem{Veretennikov:16}
A.~Veretennikov.
\newblock Ergodic {Markov} processes and poisson equations (lecture notes).
\newblock {\em ArXiv}, 2016.

\bibitem{Wang:15}
Z.~Wang, N.~de~Freitas, and M.~Lanctot.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock {\em ArXiv}, 2015.

\bibitem{Wang:16}
Z.~Wang, T.~Schaul, M.~Hessel, H.~Hasselt, M.~Lanctot, and N.~de~Freitas.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock In M.~F. Balcan and K.~Q. Weinberger, editors, {\em Proceedings of
  the 33rd International Conference on Machine Learning (ICML)}, volume~48 of
  {\em Proceedings of Machine Learning Research}, pages 1995--2003. PMLR, 2016.

\bibitem{Watkins:89}
C.~J. C.~H. Watkins.
\newblock {\em Learning from Delayed Rewards}.
\newblock PhD thesis, King's College, 1989.

\bibitem{Watkins:92}
C.~J. C.~H. Watkins and P.~Dayan.
\newblock {Q-Learning}.
\newblock {\em Machine Learning}, 8:279--292, 1992.

\bibitem{Werbos:90}
P.~J. Werbos.
\newblock A menu of designs for reinforcement learning over time.
\newblock In W.~T. Miller, R.~S. Sutton, and P.~J. Werbos, editors, {\em Neural
  Networks for Control}, pages 67--95. MIT Press, Cambridge, MA, USA, 1990.

\bibitem{Wiewiora:03}
E.~Wiewiora.
\newblock Potential-based shaping and $q$-value initialization are equivalent.
\newblock {\em Journal of Artificial Intelligence Research}, 19:205--208, 2003.

\bibitem{Wiewiora:03icml}
E.~Wiewiora, G.~Cottrell, and C.~Elkan.
\newblock Principled methods for advising reinforcement learning agents.
\newblock In {\em Proceedings of the Twentieth International Conference on
  International Conference on Machine Learning (ICML'03)}, pages 792--799,
  2003.

\bibitem{Williams:92}
R.~J. Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine Learning}, 8(3):229--256, 1992.

\bibitem{Zaremba:14arxiva}
W.~Zaremba, I.~Sutskever, and O.~Vinyals.
\newblock Recurrent neural network regularization.
\newblock {\em ArXiv}, 2014.

\bibitem{Zhang:16}
J.~Zhang, Z.~L. Lin, J.~Brandt, X.~Shen, and S.~Sclaroff.
\newblock Top-down neural attention by excitation backprop.
\newblock In {\em Proceedings of the 14th European Conference on Computer
  Vision (ECCV)}, pages 543--559, 2016.
\newblock part IV.

\end{thebibliography}
