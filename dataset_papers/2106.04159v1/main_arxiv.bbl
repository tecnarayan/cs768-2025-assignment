\begin{thebibliography}{10}

\bibitem{agarwal2012distributed}
Alekh Agarwal and John~C Duchi.
\newblock Distributed delayed stochastic optimization.
\newblock In {\em 2012 IEEE 51st IEEE Conference on Decision and Control
  (CDC)}, pages 5451--5452. IEEE, 2012.

\bibitem{agarwal2009information}
Alekh Agarwal, Martin~J Wainwright, Peter Bartlett, and Pradeep Ravikumar.
\newblock Information-theoretic lower bounds on the oracle complexity of convex
  optimization.
\newblock {\em Advances in Neural Information Processing Systems}, 22:1--9,
  2009.

\bibitem{arjevani2019lower}
Yossi Arjevani, Yair Carmon, John~C. Duchi, Dylan~J. Foster, Nathan Srebro, and
  Blake Woodworth.
\newblock Lower bounds for non-convex stochastic optimization, 2019.

\bibitem{arjevani2020tight}
Yossi Arjevani, Ohad Shamir, and Nathan Srebro.
\newblock A tight convergence analysis for stochastic gradient descent with
  delayed updates.
\newblock In {\em Algorithmic Learning Theory}, pages 111--132. PMLR, 2020.

\bibitem{aytekin2016analysis}
Arda Aytekin, Hamid~Reza Feyzmahdavian, and Mikael Johansson.
\newblock Analysis and implementation of an asynchronous optimization algorithm
  for the parameter server.
\newblock {\em arXiv preprint arXiv:1610.05507}, 2016.

\bibitem{basu2019qsparse}
Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi.
\newblock Qsparse-local-sgd: Distributed sgd with quantization, sparsification,
  and local computations.
\newblock {\em arXiv preprint arXiv:1906.02367}, 2019.

\bibitem{MLSYS2019_bd686fd6}
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
  Ingerman, Vladimir Ivanov, Chlo\'{e} Kiddon, Jakub Kone\v{c}n\'{y}, Stefano
  Mazzocchi, Brendan McMahan, Timon Van~Overveldt, David Petrou, Daniel Ramage,
  and Jason Roselander.
\newblock Towards federated learning at scale: System design.
\newblock In A.~Talwalkar, V.~Smith, and M.~Zaharia, editors, {\em Proceedings
  of Machine Learning and Systems}, volume~1, pages 374--388, 2019.

\bibitem{bubeck2013bandits}
S{\'e}bastien Bubeck, Nicolo Cesa-Bianchi, and G{\'a}bor Lugosi.
\newblock Bandits with heavy tail.
\newblock {\em IEEE Transactions on Information Theory}, 59(11):7711--7717,
  2013.

\bibitem{carmon2018accelerated}
Yair Carmon, John~C Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Accelerated methods for nonconvex optimization.
\newblock {\em SIAM Journal on Optimization}, 28(2):1751--1772, 2018.

\bibitem{eichner2019semi}
Hubert Eichner, Tomer Koren, Brendan McMahan, Nathan Srebro, and Kunal Talwar.
\newblock Semi-cyclic stochastic gradient descent.
\newblock In {\em International Conference on Machine Learning}, pages
  1764--1773. PMLR, 2019.

\bibitem{feyzmahdavian2016asynchronous}
Hamid~Reza Feyzmahdavian, Arda Aytekin, and Mikael Johansson.
\newblock An asynchronous mini-batch algorithm for regularized stochastic
  optimization.
\newblock {\em IEEE Transactions on Automatic Control}, 61(12):3740--3754,
  2016.

\bibitem{ghadimi2012optimal}
Saeed Ghadimi and Guanghui Lan.
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization i: A generic algorithmic framework.
\newblock {\em SIAM Journal on Optimization}, 22(4):1469--1492, 2012.

\bibitem{glasgow2020asynchronous}
Margalit Glasgow and Mary Wootters.
\newblock Asynchronous distributed optimization with stochastic delays.
\newblock {\em arXiv preprint arXiv:2009.10717}, 2020.

\bibitem{jin2017escape}
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham~M Kakade, and Michael~I Jordan.
\newblock How to escape saddle points efficiently.
\newblock In {\em International Conference on Machine Learning}, pages
  1724--1732. PMLR, 2017.

\bibitem{kairouz2019advances}
Peter Kairouz, H~Brendan McMahan, Brendan Avent, Aur{\'e}lien Bellet, Mehdi
  Bennis, Arjun~Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
  Rachel Cummings, et~al.
\newblock Advances and open problems in federated learning.
\newblock {\em arXiv preprint arXiv:1912.04977}, 2019.

\bibitem{pmlr-v119-karimireddy20a}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
  Stich, and Ananda~Theertha Suresh.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In {\em International Conference on Machine Learning}, pages
  5132--5143. PMLR, 2020.

\bibitem{khaled2020tighter}
Ahmed Khaled, Konstantin Mishchenko, and Peter Richt{\'a}rik.
\newblock Tighter theory for local sgd on identical and heterogeneous data.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 4519--4529. PMLR, 2020.

\bibitem{konevcny2015federated}
Jakub Kone{\v{c}}n{\`y}, Brendan McMahan, and Daniel Ramage.
\newblock Federated optimization: Distributed optimization beyond the
  datacenter.
\newblock {\em arXiv preprint arXiv:1511.03575}, 2015.

\bibitem{konevcny2016federated}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Daniel Ramage, and Peter
  Richt{\'a}rik.
\newblock Federated optimization: Distributed machine learning for on-device
  intelligence.
\newblock {\em arXiv preprint arXiv:1610.02527}, 2016.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{lecun2015lenet}
Yann LeCun et~al.
\newblock Lenet-5, convolutional neural networks.
\newblock {\em URL: http://yann. lecun. com/exdb/lenet}, 20(5):14, 2015.

\bibitem{MLSYS2020_38af8613}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
  Virginia Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock In I.~Dhillon, D.~Papailiopoulos, and V.~Sze, editors, {\em
  Proceedings of Machine Learning and Systems}, volume~2, pages 429--450, 2020.

\bibitem{Li2020On}
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.
\newblock On the convergence of fedavg on non-iid data.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{NIPS2015_452bf208}
Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji~Liu.
\newblock Asynchronous parallel stochastic gradient for nonconvex optimization.
\newblock In C.~Cortes, N.~Lawrence, D.~Lee, M.~Sugiyama, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems}, volume~28.
  Curran Associates, Inc., 2015.

\bibitem{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem{nemirovskij1983problem}
Arkadij~Semenovi{\v{c}} Nemirovskij and David~Borisovich Yudin.
\newblock Problem complexity and method efficiency in optimization.
\newblock 1983.

\bibitem{reisizadeh2020straggler}
Amirhossein Reisizadeh, Isidoros Tziotis, Hamed Hassani, Aryan Mokhtari, and
  Ramtin Pedarsani.
\newblock Straggler-resilient federated learning: Leveraging the interplay
  between statistical accuracy and system heterogeneity.
\newblock {\em arXiv preprint arXiv:2012.14453}, 2020.

\bibitem{ruan2021towards}
Yichen Ruan, Xiaoxi Zhang, Shu-Che Liang, and Carlee Joe-Wong.
\newblock Towards flexible device participation in federated learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 3403--3411. PMLR, 2021.

\bibitem{sattler2019robust}
Felix Sattler, Simon Wiedemann, Klaus-Robert M{\"u}ller, and Wojciech Samek.
\newblock Robust and communication-efficient federated learning from non-iid
  data.
\newblock {\em IEEE transactions on neural networks and learning systems},
  31(9):3400--3413, 2019.

\bibitem{smith2017federated}
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet Talwalkar.
\newblock Federated multi-task learning.
\newblock {\em arXiv preprint arXiv:1705.10467}, 2017.

\bibitem{stich2020error}
Sebastian~U Stich and Sai~Praneeth Karimireddy.
\newblock The error-feedback framework: Better rates for sgd with delayed
  gradients and compressed updates.
\newblock {\em Journal of Machine Learning Research}, 21:1--36, 2020.

\bibitem{stich2019local}
Sebastian~Urban Stich.
\newblock Local sgd converges fast and communicates little.
\newblock In {\em ICLR 2019-International Conference on Learning
  Representations}, number CONF, 2019.

\bibitem{vershynin2018high}
Roman Vershynin.
\newblock {\em High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem{wang2019cooperative}
Jianyu Wang and Gauri Joshi.
\newblock Cooperative sgd: A unified framework for the design and analysis of
  communication-efficient sgd algorithms.
\newblock In {\em ICML Workshop on Coding Theory for Machine Learning}, 2019.

\bibitem{wang2020tackling}
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H~Vincent Poor.
\newblock Tackling the objective inconsistency problem in heterogeneous
  federated optimization.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{xie2019asynchronous}
Cong Xie, Sanmi Koyejo, and Indranil Gupta.
\newblock Asynchronous federated optimization.
\newblock {\em arXiv preprint arXiv:1903.03934}, 2019.

\bibitem{yan2020distributed}
Yikai Yan, Chaoyue Niu, Yucheng Ding, Zhenzhe Zheng, Fan Wu, Guihai Chen,
  Shaojie Tang, and Zhihua Wu.
\newblock Distributed non-convex optimization with sublinear speedup under
  intermittent client availability.
\newblock {\em arXiv preprint arXiv:2002.07399}, 2020.

\bibitem{yang2021achieving}
Haibo Yang, Minghong Fang, and Jia Liu.
\newblock Achieving linear speedup with partial worker participation in non-iid
  federated learning.
\newblock {\em arXiv preprint arXiv:2101.11203}, 2021.

\bibitem{yu2019linear}
Hao Yu, Rong Jin, and Sen Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  sgd for distributed non-convex optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  7184--7193. PMLR, 2019.

\bibitem{zhang2020adaptive}
Jingzhao Zhang, Sai~Praneeth Karimireddy, Andreas Veit, Seungyeon Kim,
  Sashank~J Reddi, Sanjiv Kumar, and Suvrit Sra.
\newblock Why are adaptive methods good for attention models?, 2020.

\bibitem{zhang2020taming}
Xin Zhang, Jia Liu, and Zhengyuan Zhu.
\newblock Taming convergence for asynchronous stochastic gradient descent with
  unbounded delay in non-convex learning.
\newblock In {\em 2020 59th IEEE Conference on Decision and Control (CDC)},
  pages 3580--3585. IEEE, 2020.

\end{thebibliography}
