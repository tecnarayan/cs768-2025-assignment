\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro, Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia, Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah, Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan, Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and Zheng]{tensorflow2015-whitepaper}
Mart\'{\i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg~S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man\'{e}, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\'{e}gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems, 2015.
\newblock URL \url{https://www.tensorflow.org/}.
\newblock Software available from tensorflow.org.

\bibitem[Ahn et~al.(2020)Ahn, Yun, and Sra]{2020Ahn}
Kwangjun Ahn, Chulhee Yun, and Suvrit Sra.
\newblock Sgd with shuffling: optimal rates without component convexity and large epoch requirements.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin, editors, \emph{Advances in Neural Information Processing Systems}, volume~33, pages 17526--17535. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/file/cb8acb1dc9821bf74e6ca9068032d623-Paper.pdf}.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{pmlr-v97-allen-zhu19a}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, \emph{Proceedings of the 36th International Conference on Machine Learning}, volume~97 of \emph{Proceedings of Machine Learning Research}, pages 242--252. PMLR, 09--15 Jun 2019.
\newblock URL \url{http://proceedings.mlr.press/v97/allen-zhu19a.html}.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{pmlr-v97-arora19a}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, \emph{Proceedings of the 36th International Conference on Machine Learning}, volume~97 of \emph{Proceedings of Machine Learning Research}, pages 322--332. PMLR, 09--15 Jun 2019.
\newblock URL \url{http://proceedings.mlr.press/v97/arora19a.html}.

\bibitem[Beznosikov and Tak{\'a}{\v{c}}(2021)]{beznosikov2021random}
Aleksandr Beznosikov and Martin Tak{\'a}{\v{c}}.
\newblock Random-reshuffled sarah does not need a full gradient computations.
\newblock \emph{arXiv preprint arXiv:2111.13322}, 2021.

\bibitem[Bjorck et~al.(2021)Bjorck, Kabra, Weinberger, and Gomes]{aaai_2021_Bjorck}
Johan Bjorck, Anmol Kabra, Kilian~Q. Weinberger, and Carla Gomes.
\newblock Characterizing the loss landscape in non-negative matrix factorization.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 35\penalty0 (8):\penalty0 6768--6776, May 2021.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/16836}.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and Nocedal]{Bottou2018}
L.~Bottou, F.~E. Curtis, and J.~Nocedal.
\newblock {O}ptimization {M}ethods for {L}arge-{S}cale {M}achine {L}earning.
\newblock \emph{SIAM Rev.}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Bottou(2009)]{bottou2009curiously}
L{\'e}on Bottou.
\newblock Curiously fast convergence of some stochastic gradient descent algorithms, 2009.

\bibitem[Brutzkus et~al.(2018)Brutzkus, Globerson, Malach, and Shalev-Shwartz]{brutzkus2017sgd}
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz.
\newblock {SGD} learns over-parameterized networks that provably generalize on linearly separable data.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=rJ33wwxRb}.

\bibitem[Chollet et~al.(2015)]{chollet2015keras}
Francois Chollet et~al.
\newblock Keras.
\newblock \emph{GitHub}, 2015.
\newblock URL \url{https://github.com/fchollet/keras}.

\bibitem[Collobert and Weston(2008)]{icml_Collobert}
Ronan Collobert and Jason Weston.
\newblock A unified architecture for natural language processing: deep neural networks with multitask learning.
\newblock In William~W. Cohen, Andrew McCallum, and Sam~T. Roweis, editors, \emph{Machine Learning, Proceedings of the Twenty-Fifth International Conference {(ICML} 2008), Helsinki, Finland, June 5-9, 2008}, volume 307 of \emph{{ACM} International Conference Proceeding Series}, pages 160--167. {ACM}, 2008.
\newblock \doi{10.1145/1390156.1390177}.
\newblock URL \url{https://doi.org/10.1145/1390156.1390177}.

\bibitem[De et~al.(2017)De, Yadav, Jacobs, and Goldstein]{pmlr-v54-de17a}
Soham De, Abhay Yadav, David Jacobs, and Tom Goldstein.
\newblock {Automated Inference with Adaptive Batches}.
\newblock In Aarti Singh and Jerry Zhu, editors, \emph{Proceedings of the 20th International Conference on Artificial Intelligence and Statistics}, volume~54 of \emph{Proceedings of Machine Learning Research}, pages 1504--1513. PMLR, 20--22 Apr 2017.
\newblock URL \url{https://proceedings.mlr.press/v54/de17a.html}.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and Lacoste-Julien]{SAGA}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 1646--1654, 2014.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and Zhai]{pmlr-v97-du19c}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, \emph{Proceedings of the 36th International Conference on Machine Learning}, volume~97 of \emph{Proceedings of Machine Learning Research}, pages 1675--1685. PMLR, 09--15 Jun 2019{\natexlab{a}}.
\newblock URL \url{http://proceedings.mlr.press/v97/du19c.html}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos, and Singh]{du2019gradient}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=S1eK3i09YQ}.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{AdaGrad}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic optimization.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2121--2159, 2011.

\bibitem[Efron et~al.(2004)Efron, Hastie, Johnstone, and Tibshirani]{diabetes}
Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani.
\newblock Diabetes dataset, 2004.
\newblock URL \url{https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html}.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013stochastic}
S.~Ghadimi and G.~Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic programming.
\newblock \emph{SIAM J. Optim.}, 23\penalty0 (4):\penalty0 2341--2368, 2013.

\bibitem[Goldberg et~al.(2018)Goldberg, Hirst, Liu, and Zhang]{coling_Goldberg}
Yoav Goldberg, Graeme Hirst, Yang Liu, and Meng Zhang.
\newblock Neural network methods for natural language processing.
\newblock \emph{Comput. Linguistics}, 44\penalty0 (1), 2018.
\newblock \doi{10.1162/COLI\_r\_00312}.
\newblock URL \url{https://doi.org/10.1162/COLI\_r\_00312}.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Bulatov, Ibarz, Arnoud, and Shet]{iclrGoodfellow}
Ian~J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, and Vinay~D. Shet.
\newblock Multi-digit number recognition from street view imagery using deep convolutional neural networks.
\newblock In Yoshua Bengio and Yann LeCun, editors, \emph{2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings}, 2014.
\newblock URL \url{http://arxiv.org/abs/1312.6082}.

\bibitem[Gower et~al.(2021)Gower, Sebbouh, and Loizou]{gower2021sgd}
Robert Gower, Othmane Sebbouh, and Nicolas Loizou.
\newblock Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 1315--1323. PMLR, 2021.

\bibitem[Gürbüzbalaban et~al.(2019)Gürbüzbalaban, Ozdaglar, and Parrilo]{Gurbuzbalaban2019}
M.~Gürbüzbalaban, A.~Ozdaglar, and P.~A. Parrilo.
\newblock Why random reshuffling beats stochastic gradient descent.
\newblock \emph{Mathematical Programming}, Oct 2019.
\newblock ISSN 1436-4646.
\newblock \doi{10.1007/s10107-019-01440-w}.
\newblock URL \url{http://dx.doi.org/10.1007/s10107-019-01440-w}.

\bibitem[Haochen and Sra(2019)]{haochen2019random}
Jeff Haochen and Suvrit Sra.
\newblock Random shuffling beats sgd after finite epochs.
\newblock In \emph{International Conference on Machine Learning}, pages 2624--2633. PMLR, 2019.

\bibitem[Hardt et~al.(2018)Hardt, Ma, and Recht]{JMLR-hardtGDquasar}
Moritz Hardt, Tengyu Ma, and Benjamin Recht.
\newblock Gradient descent learns linear dynamical systems.
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0 (29):\penalty0 1--44, 2018.
\newblock URL \url{http://jmlr.org/papers/v19/16-465.html}.

\bibitem[He and Sun(2015)]{cvpr_He015}
Kaiming He and Jian Sun.
\newblock Convolutional neural networks at constrained time cost.
\newblock In \emph{{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2015, Boston, MA, USA, June 7-12, 2015}, pages 5353--5360. {IEEE} Computer Society, 2015.
\newblock \doi{10.1109/CVPR.2015.7299173}.
\newblock URL \url{https://doi.org/10.1109/CVPR.2015.7299173}.

\bibitem[Hinder et~al.(2020)Hinder, Sidford, and Sohoni]{hinder2020near}
Oliver Hinder, Aaron Sidford, and Nimit Sohoni.
\newblock Near-optimal methods for minimizing star-convex functions and beyond.
\newblock In \emph{Conference on Learning Theory}, pages 1894--1938. PMLR, 2020.

\bibitem[Jin(2020)]{jin2020convergence}
Jikai Jin.
\newblock On the convergence of first order methods for quasar-convex optimization.
\newblock \emph{arXiv preprint arXiv:2010.04937}, 2020.

\bibitem[Johnson and Zhang(2013)]{SVRG}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance reduction.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 315--323, 2013.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{polyak_condition}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under the {P}olyak-{{\L}}ojasiewicz condition.
\newblock In Paolo Frasconi, Niels Landwehr, Giuseppe Manco, and Jilles Vreeken, editors, \emph{Machine Learning and Knowledge Discovery in Databases}, pages 795--811, Cham, 2016. Springer International Publishing.

\bibitem[Khaled and Richt{\'a}rik(2020)]{khaled2020better}
Ahmed Khaled and Peter Richt{\'a}rik.
\newblock Better theory for sgd in the nonconvex world.
\newblock \emph{arXiv preprint arXiv:2002.03329}, 2020.

\bibitem[Kingma and Ba(2014)]{KingmaB14}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{CoRR}, abs/1412.6980, 2014.

\bibitem[Le~Roux et~al.(2012)Le~Roux, Schmidt, and Bach]{SAG}
Nicolas Le~Roux, Mark Schmidt, and Francis Bach.
\newblock A stochastic gradient method with an exponential convergence rate for finite training sets.
\newblock In \emph{NIPS}, pages 2663--2671, 2012.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{MNIST}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0 2278--2324, 1998.

\bibitem[Lee and Valiant(2016)]{ieee_lee_starconvex}
Jasper~C.H. Lee and Paul Valiant.
\newblock Optimizing star-convex functions.
\newblock In \emph{2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS)}, pages 603--614, 2016.
\newblock \doi{10.1109/FOCS.2016.71}.

\bibitem[Loizou et~al.(2021)Loizou, Vaswani, Hadj~Laradji, and Lacoste-Julien]{pmlr-v130-loizou21a}
Nicolas Loizou, Sharan Vaswani, Issam Hadj~Laradji, and Simon Lacoste-Julien.
\newblock Stochastic polyak step-size for sgd: An adaptive learning rate for fast convergence.
\newblock In Arindam Banerjee and Kenji Fukumizu, editors, \emph{Proceedings of The 24th International Conference on Artificial Intelligence and Statistics}, volume 130 of \emph{Proceedings of Machine Learning Research}, pages 1306--1314. PMLR, 13--15 Apr 2021.
\newblock URL \url{http://proceedings.mlr.press/v130/loizou21a.html}.

\bibitem[Ma et~al.(2018)Ma, Bassily, and Belkin]{pmlr-v80-ma18a}
Siyuan Ma, Raef Bassily, and Mikhail Belkin.
\newblock The power of interpolation: Understanding the effectiveness of {SGD} in modern over-parametrized learning.
\newblock In Jennifer Dy and Andreas Krause, editors, \emph{Proceedings of the 35th International Conference on Machine Learning}, volume~80 of \emph{Proceedings of Machine Learning Research}, pages 3325--3334. PMLR, 10--15 Jul 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/ma18a.html}.

\bibitem[Meng et~al.(2020)Meng, Vaswani, Laradji, Schmidt, and Lacoste-Julien]{pmlr-v108-meng20a}
Si~Yi Meng, Sharan Vaswani, Issam~Hadj Laradji, Mark Schmidt, and Simon Lacoste-Julien.
\newblock Fast and furious convergence: Stochastic second order methods under interpolation.
\newblock In Silvia Chiappa and Roberto Calandra, editors, \emph{Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics}, volume 108 of \emph{Proceedings of Machine Learning Research}, pages 1375--1386. PMLR, 26--28 Aug 2020.
\newblock URL \url{http://proceedings.mlr.press/v108/meng20a.html}.

\bibitem[Mishchenko et~al.(2020)Mishchenko, Khaled Ragab~Bayoumi, and Richt{\'a}rik]{mishchenko2020random}
Konstantin Mishchenko, Ahmed Khaled Ragab~Bayoumi, and Peter Richt{\'a}rik.
\newblock Random reshuffling: Simple analysis with vast improvements.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Nagaraj et~al.(2019)Nagaraj, Jain, and Netrapalli]{nagaraj2019sgd}
Dheeraj Nagaraj, Prateek Jain, and Praneeth Netrapalli.
\newblock Sgd without replacement: Sharper rates for general smooth convex functions.
\newblock In \emph{International Conference on Machine Learning}, pages 4703--4711, 2019.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and Shapiro]{Nemirovski2009}
A.~Nemirovski, A.~Juditsky, G.~Lan, and A.~Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM J. on Optimization}, 19\penalty0 (4):\penalty0 1574--1609, 2009.

\bibitem[Nesterov(2004)]{nesterov2004}
Yurii Nesterov.
\newblock \emph{Introductory lectures on convex optimization : a basic course}.
\newblock Applied optimization. Kluwer Academic Publ., Boston, Dordrecht, London, 2004.
\newblock ISBN 1-4020-7553-7.

\bibitem[Nesterov and Polyak(2006)]{nesterov2006cubic}
Yurii Nesterov and Boris~T Polyak.
\newblock Cubic regularization of {N}ewton method and its global performance.
\newblock \emph{Mathematical Programming}, 108\penalty0 (1):\penalty0 177--205, 2006.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and Tak{\'a}{\v{c}}]{Nguyen2017sarah}
Lam~M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}{\v{c}}.
\newblock Sarah: A novel method for machine learning problems using stochastic recursive gradient.
\newblock In \emph{Proceedings of the 34th International Conference on Machine Learning-Volume 70}, pages 2613--2621. JMLR. org, 2017.

\bibitem[Nguyen et~al.(2021)Nguyen, Tran-Dinh, Phan, Nguyen, and van Dijk]{nguyen2020unified}
Lam~M. Nguyen, Quoc Tran-Dinh, Dzung~T. Phan, Phuong~Ha Nguyen, and Marten van Dijk.
\newblock A unified convergence analysis for shuffling-type gradient methods.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0 (207):\penalty0 1--44, 2021.
\newblock URL \url{http://jmlr.org/papers/v22/20-1238.html}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pages 8024--8035. Curran Associates, Inc., 2019.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay]{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel, M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos, D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2825--2830, 2011.

\bibitem[Polyak(1964)]{Polyak1964}
Boris~T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{{USSR} Computational Mathematics and Mathematical Physics}, 4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Repository(2016)]{expectancy}
Global Health Observatory~Data Repository.
\newblock Life expectancy and healthy life expectancy, 2016.
\newblock URL \url{https://apps.who.int/gho/data/view.main.SDG2016LEXREGv?lang=en}.

\bibitem[Repository(1997)]{california}
StatLib Repository.
\newblock California housing, 1997.
\newblock URL \url{https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html}.

\bibitem[Robbins and Monro(1951)]{RM1951}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0 (3):\penalty0 400--407, 1951.

\bibitem[Safran and Shamir(2020)]{safran2020good}
Itay Safran and Ohad Shamir.
\newblock How good is sgd with random shuffling?
\newblock In \emph{Conference on Learning Theory}, pages 3250--3284. PMLR, 2020.

\bibitem[Sankararaman et~al.(2020)Sankararaman, De, Xu, Huang, and Goldstein]{sankararaman2020impact}
Karthik~Abinav Sankararaman, Soham De, Zheng Xu, W~Ronny Huang, and Tom Goldstein.
\newblock The impact of neural network overparameterization on gradient confusion and stochastic gradient descent.
\newblock In \emph{International conference on machine learning}, pages 8469--8479. PMLR, 2020.

\bibitem[Schmidt and Roux(2013)]{schmidt2013fast}
Mark Schmidt and Nicolas~Le Roux.
\newblock Fast convergence of stochastic gradient descent under a strong growth condition, 2013.

\bibitem[Shamir and Zhang(2013)]{pmlr-v28-shamir13}
Ohad Shamir and Tong Zhang.
\newblock Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes.
\newblock In Sanjoy Dasgupta and David McAllester, editors, \emph{Proceedings of the 30th International Conference on Machine Learning}, volume~28 of \emph{Proceedings of Machine Learning Research}, pages 71--79, Atlanta, Georgia, USA, 17--19 Jun 2013. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v28/shamir13.html}.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{J. Mach. Learn. Res.}, 19\penalty0 (1):\penalty0 2822–2878, January 2018.
\newblock ISSN 1532-4435.

\bibitem[Tran et~al.(2021)Tran, Nguyen, and Tran-Dinh]{smg_tran21b}
Trang~H Tran, Lam~M Nguyen, and Quoc Tran-Dinh.
\newblock {SMG}: A shuffling gradient-based method with momentum.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the 38th International Conference on Machine Learning}, volume 139 of \emph{Proceedings of Machine Learning Research}, pages 10379--10389. PMLR, 18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/tran21b.html}.

\bibitem[Tran et~al.(2022)Tran, Scheinberg, and Nguyen]{Tran2022_ShufflingNesterov}
Trang~H Tran, Katya Scheinberg, and Lam~M Nguyen.
\newblock {N}esterov accelerated shuffling gradient method for convex optimization.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, \emph{Proceedings of the 39th International Conference on Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning Research}, pages 21703--21732. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/tran22a.html}.

\bibitem[Vaswani et~al.(2019)Vaswani, Bach, and Schmidt]{pmlr-v89-vaswani19a}
Sharan Vaswani, Francis Bach, and Mark Schmidt.
\newblock Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron.
\newblock In Kamalika Chaudhuri and Masashi Sugiyama, editors, \emph{Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics}, volume~89 of \emph{Proceedings of Machine Learning Research}, pages 1195--1204. PMLR, 16--18 Apr 2019.
\newblock URL \url{https://proceedings.mlr.press/v89/vaswani19a.html}.

\bibitem[Zhou et~al.(2019)Zhou, Yang, Zhang, Liang, and Tarokh]{zhou2019sgd}
Yi~Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang, and Vahid Tarokh.
\newblock Sgd converges to global minimum in deep learning via star-convex path.
\newblock \emph{arXiv preprint arXiv:1901.00451}, 2019.

\bibitem[Zou and Gu(2019)]{ZouG19nips}
Difan Zou and Quanquan Gu.
\newblock An improved analysis of training over-parameterized deep neural networks.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett, editors, \emph{Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada}, pages 2053--2062, 2019.

\bibitem[Zou et~al.(2018)Zou, Cao, Zhou, and Gu]{zou2018stochastic}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu networks, 2018.

\end{thebibliography}
