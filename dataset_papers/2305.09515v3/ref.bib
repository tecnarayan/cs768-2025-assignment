% DDIM
@inproceedings{DBLP:conf/iclr/SongME21,
  author       = {Jiaming Song and
                  Chenlin Meng and
                  Stefano Ermon},
  title        = {Denoising Diffusion Implicit Models},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=St1giarCHLP}
}


% commongen
@inproceedings{lin-etal-2020-commongen,
    title = "{C}ommon{G}en: A Constrained Text Generation Challenge for Generative Commonsense Reasoning",
    author = "Lin, Bill Yuchen  and
      Zhou, Wangchunshu  and
      Shen, Ming  and
      Zhou, Pei  and
      Bhagavatula, Chandra  and
      Choi, Yejin  and
      Ren, Xiang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.165",
    doi = "10.18653/v1/2020.findings-emnlp.165",
    pages = "1823--1840",
    abstract = "Recently, large-scale pre-trained language models have demonstrated impressive performance on several commonsense-reasoning benchmark datasets. However, building machines with commonsense to compose realistically plausible sentences remains challenging. In this paper, we present a constrained text generation task, CommonGen associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts (e.g., dog, frisbee, catch, throw); the task is to generate a coherent sentence describing an everyday scenario using these concepts (e.g., {``}a man throws a frisbee and his dog catches it{''}). The CommonGen task is challenging because it inherently requires 1) relational reasoning with background commonsense knowledge and 2) compositional generalization ability to work on unseen concept combinations. Our dataset, constructed through a combination of crowdsourced and existing caption corpora, consists of 77k commonsense descriptions over 35k unique concept-sets. Experiments show that there is a large gap between state-of-the-art text generation models (e.g., T5) and human performance (31.6{\%} v.s. 63.5{\%} in SPICE metric). Furthermore, we demonstrate that the learned generative commonsense reasoning capability can be transferred to improve downstream tasks such as CommonsenseQA (76.9{\%} to 78.4 in dev accuracy) by generating additional context.",
}

% GPT3
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

% xsum
@inproceedings{narayan-etal-2018-dont,
    title = "Don{'}t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",
    author = "Narayan, Shashi  and
      Cohen, Shay B.  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1206",
    doi = "10.18653/v1/D18-1206",
    pages = "1797--1807",
    abstract = "We introduce {``}extreme summarization{''}, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question {``}What is the article about?{''}. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article{'}s topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.",
}

% cnndm
@inproceedings{NIPS2015_afdec700,
 author = {Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Teaching Machines to Read and Comprehend},
 url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf},
 volume = {28},
 year = {2015}
}


%iwslt14
@inproceedings{cettolo-etal-2014-report,
    title = "Report on the 11th {IWSLT} evaluation campaign",
    author = {Cettolo, Mauro  and
      Niehues, Jan  and
      St{\"u}ker, Sebastian  and
      Bentivogli, Luisa  and
      Federico, Marcello},
    booktitle = "Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 4-5",
    year = "2014",
    address = "Lake Tahoe, California",
    url = "https://aclanthology.org/2014.iwslt-evaluation.1",
    pages = "2--17",
    abstract = "The paper overviews the 11th evaluation campaign organized by the IWSLT workshop. The 2014 evaluation offered multiple tracks on lecture transcription and translation based on the TED Talks corpus. In particular, this year IWSLT included three automatic speech recognition tracks, on English, German and Italian, five speech translation tracks, from English to French, English to German, German to English, English to Italian, and Italian to English, and five text translation track, also from English to French, English to German, German to English, English to Italian, and Italian to English. In addition to the official tracks, speech and text translation optional tracks were offered, globally involving 12 other languages: Arabic, Spanish, Portuguese (B), Hebrew, Chinese, Polish, Persian, Slovenian, Turkish, Dutch, Romanian, Russian. Overall, 21 teams participated in the evaluation, for a total of 76 primary runs submitted. Participants were also asked to submit runs on the 2013 test set (progress test set), in order to measure the progress of systems with respect to the previous year. All runs were evaluated with objective metrics, and submissions for two of the official text translation tracks were also evaluated with human post-editing.",
}

% transformer
@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

% dinoiser
@misc{ye2023dinoiser,
      title={DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises}, 
      author={Jiasheng Ye and Zaixiang Zheng and Yu Bao and Lihua Qian and Mingxuan Wang},
      year={2023},
      eprint={2302.10025},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% MBR
@techreport{kumar2004minimum,
  title={Minimum bayes-risk decoding for statistical machine translation},
  author={Kumar, Shankar and Byrne, William},
  year={2004},
  institution={JOHNS HOPKINS UNIV BALTIMORE MD CENTER FOR LANGUAGE AND SPEECH PROCESSING (CLSP)}
}

% seqdiffuseq
@misc{yuan2022seqdiffuseq,
      title={SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers}, 
      author={Hongyi Yuan and Zheng Yuan and Chuanqi Tan and Fei Huang and Songfang Huang},
      year={2022},
      eprint={2212.10325},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% bpe
@inproceedings{kudo-richardson-2018-sentencepiece,
    title = "{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
    author = "Kudo, Taku  and
      Richardson, John",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-2012",
    doi = "10.18653/v1/D18-2012",
    pages = "66--71",
    abstract = "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at \url{https://github.com/google/sentencepiece}.",
}

% bRNN-CopyNet
@inproceedings{GuLLL16,
  author    = {Jiatao Gu and
               Zhengdong Lu and
               Hang Li and
               Victor O. K. Li},
  title     = {Incorporating Copying Mechanism in Sequence-to-Sequence Learning},
  booktitle = {{ACL} {(1)}},
  publisher = {The Association for Computer Linguistics},
  year      = {2016}
}

% ConstLeven
@inproceedings{SusantoCT20,
  author    = {Raymond Hendy Susanto and
               Shamil Chollampatt and
               Liling Tan},
  title     = {Lexically Constrained Neural Machine Translation with Levenshtein
               Transformer},
  booktitle = {{ACL}},
  pages     = {3536--3543},
  publisher = {Association for Computational Linguistics},
  year      = {2020}
}

% NAT
@article{gu2017non,
  title={Non-autoregressive neural machine translation},
  author={Gu, Jiatao and Bradbury, James and Xiong, Caiming and Li, Victor OK and Socher, Richard},
  journal={arXiv preprint arXiv:1711.02281},
  year={2017}
}

% iNAT
@article{lee2018deterministic,
  title={Deterministic non-autoregressive neural sequence modeling by iterative refinement},
  author={Lee, Jason and Mansimov, Elman and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:1802.06901},
  year={2018}
}

% LevT
@inproceedings{gu2019levenshtein,
  title={Levenshtein transformer},
  author={Gu, Jiatao and Wang, Changhan and Zhao, Junbo},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11181--11191},
  year={2019}
}

% BANG
@inproceedings{qi2021bang,
  title={Bang: Bridging autoregressive and non-autoregressive generation with large scale pretraining},
  author={Qi, Weizhen and Gong, Yeyun and Jiao, Jian and Yan, Yu and Chen, Weizhu and Liu, Dayiheng and Tang, Kewen and Li, Houqiang and Chen, Jiusheng and Zhang, Ruofei and others},
  booktitle={International Conference on Machine Learning},
  pages={8630--8639},
  year={2021},
  organization={PMLR}
}

% LSTM
@article{GreffSKSS17,
  author    = {Klaus Greff and
               Rupesh Kumar Srivastava and
               Jan Koutn{\'{\i}}k and
               Bas R. Steunebrink and
               J{\"{u}}rgen Schmidhuber},
  title     = {{LSTM:} {A} Search Space Odyssey},
  journal   = {{IEEE} Trans. Neural Networks Learn. Syst.},
  volume    = {28},
  number    = {10},
  pages     = {2222--2232},
  year      = {2017}
}

% InsT
@article{stern2019insertion,
  title={Insertion transformer: Flexible sequence generation via insertion operations},
  author={Stern, Mitchell and Chan, William and Kiros, Jamie and Uszkoreit, Jakob},
  journal={arXiv preprint arXiv:1902.03249},
  year={2019}
}

@article{DBLP:journals/csur/DongLGCLSY23,
  author    = {Chenhe Dong and
               Yinghui Li and
               Haifan Gong and
               Miaoxin Chen and
               Junxin Li and
               Ying Shen and
               Min Yang},
  title     = {A Survey of Natural Language Generation},
  journal   = {{ACM} Comput. Surv.},
  volume    = {55},
  number    = {8},
  pages     = {173:1--173:38},
  year      = {2023},
  url       = {https://doi.org/10.1145/3554727},
  doi       = {10.1145/3554727},
  timestamp = {Thu, 02 Feb 2023 16:04:35 +0100},
  biburl    = {https://dblp.org/rec/journals/csur/DongLGCLSY23.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% GLGE
@inproceedings{liu2021glge,
  title={GLGE: A New General Language Generation Evaluation Benchmark},
  author={Liu, Dayiheng and Yan, Yu and Gong, Yeyun and Qi, Weizhen and Zhang, Hang and Jiao, Jian and Chen, Weizhu and Fu, Jie and Shou, Linjun and Gong, Ming and others},
  booktitle={Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages={408--420},
  year={2021}
}

% diffusion-lm
@inproceedings{
li2022diffusionlm,
title={Diffusion-{LM} Improves Controllable Text Generation},
author={Xiang Lisa Li and John Thickstun and Ishaan Gulrajani and Percy Liang and Tatsunori Hashimoto},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=3s9IrEsjLyk}
}

% CDCD
@article{dieleman2022continuous,
  title={Continuous diffusion for categorical data},
  author={Dieleman, Sander and Sartran, Laurent and Roshannai, Arman and Savinov, Nikolay and Ganin, Yaroslav and Richemond, Pierre H and Doucet, Arnaud and Strudel, Robin and Dyer, Chris and Durkan, Conor and others},
  journal={arXiv preprint arXiv:2211.15089},
  year={2022}
}

% CMLM
@inproceedings{ghazvininejad-etal-2019-mask,
    title = "Mask-Predict: Parallel Decoding of Conditional Masked Language Models",
    author = "Ghazvininejad, Marjan  and
      Levy, Omer  and
      Liu, Yinhan  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1633",
    doi = "10.18653/v1/D19-1633",
    pages = "6112--6121",
    abstract = "Most machine translation systems generate text autoregressively from left to right. We, instead, use a masked language modeling objective to train a model to predict any subset of the target words, conditioned on both the input text and a partially masked target translation. This approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the model is least confident about. By applying this strategy for a constant number of iterations, our model improves state-of-the-art performance levels for non-autoregressive and parallel decoding translation models by over 4 BLEU on average. It is also able to reach within about 1 BLEU point of a typical left-to-right transformer model, while decoding significantly faster.",
}

% prophetnet
@inproceedings{qi-etal-2020-prophetnet,
    title = "{P}rophet{N}et: Predicting Future N-gram for Sequence-to-{S}equence{P}re-training",
    author = "Qi, Weizhen  and
      Yan, Yu  and
      Gong, Yeyun  and
      Liu, Dayiheng  and
      Duan, Nan  and
      Chen, Jiusheng  and
      Zhang, Ruofei  and
      Zhou, Ming",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.217",
    doi = "10.18653/v1/2020.findings-emnlp.217",
    pages = "2401--2410",
    abstract = "This paper presents a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of optimizing one-step-ahead prediction in the traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large-scale dataset (160GB), respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new state-of-the-art results on all these datasets compared to the models using the same scale pre-training corpus.",
}

% CNAT
@inproceedings{bao-etal-2021-non,
    title = "Non-Autoregressive Translation by Learning Target Categorical Codes",
    author = "Bao, Yu  and
      Huang, Shujian  and
      Xiao, Tong  and
      Wang, Dongqi  and
      Dai, Xinyu  and
      Chen, Jiajun",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.458",
    doi = "10.18653/v1/2021.naacl-main.458",
    pages = "5749--5759",
    abstract = "Non-autoregressive Transformer is a promising text generation model. However, current non-autoregressive models still fall behind their autoregressive counterparts in translation quality. We attribute this accuracy gap to the lack of dependency modeling among decoder inputs. In this paper, we propose CNAT, which learns implicitly categorical codes as latent variables into the non-autoregressive decoding. The interaction among these categorical codes remedies the missing dependencies and improves the model capacity. Experiment results show that our model achieves comparable or better performance in machine translation tasks than several strong baselines.",
}

% LLamA
@article{DBLP:journals/corr/abs-2302-13971,
  author    = {Hugo Touvron and
               Thibaut Lavril and
               Gautier Izacard and
               Xavier Martinet and
               Marie{-}Anne Lachaux and
               Timoth{\'{e}}e Lacroix and
               Baptiste Rozi{\`{e}}re and
               Naman Goyal and
               Eric Hambro and
               Faisal Azhar and
               Aur{\'{e}}lien Rodriguez and
               Armand Joulin and
               Edouard Grave and
               Guillaume Lample},
  title     = {LLaMA: Open and Efficient Foundation Language Models},
  journal   = {CoRR},
  volume    = {abs/2302.13971},
  year      = {2023},
  url       = {https://doi.org/10.48550/arXiv.2302.13971},
  doi       = {10.48550/arXiv.2302.13971},
  eprinttype = {arXiv},
  eprint    = {2302.13971}
}

% GPT-4
@article{DBLP:journals/corr/abs-2303-08774,
  author    = {OpenAI},
  title     = {{GPT-4} Technical Report},
  journal   = {CoRR},
  volume    = {abs/2303.08774},
  year      = {2023},
  url       = {https://doi.org/10.48550/arXiv.2303.08774},
  doi       = {10.48550/arXiv.2303.08774},
  eprinttype = {arXiv},
  eprint    = {2303.08774}
}

% Alpaca
@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

% ELMER
@inproceedings{DBLP:conf/emnlp/LiTZNW22,
  author    = {Junyi Li and
               Tianyi Tang and
               Wayne Xin Zhao and
               Jian{-}Yun Nie and
               Ji{-}Rong Wen},
  editor    = {Yoav Goldberg and
               Zornitsa Kozareva and
               Yue Zhang},
  title     = {{ELMER:} {A} Non-Autoregressive Pre-trained Language Model for Efficient
               and Effective Text Generation},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab Emirates,
               December 7-11, 2022},
  pages     = {1044--1058},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  url       = {https://aclanthology.org/2022.emnlp-main.68}
}

% BART
@inproceedings{DBLP:conf/acl/LewisLGGMLSZ20,
  author    = {Mike Lewis and
               Yinhan Liu and
               Naman Goyal and
               Marjan Ghazvininejad and
               Abdelrahman Mohamed and
               Omer Levy and
               Veselin Stoyanov and
               Luke Zettlemoyer},
  editor    = {Dan Jurafsky and
               Joyce Chai and
               Natalie Schluter and
               Joel R. Tetreault},
  title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language
               Generation, Translation, and Comprehension},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2020, Online, July 5-10, 2020},
  pages     = {7871--7880},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.acl-main.703},
  doi       = {10.18653/v1/2020.acl-main.703}
}

% DiffuSeq
@article{DBLP:journals/corr/abs-2210-08933,
  author    = {Shansan Gong and
               Mukai Li and
               Jiangtao Feng and
               Zhiyong Wu and
               Lingpeng Kong},
  title     = {DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models},
  journal   = {CoRR},
  volume    = {abs/2210.08933},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2210.08933},
  doi       = {10.48550/arXiv.2210.08933},
  eprinttype = {arXiv},
  eprint    = {2210.08933}
}

% SSD-LM
@article{DBLP:journals/corr/abs-2210-17432,
  author    = {Xiaochuang Han and
               Sachin Kumar and
               Yulia Tsvetkov},
  title     = {{SSD-LM:} Semi-autoregressive Simplex-based Diffusion Language Model
               for Text Generation and Modular Control},
  journal   = {CoRR},
  volume    = {abs/2210.17432},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2210.17432},
  doi       = {10.48550/arXiv.2210.17432},
  eprinttype = {arXiv},
  eprint    = {2210.17432}
}

% DDPM
@inproceedings{DBLP:conf/nips/HoJA20,
  author    = {Jonathan Ho and
               Ajay Jain and
               Pieter Abbeel},
  editor    = {Hugo Larochelle and
               Marc'Aurelio Ranzato and
               Raia Hadsell and
               Maria{-}Florina Balcan and
               Hsuan{-}Tien Lin},
  title     = {Denoising Diffusion Probabilistic Models},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html}
}

% stable diffusion
@InProceedings{Rombach_2022_CVPR,
    author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},
    title     = {High-Resolution Image Synthesis With Latent Diffusion Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {10684-10695}
}

% imagen
@inproceedings{NEURIPS2022_ec795aea,
 author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L and Ghasemipour, Kamyar and Gontijo Lopes, Raphael and Karagol Ayan, Burcu and Salimans, Tim and Ho, Jonathan and Fleet, David J and Norouzi, Mohammad},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {36479--36494},
 publisher = {Curran Associates, Inc.},
 title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


% Nonequilibrium
@InProceedings{pmlr-v37-sohl-dickstein15,
  title = 	 {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2256--2265},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract = 	 {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}

% D3PMs
@inproceedings{NEURIPS2021_958c5305,
 author = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and van den Berg, Rianne},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {17981--17993},
 publisher = {Curran Associates, Inc.},
 title = {Structured Denoising Diffusion Models in Discrete State-Spaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/958c530554f78bcd8e97125b70e6973d-Paper.pdf},
 volume = {34},
 year = {2021}
}

% Multinomial Diffusion
@inproceedings{NEURIPS2021_67d96d45,
 author = {Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forr\'{e}, Patrick and Welling, Max},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {12454--12465},
 publisher = {Curran Associates, Inc.},
 title = {Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/67d96d458abdef21792e6d8e590244e7-Paper.pdf},
 volume = {34},
 year = {2021}
}

% diffusionER
@misc{reid2022diffuser,
      title={DiffusER: Discrete Diffusion via Edit-based Reconstruction}, 
      author={Machel Reid and Vincent J. Hellendoorn and Graham Neubig},
      year={2022},
      eprint={2210.16886},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% diffusion-glat
@misc{qian2022diffglat,
      title={Diff-Glat: Diffusion Glancing Transformer for Parallel Sequence to Sequence Learning}, 
      author={Lihua Qian and Mingxuan Wang and Yang Liu and Hao Zhou},
      year={2022},
      eprint={2212.10240},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% Analog
@misc{chen2023analog,
      title={Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning}, 
      author={Ting Chen and Ruixiang Zhang and Geoffrey Hinton},
      year={2023},
      eprint={2208.04202},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

% LD4LG
@misc{lovelace2022latent,
      title={Latent Diffusion for Language Generation}, 
      author={Justin Lovelace and Varsha Kishore and Chao Wan and Eliot Shekhtman and Kilian Weinberger},
      year={2022},
      eprint={2212.09462},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% GENIE
@misc{lin2023text,
      title={Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise}, 
      author={Zhenghao Lin and Yeyun Gong and Yelong Shen and Tong Wu and Zhihao Fan and Chen Lin and Nan Duan and Weizhu Chen},
      year={2023},
      eprint={2212.11685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{song2020score,
  title={Score-based generative modeling through stochastic differential equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={arXiv preprint arXiv:2011.13456},
  year={2020}
}

@article{ramesh2022hierarchical,
  title={Hierarchical text-conditional image generation with clip latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  year={2022}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{wehenkel2021diffusion,
  title={Diffusion priors in variational autoencoders},
  author={Wehenkel, Antoine and Louppe, Gilles},
  journal={arXiv preprint arXiv:2106.15671},
  year={2021}
}

@article{vahdat2021score,
  title={Score-based generative modeling in latent space},
  author={Vahdat, Arash and Kreis, Karsten and Kautz, Jan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={11287--11302},
  year={2021}
}

% self-bleu
@inproceedings{zhu2018texygen,
  title={Texygen: A benchmarking platform for text generation models},
  author={Zhu, Yaoming and Lu, Sidi and Zheng, Lei and Guo, Jiaxian and Zhang, Weinan and Wang, Jun and Yu, Yong},
  booktitle={The 41st international ACM SIGIR conference on research \& development in information retrieval},
  pages={1097--1100},
  year={2018}
}

% inner-token dependency
@inproceedings{DBLP:conf/emnlp/LiCY022,
  author       = {Yafu Li and
                  Leyang Cui and
                  Yongjing Yin and
                  Yue Zhang},
  editor       = {Yoav Goldberg and
                  Zornitsa Kozareva and
                  Yue Zhang},
  title        = {Multi-Granularity Optimization for Non-Autoregressive Translation},
  booktitle    = {Proceedings of the 2022 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab Emirates,
                  December 7-11, 2022},
  pages        = {5073--5084},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://aclanthology.org/2022.emnlp-main.339},
  timestamp    = {Tue, 07 Feb 2023 17:10:51 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/LiCY022.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

%NAR SURVEY(beam search)
@article{journals/corr/abs-2204-09269,
  author    = {Yisheng Xiao and
               Lijun Wu and
               Junliang Guo and
               Juntao Li and
               Min Zhang and
               Tao Qin and
               Tie{-}Yan Liu},
  title     = {A Survey on Non-Autoregressive Generation for Neural Machine Translation
               and Beyond},
  journal   = {CoRR},
  volume    = {abs/2204.09269},
  year      = {2022}
}

% diverse beam search
@article{journals/corr/VijayakumarCSSL16,
  author    = {Ashwin K. Vijayakumar and
               Michael Cogswell and
               Ramprasaath R. Selvaraju and
               Qing Sun and
               Stefan Lee and
               David J. Crandall and
               Dhruv Batra},
  title     = {Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence
               Models},
  journal   = {CoRR},
  volume    = {abs/1610.02424},
  year      = {2016}
}

% typical sample
@article{journals/corr/abs-2202-00666,
  author    = {Clara Meister and
               Tiago Pimentel and
               Gian Wiher and
               Ryan Cotterell},
  title     = {Typical Decoding for Natural Language Generation},
  journal   = {CoRR},
  volume    = {abs/2202.00666},
  year      = {2022}
}

% topk sample
@inproceedings{LewisDF18,
  author    = {Angela Fan and
               Mike Lewis and
               Yann N. Dauphin},
  title     = {Hierarchical Neural Story Generation},
  booktitle = {{ACL} {(1)}},
  pages     = {889--898},
  publisher = {Association for Computational Linguistics},
  year      = {2018}
}

% nucleus sample
@inproceedings{HoltzmanBDFC20,
  author    = {Ari Holtzman and
               Jan Buys and
               Li Du and
               Maxwell Forbes and
               Yejin Choi},
  title     = {The Curious Case of Neural Text Degeneration},
  booktitle = {{ICLR}},
  publisher = {OpenReview.net},
  year      = {2020}
}

@article{luo2022understanding,
  title={Understanding diffusion models: A unified perspective},
  author={Luo, Calvin},
  journal={arXiv preprint arXiv:2208.11970},
  year={2022}
}

% ARDMs
@inproceedings{
hoogeboom2022autoregressive,
title={Autoregressive Diffusion Models},
author={Emiel Hoogeboom and Alexey A. Gritsenko and Jasmijn Bastings and Ben Poole and Rianne van den Berg and Tim Salimans},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=Lm8T39vLDTE}
}

% TimeGrad
@InProceedings{pmlr-v139-rasul21a,
  title = 	 {Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting},
  author =       {Rasul, Kashif and Seward, Calvin and Schuster, Ingmar and Vollgraf, Roland},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8857--8868},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/rasul21a/rasul21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/rasul21a.html},
  abstract = 	 {In this work, we propose TimeGrad, an autoregressive model for multivariate probabilistic time series forecasting which samples from the data distribution at each time step by estimating its gradient. To this end, we use diffusion probabilistic models, a class of latent variable models closely connected to score matching and energy-based methods. Our model learns gradients by optimizing a variational bound on the data likelihood and at inference time converts white noise into a sample of the distribution of interest through a Markov chain using Langevin sampling. We demonstrate experimentally that the proposed autoregressive denoising diffusion model is the new state-of-the-art multivariate probabilistic forecasting method on real-world data sets with thousands of correlated dimensions. We hope that this method is a useful tool for practitioners and lays the foundation for future research in this area.}
}
