\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbas et~al.(2023)Abbas, Zhao, Modayil, White, and
  Machado]{abbas2023loss}
Zaheer Abbas, Rosie Zhao, Joseph Modayil, Adam White, and Marlos~C Machado.
\newblock Loss of plasticity in continual deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2303.07507}, 2023.

\bibitem[Abel et~al.(2018)Abel, Jinnai, Guo, Konidaris, and
  Littman]{abel2018transfer}
David Abel, Yuu Jinnai, Yue Guo, George Konidaris, and Michael~L. Littman.
\newblock Policy and value transfer in lifelong reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2018.

\bibitem[Ammar et~al.(2015)Ammar, Tutunov, and Eaton]{ammar2015safe}
Haitham~Bou Ammar, Rasul Tutunov, and Eric Eaton.
\newblock Safe policy search for lifelong reinforcement learning with sublinear
  regret.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2015.

\bibitem[Baker et~al.(2023)Baker, New, Aguilar-Simon, Al-Halah, Arnold,
  Ben-Iwhiwhu, Brna, Brooks, Brown, Daniels, et~al.]{baker2023domain}
Megan~M Baker, Alexander New, Mario Aguilar-Simon, Ziad Al-Halah,
  S{\'e}bastien~MR Arnold, Ese Ben-Iwhiwhu, Andrew~P Brna, Ethan Brooks, Ryan~C
  Brown, Zachary Daniels, et~al.
\newblock A domain-agnostic approach for characterization of lifelong learning
  systems.
\newblock \emph{Neural Networks}, 160:\penalty0 274--296, 2023.

\bibitem[Bartlett(1992)]{bartlett1992learning}
Peter~L Bartlett.
\newblock Learning with a slowly changing distribution.
\newblock In \emph{Proceedings of the Annual Workshop on Computational Learning
  Theory}, 1992.

\bibitem[Besbes et~al.(2014)Besbes, Gur, and Zeevi]{besbes2014stochastic}
Omar Besbes, Yonatan Gur, and Assaf Zeevi.
\newblock Stochastic multi-armed-bandit problem with non-stationary rewards.
\newblock \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Bowling et~al.(2023)Bowling, Martin, Abel, and
  Dabney]{bowling2023settling}
Michael Bowling, John~D. Martin, David Abel, and Will Dabney.
\newblock Settling the reward hypothesis.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Brunskill and Li(2014)]{brunskill2014pac}
Emma Brunskill and Lihong Li.
\newblock {PAC}-inspired option discovery in lifelong reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2014.

\bibitem[Carlson et~al.(2010)Carlson, Betteridge, Kisiel, Settles, Hruschka,
  and Mitchell]{carlson2010toward}
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam~R
  Hruschka, and Tom Mitchell.
\newblock Toward an architecture for never-ending language learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2010.

\bibitem[Cassandra et~al.(1994)Cassandra, Kaelbling, and
  Littman]{cassandra1994acting}
Anthony~R. Cassandra, Leslie~Pack Kaelbling, and Michael~L. Littman.
\newblock Acting optimally in partially observable stochastic domains.
\newblock In \emph{Proceedings of the AAAI Conference on Artificiall
  Intelligence}, 1994.

\bibitem[Cohen et~al.(2019)Cohen, Catt, and Hutter]{cohen2019strongly}
Michael~K Cohen, Elliot Catt, and Marcus Hutter.
\newblock A strongly asymptotically optimal agent in general environments.
\newblock \emph{arXiv preprint arXiv:1903.01021}, 2019.

\bibitem[Dick et~al.(2014)Dick, Gy{\"o}rgy, and Szepesvari]{dick2014online}
Travis Dick, Andr{\'a}s Gy{\"o}rgy, and Csaba Szepesvari.
\newblock Online learning in {M}arkov decision processes with changing cost
  sequences.
\newblock In \emph{Procedings of the International Conference on Machine
  Learning}, 2014.

\bibitem[Dohare et~al.(2021)Dohare, Sutton, and Mahmood]{dohare2021continual}
Shibhansh Dohare, Richard~S Sutton, and A~Rupam Mahmood.
\newblock Continual backprop: Stochastic gradient descent with persistent
  randomness.
\newblock \emph{arXiv preprint arXiv:2108.06325}, 2021.

\bibitem[Dohare et~al.(2023)Dohare, Hernandez-Garcia, Rahman, Sutton, and
  Mahmood]{dohare2023loss}
Shibhansh Dohare, Juan Hernandez-Garcia, Parash Rahman, Richard Sutton, and
  A~Rupam Mahmood.
\newblock Loss of plasticity in deep continual learning.
\newblock \emph{arXiv preprint arXiv:2306.13812}, 2023.

\bibitem[Dong et~al.(2022)Dong, Van~Roy, and Zhou]{dong2022simple}
Shi Dong, Benjamin Van~Roy, and Zhengyuan Zhou.
\newblock Simple agent, complex environment: Efficient reinforcement learning
  with agent states.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (255):\penalty0 1--54, 2022.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2017.

\bibitem[French(1999)]{french1999catastrophic}
Robert~M French.
\newblock Catastrophic forgetting in connectionist networks.
\newblock \emph{Trends in cognitive sciences}, 3\penalty0 (4):\penalty0
  128--135, 1999.

\bibitem[Friedman(1953)]{friedman1953essays}
Milton Friedman.
\newblock \emph{Essays in positive economics}.
\newblock University of Chicago press, 1953.

\bibitem[Fu et~al.(2022)Fu, Yu, Littman, and Konidaris]{fu2022model}
Haotian Fu, Shangqun Yu, Michael Littman, and George Konidaris.
\newblock Model-based lifelong reinforcement learning with {B}ayesian
  exploration.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Goodfellow et~al.(2013)Goodfellow, Mirza, Xiao, Courville, and
  Bengio]{goodfellow2013empirical}
Ian~J Goodfellow, Mehdi Mirza, Da~Xiao, Aaron Courville, and Yoshua Bengio.
\newblock An empirical investigation of catastrophic forgetting in
  gradient-based neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6211}, 2013.

\bibitem[Hadsell et~al.(2020)Hadsell, Rao, Rusu, and
  Pascanu]{hadsell2020embracing}
Raia Hadsell, Dushyant Rao, Andrei~A Rusu, and Razvan Pascanu.
\newblock Embracing change: Continual learning in deep neural networks.
\newblock \emph{Trends in cognitive sciences}, 24\penalty0 (12):\penalty0
  1028--1040, 2020.

\bibitem[Hutter(2000)]{hutter2000theory}
Marcus Hutter.
\newblock A theory of universal artificial intelligence based on algorithmic
  complexity.
\newblock \emph{arXiv preprint cs/0004001}, 2000.

\bibitem[Hutter(2004)]{hutter2004universal}
Marcus Hutter.
\newblock \emph{Universal artificial intelligence: Sequential decisions based
  on algorithmic probability}.
\newblock Springer Science \& Business Media, 2004.

\bibitem[Khetarpal et~al.(2022)Khetarpal, Riemer, Rish, and
  Precup]{khetarpal2022towards}
Khimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup.
\newblock Towards continual reinforcement learning: A review and perspectives.
\newblock \emph{Journal of Artificial Intelligence Research}, 75:\penalty0
  1401--1476, 2022.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[Kumar et~al.(2023)Kumar, Marklund, Rao, Zhu, Jeon, Liu, and
  Van~Roy]{kumar2023continual}
Saurabh Kumar, Henrik Marklund, Ashish Rao, Yifan Zhu, Hong~Jun Jeon, Yueyang
  Liu, and Benjamin Van~Roy.
\newblock Continual learning as computationally constrained reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2307.04345}, 2023.

\bibitem[Lattimore(2014)]{lattimore2014theory}
Tor Lattimore.
\newblock \emph{Theory of general reinforcement learning}.
\newblock PhD thesis, The Australian National University, 2014.

\bibitem[Leike(2016)]{leike2016nonparametric}
Jan Leike.
\newblock \emph{Nonparametric general reinforcement learning}.
\newblock PhD thesis, The Australian National University, 2016.

\bibitem[Lesort et~al.(2020)Lesort, Lomonaco, Stoian, Maltoni, Filliat, and
  D{\'\i}az-Rodr{\'\i}guez]{lesort2020continual}
Timoth{\'e}e Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David
  Filliat, and Natalia D{\'\i}az-Rodr{\'\i}guez.
\newblock Continual learning for robotics: Definition, framework, learning
  strategies, opportunities and challenges.
\newblock \emph{Information fusion}, 58:\penalty0 52--68, 2020.

\bibitem[Liu et~al.(2023)Liu, Van~Roy, and Xu]{liu2023definition}
Yueyang Liu, Benjamin Van~Roy, and Kuang Xu.
\newblock A definition of non-stationary bandits.
\newblock \emph{arXiv preprint arXiv:2302.12202}, 2023.

\bibitem[Lu et~al.(2023)Lu, Roy, Dwaracherla, Ibrahimi, Osband, and
  Wen]{lu2023bitbybit}
Xiuyuan Lu, Benjamin~Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian
  Osband, and Zheng Wen.
\newblock Reinforcement learning, bit by bit.
\newblock \emph{Foundations and Trends in Machine Learning}, 16\penalty0
  (6):\penalty0 733--865, 2023.
\newblock ISSN 1935-8237.

\bibitem[Luketina et~al.(2022)Luketina, Flennerhag, Schroecker, Abel, Zahavy,
  and Singh]{luketina2022}
Jelena Luketina, Sebastian Flennerhag, Yannick Schroecker, David Abel, Tom
  Zahavy, and Satinder Singh.
\newblock Meta-gradients in non-stationary environments.
\newblock In \emph{Proceedings of the Conference on Lifelong Learning Agents},
  2022.

\bibitem[Lyle et~al.(2023)Lyle, Zheng, Nikishin, Pires, Pascanu, and
  Dabney]{lyle2023understanding}
Clare Lyle, Zeyu Zheng, Evgenii Nikishin, Bernardo~Avila Pires, Razvan Pascanu,
  and Will Dabney.
\newblock Understanding plasticity in neural networks.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2023.

\bibitem[Mai et~al.(2022)Mai, Li, Jeong, Quispe, Kim, and
  Sanner]{mai2022online}
Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott
  Sanner.
\newblock Online continual learning in image classification: An empirical
  survey.
\newblock \emph{Neurocomputing}, 469:\penalty0 28--51, 2022.

\bibitem[Majeed(2021)]{majeed2021abstractions}
Sultan~J Majeed.
\newblock \emph{Abstractions of general reinforcement Learning}.
\newblock PhD thesis, The Australian National University, 2021.

\bibitem[Majeed and Hutter(2019)]{majeed2019performance}
Sultan~Javed Majeed and Marcus Hutter.
\newblock Performance guarantees for homomorphisms beyond {M}arkov decision
  processes.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2019.

\bibitem[McCloskey and Cohen(1989)]{mccloskey1989catastrophic}
Michael McCloskey and Neal~J Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock In \emph{Psychology of learning and motivation}, volume~24, pages
  109--165. Elsevier, 1989.

\bibitem[Mitchell et~al.(2018)Mitchell, Cohen, Hruschka, Talukdar, Yang,
  Betteridge, Carlson, Dalvi, Gardner, Kisiel, et~al.]{mitchell2018never}
Tom Mitchell, William Cohen, Estevam Hruschka, Partha Talukdar, Bishan Yang,
  Justin Betteridge, Andrew Carlson, Bhavana Dalvi, Matt Gardner, Bryan Kisiel,
  et~al.
\newblock Never-ending learning.
\newblock \emph{Communications of the ACM}, 61\penalty0 (5):\penalty0 103--115,
  2018.

\bibitem[Monteleoni and Jaakkola(2003)]{monteleoni2003online}
Claire Monteleoni and Tommi Jaakkola.
\newblock Online learning of non-stationary sequences.
\newblock \emph{Advances in Neural Information Processing Systems}, 16, 2003.

\bibitem[Nguyen et~al.(2018)Nguyen, Li, Bui, and Turner]{nguyen2018variational}
Cuong~V Nguyen, Yingzhen Li, Thang~D Bui, and Richard~E Turner.
\newblock Variational continual learning.
\newblock 2018.

\bibitem[Parisi et~al.(2019)Parisi, Kemker, Part, Kanan, and
  Wermter]{parisi2019continual}
German~I Parisi, Ronald Kemker, Jose~L Part, Christopher Kanan, and Stefan
  Wermter.
\newblock Continual lifelong learning with neural networks: A review.
\newblock \emph{Neural networks}, 113:\penalty0 54--71, 2019.

\bibitem[Platanios et~al.(2020)Platanios, Saparov, and
  Mitchell]{platanios2020jelly}
Emmanouil~Antonios Platanios, Abulhair Saparov, and Tom Mitchell.
\newblock Jelly bean world: A testbed for never-ending learning.
\newblock \emph{arXiv preprint arXiv:2002.06306}, 2020.

\bibitem[Puterman(2014)]{puterman2014markov}
Martin~L Puterman.
\newblock \emph{Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Riemer et~al.(2022)Riemer, Raparthy, Cases, Subbaraj, Puelma~Touzel,
  and Rish]{riemer2022continual}
Matthew Riemer, Sharath~Chandra Raparthy, Ignacio Cases, Gopeshh Subbaraj,
  Maximilian Puelma~Touzel, and Irina Rish.
\newblock Continual learning in environments with polynomial mixing times.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Ring(1994)]{ring1994continual}
Mark~B Ring.
\newblock \emph{Continual learning in reinforcement environments}.
\newblock PhD thesis, The University of Texas at Austin, 1994.

\bibitem[Ring(1997)]{ring1997child}
Mark~B Ring.
\newblock Child: A first step towards continual learning.
\newblock \emph{Machine Learning}, 28\penalty0 (1):\penalty0 77--104, 1997.

\bibitem[Ring(2005)]{ring2005toward}
Mark~B Ring.
\newblock Toward a formal framework for continual learning.
\newblock In \emph{NeurIPS Workshop on Inductive Transfer}, 2005.

\bibitem[Rolnick et~al.(2019)Rolnick, Ahuja, Schwarz, Lillicrap, and
  Wayne]{rolnick2019experience}
David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory
  Wayne.
\newblock Experience replay for continual learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Russell and Subramanian(1994)]{russell1994provably}
Stuart~J Russell and Devika Subramanian.
\newblock Provably bounded-optimal agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 2:\penalty0
  575--609, 1994.

\bibitem[Ruvolo and Eaton(2013)]{ruvolo2013ella}
Paul Ruvolo and Eric Eaton.
\newblock {ELLA}: An efficient lifelong learning algorithm.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2013.

\bibitem[Schaul and Schmidhuber(2010)]{schaul2010metalearning}
Tom Schaul and J{\"u}rgen Schmidhuber.
\newblock Metalearning.
\newblock \emph{Scholarpedia}, 5\penalty0 (6):\penalty0 4650, 2010.

\bibitem[Schmidhuber et~al.(1998)Schmidhuber, Zhao, and
  Schraudolph]{schmidhuber1998reinforcement}
J{\"u}rgen Schmidhuber, Jieyu Zhao, and Nicol~N Schraudolph.
\newblock Reinforcement learning with self-modifying policies.
\newblock In \emph{Learning to Learn}, pages 293--309. Springer, 1998.

\bibitem[Schwarz et~al.(2018)Schwarz, Czarnecki, Luketina, Grabska-Barwinska,
  Teh, Pascanu, and Hadsell]{schwarz2018progress}
Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka
  Grabska-Barwinska, Yee~Whye Teh, Razvan Pascanu, and Raia Hadsell.
\newblock Progress \& compress: A scalable framework for continual learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2018.

\bibitem[Silver(2011)]{silver2011machine}
Daniel~L Silver.
\newblock Machine lifelong learning: Challenges and benefits for artificial
  general intelligence.
\newblock In \emph{Proceedings of the Conference on Artificial General
  Intelligence}, 2011.

\bibitem[Sutton(1992)]{sutton1992introduction}
Richard~S Sutton.
\newblock Introduction: The challenge of reinforcement learning.
\newblock In \emph{Reinforcement Learning}, pages 1--3. Springer, 1992.

\bibitem[Sutton(2004)]{suttonwebRLhypothesis}
Richard~S Sutton.
\newblock The reward hypothesis, 2004.
\newblock URL
  \url{http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html}.

\bibitem[Sutton(2022)]{sutton2022quest}
Richard~S Sutton.
\newblock The quest for a common model of the intelligent decision maker.
\newblock \emph{arXiv preprint arXiv:2202.13252}, 2022.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G. Barto.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock MIT Press, 2018.

\bibitem[Taylor and Stone(2009)]{taylor2009transfer}
Matthew~E. Taylor and Peter Stone.
\newblock Transfer learning for reinforcement learning domains: A survey.
\newblock \emph{Journal of Machine Learning Research}, 10\penalty0
  (Jul):\penalty0 1633--1685, 2009.

\bibitem[Thrun(1995)]{thrun1995thenth}
Sebastian Thrun.
\newblock Is learning the n-th thing any easier than learning the first?
\newblock \emph{Advances in Neural Information Processing Systems}, 1995.

\bibitem[Thrun(1998)]{thrun1998lifelong}
Sebastian Thrun.
\newblock Lifelong learning algorithms.
\newblock \emph{Learning to Learn}, 8:\penalty0 181--209, 1998.

\bibitem[Thrun and Mitchell(1995)]{thrun1995lifelong}
Sebastian Thrun and Tom~M Mitchell.
\newblock Lifelong robot learning.
\newblock \emph{Robotics and autonomous systems}, 15\penalty0 (1-2):\penalty0
  25--46, 1995.

\bibitem[Wilson et~al.(2007)Wilson, Fern, Ray, and Tadepalli]{wilson2007multi}
Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli.
\newblock Multi-task reinforcement learning: a hierarchical {B}ayesian
  approach.
\newblock In \emph{Proceedings of the International Conference on Machine
  learning}, 2007.

\end{thebibliography}
