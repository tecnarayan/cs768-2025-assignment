\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2019)Arora, Khandeparkar, Khodak, Plevrakis, and
  Saunshi]{arora2019theoretical}
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and
  Nikunj Saunshi.
\newblock A theoretical analysis of contrastive unsupervised representation
  learning.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Ben-David et~al.(2020)Ben-David, Rabinovitz, and
  Reichart]{ben2020perl}
Eyal Ben-David, Carmel Rabinovitz, and Roi Reichart.
\newblock Perl: Pivot-based domain adaptation for pre-trained deep
  contextualized embedding models.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:\penalty0 504--521, 2020.

\bibitem[Ben-David et~al.(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,
  and Vaughan]{ben2010theory}
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
  Jennifer~Wortman Vaughan.
\newblock A theory of learning from different domains.
\newblock \emph{Machine learning}, 79\penalty0 (1-2):\penalty0 151--175, 2010.

\bibitem[Blitzer et~al.(2007)Blitzer, Dredze, and
  Pereira]{blitzer2007biographies}
John Blitzer, Mark Dredze, and Fernando Pereira.
\newblock Biographies, bollywood, boom-boxes and blenders: Domain adaptation
  for sentiment classification.
\newblock In \emph{Proceedings of the 45th annual meeting of the association of
  computational linguistics}, pages 440--447, 2007.

\bibitem[Bobkov et~al.(1997)]{bobkov1997isoperimetric}
Sergey~G Bobkov et~al.
\newblock An isoperimetric inequality on the discrete cube, and an elementary
  proof of the isoperimetric inequality in gauss space.
\newblock \emph{The Annals of Probability}, 25\penalty0 (1):\penalty0 206--214,
  1997.

\bibitem[Cai et~al.(2021)Cai, Gao, Lee, and Lei]{cai2021theory}
Tianle Cai, Ruiqi Gao, Jason Lee, and Qi~Lei.
\newblock A theory of label propagation for subpopulation shift.
\newblock In \emph{International Conference on Machine Learning}, pages
  1170--1182. PMLR, 2021.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron2020unsupervised}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
  Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock \emph{arXiv preprint arXiv:2006.09882}, 33:\penalty0 9912--9924,
  2020.

\bibitem[Chen et~al.(2012)Chen, Xu, Weinberger, and Sha]{chen2012marginalized}
Minmin Chen, Zhixiang Xu, Kilian~Q Weinberger, and Fei Sha.
\newblock Marginalized denoising autoencoders for domain adaptation.
\newblock In \emph{Proceedings of the 29th International Coference on
  International Conference on Machine Learning}, pages 1627--1634, 2012.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Kornblith, Norouzi, and
  Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 1597--1607. PMLR,
  PMLR, 13--18 Jul 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Kornblith, Swersky, Norouzi, and
  Hinton]{chen2020big}
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey
  Hinton.
\newblock Big self-supervised models are strong semi-supervised learners.
\newblock \emph{arXiv preprint arXiv:2006.10029}, 2020{\natexlab{b}}.

\bibitem[Chen and He(2020)]{chen2020exploring}
Xinlei Chen and Kaiming He.
\newblock Exploring simple siamese representation learning.
\newblock \emph{arXiv preprint arXiv:2011.10566}, pages 15750--15758, June
  2020.

\bibitem[Chen et~al.(2020{\natexlab{c}})Chen, Fan, Girshick, and
  He]{chen2020improved}
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
\newblock Improved baselines with momentum contrastive learning.
\newblock \emph{arXiv preprint arXiv:2003.04297}, 2020{\natexlab{c}}.

\bibitem[Chen et~al.(2020{\natexlab{d}})Chen, Wei, Kumar, and
  Ma]{chen2020selftraining}
Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma.
\newblock Self-training avoids using spurious features under domain shift.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020{\natexlab{d}}.

\bibitem[Chung and Graham(1997)]{chung1997spectral}
Fan~RK Chung and Fan~Chung Graham.
\newblock \emph{Spectral graph theory}.
\newblock Number~92. American Mathematical Soc., 1997.

\bibitem[Gao et~al.(2021)Gao, Yao, and Chen]{gao2021simcse}
Tianyu Gao, Xingcheng Yao, and Danqi Chen.
\newblock Simcse: Simple contrastive learning of sentence embeddings.
\newblock \emph{arXiv preprint arXiv:2104.08821}, 2021.

\bibitem[Gretton et~al.(2008)Gretton, Smola, Huang, Schmittfull, Borgwardt, and
  Sch{\"o}lkopf]{gretton2008covariate}
Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten
  Borgwardt, and Bernhard Sch{\"o}lkopf.
\newblock Covariate shift by kernel mean matching.
\newblock In \emph{Dataset Shift in Machine Learning}. 2008.

\bibitem[HaoChen et~al.(2021)HaoChen, Wei, Gaidon, and Ma]{haochen2021provable}
Jeff~Z. HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma.
\newblock Provable guarantees for self-supervised deep learning with spectral
  contrastive loss, 2021.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 9729--9738, June 2020.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Liu, Wallace, Dziedzic, Krishnan, and
  Song]{hendrycks2020pretrained}
Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and
  Dawn Song.
\newblock Pretrained transformers improve out-of-distribution robustness.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 2744--2751, 2020.

\bibitem[Huang et~al.(2006)Huang, Gretton, Borgwardt, Sch{\"o}lkopf, and
  Smola]{huang2006correcting}
Jiayuan Huang, Arthur Gretton, Karsten~M Borgwardt, Bernhard Sch{\"o}lkopf, and
  Alex~J Smola.
\newblock Correcting sample selection bias by unlabeled data.
\newblock In \emph{Advances in neural information processing systems}, pages
  601--608, 2006.

\bibitem[Jean et~al.(2016)Jean, Burke, Xie, Davis, Lobell, and
  Ermon]{jean2016combining}
Neal Jean, Marshall Burke, Michael Xie, W.~Matthew Davis, David~B. Lobell, and
  Stefano Ermon.
\newblock Combining satellite imagery and machine learning to predict poverty.
\newblock \emph{Science}, 353, 2016.

\bibitem[Kim et~al.(2022)Kim, Wang, Sclaroff, and Saenko]{kim2022broad}
Donghyun Kim, Kaihong Wang, Stan Sclaroff, and Kate Saenko.
\newblock A broad study of pre-training for domain generalization and
  adaptation.
\newblock \emph{arXiv preprint arXiv:2203.11819}, 2022.

\bibitem[Kumar et~al.(2020)Kumar, Ma, and Liang]{kumar2020gradual}
Ananya Kumar, Tengyu Ma, and Percy Liang.
\newblock Understanding self-training for gradual domain adaptation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Kumar et~al.(2022)Kumar, Raghunathan, Jones, Ma, and
  Liang]{kumar2022fine}
Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang.
\newblock Fine-tuning can distort pretrained features and underperform
  out-of-distribution.
\newblock \emph{arXiv preprint arXiv:2202.10054}, 2022.

\bibitem[Lee et~al.(2014)Lee, Gharan, and Trevisan]{lee2014multiway}
James~R Lee, Shayan~Oveis Gharan, and Luca Trevisan.
\newblock Multiway spectral partitioning and higher-order cheeger inequalities.
\newblock \emph{Journal of the ACM (JACM)}, 61\penalty0 (6):\penalty0 1--30,
  2014.

\bibitem[Lee et~al.(2020)Lee, Lei, Saunshi, and Zhuo]{lee2020predicting}
Jason~D Lee, Qi~Lei, Nikunj Saunshi, and Jiacheng Zhuo.
\newblock Predicting what you already know helps: Provable self-supervised
  learning.
\newblock \emph{arXiv preprint arXiv:2008.01064}, 2020.

\bibitem[Louis and Makarychev(2014)]{louis2014approximation}
Anand Louis and Konstantin Makarychev.
\newblock Approximation algorithm for sparsest k-partitioning.
\newblock In \emph{Proceedings of the twenty-fifth annual ACM-SIAM symposium on
  Discrete algorithms}, pages 1244--1255. SIAM, 2014.

\bibitem[Mansour et~al.(2009)Mansour, Mohri, and
  Rostamizadeh]{mansour2009domain}
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh.
\newblock Domain adaptation: Learning bounds and algorithms.
\newblock \emph{arXiv preprint arXiv:0902.3430}, 2009.

\bibitem[Park et~al.(2020)Park, Lee, Yoo, Hur, and
  Yoon]{park2020jointcontrastive}
Changhwa Park, Jonghyun Lee, Jaeyoon Yoo, Minhoe Hur, and Sungroh Yoon.
\newblock Joint contrastive learning for unsupervised domain adaptation.
\newblock \emph{arXiv preprint arXiv:2006.10297}, 2020.

\bibitem[Peng et~al.(2019)Peng, Bai, Xia, Huang, Saenko, and
  Wang]{peng2019moment}
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo~Wang.
\newblock Moment matching for multi-source domain adaptation.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1406--1415, 2019.

\bibitem[Sagawa et~al.(2022)Sagawa, Koh, Lee, Gao, Sang Michael~Xie, Kumar, Hu,
  Yasunaga, Henrik~Marklund, David, Stavness, Guo, Leskovec, Kate~Saenko,
  Levine, Finn, and Liang]{sagawa2021wilds}
Shiori Sagawa, Pang~Wei Koh, Tony Lee, Irena Gao, Kendrick~Shen Sang
  Michael~Xie, Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Sara~Beery
  Henrik~Marklund, Etienne David, Ian Stavness, Wei Guo, Jure Leskovec,
  Tatsunori~Hashimoto Kate~Saenko, Sergey Levine, Chelsea Finn, and Percy
  Liang.
\newblock Extending the wilds benchmark for unsupervised adaptation.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Santurkar et~al.(2020)Santurkar, Tsipras, and
  Madry]{santurkar2020breeds}
Shibani Santurkar, Dimitris Tsipras, and Aleksander Madry.
\newblock Breeds: Benchmarks for subpopulation shift.
\newblock \emph{arXiv}, 2020.

\bibitem[Shen et~al.(2022)Shen, Jones, Kumar, Xie, HaoChen, Ma, and
  Liang]{shen2021does}
Kendrick Shen, Robbie Jones, Ananya Kumar, Sang~Michael Xie, Jeff~Z. HaoChen,
  Tengyu Ma, and Percy Liang.
\newblock Connect, not collapse: Explaining contrastive learning for
  unsupervised domain adaptation.
\newblock \emph{arXiv preprint arXiv:2204.00570}, 2022.

\bibitem[Shimodaira(2000)]{shimodaira2000improving}
Hidetoshi Shimodaira.
\newblock Improving predictive inference under covariate shift by weighting the
  log-likelihood function.
\newblock \emph{Journal of statistical planning and inference}, 90\penalty0
  (2):\penalty0 227--244, 2000.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{snell2017prototypical}
Jake Snell, Kevin Swersky, and Richard Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Su et~al.(2021)Su, Liu, Meng, Lan, Shu, Shareghi, and
  Collier]{su2021tacl}
Yixuan Su, Fangyu Liu, Zaiqiao Meng, Tian Lan, Lei Shu, Ehsan Shareghi, and
  Nigel Collier.
\newblock Tacl: Improving bert pre-training with token-aware contrastive
  learning, 2021.

\bibitem[Sugiyama et~al.(2007)Sugiyama, Krauledat, and
  M{\~A}{\v{z}}ller]{sugiyama2007covariate}
Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert M{\~A}{\v{z}}ller.
\newblock Covariate shift adaptation by importance weighted cross validation.
\newblock \emph{Journal of Machine Learning Research}, 8\penalty0
  (May):\penalty0 985--1005, 2007.

\bibitem[Thota and Leontidis(2021)]{thota2021contrastive}
Mamatha Thota and Georgios Leontidis.
\newblock Contrastive domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 2209--2218, 2021.

\bibitem[Tosh et~al.(2020)Tosh, Krishnamurthy, and Hsu]{tosh2020contrastive}
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu.
\newblock Contrastive estimation reveals topic posterior information to linear
  models.
\newblock \emph{arXiv:2003.02234}, 2020.

\bibitem[Tosh et~al.(2021)Tosh, Krishnamurthy, and Hsu]{tosh2021contrastive}
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu.
\newblock Contrastive learning, multi-view redundancy, and linear models.
\newblock In \emph{Algorithmic Learning Theory}, pages 1179--1206. PMLR, 2021.

\bibitem[Wang et~al.(2021)Wang, Wu, Weng, Chen, Qi, and
  Jiang]{wang2021crossdomain}
Rui Wang, Zuxuan Wu, Zejia Weng, Jingjing Chen, Guo-Jun Qi, and Yu-Gang Jiang.
\newblock Cross-domain contrastive learning for unsupervised domain adaptation.
\newblock \emph{arXiv preprint arXiv:2106.05528}, 2021.

\bibitem[Wei et~al.(2020)Wei, Shen, Chen, and Ma]{wei2020theoretical}
Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma.
\newblock Theoretical analysis of self-training with deep networks on unlabeled
  data, 2020.
\newblock URL \url{https://openreview.net/forum?id=rC8sJ4i6kaH}.

\bibitem[Wortsman et~al.(2021)Wortsman, Ilharco, Li, Kim, Hajishirzi, Farhadi,
  Namkoong, and Schmidt]{wortsman2021robust}
Mitchell Wortsman, Gabriel Ilharco, Mike Li, Jong~Wook Kim, Hannaneh
  Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt.
\newblock Robust fine-tuning of zero-shot models.
\newblock \emph{arXiv preprint arXiv:2109.01903}, 2021.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs, Gontijo-Lopes,
  Morcos, Namkoong, Farhadi, Carmon, Kornblith, et~al.]{wortsman2022model}
Mitchell Wortsman, Gabriel Ilharco, Samir~Yitzhak Gadre, Rebecca Roelofs,
  Raphael Gontijo-Lopes, Ari~S Morcos, Hongseok Namkoong, Ali Farhadi, Yair
  Carmon, Simon Kornblith, et~al.
\newblock Model soups: averaging weights of multiple fine-tuned models improves
  accuracy without increasing inference time.
\newblock \emph{arXiv preprint arXiv:2203.05482}, 2022.

\bibitem[Xie et~al.(2020)Xie, Kumar, Jones, Khani, Ma, and Liang]{xie2020n}
Sang~Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and
  Percy Liang.
\newblock In-n-out: Pre-training and self-training using auxiliary information
  for out-of-distribution robustness.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{zbontar2021barlow}
Jure Zbontar, Li~Jing, Ishan Misra, Yann LeCun, and St{\'e}phane Deny.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock \emph{arXiv preprint arXiv:2103.03230}, 2021.

\bibitem[Zhang et~al.(2019)Zhang, Liu, Long, and Jordan]{zhang2019bridging}
Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael~I Jordan.
\newblock Bridging theory and algorithm for domain adaptation.
\newblock \emph{arXiv preprint arXiv:1904.05801}, pages 7404--7413, 2019.

\bibitem[Zhao et~al.(2019)Zhao, Combes, Zhang, and Gordon]{zhao2019learning}
Han Zhao, Remi Tachet~Des Combes, Kun Zhang, and Geoffrey Gordon.
\newblock On learning invariant representations for domain adaptation.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, pages 7523--7532. PMLR, 09--15 Jun 2019.
\newblock URL \url{http://proceedings.mlr.press/v97/zhao19a.html}.

\bibitem[Ziser and Reichart(2017)]{ziser2017neural}
Yftah Ziser and Roi Reichart.
\newblock Neural structural correspondence learning for domain adaptation.
\newblock In \emph{Proceedings of the 21st Conference on Computational Natural
  Language Learning (CoNLL 2017)}, pages 400--410, 2017.

\bibitem[Ziser and Reichart(2018)]{ziser2018deep}
Yftah Ziser and Roi Reichart.
\newblock Deep pivot-based modeling for cross-language cross-domain transfer
  with minimal guidance.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 238--249, 2018.

\end{thebibliography}
