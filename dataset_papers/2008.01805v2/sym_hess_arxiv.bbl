\begin{thebibliography}{10}

\bibitem{arjevani2019spurious}
Yossi Arjevani and Michael Field.
\newblock On the principle of least symmetry breaking in shallow relu models.
\newblock {\em arXiv preprint arXiv:1912.11939}, 2019.

\bibitem{ArjevaniField2020}
Yossi Arjevani and Michael Field.
\newblock Symmetry {\&} critical points for a model shallow neural network.
\newblock {\em CoRR}, abs/2003.10576, 2020.

\bibitem{bottou1991stochastic}
L{\'e}on Bottou.
\newblock Stochastic gradient learning in neural networks.
\newblock {\em Proceedings of Neuro-N{\i}mes}, 91(8):12, 1991.

\bibitem{brutzkus2017globally}
Alon Brutzkus and Amir Globerson.
\newblock Globally optimal gradient descent for a convnet with gaussian inputs.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 605--614. JMLR. org, 2017.

\bibitem{chaudhari2019entropy}
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi,
  Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2019(12):124018, 2019.

\bibitem{dinh2017sharp}
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
\newblock Sharp minima can generalize for deep nets.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1019--1028. JMLR. org, 2017.

\bibitem{draxler2018essentially}
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred~A. Hamprecht.
\newblock Essentially no barriers in neural network energy landscape.
\newblock In Jennifer~G. Dy and Andreas Krause, editors, {\em Proceedings of
  the 35th International Conference on Machine Learning, {ICML} 2018,
  Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of
  {\em Proceedings of Machine Learning Research}, pages 1308--1317. {PMLR},
  2018.

\bibitem{du2017gradient}
Simon~S. Du, Jason~D. Lee, Yuandong Tian, Aarti Singh, and Barnab{\'{a}}s
  P{\'{o}}czos.
\newblock Gradient descent learns one-hidden-layer {CNN:} don't be afraid of
  spurious local minima.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, pages 1338--1347, 2018.

\bibitem{feizi2017porcupine}
Soheil Feizi, Hamid Javadi, Jesse Zhang, and David Tse.
\newblock Porcupine neural networks:(almost) all local optima are global.
\newblock {\em arXiv preprint arXiv:1710.02196}, 2017.

\bibitem{Field2007}
Michael~J. Field.
\newblock {\em Dynamics and symmetry}, volume~3 of {\em ICP Advanced Texts in
  Mathematics}.
\newblock Imperial College Press, London, 2007.

\bibitem{fulton1991representation}
William Fulton and Joe Harris.
\newblock Representation theory, volume 129 of.
\newblock {\em Graduate Texts in Mathematics}, 1991.

\bibitem{DBLP:conf/nips/GeLM16}
Rong Ge, Jason~D. Lee, and Tengyu Ma.
\newblock Matrix completion has no spurious local minimum.
\newblock In Daniel~D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle
  Guyon, and Roman Garnett, editors, {\em Advances in Neural Information
  Processing Systems 29: Annual Conference on Neural Information Processing
  Systems 2016, December 5-10, 2016, Barcelona, Spain}, pages 2973--2981, 2016.

\bibitem{ge2017learning}
Rong Ge, Jason~D. Lee, and Tengyu Ma.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}, 2018.

\bibitem{ghorbani2019investigation}
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao.
\newblock An investigation into neural net optimization via hessian eigenvalue
  density.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, {\em
  Proceedings of the 36th International Conference on Machine Learning, {ICML}
  2019, 9-15 June 2019, Long Beach, California, {USA}}, volume~97 of {\em
  Proceedings of Machine Learning Research}, pages 2232--2241. {PMLR}, 2019.

\bibitem{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256, 2010.

\bibitem{goodfellow2014qualitatively}
Ian~J. Goodfellow and Oriol Vinyals.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock In Yoshua Bengio and Yann LeCun, editors, {\em 3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.

\bibitem{gunasekar2018implicit}
Suriya Gunasekar, Jason~D Lee, Daniel Soudry, and Nati Srebro.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9461--9471, 2018.

\bibitem{hochreiter1997flat}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Flat minima.
\newblock {\em Neural Computation}, 9(1):1--42, 1997.

\bibitem{James1978}
GD~James.
\newblock The representation theory of the symmetric groups.
\newblock Springer, 1978.

\bibitem{jastrzkebski2017three}
Stanis{\l}aw Jastrz{\k{e}}bski, Zachary Kenton, Devansh Arpit, Nicolas Ballas,
  Asja Fischer, Yoshua Bengio, and Amos Storkey.
\newblock Three factors influencing minima in sgd.
\newblock {\em arXiv preprint arXiv:1711.04623}, 2017.

\bibitem{DBLP:journals/corr/abs-2001-00098}
Abbas Kazemipour, Brett Larsen, and Shaul Druckmann.
\newblock No spurious local minima in deep quadratic networks.
\newblock {\em CoRR}, abs/2001.00098, 2020.

\bibitem{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock {\em arXiv preprint arXiv:1609.04836}, 2016.

\bibitem{lecun2012efficient}
Yann~A LeCun, L{\'e}on Bottou, Genevieve~B Orr, and Klaus-Robert M{\"u}ller.
\newblock Efficient backprop.
\newblock In {\em Neural networks: Tricks of the trade}, pages 9--48. Springer,
  2012.

\bibitem{li2018visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6389--6399, 2018.

\bibitem{li2017convergence}
Yuanzhi Li and Yang Yuan.
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  597--607, 2017.

\bibitem{louart2018random}
Cosme Louart, Zhenyu Liao, Romain Couillet, et~al.
\newblock A random matrix approach to neural networks.
\newblock {\em The Annals of Applied Probability}, 28(2):1190--1248, 2018.

\bibitem{papyan2018full}
Vardan Papyan.
\newblock The full spectrum of deepnet hessians at scale: Dynamics with sgd
  training and sample size.
\newblock {\em arXiv preprint arXiv:1811.07062}, 2018.

\bibitem{pennington2017nonlinear}
Jeffrey Pennington and Pratik Worah.
\newblock Nonlinear random matrix theory for deep learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2637--2646, 2017.

\bibitem{pennington2018spectrum}
Jeffrey Pennington and Pratik Worah.
\newblock The spectrum of the fisher information matrix of a
  single-hidden-layer neural network.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5410--5419, 2018.

\bibitem{safran2017spurious}
Itay Safran and Ohad Shamir.
\newblock Spurious local minima are common in two-layer relu neural networks.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, pages 4430--4438, 2018.

\bibitem{sagun2016eigenvalues}
Levent Sagun, Leon Bottou, and Yann LeCun.
\newblock Eigenvalues of the hessian in deep learning: Singularity and beyond.
\newblock {\em arXiv preprint arXiv:1611.07476}, 2016.

\bibitem{sagun2017empirical}
Levent Sagun, Utku Evci, V~Ugur Guney, Yann Dauphin, and Leon Bottou.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock {\em arXiv preprint arXiv:1706.04454}, 2017.

\bibitem{shalev2010learnability}
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan.
\newblock Learnability, stability and uniform convergence.
\newblock {\em Journal of Machine Learning Research}, 11(Oct):2635--2670, 2010.

\bibitem{DBLP:journals/corr/SoudryC16}
Daniel Soudry and Yair Carmon.
\newblock No bad local minima: Data independent training error guarantees for
  multilayer neural networks.
\newblock {\em CoRR}, abs/1605.08361, 2016.

\bibitem{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock {\em The Journal of Machine Learning Research}, 19(1):2822--2878,
  2018.

\bibitem{thomas2004representations}
Charles~Benedict Thomas.
\newblock {\em Representations of finite and Lie groups}.
\newblock World Scientific, 2004.

\bibitem{Thomas2004}
Charles~Benedict Thomas.
\newblock {\em Representations of finite and Lie groups}.
\newblock World Scientific, 2004.

\bibitem{tian2017analytical}
Yuandong Tian.
\newblock An analytical formula of population gradient for two-layered relu
  network and its applications in convergence and critical point analysis.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 3404--3413. JMLR. org, 2017.

\bibitem{wu2017towards}
Lei Wu, Zhanxing Zhu, et~al.
\newblock Towards understanding generalization of deep learning: Perspective of
  loss landscapes.
\newblock {\em arXiv preprint arXiv:1706.10239}, 2017.

\bibitem{yao2018hessian}
Zhewei Yao, Amir Gholami, Qi~Lei, Kurt Keutzer, and Michael~W Mahoney.
\newblock Hessian-based analysis of large batch training and robustness to
  adversaries.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4949--4959, 2018.

\bibitem{zhang2017electron}
Qiuyi Zhang, Rina Panigrahy, Sushant Sachdeva, and Ali Rahimi.
\newblock Electron-proton dynamics in deep learning.
\newblock {\em arXiv preprint arXiv:1702.00458}, pages 1--31, 2017.

\bibitem{zhu2019anisotropic}
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma.
\newblock The anisotropic noise in stochastic gradient descent: Its behavior of
  escaping from sharp minima and regularization effects.
\newblock In {\em Proc. Int. Conf. Mach. Learn.}, pages 7654--7663, 2019.

\end{thebibliography}
