\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[gem()]{gemini}
Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.
\newblock URL \url{https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf}.

\bibitem[mov()]{movieclip}
{MovieClips}.
\newblock URL \url{https://www.movieclips.com/}.

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock {GPT-4} technical report.
\newblock \emph{arXiv}, 2023.

\bibitem[Akbari et~al.(2021)Akbari, Yuan, Qian, Chuang, Chang, Cui, and Gong]{akbari2021vatt}
Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong.
\newblock {VATT}: {T}ransformers for multimodal self-supervised learning from raw video, audio and text.
\newblock \emph{arXiv}, 2021.

\bibitem[Bertasius et~al.(2021)Bertasius, Wang, and Torresani]{bertasius2021space}
Gedas Bertasius, Heng Wang, and Lorenzo Torresani.
\newblock Is space-time attention all you need for video understanding?
\newblock In \emph{ICML}, 2021.

\bibitem[Bolya \& Hoffman(2023)Bolya and Hoffman]{bolya2023tomesd}
Daniel Bolya and Judy Hoffman.
\newblock Token merging for fast stable diffusion.
\newblock In \emph{CVPR Workshop}, 2023.

\bibitem[Bolya et~al.(2022)Bolya, Fu, Dai, Zhang, Feichtenhofer, and Hoffman]{bolya2022tome}
Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman.
\newblock Token {M}erging: {Y}our {ViT} but faster.
\newblock In \emph{ICLR}, 2022.

\bibitem[Brooks et~al.(2024)Brooks, Peebles, Holmes, DePue, Guo, Jing, Schnurr, Taylor, Luhman, Luhman, Ng, Wang, and Ramesh]{videoworldsimulators2024}
Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li~Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh.
\newblock Video generation models as world simulators.
\newblock 2024.
\newblock URL \url{https://openai.com/research/video-generation-models-as-world-simulators}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Carreira et~al.(2018)Carreira, Noland, Banki-Horvath, Hillier, and Zisserman]{carreira2018short}
Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman.
\newblock A short note about kinetics-600.
\newblock \emph{arXiv}, 2018.

\bibitem[Choromanski et~al.(2021)Choromanski, Likhosherstov, Dohan, Song, Gane, Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and Weller]{choromanski2020performer}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy~J. Colwell, and Adrian Weller.
\newblock Rethinking attention with performers.
\newblock In \emph{ICLR}, 2021.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and Salakhutdinov]{dai2019transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc~V Le, and Ruslan Salakhutdinov.
\newblock Transformer-{XL}: Attentive language models beyond a fixed-length context.
\newblock \emph{arXiv}, 2019.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv}, 2018.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiy2020vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: {T}ransformers for image recognition at scale.
\newblock In \emph{ICLR}, 2021.

\bibitem[Fan et~al.(2023)Fan, Wang, Liao, Zhu, Bhat, Santos-Villalobos, MV, and Li]{fan2023motion}
David Fan, Jue Wang, Shuai Liao, Yi~Zhu, Vimal Bhat, Hector Santos-Villalobos, Rohith MV, and Xinyu Li.
\newblock Motion-guided masking for spatiotemporal representation learning.
\newblock In \emph{ICCV}, 2023.

\bibitem[Fan et~al.(2021)Fan, Xiong, Mangalam, Li, Yan, Malik, and Feichtenhofer]{fan2021multiscale}
Haoqi Fan, Bo~Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer.
\newblock Multiscale vision transformers.
\newblock In \emph{Int. Conf. Comput. Vis.}, 2021.

\bibitem[Gotmare et~al.(2018)Gotmare, Keskar, Xiong, and Socher]{gotmare2018closer}
Akhilesh Gotmare, Nitish~Shirish Keskar, Caiming Xiong, and Richard Socher.
\newblock A closer look at deep learning heuristics: {L}earning rate restarts, warmup and distillation.
\newblock In \emph{arXiv}, 2018.

\bibitem[Gu et~al.(2021)Gu, Goel, and R{\'e}]{gu2021efficiently}
Albert Gu, Karan Goel, and Christopher R{\'e}.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock \emph{arXiv preprint arXiv:2111.00396}, 2021.

\bibitem[Hussein et~al.(2019{\natexlab{a}})Hussein, Gavves, and Smeulders]{hussein2019timeception}
Noureldien Hussein, Efstratios Gavves, and Arnold~WM. Smeulders.
\newblock Timeception for complex action recognition.
\newblock In \emph{CVPR}, 2019{\natexlab{a}}.

\bibitem[Hussein et~al.(2019{\natexlab{b}})Hussein, Gavves, and Smeulders]{hussein2019videograph}
Noureldien Hussein, Efstratios Gavves, and Arnold~WM. Smeulders.
\newblock Video{G}raph: {R}ecognizing minutes-long human activities in videos.
\newblock In \emph{ICCV Workshop}, 2019{\natexlab{b}}.

\bibitem[Islam \& Bertasius(2022)Islam and Bertasius]{islam2022vis4mer}
Md~Mohaiminul Islam and Gedas Bertasius.
\newblock Long movie clip classification with state-space video models.
\newblock In \emph{ECCV}, 2022.

\bibitem[Kay et~al.(2017)Kay, Carreira, Simonyan, Zhang, Hillier, Vijayanarasimhan, Viola, Green, Back, Natsev, Suleyman, and Zisserman]{kay2017kinetics}
Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman.
\newblock The kinetics human action video dataset.
\newblock In \emph{arXiv}, 2017.

\bibitem[Kuehne et~al.(2014)Kuehne, Arslan, and Serre]{kuehne2014breakfast}
Hilde Kuehne, Ali Arslan, and Thomas Serre.
\newblock The language of actions: {R}ecovering the syntax and semantics of goal-directed human activities.
\newblock In \emph{CVPR}, 2014.

\bibitem[Li et~al.(2024)Li, Ma, Yang, and Yang]{li2023vidtome}
Xirui Li, Chao Ma, Xiaokang Yang, and Ming-Hsuan Yang.
\newblock {VidToMe}: {V}ideo token merging for zero-shot video editing.
\newblock In \emph{CVPR}, 2024.

\bibitem[Li et~al.(2022)Li, Wu, Fan, Mangalam, Xiong, Malik, and Feichtenhofer]{li2022mvitv2}
Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo~Xiong, Jitendra Malik, and Christoph Feichtenhofer.
\newblock {MViTv2}: {I}mproved multiscale vision transformers for classification and detection.
\newblock In \emph{CVPR}, 2022.

\bibitem[Liang et~al.(2022)Liang, Ge, Tong, Song, Wang, and Xie]{liang2022not}
Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie.
\newblock Not all patches are what you need: Expediting vision transformers via token reorganizations.
\newblock \emph{arXiv preprint arXiv:2202.07800}, 2022.

\bibitem[Lin et~al.(2022)Lin, Petroni, Bertasius, Rohrbach, Chang, and Torresani]{lin2022learning}
Xudong Lin, Fabio Petroni, Gedas Bertasius, Marcus Rohrbach, Shih-Fu Chang, and Lorenzo Torresani.
\newblock Learning to recognize procedural activities with distant supervision.
\newblock In \emph{CVPR}, 2022.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo]{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
\newblock Swin transformer: {H}ierarchical vision transformer using shifted windows.
\newblock In \emph{ICCV}, 2021.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{arXiv}, 2017.

\bibitem[Meng et~al.(2022)Meng, Li, Chen, Lan, Wu, Jiang, and Lim]{meng2022adavit}
Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang Jiang, and Ser-Nam Lim.
\newblock {AdaViT}: {A}daptive vision transformers for efficient image recognition.
\newblock In \emph{CVPR}, 2022.

\bibitem[Miech et~al.(2019)Miech, Zhukov, Alayrac, Tapaswi, Laptev, and Sivic]{miech2019howto100m}
Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
\newblock {HowTo100M}: {L}earning a text-video embedding by watching hundred million narrated video clips.
\newblock In \emph{ICCV}, pp.\  2630--2640, 2019.

\bibitem[Patrick et~al.(2021)Patrick, Campbell, Asano, Misra, Metze, Feichtenhofer, Vedaldi, and Henriques]{patrick2021orthoformer}
Mandela Patrick, Dylan Campbell, Yuki Asano, Ishan Misra, Florian Metze, Christoph Feichtenhofer, Andrea Vedaldi, and Joao~F. Henriques.
\newblock Keeping your eye on the ball: {T}rajectory attention in video transformers.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Rao et~al.(2021)Rao, Zhao, Liu, Lu, Zhou, and Hsieh]{rao2021dynamicvit}
Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh.
\newblock {DynamicViT}: {E}fficient vision transformers with dynamic token sparsification.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Ren et~al.(2023)Ren, Chen, Li, Sun, and Hou]{ren2023testa}
Shuhuai Ren, Sishuo Chen, Shicheng Li, Xu~Sun, and Lu~Hou.
\newblock {TESTA}: {T}emporal-spatial token aggregation for long-form video-language understanding.
\newblock In \emph{EMNLP}, 2023.

\bibitem[Richardson(2004)]{richardson2004h}
Iain~E. Richardson.
\newblock \emph{H. 264 and {MPEG-4} video compression: video coding for next-generation multimedia}.
\newblock 2004.

\bibitem[Ridnik et~al.(2021)Ridnik, Ben-Baruch, Noy, and Zelnik-Manor]{ridnik2021imagenet}
Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor.
\newblock {ImageNet}-{21K} pretraining for the masses.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Sun et~al.(2019)Sun, Myers, Vondrick, Murphy, and Schmid]{sun2019videobert}
Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid.
\newblock {VideoBERT}: {A} joint model for video and language representation learning.
\newblock In \emph{ICCV}, 2019.

\bibitem[Sun et~al.(2022)Sun, Liu, Xue, Sone, Yang, and Fu]{sunlong2022}
Yuchong Sun, Bei Liu, Hongwei Xue, Ruihua Sone, Huan Yang, and Jianlong Fu.
\newblock Long-form video-language pre-training with multimodal temporal contrastive learning.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Tang et~al.(2019)Tang, Ding, Rao, Zheng, Zhang, Zhao, Lu, and Zhou]{tang2019coin}
Yansong Tang, Dajun Ding, Yongming Rao, Yu~Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou.
\newblock {COIN}: {A} large-scale dataset for comprehensive instructional video analysis.
\newblock In \emph{CVPR}, 2019.

\bibitem[Tang et~al.(2020)Tang, Lu, and Zhou]{tang2020comprehensive}
Yansong Tang, Jiwen Lu, and Jie Zhou.
\newblock Comprehensive instructional video analysis: {The COIN} dataset and performance evaluation.
\newblock \emph{IEEE TPAMI}, 43\penalty0 (9):\penalty0 3138--3153, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{arXiv preprint arXiv:1706.03762}, 2017.

\bibitem[Wang \& Torresani(2022)Wang and Torresani]{wang2022deformable}
Jue Wang and Lorenzo Torresani.
\newblock Deformable video transformer.
\newblock In \emph{CVPR}, 2022.

\bibitem[Wang et~al.(2022)Wang, Bertasius, Tran, and Torresani]{wang2022long}
Jue Wang, Gedas Bertasius, Du~Tran, and Lorenzo Torresani.
\newblock Long-short temporal contrastive learning of video transformers.
\newblock In \emph{CVPR}, 2022.

\bibitem[Wang et~al.(2023)Wang, Zhu, Wang, Yu, Liu, Omar, and Hamid]{wang2023s5}
Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid.
\newblock Selective structured state-spaces for long-form video understanding.
\newblock In \emph{CVPR}, 2023.

\bibitem[Wang et~al.(2021)Wang, Yang, Li, Wu, and Jiang]{wang2021efficient}
Junke Wang, Xitong Yang, Hengduo Li, Zuxuan Wu, and Yu-Gang Jiang.
\newblock Efficient video transformers with spatial-temporal token selection.
\newblock \emph{arXiv preprint arXiv:2111.11591}, 2021.

\bibitem[Wien(2015)]{wien2015high}
Mathias Wien.
\newblock High efficiency video coding.
\newblock \emph{Coding Tools and Specification}, 24, 2015.

\bibitem[Wu \& Kr\"{a}henb\"{u}hl(2021)Wu and Kr\"{a}henb\"{u}hl]{wu2021lvu}
Chao-Yuan Wu and Philipp Kr\"{a}henb\"{u}hl.
\newblock Towards long-form video understanding.
\newblock In \emph{CVPR}, 2021.

\bibitem[Wu et~al.(2022)Wu, Li, Mangalam, Fan, Xiong, Malik, and Feichtenhofer]{wu2022memvit}
Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo~Xiong, Jitendra Malik, and Christoph Feichtenhofer.
\newblock Mem{ViT}: {M}emory-augmented multiscale vision transformer for efficient long-term video recognition.
\newblock In \emph{CVPR}, 2022.

\bibitem[Yin et~al.(2021)Yin, Vahdat, Alvarez, Mallya, Kautz, and Molchanov]{yin2021adavit}
Hongxu Yin, Arash Vahdat, Jose Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov.
\newblock Adavit: Adaptive tokens for efficient vision transformer.
\newblock \emph{arXiv preprint arXiv:2112.07658}, 2021.

\bibitem[Zhou et~al.(2021)Zhou, Lin, Li, and Zheng]{zhou2021graph}
Jiaming Zhou, Kun-Yu Lin, Haoxin Li, and Wei-Shi Zheng.
\newblock Graph-based high-order relation modeling for long-term action recognition.
\newblock In \emph{CVPR}, 2021.

\end{thebibliography}
