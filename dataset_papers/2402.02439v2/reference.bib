@article{wu2023masked,
  title={Masked trajectory models for prediction, representation, and control},
  author={Wu, Philipp and Majumdar, Arjun and Stone, Kevin and Lin, Yixin and Mordatch, Igor and Abbeel, Pieter and Rajeswaran, Aravind},
  journal={arXiv preprint arXiv:2305.02968},
  year={2023}
}

@article{ajay2022conditional,
  title={Is conditional generative modeling all you need for decision-making?},
  author={Ajay, Anurag and Du, Yilun and Gupta, Abhi and Tenenbaum, Joshua and Jaakkola, Tommi and Agrawal, Pulkit},
  journal={arXiv preprint arXiv:2211.15657},
  year={2022}
}
@inproceedings{xiao2021general,
  title={A general offline reinforcement learning framework for interactive recommendation},
  author={Xiao, Teng and Wang, Donglin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={5},
  pages={4512--4520},
  year={2021}
}
@article{shi2021offline,
  title={Offline reinforcement learning for autonomous driving with safety and exploration enhancement},
  author={Shi, Tianyu and Chen, Dong and Chen, Kaian and Li, Zhaojian},
  journal={arXiv preprint arXiv:2110.07067},
  year={2021}
}
@article{kidambi2020morel,
  title={Morel: Model-based offline reinforcement learning},
  author={Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21810--21823},
  year={2020}
}
@article{janner2021offline,
  title={Offline reinforcement learning as one big sequence modeling problem},
  author={Janner, Michael and Li, Qiyang and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={1273--1286},
  year={2021}
}
@article{lyu2022double,
  title={Double Check Your State Before Trusting It: Confidence-Aware Bidirectional Offline Model-Based Imagination},
  author={Lyu, Jiafei and Li, Xiu and Lu, Zongqing},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38218--38231},
  year={2022}
}
@article{wang2021offline,
  title={Offline reinforcement learning with reverse model-based imagination},
  author={Wang, Jianhao and Li, Wenzhe and Jiang, Haozhe and Zhu, Guangxiang and Li, Siyuan and Zhang, Chongjie},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={29420--29432},
  year={2021}
}
@article{siegel2020keep,
  title={Keep doing what worked: Behavioral modelling priors for offline reinforcement learning},
  author={Siegel, Noah Y and Springenberg, Jost Tobias and Berkenkamp, Felix and Abdolmaleki, Abbas and Neunert, Michael and Lampe, Thomas and Hafner, Roland and Heess, Nicolas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:2002.08396},
  year={2020}
}
@article{wang2020critic,
  title={Critic regularized regression},
  author={Wang, Ziyu and Novikov, Alexander and Zolna, Konrad and Merel, Josh S and Springenberg, Jost Tobias and Reed, Scott E and Shahriari, Bobak and Siegel, Noah and Gulcehre, Caglar and Heess, Nicolas and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7768--7778},
  year={2020}
}
@article{chen2020bail,
  title={Bail: Best-action imitation learning for batch deep reinforcement learning},
  author={Chen, Xinyue and Zhou, Zijian and Wang, Zheng and Wang, Che and Wu, Yanqiu and Ross, Keith},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18353--18363},
  year={2020}
}
@article{brandfonbrener2021offline,
  title={Offline rl without off-policy evaluation},
  author={Brandfonbrener, David and Whitney, Will and Ranganath, Rajesh and Bruna, Joan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={4933--4946},
  year={2021}
}
@article{yu2020mopo,
  title={Mopo: Model-based offline policy optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James Y and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14129--14142},
  year={2020}
}
@article{matsushima2020deployment,
  title={Deployment-efficient reinforcement learning via model-based offline optimization},
  author={Matsushima, Tatsuya and Furuta, Hiroki and Matsuo, Yutaka and Nachum, Ofir and Gu, Shixiang},
  journal={arXiv preprint arXiv:2006.03647},
  year={2020}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{prudencio2023survey,
  title={A survey on offline reinforcement learning: Taxonomy, review, and open problems},
  author={Prudencio, Rafael Figueiredo and Maximo, Marcos ROA and Colombini, Esther Luna},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2023},
  publisher={IEEE}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{fujimoto2021minimalist,
  title={A minimalist approach to offline reinforcement learning},
  author={Fujimoto, Scott and Gu, Shixiang Shane},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={20132--20145},
  year={2021}
}
@inproceedings{agarwal2020optimistic,
  title={An optimistic perspective on offline reinforcement learning},
  author={Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
  booktitle={International Conference on Machine Learning},
  pages={104--114},
  year={2020},
  organization={PMLR}
}
@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@article{wang2022bootstrapped,
  title={Bootstrapped transformer for offline reinforcement learning},
  author={Wang, Kerong and Zhao, Hanye and Luo, Xufang and Ren, Kan and Zhang, Weinan and Li, Dongsheng},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34748--34761},
  year={2022}
}
@inproceedings{fatemi2022semi,
  title={Semi-markov offline reinforcement learning for healthcare},
  author={Fatemi, Mehdi and Wu, Mary and Petch, Jeremy and Nelson, Walter and Connolly, Stuart J and Benz, Alexander and Carnicelli, Anthony and Ghassemi, Marzyeh},
  booktitle={Conference on Health, Inference, and Learning},
  pages={119--137},
  year={2022},
  organization={PMLR}
}
@article{jaques2020human,
  title={Human-centric dialog training via offline reinforcement learning},
  author={Jaques, Natasha and Shen, Judy Hanwen and Ghandeharioun, Asma and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang Shane and Picard, Rosalind},
  journal={arXiv preprint arXiv:2010.05848},
  year={2020}
}
@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@article{agrawal2016learning,
  title={Learning to poke by poking: Experiential learning of intuitive physics},
  author={Agrawal, Pulkit and Nair, Ashvin V and Abbeel, Pieter and Malik, Jitendra and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{pathak2018zero,
  title={Zero-shot visual imitation},
  author={Pathak, Deepak and Mahmoudieh, Parsa and Luo, Guanghao and Agrawal, Pulkit and Chen, Dian and Shentu, Yide and Shelhamer, Evan and Malik, Jitendra and Efros, Alexei A and Darrell, Trevor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition workshops},
  pages={2050--2053},
  year={2018}
}

@article{liang2023adaptdiffuser,
  title={Adaptdiffuser: Diffusion models as adaptive self-evolving planners},
  author={Liang, Zhixuan and Mu, Yao and Ding, Mingyu and Ni, Fei and Tomizuka, Masayoshi and Luo, Ping},
  journal={arXiv preprint arXiv:2302.01877},
  year={2023}
}

@article{ho2022classifier,
  title={Classifier-free diffusion guidance},
  author={Ho, Jonathan and Salimans, Tim},
  journal={arXiv preprint arXiv:2207.12598},
  year={2022}
}

@article{fu2020d4rl,
  title={D4rl: Datasets for deep data-driven reinforcement learning},
  author={Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2004.07219},
  year={2020}
}

@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}



@article{kostrikov2021offline,
  title={Offline reinforcement learning with implicit q-learning},
  author={Kostrikov, Ilya and Nair, Ashvin and Levine, Sergey},
  journal={arXiv preprint arXiv:2110.06169},
  year={2021}
}

@article{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15084--15097},
  year={2021}
}

@article{lu2023synthetic,
  title={Synthetic experience replay},
  author={Lu, Cong and Ball, Philip J and Parker-Holder, Jack},
  journal={arXiv preprint arXiv:2303.06614},
  year={2023}
}


@incollection{zhang2023uncertainty,
  title={Uncertainty-Driven Trajectory Truncation for Data Augmentation in Offline Reinforcement Learning},
  author={Zhang, Junjie and Lyu, Jiafei and Ma, Xiaoteng and Yan, Jiangpeng and Yang, Jun and Wan, Le and Li, Xiu},
  booktitle={ECAI 2023},
  pages={3018--3025},
  year={2023},
  publisher={IOS Press}
}

@article{hepburn2022model,
  title={Model-based trajectory stitching for improved offline reinforcement learning},
  author={Hepburn, Charles A and Montana, Giovanni},
  journal={arXiv preprint arXiv:2211.11603},
  year={2022}
}

@article{gollwitzer1996goal,
  title={Goal effects on action and cognition},
  author={Gollwitzer, Peter M and Moskowitz, Gordon B},
  year={1996}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{hua2021learning,
  title={Learning for a robot: Deep reinforcement learning, imitation learning, transfer learning},
  author={Hua, Jiang and Zeng, Liangcai and Li, Gongfa and Ju, Zhaojie},
  journal={Sensors},
  volume={21},
  number={4},
  pages={1278},
  year={2021},
  publisher={MDPI}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{kumar2020conservative,
  title={Conservative q-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1179--1191},
  year={2020}
}

@article{janner2022planning,
  title={Planning with diffusion for flexible behavior synthesis},
  author={Janner, Michael and Du, Yilun and Tenenbaum, Joshua B and Levine, Sergey},
  journal={arXiv preprint arXiv:2205.09991},
  year={2022}
}


@article{tarasov2022corl,
  title={CORL: Research-oriented deep offline reinforcement learning library},
  author={Tarasov, Denis and Nikulin, Alexander and Akimov, Dmitry and Kurenkov, Vladislav and Kolesnikov, Sergey},
  journal={arXiv preprint arXiv:2210.07105},
  year={2022}
}


@inproceedings{lugmayr2022repaint,
  title={Repaint: Inpainting using denoising diffusion probabilistic models},
  author={Lugmayr, Andreas and Danelljan, Martin and Romero, Andres and Yu, Fisher and Timofte, Radu and Van Gool, Luc},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11461--11471},
  year={2022}
}

@inproceedings{tevet2022human,
  title={Human Motion Diffusion Model},
  author={Tevet, Guy and Raab, Sigal and Gordon, Brian and Shafir, Yoni and Cohen-or, Daniel and Bermano, Amit Haim},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}