\begin{thebibliography}{10}

\bibitem{Bottou_OptML}
L.~Bottou, F.~E. Curtis, and J.~Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em arXiv preprint arXiv:1606.04838}, 2016.

\bibitem{Duchi_adagrad}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of Machine Learning Research}, 12:2121--2159, 2011.

\bibitem{Kingma_adam}
D.~P. Kingma and J.~L. Ba.
\newblock {ADAM}: a method for stochastic optimization.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2015.

\bibitem{Hinton_RMSProp}
T.~Tieleman and G.~Hinton.
\newblock Lecture 6.e. {RMSProp}: divide the gradient by a running average of
  its recent magnitude.
\newblock {\em COURSERA: Neural Networks for Machine Learning}, 2012.

\bibitem{Nesterov_NAG}
Y.~Nesterov.
\newblock A method for unconstrained convex minimization problem with the rate
  of convergence $o(1/k^{2})$.
\newblock {\em Soviet Math Docl}, 269:543--547, 1983.

\bibitem{Beyer_ESIntro}
H.-G. Beyer and H.-P. Schwefel.
\newblock Evolution strategies: a comprehensive introduction.
\newblock {\em Natural computing}, 1(1):3--52, 2002.

\bibitem{Goldberg_GA}
D.~E. Goldberg.
\newblock {\em Genetic algorithm in search, optimization and machine learning}.
\newblock Addison-Wesley Publishing Co., 1989.

\bibitem{Igel_Neuroevol}
C.~Igel.
\newblock Neuroevolution for reinforcement learning using evolution strategies.
\newblock In {\em IEEE Congress of Evolutionary Computation (CEC)}, page
  2588â€“2595, 2003.

\bibitem{Hansen_CMA}
N.~Hansen.
\newblock The {CMA} evolution strategy: a tutorial.
\newblock {\em arXiv preprint arXiv:1604.00772}, 2016.

\bibitem{Loshchilov_LMCMA}
I.~Loshchilov.
\newblock {LM-CMA}: an alternative to {L-BFGS} for large scale black-box
  optimization.
\newblock {\em Evolutionary Computation}, 25(1):143--171, 2017.

\bibitem{Real_EvolImageClassifier}
E.~Real, S.~Moore, A.~Selle, S.~Saxena, Y.~L. Suematsu, J.~Tan, Q.~V. Le, and
  A.~Kurakin.
\newblock Large-scale evolution of image classifiers.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  2902--2911, 2017.

\bibitem{Real_RegEvolImageClassifier}
E.~Real, A.~Aggarwal, Y.~Huang, and Q.~V. Le.
\newblock Regularized evolution for image classifier architecture search.
\newblock {\em arXiv preprint arXiv:1802.01548}, 2018.

\bibitem{Liang_EvolArchDMN}
J.~Liang, E.~Meyerson, and R.~Miikkulainen.
\newblock Evolutionary architecture search for deep multitask networks.
\newblock {\em arXiv preprint arXiv:1803.03745}, 2018.

\bibitem{Ebrahimi_GradfreeArchSearch}
S.~Ebrahimi, A.~Rohrbach, and T.~Darrell.
\newblock Gradient-free policy architecture search and adaptation.
\newblock In {\em Conference on Robot Learning (CoRL)}, 2017.

\bibitem{Miikkulainen_EvolDNN}
R.~Miikkulainen, J.~Liang, E.~Meyerson, A.~Rawal, D.~Fink, O.~Francon, B.~Raju,
  H.~Shahrzad, A.~Navruzyan, N.~Nuffy, and B.~Hodjat.
\newblock Evolving deep neural networks.
\newblock {\em arXiv preprint arXiv:1703.00548}, 2017.

\bibitem{Loshchilov_CMAESHyperp}
I.~Loshchilov and F.~Hutter.
\newblock {CMA-ES} for hyperparameter optimization of deep neural networks.
\newblock In {\em International Conference on Learning Representations (ICLR),
  workshop track}, 2016.

\bibitem{Jaderberg_PopNN}
M.~Jaderberg, V.~Dalibard, S.~Osindero, W.~M. Czarnecki, J.~Donahue, A.~Razavi,
  O.~Vinyals, T.~Green, I.~Dunning, K.~Simonyan, C.~Fernando, and
  K.~Kavukcuoglu.
\newblock Population based training of neural networks.
\newblock {\em arXiv preprint arXiv:1711.09846}, 2017.

\bibitem{Morse_EvolOpt}
G.~Morse and K.~O. Stanley.
\newblock Simple evolutionary optimization can rival stochastic gradient
  descent in neural networks.
\newblock In {\em The Genetic and Evolutionary Computation Conference (GECCO)},
  pages 477--484, 2016.

\bibitem{Yang_CoopCoev}
Z.~Yang, K.~Tang, and X.~Yao.
\newblock Large scale evolutionary optimization using cooperative coevolution.
\newblock {\em Information Sciences}, 178(15):2985--2999, 2008.

\bibitem{GP_COVNET}
N.~Garcia-Pedrajas, C.~Hervas-Martinez, and J.~Munoz-Perez.
\newblock {COVNET}: a cooperative coevolutionary model for evolving artificial
  neural networks.
\newblock {\em IEEE Trans. on Neural Networks}, 14(3):575--595, 2003.

\bibitem{Yao_NE}
X.~Yao.
\newblock Evolving artificial neural networks.
\newblock {\em Proceedings of the IEEE}, 87(9):1423--1447, 1999.

\bibitem{Such_DeepNeuroevol}
F.~P. Such, V.~Madhavan, E.~Conti, J.~Lehman, K.~O. Stanley, and J.~Clune.
\newblock Deep neuroevolution: genetic algorithms are a competitive alternative
  for training deep neural networks for reinforcement learning.
\newblock {\em arXiv preprint arXiv:1712.06567}, 2017.

\bibitem{Suganuma_GPCNN}
M.~Suganuma, S.~Shirakawa, and T.~Nagao.
\newblock A genetic programming approach to designing convolutional neural
  network architectures.
\newblock In {\em The Genetic and Evolutionary Computation Conference (GECCO)},
  pages 497--504, 2017.

\bibitem{Liu_ProArchSearch}
C.~Liu, B.~Zoph, J.~Shlens, W.~Hua, L.-J. Li, F.-F. Li, A.~Yuille, J.~Huang,
  and K.~Murphy.
\newblock Progressive neural architecture search.
\newblock {\em arXiv preprint arXiv:1712.00559}, 2017.

\bibitem{Chrabaszcz_ESAtari}
P.~Chrabaszcz, I.~Loshchilov, and F.~Hutter.
\newblock Back to basics: benchmarking canonical evolution strategies for
  playing {A}tari.
\newblock {\em arXiv preprint arXiv:1802.08842}, 2018.

\bibitem{Salimans_ESAlterRL}
T.~Salimans, J.~Ho, X.~Chen, S.~Sidor, and I.~Sutskever.
\newblock Evolution strategies as a scalable alternative to reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:1703.03864}, 2017.

\bibitem{Zhang_RelationESSGD}
X.~Zhang, J.~Clune, and K.~O. Stanley.
\newblock On the relationship between the {OpenAI} evolution strategy and
  stochastic gradient descent.
\newblock {\em arXiv preprint arXiv:1712.06564}, 2017.

\bibitem{Lehman_ESNoJustFDA}
J.~Lehman, J.~Chen, J.~Clune, and K.~O. Stanley.
\newblock {ES} is more than just a traditional finite-difference approximator.
\newblock {\em arXiv preprint arXiv:1712.06568}, 2017.

\bibitem{Gomez_NECoevol}
F.~Gomez, J.~Schmidhuber, and R.~Miikkulainen.
\newblock Accelerated neural evolution through cooperatively coevolved
  synapses.
\newblock {\em Journal of Machine Learning Research}, 9:937--965, 2008.

\bibitem{GP_COVEnsemble}
N.~Garcia-Pedrajas, C.~Hervas-Martinez, and D.~Ortiz-Boyer.
\newblock Cooperative coevolution of artificial neural network ensembles for
  pattern recognition.
\newblock {\em IEEE Trans. on Evolutionary Computation}, 9(3):271--302, 2005.

\bibitem{Hinton_DNNSPM}
G.~Hinton, L.~Deng, D.~Yu, G.~Dahl, A.~Mohamed, N.~Jaitly, A.~Senior,
  V.~Vanhoucke, P.~Nguyen, T.~N. Sainath, and B.~Kingsbury.
\newblock Deep neural networks for acoustic modeling in speech recognition.
\newblock {\em IEEE Signal Processing Maganize}, pages 82--97, November 2012.

\bibitem{Hermansky_PLP}
H.~Hermansky.
\newblock Perceptual linear predictive (plp) analysis of speech,.
\newblock {\em Journal of Acoustical Society America}, 87(4):1738--1752, 1990.

\bibitem{Hochreiter97}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural Computation}, 9(8):1735--1780, 1997.

\bibitem{Dehak_ivec}
N.~Dehak, P.~Kenny, R.~Dehak, P.~Dumouchel, and P.~Ouellet.
\newblock Front-end factor analysis for speaker verification.
\newblock {\em IEEE Transactions on Audio, Speech, and Language Processing,},
  19(4):788--798, 2011.

\bibitem{cifar}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock In {\em Technical Report}, 2009.

\bibitem{resnet}
K.~{He}, X.~{Zhang}, S.~{Ren}, and J.~{Sun}.
\newblock {Deep Residual Learning for Image Recognition}.
\newblock {\em Conference on Computer Vision and Pattern Recognition
  (CVPR'15)}, 2015.

\bibitem{FB-ResNet}
https://github.com/facebook/fb.resnet.torch.

\bibitem{mikolov2010}
T.~Mikolov, M.~Karafi{\'a}t, L.~Burget, J.~Cernocky, and S.~Khudanpur.
\newblock Recurrent neural network based language model.
\newblock In {\em Interspeech}, pages 1045--1048, 2010.

\bibitem{merity2018regularizing}
S.~Merity, N.~Keskar, and R.~Socher.
\newblock Regularizing and optimizing {LSTM} language models.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{zolna2018fraternal}
K.~Zolna, D.~Arpit, D.~Suhubdy, and Y.~Bengio.
\newblock Fraternal dropout.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{JMLR:v15:srivastava14a}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em Journal of Machine Learning Research}, 15:1929--1958, 2014.

\bibitem{pmlr-v28-wan13}
L.~Wan, M.~Zeiler, S.~Zhang, Y.~LeCun, and R.~Fergus.
\newblock Regularization of neural networks using dropconnect.
\newblock In {\em International Conference on Machine Learning (ICML)},
  volume~28, pages 1058--1066, 2013.

\bibitem{Gal:2016}
Y.~Gal and Z.~Ghahramani.
\newblock A theoretically grounded application of dropout in recurrent neural
  networks.
\newblock In {\em International Conference on Neural Information Processing
  Systems (NIPS)}, NIPS'16, pages 1027--1035, 2016.

\bibitem{Ma2017}
X.~Ma, Y.~Gao, Z.~Hu, Y.~Yu, Y.~Deng, and E.~H. Hovy.
\newblock Dropout with expectation-linear regularization.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2017.

\bibitem{Laine2017}
S.~Laine and T.~Aila.
\newblock Temporal ensembling for semi-supervised learning.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2017.

\bibitem{Loshchilov2017}
I.~Loshchilov and F.~Hutter.
\newblock {SGDR}: Stochastic gradient descent with warm restarts.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2017.

\bibitem{marginaladam}
A.~C. Wilson, R.~Roelofs, M.~Stern, N.~Srebro, and B.~Recht.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In {\em Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017, 4-9 December 2017,
  Long Beach, CA, {USA}}, pages 4151--4161, 2017.

\bibitem{SWATS}
N.~Keskar and R.~Socher.
\newblock Improving generalization performance by switching from adam to {SGD}.
\newblock {\em The Sixth International Conference on Learning Representations
  (ICLR 2018)}, 2018.

\end{thebibliography}
