\begin{thebibliography}{99}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aljundi et~al.(2017{\natexlab{a}})Aljundi, Babiloni, Elhoseiny,
  Rohrbach, and Tuytelaars]{Aljundi17}
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and
  Tinne Tuytelaars.
\newblock Memory aware synapses: Learning what (not) to forget.
\newblock \emph{CoRR}, abs/1711.09601, 2017{\natexlab{a}}.

\bibitem[Aljundi et~al.(2017{\natexlab{b}})Aljundi, Chakravarty, and
  Tuytelaars]{aljundi2017expert}
Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars.
\newblock Expert gate: Lifelong learning with a network of experts.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3366--3375, 2017{\natexlab{b}}.

\bibitem[Aljundi et~al.(2019)Aljundi, Caccia, Belilovsky, Caccia, Lin, Charlin,
  and Tuytelaars]{Aljundi2019Online}
Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin,
  Laurent Charlin, and Tinne Tuytelaars.
\newblock Online continual learning with maximal interfered retrieval.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pages
  11849--11860. Curran Associates, Inc., 2019.

\bibitem[Amer and Maul(2019)]{amer2019review}
Mohammed Amer and Tom{\'a}s Maul.
\newblock A review of modularization techniques in artificial neural networks.
\newblock \emph{Artificial Intelligence Review}, 52\penalty0 (1):\penalty0
  527--561, 2019.

\bibitem[Andreas et~al.(2016{\natexlab{a}})Andreas, Rohrbach, Darrell, and
  Klein]{andreas2016learning}
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein.
\newblock Learning to compose neural networks for question answering.
\newblock \emph{arXiv preprint arXiv:1601.01705}, 2016{\natexlab{a}}.

\bibitem[Andreas et~al.(2016{\natexlab{b}})Andreas, Rohrbach, Darrell, and
  Klein]{andreas2016neural}
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein.
\newblock Neural module networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 39--48, 2016{\natexlab{b}}.

\bibitem[Antoniou et~al.(2018)Antoniou, Edwards, and
  Storkey]{antoniou2018train}
Antreas Antoniou, Harrison Edwards, and Amos Storkey.
\newblock How to train your maml.
\newblock \emph{arXiv preprint arXiv:1810.09502}, 2018.

\bibitem[Bahdanau et~al.(2018)Bahdanau, Murty, Noukhovitch, Nguyen, de~Vries,
  and Courville]{bahdanau2018systematic}
Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien~Huu Nguyen, Harm
  de~Vries, and Aaron Courville.
\newblock Systematic generalization: What is required and can it be learned?
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Ballard(1987)]{ballard1987modular}
Dana~H Ballard.
\newblock Modular learning in neural networks.
\newblock In \emph{AAAI}, pages 279--284, 1987.

\bibitem[Behrmann et~al.(2019)Behrmann, Grathwohl, Chen, Duvenaud, and
  Jacobsen]{behrmann2019invertible}
Jens Behrmann, Will Grathwohl, Ricky~TQ Chen, David Duvenaud, and
  J{\"o}rn-Henrik Jacobsen.
\newblock Invertible residual networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  573--582. PMLR, 2019.

\bibitem[Caccia and Pineau(2021)]{caccia2021special}
Lucas Caccia and Joelle Pineau.
\newblock Special: Self-supervised pretraining for continual learning.
\newblock \emph{arXiv preprint arXiv:2106.09065}, 2021.

\bibitem[Caccia et~al.(2019)Caccia, Belilovsky, Caccia, and
  Pineau]{caccia2019online}
Lucas Caccia, Eugene Belilovsky, Massimo Caccia, and Joelle Pineau.
\newblock Online learned continual compression with stacked quantization
  module.
\newblock \emph{ArXiv}, abs/1911.08019, 2019.

\bibitem[Caccia et~al.(2021)Caccia, Aljundi, Tuytelaars, Pineau, and
  Belilovsky]{caccia2021reducing}
Lucas Caccia, Rahaf Aljundi, Tinne Tuytelaars, Joelle Pineau, and Eugene
  Belilovsky.
\newblock Reducing representation drift in online continual learning.
\newblock \emph{arXiv preprint arXiv:2104.05025}, 2021.

\bibitem[Caccia et~al.(2020)Caccia, Rodriguez, Ostapenko, Normandin, Lin,
  Caccia, Laradji, Rish, Lacoste, Vazquez, et~al.]{caccia2020online}
Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin,
  Lucas Caccia, Issam Laradji, Irina Rish, Alexande Lacoste, David Vazquez,
  et~al.
\newblock Online fast adaptation and knowledge accumulation: a new approach to
  continual learning.
\newblock \emph{arXiv preprint arXiv:2003.05856}, 2020.

\bibitem[Chang et~al.(2018)Chang, Gupta, Levine, and
  Griffiths]{chang2018automatically}
Michael Chang, Abhishek Gupta, Sergey Levine, and Thomas~L Griffiths.
\newblock Automatically composing representation transformations as a means for
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Chaudhry et~al.(2019)Chaudhry, Rohrbach, Elhoseiny, Ajanthan, Dokania,
  Torr, and Ranzato]{chaudhry2019continual}
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan,
  Puneet~K Dokania, Philip~HS Torr, and M~Ranzato.
\newblock Continual learning with tiny episodic memories.
\newblock 2019.

\bibitem[Cimpoi et~al.(2014)Cimpoi, Maji, Kokkinos, Mohamed, , and
  Vedaldi]{cimpoi14describing}
M.~Cimpoi, S.~Maji, I.~Kokkinos, S.~Mohamed, , and A.~Vedaldi.
\newblock Describing textures in the wild.
\newblock In \emph{Proceedings of the {IEEE} Conf. on Computer Vision and
  Pattern Recognition ({CVPR})}, 2014.

\bibitem[Corona et~al.(2020)Corona, Fried, Devin, Klein, and
  Darrell]{corona2020modularity}
Rodolfo Corona, Daniel Fried, Coline Devin, Dan Klein, and Trevor Darrell.
\newblock Modularity improves out-of-domain instruction following.
\newblock \emph{arXiv preprint arXiv:2010.12764}, 2020.

\bibitem[Csord{\'a}s et~al.(2021)Csord{\'a}s, van Steenkiste, and
  Schmidhuber]{csordas2020neural}
R{\'o}bert Csord{\'a}s, Sjoerd van Steenkiste, and J{\"u}rgen Schmidhuber.
\newblock Are neural nets modular? inspecting functional modularity through
  differentiable weight masks.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=7uVcpu-gMD}.

\bibitem[De~Lange et~al.(2019)De~Lange, Aljundi, Masana, Parisot, Jia,
  Leonardis, Slabaugh, and Tuytelaars]{de2019continual}
Matthias De~Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu~Jia, Ales
  Leonardis, Gregory Slabaugh, and Tinne Tuytelaars.
\newblock Continual learning: A comparative study on how to defy forgetting in
  classification tasks.
\newblock \emph{arXiv preprint arXiv:1909.08383}, 2019.

\bibitem[Dinh et~al.(2014)Dinh, Krueger, and Bengio]{dinh2014nice}
Laurent Dinh, David Krueger, and Yoshua Bengio.
\newblock Nice: Non-linear independent components estimation.
\newblock \emph{arXiv preprint arXiv:1410.8516}, 2014.

\bibitem[Douillard and Lesort(2021)]{Douillard2021ContinuumSM}
Arthur Douillard and Timoth{\'e}e Lesort.
\newblock Continuum: Simple management of complex continual learning scenarios.
\newblock \emph{ArXiv}, abs/2102.06253, 2021.

\bibitem[Farquhar and Gal(2018)]{Farquhar18}
Sebastian Farquhar and Yarin Gal.
\newblock Towards robust evaluations of continual learning.
\newblock \emph{arXiv preprint arXiv:1805.09733}, 2018.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Finn et~al.(2019)Finn, Rajeswaran, Kakade, and Levine]{finn2019online}
Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine.
\newblock Online meta-learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1920--1930. PMLR, 2019.

\bibitem[French(1997)]{french1997pseudo}
Robert~M French.
\newblock Pseudo-recurrent connectionist networks: An approach to
  the'sensitivity-stability'dilemma.
\newblock \emph{Connection Science}, 9\penalty0 (4):\penalty0 353--380, 1997.

\bibitem[Goyal et~al.(2019)Goyal, Lamb, Hoffmann, Sodhani, Levine, Bengio, and
  Sch{\"o}lkopf]{goyal2019recurrent}
Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine,
  Yoshua Bengio, and Bernhard Sch{\"o}lkopf.
\newblock Recurrent independent mechanisms.
\newblock \emph{arXiv preprint arXiv:1909.10893}, 2019.

\bibitem[Goyal et~al.(2021)Goyal, Didolkar, Lamb, Badola, Ke, Rahaman, Binas,
  Blundell, Mozer, and Bengio]{goyal2021coordination}
Anirudh Goyal, Aniket Didolkar, Alex Lamb, Kartikeya Badola, Nan~Rosemary Ke,
  Nasim Rahaman, Jonathan Binas, Charles Blundell, Michael Mozer, and Yoshua
  Bengio.
\newblock Coordination among neural modules through a shared global workspace.
\newblock \emph{arXiv preprint arXiv:2103.01197}, 2021.

\bibitem[Grant et~al.(2018)Grant, Finn, Levine, Darrell, and
  Griffiths]{grant2018recasting}
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths.
\newblock Recasting gradient-based meta-learning as hierarchical bayes.
\newblock \emph{arXiv preprint arXiv:1801.08930}, 2018.

\bibitem[Gulrajani and Lopez-Paz(2020)]{gulrajani2020search}
Ishaan Gulrajani and David Lopez-Paz.
\newblock In search of lost domain generalization.
\newblock \emph{arXiv preprint arXiv:2007.01434}, 2020.

\bibitem[Gupta et~al.(2020)Gupta, Singh, Chang, Aggarwal, Arun, Qu, Hoebel,
  Patel, Gidwani, Vaswani, et~al.]{gupta2020unreasonable}
Sharut Gupta, Praveer Singh, Ken Chang, Mehak Aggarwal, Nishanth Arun,
  Liangqiong Qu, Katharina Hoebel, Jay Patel, Mishka Gidwani, Ashwin Vaswani,
  et~al.
\newblock The unreasonable effectiveness of batch-norm statistics in addressing
  catastrophic forgetting across medical institutions.
\newblock \emph{arXiv preprint arXiv:2011.08096}, 2020.

\bibitem[Hadsell et~al.(2020)Hadsell, Rao, Rusu, and
  Pascanu]{hadsell2020embracing}
Raia Hadsell, Dushyant Rao, Andrei~A Rusu, and Razvan Pascanu.
\newblock Embracing change: Continual learning in deep neural networks.
\newblock \emph{Trends in Cognitive Sciences}, 2020.

\bibitem[Harrison et~al.(2019)Harrison, Sharma, Finn, and
  Pavone]{harrison2019continuous}
James Harrison, Apoorva Sharma, Chelsea Finn, and Marco Pavone.
\newblock Continuous meta-learning without tasks.
\newblock \emph{arXiv preprint arXiv:1912.08866}, 2019.

\bibitem[He et~al.(2019{\natexlab{a}})He, Sygnowski, Galashov, Rusu, Teh, and
  Pascanu]{He2019TaskAC}
Xu~He, Jakub Sygnowski, Alexandre Galashov, Andrei~A. Rusu, Yee~Whye Teh, and
  Razvan Pascanu.
\newblock Task agnostic continual learning via meta learning.
\newblock \emph{ArXiv}, abs/1906.05201, 2019{\natexlab{a}}.

\bibitem[He et~al.(2019{\natexlab{b}})He, Sygnowski, Galashov, Rusu, Teh, and
  Pascanu]{he2019task}
Xu~He, Jakub Sygnowski, Alexandre Galashov, Andrei~A Rusu, Yee~Whye Teh, and
  Razvan Pascanu.
\newblock Task agnostic continual learning via meta learning.
\newblock \emph{arXiv preprint arXiv:1906.05201}, 2019{\natexlab{b}}.

\bibitem[Hendrycks et~al.(2018)Hendrycks, Mazeika, and
  Dietterich]{hendrycks2018deep}
Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich.
\newblock Deep anomaly detection with outlier exposure.
\newblock \emph{arXiv preprint arXiv:1812.04606}, 2018.

\bibitem[Hocquet et~al.(2020)Hocquet, Bichler, and Querlioz]{hocquet2020ova}
Guillaume Hocquet, Olivier Bichler, and Damien Querlioz.
\newblock Ova-inn: Continual learning with invertible neural networks.
\newblock \emph{arXiv preprint arXiv:2006.13772}, 2020.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pages
  448--456. PMLR, 2015.

\bibitem[Isele and Cosgun(2018)]{isele2018selective}
David Isele and Akansel Cosgun.
\newblock Selective experience replay for lifelong learning.
\newblock In \emph{AAAI conference on artificial intelligence}, 2018.

\bibitem[Javed and White(2019)]{javed2019oml}
Khurram Javed and Martha White.
\newblock Meta-learning representations for continual learning.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pages
  1820--1830. Curran Associates, Inc., 2019.
\newblock URL
  \url{http://papers.nips.cc/paper/8458-meta-learning-representations-for-continual-learning.pdf}.

\bibitem[Jerfel et~al.(2019)Jerfel, Grant, Griffiths, and
  Heller]{jerfel2019reconciling}
Ghassen Jerfel, Erin Grant, Tom Griffiths, and Katherine~A Heller.
\newblock Reconciling meta-learning and continual learning with online mixtures
  of tasks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9122--9133, 2019.

\bibitem[Khetarpal et~al.(2020)Khetarpal, Riemer, Rish, and
  Precup]{khetarpal2020continual}
Khimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup.
\newblock Towards continual reinforcement learning: A review and perspectives,
  2020.

\bibitem[Kim et~al.(2019)Kim, Kim, Kim, Kim, and Kim]{kim2019learning}
Byungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim, and Junmo Kim.
\newblock Learning not to learn: Training deep neural networks with biased
  data.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 9012--9020, 2019.

\bibitem[Kingma and Ba(2015)]{adam2015}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Yoshua Bengio and Yann LeCun, editors, \emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6980}.

\bibitem[Kingma and Welling(2013)]{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[Kirsch et~al.(2018)Kirsch, Kunze, and Barber]{kirsch2018modular}
Louis Kirsch, Julius Kunze, and David Barber.
\newblock Modular networks: Learning to decompose neural computation.
\newblock \emph{Advances in Neural Information Processing Systems},
  31:\penalty0 2408--2418, 2018.

\bibitem[Kobyzev et~al.(2020)Kobyzev, Prince, and
  Brubaker]{kobyzev2020normalizing}
Ivan Kobyzev, Simon Prince, and Marcus Brubaker.
\newblock Normalizing flows: An introduction and review of current methods.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2020.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Lake et~al.(2015)Lake, Salakhutdinov, and Tenenbaum]{lake2015human}
Brenden~M Lake, Ruslan Salakhutdinov, and Joshua~B Tenenbaum.
\newblock Human-level concept learning through probabilistic program induction.
\newblock \emph{Science}, 350\penalty0 (6266):\penalty0 1332--1338, 2015.

\bibitem[LeCun and Cortes(2010)]{lecun-mnisthandwrittendigit-2010}
Yann LeCun and Corinna Cortes.
\newblock {MNIST} handwritten digit database.
\newblock 2010.
\newblock URL \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem[Lee et~al.(2017)Lee, Yoon, Yang, and Hwang]{Dyn_expand_net_Lee}
Jeongtae Lee, Jaehong Yoon, Eunho Yang, and Sung~Ju Hwang.
\newblock Lifelong learning with dynamically expandable networks.
\newblock \emph{CoRR}, abs/1708.01547, 2017.

\bibitem[Lee et~al.(2020)Lee, Ha, Zhang, and Kim]{lee2020neural}
Soochan Lee, Junsoo Ha, Dongsu Zhang, and Gunhee Kim.
\newblock A neural dirichlet process mixture model for task-free continual
  learning.
\newblock \emph{arXiv preprint arXiv:2001.00689}, 2020.

\bibitem[Lesort et~al.(2019)Lesort, Caselles-Dupr{\'e}, Garcia-Ortiz, Goudou,
  and Filliat]{lesort2018generative}
Timoth{\'e}e Lesort, Hugo Caselles-Dupr{\'e}, Michael Garcia-Ortiz,
  Jean-Fran{\c c}ois Goudou, and David Filliat.
\newblock {Generative Models from the perspective of Continual Learning}.
\newblock In \emph{{International Joint Conference on Neural Networks
  (IJCNN)}}, 2019.

\bibitem[Lesort et~al.(2021{\natexlab{a}})Lesort, Caccia, and
  Rish]{lesort2021understanding}
Timothée Lesort, Massimo Caccia, and Irina Rish.
\newblock Understanding continual learning settings with data distribution
  drift analysis, 2021{\natexlab{a}}.

\bibitem[Lesort et~al.(2021{\natexlab{b}})Lesort, George, and
  Rish]{lesort2021continual}
Timothée Lesort, Thomas George, and Irina Rish.
\newblock Continual learning in deep networks: an analysis of the last layer,
  2021{\natexlab{b}}.

\bibitem[Li and Hoiem(2017)]{li2017learning}
Zhizhong Li and Derek Hoiem.
\newblock Learning without forgetting.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 40\penalty0 (12):\penalty0 2935--2947, 2017.

\bibitem[Lomonaco et~al.(2020)Lomonaco, Pellegrini, Rodr{\'i}guez, Caccia, She,
  Chen, Jodelet, Wang, Mai, V{\'a}zquez, Parisi, Churamani, Pickett, Laradji,
  and Maltoni]{Lomonaco2020CVPR2C}
Vincenzo Lomonaco, Lorenzo Pellegrini, Pau Rodr{\'i}guez, Massimo Caccia,
  Qi~She, Yu~Chen, Quentin Jodelet, Ruiping Wang, Zheda Mai, David V{\'a}zquez,
  German~Ignacio Parisi, Nikhil Churamani, Marc Pickett, Issam~H. Laradji, and
  Davide Maltoni.
\newblock Cvpr 2020 continual learning in computer vision competition:
  Approaches, results, current challenges and future directions.
\newblock \emph{ArXiv}, abs/2009.09929, 2020.

\bibitem[Lomonaco et~al.(2021)Lomonaco, Pellegrini, Cossu, Carta, Graffieti,
  Hayes, Lange, Masana, Pomponi, van~de Ven, Mundt, She, Cooper, Forest,
  Belouadah, Calderara, Parisi, Cuzzolin, Tolias, Scardapane, Antiga, Amhad,
  Popescu, Kanan, van~de Weijer, Tuytelaars, Bacciu, and
  Maltoni]{lomonaco2021avalanche}
Vincenzo Lomonaco, Lorenzo Pellegrini, Andrea Cossu, Antonio Carta, Gabriele
  Graffieti, Tyler~L. Hayes, Matthias~De Lange, Marc Masana, Jary Pomponi, Gido
  van~de Ven, Martin Mundt, Qi~She, Keiland Cooper, Jeremy Forest, Eden
  Belouadah, Simone Calderara, German~I. Parisi, Fabio Cuzzolin, Andreas
  Tolias, Simone Scardapane, Luca Antiga, Subutai Amhad, Adrian Popescu,
  Christopher Kanan, Joost van~de Weijer, Tinne Tuytelaars, Davide Bacciu, and
  Davide Maltoni.
\newblock Avalanche: an end-to-end library for continual learning.
\newblock In \emph{Proceedings of IEEE Conference on Computer Vision and
  Pattern Recognition}, 2nd Continual Learning in Computer Vision Workshop,
  2021.

\bibitem[Lopez-Paz and Ranzato(2017)]{lopez2017gradient}
David Lopez-Paz and Marc'Aurelio Ranzato.
\newblock Gradient episodic memory for continual learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6467--6476, 2017.

\bibitem[Madan et~al.(2021)Madan, Ke, Goyal, Sch{\"o}lkopf, and
  Bengio]{madan2021fast}
Kanika Madan, Nan~Rosemary Ke, Anirudh Goyal, Bernhard Sch{\"o}lkopf, and
  Yoshua Bengio.
\newblock Fast and slow learning of recurrent independent mechanisms.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Lc28QAB4ypz}.

\bibitem[McCloskey and Cohen(1989)]{mccloskey1989catastrophic}
Michael McCloskey and Neal~J Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock In \emph{Psychology of learning and motivation}, volume~24, pages
  109--165. Elsevier, 1989.

\bibitem[Mendez and Eaton(2020)]{mendez2020lifelong}
Jorge~A Mendez and Eric Eaton.
\newblock Lifelong learning of compositional structures.
\newblock \emph{arXiv preprint arXiv:2007.07732}, 2020.

\bibitem[Mermillod et~al.(2013)Mermillod, Bugaiska, and
  Bonin]{mermillod2013stability}
Martial Mermillod, Aur{\'e}lia Bugaiska, and Patrick Bonin.
\newblock The stability-plasticity dilemma: Investigating the continuum from
  catastrophic forgetting to age-limited learning effects.
\newblock \emph{Frontiers in psychology}, 4:\penalty0 504, 2013.

\bibitem[Meyerson and Miikkulainen(2017)]{meyerson2017beyond}
Elliot Meyerson and Risto Miikkulainen.
\newblock Beyond shared hierarchies: Deep multitask learning through soft layer
  ordering.
\newblock \emph{arXiv preprint arXiv:1711.00108}, 2017.

\bibitem[Mundt et~al.(2020)Mundt, Hong, Pliushch, and
  Ramesh]{mundt2020wholistic}
Martin Mundt, Yong~Won Hong, Iuliia Pliushch, and Visvanathan Ramesh.
\newblock A wholistic view of continual learning with deep neural networks:
  Forgotten lessons and the bridge to active and open world learning, 2020.

\bibitem[Mundt et~al.(2021)Mundt, Lang, Delfosse, and
  Kersting]{mundt2021clevacompass}
Martin Mundt, Steven Lang, Quentin Delfosse, and Kristian Kersting.
\newblock Cleva-compass: A continual learning evaluation assessment compass to
  promote research transparency and comparability, 2021.

\bibitem[Nalisnick et~al.(2018)Nalisnick, Matsukawa, Teh, Gorur, and
  Lakshminarayanan]{nalisnick2018deep}
Eric Nalisnick, Akihiro Matsukawa, Yee~Whye Teh, Dilan Gorur, and Balaji
  Lakshminarayanan.
\newblock Do deep generative models know what they don't know?
\newblock \emph{arXiv preprint arXiv:1810.09136}, 2018.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Nguyen et~al.(2018)Nguyen, Li, Bui, and Turner]{Nguyen17}
Cuong~V. Nguyen, Yingzhen Li, Thang~D. Bui, and Richard~E. Turner.
\newblock Variational continual learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Nichol et~al.(2018)Nichol, Achiam, and Schulman]{nichol2018first}
Alex Nichol, Joshua Achiam, and John Schulman.
\newblock On first-order meta-learning algorithms.
\newblock \emph{arXiv preprint arXiv:1803.02999}, 2018.

\bibitem[Normandin et~al.(2021)Normandin, Golemo, Ostapenko, Rodriguez, Riemer,
  Hurtado, Khetarpal, Zhao, Lindeborg, Lesort, Charlin, Rish, and
  Caccia]{normandin2021sequoia}
Fabrice Normandin, Florian Golemo, Oleksiy Ostapenko, Pau Rodriguez, Matthew~D
  Riemer, Julio Hurtado, Khimya Khetarpal, Dominic Zhao, Ryan Lindeborg,
  Timothée Lesort, Laurent Charlin, Irina Rish, and Massimo Caccia.
\newblock Sequoia: A software framework to unify continual learning research,
  2021.

\bibitem[Ostapenko et~al.(2019)Ostapenko, Puscas, Klein, Jahnichen, and
  Nabi]{ostapenko2019learning}
Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jahnichen, and Moin
  Nabi.
\newblock Learning to remember: A synaptic plasticity driven framework for
  continual learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 11321--11329, 2019.

\bibitem[Parascandolo et~al.(2018)Parascandolo, Kilbertus, Rojas-Carulla, and
  Sch{\"o}lkopf]{parascandolo2018learning}
Giambattista Parascandolo, Niki Kilbertus, Mateo Rojas-Carulla, and Bernhard
  Sch{\"o}lkopf.
\newblock Learning independent causal mechanisms.
\newblock In \emph{International Conference on Machine Learning}, pages
  4036--4044. PMLR, 2018.

\bibitem[Pearl(2009)]{pearl2009causality}
Judea Pearl.
\newblock \emph{Causality}.
\newblock Cambridge university press, 2009.

\bibitem[Peters et~al.(2017)Peters, Janzing, and
  Sch{\"o}lkopf]{peters2017elements}
Jonas Peters, Dominik Janzing, and Bernhard Sch{\"o}lkopf.
\newblock \emph{Elements of causal inference: foundations and learning
  algorithms}.
\newblock The MIT Press, 2017.

\bibitem[Pezeshki et~al.(2020)Pezeshki, Kaba, Bengio, Courville, Precup, and
  Lajoie]{pezeshki2020gradient}
Mohammad Pezeshki, S{\'e}kou-Oumar Kaba, Yoshua Bengio, Aaron Courville, Doina
  Precup, and Guillaume Lajoie.
\newblock Gradient starvation: A learning proclivity in neural networks.
\newblock \emph{arXiv preprint arXiv:2011.09468}, 2020.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Kolesnikov, Sperl, and
  Lampert]{rebuffi2017icarl}
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph~H
  Lampert.
\newblock icarl: Incremental classifier and representation learning.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR)}, 2017.

\bibitem[Ren et~al.(2018)Ren, Triantafillou, Ravi, Snell, Swersky, Tenenbaum,
  Larochelle, and Zemel]{ren2018meta}
Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky,
  Joshua~B Tenenbaum, Hugo Larochelle, and Richard~S Zemel.
\newblock Meta-learning for semi-supervised few-shot classification.
\newblock \emph{arXiv preprint arXiv:1803.00676}, 2018.

\bibitem[Rezende and Mohamed(2015)]{rezende2015variational}
Danilo Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In \emph{International conference on machine learning}, pages
  1530--1538. PMLR, 2015.

\bibitem[Rolnick et~al.(2019)Rolnick, Ahuja, Schwarz, Lillicrap, and
  Wayne]{rolnick2019experience}
David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory
  Wayne.
\newblock Experience replay for continual learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Rosenbaum et~al.(2019)Rosenbaum, Cases, Riemer, and
  Klinger]{rosenbaum2019routing}
Clemens Rosenbaum, Ignacio Cases, Matthew Riemer, and Tim Klinger.
\newblock Routing networks and the challenges of modular and compositional
  computation.
\newblock \emph{arXiv preprint arXiv:1904.12774}, 2019.

\bibitem[Rusu et~al.(2016)Rusu, Rabinowitz, Desjardins, Soyer, Kirkpatrick,
  Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}
Andrei~A Rusu, Neil~C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
  Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
\newblock Progressive neural networks.
\newblock \emph{arXiv preprint arXiv:1606.04671}, 2016.

\bibitem[Sch{\"o}lkopf et~al.(2012)Sch{\"o}lkopf, Janzing, Peters, Sgouritsa,
  Zhang, and Mooij]{scholkopf2012causal}
B~Sch{\"o}lkopf, D~Janzing, J~Peters, E~Sgouritsa, K~Zhang, and J~Mooij.
\newblock On causal and anticausal learning.
\newblock In \emph{29th International Conference on Machine Learning (ICML
  2012)}, pages 1255--1262. International Machine Learning Society, 2012.

\bibitem[Schwarz et~al.(2018)Schwarz, Luketina, Czarnecki, Grabska-Barwinska,
  Teh, Pascanu, and Hadsell]{schwarz2018progress}
Jonathan Schwarz, Jelena Luketina, Wojciech~M Czarnecki, Agnieszka
  Grabska-Barwinska, Yee~Whye Teh, Razvan Pascanu, and Raia Hadsell.
\newblock Progress \& compress: A scalable framework for continual learning.
\newblock \emph{arXiv preprint arXiv:1805.06370}, 2018.

\bibitem[Serra et~al.(2018{\natexlab{a}})Serra, Suris, Miron, and
  Karatzoglou]{Serra18}
Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou.
\newblock Overcoming catastrophic forgetting with hard attention to the task.
\newblock \emph{International Conference on Machine Learning (ICML)},
  2018{\natexlab{a}}.

\bibitem[Serra et~al.(2018{\natexlab{b}})Serra, Suris, Miron, and
  Karatzoglou]{serra2018overcoming}
Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou.
\newblock Overcoming catastrophic forgetting with hard attention to the task.
\newblock In \emph{International Conference on Machine Learning}, pages
  4548--4557. PMLR, 2018{\natexlab{b}}.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean]{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
  Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer, 2017.

\bibitem[Soltoggio(2015)]{soltoggio2015short}
Andrea Soltoggio.
\newblock Short-term plasticity as cause--effect hypothesis testing in distal
  reward learning.
\newblock \emph{Biological cybernetics}, 109\penalty0 (1):\penalty0 75--94,
  2015.

\bibitem[Sporns and Betzel(2016)]{sporns2016modular}
Olaf Sporns and Richard~F Betzel.
\newblock Modular brain networks.
\newblock \emph{Annual review of psychology}, 67:\penalty0 613--640, 2016.

\bibitem[Sternberg(2011)]{sternberg2011modular}
Saul Sternberg.
\newblock Modular processes in mind and brain.
\newblock \emph{Cognitive neuropsychology}, 28\penalty0 (3-4):\penalty0
  156--208, 2011.

\bibitem[Veniat et~al.(2020)Veniat, Denoyer, and Ranzato]{veniat2020efficient}
Tom Veniat, Ludovic Denoyer, and Marc'Aurelio Ranzato.
\newblock Efficient continual learning with modular networks and task-driven
  priors.
\newblock \emph{arXiv preprint arXiv:2012.12631}, 2020.

\bibitem[von Oswald et~al.(2021)von Oswald, Zhao, Kobayashi, Schug, Caccia,
  Zucchet, and Sacramento]{von2021learning}
Johannes von Oswald, Dominic Zhao, Seijin Kobayashi, Simon Schug, Massimo
  Caccia, Nicolas Zucchet, and Jo{\~a}o Sacramento.
\newblock Learning where to learn: Gradient sparsity in meta and continual
  learning.
\newblock \emph{NeurIPS 2021}, 2021.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Yao, Kwok, and
  Ni]{wang2020generalizing}
Yaqing Wang, Quanming Yao, James~T Kwok, and Lionel~M Ni.
\newblock Generalizing from a few examples: A survey on few-shot learning.
\newblock \emph{ACM Computing Surveys (CSUR)}, 53\penalty0 (3):\penalty0 1--34,
  2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Dai, Wipf, and
  Zhu]{wang2020further}
Ziyu Wang, Bin Dai, David Wipf, and Jun Zhu.
\newblock Further analysis of outlier detection with deep generative models.
\newblock \emph{arXiv preprint arXiv:2010.13064}, 2020{\natexlab{b}}.

\bibitem[Whittington and Bogacz(2017)]{whittington2017approximation}
James~CR Whittington and Rafal Bogacz.
\newblock An approximation of the error backpropagation algorithm in a
  predictive coding network with local hebbian synaptic plasticity.
\newblock \emph{Neural computation}, 29\penalty0 (5):\penalty0 1229--1262,
  2017.

\bibitem[Wu et~al.(2018)Wu, Herranz, Liu, van~de Weijer, Raducanu,
  et~al.]{wu2018memory}
Chenshen Wu, Luis Herranz, Xialei Liu, Joost van~de Weijer, Bogdan Raducanu,
  et~al.
\newblock Memory replay gans: Learning to generate new categories without
  forgetting.
\newblock \emph{Advances in Neural Information Processing Systems},
  31:\penalty0 5962--5972, 2018.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Zeno et~al.(2018)Zeno, Golan, Hoffer, and Soudry]{zeno2018task}
Chen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry.
\newblock Task agnostic continual learning using online variational bayes,
  2018.

\end{thebibliography}
