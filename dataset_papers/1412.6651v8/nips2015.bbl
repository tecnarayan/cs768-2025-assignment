\begin{thebibliography}{10}

\bibitem{bottou-98x}
Bottou, L.
\newblock Online algorithms and stochastic approximations.
\newblock In {\em Online Learning and Neural Networks}. Cambridge University
  Press, 1998.

\bibitem{2012distbelief}
Dean, J, Corrado, G, Monga, R, Chen, K, Devin, M, Le, Q, Mao, M, Ranzato, M,
  Senior, A, Tucker, P, Yang, K, and Ng, A.
\newblock Large scale distributed deep networks.
\newblock In {\em NIPS}. 2012.

\bibitem{krizhevsky2012imagenet}
Krizhevsky, A, Sutskever, I, and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in Neural Information Processing Systems 25}, pages
  1106--1114, 2012.

\bibitem{2013arXiv1312.6229S}
{Sermanet}, P, {Eigen}, D, {Zhang}, X, {Mathieu}, M, {Fergus}, R, and {LeCun},
  Y.
\newblock {OverFeat: Integrated Recognition, Localization and Detection using
  Convolutional Networks}.
\newblock {\em ArXiv}, 2013.

\bibitem{nocedal2006numerical}
Nocedal, J and Wright, S.
\newblock {\em Numerical Optimization, Second Edition}.
\newblock Springer New York, 2006.

\bibitem{polyak1992acceleration}
Polyak, B.~T and Juditsky, A.~B.
\newblock Acceleration of stochastic approximation by averaging.
\newblock {\em SIAM Journal on Control and Optimization}, 30(4):838--855, 1992.

\bibitem{Bertsekas1989}
Bertsekas, D.~P and Tsitsiklis, J.~N.
\newblock {\em Parallel and Distributed Computation}.
\newblock Prentice Hall, 1989.

\bibitem{hestenes1975optimization}
Hestenes, M.~R.
\newblock {\em Optimization theory: the finite dimensional case}.
\newblock Wiley, 1975.

\bibitem{Boyd2011}
Boyd, S, Parikh, N, Chu, E, Peleato, B, and Eckstein, J.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock {\em Found. Trends Mach. Learn.}, 3(1):1--122, 2011.

\bibitem{NIPS2014_5386}
Shamir, O.
\newblock Fundamental limits of online and distributed algorithms for
  statistical learning and estimation.
\newblock In {\em NIPS}. 2014.

\bibitem{Yadan2013}
Yadan, O, Adams, K, Taigman, Y, and Ranzato, M.
\newblock Multi-gpu training of convnets.
\newblock In {\em Arxiv}. 2013.

\bibitem{Paine2013}
Paine, T, Jin, H, Yang, J, Lin, Z, and Huang, T.
\newblock Gpu asynchronous stochastic gradient descent to speed up neural
  network training.
\newblock In {\em Arxiv}. 2013.

\bibitem{onebitparallel}
Seide, F, Fu, H, Droppo, J, Li, G, and Yu, D.
\newblock 1-bit stochastic gradient descent and application to data-parallel
  distributed training of speech dnns.
\newblock In {\em Interspeech 2014}, September 2014.

\bibitem{Bekkerman:2011:SUM:2107736.2107740}
Bekkerman, R, Bilenko, M, and Langford, J.
\newblock Scaling up machine learning: Parallel and distributed approaches.
\newblock Camridge Universityy Press, 2011.

\bibitem{ChoromanskaHMAL15}
Choromanska, A, Henaff, M.~B, Mathieu, M, Arous, G.~B, and LeCun, Y.
\newblock The loss surfaces of multilayer networks.
\newblock In {\em AISTATS}, 2015.

\bibitem{NIPS2013_4894}
Ho, Q, Cipar, J, Cui, H, Lee, S, Kim, J.~K, Gibbons, P.~B, Gibson, G.~A,
  Ganger, G, and Xing, E.~P.
\newblock More effective distributed ml via a stale synchronous parallel
  parameter server.
\newblock In {\em NIPS}. 2013.

\bibitem{DBLP:conf/icml/AzadiS14}
Azadi, S and Sra, S.
\newblock Towards an optimal stochastic alternating direction method of
  multipliers.
\newblock In {\em ICML}, 2014.

\bibitem{doi:10.1137/S0363012995282784}
Borkar, V.
\newblock Asynchronous stochastic approximations.
\newblock {\em SIAM Journal on Control and Optimization}, 36(3):840--851, 1998.

\bibitem{Nedic2001381}
Nedi{\'c}, A, Bertsekas, D, and Borkar, V.
\newblock Distributed asynchronous incremental subgradient methods.
\newblock In {\em Inherently Parallel Algorithms in Feasibility and
  Optimization and their Applications}, volume~8 of {\em Studies in
  Computational Mathematics}, pages 381 -- 407. 2001.

\bibitem{langford2009slow}
Langford, J, Smola, A, and Zinkevich, M.
\newblock Slow learners are fast.
\newblock In {\em NIPS}, 2009.

\bibitem{NIPS2011_0574}
Agarwal, A and Duchi, J.
\newblock Distributed delayed stochastic optimization.
\newblock In {\em NIPS}. 2011.

\bibitem{DBLPRechtRWN11}
Recht, B, Re, C, Wright, S.~J, and Niu, F.
\newblock {Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient
  Descent}.
\newblock In {\em NIPS}, 2011.

\bibitem{zinkevich2010parallelized}
Zinkevich, M, Weimer, M, Smola, A, and Li, L.
\newblock Parallelized stochastic gradient descent.
\newblock In {\em NIPS}, 2010.

\bibitem{Nesterov:2005:SMN:1058099.1058103}
Nesterov, Y.
\newblock Smooth minimization of non-smooth functions.
\newblock {\em Math. Program.}, 103(1):127--152, 2005.

\bibitem{lan2012optimal}
Lan, G.
\newblock An optimal method for stochastic composite optimization.
\newblock {\em Mathematical Programming}, 133(1-2):365--397, 2012.

\bibitem{icml2013_sutskever13}
Sutskever, I, Martens, J, Dahl, G, and Hinton, G.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In {\em ICML}, 2013.

\bibitem{icml2014c2_zhange14}
Zhang, R and Kwok, J.
\newblock Asynchronous distributed admm for consensus optimization.
\newblock In {\em ICML}, 2014.

\bibitem{ouyang2013stochastic}
Ouyang, H, He, N, Tran, L, and Gray, A.
\newblock Stochastic alternating direction method of multipliers.
\newblock In {\em Proceedings of the 30th International Conference on Machine
  Learning}, pages 80--88, 2013.

\bibitem{DBLPWanZZLF13}
Wan, L, Zeiler, M.~D, Zhang, S, LeCun, Y, and Fergus, R.
\newblock Regularization of neural networks using dropconnect.
\newblock In {\em ICML}, 2013.

\bibitem{cesa2004generalization}
Cesa-Bianchi, N, Conconi, A, and Gentile, C.
\newblock On the generalization ability of on-line learning algorithms.
\newblock {\em IEEE Transactions on Information Theory}, 50(9):2050--2057,
  2004.

\bibitem{nesterov2004introductory}
Nesterov, Y.
\newblock {\em Introductory lectures on convex optimization}, volume~87.
\newblock Springer Science \& Business Media, 2004.

\end{thebibliography}
