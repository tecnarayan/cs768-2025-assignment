\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aghajanyan et~al.(2021)Aghajanyan, Gupta, and
  Zettlemoyer]{aghajanyan-etal-2021-intrinsic}
Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer.
\newblock Intrinsic dimensionality explains the effectiveness of language model
  fine-tuning.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 7319--7328,
  Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.568}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.568}.

\bibitem[Akyürek et~al.(2023)Akyürek, Schuurmans, Andreas, Ma, and
  Zhou]{akyrek2023what}
Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
\newblock What learning algorithm is in-context learning? investigations with
  linear models.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=0g0X4H8yN4I}.

\bibitem[Behrmann et~al.(2019)Behrmann, Grathwohl, Chen, Duvenaud, and
  Jacobsen]{pmlr-v97-behrmann19a}
Jens Behrmann, Will Grathwohl, Ricky T.~Q. Chen, David Duvenaud, and
  Joern-Henrik Jacobsen.
\newblock Invertible residual networks.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages 573--582.
  PMLR, 09--15 Jun 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/behrmann19a.html}.

\bibitem[Ben~Zaken et~al.(2022)Ben~Zaken, Goldberg, and
  Ravfogel]{ben-zaken-etal-2022-bitfit}
Elad Ben~Zaken, Yoav Goldberg, and Shauli Ravfogel.
\newblock {B}it{F}it: Simple parameter-efficient fine-tuning for
  transformer-based masked language-models.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, pages 1--9, Dublin,
  Ireland, May 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-short.1}.
\newblock URL \url{https://aclanthology.org/2022.acl-short.1}.

\bibitem[Bojar et~al.(2014)Bojar, Buck, Federmann, Haddow, Koehn, Leveling,
  Monz, Pecina, Post, Saint-Amand, Soricut, Specia, and Tamchyna]{wmt14}
Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn,
  Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand,
  Radu Soricut, Lucia Specia, and Ale{s} Tamchyna.
\newblock Findings of the 2014 workshop on statistical machine translation.
\newblock In \emph{Proceedings of the Ninth Workshop on Statistical Machine
  Translation}, pages 12--58, Baltimore, Maryland, USA, June 2014. Association
  for Computational Linguistics.
\newblock URL \url{http://www.aclweb.org/anthology/W/W14/W14-3302}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
  Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter,
  Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
  Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
  Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners, 2020.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez,
  Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury,
  Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski,
  Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov,
  Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira,
  Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei,
  Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
  Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran,
  Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
  Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
  Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
  Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
  Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
  Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai,
  Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
  Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
  Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
  Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock Palm: Scaling language modeling with pathways, 2022.

\bibitem[Dasoulas et~al.(2021)Dasoulas, Scaman, and
  Virmaux]{dasoulas2021lipschitz}
George Dasoulas, Kevin Scaman, and Aladin Virmaux.
\newblock Lipschitz normalization for self-attention layers with application to
  graph neural networks, 2021.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\bibitem[Dong et~al.(2021)Dong, Cordonnier, and Loukas]{pmlr-v139-dong21a}
Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas.
\newblock Attention is not all you need: pure attention loses rank doubly
  exponentially with depth.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 2793--2803. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/dong21a.html}.

\bibitem[Hardt and Ma(2016)]{hardt2016identity}
Moritz Hardt and Tengyu Ma.
\newblock Identity matters in deep learning.
\newblock \emph{arXiv preprint arXiv:1611.04231}, 2016.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen]{hu2021lora}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models, 2021.

\bibitem[Huang(2003)]{huang2003learning}
Guang-Bin Huang.
\newblock Learning capability and storage capacity of two-hidden-layer
  feedforward networks.
\newblock \emph{IEEE transactions on neural networks}, 14\penalty0
  (2):\penalty0 274--281, 2003.

\bibitem[Huang and Babri(1998)]{huang1998upper}
Guang-Bin Huang and Haroon~A Babri.
\newblock Upper bounds on the number of hidden neurons in feedforward networks
  with arbitrary bounded nonlinear activation functions.
\newblock \emph{IEEE transactions on neural networks}, 9\penalty0 (1):\penalty0
  224--229, 1998.

\bibitem[Huang and Huang(1990)]{huang1990bounds}
S-C Huang and Y-F Huang.
\newblock Bounds on number of hidden neurons of multilayer perceptrons in
  classification and recognition.
\newblock In \emph{1990 IEEE International Symposium on Circuits and Systems
  (ISCAS)}, pages 2500--2503. IEEE, 1990.

\bibitem[Kim et~al.(2021)Kim, Papamakarios, and Mnih]{kim2021lipschitz}
Hyunjik Kim, George Papamakarios, and Andriy Mnih.
\newblock The lipschitz constant of self-attention, 2021.

\bibitem[Kim et~al.()Kim, Kim, and Mozafari]{kimprovable}
Junghwan Kim, Michelle Kim, and Barzan Mozafari.
\newblock Provable memorization capacity of transformers.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and
  Constant]{lester-etal-2021-power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 3045--3059, Online and Punta Cana,
  Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.243}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.243}.

\bibitem[Li and Liang(2021)]{li-liang-2021-prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 4582--4597,
  Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.353}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.353}.

\bibitem[Li et~al.(2022)Li, Bhojanapalli, Zaheer, Reddi, and
  Kumar]{pmlr-v162-li22b}
Zhiyuan Li, Srinadh Bhojanapalli, Manzil Zaheer, Sashank Reddi, and Sanjiv
  Kumar.
\newblock Robust training of neural networks using scale invariant
  architectures.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, \emph{Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of
  \emph{Proceedings of Machine Learning Research}, pages 12656--12684. PMLR,
  17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/li22b.html}.

\bibitem[Mahabadi et~al.(2021)Mahabadi, Henderson, and
  Ruder]{mahabadi2021compacter}
Rabeeh~Karimi Mahabadi, James Henderson, and Sebastian Ruder.
\newblock Compacter: Efficient low-rank hypercomplex adapter layers, 2021.

\bibitem[Malladi et~al.(2023)Malladi, Wettig, Yu, Chen, and
  Arora]{malladi2023kernelbased}
Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, and Sanjeev Arora.
\newblock A kernel-based view of language model fine-tuning, 2023.

\bibitem[Mangrulkar et~al.(2022)Mangrulkar, Gugger, Debut, Younes, and
  Sayak]{peft}
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Belkada Younes, and Paul
  Sayak.
\newblock Peft: State-of-the-art parameter-efficient fine-tuning methods.
\newblock \url{https://github.com/huggingface/peft}, 2022.

\bibitem[Nguyen and Hein(2018)]{nguyen2018optimization}
Quynh Nguyen and Matthias Hein.
\newblock Optimization landscape and expressivity of deep cnns.
\newblock In \emph{International conference on machine learning}, pages
  3730--3739. PMLR, 2018.

\bibitem[Pérez et~al.(2021)Pérez, Barceló, and Marinkovic]{JMLR:v22:20-302}
Jorge Pérez, Pablo Barceló, and Javier Marinkovic.
\newblock Attention is turing-complete.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (75):\penalty0 1--35, 2021.
\newblock URL \url{http://jmlr.org/papers/v22/20-302.html}.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Shin et~al.(2020)Shin, Razeghi, au2, Wallace, and
  Singh]{shin2020autoprompt}
Taylor Shin, Yasaman Razeghi, Robert L. Logan~IV au2, Eric Wallace, and Sameer
  Singh.
\newblock Autoprompt: Eliciting knowledge from language models with
  automatically generated prompts, 2020.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and
  Lample]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
  Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Von~Oswald et~al.(2023)Von~Oswald, Niklasson, Randazzo, Sacramento,
  Mordvintsev, Zhmoginov, and Vladymyrov]{von2023transformers}
Johannes Von~Oswald, Eyvind Niklasson, Ettore Randazzo, Jo{\~a}o Sacramento,
  Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  35151--35174. PMLR, 2023.

\bibitem[Vuckovic et~al.(2020)Vuckovic, Baratin, and des
  Combes]{vuckovic2020mathematical}
James Vuckovic, Aristide Baratin, and Remi~Tachet des Combes.
\newblock A mathematical theory of attention, 2020.

\bibitem[Wang et~al.(2019)Wang, Pruksachatkun, Nangia, Singh, Michael, Hill,
  Levy, and Bowman]{wang2019superglue}
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
  Felix Hill, Omer Levy, and Samuel Bowman.
\newblock Superglue: A stickier benchmark for general-purpose language
  understanding systems.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Wei et~al.(2021)Wei, Chen, and Ma]{DBLP:journals/corr/abs-2107-13163}
Colin Wei, Yining Chen, and Tengyu Ma.
\newblock Statistically meaningful approximation: a case study on approximating
  turing machines with transformers.
\newblock \emph{CoRR}, abs/2107.13163, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.13163}.

\bibitem[Wei et~al.(2022)Wei, Xie, and Ma]{wei2022pretrained}
Colin Wei, Sang~Michael Xie, and Tengyu Ma.
\newblock Why do pretrained language models help in downstream tasks? an
  analysis of head and prompt tuning, 2022.

\bibitem[Yamasaki(1993)]{yamasaki1993lower}
Masami Yamasaki.
\newblock The lower bound of the capacity for a neural network with multiple
  hidden layers.
\newblock In \emph{ICANN’93: Proceedings of the International Conference on
  Artificial Neural Networks Amsterdam, The Netherlands 13--16 September 1993
  3}, pages 546--549. Springer, 1993.

\bibitem[Yun et~al.(2019{\natexlab{a}})Yun, Bhojanapalli, Rawat, Reddi, and
  Kumar]{yun2019transformers}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank~J Reddi, and
  Sanjiv Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence
  functions?
\newblock \emph{arXiv preprint arXiv:1912.10077}, 2019{\natexlab{a}}.

\bibitem[Yun et~al.(2019{\natexlab{b}})Yun, Sra, and Jadbabaie]{yun2019small}
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie.
\newblock Small relu networks are powerful memorizers: a tight analysis of
  memorization capacity.
\newblock \emph{Advances in Neural Information Processing Systems}, 32,
  2019{\natexlab{b}}.

\bibitem[Yun et~al.(2020)Yun, Bhojanapalli, Rawat, Reddi, and
  Kumar]{Yun2020Are}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank Reddi, and Sanjiv
  Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence
  functions?
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=ByxRM0Ntvr}.

\bibitem[Zhang et~al.(2021)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2021understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115,
  2021.

\bibitem[Zhang et~al.(2020)Zhang, Karimireddy, Veit, Kim, Reddi, Kumar, and
  Sra]{zhang2020adaptive}
Jingzhao Zhang, Sai~Praneeth Karimireddy, Andreas Veit, Seungyeon Kim,
  Sashank~J Reddi, Sanjiv Kumar, and Suvrit Sra.
\newblock Why are adaptive methods good for attention models?, 2020.

\end{thebibliography}
