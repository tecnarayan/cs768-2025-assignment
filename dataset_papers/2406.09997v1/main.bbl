\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainsworth et~al.(2022)Ainsworth, Hayase, and Srinivasa]{ainsworthGitReBasinMerging2022}
Ainsworth, S.~K., Hayase, J., and Srinivasa, S.
\newblock Git {{Re-Basin}}: {{Merging Models}} modulo {{Permutation Symmetries}}, September 2022.

\bibitem[Andreis et~al.(2023)Andreis, Bedionita, and Hwang]{andreisSetbasedNeuralNetwork2023}
Andreis, B., Bedionita, S., and Hwang, S.~J.
\newblock Set-based {{Neural Network Encoding}}, May 2023.

\bibitem[Ashkenazi et~al.(2022)Ashkenazi, Rimon, Vainshtein, Levi, Richardson, Mintz, and Treister]{ashkenaziNeRNLearningNeural2022}
Ashkenazi, M., Rimon, Z., Vainshtein, R., Levi, S., Richardson, E., Mintz, P., and Treister, E.
\newblock {{NeRN}} -- {{Learning Neural Representations}} for {{Neural Networks}}, December 2022.

\bibitem[Benton et~al.(2021)Benton, Maddox, Lotfi, and Wilson]{bentonLossSurfaceSimplexes2021}
Benton, G.~W., Maddox, W.~J., Lotfi, S., and Wilson, A.~G.
\newblock Loss {{Surface Simplexes}} for {{Mode Connecting Volumes}} and {{Fast Ensembling}}.
\newblock In \emph{{{PMLR}}}, 2021.

\bibitem[Berardi et~al.(2022)Berardi, De~Luigi, Salti, and Di~Stefano]{berardiLearningSpaceDeep2022}
Berardi, G., De~Luigi, L., Salti, S., and Di~Stefano, L.
\newblock Learning the {{Space}} of {{Deep Models}}, June 2022.

\bibitem[Bishop(2006)]{bishopPatternRecognitionMachine2006}
Bishop, C.~M.
\newblock \emph{Pattern Recognition and Machine Learning}.
\newblock {springer}, 2006.

\bibitem[Brown et~al.(2023)Brown, Vyas, and Bansal]{brownPrivilegedConvergentBases2023}
Brown, D., Vyas, N., and Bansal, Y.
\newblock On {{Privileged}} and {{Convergent Bases}} in {{Neural Network Representations}}, July 2023.

\bibitem[Corneanu et~al.(2020)Corneanu, Escalera, and Martinez]{corneanuComputingTestingError2020}
Corneanu, C.~A., Escalera, S., and Martinez, A.~M.
\newblock Computing the {{Testing Error Without}} a {{Testing Set}}.
\newblock In \emph{2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})}. {IEEE}, June 2020.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R{\'e}]{daoFlashAttentionFastMemoryEfficient2022}
Dao, T., Fu, D.~Y., Ermon, S., Rudra, A., and R{\'e}, C.
\newblock {{FlashAttention}}: {{Fast}} and {{Memory-Efficient Exact Attention}} with {{IO-Awareness}}, June 2022.

\bibitem[Dauphin \& Schoenholz(2019)Dauphin and Schoenholz]{dauphinMetaInitInitializingLearning2019}
Dauphin, Y.~N. and Schoenholz, S.
\newblock {{MetaInit}}: {{Initializing}} learning by learning to initialize.
\newblock In \emph{Neural {{Information Processing Systems}}}, 2019.

\bibitem[De~Luigi et~al.(2023)De~Luigi, Cardace, Spezialetti, Ramirez, Salti, and Di~Stefano]{deluigiDeepLearningImplicit2023}
De~Luigi, L., Cardace, A., Spezialetti, R., Ramirez, P.~Z., Salti, S., and Di~Stefano, L.
\newblock Deep {{Learning}} on {{Implicit Neural Representations}} of {{Shapes}}, February 2023.

\bibitem[Deutsch(2018)]{deutschGeneratingNeuralNetworks2018}
Deutsch, L.
\newblock Generating {{Neural Networks}} with {{Neural Networks}}.
\newblock April 2018.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and Hamprecht]{draxlerEssentiallyNoBarriers2018}
Draxler, F., Veschgini, K., Salmhofer, M., and Hamprecht, F.
\newblock Essentially {{No Barriers}} in {{Neural Network Energy Landscape}}.
\newblock In \emph{International {{Conference}} on {{Machine Learning}}}, March 2018.

\bibitem[Eilertsen et~al.(2020)Eilertsen, J{\"o}nsson, Ropinski, Unger, and Ynnerman]{eilertsenClassifyingClassifierDissecting2020}
Eilertsen, G., J{\"o}nsson, D., Ropinski, T., Unger, J., and Ynnerman, A.
\newblock Classifying the classifier: Dissecting the weight space of neural networks.
\newblock February 2020.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finnModelAgnosticMetaLearningFast2017}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}.
\newblock In \emph{Proceedings of the 34th {{International Conference}} on {{Machine Learning}}}. {PMLR}, July 2017.

\bibitem[Fort \& Jastrzebski(2019)Fort and Jastrzebski]{fortLargeScaleStructure2019}
Fort, S. and Jastrzebski, S.
\newblock Large {{Scale Structure}} of {{Neural Network Loss Landscapes}}.
\newblock June 2019.

\bibitem[Frankle et~al.(2019)Frankle, Dziugaite, Roy, and Carbin]{frankleLinearModeConnectivity2019}
Frankle, J., Dziugaite, G., Roy, D.~M., and Carbin, M.
\newblock Linear {{Mode Connectivity}} and the {{Lottery Ticket Hypothesis}}.
\newblock December 2019.

\bibitem[Ha et~al.(2016)Ha, Dai, and Le]{haHyperNetworks2016}
Ha, D., Dai, A., and Le, Q.~V.
\newblock {{HyperNetworks}}, 2016.

\bibitem[Jiang et~al.(2019)Jiang, Krishnan, Mobahi, and Bengio]{jiangPredictingGeneralizationGap2019}
Jiang, Y., Krishnan, D., Mobahi, H., and Bengio, S.
\newblock Predicting the {{Generalization Gap}} in {{Deep Networks}} with {{Margin Distributions}}.
\newblock June 2019.

\bibitem[Knyazev et~al.(2021)Knyazev, Drozdzal, Taylor, and {Romero-Soriano}]{knyazevParameterPredictionUnseen2021}
Knyazev, B., Drozdzal, M., Taylor, G.~W., and {Romero-Soriano}, A.
\newblock Parameter {{Prediction}} for {{Unseen Deep Architectures}}.
\newblock In \emph{Conference on {{Neural Information Processing Systems}} ({{NeurIPS}})}, 2021.

\bibitem[Knyazev et~al.(2023)Knyazev, Hwang, and {Lacoste-Julien}]{knyazevCanWeScale2023b}
Knyazev, B., Hwang, D., and {Lacoste-Julien}, S.
\newblock Can {{We Scale Transformers}} to {{Predict Parameters}} of {{Diverse ImageNet Models}}?
\newblock In \emph{{{arXiv}}.Org}, March 2023.

\bibitem[Kofinas et~al.(2024)Kofinas, Knyazev, Zhang, Chen, Burghouts, Gavves, Snoek, and Zhang]{kofinas2024graph}
Kofinas, M., Knyazev, B., Zhang, Y., Chen, Y., Burghouts, G.~J., Gavves, E., Snoek, C. G.~M., and Zhang, D.~W.
\newblock Graph neural networks for learning equivariant representations of neural networks.
\newblock In \emph{International {{Conference}} on {{Learning Representations}} ({{ICLR}})}, 2024.

\bibitem[Kornblith et~al.(2019)Kornblith, Norouzi, Lee, and Hinton]{kornblithSimilarityNeuralNetwork2019}
Kornblith, S., Norouzi, M., Lee, H., and Hinton, G.
\newblock Similarity of {{Neural Network Representations Revisited}}.
\newblock May 2019.

\bibitem[Leclerc et~al.(2023)Leclerc, Ilyas, Engstrom, Park, Salman, and Madry]{leclercFFCVAcceleratingTraining2023}
Leclerc, G., Ilyas, A., Engstrom, L., Park, S.~M., Salman, H., and Madry, A.
\newblock {{FFCV}}: {{Accelerating Training}} by {{Removing Data Bottleneck}}.
\newblock In \emph{Computer {{Vision}} and {{Pattern Recognition}} ({{CVPR}})}, 2023.

\bibitem[Liaw et~al.(2018)Liaw, Liang, Nishihara, Moritz, Gonzalez, and Stoica]{liawTuneResearchPlatform2018}
Liaw, R., Liang, E., Nishihara, R., Moritz, P., Gonzalez, J.~E., and Stoica, I.
\newblock Tune: {{A Research Platform}} for {{Distributed Model Selection}} and {{Training}}.
\newblock July 2018.

\bibitem[Lucas et~al.(2021)Lucas, Bae, Zhang, Fort, Zemel, and Grosse]{lucasMonotonicLinearInterpolation2021}
Lucas, J.~R., Bae, J., Zhang, M.~R., Fort, S., Zemel, R., and Grosse, R.~B.
\newblock On {{Monotonic Linear Interpolation}} of {{Neural Network Parameters}}.
\newblock In \emph{International {{Conference}} on {{Machine Learning}}}. {PMLR}, July 2021.

\bibitem[Martin \& Mahoney(2019{\natexlab{a}})Martin and Mahoney]{martinRethinkingGeneralizationRequires2019}
Martin, C.~H. and Mahoney, M.~W.
\newblock Rethinking generalization requires revisiting old ideas: Statistical mechanics approaches and complex learning behavior, February 2019{\natexlab{a}}.

\bibitem[Martin \& Mahoney(2019{\natexlab{b}})Martin and Mahoney]{martinTraditionalHeavyTailedSelf2019}
Martin, C.~H. and Mahoney, M.~W.
\newblock Traditional and {{Heavy-Tailed Self Regularization}} in {{Neural Network Models}}.
\newblock January 2019{\natexlab{b}}.

\bibitem[Martin \& Mahoney(2020)Martin and Mahoney]{MM20_SDM}
Martin, C.~H. and Mahoney, M.~W.
\newblock Heavy-tailed {U}niversality predicts trends in test accuracies for very large pre-trained deep neural networks.
\newblock In \emph{Proceedings of the 20th SIAM International Conference on Data Mining}, 2020.

\bibitem[Martin \& Mahoney(2021)Martin and Mahoney]{martinImplicitSelfregularizationDeep2021}
Martin, C.~H. and Mahoney, M.~W.
\newblock Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0 (1), January 2021.

\bibitem[Martin et~al.(2021)Martin, Peng, and Mahoney]{martinPredictingTrendsQuality2021}
Martin, C.~H., Peng, T.~S., and Mahoney, M.~W.
\newblock Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data.
\newblock \emph{Nature Communications}, 12\penalty0 (1), July 2021.

\bibitem[Meller \& Berkouk(2023)Meller and Berkouk]{mellerSingularValueRepresentation2023}
Meller, D. and Berkouk, N.
\newblock Singular {{Value Representation}}: {{A New Graph Perspective On Neural Networks}}, February 2023.

\bibitem[Nava et~al.(2022)Nava, Kobayashi, Yin, Katzschmann, and Grewe]{navaMetaLearningClassifierFree2022}
Nava, E., Kobayashi, S., Yin, Y., Katzschmann, R.~K., and Grewe, B.~F.
\newblock Meta-{{Learning}} via {{Classifier}}(-free) {{Diffusion Guidance}}.
\newblock October 2022.

\bibitem[Nguyen et~al.(2019)Nguyen, Tran, Gupta, Rana, and Dam]{nguyenHyperVAEMinimumDescription2019}
Nguyen, P., Tran, T., Gupta, S., Rana, S., and Dam, H.-C.
\newblock {{HyperVAE}}: {{A Minimum Description Length Variational Hyper-Encoding Network}}.
\newblock 2019.

\bibitem[Nguyen(2019)]{nguyenConnectedSublevelSets2019}
Nguyen, Q.~N.
\newblock On {{Connected Sublevel Sets}} in {{Deep Learning}}.
\newblock In \emph{International {{Conference}} on {{Machine Learning}}}, January 2019.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{paszkePyTorchImperativeStyle2019}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S.
\newblock {{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}} 32}, 2019.

\bibitem[Peebles et~al.(2022)Peebles, Radosavovic, Brooks, Efros, and Malik]{peeblesLearningLearnGenerative2022}
Peebles, W., Radosavovic, I., Brooks, T., Efros, A.~A., and Malik, J.
\newblock Learning to {{Learn}} with {{Generative Models}} of {{Neural Network Checkpoints}}, September 2022.

\bibitem[Ratzlaff \& Fuxin(2019)Ratzlaff and Fuxin]{ratzlaffHyperGANGenerativeModel2019}
Ratzlaff, N. and Fuxin, L.
\newblock {{HyperGAN}}: {{A Generative Model}} for {{Diverse}}, {{Performant Neural Networks}}.
\newblock In \emph{Proceedings of the 36th {{International Conference}} on {{Machine Learning}}}. {PMLR}, May 2019.

\bibitem[Sch{\"u}rholt et~al.(2021)Sch{\"u}rholt, Kostadinov, and Borth]{schurholtSelfSupervisedRepresentationLearning2021}
Sch{\"u}rholt, K., Kostadinov, D., and Borth, D.
\newblock Self-{{Supervised Representation Learning}} on {{Neural Network Weights}} for {{Model Characteristic Prediction}}.
\newblock In \emph{Conference on {{Neural Information Processing Systems}} ({{NeurIPS}})}, volume~35, 2021.

\bibitem[Sch{\"u}rholt et~al.(2022{\natexlab{a}})Sch{\"u}rholt, Knyazev, {Gir{\'o}-i-Nieto}, and Borth]{schurholtHyperRepresentationsGenerativeModels2022}
Sch{\"u}rholt, K., Knyazev, B., {Gir{\'o}-i-Nieto}, X., and Borth, D.
\newblock Hyper-{{Representations}} as {{Generative Models}}: {{Sampling Unseen Neural Network Weights}}.
\newblock In \emph{Thirty-Sixth {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})}, September 2022{\natexlab{a}}.

\bibitem[Sch{\"u}rholt et~al.(2022{\natexlab{b}})Sch{\"u}rholt, Knyazev, {Gir{\'o}-i-Nieto}, and Borth]{schurholtHyperRepresentationsPreTrainingTransfer2022}
Sch{\"u}rholt, K., Knyazev, B., {Gir{\'o}-i-Nieto}, X., and Borth, D.
\newblock Hyper-{{Representations}} for {{Pre-Training}} and {{Transfer Learning}}.
\newblock In \emph{First {{Workshop}} of {{Pre-training}}: {{Perspectives}}, {{Pitfalls}}, and {{Paths Forward}} at {{ICML}} 2022}, 2022{\natexlab{b}}.

\bibitem[Sch{\"u}rholt et~al.(2022{\natexlab{c}})Sch{\"u}rholt, Taskiran, Knyazev, {Gir{\'o}-i-Nieto}, and Borth]{schurholtModelZoosDataset2022}
Sch{\"u}rholt, K., Taskiran, D., Knyazev, B., {Gir{\'o}-i-Nieto}, X., and Borth, D.
\newblock Model {{Zoos}}: {{A Dataset}} of {{Diverse Populations}} of {{Neural Network Models}}.
\newblock In \emph{Thirty-Sixth {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}}) {{Datasets}} and {{Benchmarks Track}}}, September 2022{\natexlab{c}}.

\bibitem[S{\'e}mery(2024)]{semeryOsmrImgclsmob2024}
S{\'e}mery, O.
\newblock Osmr/imgclsmob, January 2024.

\bibitem[Smith \& Topin(2018)Smith and Topin]{smithSuperConvergenceVeryFast2018}
Smith, L.~N. and Topin, N.
\newblock Super-{{Convergence}}: {{Very Fast Training}} of {{Neural Networks Using Large Learning Rates}}, May 2018.

\bibitem[Unterthiner et~al.(2020)Unterthiner, Keysers, Gelly, Bousquet, and Tolstikhin]{unterthinerPredictingNeuralNetwork2020}
Unterthiner, T., Keysers, D., Gelly, S., Bousquet, O., and Tolstikhin, I.
\newblock Predicting {{Neural Network Accuracy}} from {{Weights}}.
\newblock February 2020.

\bibitem[Vaswani et~al.(2021)Vaswani, Ramachandran, Srinivas, Parmar, Hechtman, and Shlens]{vaswaniScalingLocalSelfAttention2021}
Vaswani, A., Ramachandran, P., Srinivas, A., Parmar, N., Hechtman, B., and Shlens, J.
\newblock Scaling {{Local Self-Attention}} for {{Parameter Efficient Visual Backbones}}.
\newblock In \emph{Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}, 2021.

\bibitem[Wang et~al.(2023)Wang, Chen, Yu, Cheung, and LeCun]{wangCompactOptimalDeep2023}
Wang, J., Chen, Y., Yu, S.~X., Cheung, B., and LeCun, Y.
\newblock Compact and {{Optimal Deep Learning}} with {{Recurrent Parameter Generators}}.
\newblock In \emph{2023 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})}. {IEEE}, January 2023.

\bibitem[Wortsman et~al.(2021)Wortsman, Horton, Guestrin, Farhadi, and Rastegari]{wortsmanLearningNeuralNetwork2021}
Wortsman, M., Horton, M.~C., Guestrin, C., Farhadi, A., and Rastegari, M.
\newblock Learning {{Neural Network Subspaces}}.
\newblock In \emph{International {{Conference}} on {{Machine Learning}}}. {PMLR}, July 2021.

\bibitem[Yak et~al.(2019)Yak, Gonzalvo, and Mazzawi]{yakTaskArchitectureIndependentGeneralization2019}
Yak, S., Gonzalvo, J., and Mazzawi, H.
\newblock Towards {{Task}} and {{Architecture-Independent Generalization Gap Predictors}}.
\newblock June 2019.

\bibitem[Yang et~al.(2022)Yang, Theisen, Hodgkinson, Gonzalez, Ramchandran, Martin, and Mahoney]{YTHx22_TR}
Yang, Y., Theisen, R., Hodgkinson, L., Gonzalez, J.~E., Ramchandran, K., Martin, C.~H., and Mahoney, M.~W.
\newblock Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data.
\newblock Technical Report Preprint: arXiv:2202.02842, 2022.

\bibitem[Zhang et~al.(2019)Zhang, Ren, and Urtasun]{zhangGraphHyperNetworksNeural2019}
Zhang, C., Ren, M., and Urtasun, R.
\newblock Graph {{HyperNetworks}} for {{Neural Architecture Search}}.
\newblock In \emph{International {{Conference}} on {{Learning Representations}} ({{ICLR}})}, 2019.

\bibitem[Zhang et~al.(2023)Zhang, Kofinas, Zhang, Chen, Burghouts, and Snoek]{zhangNeuralNetworksAre2023}
Zhang, D.~W., Kofinas, M., Zhang, Y., Chen, Y., Burghouts, G.~J., and Snoek, C. G.~M.
\newblock Neural {{Networks Are Graphs}}!{{Graph Neural Networks}} for {{Equivariant Processing}} of {{Neural Networks}}.
\newblock July 2023.

\bibitem[Zhmoginov et~al.(2022)Zhmoginov, Sandler, and Vladymyrov]{zhmoginovHyperTransformerModelGeneration2022}
Zhmoginov, A., Sandler, M., and Vladymyrov, M.
\newblock {{HyperTransformer}}: {{Model Generation}} for {{Supervised}} and {{Semi-Supervised Few-Shot Learning}}.
\newblock In \emph{International {{Conference}} on {{Machine Learning}} ({{ICML}})}, January 2022.

\end{thebibliography}
