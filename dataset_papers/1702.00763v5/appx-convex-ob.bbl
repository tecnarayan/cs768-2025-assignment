\newcommand{\noopsort}[1]{}
\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2017)Agarwal, {Allen-Zhu}, Bullins, Hazan, and
  Ma]{AABHM2016}
Naman Agarwal, Zeyuan {Allen-Zhu}, Brian Bullins, Elad Hazan, and Tengyu Ma.
\newblock {Finding Approximate Local Minima for Nonconvex Optimization in
  Linear Time}.
\newblock In \emph{STOC}, 2017.
\newblock Full version available at \url{http://arxiv.org/abs/1611.01146}.

\bibitem[{Allen-Zhu}(2017)]{Allenzhu2016Katyusha}
Zeyuan {Allen-Zhu}.
\newblock {Katyusha: The First Direct Acceleration of Stochastic Gradient
  Methods}.
\newblock In \emph{STOC}, 2017.
\newblock Full version available at \url{http://arxiv.org/abs/1603.05953}.

\bibitem[{Allen-Zhu}(2018{\natexlab{a}})]{Allenzhu2017-natasha2}
Zeyuan {Allen-Zhu}.
\newblock {Natasha 2: Faster Non-Convex Optimization Than SGD}.
\newblock In \emph{NIPS}, 2018{\natexlab{a}}.
\newblock Full version available at \url{http://arxiv.org/abs/1708.08694}.

\bibitem[{Allen-Zhu}(2018{\natexlab{b}})]{Allenzhu2018-katyushaX}
Zeyuan {Allen-Zhu}.
\newblock {Katyusha X: Practical Momentum Method for Stochastic
  Sum-of-Nonconvex Optimization}.
\newblock In \emph{ICML}, 2018{\natexlab{b}}.
\newblock Full version available at \url{http://arxiv.org/abs/1802.03866}.

\bibitem[{Allen-Zhu}(2018{\natexlab{c}})]{Allenzhu2018-sgd3}
Zeyuan {Allen-Zhu}.
\newblock {How To Make the Gradients Small Stochastically: Even Faster Convex
  and Nonconvex SGD}.
\newblock In \emph{NIPS}, 2018{\natexlab{c}}.
\newblock Full version available at \url{http://arxiv.org/abs/1801.02982}.

\bibitem[{Allen-Zhu} and Hazan(2016)]{AH2016-nonconvex}
Zeyuan {Allen-Zhu} and Elad Hazan.
\newblock {Variance Reduction for Faster Non-Convex Optimization}.
\newblock In \emph{ICML}, 2016.
\newblock Full version available at \url{http://arxiv.org/abs/1603.05643}.

\bibitem[{Allen-Zhu} and Li(2016)]{AL2016-kSVD}
Zeyuan {Allen-Zhu} and Yuanzhi Li.
\newblock {LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain}.
\newblock In \emph{NIPS}, 2016.
\newblock Full version available at \url{http://arxiv.org/abs/1607.03463}.

\bibitem[{Allen-Zhu} and Li(2017{\natexlab{a}})]{AL2016-kCCA}
Zeyuan {Allen-Zhu} and Yuanzhi Li.
\newblock {Doubly Accelerated Methods for Faster CCA and Generalized
  Eigendecomposition}.
\newblock In \emph{ICML}, 2017{\natexlab{a}}.

\bibitem[{Allen-Zhu} and Li(2017{\natexlab{b}})]{AL2017-MMWU}
Zeyuan {Allen-Zhu} and Yuanzhi Li.
\newblock {Follow the Compressed Leader: Faster Online Learning of Eigenvectors
  and Faster MMWU}.
\newblock In \emph{ICML}, 2017{\natexlab{b}}.
\newblock Full version available at \url{http://arxiv.org/abs/1701.01722}.

\bibitem[{Allen-Zhu} and Li(2018)]{AllenLi2017-neon2}
Zeyuan {Allen-Zhu} and Yuanzhi Li.
\newblock {Neon2: Finding Local Minima via First-Order Oracles}.
\newblock In \emph{NIPS}, 2018.
\newblock Full version available at \url{http://arxiv.org/abs/1711.06673}.

\bibitem[{Allen-Zhu} and Orecchia(2017)]{AO-survey-nesterov}
Zeyuan {Allen-Zhu} and Lorenzo Orecchia.
\newblock {Linear Coupling: An Ultimate Unification of Gradient and Mirror
  Descent}.
\newblock In \emph{Proceedings of the 8th Innovations in Theoretical Computer
  Science}, ITCS~'17, 2017.
\newblock Full version available at \url{http://arxiv.org/abs/1407.1537}.

\bibitem[{Allen-Zhu} and Yuan(2016)]{AY2015-univr}
Zeyuan {Allen-Zhu} and Yang Yuan.
\newblock {Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex
  Objectives}.
\newblock In \emph{ICML}, 2016.

\bibitem[Carmon et~al.(2016)Carmon, Duchi, Hinder, and Sidford]{CarmonDHS2016}
Yair Carmon, John~C. Duchi, Oliver Hinder, and Aaron Sidford.
\newblock {Accelerated Methods for Non-Convex Optimization}.
\newblock \emph{ArXiv e-prints}, abs/1611.00756, November 2016.

\bibitem[Chen and Yang(2018)]{ChenYang2018}
Zaiyi Chen and Tianbao Yang.
\newblock A variance reduction method for non-convex optimization with improved
  convergence under large condition number.
\newblock \emph{ArXiv e-prints}, abs/1809.06754, 9 2018.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  {Lacoste-Julien}]{Defazio2014-SAGA}
Aaron Defazio, Francis Bach, and Simon {Lacoste-Julien}.
\newblock {SAGA: A Fast Incremental Gradient Method With Support for
  Non-Strongly Convex Composite Objectives}.
\newblock In \emph{NIPS}, 2014.

\bibitem[Frostig et~al.(2015)Frostig, Ge, Kakade, and
  Sidford]{FrostigGKS2015-Catalyst}
Roy Frostig, Rong Ge, Sham~M. Kakade, and Aaron Sidford.
\newblock {Un-regularizing: approximate proximal point and faster stochastic
  algorithms for empirical risk minimization}.
\newblock In \emph{ICML}, 2015.

\bibitem[Garber et~al.(2016)Garber, Hazan, Jin, Kakade, Musco, Netrapalli, and
  Sidford]{GarberHazan-et-al-2016-ICML}
Dan Garber, Elad Hazan, Chi Jin, Sham~M. Kakade, Cameron Musco, Praneeth
  Netrapalli, and Aaron Sidford.
\newblock Robust shift-and-invert preconditioning: Faster and more sample
  efficient algorithms for eigenvector computation.
\newblock In \emph{ICML}, 2016.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{GeHJY2015}
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.
\newblock Escaping from saddle points---online stochastic gradient for tensor
  decomposition.
\newblock In \emph{Proceedings of the 28th Annual Conference on Learning
  Theory}, COLT 2015, 2015.

\bibitem[Ghadimi and Lan(2013)]{GhadimiLan2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Johnson and Zhang(2013)]{JohnsonZhang2013-SVRG}
Rie Johnson and Tong Zhang.
\newblock {Accelerating stochastic gradient descent using predictive variance
  reduction}.
\newblock In \emph{Advances in Neural Information Processing Systems}, NIPS
  2013, pages 315--323, 2013.

\bibitem[Lan and Yang(2018)]{lan2018accelerated}
Guanghui Lan and Yu~Yang.
\newblock Accelerated stochastic algorithms for nonconvex finite-sum and
  multi-block optimization.
\newblock \emph{ArXiv e-prints}, abs/1805.05411, 2018.

\bibitem[Lee et~al.(2016)Lee, Simchowitz, Jordan, and Recht]{LeeSJR2016}
Jason~D. Lee, Max Simchowitz, Michael~I. Jordan, and Benjamin Recht.
\newblock Gradient descent only converges to minimizers.
\newblock In \emph{Proceedings of the 29th Conference on Learning Theory,
  {COLT} 2016, New York, USA, June 23-26, 2016}, pages 1246--1257, 2016.

\bibitem[Lei et~al.(2017)Lei, Ju, Chen, and Jordan]{LeiJCJ2017}
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael~I Jordan.
\newblock {Nonconvex Finite-Sum Optimization Via SCSG Methods}.
\newblock In \emph{NIPS}, 2017.

\bibitem[Lin et~al.(2015)Lin, Mairal, and Harchaoui]{LinMH2015-Catalyst}
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.
\newblock {A Universal Catalyst for First-Order Optimization}.
\newblock In \emph{NIPS}, 2015.

\bibitem[Nesterov(2004)]{Nesterov2004}
Yurii Nesterov.
\newblock \emph{Introductory Lectures on Convex Programming Volume: A Basic
  course}, volume~I.
\newblock Kluwer Academic Publishers, 2004.
\newblock ISBN 1402075537.

\bibitem[Nesterov(2008)]{nesterov2008cubic}
Yurii Nesterov.
\newblock Accelerating the cubic regularization of newton's method on convex
  problems.
\newblock \emph{Mathematical Programming}, 112\penalty0 (1):\penalty0 159--181,
  2008.

\bibitem[Nesterov(2012)]{Nesterov2012make}
Yurii Nesterov.
\newblock How to make the gradients small.
\newblock \emph{Optima}, 88:\penalty0 10--11, 2012.

\bibitem[Reddi et~al.(2016)Reddi, Hefny, Sra, Poczos, and
  Smola]{Reddi2016-nonconvexSVRG}
Sashank~J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{ICML}, 2016.

\bibitem[Schmidt et~al.(2013)Schmidt, {Le Roux}, and Bach]{Schmidt2013-SAG}
Mark Schmidt, Nicolas {Le Roux}, and Francis Bach.
\newblock {Minimizing finite sums with the stochastic average gradient}.
\newblock \emph{ArXiv e-prints}, abs/1309.2388, September 2013.
\newblock Preliminary version appeared in NIPS 2012.

\bibitem[{Shalev-Shwartz}(2016)]{Shalev-Shwartz2015-SDCAwithoutDual}
Shai {Shalev-Shwartz}.
\newblock {SDCA without Duality, Regularization, and Individual Convexity}.
\newblock In \emph{ICML}, 2016.

\bibitem[Xu and Yang(2017)]{XuYang-neon}
Yi~Xu and Tianbao Yang.
\newblock {First-order Stochastic Algorithms for Escaping From Saddle Points in
  Almost Linear Time}.
\newblock \emph{ArXiv e-prints}, abs/1711.01944, November 2017.

\bibitem[Zhang et~al.(2013)Zhang, Mahdavi, and Jin]{MahdaviZhangJin2013-sc}
Lijun Zhang, Mehrdad Mahdavi, and Rong Jin.
\newblock Linear convergence with condition number independent access of full
  gradients.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  980--988, 2013.

\end{thebibliography}
