\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen{-}Zhu and Li(2019{\natexlab{a}})]{al192}
Zeyuan Allen{-}Zhu and Yuanzhi Li.
\newblock Can {SGD} learn recurrent neural networks with provable
  generalization?
\newblock \emph{CoRR}, abs/1902.01028, 2019{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1902.01028}.

\bibitem[Allen{-}Zhu and Li(2019{\natexlab{b}})]{zz123}
Zeyuan Allen{-}Zhu and Yuanzhi Li.
\newblock What can resnet learn efficiently, going beyond kernels?
\newblock \emph{CoRR}, abs/1905.10337, 2019{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/1905.10337}.

\bibitem[{Allen-Zhu} et~al.(2018){Allen-Zhu}, Li, and Liang]{all18}
Zeyuan {Allen-Zhu}, Yuanzhi Li, and Yingyu Liang.
\newblock {Learning and Generalization in Overparameterized Neural Networks,
  Going Beyond Two Layers}.
\newblock \emph{arXiv preprint arXiv:1811.04918}, November 2018.

\bibitem[Allen-Zhu et~al.(2018)Allen-Zhu, Li, and Song]{allen2018convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock On the convergence rate of training recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1810.12065}, 2018.

\bibitem[{Allen-Zhu} et~al.(2018{\natexlab{a}}){Allen-Zhu}, Li, and
  Song]{als18}
Zeyuan {Allen-Zhu}, Yuanzhi Li, and Zhao Song.
\newblock On the convergence rate of training recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1810.12065}, 2018{\natexlab{a}}.

\bibitem[{Allen-Zhu} et~al.(2018{\natexlab{b}}){Allen-Zhu}, Li, and
  Song]{als18dnn}
Zeyuan {Allen-Zhu}, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock \emph{arXiv preprint arXiv:1811.03962}, November 2018{\natexlab{b}}.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{adhlw19}
Sanjeev Arora, Simon~S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock \emph{CoRR}, abs/1901.08584, 2019.
\newblock URL \url{http://arxiv.org/abs/1901.08584}.

\bibitem[Bartlett and Mendelson(2002)]{bartlett2002rademacher}
Peter~L Bartlett and Shahar Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 463--482, 2002.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet2002stability}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock \emph{Journal of machine learning research}, 2\penalty0
  (Mar):\penalty0 499--526, 2002.

\bibitem[Chen and Gu(2018)]{chen2018closing}
Jinghui Chen and Quanquan Gu.
\newblock Closing the generalization gap of adaptive gradient methods in
  training deep neural networks.
\newblock \emph{arXiv preprint arXiv:1806.06763}, 2018.

\bibitem[Dai and Zhu(2018)]{dai2018towards}
Xiaowu Dai and Yuhua Zhu.
\newblock Towards theoretical understanding of large batch training in
  stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:1812.00542}, 2018.

\bibitem[Du et~al.(2018{\natexlab{a}})Du, Lee, Tian, P{\'{o}}czos, and
  Singh]{dltps18}
Simon~S. Du, Jason~D. Lee, Yuandong Tian, Barnab{\'{a}}s P{\'{o}}czos, and
  Aarti Singh.
\newblock Gradient descent learns one-hidden-layer {CNN:} don't be afraid of
  spurious local minima.
\newblock In \emph{International Conference on Machine Learning (ICML)}.
  http://arxiv.org/abs/1712.00779, 2018{\natexlab{a}}.

\bibitem[Du et~al.(2018{\natexlab{b}})Du, Zhai, Poczos, and Singh]{dzps18}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock {Gradient Descent Provably Optimizes Over-parameterized Neural
  Networks}.
\newblock \emph{ArXiv e-prints}, 2018{\natexlab{b}}.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[{Ge} et~al.(2019){Ge}, {Kakade}, {Kidambi}, and
  {Netrapalli}]{ge2019step}
Rong {Ge}, Sham~M. {Kakade}, Rahul {Kidambi}, and Praneeth {Netrapalli}.
\newblock {The Step Decay Schedule: A Near Optimal, Geometrically Decaying
  Learning Rate Procedure}.
\newblock \emph{arXiv e-prints}, art. arXiv:1904.12838, Apr 2019.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{hzrs16}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{hoffer2017train}
Elad Hoffer, Itay Hubara, and Daniel Soudry.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1731--1741, 2017.

\bibitem[Hu et~al.(2017)Hu, Li, Li, and Liu]{hu2017diffusion}
Wenqing Hu, Chris~Junchi Li, Lei Li, and Jian-Guo Liu.
\newblock On the diffusion approximation of nonconvex stochastic gradient
  descent.
\newblock \emph{arXiv preprint arXiv:1705.07562}, 2017.

\bibitem[Jastrz{\k{e}}bski et~al.(2018)Jastrz{\k{e}}bski, Kenton, Ballas,
  Fischer, Bengio, and Storkey]{jastrzkebski2018dnn}
Stanis{\l}aw Jastrz{\k{e}}bski, Zachary Kenton, Nicolas Ballas, Asja Fischer,
  Yoshua Bengio, and Amos Storkey.
\newblock Dnn's sharpest directions along the sgd trajectory.
\newblock \emph{arXiv preprint arXiv:1807.05031}, 2018.

\bibitem[Keskar and Socher(2017)]{keskar2017improving}
Nitish~Shirish Keskar and Richard Socher.
\newblock Improving generalization performance by switching from adam to sgd.
\newblock \emph{arXiv preprint arXiv:1712.07628}, 2017.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kleinberg et~al.(2018)Kleinberg, Li, and Yuan]{yy1241}
Robert Kleinberg, Yuanzhi Li, and Yang Yuan.
\newblock An alternative view: When does {SGD} escape local minima?
\newblock \emph{CoRR}, abs/1802.06175, 2018.
\newblock URL \url{http://arxiv.org/abs/1802.06175}.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{ksh12}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem[Li and Liang(2018)]{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8157--8166, 2018.

\bibitem[Li et~al.(2017)Li, Ma, and Zhang]{lmz17}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix recovery.
\newblock \emph{CoRR}, abs/1712.09203, 2017.
\newblock URL \url{http://arxiv.org/abs/1712.09203}.

\bibitem[Loshchilov and Hutter(2016)]{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{arXiv preprint arXiv:1608.03983}, 2016.

\bibitem[Luo et~al.(2019)Luo, Xiong, Liu, and Sun]{luo2019adaptive}
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu~Sun.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock \emph{arXiv preprint arXiv:1902.09843}, 2019.

\bibitem[Mangalam and Prabhu(2019)]{mangalam2019do}
Karttikeya Mangalam and Vinay Prabhu.
\newblock Do deep neural networks learn shallow learnable examples first?
\newblock June 2019.

\bibitem[{Nakkiran} et~al.(2019){Nakkiran}, {Kaplun}, {Kalimeris}, {Yang},
  {Edelman}, {Zhang}, and {Barak}]{nakkiran2019sgd}
Preetum {Nakkiran}, Gal {Kaplun}, Dimitris {Kalimeris}, Tristan {Yang},
  Benjamin~L. {Edelman}, Fred {Zhang}, and Boaz {Barak}.
\newblock {SGD on Neural Networks Learns Functions of Increasing Complexity}.
\newblock \emph{arXiv e-prints}, art. arXiv:1905.11604, May 2019.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Smith(2017)]{smith2017cyclical}
Leslie~N Smith.
\newblock Cyclical learning rates for training neural networks.
\newblock In \emph{2017 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pages 464--472. IEEE, 2017.

\bibitem[Smith and Le(2017)]{smith2017bayesian}
Samuel~L Smith and Quoc~V Le.
\newblock A bayesian perspective on generalization and stochastic gradient
  descent.
\newblock \emph{arXiv preprint arXiv:1710.06451}, 2017.

\bibitem[Smith et~al.(2017)Smith, Kindermans, Ying, and Le]{smith2017don}
Samuel~L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc~V Le.
\newblock Don't decay the learning rate, increase the batch size.
\newblock \emph{arXiv preprint arXiv:1711.00489}, 2017.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Tieleman and Hinton(2012)]{tieleman2012lecture}
Tijmen Tieleman and Geoffrey Hinton.
\newblock Lecture 6.5-rmsprop, coursera: Neural networks for machine learning.
\newblock \emph{University of Toronto, Technical Report}, 2012.

\bibitem[Wen et~al.(2019)Wen, Luk, Gazeau, Zhang, Chan, and
  Ba]{wen2019interplay}
Yeming Wen, Kevin Luk, Maxime Gazeau, Guodong Zhang, Harris Chan, and Jimmy Ba.
\newblock Interplay between optimization and generalization of stochastic
  gradient descent with covariance noise.
\newblock \emph{arXiv preprint arXiv:1902.08234}, 2019.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and
  Recht]{wilson2017marginal}
Ashia~C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin
  Recht.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4148--4158, 2017.

\bibitem[Xing et~al.(2018)Xing, Arpit, Tsirigotis, and Bengio]{xing2018walk}
Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio.
\newblock A walk with sgd.
\newblock \emph{arXiv preprint arXiv:1802.08770}, 2018.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you2017large}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Large batch training of convolutional networks.
\newblock \emph{arXiv preprint arXiv:1708.03888}, 2017.

\bibitem[Zagoruyko and Komodakis(2016)]{zk16}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zeiler(2012)]{zeiler2012adadelta}
Matthew~D Zeiler.
\newblock Adadelta: an adaptive learning rate method.
\newblock \emph{arXiv preprint arXiv:1212.5701}, 2012.

\end{thebibliography}
