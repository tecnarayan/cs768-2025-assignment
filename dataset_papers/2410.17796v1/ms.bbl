\begin{thebibliography}{10}

\bibitem{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  322--332. PMLR, 2019.

\bibitem{atkinson2012spherical}
Kendall Atkinson and Weimin Han.
\newblock {\em Spherical harmonics and approximations on the unit sphere: an
  introduction}, volume 2044 of {\em Lecture Notes in Mathematics}.
\newblock Springer, Heidelberg, 2012.

\bibitem{aubin2020generalization}
Benjamin Aubin, Florent Krzakala, Yue Lu, and Lenka Zdeborov{\'a}.
\newblock Generalization error in high-dimensional perceptrons: Approaching
  bayes error with convex optimization.
\newblock {\em Advances in Neural Information Processing Systems},
  33:12199--12210, 2020.

\bibitem{bach2016breaking}
Francis Bach.
\newblock Breaking the curse of dimensionality with convex neural networks,
  2016.

\bibitem{bach2023high}
Francis Bach.
\newblock High-dimensional analysis of double descent for linear regression
  with random projections.
\newblock {\em arXiv preprint arXiv:2303.01372}, 2023.

\bibitem{bartlett2020benign}
Peter~L Bartlett, Philip~M Long, G{\'a}bor Lugosi, and Alexander Tsigler.
\newblock Benign overfitting in linear regression.
\newblock {\em Proceedings of the National Academy of Sciences},
  117(48):30063--30070, 2020.

\bibitem{barzilai2023generalization}
Daniel Barzilai and Ohad Shamir.
\newblock Generalization in kernel regression under realistic assumptions,
  2023.

\bibitem{belkin18understand}
Mikhail Belkin, Siyuan Ma, and Soumik Mandal.
\newblock To understand deep learning we need to understand kernel learning.
\newblock In Jennifer Dy and Andreas Krause, editors, {\em Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of {\em
  Proceedings of Machine Learning Research}, pages 541--549. PMLR, 10--15 Jul
  2018.

\bibitem{bietti2019inductive}
Alberto Bietti and Julien Mairal.
\newblock On the inductive bias of neural tangent kernels.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{bordelon2020spectrum}
Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan.
\newblock Spectrum dependent learning curves in kernel regression and wide
  neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1024--1034. PMLR, 2020.

\bibitem{BorthwickSpectralTheorem_2020}
David Borthwick.
\newblock {\em Spectral theory---basic concepts and applications}, volume 284
  of {\em Graduate Texts in Mathematics}.
\newblock Springer, Cham, [2020] \copyright 2020.

\bibitem{caponnetto2007optimal}
Andrea Caponnetto and Ernesto De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock {\em Foundations of Computational Mathematics}, 7:331--368, 2007.

\bibitem{cheng2023theoretical}
Tin~Sum Cheng, Aurelien Lucchi, Ivan Dokmani{\'c}, Anastasis Kratsios, and
  David Belius.
\newblock A theoretical analysis of the test error of finite-rank kernel ridge
  regression.
\newblock {\em Annual Conference on Neural Information Processing Systems},
  2023.

\bibitem{cheng2024characterizing}
Tin~Sum Cheng, Aurelien Lucchi, Anastasis Kratsios, and David Belius.
\newblock Characterizing overfitting in kernel ridgeless regression through the
  eigenspectrum.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2024.

\bibitem{cohen2023neural}
Samuel~N Cohen, Deqing Jiang, and Justin Sirignano.
\newblock Neural q-learning for solving pdes.
\newblock {\em Journal of Machine Learning Research}, 24(236), 2023.

\bibitem{cui2021generalization}
Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov{\'a}.
\newblock Generalization error rates in kernel regression: The crossover from
  the noiseless to noisy regime.
\newblock {\em Advances in Neural Information Processing Systems},
  34:10131--10143, 2021.

\bibitem{Donnelly_JFA_BoundsEigenvufncionsLap}
Harold Donnelly.
\newblock Bounds for eigenfunctions of the {L}aplacian on compact {R}iemannian
  manifolds.
\newblock {\em J. Funct. Anal.}, 187(1):247--261, 2001.

\bibitem{fujii1993norm}
Junichi Fujii, Masatoshi Fujii, Takayuki Furuta, and Ritsuo Nakamoto.
\newblock Norm inequalities equivalent to heinz inequality.
\newblock {\em Proceedings of the American Mathematical Society},
  118(3):827--830, 1993.

\bibitem{gavrilopoulos2024geometrical}
Georgios Gavrilopoulos, Guillaume Lecué, and Zong Shang.
\newblock A geometrical analysis of kernel ridge regression and its
  applications, 2024.

\bibitem{goldt2022gaussian}
Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc
  M{\'e}zard, and Lenka Zdeborov{\'a}.
\newblock The gaussian equivalence of generative models for learning with
  shallow neural networks.
\newblock In {\em Mathematical and Scientific Machine Learning}, pages
  426--471. PMLR, 2022.

\bibitem{goldt2020modeling}
Sebastian Goldt, Marc M\'ezard, Florent Krzakala, and Lenka Zdeborov\'a.
\newblock Modeling the influence of data structure on learning in neural
  networks: The hidden manifold model.
\newblock {\em Phys. Rev. X}, 10:041044, Dec 2020.

\bibitem{haas2024mind}
Moritz Haas, David Holzm{\"u}ller, Ulrike Luxburg, and Ingo Steinwart.
\newblock Mind the spikes: Benign overfitting of kernels and neural networks in
  fixed dimension.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{hastie2022surprises}
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan~J Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock {\em Annals of statistics}, 50(2):949, 2022.

\bibitem{hubbert2023sobolev}
Simon Hubbert, Emilio Porcu, Chris~J. Oates, and Mark Girolami.
\newblock Sobolev spaces, kernels and discrepancies over hyperspheres.
\newblock {\em Transactions on Machine Learning Research}, 2023.

\bibitem{jacot2018NTK}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, {\em Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem{jiang2023global}
Deqing Jiang, Justin Sirignano, and Samuel~N Cohen.
\newblock Global convergence of deep galerkin and pinns methods for solving
  partial differential equations.
\newblock {\em arXiv preprint arXiv:2305.06000}, 2023.

\bibitem{koltchinskii2017concentration}
Vladimir Koltchinskii and Karim Lounici.
\newblock Concentration inequalities and moment bounds for sample covariance
  operators.
\newblock {\em Bernoulli}, 23(1):110--133, 2017.

\bibitem{LarssonCohn_LpHermitePolyWienerChaos}
Lars Larsson-Cohn.
\newblock {$L^p$}-norms of {H}ermite polynomials and an extremal problem on
  {W}iener chaos.
\newblock {\em Ark. Mat.}, 40(1):133--144, 2002.

\bibitem{li2022saturation}
Yicheng Li, Haobo Zhang, and Qian Lin.
\newblock On the saturation effect of kernel ridge regression.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2022.

\bibitem{li2023kernel}
Yicheng Li, Haobo Zhang, and Qian Lin.
\newblock Kernel interpolation generalizes poorly.
\newblock {\em Biometrika}, 2023.

\bibitem{li2023on}
Yicheng Li, Haobo Zhang, and Qian Lin.
\newblock On the asymptotic learning curves of kernel ridge regression under
  power-law decay.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.

\bibitem{liang2020just}
Tengyuan Liang and Alexander Rakhlin.
\newblock Just interpolate: Kernel “ridgeless” regression can generalize.
\newblock {\em The Annals of Statistics}, 48(3), Jun 2020.

\bibitem{long2024duality}
Jihao Long, Xiaojun Peng, and Lei Wu.
\newblock A duality analysis of kernel ridge regression in the noiseless
  regime, 2024.

\bibitem{loureiro2021learning}
Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala,
  Marc Mezard, and Lenka Zdeborov{\'a}.
\newblock Learning curves of generic features maps for realistic datasets with
  a teacher-student model.
\newblock {\em Advances in Neural Information Processing Systems},
  34:18137--18151, 2021.

\bibitem{mallinar2022benign}
Neil Mallinar, James~B. Simon, Amirhesam Abedsoltan, Parthe Pandit, Mikhail
  Belkin, and Preetum Nakkiran.
\newblock Benign, tempered, or catastrophic: A taxonomy of overfitting, 2022.

\bibitem{misiakiewicz2024nonasymptotic}
Theodor Misiakiewicz and Basil Saeed.
\newblock A non-asymptotic theory of kernel ridge regression: deterministic
  equivalents, test error, and gcv estimator, 2024.

\bibitem{pardoux1998backward}
\'{E}tienne Pardoux.
\newblock Backward stochastic differential equations and viscosity solutions of
  systems of semilinear parabolic and elliptic {PDE}s of second order.
\newblock In {\em Stochastic analysis and related topics, {VI} ({G}eilo,
  1996)}, volume~42 of {\em Progr. Probab.}, pages 79--127. Birkh\"{a}user
  Boston, Boston, MA, 1998.

\bibitem{pardoux2005backward}
Etienne Pardoux and Shige Peng.
\newblock Backward stochastic differential equations and quasilinear parabolic
  partial differential equations.
\newblock In {\em Stochastic Partial Differential Equations and Their
  Applications: Proceedings of IFIP WG 7/1 International Conference University
  of North Carolina at Charlotte, NC June 6--8, 1991}, pages 200--217.
  Springer, 2005.

\bibitem{rosasco2010learning}
Lorenzo Rosasco, Mikhail Belkin, and Ernesto De~Vito.
\newblock On learning with integral operators.
\newblock {\em Journal of Machine Learning Research}, 11(2), 2010.

\bibitem{SaitohSawano_RKHSBook_2016}
Saburou Saitoh and Yoshihiro Sawano.
\newblock {\em Theory of reproducing kernels and applications}, volume~44 of
  {\em Developments in Mathematics}.
\newblock Springer, Singapore, 2016.

\bibitem{schoenberg1942positive}
Isaac~J Schoenberg.
\newblock Positive definite functions on spheres.
\newblock {\em Duke Mathematical Journal}, 9(1):96 -- 108, 1942.

\bibitem{seddik2020random}
Mohamed El~Amine Seddik, Cosme Louart, Mohamed Tamaazousti, and Romain
  Couillet.
\newblock Random matrix theory proves that deep learning representations of
  gan-data behave as gaussian mixtures.
\newblock In {\em International Conference on Machine Learning}, pages
  8573--8582. PMLR, 2020.

\bibitem{simon2015operator}
Barry Simon.
\newblock {\em Operator theory}, volume~4.
\newblock American Mathematical Soc., 2015.

\bibitem{simon2023eigenlearning}
James~B Simon, Madeline Dickens, Dhruva Karkada, and Michael Deweese.
\newblock The eigenlearning framework: A conservation law perspective on kernel
  ridge regression and wide neural networks.
\newblock {\em Transactions on Machine Learning Research}, 2023.

\bibitem{steinwart2008support}
Ingo Steinwart and Andreas Christmann.
\newblock {\em Support vector machines}.
\newblock Springer Science \& Business Media, 2008.

\bibitem{tsigler2023benign}
Alexander Tsigler and Peter~L Bartlett.
\newblock Benign overfitting in ridge regression.
\newblock {\em J. Mach. Learn. Res.}, 24:123--1, 2023.

\bibitem{vershynin2012introduction}
Roman Vershynin.
\newblock {\em Introduction to the non-asymptotic analysis of random matrices},
  page 210–268.
\newblock Cambridge University Press, 2012.

\bibitem{Wainwright2019high}
Martin~J. Wainwright.
\newblock {\em High-Dimensional Statistics: A Non-Asymptotic Viewpoint}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics.
  Cambridge University Press, 2019.

\bibitem{zhang2017understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization, 2017.

\bibitem{zhang2023optimality}
Haobo Zhang, Yicheng Li, and Qian Lin.
\newblock On the optimality of misspecified spectral algorithms, 2023.

\bibitem{zhivotovskiy2024dimension}
Nikita Zhivotovskiy.
\newblock Dimension-free bounds for sums of independent matrices and simple
  tensors via the variational principle.
\newblock {\em Electronic Journal of Probability}, 29:1--28, 2024.

\end{thebibliography}
