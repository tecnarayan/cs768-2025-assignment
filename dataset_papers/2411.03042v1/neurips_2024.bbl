\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alexander(1990)]{alexander1990solving}
Roger Alexander.
\newblock Solving ordinary differential equations i: Nonstiff problems (e. hairer, sp norsett, and g. wanner).
\newblock \emph{Siam Review}, 32\penalty0 (3):\penalty0 485, 1990.

\bibitem[Ascher and Petzold(1998)]{ascher1998computer}
Uri~M Ascher and Linda~R Petzold.
\newblock \emph{Computer methods for ordinary differential equations and differential-algebraic equations}.
\newblock Siam, 1998.

\bibitem[Baevski and Auli(2019)]{baevski2018adaptive}
Alexei Baevski and Michael Auli.
\newblock Adaptive input representations for neural language modeling.
\newblock In \emph{Proc. of ICLR}, 2019.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proc. of AAAI}, volume~34, pages 7432--7439, 2020.

\bibitem[Butcher(2009)]{butcher2009backward}
John~C. Butcher.
\newblock Numerical methods for ordinary differential equations: early days.
\newblock In Adhemar Bultheel and Ronald Cools, editors, \emph{The Birth of Numerical Analysis}, pages 35--44. World Scientific, 2009.

\bibitem[Butcher(1996)]{butcher1996history}
John~Charles Butcher.
\newblock A history of runge-kutta methods.
\newblock \emph{Applied numerical mathematics}, pages 247--260, 1996.

\bibitem[Chang et~al.(2018)Chang, Meng, Haber, Ruthotto, Begert, and Holtham]{chang2018reversible}
Bo~Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, and Elliot Holtham.
\newblock Reversible architectures for arbitrarily deep residual neural networks.
\newblock In \emph{Proc. of AAAI}, pages 2811--2818, 2018.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and Duvenaud]{chen2018neural}
Tian~Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud.
\newblock Neural ordinary differential equations.
\newblock In \emph{Proc. of NeurIPS}, pages 6572--6583, 2018.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{Proc. of NAACL}, pages 4171--4186, 2019.

\bibitem[Diethelm et~al.(2002)Diethelm, Ford, and Freed]{diethelm2002predictor}
Kai Diethelm, Neville~J Ford, and Alan~D Freed.
\newblock A predictor-corrector approach for the numerical solution of fractional differential equations.
\newblock \emph{Nonlinear Dynamics}, 29\penalty0 (1):\penalty0 3--22, 2002.

\bibitem[Dutta et~al.(2021)Dutta, Gautam, Chakrabarti, and Chakraborty]{dutta2021redesigning}
Subhabrata Dutta, Tanya Gautam, Soumen Chakrabarti, and Tanmoy Chakraborty.
\newblock Redesigning the transformer architecture with insights from multi-particle dynamical systems.
\newblock pages 5531--5544, 2021.

\bibitem[Gao et~al.(2024)Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster, Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds, Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and Zou]{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
\newblock A framework for few-shot language model evaluation, 07 2024.

\bibitem[Gu and Dao(2023)]{gu2023mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock \emph{CoRR}, abs/2312.00752, 2023.

\bibitem[Haber et~al.(2018)Haber, Ruthotto, Holtham, and Jun]{haber2018learning}
Eldad Haber, Lars Ruthotto, Elliot Holtham, and Seong{-}Hwan Jun.
\newblock Learning across scales - multiscale methods for convolution neural networks.
\newblock In \emph{Proc. of AAAI}, pages 3142--3148, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.}, 2016.

\bibitem[He et~al.(2019)He, Mo, Wang, Liu, Yang, and Cheng]{he2019ode}
Xiangyu He, Zitao Mo, Peisong Wang, Yang Liu, Mingyuan Yang, and Jian Cheng.
\newblock Ode-inspired network design for single image super-resolution.
\newblock In \emph{Proc. of in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.}, pages 1732--1741, 2019.

\bibitem[Hermann et~al.(2015)Hermann, Kocisk{\'{y}}, Grefenstette, Espeholt, Kay, Suleyman, and Blunsom]{hermann2015teaching}
Karl~Moritz Hermann, Tom{\'{a}}s Kocisk{\'{y}}, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.
\newblock Teaching machines to read and comprehend.
\newblock In \emph{Proc. of NeurIPS}, pages 1693--1701, 2015.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In \emph{Proc. of NeurIPS}, pages 6840--6851, 2020.

\bibitem[Hunter(1986)]{hunter1986exponentially}
J~Stuart Hunter.
\newblock The exponentially weighted moving average.
\newblock \emph{Journal of quality technology}, 18\penalty0 (4):\penalty0 203--210, 1986.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~Las~Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed]{jiang2023mistral7B}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~Las~Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L{\'{e}}lio~Renard Lavaud, Marie{-}Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timoth{\'{e}}e Lacroix, and William~El Sayed.
\newblock Mistral 7b.
\newblock \emph{CoRR}, abs/2310.06825, 2023.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{Proc. of ICLR}, 2015.

\bibitem[Kutta(1901)]{kutta1901beitrag}
Wilhelm Kutta.
\newblock Beitrag zur naherungsweisen integration totaler differentialgleichungen.
\newblock \emph{Z. Math. Phys.}, pages 435--453, 1901.

\bibitem[Larsson et~al.(2017)Larsson, Maire, and Shakhnarovich]{larsson2017fractalnet}
Gustav Larsson, Michael Maire, and Gregory Shakhnarovich.
\newblock Fractalnet: Ultra-deep neural networks without residuals.
\newblock In \emph{Proc. of ICLR}, 2017.

\bibitem[Lei~Ba et~al.(2016)Lei~Ba, Kiros, and Hinton]{lei2016layer}
Jimmy Lei~Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{ArXiv preprint}, 2016.

\bibitem[Lezama et~al.(2023)Lezama, Salimans, Jiang, Chang, Ho, and Essa]{lezama2023discrete}
Jos{\'{e}} Lezama, Tim Salimans, Lu~Jiang, Huiwen Chang, Jonathan Ho, and Irfan Essa.
\newblock Discrete predictor-corrector diffusion models for image synthesis.
\newblock In \emph{Proc. of ICLR}, 2023.

\bibitem[Li et~al.(2020)Li, Wang, Liu, Jiang, Du, Xiao, Wang, and Zhu]{li-etal-2020-shallow}
Bei Li, Ziyang Wang, Hui Liu, Yufan Jiang, Quan Du, Tong Xiao, Huizhen Wang, and Jingbo Zhu.
\newblock Shallow-to-deep training for neural machine translation.
\newblock In \emph{Proc. of EMNLP}, pages 995--1005, 2020.

\bibitem[Li et~al.(2021)Li, Wang, Liu, Du, Xiao, Zhang, and Zhu]{li2021learning}
Bei Li, Ziyang Wang, Hui Liu, Quan Du, Tong Xiao, Chunliang Zhang, and Jingbo Zhu.
\newblock Learning light-weight translation models from deep transformer.
\newblock In \emph{Proc. of AAAI}, pages 13217--13225, 2021.

\bibitem[Li et~al.(2022)Li, Du, Zhou, Jing, Zhou, Zeng, Xiao, Zhu, Liu, and Zhang]{li-etal-2022-ode}
Bei Li, Quan Du, Tao Zhou, Yi~Jing, Shuhan Zhou, Xin Zeng, Tong Xiao, JingBo Zhu, Xuebo Liu, and Min Zhang.
\newblock Ode transformer: An ordinary differential equation-inspired model for sequence generation.
\newblock In \emph{Proc. of ACL}, page 8335â€“8351, 2022.

\bibitem[Lin(2004)]{lin-2004-rouge}
Chin-Yew Lin.
\newblock {ROUGE}: A package for automatic evaluation of summaries.
\newblock In \emph{Text Summarization Branches Out}, pages 74--81, 2004.

\bibitem[Liu et~al.(2022)Liu, Ren, Lin, and Zhao]{liu2022pseudo}
Luping Liu, Yi~Ren, Zhijie Lin, and Zhou Zhao.
\newblock Pseudo numerical methods for diffusion models on manifolds.
\newblock 2022.

\bibitem[Liu et~al.(2020)Liu, Wang, Wong, Ding, Chao, and Tu]{liu2020understanding}
Xuebo Liu, Longyue Wang, Derek~F Wong, Liang Ding, Lidia~S Chao, and Zhaopeng Tu.
\newblock Understanding and improving encoder layer fusion in sequence-to-sequence learning.
\newblock In \emph{Proc. of ICLR}, 2020.

\bibitem[Liu et~al.(2024)Liu, Zeng, Meng, and Zhou]{liu2023branchnorm}
Yijin Liu, Xianfeng Zeng, Fandong Meng, and Jie Zhou.
\newblock Branchnorm: Robustly scaling extremely deep transformers.
\newblock In \emph{Findings of ACL}, pages 11675--11687, 2024.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo]{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted windows.
\newblock In \emph{Proc. of CVPR}, pages 10012--10022, 2021.

\bibitem[Lu et~al.(2022{\natexlab{a}})Lu, Zhou, Bao, Chen, Li, and Zhu]{lu2022dpm}
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.
\newblock Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.
\newblock In \emph{Proc. of NeurIPS}, 2022{\natexlab{a}}.

\bibitem[Lu et~al.(2022{\natexlab{b}})Lu, Zhou, Bao, Chen, Li, and Zhu]{lu2022dpm++}
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.
\newblock Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models.
\newblock \emph{arXiv preprint arXiv:2211.01095}, 2022{\natexlab{b}}.

\bibitem[Lu et~al.(2018)Lu, Zhong, Li, and Dong]{yiping2018beyond}
Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong.
\newblock Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations.
\newblock In \emph{Proc. of ICML}, pages 3282--3291, 2018.

\bibitem[Lu et~al.(2019)Lu, Li, He, Sun, Dong, Qin, Wang, and Liu]{lu2019understanding}
Yiping Lu, Zhuohan Li, Di~He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and Tie-Yan Liu.
\newblock Understanding and improving transformer from a multi-particle dynamic system point of view.
\newblock \emph{ArXiv preprint}, 2019.

\bibitem[Mehta et~al.(2020)Mehta, Ghazvininejad, Iyer, Zettlemoyer, and Hajishirzi]{mehta2020delight}
Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, and Hannaneh Hajishirzi.
\newblock Delight: Very deep and light-weight transformer.
\newblock \emph{ArXiv preprint}, 2020.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and Socher]{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock In \emph{Proc. of ICLR}, 2016.

\bibitem[Mikolov et~al.(2011)Mikolov, Deoras, Kombrink, Burget, and {\v{C}}ernock{\`y}]{mikolov2011empirical}
Tom{\'a}{\v{s}} Mikolov, Anoop Deoras, Stefan Kombrink, Luk{\'a}{\v{s}} Burget, and Jan {\v{C}}ernock{\`y}.
\newblock Empirical evaluation and combination of advanced language modeling techniques.
\newblock In \emph{Proc. of INTERSPEECH}, 2011.

\bibitem[Nallapati et~al.(2016)Nallapati, Zhou, dos Santos, Gulcehre, and Xiang]{nallapati2016abstractive}
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang.
\newblock Abstractive text summarization using sequence-to-sequence {RNN}s and beyond.
\newblock In \emph{Proc. of CoNLL}, pages 280--290, 2016.

\bibitem[Newell et~al.(1959)Newell, Shaw, and Simon]{Newell1959problemsolving}
Allen Newell, J.~C. Shaw, and Herbert~A. Simon.
\newblock Report on a general problem-solving program.
\newblock In \emph{Proc. of ICIP}, pages 256--264, 1959.

\bibitem[Nichol and Dhariwal(2021)]{nichol2021improved}
Alexander~Quinn Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock In \emph{Proc. of ICML}, pages 8162--8171, 2021.

\bibitem[Ott et~al.(2018)Ott, Edunov, Grangier, and Auli]{ott2018scaling}
Myle Ott, Sergey Edunov, David Grangier, and Michael Auli.
\newblock Scaling neural machine translation.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation: Research Papers}, pages 1--9, 2018.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and Auli]{ott2019fairseq}
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock In \emph{Proc. of NAACL}, pages 48--53, 2019.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi, Pezzelle, Baroni, Boleda, and Fern{\'a}ndez]{paperno2016lambada}
Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern{\'a}ndez.
\newblock The lambada dataset: Word prediction requiring a broad discourse context.
\newblock \emph{arXiv preprint arXiv:1606.06031}, 2016.

\bibitem[Press et~al.(2021)Press, Smith, and Lewis]{shortformer2021press}
Ofir Press, Noah~A. Smith, and Mike Lewis.
\newblock Shortformer: Better language modeling using shorter inputs.
\newblock In \emph{Proc. of ACL}, pages 5493--5505, 2021.

\bibitem[Rei et~al.(2020)Rei, Stewart, Farinha, and Lavie]{rei-etal-2020-comet}
Ricardo Rei, Craig Stewart, Ana~C Farinha, and Alon Lavie.
\newblock {COMET}: A neural framework for {MT} evaluation.
\newblock In \emph{Proc. of EMNLP}, pages 2685--2702, 2020.

\bibitem[Runge(1895)]{runge1895numerische}
Carl Runge.
\newblock {\"U}ber die numerische aufl{\"o}sung von differentialgleichungen.
\newblock \emph{Mathematische Annalen}, pages 167--178, 1895.

\bibitem[Ruthotto and Haber(2019)]{ruthottohaber2019}
Lars Ruthotto and Eldad Haber.
\newblock Deep neural networks motivated by partial differential equations.
\newblock \emph{Journal of Mathematical Imaging and Vision volume}, pages 352--364, 2019.

\bibitem[Sakaguchi et~al.(2020)Sakaguchi, Bras, Bhagavatula, and Choi]{Sakaguchi2020Winograde}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock In \emph{Proc. of AAAI}, pages 8732--8740, 2020.

\bibitem[Sander et~al.(2021)Sander, Ablin, Blondel, and Peyr{\'{e}}]{meila2021momentum}
Michael~E. Sander, Pierre Ablin, Mathieu Blondel, and Gabriel Peyr{\'{e}}.
\newblock Momentum residual neural networks.
\newblock In \emph{Proc. of ICML}, pages 9276--9287, 2021.

\bibitem[Sennrich et~al.(2016)Sennrich, Haddow, and Birch]{sennrich-subword-neural}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{Proc. of ACL}, pages 1715--1725, 2016.

\bibitem[Shaw et~al.(2018)Shaw, Uszkoreit, and Vaswani]{shaw-etal-2018-self}
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
\newblock Self-attention with relative position representations.
\newblock In \emph{Proc. of NAACL}, pages 464--468, 2018.

\bibitem[So et~al.(2019)So, Le, and Liang]{so2019evolved}
David~R. So, Quoc~V. Le, and Chen Liang.
\newblock The evolved transformer.
\newblock In \emph{Proc. of ICML}, pages 5877--5886, 2019.

\bibitem[Song et~al.(2021)Song, Meng, and Ermon]{song2020denoising}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock In \emph{Proc. of ICLR}, 2021.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`{e}}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{hugo2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie{-}Anne Lachaux, Timoth{\'{e}}e Lacroix, Baptiste Rozi{\`{e}}re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur{\'{e}}lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{CoRR}, abs/2302.13971, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Proc. of NeurIPS}, pages 5998--6008, 2017.

\bibitem[Wang et~al.(2019{\natexlab{a}})Wang, Singh, Michael, Hill, Levy, and Bowman]{wang2019glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R. Bowman.
\newblock {GLUE:} {A} multi-task benchmark and analysis platform for natural language understanding.
\newblock In \emph{Proc. of ICLR}. OpenReview.net, 2019{\natexlab{a}}.

\bibitem[Wang et~al.(2024)Wang, Ma, Dong, Huang, Zhang, and Wei]{wang2022deepnet}
Hongyu Wang, Shuming Ma, Li~Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.
\newblock Deepnet: Scaling transformers to 1,000 layers.
\newblock \emph{{IEEE} Trans. Pattern Anal. Mach. Intell.}, 46\penalty0 (10):\penalty0 6761--6774, 2024.

\bibitem[Wang et~al.(2019{\natexlab{b}})Wang, Li, Xiao, Zhu, Li, Wong, and Chao]{wang-etal-2019-learning}
Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek~F. Wong, and Lidia~S. Chao.
\newblock Learning deep transformer models for machine translation.
\newblock In \emph{Proc. of ACL}, pages 1810--1822, 2019{\natexlab{b}}.

\bibitem[Weinan(2017)]{weinan2017proposal}
E~Weinan.
\newblock A proposal on machine learning via dynamical systems.
\newblock \emph{Communications in Mathematics and Statistics}, pages 1--11, 2017.

\bibitem[Welbl et~al.(2017)Welbl, Liu, and Gardner]{SciQA2017}
Johannes Welbl, Nelson~F. Liu, and Matt Gardner.
\newblock Crowdsourcing multiple choice science questions.
\newblock In \emph{Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017, Copenhagen, Denmark, September 7, 2017}, pages 94--106, 2017.

\bibitem[Wu et~al.(2022)Wu, Wu, Xu, Wang, and Long]{wu2022flowformer}
Haixu Wu, Jialong Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long.
\newblock Flowformer: Linearizing transformers with conservation flows.
\newblock In \emph{Proc. of ICML}, pages 24226--24242. {PMLR}, 2022.

\bibitem[Wu et~al.(2020)Wu, Liu, Lin, Lin, and Han]{wu2020lite}
Zhanghao Wu, Zhijian Liu, Ji~Lin, Yujun Lin, and Song Han.
\newblock Lite transformer with long-short range attention.
\newblock In \emph{Proc. of ICLR}, 2020.

\bibitem[Xu et~al.(2022)Xu, Liu, Tegmark, and Jaakkola]{xu2022poisson}
Yilun Xu, Ziming Liu, Max Tegmark, and Tommi Jaakkola.
\newblock Poisson flow generative models.
\newblock \emph{arXiv preprint arXiv:2209.11178}, 2022.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock \emph{arXiv preprint arXiv:1905.07830}, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Williams, Titov, and Sennrich]{zhang-etal-2020-improving}
Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich.
\newblock Improving massively multilingual neural machine translation and zero-shot translation.
\newblock In \emph{Proc. of ACL}, pages 1628--1639, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Zhang, Kong, Wei, and Jiang]{zhang2021continuous}
Jing Zhang, Peng Zhang, Baiwen Kong, Junqiu Wei, and Xin Jiang.
\newblock Continuous self-attention models with neural {ODE} networks.
\newblock In \emph{Proc. of AAAI}, pages 14393--14401, 2021.

\bibitem[Zhang et~al.(2017)Zhang, Li, Loy, and Lin]{zhang2017polynet}
Xingcheng Zhang, Zhizhong Li, Chen~Change Loy, and Dahua Lin.
\newblock Polynet: {A} pursuit of structural diversity in very deep networks.
\newblock In \emph{Proc. of in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.}, pages 3900--3908, 2017.

\bibitem[Zhu et~al.(2023)Zhu, Chang, and Fu]{zhumai2023rk}
Mai Zhu, Bo~Chang, and Chong Fu.
\newblock Convolutional neural networks combined with runge-kutta methods.
\newblock \emph{Neural Comput. Appl.}, 35\penalty0 (2):\penalty0 1629--1643, 2023.

\end{thebibliography}
