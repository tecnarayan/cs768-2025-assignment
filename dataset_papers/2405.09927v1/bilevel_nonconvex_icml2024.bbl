\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arbel \& Mairal(2022{\natexlab{a}})Arbel and
  Mairal]{arbel2022amortized}
Arbel, M. and Mairal, J.
\newblock Amortized implicit differentiation for stochastic bilevel
  optimization.
\newblock In \emph{ICLR}, 2022{\natexlab{a}}.

\bibitem[Arbel \& Mairal(2022{\natexlab{b}})Arbel and Mairal]{arbel2022non}
Arbel, M. and Mairal, J.
\newblock Non-convex bilevel games with critical point selection maps.
\newblock In \emph{NeurIPS}, volume~35, pp.\  8013--8026, 2022{\natexlab{b}}.

\bibitem[Bai et~al.(2023)Bai, Ye, and Zeng]{bai2023optimality}
Bai, K., Ye, J.~J., and Zeng, S.
\newblock Optimality conditions for bilevel programs via {M}oreau envelope
  reformulation.
\newblock \emph{arXiv preprint arXiv:2311.14857}, 2023.

\bibitem[Beck(2017)]{beck2017first}
Beck, A.
\newblock \emph{First-order methods in optimization}.
\newblock SIAM, 2017.

\bibitem[Bergstra et~al.(2013)Bergstra, Yamins, and Cox]{bergstra2013making}
Bergstra, J., Yamins, D., and Cox, D.
\newblock Making a science of model search: Hyperparameter optimization in
  hundreds of dimensions for vision architectures.
\newblock In \emph{ICML}, pp.\  115--123, 2013.

\bibitem[Bertrand et~al.(2020)Bertrand, Klopfenstein, Blondel, Vaiter,
  Gramfort, and Salmon]{bertrand2020implicit}
Bertrand, Q., Klopfenstein, Q., Blondel, M., Vaiter, S., Gramfort, A., and
  Salmon, J.
\newblock Implicit differentiation of lasso-type models for hyperparameter
  optimization.
\newblock In \emph{ICML}, pp.\  810--821, 2020.

\bibitem[Bertrand et~al.(2022)Bertrand, Klopfenstein, Massias, Blondel, Vaiter,
  Gramfort, and Salmon]{bertrand2022implicit}
Bertrand, Q., Klopfenstein, Q., Massias, M., Blondel, M., Vaiter, S., Gramfort,
  A., and Salmon, J.
\newblock Implicit differentiation for fast hyperparameter selection in
  non-smooth convex learning.
\newblock \emph{JMLR}, 23\penalty0 (1):\penalty0 6680--6722, 2022.

\bibitem[B{\"o}hm \& Wright(2021)B{\"o}hm and Wright]{bohm2021variable}
B{\"o}hm, A. and Wright, S.~J.
\newblock Variable smoothing for weakly convex composite functions.
\newblock \emph{JOTA}, 188:\penalty0 628--649, 2021.

\bibitem[Chen et~al.(2023)Chen, Ma, and Zhang]{chen2023near}
Chen, L., Ma, Y., and Zhang, J.
\newblock Near-optimal fully first-order algorithms for finding stationary
  points in bilevel optimization.
\newblock \emph{arXiv preprint arXiv:2306.14853}, 2023.

\bibitem[Chen et~al.(2021)Chen, Sun, and Yin]{chen2021closing}
Chen, T., Sun, Y., and Yin, W.
\newblock Closing the gap: Tighter analysis of alternating stochastic gradient
  methods for bilevel problems.
\newblock In \emph{NeurIPS}, volume~34, pp.\  25294--25307, 2021.

\bibitem[Chen et~al.(2019)Chen, Xie, Wu, and Tian]{chen2019progressive}
Chen, X., Xie, L., Wu, J., and Tian, Q.
\newblock Progressive differentiable architecture search: Bridging the depth
  gap between search and evaluation.
\newblock In \emph{ICCV}, pp.\  1294--1303, 2019.

\bibitem[Chen et~al.(2022)Chen, Kailkhura, and Zhou]{chen2022fast}
Chen, Z., Kailkhura, B., and Zhou, Y.
\newblock A fast and convergent proximal algorithm for regularized nonconvex
  and nonsmooth bi-level optimization.
\newblock \emph{arXiv preprint arXiv:2203.16615}, 2022.

\bibitem[Dagr{\'e}ou et~al.(2022)Dagr{\'e}ou, Ablin, Vaiter, and
  Moreau]{dagreou2022framework}
Dagr{\'e}ou, M., Ablin, P., Vaiter, S., and Moreau, T.
\newblock A framework for bilevel optimization that enables stochastic and
  global variance reduction algorithms.
\newblock In \emph{NeurIPS}, volume~35, pp.\  26698--26710, 2022.

\bibitem[Elsken et~al.(2020)Elsken, Staffler, Metzen, and
  Hutter]{elsken2020meta}
Elsken, T., Staffler, B., Metzen, J.~H., and Hutter, F.
\newblock Meta-learning of neural architectures for few-shot learning.
\newblock In \emph{CVPR}, pp.\  12365--12375, 2020.

\bibitem[Feng \& Simon(2018)Feng and Simon]{feng2018gradient}
Feng, J. and Simon, N.
\newblock Gradient-based regularization parameter selection for problems with
  nonsmooth penalty functions.
\newblock \emph{JCGS}, 27\penalty0 (2):\penalty0 426--435, 2018.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{ICML}, pp.\  1126--1135, 2017.

\bibitem[Franceschi et~al.(2017)Franceschi, Donini, Frasconi, and
  Pontil]{franceschi2017forward}
Franceschi, L., Donini, M., Frasconi, P., and Pontil, M.
\newblock Forward and reverse gradient-based hyperparameter optimization.
\newblock In \emph{ICML}, pp.\  1165--1173, 2017.

\bibitem[Franceschi et~al.(2018)Franceschi, Frasconi, Salzo, Grazzi, and
  Pontil]{franceschi2018bilevel}
Franceschi, L., Frasconi, P., Salzo, S., Grazzi, R., and Pontil, M.
\newblock Bilevel programming for hyperparameter optimization and
  meta-learning.
\newblock In \emph{ICML}, pp.\  1568--1577, 2018.

\bibitem[Gao et~al.(2022)Gao, Ye, Yin, Zeng, and Zhang]{gao2022value}
Gao, L.~L., Ye, J.~J., Yin, H., Zeng, S., and Zhang, J.
\newblock Value function based difference-of-convex algorithm for bilevel
  hyperparameter selection problems.
\newblock In \emph{ICML}, pp.\  7164--7182, 2022.

\bibitem[Gao et~al.(2023)Gao, Ye, Yin, Zeng, and Zhang]{gao2023moreau}
Gao, L.~L., Ye, J.~J., Yin, H., Zeng, S., and Zhang, J.
\newblock Moreau envelope based difference-of-weakly-convex reformulation and
  algorithm for bilevel programs.
\newblock \emph{arXiv preprint arXiv:2306.16761}, 2023.

\bibitem[Ghadimi \& Wang(2018)Ghadimi and Wang]{ghadimi2018approximation}
Ghadimi, S. and Wang, M.
\newblock Approximation methods for bilevel programming.
\newblock \emph{arXiv preprint arXiv:1802.02246}, 2018.

\bibitem[Grazzi et~al.(2020)Grazzi, Franceschi, Pontil, and
  Salzo]{grazzi2020iteration}
Grazzi, R., Franceschi, L., Pontil, M., and Salzo, S.
\newblock On the iteration complexity of hypergradient computation.
\newblock In \emph{ICML}, pp.\  3748--3758, 2020.

\bibitem[Hong et~al.(2023)Hong, Wai, Wang, and Yang]{hong2020two}
Hong, M., Wai, H.-T., Wang, Z., and Yang, Z.
\newblock A two-timescale framework for bilevel optimization: Complexity
  analysis and application to actor-critic.
\newblock \emph{SIOPT}, 33\penalty0 (1):\penalty0 147--180, 2023.

\bibitem[Huang(2023{\natexlab{a}})]{huang2023adaptive}
Huang, F.
\newblock Adaptive mirror descent bilevel optimization.
\newblock \emph{arXiv preprint arXiv:2311.04520}, 2023{\natexlab{a}}.

\bibitem[Huang(2023{\natexlab{b}})]{huang2023momentumbased}
Huang, F.
\newblock On momentum-based gradient methods for bilevel optimization with
  nonconvex lower-level.
\newblock \emph{arXiv preprint arXiv:2303.03944}, 2023{\natexlab{b}}.

\bibitem[Huang et~al.(2022)Huang, Li, Gao, and Huang]{huang2021enhanced}
Huang, F., Li, J., Gao, S., and Huang, H.
\newblock Enhanced bilevel optimization via {B}regman distance.
\newblock In \emph{NeurIPS}, volume~35, pp.\  28928--28939, 2022.

\bibitem[Ji \& Liang(2022)Ji and Liang]{ji2021lower}
Ji, K. and Liang, Y.
\newblock Lower bounds and accelerated algorithms for bilevel optimization.
\newblock \emph{JMLR}, 23:\penalty0 1--56, 2022.

\bibitem[Ji et~al.(2020{\natexlab{a}})Ji, Lee, Liang, and
  Poor]{KaiyiJi2020ConvergenceOM}
Ji, K., Lee, J.~D., Liang, Y., and Poor, H.~V.
\newblock Convergence of meta-learning with task-specific adaptation over
  partial parameters.
\newblock In \emph{NeurIPS}, volume~33, pp.\  11490--11500, 2020{\natexlab{a}}.

\bibitem[Ji et~al.(2020{\natexlab{b}})Ji, Yang, and Liang]{ji2020bilevel}
Ji, K., Yang, J., and Liang, Y.
\newblock Bilevel optimization: Nonasymptotic analysis and faster algorithms.
\newblock \emph{arXiv preprint arXiv:2010.07962}, 2020{\natexlab{b}}.

\bibitem[Ji et~al.(2022)Ji, Liu, Liang, and Ying]{ji2022will}
Ji, K., Liu, M., Liang, Y., and Ying, L.
\newblock Will bilevel optimizers benefit from loops.
\newblock In \emph{NeurIPS}, volume~35, pp.\  3011--3023, 2022.

\bibitem[Kwon et~al.(2023{\natexlab{a}})Kwon, Kwon, Wright, and
  Nowak]{kwon2023penalty}
Kwon, J., Kwon, D., Wright, S., and Nowak, R.
\newblock On penalty methods for nonconvex bilevel optimization and first-order
  stochastic approximation.
\newblock \emph{arXiv preprint arXiv:2309.01753}, 2023{\natexlab{a}}.

\bibitem[Kwon et~al.(2023{\natexlab{b}})Kwon, Kwon, Wright, and
  Nowak]{kwon2023fully}
Kwon, J., Kwon, D., Wright, S., and Nowak, R.~D.
\newblock A fully first-order method for stochastic bilevel optimization.
\newblock In \emph{ICML}, pp.\  18083--18113, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2020)Li, Gu, and Huang]{li2020improved}
Li, J., Gu, B., and Huang, H.
\newblock Improved bilevel model: Fast and optimal algorithm with theoretical
  guarantee.
\newblock \emph{arXiv preprint arXiv:2009.00690}, 2020.

\bibitem[Li et~al.(2022)Li, Gu, and Huang]{li2022fully}
Li, J., Gu, B., and Huang, H.
\newblock A fully single loop algorithm for bilevel optimization without
  hessian inverse.
\newblock In \emph{AAAI}, volume~36, pp.\  7426--7434, 2022.

\bibitem[Liu et~al.(2018)Liu, Simonyan, and Yang]{liu2018darts}
Liu, H., Simonyan, K., and Yang, Y.
\newblock {DARTS}: Differentiable architecture search.
\newblock In \emph{ICLR}, 2018.

\bibitem[Liu et~al.(2020)Liu, Mu, Yuan, Zeng, and Zhang]{liu2020generic}
Liu, R., Mu, P., Yuan, X., Zeng, S., and Zhang, J.
\newblock A generic first-order algorithmic framework for bi-level programming
  beyond lower-level singleton.
\newblock In \emph{ICML}, pp.\  6305--6315, 2020.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Gao, Zhang, Meng, and
  Lin]{liu2021investigating}
Liu, R., Gao, J., Zhang, J., Meng, D., and Lin, Z.
\newblock Investigating bi-level optimization for learning and vision from a
  unified perspective: A survey and beyond.
\newblock \emph{IEEE TPAMI}, 44\penalty0 (12):\penalty0 10045--10067,
  2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Liu, Yuan, Zeng, and
  Zhang]{liu2021value}
Liu, R., Liu, X., Yuan, X., Zeng, S., and Zhang, J.
\newblock A value-function-based interior-point method for non-convex bi-level
  optimization.
\newblock In \emph{ICML}, pp.\  6882--6892, 2021{\natexlab{b}}.

\bibitem[Liu et~al.(2021{\natexlab{c}})Liu, Liu, Zeng, and
  Zhang]{liu2021towards}
Liu, R., Liu, Y., Zeng, S., and Zhang, J.
\newblock Towards gradient-based bilevel optimization with non-convex followers
  and beyond.
\newblock In \emph{NeurIPS}, volume~34, pp.\  8662--8675, 2021{\natexlab{c}}.

\bibitem[Liu et~al.(2022)Liu, Mu, Yuan, Zeng, and Zhang]{liu2022general}
Liu, R., Mu, P., Yuan, X., Zeng, S., and Zhang, J.
\newblock A general descent aggregation framework for gradient-based bi-level
  optimization.
\newblock \emph{IEEE TPAMI}, 45\penalty0 (1):\penalty0 38--57, 2022.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Gao, Liu, and
  Fan]{liu2023learningBRC}
Liu, R., Gao, J., Liu, X., and Fan, X.
\newblock Learning with constraint learning: New perspective, solution strategy
  and various applications.
\newblock \emph{arXiv preprint arXiv:2307.15257}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Liu, Yao, Zeng, and
  Zhang]{pmlr-v202-liu23y}
Liu, R., Liu, Y., Yao, W., Zeng, S., and Zhang, J.
\newblock Averaged method of multipliers for bi-level optimization without
  lower-level strong convexity.
\newblock In \emph{ICML}, pp.\  21839--21866, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2023{\natexlab{c}})Liu, Liu, Zeng, and
  Zhang]{liu2023augmenting}
Liu, R., Liu, Y., Zeng, S., and Zhang, J.
\newblock Augmenting iterative trajectory for bilevel optimization:
  Methodology, analysis and extensions.
\newblock \emph{arXiv preprint arXiv:2303.16397}, 2023{\natexlab{c}}.

\bibitem[Lu(2023)]{lu2023slm}
Lu, S.
\newblock {SLM}: A smoothed first-order {L}agrangian method for structured
  constrained nonconvex optimization.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Lu \& Mei(2023)Lu and Mei]{lu2023first}
Lu, Z. and Mei, S.
\newblock First-order penalty methods for bilevel optimization.
\newblock \emph{arXiv preprint arXiv:2301.01716}, 2023.

\bibitem[Mackay et~al.(2019)Mackay, Vicol, Lorraine, Duvenaud, and
  Grosse]{mackay2018self}
Mackay, M., Vicol, P., Lorraine, J., Duvenaud, D., and Grosse, R.
\newblock Self-tuning networks: Bilevel optimization of hyperparameters using
  structured best-response functions.
\newblock In \emph{ICLR}, 2019.

\bibitem[Mairal et~al.(2011)Mairal, Bach, and Ponce]{mairal2011task}
Mairal, J., Bach, F., and Ponce, J.
\newblock Task-driven dictionary learning.
\newblock \emph{IEEE TPAMI}, 34\penalty0 (4):\penalty0 791--804, 2011.

\bibitem[Mordukhovich(2018)]{mordukhovich2018variational}
Mordukhovich, B.~S.
\newblock \emph{Variational analysis and applications}, volume~30.
\newblock 2018.

\bibitem[Okuno et~al.(2018)Okuno, Takeda, and Kawana]{okuno2018hyperparameter}
Okuno, T., Takeda, A., and Kawana, A.
\newblock Hyperparameter learning via bilevel nonsmooth optimization.
\newblock \emph{arXiv preprint arXiv:1806.01520}, 2018.

\bibitem[Pedregosa(2016)]{pedregosa2016hyperparameter}
Pedregosa, F.
\newblock Hyperparameter optimization with approximate gradient.
\newblock In \emph{ICML}, pp.\  737--746, 2016.

\bibitem[Rajeswaran et~al.(2019)Rajeswaran, Finn, Kakade, and
  Levine]{rajeswaran2019meta}
Rajeswaran, A., Finn, C., Kakade, S.~M., and Levine, S.
\newblock Meta-learning with implicit gradients.
\newblock In \emph{NeurIPS}, volume~32, 2019.

\bibitem[Rockafellar(1974)]{rockafellar1974conjugate}
Rockafellar, R.~T.
\newblock \emph{Conjugate duality and optimization}.
\newblock SIAM, 1974.

\bibitem[Shen \& Chen(2023)Shen and Chen]{ShenC23}
Shen, H. and Chen, T.
\newblock On penalty-based bilevel gradient descent method.
\newblock In \emph{ICML}, pp.\  30992--31015, 2023.

\bibitem[Sow et~al.(2022{\natexlab{a}})Sow, Ji, Guan, and
  Liang]{sow2022constrained}
Sow, D., Ji, K., Guan, Z., and Liang, Y.
\newblock A constrained optimization approach to bilevel optimization with
  multiple inner minima.
\newblock \emph{arXiv preprint arXiv:2203.01123}, 2022{\natexlab{a}}.

\bibitem[Sow et~al.(2022{\natexlab{b}})Sow, Ji, and Liang]{sow2022convergence}
Sow, D., Ji, K., and Liang, Y.
\newblock On the convergence theory for hessian-free bilevel algorithms.
\newblock In \emph{NeurIPS}, volume~35, pp.\  4136--4149, 2022{\natexlab{b}}.

\bibitem[Xiao et~al.(2023)Xiao, Lu, and Chen]{xiao2023generalized}
Xiao, Q., Lu, S., and Chen, T.
\newblock An alternating optimization method for bilevel problems under the
  {P}olyak-{{\L}}ojasiewicz condition.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Xu et~al.(2019)Xu, Xie, Zhang, Chen, Qi, Tian, and Xiong]{xu2019pc1}
Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G.-J., Tian, Q., and Xiong, H.
\newblock Pc-darts: Partial channel connections for memory-efficient
  architecture search.
\newblock In \emph{ICLR}, 2019.

\bibitem[Yang et~al.(2023{\natexlab{a}})Yang, Luo, Li, and
  Jordan]{yang2023accelerating}
Yang, H., Luo, L., Li, C.~J., and Jordan, M.~I.
\newblock Accelerating inexact hypergradient descent for bilevel optimization.
\newblock \emph{arXiv preprint arXiv:2307.00126}, 2023{\natexlab{a}}.

\bibitem[Yang et~al.(2023{\natexlab{b}})Yang, Xiao, and Ji]{yang2023achieving}
Yang, Y., Xiao, P., and Ji, K.
\newblock Achieving ${O}(\epsilon^{-1.5})$ complexity in
  {H}essian/{J}acobian-free stochastic bilevel optimization.
\newblock In \emph{NeurIPS}, 2023{\natexlab{b}}.

\bibitem[Yao et~al.(2024)Yao, Yu, Zeng, and Zhang]{yao2024constrained}
Yao, W., Yu, C., Zeng, S., and Zhang, J.
\newblock Constrained bi-level optimization: Proximal lagrangian value function
  approach and hessian-free algorithm.
\newblock In \emph{ICLR}, 2024.

\bibitem[Ye \& Zhu(1995)Ye and Zhu]{ye1995optimality}
Ye, J.~J. and Zhu, D.
\newblock Optimality conditions for bilevel programming problems.
\newblock \emph{Optimization}, 33\penalty0 (1):\penalty0 9--27, 1995.

\bibitem[Ye et~al.(2023)Ye, Yuan, Zeng, and Zhang]{ye2023difference}
Ye, J.~J., Yuan, X., Zeng, S., and Zhang, J.
\newblock Difference of convex algorithms for bilevel programs with
  applications in hyperparameter selection.
\newblock \emph{MP}, 198\penalty0 (2):\penalty0 1583--1616, 2023.

\bibitem[Ye et~al.(2022)Ye, Liu, Wright, Stone, and Liu]{NeurIPS2022-Liu}
Ye, M., Liu, B., Wright, S., Stone, P., and Liu, Q.
\newblock Bome! bilevel optimization made easy: A simple first-order approach.
\newblock In \emph{NeurIPS}, volume~35, pp.\  17248--17262, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Khanduri, Tsaknakis, Yao, Hong, and
  Liu]{zhang2023introduction}
Zhang, Y., Khanduri, P., Tsaknakis, I., Yao, Y., Hong, M., and Liu, S.
\newblock An introduction to bi-level optimization: Foundations and
  applications in signal processing and machine learning.
\newblock \emph{arXiv preprint arXiv:2308.00788}, 2023.

\bibitem[Z{\"u}gner \& G{\"u}nnemann(2018)Z{\"u}gner and
  G{\"u}nnemann]{zugner2018adversarial}
Z{\"u}gner, D. and G{\"u}nnemann, S.
\newblock Adversarial attacks on graph neural networks via meta learning.
\newblock In \emph{ICLR}, 2018.

\end{thebibliography}
