\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alkousa et~al.(2020)Alkousa, Gasnikov, Dvinskikh, Kovalev, and
  Stonyakin]{alkousa2020accelerated}
Alkousa, M., Gasnikov, A., Dvinskikh, D., Kovalev, D., and Stonyakin, F.
\newblock Accelerated methods for saddle-point problem.
\newblock \emph{Computational Mathematics and Mathematical Physics},
  60\penalty0 (11):\penalty0 1787--1809, 2020.

\bibitem[Arioli \& Scott(2014)Arioli and Scott]{arioli2014chebyshev}
Arioli, M. and Scott, J.
\newblock Chebyshev acceleration of iterative refinement.
\newblock \emph{Numerical Algorithms}, 66\penalty0 (3):\penalty0 591--608,
  2014.

\bibitem[Arjevani et~al.(2020)Arjevani, Bruna, Can, G{\"u}rb{\"u}zbalaban,
  Jegelka, and Lin]{arjevani2020ideal}
Arjevani, Y., Bruna, J., Can, B., G{\"u}rb{\"u}zbalaban, M., Jegelka, S., and
  Lin, H.
\newblock Ideal: Inexact decentralized accelerated augmented lagrangian method.
\newblock \emph{arXiv preprint arXiv:2006.06733}, 2020.

\bibitem[Azizian et~al.(2020)Azizian, Scieur, Mitliagkas, Lacoste-Julien, and
  Gidel]{azizian2020accelerating}
Azizian, W., Scieur, D., Mitliagkas, I., Lacoste-Julien, S., and Gidel, G.
\newblock Accelerating smooth games by manipulating spectral shapes.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1705--1715. PMLR, 2020.

\bibitem[Bauschke \& Combettes(2011)Bauschke and
  Combettes]{Convex-Analysis-and-Monotone-Operator-Theory}
Bauschke, H.~H. and Combettes, P.~L.
\newblock \emph{Convex Analysis and Monotone Operator Theory in Hilbert
  Spaces}.
\newblock Springer, 2011.

\bibitem[Chambolle \& Pock(2011)Chambolle and Pock]{chambolle2011first}
Chambolle, A. and Pock, T.
\newblock A first-order primal-dual algorithm for convex problems with
  applications to imaging.
\newblock \emph{Journal of mathematical imaging and vision}, 40\penalty0
  (1):\penalty0 120--145, 2011.

\bibitem[Chambolle \& Pock(2016)Chambolle and Pock]{chambolle2016introduction}
Chambolle, A. and Pock, T.
\newblock An introduction to continuous optimization for imaging.
\newblock \emph{Acta Numerica}, 25:\penalty0 161--319, 2016.

\bibitem[Cohen et~al.(2020)Cohen, Sidford, and Tian]{cohen2020relative}
Cohen, M.~B., Sidford, A., and Tian, K.
\newblock Relative lipschitzness in extragradient methods and a direct recipe
  for acceleration.
\newblock \emph{arXiv preprint arXiv:2011.06572}, 2020.

\bibitem[Daskalakis et~al.(2018)Daskalakis, Ilyas, Syrgkanis, and
  Zeng]{daskalakis2018training}
Daskalakis, C., Ilyas, A., Syrgkanis, V., and Zeng, H.
\newblock Training gans with optimism.
\newblock In \emph{International Conference on Learning Representations (ICLR
  2018)}, 2018.

\bibitem[Du \& Hu(2019)Du and Hu]{du2019linear}
Du, S.~S. and Hu, W.
\newblock Linear convergence of the primal-dual gradient method for
  convex-concave saddle point problems without strong convexity.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  196--205. PMLR, 2019.

\bibitem[Du et~al.(2017)Du, Chen, Li, Xiao, and Zhou]{du2017stochastic}
Du, S.~S., Chen, J., Li, L., Xiao, L., and Zhou, D.
\newblock Stochastic variance reduction methods for policy evaluation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1049--1058. PMLR, 2017.

\bibitem[Gidel et~al.(2018)Gidel, Berard, Vignoud, Vincent, and
  Lacoste-Julien]{gidel2018variational}
Gidel, G., Berard, H., Vignoud, G., Vincent, P., and Lacoste-Julien, S.
\newblock A variational inequality perspective on generative adversarial
  networks.
\newblock \emph{arXiv preprint arXiv:1802.10551}, 2018.

\bibitem[Gidel et~al.(2019)Gidel, Hemmat, Pezeshki, Le~Priol, Huang,
  Lacoste-Julien, and Mitliagkas]{gidel2019negative}
Gidel, G., Hemmat, R.~A., Pezeshki, M., Le~Priol, R., Huang, G.,
  Lacoste-Julien, S., and Mitliagkas, I.
\newblock Negative momentum for improved game dynamics.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  1802--1811. PMLR, 2019.

\bibitem[Ibrahim et~al.(2020)Ibrahim, Azizian, Gidel, and
  Mitliagkas]{ibrahim2020linear}
Ibrahim, A., Azizian, W., Gidel, G., and Mitliagkas, I.
\newblock Linear lower bounds and conditioning of differentiable games.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4583--4593. PMLR, 2020.

\bibitem[Keriven et~al.(2018)Keriven, Bourrier, Gribonval, and
  P{\'e}rez]{keriven2018sketching}
Keriven, N., Bourrier, A., Gribonval, R., and P{\'e}rez, P.
\newblock Sketching for large-scale learning of mixture models.
\newblock \emph{Information and Inference: A Journal of the IMA}, 7\penalty0
  (3):\penalty0 447--508, 2018.

\bibitem[Korpelevich(1976)]{korpelevich1976extragradient}
Korpelevich, G.~M.
\newblock The extragradient method for finding saddle points and other
  problems.
\newblock \emph{Matecon}, 12:\penalty0 747--756, 1976.

\bibitem[Kovalev et~al.(2020)Kovalev, Salim, and
  Richt{\'a}rik]{kovalev2020optimal}
Kovalev, D., Salim, A., and Richt{\'a}rik, P.
\newblock Optimal and practical algorithms for smooth and strongly convex
  decentralized optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Lei et~al.(2017)Lei, Yen, Wu, Dhillon, and Ravikumar]{lei2017doubly}
Lei, Q., Yen, I. E.-H., Wu, C.-y., Dhillon, I.~S., and Ravikumar, P.
\newblock Doubly greedy primal-dual coordinate descent for sparse empirical
  risk minimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2034--2042. PMLR, 2017.

\bibitem[Li et~al.(2020)Li, Lin, and Fang]{li2020optimal}
Li, H., Lin, Z., and Fang, Y.
\newblock Optimal accelerated variance reduced extra and diging for strongly
  convex and smooth decentralized optimization.
\newblock \emph{arXiv e-prints}, pp.\  arXiv--2009, 2020.

\bibitem[Liang \& Stokes(2019)Liang and Stokes]{liang2019interaction}
Liang, T. and Stokes, J.
\newblock Interaction matters: A note on non-asymptotic local convergence of
  generative adversarial networks.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  907--915. PMLR, 2019.

\bibitem[Lin et~al.(2020)Lin, Jin, and Jordan]{lin2020near}
Lin, T., Jin, C., and Jordan, M.~I.
\newblock Near-optimal algorithms for minimax optimization.
\newblock In \emph{Conference on Learning Theory}, pp.\  2738--2779. PMLR,
  2020.

\bibitem[Mishchenko et~al.(2020)Mishchenko, Kovalev, Shulgin, Richt{\'a}rik,
  and Malitsky]{mishchenko2020revisiting}
Mishchenko, K., Kovalev, D., Shulgin, E., Richt{\'a}rik, P., and Malitsky, Y.
\newblock Revisiting stochastic extragradient.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  4573--4582. PMLR, 2020.

\bibitem[Mokhtari et~al.(2020)Mokhtari, Ozdaglar, and
  Pattathil]{mokhtari2020unified}
Mokhtari, A., Ozdaglar, A., and Pattathil, S.
\newblock A unified analysis of extra-gradient and optimistic gradient methods
  for saddle point problems: Proximal point approach.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1497--1507. PMLR, 2020.

\bibitem[Nedic et~al.(2017)Nedic, Olshevsky, and Shi]{nedic2017achieving}
Nedic, A., Olshevsky, A., and Shi, W.
\newblock Achieving geometric convergence for distributed optimization over
  time-varying graphs.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0
  2597--2633, 2017.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Nesterov, Y.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Nesterov \& Scrimali(2006)Nesterov and Scrimali]{nesterov2006solving}
Nesterov, Y. and Scrimali, L.
\newblock Solving strongly monotone variational and quasi-variational
  inequalities.
\newblock 2006.

\bibitem[Nesterov(1983)]{nesterov1983method}
Nesterov, Y.~E.
\newblock A method for solving the convex programming problem with convergence
  rate o (1/k\^{} 2).
\newblock In \emph{Dokl. akad. nauk Sssr}, volume 269, pp.\  543--547, 1983.

\bibitem[Peyr{\'e} et~al.(2019)Peyr{\'e}, Cuturi,
  et~al.]{peyre2019computational}
Peyr{\'e}, G., Cuturi, M., et~al.
\newblock Computational optimal transport: With applications to data science.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  11\penalty0 (5-6):\penalty0 355--607, 2019.

\bibitem[Salim et~al.(2021)Salim, Condat, Kovalev, and
  Richt{\'a}rik]{salim2021optimal}
Salim, A., Condat, L., Kovalev, D., and Richt{\'a}rik, P.
\newblock An optimal algorithm for strongly convex minimization under affine
  constraints.
\newblock \emph{arXiv preprint arXiv:2102.11079}, 2021.

\bibitem[Scaman et~al.(2017)Scaman, Bach, Bubeck, Lee, and
  Massouli{\'e}]{scaman2017optimal}
Scaman, K., Bach, F., Bubeck, S., Lee, Y.~T., and Massouli{\'e}, L.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock In \emph{international conference on machine learning}, pp.\
  3027--3036. PMLR, 2017.

\bibitem[Wang \& Xiao(2017)Wang and Xiao]{wang2017exploiting}
Wang, J. and Xiao, L.
\newblock Exploiting strong convexity from data with primal-dual first-order
  algorithms.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3694--3702. PMLR, 2017.

\bibitem[Wang \& Li(2020)Wang and Li]{wang2020improved}
Wang, Y. and Li, J.
\newblock Improved algorithms for convex-concave minimax optimization.
\newblock \emph{arXiv preprint arXiv:2006.06359}, 2020.

\bibitem[Xiao et~al.(2019)Xiao, Yu, Lin, and Chen]{xiao2019dscovr}
Xiao, L., Yu, A.~W., Lin, Q., and Chen, W.
\newblock Dscovr: Randomized primal-dual block coordinate algorithms for
  asynchronous distributed optimization.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 1634--1691, 2019.

\bibitem[Xie et~al.(2021)Xie, Han, and Zhang]{xie2021dippa}
Xie, G., Han, Y., and Zhang, Z.
\newblock Dippa: An improved method for bilinear saddle point problems.
\newblock \emph{arXiv preprint arXiv:2103.08270}, 2021.

\bibitem[Ye et~al.(2020)Ye, Luo, Zhou, and Zhang]{ye2020multi}
Ye, H., Luo, L., Zhou, Z., and Zhang, T.
\newblock Multi-consensus decentralized accelerated gradient descent.
\newblock \emph{arXiv preprint arXiv:2005.00797}, 2020.

\bibitem[Zargham et~al.(2013)Zargham, Ribeiro, Ozdaglar, and
  Jadbabaie]{zargham2013accelerated}
Zargham, M., Ribeiro, A., Ozdaglar, A., and Jadbabaie, A.
\newblock Accelerated dual descent for network flow optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 59\penalty0
  (4):\penalty0 905--920, 2013.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Wang, Lessard, and
  Grosse]{zhang2021don}
Zhang, G., Wang, Y., Lessard, L., and Grosse, R.
\newblock Don't fix what ain't broke: Near-optimal local convergence of
  alternating gradient descent-ascent for minimax optimization.
\newblock \emph{arXiv preprint arXiv:2102.09468}, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Hong, and
  Zhang]{zhang2021lower}
Zhang, J., Hong, M., and Zhang, S.
\newblock On lower iteration complexity bounds for the convex concave saddle
  point problems.
\newblock \emph{Mathematical Programming}, pp.\  1--35, 2021{\natexlab{b}}.

\bibitem[Zhang \& Lin(2015)Zhang and Lin]{zhang2015stochastic}
Zhang, Y. and Lin, X.
\newblock Stochastic primal-dual coordinate method for regularized empirical
  risk minimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  353--361. PMLR, 2015.

\end{thebibliography}
