\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2012)Agarwal, Bartlett, Ravikumar, and
  Wainwright]{Agarwal2012InformationTheoreticLB}
A.~Agarwal, P.~L. Bartlett, P.~Ravikumar, and M.~J. Wainwright.
\newblock Information-theoretic lower bounds on the oracle complexity of
  stochastic convex optimization.
\newblock \emph{IEEE Transactions on Information Theory}, 58\penalty0
  (5):\penalty0 3235--3249, 2012.

\bibitem[Arjevani et~al.(2019)Arjevani, Carmon, Duchi, Foster, Srebro, and
  Woodworth]{Arjevani2019LowerBF}
Y.~Arjevani, Y.~Carmon, J.~C. Duchi, D.~J. Foster, N.~Srebro, and B.~E.
  Woodworth.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock \emph{ArXiv e-prints}, arXiv:1912.02365, 2019.

\bibitem[Balasubramanian et~al.(2021)Balasubramanian, Ghadimi, and
  Nguyen]{balasubramanian2020stochastic}
K.~Balasubramanian, S.~Ghadimi, and A.~Nguyen.
\newblock Stochastic multi-level composition optimization algorithms with
  level-independent convergence rates.
\newblock \emph{ArXiv e-prints}, arXiv:2008.10526, 2021.

\bibitem[Chen et~al.(2021)Chen, Sun, and Yin]{chen2021solving}
T.~Chen, Y.~Sun, and W.~Yin.
\newblock Solving stochastic compositional optimization is nearly as easy as
  solving stochastic optimization.
\newblock \emph{IEEE Transactions on Signal Processing}, 69:\penalty0
  4937--4948, 2021.

\bibitem[Coates et~al.(2011)Coates, Ng, and Lee]{Coates2011STL10}
A.~Coates, A.~Ng, and H.~Lee.
\newblock An analysis of single-layer networks in unsupervised feature
  learning.
\newblock In \emph{Proceedings of the 14th International Conference on
  Artificial Intelligence and Statistics}, pages 213--223, 2011.

\bibitem[Cutkosky and Orabona(2019)]{cutkosky2019momentum}
A.~Cutkosky and F.~Orabona.
\newblock Momentum-based variance reduction in non-convex {SGD}.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pages
  15210--15219, 2019.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste{-}Julien]{DBLP:conf/nips/DefazioBL14}
A.~Defazio, F.~R. Bach, and S.~Lacoste{-}Julien.
\newblock {SAGA:} {A} fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems 27}, pages
  1646--1654, 2014.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{Fang2018SPIDERNN}
C.~Fang, C.~J. Li, Z.~Lin, and T.~Zhang.
\newblock Spider: Near-optimal non-convex optimization via stochastic path
  integrated differential estimator.
\newblock \emph{ArXiv e-prints}, arXiv:1807.01695, 2018.

\bibitem[Ghadimi et~al.(2020)Ghadimi, Ruszczynski, and Wang]{Ghadimi2020AST}
S.~Ghadimi, A.~Ruszczynski, and M.~Wang.
\newblock A single timescale stochastic approximation method for nested
  stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 30\penalty0 (1):\penalty0
  960--979, 2020.

\bibitem[Guo et~al.(2021)Guo, Xu, Yin, Jin, and Yang]{guo2022stochastic}
Z.~Guo, Y.~Xu, W.~Yin, R.~Jin, and T.~Yang.
\newblock On stochastic moving-average estimators for non-convex optimization.
\newblock \emph{ArXiv e-prints}, arXiv:2104.14840, 2021.

\bibitem[Hu et~al.(2020)Hu, Zhang, Chen, and He]{hu2020biased}
Y.~Hu, S.~Zhang, X.~Chen, and N.~He.
\newblock Biased stochastic first-order methods for conditional stochastic
  optimization and applications in meta learning.
\newblock In \emph{Advances in Neural Information Processing Systems 33}, 2020.

\bibitem[Jiang et~al.(2022)Jiang, Wang, Wang, Zhang, and
  Yang]{jiang2022optimal}
W.~Jiang, B.~Wang, Y.~Wang, L.~Zhang, and T.~Yang.
\newblock Optimal algorithms for stochastic multi-level compositional
  optimization.
\newblock \emph{ArXiv e-prints}, arXiv:2202.07530, 2022.

\bibitem[Johnson and Zhang(2013)]{NIPS2013_ac1dd209}
R.~Johnson and T.~Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems 26}, 2013.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{Karimi2016LinearCO}
H.~Karimi, J.~Nutini, and M.~Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the {P}olyak-{{\L}}ojasiewicz condition.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases},
  pages 795--811, 2016.

\bibitem[Krizhevsky(2009)]{Krizhevsky2009Cifar10}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Masters Thesis, Deptartment of Computer Science, University of
  Toronto}, 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{LeCun1998MNIST}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock In \emph{Proceedings of the IEEE}, pages 2278--2324, 1998.

\bibitem[Li et~al.(2021)Li, Bao, Zhang, and Richtarik]{pmlr-v139-li21a}
Z.~Li, H.~Bao, X.~Zhang, and P.~Richtarik.
\newblock Page: A simple and optimal probabilistic gradient estimator for
  nonconvex optimization.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, pages 6286--6295, 2021.

\bibitem[Liu et~al.(2018)Liu, Liu, Hsieh, and
  Tao]{DBLP:journals/corr/abs-1809-02505}
L.~Liu, J.~Liu, C.~Hsieh, and D.~Tao.
\newblock Stochastically controlled stochastic gradient for the convex and
  non-convex composition problem.
\newblock \emph{ArXiv e-prints}, arXiv:1809.02505, 2018.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{Netzer2011SVHN}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In \emph{Advances in Neural Information Processing Systems Workshop
  on Deep Learning and Unsupervised Feature Learning}, 2011.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{a}{{c}}]{arxiv.1703.00102}
L.~M. Nguyen, J.~Liu, K.~Scheinberg, and M.~Tak{a}{{c}}.
\newblock {SARAH:} {A} novel method for machine learning problems using
  stochastic recursive gradient.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, pages 2613--2621, 2017.

\bibitem[Qi et~al.(2021{\natexlab{a}})Qi, Guo, Xu, Jin, and Yang]{qi2021online}
Q.~Qi, Z.~Guo, Y.~Xu, R.~Jin, and T.~Yang.
\newblock An online method for a class of distributionally robust optimization
  with non-convex objectives.
\newblock \emph{ArXiv e-prints}, arXiv:2006.10138, 2021{\natexlab{a}}.

\bibitem[Qi et~al.(2021{\natexlab{b}})Qi, Luo, Xu, Ji, and
  Yang]{qi2021stochastic}
Q.~Qi, Y.~Luo, Z.~Xu, S.~Ji, and T.~Yang.
\newblock Stochastic optimization of areas under precision-recall curves with
  provable convergence.
\newblock In \emph{Advances in Neural Information Processing Systems 34}, pages
  1752--1765, 2021{\natexlab{b}}.

\bibitem[Roux et~al.(2012)Roux, Schmidt, and Bach]{DBLP:conf/nips/RouxSB12}
N.~L. Roux, M.~Schmidt, and F.~R. Bach.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock In \emph{Advances in Neural Information Processing Systems 25}, pages
  2672--2680, 2012.

\bibitem[Wang and Yang(2022)]{dependent2022}
B.~Wang and T.~Yang.
\newblock Finite-sum coupled compositional stochastic optimization: Theory and
  applications.
\newblock \emph{ArXiv e-prints}, arXiv:2202.12396, 2022.

\bibitem[Wang et~al.(2021)Wang, Yang, Zhang, and Yang]{wang2021momentum}
G.~Wang, M.~Yang, L.~Zhang, and T.~Yang.
\newblock Momentum accelerates the convergence of stochastic {AUPRC}
  maximization.
\newblock \emph{ArXiv e-prints}, arXiv:2107.01173, 2021.

\bibitem[Wang et~al.(2016)Wang, Liu, and Fang]{wang2016accelerating}
M.~Wang, J.~Liu, and E.~Fang.
\newblock Accelerating stochastic composition optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 29}, pages
  1714--1722, 2016.

\bibitem[Wang et~al.(2017)Wang, Fang, and Liu]{wang2017stochastic}
M.~Wang, E.~X. Fang, and H.~Liu.
\newblock Stochastic compositional gradient descent: algorithms for minimizing
  compositions of expected-value functions.
\newblock \emph{Mathematical Programming}, 161\penalty0 (1-2):\penalty0
  419--449, 2017.

\bibitem[Wang et~al.(2018)Wang, Ji, Zhou, Liang, and
  Tarokh]{Wang2018SpiderBoostAC}
Z.~Wang, K.~Ji, Y.~Zhou, Y.~Liang, and V.~Tarokh.
\newblock Spiderboost: A class of faster variance-reduced algorithms for
  nonconvex optimization.
\newblock \emph{ArXiv e-prints}, arXiv:1810.10690, 2018.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{Xiao2017Fashion-MNIST}
H.~Xiao, K.~Rasul, and R.~Vollgraf.
\newblock {Fashion-MNIST}: A novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{ArXiv e-prints}, arXiv:1708.07747, 2017.

\bibitem[Yang et~al.(2019)Yang, Wang, and Fang]{Yang2019MultilevelSG}
S.~Yang, M.~Wang, and E.~X. Fang.
\newblock Multilevel stochastic gradient methods for nested composition
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 29\penalty0 (1):\penalty0
  616--659, 2019.

\bibitem[Yuan et~al.(2019{\natexlab{a}})Yuan, Lian, Li, Liu, and
  Hu]{Yuan2019EfficientSN}
H.~Yuan, X.~Lian, C.~J. Li, J.~Liu, and W.~Hu.
\newblock Efficient smooth non-convex stochastic compositional optimization via
  stochastic recursive gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems 33}, pages
  14905--14916, 2019{\natexlab{a}}.

\bibitem[Yuan et~al.(2019{\natexlab{b}})Yuan, Yan, Jin, and
  Yang]{yuan2019stagewise}
Z.~Yuan, Y.~Yan, R.~Jin, and T.~Yang.
\newblock Stagewise training accelerates convergence of testing error over sgd.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2604--2614, 2019{\natexlab{b}}.

\bibitem[Yuan et~al.(2020)Yuan, Yan, Sonka, and Yang]{robustdeepAUC}
Z.~Yuan, Y.~Yan, M.~Sonka, and T.~Yang.
\newblock Robust deep auc maximization: A new surrogate loss and empirical
  studies on medical image classification.
\newblock \emph{ArXiv e-prints}, arXiv:2012.03173, 2020.

\bibitem[Zhang and Xiao(2019)]{Zhang2019ASC}
J.~Zhang and L.~Xiao.
\newblock A stochastic composite gradient method with incremental variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems 33}, pages
  9075--9085, 2019.

\bibitem[Zhang and Xiao(2021)]{Zhang2021MultiLevelCS}
J.~Zhang and L.~Xiao.
\newblock Multilevel composite stochastic optimization via nested variance
  reduction.
\newblock \emph{SIAM Journal on Optimization}, 31\penalty0 (2):\penalty0
  1131--1157, 2021.

\bibitem[Zhang et~al.(2013)Zhang, Mahdavi, and Jin]{NIPS:2013:Zhang}
L.~Zhang, M.~Mahdavi, and R.~Jin.
\newblock Linear convergence with condition number independent access of full
  gradients.
\newblock In \emph{Advance in Neural Information Processing Systems 26}, pages
  980--988, 2013.

\bibitem[Zhang and Lan(2021)]{Zhang2020OptimalAF}
Z.~Zhang and G.~Lan.
\newblock Optimal algorithms for convex nested stochastic composite
  optimization.
\newblock \emph{ArXiv e-prints}, arXiv:2011.10076, 2021.

\bibitem[Zhu et~al.(2022)Zhu, Wu, and Yang]{AUROC2022}
D.~Zhu, X.~Wu, and T.~Yang.
\newblock Benchmarking deep {AUROC} optimization: Loss functions and
  algorithmic choices.
\newblock \emph{ArXiv e-prints}, arXiv:2203.14177, 2022.

\end{thebibliography}
