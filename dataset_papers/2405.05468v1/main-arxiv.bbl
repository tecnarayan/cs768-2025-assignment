\begin{thebibliography}{}

\bibitem[Agarwal et~al., 2019]{agarwal2019reinforcement}
Agarwal, A., Jiang, N., Kakade, S.~M., and Sun, W. (2019).
\newblock Reinforcement learning: Theory and algorithms.
\newblock {\em CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep}.

\bibitem[Antos et~al., 2008]{antos2008learning}
Antos, A., Szepesv{\'a}ri, C., and Munos, R. (2008).
\newblock Learning near-optimal policies with {B}ellman-residual minimization based fitted policy iteration and a single sample path.
\newblock {\em Machine Learning}, 71(1):89--129.

\bibitem[Bertsimas et~al., 2018]{bertsimas2018dro}
Bertsimas, D., Gupta, V., and Kallus, N. (2018).
\newblock Data-driven robust optimization.
\newblock {\em Math. Program.}, 167(2):235–292.

\bibitem[Blanchet et~al., 2019]{blanchet2019dro}
Blanchet, J., Kang, Y., and Murthy, K. (2019).
\newblock Robust wasserstein profile inference and applications to machine learning.
\newblock {\em Journal of Applied Probability}, 56(3):830–857.

\bibitem[Blanchet et~al., 2023]{blanchet2023double}
Blanchet, J., Lu, M., Zhang, T., and Zhong, H. (2023).
\newblock Double pessimism is provably efficient for distributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage.
\newblock {\em Advances in Neural Information Processing Systems}, 36.

\bibitem[Botvinick et~al., 2019]{botvinick2019reinforcement}
Botvinick, M., Ritter, S., Wang, J.~X., Kurth-Nelson, Z., Blundell, C., and Hassabis, D. (2019).
\newblock Reinforcement learning, fast and slow.
\newblock {\em Trends in cognitive sciences}, 23(5):408--422.

\bibitem[Bruns-Smith and Zhou, 2023]{bruns2023robust}
Bruns-Smith, D. and Zhou, A. (2023).
\newblock Robust fitted-q-evaluation and iteration under sequentially exogenous unobserved confounders.
\newblock {\em arXiv preprint arXiv:2302.00662}.

\bibitem[Chen and Jiang, 2019]{chen2019information}
Chen, J. and Jiang, N. (2019).
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 1042--1051.

\bibitem[Chen et~al., 1996]{chen1996design}
Chen, J., Patton, R.~J., and Zhang, H.-Y. (1996).
\newblock Design of unknown input observers and robust fault detection filters.
\newblock {\em International Journal of control}, 63(1):85--105.

\bibitem[Chen et~al., 2020]{chen2020distributionally}
Chen, R., Paschalidis, I.~C., et~al. (2020).
\newblock Distributionally robust learning.
\newblock {\em Foundations and Trends{\textregistered} in Optimization}, 4(1-2):1--243.

\bibitem[Chen et~al., 2022]{chen2022finite}
Chen, Z., Khodadadian, S., and Maguluri, S.~T. (2022).
\newblock Finite-sample analysis of off-policy natural actor--critic with linear function approximation.
\newblock {\em IEEE Control Systems Letters}, 6:2611--2616.

\bibitem[Corporation, 2021]{isaac-sim2real-2021}
Corporation, N. (2021).
\newblock Closing the sim2real gap with nvidia isaac sim and nvidia isaac replicator.

\bibitem[Csisz{\'a}r, 1967]{csiszar1967information}
Csisz{\'a}r, I. (1967).
\newblock Information-type measures of difference of probability distributions and indirect observation.
\newblock {\em studia scientiarum Mathematicarum Hungarica}, 2:229--318.

\bibitem[Du et~al., 2021]{du2021bilinear}
Du, S., Kakade, S., Lee, J., Lovett, S., Mahajan, G., Sun, W., and Wang, R. (2021).
\newblock Bilinear classes: A structural framework for provable generalization in rl.
\newblock In {\em International Conference on Machine Learning}, pages 2826--2836.

\bibitem[Duchi and Namkoong, 2018]{duchi2018learning}
Duchi, J. and Namkoong, H. (2018).
\newblock Learning models with uniform performance via distributionally robust optimization.
\newblock {\em arXiv preprint arXiv:1810.08750}.

\bibitem[Farahmand et~al., 2010]{farahmand2010error}
Farahmand, A.-m., Szepesv{\'a}ri, C., and Munos, R. (2010).
\newblock Error propagation for approximate policy and value iteration.
\newblock {\em Advances in Neural Information Processing Systems}, 23.

\bibitem[Fawzi et~al., 2022]{fawzi2022discovering}
Fawzi, A., Balog, M., Huang, A., Hubert, T., Romera-Paredes, B., Barekatain, M., Novikov, A., R~Ruiz, F.~J., Schrittwieser, J., Swirszcz, G., et~al. (2022).
\newblock Discovering faster matrix multiplication algorithms with reinforcement learning.
\newblock {\em Nature}, 610(7930):47--53.

\bibitem[Foster et~al., 2022]{foster2022offline}
Foster, D.~J., Krishnamurthy, A., Simchi-Levi, D., and Xu, Y. (2022).
\newblock Offline reinforcement learning: Fundamental barriers for value function approximation.
\newblock {\em arXiv preprint arXiv:2111.10919}.

\bibitem[Fujimoto and Gu, 2021]{fujimoto2021minimalist}
Fujimoto, S. and Gu, S.~S. (2021).
\newblock A minimalist approach to offline reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 34:20132--20145.

\bibitem[Fujimoto et~al., 2019]{fujimoto2019off}
Fujimoto, S., Meger, D., and Precup, D. (2019).
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em International Conference on Machine Learning}, pages 2052--2062.

\bibitem[Gao and Kleywegt, 2022]{gao-2022-distributionally}
Gao, R. and Kleywegt, A. (2022).
\newblock Distributionally robust stochastic optimization with wasserstein distance.
\newblock {\em Mathematics of Operations Research}.

\bibitem[Huang et~al., 2023]{huang2023reinforcement}
Huang, A., Chen, J., and Jiang, N. (2023).
\newblock Reinforcement learning in low-rank mdps with density features.
\newblock In {\em International Conference on Machine Learning}, pages 13710--13752.

\bibitem[Iyengar, 2005]{iyengar2005robust}
Iyengar, G.~N. (2005).
\newblock Robust dynamic programming.
\newblock {\em Mathematics of Operations Research}, 30(2):257--280.

\bibitem[Jin et~al., 2021a]{jin2021bellman}
Jin, C., Liu, Q., and Miryoosefi, S. (2021a).
\newblock Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms.
\newblock {\em Advances in neural information processing systems}, 34:13406--13418.

\bibitem[Jin et~al., 2021b]{jin2021non}
Jin, J., Zhang, B., Wang, H., and Wang, L. (2021b).
\newblock Non-convex distributionally robust optimization: Non-asymptotic analysis.
\newblock {\em Advances in Neural Information Processing Systems}, 34:2771--2782.

\bibitem[Kostrikov et~al., 2021]{kostrikov2021offline}
Kostrikov, I., Fergus, R., Tompson, J., and Nachum, O. (2021).
\newblock Offline reinforcement learning with fisher divergence critic regularization.
\newblock In {\em International Conference on Machine Learning}, pages 5774--5783.

\bibitem[Kumar et~al., 2019]{kumar2019stabilizing}
Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S. (2019).
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 11784--11794.

\bibitem[Kumar et~al., 2020]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S. (2020).
\newblock Conservative q-learning for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 33:1179--1191.

\bibitem[Lange et~al., 2012]{lange2012batch}
Lange, S., Gabel, T., and Riedmiller, M. (2012).
\newblock Batch reinforcement learning.
\newblock In {\em Reinforcement learning}, pages 45--73. Springer.

\bibitem[Lattimore and Szepesv{\'a}ri, 2020]{lattimore2020bandit}
Lattimore, T. and Szepesv{\'a}ri, C. (2020).
\newblock {\em Bandit algorithms}.
\newblock Cambridge University Press.

\bibitem[Lesort et~al., 2020]{lesort2020continual}
Lesort, T., Lomonaco, V., Stoian, A., Maltoni, D., Filliat, D., and D{\'\i}az-Rodr{\'\i}guez, N. (2020).
\newblock Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges.
\newblock {\em Information fusion}, 58:52--68.

\bibitem[Levine et~al., 2020]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J. (2020).
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on open problems.
\newblock {\em arXiv preprint arXiv:2005.01643}.

\bibitem[Levy et~al., 2020]{levy2020large}
Levy, D., Carmon, Y., Duchi, J.~C., and Sidford, A. (2020).
\newblock Large-scale methods for distributionally robust optimization.
\newblock {\em Advances in Neural Information Processing Systems}, 33:8847--8860.

\bibitem[Liang et~al., 2023]{liang2023single}
Liang, Z., Ma, X., Blanchet, J., Zhang, J., and Zhou, Z. (2023).
\newblock Single-trajectory distributionally robust reinforcement learning.
\newblock {\em arXiv preprint arXiv:2301.11721}.

\bibitem[Liu et~al., 2020]{liu2020provably}
Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E. (2020).
\newblock Provably good batch off-policy reinforcement learning without great exploration.
\newblock In {\em Neural Information Processing Systems}.

\bibitem[Liu et~al., 2022]{liu2022distributionally}
Liu, Z., Bai, Q., Blanchet, J., Dong, P., Xu, W., Zhou, Z., and Zhou, Z. (2022).
\newblock Distributionally robust $ q $-learning.
\newblock In {\em International Conference on Machine Learning}, pages 13623--13643.

\bibitem[Mankowitz et~al., 2020]{Mankowitz2020Robust}
Mankowitz, D.~J., Levine, N., Jeong, R., Abdolmaleki, A., Springenberg, J.~T., Shi, Y., Kay, J., Hester, T., Mann, T., and Riedmiller, M. (2020).
\newblock Robust reinforcement learning for continuous control with model misspecification.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Mannor et~al., 2016]{mannor2016robust}
Mannor, S., Mebel, O., and Xu, H. (2016).
\newblock Robust mdps with k-rectangular uncertainty.
\newblock {\em Mathematics of Operations Research}, 41(4):1484--1509.

\bibitem[Maraun, 2016]{maraun2016bias}
Maraun, D. (2016).
\newblock Bias correcting climate change simulations-a critical review.
\newblock {\em Current Climate Change Reports}, 2:211--220.

\bibitem[Mirhoseini et~al., 2021]{mirhoseini2021graph}
Mirhoseini, A., Goldie, A., Yazgan, M., Jiang, J.~W., Songhori, E., Wang, S., Lee, Y.-J., Johnson, E., Pathak, O., Nazi, A., et~al. (2021).
\newblock A graph placement methodology for fast chip design.
\newblock {\em Nature}, 594(7862):207--212.

\bibitem[Munos, 2003]{munos2003error}
Munos, R. (2003).
\newblock Error bounds for approximate policy iteration.
\newblock In {\em ICML}, volume~3, pages 560--567.

\bibitem[Munos, 2007]{munos2007performance}
Munos, R. (2007).
\newblock Performance bounds in l\_p-norm for approximate value iteration.
\newblock {\em SIAM journal on control and optimization}, 46(2):541--561.

\bibitem[Munos and Szepesv{{\'a}}ri, 2008]{munos08a}
Munos, R. and Szepesv{{\'a}}ri, C. (2008).
\newblock Finite-time bounds for fitted value iteration.
\newblock {\em Journal of Machine Learning Research}, 9(27):815--857.

\bibitem[Namkoong and Duchi, 2016]{namkoong2016stochastic}
Namkoong, H. and Duchi, J.~C. (2016).
\newblock Stochastic gradient methods for distributionally robust optimization with f-divergences.
\newblock {\em Advances in neural information processing systems}, 29.

\bibitem[Nilim and El~Ghaoui, 2005]{nilim2005robust}
Nilim, A. and El~Ghaoui, L. (2005).
\newblock Robust control of {M}arkov decision processes with uncertain transition matrices.
\newblock {\em Operations Research}, 53(5):780--798.

\bibitem[Panaganti, 2023]{panaganti2023thesis}
Panaganti, K. (2023).
\newblock {\em Robust Reinforcement Learning: Theory and Algorithms}.
\newblock PhD thesis, Texas A\&M University.

\bibitem[Panaganti and Kalathil, 2021a]{panaganti2020robust}
Panaganti, K. and Kalathil, D. (2021a).
\newblock Robust reinforcement learning using least squares policy iteration with provable performance guarantees.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages 511--520.

\bibitem[Panaganti and Kalathil, 2021b]{panaganti2021sample}
Panaganti, K. and Kalathil, D. (2021b).
\newblock Sample complexity of model-based robust reinforcement learning.
\newblock In {\em 2021 60th IEEE Conference on Decision and Control (CDC)}, pages 2240--2245.

\bibitem[Panaganti and Kalathil, 2022]{panaganti22a}
Panaganti, K. and Kalathil, D. (2022).
\newblock Sample complexity of robust reinforcement learning with a generative model.
\newblock In {\em International Conference on Artificial Intelligence and Statistics (AISTATS)}, pages 9582--9602.

\bibitem[Panaganti et~al., 2022]{panaganti-rfqi}
Panaganti, K., Xu, Z., Kalathil, D., and Ghavamzadeh, M. (2022).
\newblock Robust reinforcement learning using offline data.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)}.

\bibitem[Panaganti et~al., 2023a]{panaganti2023bridging}
Panaganti, K., Xu, Z., Kalathil, D., and Ghavamzadeh, M. (2023a).
\newblock Bridging distributionally robust learning and offline rl: An approach to mitigate distribution shift and partial data coverage.
\newblock {\em arXiv preprint arXiv:2310.18434}.

\bibitem[Panaganti et~al., 2023b]{panaganti2023distributionally}
Panaganti, K., Xu, Z., Kalathil, D., and Ghavamzadeh, M. (2023b).
\newblock Distributionally robust behavioral cloning for robust imitation learning.
\newblock In {\em 2023 62nd IEEE Conference on Decision and Control (CDC)}, pages 1342--1347.

\bibitem[Pioch et~al., 2009]{pioch2009adversarial}
Pioch, N.~J., Melhuish, J., Seidel, A., Santos~Jr, E., Li, D., and Gorniak, M. (2009).
\newblock Adversarial intent modeling using embedded simulation and temporal bayesian knowledge bases.
\newblock In {\em Modeling and Simulation for Military Operations IV}, volume 7348, pages 115--126.

\bibitem[Robey et~al., 2020]{robey2020model}
Robey, A., Hassani, H., and Pappas, G.~J. (2020).
\newblock Model-based robust deep learning: Generalizing to natural, out-of-distribution data.
\newblock {\em arXiv preprint arXiv:2005.10247}.

\bibitem[Rockafellar and Wets, 2009]{rockafellar2009variational}
Rockafellar, R.~T. and Wets, R. J.-B. (2009).
\newblock {\em Variational analysis}, volume 317.
\newblock Springer Science \& Business Media.

\bibitem[Russel and Petrik, 2019]{russel2019beyond}
Russel, R.~H. and Petrik, M. (2019).
\newblock Beyond confidence regions: Tight bayesian ambiguity sets for robust mdps.
\newblock {\em Advances in Neural Information Processing Systems}.

\bibitem[Scherrer et~al., 2015]{scherrer2015approximate}
Scherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner, B., and Geist, M. (2015).
\newblock Approximate modified policy iteration and its application to the game of tetris.
\newblock {\em J. Mach. Learn. Res.}, 16(49):1629--1676.

\bibitem[Schmidt et~al., 2015]{schmidt2015depth}
Schmidt, T., Hertkorn, K., Newcombe, R., Marton, Z., Suppa, M., and Fox, D. (2015).
\newblock Depth-based tracking with physical constraints for robot manipulation.
\newblock In {\em 2015 IEEE International Conference on Robotics and Automation (ICRA)}, pages 119--126.

\bibitem[Schulman et~al., 2015]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015).
\newblock Trust region policy optimization.
\newblock In {\em International conference on machine learning}, pages 1889--1897.

\bibitem[Schulman et~al., 2017]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017).
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}.

\bibitem[Shah et~al., 2018]{shah2018airsim}
Shah, S., Dey, D., Lovett, C., and Kapoor, A. (2018).
\newblock Airsim: High-fidelity visual and physical simulation for autonomous vehicles.
\newblock In {\em Field and Service Robotics: Results of the 11th International Conference}, pages 621--635. Springer.

\bibitem[Shalev-Shwartz and Ben-David, 2014]{shalev2014understanding}
Shalev-Shwartz, S. and Ben-David, S. (2014).
\newblock {\em Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press.

\bibitem[Shapiro, 2017]{shapiro-2017-dro}
Shapiro, A. (2017).
\newblock Distributionally robust stochastic programming.
\newblock {\em SIAM Journal on Optimization}, 27(4):2258--2275.

\bibitem[Shi and Chi, 2022]{shi2022distributionally}
Shi, L. and Chi, Y. (2022).
\newblock Distributionally robust model-based offline reinforcement learning with near-optimal sample complexity.
\newblock {\em arXiv preprint arXiv:2208.05767}.

\bibitem[Shi et~al., 2023]{shi2023curious}
Shi, L., Li, G., Wei, Y., Chen, Y., Geist, M., and Chi, Y. (2023).
\newblock The curious price of distributional robustness in reinforcement learning with a generative model.
\newblock {\em Advances in Neural Information Processing Systems}, 36.

\bibitem[Silver et~al., 2018]{silver2018general}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al. (2018).
\newblock A general reinforcement learning algorithm that masters chess, shogi, and go through self-play.
\newblock {\em Science}, 362(6419):1140--1144.

\bibitem[Sinha et~al., 2017]{sinha2017certifiable}
Sinha, A., Namkoong, H., and Duchi, J.~C. (2017).
\newblock Certifiable distributional robustness with principled adversarial training. corr, abs/1710.10571.
\newblock {\em arXiv preprint arXiv:1710.10571}.

\bibitem[Song et~al., 2023]{song2023hybrid}
Song, Y., Zhou, Y., Sekhari, A., Bagnell, D., Krishnamurthy, A., and Sun, W. (2023).
\newblock Hybrid rl: Using both offline and online data can make rl efficient.
\newblock In {\em The Eleventh International Conference on Learning Representations}.

\bibitem[S{\"u}nderhauf et~al., 2018]{sunderhauf2018limits}
S{\"u}nderhauf, N., Brock, O., Scheirer, W., Hadsell, R., Fox, D., Leitner, J., Upcroft, B., Abbeel, P., Burgard, W., Milford, M., et~al. (2018).
\newblock The limits and potentials of deep learning for robotics.
\newblock {\em The International journal of robotics research}, 37(4-5):405--420.

\bibitem[Szepesv{\'a}ri and Munos, 2005]{szepesvari2005finite}
Szepesv{\'a}ri, C. and Munos, R. (2005).
\newblock Finite time bounds for sampling based fitted value iteration.
\newblock In {\em Proceedings of the 22nd international conference on Machine learning}, pages 880--887.

\bibitem[Van~Erven et~al., 2015]{van2015fast}
Van~Erven, T., Grunwald, P., Mehta, N.~A., Reid, M., Williamson, R., et~al. (2015).
\newblock Fast rates in statistical and online learning.
\newblock {\em JMLR}.

\bibitem[Vershynin, 2018]{vershynin2018high}
Vershynin, R. (2018).
\newblock {\em High-Dimensional Probability: An Introduction with Applications in Data Science}, volume~47.
\newblock Cambridge University press.

\bibitem[Wang et~al., 2021]{wang2021what}
Wang, R., Foster, D., and Kakade, S.~M. (2021).
\newblock What are the statistical limits of offline {\{}rl{\}} with linear function approximation?
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Wang et~al., 2023a]{wang2023finite}
Wang, S., Si, N., Blanchet, J., and Zhou, Z. (2023a).
\newblock A finite sample complexity bound for distributionally robust q-learning.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 3370--3398.

\bibitem[Wang et~al., 2023b]{wang2023sample}
Wang, S., Si, N., Blanchet, J., and Zhou, Z. (2023b).
\newblock Sample complexity of variance-reduced distributionally robust q-learning.
\newblock {\em arXiv preprint arXiv:2305.18420}.

\bibitem[Wang et~al., 2023c]{wang2023distributionally}
Wang, Y., Hu, Y., Xiong, J., and Zou, S. (2023c).
\newblock Achieving minimax optimal sample complexity of offline reinforcement learning: A dro-based approach.
\newblock {\em arXiv preprint arXiv:2305.13289v2}.

\bibitem[Wang and Zou, 2021]{wang2021online}
Wang, Y. and Zou, S. (2021).
\newblock Online robust reinforcement learning with model uncertainty.
\newblock {\em Advances in Neural Information Processing Systems}, 34:7193--7206.

\bibitem[Wang and Zou, 2022]{wang2022policy}
Wang, Y. and Zou, S. (2022).
\newblock Policy gradient method for robust reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 23484--23526.

\bibitem[Wiesemann et~al., 2013]{wiesemann2013robust}
Wiesemann, W., Kuhn, D., and Rustem, B. (2013).
\newblock Robust {M}arkov decision processes.
\newblock {\em Mathematics of Operations Research}, 38(1):153--183.

\bibitem[Xie et~al., 2021]{xie2021bellman}
Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A. (2021).
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 34.

\bibitem[Xu and Mannor, 2010]{xu2010distributionally}
Xu, H. and Mannor, S. (2010).
\newblock Distributionally robust {M}arkov decision processes.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 2505--2513.

\bibitem[Xu$^{*}$ et~al., 2023]{xu-panaganti-2023samplecomplexity}
Xu$^{*}$, Z., Panaganti$^{*}$, K., and Kalathil, D. (2023).
\newblock Improved sample complexity bounds for distributionally robust reinforcement learning.
\newblock In {\em Proceedings of The 25th International Conference on Artificial Intelligence and Statistics}. Conference on Artificial Intelligence and Statistics.

\bibitem[Yang et~al., 2021]{yang2021generalized}
Yang, J., Zhou, K., Li, Y., and Liu, Z. (2021).
\newblock Generalized out-of-distribution detection: A survey.
\newblock {\em arXiv preprint arXiv:2110.11334}.

\bibitem[Yang et~al., 2023]{yang2023avoiding}
Yang, W., Wang, H., Kozuno, T., Jordan, S.~M., and Zhang, Z. (2023).
\newblock Avoiding model estimation in robust markov decision processes with a generative model.
\newblock {\em arXiv preprint arXiv:2302.01248}.

\bibitem[Yu and Xu, 2015]{yu2015distributionally}
Yu, P. and Xu, H. (2015).
\newblock Distributionally robust counterpart in {M}arkov decision processes.
\newblock {\em IEEE Transactions on Automatic Control}, 61(9):2538--2543.

\bibitem[Zhang et~al., 2023]{zhang2023regularized}
Zhang, R., Hu, Y., and Li, N. (2023).
\newblock Regularized robust mdps and risk-sensitive mdps: Equivalence, policy gradient, and sample complexity.
\newblock {\em arXiv preprint arXiv:2306.11626}.

\bibitem[Zhou et~al., 2023]{zhou2023natural}
Zhou, R., Liu, T., Cheng, M., Kalathil, D., Kumar, P., and Tian, C. (2023).
\newblock Natural actor-critic for robust reinforcement learning with function approximation.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems}.

\end{thebibliography}
