@inproceedings{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  booktitle={NeurIPS},
  year={2022}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{zhou2022large,
  title={Large language models are human-level prompt engineers},
  author={Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  journal={arXiv preprint arXiv:2211.01910},
  year={2022}
}

@article{yu2024evaluating,
  title={Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models},
  author={Yu, Simon Chi Lok and He, Jie and Minervini, Pasquale and Pan, Jeff Z},
  journal={arXiv preprint arXiv:2405.15984},
  year={2024}
}

@inproceedings{valmeekam2023planning,
  title={On the planning abilities of large language models-a critical investigation},
  author={Valmeekam, Karthik and Marquez, Matthew and Sreedharan, Sarath and Kambhampati, Subbarao},
  booktitle={NeurIPS},
  year={2023}
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@article{xie2021explanation,
  title={An explanation of in-context learning as implicit bayesian inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2111.02080},
  year={2021}
}

@article{tian2023r,
  title={R $\^{} 3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context},
  author={Tian, Qingyuan and Zhu, Hanlun and Wang, Lei and Li, Yang and Lan, Yunshi},
  journal={arXiv preprint arXiv:2310.16535},
  year={2023}
}

@article{song2023large,
  title={Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT},
  author={Song, Xiaoshuai and He, Keqing and Wang, Pei and Dong, Guanting and Mou, Yutao and Wang, Jingang and Xian, Yunsen and Cai, Xunliang and Xu, Weiran},
  journal={arXiv preprint arXiv:2310.10176},
  year={2023}
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@inproceedings{qin2022representation,
  title={Representation learning for context-dependent decision-making},
  author={Qin, Yuzhen and Menara, Tommaso and Oymak, Samet and Ching, ShiNung and Pasqualetti, Fabio},
  booktitle={ACC},
  year={2022},
}

@article{abdelnabi2023llm,
  title={Llm-deliberation: Evaluating llms with interactive multi-agent negotiation games},
  author={Abdelnabi, Sahar and Gomaa, Amr and Sivaprasad, Sarath and Sch{\"o}nherr, Lea and Fritz, Mario},
  journal={arXiv preprint arXiv:2309.17234},
  year={2023}
}

@article{jin2021disease,
  title={What disease does this patient have? a large-scale open domain question answering dataset from medical exams},
  author={Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  journal={Applied Sciences},
  year={2021},
}

@inproceedings{lake2018generalization,
  title={Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks},
  author={Lake, Brenden and Baroni, Marco},
  booktitle={ICML},
  year={2018},
}

@article{robey2023smoothllm,
  title={Smoothllm: Defending large language models against jailbreaking attacks},
  author={Robey, Alexander and Wong, Eric and Hassani, Hamed and Pappas, George J},
  journal={arXiv preprint arXiv:2310.03684},
  year={2023}
}

@inproceedings{cohen2019certified,
  title={Certified adversarial robustness via randomized smoothing},
  author={Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
  booktitle={ICML},
  year={2019},
}

@article{chia2023contrastive,
  title={Contrastive Chain-of-Thought Prompting},
  author={Chia, Yew Ken and Chen, Guizhen and Tuan, Luu Anh and Poria, Soujanya and Bing, Lidong},
  journal={arXiv preprint arXiv:2311.09277},
  year={2023}
}

@article{wei2023larger,
  title={Larger language models do in-context learning differently},
  author={Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2303.03846},
  year={2023}
}

@article{huang2022selfimprove,
  title={Large language models can self-improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022}
}

@article{pan2023automatically,
  title={Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies},
  author={Pan, Liangming and Saxon, Michael and Xu, Wenda and Nathani, Deepak and Wang, Xinyi and Wang, William Yang},
  journal={arXiv preprint arXiv:2308.03188},
  year={2023}
}

@inproceedings{shi2023large,
  title={Large language models can be easily distracted by irrelevant context},
  author={Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed H and Sch{\"a}rli, Nathanael and Zhou, Denny},
  booktitle={ICML},
  year={2023},
}

@inproceedings{huang2023large,
  title={Large language models cannot self-correct reasoning yet},
  author={Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng, Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou, Denny},
  booktitle={ICLR},
  year={2024},
}

@article{freeman2023frontier,
  title={Frontier Language Models are not Robust to Adversarial Arithmetic, or" What do I need to say so you agree 2+ 2= 5?},
  author={Freeman, C Daniel and Culp, Laura and Parisi, Aaron and Bileschi, Maxwell L and Elsayed, Gamaleldin F and Rizkowsky, Alex and Simpson, Isabelle and Alemi, Alex and Nova, Azade and Adlam, Ben and others},
  journal={arXiv preprint arXiv:2311.07587},
  year={2023}
}

@article{zhang2023certified,
  title={Certified Robustness for Large Language Models with Self-Denoising},
  author={Zhang, Zhen and Zhang, Guanhua and Hou, Bairu and Fan, Wenqi and Li, Qing and Liu, Sijia and Zhang, Yang and Chang, Shiyu},
  journal={arXiv preprint arXiv:2307.07171},
  year={2023}
}

@inproceedings{tian2023r3,
  title={R3 Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context},
  author={Tian, Qingyuan and Zhu, Hanlun and Wang, Lei and Li, Yang and Lan, Yunshi},
  booktitle={EMNLP},
  year={2023}
}

@article{lei2023s,
  title={S $\hat{3}$ HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering},
  author={Lei, Fangyu and Li, Xiang and Wei, Yifan and He, Shizhu and Huang, Yiming and Zhao, Jun and Liu, Kang},
  journal={arXiv preprint arXiv:2305.11725},
  year={2023}
}

@article{zeng2023certified,
  title={Certified robustness to text adversarial attacks by randomized [mask]},
  author={Zeng, Jiehang and Xu, Jianhan and Zheng, Xiaoqing and Huang, Xuanjing},
  journal={Computational Linguistics},
  year={2023},
}

@article{xi2023self,
  title={Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement},
  author={Xi, Zhiheng and Jin, Senjie and Zhou, Yuhao and Zheng, Rui and Gao, Songyang and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2305.14497},
  year={2023}
}

@inproceedings{zheng2023noisy,
  title={Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis},
  author={Zheng, Hongyi and Saparov, Abulhair},
  booktitle={EMNLP},
  year={2023}
}

@article{wang2022towards,
  title={Towards understanding chain-of-thought prompting: An empirical study of what matters},
  author={Wang, Boshi and Min, Sewon and Deng, Xiang and Shen, Jiaming and Wu, You and Zettlemoyer, Luke and Sun, Huan},
  journal={arXiv preprint arXiv:2212.10001},
  year={2022}
}

@article{schaeffer2023invalid,
  title={Invalid Logic, Equivalent Gains: The Bizarreness of Reasoning in Language Model Prompting},
  author={Schaeffer, Rylan and Pistunova, Kateryna and Khanna, Samar and Consul, Sarthak and Koyejo, Sanmi},
  journal={arXiv preprint arXiv:2307.10573},
  year={2023}
}

@inproceedings{wang2023self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  booktitle={ICLR},
  year={2023}
}

@article{sinha2019clutrr,
  title={CLUTRR: A diagnostic benchmark for inductive reasoning from text},
  author={Sinha, Koustuv and Sodhani, Shagun and Dong, Jin and Pineau, Joelle and Hamilton, William L},
  journal={arXiv preprint arXiv:1908.06177},
  year={2019}
}

@article{wu2023reasoning,
  title={Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks},
  author={Wu, Zhaofeng and Qiu, Linlu and Ross, Alexis and Aky{\"u}rek, Ekin and Chen, Boyuan and Wang, Bailin and Kim, Najoung and Andreas, Jacob and Kim, Yoon},
  journal={arXiv preprint arXiv:2307.02477},
  year={2023}
}

@book{mayer1977thinking, 
title={Thinking and problem solving: An introduction to human cognition and learning.}, 
author={Mayer, Richard E}, 
year={1977}, 
publisher={Scott, Foresman} 
}

@article{choi2018quac, 
title={QuAC: Question answering in context}, 
author={Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke},
journal={arXiv preprint arXiv:1808.07036},
year={2018} 
}

@article{dong2022survey,
  title={A survey for in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}

@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={NeurIPS},
  year={2020}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@inproceedings{jia2017adversarial,
  title={Adversarial examples for evaluating reading comprehension systems},
  author={Jia, Robin and Liang, Percy},
  booktitle={EMNLP},
  year={2017}
}

@inproceedings{pandia2021sorting,
  title={Sorting through the noise: Testing robustness of information processing in pre-trained language models},
  author={Pandia, Lalchand and Ettinger, Allyson},
  booktitle={EMNLP},
  year={2021}
}

@article{yang2023large,
  title={Large language models as optimizers},
  author={Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V and Zhou, Denny and Chen, Xinyun},
  journal={arXiv preprint arXiv:2309.03409},
  year={2023}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  year={2023}
}

@article{fu2022complexity,
  title={Complexity-based prompting for multi-step reasoning},
  author={Fu, Yao and Peng, Hao and Sabharwal, Ashish and Clark, Peter and Khot, Tushar},
  journal={arXiv preprint arXiv:2210.00720},
  year={2022}
}

@article{li2022advance,
  title={On the advance of making language models better reasoners},
  author={Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2206.02336},
  year={2022}
}

@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{floridi2020gpt,
  title={GPT-3: Its nature, scope, limits, and consequences},
  author={Floridi, Luciano and Chiriatti, Massimo},
  journal={Minds and Machines},
  year={2020},
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{wan2023better,
  title={Better zero-shot reasoning with self-adaptive prompting},
  author={Wan, Xingchen and Sun, Ruoxi and Dai, Hanjun and Arik, Sercan O and Pfister, Tomas},
  journal={arXiv preprint arXiv:2305.14106},
  year={2023}
}

@article{tyen2023llms,
  title={LLMs cannot find reasoning errors, but can correct them!},
  author={Tyen, Gladys and Mansoor, Hassan and Chen, Peter and Mak, Tony and C{\u{a}}rbune, Victor},
  journal={arXiv preprint arXiv:2311.08516},
  year={2023}
}

@article{stechly2023gpt,
  title={GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems},
  author={Stechly, Kaya and Marquez, Matthew and Kambhampati, Subbarao},
  journal={arXiv preprint arXiv:2310.12397},
  year={2023}
}

@article{zhao2023survey,
title={A survey of large language models},
author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
journal={arXiv preprint arXiv:2303.18223},
year={2023}
}

@inproceedings{he2020momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={ICML},
  year={2020},
}

@inproceedings{khosla2020supervised,
  title={Supervised contrastive learning},
  author={Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  booktitle={NeurIPS},
  year={2020}
}

@article{madaan2023self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2303.17651},
  year={2023}
}

@inproceedings{shinn2023reflexion,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik R and Yao, Shunyu},
  booktitle={NeurIPS},
  year={2023}
}

@article{zhao2023explainability,
  title={Explainability for large language models: A survey},
  author={Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
  journal={ACM Transactions on Intelligent Systems and Technology},
  year={2023},
}

@article{kim2023language,
  title={Language models can solve computer tasks},
  author={Kim, Geunwoo and Baldi, Pierre and McAleer, Stephen},
  journal={arXiv preprint arXiv:2303.17491},
  year={2023}
}

@article{welleck2022generating,
  title={Generating sequences by learning to self-correct},
  author={Welleck, Sean and Lu, Ximing and West, Peter and Brahman, Faeze and Shen, Tianxiao and Khashabi, Daniel and Choi, Yejin},
  journal={arXiv preprint arXiv:2211.00053},
  year={2022}
}

@article{jiang2023selfevolve,
  title={SelfEvolve: A Code Evolution Framework via Large Language Models},
  author={Jiang, Shuyang and Wang, Yuhao and Wang, Yu},
  journal={arXiv preprint arXiv:2306.02907},
  year={2023}
}

@article{ye2023selfee,
  title={Selfee: Iterative self-revising llm empowered by self-feedback generation},
  author={Ye, Seonghyeon and Jo, Yongrae and Kim, Doyoung and Kim, Sungdong and Hwang, Hyeonbin and Seo, Minjoon},
  journal={Blog post, May},
  year={2023}
}

@article{akyurek2023rl4f,
  title={RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs},
  author={Aky{\"u}rek, Afra Feyza and Aky{\"u}rek, Ekin and Madaan, Aman and Kalyan, Ashwin and Clark, Peter and Wijaya, Derry and Tandon, Niket},
  journal={arXiv preprint arXiv:2305.08844},
  year={2023}
}


@article{scheurer2023training,
  title={Training language models with language feedback at scale},
  author={Scheurer, J{\'e}r{\'e}my and Campos, Jon Ander and Korbak, Tomasz and Chan, Jun Shern and Chen, Angelica and Cho, Kyunghyun and Perez, Ethan},
  journal={arXiv preprint arXiv:2303.16755},
  year={2023}
}

@article{li2023self,
  title={Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models},
  author={Li, Miaoran and Peng, Baolin and Zhang, Zhu},
  journal={arXiv preprint arXiv:2305.14623},
  year={2023}
}


@article{yu2023improving,
  title={Improving Language Models via Plug-and-Play Retrieval Feedback},
  author={Yu, Wenhao and Zhang, Zhihan and Liang, Zhenwen and Jiang, Meng and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2305.14002},
  year={2023}
}

@article{chen2022codet,
  title={Codet: Code generation with generated tests},
  author={Chen, Bei and Zhang, Fengji and Nguyen, Anh and Zan, Daoguang and Lin, Zeqi and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2207.10397},
  year={2022}
}

@article{paul2023refiner,
  title={Refiner: Reasoning feedback on intermediate representations},
  author={Paul, Debjit and Ismayilzada, Mete and Peyrard, Maxime and Borges, Beatriz and Bosselut, Antoine and West, Robert and Faltings, Boi},
  journal={arXiv preprint arXiv:2304.01904},
  year={2023}
}

@article{gero2023self,
  title={Self-Verification Improves Few-Shot Clinical Information Extraction},
  author={Gero, Zelalem and Singh, Chandan and Cheng, Hao and Naumann, Tristan and Galley, Michel and Gao, Jianfeng and Poon, Hoifung},
  journal={arXiv preprint arXiv:2306.00024},
  year={2023}
}

@article{chandler1991cognitive,
  title={Cognitive load theory and the format of instruction},
  author={Chandler, Paul and Sweller, John},
  journal={Cognition and instruction},
  year={1991},
}

@article{koo2023uncovering,
  title={Uncovering the Risks and Drawbacks Associated with the Use of Synthetic Data for Grammatical Error Correction},
  author={Koo, Seonmin and Park, Chanjun and Lee, Seolhwa and Seo, Jaehyung and Eo, Sugyeong and Moon, Hyeonseok and Lim, Heuiseok},
  journal={IEEE Access},
  year={2023},
}

@inproceedings{sambasivan2021everyone,
  title={“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI},
  author={Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora M},
  booktitle={CHI},
  year={2021}
}

@inproceedings{zhang2023automatic,
  title={Automatic Chain of Thought Prompting in Large Language Models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  booktitle={ICLR},
  year={2023}
}

@article{zhang2023hallucination,
      title={Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models}, 
      author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and Wang, Longyue and Luu, Anh Tuan and Bi, Wei and Shi, Freda and Shi, Shuming},
      journal={arXiv preprint arXiv:2309.01219},
      year={2023}
}

@article{huang2023survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={arXiv preprint arXiv:2311.05232},
  year={2023}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  year={2019}
}

@inproceedings{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  booktitle={NeurIPS},
  year={2022}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@inproceedings{min2022rethinking,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  booktitle={EMNLP},
  year={2022}
}

@inproceedings{zheng2024judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle={ICML},
  year={2021},
}

@article{bang2023multitask,
  title={A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity},
  author={Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and others},
  journal={arXiv preprint arXiv:2302.04023},
  year={2023}
}

@inproceedings{perez2021true,
  title={True few-shot learning with language models},
  author={Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun},
  booktitle={NeurIPS},
  year={2021}
}

@article{lu2021fantastically,
  title={Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity},
  author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  journal={arXiv preprint arXiv:2104.08786},
  year={2021}
}

@article{zhang2022active,
  title={Active example selection for in-context learning},
  author={Zhang, Yiming and Feng, Shi and Tan, Chenhao},
  journal={arXiv preprint arXiv:2211.04486},
  year={2022}
}


@article{mishra2021reframing,
  title={Reframing Instructional Prompts to GPTk's Language},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Choi, Yejin and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2109.07830},
  year={2021}
}

@article{gan2023sensitivity,
  title={Sensitivity and Robustness of Large Language Models to Prompt in Japanese},
  author={Gan, Chengguang and Mori, Tatsunori},
  journal={arXiv preprint arXiv:2305.08714},
  year={2023}
}

@inproceedings{ye2022unreliability,
  title={The unreliability of explanations in few-shot prompting for textual reasoning},
  author={Ye, Xi and Durrett, Greg},
  booktitle={NeurIPS},
  year={2022}
}

@article{ye2022complementary,
  title={Complementary explanations for effective in-context learning},
  author={Ye, Xi and Iyer, Srinivasan and Celikyilmaz, Asli and Stoyanov, Ves and Durrett, Greg and Pasunuru, Ramakanth},
  journal={arXiv preprint arXiv:2211.13892},
  year={2022}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@inproceedings{schaeffer2023emergent,
  title={Are emergent abilities of Large Language Models a mirage?},
  author={Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  booktitle={NeurIPS},
  year={2023}
}

@book{devore1993constructive,
  title={Constructive approximation},
  author={DeVore, Ronald A and Lorentz, George G},
  year={1993},
}

@article{kleijn2012bernstein,
  title={The Bernstein-Von-Mises theorem under misspecification},
  author={Kleijn, BJK and van der Vaart, AW},
  journal={Electronic Journal of Statistics},
  year={2012}
}

@article{saparov2022language,
  title={Language models are greedy reasoners: A systematic formal analysis of chain-of-thought},
  author={Saparov, Abulhair and He, He},
  journal={arXiv preprint arXiv:2210.01240},
  year={2022}
}

@inproceedings{zelikman2022star,
  title={Star: Bootstrapping reasoning with reasoning},
  author={Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
  booktitle={NeurIPS},
  year={2022}
}

@article{koh2022inversion,
  title={The inversion of majority/minority at the de/reterritorialised urban higher education enclave: Xiamen University Malaysia},
  author={Koh, Sin Yee},
  journal={Urban Studies},
  year={2022},
}

@article{thorstad2023cognitive,
  title={Cognitive bias in large language models: Cautious optimism meets anti-Panglossian meliorism},
  author={Thorstad, David},
  journal={arXiv preprint arXiv:2311.10932},
  year={2023}
}

@inproceedings{jones2022capturing,
  title={Capturing failures of large language models via human cognitive biases},
  author={Jones, Erik and Steinhardt, Jacob},
  booktitle={NeurIPS},
  year={2022}
}

@article{zhu2024large,
  title={Can Large Language Models Understand Context?}, 
  author={Yilun Zhu and Joel Ruben Antony Moniz and Shruti Bhargava and Jiarui Lu and Dhivya Piraviperumal and Site Li and Yuan Zhang and Hong Yu and Bo-Hsiang Tseng},
  journal={arXiv preprint arXiv:2402.00858},
  year={2024}
}

@inproceedings{he2024can,
  title={Can Large Language Models Understand Real-World Complex Instructions?},
  author={He, Qianyu and Zeng, Jie and Huang, Wenhao and Chen, Lina and Xiao, Jin and He, Qianxi and Zhou, Xunzhe and Liang, Jiaqing and Xiao, Yanghua},
  booktitle={AAAI},
  year={2024}
}

@article{yan2024large,
  title={Do Large Language Models Understand Logic or Just Mimick Context?},
  author={Yan, Junbing and Wang, Chengyu and Huang, Jun and Zhang, Wei},
  journal={arXiv preprint arXiv:2402.12091},
  year={2024}
}

@article{gao2023ambiguity,
  title={Ambiguity-aware in-context learning with large language models},
  author={Gao, Lingyu and Chaudhary, Aditi and Srinivasan, Krishna and Hashimoto, Kazuma and Raman, Karthik and Bendersky, Michael},
  journal={arXiv preprint arXiv:2309.07900},
  year={2023}
}

@article{liu2023we,
  title={We're afraid language models aren't modeling ambiguity},
  author={Liu, Alisa and Wu, Zhaofeng and Michael, Julian and Suhr, Alane and West, Peter and Koller, Alexander and Swayamdipta, Swabha and Smith, Noah A and Choi, Yejin},
  journal={arXiv preprint arXiv:2304.14399},
  year={2023}
}

@article{agrawal2023language,
  title={Do Language Models Know When They're Hallucinating References?},
  author={Agrawal, Ayush and Mackey, Lester and Kalai, Adam Tauman},
  journal={arXiv preprint arXiv:2305.18248},
  year={2023}
}


@article{azaria2023internal,
  title={The internal state of an llm knows when its lying},
  author={Azaria, Amos and Mitchell, Tom},
  journal={arXiv preprint arXiv:2304.13734},
  year={2023}
}

@article{zhang2023mitigating,
  title={Mitigating language model hallucination with interactive question-knowledge alignment},
  author={Zhang, Shuo and Pan, Liangming and Zhao, Junzhou and Wang, William Yang},
  journal={arXiv preprint arXiv:2305.13669},
  year={2023}
}

@article{li2023metaagents,
  title={Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents},
  author={Li, Yuan and Zhang, Yixuan and Sun, Lichao},
  journal={arXiv preprint arXiv:2310.06500},
  year={2023}
}

@article{stacey2024lucid,
  title={LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues},
  author={Stacey, Joe and Cheng, Jianpeng and Torr, John and Guigue, Tristan and Driesen, Joris and Coca, Alexandru and Gaynor, Mark and Johannsen, Anders},
  journal={arXiv preprint arXiv:2403.00462},
  year={2024}
}

@article{bernadine2013breaking,
  title={Breaking the Camel's Back: Can Cognitive Overload be Quantified in the Human Brain?},
  author={Bernadine, Cocks and Nanda, Nandagopal and Vijayalakshmi, Rb and Thilaga, Mb and Naga, Dasari and Nabaraj, Dahal},
  journal={Procedia-Social and Behavioral Sciences},
  year={2013},
}

@book{rutkowski2018emotional,
  title={Emotional and cognitive overload: the dark side of information technology},
  author={Rutkowski, Anne-Fran{\c{c}}oise and Saunders, Carol},
  year={2018},
  publisher={Routledge}
}

@article{morewedge2010associative,
  title={Associative processes in intuitive judgment},
  author={Morewedge, Carey K and Kahneman, Daniel},
  journal={Trends in cognitive sciences},
  year={2010},
}
@article{wimmer2012preference,
  title={Preference by association: how memory mechanisms in the hippocampus bias decisions},
  author={Wimmer, G Elliott and Shohamy, Daphna},
  journal={Science},
  year={2012},
}

@article{havrilla2024understanding,
  title={Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought},
  author={Havrilla, Alex and Iyer, Maia},
  journal={arXiv preprint arXiv:2402.04004},
  year={2024}
}
@article{griffiths2020understanding,
  title={Understanding human intelligence through human limitations},
  author={Griffiths, Thomas L},
  journal={Trends in Cognitive Sciences},
  year={2020},
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  year={2016},
}

@article{rokicki2014diameter,
  title={The diameter of the rubik's cube group is twenty},
  author={Rokicki, Tomas and Kociemba, Herbert and Davidson, Morley and Dethridge, John},
  journal={SIAM REVIEW},
  year={2014},
}

@article{tversky1974judgment,
  title={Judgment under Uncertainty: Heuristics and Biases: Biases in judgments reveal some heuristics of thinking under uncertainty.},
  author={Tversky, Amos and Kahneman, Daniel},
  journal={Science},
  year={1974},
}

@article{del2017modeling,
  title={Modeling confirmation bias and polarization},
  author={Del Vicario, Michela and Scala, Antonio and Caldarelli, Guido and Stanley, H Eugene and Quattrociocchi, Walter},
  journal={Scientific reports},
  year={2017},
}

@article{janis2008groupthink,
  title={Groupthink},
  author={Janis, Irving L},
  journal={IEEE Engineering Management Review},
  year={2008},
}

@book{norris2000emotional,
  title={Emotional reasoning},
  author={Norris, Paul},
  year={2000},
  publisher={University of Massachusetts Amherst}
}

@article{shaikh2022second,
  title={On second thought, let's not think step by step! Bias and toxicity in zero-shot reasoning},
  author={Shaikh, Omar and Zhang, Hongxin and Held, William and Bernstein, Michael and Yang, Diyi},
  journal={arXiv preprint arXiv:2212.08061},
  year={2022}
}

@inproceedings{garg2022can,
  title={What can transformers learn in-context? a case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  booktitle={NeurIPS},
  year={2022}
}

@article{gebru2021datasheets,
  title={Datasheets for datasets},
  author={Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Iii, Hal Daum{\'e} and Crawford, Kate},
  journal={Communications of the ACM},
  year={2021},
}

@inproceedings{liu2021probabilistic,
  title={Probabilistic margins for instance reweighting in adversarial training},
  author={Liu, Feng and Han, Bo and Liu, Tongliang and Gong, Chen and Niu, Gang and Zhou, Mingyuan and Sugiyama, Masashi and others},
  booktitle={NeurIPS},
  year={2021}
}

@article{gong2021instance,
  title={Instance-dependent positive and unlabeled learning with labeling bias estimation},
  author={Gong, Chen and Wang, Qizhou and Liu, Tongliang and Han, Bo and You, Jane and Yang, Jian and Tao, Dacheng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
}

@inproceedings{chen2022ciga,
  title       = {Learning Causally Invariant Representations for Out-of-Distribution Generalization on Graphs},
  author      = {Yongqiang Chen and Yonggang Zhang and Yatao Bian and Han Yang and Kaili Ma and Binghui Xie and Tongliang Liu and Bo Han and James Cheng},
  booktitle={NeurIPS},
  year        = {2022}
}

@inproceedings{
    chen2023understanding,
    title={Understanding and Improving Feature Learning for Out-of-Distribution Generalization},
    author={Yongqiang Chen and Wei Huang and Kaiwen Zhou and Yatao Bian and Bo Han and James Cheng},
    booktitle={NeurIPS},
    year={2023},
}

@article{liang2023knowledge,
  title={Knowledge graph contrastive learning based on relation-symmetrical structure},
  author={Liang, Ke and Liu, Yue and Zhou, Sihang and Tu, Wenxuan and Wen, Yi and Yang, Xihong and Dong, Xiangjun and Liu, Xinwang},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2023},
}

@article{liang2024survey,
  title={A survey of knowledge graph reasoning on graph types: Static, dynamic, and multi-modal},
  author={Liang, Ke and Meng, Lingyuan and Liu, Meng and Liu, Yue and Tu, Wenxuan and Wang, Siwei and Zhou, Sihang and Liu, Xinwang and Sun, Fuchun and He, Kunlun},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024}
}

@inproceedings{zhou2024less,
    title       = {Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge Graphs},
    author      = {Zhanke Zhou and Yongqi Zhang and Jiangchao Yao and Quanming Yao and Bo Han},
    booktitle   = {ICLR},
    year        = {2024}
}

@article{li2023deepinception,
  title={Deepinception: Hypnotize large language model to be jailbreaker},
  author={Li, Xuan and Zhou, Zhanke and Zhu, Jianing and Yao, Jiangchao and Liu, Tongliang and Han, Bo},
  journal={arXiv preprint arXiv:2311.03191},
  year={2023}
}


@inproceedings{zhou2023mcgra,
  title       = {On Strengthening and Defending Graph Reconstruction Attack with Markov Chain Approximation},
  author      = {Zhanke Zhou and Chenyu Zhou and Xuan Li and Jiangchao Yao and Quanming Yao and Bo Han},
  booktitle   = {ICML},
  year        = {2023}
}

@inproceedings{zhou2023combating,
title={Combating Bilateral Edge Noise for Robust Link Prediction},
author={Zhanke Zhou and Jiangchao Yao and Jiaxu Liu and Xiawei Guo and Quanming Yao and Li He and Liang Wang and Bo Zheng and Bo Han},
booktitle={NeurIPS},
year={2023},
}

@inproceedings{zhang2023adaprop,
  title={AdaProp: Learning Adaptive Propagation for Graph Neural Network based Knowledge Graph Reasoning},
  author={Zhang, Yongqi and Zhou, Zhanke and Yao, Quanming and Chu, Xiaowen and Han, Bo},
  booktitle={SIGKDD},
  year={2023}
}

@article{tang2024fusionllm,
      title={FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs with Adaptive Compression}, 
      author={Zhenheng Tang and Xueze Kang and Yiming Yin and Xinglin Pan and Yuxin Wang and Xin He and Qiang Wang and Rongfei Zeng and Kaiyong Zhao and Shaohuai Shi and Amelie Chi Zhou and Bo Li and Bingsheng He and Xiaowen Chu},
journal={arXiv preprint arXiv:2410.12707},
year={2024}
}

@article{tang2023fusionai,
  title={FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs},
  author={Tang, Zhenheng and Wang, Yuxin and He, Xin and Zhang, Longteng and Pan, Xinglin and Wang, Qiang and Zeng, Rongfei and Zhao, Kaiyong and Shi, Shaohuai and He, Bingsheng and others},
  journal={arXiv preprint arXiv:2309.01172},
  year={2023}
}

@inproceedings{han2018co,
  title={Co-teaching: Robust training of deep neural networks with extremely noisy labels},
  author={Han, Bo and Yao, Quanming and Yu, Xingrui and Niu, Gang and Xu, Miao and Hu, Weihua and Tsang, Ivor and Sugiyama, Masashi},
  booktitle={NeurIPS},
  year={2018}
}