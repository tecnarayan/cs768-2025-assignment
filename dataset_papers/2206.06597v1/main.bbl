\begin{thebibliography}{71}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arbelaez et~al.(2010)Arbelaez, Maire, Fowlkes, and
  Malik]{arbelaez2010contour}
Arbelaez, P., Maire, M., Fowlkes, C., and Malik, J.
\newblock Contour detection and hierarchical image segmentation.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 33\penalty0 (5):\penalty0 898--916, 2010.

\bibitem[Bachmayr et~al.(2016)Bachmayr, Schneider, and
  Uschmajew]{bachmayr2016tensor}
Bachmayr, M., Schneider, R., and Uschmajew, A.
\newblock Tensor networks and hierarchical tensors for the solution of
  high-dimensional partial differential equations.
\newblock \emph{Foundations of Computational Mathematics}, 16\penalty0
  (6):\penalty0 1423--1472, 2016.

\bibitem[Back(1996)]{back1996evolutionary}
Back, T.
\newblock \emph{Evolutionary algorithms in theory and practice: evolution
  strategies, evolutionary programming, genetic algorithms}.
\newblock Oxford university press, 1996.

\bibitem[Batselier(2018)]{batselier2018trouble}
Batselier, K.
\newblock The trouble with tensor ring decompositions.
\newblock \emph{arXiv preprint arXiv:1811.03813}, 2018.

\bibitem[Bean(1994)]{bean1994genetic}
Bean, J.~C.
\newblock Genetic algorithms and random keys for sequencing and optimization.
\newblock \emph{ORSA journal on computing}, 6\penalty0 (2):\penalty0 154--160,
  1994.

\bibitem[Bengua et~al.(2017)Bengua, Phien, Tuan, and Do]{bengua2017efficient}
Bengua, J.~A., Phien, H.~N., Tuan, H.~D., and Do, M.~N.
\newblock Efficient tensor completion for color image and video recovery:
  Low-rank tensor train.
\newblock \emph{IEEE Transactions on Image Processing}, 26\penalty0
  (5):\penalty0 2466--2479, 2017.

\bibitem[Cai \& Li(2021)Cai and Li]{cai2021blind}
Cai, Y. and Li, P.
\newblock A blind block term decomposition of high order tensors.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  6868--6876, 2021.

\bibitem[Chang et~al.(1995)Chang, Gasarch, and Toran]{chang1995finding}
Chang, R., Gasarch, W., and Toran, J.
\newblock On finding the number of graph automorphisms.
\newblock In \emph{Proceedings of Structure in Complexity Theory. Tenth Annual
  IEEE Conference}, pp.\  288--298. IEEE, 1995.

\bibitem[Cheng et~al.(2020)Cheng, Li, Fan, and Bao]{cheng2020novel}
Cheng, Z., Li, B., Fan, Y., and Bao, Y.
\newblock A novel rank selection scheme in tensor ring decomposition based on
  reinforcement learning for deep neural networks.
\newblock In \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp.\  3292--3296. IEEE, 2020.

\bibitem[Cichocki et~al.(2016)Cichocki, Lee, Oseledets, Phan, Zhao, Mandic,
  et~al.]{cichocki2016tensor}
Cichocki, A., Lee, N., Oseledets, I., Phan, A.-H., Zhao, Q., Mandic, D.~P.,
  et~al.
\newblock Tensor networks for dimensionality reduction and large-scale
  optimization: Part 1 low-rank tensor decompositions.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  9\penalty0 (4-5):\penalty0 249--429, 2016.

\bibitem[Cincio et~al.(2008)Cincio, Dziarmaga, and Rams]{cincio2008multiscale}
Cincio, L., Dziarmaga, J., and Rams, M.~M.
\newblock Multiscale entanglement renormalization ansatz in two dimensions:
  quantum ising model.
\newblock \emph{Physical Review Letters}, 100\penalty0 (24):\penalty0 240603,
  2008.

\bibitem[Cohen et~al.(2016)Cohen, Sharir, and Shashua]{cohen2016expressive}
Cohen, N., Sharir, O., and Shashua, A.
\newblock On the expressive power of deep learning: A tensor analysis.
\newblock In \emph{Conference on learning theory}, pp.\  698--728. PMLR, 2016.

\bibitem[Dua \& Graff(2017)Dua and Graff]{Dua2019}
Dua, D. and Graff, C.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Falc{\'o} et~al.(2020)Falc{\'o}, Nouy, et~al.]{falco2020geometry}
Falc{\'o}, A., Nouy, W.~H., et~al.
\newblock Geometry of tree-based tensor formats in tensor banach spaces.
\newblock \emph{arXiv preprint arXiv:2011.08466}, 2020.

\bibitem[Flake \& Lawrence(2002)Flake and Lawrence]{flake2002efficient}
Flake, G.~W. and Lawrence, S.
\newblock Efficient {SVM} regression training with {SMO}.
\newblock \emph{Machine Learning}, 46\penalty0 (1):\penalty0 271--290, 2002.

\bibitem[Glasser et~al.(2019)Glasser, Sweke, Pancotti, Eisert, and
  Cirac]{glasser2019expressive}
Glasser, I., Sweke, R., Pancotti, N., Eisert, J., and Cirac, I.
\newblock Expressive power of tensor-network factorizations for probabilistic
  modeling.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Goldwasser et~al.(1989)Goldwasser, Micali, and
  Rackoff]{goldwasser1989knowledge}
Goldwasser, S., Micali, S., and Rackoff, C.
\newblock The knowledge complexity of interactive proof systems.
\newblock \emph{SIAM Journal on computing}, 18\penalty0 (1):\penalty0 186--208,
  1989.

\bibitem[Golovin et~al.(2019)Golovin, Karro, Kochanski, Lee, Song, and
  Zhang]{golovin2019gradientless}
Golovin, D., Karro, J., Kochanski, G., Lee, C., Song, X., and Zhang, Q.
\newblock Gradientless descent: High-dimensional zeroth-order optimization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Haberstich et~al.(2021)Haberstich, Nouy, and
  Perrin]{haberstich2021active}
Haberstich, C., Nouy, A., and Perrin, G.
\newblock Active learning of tree tensor networks using optimal least-squares.
\newblock \emph{arXiv preprint arXiv:2104.13436}, 2021.

\bibitem[Hackbusch \& K{\"u}hn(2009)Hackbusch and K{\"u}hn]{hackbusch2009new}
Hackbusch, W. and K{\"u}hn, S.
\newblock A new scheme for the tensor representation.
\newblock \emph{Journal of Fourier analysis and applications}, 15\penalty0
  (5):\penalty0 706--722, 2009.

\bibitem[Hashemizadeh et~al.(2020)Hashemizadeh, Liu, Miller, and
  Rabusseau]{hashemizadeh2020adaptive}
Hashemizadeh, M., Liu, M., Miller, J., and Rabusseau, G.
\newblock Adaptive tensor learning with tensor networks.
\newblock \emph{arXiv preprint arXiv:2008.05437}, 2020.

\bibitem[Hawkins \& Zhang(2021)Hawkins and Zhang]{hawkins2021bayesian}
Hawkins, C. and Zhang, Z.
\newblock Bayesian tensorized neural networks with automatic rank selection.
\newblock \emph{Neurocomputing}, 453:\penalty0 172--180, 2021.

\bibitem[Hayashi et~al.(2019)Hayashi, Yamaguchi, Sugawara, and
  Maeda]{hayashi2019exploring}
Hayashi, K., Yamaguchi, T., Sugawara, Y., and Maeda, S.-i.
\newblock Exploring unexplored tensor network decompositions for convolutional
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5553--5563, 2019.

\bibitem[Huggins et~al.(2019)Huggins, Patil, Mitchell, Whaley, and
  Stoudenmire]{huggins2019towards}
Huggins, W., Patil, P., Mitchell, B., Whaley, K.~B., and Stoudenmire, E.~M.
\newblock Towards quantum machine learning with tensor networks.
\newblock \emph{Quantum Science and technology}, 4\penalty0 (2):\penalty0
  024001, 2019.

\bibitem[Izmailov et~al.(2018)Izmailov, Novikov, and
  Kropotov]{izmailov2018scalable}
Izmailov, P., Novikov, A., and Kropotov, D.
\newblock Scalable {G}aussian processes with billions of inducing inputs via
  tensor train decomposition.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  726--735. PMLR, 2018.

\bibitem[Khavari \& Rabusseau(2021)Khavari and Rabusseau]{khavari2021lower}
Khavari, B. and Rabusseau, G.
\newblock Lower and upper bounds on the pseudo-dimension of tensor network
  models.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kodryan et~al.(2020)Kodryan, Kropotov, and Vetrov]{kodryan2020mars}
Kodryan, M., Kropotov, D., and Vetrov, D.
\newblock Mars: Masked automatic ranks selection in tensor decompositions.
\newblock \emph{arXiv preprint arXiv:2006.10859}, 2020.

\bibitem[Kossaifi et~al.(2020)Kossaifi, Lipton, Kolbeinsson, Khanna,
  Furlanello, and Anandkumar]{kossaifi2020tensor}
Kossaifi, J., Lipton, Z.~C., Kolbeinsson, A., Khanna, A., Furlanello, T., and
  Anandkumar, A.
\newblock Tensor regression networks.
\newblock \emph{Journal of Machine Learning Research}, 21:\penalty0 1--21,
  2020.

\bibitem[Koutra et~al.(2011)Koutra, Parikh, Ramdas, and
  Xiang]{koutra2011algorithms}
Koutra, D., Parikh, A., Ramdas, A., and Xiang, J.
\newblock Algorithms for graph similarity and subgraph matching.
\newblock In \emph{Proc. Ecol. Inference Conf}, volume~17, 2011.

\bibitem[Krasikov et~al.(2006)Krasikov, Lev, and Thatte]{krasikov2006upper}
Krasikov, I., Lev, A., and Thatte, B.
\newblock Upper bounds on the automorphism group of a graph.
\newblock \emph{Discrete Math.}, 256\penalty0 (math. CO/0609425):\penalty0
  489--493, 2006.

\bibitem[Landsberg et~al.(2011)Landsberg, Qi, and Ye]{landsberg2011geometry}
Landsberg, J.~M., Qi, Y., and Ye, K.
\newblock On the geometry of tensor network states.
\newblock \emph{arXiv preprint arXiv:1105.4449}, 2011.

\bibitem[Latorre(2005)]{latorre2005image}
Latorre, J.~I.
\newblock Image compression and entanglement.
\newblock \emph{arXiv preprint quant-ph/0510031}, 2005.

\bibitem[Li \& Sun(2020)Li and Sun]{li2020evolutionary}
Li, C. and Sun, Z.
\newblock Evolutionary topology search for tensor network decomposition.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (ICML)}, 2020.

\bibitem[Li et~al.(2020)Li, Sun, and Zhao]{li2020high}
Li, C., Sun, Z., and Zhao, Q.
\newblock High-order learning model via fractional tensor network
  decomposition.
\newblock In \emph{First Workshop on Quantum Tensor Networks in Machine
  Learning, 34th Conference on Neural Information Processing Systems (NeurIPS
  2020)}, 2020.

\bibitem[Li et~al.(2021)Li, Pan, Chen, Ding, Zhao, and Xu]{li2021heuristic}
Li, N., Pan, Y., Chen, Y., Ding, Z., Zhao, D., and Xu, Z.
\newblock Heuristic rank selection with progressively searching tensor ring
  network.
\newblock \emph{Complex \& Intelligent Systems}, pp.\  1--15, 2021.

\bibitem[Liu et~al.(2021)Liu, Li, Zhang, and Zhang]{liu2021tensor}
Liu, J., Li, S., Zhang, J., and Zhang, P.
\newblock Tensor networks for unsupervised machine learning.
\newblock \emph{arXiv preprint arXiv:2106.12974}, 2021.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Chen, Kailkhura, Zhang, Hero~III,
  and Varshney]{liu2020primer}
Liu, S., Chen, P.-Y., Kailkhura, B., Zhang, G., Hero~III, A.~O., and Varshney,
  P.~K.
\newblock A primer on zeroth-order optimization in signal processing and
  machine learning: Principals, recent advances, and applications.
\newblock \emph{IEEE Signal Processing Magazine}, 37\penalty0 (5):\penalty0
  43--54, 2020{\natexlab{a}}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Lu, Chen, Feng, Xu, Al-Dujaili,
  Hong, and O’Reilly]{liu2020min}
Liu, S., Lu, S., Chen, X., Feng, Y., Xu, K., Al-Dujaili, A., Hong, M., and
  O’Reilly, U.-M.
\newblock Min-max optimization without gradients: Convergence and applications
  to black-box evasion and poisoning attacks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6282--6293. PMLR, 2020{\natexlab{b}}.

\bibitem[Long et~al.(2021)Long, Zhu, Liu, and Liu]{long2021bayesian}
Long, Z., Zhu, C., Liu, J., and Liu, Y.
\newblock Bayesian low rank tensor ring for image recovery.
\newblock \emph{IEEE Transactions on Image Processing}, 30:\penalty0
  3568--3580, 2021.

\bibitem[L{\"u}ck(2008)]{luck2008survey}
L{\"u}ck, W.
\newblock Survey on geometric group theory.
\newblock \emph{arXiv preprint arXiv:0806.3771}, 2008.

\bibitem[Markov \& Shi(2008)Markov and Shi]{markov2008simulating}
Markov, I.~L. and Shi, Y.
\newblock Simulating quantum computation by contracting tensor networks.
\newblock \emph{SIAM Journal on Computing}, 38\penalty0 (3):\penalty0 963--981,
  2008.

\bibitem[Mickelin \& Karaman(2020)Mickelin and Karaman]{mickelin2020algorithms}
Mickelin, O. and Karaman, S.
\newblock On algorithms for and computing with the tensor ring decomposition.
\newblock \emph{Numerical Linear Algebra with Applications}, 27\penalty0
  (3):\penalty0 e2289, 2020.

\bibitem[Miller et~al.(2021)Miller, Rabusseau, and Terilla]{miller2021tensor}
Miller, J., Rabusseau, G., and Terilla, J.
\newblock Tensor networks for probabilistic sequence modeling.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3079--3087. PMLR, 2021.

\bibitem[Nie et~al.(2021)Nie, Wang, and Tian]{nie2021adaptive}
Nie, C., Wang, H., and Tian, L.
\newblock Adaptive tensor networks decomposition.
\newblock In \emph{BMVC}, 2021.

\bibitem[Novikov et~al.(2015)Novikov, Podoprikhin, Osokin, and
  Vetrov]{novikov2015tensorizing}
Novikov, A., Podoprikhin, D., Osokin, A., and Vetrov, D.~P.
\newblock Tensorizing neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  442--450, 2015.

\bibitem[Novikov et~al.(2021)Novikov, Panov, and Oseledets]{novikov2021tensor}
Novikov, G.~S., Panov, M.~E., and Oseledets, I.~V.
\newblock Tensor-train density estimation.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  1321--1331.
  PMLR, 2021.

\bibitem[Oseledets(2011)]{oseledets2011tensor}
Oseledets, I.~V.
\newblock Tensor-train decomposition.
\newblock \emph{SIAM Journal on Scientific Computing}, 33\penalty0
  (5):\penalty0 2295--2317, 2011.

\bibitem[Papadimitriou \& Yannakakis(1982)Papadimitriou and
  Yannakakis]{papadimitriou1982complexity}
Papadimitriou, C.~H. and Yannakakis, M.
\newblock The complexity of facets (and some facets of complexity).
\newblock In \emph{Proceedings of the fourteenth annual ACM symposium on Theory
  of computing}, pp.\  255--260, 1982.

\bibitem[Qiu et~al.(2021)Qiu, Li, Weng, Sun, He, and Zhao]{qiu2021memory}
Qiu, H., Li, C., Weng, Y., Sun, Z., He, X., and Zhao, Q.
\newblock On the memory mechanism of tensor-power recurrent models.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3682--3690. PMLR, 2021.

\bibitem[Razin et~al.(2021)Razin, Maman, and Cohen]{razin2021implicit}
Razin, N., Maman, A., and Cohen, N.
\newblock Implicit regularization in tensor factorization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8913--8924. PMLR, 2021.

\bibitem[Razin et~al.(2022)Razin, Maman, and Cohen]{razin2022implicit}
Razin, N., Maman, A., and Cohen, N.
\newblock Implicit regularization in hierarchical tensor factorization and deep
  convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:2201.11729}, 2022.

\bibitem[Reyes \& Stoudenmire(2020)Reyes and Stoudenmire]{reyes2020multi}
Reyes, J. and Stoudenmire, M.
\newblock A multi-scale tensor network architecture for classification and
  regression.
\newblock \emph{arXiv preprint arXiv:2001.08286}, 2020.

\bibitem[Richter et~al.(2021)Richter, Sallandt, and
  N{\"u}sken]{pmlr-v139-richter21a}
Richter, L., Sallandt, L., and N{\"u}sken, N.
\newblock Solving high-dimensional parabolic {PDE}s using the tensor train
  format.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  8998--9009. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/richter21a.html}.

\bibitem[Russell \& Norvig(1995)Russell and Norvig]{russell1995artificial}
Russell, S. and Norvig, P.
\newblock \emph{Artificial Intelligence: A Modern Approach}.
\newblock CUMINCAD, 1995.

\bibitem[Savarese et~al.(2021)Savarese, McAllester, Babu, and
  Maire]{savarese2021domain}
Savarese, P., McAllester, D., Babu, S., and Maire, M.
\newblock Domain-independent dominance of adaptive methods.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  16286--16295, 2021.

\bibitem[Sedighin et~al.(2021)Sedighin, Cichocki, and
  Phan]{sedighin2021adaptive}
Sedighin, F., Cichocki, A., and Phan, A.-H.
\newblock Adaptive rank selection for tensor ring decomposition.
\newblock \emph{IEEE Journal of Selected Topics in Signal Processing},
  15\penalty0 (3):\penalty0 454--463, 2021.

\bibitem[Singh(2021)]{singh2021continuum}
Singh, S.
\newblock Continuum-armed bandits: A function space perspective.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  2620--2628. PMLR, 2021.

\bibitem[Snyder \& Daskin(2006)Snyder and Daskin]{snyder2006random}
Snyder, L.~V. and Daskin, M.~S.
\newblock A random-key genetic algorithm for the generalized traveling salesman
  problem.
\newblock \emph{European journal of operational research}, 174\penalty0
  (1):\penalty0 38--53, 2006.

\bibitem[Tao \& Zhao(2020)Tao and Zhao]{tao2020bayesian}
Tao, Z. and Zhao, Q.
\newblock Bayesian tensor ring decomposition for low rank tensor completion.
\newblock In \emph{International Workshop on Tensor Network Representations in
  Machine Learning, IJCAI}, 2020.

\bibitem[T{\"u}fekci(2014)]{tufekci2014prediction}
T{\"u}fekci, P.
\newblock Prediction of full load electrical power output of a base load
  operated combined cycle power plant using machine learning methods.
\newblock \emph{International Journal of Electrical Power \& Energy Systems},
  60:\penalty0 126--140, 2014.

\bibitem[Verstraete \& Cirac(2004)Verstraete and
  Cirac]{verstraete2004renormalization}
Verstraete, F. and Cirac, J.~I.
\newblock Renormalization algorithms for quantum-many body systems in two and
  higher dimensions.
\newblock \emph{arXiv preprint cond-mat/0407066}, 2004.

\bibitem[Wang et~al.(2017)Wang, Aggarwal, and Aeron]{wang2017efficient}
Wang, W., Aggarwal, V., and Aeron, S.
\newblock Efficient low rank tensor ring completion.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  5697--5705, 2017.

\bibitem[Weber(1997)]{weber1997usc}
Weber, A.~G.
\newblock The {USC-SIPI} image database version 5.
\newblock \emph{USC-SIPI Report}, 315\penalty0 (1), 1997.

\bibitem[Ye \& Lim(2019)Ye and Lim]{Ye2019Tensor}
Ye, K. and Lim, L.-H.
\newblock Tensor network ranks.
\newblock \emph{arXiv preprint arXiv:1801.02662}, 2019.

\bibitem[Yokota et~al.(2016)Yokota, Zhao, and Cichocki]{yokota2016smooth}
Yokota, T., Zhao, Q., and Cichocki, A.
\newblock Smooth parafac decomposition for tensor completion.
\newblock \emph{IEEE Transactions on Signal Processing}, 64\penalty0
  (20):\penalty0 5423--5436, 2016.

\bibitem[Yuan et~al.(2019{\natexlab{a}})Yuan, Li, Mandic, Cao, and
  Zhao]{yuan2019tensor}
Yuan, L., Li, C., Mandic, D., Cao, J., and Zhao, Q.
\newblock Tensor ring decomposition with rank minimization on latent space: An
  efficient approach for tensor completion.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  9151--9158, 2019{\natexlab{a}}.

\bibitem[Yuan et~al.(2019{\natexlab{b}})Yuan, Zhao, Gui, and Cao]{yuan2019high}
Yuan, L., Zhao, Q., Gui, L., and Cao, J.
\newblock High-order tensor completion via gradient-based optimization under
  tensor train format.
\newblock \emph{Signal Processing: Image Communication}, 73:\penalty0 53--61,
  2019{\natexlab{b}}.

\bibitem[Zhao et~al.(2015)Zhao, Zhang, and Cichocki]{zhao2015bayesian}
Zhao, Q., Zhang, L., and Cichocki, A.
\newblock Bayesian {CP} factorization of incomplete tensors with automatic rank
  determination.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 37\penalty0 (9):\penalty0 1751--1763, 2015.

\bibitem[Zhao et~al.(2016)Zhao, Zhou, Xie, Zhang, and Cichocki]{zhao2016tensor}
Zhao, Q., Zhou, G., Xie, S., Zhang, L., and Cichocki, A.
\newblock Tensor ring decomposition.
\newblock \emph{arXiv preprint arXiv:1606.05535}, 2016.

\bibitem[Zheng et~al.(2021)Zheng, Huang, Zhao, Zhao, and Jiang]{zheng2021fully}
Zheng, Y.-B., Huang, T.-Z., Zhao, X.-L., Zhao, Q., and Jiang, T.-X.
\newblock Fully-connected tensor network decomposition and its application to
  higher-order tensor completion.
\newblock In \emph{Proc. AAAI}, volume~35, pp.\  11071--11078, 2021.

\end{thebibliography}
