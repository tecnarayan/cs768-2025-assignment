@article{chen2016statistical,
  title={Statistical inference for model parameters in stochastic gradient descent},
  author={Chen, Xi and Lee, Jason D and Tong, Xin T and Zhang, Yichen},
  journal={arXiv preprint arXiv:1610.08637},
  year={2016}
}

@inproceedings{li2018statistical,
  title={Statistical inference using SGD},
  author={Li, Tianyang and Liu, Liu and Kyrillidis, Anastasios and Caramanis, Constantine},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@inproceedings{chaudhari2018stochastic,
  title={Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks},
  author={Chaudhari, Pratik and Soatto, Stefano},
  booktitle={2018 Information Theory and Applications Workshop (ITA)},
  pages={1--10},
  year={2018},
  organization={IEEE}
}




@incollection{karatzas1998brownian,
  title={Brownian motion},
  author={Karatzas, Ioannis and Shreve, Steven E},
  booktitle={Brownian Motion and Stochastic Calculus},
  pages={47--127},
  year={1998},
  publisher={Springer}
}

@book{nelson1967dynamical,
  title={Dynamical theories of {B}rownian motion},
  author={Nelson, Edward},
  volume={3},
  year={1967},
  publisher={Princeton university press}
}



@inproceedings{li2019stochastic,
  title={Stochastic {R}unge-{K}utta Accelerates {L}angevin {M}onte {C}arlo and Beyond},
  author={Li, Xuechen and Wu, Yi and Mackey, Lester},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7746--7758},
  year={2019}
}

@inproceedings{he2019control,
  title={Control Batch Size and Learning Rate to Generalize Well: Theoretical and Empirical Evidence},
  author={He, Fengxiang and Liu, Tongliang and Tao, Dacheng},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1141--1150},
  year={2019}
}

@inproceedings{erdogdu2018global,
  title={Global non-convex optimization with discretized diffusions},
  author={Erdogdu, Murat A and Mackey, Lester and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9671--9680},
  year={2018}
}

@article{chatzigeorgiou2013bounds,
  title={Bounds on the {L}ambert function and their application to the outage analysis of user cooperation},
  author={Chatzigeorgiou, Ioannis},
  journal={IEEE Communications Letters},
  volume={17},
  number={8},
  pages={1505--1508},
  year={2013},
  publisher={IEEE}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@techreport{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey},
  year={2009},
  institution={Citeseer}
}

@article{cheng2019quantitative,
    title={Quantitative Central Limit Theorems for Discrete Stochastic Processes},
    author={Cheng, Xiang and Bartlett, Peter L and Jordan, Michael I},
    journal={arXiv preprint arXiv:1902.00832},
    year={2019}
}

@article{gorham2016measuring,
  title={Measuring sample quality with diffusions},
  author={Gorham, Jackson and Duncan, Andrew B and Vollmer, Sebastian J and Mackey, Lester},
  journal={arXiv preprint arXiv:1611.06972},
  year={2016}
}
@article{eldan2018clt,
    title={The {CLT} in high dimensions: quantitative bounds via martingale embedding},
    author={Eldan, Ronen and Mikulincer, Dan and Zhai, Alex},
    journal={arXiv preprint arXiv:1806.09087},
    year={2018}
}

@article{jastrzkebski2017three,
    title={Three factors influencing minima in {SGD}},
    author={Jastrz{\k{e}}bski, Stanis{\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
    journal={arXiv preprint arXiv:1711.04623},
    year={2017}
}

@article{keskar2016large,
    title={On large-batch training for deep learning: Generalization gap and sharp minima},
    author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
    journal={arXiv preprint arXiv:1609.04836},
    year={2016}
}

@article{kleinberg2018alternative,
    title={An Alternative View: When Does {SGD} Escape Local Minima?},
    author={Kleinberg, Robert and Li, Yuanzhi and Yuan, Yang},
    journal={arXiv preprint arXiv:1802.06175},
    year={2018}
}

@article{dalalyan2017theoretical,
    title={Theoretical guarantees for approximate sampling from smooth and log-concave densities},
    author={Dalalyan, Arnak S},
    journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
    volume={79},
    number={3},
    pages={651--676},
    year={2017},
    publisher={Wiley Online Library}
}

@article{durmus2016high,
    title={High-dimensional {B}ayesian inference via the unadjusted Langevin algorithm},
    author={Durmus, Alain and Moulines, Eric},
    journal={arXiv preprint arXiv:1605.01559},
    year={2016}
}


@article{eberle2016reflection,
    title={Reflection couplings and contraction rates for diffusions},
    author={Eberle, Andreas},
    journal={Probability theory and related fields},
    volume={166},
    number={3-4},
    pages={851--886},
    year={2016},
    publisher={Springer}
}

@article{eberle2011reflection,
    title={Reflection coupling and {W}asserstein contractivity without convexity},
    author={Eberle, Andreas},
    journal={Comptes Rendus Mathematique},
    volume={349},
    number={19-20},
    pages={1101--1104},
    year={2011},
    publisher={Elsevier}
}

@article{bou2018coupling,
    title={Coupling and convergence for {H}amiltonian {M}onte {C}arlo},
    author={Bou-Rabee, Nawaf and Eberle, Andreas and Zimmer, Raphael},
    journal={arXiv preprint arXiv:1805.00452},
    year={2018}
}

@article{ma2018sampling,
    title={Sampling can be faster than optimization},
    author={Ma, Yi-An and Chen, Yuansi and Jin, Chi and Flammarion, Nicolas and Jordan, Michael I},
    journal={arXiv preprint arXiv:1811.08413},
    year={2018}
}

@article{cheng2018sharp,
    title={Sharp convergence rates for {L}angevin dynamics in the nonconvex setting},
    author={Cheng, Xiang and Chatterji, Niladri S and Abbasi-Yadkori, Yasin and Bartlett, Peter L and Jordan, Michael I},
    journal={arXiv preprint arXiv:1805.01648},
    year={2018}
}

@inproceedings{mandt2016variational,
    title={A variational analysis of stochastic gradient algorithms},
    author={Mandt, Stephan and Hoffman, Matthew and Blei, David},
    booktitle={International Conference on Machine Learning},
    pages={354--363},
    year={2016}
}

@article{anastasiou2019normal,
    title={Normal Approximation for Stochastic Gradient Descent via Non-Asymptotic Rates of Martingale {CLT}},
    author={Anastasiou, Andreas and Balasubramanian, Krishnakumar and Erdogdu, Murat A},
    journal={arXiv preprint arXiv:1904.02130},
    year={2019}
}

@inproceedings{hoffer2017train,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1731--1741},
  year={2017}
}

@article{zhang2019adam,
  title={Why ADAM beats SGD for attention models},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank J and Kumar, Sanjiv and Sra, Suvrit},
  journal={arXiv preprint arXiv:1912.03194},
  year={2019}
}
@article{simsekli2019tail,
  title={A tail-index analysis of stochastic gradient noise in deep neural networks},
  author={Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  journal={arXiv preprint arXiv:1901.06053},
  year={2019}
}
