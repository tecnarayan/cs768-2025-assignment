
@inproceedings{
eysenbach2021clearning,
title={C-Learning: Learning to Achieve Goals via Recursive Classification},
author={Benjamin Eysenbach and Ruslan Salakhutdinov and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=tc5qisoB-C}
}

@article{Lu2019PredictiveCF,
  title={Predictive Coding for Boosting Deep Reinforcement Learning with Sparse Rewards},
  author={Xingyu Lu and Stas Tiomkin and P. Abbeel},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.13414}
}

@inproceedings{tian2021modelbased,
title={Model-Based Visual Planning with Self-Supervised Functional Distances},
author={Stephen Tian and Suraj Nair and Frederik Ebert and Sudeep Dasari and Benjamin Eysenbach and Chelsea Finn and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=UcoXdfrORC}
}

@inproceedings{liu2020ropes,
  title = 	 {Hallucinative Topological Memory for Zero-Shot Visual Planning},
  author =       {Liu, Kara and Kurutach, Thanard and Tung, Christine and Abbeel, Pieter and Tamar, Aviv},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6259--6270},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/liu20h/liu20h.pdf},
  url = 	 {https://proceedings.mlr.press/v119/liu20h.html},
}


@inproceedings{wang2020ropes,
author    = {Angelina Wang and
               Thanard Kurutach and
               Pieter Abbeel and
               Aviv Tamar},
  editor    = {Antonio Bicchi and
               Hadas Kress{-}Gazit and
               Seth Hutchinson},
  title     = {Learning Robotic Manipulation through Visual Planning and Acting},
  booktitle = {Robotics: Science and Systems XV, University of Freiburg, Freiburg
               im Breisgau, Germany, June 22-26, 2019},
  year      = {2019},
  url       = {https://doi.org/10.15607/RSS.2019.XV.074},
  doi       = {10.15607/RSS.2019.XV.074},
  timestamp = {Thu, 01 Apr 2021 15:25:13 +0200},
  biburl    = {https://dblp.org/rec/conf/rss/WangKAT19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{chebotar2021actionable,
author    = {Yevgen Chebotar and
               Karol Hausman and
               Yao Lu and
               Ted Xiao and
               Dmitry Kalashnikov and
               Jacob Varley and
               Alex Irpan and
               Benjamin Eysenbach and
               Ryan Julian and
               Chelsea Finn and
               Sergey Levine},
  editor    = {Marina Meila and
               Tong Zhang},
  title     = {Actionable Models: Unsupervised Offline Reinforcement Learning of
               Robotic Skills},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning,
               {ICML} 2021, 18-24 July 2021, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {1518--1528},
  publisher = {{PMLR}},
  year      = {2021},
  url       = {http://proceedings.mlr.press/v139/chebotar21a.html},
  timestamp = {Wed, 25 Aug 2021 17:11:17 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/ChebotarHLXKVIE21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{kalashnikov2021mtopt,
   author = {Dmitry Kalashnikov and Jacob Varley and Yevgen Chebotar and Benjamin Swanson and Rico Jonschkowski and Chelsea Finn and Sergey Levine and Karol Hausman},
   journal = {Conference on Robot Learning (CoRL)},
   month = {4},
   title = {MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale},
   url = {https://arxiv.org/abs/2104.08212v2},
   year = {2021},
}

@inproceedings{shah2021rapid,
title={Rapid Exploration for Open-World Navigation with Latent Goal Models},
author={Dhruv Shah and Benjamin Eysenbach and Nicholas Rhinehart and Sergey Levine},
booktitle={5th Annual Conference on Robot Learning },
year={2021},
}

@inproceedings{nasiriany2019leap,
	title = {Planning with {Goal}-{Conditioned} {Policies}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/file/c8cc6e90ccbff44c9cee23611711cdc4-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Nasiriany, Soroush and Pong, Vitchyr and Lin, Steven and Levine, Sergey},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
}

@article{kostrikov2021iql,
 author    = {Ilya Kostrikov and
               Ashvin Nair and
               Sergey Levine},
  title     = {Offline Reinforcement Learning with Implicit Q-Learning},
  journal   = {CoRR},
  volume    = {abs/2110.06169},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.06169},
  eprinttype = {arXiv},
  eprint    = {2110.06169},
  timestamp = {Fri, 22 Oct 2021 13:33:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-06169.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{nair19ccrig,
    author    = {Ashvin Nair and Shikhar Bahl and Alexander Khazatsky and Vitchyr Pong and Glen Berseth and Sergey Levine},
    title     = {Contextual Imagined Goals for Self-Supervised Robotic Learning},
    booktitle = {Conference on Robot Learning (CoRL)},
    year      = {2019}
}

  
@inproceedings{lan2021metrics,
	title = {Metrics and continuity in reinforcement learning},
    booktitle={AAAI Conference on Artificial Intelligence},
	author = {Lan, Charline Le and Bellemare, Marc G. and Castro, Pablo Samuel},
	year = {2021},
}

@inproceedings{larsen1989bisim,
author = {Larsen, K. G. and Skou, A.},
title = {Bisimulation through Probabilistic Testing (Preliminary Report)},
year = {1989},
isbn = {0897912942},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/75277.75307},
doi = {10.1145/75277.75307},
booktitle = {Symposium on Principles of Programming Languages},
pages = {344–352},
numpages = {9},
}


@inproceedings{laskin_srinivas2020curl,
  title={{CURL}: Contrastive unsupervised representations for reinforcement learning},
  author={Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={5639--5650},
  year={2020},
  organization={PMLR}
}

@inproceedings{castro20bisimulation,
  author    = {Pablo Samuel Castro},
  title     = {Scalable methods for computing state similarity in deterministic {M}arkov Decision Processes},
  year      = {2020},
  booktitle = {Association for the Advancement of Artificial Intelligence (AAAI)},
}

@article{konidaris2012transfer,
author = {Konidaris, George and Scheidwasser, Ilya and Barto, Andrew G.},
title = {Transfer in Reinforcement Learning via Shared Features},
year = {2012},
issue_date = {3/1/2012},
publisher = {JMLR.org},
volume = {13},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = {May},
pages = {1333–1371},
numpages = {39},
keywords = {transfer, skills, shaping, reinforcement learning}
}
@inproceedings{fernandez2006reuse,
author = {Fern\'{a}ndez, Fernando and Veloso, Manuela},
title = {Probabilistic Policy Reuse in a Reinforcement Learning Agent},
year = {2006},
isbn = {1595933034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1160633.1160762},
doi = {10.1145/1160633.1160762},
booktitle = {Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multiagent Systems},
pages = {720–727},
numpages = {8},
location = {Hakodate, Japan},
series = {AAMAS '06}
}

@inproceedings{carroll2005tasksim,
author = {Carroll, James and Seppi, Kevin},
year = {2005},
month = {01},
pages = {803 - 808 vol. 2},
title = {Task similarity measures for transfer in reinforcement learning task libraries},
isbn = {0-7803-9048-2},
doi = {10.1109/IJCNN.2005.1555955},
booktitle={In Proceedings of the International Joint Conference on Neural Networks}
}

@inproceedings{song2016mdpmetric,
author = {Song, Jinhua and Gao, Yang and Wang, Hao and An, Bo},
title = {Measuring the Distance Between Finite Markov Decision Processes},
year = {2016},
isbn = {9781450342391},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents and Multiagent Systems},
pages = {468–476},
numpages = {9},
keywords = {hausdorff metric, markov decision process, transfer learning, kantorovich metric, reinforcement learning},
location = {Singapore, Singapore},
series = {AAMAS '16}
}

@inproceedings{ferns2014bisim_metrics,
  title={Bisimulation Metrics are Optimal Value Functions.},
  author={Ferns, Norman and Precup, Doina},
  booktitle={Uncertainty in Artificial Intelligence (UAI)},
  pages={210--219},
  year={2014},
}
  
@inproceedings{ferns2004bisimulation,
 author = {Ferns, Norm and Panangaden, Prakash and Precup, Doina},
 title = {Metrics for Finite {Markov} Decision Processes},
 booktitle = {Uncertainty in Artificial Intelligence (UAI)},
 year = {2004},
 isbn = {0-9749039-0-6},
 location = {Banff, Canada},
 pages = {162--169},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=1036843.1036863},
 acmid = {1036863},
} 


@inproceedings{fu_learning_2021,
	title = {Learning {Task} {Informed} {Abstractions}},
	url = {http://proceedings.mlr.press/v139/fu21b.html},
	language = {en},
	urldate = {2021-08-17},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Fu, Xiang and Yang, Ge and Agrawal, Pulkit and Jaakkola, Tommi},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {3480--3491},
	file = {Full Text PDF:/Users/amyzhang/Dropbox (Personal)/Zotero/storage/AHAT76WJ/Fu et al. - 2021 - Learning Task Informed Abstractions.pdf:application/pdf;Supplementary PDF:/Users/amyzhang/Dropbox (Personal)/Zotero/storage/NBVTAY8H/Fu et al. - 2021 - Learning Task Informed Abstractions.pdf:application/pdf},
}


@inproceedings{zhang2020invariant,
    title={Invariant Causal Prediction for Block {MDP}s},
    author={Amy Zhang and Clare Lyle and Shagun Sodhani and Angelos Filos and Marta Kwiatkowska and Joelle Pineau and Yarin Gal and Doina Precup},
    year={2020},
    booktitle={International Conference on Machine Learning (ICML)},
}

@inproceedings{gelada2019deepmdp,
  title = 	 {{DeepMDP}: Learning Continuous Latent Space Models for Representation Learning},
  author = 	 {Gelada, Carles and Kumar, Saurabh and Buckman, Jacob and Nachum, Ofir and Bellemare, Marc G.},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  year = 	 {2019},
}

@inproceedings{castro2021mico,
title={{MIC}o: Improved representations via sampling-based state similarity for Markov decision processes},
author={Pablo Samuel Castro and Tyler Kastner and Prakash Panangaden and Mark Rowland},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=wFp6kmQELgu}
}

@inproceedings{khazatsky2021val,
  author    = {Alexander Khazatsky and
               Ashvin Nair and
               Daniel Jing and
               Sergey Levine},
  title     = {What Can {I} Do Here? Learning New Skills by Imagining Visual Affordances},
  booktitle = {{IEEE} International Conference on Robotics and Automation, {ICRA}
               2021, Xi'an, China, May 30 - June 5, 2021},
  pages     = {14291--14297},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/ICRA48506.2021.9561692},
  doi       = {10.1109/ICRA48506.2021.9561692},
  timestamp = {Mon, 25 Oct 2021 11:20:08 +0200},
  biburl    = {https://dblp.org/rec/conf/icra/KhazatskyNJL21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{han2021learning,
title={Learning Domain Invariant Representations in Goal-conditioned Block {MDP}s},
author={Beining Han and Chongyi Zheng and Harris Chan and Keiran Paster and Michael R. Zhang and Jimmy Ba},
booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
year={2021},
url={https://openreview.net/forum?id=oepSB9bsoCF}
}

@inproceedings{ghosh2018learning,
title={Learning Actionable Representations with Goal Conditioned Policies},
author={Dibya Ghosh and Abhishek Gupta and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Hye9lnCct7},
}

@inproceedings{agarwal2021contrastive,
title={Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning},
author={Rishabh Agarwal and Marlos C. Machado and Pablo Samuel Castro and Marc G Bellemare},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=qda7-sVg84}
}


@inproceedings{rakelly_which_2021,
	title = {Which {Mutual}-{Information} {Representation} {Learning} {Objectives} are {Sufficient} for {Control}?},
	url = {http://arxiv.org/abs/2106.07278},
     booktitle = {Advances in Neural Information Processing Systems},
     publisher = {Curran Associates, Inc.},
     volume = {34},
     year = {2021},
	author = {Rakelly, Kate and Gupta, Abhishek and Florensa, Carlos and Levine, Sergey},
}

@inproceedings{anand_unsupervised_2019,
	title = {Unsupervised {State} {Representation} {Learning} in {Atari}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/file/6fb52e71b837628ac16539c1ff911667-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Anand, Ankesh and Racah, Evan and Ozair, Sherjil and Bengio, Yoshua and Côté, Marc-Alexandre and Hjelm, R Devon},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
}

@article{Sermanet2017TCN,
  author    = {Pierre Sermanet and
               Corey Lynch and
               Yevgen Chebotar and
               Jasmine Hsu and
               Eric Jang and
               Stefan Schaal and
               Sergey Levine},
  title     = {Time-Contrastive Networks: Self-Supervised Learning from Video},
  journal   = {Proceedings of International Conference in Robotics and Automation (ICRA)}},
  year      = {2018},
  url       = {http://arxiv.org/abs/1704.06888},
  biburl    = {https://github.com/sermanet/home/blob/master/docs/bib/Sermanet2017TCN.bib},
}

@inproceedings{li2006stateabs,
  title={Towards a Unified Theory of State Abstraction for {MDPs}},
  author={Li, Lihong and Walsh, Thomas J and Littman, Michael L},
  booktitle={International Symposium on Artificial Intelligence and Mathematics (ISAIM)},
  year={2006}
}

@inproceedings{liu2021returnbased,
title={Return-Based Contrastive Representation Learning for Reinforcement  Learning},
author={Guoqing Liu and Chuheng Zhang and Li Zhao and Tao Qin and Jinhua Zhu and Li Jian and Nenghai Yu and Tie-Yan Liu},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=_TM6rT7tXke}
}

@article{Givan2003EquivalenceNA,
  title={Equivalence notions and model minimization in {Markov} decision processes},
  author={Robert Givan and Thomas L. Dean and Matthew Greig},
  journal={Artificial Intelligence},
  year={2003},
  volume={147},
  pages={163-223}
}


@InProceedings{Dwibedi_2019_CVPR,
author = {Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},
title = {Temporal Cycle-Consistency Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@misc{simsiam,
      title={Exploring Simple Siamese Representation Learning}, 
      author={Xinlei Chen and Kaiming He},
      year={2020},
      eprint={2011.10566},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{byol,
 author = {Grill, Jean-Bastien and Strub, Florian and Altch\'{e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and Piot, Bilal and kavukcuoglu, koray and Munos, Remi and Valko, Michal},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {21271--21284},
 publisher = {Curran Associates, Inc.},
 title = {Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{ferns2011contbisim,
author = {Ferns, Norm and Panangaden, Prakash and Precup, Doina},
title = {Bisimulation Metrics for Continuous {Markov} Decision Processes},
year = {2011},
issue_date = {November 2011},
volume = {40},
number = {6},
issn = {0097-5397},
url = {https://doi.org/10.1137/10080484X},
doi = {10.1137/10080484X},
journal = {Society for Industrial and Applied Mathematics},
month = dec,
pages = {1662–1714},
numpages = {53},
}
  
  
@misc{raileanu2021drac,
      title={Automatic Data Augmentation for Generalization in Deep Reinforcement Learning}, 
      author={Roberta Raileanu and Max Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},
      year={2021},
      eprint={2006.12862},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{yarats2019sacae,
  title={Improving Sample Efficiency in Model-Free Reinforcement Learning from Images},
  author={Denis Yarats and Amy Zhang and Ilya Kostrikov and Brandon Amos and Joelle Pineau and Rob Fergus},
  booktitle={Association for the Advancement of Artificial Intelligence (AAAI)},
  year={2021}
}

@inproceedings{yarats2021drq,
title={Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels},
author={Denis Yarats and Ilya Kostrikov and Rob Fergus},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=GY6-6sTvGaf}
}

@INPROCEEDINGS{zhu2017cyclegan,
  author={J. {Zhu} and T. {Park} and P. {Isola} and A. A. {Efros}},
  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks}, 
  year={2017},
  volume={},
  number={},
  pages={2242-2251},
  doi={10.1109/ICCV.2017.244}}

@INPROCEEDINGS{Smith2020avid, 
    AUTHOR    = {Laura Smith AND Nikita Dhawan AND Marvin Zhang AND Pieter Abbeel AND Sergey Levine}, 
    TITLE     = {{AVID: Learning Multi-Stage Tasks via Pixel-Level Translation of Human Videos}}, 
    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, 
    YEAR      = {2020}, 
    ADDRESS   = {Corvalis, Oregon, USA}, 
    MONTH     = {July}, 
    DOI       = {10.15607/RSS.2020.XVI.024} 
} 
  
   
@misc{eysenbach2021replacing,
      title={Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification}, 
      author={Benjamin Eysenbach and Sergey Levine and Ruslan Salakhutdinov},
      year={2021},
      eprint={2103.12656},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@techreport{deepmindcontrolsuite2018,
  title = {{DeepMind} Control Suite},
  author = {Yuval Tassa and Yotam Doron and Alistair Muldal and Tom Erez
            and Yazhe Li and Diego de Las Casas and David Budden and Abbas
            Abdolmaleki and Josh Merel and Andrew Lefrancq and Timothy Lillicrap
            and Martin Riedmiller},
  year = 2018,
  month = jan,
  howpublished = {https://arxiv.org/abs/1801.00690},
  url = {https://arxiv.org/abs/1801.00690},
  volume = {abs/1504.04804},
  institution = {DeepMind}
}


@inproceedings{ghosh2021learning,
title={Learning to Reach Goals via Iterated Supervised Learning},
author={Dibya Ghosh and Abhishek Gupta and Ashwin Reddy and Justin Fu and Coline Manon Devin and Benjamin Eysenbach and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=rALA0Xo6yNJ}
}

@inproceedings{
zhang2021dbc,
title={Learning Invariant Representations for Reinforcement Learning without Reconstruction},
author={Amy Zhang and Rowan Thomas McAllister and Roberto Calandra and Yarin Gal and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=-2FCwDKRREu}
}

@inproceedings{chang2020semanticvisualnav,
 author = {Chang, Matthew and Gupta, Arjun and Gupta, Saurabh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {4283--4294},
 publisher = {Curran Associates, Inc.},
 title = {Semantic Visual Navigation by Watching YouTube Videos},
 volume = {33},
 year = {2020}
}

@inproceedings{schmeckpeper2020reinforcement,
    title={Reinforcement Learning with Videos: Combining Offline Observations with Interaction},
    author={Karl Schmeckpeper and Oleh Rybkin and Kostas Daniilidis and Sergey Levine and Chelsea Finn},
    year={2020},
    volume = {100}, series = {Proceedings of Machine Learning Research}, address = {}, month = {30 Oct--01 Nov}, publisher = {PMLR},
    booktitle = {Proceedings of the Conference on Robot Learning},
}

@inproceedings{schulman2015computation,
abstract = {In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs-directed acyclic graphs that include both deterministic functions and conditional probability distributions-and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.},
archivePrefix = {arXiv},
arxivId = {1506.05254v3},
author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {1506.05254v3},
file = {::},
title = {{Gradient Estimation Using Stochastic Computation Graphs}},
year = {2015}
}
@inproceedings{hart2010affordances,
abstract = {In this paper we present a framework for guiding autonomous learning in robot systems. The paradigm we introduce allows a robot to acquire new skills according to an intrinsic motivation function that finds behavioral affordances. Affordances-in the sense of Gibson [6]-describe the latent possibilities for action in the environment and provide a direct means of organizing functional knowledge in embodied systems. We begin by showing how a robot can assemble closed-loop action primitives from its sensory and motor resources and then show how these primitives can be sequenced into multi-objective policies. We then show how these policies can be assembled hierarchically to support incremental and cumulative learning. The main contribution of this paper demonstrates how the proposed intrinsic motivator for affordance discovery can cause a robot to both acquire such hierarchical policies using reinforcement learning and then to generalize these policies to new contexts. As the framework is described, its effectiveness and applicability is demonstrated through a longitudinal learning experiment on a bimanual robot.},
author = {Hart, Stephen and Grupen, Roderic},
booktitle = {IEEE Transactions on Autonomous Mental Development},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Hart, Grupen - 2010 - Learning Generalizable Control Programs.pdf:pdf},
keywords = {Generalization,Incremental Learning,Index Terms-Cognitive Architectures,Intrinsic Motivation,Reinforcement Learning,Schema},
title = {{Learning Generalizable Control Programs}},
year = {2010}
}

@inproceedings{abel2014affordances,
abstract = {Planning algorithms for non-deterministic domains are often intractable in large state spaces due to the well-known "curse of dimensionality." Existing approaches to address this problem fail to prevent the planner from considering many actions which would be obviously irrelevant to a human solving the same problem. We formalize the notion of affordances [7] as knowledge added to an MDP that prunes actions in a state-and reward-general way. This pruning significantly reduces the number of state-action pairs the agent needs to evaluate in order to act optimally. We demonstrate our approach in the Minecraft domain, showing significant increase in speed and reduction in state-space exploration during planning. Further, we provide a learning framework that enables an agent to learn affordances through experience, removing the agent's dependence on the expert. We provide preliminary results indicating that the learning process effectively produces affordances that help solve an MDP faster.},
author = {Abel, David and Barth-Maron, Gabriel and Macglashan, James and Tellex, Stefanie},
booktitle = {RSS Workshop on Affordances in Vision for Cognitive Robotics},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Abel et al. - Unknown - Affordance-Aware Planning.pdf:pdf},
title = {{Toward Affordance-Aware Planning}},
url = {https://vimeo.com/88689171},
year = {2014}
}
@inproceedings{chentanez2005intrinsically,
  title={Intrinsically motivated reinforcement learning},
  author={Chentanez, Nuttapong and Barto, Andrew G and Singh, Satinder P},
  booktitle={Advances in neural information processing systems},
  pages={1281--1288},
  year={2005}
}
@inproceedings{lopes2012exploration,
  title={Exploration in model-based reinforcement learning by empirically estimating learning progress},
  author={Lopes, Manuel and Lang, Tobias and Toussaint, Marc and Oudeyer, Pierre-Yves},
  booktitle={Advances in Neural Information Processing Systems},
  pages={206--214},
  year={2012}
}

@inproceedings{jayaraman-iccv2015,
author = {Dinesh Jayaraman and Kristen Grauman},
title = {{Learning image representations tied to egomotion}},
booktitle = {ICCV},
year = {2015}
}


@InProceedings{yang2020plan2vec,
  title = 	 {Plan2Vec: Unsupervised Representation Learning by Latent Plans},
  author =       {Yang, Ge and Zhang, Amy and Morcos, Ari and Pineau, Joelle and Abbeel, Pieter and Calandra, Roberto},
  booktitle = 	 {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
  pages = 	 {935--946},
  year = 	 {2020},
  editor = 	 {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
  volume = 	 {120},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--11 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v120/yang20b/yang20b.pdf},
  url = 	 {https://proceedings.mlr.press/v120/yang20b.html},
  abstract = 	 {In this paper, we introducePlan2Vec, an model-based method to learn state representation fromsequences of off-policy observation data via planning. In contrast to prior methods, plan2vec doesnot require grounding via expert trajectories or actions, opening it up to many unsupervised learningscenarios. When applied to control, plan2vec learns a representation that amortizes the planningcost, enabling test time planning complexity that is linear in planning depth rather than exhaustiveover the entire state space. We demonstrate the effectiveness of Plan2Vec on one simulated andtwo real-world image datasets, showing that Plan2Vec can effectively acquire representations thatcarry long-range structure to accelerate planning. Additional results and videos can be found athttps://sites.google.com/view/plan2vec}
}

@MISC{coumans2021,
author =   {Erwin Coumans and Yunfei Bai},
title =    {PyBullet, a Python module for physics simulation for games, robotics and machine learning},
howpublished = {\url{http://pybullet.org}},
year = {2016--2021}
}
@inproceedings{robosuite2020,
  title={robosuite: A Modular Simulation Framework and Benchmark for Robot Learning},
  author={Yuke Zhu and Josiah Wong and Ajay Mandlekar and Roberto Mart\'{i}n-Mart\'{i}n},
  booktitle={arXiv preprint arXiv:2009.12293},
  year={2020}
}

@article{jonschowski2015priors,
   abstract = {Robot learning is critically enabled by the availability of appropriate state representations. We propose a robotics-specific approach to learning such state representations. As robots accomplish tasks by interacting with the physical world, we can facilitate representation learning by considering the structure imposed by physics; this structure is reflected in the changes that occur in the world and in the way a robot can effect them. By exploiting this structure in learning, robots can obtain state representations consistent with the aspects of physics relevant to the learning task. We name this prior knowledge about the structure of interactions with the physical world robotic priors. We identify five robotic priors and explain how they can be used to learn pertinent state representations. We demonstrate the effectiveness of this approach in simulated and real robotic experiments with distracting moving objects. We show that our method extracts task-relevant state representations from high-dimensional observations, even in the presence of task-irrelevant distractions. We also show that the state representations learned by our method greatly improve generalization in reinforcement learning.},
   author = {Rico Jonschkowski and Oliver Brock},
   doi = {10.1007/S10514-015-9459-7/FIGURES/7},
   issn = {15737527},
   issue = {3},
   journal = {Autonomous Robots},
   keywords = {Prior knowledge,Reinforcement learning,Representation learning,Robot learning},
   month = {10},
   pages = {407-428},
   publisher = {Kluwer Academic Publishers},
   title = {Learning state representations with robotic priors},
   volume = {39},
   url = {https://link.springer.com/article/10.1007/s10514-015-9459-7},
   year = {2015},
}

@inproceedings{jonschkowski2017pve,
   abstract = {We propose position-velocity encoders (PVEs) which learn---without
supervision---to encode images to positions and velocities of task-relevant
objects. PVEs encode a single image into a low-dimensional position state and
compute the velocity state from finite differences in position. In contrast to
autoencoders, position-velocity encoders are not trained by image
reconstruction, but by making the position-velocity representation consistent
with priors about interacting with the physical world. We applied PVEs to
several simulated control tasks from pixels and achieved promising preliminary
results.},
   author = {Rico Jonschkowski and Roland Hafner and Jonathan Scholz and Martin Riedmiller},
   isbn = {1705.09805v3},
   journal = {Robotics: Science and Systems},
   month = {5},
   title = {PVEs: Position-Velocity Encoders for Unsupervised Learning of Structured State Representations},
   url = {https://arxiv.org/abs/1705.09805v3},
   year = {2017},
}


@inproceedings{khetarpal2020affordances,
abstract = {Reinforcement learning algorithms usually assume that all actions are always available to an agent. However, both people and animals understand the general link between the features of their environment and the actions that are feasible. Gibson (1977) coined the term "affordances" to describe the fact that certain states enable an agent to do certain actions, in the context of embodied agents. In this paper, we develop a theory of affor-dances for agents who learn and plan in Markov Decision Processes. Affordances play a dual role in this case. On one hand, they allow faster planning , by reducing the number of actions available in any given situation. On the other hand, they facilitate more efficient and precise learning of transition models from data, especially when such models require function approximation. We establish these properties through theoretical results as well as illustrative examples. We also propose an approach to learn affordances and use it to estimate transition models that are simpler and generalize better.},
archivePrefix = {arXiv},
arxivId = {2006.15085v1},
author = {Khetarpal, Khimya and Ahmed, Zafarali and Comanici, Gheorghe and Abel, David and Precup, Doina},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {2006.15085v1},
title = {{What can I do here? A Theory of Affordances in Reinforcement Learning}},
year = {2020}
}
@article{min2016affordances,
abstract = {Affordances capture the relationships between a robot and the environment in terms of the actions that the robot is able to perform. The notable characteristic of affordance-based perception is that an object is perceived by what it affords (e.g., graspable and rollable), instead of identities (e.g., name, color, and shape). Affordances play an important role in basic robot capabilities such as recognition, planning, and prediction. The key challenges in affordance research are: 1) how to automatically discover the distinctive features that specify an affordance in an online and incremental manner and 2) how to generalize these features to novel environments. This survey provides an entry point for interested researchers, including: 1) a general overview; 2) classification and critical analysis of existing work; 3) discussion of how affordances are useful in developmental robotics; 4) some open questions about how to use the affordance concept; and 5) a few promising research directions. Index Terms-Affordance, autonomous mental development (AMD), developmental robotics, embodied intelligence (EI), infant learning, intrinsic motivation.},
author = {Min, Huaqing and Yi, An and Luo, Ronghua and Zhu, Jinhui and Bi, Sheng},
doi = {10.1109/TCDS.2016.2614992},
file = {::},
journal = {IEEE Transactions on Cognitive and Developmental Systems},
number = {4},
title = {{Affordance Research in Developmental Robotics: A Survey}},
url = {http://www.ieee.org/publications{\_}standards/publications/rights/index.html},
volume = {8},
year = {2016}
}
@article{yamanobe2018affordances,
author = {Yamanobe, Natsuki and Wan, Weiwei and Ramirez-Alpizar, Ixchel G. and Petit, Damien and Tsuji, Tokuo and Akizuki, Shuichi and Hashimoto, Manabu and Nagata, Kazuyuki and Harada, Kensuke},
year = {2018},
month = {01},
pages = {327-337},
title = {A Brief Review of Affordance in Robotic Manipulation Research},
volume = {36},
journal = {Journal of the Robotics Society of Japan},
doi = {10.7210/jrsj.36.327}
}
@article{zech2017affordances,
author = {Philipp Zech and Simon Haller and Safoura Rezapour Lakani and Barry Ridge and Emre Ugur and Justus Piater},
title ={Computational models of affordance in robotics: a taxonomy and systematic classification},
journal = {Adaptive Behavior},
volume = {25},
number = {5},
pages = {235-271},
year = {2017},
doi = {10.1177/1059712317726357},
URL = {https://doi.org/10.1177/1059712317726357},
eprint = {https://doi.org/10.1177/1059712317726357}
}
@techreport{hassanin2018affordances,
abstract = {Nowadays, robots are dominating the manufacturing, entertainment and healthcare industries. Robot vision aims to equip robots with the ability to discover information, understand it and interact with the environment. These capabilities require an agent to effectively understand object affordances and functionalities in complex visual domains. In this literature survey, we first focus on 'Visual affordances' and summarize the state of the art as well as open problems and research gaps. Specifically, we discuss sub-problems such as affordance detection, categorization, segmentation and high-level reasoning. Furthermore, we cover functional scene understanding and the prevalent functional descriptors used in the literature. The survey also provides necessary background to the problem, sheds light on its significance and highlights the existing challenges for affordance and functionality learning.},
archivePrefix = {arXiv},
arxivId = {1807.06775v1},
author = {Hassanin, Mohammed and Khan, Salman and Tahtali, Murat},
eprint = {1807.06775v1},
file = {::},
keywords = {Index Terms-affordance prediction,deep learning,functional scene understanding,object detection !},
number = {1},
title = {{Visual Affordance and Function Understanding: A Survey}},
volume = {6},
year = {2018}
}
@inproceedings{xu2021affordances,
abstract = {Planning in realistic environments requires searching in large planning spaces. Affordances are a powerful concept to simplify this search, because they model what actions can be successful in a given situation. However, the classical notion of affordance is not suitable for long horizon planning because it only informs the robot about the immediate outcome of actions instead of what actions are best for achieving a long-term goal. In this paper, we introduce a new affordance representation that enables the robot to reason about the long-term effects of actions through modeling what actions are afforded in the future, thereby informing the robot the best actions to take next to achieve a task goal. Based on the new representation, we develop a learning-to-plan method, Deep Affordance Foresight (DAF), that learns partial environment models of affordances of parameterized motor skills through trial-and-error. We evaluate DAF on two challenging manipulation domains and show that it can effectively learn to carry out multi-step tasks, share learned affordance representations among different tasks, and learn to plan with high-dimensional image inputs. Additional material at https://sites.google.com/stanford.edu/daf},
archivePrefix = {arXiv},
arxivId = {2011.08424v1},
author = {Xu, Danfei and Mandlekar, Ajay and Mart{\'{i}}n-Mart{\'{i}}n, Roberto and Zhu, Yuke and Savarese, Silvio and Fei-Fei, Li},
booktitle = {International Conference on Robotics and Automation (ICRA)},
eprint = {2011.08424v1},
file = {::},
title = {{Deep Affordance Foresight: Planning Through What Can Be Done in the Future}},
url = {https://sites.google.com/stanford.edu/daf},
year = {2021}
}
@techreport{colas2021gepsurvey,
abstract = {Building autonomous machines that can explore open-ended environments, discover possible interactions and autonomously build repertoires of skills is a general objective of artificial intelligence. Developmental approaches argue that this can only be achieved by autonomous and intrinsically motivated learning agents that can generate, select and learn to solve their own problems. In recent years, we have seen a convergence of developmental approaches, and developmental robotics in particular, with deep reinforcement learning (rl) methods, forming the new domain of developmental machine learning. Within this new domain, we review here a set of methods where deep rl algorithms are trained to tackle the developmental robotics problem of the autonomous acquisition of open-ended repertoires of skills. Intrinsically motivated goal-conditioned rl algorithms train agents to learn to represent, generate and pursue their own goals. The self-generation of goals requires the learning of compact goal encodings as well as their associated goal-achievement functions, which results in new challenges compared to traditional rl algorithms designed to tackle pre-defined sets of goals using external reward signals. This paper proposes a typology of these methods at the intersection of deep rl and developmental approaches, surveys recent approaches and discusses future avenues.},
archivePrefix = {arXiv},
year = {2021},
arxivId = {2012.09830v2},
author = {Colas, C{\'{e}}dric and Karch, Tristan and Sigaud, Olivier and Oudeyer, Pierre-Yves},
eprint = {2012.09830v2},
file = {::},
title = {{Intrinsically Motivated Goal-Conditioned Reinforcement Learning: a Short Survey}}
}

@inproceedings{rakelly2019pearl,
abstract = {Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While meta-reinforcement learning (meta-RL) algorithms can enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience , limiting their sample efficiency. They also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness on sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilis-tic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100x as well as in asymptotic performance on several meta-RL benchmarks.},
author = {Rakelly, Kate and Zhou, Aurick and Quillen, Deirdre and Finn, Chelsea and Levine, Sergey},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables}},
url = {https://github.com/katerakelly/oyster.},
year = {2019}
}

@inproceedings{yu2019pcgrad,
abstract = {While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference , and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance.},
archivePrefix = {arXiv},
arxivId = {2001.06782v2},
author = {Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},
eprint = {2001.06782v2},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Yu et al. - Unknown - Gradient Surgery for Multi-Task Learning.pdf:pdf},
title = {{Gradient Surgery for Multi-Task Learning}},
year = {2019}
}
@techreport{kidambi2020morel,
abstract = {In offline reinforcement learning (RL), the goal is to learn a successful policy using only a dataset of historical interactions with the environment, without any additional online interactions. This serves as an extreme test for an agent's ability to effectively use historical data, which is critical for efficient RL. Prior work in offline RL has been confined almost exclusively to model-free RL approaches. In this work, we present MOReL , an algorithmic framework for model-based RL in the offline setting. This framework consists of two steps: (a) learning a pessimistic MDP model using the offline dataset; (b) learning a near-optimal policy in the learned pessimistic MDP. The construction of the pessimistic MDP is such that for any policy, the performance in the real environment is lower bounded by the performance in the pessimistic MDP. This enables the pessimistic MDP to serve as a good surrogate for the purposes of policy evaluation and learning. Overall, our framework MOReL is amenable to detailed theoretical analysis, enables easy and transparent design of practical algorithms, and leads to state-of-the-art results on widely used offline benchmark tasks.},
archivePrefix = {arXiv},
arxivId = {2005.05951v1},
author = {Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
eprint = {2005.05951v1},
title = {{MOReL : Model-Based Offline Reinforcement Learning}}
}
@inproceedings{kumar2020cql,
   abstract = {Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a principled policy improvement procedure. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic implementations. On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions.},
   author = {Aviral Kumar and Aurick Zhou and George Tucker and Sergey Levine},
   journal = {Advances in Neural Information Processing Systems (NeurIPS)},
   title = {Conservative Q-Learning for Offline Reinforcement Learning},
   year = {2020},
}

@techreport{Rajeswarana,
abstract = {Model-based reinforcement learning (MBRL) has recently gained immense interest due to its potential for sample efficiency and ability to incorporate off-policy data. However, designing stable and efficient MBRL algorithms using rich function approximators have remained challenging. To help expose the practical challenges in MBRL and simplify algorithm design from the lens of abstraction, we develop a new framework that casts MBRL as a game between: (1) a policy player, which attempts to maximize rewards under the learned model; (2) a model player, which attempts to fit the real-world data collected by the policy player. For algorithm development, we construct a Stackelberg game between the two players, and show that it can be solved with approximate bi-level optimization. This gives rise to two natural families of algorithms for MBRL based on which player is chosen as the leader in the Stackelberg game. Together, they encapsulate, unify, and generalize many previous MBRL algorithms. Furthermore, our framework is consistent with and provides a clear basis for heuristics known to be important in practice from prior works. Finally, through experiments we validate that our proposed algorithms are highly sample efficient, match the asymptotic performance of model-free policy gradient, and scale gracefully to high-dimensional tasks like dexterous hand manipulation.},
archivePrefix = {arXiv},
arxivId = {2004.07804v1},
author = {Rajeswaran, Aravind and Mordatch, Igor and Kumar, Vikash},
eprint = {2004.07804v1},
file = {::},
title = {{A Game Theoretic Framework for Model Based Reinforcement Learning}},
url = {https://sites.google.com/view/mbrl-game.}
}
@techreport{Grill,
abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods intrinsically rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3{\%} top-1 classification accuracy on ImageNet using the standard linear evaluation protocol with a ResNet-50 architecture and 79.6{\%} with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks.},
archivePrefix = {arXiv},
arxivId = {2006.07733v1},
author = {Grill, Jean-Bastien and Strub, Florian and Altch{\'{e}}, Florent and Tallec, Corentin and Richemond, Pierre H and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, R{\'{e}}mi and Valko, Michal},
eprint = {2006.07733v1},
file = {::},
title = {{Bootstrap Your Own Latent A New Approach to Self-Supervised Learning}}
}
@techreport{Sutton,
author = {Sutton, Richard S and Mcallester, David and Singh, Satinder and Mansour, Yishay},
file = {::},
title = {{Policy Gradient Methods for Reinforcement Learning with Function Approximation}}
}


@techreport{Ghosh,
archivePrefix = {arXiv},
arxivId = {2006.11266v2},
author = {Ghosh, Dibya and Machado, Marlos C and {Le Roux}, Nicolas},
eprint = {2006.11266v2},
file = {::},
title = {{An operator view of policy gradient methods}}
}
@techreport{Tang,
abstract = {We propose a graphical model framework for goal-conditioned reinforcement learning (RL), with an expectation maximization (EM) algorithm that operates on the lower bound of the RL objective. The E-step provides a natural interpretation of how 'learning in hindsight' techniques, such as hindsight experience replay (HER), to handle extremely sparse goal-conditioned rewards. The M-step reduces policy optimization to supervised learning updates, which greatly stabilizes end-to-end training on high-dimensional inputs such as images. We show that the combined algorithm, hindsight expectation maximization (hEM) significantly outperforms model-free baselines on a wide range of goal-conditioned benchmarks with sparse rewards.},
archivePrefix = {arXiv},
arxivId = {2006.07549v1},
author = {Tang, Yunhao and Kucukelbir, Alp},
eprint = {2006.07549v1},
file = {::},
title = {{Hindsight Expectation Maximization for Goal-conditioned Reinforcement Learning}}
}
@inproceedings{jiang2016doublyrobust,
abstract = {We study the problem of off-policy value evaluation in reinforcement learning (RL), where one aims to estimate the value of a new policy based on data collected by a different policy. This problem is often a critical step when applying RL to real-world problems. Despite its importance, existing general methods either have uncontrolled bias or suffer high variance. In this work, we extend the doubly robust estimator for bandits to sequential decision-making problems, which gets the best of both worlds: it is guaranteed to be un-biased and can have a much lower variance than the popular importance sampling estimators. We demonstrate the estimator's accuracy in several benchmark problems, and illustrate its use as a subroutine in safe policy improvement. We also provide theoretical results on the inherent hardness of the problem, and show that our estimator can match the lower bound in certain scenarios.},
archivePrefix = {arXiv},
arxivId = {1511.03722v3},
author = {Jiang, Nan and Li, Lihong},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1511.03722v3},
file = {::},
title = {{Doubly Robust Off-policy Value Evaluation for Reinforcement Learning}},
year = {2016}
}
@inproceedings{hallak2015offpolicy,
abstract = {Off-policy learning in dynamic decision problems is essential for providing strong evidence that a new policy is better than the one in use. But how can we prove superiority without testing the new policy? To answer this question, we introduce the G-SCOPE algorithm that evaluates a new policy based on data generated by the existing policy. Our algorithm is both computa-tionally and sample efficient because it greedily learns to exploit factored structure in the dynamics of the environment. We present a finite sample analysis of our approach and show through experiments that the algorithm scales well on high-dimensional problems with few samples.},
author = {Hallak, Assaf and Schnitzler, Francois and Mann, Timothy and Mannor, Shie},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Off-policy Model-based Learning under Unknown Factored Dynamics}},
year = {2015}
}
@inproceedings{hallak2016td,
abstract = {We consider the off-policy evaluation problem in Markov decision processes with function approximation. We propose a generalization of the recently introduced emphatic temporal differences (ETD) algorithm (Sutton, Mahmood, and White, 2015), which encompasses the original ETD($\lambda$), as well as several other off-policy evaluation algorithms as special cases. We call this framework ETD($\lambda$, $\beta$), where our introduced parameter $\beta$ controls the decay rate of an importance-sampling term. We study conditions under which the projected fixed-point equation underlying ETD($\lambda$, $\beta$) involves a contraction operator , allowing us to present the first asymptotic error bounds (bias) for ETD($\lambda$, $\beta$). Our results show that the original ETD algorithm always involves a contraction operator, and its bias is bounded. Moreover, by controlling $\beta$, our proposed generalization allows trading-off bias for variance reduction, thereby achieving a lower total error.},
archivePrefix = {arXiv},
arxivId = {1509.05172v2},
author = {Hallak, Assaf and Tamar, Aviv and Munos, R{\'{e}}mi and Mannor, Shie},
booktitle = {Association for the Advancement of Artificial Intelligence (AAAI)},
eprint = {1509.05172v2},
file = {::},
keywords = {()},
title = {{Generalized Emphatic Temporal Difference Learning: Bias-Variance Analysis}},
year = {2016}
}
@inproceedings{hallak2017onlineoffpolicy,
archivePrefix = {arXiv},
arxivId = {1702.07121v1},
author = {Hallak, Assaf and Mannor, Shie},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1702.07121v1},
file = {::},
title = {{Consistent On-Line Off-Policy Evaluation}},
year = {2017}
}
@techreport{levine2020offlinetutorial,
archivePrefix = {arXiv},
arxivId = {2005.01643v1},
author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
eprint = {2005.01643v1},
file = {::},
year = {2020},
title = {{Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems}}
}
@techreport{Munos2016,
archivePrefix = {arXiv},
arxivId = {1606.02647v2},
author = {Munos, R{\'{e}}mi and Deepmind, Google and Stepleton, Thomas and Harutyunyan, Anna and Bellemare, Marc G},
eprint = {1606.02647v2},
file = {::},
title = {{Safe and efficient off-policy reinforcement learning}},
year = {2016}
}
@article{ramapuram2017lifelonggm,
archivePrefix = {arXiv},
arxivId = {1705.09847},
author = {Ramapuram, Jason and Gregorova, Magda and Kalousis, Alexandros},
eprint = {1705.09847},
file = {::},
journal = {Neurocomputing},
month = {may},
publisher = {Elsevier BV},
title = {{Lifelong Generative Modeling}},
url = {http://arxiv.org/abs/1705.09847},
year = {2017}
}
@techreport{Foerster2018,
archivePrefix = {arXiv},
arxivId = {1802.05098v3},
author = {Foerster, Jakob and Farquhar, Gregory and Al-Shedivat, Maruan and Rockt{\"{a}}schel, Tim and Xing, Eric P and Whiteson, Shimon},
eprint = {1802.05098v3},
file = {::},
title = {{DiCE: The Infinitely Differentiable Monte Carlo Estimator}},
year = {2018}
}
@techreport{Wirth,
author = {Wirth, Christian and Neumann, Gerhard},
file = {::},
title = {{Model-Free Preference-based Reinforcement Learning}},
url = {http://homepages.inf.ed.ac.uk/imurray2/pub/10ess/}
}


@InProceedings{tang2021hem,
  title = 	 { Hindsight Expectation Maximization for Goal-conditioned Reinforcement Learning },
  author =       {Tang, Yunhao and Kucukelbir, Alp},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2863--2871},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/tang21b/tang21b.pdf},
  url = 	 {https://proceedings.mlr.press/v130/tang21b.html},
  abstract = 	 { We propose a graphical model framework for goal-conditioned RL, with an EM algorithm that operates on the lower bound of the RL objective. The E-step provides a natural interpretation of how ’learning in hindsight’ techniques, such as HER, to handle extremely sparse goal-conditioned rewards. The M-step reduces policy optimization to supervised learning updates, which greatly stabilizes end-to-end training on high-dimensional inputs such as images. We show that the combined algorithm, hEM significantly outperforms model-free baselines on a wide range of goal-conditioned benchmarks with sparse rewards. }
}

@techreport{Wang,
author = {Wang, Qing and Xiong, Jiechao and Han, Lei and Sun, Peng and Liu, Han and Zhang, Tong},
file = {::},
title = {{Exponentially Weighted Imitation Learning for Batched Historical Data}}
}
@inproceedings{schaal97lfd,
author = {Schaal, Stefan},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
doi = {10.1016/j.robot.2004.03.001},
file = {::;::},
isbn = {1558604863},
issn = {1049-5258},
keywords = {2-2 Hikaridai,619-02 Kyoto,801 Atlantic Drive,Atlanta,GA 30332-0280 ATR Human Information Processing,Georgia Tech,Seiko-cho,Soraku-gun,http://wwwccgatechedulfac/StefanSchaal College of},
number = {9},
pages = {1040--1046},
pmid = {11540378},
title = {{Learning from demonstration}},
url = {http://www.cc.gatech.edulfac http://wwwiaim.ira.uka.de/users/rogalla/WebOrdnerMaterial/ml-robotlearning.pdf http://www.cc.gatech.edulfac/Stefan.Schaal},
year = {1997}
}
@inproceedings{atkeson1997lfd,
author = {Atkeson, Christopher G and Schaal, Stefan},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Robot Learning From Demonstration}},
url = {http://www.cc.gatech.edu/fac/fChris.},
year = {1997}
}
@techreport{Schroecker,
archivePrefix = {arXiv},
arxivId = {2002.06473v1},
author = {Schroecker, Yannick and Isbell, Charles},
eprint = {2002.06473v1},
file = {::},
title = {{Universal Value Density Estimation for Imitation Learning and Goal-Conditioned Reinforcement Learning}}
}

@inproceedings{devlin2019bert,
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
booktitle = {Association for Compuational Linguistics (ACL)},
eprint = {1810.04805},
file = {::},
month = {oct},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {http://arxiv.org/abs/1810.04805},
year = {2019}
}
@techreport{Dempster1977,
author = {Dempster, A P and Laird, ; N M and Rubin, ; D B},
booktitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
file = {::},
number = {1},
pages = {1--38},
title = {{Maximum Likelihood from Incomplete Data via the EM Algorithm}},
volume = {39},
year = {1977}
}
@techreport{Dayan1996,
author = {Dayan, Peter and Hinton, Geoffrey E},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Dayan, Hinton - 1996 - Using EM for Reinforcement Learning.pdf:pdf},
title = {{Using EM for Reinforcement Learning}},
year = {1996}
}
@techreport{Nachum,
archivePrefix = {arXiv},
arxivId = {1702.08892v3},
author = {Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Brain, Google},
eprint = {1702.08892v3},
file = {::},
title = {{Bridging the Gap Between Value and Policy Based Reinforcement Learning}},
url = {https://github.com/tensorflow/models/tree/}
}
@techreport{Nachum2018,
archivePrefix = {arXiv},
arxivId = {1803.02348v3},
author = {Nachum, Ofir and Norouzi, Mohammad and Tucker, George and Schuurmans, Dale},
eprint = {1803.02348v3},
file = {::},
title = {{Smoothed Action Value Functions for Learning Gaussian Policies}},
year = {2018}
}
@techreport{Lee,
abstract = {Exploration is critical to a reinforcement learning agent's performance in its given environment. Prior exploration methods are often based on using heuristic auxiliary predictions to guide policy behavior, lacking a mathematically-grounded objective with clear properties. In contrast, we recast exploration as a problem of State Marginal Matching (SMM), where we aim to learn a policy for which the state marginal distribution matches a given target state distribution. The target distribution is a uniform distribution in most cases, but can incorporate prior knowledge if available. In effect, SMM amortizes the cost of learning to explore in a given environment. The SMM objective can be viewed as a two-player, zero-sum game between a state density model and a para-metric policy, an idea that we use to build an algorithm for optimizing the SMM objective. Using this formalism, we further demonstrate that prior work approximately maximizes the SMM objective , offering an explanation for the success of these methods. On both simulated and real-world tasks, we demonstrate that agents that directly optimize the SMM objective explore faster and adapt more quickly to new tasks as compared to prior exploration methods. 1},
archivePrefix = {arXiv},
arxivId = {1906.05274v3},
author = {Lee, Lisa and Eysenbach, Benjamin and Parisotto, Emilio and Xing, Eric and Levine, Sergey and Salakhutdinov, Ruslan},
eprint = {1906.05274v3},
file = {::},
isbn = {1906.05274v3},
title = {{Efficient Exploration via State Marginal Matching}},
url = {https://sites.google.com/}
}
@inproceedings{ding2019gcil,
abstract = {Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might require many samples to discover how to reach certain areas of the state-space. In this work we propose a novel algorithm goalGAIL, which incorporates demonstrations to drastically speed up the convergence to a policy able to reach any goal, surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, we show our method can also be used when the available expert trajectories do not contain the actions or when the expert is suboptimal, which makes it applicable when only kinesthetic, third-person or noisy demonstrations are available. Our code is open-source 2 .},
author = {Ding, Yiming and Florensa, Carlos and Phielipp, Mariano and Abbeel, Pieter},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {::},
title = {{Goal-conditioned Imitation Learning}},
url = {https://sites.google.com/view/goalconditioned-il/},
year = {2019}
}
@inproceedings{zhang2020gendice,
abstract = {An important problem that arises in reinforcement learning and Monte Carlo methods is estimating quantities defined by the stationary distribution of a Markov chain. In many real-world applications, access to the underlying transition operator is limited to a fixed set of data that has already been collected, without additional interaction with the environment being available. We show that consistent estimation remains possible in this challenging scenario, and that effective estimation can still be achieved in important applications. Our approach is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, derived from fundamental properties of the stationary distribution, and exploiting constraint reformulations based on variational divergence minimization. The resulting algorithm, GenDICE, is straightforward and effective. We prove its consistency under general conditions, provide an error analysis, and demonstrate strong empirical performance on benchmark problems, including off-line PageRank and off-policy policy evaluation.},
archivePrefix = {arXiv},
arxivId = {2002.09072},
author = {Zhang, Ruiyi and Dai, Bo and Li, Lihong and Schuurmans, Dale},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {2002.09072},
file = {::},
month = {feb},
title = {{GenDICE: Generalized Offline Estimation of Stationary Values}},
url = {http://arxiv.org/abs/2002.09072},
year = {2020}
}
@inproceedings{wu2019brac,
abstract = {In reinforcement learning (RL) research, it is common to assume access to direct online interactions with the environment. However in many real-world applications, access to the environment is limited to a fixed offline dataset of logged experience. In such settings, standard RL algorithms have been shown to diverge or otherwise yield poor performance. Accordingly, recent work has suggested a number of remedies to these issues. In this work, we introduce a general framework, behavior regularized actor critic (BRAC), to empirically evaluate recently proposed methods as well as a number of simple baselines across a variety of offline continuous control tasks. Surprisingly, we find that many of the technical complexities introduced in recent methods are unnecessary to achieve strong performance. Additional ablations provide insights into which design choices matter most in the offline RL setting.},
archivePrefix = {arXiv},
arxivId = {1911.11361},
author = {Wu, Yifan and Tucker, George and Nachum, Ofir},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {1911.11361},
file = {::},
month = {nov},
title = {{Behavior Regularized Offline Reinforcement Learning}},
url = {http://arxiv.org/abs/1911.11361},
year = {2020}
}
@inproceedings{nachum2019dualdice,
abstract = {In many real-world reinforcement learning applications, access to the environment is limited to a fixed dataset, instead of direct (online) interaction with the environment. When using this data for either evaluation or training of a new policy, accurate estimates of discounted stationary distribution ratios -- correction terms which quantify the likelihood that the new policy will experience a certain state-action pair normalized by the probability with which the state-action pair appears in the dataset -- can improve accuracy and performance. In this work, we propose an algorithm, DualDICE, for estimating these quantities. In contrast to previous approaches, our algorithm is agnostic to knowledge of the behavior policy (or policies) used to generate the dataset. Furthermore, it eschews any direct use of importance weights, thus avoiding potential optimization instabilities endemic of previous methods. In addition to providing theoretical guarantees, we present an empirical study of our algorithm applied to off-policy policy evaluation and find that our algorithm significantly improves accuracy compared to existing techniques.},
archivePrefix = {arXiv},
arxivId = {1906.04733},
author = {Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {1906.04733},
file = {::},
month = {jun},
title = {{DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections}},
url = {http://arxiv.org/abs/1906.04733},
year = {2019}
}
@inproceedings{degris2012,
archivePrefix = {arXiv},
arxivId = {1205.4839},
author = {Degris, Thomas and White, Martha and Sutton, Richard S.},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1205.4839},
file = {::},
keywords = {3D printing,biped locomotion,humanoid robotics,morphological computation,physical human-robot interaction},
month = {may},
title = {{Off-Policy Actor-Critic}},
url = {http://arxiv.org/abs/1205.4839},
year = {2012}
}
@techreport{Zhu,
abstract = {A crucial capability of real-world intelligent agents is their ability to plan a sequence of actions to achieve their goals in the visual world. In this work, we address the problem of visual semantic planning: the task of predicting a sequence of actions from visual observations that transform a dynamic environment from an initial state to a goal state. Doing so entails knowledge about objects and their affor-dances, as well as actions and their preconditions and effects. We propose learning these through interacting with a visual and dynamic environment. Our proposed solution involves bootstrapping reinforcement learning with imitation learning. To ensure cross task generalization, we develop a deep predictive model based on successor representations. Our experimental results show near optimal results across a wide range of tasks in the challenging THOR environment.},
archivePrefix = {arXiv},
arxivId = {1705.08080v2},
author = {Zhu, Yuke and Gordon, Daniel and Kolve, Eric and Fox, Dieter and Fei-Fei, Li and Gupta, Abhinav and Mottaghi, Roozbeh and Farhadi, Ali},
eprint = {1705.08080v2},
file = {::},
title = {{Visual Semantic Planning using Deep Successor Representations}}
}
@article{Burda2015,
abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
archivePrefix = {arXiv},
arxivId = {1509.00519},
author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
eprint = {1509.00519},
file = {::},
journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
month = {sep},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Importance Weighted Autoencoders}},
url = {http://arxiv.org/abs/1509.00519},
year = {2015}
}
@inproceedings{Hershey,
abstract = {The Kullback Leibler (KL) Divergence is a widely used tool in statistics and pattern recognition. The KL divergence between two Gaus-sian Mixture Models (GMMs) is frequently needed in the fields of speech and image recognition. Unfortunately the KL divergence between two GMMs is not analytically tractable, nor does any efficient computational algorithm exist. Some techniques cope with this problem by replacing the KL divergence with other functions that can be computed efficiently. We introduce two new methods, the variational approximation and the variational upper bound, and compare them to existing methods. We discuss seven different techniques in total and weigh the benefits of each one against the others. To conclude we evaluate the performance of each one through numerical experiments. Index Terms-Kullback Leibler divergence, variational methods , gaussian mixture models, unscented transformation.},
author = {Hershey, John R and Olsen, Peder A},
title = {{APPROXIMATING THE KULLBACK LEIBLER DIVERGENCE BETWEEN GAUSSIAN MIXTURE MODELS}}
}
@techreport{levine2018tutorial,
abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
archivePrefix = {arXiv},
arxivId = {1805.00909v3},
author = {Levine, Sergey},
eprint = {1805.00909v3},
file = {::},
title = {{Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review}},
year = {2018}
}
@techreport{degrave2019quinoa,
abstract = {We present an algorithm for learning an approximate action-value soft Q-function in the relative entropy regularised reinforcement learning setting, for which an optimal improved policy can be recovered in closed form. We use recent advances in normalising flows for parametrising the policy together with a learned value-function; and show how this combination can be used to implicitly represent Q-values of an arbitrary policy in continuous action space. Using simple temporal difference learning on the Q-values then leads to a unified objective for policy and value learning. We show how this approach considerably simplifies standard Actor-Critic off-policy algorithms, removing the need for a policy optimisation step. We perform experiments on a range of established reinforcement learning benchmarks, demonstrating that our approach allows for complex, multimodal policy distributions in continuous action spaces, while keeping the process of sampling from the policy both fast and exact.},
archivePrefix = {arXiv},
arxivId = {1911.01831v1},
author = {Degrave, Jonas and Abdolmaleki, Abbas and Springenberg, Jost Tobias and Heess, Nicolas and Riedmiller, Martin},
eprint = {1911.01831v1},
file = {::},
pages = {2018--2030},
title = {{Quinoa: a Q-function You Infer Normalized Over Actions}}
}
@inproceedings{gu2017qp,
abstract = {Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.},
archivePrefix = {arXiv},
arxivId = {1611.02247v3},
author = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E and Levine, Sergey},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {1611.02247v3},
file = {::},
title = {{Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic}},
year = {2017}
}
@inproceedings{zhu2017cyclegan,
abstract = {Monet photo photo Monet Figure 1: Given any two unordered image collections X and Y , our algorithm learns to automatically "translate" an image from one into the other and vice versa: (left) Monet paintings and landscape photos from Flickr; (center) zebras and horses from ImageNet; (right) summer and winter Yosemite photos from Flickr. Example application (bottom): using a collection of paintings of famous artists, our method learns to render natural photographs into the respective styles. Abstract Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to enforce F (G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593v6},
author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A and Research, Berkeley Ai},
booktitle = {International Conference on Computer Vision (ICCV)},
eprint = {1703.10593v6},
file = {::},
title = {{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Monet Photos}},
year = {2017}
}
@inproceedings{eysenbach2020rewriting,
	title = {Rewriting {History} with {Inverse} {RL}: {Hindsight} {Inference} for {Policy} {Improvement}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/a97da629b098b75c294dffdc3e463904-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Eysenbach, Ben and Geng, Xinyang and Levine, Sergey and Salakhutdinov, Russ R},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {14783--14795},
}


@inproceedings{smith2020avid,
archivePrefix = {arXiv},
arxivId = {1912.04443v1},
author = {Smith, Laura and Dhawan, Nikita and Zhang, Marvin and Abbeel, Pieter and Levine, Sergey},
eprint = {1912.04443v1},
file = {::},
title = {{AVID: Learning Multi-Stage Tasks via Pixel-Level Translation of Human Videos}},
url = {https://sites.google.com/view/icra20avid}
}
@inproceedings{siegel2020keepdoing,
archivePrefix = {arXiv},
arxivId = {2002.08396},
author = {Siegel, Noah Y. and Springenberg, Jost Tobias and Berkenkamp, Felix and Abdolmaleki, Abbas and Neunert, Michael and Lampe, Thomas and Hafner, Roland and Heess, Nicolas and Riedmiller, Martin},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {2002.08396},
file = {::},
month = {feb},
title = {{Keep Doing What Worked: Behavioral Modelling Priors for Offline Reinforcement Learning}},
url = {http://arxiv.org/abs/2002.08396},
year = {2020}
}
@inproceedings{zhu2019hands,
abstract = {Dexterous multi-fingered robotic hands can perform a wide range of manipulation skills, making them an appealing component for general-purpose robotic manipulators. However, such hands pose a major challenge for autonomous control, due to the high dimensionality of their configuration space and complex intermittent contact interactions. In this work, we propose deep reinforcement learning (deep RL) as a scalable solution for learning complex, contact rich behaviors with multi-fingered hands. Deep RL provides an end-to-end approach to directly map sensor readings to actions, without the need for task specific models or policy classes. We show that contact-rich manipulation behavior with multi-fingered hands can be learned by directly training with model-free deep RL algorithms in the real world, with minimal additional assumption and without the aid of simulation. We learn a variety of complex behaviors on two different low-cost hardware platforms. We show that each task can be learned entirely from scratch, and further study how the learning process can be further accelerated by using a small number of human demonstrations to bootstrap learning. Our experiments demonstrate that complex multi-fingered manipulation skills can be learned in the real world in about 4-7 hours for most tasks, and that demonstrations can decrease this to 2-3 hours, indicating that direct deep RL training in the real world is a viable and practical alternative to simulation and model-based control. $\backslash$url{\{}https://sites.google.com/view/deeprl-handmanipulation{\}}},
archivePrefix = {arXiv},
arxivId = {1810.06045},
author = {Zhu, Henry and Gupta, Abhishek and Rajeswaran, Aravind and Levine, Sergey and Kumar, Vikash},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
eprint = {1810.06045},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Zhu et al. - 2018 - Dexterous Manipulation with Deep Reinforcement Learning Efficient, General, and Low-Cost.pdf:pdf;::},
month = {oct},
pages = {3651--3657},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Dexterous Manipulation with Deep Reinforcement Learning: Efficient, General, and Low-Cost}},
url = {http://arxiv.org/abs/1810.06045},
volume = {2019-May},
year = {2019}
}
@inproceedings{neumann2008fqiawr,
abstract = {Recently, fitted Q-iteration (FQI) based methods have become more popular due to their increased sample efficiency, a more stable learning process and the higher quality of the resulting policy. However, these methods remain hard to use for continuous action spaces which frequently occur in real-world tasks, e.g., in robotics and other technical applications. The greedy action selection commonly used for the policy improvement step is particularly problematic as it is expensive for continuous actions, can cause an unstable learning process, introduces an optimization bias and results in highly non-smooth policies unsuitable for real-world systems. In this paper, we show that by using a soft-greedy action selection the policy improvement step used in FQI can be simplified to an inexpensive advantage-weighted regression. With this result, we are able to derive a new, computationally efficient FQI algorithm which can even deal with high dimensional action spaces.},
author = {Neumann, Gerhard and Peters, Jan},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {::},
title = {{Fitted Q-iteration by Advantage Weighted Regression}},
year = {2008}
}
@inproceedings{gupta2019relay,
abstract = {We present relay policy learning, a method for imitation and reinforcement learning that can solve multi-stage, long-horizon robotic tasks. This general and universally-applicable, two-phase approach consists of an imitation learning stage that produces goal-conditioned hierarchical policies, and a reinforcement learning phase that finetunes these policies for task performance. Our method, while not necessarily perfect at imitation learning, is very amenable to further improvement via environment interaction, allowing it to scale to challenging long-horizon tasks. We simplify the long-horizon policy learning problem by using a novel data-relabeling algorithm for learning goal-conditioned hierarchical policies, where the low-level only acts for a fixed number of steps, regardless of the goal achieved. While we rely on demonstration data to bootstrap policy learning, we do not assume access to demonstrations of every specific tasks that is being solved, and instead leverage unstructured and unsegmented demonstrations of semantically meaningful behaviors that are not only less burdensome to provide, but also can greatly facilitate further improvement using reinforcement learning. We demonstrate the effectiveness of our method on a number of multi-stage, long-horizon manipulation tasks in a challenging kitchen simulation environment. Videos are available at https://relay-policy-learning.github.io/},
archivePrefix = {arXiv},
arxivId = {1910.11956},
author = {Gupta, Abhishek and Kumar, Vikash and Lynch, Corey and Levine, Sergey and Hausman, Karol},
booktitle = {Conference on Robot Learning (CoRL)},
eprint = {1910.11956},
file = {::},
month = {oct},
title = {{Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning}},
url = {http://arxiv.org/abs/1910.11956},
year = {2019}
}
@article{fu2020d4rl,
abstract = {The offline reinforcement learning (RL) problem, also referred to as batch RL, refers to the setting where a policy must be learned from a dataset of previously collected data, without additional online data collection. In supervised learning, large datasets and complex deep neural networks have fueled impressive progress, but in contrast, conventional RL algorithms must collect large amounts of on-policy data and have had little success leveraging previously collected datasets. As a result, existing RL benchmarks are not well-suited for the offline setting, making progress in this area difficult to measure. To design a benchmark tailored to offline RL, we start by outlining key properties of datasets relevant to applications of offline RL. Based on these properties, we design a set of benchmark tasks and datasets that evaluate offline RL algorithms under these conditions. Examples of such properties include: datasets generated via hand-designed controllers and human demonstrators, multi-objective datasets, where an agent can perform different tasks in the same environment, and datasets consisting of a heterogeneous mix of high-quality and low-quality trajectories. By designing the benchmark tasks and datasets to reflect properties of real-world offline RL problems, our benchmark will focus research effort on methods that drive substantial improvements not just on simulated benchmarks, but ultimately on the kinds of real-world problems where offline RL will have the largest impact.},
archivePrefix = {arXiv},
arxivId = {2004.07219},
author = {Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
eprint = {2004.07219},
file = {::},
month = {apr},
title = {{D4RL: Datasets for Deep Data-Driven Reinforcement Learning}},
url = {http://arxiv.org/abs/2004.07219},
year = {2020}
}
@inproceedings{konda2000actorcritic,
abstract = {We propose and analyze a class of actor-critic algorithms for simulation-based optimization of a Markov decision process over a parameterized family of randomized stationary policies. These are two-timescale algorithms in which the critic uses TD learning with a linear approximation architecture and the actor is updated in an approximate gradient direction based on information provided by the critic. We show that the features for the critic should span a subspace prescribed by the choice of parameterization of the actor. We conclude by discussing convergence properties and some open problems.},
author = {Konda, Vijay R and Tsitsiklis, John N},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {::},
title = {{Actor-Critic Algorithms}},
year = {2000}
}
@inproceedings{chebotar2019simtoreal,
abstract = {Fig. 1. Policies for opening a cabinet drawer and swing-peg-in-hole tasks trained by alternatively performing reinforcement learning with multiple agents in simulation and updating simulation parameter distribution using a few real world policy executions. Abstract-We consider the problem of transferring policies to the real world by training on a distribution of simulated scenarios. Rather than manually tuning the randomization of simulations, we adapt the simulation parameter distribution using a few real world roll-outs interleaved with policy training. In doing so, we are able to change the distribution of simulations to improve the policy transfer by matching the policy behavior in simulation and the real world. We show that policies trained with our method are able to reliably transfer to different robots in two real world tasks: swing-peg-in-hole and opening a cabinet drawer. The video of our experiments can be found at https: //sites.google.com/view/simopt.},
archivePrefix = {arXiv},
arxivId = {1810.05687v4},
author = {Chebotar, Yevgen and Handa, Ankur and Makoviychuk, Viktor and Macklin, Miles and Issac, Jan and Ratliff, Nathan and Fox, Dieter},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
eprint = {1810.05687v4},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Chebotar et al. - 2019 - Closing the Sim-to-Real Loop Adapting Simulation Randomization with Real World Experience.pdf:pdf;::},
title = {{Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience}},
url = {https://arxiv.org/pdf/1810.05687.pdf},
year = {2019}
}
@article{ke2019fdivergence,
abstract = {We address the problem of imitation learning with multi-modal demonstrations. Instead of attempting to learn all modes, we argue that in many tasks it is sufficient to imitate any one of them. We show that the state-of-the-art methods such as GAIL and behavior cloning, due to their choice of loss function, often incorrectly interpolate between such modes. Our key insight is to minimize the right divergence between the learner and the expert state-action distributions, namely the reverse KL divergence or I-projection. We propose a general imitation learning framework for estimating and minimizing any f-Divergence. By plugging in different divergences, we are able to recover existing algorithms such as Behavior Cloning (Kullback-Leibler), GAIL (Jensen Shannon) and Dagger (Total Variation). Empirical results show that our approximate I-projection technique is able to imitate multi-modal behaviors more reliably than GAIL and behavior cloning.},
archivePrefix = {arXiv},
arxivId = {1905.12888},
author = {Ke, Liyiming and Barnes, Matt and Sun, Wen and Lee, Gilwoo and Choudhury, Sanjiban and Srinivasa, Siddhartha},
eprint = {1905.12888},
file = {::},
month = {may},
title = {{Imitation Learning as {\$}f{\$}-Divergence Minimization}},
url = {http://arxiv.org/abs/1905.12888},
year = {2019}
}
@techreport{Gordon1995,
author = {Gordon, Geoffrey},
file = {::},
institution = {Carnegie Mellon University},
title = {{Stable Function Approximation in Dynamic Programming}},
year = {1995}
}
@article{James2018,
abstract = {Real world data, especially in the domain of robotics, is notoriously costly to collect. One way to circumvent this can be to leverage the power of simulation to produce large amounts of labelled data. However, training models on simulated images does not readily transfer to real-world ones. Using domain adaptation methods to cross this "reality gap" requires a large amount of unlabelled real-world data, whilst domain randomization alone can waste modeling power. In this paper, we present Randomized-to-Canonical Adaptation Networks (RCANs), a novel approach to crossing the visual reality gap that uses no real-world data. Our method learns to translate randomized rendered images into their equivalent non-randomized, canonical versions. This in turn allows for real images to also be translated into canonical sim images. We demonstrate the effectiveness of this sim-to-real approach by training a vision-based closed-loop grasping reinforcement learning agent in simulation, and then transferring it to the real world to attain 70{\%} zero-shot grasp success on unseen objects, a result that almost doubles the success of learning the same task directly on domain randomization alone. Additionally, by joint finetuning in the real-world with only 5,000 real-world grasps, our method achieves 91{\%}, attaining comparable performance to a state-of-the-art system trained with 580,000 real-world grasps, resulting in a reduction of real-world data by more than 99{\%}.},
archivePrefix = {arXiv},
arxivId = {1812.07252},
author = {James, Stephen and Wohlhart, Paul and Kalakrishnan, Mrinal and Kalashnikov, Dmitry and Irpan, Alex and Ibarz, Julian and Levine, Sergey and Hadsell, Raia and Bousmalis, Konstantinos},
eprint = {1812.07252},
file = {::},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Deep Learning,Robotics + Driving},
month = {dec},
pages = {12619--12629},
publisher = {IEEE Computer Society},
title = {{Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks}},
url = {http://arxiv.org/abs/1812.07252},
volume = {2019-June},
year = {2018}
}
@inproceedings{mehta2019adr,
abstract = {Domain randomization is a popular technique for improving domain transfer, often used in a zero-shot setting when the target domain is unknown or cannot easily be used for training. In this work, we empirically examine the effects of domain randomization on agent generalization. Our experiments show that domain randomization may lead to suboptimal, high-variance policies, which we attribute to the uniform sampling of environment parameters. We propose Active Domain Randomization, a novel algorithm that learns a parameter sampling strategy. Our method looks for the most informative environment variations within the given randomization ranges by leveraging the discrepancies of policy rollouts in randomized and reference environment instances. We find that training more frequently on these instances leads to better overall agent generalization. Our experiments across various physics-based simulated and real-robot tasks show that this enhancement leads to more robust, consistent policies.},
archivePrefix = {arXiv},
arxivId = {1904.04762},
author = {Mehta, Bhairav and Diaz, Manfred and Golemo, Florian and Pal, Christopher J. and Paull, Liam},
booktitle = {Conference on Robot Learning (CoRL)},
eprint = {1904.04762},
file = {::},
month = {apr},
title = {{Active Domain Randomization}},
url = {http://arxiv.org/abs/1904.04762},
year = {2019}
}
@techreport{Arndt,
abstract = {Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance.},
archivePrefix = {arXiv},
arxivId = {1909.12906v1},
author = {Arndt, Karol and Hazara, Murtaza and Ghadirzadeh, Ali and Kyrki, Ville},
eprint = {1909.12906v1},
file = {::},
title = {{Meta Reinforcement Learning for Sim-to-real Domain Adaptation}}
}
@inproceedings{Song,
abstract = {Learning adaptable policies is crucial for robots to operate autonomously in our complex and quickly changing world. In this work, we present a new meta-learning method that allows robots to quickly adapt to changes in dynamics. In contrast to gradient-based meta-learning algorithms that rely on second-order gradient estimation, we introduce a more noise-tolerant Batch Hill-Climbing adaptation operator and combine it with meta-learning based on evolutionary strategies. Our method significantly improves adaptation to changes in dynamics in high noise settings, which are common in robotics applications. We validate our approach on a quadruped robot that learns to walk while subject to changes in dynamics. We observe that our method significantly outperforms prior gradient-based approaches, enabling the robot to adapt its policy to changes based on less than 3 minutes of real data.},
archivePrefix = {arXiv},
arxivId = {2003.01239v1},
author = {Song, Xingyou and Yang, Yuxiang and Choromanski, Krzysztof and Caluwaerts, Ken and Gao, Wenbo and Finn, Chelsea and Tan, Jie},
eprint = {2003.01239v1},
file = {::},
title = {{Rapidly Adaptable Legged Robots via Evolutionary Meta-Learning}},
url = {http://g.co/}
}
@inproceedings{bharadhwaj2019simtoreal,
abstract = {Learning effective visuomotor policies for robots purely from data is challenging, but also appealing since a learning-based system should not require manual tuning or calibration. In the case of a robot operating in a real environment the training process can be costly, time-consuming, and even dangerous since failures are common at the start of training. For this reason, it is desirable to be able to leverage simulation and off-policy data to the extent possible to train the robot. In this work, we introduce a robust framework that plans in simulation and transfers well to the real environment. Our model incorporates a gradient-descent based planning module, which, given the initial image and goal image, encodes the images to a lower dimensional latent state and plans a trajectory to reach the goal. The model, consisting of the encoder and planner modules, is first trained through a meta-learning strategy in simulation. We subsequently perform adversarial domain transfer on the encoder by using a bank of unlabelled but random images from the simulation and real environments to enable the encoder to map images from the real and simulated environments to a similarly distributed latent representation. By fine tuning the entire model (encoder + planner) with only a few real world expert demonstrations, we show successful planning performances in different navigation tasks.},
author = {Bharadhwaj, Homanga and Wang, Zihan and Bengio, Yoshua and Paull, Liam},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2019.8794310},
isbn = {9781538660263},
issn = {10504729},
month = {may},
pages = {782--788},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A data-efficient framework for training and sim-to-real transfer of navigation policies}},
volume = {2019-May},
year = {2019}
}
@article{openai2019cube,
abstract = {We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: https://openai.com/blog/solving-rubiks-cube/},
archivePrefix = {arXiv},
arxivId = {1910.07113},
author = {OpenAI and Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and Schneider, Jonas and Tezak, Nikolas and Tworek, Jerry and Welinder, Peter and Weng, Lilian and Yuan, Qiming and Zaremba, Wojciech and Zhang, Lei},
eprint = {1910.07113},
file = {::},
month = {oct},
title = {{Solving Rubik's Cube with a Robot Hand}},
url = {http://arxiv.org/abs/1910.07113},
year = {2019}
}
@inproceedings{ramos2019bayessim,
abstract = {We introduce BayesSim 1 , a framework for robotics simulations allowing a full Bayesian treatment for the parameters of the simulator. As simulators become more sophisticated and able to represent the dynamics more accurately, fundamental problems in robotics such as motion planning and perception can be solved in simulation and solutions transferred to the physical robot. However, even the most complex simulator might still not be able to represent reality in all its details either due to inaccurate parametrization or simplistic assumptions in the dynamic models. BayesSim provides a principled framework to reason about the uncertainty of simulation parameters. Given a black box simulator (or generative model) that outputs trajectories of state and action pairs from unknown simulation parameters, followed by trajectories obtained with a physical robot, we develop a likelihood-free inference method that computes the posterior distribution of simulation parameters. This posterior can then be used in problems where Sim2Real is critical, for example in policy search. We compare the performance of BayesSim in obtaining accurate posteriors in a number of classical control and robotics problems. Results show that the posterior computed from BayesSim can be used for domain randomization outperforming alternative methods that randomize based on uniform priors.},
archivePrefix = {arXiv},
arxivId = {1906.01728v1},
author = {Ramos, Fabio and {Carvalhaes Possas}, Rafael and Fox, Dieter},
booktitle = {Robotics: Science and Systems (RSS)},
eprint = {1906.01728v1},
file = {::},
title = {{BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators}},
url = {https://github.com/rafaelpossas/bayes{\_}sim},
year = {2019}
}
@article{marvel2018insertionsearch,
author = {Marvel, Jeremy A and Bostelman, Roger and Falco, Joe},
journal = {ACM computing surveys},
number = {1},
pages = {14},
title = {{Multi-Robot Assembly Strategies and Metrics}},
volume = {51},
year = {2018}
}
@article{schoettler2019insertion,
abstract = {Connector insertion and many other tasks commonly found in modern manufacturing settings involve complex contact dynamics and friction. Since it is difficult to capture related physical effects with first-order modeling, traditional control methods often result in brittle and inaccurate controllers, which have to be manually tuned. Reinforcement learning (RL) methods have been demonstrated to be capable of learning controllers in such environments from autonomous interaction with the environment, but running RL algorithms in the real world poses sample efficiency and safety challenges. Moreover, in practical real-world settings we cannot assume access to perfect state information or dense reward signals. In this paper, we consider a variety of difficult industrial insertion tasks with visual inputs and different natural reward specifications, namely sparse rewards and goal images. We show that methods that combine RL with prior information, such as classical controllers or demonstrations, can solve these tasks from a reasonable amount of real-world interaction.},
archivePrefix = {arXiv},
arxivId = {1906.05841v2},
author = {Schoettler, Gerrit and Nair, Ashvin and Luo, Jianlan and Bahl, Shikhar and {Aparicio Ojea}, Juan and Solowjow, Eugen and Levine, Sergey},
eprint = {1906.05841v2},
file = {::},
journal = {arXiv preprint arXiv: 1906.05841},
title = {{Deep Reinforcement Learning for Industrial Insertion Tasks with Visual Inputs and Natural Rewards}},
year = {2019}
}
@inproceedings{rawlik2012rss,
abstract = {We present a reformulation of the stochastic optimal control problem in terms of KL divergence minimisation, not only providing a unifying perspective of previous approaches in this area, but also demonstrating that the formalism leads to novel practical approaches to the control problem. Specifically, a natural relaxation of the dual formulation gives rise to exact iterative solutions to the finite and infinite horizon stochastic optimal control problem, while direct application of Bayesian inference methods yields instances of risk sensitive control. We furthermore study corresponding formulations in the reinforcement learning setting and present model free algorithms for problems with both discrete and continuous state and action spaces. Evaluation of the proposed methods on the standard Gridworld and Cart-Pole benchmarks verifies the theoretical insights and shows that the proposed methods improve upon current approaches.},
author = {Rawlik, Konrad and Toussaint, Marc and Vijayakumar, Sethu},
booktitle = {Robotics: Science and Systems (RSS)},
file = {::},
title = {{On Stochastic Optimal Control and Reinforcement Learning by Approximate Inference}},
url = {http://arxiv.org/abs/1009.3958},
year = {2012}
}
@article{williams1992reinforce,
author = {Williams, Ronald J},
file = {::},
journal = {Machine Learning},
pages = {229--256},
title = {{Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning}},
year = {1992}
}
@article{agarwal2018simplicity,
abstract = {This paper advocates the use of offline (batch) reinforcement learning (RL) to help (1) isolate the contributions of exploitation vs. exploration in off-policy deep RL, (2) improve reproducibility of deep RL research, and (3) facilitate the design of simpler deep RL algorithms. We propose an offline RL benchmark on Atari 2600 games comprising all of the replay data of a DQN agent. Using this benchmark, we demonstrate that recent off-policy deep RL algorithms, even when trained solely on logged DQN data, can outperform online DQN. We present Random Ensemble Mixture (REM), a simple Q-learning algorithm that enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. The REM algorithm outperforms more complex RL agents such as C51 and QR-DQN on the offline Atari benchmark and performs comparably in the online setting.},
archivePrefix = {arXiv},
arxivId = {1907.04543v2},
author = {Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
eprint = {1907.04543v2},
file = {::},
title = {{Striving for Simplicity in Off-Policy Deep Reinforcement Learning}}
}
@inproceedings{dasari2019robonet,
abstract = {Robot learning has emerged as a promising tool for taming the complexity and diversity of the real world. Methods based on high-capacity models, such as deep networks, hold the promise of providing effective generalization to a wide range of open-world environments. However, these same methods typically require large amounts of diverse training data to generalize effectively. In contrast, most robotic learning experiments are small-scale, single-domain, and single-robot. This leads to a frequent tension in robotic learning: how can we learn generalizable robotic controllers without having to collect impractically large amounts of data for each separate experiment? In this paper, we propose RoboNet, an open database for sharing robotic experience, which provides an initial pool of 15 million video frames, from 7 different robot platforms, and study how it can be used to learn generalizable models for vision-based robotic manipulation. We combine the dataset with two different learning algorithms: visual foresight, which uses forward video prediction models, and supervised inverse models. Our experiments test the learned algorithms' ability to work across new objects, new tasks, new scenes, new camera viewpoints, new grippers, or even entirely new robots. In our final experiment, we find that by pre-training on RoboNet and fine-tuning on data from a held-out Franka or Kuka robot, we can exceed the performance of a robot-specific training approach that uses 4x-20x more data. For videos and data, see the project webpage: https://www.robonet.wiki/},
archivePrefix = {arXiv},
arxivId = {1910.11215},
author = {Dasari, Sudeep and Ebert, Frederik and Tian, Stephen and Nair, Suraj and Bucher, Bernadette and Schmeckpeper, Karl and Singh, Siddharth and Levine, Sergey and Finn, Chelsea},
booktitle = {Conference on Robot Learning (CoRL)},
eprint = {1910.11215},
file = {::},
month = {oct},
title = {{RoboNet: Large-Scale Multi-Robot Learning}},
url = {http://arxiv.org/abs/1910.11215},
year = {2019}
}
@article{Torralba2008,
abstract = {With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 × 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset, such as people, we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors. {\textcopyright} 2008 IEEE.},
author = {Torralba, Antonio and Fergus, Rob and Freeman, William T.},
doi = {10.1109/TPAMI.2008.128},
file = {::},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Internet images,Large data sets,Nearest neighbor methods,Object recognition,Tiny images},
number = {11},
pages = {1958--1970},
pmid = {18787244},
title = {{80 million tiny images: A large data set for nonparametric object and scene recognition}},
volume = {30},
year = {2008}
}
@inproceedings{andrychowicz2016metalearning,
abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
archivePrefix = {arXiv},
arxivId = {1606.04474},
author = {Andrychowicz, Marcin and Denil, Misha and Colmenarejo, Sergio G{\'{o}}mez and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and {De Freitas}, Nando},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {1606.04474},
issn = {10495258},
pages = {3988--3996},
publisher = {Neural information processing systems foundation},
title = {{Learning to learn by gradient descent by gradient descent}},
year = {2016}
}
@article{duan2016rl2,
abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL{\$}{\^{}}2{\$}, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL{\$}{\^{}}2{\$} experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL{\$}{\^{}}2{\$} is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL{\$}{\^{}}2{\$} on a vision-based navigation task and show that it scales up to high-dimensional problems.},
archivePrefix = {arXiv},
arxivId = {1611.02779},
author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
eprint = {1611.02779},
month = {nov},
title = {{RL{\$}{\^{}}2{\$}: Fast Reinforcement Learning via Slow Reinforcement Learning}},
url = {http://arxiv.org/abs/1611.02779},
year = {2016}
}
@inproceedings{fujimoto19bcq,
abstract = {Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks.},
archivePrefix = {arXiv},
arxivId = {1812.02900},
author = {Fujimoto, Scott and Meger, David and Precup, Doina},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1812.02900},
month = {dec},
title = {{Off-Policy Deep Reinforcement Learning without Exploration}},
url = {http://arxiv.org/abs/1812.02900},
year = {2019}
}
@article{oord2018cpc,
abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
archivePrefix = {arXiv},
arxivId = {1807.03748},
author = {{van den Oord}, Aaron and {Li}, Yazhe and {Vinyals}, Oriol},
eprint = {1807.03748},
file = {:Users/ashvin/Documents/research/2019/nips{\_}reading{\_}list/CPC.pdf:pdf;::},
title = {{Representation Learning with Contrastive Predictive Coding}},
url = {http://arxiv.org/abs/1807.03748 https://arxiv.org/pdf/1807.03748.pdf},
year = {2018}
}
@inproceedings{kumar19bear,
abstract = {Off-policy reinforcement learning aims to leverage experience collected from prior policies for sample-efficient learning. However, in practice, commonly used off-policy approximate dynamic programming methods based on Q-learning and actor-critic methods are highly sensitive to the data distribution, and can make only limited progress without collecting additional on-policy data. As a step towards more robust off-policy algorithms, we study the setting where the off-policy experience is fixed and there is no further interaction with the environment. We identify bootstrapping error as a key source of instability in current methods. Bootstrapping error is due to bootstrapping from actions that lie outside of the training data distribution, and it accumulates via the Bellman backup operator. We theoretically analyze bootstrapping error, and demonstrate how carefully constraining action selection in the backup can mitigate it. Based on our analysis, we propose a practical algorithm, bootstrapping error accumulation reduction (BEAR). We demonstrate that BEAR is able to learn robustly from different off-policy distributions, including random and suboptimal demonstrations, on a range of continuous control tasks.},
archivePrefix = {arXiv},
arxivId = {1906.00949},
author = {Kumar, Aviral and Fu, Justin and Tucker, George and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {1906.00949},
month = {jun},
title = {{Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction}},
url = {http://arxiv.org/abs/1906.00949},
year = {2019}
}
@article{peng2019awr,
abstract = {In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.},
archivePrefix = {arXiv},
arxivId = {1910.00177},
author = {Peng, Xue Bin and Kumar, Aviral and Zhang, Grace and Levine, Sergey},
eprint = {1910.00177},
file = {::},
month = {sep},
title = {{Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning}},
url = {http://arxiv.org/abs/1910.00177},
year = {2019}
}
@article{james2019rlbench,
abstract = {We present a challenging new benchmark and learning-environment for robot learning: RLBench. The benchmark features 100 completely unique, hand-designed tasks ranging in difficulty, from simple target reaching and door opening, to longer multi-stage tasks, such as opening an oven and placing a tray in it. We provide an array of both proprioceptive observations and visual observations, which include rgb, depth, and segmentation masks from an over-the-shoulder stereo camera and an eye-in-hand monocular camera. Uniquely, each task comes with an infinite supply of demos through the use of motion planners operating on a series of waypoints given during task creation time; enabling an exciting flurry of demonstration-based learning. RLBench has been designed with scalability in mind; new tasks, along with their motion-planned demos, can be easily created and then verified by a series of tools, allowing users to submit their own tasks to the RLBench task repository. This large-scale benchmark aims to accelerate progress in a number of vision-guided manipulation research areas, including: reinforcement learning, imitation learning, multi-task learning, geometric computer vision, and in particular, few-shot learning. With the benchmark's breadth of tasks and demonstrations, we propose the first large-scale few-shot challenge in robotics. We hope that the scale and diversity of RLBench offers unparalleled research opportunities in the robot learning community and beyond.},
archivePrefix = {arXiv},
arxivId = {1909.12271},
author = {James, Stephen and Ma, Zicong and Arrojo, David Rovick and Davison, Andrew J.},
eprint = {1909.12271},
file = {::},
month = {sep},
title = {{RLBench: The Robot Learning Benchmark {\&} Learning Environment}},
url = {http://arxiv.org/abs/1909.12271},
year = {2019}
}
@inproceedings{chhatpar2001insertion,
author = {Chhatpar, S. R. and {M.S. Branicky}},
booktitle = {International Conference on Intelligent Robots and Systems (IROS)},
title = {{Search strategies for peg-in-hole assemblies with position uncertainty}},
year = {2001}
}
@article{sermanet2018tcn,
abstract = {We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a triplet loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at sermanet.github.io/imitate.},
archivePrefix = {arXiv},
arxivId = {1704.06888},
author = {Sermanet, Pierre and Lynch, Corey and Chebotar, Yevgen and Hsu, Jasmine and Jang, Eric and Schaal, Stefan and Levine, Sergey and Brain, Google},
doi = {10.1109/ICRA.2018.8462891},
eprint = {1704.06888},
file = {:Users/ashvin/Documents/research/2019/nips{\_}reading{\_}list/TCN.pdf:pdf},
isbn = {9781538630815},
issn = {10504729},
journal = {IEEE International Conference on Robotics and Automation (ICRA)},
pages = {1134--1141},
title = {{Time-Contrastive Networks: Self-Supervised Learning from Video}},
year = {2018}
}
@article{tobin2017domainrandomization,
author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
journal = {International Conference on Intelligent Robots and Systems (IROS)},
title = {{Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World}},
year = {2017}
}
@inproceedings{wortsman2019learntolearn,
author = {Wortsman, Mitchell and Ehsani, Kiana and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {{Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using Meta-Learning}},
year = {2019}
}
@inproceedings{chen19surrogate,
author = {Chen, Minmin and Gummadi, Ramki and Harris, Chris and Schuurmans, Dale},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {:Users/ashvin/Documents/research/2019/nips{\_}reading{\_}list/9086-surrogate-objectives-for-batch-policy-optimization-in-one-step-decision-making.pdf:pdf},
title = {{Surrogate Objectives for Batch Policy Optimization in One-step Decision Making}},
year = {2019}
}
@inproceedings{nachum2018hiro,
abstract = {Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher-and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher-and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We term the resulting HRL agent HIRO and find that it is generally applicable and highly sample-efficient. Our experiments show that HIRO can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, 1 learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques. 2},
archivePrefix = {arXiv},
arxivId = {arXiv:1805.08296v2},
author = {Nachum, Ofir and Gu, Shane Shixiang and Lee, Honglak and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {arXiv:1805.08296v2},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Nachum et al. - 2018 - Data-Efficient Hierarchical Reinforcement Learning.pdf:pdf;::},
title = {{Data-Efficient Hierarchical Reinforcement Learning}},
url = {https://sites.google.com/view/efficient-hrl},
year = {2018}
}
@inproceedings{zamir2018taskonomy,
abstract = {Do visual tasks have a relationship, or are they unre-lated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable values ; it is the concept underlying transfer learning and provides a principled way for identifying redundancies across tasks, e.g., to seamlessly reuse supervision among related tasks or solve many tasks in one system without piling up the complexity. We proposes a fully computational approach for model-ing the structure of space of visual tasks. This is done via finding (first and higher-order) transfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D, and semantic tasks in a latent space. The product is a computational taxonomic map for task transfer learning. We study the consequences of this structure, e.g. nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. For example, we show that the total number of labeled datapoints needed for solving a set of 10 tasks can be reduced by roughly 2 3 (compared to training independently) while keeping the performance nearly the same. We provide a set of tools for computing and probing this taxo-nomical structure including a solver that users can employ to devise efficient supervision policies for their use cases.},
archivePrefix = {arXiv},
arxivId = {1804.08328v1},
author = {Zamir, Amir R and Sax, Alexander and Shen, William and Guibas, Leonidas and Malik, Jitendra and Savarese, Silvio},
booktitle = {Conference on Vision and Pattern Recognition (CVPR)},
eprint = {1804.08328v1},
file = {::},
title = {{Taskonomy: Disentangling Task Transfer Learning}},
url = {http://taskonomy.vision/},
year = {2018}
}
@inproceedings{sax2018midlevel,
abstract = {How much does having visual priors about the world (e.g. the fact that the world is 3D) assist in learning to perform downstream motor tasks (e.g. delivering a package)? We study this question by integrating a generic perceptual skill set (e.g. a distance estimator, an edge detector , etc.) within a reinforcement learning framework-see Fig. 1. This skill set (hereafter mid-level perception) provides the policy with a more processed state of the world compared to raw images. We find that using a mid-level perception confers significant advantages over training end-to-end from scratch (i.e. not leveraging priors) in navigation-oriented tasks. Agents are able to generalize to situations where the from-scratch approach fails and training becomes significantly more sample efficient. However, we show that realizing these gains requires careful selection of the mid-level perceptual skills. Therefore, we refine our findings into an efficient max-coverage feature set that can be adopted in lieu of raw images. We perform our study in completely separate buildings for training and testing and compare against state-of-the-art feature learning methods and visually blind baseline policies.},
archivePrefix = {arXiv},
arxivId = {1812.11971v3},
author = {Sax, Alexander and Emi, Bradley and Zamir, Amir and Guibas, Leonidas and Savarese, Silvio and Malik, Jitendra},
booktitle = {Conference on Robot Learning (CoRL)},
eprint = {1812.11971v3},
file = {::},
title = {{Mid-Level Visual Representations Improve Generalization and Sample Efficiency for Learning Visuomotor Policies}},
url = {http://perceptual.actor/},
year = {2019}
}
@inproceedings{Johnsona,
abstract = {We propose a general modeling and inference framework that combines the complementary strengths of probabilistic graphical models and deep learning methods. Our model family composes latent graphical models with neural network observation likelihoods. For inference, we use recognition networks to produce local evidence potentials, then combine them with the model distribution using efficient message-passing algorithms. All components are trained simultaneously with a single stochastic variational inference objective. We illustrate this framework by automatically segmenting and categorizing mouse behavior from raw depth video, and demonstrate several other example models.},
archivePrefix = {arXiv},
arxivId = {1603.06277v5},
author = {Johnson, Matthew James and Duvenaud, David and Wiltschko, Alexander B and Datta, Sandeep R and Adams, Ryan P},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {1603.06277v5},
file = {::},
title = {{Composing graphical models with neural networks for structured representations and fast inference}},
url = {https://arxiv.org/pdf/1603.06277.pdf}
}
@techreport{Rolnick2019,
abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts , may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.},
archivePrefix = {arXiv},
arxivId = {1906.05433v1},
author = {Rolnick, David and Donti, Priya L and Kaack, Lynn H and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra and Maharaj, Tegan and Sherwin, Evan D and Mukkavilli, S Karthik and Kording, Konrad P and Gomes, Carla and Ng, Andrew Y and Hassabis, Demis and Platt, John C and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
eprint = {1906.05433v1},
file = {::},
title = {{Tackling Climate Change with Machine Learning}},
url = {https://arxiv.org/pdf/1906.05433.pdf},
volume = {16},
year = {2019}
}
@techreport{Jurgenson,
abstract = {Many AI problems, in robotics and other domains, are goal-directed, essentially seeking a trajectory leading to some goal state. In such problems, the way we choose to represent a trajectory underlies algorithms for trajectory prediction and optimization. Interestingly, most all prior work in imitation and reinforcement learning builds on a sequential trajectory representation-calculating the next state in the trajectory given its predecessors. We propose a different perspective: a goal-conditioned trajectory can be represented by first selecting an intermediate state between start and goal, partitioning the trajectory into two. Then, recursively, predicting intermediate points on each sub-segment, until a complete trajectory is obtained. We call this representation a sub-goal tree, and building on it, we develop new methods for trajectory prediction, learning, and optimization. We show that in a supervised learning setting, sub-goal trees better account for trajectory variability, and can predict trajectories exponentially faster at test time by leveraging a concurrent computation. Then, for optimization, we derive a new dynamic programming equation for sub-goal trees, and use it to develop new planning and reinforcement learning algorithms. These algorithms, which are not based on the standard Bellman equation, naturally account for hierarchical sub-goal structure in a task. Empirical results on motion planning domains show that the sub-goal tree framework significantly improves both accuracy and prediction time.},
archivePrefix = {arXiv},
arxivId = {1906.05329v1},
author = {Jurgenson, Tom and Tamar, Aviv},
eprint = {1906.05329v1},
file = {::},
title = {{Sub-Goal Trees-a Framework for Goal-Directed Trajectory Prediction and Optimization}},
url = {https://arxiv.org/pdf/1906.05329.pdf}
}
@article{Lin2019,
abstract = {To perform robot manipulation tasks, a low dimension state of the environment typically needs to be estimated. However, designing a state estimator can sometimes be difficult , especially in environments with deformable objects. An alternative is to learn an end-to-end policy that maps directly from high dimensional sensor inputs to actions. However, if this policy is trained with reinforcement learning, then without a state estimator, it is hard to specify a reward function based on continuous and high dimensional observations. To meet this challenge, we propose a simple indicator reward function for goal-conditioned reinforcement learning: we only give a positive reward when the robot's observation exactly matches a target goal observation. We show that by utilizing the goal relabeling technique, we can learn with the indicator reward function even in continuous state spaces, in which we do not expect two observations to ever be identical. We propose two methods to further speed up convergence with indicator rewards: reward balancing and reward filtering. We show comparable performance between our method and an oracle which uses the ground-truth state for computing rewards, even though our method only operates on raw observations and does not have access to the ground-truth state. We demonstrate our method in complex tasks in continuous state spaces such as rope manipulation from RGB-D images, without knowledge of the ground truth state.},
archivePrefix = {arXiv},
arxivId = {1905.07866v1},
author = {Lin, Xingyu and {Singh Baweja}, Harjatin and Held, David},
eprint = {1905.07866v1},
file = {::},
journal = {arXiv preprint arXiv:1905.07866},
keywords = {lin2019rlwithoutstate},
title = {{Reinforcement Learning without Ground-Truth State}},
url = {https://arxiv.org/pdf/1905.07866.pdf},
year = {2019}
}
@inproceedings{sohn2015cvae,
abstract = {Supervised deep learning has been successfully applied to many recognition problems. Although it can approximate a complex many-to-one function well when a large amount of training data is provided, it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions. In this work, we develop a deep conditional generative model for structured output prediction using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient varia-tional Bayes, and allows for fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build robust structured prediction algorithms, such as input noise-injection and multi-scale prediction objective at training. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference. Furthermore, the proposed training methods are complimentary, which leads to strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
author = {Sohn, Kihyuk and Yan, Xinchen and Lee, Honglak},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {::},
title = {{Learning Structured Output Representation using Deep Conditional Generative Models}},
url = {https://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf},
year = {2015}
}
@article{hwangbo2019quadruped,
abstract = {Legged robots pose one of the greatest challenges in robotics. Dynamic and agile maneuvers of animals cannot be imitated by existing methods that are crafted by humans. A compelling alternative is reinforcement learning, which requires minimal craftsmanship and promotes the natural evolution of a control policy. However, so far, reinforcement learning research for legged robots is mainly limited to simulation, and only few and comparably simple examples have been deployed on real systems. The primary reason is that training with real robots, particularly with dynamically balancing systems, is complicated and expensive. In the present work, we introduce a method for training a neural network policy in simulation and transferring it to a state-of-the-art legged system, thereby leveraging fast, automated, and cost-effective data generation schemes. The approach is applied to the ANYmal robot, a sophisticated medium-dog–sized quadrupedal system. Using policies trained in simulation, the quadrupedal machine achieves locomotion skills that go beyond what had been achieved with prior methods: ANYmal is capable of precisely and energy-efficiently following high-level body velocity commands, running faster than before, and recovering from falling even in complex configurations.},
author = {Hwangbo, Jemin and Lee, Joonho and Dosovitskiy, Alexey and Bellicoso, Dario and Tsounis, Vassilios and Koltun, Vladlen and Hutter, Marco},
doi = {10.1126/scirobotics.aau5872},
issn = {2470-9476},
journal = {Science Robotics},
month = {jan},
number = {26},
pages = {5872},
publisher = {Science Robotics},
title = {{Learning agile and dynamic motor skills for legged robots}},
url = {http://robotics.sciencemag.org/lookup/doi/10.1126/scirobotics.aau5872},
volume = {4},
year = {2019}
}
@inproceedings{tan2018quadruped,
abstract = {Designing agile locomotion for quadruped robots often requires extensive expertise and tedious manual tuning. In this paper, we present a system to automate this process by leveraging deep reinforcement learning techniques. Our system can learn quadruped locomotion from scratch using simple reward signals. In addition, users can provide an open loop reference to guide the learning process when more control over the learned gait is needed. The control policies are learned in a physics simulator and then deployed on real robots. In robotics, policies trained in simulation often do not transfer to the real world. We narrow this reality gap by improving the physics simulator and learning robust policies. We improve the simulation using system identification, developing an accurate actuator model and simulating latency. We learn robust controllers by randomizing the physical environments, adding perturbations and designing a compact observation space. We evaluate our system on two agile locomotion gaits: trotting and galloping. After learning in simulation, a quadruped robot can successfully perform both gaits in the real world.},
archivePrefix = {arXiv},
arxivId = {1804.10332v2},
author = {Tan, Jie and Zhang, Tingnan and Coumans, Erwin and Iscen, Atil and Bai, Yunfei and Hafner, Danijar and Bohez, Steven and Vanhoucke, Vincent and Brain, Google and Deepmind, Google},
booktitle = {Robotics: Science and Systems (RSS)},
eprint = {1804.10332v2},
file = {::},
isbn = {1804.10332v2},
title = {{Sim-to-Real: Learning Agile Locomotion For Quadruped Robots}},
url = {https://arxiv.org/pdf/1804.10332.pdf},
year = {2018}
}
@inproceedings{peng2018openai,
abstract = {Simulations are attractive environments for training agents as they provide an abundant source of data and alleviate certain safety concerns during the training process. But the behaviours developed by agents in simulation are often specific to the characteristics of the simulator. Due to modeling error, strategies that are successful in simulation may not transfer to their real world counterparts. In this paper, we demonstrate a simple method to bridge this "reality gap". By randomizing the dynamics of the simulator during training, we are able to develop policies that are capable of adapting to very different dynamics, including ones that differ significantly from the dynamics on which the policies were trained. This adaptivity enables the policies to generalize to the dynamics of the real world without any training on the physical system. Our approach is demonstrated on an object pushing task using a robotic arm. Despite being trained exclusively in simulation, our policies are able to maintain a similar level of performance when deployed on a real robot, reliably moving an object to a desired location from random initial configurations. We explore the impact of various design decisions and show that the resulting policies are robust to significant calibration error.},
author = {Peng, Xue Bin and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {::},
isbn = {9781538630815},
keywords = {Deep Learning in Robotics and Automation,Learning and Adaptive Systems,Robust/Adaptive Control of Robotic Systems},
title = {{Sim-to-Real Transfer of Robotic Control with Dynamics Randomization}},
url = {https://xbpeng.github.io/projects/SimToReal/2018{\_}SimToReal.pdf},
year = {2018}
}
@inproceedings{sadeghi2017simtoreal,
abstract = {Environment feedback t = 0 t = 1 t = 2 t = 3 t = 4 t = H {\ldots} Action Collision with wall Collision with Furniture No Collision Different Platforms time Training entirely in simulation Test in real world Fig. 1. We propose the Collision Avoidance via Deep Reinforcement Learning algorithm for indoor flight which is entirely trained in a simulated CAD environment. Left: CAD 2 RL uses single image inputs from a monocular camera, is exclusively trained in simulation, and does not see any real images at training time. Training is performed using a Monte Carlo policy evaluation method, which performs rollouts for multiple actions from each initial state and trains a deep network to predict long-horizon collision probabilities of each action. Right: CAD 2 RL generalizes to real indoor flight. Abstract-Deep reinforcement learning has emerged as a promising and powerful technique for automatically acquiring control policies that can process raw sensory inputs, such as images, and perform complex behaviors. However, extending deep RL to real-world robotic tasks has proven challenging, particularly in safety-critical domains such as autonomous flight, where a trial-and-error learning process is often impractical. In this paper, we explore the following question: can we train vision-based navigation policies entirely in simulation, and then transfer them into the real world to achieve real-world flight without a single real training image? We propose a learning method that we call CAD 2 RL, which can be used to perform collision-free indoor flight in the real world while being trained entirely on 3D CAD models. Our method uses single RGB images from a monocular camera, without needing to explicitly reconstruct the 3D geometry of the environment or perform explicit motion planning. Our learned collision avoidance policy is represented by a deep convolutional neural network that directly processes raw monocular images and outputs velocity commands. This policy is trained entirely on simulated images, with a Monte Carlo policy evaluation algorithm that directly optimizes the network's ability to produce collision-free flight. By highly randomizing the rendering settings for our simulated training set, we show that we can train a policy that generalizes to the real world, without requiring the simulator to be particularly realistic or high-fidelity. We evaluate our method by flying a real quadrotor through indoor environments, and further evaluate the design choices in our simulator through a series of ablation studies on depth prediction. For supplementary video see: https://youtu.be/nXBWmzFrj5s},
archivePrefix = {arXiv},
arxivId = {1611.04201v4},
author = {Sadeghi, Fereshteh and Levine, Sergey},
booktitle = {Robotics: Science and Systems (RSS)},
eprint = {1611.04201v4},
file = {::},
title = {{CAD 2 RL: Real Single-Image Flight Without a Single Real Image}},
url = {https://youtu.be/nXBWmzFrj5s},
year = {2017}
}
@inproceedings{zhou2019epi,
abstract = {A key challenge in reinforcement learning (RL) is environment generalization: a policy trained to solve a task in one environment often fails to solve the same task in a slightly different test environment. A common approach to improve inter-environment transfer is to learn policies that are invariant to the distribution of testing environments. However, we argue that instead of being invariant, the policy should identify the specific nuances of an environment and exploit them to achieve better performance. In this work, we propose the "Environment-Probing" Interaction (EPI) policy, a policy that probes a new environment to extract an implicit understanding of that environment's behavior. Once this environment-specific information is obtained, it is used as an additional input to a task-specific policy that can now perform environment-conditioned actions to solve a task. To learn these EPI-policies, we present a reward function based on transition predictability. Specifically, a higher reward is given if the trajectory generated by the EPI-policy can be used to better predict transitions. We experimentally show that EPI-conditioned task-specific policies significantly outperform commonly used policy generalization methods on novel testing environments.},
author = {Zhou, Wenxuan and Pinto, Lerrel and Gupta, Abhinav},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {::},
title = {{Environment Probing Interaction Policies}},
url = {https://openreview.net/pdf?id=ryl8-3AcFX},
year = {2019}
}
@inproceedings{wadefarley2019discern,
abstract = {Learning to control an environment without hand-crafted rewards or expert data remains challenging and is at the frontier of reinforcement learning research. We present an unsupervised learning algorithm to train agents to achieve perceptually-specified goals using only a stream of observations and actions. Our agent simultaneously learns a goal-conditioned policy and a goal achievement reward function that measures how similar a state is to the goal state. This dual optimization leads to a cooperative game, giving rise to a learned reward function that reflects similarity in controllable aspects of the environment instead of distance in the space of observations. We demonstrate the efficacy of our agent to learn, in an unsupervised manner, to reach a diverse set of goals on three domains-Atari, the DeepMind Control Suite and DeepMind Lab.},
archivePrefix = {arXiv},
arxivId = {1811.11359v1},
author = {Warde-Farley, David and {Van De Wiele}, Tom and Kulkarni, Tejas and Ionescu, Catalin and Hansen, Steven and Volodymyr, Mnih},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {1811.11359v1},
file = {::},
isbn = {1811.11359v1},
title = {{Unsupervised Control Through Non-Parametric Discriminative Rewards}},
url = {https://arxiv.org/pdf/1811.11359.pdf},
year = {2019}
}
@inproceedings{fu2017ex2,
abstract = {Deep reinforcement learning algorithms have been shown to learn complex tasks using highly general policy classes. However, sparse reward problems remain a significant challenge. Exploration methods based on novelty detection have been particularly successful in such settings but typically require generative or predictive models of the observations, which can be difficult to train when the observations are very high-dimensional and complex, as in the case of raw images. We propose a novelty detection algorithm for exploration that is based entirely on discriminatively trained exemplar models, where classifiers are trained to discriminate each visited state against all others. Intuitively, novel states are easier to distinguish against other states seen during training. We show that this kind of discriminative modeling corresponds to implicit density estimation, and that it can be combined with count-based exploration to produce competitive results on a range of popular benchmark tasks, including state-of-the-art results on challenging egocentric observations in the vizDoom benchmark.},
author = {Fu, Justin and Co-Reyes, John D and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Fu, Co-Reyes, Levine - 2017 - EX 2 Exploration with Exemplar Models for Deep Reinforcement Learning.pdf:pdf;::},
title = {{EX 2 : Exploration with Exemplar Models for Deep Reinforcement Learning}},
url = {https://papers.nips.cc/paper/6851-ex2-exploration-with-exemplar-models-for-deep-reinforcement-learning.pdf https://arxiv.org/pdf/1703.01260.pdf},
year = {2017}
}
@inproceedings{pong2020skewfit,
abstract = {In standard reinforcement learning, each new skill requires a manually-designed reward function, which takes considerable manual effort and engineering. Self-supervised goal setting has the potential to automate this process, enabling an agent to propose its own goals and acquire skills that achieve these goals. However, such methods typically rely on manually-designed goal distributions, or heuristics to force the agent to explore a wide range of states. We propose a formal exploration objective for goal-reaching policies that maximizes state coverage. We show that this objective is equivalent to maximizing the entropy of the goal distribution together with goal reaching performance, where goals correspond to entire states. We present an algorithm called Skew-Fit for learning such a maximum-entropy goal distribution, and show that under certain regularity conditions, our method converges to a uniform distribution over the set of valid states, even when we do not know this set beforehand. Skew-Fit enables self-supervised agents to autonomously choose and practice reaching diverse goals. Our experiments show that it can learn a variety of manipulation tasks from images, including opening a door with a real robot, entirely from scratch and without any manually-designed reward function.},
archivePrefix = {arXiv},
arxivId = {1903.03698v2},
author = {Pong, Vitchyr H and Dalal, Murtaza and Lin, Steven and Nair, Ashvin and Bahl, Shikhar and Levine, Sergey},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1903.03698v2},
file = {::},
mendeley-groups = {Old Research,New Research},
title = {{Skew-Fit: State-Covering Self-Supervised Reinforcement Learning}},
url = {https://arxiv.org/pdf/1903.03698.pdf},
year = {2020}
}
@inproceedings{zhang2019solar,
abstract = {Model-based reinforcement learning (RL) has proven to be a data efficient approach for learning control tasks but is difficult to utilize in domains with complex observations such as images. In this paper, we present a method for learning representations that are suitable for iterative model-based policy improvement, even when the underlying dynamical system has complex dynamics and image observations, in that these representations are optimized for inferring simple dynamics and cost models given data from the current policy. This enables a model-based RL method based on the linear-quadratic regulator (LQR) to be used for systems with image observations. We evaluate our approach on a range of robotics tasks, including manipulation with a real-world robotic arm directly from images. We find that our method produces substantially better final performance than other model-based RL methods while being significantly more efficient than model-free RL.},
archivePrefix = {arXiv},
arxivId = {1808.09105},
author = {Zhang, Marvin and Vikram, Sharad and Smith, Laura and Abbeel, Pieter and Johnson, Matthew J. and Levine, Sergey},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1808.09105},
file = {::},
month = {aug},
title = {{SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning}},
url = {http://arxiv.org/abs/1808.09105},
year = {2019}
}
@inproceedings{fu2018vice,
abstract = {The design of a reward function often poses a major practical challenge to real-world applications of reinforcement learning. Approaches such as inverse reinforcement learning attempt to overcome this challenge, but require expert demonstrations, which can be difficult or expensive to obtain in practice. We propose variational inverse control with events (VICE), which generalizes inverse reinforcement learning methods to cases where full demonstrations are not needed, such as when only samples of desired goal states are available. Our method is grounded in an alternative perspective on control and reinforcement learning, where an agent's goal is to maximize the probability that one or more events will happen at some point in the future, rather than maximizing cumulative rewards. We demonstrate the effectiveness of our methods on continuous control tasks, with a focus on high-dimensional observations like images where rewards are hard or even impossible to specify.},
archivePrefix = {arXiv},
arxivId = {1805.11686},
author = {Fu, Justin and Singh, Avi and Ghosh, Dibya and Yang, Larry and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {1805.11686},
file = {::},
month = {may},
title = {{Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition}},
url = {http://arxiv.org/abs/1805.11686},
year = {2018}
}
@misc{hessel2018rainbow,
archivePrefix = {arXiv},
arxivId = {1710.02298v1},
author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
booktitle = {Association for the Advancement of Artificial Intelligence (AAAI)},
eprint = {1710.02298v1},
file = {::},
isbn = {1710.02298v1},
title = {{Rainbow: Combining Improvements in Deep Reinforcement Learning}},
url = {www.aaai.org https://arxiv.org/pdf/1710.02298.pdf},
year = {2018}
}
@inproceedings{singh2019raq,
abstract = {The combination of deep neural network models and reinforcement learning algorithms can make it possible to learn policies for robotic behaviors that directly read in raw sensory inputs, such as camera images, effectively subsuming both estimation and control into one model. However, real-world applications of reinforcement learning must specify the goal of the task by means of a manually programmed reward function, which in practice requires either designing the very same perception pipeline that end-to-end reinforcement learning promises to avoid, or else instrumenting the environment with additional sensors to determine if the task has been performed successfully. In this paper, we propose an approach for removing the need for manual engineering of reward specifications by enabling a robot to learn from a modest number of examples of successful outcomes, followed by actively solicited queries, where the robot shows the user a state and asks for a label to determine whether that state represents successful completion of the task. While requesting labels for every single state would amount to asking the user to manually provide the reward signal, our method requires labels for only a tiny fraction of the states seen during training, making it an efficient and practical approach for learning skills without manually engineered rewards. We evaluate our method on real-world robotic manipulation tasks where the observations consist of images viewed by the robot's camera. In our experiments, our method effectively learns to arrange objects, place books, and drape cloth, directly from images and without any manually specified reward functions, and with only 1-4 hours of interaction with the real world.},
archivePrefix = {arXiv},
arxivId = {1904.07854},
author = {Singh, Avi and Yang, Larry and Hartikainen, Kristian and Finn, Chelsea and Levine, Sergey},
booktitle = {Robotics: Science and Systems (RSS)},
eprint = {1904.07854},
file = {::},
month = {apr},
title = {{End-to-End Robotic Reinforcement Learning without Reward Engineering}},
url = {http://arxiv.org/abs/1904.07854},
year = {2019}
}
@inproceedings{yu2019dpn,
abstract = {While reinforcement learning (RL) has the potential to enable robots to autonomously acquire a wide range of skills, in practice, RL usually requires manual, per-task engineering of reward functions, especially in real world settings where aspects of the environment needed to compute progress are not directly accessible. To enable robots to autonomously learn skills, we instead consider the problem of reinforcement learning without access to rewards. We aim to learn an unsupervised embedding space under which the robot can measure progress towards a goal for itself. Our approach explicitly optimizes for a metric space under which action sequences that reach a particular state are optimal when the goal is the final state reached. This enables learning effective and control-centric representations that lead to more autonomous reinforcement learning algorithms. Our experiments on three simulated environments and two real-world manipulation problems show that our method can learn effective goal metrics from unlabeled interaction, and use the learned goal metrics for autonomous reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1902.05542v1},
author = {Yu, Tianhe and Shevchuk, Gleb and Sadigh, Dorsa and Finn, Chelsea},
booktitle = {Robotics: Science and Systems (RSS)},
eprint = {1902.05542v1},
file = {::},
title = {{Unsupervised Visuomotor Control through Distributional Planning Networks}},
url = {https://arxiv.org/pdf/1902.05542.pdf},
year = {2019}
}
@book{Szepesvari2009a,
author = {Szepesv{\'{a}}ri, Csaba},
file = {::},
title = {{Algorithms for Reinforcement Learning}},
url = {https://sites.ualberta.ca/{~}szepesva/papers/RLAlgsInMDPs.pdf},
year = {2009}
}
@inproceedings{vanhasselt2016doubledqn,
abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common , whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestima-tions, as hypothesized, but that this also leads to much better performance on several games. The goal of reinforcement learning (Sutton and Barto, 1998) is to learn good policies for sequential decision problems, by optimizing a cumulative future reward signal. Q-learning (Watkins, 1989) is one of the most popular reinforcement learning algorithms, but it is known to sometimes learn un-realistically high action values because it includes a maxi-mization step over estimated action values, which tends to prefer overestimated to underestimated values. In previous work, overestimations have been attributed to insufficiently flexible function approximation (Thrun and Schwartz, 1993) and noise (van Hasselt, 2010, 2011). In this paper, we unify these views and show overestimations can occur when the action values are inaccurate, irrespective of the source of approximation error. Of course, imprecise value estimates are the norm during learning, which indicates that overestimations may be much more common than previously appreciated. It is an open question whether, if the overestimations do occur, this negatively affects performance in practice. Overoptimistic value estimates are not necessarily a problem in and of themselves. If all values would be uniformly higher then the relative action preferences are preserved and we would not expect the resulting policy to be any worse. Furthermore, it is known that sometimes it is good to be optimistic: optimism in the face of uncertainty is a well-known exploration technique (Kaelbling et al., 1996). If, however, the overestimations are not uniform and not concentrated at states about which we wish to learn more, then they might negatively affect the quality of the resulting policy. Thrun and Schwartz (1993) give specific examples in which this leads to suboptimal policies, even asymptotically. To test whether overestimations occur in practice and at scale, we investigate the performance of the recent DQN algorithm (Mnih et al., 2015). DQN combines Q-learning with a flexible deep neural network and was tested on a varied and large set of deterministic Atari 2600 games, reaching human-level performance on many games. In some ways, this setting is a best-case scenario for Q-learning, because the deep neural network provides flexible function approximation with the potential for a low asymptotic approximation error, and the determinism of the environments prevents the harmful effects of noise. Perhaps surprisingly, we show that even in this comparatively favorable setting DQN sometimes substantially overestimates the values of the actions. We show that the idea behind the Double Q-learning algorithm (van Hasselt, 2010), which was first proposed in a tab-ular setting, can be generalized to work with arbitrary function approximation, including deep neural networks. We use this to construct a new algorithm we call Double DQN. We then show that this algorithm not only yields more accurate value estimates, but leads to much higher scores on several games. This demonstrates that the overestimations of DQN were indeed leading to poorer policies and that it is beneficial to reduce them. In addition, by improving upon DQN we obtain state-of-the-art results on the Atari domain.},
archivePrefix = {arXiv},
arxivId = {1509.06461v3},
author = {{Van Hasselt}, Hado and Guez, Arthur and Silver, David},
booktitle = {Association for the Advancement of Artificial Intelligence (AAAI)},
eprint = {1509.06461v3},
file = {::},
isbn = {1509.06461v3},
title = {{Deep Reinforcement Learning with Double Q-learning}},
url = {www.aaai.org},
year = {2016}
}
@inproceedings{johnson2000hedging,
author = {Johnson, Eric and Calise, Anthony},
booktitle = {Advances in Navigation Guidance and Control Technology Workshop},
title = {{Pseudo-Control Hedging: A New Method For Adaptive Control}},
url = {https://www.researchgate.net/publication/2492500{\_}Pseudo-Control{\_}Hedging{\_}A{\_}New{\_}Method{\_}For{\_}Adaptive{\_}Control},
year = {2000}
}
@inproceedings{johannink18residualrl,
abstract = {Conventional feedback control methods can solve various types of robot control problems very efficiently by capturing the structure with explicit models, such as rigid body equations of motion. However, many control problems in modern manufacturing deal with contacts and friction, which are difficult to capture with first-order physical modeling. Hence, applying control design methodologies to these kinds of problems often results in brittle and inaccurate controllers, which have to be manually tuned for deployment. Reinforcement learning (RL) methods have been demonstrated to be capable of learning continuous robot controllers from interactions with the environment, even for problems that include friction and contacts. In this paper, we study how we can solve difficult control problems in the real world by decomposing them into a part that is solved efficiently by conventional feedback control methods, and the residual which is solved with RL. The final control policy is a superposition of both control signals. We demonstrate our approach by training an agent to successfully perform a real-world block assembly task involving contacts and unstable objects.},
archivePrefix = {arXiv},
arxivId = {1812.03201v2},
author = {Johannink, Tobias and Bahl, Shikhar and Nair, Ashvin and Luo, Jianlan and Kumar, Avinash and Loskyll, Matthias and {Aparicio Ojea}, Juan and Solowjow, Eugen and Levine, Sergey},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
eprint = {1812.03201v2},
file = {::},
title = {{Residual Reinforcement Learning for Robot Control}},
url = {https://arxiv.org/pdf/1812.03201.pdf},
year = {2019}
}
@inproceedings{li2014usbgelsight,
abstract = {Robust manipulation and insertion of small parts can be challenging because of the small tolerances typically involved. The key to robust control of these kinds of manipulation interactions is accurate tracking and control of the parts involved. Typically, this is accomplished using visual servoing or force-based control. However, these approaches have drawbacks. Instead, we propose a new approach that uses tactile sensing to accurately localize the pose of a part grasped in the robot hand. Using a feature-based matching technique in conjunction with a newly developed tactile sensing technology known as GelSight that has much higher resolution than competing methods, we synthesize high-resolution height maps of object surfaces. As a result of these high-resolution tactile maps, we are able to localize small parts held in a robot hand very accurately. We quantify localization accuracy in benchtop experiments and experimentally demonstrate the practicality of the approach in the context of a small parts insertion problem.},
author = {Li, Rui and Platt, Robert and Yuan, Wenzhen and {Ten Pas}, Andreas and Roscup, Nathan and Srinivasan, Mandayam A and Adelson, Edward},
booktitle = {International Conference on Intelligent Robots and Systems (IROS)},
file = {::},
title = {{Localization and Manipulation of Small Parts Using GelSight Tactile Sensing}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.448.7123{\&}rep=rep1{\&}type=pdf},
year = {2014}
}
@inproceedings{kalashnikov2018qtopt,
abstract = {In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control , whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96{\%} grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations. 4},
archivePrefix = {arXiv},
arxivId = {1806.10293v3},
author = {Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and Levine, Sergey},
booktitle = {Conference on Robot Learning (CoRL)},
eprint = {1806.10293v3},
file = {::},
title = {{QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation}},
url = {https://goo.gl/ykQn6g.},
year = {2018}
}
@inproceedings{nair2018rig,
abstract = {For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires. Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images. In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised "practice" phase where it imagines goals and attempts to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching. We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.},
archivePrefix = {arXiv},
arxivId = {arXiv:1807.04742v1},
author = {Nair, Ashvin and Pong, Vitchyr and Dalal, Murtaza and Bahl, Shikhar and Lin, Steven and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {arXiv:1807.04742v1},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Nair et al. - 2018 - Visual Reinforcement Learning with Imagined Goals.pdf:pdf;::},
title = {{Visual Reinforcement Learning with Imagined Goals}},
url = {https://sites.google.com/site/},
year = {2018}
}
@article{kroemer2010grasping,
abstract = {Grasping an ob jet is a task that inherently needs to be treated in a hybrid fashion. The system must deide both where and how to grasp the ob jet. While seleting where to grasp requires learning about the ob jet as a whole, the exeution only needs to reatively adapt to the ontext lose to the grasp's loation. We propose a hierarhial ontroller that reets the struture of these two sub-problems, and attempts to learn solutions that work for both. A hybrid arhiteture is employed by the ontroller to make use of various mahine learning methods that an ope with the large amount of unertainty inherent to the task. The ontroller's upper level selets where to grasp the ob jet using a reinforement learner, while the lower level omprises an imitation learner and a vision-based reative ontroller to determine appropriate grasping motions. The resulting system is able to quikly learn good grasps of a novel ob jet in an unstrutured environment, by exeuting smooth reahing motions and preshaping the hand depending on the ob jet's geometry. The system was evaluated both in simulation and on a real robot.},
author = {Kroemer, Oliver and Detry, Renaud and Piater, Justus and Peters, Jan},
file = {::},
journal = {Robotics and Autonomous systems},
number = {9},
pages = {1105--1116},
title = {{Combining Active Learning and Reactive Control for Robot Grasping}},
url = {https://orbi.uliege.be//bitstream/2268/60643/1/Kroemer-2010-RAS-author-postprint.pdf},
volume = {58},
year = {2010}
}
@inproceedings{lee2018videoprediction,
abstract = {Being able to predict what may happen in the future requires an in-depth understanding of the physical and causal rules that govern the world. A model that is able to do so has a number of appealing applications, from robotic planning to representation learning. However, learning to predict raw future observations, such as frames in a video, is exceedingly challenging-the ambiguous nature of the problem can cause a naively designed model to average together possible futures into a single , blurry prediction. Recently, this has been addressed by two distinct approaches: (a) latent variational variable models that explicitly model underlying stochasticity and (b) adversarially-trained models that aim to produce naturalistic images. However, a standard latent variable model can struggle to produce realistic results, and a standard adversarially-trained model underutilizes latent variables and fails to produce diverse predictions. We show that these distinct methods are in fact complementary. Combining the two produces predictions that look more realistic to human raters and better cover the range of possible futures. Our method outperforms prior and concurrent work in these aspects.},
archivePrefix = {arXiv},
arxivId = {1804.01523v1},
author = {Lee, Alex X and Zhang, Richard and Ebert, Frederik and Abbeel, Pieter and Finn, Chelsea and Levine, Sergey},
eprint = {1804.01523v1},
file = {::},
title = {{Stochastic Adversarial Video Prediction}},
url = {https://alexlee-gk.github.io/video{\_}prediction}
}
@article{ebert2018journal,
abstract = {Deep reinforcement learning (RL) algorithms can learn complex robotic skills from raw sensory inputs, but have yet to achieve the kind of broad generalization and applicability demonstrated by deep learning methods in supervised domains. We present a deep RL method that is practical for real-world robotics tasks, such as robotic manipulation, and generalizes effectively to never-before-seen tasks and objects. In these settings, ground truth reward signals are typically unavailable, and we therefore propose a self-supervised model-based approach, where a predictive model learns to directly predict the future from raw sensory readings, such as camera images. At test time, we explore three distinct goal specification methods: designated pixels, where a user specifies desired object manipulation tasks by selecting particular pixels in an image and corresponding goal positions, goal images, where the desired goal state is specified with an image, and image classifiers, which define spaces of goal states. Our deep predictive models are trained using data collected autonomously and continuously by a robot interacting with hundreds of objects, without human supervision. We demonstrate that visual MPC can generalize to never-before-seen objects-both rigid and deformable-and solve a range of user-defined object manipulation tasks using the same model. ! Figure 1: Our approach trains a single model from unsupervised interaction that generalizes to a wide range of tasks and objects, while allowing flexibility in goal specification and both rigid and deformable objects not seen during training. Each row shows an example trajectory. From left to right, we show the task definition, the video predictions for the planned actions, and the actual executions. Tasks can be defined as (top) moving pixels corresponding to objects, (bottom left) providing a goal image, or (bottom right) providing a few example goals. Best viewed in PDF.},
archivePrefix = {arXiv},
arxivId = {1812.00568v1},
author = {Ebert, Frederik and Finn, Chelsea and Dasari, Sudeep and Xie, Annie and Lee, Alex and Levine, Sergey},
eprint = {1812.00568v1},
file = {::},
title = {{Visual Foresight: Model-Based Deep Reinforcement Learning for Vision-Based Robotic Control}},
url = {https://sites.google.com/view/visualforesight}
}
@inproceedings{ebert2018retrying,
abstract = {Prediction is an appealing objective for self-supervised learning of behavioral skills, particularly for autonomous robots. However, effectively utilizing predictive models for control, especially with raw image inputs, poses a number of major challenges. How should the predictions be used? What happens when they are inaccurate? In this paper, we tackle these questions by proposing a method for learning robotic skills from raw image observations, using only autonomously collected experience. We show that even an imperfect model can complete complex tasks if it can continuously retry, but this requires the model to not lose track of the objective (e.g., the object of interest). To enable a robot to continuously retry a task, we devise a self-supervised algorithm for learning image registration, which can keep track of objects of interest for the duration of the trial. We demonstrate that this idea can be combined with a video-prediction based controller to enable complex behaviors to be learned from scratch using only raw visual inputs, including grasping, repositioning objects, and non-prehensile manipulation. Our real-world experiments demonstrate that a model trained with 160 robot hours of autonomously collected, unlabeled data is able to successfully perform complex manipulation tasks with a wide range of objects not seen during training.},
archivePrefix = {arXiv},
arxivId = {1810.03043v1},
author = {Ebert, Frederik and Dasari, Sudeep and Lee, Alex X and Levine, Sergey and Finn, Chelsea},
booktitle = {Conference on Robot Learning (CoRL)},
eprint = {1810.03043v1},
file = {::},
title = {{Robustness via Retrying: Closed-Loop Robotic Manipulation with Self-Supervised Learning}},
url = {https://arxiv.org/pdf/1810.03043.pdf},
year = {2018}
}
@inproceedings{he2016resnet,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions 1 , where we also won the 1st places on the tasks of ImageNet detection, ImageNet local-ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385v1},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
eprint = {1512.03385v1},
file = {::},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://image-net.org/challenges/LSVRC/2015/},
year = {2016}
}
@inproceedings{srouji18structuredcontrolnets,
abstract = {In recent years, Deep Reinforcement Learning has made impressive advances in solving several important benchmark problems for sequential decision making. Many control applications use a generic multilayer perceptron (MLP) for non-vision parts of the policy network. In this work, we propose a new neural network architecture for the policy network representation that is simple yet effective. The proposed Structured Control Net (SCN) splits the generic MLP into two separate sub-modules: a nonlinear control module and a linear control module. Intuitively, the nonlinear control is for forward-looking and global control, while the linear control stabilizes the local dynamics around the residual of global control. We hypothesize that this will bring together the benefits of both linear and nonlinear policies: improve training sample efficiency, final episodic reward, and generalization of learned policy, while requiring a smaller network and being generally applicable to different training methods. We validated our hypothesis with competitive results on simulations from OpenAI MuJoCo, Roboschool, Atari, and a custom 2D urban driving environment, with various ablation and generalization tests, trained with multiple black-box and policy gradient training methods. The proposed architecture has the potential to improve upon broader control tasks by incorporating problem specific priors into the architecture. As a case study, we demonstrate much improved performance for locomotion tasks by emulating the biological central pattern generators (CPGs) as the nonlinear part of the architecture.},
archivePrefix = {arXiv},
arxivId = {1802.08311},
author = {Srouji, Mario and Zhang, Jian and Salakhutdinov, Ruslan},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1802.08311},
file = {::},
title = {{Structured Control Nets for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1802.08311},
year = {2018}
}
@article{silver18residualpolicylearning,
abstract = {We present Residual Policy Learning (RPL): a simple method for improving nondifferentiable policies using model-free deep reinforcement learning. RPL thrives in complex robotic manipulation tasks where good but imperfect controllers are available. In these tasks, reinforcement learning from scratch remains data-inefficient or intractable, but learning a residual on top of the initial controller can yield substantial improvement. We study RPL in five challenging MuJoCo tasks involving partial observability, sensor noise, model misspecification, and controller miscalibration. By combining learning with control algorithms, RPL can perform long-horizon, sparse-reward tasks for which reinforcement learning alone fails. Moreover, we find that RPL consistently and substantially improves on the initial controllers. We argue that RPL is a promising approach for combining the complementary strengths of deep reinforcement learning and robotic control, pushing the boundaries of what either can achieve independently.},
archivePrefix = {arXiv},
arxivId = {1812.06298},
author = {Silver, Tom and Allen, Kelsey and Tenenbaum, Josh and Kaelbling, Leslie},
eprint = {1812.06298},
file = {::},
journal = {arXiv preprint arXiv:1812.06298},
title = {{Residual Policy Learning}},
url = {http://arxiv.org/abs/1812.06298},
year = {2018}
}
@misc{Valtonen2006,
abstract = {The three-body problem, which describes three masses interacting through Newtonian gravity without any restrictions imposed on the initial positions and velocities of these masses, has attracted the attention of many scientists for more than 300 years. In this paper, we present a review of the three-body problem in the context of both historical and modern developments. We describe the general and restricted (circular and elliptic) three-body problems, different analytical and numerical methods of finding solutions, methods for performing stability analysis, search for periodic orbits and resonances, and application of the results to some interesting astronomical and space dynamical settings. We also provide a brief presentation of the general and restricted relativistic three-body problems, and discuss their astronomical applications.},
archivePrefix = {arXiv},
arxivId = {1508.02312},
author = {Liu, Cixin},
booktitle = {The Three-Body Problem},
doi = {10.1017/CBO9780511616006},
eprint = {1508.02312},
file = {:Users/ashvin/Documents/books/The Three-Body Problem.epub:epub},
isbn = {9780511616006},
issn = {21643725},
pages = {1--345},
pmid = {24913140},
title = {{The three-body problem}},
year = {2006}
}
@misc{ArundhatiRoy1997,
author = {{Arundhati Roy} and "Roy, Arundhati"},
file = {:Users/ashvin/Documents/books/God of Small Things.epub:epub},
isbn = {679457313},
issn = {01400460},
title = {{God of Small Things}},
year = {1997}
}
@misc{LaFollette2015,
abstract = {A groundbreaking investigation into the origins of morality, which turns out to be the basis for religion and politics. The book explains the American culture wars and refutes the "New Atheists."},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Haidt, Jonathan},
booktitle = {Philosophical Psychology},
doi = {10.1080/03057240.2012.723940},
eprint = {arXiv:1011.1669v3},
file = {:Users/ashvin/Documents/books/The Righteous Mind{\_} Why Good People Are Divided by Politics and Religion.epub:epub},
isbn = {978-0307377906},
issn = {0305-7240},
number = {3},
pages = {452--465},
pmid = {16903156},
title = {{The Righteous Mind: Why Good People Are Divided by Politics and Religion}},
url = {http://www.tandfonline.com/doi/abs/10.1080/09515089.2013.838752},
volume = {28},
year = {2015}
}
@misc{Harari2015,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Harari, Yuval Noah},
booktitle = {An Animal of No Significance},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/ashvin/Documents/books/Sapiens{\_} A Brief History of Humankind.epub:epub},
isbn = {9788578110796},
issn = {1098-6596},
pages = {1--9},
pmid = {25246403},
title = {{Sapiens - A Brief History of Humankind}},
year = {2015}
}
@inproceedings{chua18probabilisticdynamics,
abstract = {Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance, especially those with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g. 25 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).},
archivePrefix = {arXiv},
arxivId = {1805.12114v1},
author = {Chua, Kurtland and Calandra, Roberto and Mcallister, Rowan and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {1805.12114v1},
file = {::},
title = {{Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models}},
url = {https://arxiv.org/pdf/1805.12114.pdf},
year = {2018}
}
@techreport{Shyam,
abstract = {Efficient exploration is an unsolved problem in Reinforcement Learning. We introduce Model-Based Active eXploration (MAX), an algorithm that actively explores the environment. It minimizes data required to comprehensively model the environment by planning to observe novel events, instead of merely reacting to novelty encountered by chance. Non-stationarity induced by traditional exploration bonus techniques is avoided by constructing fresh exploration policies only at time of action. In semi-random toy environments where directed exploration is critical to make progress, our algorithm is at least an order of magnitude more efficient than strong baselines.},
archivePrefix = {arXiv},
arxivId = {1810.12162v1},
author = {Shyam, Pranav and Jaskowski, Wojciech and Gomez, Faustino},
eprint = {1810.12162v1},
file = {::},
title = {{Model-Based Active Exploration}},
url = {https://arxiv.org/pdf/1810.12162.pdf}
}
@techreport{Nickel,
abstract = {Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, state-of-the-art embedding methods typically do not account for latent hierarchical structures which are characteristic for many complex symbolic datasets. In this work, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space-or more precisely into an n-dimensional Poincar{\'{e}} ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We present an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincar{\'{e}} embeddings can outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.},
author = {Nickel, Maximilian and Kiela, Douwe},
file = {::},
title = {{Poincar{\'{e}} Embeddings for Learning Hierarchical Representations}},
url = {https://papers.nips.cc/paper/7213-poincare-embeddings-for-learning-hierarchical-representations.pdf}
}
@inproceedings{colas2019curious,
abstract = {In open-ended and changing environments, agents face a wide range of potential tasks that may or may not come with associated reward functions. Such autonomous learning agents must be able to generate their own tasks through a process of intrin-sically motivated exploration, some of which might prove easy, others impossible. For this reason, they should be able to actively select which task to practice at any given moment, to maximize their overall mastery on the set of learnable tasks. This paper proposes CURIOUS, an extension of Universal Value Function Approximators that enables intrinsically motivated agents to learn to achieve both multiple tasks and multiple goals within a unique policy, leveraging hindsight learning. Agents focus on achievable tasks first, using an automated curriculum learning mechanism that biases their attention towards tasks maximizing the absolute learning progress. This mechanism provides robustness to catastrophic forgetting (by refocusing on tasks where performance decreases) and distracting tasks (by avoiding tasks with no absolute learning progress). Furthermore, we show that having two levels of parameterization (tasks and goals within tasks) enables more efficient learning of skills in an environment with a modular physical structure (e.g. multiple objects) as compared to flat, goal-parameterized RL with hindsight experience replay.},
archivePrefix = {arXiv},
arxivId = {1810.06284v1},
author = {Colas, C{\'{e}}dric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1810.06284v1},
file = {::},
title = {{CURIOUS: Intrinsically Motivated Multi-Task, Multi-Goal Reinforcement Learning}},
url = {https://arxiv.org/pdf/1810.06284.pdf},
year = {2019}
}
@book{goodfellow2016deeplearningbook,
abstract = {Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
doi = {10.1038/nmeth.3707},
eprint = {arXiv:1312.6184v5},
file = {:Users/ashvin/Downloads/deeplearningbook.pdf:pdf},
isbn = {978-0262035613},
issn = {0028-0836},
number = {7553},
pages = {800},
pmid = {26017442},
title = {{Deep Learning}},
url = {http://goodfeli.github.io/dlbook/{\%}0Ahttp://dx.doi.org/10.1038/nature14539},
volume = {521},
year = {2016}
}
@inproceedings{kahn2018navigation,
abstract = {Enabling robots to autonomously navigate complex environments is essential for real-world deployment. Prior methods approach this problem by having the robot maintain an internal map of the world, and then use a localization and planning method to navigate through the internal map. However, these approaches often include a variety of assumptions , are computationally intensive, and do not learn from failures. In contrast, learning-based methods improve as the robot acts in the environment, but are difficult to deploy in the real-world due to their high sample complexity. To address the need to learn complex policies with few samples, we propose a generalized computation graph that subsumes value-based model-free methods and model-based methods, with specific in-stantiations interpolating between model-free and model-based. We then instantiate this graph to form a navigation model that learns from raw images and is sample efficient. Our simulated car experiments explore the design decisions of our navigation model, and show our approach outperforms single-step and N-step double Q-learning. We also evaluate our approach on a real-world RC car and show it can learn to navigate through a complex indoor environment with a few hours of fully autonomous, self-supervised training. Videos of the experiments and code can be found at github.com/gkahn13/gcg},
archivePrefix = {arXiv},
arxivId = {1709.10489v3},
author = {Kahn, Gregory and Villaflor, Adam and Ding, Bosen and Abbeel, Pieter and Levine, Sergey},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
eprint = {1709.10489v3},
file = {::},
title = {{Self-supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation}},
url = {https://arxiv.org/pdf/1709.10489.pdf},
year = {2018}
}
@article{Ollowing2009,
file = {:Users/ashvin/Downloads/00602547104676a31d155c0b1af82871d8f1e115.pdf:pdf},
pages = {1--12},
title = {{FROM LANGUAGE TO GOALS: INVERSE REINFORCE- MENT LEARNING FOR VISION-BASED INSTRUCTION FOLLOWING}},
year = {2009}
}
@article{Qa2007,
file = {:Users/ashvin/Downloads/cb65b4946fec0924b0f2f757db5e936e6175c862.pdf:pdf},
number = {0},
pages = {1--8},
title = {{CURIOSITY-DRIVEN EXPERIENCE PRIORITIZATION VIA DENSITY ESTIMATION}},
volume = {33},
year = {2007}
}
@article{,
file = {:Users/ashvin/Downloads/175cb1413aa109f9ab1332986c8926a2441b1541.pdf:pdf},
pages = {1--10},
title = {{UNSUPERVISED EXPLORATION WITH DEEP MODEL-BASED REINFORCEMENT LEARNING}},
year = {2019}
}
@article{Pere2018a,
archivePrefix = {arXiv},
arxivId = {arXiv:1803.00781v2},
eprint = {arXiv:1803.00781v2},
file = {:Users/ashvin/Downloads/2e66b4fbf44d3d11bda2730735f4df7aea42df04.pdf:pdf},
keywords = {autonomous goal setting,deep neural network,diversity,exploration,learning,unsupervised},
pages = {1--26},
title = {{LEARNING TO UNDERSTAND GOAL SPECIFICATIONS BY MODELLING REWARD}},
year = {2018}
}
@inproceedings{hausman2018skillembedding,
author = {Hausman, Karol and Springenberg, Jost Tobias and Wang, Ziyu and Heess, Nicolas and Riedmiller, Martin},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {:Users/ashvin/Downloads/29c35690ff52463c84d9456ab511e4c944ddbea4.pdf:pdf},
pages = {1--16},
title = {{Learning an Embedding Space for Transferable Robot Skills}},
year = {2018}
}
@article{chawla2002smote,
abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (nor-mal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
author = {Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
file = {::},
journal = {Journal of Artificial Intelligence Research},
pages = {321--357},
title = {{SMOTE: Synthetic Minority Over-sampling Technique}},
url = {https://arxiv.org/pdf/1106.1813.pdf},
volume = {16},
year = {2002}
}
@inproceedings{eysenbach18diayn,
abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ("Diversity is All You Need"), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.06070v5},
author = {Eysenbach, Benjamin and Brain, Google and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {arXiv:1802.06070v5},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Eysenbach et al. - Unknown - Diversity is All You Need Learning Skills without a Reward Function.pdf:pdf},
title = {{Diversity is All You Need: Learning Skills without a Reward Function}},
url = {https://sites.google.com/view/diayn/},
year = {2019}
}
@inproceedings{gupta18structuredexploration,
abstract = {Exploration is a fundamental challenge in reinforcement learning (RL). Many of the current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we explore how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm-model agnostic exploration with structured noise (MAESN)-to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy , producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods , RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: lo-comotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.07245v1},
author = {Gupta, Abhishek and Mendonca, Russell and Liu, Yuxuan and Abbeel, Pieter and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {arXiv:1802.07245v1},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Gupta et al. - 2018 - Meta-Reinforcement Learning of Structured Exploration Strategies(2).pdf:pdf},
title = {{Meta-Reinforcement Learning of Structured Exploration Strategies}},
url = {https://arxiv.org/pdf/1802.07245.pdf},
year = {2018}
}
@inproceedings{tang2017hashtag,
abstract = {Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.04717v3},
author = {Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Chen, Xi and Duan, Yan and Schulman, John and {De Turck}, Filip and Abbeel, Pieter},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {arXiv:1611.04717v3},
file = {::},
title = {{{\#}Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1611.04717.pdf},
year = {2017}
}
@inproceedings{rezende2014stochasticbackprop,
abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed genera-tive models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic back-propagation-rules for gradient backpropa-gation through stochastic variables-and derive an algorithm that allows for joint optimi-sation of the parameters of both the genera-tive and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1401.4082v3},
author = {Rezende, Danilo J and Mohamed, Shakir and Wierstra, Daan},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {arXiv:1401.4082v3},
file = {::},
title = {{Stochastic Backpropagation and Approximate Inference in Deep Generative Models}},
url = {https://arxiv.org/pdf/1401.4082.pdf},
year = {2014}
}
@inproceedings{Oh2017,
abstract = {As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new objective which encourages learning correspondences between similar subtasks by making analogies. For generalization over sequential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1706.05064v2},
author = {Oh, Junhyuk and Singh, Satinder and Lee, Honglak and Kohli, Pushmeet},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {arXiv:1706.05064v2},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Oh et al. - Unknown - Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning(2).pdf:pdf;::},
title = {{Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1706.05064.pdf https://sites.google.com/a/umich.},
year = {2017}
}
@article{Tang2016,
abstract = {Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.},
archivePrefix = {arXiv},
arxivId = {1611.04717},
author = {Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Chen, Xi and Duan, Yan and Schulman, John and {De Turck}, Filip and Abbeel, Pieter},
eprint = {1611.04717},
file = {::},
month = {nov},
title = {{{\#}Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1611.04717},
year = {2016}
}
@inproceedings{giusti15trails,
author = {Giusti, Alessandro and Guzzi, J{\'{e}}r{\^{o}}me Jerome and Cirean, Dan C and He, Fang-Lin and Rodr{\'{i}}guez, Juan P and Fontana, Flavio and Faessler, Matthias and Forster, Christian and Schmidhuber, J{\"{u}}rgen Jurgen and Caro, Gianni Di and Scaramuzza, Davide and Gambardella, Luca M and Ciresan, Dan C. and He, Fang-Lin and Rodriguez, Juan P. and Fontana, Flavio and Faessler, Matthias and Forster, Christian and Schmidhuber, J{\"{u}}rgen Jurgen and Caro, Gianni Di and Scaramuzza, Davide and Gambardella, Luca M},
booktitle = {IEEE Robotics and Automation Letters (RAL)},
doi = {10.1109/LRA.2015.2509024},
file = {::},
isbn = {9781467380256},
issn = {2377-3766},
keywords = {Aerial Robotics,Deep Learning,Index Terms—Visual-Based Navigation,Ma-chine Learning},
number = {2},
pages = {2377--3766},
pmid = {12546789},
title = {{A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots}},
url = {http://bit.ly/perceivingtrails. http://ieeexplore.ieee.org/document/7358076/},
volume = {1},
year = {2015}
}
@inproceedings{haarnoja2018sac,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing en-tropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation , our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
archivePrefix = {arXiv},
arxivId = {arXiv:1801.01290v2},
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {arXiv:1801.01290v2},
file = {::},
title = {{Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}},
url = {https://arxiv.org/pdf/1801.01290.pdf},
year = {2018}
}
@article{wang2018differentiablepogramming,
abstract = {Deep learning has seen tremendous success over the past decade in computer vision, machine translation, and gameplay. This success rests in crucial ways on gradient-descent optimization and the ability to "learn" parameters of a neural network by backpropagating observed errors. However, neural network architectures are growing increasingly sophisticated and diverse, which motivates an emerging quest for even more general forms of differentiable programming, where arbitrary parameterized computations can be trained by gradient descent. In this paper, we take a fresh look at automatic differentiation (AD) techniques, and especially aim to demystify the reverse-mode form of AD that generalizes backpropagation in neural networks. We uncover a tight connection between reverse-mode AD and delimited continuations, which permits implementing reverse-mode AD purely via operator overloading and without any auxiliary data structures. We further show how this formulation of AD can be fruitfully combined with multi-stage programming (staging), leading to a highly efficient implementation that combines the performance benefits of deep learning frameworks based on explicit reified computation graphs (e.g., TensorFlow) with the expressiveness of pure library approaches (e.g., PyTorch).},
archivePrefix = {arXiv},
arxivId = {arXiv:1803.10228v1},
author = {Wang, Fei and Decker, James},
eprint = {arXiv:1803.10228v1},
file = {::},
journal = {arXiv preprint arXiv:1803.10228},
title = {{Demystifying Differentiable Programming: Shift/Reset the Penultimate Backpropagator}},
url = {http://colah.github.io/posts/2015-09-NN-Types-FP/},
year = {2018}
}
@inproceedings{loftin2014discretehumanfeedback,
abstract = {This paper introduces two novel algorithms for learning behaviors from human-provided rewards. The primary novelty of these algorithms is that instead of treating the feedback as a numeric reward signal, they interpret feedback as a form of discrete communication that de-pends on both the behavior the trainer is trying to teach and the teaching strategy used by the trainer. For ex-ample, some human trainers use a lack of feedback to indicate whether actions are correct or incorrect, and interpreting this lack of feedback accurately can signif-icantly improve learning speed. Results from user stud-ies show that humans use a variety of training strategies in practice and both algorithms can learn a contextual bandit task faster than algorithms that treat the feed-back as numeric. Simulated trainers are also employed to evaluate the algorithms in both contextual bandit and sequential decision-making tasks with similar results.},
author = {Loftin, Robert and Macglashan, James and Peng, Bei and Taylor, Matthew E and Littman, Michael L and Huang, Jeff and Roberts, David L},
booktitle = {AAAI Conference on Artificial Intelligence},
file = {:Users/ashvin/Downloads/8579-38243-1-PB.pdf:pdf},
isbn = {9781577356783},
keywords = {Humans and AI},
pages = {937--943},
title = {{A Strategy-Aware Technique for Learning Behaviors from Discrete Human Feedback}},
year = {2014}
}
@inproceedings{saunders2018trialwithouterror,
abstract = {AI systems are increasingly applied to complex tasks that involve interaction with humans. During training, such systems are potentially dangerous, as they haven't yet learned to avoid actions that could cause serious harm. How can an AI system explore and learn without making a single mistake that harms humans or otherwise causes serious damage? For model-free reinforcement learning, having a human "in the loop" and ready to intervene is currently the only way to prevent all catastrophes. We formalize human intervention for RL and show how to reduce the human labor required by training a supervised learner to imitate the human's intervention decisions. We evaluate this scheme on Atari games, with a Deep RL agent being overseen by a human for four hours. When the class of catastrophes is simple, we are able to prevent all catastrophes without affecting the agent's learning (whereas an RL baseline fails due to catastrophic forgetting). However, this scheme is less successful when catastrophes are more complex: it reduces but does not eliminate catastrophes and the supervised learner fails on adversarial examples found by the agent. Extrapolating to more challenging environments, we show that our implementation would not scale (due to the infeasible amount of human labor required). We outline extensions of the scheme that are necessary if we are to train model-free agents without a single catastrophe. Link to videos that illustrate our approach on Atari games.},
archivePrefix = {arXiv},
arxivId = {arXiv:1707.05173v1},
author = {Saunders, William and Sastry, Girish and Stuhlm{\"{u}}ller, Andreas and Evans, Owain},
booktitle = {International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
eprint = {arXiv:1707.05173v1},
file = {::},
title = {{Trial without Error: Towards Safe Reinforcement Learning via Human Intervention}},
url = {https://arxiv.org/pdf/1707.05173.pdf},
year = {2018}
}
@inproceedings{torrey2013teachingbudget,
abstract = {This paper introduces a teacher-student framework for reinforcement learning. In this framework, a teacher agent instructs a student agent by suggesting actions the student should take as it learns. However, the teacher may only give such advice a limited number of times. We present several novel algorithms that teachers can use to budget their advice effectively, and we evaluate them in two experimental domains: Mountain Car and Pac-Man. Our results show that the same amount of advice, given at different moments, can have different effects on student learning, and that teachers can significantly affect student learning even when students use different learning methods and state representations.},
author = {Torrey, Lisa and Taylor, Matthew E},
booktitle = {International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
file = {::},
title = {{Teaching on a Budget: Agents Advising Agents in Reinforcement Learning}},
url = {www.ifaamas.org},
year = {2013}
}
@inproceedings{subramanian2016efd,
abstract = {Reinforcement Learning (RL) has been effectively used to solve complex problems given careful design of the problem and algorithm parameters. However standard RL approaches do not scale particularly well with the size of the problem and often require extensive engineering on the part of the designer to minimize the search space. To alleviate this problem, we present a model-free policy-based approach called Exploration from Demonstration (EfD) that uses human demonstrations to guide search space exploration. We use statistical measures of RL algorithms to provide feedback to the user about the agent's uncertainty and use this to solicit targeted demonstrations useful from the agent's perspective. The demonstrations are used to learn an exploration policy that actively guides the agent towards important aspects of the problem. We instantiate our approach in a gridworld and a popular arcade game and validate its performance under different experimental conditions. We show how EfD scales to large problems and provides convergence speed-ups over traditional exploration and interactive learning methods.},
author = {Subramanian, Kaushik and Isbell, Charles L and Thomaz, Andrea L},
booktitle = {International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
file = {::},
title = {{Exploration from Demonstration for Interactive Reinforcement Learning}},
url = {www.ifaamas.org},
year = {2016}
}
@inproceedings{frank2008rareevents,
abstract = {We consider the task of reinforcement learning in an environment in which rare significant events occur independently of the actions selected by the controlling agent. If these events are sampled according to their natural probability of occurring , convergence of conventional reinforcement learning algorithms is likely to be slow, and the learning algorithms may exhibit high variance. In this work, we assume that we have access to a simulator, in which the rare event probabilities can be artificially altered. Then, importance sampling can be used to learn with this simulation data. We introduce algorithms for policy evaluation, using both tabular and function approximation representations of the value function. We prove that in both cases, the reinforcement learning algorithms converge. In the tabular case, we also analyze the bias and variance of our approach compared to TD-learning. We evaluate empirically the performance of the algorithm on random Markov Decision Processes, as well as on a large network planning task.},
author = {Frank, Jordan and Mannor, Shie and Precup, Doina},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Reinforcement Learning in the Presence of Rare Events}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.4737{\&}rep=rep1{\&}type=pdf},
year = {2008}
}
@inproceedings{daniel2014activereward,
abstract = {While reward functions are an essential component of many robot learning methods, defining such functions remains a hard problem in many practical applications. For tasks such as grasping, there are no reliable success measures available. Defining reward functions by hand requires extensive task knowledge and often leads to undesired emergent behavior. Instead, we propose to learn the reward function through active learning, querying human expert knowledge for a subset of the agent's rollouts. We introduce a framework, wherein a traditional learning algorithm interplays with the reward learning component, such that the evolution of the action learner guides the queries of the reward learner. We demonstrate results of our method on a robot grasping task and show that the learned reward function generalizes to a similar task.},
author = {Daniel, Christian and Viering, Malte and Metz, Jan and Kroemer, Oliver and Peters, Jan},
booktitle = {Robotics: Science and Systems (RSS)},
file = {::},
title = {{Active Reward Learning}},
url = {http://www.roboticsproceedings.org/rss10/p31.pdf},
year = {2014}
}
@techreport{Fu,
abstract = {Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose AIRL, a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly out-performs prior methods in these transfer settings.},
archivePrefix = {arXiv},
arxivId = {arXiv:1710.11248v2},
author = {Fu, Justin and Luo, Katie and Levine, Sergey},
eprint = {arXiv:1710.11248v2},
file = {::},
title = {{LEARNING ROBUST REWARDS WITH ADVERSARIAL INVERSE REINFORCEMENT LEARNING}},
url = {https://arxiv.org/pdf/1710.11248.pdf}
}
@book{Keener2006,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Keener, Robert W.},
booktitle = {Design},
doi = {10.1016/j.peva.2007.06.006},
eprint = {arXiv:1011.1669v3},
file = {:Users/ashvin/Documents/stat210/2010{\_}Book{\_}TheoreticalStatistics.pdf:pdf},
isbn = {9780387781884},
issn = {01621459},
pages = {618},
pmid = {10911016},
title = {{Theoretical Statistics}},
url = {http://books.google.com/books?id=9tv0taI8l6YC},
volume = {102},
year = {2006}
}
@inproceedings{zamir2018taskonomy,
abstract = {Do visual tasks have a relationship, or are they unre-lated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable values ; it is the concept underlying transfer learning and provides a principled way for identifying redundancies across tasks, in order to, for instance, seamlessly reuse supervision among related tasks or solve many tasks in one system without piling up the complexity. We propose a fully computational approach for model-ing the structure of the space of visual tasks. This is done via finding (first and higher-order) transfer learning dependencies across a dictionary of twenty-six 2D, 2.5D, 3D, and semantic tasks in a latent space. The product is a computational taxonomic map for task transfer learning. We study the consequences of this structure, e.g. nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. For example, we show that the total number of labeled datapoints needed for solving a set of 10 tasks can be reduced by roughly 2 3 (compared to training independently) while keeping the performance nearly the same. We provide a set of tools for computing and probing this taxo-nomical structure including a solver that users can employ to devise efficient supervision policies for their use cases.},
author = {Zamir, Amir R and Sax, Alexander and Shen, William and Guibas, Leonidas and Malik, Jitendra and Savarese, Silvio},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {::},
title = {{Taskonomy: Disentangling Task Transfer Learning}},
url = {http://taskonomy.vision/},
year = {2018}
}
@inproceedings{Grant2018,
abstract = {Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al. (2017) as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1801.08930v1},
author = {Grant, Erin and Finn, Chelsea and Levine, Sergey and Darrell, Trevor and Griffiths, Thomas},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {arXiv:1801.08930v1},
file = {::},
title = {{Recasting Gradient-Based Meta-Learning As Hierarchical Bayes}},
url = {https://arxiv.org/pdf/1801.08930.pdf},
year = {2018}
}
@inproceedings{andreas2018latentlanguage,
abstract = {The named concepts and compositional operators present in natural language provide a rich source of information about the kinds of abstractions humans use to navigate the world. Can this linguistic background knowledge improve the generality and efficiency of learned classifiers and control policies? This paper aims to show that using the space of natural language strings as a parameter space is an effective way to capture natural task structure. In a pretraining phase, we learn a language interpretation model that transforms inputs (e.g. images) into outputs (e.g. labels) given natural language descriptions. To learn a new concept (e.g. a classifier), we search directly in the space of descriptions to minimize the inter-preter's loss on training examples. Crucially, our models do not require language data to learn these concepts: language is used only in pretraining to impose structure on subsequent learning. Results on image classification, text editing, and reinforcement learning show that, in all settings, models with a linguistic param-eterization outperform those without. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1711.00482v1},
author = {Andreas, Jacob and Klein, Dan and Levine, Sergey},
booktitle = {North American Chapter of the Association for Computational Linguistics (NAACL)},
eprint = {arXiv:1711.00482v1},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Andreas, Klein, Levine - 2018 - Learning with Latent Language(2).pdf:pdf},
title = {{Learning with Latent Language}},
url = {http://github.com/},
year = {2018}
}
@phdthesis{Duvallet2015,
author = {Duvallet, Felix},
file = {::},
school = {Carnegie Mellon University},
title = {{Natural Language Direction Following for Robots in Unstructured Unknown Environments}},
url = {https://felixduvallet.github.io/pubs/dissertation-felixd.pdf},
year = {2015}
}
@inproceedings{Belousov2016,
abstract = {Two seemingly contradictory theories attempt to explain how humans move to intercept an airborne ball. One theory posits that humans predict the ball trajectory to optimally plan future actions; the other claims that, instead of performing such complicated computations, humans employ heuristics to reactively choose appropriate actions based on immediate visual feedback. In this paper, we show that interception strategies appearing to be heuristics can be understood as computational solutions to the optimal control problem faced by a ball-catching agent acting under uncertainty. Modeling catching as a continuous partially observable Markov decision process and employing stochastic optimal control theory, we discover that the four main heuristics described in the literature are optimal solutions if the catcher has sufficient time to continuously visually track the ball. Specifically, by varying model parameters such as noise, time to ground contact, and perceptual latency, we show that different strategies arise under different circumstances. The catcher's policy switches between generating reactive and predictive behavior based on the ratio of system to observation noise and the ratio between reaction time and task duration. Thus, we provide a rational account of human ball-catching behavior and a unifying explanation for seemingly contradictory theories of target interception on the basis of stochastic optimal control.},
author = {Belousov, Boris and Neumann, Gerhard and Rothkopf, Constantin A and Peters, Jan},
booktitle = {Neural Information Processing Systems (NIPS)},
file = {::},
title = {{Catching heuristics are optimal control policies}},
url = {https://www.ias.informatik.tu-darmstadt.de/uploads/Site/EditPublication/Belousov{\_}ANIPS{\_}2016.pdf},
year = {2016}
}
@inproceedings{Fish2018,
abstract = {Humans effortlessly "program" one another by communicating goals and desires in natural language. In contrast, humans program robotic behaviours by indicating desired object locations and poses to be achieved [5], by providing RGB images of goal configurations [19], or supplying a demonstration to be imitated [17]. None of these methods generalize across environment variations, and they convey the goal in awkward technical terms. This work proposes joint learning of natural language grounding and in-structable behavioural policies reinforced by perceptual detectors of natural language expressions, grounded to the sensory inputs of the robotic agent. Our supervision is narrated visual demonstrations (NVD), which are visual demonstrations paired with verbal narration (as opposed to being silent). We introduce a dataset of NVD where teachers perform activities while describing them in detail. We map the teachers' descriptions to perceptual reward detectors, and use them to train corresponding behavioural policies in simulation. We empirically show that our instructable agents (i) learn visual reward detectors using a small number of examples by exploiting hard negative mined configurations from demonstration dynamics, (ii) develop pick-and-place policies using learned visual reward detectors, (iii) benefit from object-factorized state representations that mimic the syntactic structure of natural language goal expressions, and (iv) can execute behaviours that involve novel objects in novel locations at test time, instructed by natural language.},
archivePrefix = {arXiv},
arxivId = {arXiv:1804.10692v1},
author = {Fish, Hsiao-Yu and Adam, Tung and Harley, W and Huang, Liang-Kang and Fragkiadaki, Katerina},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
eprint = {arXiv:1804.10692v1},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Fish et al. - 2018 - Reward Learning from Narrated Demonstrations.pdf:pdf},
title = {{Reward Learning from Narrated Demonstrations}},
url = {https://arxiv.org/pdf/1804.10692.pdf},
year = {2018}
}
@inproceedings{Li2017,
abstract = {The goal of imitation learning is to mimic expert behavior without access to an explicit reward signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are typically not explicitly modeled. In this paper, we propose a new algorithm that can infer the latent structure of expert demonstrations in an unsupervised way. Our method, built on top of Generative Adversarial Imitation Learning, can not only imitate complex behaviors, but also learn interpretable and meaningful representations of complex behavioral data, including visual demonstrations. In the driving domain, we show that a model learned from human demonstrations is able to both accurately reproduce a variety of behaviors and accurately anticipate human actions using raw visual inputs. Compared with various baselines, our method can better capture the latent structure underlying expert demonstrations, often recovering semantically meaningful factors of variation in the data.},
author = {Li, Yunzhu and Song, Jiaming and Ermon, Stefano},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {::},
title = {{InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations}},
url = {https://arxiv.org/pdf/1703.08840.pdf},
year = {2017}
}
@inproceedings{Odonoghue2018,
abstract = {We consider the exploration/exploitation problem in reinforcement learning. For exploitation, it is well known that the Bellman equation connects the value at any time-step to the expected value at subsequent time-steps. In this paper we consider a similar uncertainty Bellman equation (UBE), which connects the uncertainty at any time-step to the expected uncertainties at subsequent time-steps, thereby extending the potential exploratory benefit of a policy beyond individual time-steps. We prove that the unique fixed point of the UBE yields an upper bound on the variance of the posterior distribution of the Q-values induced by any policy. This bound can be much tighter than traditional count-based bonuses that compound standard deviation rather than variance. Importantly, and unlike several existing approaches to optimism, this method scales naturally to large systems with complex generalization. Substituting our UBE-exploration strategy for-greedy improves DQN performance on 51 out of 57 games in the Atari suite.},
archivePrefix = {arXiv},
arxivId = {arXiv:1709.05380v3},
author = {O'donoghue, Brendan and Osband, Ian and Munos, Remi and Mnih, Volodymyr},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {arXiv:1709.05380v3},
file = {::},
title = {{The Uncertainty Bellman Equation and Exploration}},
url = {https://arxiv.org/pdf/1709.05380.pdf},
year = {2018}
}
@inproceedings{Dinh2017a,
abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling , inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations , a set of powerful, stably invertible, and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact and efficient sampling, exact and efficient inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation, and latent variable manipulations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1605.08803v3},
author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {arXiv:1605.08803v3},
file = {::},
title = {{Density Estimation Using Real NVP}},
url = {https://arxiv.org/pdf/1605.08803.pdf},
year = {2017}
}
@inproceedings{Dinh2015,
abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the determinant of the Jacobian and inverse Jacobian is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.8516v6},
author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {arXiv:1410.8516v6},
file = {::},
title = {{NICE: Non-linear Independent Components Estimation}},
url = {https://arxiv.org/pdf/1410.8516.pdf},
year = {2015}
}
@article{Plappert2018,
abstract = {The purpose of this technical report is twofold. First of all, it introduces a suite of challenging continuous control tasks (integrated with OpenAI Gym) based on currently existing robotics hardware. The tasks include pushing, sliding and pick {\&} place with a Fetch robotic arm as well as in-hand object manipulation with a Shadow Dexterous Hand. All tasks have sparse binary rewards and follow a Multi-Goal Reinforcement Learning (RL) framework in which an agent is told what to do using an additional input. The second part of the paper presents a set of concrete research ideas for improving RL algorithms, most of which are related to Multi-Goal RL and Hindsight Experience Replay.},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.09464v2},
author = {Plappert, Matthias and Andrychowicz, Marcin and Ray, Alex and Mcgrew, Bob and Baker, Bowen and Powell, Glenn and Schneider, Jonas and Tobin, Josh and Chociej, Maciek and Welinder, Peter and Kumar, Vikash and Zaremba, Wojciech},
eprint = {arXiv:1802.09464v2},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Plappert et al. - Unknown - Multi-Goal Reinforcement Learning Challenging Robotics Environments and Request for Research.pdf:pdf},
journal = {arXiv preprint arXiv:1802.09464},
title = {{Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research}},
url = {http://fetchrobotics.com/},
year = {2018}
}
@inproceedings{lee2017servo,
author = {Lee, Alex and Levine, Sergey and Abbeel, Pieter},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {::},
title = {{Learning Visual Servoing with Deep Features and Fitted Q-Iteration}},
url = {https://arxiv.org/pdf/1703.11000.pdf},
year = {2017}
}
@inproceedings{Srinivas2018,
abstract = {A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization. To this end, we introduce universal planning networks (UPN). UPNs embed differen-tiable planning within a goal-directed policy. This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images. The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image-based goals. We were able to achieve successful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1804.00645v2},
author = {Srinivas, Aravind and Jabri, Allan and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {arXiv:1804.00645v2},
file = {::},
title = {{Universal Planning Networks}},
url = {https://sites.google.},
year = {2018}
}
@article{Baranes2012,
abstract = {Highlights: 1) SAGG-RIAC is an architecture for active learning of inverse models in high-dimensional redundant spaces 2) This allows a robot to learn efficiently distributions of parameterized motor policies that solve a corresponding distribution of parameterized tasks 3) Active sampling of parameterized tasks, called active goal exploration, can be significantly faster than direct active sampling of parameterized policies 4) Active developmental exploration, based on competence progress, autonomously drives the system to progressively explore tasks of increasing learning complexity. Abstract We introduce the Self-Adaptive Goal Generation-Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsically motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots. This allows a robot to efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals. The architecture makes the robot sample actively novel parameterized tasks in the task space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy parameters that allow to solve it. For both learning and generalization, the system leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.4862v1},
author = {Baranes, Adrien and Oudeyer, Pierre-Yves},
doi = {10.1016/j.robot.2012.05.008},
eprint = {arXiv:1301.4862v1},
file = {::},
journal = {Robotics and Autonomous Systems},
number = {1},
pages = {49--73},
title = {{Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots}},
url = {http://dx.doi.org/10.1016/j.robot.2012.05.008},
volume = {61},
year = {2012}
}
@inproceedings{thomas2017independentlycontrollable,
abstract = {It has been postulated that a good representation is one that disentangles the underlying explanatory factors of variation. However, it remains an open question what kind of training framework could potentially achieve that. Whereas most previous work focuses on the static setting (e.g., with images), we postulate that some of the causal factors could be discovered if the learner is allowed to interact with its environment. The agent can experiment with different actions and observe their effects. More specifically, we hypothesize that some of these factors correspond to aspects of the environment which are independently controllable, i.e., that there exists a policy and a learnable feature for each such aspect of the environment, such that this policy can yield changes in that feature with minimal changes to other features that explain the statistical variations in the observed data. We propose a specific objective function to find such factors and verify experimentally that it can indeed disentangle independently controllable aspects of the environment without any extrinsic reward signal.},
author = {Thomas, Valentin and Pondard, Jules and Bengio, Emmanuel and Sarfati, Marc and Beaudoin, Philippe and Meurs, Marie-Jean and Pineau, Joelle and Precup, Doina and Bengio, Yoshua and Thoma, Valentin and Pineau, Joelle and Precup, Doina and Bengio, Yoshua},
booktitle = {NIPS Workshop},
file = {::},
keywords = {controllable features Acknowledgements,representation learning},
title = {{Independently Controllable Factors}},
url = {https://arxiv.org/pdf/1703.07718.pdf https://arxiv.org/pdf/1708.01289.pdf},
year = {2017}
}
@inproceedings{Pere2018,
abstract = {Intrinsically motivated goal exploration algorithms enable machines to discover repertoires of policies that produce a diversity of effects in complex environ-ments. These exploration algorithms have been shown to allow real world robots to acquire skills such as tool use in high-dimensional continuous state and action spaces. However, they have so far assumed that self-generated goals are sampled in a specifically engineered feature space, limiting their autonomy. In this work, we propose to use deep representation learning algorithms to learn an adequate goal space. This is a developmental 2-stage approach: first, in a perceptual learn-ing stage, deep learning algorithms use passive raw sensor observations of world changes to learn a corresponding latent space; then goal exploration happens in a second stage by sampling goals in this latent space. We present experiments where a simulated robot arm interacts with an object, and we show that explo-ration algorithms using such learned representations can match the performance obtained using engineered representations.},
author = {P{\'{e}}r{\'{e}}, Alexandre and Forestier, Sebastien and Sigaud, Olivier and Oudeyer, Pierre-Yves},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {::},
title = {{Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration}},
url = {https://arxiv.org/pdf/1803.00781.pdf},
year = {2018}
}
@article{Fernando,
abstract = {The scope of the Baldwin effect was recently called into question by two papers that closely examined the seminal work of Hinton and Nowlan. To this date there has been no demonstration of its necessity in empirically challenging tasks. Here we show that the Baldwin effect is capable of evolving few-shot supervised and re-inforcement learning mechanisms, by shaping the hyperparameters and the initial parameters of deep learning algorithms. Furthermore it can genetically accom-modate strong learning biases on the same set of problems as a recent machine learning algorithm called MAML " Model Agnostic Meta-Learning " which uses second-order gradients instead of evolution to learn a set of reference parameters (initial weights) that can allow rapid adaptation to tasks sampled from a distri-bution. Whilst in simple cases MAML is more data efficient than the Baldwin effect, the Baldwin effect is more general in that it does not require gradients to be backpropagated to the reference parameters or hyperparameters, and permits effectively any number of gradient updates in the inner loop. The Baldwin effect learns strong learning dependent biases, rather than purely genetically accommo-dating fixed behaviours in a learning independent manner.},
author = {Fernando, Chrisantha and Sygnowski, Jakub and Osindero, Simon and Wang, Jane and Schaul, Tom and Teplyashin, Denis and Sprechmann, Pablo and Pritzel, Alexander and Rusu, Andrei A},
file = {::},
title = {{Meta-Learning by the Baldwin Effect}},
url = {https://arxiv.org/pdf/1806.07917.pdf}
}
@inproceedings{Oh2018,
abstract = {This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent's past good de-cisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empiri-cal results show that SIL significantly improves advantage actor-critic (A2C) on several hard ex-ploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that SIL improves proximal policy optimization (PPO) on MuJoCo tasks.},
author = {Oh, Junhyuk and Guo, Yijie and Singh, Satinder and Lee, Honglak},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Self-Imitation Learning}},
url = {https://arxiv.org/pdf/1806.05635v1.pdf},
year = {2018}
}
@inproceedings{Dumoulin2017,
abstract = {We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and gen-eration networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with state-of-the-art on the semi-supervised SVHN and CIFAR10 tasks.},
author = {Dumoulin, Vincent and Belghazi, Ishmael and Poole, Ben and Mastropietro, Olivier and Lamb, Alex and Arjovsky, Martin and Courville, Aaron},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {::},
title = {{Adversarially Learned Inference}},
url = {https://arxiv.org/pdf/1606.00704.pdf},
year = {2017}
}
@inproceedings{thomas2018cad,
abstract = {— In this work, motivated by recent manufacturing trends, we investigate autonomous robotic assembly. Indus-trial assembly tasks require contact-rich manipulation skills, which are challenging to acquire using classical control and motion planning approaches. Consequently, robot controllers for assembly domains are presently engineered to solve a particular task, and cannot easily handle variations in the product or environment. Reinforcement learning (RL) is a promising approach for autonomously acquiring robot skills that involve contact-rich dynamics. However, RL relies on random exploration for learning a control policy, which requires many robot executions, and often gets trapped in locally suboptimal solutions. Instead, we posit that prior knowledge, when available, can improve RL performance. We exploit the fact that in modern assembly domains, geometric information about the task is readily available via the CAD design files. We propose to leverage this prior knowledge by guiding RL along a geometric motion plan, calculated using the CAD data. We show that our approach effectively improves over traditional control approaches for tracking the motion plan, and can solve assembly tasks that require high precision, even without accurate state estimation. In addition, we propose a neural network architecture that can learn to track the motion plan, thereby generalizing the assembly controller to changes in the object positions.},
author = {Thomas, Garrett and Chien, Melissa and Tamar, Aviv and Ojea, Juan Aparicio and Abbeel, Pieter},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {::},
title = {{Learning Robotic Assembly from CAD}},
url = {https://arxiv.org/pdf/1803.07635.pdf},
year = {2018}
}
@inproceedings{ng1999rewardshaping,
author = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Policy invariance under reward transformations: Theory and application to reward shaping}},
url = {https://www-cs.stanford.edu/people/ang/papers/shaping-icml99.pdf},
year = {1999}
}
@inproceedings{rajeswaran2018dextrous,
abstract = {— Dexterous multi-fingered hands are extremely versatile and provide a generic way to perform multiple tasks in human-centric environments. However, effectively control-ling them remains challenging due to their high dimensionality and large number of potential contacts. Deep reinforcement learning (DRL) provides a model-agnostic approach to control complex dynamical systems, but has not been shown to scale to high-dimensional dexterous manipulation. Furthermore, deployment of DRL on physical systems remains challenging due to sample inefficiency. Thus, the success of DRL in robotics has thus far been limited to simpler manipulators and tasks. In this work, we show that model-free DRL with natural policy gradients can effectively scale up to complex manipulation tasks with a high-dimensional 24-DoF hand, and solve them from scratch in simulated experiments. Furthermore, with the use of a small number of human demonstrations, the sample complexity can be significantly reduced, and enable learning within the equivalent of a few hours of robot experience. We demonstrate successful policies for multiple complex tasks: object relocation, in-hand manipulation, tool use, and door opening. Supplementary video link 1 .},
author = {Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Schulman, John and Todorov, Emanuel and Levine, Sergey},
booktitle = {Robotics: Science and Systems},
file = {::},
title = {{Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations}},
url = {https://arxiv.org/pdf/1709.10087.pdf},
year = {2018}
}
@inproceedings{agrawal2016poking,
abstract = {We investigate an experiential learning paradigm for acquiring an internal model of intuitive physics. Our model is evaluated on a real-world robotic manipulation task that requires displacing objects to target locations by poking. The robot gathered over 400 hours of experience by executing more than 100K pokes on different objects. We propose a novel approach based on deep neural networks for modeling the dynamics of robot's interactions directly from images, by jointly estimating forward and inverse models of dynamics. The inverse model objective provides supervision to construct informative visual features, which the forward model can then predict and in turn regularize the feature space for the inverse model. The interplay between these two objectives creates useful, accurate models that can then be used for multi-step decision making. This formulation has the additional benefit that it is possible to learn forward models in an abstract feature space and thus alleviate the need of predicting pixels. Our experiments show that this joint modeling approach outperforms alternative methods.},
archivePrefix = {arXiv},
arxivId = {1606.07419},
author = {Agrawal, Pulkit and Nair, Ashvin and Abbeel, Pieter and Malik, Jitendra and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {1606.07419},
file = {::},
issn = {10495258},
title = {{Learning to Poke by Poking: Experiential Learning of Intuitive Physics}},
url = {http://arxiv.org/abs/1606.07419},
year = {2016}
}
@inproceedings{lange2012autonomous,
abstract = {We propose a learning architecture, that is able to do reinforcement learning based on raw visual input data. In contrast to previous approaches, not only the control policy is learned. In order to be successful, the system must also autonomously learn, how to extract relevant information out of a high-dimensional stream of input information, for which the semantics are not provided to the learning system. We give a first proof-of-concept of this novel learning architecture on a challenging benchmark, namely visual control of a racing slot car. The resulting policy, learned only by success or failure, is hardly beaten by an experienced human player.},
author = {Lange, Sascha and Riedmiller, Martin and Voigtlander, Arne and Voigtl{\"{a}}nder, Arne},
booktitle = {International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN.2012.6252823},
file = {:Users/ashvin/Downloads/Autonomous{\_}reinforcement{\_}learning{\_}on{\_}raw{\_}visual{\_}in.pdf:pdf},
isbn = {9781467314909},
issn = {2161-4393},
number = {June},
organization = {IEEE},
pages = {1--8},
title = {{Autonomous reinforcement learning on raw visual input data in a real world application}},
year = {2012}
}
@inproceedings{kingma2014vae,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differ-entiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using stan-dard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made espe-cially efficient by fitting an approximate inference model (also called a recogni-tion model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
author = {Kingma, Diederik P and Welling, Max},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Welling - Unknown - Auto-Encoding Variational Bayes(2).pdf:pdf;:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Welling - Unknown - Auto-Encoding Variational Bayes.pdf:pdf},
title = {{Auto-Encoding Variational Bayes}},
url = {https://arxiv.org/pdf/1312.6114.pdf},
year = {2014}
}
@article{levine2016gps,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
archivePrefix = {arXiv},
arxivId = {1504.00702},
author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1504.00702},
file = {::;::},
isbn = {9781479969227},
issn = {15337928},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {Neural Networks,Optimal Control,Reinforcement Learning,Vision},
number = {1},
pages = {1334--1373},
pmid = {15003161},
publisher = {JMLR. org},
title = {{End-to-End Training of Deep Visuomotor Policies}},
url = {https://arxiv.org/pdf/1504.00702.pdf},
volume = {17},
year = {2016}
}
@article{bojarski2016nvidia,
abstract = {We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).},
archivePrefix = {arXiv},
arxivId = {1604.07316},
author = {Bojarski, Mariusz and {Del Testa}, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol and Testa, Davide Del and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
eprint = {1604.07316},
file = {::;::},
journal = {CoRR},
pages = {1--9},
title = {{End to End Learning for Self-Driving Cars}},
url = {https://arxiv.org/pdf/1604.07316.pdf http://arxiv.org/abs/1604.07316},
volume = {abs/1604.0},
year = {2016}
}
@inproceedings{mnih2016asynchronous,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforce-ment learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
author = {Mnih, Volodymyr and {Puigdom{\`{e}}nech Badia}, Adri{\`{a}} and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P and Silver, David and Kavukcuoglu, Koray and Com, Korayk@google and Deepmind, Google},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1602.01783.pdf},
year = {2016}
}
@inproceedings{hester17dqfd,
archivePrefix = {arXiv},
arxivId = {1704.03732},
author = {Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Dulac-Arnold, Gabriel and Osband, Ian and Agapiou, John and Leibo, Joel Z and Gruslys, Audrunas},
booktitle = {AAAI Conference on Artificial Intelligence},
eprint = {1704.03732},
file = {::},
title = {{Learning from Demonstrations for Real World Reinforcement Learning}},
url = {https://arxiv.org/pdf/1704.03732.pdf http://arxiv.org/abs/1704.03732},
year = {2018}
}
@article{jordan1992forward,
abstract = {Internal models of the environment h a v e an important role to play in adap-tive systems in general and are of particular importance for the supervised learning paradigm. In this paper we demonstrate that certain classical prob-lems associated with the notion of the eteacher" in supervised learning can be solved by judicious use of learned internal models as components of the adap-tive system. In particular, we show h o w supervised learning algorithms can be utilized in cases in which an unknown dynamical system intervenes between actions and desired outcomes. Our approach applies to any supervised learning algorithm that is capable of learning in multi-layer networks.},
author = {Jordan, Michael I and Rumelhart, David E and Mozer, Michael and Barto, Andrew and Jacobs, Robert and Loeb, Eric and Mcclelland, James},
file = {::},
journal = {Cognitive Science},
number = {3},
pages = {307--354},
publisher = {Wiley Online Library},
title = {{Forward models: Supervised learning with a distal teacher}},
url = {https://pdfs.semanticscholar.org/bea6/713f0bdd8069d734846fc532660c3152d027.pdf},
volume = {16},
year = {1992}
}
@inproceedings{torralba,
author = {Vondrick, Carl and Pirsiavash, Hamed and Torralba, Antonio},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {{Anticipating Visual Representations from Unlabeled Video}},
year = {2016}
}
@inproceedings{nair2017icra,
abstract = {{\textcopyright} 2017 IEEE. Manipulation of deformable objects, such as ropes and cloth, is an important but challenging problem in robotics. We present a learning-based system where a robot takes as input a sequence of images of a human manipulating a rope from an initial to goal configuration, and outputs a sequence of actions that can reproduce the human demonstration, using only monocular images as input. To perform this task, the robot learns a pixel-level inverse dynamics model of rope manipulation directly from images in a self-supervised manner, using about 60K interactions with the rope collected autonomously by the robot. The human demonstration provides a high-level plan of what to do and the low-level inverse model is used to execute the plan. We show that by combining the high and low-level plans, the robot can successfully manipulate a rope into a variety of target shapes using only a sequence of human-provided images for direction.},
archivePrefix = {arXiv},
arxivId = {1703.02018},
author = {Nair, Ashvin and Chen, Dian and Agrawal, Pulkit and Isola, Phillip and Abbeel, Pieter and Malik, Jitendra and Levine, Sergey and Chen, Dian and Isola, Phillip and Abbeel, Pieter and Malik, Jitendra and Levine, Sergey},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2017.7989247},
eprint = {1703.02018},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/rope-manipulation-Nair17.pdf:pdf},
isbn = {9781509046331},
issn = {10504729},
title = {{Combining Self-Supervised Learning and Imitation for Vision-Based Rope Manipulation}},
year = {2017}
}


@article{kineticsdataset,
  author    = {Will Kay and
               Jo{\~{a}}o Carreira and
               Karen Simonyan and
               Brian Zhang and
               Chloe Hillier and
               Sudheendra Vijayanarasimhan and
               Fabio Viola and
               Tim Green and
               Trevor Back and
               Paul Natsev and
               Mustafa Suleyman and
               Andrew Zisserman},
  title     = {The Kinetics Human Action Video Dataset},
  journal   = {Computing Research Repository (CoRR)},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.06950},
  archivePrefix = {arXiv},
  eprint    = {1705.06950},
  timestamp = {Mon, 13 Aug 2018 16:46:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KayCSZHVVGBNSZ17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{andrychowicz2017her,
archivePrefix = {arXiv},
arxivId = {1707.01495},
author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and Mcgrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {1707.01495},
file = {::},
title = {{Hindsight Experience Replay}},
url = {https://arxiv.org/pdf/1707.01495.pdf http://arxiv.org/abs/1707.01495},
year = {2017}
}

@article{zhang2018natrl,
author    = {Amy Zhang and
               Yuxin Wu and
               Joelle Pineau},
  title     = {Natural Environment Benchmarks for Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1811.06032},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.06032},
  eprinttype = {arXiv},
  eprint    = {1811.06032},
  timestamp = {Sat, 23 Jan 2021 01:12:03 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-06032.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ebert2017videoprediction,
abstract = {In order to autonomously learn wide repertoires of complex skills, robots must be able to learn from their own autonomously collected data, without human supervision. One learning signal that is always available for autonomously collected data is prediction. If a robot can learn to predict the future, it can use this predictive model to take actions to produce desired outcomes, such as mov-ing an object to a particular location. However, in complex open-world scenarios, designing a representation for prediction is difficult. In this work, we instead aim to enable self-supervised robot learning through direct video prediction: instead of attempting to design a good representation, we directly predict what the robot will see next, and then use this model to achieve desired goals. A key challenge in video prediction for robotic manipulation is handling complex spatial arrange-ments such as occlusions. To that end, we introduce a video prediction model that can keep track of objects through occlusion by incorporating temporal skip-connections. Together with a novel planning criterion and action space formu-lation, we demonstrate that this model substantially outperforms prior work on video prediction-based control. Our results show manipulation of objects not seen during training, handling multiple objects, and pushing objects around obstruc-tions. These results represent a significant advance in the range and complexity of skills that can be performed entirely with self-supervised robot learning.},
author = {Ebert, Frederik and Finn, Chelsea and Lee, Alex X and Levine, Sergey},
booktitle = {Conference on Robot Learning (CoRL)},
file = {::},
keywords = {deep learning,manipulation,model-based rein-forcement learning,video prediction},
title = {{Self-Supervised Visual Planning with Temporal Skip Connections}},
url = {https://128.84.21.199/pdf/1710.05268.pdf https://arxiv.org/pdf/1710.05268.pdf},
year = {2017}
}
@inproceedings{watter2015embed,
abstract = {We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.},
archivePrefix = {arXiv},
arxivId = {1506.07365},
author = {Watter, Manuel and Springenberg, Jost Tobias and Boedecker, Joschka and Riedmiller, Martin},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {1506.07365},
file = {::;::},
issn = {10495258},
pages = {2728--2736},
title = {{Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images}},
url = {https://arxiv.org/pdf/1506.07365.pdf http://arxiv.org/abs/1506.07365},
year = {2015}
}

@article{jaime1983analogy,
author = {Carbonell, Jaime},
year = {1983},
month = {12},
pages = {},
title = {Learning by Analogy: Formulating and Generalizing Plans from Past Experience},
isbn = {978-3-662-12407-9},
doi = {10.1007/978-3-662-12405-5_5}
}

@inproceedings{lange2010deep,
  author = {Lange, Sascha and Riedmiller, Martin A.},
  biburl = {https://www.bibsonomy.org/bibtex/2dfc9feb2a003d07d75c58edb9923b8ca/dblp},
  booktitle = {IJCNN},
  ee = {http://dx.doi.org/10.1109/IJCNN.2010.5596468},
  interhash = {0fbdebb8b96b35761e40c6d066995d8a},
  intrahash = {dfc9feb2a003d07d75c58edb9923b8ca},
  isbn = {978-1-4244-6916-1},
  keywords = {dblp},
  pages = {1-8},
  publisher = {IEEE},
  timestamp = {2015-06-21T08:10:50.000+0200},
  title = {Deep auto-encoder neural networks in reinforcement learning.},
  url = {http://dblp.uni-trier.de/db/conf/ijcnn/ijcnn2010.html#LangeR10},
  year = 2010
}

@article{popov17stacking,
abstract = {Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.},
archivePrefix = {arXiv},
arxivId = {1704.03073},
author = {Popov, Ivaylo and Heess, Nicolas and Lillicrap, Timothy and Hafner, Roland and Barth-maron, Gabriel and Vecerik, Matej and Lampe, Thomas and Tassa, Yuval and Erez, Tom and Riedmiller, Martin and Deepmind, Martin Riedmiller},
eprint = {1704.03073},
file = {::},
journal = {CoRR},
keywords = {popov2017stacking},
title = {{Data-efficient Deep Reinforcement Learning for Dexterous Manipulation}},
url = {https://arxiv.org/pdf/1704.03073.pdf},
volume = {abs/1704.0},
year = {2017}
}
@inproceedings{rauber2017hindsight,
abstract = {Goal-conditional policies allow reinforcement learning agents to pursue specific goals during different episodes. In addition to their potential to generalize desired behavior to unseen goals, such policies may also help in defining options for arbi-trary subgoals, enabling higher-level planning. While trying to achieve a specific goal, an agent may also be able to exploit information about the degree to which it has achieved alternative goals. Reinforcement learning agents have only recently been endowed with such capacity for hindsight, which is highly valuable in environ-ments with sparse rewards. In this paper, we show how hindsight can be introduced to likelihood-ratio policy gradient methods, generalizing this capacity to an entire class of highly successful algorithms. Our preliminary experiments suggest that hindsight may increase the sample efficiency of policy gradient methods.},
author = {Rauber, Paulo and Mutz, Filipe and Schmidhuber, Juergen J{\"{u}}rgen},
booktitle = {CoRR},
file = {::},
title = {{Hindsight policy gradients}},
url = {https://arxiv.org/pdf/1711.06006.pdf},
volume = {abs/1711.0},
year = {2017}
}
@inproceedings{lillicrap2015continuous,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the de-terministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our al-gorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is com-petitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies " end-to-end " : directly from raw pixel in-puts.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
booktitle = {International Conference on Learning Representations (ICLR)},
doi = {10.1613/jair.301},
eprint = {9605103},
file = {::;:Users/ashvin/code/kindlize/pdfs/deeprl/continuous-control-Lillicrap16.pdf:pdf},
isbn = {0-7803-3213-X},
issn = {10769757},
pmid = {17255001},
primaryClass = {cs},
title = {{Continuous control with deep reinforcement learning}},
url = {https://arxiv.org/pdf/1509.02971.pdf},
year = {2016}
}
@inproceedings{kober2008mp,
abstract = {Many motor skills in humanoid robotics can be learned using parametrized motor primitives as done in imitation learning. However, most interesting motor learn-ing problems are high-dimensional reinforcement learning problems often beyond the reach of current methods. In this paper, we extend previous work on policy learning from the immediate reward case to episodic reinforcement learning. We show that this results in a general, common framework also connected to pol-icy gradient methods and yielding a novel algorithm for policy learning that is particularly well-suited for dynamic motor primitives. The resulting algorithm is an EM-inspired algorithm applicable to complex motor learning tasks. We compare this algorithm to several well-known parametrized policy search methods and show that it outperforms them. We apply it in the context of motor learning and show that it can learn a complex Ball-in-a-Cup task using a real Barrett WAM TM robot arm.},
author = {Kober, Jens and Peter, J.},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
doi = {10.1007/978-3-319-03194-1_4},
file = {::},
isbn = {1099401052236},
issn = {1610742X},
pages = {83--117},
title = {{Policy search for motor primitives in robotics}},
url = {http://papers.nips.cc/paper/3545-policy-search-for-motor-primitives-in-robotics.pdf},
volume = {97},
year = {2008}
}
@inproceedings{deisenroth2011pilco,
abstract = {In this paper, we introduce pilco, a practi-cal, data-efficient model-based policy search method. Pilco reduces model bias, one of the key problems of model-based reinforce-ment learning, in a principled way. By learn-ing a probabilistic dynamics model and ex-plicitly incorporating model uncertainty into long-term planning, pilco can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-of-the-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprece-dented learning efficiency on challenging and high-dimensional control tasks.},
author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
pages = {465--472},
title = {{PILCO: A model-based and data-efficient approach to policy search}},
url = {http://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf},
year = {2011}
}
@inproceedings{gu2016naf,
abstract = {Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of model-free algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized adantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.},
archivePrefix = {arXiv},
arxivId = {1603.00748},
author = {Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey},
booktitle = {International Conference on Machine Learning (ICML)},
doi = {10.3390/robotics2030122},
eprint = {1603.00748},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/continuous-deepq-qlearning-with-model-based-acceleration-Gu16.pdf:pdf;::},
isbn = {3405062780},
issn = {{\textless}null{\textgreater}},
pmid = {21487784},
title = {{Continuous Deep Q-Learning with Model-based Acceleration}},
url = {https://arxiv.org/pdf/1603.00748.pdf http://arxiv.org/abs/1603.00748},
year = {2016}
}
@inproceedings{oh2015action,
abstract = {Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the pro-posed architectures are able to generate visually-realistic frames that are also use-ful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.},
author = {Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard and Singh, Satinder},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {::},
title = {{Action-Conditional Video Prediction using Deep Networks in Atari Games}},
url = {https://arxiv.org/pdf/1507.08750v1.pdf},
year = {2015}
}
@article{henderson2017deep,
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
journal = {arXiv preprint arXiv:1709.06560},
title = {{Deep reinforcement learning that matters}},
year = {2017}
}
@inproceedings{inoue1985hand,
author = {Inoue, H and Inaba, M},
booktitle = {Robotics Research: The First International Symposium},
title = {{Hand-eye coordination in rope handling}},
volume = {1},
year = {1985}
}
@book{winograd72shrdlr,
author = {Winograd, Terry},
publisher = {Academic Press},
title = {{Understanding Natural Language}},
year = {1972}
}
@article{billiards,
author = {Fragkiadaki, Katerina and Agrawal, Pulkit and Levine, Sergey and Malik, Jitendra},
journal = {International Conference on Learning Representations (ICLR)},
title = {{Learning Visual Predictive Models of Physics for Playing Billiards}},
year = {2016}
}
@article{burgess2018understanding,
author = {Burgess, Christopher P and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
journal = {arXiv preprint arXiv:1804.03599},
title = {{Understanding disentangling in {\$}\beta{\$}-VAE}},
year = {2018}
}
@article{machado2018eigenoption,
author = {Machado, Marlos C and Rosenbaum, Clemens and Guo, Xiaoxiao and Liu, Miao and Tesauro, Gerald and Campbell, Murray},
journal = {International Conference on Learning Representations (ICLR)},
title = {{Eigenoption Discovery through the Deep Successor Representation}},
year = {2018}
}
@inproceedings{kopicki2011learning,
author = {Kopicki, Marek and Zurek, Sebastian and Stolkin, Rustam and M{\"{o}}rwald, Thomas and Wyatt, Jeremy},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {5722--5729},
title = {{Learning to predict how rigid objects behave under simple manipulation}},
year = {2011}
}
@article{kuniyoshi1994learning,
author = {Kuniyoshi, Yasuo and Inaba, Masayuki and Inoue, Hirochika},
journal = {IEEE Transactions on robotics and automation},
number = {6},
pages = {799--822},
publisher = {IEEE},
title = {{Learning by watching: Extracting reusable task knowledge from visual observation of human performance}},
volume = {10},
year = {1994}
}
@article{jaderberg2016auxiliary,
author = {Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray},
journal = {International Conference on Learning Representations (ICLR)},
title = {{Reinforcement learning with unsupervised auxiliary tasks}},
year = {2017}
}
@book{Bellman:DynamicProgramming,
  abstract = {{An introduction to the mathematical theory of multistage decision processes, this text takes a "functional equation" approach to the discovery of optimum policies. Written by a leading developer of such policies, it presents a series of methods, uniqueness and existence theorems, and examples for solving the relevant equations. The text examines existence and uniqueness theorems, the optimal inventory equation, bottleneck problems in multistage production processes, a new formalism in the calculus of variation, strategies behind multistage games, and Markovian decision processes. Each chapter concludes with a problem set that Eric V. Denardo of Yale University, in his informative new introduction, calls "a rich lode of applications and research topics." 1957 edition. 37 figures.}},
  added-at = {2011-08-17T16:08:47.000+0200},
  author = {Bellman, Richard},
  biburl = {https://www.bibsonomy.org/bibtex/29cdd821222218ded252c8ba5cd712666/pcbouman},
  interhash = {acf948462171ca060064a7ded257a792},
  intrahash = {9cdd821222218ded252c8ba5cd712666},
  isbn = {9780486428093},
  keywords = {book dynamic programming},
  publisher = {Dover Publications},
  timestamp = {2011-08-18T09:10:27.000+0200},
  title = {{Dynamic Programming}},
  year = 1957
}



@inproceedings{barreto_successor_2017,
	title = {Successor {Features} for {Transfer} in {Reinforcement} {Learning}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/file/350db081a661525235354dd3e19b8c05-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Barreto, Andre and Dabney, Will and Munos, Remi and Hunt, Jonathan J and Schaul, Tom and van Hasselt, Hado P and Silver, David},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@article{hansen2001completely,
author = {Hansen, Nikolaus and Ostermeier, Andreas},
journal = {Evolutionary computation},
number = {2},
pages = {159--195},
publisher = {MIT Press},
title = {{Completely derandomized self-adaptation in evolution strategies}},
volume = {9},
year = {2001}
}
@article{pinto2016curious,
author = {Pinto, Lerrel and Gandhi, Dhiraj and Han, Yuanfeng and Park, Yong-Lae and Gupta, Abhinav},
journal = {European Conference on Computer Vision (ECCV)},
title = {{The Curious Robot: Learning Visual Representations via Physical Interactions}},
year = {2016}
}
@inproceedings{lau2011automatic,
author = {Lau, Manfred and Mitani, Jun and Igarashi, Takeo},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {3733--3738},
title = {{Automatic learning of pushing strategy for delivery of irregular-shaped objects}},
year = {2011}
}
@article{kingma2014adam,
author = {Kingma, Diederik and Ba, Jimmy},
journal = {International Conference on Learning Representations (ICLR)},
title = {{Adam: A method for stochastic optimization}},
year = {2015}
}
@article{ponomarenko2015image,
author = {Ponomarenko, Nikolay and Jin, Lina and Ieremeiev, Oleg and Lukin, Vladimir and Egiazarian, Karen and Astola, Jaakko and Vozel, Benoit and Chehdi, Kacem and Carli, Marco and Battisti, Federica and Others},
journal = {Signal Processing: Image Communication},
pages = {57--77},
publisher = {Elsevier},
title = {{Image database TID2013: Peculiarities, results and perspectives}},
volume = {30},
year = {2015}
}
@article{schmidhuber1992learning,
author = {Schmidhuber, J{\"{u}}rgen},
journal = {Neural Computation},
number = {6},
pages = {863--879},
publisher = {MIT Press},
title = {{Learning factorial codes by predictability minimization}},
volume = {4},
year = {1992}
}
@inproceedings{abbeel2004apprenticeship,
author = {Abbeel, Pieter and Ng, Andrew Y},
booktitle = {International Conference on Machine Learning (ICML)},
pages = {1},
title = {{Apprenticeship learning via inverse reinforcement learning}},
year = {2004}
}
@article{aksoy2011learning,
author = {Aksoy, Eren Erdal and Abramov, Alexey and D{\"{o}}rr, Johannes and Ning, Kejun and Dellen, Babette and W{\"{o}}rg{\"{o}}tter, Florentin},
journal = {The International Journal of Robotics Research},
pages = {0278364911410459},
publisher = {Sage Publications},
title = {{Learning the semantics of object--action relations by observation}},
year = {2011}
}
@inproceedings{saha2006motion,
author = {Saha, Mitul and Isto, Pekka},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {2478--2484},
title = {{Motion planning for robotic manipulation of deformable linear objects}},
year = {2006}
}
@article{ha2018world,
author = {Ha, David and Schmidhuber, J{\"{u}}rgen},
journal = {arXiv preprint arXiv:1803.10122},
title = {{World Models}},
year = {2018}
}
@inproceedings{DBLP:conf/bmvc/KyriazisOA11,
author = {Kyriazis, Nikolaos and Oikonomidis, Iason and Argyros, Antonis A},
booktitle = {British Machine Vision Conference, {\{}BMVC{\}} 2011, Dundee, UK, August 29 - September 2, 2011. Proceedings},
doi = {10.5244/C.25.43},
pages = {1--11},
title = {{Binding Computer Vision to Physics Based Simulation: The Case Study of a Bouncing Ball}},
url = {http://dx.doi.org/10.5244/C.25.43},
year = {2011}
}
@article{lee2013syntactic,
author = {Lee, Kyuhwa and Su, Yanyu and Kim, Tae-Kyun and Demiris, Yiannis},
journal = {Robotics and Autonomous Systems},
number = {12},
pages = {1323--1334},
publisher = {Elsevier},
title = {{A syntactic approach to robot imitation learning using probabilistic activity grammars}},
volume = {61},
year = {2013}
}
@inproceedings{katz2008manipulating,
author = {Katz, Dov and Brock, Oliver},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {272--277},
title = {{Manipulating articulated objects with interactive perception}},
year = {2008}
}
@article{florensa2017stochastic,
author = {Florensa, Carlos and Duan, Yan and Abbeel, Pieter},
journal = {arXiv preprint arXiv:1704.03012},
title = {{Stochastic neural networks for hierarchical reinforcement learning}},
year = {2017}
}
@article{mccloskey1983intuitive,
author = {McCloskey, Michael},
journal = {Scientific american},
number = {4},
pages = {122--130},
title = {{Intuitive physics}},
volume = {248},
year = {1983}
}
@article{pinto2015supersizing,
author = {Pinto, Lerrel and Gupta, Abhinav},
journal = {IEEE International Conference on Robotics and Automation (ICRA)},
title = {{Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours}},
year = {2016}
}
@inproceedings{schulman2013warping,
author = {Schulman, John and Gupta, Ankush and Venkatesan, Sibi and Tayson-Frederick, Mallory and Abbeel, Pieter},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
title = {{A Case Study of Trajectory Transfer Through Non-Rigid Registration for a Simplified Suturing Scenario}},
year = {2013}
}
@article{mericcli2015push,
author = {Meri{\c{c}}li, Tekin and Veloso, Manuela and Ak$\backslash$in, H Levent},
journal = {Autonomous Robots},
number = {3},
pages = {317--329},
publisher = {Springer},
title = {{Push-manipulation of complex passive mobile objects using experimentally acquired motion models}},
volume = {38},
year = {2015}
}
@inproceedings{kietzmann2009neuro,
author = {Kietzmann, Tim C and Riedmiller, Martin},
booktitle = {International Conference on Machine Learning (ICML)},
organization = {IEEE},
pages = {311--316},
title = {{The neuro slot car racer: Reinforcement learning in a real world setting}},
year = {2009}
}
@inproceedings{chui2000tpsrpm,
author = {Chui, Haili and Rangarajan, Anand},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {{A new algorithm for non-rigid point matching}},
year = {2000}
}
@inproceedings{todorov2003unsupervised,
author = {Todorov, Emanuel and Ghahramani, Zoubin},
booktitle = {Engineering in Medicine and Biology Society, 2003. Proceedings of the 25th Annual International Conference of the IEEE},
organization = {IEEE},
pages = {1750--1753},
title = {{Unsupervised learning of sensory-motor primitives}},
volume = {2},
year = {2003}
}
@article{zhang2018unreasonable,
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
journal = {Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {{The Unreasonable Effectiveness of Deep Features as a Perceptual Metric}},
year = {2018}
}
@article{pinto2017asymmetric,
author = {Pinto, Lerrel and Andrychowicz, Marcin and Welinder, Peter and Zaremba, Wojciech and Abbeel, Pieter},
journal = {Robotics: Science and Systems (RSS)},
title = {{Asymmetric Actor Critic for Image-Based Robot Learning}},
year = {2018}
}
@article{desjardins2012disentangling,
author = {Desjardins, Guillaume and Courville, Aaron and Bengio, Yoshua},
journal = {CoRR},
title = {{Disentangling factors of variation via generative entangling}},
volume = {abs/1210.5},
year = {2012}
}
@book{crowell2012introduction,
author = {Crowell, Richard H and Fox, Ralph Hartzler},
publisher = {Springer Science {\&} Business Media},
title = {{Introduction to knot theory}},
volume = {57},
year = {2012}
}
@article{lerer2016learning,
author = {Lerer, Adam and Gross, Sam and Fergus, Rob},
journal = {International Conference on Machine Learning (ICML)},
title = {{Learning Physical Intuition of Block Towers by Example}},
year = {2016}
}
@article{cheung2014discovering,
author = {Cheung, Brian and Livezey, Jesse A and Bansal, Arjun K and Olshausen, Bruno A},
journal = {arXiv preprint arXiv:1412.6583},
title = {{Discovering hidden factors of variation in deep networks}},
year = {2014}
}
@inproceedings{schulman2013generalization,
author = {Schulman, John and Ho, Jonathan and Lee, Cameron and Abbeel, Pieter},
booktitle = {Proceedings of the 16th International Symposium on Robotics Research (ISRR)},
title = {{Generalization in robotic manipulation through the use of non-rigid registration}},
year = {2013}
}
@article{wolpert1995internal,
author = {Wolpert, Daniel M and Ghahramani, Zoubin and Jordan, Michael I},
journal = {Science-AAAS-Weekly Paper Edition},
number = {5232},
pages = {1880--1882},
publisher = {New York, NY:[sn] 1880-},
title = {{An internal model for sensorimotor integration}},
volume = {269},
year = {1995}
}
@inproceedings{krizhevsky2012imagenet,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
pages = {1097--1105},
title = {{Imagenet classification with deep convolutional neural networks}},
year = {2012}
}
@article{huh2016makes,
author = {Huh, Minyoung and Agrawal, Pulkit and Efros, Alexei A},
journal = {arXiv preprint arXiv:1608.08614},
title = {{What makes ImageNet good for transfer learning?}},
year = {2016}
}
@article{dogar2012planning,
author = {Dogar, Mehmet R and Srinivasa, Siddhartha S},
journal = {Autonomous Robots},
number = {3},
pages = {217--236},
publisher = {Springer},
title = {{A planning framework for non-prehensile manipulation under clutter and uncertainty}},
volume = {33},
year = {2012}
}
@article{levine2017grasping,
author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Quillen, Deirdre},
journal = {International Journal of Robotics Research},
title = {{Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection}},
year = {2017}
}
@article{higgins2017darla,
author = {Higgins, Irina and Pal, Arka and Rusu, Andrei A and Matthey, Loic and Burgess, Christopher P and Pritzel, Alexander and Botvinick, Matthew and Blundell, Charles and Lerchner, Alexander},
journal = {International Conference on Machine Learning (ICML)},
title = {{Darla: Improving zero-shot transfer in reinforcement learning}},
year = {2017}
}
@article{fujimoto2018td3,
author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
journal = {International Conference on Machine Learning (ICML)},
title = {{Addressing Function Approximation Error in Actor-Critic Methods}},
year = {2018}
}
@article{mayer2008system,
author = {Mayer, Hermann and Gomez, Faustino and Wierstra, Daan and Nagy, Istvan and Knoll, Alois and Schmidhuber, J{\"{u}}rgen},
journal = {Advanced Robotics},
number = {13-14},
pages = {1521--1537},
publisher = {Taylor {\&} Francis},
title = {{A system for robotic heart surgery that learns to tie knots using recurrent neural networks}},
volume = {22},
year = {2008}
}
@inproceedings{hamrick2011internal,
author = {Hamrick, Jessica and Battaglia, Peter and Tenenbaum, Joshua B},
booktitle = {Proceedings of the 33rd annual conference of the cognitive science society},
organization = {Cognitive Science Society Austin, TX},
pages = {1545--1550},
title = {{Internal physics models guide probabilistic judgments about object dynamics}},
year = {2011}
}
@article{mayne2014model,
abstract = {Abstract This paper recalls a few past achievements in Model Predictive Control, gives an overview of some current developments and suggests a few avenues for future research. },
author = {Mayne, David Q},
doi = {http://dx.doi.org/10.1016/j.automatica.2014.10.128},
issn = {0005-1098},
journal = {Automatica},
keywords = {Model predictive control},
number = {12},
pages = {2967--2986},
title = {{Model predictive control: Recent developments and future promise}},
url = {http://www.sciencedirect.com/science/article/pii/S0005109814005160},
volume = {50},
year = {2014}
}
@article{mnih2015human,
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Others},
journal = {Nature},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
volume = {518},
year = {2015}
}
@article{mottaghi2015newtonian,
author = {Mottaghi, Roozbeh and Bagherinezhad, Hessam and Rastegari, Mohammad and Farhadi, Ali},
journal = {arXiv preprint arXiv:1511.04048},
title = {{Newtonian Image Understanding: Unfolding the Dynamics of Objects in Static Images}},
year = {2015}
}
@inproceedings{stilman2007manipulation,
author = {Stilman, Mike and Schamburek, Jan-Ullrich and Kuffner, James and Asfour, Tamim},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {3327--3332},
title = {{Manipulation planning among movable obstacles}},
year = {2007}
}
@article{higgins2016beta,
author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
journal = {International Conference on Learning Representations (ICLR)},
title = {{{\$}\backslashbeta{\$}-VAE: Learning basic visual concepts with a constrained variational framework}},
year = {2017}
}
@article{haruno2001mosaic,
author = {Haruno, Masahiko and Wolpert, David H and Kawato, Mitsuo},
journal = {Neural computation},
number = {10},
pages = {2201--2220},
publisher = {MIT Press},
title = {{Mosaic model for sensorimotor learning and control}},
volume = {13},
year = {2001}
}
@inproceedings{lenz2015deepMPC,
author = {Lenz, Ian and Knepper, Ross and Saxena, Ashutosh},
booktitle = {Robotics: Science and Systems (RSS)},
title = {{DeepMPC: Learning Deep Latent Features for Model Predictive Control}},
year = {2015}
}
@article{hopcroft1991case,
author = {Hopcroft, John E and Kearney, Joseph K and Krafft, Dean B},
journal = {The International Journal of Robotics Research},
number = {1},
pages = {41--50},
publisher = {Sage Publications},
title = {{A case study of flexible object manipulation}},
volume = {10},
year = {1991}
}
@article{rusu2016sim,
author = {Rusu, Andrei A and Vecerik, Matej and Roth{\"{o}}rl, Thomas and Heess, Nicolas and Pascanu, Razvan and Hadsell, Raia},
journal = {Conference on Robot Learning (CoRL)},
title = {{Sim-to-real robot learning from pixels with progressive nets}},
year = {2017}
}
@techreport{winograd1971procedures,
author = {Winograd, Terry},
institution = {DTIC Document},
title = {{Procedures as a representation for data in a computer program for understanding natural language}},
year = {1971}
}
@inproceedings{5459407,
author = {Brubaker, Marcus A and Sigal, L and Fleet, D J},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/ICCV.2009.5459407},
issn = {1550-5499},
keywords = {Biological system modeling,Geometry,Gravity,Humans},
month = {sep},
pages = {2389--2396},
title = {{Estimating contact dynamics}},
year = {2009}
}
@article{eysenbach2018diayn,
author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
journal = {arXiv preprint arXiv:1802.06070},
title = {{Diversity is All You Need: Learning Skills without a Reward Function}},
year = {2018}
}
@inproceedings{agarwal1997nonholonomic,
author = {Agarwal, Pankaj K and Latombe, Jean-Claude and Motwani, Rajeev and Raghavan, Prabhakar},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
organization = {Citeseer},
pages = {3124--3129},
title = {{Nonholonomic path planning for pushing a disk among obstacles}},
year = {1997}
}
@inproceedings{conf/eccv/BhatSP02,
author = {Bhat, Kiran S and Seitz, Steven M and Popovic, Jovan},
booktitle = {European Conference on Computer Vision (ECCV)},
editor = {Heyden, Anders and Sparr, Gunnar and Nielsen, Mads and Johansen, Peter},
isbn = {3-540-43745-2},
keywords = {dblp},
pages = {551--565},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Computing the Physical Parameters of Rigid-Body Motion from Video.}},
url = {http://dblp.uni-trier.de/db/conf/eccv/eccv2002-1.html{\#}BhatSP02},
volume = {2350},
year = {2002}
}
@article{wahlstrom2015from,
author = {Wahlstr{\"{o}}m, Niklas and Sch{\"{o}}n, Thomas B and Deisenroth, Marc Peter},
journal = {CoRR},
title = {{From Pixels to Torques: Policy Learning with Deep Dynamical Models}},
volume = {abs/1502.0},
year = {2015}
}
@inproceedings{wu2015galileo,
author = {Wu, Jiajun and Yildirim, Ilker and Lim, Joseph J and Freeman, Bill and Tenenbaum, Josh},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
pages = {127--135},
title = {{Galileo: Perceiving Physical Object Properties by Integrating a Physics Engine with Deep Learning}},
year = {2015}
}
@article{argall2009survey,
author = {Argall, Brenna D and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
journal = {Robotics and autonomous systems},
number = {5},
pages = {469--483},
publisher = {Elsevier},
title = {{A survey of robot learning from demonstration}},
volume = {57},
year = {2009}
}
@article{shelhamer2016loss,
author = {Shelhamer, Evan and Mahmoudieh, Parsa and Argus, Max and Darrell, Trevor},
journal = {arXiv preprint arXiv:1612.07307},
title = {{Loss is its own reward: Self-supervision for reinforcement learning}},
year = {2016}
}
@inproceedings{yamakawa2007one,
author = {Yamakawa, Yuji and Namiki, Akio and Ishikawa, Masatoshi and Shimojo, Makoto},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
organization = {IEEE},
pages = {703--708},
title = {{One-handed knotting of a flexible rope with a high-speed multifingered hand having tactile sensors}},
year = {2007}
}
@article{maaten2008visualizing,
author = {van der Maaten, Laurens and Hinton, Geoffrey},
journal = {Journal of Machine Learning Research (JMLR)},
number = {Nov},
pages = {2579--2605},
title = {{Visualizing data using t-SNE}},
volume = {9},
year = {2008}
}
@phdthesis{bell2010flexible,
author = {Bell, Matthew},
school = {Dartmouth College, Hanover, New Hampshire},
title = {{Flexible object manipulation}},
year = {2010}
}
@inproceedings{bellemare2016unifying,
author = {Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
pages = {1471--1479},
title = {{Unifying count-based exploration and intrinsic motivation}},
year = {2016}
}
@inproceedings{maitin2010cloth,
author = {Maitin-Shepard, Jeremy and Cusumano-Towner, Marco and Lei, Jinna and Abbeel, Pieter},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {2308--2315},
title = {{Cloth grasp point detection based on multiple-view geometric cues with application to robotic towel folding}},
year = {2010}
}
@inproceedings{miller2011parametrized,
author = {Miller, Stephen and Fritz, Mario and Darrell, Trevor and Abbeel, Pieter},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {4861--4868},
title = {{Parametrized shape models for clothing}},
year = {2011}
}
@inproceedings{sutton2011horde,
abstract = {Maintaining accurate world knowledge in a complex and changing environment is a perennial problem for robots and other artificial intelligence systems. Our architecture for addressing this problem, called Horde, consists of a large number of independent reinforcement learning sub-agents, or demons. Each demon is responsible for answering a sin-gle predictive or goal-oriented question about the world, thereby contributing in a factored, modular way to the sys-tem's overall knowledge. The questions are in the form of a value function, but each demon has its own policy, re-ward function, termination function, and terminal-reward function unrelated to those of the base problem. Learning proceeds in parallel by all demons simultaneously so as to extract the maximal training information from whatever ac-tions are taken by the system as a whole. Gradient-based temporal-difference learning methods are used to learn ef-ficiently and reliably with function approximation in this off-policy setting. Horde runs in constant time and memory per time step, and is thus suitable for learning online in real-time applications such as robotics. We present results using Horde on a multi-sensored mobile robot to successfully learn goal-oriented behaviors and long-term predictions from off-policy experience. Horde is a significant incremental step towards a real-time architecture for efficient learning of gen-eral knowledge from unsupervised sensorimotor interaction.},
author = {Sutton, Richard S and Modayil, Joseph and Delp, Michael and Degris, Thomas and Pilarski, Patrick M and White, Adam and Precup, Doina},
booktitle = {International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
file = {::},
keywords = {Categories and,Descriptors,Subject},
pages = {761--768},
title = {{Horde: A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction}},
url = {https://www.cs.swarthmore.edu/{~}meeden/DevelopmentalRobotics/horde1.pdf},
volume = {10},
year = {2011}
}
@inproceedings{ziebart2008maxent,
abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods. We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories. Copyright {\textcopyright} 2008.},
archivePrefix = {arXiv},
arxivId = {arXiv:1507.04888v2},
author = {Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K},
booktitle = {AAAI Conference on Artificial Intelligence},
eprint = {arXiv:1507.04888v2},
file = {::},
isbn = {9781577353683 (ISBN)},
issn = {10450823},
keywords = {Artificial intelligence,Bionics,Driving behaviors,Existing methods,Imitation learnings,Inverse problems,Maximum entropies,New approaches,Optimal policies,Partial trajectories,Performance guarantees,Principle of Maximum entropies,Probabilistic approaches,Probability,Probability distributions,Reinforcement,Reinforcement learning,Route preferences,Utility functions},
pages = {1433--1438},
title = {{Maximum Entropy Inverse Reinforcement Learning.}},
url = {https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf http://www.scopus.com/inward/record.url?eid=2-s2.0-57749097473{\&}partnerID=40{\%}5Cnhttp://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf},
year = {2008}
}
@article{Gu2016b,
abstract = {Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations.},
archivePrefix = {arXiv},
arxivId = {1610.00633},
author = {Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
doi = {10.1038/nature20101},
eprint = {1610.00633},
file = {::},
isbn = {0896-6273},
issn = {0028-0836},
journal = {IEEE International Conference on Robotics and Automation (ICRA)},
pmid = {26774160},
title = {{Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates}},
url = {https://arxiv.org/pdf/1610.00633.pdf http://arxiv.org/abs/1610.00633},
year = {2017}
}
@article{deisenroth2011stacking,
abstract = {—Over the last years, there has been substantial progress in robust manipulation in unstructured environments. The long-term goal of our work is to get away from precise, but very expensive robotic systems and to develop affordable, potentially imprecise, self-adaptive manipulator systems that can interactively perform tasks such as playing with children. In this paper, we demonstrate how a low-cost off-the-shelf robotic system can learn closed-loop policies for a stacking task in only a handful of trials—from scratch. Our manipulator is inaccurate and provides no pose feedback. For learning a controller in the work space of a Kinect-style depth camera, we use a model-based reinforcement learning technique. Our learning method is data efficient, reduces model bias, and deals with several noise sources in a principled way during long-term planning. We present a way of incorporating state-space constraints into the learning process and analyze the learning gain by exploiting the sequential structure of the stacking task.},
author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward and Fox, Dieter},
file = {::},
journal = {Robotics: Science and Systems (RSS)},
pages = {57--64},
title = {{Learning to Control a Low-Cost Manipulator using Data-Efficient Reinforcement Learning}},
url = {http://www.roboticsproceedings.org/rss07/p08.pdf},
volume = {VII},
year = {2011}
}
@inproceedings{ross2011dagger,
abstract = {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the com-mon i.i.d. assumptions made in statistical learn-ing. This leads to poor performance in theory and often in practice. Some recent approaches (Daum{\'{e}} III et al., 2009; Ross and Bagnell, 2010) provide stronger guarantees in this setting, but re-main somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning set-ting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.},
author = {Ross, St{\'{e}}phane and Gordon, Geoffrey J and Bagnell, J Andrew},
booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
file = {::;::},
title = {{A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning}},
url = {https://arxiv.org/pdf/1011.0686.pdf https://www.cs.cmu.edu/{~}sross1/publications/Ross-AIStats11-NoRegret.pdf},
year = {2011}
}
@article{Kaelbling2011,
abstract = {— In this paper we outline an approach to the integration of task planning and motion planning that has the following key properties: It is aggressively hierarchical. It makes choices and commits to them in a top-down fashion in an attempt to limit the length of plans that need to be constructed, and thereby exponentially decrease the amount of search required. Importantly, our approach also limits the need to project the effect of actions into the far future. It operates on detailed, continuous geometric representations and partial symbolic descriptions. It does not require a complete symbolic representation of the input geometry or of the geometric effect of the task-level operations.},
author = {Kaelbling, Leslie Pack and Lozano-Perez, Tomas},
doi = {10.1109/ICRA.2011.5980391},
file = {::},
isbn = {9781612843865},
issn = {10504729},
journal = {IEEE International Conference on Robotics and Automation (ICRA)},
pages = {1470--1477},
title = {{Hierarchical task and motion planning in the now}},
url = {http://people.csail.mit.edu/lpk/papers/hpn2.pdf http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5980391},
year = {2011}
}
@inproceedings{todorov12mujoco,
abstract = {We describe a new physics engine tailored to model-based control. Multi-joint dynamics are represented in generalized coordinates and computed via recursive algorithms. Contact responses are computed via efficient new algorithms we have developed, based on the modern velocity-stepping approach which avoids the difficulties with spring-dampers. Models are specified using either a high-level C++ API or an intuitive XML file format. A built-in compiler transforms the user model into an optimized data structure used for runtime computation. The engine can compute both forward and inverse dynamics. The latter are well-definedevenin the presence of contacts and equality constraints. Themodel can include tendon wrapping as well as actuator activation states (e.g. pneumatic cylinders or muscles). To facilitate optimal control applications and in particular sampling and finite differencing, the dynamics can be evaluated for different states and controls in parallel. Around 400,000 dynamics evaluations per second are possible on a 12-core machine, for a 3D homanoid with 18 dofs and 6 active contacts. We have already used the engine in a number of control applications. It will soon be made publicly available.},
author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2012.6386109},
file = {::},
isbn = {9781467317375},
issn = {21530858},
pages = {5026--5033},
title = {{MuJoCo: A physics engine for model-based control}},
url = {https://homes.cs.washington.edu/{~}todorov/papers/TodorovIROS12.pdf},
year = {2012}
}
@inproceedings{kim2013apid,
abstract = {We propose a Learning from Demonstration (LfD) algorithm which leverages ex-pert data, even if they are very few or inaccurate. We achieve this by using both expert data, as well as reinforcement signals gathered through trial-and-error inter-actions with the environment. The key idea of our approach, Approximate Policy Iteration with Demonstration (APID), is that expert's suggestions are used to de-fine linear constraints which guide the optimization performed by Approximate Policy Iteration. We prove an upper bound on the Bellman error of the estimate computed by APID at each iteration. Moreover, we show empirically that APID outperforms pure Approximate Policy Iteration, a state-of-the-art LfD algorithm, and supervised learning in a variety of scenarios, including when very few and/or suboptimal demonstrations are available. Our experiments include simulations as well as a real robot path-finding task.},
author = {Kim, Beomjoon and Farahmand, Amir-Massoud and Pineau, Joelle and Precup, Doina},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {::},
title = {{Learning from Limited Demonstrations}},
url = {https://papers.nips.cc/paper/4918-learning-from-limited-demonstrations.pdf},
year = {2013}
}
@inproceedings{schaul2015uva,
abstract = {Value functions are a core component of rein- forcement learning systems. The main idea is to to construct a single function approximator V (s; $\theta$) that estimates the long-term reward from any state s, using parameters $\theta$. In this paper we introduce universal value function approx- imators (UVFAs) V (s, g; $\theta$) that generalise not just over states s but also over goals g. We de- velop an efficient technique for supervised learn- ing of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a re- inforcement learning algorithm that updates the UVFAsolely from observed rewards. Finally, we demonstrate that a UVFAcan successfully gener- alise to previously unseen goals. 1.},
author = {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
isbn = {9781510810587},
pages = {1312--1320},
title = {{Universal Value Function Approximators}},
url = {http://proceedings.mlr.press/v37/schaul15.pdf http://jmlr.org/proceedings/papers/v37/schaul15.html},
year = {2015}
}
@inproceedings{finn2016deep,
abstract = {Reinforcement learning provides a powerful and flexible framework for automated acquisition of robotic motion skills. However, applying reinforcement learning requires a sufficiently detailed representation of the state, including the configuration of task-relevant objects. We present an approach that automates state-space construction by learning a state representation directly from camera images. Our method uses a deep spatial autoencoder to acquire a set of feature points that describe the environment for the current task, such as the positions of objects, and then learns a motion skill with these feature points using an efficient reinforcement learning method based on local linear models. The resulting controller reacts continuously to the learned feature points, allowing the robot to dynamically manipulate objects in the world with closed-loop control. We demonstrate our method with a PR2 robot on tasks that include pushing a free-standing toy block, picking up a bag of rice using a spatula, and hanging a loop of rope on a hook at various positions. In each task, our method automatically learns to track task-relevant objects and manipulate their configuration with the robot's arm.},
archivePrefix = {arXiv},
arxivId = {1509.06113},
author = {Finn, Chelsea and Tan, Xin Yu and Duan, Yan and Darrell, Trevor and Levine, Sergey and Abbeel, Pieter},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2016.7487173},
eprint = {1509.06113},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/spatial-autoencoders-Finn15.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
organization = {IEEE},
pages = {512--519},
title = {{Deep spatial autoencoders for visuomotor learning}},
volume = {2016-June},
year = {2016}
}
@inproceedings{schulman2015trpo,
abstract = {We describe a iterative procedure for optimizing policies, with guaranteed monotonic improve-ment. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effec-tive for optimizing large nonlinear policies such as neural networks. Our experiments demon-strate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. De-spite its approximations that deviate from the theory, TRPO tends to give monotonic improve-ment, with little tuning of hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1502.05477},
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
booktitle = {International Conference on Machine Learning (ICML)},
doi = {10.1063/1.4927398},
eprint = {1502.05477},
file = {::;::},
isbn = {0375-9687},
issn = {2158-3226},
title = {{Trust Region Policy Optimization}},
url = {https://arxiv.org/pdf/1502.05477.pdf http://arxiv.org/abs/1502.05477},
year = {2015}
}
@inproceedings{kolev2015physically,
abstract = {— Successful model based control relies heavily on proper system identification and accurate state estimation. We present a framework for solving these problems in the context of robotic control applications. We are particularly interested in robotic manipulation tasks, which are especially hard due to the non-linear nature of contact phenomena. We developed a solution that solves both the problems of estimation and system identification jointly. We show that these two problems are difficult to solve separately in the presence of discontinuous phenomena such as contacts. The problem is posed as a joint optimization across both trajectory and model parameters and solved via Newton's method. We present several challenges we encountered while modeling contacts and performing state estimation and propose solutions within the MuJoCo physics engine. We present experimental results performed on our manip-ulation system consisting of 3-DOF Phantom Haptic Devices, turned into finger manipulators. Cross-validation between dif-ferent datasets, as well as leave-one-out cross-validation show that our method is robust and is able to accurately explain sensory data.},
author = {Kolev, Svetoslav and Todorov, Emanuel},
booktitle = {IEEE-RAS International Conference on Humanoid Robots (Humanoids)},
file = {::},
organization = {IEEE},
pages = {1036--1043},
title = {{Physically consistent state estimation and system identification for contacts}},
url = {http://homes.cs.washington.edu/{~}todorov/papers/KolevHumanoids15.pdf},
year = {2015}
}
@inproceedings{peters2010reps,
abstract = {Policy search is a successful approach to reinforcement learning. However, policy improvements often result in the loss of information. Hence, it has been marred by premature convergence and implausible solutions. As first suggested in the context of covariant policy gradients (Bagnell and Schneider 2003), many of these problems may be addressed by constraining the infor- mation loss. In this paper, we continue this path of rea- soning and suggest the Relative Entropy Policy Search (REPS) method. The resulting method differs signif- icantly from previous policy gradient approaches and yields an exact update step. It works well on typical reinforcement learning benchmark problems. Introduction},
author = {Peters, Jan and M{\"{u}}lling, Katharina and Alt{\"{u}}n, Yasemin},
booktitle = {AAAI Conference on Artificial Intelligence},
file = {::},
pages = {1607--1612},
title = {{Relative Entropy Policy Search}},
url = {https://pdfs.semanticscholar.org/ff47/526838ce85d77a50197a0c5f6ee5095156aa.pdf http://www-clmc.usc.edu/publications/P/Peters{\_}POTTNCOAIPGAT{\_}2010.pdf},
year = {2010}
}
@inproceedings{ho2016gail,
abstract = {Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains signif-icant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.},
author = {Ho, Jonathan and Ermon, Stefano},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {::},
title = {{Generative Adversarial Imitation Learning}},
url = {https://arxiv.org/pdf/1606.03476.pdf},
year = {2016}
}
@inproceedings{chen2016infogan,
abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
archivePrefix = {arXiv},
arxivId = {1606.03657},
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {1606.03657},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/infogan-Chen16.pdf:pdf},
pages = {2172--2180},
title = {{Infogan: Interpretable representation learning by information maximizing generative adversarial nets}},
url = {http://arxiv.org/abs/1606.03657},
year = {2016}
}
@article{silver2016alphago,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
archivePrefix = {arXiv},
arxivId = {1610.00633},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
eprint = {1610.00633},
file = {::},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
month = {jan},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961 http://10.0.4.14/nature16961 http://www.nature.com/nature/journal/v529/n7587/abs/nature16961.html{\#}supplementary-information http://airesearch.com/wp-content/uploads/2016/01/deepmind-mastering-go.pdf},
volume = {529},
year = {2016}
}
@inproceedings{nakanishi2004bipedlfd,
abstract = {In this paper, we introduce a framework for learning biped locomotion using dynamical movement primitives based on non-linear oscillators. Our ultimate goal is to establish a design principle of a controller in order to achieve natural human-like locomotion. We suggest dynamical movement primitives as a central pattern generator (CPG) of a biped robot, an approach we have previously proposed for learning and encoding complex human movements. Demonstrated trajectories are learned through movement primitives by locally weighted regression, and the frequency of the learned trajectories is adjusted automatically by a novel frequency adaptation algorithm based on phase resetting and entrainment of coupled oscillators. Numerical simulations and experimental implementation on a physical robot demonstrate the effectiveness of the proposed locomotion controller. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Nakanishi, Jun and Morimoto, Jun and Endo, Gen and Cheng, Gordon and Schaal, Stefan and Kawato, Mitsuo},
booktitle = {Robotics and Autonomous Systems},
doi = {10.1016/j.robot.2004.03.003},
file = {::},
isbn = {0921-8890},
issn = {09218890},
keywords = {Biped locomotion,Dynamical movement primitives,Frequency adaptation,Learning from demonstration,Phase resetting},
number = {2-3},
pages = {79--91},
title = {{Learning from demonstration and adaptation of biped locomotion}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.77.534{\&}rep=rep1{\&}type=pdf},
volume = {47},
year = {2004}
}
@inproceedings{schaal2001augmentation,
abstract = {Real-time modeling of complex nonlinear dynamic processes has become increasingly impor-tant in various areas of robotics and human aug-mentation. To address such problems, we have been developing special statistical learning methods that meet the demands of on-line learning, in particular the need for low computational complexity, rapid learning, and scalability to high-dimensional spaces. In this paper, we introduce a novel algorithm that possesses all the necessary properties by combining methods from probabilistic and nonparametric learning. We demonstrate the applicability of our methods for three different applications in humanoid robotics, i.e., the on-line learning of a full-body in-verse dynamics model, an inverse kinematics model, and imitation learning. The latter application will also introduce a novel method to shape attractor landscapes of dynamical system by means of statis-tical learning.},
author = {Schaal, Stefan and Vijayakumar, Sethu and Souza, Aaron D ' and Ijspeert, Auke and Nakanishi, Jun},
booktitle = {International Symposium on Robotics Research},
file = {::},
title = {{Real-Time Statistical Learning For Robotics and Human Augmentation}},
url = {http://www-clmc.usc.edu}
}
@article{Kavraki1996,
author = {Kavraki, Lydia and Svestka, Petr and Latombe, J-C and Overmars, Mark},
file = {::},
journal = {IEEE Transactions on Robotics and Automation},
number = {4},
pages = {566--580},
title = {{Probabilistic roadmaps for path planning in high-dimensional configuration spaces}},
volume = {12},
year = {1996}
}
@inproceedings{pathak2017curiosity,
abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
archivePrefix = {arXiv},
arxivId = {1705.05363},
author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1705.05363},
file = {::},
organization = {IEEE},
pages = {488--489},
title = {{Curiosity-Driven Exploration by Self-Supervised Prediction}},
year = {2017}
}
@article{smith2005development,
abstract = {The embodiment hypothesis is the idea that intelligence emerges in the interaction of an agent with an environment and as a result of sensorimotor activity. In this paper we offer six lessons for developing embodied intelligent agents suggested by research in developmental psychology. We argue that starting as a baby grounded in a physical, social and linguistic world is crucial to the development of the flexible and inventive intelligence that characterizes humankind.},
author = {Smith, Linda and Gasser, Michael},
file = {::},
journal = {Artificial life},
keywords = {cognition,development,embodiment,language,motor control},
number = {1-2},
pages = {13--29},
publisher = {MIT Press},
title = {{The development of embodied cognition: Six lessons from babies}},
url = {https://www.cogsci.msu.edu/DSS/2010-2011/Smith/6lessons.pdf},
volume = {11},
year = {2005}
}
@inproceedings{ng2000irl,
author = {Ng, Andrew and Russell, Stuart},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Algorithms for Inverse Reinforcement Learning}},
url = {http://ai.stanford.edu/{~}ang/papers/icml00-irl.pdf},
year = {2000}
}
@inproceedings{srivastava14tamp,
abstract = {— The need for combined task and motion planning in robotics is well understood. Solutions to this problem have typically relied on special purpose, integrated implementations of task planning and motion planning algorithms. We propose a new approach that uses off-the-shelf task planners and motion planners and makes no assumptions about their implementa-tion. Doing so enables our approach to directly build on, and benefit from, the vast literature and latest advances in task planning and motion planning. It uses a novel representational abstraction and requires only that failures in computing a mo-tion plan for a high-level action be identifiable and expressible in the form of logical predicates at the task level. We evaluate the approach and illustrate its robustness through a number of experiments using a state-of-the-art robotics simulator and a PR2 robot. These experiments show the system accomplishing a diverse set of challenging tasks such as taking advantage of a tray when laying out a table for dinner and picking objects from cluttered environments where other objects need to be re-arranged before the target object can be reached.},
author = {Srivastava, Siddharth and Fang, Eugene and Riano, Lorenzo and Chitnis, Rohan and Russell, Stuart and Abbeel, Pieter},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {::},
title = {{Combined Task and Motion Planning Through an Extensible Planner-Independent Interface Layer}},
url = {https://people.eecs.berkeley.edu/{~}russell/papers/icra14-planrob.pdf},
year = {2014}
}
@article{peters2003naturalac,
abstract = {Reinforcement learning offers a promising framework to take planning for real-world systems towards true autonomy and versatility. However, apply-ing reinforcement learning to high dimensional movement systems (such as real-world robots) in the presence of uncertainty and continuous state-action spaces remains an unsolved problem. In order to make progress towards solving this issue, we focus on a particular type of reinforcement learning methods, i.e., policy gradient methods. These methods are particularly in-teresting to the robotics community as they seem to scale better to continuous state-action problems and have been successfully applied on a variety of high-dimensional robots. However, the main disadvantages of these methods have been the high variance in the gradient estimate, the very slow convergence, and the dependence on baseline functions. In this poster, we show how these policy gradients can be improved in respect to each of these problems. Our approach to policy gradients focuses on the natural policy gradient instead of the regular policy gradient. Natural policy gradients for reinforce-ment learning have first been suggested by Kakade [2] as 'average natural policy gradients', and subsequently been shown to be the true natural policy gradient by Bagnell {\&} Schneider [1], and Peters et al. [3]. As shown by Kakade, natural policy gradients are particularly interesting due to the fact that they equal the parameters of the compatible function approximation. We present a general algorithm for estimating the natural gradient, the Nat-ural Actor-Critic algorithm. This algorithm uses the fact that the compatible function approximation represents an advantage function which can be em-1 bedded cleanly into the Bellman equation. It can be used in two different ways, i.e., in form of general temporal difference learning where reasonable basis functions for the value function are required in order to obtain an un-biased gradient, and in form of start-state reinforcement learning where the gradient is estimated using the property that the sum of all advantages along a roll-out has to equal the sum of rewards plus a single value function offset parameter. The later method is guaranteed to yield an unbiased estimate of the gradient and is well suited for learning and refining parameterized policies, even in the light of incomplete state information. We show two examples of the application of the Natural Actor-Critic algorithm, one where it by far outperforms non-natural policy gradients in the classical cart-pole balancing system, and one for learning nonlinear dy-namic motor primitives for humanoid robot control. From our experience, the start-state Natural Actor-Critic algorithm seems to be one of the most efficient model-free reinforcement learning techniques and offers a promising route for the development of reinforcement learning techniques for truly high dimensionally continuous state-action systems.},
author = {Peters, Jan and Vijayakumar, Sethu and Schaal, Stefan},
file = {::},
journal = {Neurocomputing},
pages = {1180--1190},
title = {{Natural Actor Critic}},
url = {http://kyb.tuebingen.mpg.de/fileadmin/user{\_}upload/files/publications/attachments/NIPS-Workshop-2003-Peters{\_}{\%}5B0{\%}5D.pdf},
volume = {71},
year = {2008}
}
@inproceedings{boots2014traces,
author = {Boots, Byron and Byravan, Arunkumar and Fox, Dieter},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2014.6907443},
pages = {4021--4028},
title = {{Learning predictive models of a depth camera {\&} manipulator from raw execution traces}},
url = {http://dx.doi.org/10.1109/ICRA.2014.6907443},
year = {2014}
}
@inproceedings{florensa2017resets,
archivePrefix = {arXiv},
arxivId = {1707.05300},
author = {Florensa, Carlos and Held, David and Wulfmeier, Markus and Abbeel, Pieter},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {1707.05300},
file = {::},
keywords = {Automatic Curricu-lum Generation,Reinforcement Learning,Robotic Manipulation},
title = {{Reverse Curriculum Generation for Reinforcement Learning}},
url = {https://arxiv.org/pdf/1707.05300.pdf http://arxiv.org/abs/1707.05300},
year = {2018}
}
@article{peters2008baseball,
abstract = {Autonomous learning is one of the hallmarks of human and animal behavior, and understanding the principles of learning will be crucial in order to achieve true autonomy in advanced machines like humanoid robots. In this paper, we examine learning of complex motor skills with human-like limbs. While supervised learning can offer useful tools for bootstrapping behavior, e.g.,??by learning from demonstration, it is only reinforcement learning that offers a general approach to the final trial-and-error improvement that is needed by each individual acquiring a skill. Neither neurobiological nor machine learning studies have, so far, offered compelling results on how reinforcement learning can be scaled to the high-dimensional continuous state and action spaces of humans or humanoids. Here, we combine two recent research developments on learning motor control in order to achieve this scaling. First, we interpret the idea of modular motor control by means of motor primitives as a suitable way to generate parameterized control policies for reinforcement learning. Second, we combine motor primitives with the theory of stochastic policy gradient learning, which currently seems to be the only feasible framework for reinforcement learning for humanoids. We evaluate different policy gradient methods with a focus on their applicability to parameterized motor primitives. We compare these algorithms in the context of motor primitive learning, and show that our most modern algorithm, the Episodic Natural Actor-Critic outperforms previous algorithms by at least an order of magnitude. We demonstrate the efficiency of this reinforcement learning method in the application of learning to hit a baseball with an anthropomorphic robot arm. ?? 2008 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.3159v1},
author = {Peters, Jan and Schaal, Stefan},
doi = {10.1016/j.neunet.2008.02.003},
eprint = {arXiv:1411.3159v1},
file = {::},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Motor primitives,Motor skills,Natural Actor-Critic,Natural gradients,Policy gradient methods,Reinforcement learning},
number = {4},
pages = {682--697},
pmid = {18482830},
title = {{Reinforcement learning of motor skills with policy gradients}},
url = {https://pdfs.semanticscholar.org/eb5b/459c8a3e56064158fb3514eeab763486e437.pdf},
volume = {21},
year = {2008}
}
@inproceedings{kalakrishnan09terraintemplates,
abstract = {— We address the problem of foothold selection in robotic legged locomotion over very rough terrain. The difficulty of the problem we address here is comparable to that of human rock-climbing, where foot/hand-hold selection is one of the most critical aspects. Previous work in this domain typically involves defining a reward function over footholds as a weighted linear combination of terrain features. However, a significant amount of effort needs to be spent in designing these features in order to model more complex decision functions, and hand-tuning their weights is not a trivial task. We propose the use of terrain templates, which are discretized height maps of the terrain under a foothold on different length scales, as an alternative to manually designed features. We describe an algo-rithm that can simultaneously learn a small set of templates and a foothold ranking function using these templates, from expert-demonstrated footholds. Using the LittleDog quadruped robot, we experimentally show that the use of terrain templates can produce complex ranking functions with higher performance than standard terrain features, and improved generalization to unseen terrain.},
author = {Kalakrishnan, Mrinal and Buchli, Jonas and Pastor, Peter and Schaal, Stefan},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
file = {::},
title = {{Learning Locomotion over Rough Terrain using Terrain Templates}},
url = {https://pdfs.semanticscholar.org/02db/2c0100ffb02592e8738d0ffcf454224f4b1b.pdf},
year = {2009}
}
@inproceedings{finn16guidedcostlearning,
abstract = {Reinforcement learning can acquire complex be-haviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is chal-lenging in practice. We explore how inverse op-timal control (IOC) can be used to learn behav-iors from demonstrations, with applications to torque control of high-dimensional robotic sys-tems. Our method addresses two key challenges in inverse optimal control: first, the need for in-formative features and effective regularization to impose structure on the cost, and second, the dif-ficulty of learning the cost function under un-known dynamics for high-dimensional continu-ous systems. To address the former challenge, we present an algorithm capable of learning ar-bitrary nonlinear cost functions, such as neural networks, without meticulous feature engineer-ing. To address the latter challenge, we formu-late an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manip-ulation problems, demonstrating substantial im-provement over prior methods both in terms of task complexity and sample efficiency.},
author = {Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization}},
url = {https://arxiv.org/pdf/1603.00448.pdf},
year = {2016}
}
@inproceedings{bahdanau14attention,
abstract = {Neural machine translation is a recently proposed approach to machine transla-tion. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu-ral machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architec-ture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {::},
title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
url = {https://arxiv.org/pdf/1409.0473.pdf},
year = {2015}
}
@inproceedings{duan2017oneshotimitation,
abstract = {Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning. Specifically, we consider the setting where there is a very large set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. The use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks. Videos available at https://bit.ly/one-shot-imitation .},
archivePrefix = {arXiv},
arxivId = {1703.07326},
author = {Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly C. and Ho, Jonathan and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {1703.07326},
file = {::},
title = {{One-Shot Imitation Learning}},
year = {2017}
}
@inproceedings{pong2018tdm,
abstract = {Model-free reinforcement learning (RL) is a powerful, general tool for learning complex behaviors. However, its sample efficiency is often impractically large for solving challenging real-world problems, even with off-policy algorithms such as Q-learning. A limiting factor in classic model-free RL is that the learning signal consists only of scalar rewards, ignoring much of the rich information contained in state transition tuples. Model-based RL uses this information, by training a predictive model, but often does not achieve the same asymptotic performance as model-free RL due to model bias. We introduce temporal difference models (TDMs), a family of goal-conditioned value functions that can be trained with model-free learning and used for model-based control. TDMs combine the bene-fits of model-free and model-based RL: they leverage the rich information in state transitions to learn very efficiently, while still attaining asymptotic performance that exceeds that of direct model-based RL methods. Our experimental results show that, on a range of continuous control tasks, TDMs provide a substantial im-provement in efficiency compared to state-of-the-art model-based and model-free methods.},
author = {Pong, Vitchyr and Gu, Shixiang and Dalal, Murtaza and Levine, Sergey},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {::},
title = {{Temporal Difference Models: Model-Free Deep RL For Model-Based Control}},
url = {https://arxiv.org/pdf/1802.09081.pdf},
year = {2018}
}
@inproceedings{finn2016visualforesight,
abstract = {A key challenge in scaling up robot learning to many skills and environments is removing the need for human supervision, so that robots can collect their own data and improve their own performance without being limited by the cost of requesting human feedback. Model-based reinforcement learning holds the promise of enabling an agent to learn to predict the effects of its actions, which could provide flexible predictive models for a wide range of tasks and environments, without detailed human supervision. We develop a method for combining deep action-conditioned video prediction models with model-predictive control that uses entirely unlabeled training data. Our approach does not require a calibrated camera, an instrumented training set-up, nor precise sensing and actuation. Our results show that our method enables a real robot to perform nonprehensile manipulation -- pushing objects -- and can handle novel objects not seen during training.},
archivePrefix = {arXiv},
arxivId = {1610.00696},
author = {Finn, Chelsea and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {1610.00696},
file = {::;::},
title = {{Deep Visual Foresight for Planning Robot Motion}},
url = {https://arxiv.org/pdf/1610.00696.pdf http://arxiv.org/abs/1610.00696},
year = {2016}
}
@inproceedings{ijspeert2002attractor,
abstract = {Many control problems take place in continuous state-action spaces,$\backslash$n$\backslash$ne.g., as in manipulator robotics, where the control objective is of-$\backslash$n$\backslash$nten de�ned as �nding a desired trajectory that reaches a particular$\backslash$n$\backslash$ngoal state. While reinforcement learning o�ers a theoretical frame-$\backslash$n$\backslash$nwork to learn such control policies from scratch, its applicability$\backslash$nto$\backslash$n$\backslash$nhigher dimensional continuous state-action spaces remains rather$\backslash$n$\backslash$nlimited to date. Instead of learning from scratch, in this paper we$\backslash$n$\backslash$nsuggest to learn a desired complex control policy by transforming$\backslash$n$\backslash$nan existing simple canonical control policy. For this purpose, we$\backslash$n$\backslash$nrepresent canonical policies in terms of di�erential equations with$\backslash$n$\backslash$nwell-de�ned attractor properties. By nonlinearly transforming the$\backslash$n$\backslash$ncanonical attractor dynamics using techniques from nonparametric$\backslash$n$\backslash$nregression, almost arbitrary new nonlinear policies can be gener-$\backslash$n$\backslash$nated without losing the stability properties of the canonical sys-$\backslash$n$\backslash$ntem. We demonstrate our techniques in the context of learning a$\backslash$n$\backslash$nset of movement skills for a humanoid robot from demonstrations$\backslash$n$\backslash$nof a human teacher. Policies are acquired rapidly, and, due to the$\backslash$n$\backslash$nproperties of well formulated di�erential equations, can be re-used$\backslash$n$\backslash$nand modi�ed on-line under dynamic changes of the environment.$\backslash$n$\backslash$nThe linear parameterization of nonparametric regression moreover$\backslash$n$\backslash$nlends itself to recognize and classify previously learned movement$\backslash$n$\backslash$nskills. Evaluations in simulations and on an actual 30 degree-of-$\backslash$n$\backslash$nfreedom humanoid robot exemplify the feasibility and robustness$\backslash$n$\backslash$nof our approach.},
author = {Ijspeert, Auke Jan and Nakanishi, Jun and Schaal, Stefan},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {::},
isbn = {1049-5258},
issn = {10495258},
keywords = {reinforcement},
pages = {1547--1554},
title = {{Learning Attractor Landscapes for Learning Motor Primitives}},
url = {https://papers.nips.cc/paper/2140-learning-attractor-landscapes-for-learning-motor-primitives.pdf},
year = {2002}
}
@inproceedings{pathak2018zeroshot,
abstract = {The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both what and how to imitate. We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss. In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference. The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task. Our method is " zero-shot " in the sense that the agent never has access to expert actions during training or for the task demonstration at inference. We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot. Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance. Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/.},
author = {Pathak, Deepak and Mahmoudieh, Parsa and Luo, Guanghao and Agrawal, Pulkit and Chen, Dian and Shentu, Yide and Shelhamer, Evan and Malik, Jitendra and Efros, Alexei A and Darrell, Trevor},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {::},
title = {{Zero-Shot Visual Imitation}},
url = {https://arxiv.org/pdf/1804.08606.pdf},
year = {2018}
}
@inproceedings{pomerleau1989alvinn,
abstract = {ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer back-propagation network designed for the task of road following. Currently ALVINN takes images from a camera and a laser range finder as input and produces as output the direction the vehicle should travel in order to follow the road. Training has been conducted using simulated road images. Successful tests on the Carnegie Mellon autonomous navigation test vehicle indicate that the network can effectively follow real roads under certain field conditions. The representation developed to perform the task differs dramatically when the network is trained under various conditions, suggesting the possibility of a novel adaptive autonomous navigation system capable of tailoring its processing to the conditions at hand.},
author = {Pomerleau, Dean A},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {::},
isbn = {1-558-60015-9},
pages = {305--313},
title = {{Alvinn: An autonomous land vehicle in a neural network}},
url = {http://repository.cmu.edu/compsci},
year = {1989}
}
@inproceedings{sun17deeplyaggrevated,
abstract = {Researchers have demonstrated state-of-the-art performance in sequential decision making prob-lems (e.g., robotics control, sequential predic-tion) with deep neural network models. One often has access to near-optimal oracles that achieve good performance on the task during training. We demonstrate that AggreVaTeD — a policy gradient extension of the Imitation Learn-ing (IL) approach of (Ross {\&} Bagnell, 2014) — can leverage such an oracle to achieve faster and better solutions with less training data than a less-informed Reinforcement Learning (RL) technique. Using both feedforward and recur-rent neural predictors, we present stochastic gra-dient procedures on a sequential prediction task, dependency-parsing from raw image data, as well as on various high dimensional robotics con-trol problems. We also provide a comprehen-sive theoretical study of IL that demonstrates we can expect up to exponentially lower sample complexity for learning with AggreVaTeD than with RL algorithms, which backs our empirical findings. Our results and theory indicate that the proposed approach can achieve superior per-formance with respect to the oracle when the demonstrator is sub-optimal.},
author = {Sun, Wen and Venkatraman, Arun and Gordon, Geoffrey J and Boots, Byron and Edu, Bboots@cc Gatech and Bagnell, J Andrew},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction}},
url = {https://arxiv.org/pdf/1703.01030.pdf},
year = {2017}
}
@article{wakamatsu2006knotting,
author = {Wakamatsu, Hidefumi and Arai, Eiji and Hirai, Shinichi},
journal = {The International Journal of Robotics Research},
number = {4},
pages = {371--395},
publisher = {SAGE Publications},
title = {{Knotting/unknotting manipulation of deformable linear objects}},
volume = {25},
year = {2006}
}
@article{Ross2014,
abstract = {Recent work has demonstrated that problems-- particularly imitation learning and structured prediction-- where a learner's predictions influence the input-distribution it is tested on can be naturally addressed by an interactive approach and analyzed using no-regret online learning. These approaches to imitation learning, however, neither require nor benefit from information about the cost of actions. We extend existing results in two directions: first, we develop an interactive imitation learning approach that leverages cost information; second, we extend the technique to address reinforcement learning. The results provide theoretical support to the commonly observed successes of online approximate policy iteration. Our approach suggests a broad new family of algorithms and provides a unifying view of existing techniques for imitation and reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1406.5979},
author = {Ross, St{\'{e}}phane Stephane and Bagnell, J Andrew},
eprint = {1406.5979},
file = {::},
journal = {CoRR},
keywords = {()},
title = {{Reinforcement and Imitation Learning via Interactive No-Regret Learning}},
url = {https://arxiv.org/pdf/1406.5979.pdf http://arxiv.org/abs/1406.5979},
volume = {abs/1406.5},
year = {2014}
}
@article{vecerik17ddpgfd,
abstract = {We propose a general and model-free approach for Reinforcement Learning (RL) on real robotics with sparse rewards. We build upon the Deep Deterministic Policy Gradient (DDPG) algorithm to use demonstrations. Both demonstrations and actual interactions are used to fill a replay buffer and the sam-pling ratio between demonstrations and transitions is automatically tuned via a prioritized replay mechanism. Typically, carefully engineered shaping rewards are required to enable the agents to efficiently explore on high dimensional control problems such as robotics. They are also required for model-based acceleration methods relying on local solvers such as iLQG (e.g. Guided Policy Search and Nor-malized Advantage Function). The demonstrations replace the need for carefully engineered rewards, and reduce the exploration problem encountered by classical RL approaches in these domains. Demonstrations are collected by a robot kines-thetically force-controlled by a human demonstrator. Results on four simulated insertion tasks show that DDPG from demonstrations out-performs DDPG, and does not require engineered rewards. Finally, we demonstrate the method on a real robotics task consisting of inserting a clip (flexible object) into a rigid object.},
author = {Ve{\v{c}}er{\'{i}}k, Matej and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Roth{\"{o}}rl, Thomas and Lampe, Thomas and Riedmiller, Martin},
file = {::},
journal = {CoRR},
keywords = {Apprenticeship,Demonstrations,Learning,Robot},
title = {{Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards}},
url = {https://arxiv.org/pdf/1707.08817.pdf},
volume = {abs/1707.0},
year = {2017}
}
@inproceedings{reed2014learning,
author = {Reed, Scott and Sohn, Kihyuk and Zhang, Yuting and Lee, Honglak},
booktitle = {International Conference on Machine Learning (ICML)},
pages = {1431--1439},
title = {{Learning to disentangle factors of variation with manifold interaction}},
year = {2014}
}
@inproceedings{yang2015robot,
author = {Yang, Yezhou and Li, Yi and Ferm{\"{u}}ller, Cornelia and Aloimonos, Yiannis},
booktitle = {AAAI Conference on Artificial Intelligence},
pages = {3686--3693},
title = {{Robot Learning Manipulation Action Plans by" Watching" Unconstrained Videos from the World Wide Web.}},
year = {2015}
}
@article{michotte1963perception,
author = {Michotte, Albert},
publisher = {Basic Books},
title = {{The perception of causality.}},
year = {1963}
}
@article{levy2017hierarchical,
author = {Levy, Andrew and Platt, Robert and Saenko, Kate},
journal = {arXiv preprint arXiv:1712.00948},
title = {{Hierarchical Actor-Critic}},
year = {2017}
}
@inproceedings{morita2003knot,
author = {Morita, Takuma and Takamatsu, Jun and Ogawara, Koichi and Kimura, Hiroshi and Ikeuchi, Katsushi},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
organization = {IEEE},
pages = {3887--3892},
title = {{Knot planning from observation}},
volume = {3},
year = {2003}
}
@book{lavalle2006planning,
author = {LaValle, Steven M},
publisher = {Cambridge university press},
title = {{Planning algorithms}},
year = {2006}
}
@inproceedings{kakade2002approximatelyoptimal,
abstract = {Abstract In order to solve realistic reinforcement learning problems, it is critical that approximate algorithms be used. In this paper, we present the conservative policy iteration algorithm which finds an" approximately" optimal policy, given access to a restart ... $\backslash$n},
author = {Kakade, Sham and Langford, John},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
isbn = {1-55860-873-7},
pages = {267--274},
title = {{Approximately Optimal Approximate Reinforcement Learning}},
url = {http://www.cs.cmu.edu/afs/cs/Web/People/jcl/papers/aoarl/Final.pdf},
year = {2002}
}
@inproceedings{hosu2016humancheckpoint,
abstract = {This paper introduces a novel method for learning how to play the most difficult Atari 2600 games from the Arcade Learn-ing Environment using deep reinforcement learning. The proposed method, called human checkpoint replay, consists in using check-points sampled from human gameplay as starting points for the learn-ing process. This is meant to compensate for the difficulties of current exploration strategies, such as $\epsilon$-greedy, to find successful control policies in games with sparse rewards. Like other deep reinforce-ment learning architectures, our model uses a convolutional neural network that receives only raw pixel inputs to estimate the state value function. We tested our method on Montezuma's Revenge and Pri-vate Eye, two of the most challenging games from the Atari platform. The results we obtained show a substantial improvement compared to previous learning approaches, as well as over a random player. We also propose a method for training deep reinforcement learning agents using human gameplay experience, which we call human ex-perience replay.},
author = {Hosu, Ionel-Alexandru and Rebedea, Traian},
booktitle = {Workshop on Evaluating General Purpose AI},
file = {::},
title = {{Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay}},
url = {https://arxiv.org/pdf/1607.05077.pdf},
year = {2016}
}
@article{Dragan2017,
abstract = {Robots interacting with the physical world plan with models of physics. We advocate that robots interacting with people need to plan with models of cognition. This writeup summarizes the insights we have gained in integrating computational cognitive models of people into robotics planning and control. It starts from a general game-theoretic formulation of interaction, and analyzes how different approximations result in different useful coordination behaviors for the robot during its interaction with people.},
archivePrefix = {arXiv},
arxivId = {1705.04226},
author = {Dragan, Anca D.},
eprint = {1705.04226},
file = {::},
month = {may},
title = {{Robot Planning with Mathematical Models of Human State and Action}},
url = {https://arxiv.org/pdf/1705.04226.pdf http://arxiv.org/abs/1705.04226},
year = {2017}
}
@inproceedings{mnih2013atari,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
booktitle = {NIPS Workshop on Deep Learning},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {::;::;::},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
pages = {1--9},
pmid = {25719670},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1312.5602.pdf http://arxiv.org/abs/1312.5602 https://www.cs.toronto.edu/{~}vmnih/docs/dqn.pdf},
year = {2013}
}
@inproceedings{rajeswaran2017simplicity,
abstract = {This work shows that policies with simple lin-ear and RBF parameterizations can be trained to solve a variety of continuous control tasks, in-cluding the OpenAI gym benchmarks. The per-formance of these trained policies are competi-tive with state of the art results, obtained with more elaborate parameterizations such as fully connected neural networks. Furthermore, exist-ing training and testing scenarios are shown to be very limited and prone to over-fitting, thus giving rise to only trajectory centric policies. Training with a diverse initial state distribution is shown to produce more global policies with better gen-eralization. This allows for interactive control scenarios where the system recovers from large on-line perturbations; as shown in the video.},
author = {Rajeswaran, Aravind and Lowrey, Kendall and Todorov, Emanuel and Kakade, Sham},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {::},
title = {{Towards Generalization and Simplicity in Continuous Control}},
url = {https://arxiv.org/pdf/1703.02660.pdf https://homes.cs.washington.edu/{~}todorov/papers/RajeswaranICML17.pdf},
year = {2017}
}
@inproceedings{nair2018demonstrations,
abstract = {— Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponen-tially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.},
author = {Nair, Ashvin and Mcgrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {::},
title = {{Overcoming Exploration in Reinforcement Learning with Demonstrations}},
url = {https://arxiv.org/pdf/1709.10089.pdf},
year = {2018}
}
@inproceedings{vezhnevets2017fun,
abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.},
archivePrefix = {arXiv},
arxivId = {1703.01161},
author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1703.01161},
file = {::;::;::},
title = {{FeUdal Networks for Hierarchical Reinforcement Learning}},
url = {https://arxiv.org/pdf/1703.01161.pdf http://arxiv.org/abs/1703.01161},
year = {2017}
}
@inproceedings{wang2016duelingdqn,
abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this pa-per, we present a new neural network architec-ture for model-free reinforcement learning. Our dueling network represents two separate estima-tors: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to general-ize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architec-ture leads to better policy evaluation in the pres-ence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Com, Mtthss@google and {Van Hasselt}, Hado and Lanctot, Marc},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Dueling Network Architectures for Deep Reinforcement Learning}},
url = {https://arxiv.org/pdf/1511.06581.pdf},
year = {2016}
}
@inproceedings{silver2014dpg,
abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic pol-icy gradient has a particularly appealing form: it is the expected gradient of the action-value func-tion. This simple form means that the deter-ministic policy gradient can be estimated much more efficiently than the usual stochastic pol-icy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counter-parts in high-dimensional action spaces.},
author = {Silver, David and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Deterministic Policy Gradient Algorithms}},
url = {http://proceedings.mlr.press/v32/silver14.pdf},
year = {2014}
}
@inproceedings{liu2018imitation,
abstract = {Imitation learning is an effective approach for autonomous systems to acquire control policies when an explicit reward function is unavailable, using supervision provided as demonstrations from an expert, typically a human operator. However, standard imitation learning methods assume that the agent receives examples of observation-action tuples that could be provided, for instance, to a supervised learning algorithm. This stands in contrast to how humans and animals imitate: we observe another person performing some behavior and then figure out which actions will realize that behavior, compensating for changes in viewpoint, surroundings, and embodiment. We term this kind of imitation learning as imitation-from-observation and propose an imitation learning method based on video prediction with context translation and deep reinforcement learning. This lifts the assumption in imitation learning that the demonstration should consist of observations and actions in the same environment, and enables a variety of interesting applications, including learning robotic skills that involve tool use simply by observing videos of human tool use. Our experimental results show that our approach can perform imitation-from-observation for a variety of real-world robotic tasks modeled on common household chores, acquiring skills such as sweeping from videos of a human demonstrator. Videos can be found at https://sites.google.com/ site/imitationfromobservation/},
author = {Liu, Yuxuan and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {::},
title = {{Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation}},
url = {https://arxiv.org/pdf/1707.03374.pdf},
year = {2018}
}
@inproceedings{Groshev,
abstract = {We consider the problem of learning for planning, where knowledge acquired while planning is reused to plan faster in new problem instances. For robotic tasks, among others, plan execution can be captured as a sequence of visual images. For such domains, we propose to use deep neural networks in learning for planning, based on learning a reactive policy that imitates execution traces produced by a planner. We investigate architectural properties of deep networks that are suitable for learning long-horizon planning behavior, and explore how to learn, in addition to the policy, a heuristic function that can be used with classical planners or search algorithms such as A * . Our results on the challenging Sokoban domain show that, with a suitable network design, complex decision making policies and powerful heuristic functions can be learned through imitation. Videos available at https://sites.google.com/site/learn2plannips/.},
author = {Groshev, Edward and Tamar, Aviv and Srivastava, Siddharth and Abbeel, Pieter},
file = {::},
title = {{Learning Generalized Reactive Policies using Deep Neural Networks}},
url = {https://arxiv.org/pdf/1708.07280.pdf}
}
@book{sutton1998rl,
author = {Sutton, Richard S and Barto, Andrew G},
file = {::},
title = {{Reinforcement Learning: An Introduction}},
url = {http://incompleteideas.net/sutton/book/bookdraft2016sep.pdf https://webdocs.cs.ualberta.ca/{~}sutton/book/bookdraft2016sep.pdf},
year = {1998}
}
@inproceedings{heess2015svg,
abstract = {We present a unified framework for learning continuous control policies using backpropagation. It supports stochastic control by treating stochasticity in the Bellman equation as a deterministic function of exogenous noise. The product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions. We use learned models but only require observations from the environment in-stead of observations from model-predicted trajectories, minimizing the impact of compounded model errors. We apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation. One of these variants, SVG(1), shows the effectiveness of learning models, value functions, and policies simultaneously in continuous domains.},
author = {Heess, Nicolas and Wayne, Greg and Silver, David and Lillicrap, Timothy and Tassa, Yuval and Erez, Tom and Deepmind, Google},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {::},
title = {{Learning Continuous Control Policies by Stochastic Value Gradients}},
url = {https://arxiv.org/pdf/1510.09142.pdf},
year = {2015}
}
@inproceedings{jang2017grasping,
abstract = {We consider the task of semantic robotic grasping, in which a robot picks up an object of a user-specified class using only monocular images. Inspired by the two-stream hypothesis of visual reasoning, we present a semantic grasping framework that learns object detection, classification, and grasp planning in an end-to-end fashion. A " ventral stream " recognizes object class while a " dorsal stream " simultaneously interprets the geometric relationships necessary to exe-cute successful grasps. We leverage the autonomous data collection capabilities of robots to obtain a large self-supervised dataset for training the dorsal stream, and use semi-supervised label propagation to train the ventral stream with only a modest amount of human supervision. We experimentally show that our approach improves upon grasping systems whose components are not learned end-to-end, including a baseline method that uses bounding box detection. Furthermore, we show that jointly training our model with auxiliary data consisting of non-semantic grasping data, as well as semantically labeled images without grasp actions, has the potential to substantially improve semantic grasping performance 1 .},
author = {Jang, Eric and Brain, Google and Vijayanarasimhan, Sudheendra and Pastor, Peter and Ibarz, Julian and Levine, Sergey},
booktitle = {Conference on Robot Learning (CoRL)},
file = {::;::},
keywords = {deep learning,semantic grasping},
title = {{End-to-End Learning of Semantic Grasping}},
url = {https://arxiv.org/pdf/1707.01932.pdf},
year = {2017}
}
@inproceedings{wu2017acktr,
abstract = {In this work, we propose to apply trust region optimization to deep reinforce-ment learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the Mu-JoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2-to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines.},
author = {Wu, Yuhuai and Mansimov, Elman and Liao, Shun and Grosse, Roger and Ba, Jimmy},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {::;::},
title = {{Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation}},
url = {https://arxiv.org/pdf/1708.05144.pdf},
year = {2017}
}
@inproceedings{mishra2017icml,
abstract = {We introduce a method for learning the dynamics of complex nonlinear systems based on deep gen-erative models over temporal segments of states and actions. Unlike dynamics models that oper-ate over individual discrete timesteps, we learn the distribution over future state trajectories con-ditioned on past state, past action, and planned future action trajectories, as well as a latent prior over action trajectories. Our approach is based on convolutional autoregressive models and vari-ational autoencoders. It makes stable and accu-rate predictions over long horizons for complex, stochastic systems, effectively expressing uncer-tainty and modeling the effects of collisions, sen-sory noise, and action delays. The learned dy-namics model and action prior can be used for end-to-end, fully differentiable trajectory opti-mization and model-based policy optimization, which we use to evaluate the performance and sample-efficiency of our method.},
author = {Mishra, Nikhil and Abbeel, Pieter and Mordatch, Igor},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Prediction and Control with Temporal Segment Models}},
url = {https://arxiv.org/pdf/1703.04070.pdf},
year = {2017}
}
@inproceedings{haarnoja2017sql,
abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learn-ing maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that ex-presses the optimal policy via a Boltzmann dis-tribution. We use the recently proposed amor-tized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved explo-ration and compositionality that allows transfer-ring skills between tasks, which we confirm in simulated experiments with swimming and walk-ing robots. We also draw a connection to actor-critic methods, which can be viewed perform-ing approximate inference on the corresponding energy-based model.},
author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::;::},
title = {{Reinforcement Learning with Deep Energy-Based Policies}},
url = {https://arxiv.org/pdf/1702.08165.pdf},
year = {2017}
}
@inproceedings{christiano2017humanpreferences,
abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
archivePrefix = {arXiv},
arxivId = {1706.03741},
author = {Christiano, Paul F and Leike, Jan and Brown, Tom B and Martic, Miljan and Legg, Shane and Amodei, Dario},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {1706.03741},
file = {::;::},
title = {{Deep reinforcement learning from human preferences}},
url = {https://arxiv.org/pdf/1706.03741.pdf http://arxiv.org/abs/1706.03741},
year = {2017}
}
@book{szepesvari2010,
abstract = {Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective.What distin- guishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the al- gorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book,we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming.We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.},
author = {Szepesv{\'{a}}ri, Csaba},
booktitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
doi = {10.2200/S00268ED1V01Y201005AIM009},
file = {::},
isbn = {9781608454921},
issn = {1939-4608},
number = {1},
pages = {1--103},
title = {{Algorithms for Reinforcement Learning}},
url = {https://sites.ualberta.ca/{~}szepesva/papers/RLAlgsInMDPs.pdf},
volume = {4},
year = {2010}
}
@inproceedings{goodfellow2014gan,
abstract = {We propose a new framework for estimating generative models via an adversar-ial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The train-ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net-works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
author = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow et al. - Unknown - Generative Adversarial Nets.pdf:pdf;:Users/ashvin/code/kindlize/pdfs/deeprl/gans-Goodfellow14.pdf:pdf},
title = {{Generative Adversarial Nets}},
url = {https://arxiv.org/pdf/1406.2661.pdf},
year = {2014}
}
@article{Sunderhauf2018,
abstract = {The application of deep learning in robotics leads to very specific problems and research questions that are typically not addressed by the computer vision and machine learning communities. In this paper we discuss a number of robotics-specific learning, reasoning, and embodiment challenges for deep learning. We explain the need for better evaluation metrics, highlight the importance and unique challenges for deep robotic learning in simulation, and explore the spectrum between purely data-driven and model-driven approaches. We hope this paper provides a motivating overview of important research directions to overcome the current limitations, and help fulfill the promising potentials of deep learning in robotics.},
archivePrefix = {arXiv},
arxivId = {1804.06557},
author = {S{\"{u}}nderhauf, Niko and Brock, Oliver and Scheirer, Walter and Hadsell, Raia and Fox, Dieter and Leitner, J{\"{u}}rgen and Upcroft, Ben and Abbeel, Pieter and Burgard, Wolfram and Milford, Michael and Corke, Peter},
doi = {10.1177/ToBeAssigned},
eprint = {1804.06557},
file = {::},
isbn = {0037549716666},
issn = {1756-8277},
journal = {International Journal of Robotics Research},
number = {4-5},
pages = {405--420},
title = {{The Limits and Potentials of Deep Learning for Robotics}},
url = {http://arxiv.org/abs/1804.06557},
volume = {37},
year = {2018}
}
@inproceedings{Todorov2018,
abstract = {— We develop a general control framework where a low-level optimizer is built into the robot dynamics. This optimizer together with the robot constitute a goal directed dynamical system, controlled on a higher level. The high level command is a cost function. It can encode desired accelerations, end-effector poses, center of pressure, and other intuitive features that have been studied before. Unlike the currently popular quadratic programming framework, which comes with performance guarantees at the expense of modeling flexibility, the optimization problem we solve at each time step is non-convex and non-smooth. Nevertheless, by exploiting the unique properties of the soft-constraint physics model we have recently developed, we are able to design an efficient solver for goal directed dynamics. It is only two times slower than the forward dynamics solver, and is much faster than real time. The simulation results reveal that complex movements can be generated via greedy optimization of simple costs. This new computational infrastructure can facilitate teleoperation, feature-based control, deep learning of control policies, and trajectory optimization. It will become a standard feature in future releases of the MuJoCo simulator.},
author = {Todorov, Emanuel},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {::;::},
isbn = {9781538630808},
title = {{Goal Directed Dynamics}},
url = {https://homes.cs.washington.edu/{~}todorov/papers/TodorovICRA18.pdf},
year = {2018}
}
@article{Keramati,
abstract = {Humans learn to play video games significantly faster than state-of-the-art rein-forcement learning (RL) algorithms. Inspired by this, we introduce strategic object oriented reinforcement learning (SOORL) to learn simple dynamics model through automatic model selection and perform efficient planning with strategic exploration. We compare different exploration strategies in a model-based setting in which exact planning is impossible. Additionally, we test our approach on perhaps the hardest Atari game Pitfall! and achieve significantly improved exploration and performance over prior methods.},
archivePrefix = {arXiv},
arxivId = {1806.00175},
author = {Keramati, Ramtin and Whang, Jay and Cho, Patrick and Brunskill, Emma},
eprint = {1806.00175},
file = {::},
pages = {1--10},
title = {{Strategic Object Oriented Reinforcement Learning}}
}
@article{Johnson,
author = {Johnson, Matthew James and Duvenaud, David and Wiltschko, Alexander B and Datta, Sandeep R and Adams, Ryan P},
file = {::;::;::},
title = {{Composing graphical models with neural networks for structured representations and fast inference}}
}
@inproceedings{Arjovsky2017,
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Wasserstein GAN}},
url = {https://arxiv.org/pdf/1701.07875.pdf},
year = {2017}
}
@article{Kalakrishnan2011,
abstract = {We present a control architecture for fast quadruped locomotion over rough terrain. We approach the problem by decomposing it into many sub-systems, in which we apply state-of-the-art learning, planning, optimization, and control techniques to achieve robust, fast locomotion. Unique features of our control strategy include: (1) a system that learns optimal foothold choices from expert demonstration using terrain templates, (2) a body trajectory optimizer based on the Zero-Moment Point (ZMP) stability criterion, and (3) a floating-base inverse dynamics controller that, in conjunction with force control, allows for robust, compliant locomotion over unperceived obstacles. We evaluate the performance of our controller by testing it on the LittleDog quadruped robot, over a wide variety of rough terrains of varying difficulty levels. The terrain that the robot was tested on includes rocks, logs, steps, barriers, and gaps, with obstacle sizes up to the leg length of the robot. We demonstrate the generalization ability of this controller by presenting results from testing performed by an independent external test team on terrain that has never been shown to us. {\textcopyright} 2011 The Author(s).},
author = {Kalakrishnan, Mrinal and Buchli, Jonas and Pastor, Peter and Mistry, Michael and Schaal, Stefan},
doi = {10.1177/0278364910388677},
file = {::;::},
isbn = {0278-3649},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {floating base inverse,locomotion planning and control,quadruped locomotion,template learning,zmp optimization},
number = {2},
pages = {236--258},
title = {{Learning, planning, and control for quadruped locomotion over challenging terrain}},
url = {https://www.researchgate.net/profile/Michael{\_}Mistry/publication/220122611{\_}Learning{\_}planning{\_}and{\_}control{\_}for{\_}quadruped{\_}locomotion{\_}over{\_}challenging{\_}terrain/links/02bfe50ca950d456c2000000.pdf},
volume = {30},
year = {2011}
}
@inproceedings{Lambert2018,
abstract = {We consider the problems of learning forward models that map state to high-dimensional images and inverse models that map high-dimensional images to state in robotics. Specifically, we present a perceptual model for generating video frames from state with deep networks, and provide a framework for its use in tracking and prediction tasks. We show that our proposed model greatly outperforms standard deconvolutional methods and GANs for image generation, producing clear, photo-realistic images. We also develop a convolutional neural network model for state estimation and compare the result to an Extended Kalman Filter to estimate robot trajectories. We validate all models on a real robotic system.},
archivePrefix = {arXiv},
arxivId = {1710.11311},
author = {Lambert, Alexander and Shaban, Amirreza and Raj, Amit and Liu, Zhen and Boots, Byron},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
eprint = {1710.11311},
file = {::},
isbn = {9781538630808},
title = {{Deep Forward and Inverse Perceptual Models for Tracking and Prediction}},
url = {http://arxiv.org/abs/1710.11311},
year = {2018}
}
@article{Wayne2018,
abstract = {Animals execute goal-directed behaviours despite the limited range and scope of their sensors. To cope, they explore environments and store memories maintaining estimates of important information that is not presently available. Recently, progress has been made with artificial intelligence (AI) agents that learn to perform tasks from sensory input, even at a human level, by merging reinforcement learning (RL) algorithms with deep neural networks, and the excitement surrounding these results has led to the pursuit of related ideas as explanations of non-human animal learning. However, we demonstrate that contemporary RL algorithms struggle to solve simple tasks when enough information is concealed from the sensors of the agent, a property called "partial observability". An obvious requirement for handling partially observed tasks is access to extensive memory, but we show memory is not enough; it is critical that the right information be stored in the right format. We develop a model, the Memory, RL, and Inference Network (MERLIN), in which memory formation is guided by a process of predictive modeling. MERLIN facilitates the solution of tasks in 3D virtual reality environments for which partial observability is severe and memories must be maintained over long durations. Our model demonstrates a single learning agent architecture that can solve canonical behavioural tasks in psychology and neurobiology without strong simplifying assumptions about the dimensionality of sensory input or the duration of experiences.},
archivePrefix = {arXiv},
arxivId = {1803.10760},
author = {Wayne, Greg and Hung, Chia-Chun and Amos, David and Mirza, Mehdi and Ahuja, Arun and Grabska-Barwinska, Agnieszka and Rae, Jack and Mirowski, Piotr and Leibo, Joel Z. and Santoro, Adam and Gemici, Mevlana and Reynolds, Malcolm and Harley, Tim and Abramson, Josh and Mohamed, Shakir and Rezende, Danilo and Saxton, David and Cain, Adam and Hillier, Chloe and Silver, David and Kavukcuoglu, Koray and Botvinick, Matt and Hassabis, Demis and Lillicrap, Timothy},
eprint = {1803.10760},
file = {::},
title = {{Unsupervised Predictive Memory in a Goal-Directed Agent}},
url = {http://arxiv.org/abs/1803.10760},
year = {2018}
}
@article{Mirowski2018,
abstract = {Navigating through unstructured environments is a basic capability of intelligent creatures, and thus is of fundamental interest in the study and development of artificial intelligence. Long-range navigation is a complex cognitive task that relies on developing an internal representation of space, grounded by recognisable landmarks and robust visual processing, that can simultaneously support continuous self-localisation ("I am here") and a representation of the goal ("I am going there"). Building upon recent research that applies deep reinforcement learning to maze navigation problems, we present an end-to-end deep reinforcement learning approach that can be applied on a city scale. Recognising that successful navigation relies on integration of general policies with locale-specific knowledge, we propose a dual pathway architecture that allows locale-specific features to be encapsulated, while still enabling transfer to multiple cities. We present an interactive navigation environment that uses Google StreetView for its photographic content and worldwide coverage, and demonstrate that our learning method allows agents to learn to navigate multiple cities and to traverse to target destinations that may be kilometres away. A video summarizing our research and showing the trained agent in diverse city environments as well as on the transfer task is available at: https://sites.google.com/view/streetlearn.},
archivePrefix = {arXiv},
arxivId = {1804.00168},
author = {Mirowski, Piotr and Grimes, Matthew Koichi and Malinowski, Mateusz and Hermann, Karl Moritz and Anderson, Keith and Teplyashin, Denis and Simonyan, Karen and Kavukcuoglu, Koray and Zisserman, Andrew and Hadsell, Raia},
eprint = {1804.00168},
file = {::},
title = {{Learning to Navigate in Cities Without a Map}},
url = {http://arxiv.org/abs/1804.00168},
year = {2018}
}
@article{Hunt1992,
abstract = {This paper focuses on the promise of artificial neural networks in the realm of modelling, identification and control of nonlinear systems. The basic ideas and techniques of artificial neural networks are presented in language and notation familiar to control engineers. Applications of a variety of neural network architectures in control are surveyed. We explore the links between the fields of control science and neural networks in a unified presentation and identify key areas for future research. {\textcopyright} 1992.},
archivePrefix = {arXiv},
arxivId = {0005-1098/92 {\$}5.00 + 0.00},
author = {Hunt, K. J. and Sbarbaro, D. and Zbikowski, R. and Gawthrop, P. J.},
doi = {10.1016/0005-1098(92)90053-I},
eprint = {92 {\$}5.00 + 0.00},
file = {::},
isbn = {ISSN{\~{}}{\~{}}0005-1098},
issn = {00051098},
journal = {Automatica},
keywords = {Neural networks,nonlinear control systems,nonlinear systems modelling,systems identification},
number = {6},
pages = {1083--1112},
primaryClass = {0005-1098},
title = {{Neural networks for control systems-A survey}},
volume = {28},
year = {1992}
}
@inproceedings{kaelbling1993goals,
abstract = {Temporal diierence methods solve the tem-poral credit assignment problem for reinforce-ment learning. An important subproblem of general reinforcement learning is learning to achieve dynamic goals. Although existing tem-poral diierence methods, such as Q learning, can be applied to this problem, they do not take advantage of its special structure. This pa-per presents the DG-learning algorithm, which learns eeciently to achieve dynamically chang-ing goals and exhibits good knowledge transfer between goals. In addition, this paper shows how traditional relaxation techniques can be applied to the problem. Finally, experimen-tal results are given that demonstrate the su-periority of DG learning over Q learning in a moderately large, synthetic, non-deterministic domain.},
author = {Kaelbling, L P},
booktitle = {International Joint Conference on Artificial Intelligence (IJCAI)},
file = {::},
keywords = {learning (artificial intelligence),temporal reason},
pages = {1094 -- 8},
title = {{Learning to achieve goals}},
volume = {vol.2},
year = {1993}
}
@article{Cheney2013,
abstract = {In 1994 Karl Sims showed that computational evolution can produce interesting morphologies that resemble natural or-ganisms. Despite nearly two decades of work since, evolved morphologies are not obviously more complex or natural, and the field seems to have hit a complexity ceiling. One hypothesis for the lack of increased complexity is that most work, including Sims', evolves morphologies composed of rigid elements, such as solid cubes and cylinders, limiting the design space. A second hypothesis is that the encod-ings of previous work have been overly regular, not allow-ing complex regularities with variation. Here we test both hypotheses by evolving soft robots with multiple materials and a powerful generative encoding called a compositional pattern-producing network (CPPN). Robots are selected for locomotion speed. We find that CPPNs evolve faster robots than a direct encoding and that the CPPN morphologies appear more natural. We also find that locomotion per-formance increases as more materials are added, that di-versity of form and behavior can be increased with di↵er-ent cost functions without stifling performance, and that organisms can be evolved at di↵erent levels of resolution. These findings suggest the ability of generative soft-voxel systems to scale towards evolving a large diversity of com-plex, natural, multi-material creatures. Our results suggest that future work that combines the evolution of CPPN-encoded soft, multi-material robots with modern diversity-encouraging techniques could finally enable the creation of creatures far more complex and interesting than those pro-duced by Sims nearly twenty years ago.},
author = {Cheney, Nick and Maccurdy, Robert and Clune, Jeff and Lipson, Hod},
file = {::},
keywords = {Algorithms,CPPN-NEAT,Categories and Subject Descriptors,Design,Evolving Morphologies,Experimentation Keywords,Generative Encodings,Genetic Algorithms,HyperNEAT,I211 [Distributed Artificial Intelligence],Intelligent Agents General Terms,Soft-Robotics},
title = {{Unshackling Evolution: Evolving Soft Robots with Multiple Materials and a Powerful Generative Encoding}},
url = {http://jeffclune.com/publications/2013{\_}Softbots{\_}GECCO.pdf},
year = {2013}
}
@article{Levine2018,
abstract = {The framework of reinforcement learning or optimal control provides a mathe-matical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learn-ing and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: for-malizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy rein-forcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynam-ics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
archivePrefix = {arXiv},
arxivId = {arXiv:1805.00909v2},
author = {Levine, Sergey},
eprint = {arXiv:1805.00909v2},
file = {::},
title = {{Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review}},
url = {https://arxiv.org/pdf/1805.00909.pdf},
year = {2018}
}
@article{Lesort2018,
abstract = {Representation learning algorithms are designed to learn abstract features that character-ize data. State representation learning (SRL) focuses on a particular kind of representation learning where learned features are in low dimension, evolve through time, and are influenced by actions of an agent. As the representation learned captures the variation in the environment generated by agents, this kind of representation is particularly suitable for robotics and con-trol scenarios. In particular, the low dimension helps to overcome the curse of dimensionality, provides easier interpretation and utilization by humans and can help improve performance and speed in policy learning algorithms such as reinforcement learning. This survey aims at covering the state-of-the-art on state representation learning in the most recent years. It reviews different SRL methods that involve interaction with the envi-ronment, their implementations and their applications in robotics control tasks (simulated or real). In particular, it highlights how generic learning objectives are differently exploited in the reviewed algorithms. Finally, it discusses evaluation methods to assess the representation learned and summarizes current and future lines of research.},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.04181v1},
author = {Lesort, Timoth{\'{e}}e and D{\'{i}}az-Rodr{\'{i}}guez, Natalia and Goudou, Jean-Fran{\c{c}}ois and Filliat, David},
eprint = {arXiv:1802.04181v1},
file = {::},
keywords = {Disentanglement of control factors,Learning Disentangled Representations,Low Dimensional Embedding Learning,Reinforcement Learn-ing,Robotics,State Representation Learning},
title = {{State Representation Learning for Control: An Overview}},
url = {https://arxiv.org/pdf/1802.04181.pdf},
year = {2018}
}
@inproceedings{konda2015odometry,
abstract = {We present an approach to predicting velocity and direction changes from visual information (" visual odom-etry ") using an end-to-end, deep learning-based architecture. The architecture uses a single type of compu-tational module and learning rule to extract visual motion, depth, and finally odometry information from the raw data. Representations of depth and motion are extracted by detecting synchrony across time and stereo channels using network layers with multiplicative interactions. The extracted representations are turned into information about changes in velocity and direction using a convolutional neural network. Preliminary results show that the architecture is capable of learning the resulting mapping from video to egomotion.},
author = {Konda, Kishore and Memisevic, Roland},
booktitle = {International Conference on Computer Vision Theory and Applications (VISAPP)},
file = {::},
keywords = {Convolutional networks,Motion,Stereo,Visual odometry},
title = {{Learning visual odometry with a convolutional network}},
url = {http://www.iro.umontreal.ca/{~}memisevr/pubs/VISAPP2015.pdf},
year = {2015}
}
@inproceedings{coates2008demos,
abstract = {We consider the problem of learning to follow a desired trajectory when given a small num-ber of demonstrations from a sub-optimal ex-pert. We present an algorithm that (i) ex-tracts the—initially unknown—desired tra-jectory from the sub-optimal expert's demon-strations and (ii) learns a local model suit-able for control along the learned trajectory. We apply our algorithm to the problem of autonomous helicopter flight. In all cases, the autonomous helicopter's performance ex-ceeds that of our expert helicopter pilot's demonstrations. Even stronger, our results significantly extend the state-of-the-art in au-tonomous helicopter aerobatics. In particu-lar, our results include the first autonomous tic-tocs, loops and hurricane, vastly superior performance on previously performed aero-batic maneuvers (such as in-place flips and rolls), and a complete airshow, which requires autonomous transitions between these and various other maneuvers.},
author = {Coates, Adam and Abbeel, Pieter and Ng, Andrew Y},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Learning for Control from Multiple Demonstrations}},
url = {https://www-cs.stanford.edu/people/ang/papers/icml08-LearningForControlFromMultipleDemonstrations.pdf},
year = {2008}
}
@inproceedings{we2018mpo,
author = {Abdolmaleki, Abbas and Springenberg, Jost Tobias and Tassa, Yuval and Munos, Remi and Heess, Nicolas and Riedmiller, Martin},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Abdolmaleki et al. - 2018 - Maximum a Posteriori Policy Optimisation(4).pdf:pdf},
pages = {1--19},
title = {{Maximum a Posteriori Policy Optimisation}},
year = {2018}
}
@article{Silver,
abstract = {One of the key challenges of artificial intelli-gence is to learn models that are effective in the context of planning. In this document we intro-duce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled for-ward multiple " imagined " planning steps. Each forward pass of the predictron accumulates in-ternal rewards and values over multiple plan-ning depths. The predictron is trained end-to-end so as to make these accumulated values ac-curately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neu-ral network architectures.},
author = {Silver, David and Hasselt, Hado Van and Hessel, Matteo and Schaul, Tom and Guez, Arthur and Harley, Tim and Dulac-Arnold, Gabriel and Reichert, David and Rabinowitz, Neil and Barreto, Andre and Degris, Thomas},
file = {::},
title = {{The Predictron: End-To-End Learning and Planning}},
url = {https://arxiv.org/pdf/1612.08810.pdf}
}
@article{Ritchie,
abstract = {Probabilistic programming languages (PPLs) are a powerful modeling tool, able to represent any computable probability distribution. Unfortunately, probabilistic program inference is often intractable, and existing PPLs mostly rely on expensive, approximate sampling-based methods. To alleviate this problem, one could try to learn from past inferences, so that future inferences run faster. This strategy is known as amortized inference; it has recently been applied to Bayesian net-works [28, 22] and deep generative models [20, 15, 24]. This paper proposes a system for amortized inference in PPLs. In our system, amortization comes in the form of a parameterized guide program. Guide programs have similar structure to the original program, but can have richer data flow, including neural network components. These networks can be optimized so that the guide approximately samples from the posterior distribution defined by the original program. We present a flexible interface for defining guide programs and a stochastic gradient-based scheme for optimizing guide parameters, as well as some preliminary results on automatically deriving guide programs. We explore in detail the common machine learning pattern in which a 'local' model is specified by 'global' random values and used to generate independent observed data points; this gives rise to amortized local inference supporting global model learning.},
author = {Ritchie, Daniel and Horsfall, Paul and Goodman, Noah D},
file = {::},
title = {{Deep Amortized Inference for Probabilistic Programs}},
url = {https://arxiv.org/pdf/1610.05735.pdf}
}
@article{Blei2017,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1601.00670v7},
author = {Blei, David M and Kucukelbir, Alp and Mcauliffe, Jon D},
eprint = {arXiv:1601.00670v7},
file = {::},
keywords = {Algorithms,Computationally Intensive Methods,Statistical Computing},
title = {{Variational Inference: A Review for Statisticians}},
url = {https://arxiv.org/pdf/1601.00670.pdf},
year = {2017}
}
@article{Hashim2017,
abstract = {{\textcopyright} 2017 Australian and New Zealand Society of Cardiac and Thoracic Surgeons (ANZSCTS) and the Cardiac Society of Australia and New Zealand (CSANZ). The revision of an internal mammary artery graft anastomosis because of a technical error can be time consuming and complicated and may lead to complications. Here, we describe the technical details and our early experience of using a standard transit-time flowmeter to exclude technical errors and facilitate rapid decision making for anastomosis revision in an arrested heart during aortic cross-clamping in the absence of ultrasound guidance.},
author = {Hashim, S.A. and Amin, M.A. and Nair, A. and {Raja Mokhtar}, R.A. and Krishnasamy, S. and Cheng, K.},
doi = {10.1016/j.hlc.2017.11.011},
issn = {14442892},
journal = {Heart Lung and Circulation},
keywords = {CABG device,Coronary artery bypass graft},
title = {{A Flowmeter Technique to Exclude Internal Mammary Artery Anastomosis Error in an Arrested Heart}},
year = {2017}
}
@article{Hestness,
abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents—the "steepness" of the learning curve—yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.},
author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and {Mostofa Ali Patwary}, Md and Yang, Yang and Zhou, Yanqi and Research, Baidu},
file = {::},
title = {{Deep Learning Scaling Is Predictable, Empirically}},
url = {https://arxiv.org/pdf/1712.00409.pdf}
}
@article{Legg2007,
abstract = {A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: We take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense. We then show how this formal definition is related to the theory of universal optimal learning agents. Finally, we survey the many other tests and definitions of intelligence that have been proposed for machines.},
archivePrefix = {arXiv},
arxivId = {arXiv:0712.3329v1},
author = {Legg, Shane and Hutter, Marcus},
eprint = {arXiv:0712.3329v1},
file = {::},
keywords = {AIXI,Complexity theory,Intel-ligence definitions,Intelligence,Intelligence tests,Theoretical foundations,Turing test},
title = {{Universal Intelligence: A Definition of Machine Intelligence}},
url = {www.vetta.org/shane www.hutter1.net},
year = {2007}
}
@article{Behrouzi,
abstract = {Most algorithms for reinforcement learning work by estimat-ing action-value functions. Here we present a method that uses Lagrange multipliers, the costate equation, and multi-layer neural networks to compute policy gradients. We show that this method can find solutions to time-optimal control problems, driving nonlinear mechanical systems quickly to a target configuration. On these tasks its performance is com-parable to that of deep deterministic policy gradient, a recent action-value method. Research in reinforcement learning has shown the effective-ness of algorithms based on action-value functions (also known as Q-functions) or closely related quantities such as value-or advantage functions [1-6]. Here we present a dif-ferent approach based on Lagrange multipliers and the co-state equation [7, 8], which may work better in some tasks. As usual in reinforcement learning, the setting involves an agent embedded in an environment which evolves through time according to a rule or function f, called the environment dynamics. For instance, the agent might be a brain and the environment its body, in which case f might represent the mechanics of that body. If st is the state of the environment at time t, and at is the action taken by the agent at this time (say, the motor commands issued by the brain), then at the next time step, t + ∆t, the state takes a new value},
author = {Behrouzi, Bita and Tweed, Douglas},
file = {::},
title = {{Lagrange policy gradient}},
url = {https://arxiv.org/pdf/1711.05817.pdf}
}
@article{Klein,
author = {Klein, Dan},
file = {::},
title = {{Lagrange Multipliers without Permanent Scarring}},
url = {https://people.eecs.berkeley.edu/{~}klein/papers/lagrange-multipliers.pdf}
}
@article{I,
abstract = {Theories of reward learning in neuroscience have focused on two families of algorithms, thought to capture deliberative vs. habitual choice. " Model-based " algorithms compute the value of candidate actions from scratch, whereas " model-free " algorithms make choice more efficient but less flexible by storing pre-computed action values. We examine an intermediate algorithmic family, the successor representation (SR), which balances flexibility and efficiency by storing partially computed action values: predictions about future events. These pre-computation strategies differ in how they update their choices following changes in a task. SR's reliance on stored predictions about future states predicts a unique signature of insensitivity to changes in the task's sequence of events, but flexible adjustment following changes to rewards. We provide evidence for such differential sensitivity in two behavioral studies with humans. These results suggest that the SR is a computational substrate for semi-flexible choice in humans, introducing a subtler, more cognitive notion of habit.},
author = {I, Momennejad and Em, Russek and Jh, Cheong and Mm, Botvinick and Nd, Daw and Sj, Gershman},
doi = {10.1101/083824},
file = {::},
keywords = {Decision making,Human behavior,Model-based,Planning,Predictive representation Acknowledgements,Reinforcement learning,Retrospective revaluation,Successor representation},
title = {{The successor representation in human reinforcement learning}},
url = {https://www.biorxiv.org/content/biorxiv/early/2017/07/04/083824.full.pdf}
}
@article{Dayan,
abstract = {Estimation of returns over time, the focus of temporal difference (TD) algorithms, imposes particular constraints on good function approximators or representations. Appropriate generalisation between states is determined by how similar their succes-sors are, and representations should follow suit. This paper shows how TD machinery can be used to learn such representations, and illustrates, using a navigation task, the appropriately distributed nature of the result.},
author = {Dayan, Peter},
file = {::},
title = {{Improving Generalisation for Temporal Difference Learning: The Successor Representation}},
url = {http://www.gatsby.ucl.ac.uk/{~}dayan/papers/d93b.pdf}
}
@article{Norouzi2017,
abstract = {A key problem in structured output prediction is direct optimization of the task reward function that matters for test evaluation. This paper presents a simple and computationally efficient approach to incorporate task reward into a maximum likelihood framework. By establishing a link between the log-likelihood and ex-pected reward objectives, we show that an optimal regularized expected reward is achieved when the conditional distribution of the outputs given the inputs is pro-portional to their exponentiated scaled rewards. Accordingly, we present a frame-work to smooth the predictive probability of the outputs using their corresponding rewards. We optimize the conditional log-probability of augmented outputs that are sampled proportionally to their exponentiated scaled rewards. Experiments on neural sequence to sequence models for speech recognition and machine transla-tion show notable improvements over a maximum likelihood baseline by using reward augmented maximum likelihood (RAML), where the rewards are defined as the negative edit distance between the outputs and the ground truth labels.},
archivePrefix = {arXiv},
arxivId = {arXiv:1609.00150v3},
author = {Norouzi, Mohammad and Bengio, Samy and Chen, Zhifeng and Jaitly, Navdeep and Schuster, Mike and Wu, Yonghui and Schuurmans, Dale},
eprint = {arXiv:1609.00150v3},
file = {::},
keywords = {()},
title = {{Reward Augmented Maximum Likelihood for Neural Structured Prediction}},
url = {https://arxiv.org/pdf/1609.00150.pdf},
year = {2017}
}

@inproceedings{laskin2020rad,
	title = {Reinforcement {Learning} with {Augmented} {Data}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/e615c82aba461681ade82da2da38004a-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Laskin, Misha and Lee, Kimin and Stooke, Adam and Pinto, Lerrel and Abbeel, Pieter and Srinivas, Aravind},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {19884--19895},
}

@inproceedings{peters2007rwr,
abstract = {Many robot control problems of practical im-portance, including operational space con-trol, can be reformulated as immediate re-ward reinforcement learning problems. How-ever, few of the known optimization or re-inforcement learning algorithms can be used in online learning control for robots, as they are either prohibitively slow, do not scale to interesting domains of complex robots, or re-quire trying out policies generated by random search, which are infeasible for a physical sys-tem. Using a generalization of the EM-base reinforcement learning framework suggested by Dayan {\&} Hinton, we reduce the prob-lem of learning with immediate rewards to a reward-weighted regression problem with an adaptive, integrated reward transforma-tion for faster convergence. The resulting al-gorithm is efficient, learns smoothly without dangerous jumps in solution space, and works well in applications of complex high degree-of-freedom robots.},
author = {Peters, Jan and Schaal, Stefan},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Peters, Schaal - Unknown - Reinforcement Learning by Reward-weighted Regression for Operational Space Control(3).pdf:pdf},
title = {{Reinforcement Learning by Reward-weighted Regression for Operational Space Control}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.6266{\&}rep=rep1{\&}type=pdf},
year = {2007}
}
@article{Lecun2006,
abstract = {To appear in " Predicting Structured Data " , G. Bakir, T. Hofman, B. Sch{\"{o}}lkopf, A. Smola, B. Taskar (eds) MIT Press, 2006 Abstract Energy-Based Models (EBMs) capture dependencies between variables by as-sociating a scalar energy to each configuration of the variables. Inference consists in clamping the value of observed variables and finding configurations of the re-maining variables that minimize the energy. Learning consists in finding an energy function in which observed configurations of the variables are given lower energies than unobserved ones. The EBM approach provides a common theoretical frame-work for many learning models, including traditional discriminative and genera-tive approaches, as well as graph-transformer networks, conditional random fields, maximum margin Markov networks, and several manifold learning methods. Probabilistic models must be properly normalized, which sometimes requires evaluating intractable integrals over the space of all possible variable configura-tions. Since EBMs have no requirement for proper normalization, this problem is naturally circumvented. EBMs can be viewed as a form of non-probabilistic factor graphs, and they provide considerably more flexibility in the design of architec-tures and training criteria than probabilistic approaches.},
author = {Lecun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, Marc Aurelio and Huang, Fu Jie},
file = {::},
title = {{A Tutorial on Energy-Based Learning}},
url = {http://yann.lecun.com},
year = {2006}
}
@article{Anderson1999,
abstract = {History and definition: The term " Monte Carlo " was apparently first used by Ulam and von Neumann as a Los Alamos code word for the stochastic simulations they applied to building better atomic bombs. Their methods, involving the laws of chance, were aptly named after the inter-national gaming destination; the moniker stuck and soon after the War a wide range of sticky problems yielded to the new techniques. Despite the widespread use of the methods, and numerous descriptions of them in articles and monographs, it is virtually impossible to find a succint defini-tion of " Monte Carlo method " in the literature. Perhaps this is owing to the intuitive nature of the topic which spawns many definitions by way of specific examples. Some authors prefer to use the term " stochastic simulation " for almost everything, reserving " Monte Carlo " only for Monte Carlo Integration and Monte Carlo Tests (cf. Ripley 1987). Others seem less concerned about blurring the distinction between simulation studies and Monte Carlo methods. Be that as it may, a suitable definition can be good to have, if for nothing other than to avoid the awkwardness of trying to define the Monte Carlo method by appealing to a whole bevy of examples of it. Since I am (so Elizabeth claims!) unduly influenced by my advisor's ways of thinking, I like to define Monte Carlo in the spirit of definitions she has used before. In particular, I use: Definition: Monte Carlo is the art of approximating an expectation by the sample mean of a function of simulated random variables. We will find that this definition is broad enough to cover everything that has been called Monte Carlo, and yet makes clear its essence in very familiar terms: Monte Carlo is about invoking laws of large numbers to approximate expectations. 1 While most Monte Carlo simulations are done by computer today, there were many applications of Monte Carlo methods using coin-flipping, card-drawing, or needle-tossing (rather than computer-generated pseudo-random numbers) as early as the turn of the century—long before the name Monte Carlo arose. In more mathematical terms: Consider a (possibly multidimensional) random variable X having probability mass function or probability density function f X (x) which is greater than zero on a set of values X . Then the expected value of a function g of X is},
author = {Anderson, Eric C},
file = {::},
title = {{Lecture Notes for Stat 578C Monte Carlo Methods and Importance Sampling}},
url = {http://ib.berkeley.edu/labs/slatkin/eriq/classes/guest{\_}lect/mc{\_}lecture{\_}notes.pdf},
year = {1999}
}
@article{Bengioa,
abstract = {Neuroscientists have long criticised deep learn-ing algorithms as incompatible with current knowledge of neurobiology. We explore more bi-ologically plausible versions of deep representa-tion learning, focusing here mostly on unsuper-vised learning but developing a learning mecha-nism that could account for supervised, unsuper-vised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple up-date rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective func-tion (be it supervised, unsupervised, or reward-driven). The second main idea is that this corre-sponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteri-ors, implemented by neural dynamics. Another contribution of this paper is that the gradients re-quired for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the proba-bilistic interpretation of auto-encoders to justify improved sampling schemes based on the gener-ative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Mesnard, Thomas and Lin, Zhouhan},
file = {::},
title = {{Towards Biologically Plausible Deep Learning}},
url = {https://arxiv.org/pdf/1502.04156.pdf}
}
@article{Rivest,
abstract = {Successful application of reinforcement learning algorithms often involves considerable hand-crafting of the necessary non-linear features to reduce the complexity of the value functions and hence to promote convergence of the algorithm. In contrast, the human brain readily and autonomously finds the complex features when provided with sufficient training. Recent work in machine learning and neurophysiology has demonstrated the role of the basal ganglia and the frontal cortex in mammalian reinforcement learning. This paper develops and explores new reinforcement learning algorithms inspired by neurological evidence that provides potential new approaches to the feature construction problem. The algorithms are compared and evaluated on the Acrobot task.},
author = {Rivest, Fran{\c{c}}ois and Bengio, Yoshua and Kalaska, John},
file = {::},
title = {{Brain Inspired Reinforcement Learning}},
url = {https://papers.nips.cc/paper/2749-brain-inspired-reinforcement-learning.pdf}
}
@inproceedings{Dosovitskiy,
abstract = {We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by in-teracting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sen-sory input from a complex three-dimensional environment. The presented formu-lation enables learning without a fixed goal at training time, and pursuing dynam-ically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.},
author = {Dosovitskiy, Alexey and Koltun, Vladlen},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {::},
title = {{Learning To Act By Predicting The Future}},
url = {https://arxiv.org/pdf/1611.01779.pdf},
year = {2017}
}
@article{Silvera,
author = {Silver, David},
file = {::},
title = {{Lecture 7: Policy Gradient}},
url = {http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching{\_}files/pg.pdf}
}
@article{Levine,
author = {Levine, Sergey},
file = {::},
title = {{Connections Between Inference and Control CS 294-112: Deep Reinforcement Learning}},
url = {http://rll.berkeley.edu/deeprlcourse/f17docs/lecture{\_}11{\_}control{\_}and{\_}inference.pdf}
}
@article{Ziebart,
abstract = {The principle of maximum entropy provides a powerful framework for statistical models of joint, conditional, and marginal distribu-tions. However, there are many important distributions with elements of interaction and feedback where its applicability has not been established. This work presents the principle of maximum causal entropy—an approach based on causally conditioned probabilities that can appropriately model the availabil-ity and influence of sequentially revealed side information. Using this principle, we de-rive models for sequential data with revealed information, interaction, and feedback, and demonstrate their applicability for statisti-cally framing inverse optimal control and de-cision prediction tasks.},
author = {Ziebart, Brian D and Bagnell, J Andrew and Dey, Anind K},
file = {::},
title = {{Modeling Interaction via the Principle of Maximum Causal Entropy}},
url = {http://www.cs.cmu.edu/{~}bziebart/publications/maximum-causal-entropy.pdf}
}
@inproceedings{bojanowski2017unsupervised,
abstract = {Convolutional neural networks provide visual features that perform remarkably well in many computer vision applications. However, train-ing these networks requires significant amounts of supervision. This paper introduces a generic framework to train deep networks, end-to-end, with no supervision. We propose to fix a set of target representations, called Noise As Targets (NAT), and to constrain the deep features to align to them. This domain agnostic approach avoids the standard unsupervised learning issues of triv-ial solutions and collapsing of features. Thanks to a stochastic batch reassignment strategy and a separable square loss function, it scales to mil-lions of images. The proposed approach pro-duces representations that perform on par with state-of-the-art unsupervised methods on Ima-geNet and PASCAL VOC.},
author = {Bojanowski, Piotr and Joulin, Armand},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Unsupervised Learning by Predicting Noise}},
url = {https://arxiv.org/pdf/1704.05310.pdf},
year = {2017}
}
@article{Rolf,
abstract = {We discuss the difficulty to learn control skills in high-dimensional domains that can not be exhaustively explored. We show how infant-development can serve as a role-model for highly efficient exploration: infants neither explore exhaustively, nor do they learn very versatile skills right in the beginning. Rather, they attempt goal-directed exploration to achieve feedforward control as a first step without re-quiring full knowledge of the world. This article reviews recent efforts to mimick such pathways by means of " goal babbling " , which have led to a series of algo-rithms that allow for a likewise efficient learning. We show that it permits to learn inverse models from examples even in the presence of non-convex solution sets by utilizing a reward-weighted regression scheme, and that a human-competitive learning speed can be achieved if online learning is applied " in the loop " . Results are verified on the " Bionic Handling Assistant " , a novel bionic robot that instan-tiates a wide spread of problems like high dimensions, non-stationary behavior, highly constrained actuators, sensory noise, and very slow response-behavior.},
author = {Rolf, Matthias and Asada, Minoru},
file = {::},
title = {{Learning Inverse Models in High Dimensions with Goal Babbling and Reward-Weighted Averaging}},
url = {http://acl.mit.edu/amlsc/files/amlsc13{\_}submission{\_}7.pdf}
}
@article{Kremer,
abstract = {In machine learning, active learning refers to algorithms that autonomously select the data points from which they will learn. There are many data mining appli-cations in which large amounts of unlabeled data are readily available, but labels (e.g., human annotations or results from complex experiments) are costly to ob-tain. In such scenarios, an active learning algorithm aims at identifying data points that, if labeled and used for training, would most improve the learned model. La-bels are then obtained only for the most promising data points. This speeds up learning and reduces labeling costs. Support vector machine (SVM) classifiers are particularly well-suited for active learning due to their convenient mathemat-ical properties. They perform linear classification, typically in a kernel-induced feature space, which makes measuring the distance of a data point from the de-cision boundary straightforward. Furthermore, heuristics can efficiently estimate how strongly learning from a data point influences the current model. This infor-mation can be used to actively select training samples. After a brief introduction to the active learning problem, we discuss different query strategies for select-ing informative data points and review how these strategies give rise to different variants of active learning with SVMs.},
author = {Kremer, Jan and Pedersen, Kim Steenstrup and Igel, Christian},
file = {::},
title = {{Active Learning with Support Vector Machines}},
url = {http://image.diku.dk/jank/papers/WIREs2014.pdf}
}
@article{Jonas2017,
author = {Jonas, Eric and Kording, Konrad Paul},
doi = {10.1371/journal.pcbi.1005268},
editor = {Diedrichsen, J{\"{o}}rn},
file = {::},
issn = {1553-7358},
journal = {PLOS Computational Biology},
month = {jan},
number = {1},
pages = {e1005268},
publisher = {Public Library of Science},
title = {{Could a Neuroscientist Understand a Microprocessor?}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1005268},
volume = {13},
year = {2017}
}
@article{Czarnecki,
abstract = {At the heart of deep learning we aim to use neural networks as function approxi-mators – training them to produce outputs from inputs in emulation of a ground truth function or data creation process. In many cases we only have access to input-output pairs from the ground truth, however it is becoming more common to have access to derivatives of the target output with respect to the input – for exam-ple when the ground truth function is itself a neural network such as in network compression or distillation. Generally these target derivatives are not computed, or are ignored. This paper introduces Sobolev Training for neural networks, which is a method for incorporating these target derivatives in addition the to target values while training. By optimising neural networks to not only approximate the func-tion's outputs but also the function's derivatives we encode additional information about the target function within the parameters of the neural network. Thereby we can improve the quality of our predictors, as well as the data-efficiency and generalization capabilities of our learned function approximation. We provide theoretical justifications for such an approach as well as examples of empirical evidence on three distinct domains: regression on classical optimisation datasets, distilling policies of an agent playing Atari, and on large-scale applications of synthetic gradients. In all three domains the use of Sobolev Training, employing target derivatives in addition to target values, results in models with higher accuracy and stronger generalisation.},
author = {Czarnecki, Wojciech Marian and Osindero, Simon and Jaderberg, Max and Swirszcz, Grzegorz and Pascanu, Razvan},
file = {::},
title = {{Sobolev Training for Neural Networks}},
url = {https://arxiv.org/pdf/1706.04859.pdf}
}
@article{Odonoghue2017,
abstract = {We consider the exploration/exploitation problem in reinforcement learning. For exploitation, it is well known that the Bellman equation connects the value at any time-step to the expected value at subsequent time-steps. In this paper we consider a similar uncertainty Bellman equation (UBE), which connects the uncertainty at any time-step to the expected uncertainties at subsequent time-steps, thereby extending the potential exploratory benefit of a policy beyond individual time-steps. We prove that the unique fixed point of the UBE yields an upper bound on the variance of the estimated value of any fixed policy. This bound can be much tighter than traditional count-based bonuses that compound standard deviation rather than variance. Importantly, and unlike several existing approaches to optimism, this method scales naturally to large systems with complex generalization. Substituting our UBE-exploration strategy for -greedy improves DQN performance on 51 out of 57 games in the Atari suite.},
author = {{O 'donoghue}, Brendan and Osband, Ian and Munos, Remi and Deepmind, Volodymyr Mnih},
file = {::},
title = {{The Uncertainty Bellman Equation and Exploration}},
url = {https://arxiv.org/pdf/1709.05380.pdf},
year = {2017}
}
@article{Vasilaki2017,
abstract = {The Epicurean Philosophy is commonly thought as sim-plistic and hedonistic. Here I discuss how this is a misconception and explore its link to Reinforcement Learning. Based on the letters of Epicurus, I construct an objective function for hedonism which turns out to be equivalent of the Reinforcement Learning objective function when omitting the discount factor. I then dis-cuss how Plato and Aristotle 's views that can be also loosely linked to Reinforcement Learning, as well as their weaknesses in relationship to it. Finally, I em-phasise the close affinity of the Epicurean views and the Bellman equation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1710.04582v1},
author = {Vasilaki, Eleni},
eprint = {arXiv:1710.04582v1},
file = {::},
title = {{Is Epicurus the father of Reinforcement Learning?}},
url = {https://arxiv.org/pdf/1710.04582.pdf},
year = {2017}
}
@inproceedings{osband2016bootstrapped,
abstract = {Efficient exploration remains a major challenge for reinforcement learning (RL). Common dithering strategies for exploration, such as -greedy, do not carry out temporally-extended (or deep) exploration; this can lead to exponentially larger data requirements. However, most algorithms for statistically efficient RL are not computationally tractable in complex en-vironments. Randomized value functions offer a promising approach to efficient exploration with generalization, but existing algorithms are not compatible with nonlinearly parameterized value functions. As a first step towards addressing such contexts we develop bootstrapped DQN. We demon-strate that bootstrapped DQN can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strat-egy. In the Arcade Learning Environment bootstrapped DQN substantially improves learning speed and cumulative performance across most games.},
author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Roy, Benjamin Van},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {::},
title = {{Deep Exploration via Bootstrapped DQN}},
url = {https://arxiv.org/pdf/1602.04621.pdf},
year = {2016}
}
@inproceedings{byravan2018se3pose,
abstract = {— In this work, we present an approach to deep visuomotor control using structured deep dynamics models. Our deep dynamics model, a variant of SE3-Nets, learns a low-dimensional pose embedding for visuomotor control via an encoder-decoder structure. Unlike prior work, our dynamics model is structured: given an input scene, our network ex-plicitly learns to segment salient parts and predict their pose-embedding along with their motion modeled as a change in the pose space due to the applied actions. We train our model using a pair of point clouds separated by an action and show that given supervision only in the form of point-wise data associations between the frames our network is able to learn a meaningful segmentation of the scene along with consistent poses. We further show that our model can be used for closed-loop control directly in the learned low-dimensional pose space, where the actions are computed by minimizing error in the pose space using gradient-based methods, similar to traditional model-based control. We present results on controlling a Baxter robot from raw depth data in simulation and in the real world and compare against two baseline deep networks. Our method runs in real-time, achieves good prediction of scene dynamics and outperforms the baseline methods on multiple control runs. Video results can be found at: https://rse-lab.cs. washington.edu/se3-structured-deep-ctrl/},
author = {Byravan, Arunkumar and Leeb, Felix and Meier, Franziska and Fox, Dieter},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {::},
title = {{SE3-Pose-Nets: Structured Deep Dynamics Models for Visuomotor Planning and Control}},
url = {https://arxiv.org/pdf/1710.00489.pdf},
year = {2018}
}
@article{Pritzel,
abstract = {Deep reinforcement learning methods attain super-human performance in a wide range of en-vironments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep rein-forcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience con-taining slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforce-ment learning agents.},
author = {Pritzel, Alexander and Uria, Benigno and Srinivasan, Sriram and Com, Adriap@google and Vinyals, Oriol and Hassabis, Demis and Wierstra, Daan and Blundell, Charles},
file = {::},
title = {{Neural Episodic Control Adr{\`{i}} aPuigdo enech}},
url = {https://arxiv.org/pdf/1703.01988.pdf}
}
@article{Venkatraman,
abstract = {Recurrent neural networks (RNNs) are a vital modeling technique that rely on internal states learned indirectly by optimization of a supervised, unsupervised, or reinforcement training loss. RNNs are used to model dynamic processes that are characterized by underlying latent states whose form is often unknown, precluding its analytic representation inside an RNN. In the Predictive-State Representation (PSR) literature, latent state processes are modeled by an internal state representa-tion that directly models the distribution of future observations, and most recent work in this area has relied on explicitly representing and targeting sufficient statis-tics of this probability distribution. We seek to combine the advantages of RNNs and PSRs by augmenting existing state-of-the-art recurrent neural networks with PREDICTIVE-STATE DECODERS (PSDs), which add supervision to the network's internal state representation to target predicting future observations. PSDs are simple to implement and easily incorporated into existing training pipelines via additional loss regularization. We demonstrate the effectiveness of PSDs with experimental results in three different domains: probabilistic filtering, Imitation Learning, and Reinforcement Learning. In each, our method improves statistical performance of state-of-the-art recurrent baselines and does so with fewer iterations and less data.},
author = {Venkatraman, Arun and Rhinehart, Nicholas and Sun, Wen and Pinto, Lerrel and Hebert, Martial and Boots, Byron and Kitani, Kris M and Bagnell, J Andrew},
file = {::},
title = {{Predictive-State Decoders: Encoding the Future into Recurrent Networks}},
url = {https://arxiv.org/pdf/1709.08520.pdf}
}
@inproceedings{kaiser2017rareevents,
author = {Kaiser, Lukasz and Nachum, Ofir and Aurko, Roy and Bengio, Samy},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {::},
title = {{Learning to Remember Rare Events}},
year = {2017}
}
@article{Ghahramani2015,
abstract = {T he key idea behind the probabilistic framework to machine learn-ing is that learning can be thought of as inferring plausible models to explain observed data. A machine can use such models to make predictions about future data, and take decisions that are rational given these predictions. Uncertainty plays a fundamental part in all of this. Observed data can be consistent with many models, and therefore which model is appropriate, given the data, is uncertain. Similarly, predictions about future data and the future consequences of actions are uncertain. Probability theory provides a framework for modelling uncertainty. This Review starts with an introduction to the probabilistic approach to machine learning and Bayesian inference, and then discusses some of the state-of-the-art advances in the field. Many aspects of learning and intelligence crucially depend on the careful probabilistic representation of uncertainty. Probabilistic approaches have only recently become a main-stream approach to artificial intelligence 1 , robotics 2 and machine learn-ing 3,4 . Even now, there is controversy in these fields about how important it is to fully represent uncertainty. For example, advances using deep neural networks to solve challenging pattern-recognition problems such as speech recognition 5 , image classification 6,7 , and prediction of words in text 8 , do not overtly represent the uncertainty in the structure or parameters of those neural networks. However, my focus will not be on these types of pattern-recognition problems, characterized by the availability of large amounts of data, but on problems for which uncertainty is really a key ingredient, for example where a decision may depend on the amount of uncertainty. I highlight five areas of current research at the frontier of probabilistic machine learning, emphasizing areas that are of broad relevance to sci-entists across many fields: probabilistic programming, which is a general framework for expressing probabilistic models as computer programs and which could have a major impact on scientific modelling; Bayes-ian optimization, which is an approach to globally optimizing unknown functions; probabilistic data compression; automating the discovery of plausible and interpretable models from data; and hierarchical modelling for learning many related models, for example for personalized medicine or recommendation. Although considerable challenges remain, the com-ing decade promises substantial advances in artificial intelligence and machine learning based on the probabilistic framework.},
author = {Ghahramani, Zoubin},
doi = {10.1038/nature14541},
file = {::},
journal = {Nature},
title = {{Probabilistic modelling and representing uncertainty}},
url = {https://www.cse.iitk.ac.in/users/piyush/courses/pml{\_}winter16/nature14541.pdf},
volume = {521},
year = {2015}
}
@article{Brooks1991,
abstract = {Artificial intelligence research has foundered on the issue of representation. When intelligence is approached in an incremental manner, with strict reliance on interfacing to the real world through perception and action, reliance on representation disappears. In this paper we outline our approach to incrementally building complete intelligent Creatures. The fundamental decomposition of the intelligent system is not into independent information processing units which must interface with each other via representations. Instead, the intelligent system is decomposed into independent and parallel activity producers which all interface directly to the world through perception and action, rather than interface to each other particularly much. The notions of central and peripheral systems evaporateeverything is both central and peripheral. Based on these principles we have built a very successful series of mobile robots which operate without supervision as Creatures in standard office environments.},
author = {Brooks, Rodney A},
file = {::},
journal = {Artificial Intelligence},
pages = {139--159},
title = {{Intelligence without representation*}},
url = {http://people.csail.mit.edu/brooks/papers/representation.pdf},
volume = {47},
year = {1991}
}
@inproceedings{held2018goalgan,
abstract = {Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to vary-ing locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing in its environment. We use a generator network to propose tasks for the agent to try to achieve, each task being specified as reaching a certain parametrized sub-set of the state-space. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus auto-matically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.},
author = {Held, David and Geng, Xinyang and Florensa, Carlos and Abbeel, Pieter},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Automatic Goal Generation for Reinforcement Learning Agents}},
url = {https://arxiv.org/pdf/1705.06366.pdf},
year = {2018}
}
@inproceedings{schulman2016gae,
abstract = {Policy gradient methods are an appealing approach in reinforcement learning be-cause they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the diffi-culty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substan-tially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD($\lambda$). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomo-tion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy repre-sentations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experi-ence required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael I and Abbeel, Pieter},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {::},
title = {{High-Dimensional Continuous Control Using Generalized Advantage Estimation}},
url = {https://arxiv.org/pdf/1506.02438.pdf},
year = {2016}
}
@article{Pastor2011,
abstract = {on PR2},
author = {Pastor, Peter},
doi = {10.1109/ICRA.2011.5980200},
file = {::},
isbn = {978-1-61284-386-5},
issn = {1050-4729},
journal = {IEEE International Conference on Robotics and Automation (ICRA)},
pages = {3828--3834},
title = {{Skill Learning and Task Outcome Prediction for Manipulation}},
url = {http://www.cs.utexas.edu/{~}sniekum/classes/RLFD-F16/papers/Pastor11.pdf},
year = {2011}
}
@article{Ghadirzadeh,
abstract = {— Skilled robot task learning is best implemented by predictive action policies due to the inherent latency of sensorimotor processes. However, training such predictive poli-cies is challenging as it involves finding a trajectory of motor activations for the full duration of the action. We propose a data-efficient deep predictive policy training (DPPT) framework with a deep neural network policy architecture which maps an image observation to a sequence of motor activations. The architecture consists of three sub-networks referred to as the perception, policy and behavior super-layers. The perception and behavior super-layers force an abstraction of visual and motor data trained with synthetic and simulated training samples, respectively. The policy super-layer is a small sub-network with fewer parameters that maps data in-between the abstracted manifolds. It is trained for each task using methods for policy search reinforcement learning. We demonstrate the suitability of the proposed architecture and learning framework by training predictive policies for skilled object grasping and ball throwing on a PR2 robot. The effectiveness of the method is illustrated by the fact that these tasks are trained using only about 180 real robot attempts with qualitative terminal rewards.},
author = {Ghadirzadeh, Ali and Maki, Atsuto and Kragic, Danica and Bj{\"{o}}rkman, M{\aa}rten},
file = {::},
title = {{Deep Predictive Policy Training using Reinforcement Learning}},
url = {https://arxiv.org/pdf/1703.00727.pdf}
}
@article{Plappert,
abstract = {Deep reinforcement learning (RL) methods generally engage in exploratory be-havior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require signif-icantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off-and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually.},
author = {Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and Sidor, Szymon and Chen, Richard Y and Chen, Xi and Asfour, Tamim and Abbeel, Pieter and Openai, Marcin Andrychowicz},
file = {::},
title = {{Parameter Space Noise for Exploration}},
url = {https://arxiv.org/pdf/1706.01905.pdf}
}
@article{Rajeswaran,
abstract = {Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are repre-sented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation.},
author = {Rajeswaran, Aravind and Ghotra, Sarvjeet and Ravindran, Balaraman and Levine, Sergey},
file = {::},
title = {{EPOPT: LEARNING ROBUST NEURAL NETWORK POLICIES USING MODEL ENSEMBLES}},
url = {https://arxiv.org/pdf/1610.01283.pdf}
}
@inproceedings{Bansal,
abstract = {— Real-world robots are becoming increasingly com-plex and commonly act in poorly understood environments where it is extremely challenging to model or learn their true dynamics. Therefore, it might be desirable to take a task-specific approach, wherein the focus is on explicitly learning the dynamics model which achieves the best control performance for the task at hand, rather than learning the true dynamics. In this work, we use Bayesian optimization in an active learning framework where a locally linear dynamics model is learned with the intent of maximizing the control performance, and used in conjunction with optimal control schemes to efficiently design a controller for a given task. This model is updated directly based on the performance observed in experiments on the physical system in an iterative manner until a desired performance is achieved. We demonstrate the efficacy of the proposed approach through simulations and real experiments on a quadrotor testbed.},
author = {Bansal, Somil and Calandra, Roberto and Xiao, Ted and Levine, Sergey and Tomlin, Claire J},
booktitle = {IEEE Conference on Decision and Control (CDC)},
file = {::},
title = {{Goal-Driven Dynamics Learning via Bayesian Optimization}},
url = {https://arxiv.org/pdf/1703.09260.pdf},
year = {2017}
}
@article{Calandra2017,
author = {Calandra, Roberto},
file = {::},
title = {{Goal-Driven Dynamics Learning for Model-Based RL Motivation}},
url = {http://www.robertocalandra.com/wp-content/uploads/2017-04-20-dali.pdf},
year = {2017}
}
@inproceedings{Tamar2016,
abstract = {We introduce the value iteration network: a fully differentiable neural network with a `planning module' embedded within. Value iteration networks are suitable for making predictions about outcomes that involve planning-based reasoning, such as predicting a desired trajectory from an observation of a map. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate our value iteration networks on the task of predicting optimal obstacle-avoiding trajectories from an image of a landscape, both on synthetic data, and on challenging raw images of the Mars terrain.},
archivePrefix = {arXiv},
arxivId = {1602.02867},
author = {Tamar, Aviv and Levine, Sergey and Abbeel, Pieter},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {1602.02867},
issn = {10495258},
pmid = {172808},
title = {{Value Iteration Networks}},
year = {2016}
}
@inproceedings{houthooft2016vime,
abstract = {Scalable and effective exploration remains a key challenge in reinforcement learn-ing (RL). While there are methods with optimality guarantees in the setting of dis-crete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an explo-ration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using varia-tional inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
archivePrefix = {arXiv},
arxivId = {1605.09674},
author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and {De Turck}, Filip and Abbeel, Pieter},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {1605.09674},
issn = {10495258},
title = {{Variational Information Maximizing Exploration}},
year = {2016}
}
@article{Reed,
abstract = {We propose the neural programmer-interpreter (NPI): a recurrent and composi-tional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value pro-gram memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-to-sequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of com-putation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these pro-grams and all 21 associated subprograms.},
author = {Reed, Scott and {De Freitas}, Nando},
file = {::},
title = {{Neural Programmer-Interpreters}},
url = {https://arxiv.org/pdf/1511.06279v3.pdf}
}
@inproceedings{nagaband2018mbmf,
abstract = {Model-free deep reinforcement learning methods have successfully learned com-plex behavioral strategies for a wide range of tasks, but typically require many samples to achieve good performance. Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks. In this work, we demonstrate that medium-sized neural network models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits to accomplish various complex locomotion tasks. We also propose using deep neural network dynamics models to initialize a model-free learner, in order to com-bine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods. We perform this pre-initialization by using rollouts from the trained model-based controller as supervision to pre-train a policy, and then fine-tune the policy using a model-free method. We empirically demon-strate that this resulting hybrid algorithm can drastically accelerate model-free learning and outperform purely model-free learners on several MuJoCo locomo-tion benchmark tasks, achieving sample efficiency gains over a purely model-free learner of 330× on swimmer, 26× on hopper, 4× on half-cheetah, and 3× on ant. Videos can be found at https://sites.google.com/view/mbmf.},
author = {Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S and Levine, Sergey},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {::},
title = {{Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning}},
url = {https://arxiv.org/pdf/1708.02596.pdf},
year = {2018}
}
@article{Devin,
abstract = {Robotic manipulation in complex open-world scenarios requires both reliable physical manipulation skills and effective and generalizable perception. In this paper, we propose a method where general purpose pretrained visual mod-els serve as an object-centric prior for the perception system of a learned policy. We devise an object-level attentional mechanism that can be used to determine rel-evant objects from a few demonstrations, and then immediately incorporate those objects into a learned policy. A task-independent meta-attention locates possible objects in the scene, and a task-specific attention identifies which objects are pre-dictive of the demonstrations. The scope of the task-specific attention is easily adjusted by showing demonstrations with distractor objects or with diverse rele-vant objects. Our results indicate that this approach exhibits good generalization across object instances using very few samples, and can be used to learn a variety of manipulation tasks using reinforcement learning.},
author = {Devin, Coline and Abbeel, Pieter and Darrell, Trevor and Levine, Sergey},
file = {::},
keywords = {manipulation,reinforcement learning,vision},
title = {{Deep Object-Centric Representations for Generalizable Robot Learning}},
url = {https://arxiv.org/pdf/1708.04225.pdf}
}
@article{Rahmatizadeh,
abstract = {— Robots assisting disabled or elderly people in activities of daily living must perform complex manipulation tasks. These tasks are dependent on the user's environment and preferences. Thus, learning from demonstration (LfD) is a promising choice that would allow the non-expert user to teach the robot different tasks. Unfortunately, learning general solu-tions from raw demonstrations requires a significant amount of data. Performing this number of physical demonstrations is unfeasible for a disabled user. In this paper we propose an approach where the user demon-strates the manipulation task in a virtual environment. The collected demonstrations are used to train an LSTM recurrent neural network that can act as the controller for the robot. We show that the controller learned from virtual demonstrations can be used to successfully perform the manipulation tasks on a physical robot.},
author = {Rahmatizadeh, Rouhollah and Abolghasemi, Pooya and Behal, Aman and B{\"{o}}l{\"{o}}ni, Ladislau},
file = {::},
title = {{Learning real manipulation tasks from virtual demonstrations using LSTM}},
url = {https://arxiv.org/pdf/1603.03833.pdf}
}
@article{Wulfmeier,
abstract = {Training robots for operation in the real world is a complex, time con-suming and potentially expensive task. Despite significant success of reinforce-ment learning in games and simulations, research in real robot applications has not been able to match similar progress. While sample complexity can be reduced by training policies in simulation, these can perform sub-optimally on the real platform given imperfect calibration of model dynamics. We present an approach -supplemental to fine tuning on the real robot -to further benefit from parallel access to a simulator during training. The developed approach harnesses auxiliary rewards to guide the exploration for the real world agent based on the proficiency of the agent in simulation and vice versa. In this context, we demonstrate em-pirically that the reciprocal alignment for both agents provides further benefit as the agent in simulation can adjust to optimize its behaviour for states commonly visited by the real-world agent.},
author = {Wulfmeier, Markus and Posner, Ingmar and Abbeel, Pieter},
file = {::},
keywords = {Adversarial Learning,Robotics,Simulation,Transfer Learning},
title = {{Mutual Alignment Transfer Learning}},
url = {https://arxiv.org/pdf/1707.07907.pdf}
}
@article{Singh,
abstract = {We tackle the problem of learning robotic sensorimotor control policies that can generalize to visually diverse and unseen environments. Achieving broad generalization typi-cally requires large datasets, which are difficult to obtain for task-specific interactive processes such as reinforce-ment learning or learning from demonstration. However, much of the visual diversity in the world can be captured through passively collected datasets of images or videos. In our method, which we refer to as GPLAC (Generalized Policy Learning with Attentional Classifier), we use both interaction data and weakly labeled image data to augment the generalization capacity of sensorimotor policies. Our method combines multitask learning on action selection and an auxiliary binary classification objective, together with a convolutional neural network architecture that uses an attentional mechanism to avoid distractors. We show that pairing interaction data from just a single environment with a diverse dataset of weakly labeled data results in greatly improved generalization to unseen environments, and show that this generalization depends on both the auxiliary ob-jective and the attentional architecture that we propose. We demonstrate our results in both simulation and on a real robotic manipulator, and demonstrate substantial improve-ment over standard convolutional architectures and domain adaptation methods.},
author = {Singh, Avi and Yang, Larry and Levine, Sergey},
file = {::},
title = {{GPLAC: Generalizing Vision-Based Robotic Skills using Weakly Labeled Images}},
url = {https://arxiv.org/pdf/1708.02313.pdf}
}
@inproceedings{murali2018cassl,
abstract = {Recent self-supervised learning approaches focus on using a few thou-sand data points to learn policies for high-level, low-dimensional action spaces. However, scaling this framework for high-dimensional control require either scal-ing up the data collection efforts or using a clever sampling strategy for training. We present a novel approach -Curriculum Accelerated Self-Supervised Learn-ing (CASSL) -to train policies that map visual information to high-level, higher-dimensional action spaces. CASSL orders the sampling of training data based on control dimensions: the learning and sampling are focused on few control pa-rameters before other parameters. The right curriculum for learning is suggested by variance-based global sensitivity analysis of the control space. We apply our CASSL framework to learning how to grasp using an adaptive, underactuated multi-fingered gripper, a challenging system to control. Our experimental results indicate that CASSL provides significant improvement and generalization com-pared to baseline methods such as staged curriculum learning (8{\%} increase) and complete end-to-end learning with random exploration (14{\%} improvement) tested on a set of novel objects. Supplementary video: youtube.com/iCQsM7EE4HI.},
author = {Murali, Adithyavairavan and Pinto, Lerrel and Gandhi, Dhiraj and Gupta, Abhinav},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {::},
title = {{CASSL: Curriculum Accelerated Self-Supervised Learning}},
url = {https://arxiv.org/pdf/1708.01354.pdf},
year = {2018}
}
@inproceedings{vaswani2017transformers,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {::},
title = {{Attention Is All You Need}},
url = {https://arxiv.org/pdf/1706.03762.pdf},
year = {2017}
}
@article{Oh,
abstract = {This paper proposes a novel deep reinforcement learning (RL) architecture, called Value Prediction Network (VPN), which integrates model-free and model-based RL methods into a single neural network. In contrast to typical model-based RL methods, VPN learns a dynamics model whose abstract states are trained to make option-conditional predictions of future values (discounted sum of re-wards) rather than of future observations. Our experimental results show that VPN has several advantages over both model-free and model-based baselines in a stochastic environment where careful planning is required but building an accurate observation-prediction model is difficult. Furthermore, VPN outperforms Deep Q-Network (DQN) on several Atari games even with short-lookahead planning, demonstrating its potential as a new way of learning a good state representation.},
author = {Oh, Junhyuk and Singh, Satinder and Lee, Honglak},
file = {::},
title = {{Value Prediction Network}},
url = {https://arxiv.org/pdf/1707.03497.pdf}
}
@article{schulman2017ppo,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which al-ternate between sampling data through interaction with the environment, and optimizing a " surrogate " objective function using stochastic gradient ascent. Whereas standard policy gra-dient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimiza-tion (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, includ-ing simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Openai, Oleg Klimov},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Schulman et al. - Unknown - Proximal Policy Optimization Algorithms(2).pdf:pdf},
title = {{Proximal Policy Optimization Algorithms}},
url = {https://arxiv.org/pdf/1707.06347.pdf},
year = {2017}
}
@inproceedings{bellemare2017distributional,
abstract = {In this paper we argue for the fundamental impor-tance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the com-mon approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of liter-ature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy eval-uation and control settings, exposing a signifi-cant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equa-tion to the learning of approximate value distri-butions. We evaluate our algorithm using the suite of games from the Arcade Learning En-vironment. We obtain both state-of-the-art re-sults and anecdotal evidence demonstrating the importance of the value distribution in approxi-mate reinforcement learning. Finally, we com-bine theoretical and empirical evidence to high-light the ways in which the value distribution im-pacts learning in the approximate setting.},
author = {Bellemare, Marc G and Dabney, Will and Munos, R{\'{e}}mi},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{A Distributional Perspective on Reinforcement Learning}},
url = {https://arxiv.org/pdf/1707.06887.pdf},
year = {2017}
}
@article{pearl1999reasoning,
author = {Pearl, Judea},
file = {::},
journal = {International Joint Conference on Artificial Intelligence (IJCAI)},
title = {{Reasoning with Cause and Effect}},
url = {http://bayes.cs.ucla.edu/IJCAI99/ijcai-99.pdf},
year = {1999}
}
@inproceedings{Rahmatizadeha,
abstract = {In this paper, we propose a multi-task learning from demonstration method that works using raw images as input to autonomously accomplish a wide variety of tasks in the real world using a low-cost robotic arm. The controller is a single recurrent neural network that can generate robot arm trajectories to perform different manipulation tasks. In order to learn complex skills from relatively few demonstrations, we share parameters across different tasks. Our network also combines VAE-GAN-based reconstruction with autoregressive multimodal action prediction for improved data efficiency. Our results show that weight sharing and reconstruction substantially improve generalization and robustness, and that training on multiple tasks simultaneously greatly improves the success rate on all of the tasks. Our experiments, performed on a real-world low-cost Lynxmotion arm, illustrate a variety of picking and placing tasks, as well as non-prehensile manipulation.},
author = {Rahmatizadeh, Rouhollah and Abolghasemi, Pooya and B{\"{o}}l{\"{o}}ni, Ladislau and Levine, Sergey},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {::},
keywords = {Affordable assistive robotics,Multi-task learning,Robot manipulation},
title = {{Vision-Based Multi-Task Manipulation for Inexpensive Robots Using End-To-End Learning from Demonstration}},
url = {https://arxiv.org/pdf/1707.02920.pdf},
year = {2018}
}
@article{Darvish,
abstract = {The Industry 4.0 paradigm emphasizes the crucial benefits that collabora-tive robots, i.e., robots able to work alongside and together with humans, could bring to the whole production process. In this context, an enabling technol-ogy yet unreached is the design of flexible robots able to deal at all levels with humans' intrinsic variability, which is not only a necessary element for a com-fortable working experience for the person, but also a precious capability for efficiently dealing with unexpected events. In this paper, a sensing, represen-tation, planning and control architecture for flexible human-robot cooperation, referred to as FlexHRC, is proposed. FlexHRC relies on wearable sensors for human action recognition, AND/OR graphs for the representation of and rea-soning upon cooperation models, and a Task Priority framework to decouple action planning from robot motion planning and control.},
author = {Darvish, Kourosh and Wanderlingh, Francesco and Bruno, Barbara and Simetti, Enrico and Mastrogiovanni, Fulvio and Casalino, Giuseppe},
file = {::},
keywords = {AND/OR graph,Human-Robot Cooperation,Smart Factory,Task Priority control,Wearable Sensing},
title = {{Flexible Human-Robot Cooperation Models for Assisted Shop-floor Tasks}},
url = {https://arxiv.org/pdf/1707.02591.pdf}
}
@article{Lopez-Paz,
abstract = {One major obstacle towards artificial intelligence is the poor ability of models to quickly solve new problems, without forgetting previously acquired knowledge. To better understand this issue, we study the problem of learning over a continuum of data, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model to learn over continuums of data, called Gradient of Episodic Memory (GEM), which alleviates forgetting while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.},
author = {Lopez-Paz, David and Ranzato, Marc Aurelio},
file = {::},
title = {{Gradient Episodic Memory for Continuum Learning}},
url = {https://arxiv.org/pdf/1706.08840.pdf}
}
@article{Justesen,
abstract = {—The real-time strategy game StarCraft has proven to be a challenging environment for artificial intelligence techniques, and as a result, current state-of-the-art solutions consist of numerous hand-crafted modules. In this paper, we show how macromanagement decisions in StarCraft can be learned directly from game replays using deep learning. Neural networks are trained on 789,571 state-action pairs extracted from 2,005 replays of highly skilled players, achieving top-1 and top-3 error rates of 54.6{\%} and 22.9{\%} in predicting the next build action. By integrating the trained network into UAlbertaBot, an open source StarCraft bot, the system can significantly outperform the game's built-in Terran bot, and play competitively against UAlbertaBot with a fixed rush strategy. To our knowledge, this is the first time macromanagement tasks are learned directly from replays in StarCraft. While the best hand-crafted strategies are still the state-of-the-art, the deep network approach is able to express a wide range of different strategies and thus improving the network's performance further with deep reinforcement learning is an immediately promising avenue for future research. Ultimately this approach could lead to strong StarCraft bots that are less reliant on hard-coded strategies.},
author = {Justesen, Niels and Risi, Sebastian},
file = {::},
title = {{Learning Macromanagement in StarCraft from Replays using Deep Learning}},
url = {https://njustesen.files.wordpress.com/2017/07/njustesen2017learning.pdf}
}
@article{Merel,
abstract = {Rapid progress in deep reinforcement learning has made it increasingly feasible to train controllers for high-dimensional humanoid bodies. However, methods that use pure reinforcement learning with simple reward functions tend to pro-duce non-humanlike and overly stereotyped movement behaviors. In this work, we extend generative adversarial imitation learning to enable training of generic neural network policies to produce humanlike movement patterns from limited demonstrations consisting only of partially observed state features, without access to actions, even when the demonstrations come from a body with different and unknown physical parameters. We leverage this approach to build sub-skill policies from motion capture data and show that they can be reused to solve tasks when controlled by a higher level controller. [video abstract]},
author = {Merel, Josh and Tassa, Yuval and Tb, Dhruva and Srinivasan, Sriram and Lemmon, Jay and Wang, Ziyu and Wayne, Greg and Heess, Nicolas},
file = {::},
title = {{Learning human behaviors from motion capture by adversarial imitation}},
url = {https://arxiv.org/pdf/1707.02201.pdf}
}
@inproceedings{donahue2017bigan,
abstract = {The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.},
archivePrefix = {arXiv},
arxivId = {1605.09782},
author = {Donahue, Jeff and Kr{\"{a}}henb{\"{u}}hl, Philipp and Darrell, Trevor},
booktitle = {International Conference on Learning Representations (ICLR)},
eprint = {1605.09782},
month = {may},
title = {{Adversarial Feature Learning}},
url = {http://arxiv.org/abs/1605.09782},
year = {2017}
}
@inproceedings{oord2016pixelcnn,
abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
archivePrefix = {arXiv},
arxivId = {1606.05328},
author = {van den Oord, Aaron and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1606.05328},
file = {::},
month = {jun},
pages = {4797--4805},
publisher = {Neural information processing systems foundation},
title = {{Conditional Image Generation with PixelCNN Decoders}},
url = {http://arxiv.org/abs/1606.05328},
year = {2016}
}
@article{Cabi,
abstract = {This paper introduces the Intentional Unintentional (IU) agent. This agent endows the deep deterministic policy gradients (DDPG) agent for contin-uous control with the ability to solve several tasks simultaneously. Learning to solve many tasks simultaneously has been a long-standing, core goal of artificial intelligence, inspired by infant development and motivated by the desire to build flexible robot manipulators capable of many diverse behaviours. We show that the IU agent not only learns to solve many tasks simultaneously but it also learns faster than agents that target a single task at-a-time. In some cases, where the single task DDPG method completely fails, the IU agent successfully solves the task. To demonstrate this, we build a playroom environment using the MuJoCo physics engine, and introduce a grounded formal language to automatically generate tasks.},
author = {Cabi, Serkan and {G{\'{o}}mez Colmenarejo}, Sergio and Hoffman, Matthew W and Denil, Misha and Wang, Ziyu and {De Freitas}, Nando},
file = {::},
keywords = {Deep deterministic policy gradients,control,multi-task,physics},
title = {{The Intentional Unintentional Agent: Learning to Solve Many Continuous Control Tasks Simultaneously}},
url = {https://arxiv.org/pdf/1707.03300.pdf}
}
@inproceedings{finn2017maml,
abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is com-patible with any model trained with gradient de-scent and applicable to a variety of different learning problems, including classification, re-gression, and reinforcement learning. The goal of meta-learning is to train a model on a vari-ety of learning tasks, such that it can solve new learning tasks using only a small number of train-ing samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and acceler-ates fine-tuning for policy gradient reinforcement learning with neural network policies.},
author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}},
url = {https://arxiv.org/pdf/1703.03400.pdf},
year = {2017}
}
@article{Lake,
abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving perfor-mance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recog-nition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
author = {Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
file = {::},
title = {{Building Machines That Learn and Think Like People}},
url = {https://arxiv.org/pdf/1604.00289.pdf}
}
@article{Selsam,
abstract = {Noisy data, non-convex objectives, model mis-specification, and numerical instability can all cause undesired behaviors in machine learning systems. As a result, detecting actual imple-mentation errors can be extremely difficult. We demonstrate a methodology in which developers use an interactive proof assistant to both imple-ment their system and to state a formal theorem defining what it means for their system to be cor-rect. The process of proving this theorem inter-actively in the proof assistant exposes all imple-mentation errors since any error in the program would cause the proof to fail. As a case study, we implement a new system, Certigrad, for opti-mizing over stochastic computation graphs, and we generate a formal (i.e. machine-checkable) proof that the gradients sampled by the system are unbiased estimates of the true mathematical gradients. We train a variational autoencoder us-ing Certigrad and find the performance compara-ble to training the same model in TensorFlow.},
author = {Selsam, Daniel and Liang, Percy and Dill, David L},
file = {::},
title = {{Developing Bug-Free Machine Learning Systems With Formal Mathematics}},
url = {https://arxiv.org/pdf/1706.08605.pdf}
}
@inproceedings{schaul2016prioritized,
abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David and Deepmind, Google},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {::},
title = {{Prioritized Experience Replay}},
url = {https://arxiv.org/pdf/1511.05952.pdf},
year = {2016}
}
@article{Parisotto,
abstract = {The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. To-wards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultane-ously, and then generalize its knowledge to new domains. This method, termed " Actor-Mimic " , exploits the use of deep reinforcement learning and model com-pression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of general-izing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.},
author = {Parisotto, Emilio and Ba, Jimmy and Salakhutdinov, Ruslan},
file = {::},
title = {{ACTOR-MIMIC DEEP MULTITASK AND TRANSFER REINFORCEMENT LEARNING}},
url = {https://arxiv.org/pdf/1511.06342.pdf}
}
@article{Morales,
abstract = {Fig. 1: Cylindrical robot guiding a box to (purple) goal region. Abstract—We address the motion planning problem for a single robot that manipulates passive objects in an otherwise static environment. We handle uncertainties in passive object responses while abstracting their dynamics by creating a roadmap that encodes the possible passive object responses. The robot extracts reference paths from the passive roadmap to plan approaching motions with an active roadmap. When the robot acts on the object, it receives response feedback and reacts accordingly. We apply this approach to pushing, a basic manipulation primitive. We propose four different pushing strategies, one of them using reinforcement learning. Our simulations show the effectiveness of the approach and of each pushing strategy.},
author = {Morales, Marco and Rodriguez, Samuel and Thomas, Shawna and Amato, Nancy M},
file = {::},
title = {{Sampling-Based Planning and Local Reactive Strategies for Environment Manipulation}},
url = {http://rss2017ws.is.tuebingen.mpg.de/abstracts/push-planning-rss17-ws{\_}contact-camera-ready.pdf}
}
@article{Hosseini,
author = {Hosseini, Khaled},
file = {::},
title = {{A THOUSAND SPLENDID SUNS}},
url = {http://english4success.ru/Upload/books/1806.pdf}
}
@book{handmaid,
file = {::},
title = {{The Handmaid's Tale}}
}
@article{VidiadharSurajprasadNaipaul,
abstract = {On the house in Sikkim Street Mr. Biswas owed, and had been owing for four years, three thousand dollars. The interest on this, at eight per cent, came to twenty dollars a month; the ground rent was ten dollars. Two children were at school. The two older children, on whom Mr. Biswas might have depended, were both abroad on scholarships. It gave Mr. Biswas some satisfaction that in the circumstances Shama did not run straight off to her mother to beg for help. Ten years before that would have been her first thought. Now she tried to comfort Mr. Biswas, and devised plans on her own. " Potatoes, " she said. " We can start selling potatoes. The price around here is eight cents a pound. If we buy at five and sell at seven— " " Trust the Tulsi bad blood, " Mr. Biswas said. " I know that the pack of you Tulsis are financial geniuses. But have a good look around and count the number of people selling potatoes. Better to sell the old car. " " No. Not the car. Don't worry. We'll manage. " " Yes, " Mr. Biswas said irritably. " We'll manage. " No more was heard of the potatoes, and Mr. Biswas never threatened again to sell the car. He didn't now care to do anything against his wife's wishes. He had grown to accept her judgement and to respect her optimism. He trusted her. Since they had moved to the house Shama had learned a new loyalty, to him and to their children; away from her mother and sisters, she was able to express this without shame, and to Mr. Biswas this was a triumph almost as big as the acquiring of his own house. He thought of the house as his own, though for years it had been irretrievably mortgaged. And during these months of illness and despair he was struck again and again by the wonder of being in his own house, the audacity of it: to walk in through his own front gate, to bar entry to whoever he wished, to close his doors and windows every night, to hear no noises except those of his family, to wander freely from room to room and about his yard, instead of being condemned, as before, to retire the moment he got home to the crowded room in one or the other of Mrs. Tulsi's houses, crowded with Shama's sisters, their husbands, their children. As a boy he had moved from one house of strangers to another; and since his marriage he felt he had lived nowhere but in the houses of the Tulsis, at Hanuman House in Arwacas, in the decaying wooden house at Shorthills, in the clumsy concrete house in Port of Spain. And now at the end he found himself in his own house, on his own half-lot of land, his own portion of the earth. That he should have been responsible for this seemed to him, in these last months, stupendous. The house could be seen from two or three streets away and was known all over St. James. It was like a huge and squat sentry-box: tall, square, two-storeyed, with a pyramidal roof of corrugated iron. It had been designed and built by a solicitor's clerk who built houses in his spare time. The solicitor's clerk had many contacts. He bought land which the City Council had announced was not for sale; he persuaded estate owners to split whole lots into half-lots; he bought lots of barely reclaimed swamp land near Mucurapo and got permission to build on them. On whole lots or three-quarter-lots he built one-storey houses, twenty feet by twenty-six, which could pass unnoticed; on half-lots he built two-storey houses, twenty feet by thirteen, which were distinctive. All},
author = {{Vidiadhar Surajprasad Naipaul}, By and {Riaz Islamabad -Pakistan}, Shahid},
file = {::},
title = {{A House for Mr. Biswas}},
url = {http://rgi.edu.in/rgi{\_}pdf/A{\_}House{\_}for{\_}Mr{\_}Biswas{\_}By{\_}Vidiadhar{\_}Surajprasad{\_}Naipaul.pdf}
}
@article{GabrielGarciaMarquez,
author = {{Gabriel Garcia Marquez}, By and {Riaz Islamabad -Pakistan}, Shahid},
file = {::},
title = {{One Hundred Years of Solitude}},
url = {http://202.74.245.22:8080/xmlui/bitstream/handle/123456789/164/Marquez.pdf?sequence=1}
}
@inproceedings{zhang2017tensegrity,
abstract = {— Tensegrity robots, composed of rigid rods con-nected by elastic cables, have a number of unique properties that make them appealing for use as planetary exploration rovers. However, control of tensegrity robots remains a difficult problem due to their unusual structures and complex dynamics. In this work, we show how locomotion gaits can be learned automatically using a novel extension of mirror descent guided policy search (MDGPS) applied to periodic locomotion move-ments, and we demonstrate the effectiveness of our approach on tensegrity robot locomotion. We evaluate our method with real-world and simulated experiments on the SUPERball tensegrity robot, showing that the learned policies generalize to changes in system parameters, unreliable sensor measurements, and variation in environmental conditions, including varied terrains and a range of different gravities. Our experiments demonstrate that our method not only learns fast, power-efficient feedback policies for rolling gaits, but that these policies can succeed with only the limited onboard sensing provided by SUPERball's accelerometers. We compare the learned feedback policies to learned open-loop policies and hand-engineered controllers, and demonstrate that the learned policy enables the first continuous, reliable locomotion gait for the real SUPERball robot. Our code and other supplementary materials are available from http://rll.berkeley.edu/drl{\_}tensegrity},
author = {Zhang, Marvin and Geng, Xinyang and Bruce, Jonathan and Caluwaerts, Ken and Vespignani, Massimo and Sunspiral, Vytas and Abbeel, Pieter and Levine, Sergey},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {::},
title = {{Deep Reinforcement Learning for Tensegrity Robot Locomotion}},
url = {https://arxiv.org/pdf/1609.09049.pdf},
year = {2017}
}
@inproceedings{bacon2017optioncritic,
abstract = {Temporal abstraction is key to scaling up learning and plan-ning in reinforcement learning. While planning with tempo-rally extended actions is well understood, creating such ab-stractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup {\&} Singh, 1999; Precup, 2000]. We derive policy gra-dient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the pol-icy over options, and without the need to provide any addi-tional rewards or subgoals. Experimental results in both dis-crete and continuous environments showcase the flexibility and efficiency of the framework.},
author = {Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
booktitle = {AAAI Conference on Artificial Intelligence},
file = {::},
title = {{The Option-Critic Architecture}},
url = {https://arxiv.org/pdf/1609.05140.pdf},
year = {2017}
}
@article{Szita,
abstract = {The exploration-exploitation dilemma has been an intriguing and unsolved problem within the framework of reinforcement learn-ing. " Optimism in the face of uncertainty " and model building play central roles in ad-vanced exploration methods. Here, we inte-grate several concepts and obtain a fast and simple algorithm. We show that the proposed algorithm finds a near-optimal policy in poly-nomial time, and give experimental evidence that it is robust and efficient compared to its ascendants.},
author = {Szita, Istv{\'{a}}n},
file = {::},
title = {{The Many Faces of Optimism: a Unifying Approach}},
url = {http://icml2008.cs.helsinki.fi/papers/490.pdf}
}
@article{Singha,
abstract = {Reinforcement learning has achieved broad and successful ap-plication in cognitive science in part because of its general for-mulation of the adaptive control problem as the maximization of a scalar reward function. The computational reinforcement learning framework is motivated by correspondences to ani-mal reward processes, but it leaves the source and nature of the rewards unspecified. This paper advances a general computa-tional framework for reward that places it in an evolutionary context, formulating a notion of an optimal reward function given a fitness function and some distribution of environments. Novel results from computational experiments show how tra-ditional notions of extrinsically and intrinsically motivated be-haviors may emerge from such optimal reward functions. In the experiments these rewards are discovered through auto-mated search rather than crafted by hand. The precise form of the optimal reward functions need not bear a direct relationship to the fitness function, but may nonetheless confer significant advantages over rewards based only on fitness.},
author = {Singh, Satinder and Lewis, Richard L and Barto, Andrew G},
file = {::},
title = {{Where Do Rewards Come From?}},
url = {https://web.eecs.umich.edu/{~}baveja/Papers/singh-lewis-barto-2009-cogsci.pdf}
}
@article{Kendall,
abstract = {Deep learning has shown to be effective for robust and real-time monocular image relocalisation. In particular, PoseNet [22] is a deep convolutional neural network which learns to regress the 6-DOF camera pose from a single im-age. It learns to localize using high level features and is robust to difficult lighting, motion blur and unknown cam-era intrinsics, where point based SIFT registration fails. However, it was trained using a naive loss function, with hyper-parameters which require expensive tuning. In this paper, we give the problem a more fundamental theoreti-cal treatment. We explore a number of novel loss functions for learning camera pose which are based on geometry and scene reprojection error. Additionally we show how to au-tomatically learn an optimal weighting to simultaneously regress position and orientation. By leveraging geometry, we demonstrate that our technique significantly improves PoseNet's performance across datasets ranging from indoor rooms to a small city.},
author = {Kendall, Alex and Cipolla, Roberto},
file = {::},
title = {{Geometric loss functions for camera pose regression with deep learning}},
url = {https://arxiv.org/pdf/1704.00390.pdf}
}
@article{Zenke,
abstract = {While deep learning has led to remarkable ad-vances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging com-plex molecular machinery to solve many tasks simultaneously. In this study, we introduce in-telligent synapses that bring some of this bio-logical complexity into artificial neural networks. Each synapse accumulates task relevant informa-tion over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintain-ing computational efficiency.},
author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
file = {::},
title = {{Continual Learning Through Synaptic Intelligence}},
url = {https://arxiv.org/pdf/1703.04200.pdf}
}
@article{Amodei,
abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing atten-tion to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (" avoiding side effects " and " avoiding reward hacking "), an objective function that is too expensive to evaluate frequently (" scalable supervision "), or undesirable behavior during the learning process (" safe exploration " and " distributional shift "). We review previous work in these areas as well as suggesting re-search directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Openai, John Schulman and Man{\'{e}}, Dan and Brain, Google},
file = {::},
title = {{Concrete Problems in AI Safety}},
url = {https://arxiv.org/pdf/1606.06565.pdf}
}
@article{Kirkpatrick,
abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
file = {::},
title = {{Overcoming catastrophic forgetting in neural networks}},
url = {https://arxiv.org/pdf/1612.00796.pdf}
}
@article{Goodfellow,
abstract = {Catastrophic forgetting is a problem faced by many machine learning models and al-gorithms. When trained on one task, then trained on a second task, many machine learning models " forget " how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catas-trophic forgetting problem occurs for mod-ern neural networks, comparing both estab-lished and recent gradient-based training al-gorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catas-trophic forgetting. We find that it is always best to train using the dropout algorithm– the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve be-tween these two extremes. We find that dif-ferent tasks and relationships between tasks result in very different rankings of activation function performance. This suggests that the choice of activation function should always be cross-validated.},
author = {Goodfellow, Ian J and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
file = {::},
title = {{An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks}},
url = {https://arxiv.org/pdf/1312.6211.pdf}
}
@inproceedings{ruvolo2013ella,
abstract = {The problem of learning multiple consecu-tive tasks, known as lifelong learning, is of great importance to the creation of intelli-gent, general-purpose, and flexible machines. In this paper, we develop a method for on-line multi-task learning in the lifelong learn-ing setting. The proposed Efficient Life-long Learning Algorithm (ELLA) maintains a sparsely shared basis for all task models, transfers knowledge from the basis to learn each new task, and refines the basis over time to maximize performance across all tasks. We show that ELLA has strong connections to both online dictionary learning for sparse coding and state-of-the-art batch multi-task learning methods, and provide robust the-oretical performance guarantees. We show empirically that ELLA yields nearly identi-cal performance to batch multi-task learning while learning tasks sequentially in three or-ders of magnitude (over 1,000x) less time.},
author = {Ruvolo, Paul and Eaton, Eric},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{ELLA: An Efficient Lifelong Learning Algorithm}},
url = {http://proceedings.mlr.press/v28/ruvolo13.pdf},
year = {2013}
}
@inproceedings{williams2017mpc,
abstract = {— We introduce an information theoretic model pre-dictive control (MPC) algorithm capable of handling complex cost criteria and general nonlinear dynamics. The generality of the approach makes it possible to use multi-layer neural networks as dynamics models, which we incorporate into our MPC algorithm in order to solve model-based reinforcement learning tasks. We test the algorithm in simulation on a cart-pole swing up and quadrotor navigation task, as well as on actual hardware in an aggressive driving task. Empirical results demonstrate that the algorithm is capable of achieving a high level of performance and does so only utilizing data collected from the system.},
author = {Williams, Grady and Wagener, Nolan and Goldfain, Brian and Drews, Paul and Rehg, James M and Boots, Byron and Theodorou, Evangelos A},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {::},
title = {{Information Theoretic MPC for Model-Based Reinforcement Learning}},
url = {http://www.cc.gatech.edu/{~}bboots3/files/InformationTheoreticMPC.pdf},
year = {2017}
}
@article{Montgomery,
abstract = {Guided policy search algorithms can be used to optimize complex nonlinear poli-cies, such as deep neural networks, without directly computing policy gradients in the high-dimensional parameter space. Instead, these methods use supervised learning to train the policy to mimic a " teacher " algorithm, such as a trajectory optimizer or a trajectory-centric reinforcement learning method. Guided policy search methods provide asymptotic local convergence guarantees by construction, but it is not clear how much the policy improves within a small, finite number of iterations. We show that guided policy search algorithms can be interpreted as an approximate variant of mirror descent, where the projection onto the constraint manifold is not exact. We derive a new guided policy search algorithm that is sim-pler and provides appealing improvement and convergence guarantees in simplified convex and linear settings, and show that in the more general nonlinear setting, the error in the projection step can be bounded. We provide empirical results on several simulated robotic navigation and manipulation tasks that show that our method is stable and achieves similar or better performance when compared to prior guided policy search methods, with a simpler formulation and fewer hyperparameters.},
author = {Montgomery, William and Levine, Sergey},
file = {::},
title = {{Guided Policy Search as Approximate Mirror Descent}},
url = {https://arxiv.org/pdf/1607.04614.pdf}
}
@inproceedings{chebotar2017pilqr,
abstract = {Reinforcement learning (RL) algorithms for real-world robotic applications need a data-efficient learning process and the ability to handle com-plex, unknown dynamical systems. These re-quirements are handled well by model-based and model-free RL approaches, respectively. In this work, we aim to combine the advantages of these two types of methods in a principled manner. By focusing on time-varying linear-Gaussian poli-cies, we enable a model-based algorithm based on the linear quadratic regulator (LQR) that can be integrated into the model-free framework of path integral policy improvement (PI 2). We can further combine our method with guided pol-icy search (GPS) to train arbitrary parameterized policies such as deep neural networks. Our sim-ulation and real-world experiments demonstrate that this method can solve challenging manipula-tion tasks with comparable or better performance than model-free methods while maintaining the sample efficiency of model-based methods.},
author = {Chebotar, Yevgen and Hausman, Karol and Zhang, Marvin and Sukhatme, Gaurav and Schaal, Stefan and Levine, Sergey},
booktitle = {International Conference on Machine Learning (ICML)},
file = {::},
title = {{Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning}},
url = {https://arxiv.org/pdf/1703.03078.pdf},
year = {2017}
}
@article{Klambauer,
abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are " scaled exponential linear units " (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance — even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization schemes, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs, and other machine learning methods such as random forests and support vector machines. For FNNs we considered (i) ReLU networks without normalization, (ii) batch normalization, (iii) layer normalization, (iv) weight normalization, (v) highway networks, and (vi) residual networks. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
author = {Klambauer, G{\"{u}}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
file = {::},
title = {{Self-Normalizing Neural Networks}},
url = {https://arxiv.org/pdf/1706.02515.pdf}
}
@article{Watters,
abstract = {From just a glance, humans can make rich predictions about the future state of a wide range of physical systems. On the other hand, modern approaches from engineering, robotics, and graphics are often restricted to narrow domains and require direct measurements of the underlying states. We introduce the Visual Interaction Network, a general-purpose model for learning the dynamics of a physical system from raw visual observations. Our model consists of a perceptual front-end based on convolutional neural networks and a dynamics predictor based on interaction networks. Through joint training, the perceptual front-end learns to parse a dynamic visual scene into a set of factored latent object representations. The dynamics predictor learns to roll these states forward in time by computing their interactions and dynamics, producing a predicted physical trajectory of arbitrary length. We found that from just six input video frames the Visual Interaction Network can generate accurate future trajectories of hundreds of time steps on a wide range of physical systems. Our model can also be applied to scenes with invisible objects, inferring their future states from their effects on the visible objects, and can implicitly infer the unknown mass of objects. Our results demonstrate that the perceptual module and the object-based dynamics predictor module can induce factored latent representations that support accurate dynamical predictions. This work opens new opportunities for model-based decision-making and planning from raw sensory observations in complex physical environments.},
author = {Watters, Nicholas and Tacchetti, Andrea and Weber, Th{\'{e}}ophane and Pascanu, Razvan and Battaglia, Peter and Zoran, Daniel},
file = {::},
title = {{Visual Interaction Networks}},
url = {https://arxiv.org/pdf/1706.01433.pdf}
}
@article{Lowe,
abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Openai, Igor Mordatch},
file = {::},
title = {{Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments}},
url = {https://arxiv.org/pdf/1706.02275.pdf}
}
@book{Bender2013,
author = {Bender, Emily},
file = {::},
title = {{Linguistic Fundamentals for Natural Language Processing}},
year = {2013}
}
@article{Ehrhardt,
abstract = {While the basic laws of Newtonian mechanics are well understood, explaining a physical scenario still requires manually modeling the problem with suitable equations and associated parameters. In order to adopt such models for artificial intelligence, researchers have handcrafted the relevant states, and then used neural networks to learn the state transitions using simulation runs as training data. Un-fortunately, such approaches can be unsuitable for modeling complex real-world scenarios, where manually authoring relevant state spaces tend to be challenging. In this work, we investigate if neural networks can implicitly learn physical states of real-world mechanical processes only based on visual data, and thus enable long-term physical extrapolation. We develop a recurrent neural network architecture for this task and also characterize resultant uncertainties in the form of evolving variance estimates. We evaluate our setup to extrapolate motion of a rolling ball on bowl of varying shape and orientation using only images as input, and report competitive results with approaches that assume access to internal physics models and parameters.},
author = {Ehrhardt, S{\'{e}}bastien and Monszpart, Aron and Vedaldi, Andrea and Mitra, Niloy},
file = {::},
title = {{Learning to Represent Mechanics via Long-term Extrapolation and Interpolation}},
url = {https://arxiv.org/pdf/1706.02179.pdf}
}
@inproceedings{Battaglia,
abstract = {Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object-and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.},
author = {Battaglia, Peter W and Pascanu, Razvan and Lai, Matthew and Rezende, Danilo and Kavukcuoglu, Koray},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {::},
title = {{Interaction Networks for Learning about Objects, Relations and Physics}},
url = {https://arxiv.org/pdf/1612.00222.pdf},
year = {2016}
}
@article{Theodorou2010,
abstract = {With the goal to generate more scalable algorithms with higher efficiency and fewer open parame-ters, reinforcement learning (RL) has recently moved towards combining classical techniques from optimal control and dynamic programming with modern learning techniques from statistical esti-mation theory. In this vein, this paper suggests to use the framework of stochastic optimal control with path integrals to derive a novel approach to RL with parameterized policies. While solidly grounded in value function estimation and optimal control based on the stochastic Hamilton-Jacobi-Bellman (HJB) equations, policy improvements can be transformed into an approximation problem of a path integral which has no open algorithmic parameters other than the exploration noise. The resulting algorithm can be conceived of as model-based, semi-model-based, or even model free, depending on how the learning problem is structured. The update equations have no danger of numerical instabilities as neither matrix inversions nor gradient learning rates are required. Our new algorithm demonstrates interesting similarities with previous RL research in the framework of probability matching and provides intuition why the slightly heuristically motivated probability matching approach can actually perform well. Empirical evaluations demonstrate significant per-formance improvements over gradient-based policy learning and scalability to high-dimensional control problems. Finally, a learning experiment on a simulated 12 degree-of-freedom robot dog illustrates the functionality of our algorithm in a complex robot learning scenario. We believe that Policy Improvement with Path Integrals (PI 2) offers currently one of the most efficient, numeri-cally robust, and easy to implement algorithms for RL based on trajectory roll-outs.},
author = {Theodorou, Evangelos A and Buchli, Jonas and Org, Jonas@buchli and Schaal, Stefan and Edu, Sschaal@usc},
file = {::},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {parameterized policies,reinforcement learning,stochastic optimal control},
pages = {3137--3181},
title = {{A Generalized Path Integral Control Approach to Reinforcement Learning}},
url = {http://www.jmlr.org/papers/volume11/theodorou10a/theodorou10a.pdf},
volume = {11},
year = {2010}
}
@article{Santoro,
abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
author = {Santoro, Adam and Raposo, David and Barrett, David G T and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy and London, Deepmind},
file = {::},
title = {{A simple neural network module for relational reasoning}},
url = {https://arxiv.org/pdf/1706.01427.pdf}
}
@article{Egan,
author = {Egan, Greg},
file = {::},
title = {{Crystal Nights}},
url = {http://ttapress.com/downloads/CrystalNights.pdf}
}
@article{Schenck,
abstract = {—Recent advances in AI and robotics have claimed many incredible results with deep learning, yet no work to date has applied deep learning to the problem of liquid perception and reasoning. In this paper, we apply fully-convolutional deep neural networks to the tasks of detecting and tracking liquids. We evaluate three models: a single-frame network, multi-frame network, and a LSTM recurrent network. Our results show that the best liquid detection results are achieved when aggregating data over multiple frames, in contrast to standard image segmen-tation. They also show that the LSTM network outperforms the other two in both tasks. This suggests that LSTM-based neural networks have the potential to be a key component for enabling robots to handle liquids using robust, closed-loop controllers.},
author = {Schenck, Connor and Fox, Dieter},
file = {::},
title = {{Detection and Tracking of Liquids with Fully Convolutional Networks}},
url = {https://arxiv.org/pdf/1606.06266.pdf}
}
@article{Vinyals,
abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6{\%} to 93.2{\%} and from 88.0{\%} to 93.8{\%} on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
author = {Vinyals, Oriol and Deepmind, Google and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
file = {::},
title = {{Matching Networks for One Shot Learning}},
url = {https://arxiv.org/pdf/1606.04080.pdf}
}
@inproceedings{zaremba2015execute,
author = {Zaremba, Wojciech and Sutskever, Ilya},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {::},
title = {{Learning to Execute}},
year = {2015}
}
@inproceedings{sutskever2014seq2seq,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the pas-sive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.3215v3},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {arXiv:1409.3215v3},
file = {::},
title = {{Sequence to Sequence Learning with Neural Networks}},
year = {2014}
}
@inproceedings{vinyals2015grammar,
abstract = {Syntactic constituency parsing is a fundamental problem in natural language pro-cessing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and in-efficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic cor-pus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.},
author = {Vinyals, Oriol and Kaiser, Lukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {::},
title = {{Grammar as a Foreign Language}},
url = {https://arxiv.org/pdf/1412.7449.pdf},
year = {2015}
}

@InProceedings{choi2021variationalgc,
  title = 	 {Variational Empowerment as Representation Learning for Goal-Conditioned Reinforcement Learning},
  author =       {Choi, Jongwook and Sharma, Archit and Lee, Honglak and Levine, Sergey and Gu, Shixiang Shane},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {1953--1963},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/choi21b/choi21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/choi21b.html},
}

@inproceedings{riedmiller2005nfq,
abstract = {This paper introduces NFQ, an algorithm for efficient and ef-fective training of a Q-value function represented by a multi-layer percep-tron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.},
author = {Riedmiller, Martin},
booktitle = {European Conference on Machine Learning (ECML)},
file = {::},
title = {{Neural Fitted Q Iteration -First Experiences with a Data Efficient Neural Reinforcement Learning Method}},
url = {http://ml.informatik.uni-freiburg.de/{\_}media/publications/rieecml05.pdf},
year = {2005}
}
@article{Scott2008,
abstract = {We show that ordinary bananas exhibit closed loops of switched charge versus applied voltage that are nearly identical to those misinterpreted as ferroelectric hysteresis loops in crystals. The 'ferroelectric' properties of bananas are contrasted with those of the real ferroelectric material Ba 2 NaNb 5 O 15 , often nicknamed 'bananas'.},
author = {Scott, J F},
doi = {10.1088/0953-8984/20/02/021001},
file = {::},
journal = {J. Phys.: Condens. Matter},
pages = {21001--2},
title = {{Ferroelectrics go bananas}},
url = {http://physics.rutgers.edu/{~}pchandra/physics681/bananas.pdf},
volume = {20},
year = {2008}
}
@article{Hutchings,
author = {Hutchings, Michael},
file = {::},
title = {{Introduction to mathematical arguments}},
url = {https://math.berkeley.edu/{~}hutching/teach/proofs.pdf}
}
@article{Dietterich,
author = {Dietterich, Thomas and Bishop, Christopher and Heckerman, David and Jordan, Michael and Kearns, Michael and Sutton, Richard S and Barto, Andrew G},
file = {::},
title = {{Probabilistic Graphical Models Adaptive Computation and Machine Learning}},
url = {http://miruvor.weebly.com/uploads/2/3/9/3/23933635/probabilistic{\_}graphical{\_}models.pdf}
}
@article{Gregora,
abstract = {In this paper we introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. The algorithms also provide an explicit measure of empowerment in a given state that can be used by an empowerment maximizing agent. The algorithm scales well with function approximation and we demonstrate the applicability of the algorithm on a range of tasks.},
author = {Gregor, Karol and Rezende, Danilo and Deepmind, Daan Wierstra},
file = {::},
title = {{Variational Intrinsic Control}},
url = {https://arxiv.org/pdf/1611.07507.pdf}
}
@article{Walker,
abstract = {Current approaches in video forecasting attempt to gen-erate videos directly in pixel space using Generative Ad-versarial Networks (GANs) or Variational Autoencoders (VAEs). However, since these approaches try to model all the structure and scene dynamics at once, in unconstrained settings they often generate uninterpretable results. Our in-sight is to model the forecasting problem at a higher level of abstraction. Specifically, we exploit human pose detec-tors as a free source of supervision and break the video forecasting problem into two discrete steps. First we ex-plicitly model the high level structure of active objects in the scene—humans—and use a VAE to model the possible future movements of humans in the pose space. We then use the future poses generated as conditional information to a GAN to predict the future frames of the video in pixel space. By using the structured space of pose as an intermediate representation, we sidestep the problems that GANs have in generating video pixels directly. We show through quantita-tive and qualitative evaluation that our method outperforms state-of-the-art methods for video prediction.},
author = {Walker, Jacob and Marino, Kenneth and Gupta, Abhinav and Hebert, Martial},
file = {::},
title = {{The Pose Knows: Video Forecasting by Generating Pose Futures}},
url = {https://arxiv.org/pdf/1705.00053.pdf}
}
@article{Writer,
author = {Writer, The Anonymous},
file = {::},
title = {{KABULIWALA Rabindranath Tagore}}
}
@inproceedings{peng2017actionspace,
abstract = {The use of deep reinforcement learning allows for high-dimensional state descrip-tors, but little is known about how the choice of action representation impacts the learning difficulty and the resulting performance. We compare the impact of four different action parameterizations (torques, muscle-activations, target joint angles, and target joint-angle velocities) in terms of learning time, policy robust-ness, motion quality, and policy query rates. Our results are evaluated on a gait-cycle imitation task for multiple planar articulated figures and multiple gaits. We demonstrate that the local feedback provided by higher-level action parameteriza-tions can significantly impact the learning, robustness, and quality of the resulting policies.},
author = {{Bin Peng}, Xue and van de Panne, Michiel},
booktitle = {ACM SIGGRAPH / Eurographics Symposium on Computer Animation (SCA)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Bin Peng, van de Panne - Unknown - LEARNING LOCOMOTION SKILLS USING DEEPRL DOES THE CHOICE OF ACTION SPACE MATTER.pdf:pdf},
title = {{LEARNING LOCOMOTION SKILLS USING DEEPRL: DOES THE CHOICE OF ACTION SPACE MATTER?}},
url = {https://arxiv.org/pdf/1611.01055.pdf},
year = {2017}
}
@misc{Gal,
author = {Gal, Yarin},
title = {{What my deep model doesn't know...}},
url = {http://mlg.eng.cam.ac.uk/yarin/blog{\_}3d801aa532c1ce.html},
urldate = {2017-04-28}
}
@inproceedings{gandhi2017fly,
abstract = {— How do you learn to navigate an Unmanned Aerial Vehicle (UAV) and avoid obstacles? One approach is to use a small dataset collected by human experts: however, high capacity learning algorithms tend to overfit when trained with little data. An alternative is to use simulation. But the gap between simulation and real world remains large especially for perception problems. The reason most research avoids using large-scale real data is the fear of crashes! In this paper, we propose to bite the bullet and collect a dataset of crashes itself! We build a drone whose sole purpose is to crash into objects: it samples naive trajectories and crashes into random objects. We crash our drone 11,500 times to create one of the biggest UAV crash dataset. This dataset captures the different ways in which a UAV can crash. We use all this negative flying data in con-junction with positive data sampled from the same trajectories to learn a simple yet powerful policy for UAV navigation. We show that this simple self-supervised model is quite effective in navigating the UAV even in extremely cluttered environments with dynamic obstacles including humans. For supplementary video see: https://youtu.be/HbHqC8HimoI},
author = {Gandhi, Dhiraj and Pinto, Lerrel and Gupta, Abhinav},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Gandhi, Pinto, Gupta - Unknown - Learning to Fly by Crashing.pdf:pdf},
title = {{Learning to Fly by Crashing}},
url = {https://arxiv.org/pdf/1704.05588.pdf},
year = {2017}
}
@article{Schulman,
abstract = {Two of the leading approaches for model-free reinforcement learning are policy gradient methods and Q-learning methods. Q-learning methods can be effective and sample-efficient when they work, however, it is not well-understood why they work, since empirically, the Q-values they estimate are very inaccurate. A partial explanation may be that Q-learning methods are secretly implementing policy gradient updates: we show that there is a precise equivalence between Q-learning and policy gradient methods in the setting of entropy-regularized reinforcement learning, that " soft " (entropy-regularized) Q-learning is exactly equivalent to a policy gradient method. We also point out a connection between Q-learning methods and natural policy gradient methods. Experimentally, we explore the entropy-regularized versions of Q-learning and policy gradients, and we find them to perform as well as (or slightly better than) the standard variants on the Atari benchmark. We also show that the equivalence holds in practical settings by constructing a Q-learning method that closely matches the learning dynamics of A3C without using a target network or exploration schedule.},
author = {Schulman, John and Chen, Xi and Abbeel, Pieter},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Schulman, Chen, Abbeel - Unknown - Equivalence Between Policy Gradients and Soft Q-Learning.pdf:pdf},
title = {{Equivalence Between Policy Gradients and Soft Q-Learning}},
url = {https://arxiv.org/pdf/1704.06440.pdf}
}
@article{Toussaint2012,
author = {Toussaint, Marc},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Toussaint - 2012 - Lecture Notes Some notes on gradient descent.pdf:pdf},
title = {{Lecture Notes: Some notes on gradient descent}},
url = {https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gradientDescent.pdf},
year = {2012}
}
@article{Doersch2016,
abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, in-cluding handwritten digits [1, 2], faces [1, 3, 4], house numbers [5, 6], CIFAR images [6], physical models of scenes [4], segmentation [7], and predicting the future from static images [8]. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
author = {Doersch, Carl},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:pdf},
keywords = {neural networks,structured prediction,unsupervised learning,variational autoencoders},
title = {{Tutorial on Variational Autoencoders}},
url = {https://arxiv.org/pdf/1606.05908.pdf},
year = {2016}
}
@article{Kang,
abstract = {Video is one of the fastest-growing sources of data and is rich with interesting semantic information. Furthermore, recent advances in computer vision, in the form of deep convolutional neural networks (CNNs), have made it possible to query this semantic information with near-human accuracy (in the form of image tagging). However, performing inference with state-of-the-art CNNs is computationally expensive: analyzing videos in real time (at 30 frames/sec) requires a {\$}1200 GPU per video stream, posing a serious computational barrier to CNN adoption in large-scale video data management systems. In response, we present NOSCOPE, a system that uses cost-based optimization to assemble a specialized video processing pipeline for each input video stream, greatly accelerating subsequent CNN-based queries on the video. As NOSCOPE observes a video, it trains two types of pipeline components (which we call filters) to exploit the locality in the video stream: difference detectors that exploit temporal locality between frames, and specialized models that are tailored to a specific scene and query (i.e., exploit environmental and query-specific locality). We show that the optimal set of filters and their parameters depends significantly on the video stream and query in question, so NOSCOPE introduces an efficient cost-based optimizer for this problem to select them. With this approach, our NOSCOPE prototype achieves up to 120-3,200× speed-ups (318-8,500× real-time) on binary classification tasks over real-world webcam and surveillance video while maintaining accuracy within 1-5{\%} of a state-of-the-art CNN.},
author = {Kang, Daniel and Emmons, John and Abuzaid, Firas and Bailis, Peter and Zaharia, Matei and Infolab, Stanford},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kang et al. - Unknown - Optimizing Deep CNN-Based Queries over Video Streams at Scale.pdf:pdf},
title = {{Optimizing Deep CNN-Based Queries over Video Streams at Scale}},
url = {https://arxiv.org/pdf/1703.02529.pdf}
}
@article{Chiappa,
abstract = {Models that can simulate how environments change in response to actions can be used by agents to plan and act efficiently. We improve on previous environment simulators from high-dimensional pixel observations by introducing recurrent neural networks that are able to make temporally and spatially coherent predictions for hundreds of time-steps into the future. We present an in-depth analysis of the factors affecting performance, providing the most extensive attempt to advance the understanding of the properties of these models. We address the issue of computationally inefficiency with a model that does not need to generate a high-dimensional image at each time-step. We show that our approach can be used to improve exploration and is adaptable to many diverse environments, namely 10 Atari games, a 3D car racing environment, and complex 3D mazes.},
author = {Chiappa, Silvia and Racaniere, S{\'{e}}bastien and Wierstra, Daan and Mohamed, Shakir},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Chiappa et al. - Unknown - RECURRENT ENVIRONMENT SIMULATORS.pdf:pdf},
title = {{Recurrent Environment Simulators}},
url = {https://arxiv.org/pdf/1704.02254.pdf}
}
@article{Li,
abstract = {The goal of imitation learning is to match ex-ample expert behavior, without access to a re-inforcement signal. Expert demonstrations pro-vided by humans, however, often show signifi-cant variability due to latent factors that are not explicitly modeled. We introduce an extension to the Generative Adversarial Imitation Learn-ing method that can infer the latent structure of human decision-making in an unsupervised way. Our method can not only imitate complex be-haviors, but also learn interpretable and mean-ingful representations. We demonstrate that the approach is applicable to high-dimensional en-vironments including raw visual inputs. In the highway driving domain, we show that a model learned from demonstrations is able to both pro-duce different styles of human-like driving be-haviors and accurately anticipate human actions. Our method surpasses various baselines in terms of performance and functionality.},
author = {Li, Yunzhu and Song, Jiaming and Ermon, Stefano},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Li, Song, Ermon - Unknown - Inferring The Latent Structure of Human Decision-Making from Raw Visual Inputs.pdf:pdf},
title = {{Inferring The Latent Structure of Human Decision-Making from Raw Visual Inputs}},
url = {https://arxiv.org/pdf/1703.08840.pdf}
}
@article{Krishnan,
abstract = {Kalman Filters are one of the most influential models of time-varying phenomena. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption in a variety of disciplines. Motivated by recent variational methods for learning deep generative models, we introduce a unified algorithm to efficiently learn a broad spectrum of Kalman filters. Of particular interest is the use of temporal generative models for counterfactual inference. We investigate the efficacy of such models for counterfactual inference, and to that end we introduce the " Healing MNIST " dataset where long-term structure, noise and actions are applied to sequences of digits. We show the efficacy of our method for modeling this dataset. We further show how our model can be used for coun-terfactual inference for patients, based on electronic health record data of 8,000 patients over 4.5 years.},
author = {Krishnan, Rahul G and Shalit, Uri and Sontag, David},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Krishnan, Shalit, Sontag - Unknown - Deep Kalman Filters.pdf:pdf},
title = {{Deep Kalman Filters}},
url = {https://arxiv.org/pdf/1511.05121.pdf}
}
@article{Gub,
abstract = {Deep neural networks are powerful parametric models that can be trained effi-ciently using the backpropagation algorithm. Stochastic neural networks com-bine the power of large parametric functions with that of graphical models, which makes it possible to learn very complex distributions. However, as backpropaga-tion is not directly applicable to stochastic networks that include discrete sampling operations within their computational graph, training such networks remains diffi-cult. We present MuProp, an unbiased gradient estimator for stochastic networks, designed to make this task easier. MuProp improves on the likelihood-ratio esti-mator by reducing its variance using a control variate based on the first-order Tay-lor expansion of a mean-field network. Crucially, unlike prior attempts at using backpropagation for training stochastic networks, the resulting estimator is unbi-ased and well behaved. Our experiments on structured output prediction and dis-crete latent variable modeling demonstrate that MuProp yields consistently good performance across a range of difficult tasks.},
author = {Gu, Shixiang and Levine, Sergey and Sutskever, Ilya and Mnih, Andriy},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Gu et al. - Unknown - MUPROP UNBIASED BACKPROPAGATION FOR STOCHASTIC NEURAL NETWORKS.pdf:pdf},
title = {{MUPROP: UNBIASED BACKPROPAGATION FOR STOCHASTIC NEURAL NETWORKS}},
url = {https://arxiv.org/pdf/1511.05176.pdf}
}
@article{Gregor,
abstract = {This paper introduces the Deep Recurrent Atten-tive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distin-guished from real data with the naked eye.},
author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and {Jimenez Rezende}, Danilo and Wierstra, Daan},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Gregor et al. - Unknown - DRAW A Recurrent Neural Network For Image Generation.pdf:pdf},
title = {{DRAW: A Recurrent Neural Network For Image Generation}},
url = {https://arxiv.org/pdf/1502.04623.pdf}
}
@article{Zhu2016,
author = {Zhu, Richard and Kang, Andrew},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Zhu, Kang - 2016 - Imitation Learning.pdf:pdf},
title = {{Imitation Learning}},
url = {http://www.yisongyue.com/courses/cs159/lectures/imitation-learning-3.pdf},
year = {2016}
}
@article{Berthelot,
abstract = {We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks. This method balances the generator and discriminator during training. Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality. We also derive a way of controlling the trade-off between image diversity and visual quality. We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions. This is achieved while using a relatively simple model architecture and a standard training procedure.},
author = {Berthelot, David and Schumm, Tom and Metz, Luke},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Berthelot, Schumm, Metz - Unknown - BEGAN Boundary Equilibrium Generative Adversarial Networks.pdf:pdf},
title = {{BEGAN: Boundary Equilibrium Generative Adversarial Networks}},
url = {https://arxiv.org/pdf/1703.10717.pdf}
}
@inproceedings{pinto2017robust,
abstract = {Deep neural networks coupled with fast simula-tion and improved computation have led to re-cent successes in the field of reinforcement learn-ing (RL). However, most current RL-based ap-proaches fail to generalize since: (a) the gap be-tween simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H ∞ control methods, we note that both modeling er-rors and differences in training and test scenar-ios can be viewed as extra forces/disturbances in the system. This paper proposes the idea of ro-bust adversarial reinforcement learning (RARL), where we train an agent to operate in the pres-ence of a destabilizing adversary that applies dis-turbance forces to the system. The jointly trained adversary is reinforced – that is, it learns an op-timal destabilization policy. We formulate the policy learning as a zero-sum, minimax objec-tive function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper and Walker2d) conclusively demonstrate that our method (a) improves train-ing stability; (b) is robust to differences in train-ing/test conditions; and c) outperform the base-line even in the absence of the adversary.},
author = {Pinto, Lerrel and Davidson, James and Sukthankar, Rahul and Gupta, Abhinav},
booktitle = {International Conference on Machine Learning (ICML)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Pinto et al. - Unknown - Robust Adversarial Reinforcement Learning.pdf:pdf},
title = {{Robust Adversarial Reinforcement Learning}},
url = {https://arxiv.org/pdf/1703.02702.pdf},
year = {2017}
}
@article{Hanna2017,
abstract = {Robot learning in simulation is a promising alternative to the prohibitive sample cost of learning in the physical world. Unfortunately, policies learned in simulation often perform worse than hand-coded policies when applied on the physi-cal robot. Grounded simulation learning (GSL) promises to address this issue by altering the simulator to better match the real world. This paper proposes a new algorithm for GSL – Grounded Action Transformation – and applies it to learn-ing of humanoid bipedal locomotion. Our approach results in a 43.27{\%} improvement in forward walk velocity compared to a state-of-the art hand-coded walk. We further evaluate our methodology in controlled experiments using a second, higher-fidelity simulator in place of the real world. Our results contribute to a deeper understanding of grounded simulation learning and demonstrate its effectiveness for learning robot control policies.},
author = {Hanna, Josiah P and Stone, Peter},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Hanna, Stone - 2017 - Grounded Action Transformation for Robot Learning in Simulation.pdf:pdf},
title = {{Grounded Action Transformation for Robot Learning in Simulation}},
url = {https://www.cs.utexas.edu/{~}AustinVilla/papers/AAAI17-Hanna.pdf},
year = {2017}
}
@article{Luan,
abstract = {Figure 1: Given a reference style image (a) and an input image (b), we seek to create an output image of the same scene as the input, but with the style of the reference image. The Neural Style algorithm [5] (c) successfully transfers colors, but also introduces distortions that make the output look like a painting, which is undesirable in the context of photo style transfer. In comparison, our result (d) transfers the color of the reference style image equally well while preserving the photorealism of the output. On the right (e), we show 3 insets of (b), (c), and (d) (in that order). Zoom in to compare results. Abstract This paper introduces a deep-learning approach to pho-tographic style transfer that handles a large variety of image content while faithfully transferring the reference style. Our approach builds upon recent work on painterly transfer that separates style from the content of an image by considering different layers of a neural network. However, as is, this ap-proach is not suitable for photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom CNN layer through which we can backpropagate. We show that this approach successfully suppresses distortion and yields satisfying pho-torealistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.},
author = {Luan, Fujun and Adobe, Sylvain Paris and {Shechtman Adobe}, Eli and Bala, Kavita},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Luan et al. - Unknown - Deep Photo Style Transfer.pdf:pdf},
title = {{Deep Photo Style Transfer}},
url = {https://arxiv.org/pdf/1703.07511.pdf}
}
@inproceedings{kakade2001npg,
abstract = {We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the param-eter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradi-ent is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sut-ton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris.},
author = {Kakade, Sham},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Kakade - Unknown - A Natural Policy Gradient.pdf:pdf},
title = {{A Natural Policy Gradient}},
url = {http:},
year = {2001}
}
@inproceedings{stadie2016exploration,
author = {Stadie, Bradly C and Levine, Sergey and Abbeel, Pieter},
booktitle = {International Conference on Learning Representations (ICLR)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - INCENTIVIZING EXPLORATION IN REINFORCEMENT LEARNING WITH DEEP PREDICTIVE MODELS.pdf:pdf},
title = {{Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models}},
url = {https://arxiv.org/pdf/1507.00814.pdf},
year = {2016}
}
@article{Griffiths2014,
abstract = {a b s t r a c t The cognitive revolution offered an alternative to merely analyzing human behavior, using the notion of computation to rigorously express hypotheses about the mind. Computation also gives us new tools for testing these hypotheses – large behavioral databases generated by human interactions with computers and with one another. This kind of data is typically analyzed by computer scientists, who focus on predicting people's behavior based on their history. A new cognitive revolution is needed, demonstrating the value of minds as inter-vening variables in these analyses and using the results to evaluate models of human cognition. {\'{O}} 2014 Elsevier B.V. All rights reserved. Over 60 years ago, the cognitive revolution made legit-imate the scientific study of the mind (Gardner, 1987; Miller, 2003). Formal models of cognition made it possible to postulate processes that lie between a person's history and their actions, offering an alternative to the rigid stimulus-response structure of Behaviorism. Using new mathematical ideas – in particular, the notion of computa-tion – a generation of researchers discovered a way to rigorously state hypotheses about how human minds work. I believe that we stand on the brink of a new revolu-tion, with equally far-reaching consequences and an equally important role for computation. A revolution in how we test those hypotheses. While the decades since the cognitive revolution have seen significant innovations in the kinds of computational models researchers have explored, the methods used to evaluate those models have remained fundamentally the same. In fact, those methods have arguably remained the same for over a century, being based on the small-scale laboratory science that characterized the first psychologi-cal research (Mandler, 2007). If you want to answer a question about the human mind (or publish a paper in Cog-nition) you formulate some hypotheses, bring an appropri-ate number of people into the laboratory, and have them carry out a task that distinguishes between those hypotheses. But while we have remained focused on the events in our laboratories, the world outside those laboratories has changed. The internet offers a way to reach thousands of people in seconds. Human lives are lived more and more through our computers and our mobile phones. And the people with the most data about human behavior are no longer psychologists. They are computer scientists. The mouse clicks and keystrokes of our online interac-tions are data, and figuring out how to make the best use of those data has become an important part of computer science. Recommendation systems that tell you which books you might be interested in, services that suggest related news stories, search engines that make use of the tags people apply to images, algorithms that select the advertisements you are most likely to click on. . . all are sig-nificant areas of research in computer science, and all are fundamentally based on the study of human behavior. They are also all missed opportunities for cognitive science. Recommendation systems need to divine human pref-erences – a problem that has been studied by both},
author = {Griffiths, Thomas L},
doi = {10.1016/j.cognition.2014.11.026},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Griffiths - 2014 - Manifesto for a new (computational) cognitive revolution.pdf:pdf},
journal = {COGNITION},
keywords = {Big data,Computational modeling,Crowdsourcing},
title = {{Manifesto for a new (computational) cognitive revolution}},
url = {http://dx.doi.org/10.1016/j.cognition.2014.11.026},
year = {2014}
}
@article{Britz,
abstract = {Neural Machine Translation (NMT) has shown remarkable progress over the past few years with production systems now being deployed to end-users. One major drawback of current architectures is that they are expensive to train, typically re-quiring days to weeks of GPU time to converge. This makes exhaustive hyper-parameter search, as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analy-sis of NMT architecture hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architec-tures. As part of this contribution, we release an open-source NMT framework 1 that enables researchers to easily experi-ment with novel techniques and reproduce state of the art results.},
author = {Britz, Denny and Goldie, Anna and Luong, Minh-Thang and Le, Quoc and Brain, Google},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Britz et al. - Unknown - Massive Exploration of Neural Machine Translation Architectures.pdf:pdf},
title = {{Massive Exploration of Neural Machine Translation Architectures}},
url = {https://arxiv.org/pdf/1703.03906.pdf}
}
@inproceedings{He2017,
abstract = {Recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring con-tinuous state and action spaces. However, to the best of our knowledge no previous work has succeeded at using deep neural networks in structured (parameterized) continuous action spaces. To fill this gap, this paper focuses on learning within the domain of simulated RoboCup soccer, which features a small set of discrete action types, each of which is parameterized with continuous variables. The best learned agents can score goals more reliably than the 2012 RoboCup champion agent. As such, this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space MDPs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1511.06434v1},
author = {He, Ji and Chen, Jianshu and He, Xiaodong and Gao, Jianfeng and Li, Lihong and Deng, Li and Ostendorf, Mari},
booktitle = {International Conference on Learning Representations (ICLR)},
doi = {10.1051/0004-6361/201527329},
eprint = {arXiv:1511.06434v1},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/rl-squared-Duan16.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
number = {1999},
pages = {1--17},
pmid = {23459267},
title = {{RL{\^{}}2: Fast Reinforcement Learning via Slow Reinforcement Learning}},
year = {2017}
}
@article{Guo2015,
author = {Guo, Philip J},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/pguo-PhD-grind.pdf:pdf},
title = {{pguo-PhD-grind}},
year = {2015}
}
@article{Graves2014,
abstract = {We extend the capabilities of neural networks by coupling them to external memory re-sources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demon-strate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.5401v2},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
doi = {10.3389/neuro.12.006.2007},
eprint = {arXiv:1410.5401v2},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/neural-turing-machines-Graves14.pdf:pdf},
isbn = {0028-0836},
issn = {2041-1723},
journal = {CoRR},
pages = {1--26},
pmid = {18958277},
title = {{Neural Turing Machines}},
url = {http://arxiv.org/abs/1410.5401},
volume = {abs/1410.5},
year = {2014}
}
@inproceedings{Oord2016,
abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
archivePrefix = {arXiv},
arxivId = {1601.06759},
author = {van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
booktitle = {International Conference on Machine Learning (ICML)},
eprint = {1601.06759},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/pixel-rnns-Oord16.pdf:pdf},
isbn = {9781510829008},
title = {{Pixel Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1601.06759},
volume = {48},
year = {2016}
}
@inproceedings{VanHasselt2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1509.06461v1},
author = {van Hasselt, H and Guez, A and Silver, D},
booktitle = {AAAI Conference on Artificial Intelligence},
doi = {10.1016/j.artint.2015.09.002},
eprint = {arXiv:1509.06461v1},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/double-dqn-Hasselt16.pdf:pdf},
isbn = {9781577357605},
issn = {00043702},
pmid = {26150344},
title = {{Deep reinforcement learning with double Q-learning}},
year = {2016}
}
@article{Bousmalis2016,
abstract = {The cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive. One approach circumventing this cost is training models on synthetic data where annotations are provided automatically. Despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied. Ex-isting approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted. However, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain. We suggest that explicitly modeling what is unique to each domain can improve a model's ability to extract domain–invariant features. Inspired by work on private–shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains. Our model is trained not only to perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains. Our novel architecture results in a model that outperforms the state–of–the–art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process.},
archivePrefix = {arXiv},
arxivId = {1608.06019},
author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
eprint = {1608.06019},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/domain-separation-networks-Bousmalis2016.pdf:pdf},
journal = {Advances in Neural Information Processing Systems (NeurIPS)},
number = {Nips},
title = {{Domain Separation Networks}},
year = {2016}
}
@inproceedings{Finn2016,
abstract = {A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 50,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a "visual imagination" of different futures based on different courses of action. Our experiments show that our proposed method not only produces more accurate video predictions, but also more accurately predicts object motion, when compared to prior methods.},
archivePrefix = {arXiv},
arxivId = {1605.07157},
author = {Finn, Chelsea and Goodfellow, Ian and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
eprint = {1605.07157},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/unsupervised-learning-physical-interaction-through-video-prediction-Finn16.pdf:pdf},
title = {{Unsupervised Learning for Physical Interaction through Video Prediction}},
url = {http://arxiv.org/abs/1605.07157},
year = {2016}
}
@inproceedings{oord2017vqvae,
abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
archivePrefix = {arXiv},
arxivId = {1711.00937},
author = {van den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1711.00937},
file = {::},
month = {nov},
pages = {6307--6316},
publisher = {Neural information processing systems foundation},
title = {{Neural Discrete Representation Learning}},
url = {http://arxiv.org/abs/1711.00937},
volume = {2017-Decem},
year = {2017}
}
@inproceedings{lynch2019play,
abstract = {Acquiring a diverse repertoire of general-purpose skills remains an open challenge for robotics. In this work, we propose self-supervising control on top of human teleoperated play data as a way to scale up skill learning. Play has two properties that make it attractive compared to conventional task demonstrations. Play is cheap, as it can be collected in large quantities quickly without task segmenting, labeling, or resetting to an initial state. Play is naturally rich, covering {\~{}}4x more interaction space than task demonstrations for the same amount of collection time. To learn control from play, we introduce Play-LMP, a self-supervised method that learns to organize play behaviors in a latent space, then reuse them at test time to achieve specific goals. Combining self-supervised control with a diverse play dataset shifts the focus of skill learning from a narrow and discrete set of tasks to the full continuum of behaviors available in an environment. We find that this combination generalizes well empirically---after self-supervising on unlabeled play, our method substantially outperforms individual expert-trained policies on 18 difficult user-specified visual manipulation tasks in a simulated robotic tabletop environment. We additionally find that play-supervised models, unlike their expert-trained counterparts, are more robust to perturbations and exhibit retrying-till-success behaviors. Finally, we find that our agent organizes its latent plan space around functional tasks, despite never being trained with task labels. Videos, code and data are available at learning-from-play.github.io},
archivePrefix = {arXiv},
arxivId = {1903.01973},
author = {Lynch, Corey and Khansari, Mohi and Xiao, Ted and Kumar, Vikash and Tompson, Jonathan and Levine, Sergey and Sermanet, Pierre},
booktitle = {Conference on Robot Learning (CoRL)},
eprint = {1903.01973},
file = {::},
month = {mar},
title = {{Learning Latent Plans from Play}},
url = {http://arxiv.org/abs/1903.01973},
year = {2019}
}
@book{gibson1979ecologicalapproach,
author = {Gibson, James},
title = {{The Ecological Approach to Visual Perception}},
year = {1979}
}
@book{berger2014development,
author = {Berger, Kathleen},
title = {{The Developing Person Through the Life Span}},
year = {2014}
}
@article{Fernando2017,
archivePrefix = {arXiv},
arxivId = {1701.08734},
author = {Fernando, Chrisantha and Banarse, Dylan and Blundell, Charles and Zwols, Yori and Ha, David and Rusu, Andrei A. and Pritzel, Alexander and Wierstra, Daan},
eprint = {1701.08734},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/pathnet-Fernando17.pdf:pdf},
title = {{PathNet: Evolution Channels Gradient Descent in Super Neural Networks}},
year = {2017}
}
@article{Hardt2004,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.03530v2},
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
eprint = {arXiv:1611.03530v2},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/understanding-deep-learning-rethinking-generalization-Zhang16.pdf:pdf},
journal = {CoRR},
title = {{Understanding Deep Learning Requires Rethinking Generalization}},
year = {2016}
}
@article{Lia,
author = {Li, Yuxi},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/deep-rl-overview-Li16.pdf:pdf},
title = {{Deep Reinforcement Learning: An Overview}}
}
@article{Tang2017,
author = {Tang, Shuai and Jolla, La and Jin, Hailin and Fang, Chen and Wang, Zhaowen and Jose, San},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/dcgan-Radford16.pdf:pdf;:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Tang et al. - 2017 - Unsupervised Sentence Representation Learning.pdf:pdf},
pages = {1--10},
title = {{Unsupervised Sentence Representation Learning}},
year = {2017}
}
@article{Fukui2016,
abstract = {Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise multiplication or addition, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.},
archivePrefix = {arXiv},
arxivId = {1606.01847},
author = {Fukui, Akira and Park, Dong Huk and Yang, Daylen and Rohrbach, Anna and Darrell, Trevor and Rohrbach, Marcus},
eprint = {1606.01847},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/compact-bilinear-pooling-visual-qa-Fukui16.pdf:pdf},
isbn = {978-1-945626-25-8},
journal = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},
pages = {9},
pmid = {121187},
title = {{Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding}},
url = {http://arxiv.org/abs/1606.01847},
year = {2016}
}
@article{Kaufmann2016,
abstract = {In this paper we propose and investigate a novel end-to-end method for automatically generating short email responses, called Smart Reply. It generates semantically diverse sug- gestions that can be used as complete email responses with just one tap on mobile. The system is currently used in In- box by Gmail and is responsible for assisting with 10{\%} of all mobile responses. It is designed to work at very high throughput and process hundreds of millions of messages daily. The system exploits state-of-the-art, large-scale deep learning. We describe the architecture of the system as well as the challenges that we faced while building it, like response di- versity and scalability. We also introduce a new method for semantic clustering of user-generated content that requires only a modest amount of explicitly labeled data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1606.04870v1},
author = {Kaufmann, Tobias and Ravi, Sujith},
doi = {http://dx.doi.org/10.1145/2939672.2939801},
eprint = {arXiv:1606.04870v1},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/smart-reply-Kannan16.pdf:pdf},
isbn = {9781450342322},
issn = {0146-4833},
journal = {Conference on Knowledge Discovery and Data Mining (KDD)},
keywords = {clustering,deep learning,email,lstm,semantics},
title = {{Smart Reply : Automated Response Suggestion for Email}},
year = {2016}
}
@article{oord2016wavenet,
author = {van den Oord, Aaron},
file = {:Users/ashvin/code/kindlize/pdfs/deeprl/wavenet-Oord16.pdf:pdf},
title = {{Wavenet: A Generative Model for Raw Audio}},
year = {2016}
}
@article{Mordatch,
abstract = {By capturing statistical patterns in large cor-pora, machine learning has enabled significant advances in natural language processing, includ-ing in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply cap-turing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. To-wards this end, we propose a multi-agent learn-ing environment and learning methods that bring about emergence of a basic compositional lan-guage. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal com-munication such as pointing and guiding when language communication is unavailable.},
author = {Mordatch, Igor and Abbeel, Pieter},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Mordatch, Abbeel - Unknown - Emergence of Grounded Compositional Language in Multi-Agent Populations.pdf:pdf},
title = {{Emergence of Grounded Compositional Language in Multi-Agent Populations}},
url = {https://arxiv.org/pdf/1703.04908.pdf}
}
@article{Jain,
abstract = {This work characterizes the benefits of averaging techniques widely used in conjunction with stochastic gradient descent (SGD). In particular, this work sharply analyzes: (1) mini-batching, a method of averaging many samples of the gradient to both reduce the variance of a stochastic gradient estimate and for parallelizing SGD and (2) tail-averaging, a method involving averaging the final few iterates of SGD in order to decrease the variance in SGD's final iterate. This work presents the first tight non-asymptotic generalization error bounds for these schemes for the stochastic approximation problem of least squares regression. Furthermore, this work establishes a precise problem-dependent extent to which mini-batching can be used to yield provable near-linear parallelization speedups over SGD with batch size one. These results are utilized in providing a highly parallelizable SGD algorithm that obtains the optimal statistical error rate with nearly the same number of serial updates as batch gradient descent, which improves significantly over existing SGD-style methods. Finally, this work sheds light on some fundamental differences in SGD's behavior when dealing with agnostic noise in the (non-realizable) least squares regression problem. In particular, the work shows that the stepsizes that ensure optimal statistical error rates for the agnostic case must be a function of the noise properties. The central analysis tools used by this paper are obtained through generalizing the operator view of averaged SGD, introduced by D{\'{e}}fossez and Bach [1] followed by developing a novel analysis in bounding these operators to characterize the generalization error. These techniques may be of broader interest in analyzing various computational aspects of stochastic approximation.},
author = {Jain, Prateek and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Jain et al. - Unknown - Parallelizing Stochastic Approximation Through Mini-Batching and Tail-Averaging.pdf:pdf},
title = {{Parallelizing Stochastic Approximation Through Mini-Batching and Tail-Averaging}},
url = {https://arxiv.org/pdf/1610.03774.pdf}
}
@inproceedings{byravan2017se3,
abstract = {We introduce SE3-Nets which are deep networks designed to model rigid body motion from raw point cloud data. Based only on pairs of depth images along with an action vector and point wise data associations, SE3-Nets learn to segment effected object parts and predict their motion resulting from the applied force. Rather than learning point wise flow vectors, SE3-Nets predict SE3 transformations for different parts of the scene. Using simulated depth data of a table top scene and a robot manipulator, we show that the structure underlying SE3-Nets enables them to generate a far more consistent prediction of object motion than traditional flow based networks. I. INTRODUCTION The ability to predict how an environment changes based on forces applied to it is fundamental for a robot to achieve specific goals. For instance, in order to arrange objects on a table into a desired configuration, a robot has to be able to reason about where and how to push individual objects, which requires some understanding of physical quantities such as object boundaries, mass, surface friction, and their relationship to forces. A standard approach in robot control is to use a physical model of the environment and perform optimal control to find a policy that leads to the goal state. For instance, extensive work utilizing the MuJoCo physics engine [30] has shown how strong physics models can enable solutions to optimal control problems and policy learning in complex and contact-rich environments [11, 22]. A fundamental problem of such models is that they rely on very accurate estimates of the state of the system [37]. Unfortunately, estimating values such as the mass distribution and surface friction of an object using visual information and force feedback is extremely difficult. This is one of the main reasons why humans are still far better than robots at performing even simple tasks such as pushing an object along a desired trajectory. Humans achieve this even though their control policies and decision making are informed only by approximate notions of physics [5, 27]. Research has shown that these mental models are learned from a young age, potentially by observing the effect of actions on the physical world [4]. Learning such a model of physical intuition can help robots reason about the effect of their actions, which is a critical component of operating in complex real-world environments. In this work, we explore the use of deep learning to model this concept of "physical intuition", learning a model that predicts changes to the environment based on specific actions. Our model uses motion cues to learn to segment the scene into "salient" objects (much akin to a saliency map [17])},
author = {Byravan, Arunkumar and Fox, Dieter},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
file = {:Users/ashvin/Library/Application Support/Mendeley Desktop/Downloaded/Byravan, Fox - Unknown - SE3-Nets Learning Rigid Body Motion using Deep Neural Networks.pdf:pdf},
title = {{SE3-Nets: Learning Rigid Body Motion using Deep Neural Networks}},
url = {https://arxiv.org/pdf/1606.02378.pdf},
year = {2017}
}
@inproceedings{kohl2004quadruped,
abstract = {For a robot, the ability to get from one place to another is one of the most basic skills. However, locomotion on legged robots is a challenging multidimensional control problem. This paper presents a machine learning approach to legged locomotion, with all training done on the physical robots. The main contributions are a specification of our fully automated learning environment and a detailed empirical comparison of four different machine learning algorithms for learning quadrupedal locomotion. The resulting learned walk is considerably faster than all previously reported hand-coded walks for the same robot platform.},
author = {Kohl, Nate and Stone, Peter},
booktitle = {AAAI Conference on Artificial Intelligence},
file = {::},
pages = {611--616},
title = {{Machine Learning for Fast Quadrupedal Locomotion}},
url = {http://www.cs.utexas.edu/˜{\%}7Bnate,pstone{\%}7D},
year = {2004}
}
@article{benbrahim1997biped,
abstract = {This paper presents some results from a study of biped dynamic walking using reinforcement learning. During this study a hardware biped robot was built, a new reinforcement learning algorithm as well as a new learning architecture were developed. The biped learned dynamic walking without any previous knowledge about its dynamic model. The self scaling reinforcement (SSR) learning algorithm was developed in order to deal with the problem of reinforcement learning in continuous action domains. The learning architecture was developed in order to solve complex control problems. It uses different modules that consist of simple controllers and small neural networks. The architecture allows for easy incorporation of new modules that represent new knowledge, or new requirements for the desired task.},
author = {Benbrahim, Hamid and Franklin, Judy A.},
doi = {10.1016/S0921-8890(97)00043-2},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Biped robot,Biped walking,Legged robot,Reinforcement learning,Robot learning},
month = {dec},
number = {3-4},
pages = {283--302},
publisher = {Elsevier},
title = {{Biped dynamic walking using reinforcement learning}},
volume = {22},
year = {1997}
}
@article{nair2020awac,
abstract = {Reinforcement learning provides an appealing formalism for learning control policies from experience. However, the classic active formulation of reinforcement learning necessitates a lengthy active exploration process for each behavior, making it difficult to apply in real-world settings. If we can instead allow reinforcement learning to effectively use previously collected data to aid the online learning process, where the data could be expert demonstrations or more generally any prior experience, we could make reinforcement learning a substantially more practical tool. While a number of recent methods have sought to learn offline from previously collected data, it remains exceptionally difficult to train a policy with offline data and improve it further with online reinforcement learning. In this paper we systematically analyze why this problem is so challenging, and propose a novel algorithm that combines sample-efficient dynamic programming with maximum likelihood policy updates, providing a simple and effective framework that is able to leverage large amounts of offline data and then quickly perform online fine-tuning of reinforcement learning policies. We show that our method enables rapid learning of skills with a combination of prior demonstration data and online experience across a suite of difficult dexterous manipulation and benchmark tasks.},
archivePrefix = {arXiv},
arxivId = {2006.09359},
author = {Nair, Ashvin and Dalal, Murtaza and Gupta, Abhishek and Levine, Sergey},
eprint = {2006.09359},
file = {::},
mendeley-groups = {New Research},
month = {jun},
title = {{Accelerating Online Reinforcement Learning with Offline Datasets}},
url = {http://arxiv.org/abs/2006.09359},
year = {2020}
}
@inproceedings{devin2019cpv,
	title = {Compositional {Plan} {Vectors}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/file/00989c20ff1386dc386d8124ebcba1a5-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Devin, Coline and Geng, Daniel and Abbeel, Pieter and Darrell, Trevor and Levine, Sergey},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
}

@article{reed2015analogy,
   abstract = {In addition to identifying the content within a single image, relating images and generating related images are critical tasks for image understanding. Recently, deep convolutional networks have yielded breakthroughs in predicting image labels , annotations and captions, but have only just begun to be used for generating high-quality images. In this paper we develop a novel deep network trained end-to-end to perform visual analogy making, which is the task of transforming a query image according to an example pair of related images. Solving this problem requires both accurately recognizing a visual relationship and generating a transformed query image accordingly. Inspired by recent advances in language mod-eling, we propose to solve visual analogies by learning to map images to a neural embedding in which analogical reasoning is simple, such as by vector subtraction and addition. In experiments, our model effectively models visual analogies on several datasets: 2D shapes, animated video game sprites, and 3D car models.},
   author = {Scott E. Reed and Yi Zhang and Yuting Zhang and Honglak Lee},
   journal = {Advances in Neural Information Processing Systems},
   title = {Deep Visual Analogy-Making},
   volume = {28},
   year = {2015},
}
@inproceedings{devin2018grasp2vec,
  author    = {Eric Jang and
               Coline Devin and
               Vincent Vanhoucke and
               Sergey Levine},
  title     = {Grasp2Vec: Learning Object Representations from Self-Supervised Grasping},
  booktitle = {2nd Annual Conference on Robot Learning, CoRL 2018, Z{\"{u}}rich,
               Switzerland, 29-31 October 2018, Proceedings},
  series    = {Proceedings of Machine Learning Research},
  volume    = {87},
  pages     = {99--112},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v87/jang18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:24 +0200},
  biburl    = {https://dblp.org/rec/conf/corl/JangDVL18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mikolov2013word2vec,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
}

