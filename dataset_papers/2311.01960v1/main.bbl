\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahle et~al.(2020)Ahle, Kapralov, Knudsen, Pagh, Velingker, Woodruff,
  and Zandieh]{ahle2020oblivious}
Thomas~D Ahle, Michael Kapralov, Jakob~BT Knudsen, Rasmus Pagh, Ameya
  Velingker, David~P Woodruff, and Amir Zandieh.
\newblock Oblivious sketching of high-degree polynomial kernels.
\newblock In \emph{Proceedings of the Fourteenth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 141--160. SIAM, 2020.

\bibitem[Alman and Song(2023)]{alman2023fast}
Josh Alman and Zhao Song.
\newblock Fast attention requires bounded entries.
\newblock \emph{arXiv preprint arXiv:2302.13214}, 2023.

\bibitem[Alman et~al.(2020)Alman, Chu, Schild, and Song]{alman2020algorithms}
Josh Alman, Timothy Chu, Aaron Schild, and Zhao Song.
\newblock Algorithms and hardness for linear algebra on geometric graphs.
\newblock In \emph{2020 IEEE 61st Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 541--552. IEEE, 2020.

\bibitem[Backurs et~al.(2017)Backurs, Indyk, and Schmidt]{backurs2017fine}
Arturs Backurs, Piotr Indyk, and Ludwig Schmidt.
\newblock On the fine-grained complexity of empirical risk minimization: Kernel
  methods and neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Bakshi et~al.(2020)Bakshi, Chepurko, and Woodruff]{bakshi2020robust}
Ainesh Bakshi, Nadiia Chepurko, and David~P Woodruff.
\newblock Robust and sample optimal algorithms for {PSD} low rank
  approximation.
\newblock In \emph{2020 IEEE 61st Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 506--516. IEEE, 2020.

\bibitem[Chen and Williams(2019)]{cw18}
Lijie Chen and Ryan Williams.
\newblock An equivalence class for orthogonal vectors.
\newblock In \emph{Proceedings of the Thirtieth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 21--40. SIAM, 2019.

\bibitem[Choromanski et~al.(2021)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarl{\'{o}}s, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and
  Weller]{choromanski2021performer}
Krzysztof~Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tam{\'{a}}s Sarl{\'{o}}s, Peter Hawkins, Jared~Quincy Davis,
  Afroz Mohiuddin, Lukasz Kaiser, David~Benjamin Belanger, Lucy~J. Colwell, and
  Adrian Weller.
\newblock Rethinking attention with {P}erformers.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021}, 2021.

\bibitem[Choromanski et~al.(2022)Choromanski, Lin, Chen, Sehanobish, Ma, Jain,
  Varley, Zeng, Ryoo, Likhosherstov, Kalashnikov, Sindhwani, and
  Weller]{choromanski2022hybrid}
Krzysztof~Marcin Choromanski, Han Lin, Haoxian Chen, Arijit Sehanobish, Yuanzhe
  Ma, Deepali Jain, Jake Varley, Andy Zeng, Michael~S. Ryoo, Valerii
  Likhosherstov, Dmitry Kalashnikov, Vikas Sindhwani, and Adrian Weller.
\newblock Hybrid random features.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022}, 2022.

\bibitem[Clarkson and Woodruff(2013)]{cw13}
Kenneth~L. Clarkson and David~P. Woodruff.
\newblock Low rank approximation and regression in input sparsity time.
\newblock In \emph{Symposium on Theory of Computing Conference, STOC 2013},
  pages 81--90. {ACM}, 2013.

\bibitem[Feller(1968)]{feller1968probability}
William Feller.
\newblock \emph{An Introduction to Probability Theory and Its Applications},
  volume~1.
\newblock Wiley, January 1968.
\newblock ISBN 0471257087.

\bibitem[Han et~al.(2020)Han, Avron, and Shin]{han2020polynomial}
Insu Han, Haim Avron, and Jinwoo Shin.
\newblock Polynomial tensor sketch for element-wise function of low-rank
  matrix.
\newblock In \emph{International Conference on Machine Learning}, pages
  3984--3993. PMLR, 2020.

\bibitem[Impagliazzo and Paturi(2001)]{impagliazzo2001complexity}
Russell Impagliazzo and Ramamohan Paturi.
\newblock On the complexity of k-{SAT}.
\newblock \emph{Journal of Computer and System Sciences}, 62\penalty0
  (2):\penalty0 367--375, 2001.

\bibitem[Jiang et~al.(2021)Jiang, Li, Sun, Wang, and Woodruff]{jiang2021single}
Yifei Jiang, Yi~Li, Yiming Sun, Jiaxin Wang, and David Woodruff.
\newblock Single pass entrywise-transformed low rank approximation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  4982--4991. PMLR, 2021.

\bibitem[Kacham et~al.(2023)Kacham, Mirrokni, and
  Zhong]{kacham2023polysketchformer}
Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong.
\newblock Polysketchformer: Fast transformers via sketches for polynomial
  kernels.
\newblock \emph{arXiv preprint arXiv:2310.01655}, 2023.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{katharopoulos2020transformerrnn}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are {RNNs}: Fast autoregressive transformers with linear
  attention.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 5156--5165. {PMLR},
  2020.
\newblock URL \url{http://proceedings.mlr.press/v119/katharopoulos20a.html}.

\bibitem[Keles et~al.(2023)Keles, Wijewardena, and
  Hegde]{keles2022computational}
Feyza~Duman Keles, Pruthuvi~Mahesakya Wijewardena, and Chinmay Hegde.
\newblock On the computational complexity of self-attention.
\newblock In \emph{International Conference on Algorithmic Learning Theory
  (ALT)}, 2023.

\bibitem[Levy and Goldberg(2014)]{levy2014neural}
Omer Levy and Yoav Goldberg.
\newblock Neural word embedding as implicit matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Li et~al.(2015)Li, Zhu, and Miao]{li2015generative}
Shaohua Li, Jun Zhu, and Chunyan Miao.
\newblock A generative word embedding model and its low rank positive
  semidefinite solution.
\newblock In \emph{Proceedings of the 2015 Conference on Empirical Methods in
  Natural Language Processing}, pages 1599--1609, 2015.

\bibitem[Liang et~al.(2020)Liang, Song, Wang, Yang, and
  Yang]{liang2020sketching}
Yingyu Liang, Zhao Song, Mengdi Wang, Lin Yang, and Xin Yang.
\newblock Sketching transformed matrices with applications to natural language
  processing.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 467--481. PMLR, 2020.

\bibitem[Likhosherstov et~al.(2022)Likhosherstov, Choromanski, Dubey, Liu,
  Sarlos, and Weller]{likhosherstov2022chef}
Valerii Likhosherstov, Krzysztof~M Choromanski, Kumar~Avinava Dubey, Frederick
  Liu, Tamas Sarlos, and Adrian Weller.
\newblock Chefs' random tables: Non-trigonometric random features.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  34559--34573, 2022.

\bibitem[Lokshtanov et~al.(2013)Lokshtanov, Marx, Saurabh,
  et~al.]{lokshtanov2013lower}
Daniel Lokshtanov, D{\'a}niel Marx, Saket Saurabh, et~al.
\newblock Lower bounds based on the exponential time hypothesis.
\newblock \emph{Bulletin of EATCS}, 3\penalty0 (105), 2013.

\bibitem[Musco and Musco(2017)]{musco2017recursive}
Cameron Musco and Christopher Musco.
\newblock Recursive sampling for the {N}ystrom method.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Musco and Woodruff(2017)]{musco2017sublinear}
Cameron Musco and David~P Woodruff.
\newblock Sublinear time low-rank approximation of positive semidefinite
  matrices.
\newblock In \emph{2017 IEEE 58th Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 672--683. IEEE, 2017.

\bibitem[Rahimi and Recht(2007)]{rahimi2007random}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2007.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Bahri, and Metzler]{tay2022efficient}
Yi~Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
\newblock Efficient transformers: A survey.
\newblock \emph{ACM Computing Surveys}, 55\penalty0 (6):\penalty0 1--28, 2022.

\bibitem[Tsai et~al.(2019)Tsai, Bai, Yamada, Morency, and
  Salakhutdinov]{tsai2019transformerkernel}
Yao{-}Hung~Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis{-}Philippe Morency,
  and Ruslan Salakhutdinov.
\newblock Transformer dissection: An unified understanding for transformer's
  attention via the lens of kernel.
\newblock In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors,
  \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural
  Language Processing and the 9th International Joint Conference on Natural
  Language Processing, {EMNLP-IJCNLP} 2019, Hong Kong, China, November 3-7,
  2019}, pages 4343--4352. Association for Computational Linguistics, 2019.
\newblock \doi{10.18653/v1/D19-1443}.
\newblock URL \url{https://doi.org/10.18653/v1/D19-1443}.

\bibitem[Vassilevska~Williams(2015)]{vassilevska2015hardness}
Virginia Vassilevska~Williams.
\newblock Hardness of easy problems: Basing hardness on popular conjectures
  such as the strong exponential time hypothesis (invited talk).
\newblock In \emph{10th International Symposium on Parameterized and Exact
  Computation (IPEC 2015)}. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik,
  2015.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems 2017},
  pages 5998--6008, 2017.

\bibitem[Williams(2005)]{w05}
Ryan Williams.
\newblock A new algorithm for optimal 2-constraint satisfaction and its
  implications.
\newblock \emph{Theor. Comput. Sci.}, 348\penalty0 (2-3):\penalty0 357--365,
  2005.

\bibitem[Woodruff(2014)]{woodruff2014sketching}
David~P Woodruff.
\newblock Sketching as a tool for numerical linear algebra.
\newblock \emph{Foundations and Trends{\textregistered} in Theoretical Computer
  Science}, 10\penalty0 (1--2):\penalty0 1--157, 2014.

\bibitem[Woodruff and Zhong(2016)]{woodruff2016distributed}
David~P Woodruff and Peilin Zhong.
\newblock Distributed low rank approximation of implicit functions of a matrix.
\newblock In \emph{2016 IEEE 32nd International Conference on Data Engineering
  (ICDE)}, pages 847--858. IEEE, 2016.

\end{thebibliography}
