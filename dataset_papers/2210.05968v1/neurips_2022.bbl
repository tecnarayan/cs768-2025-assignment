\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Athalye et~al.(2018)Athalye, Engstrom, Ilyas, and
  Kwok]{athalye2018synthesizing}
Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok.
\newblock Synthesizing robust adversarial examples.
\newblock In \emph{International conference on machine learning}, pages
  284--293. PMLR, 2018.

\bibitem[Cheng et~al.(2019)Cheng, Le, Chen, Zhang, Yi, and
  Hsieh]{cheng2018queryefficient}
Minhao Cheng, Thong Le, Pin-Yu Chen, Huan Zhang, JinFeng Yi, and Cho-Jui Hsieh.
\newblock Query-efficient hard-label black-box attack: An optimization-based
  approach.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Cheng et~al.(2020)Cheng, Singh, Chen, Chen, Liu, and
  Hsieh]{Cheng2020Sign-OPT}
Minhao Cheng, Simranjit Singh, Patrick~H. Chen, Pin-Yu Chen, Sijia Liu, and
  Cho-Jui Hsieh.
\newblock Sign-opt: A query-efficient hard-label adversarial attack.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Demontis et~al.(2019)Demontis, Melis, Pintor, Jagielski, Biggio,
  Oprea, Nita-Rotaru, and Roli]{demontis2019adversarial}
Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Biggio,
  Alina Oprea, Cristina Nita-Rotaru, and Fabio Roli.
\newblock Why do adversarial attacks transfer? explaining transferability of
  evasion and poisoning attacks.
\newblock In \emph{28th USENIX security symposium (USENIX security 19)}, pages
  321--338, 2019.

\bibitem[Dong et~al.(2018)Dong, Liao, Pang, Su, Zhu, Hu, and
  Li]{dong2018boosting}
Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and
  Jianguo Li.
\newblock Boosting adversarial attacks with momentum.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 9185--9193, 2018.

\bibitem[Dong et~al.(2019)Dong, Pang, Su, and Zhu]{dong2019evading}
Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu.
\newblock Evading defenses to transferable adversarial examples by
  translation-invariant attacks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 4312--4321, 2019.

\bibitem[Dong et~al.(2020)Dong, Fu, Yang, Pang, Su, Xiao, and
  Zhu]{dong2020benchmarking}
Yinpeng Dong, Qi-An Fu, Xiao Yang, Tianyu Pang, Hang Su, Zihao Xiao, and Jun
  Zhu.
\newblock Benchmarking adversarial robustness on image classification.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 321--331, 2020.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021an}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Fan et~al.(2020)Fan, Wu, Li, Zhang, Li, Li, and Yang]{fan2020sparse}
Yanbo Fan, Baoyuan Wu, Tuanhui Li, Yong Zhang, Mingyang Li, Zhifeng Li, and
  Yujiu Yang.
\newblock Sparse adversarial attack via perturbation factorization.
\newblock In \emph{European conference on computer vision}, pages 35--50.
  Springer, 2020.

\bibitem[Feng et~al.(2022)Feng, Wu, Fan, Liu, Li, and Xia]{Feng_2022_CVPR}
Yan Feng, Baoyuan Wu, Yanbo Fan, Li~Liu, Zhifeng Li, and Shu-Tao Xia.
\newblock Boosting black-box attack with partially transferred conditional
  adversarial distribution.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 15095--15104, June 2022.

\bibitem[Gao et~al.(2020)Gao, Zhang, Song, Liu, and Shen]{gao2020patch}
Lianli Gao, Qilong Zhang, Jingkuan Song, Xianglong Liu, and Heng~Tao Shen.
\newblock Patch-wise attack for fooling deep neural network.
\newblock In \emph{European Conference on Computer Vision}, pages 307--322.
  Springer, 2020.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem[Gubri et~al.(2022)Gubri, Cordy, Papadakis, Traon, and
  Sen]{gubri2022lgv}
Martin Gubri, Maxime Cordy, Mike Papadakis, Yves~Le Traon, and Koushik Sen.
\newblock Lgv: Boosting adversarial example transferability from large
  geometric vicinity.
\newblock \emph{arXiv preprint arXiv:2207.13129}, 2022.

\bibitem[Guo et~al.(2020)Guo, Li, and Chen]{GuoLinBP}
Yiwen Guo, Qizhang Li, and Hao Chen.
\newblock Backpropagating linearly improves transferability of adversarial
  examples.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 85--95, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem[Huang et~al.(2019)Huang, Katsman, He, Gu, Belongie, and
  Lim]{huang2019enhancing}
Qian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie, and Ser-Nam Lim.
\newblock Enhancing adversarial example transferability with an intermediate
  level attack.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 4733--4742, 2019.

\bibitem[Ilyas et~al.(2018)Ilyas, Engstrom, Athalye, and Lin]{ilyas2018black}
Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin.
\newblock Black-box adversarial attacks with limited queries and information.
\newblock In \emph{International Conference on Machine Learning}, pages
  2137--2146. PMLR, 2018.

\bibitem[Inkawhich et~al.(2020{\natexlab{a}})Inkawhich, Liang, Carin, and
  Chen]{Inkawhich2020Transferable}
Nathan Inkawhich, Kevin Liang, Lawrence Carin, and Yiran Chen.
\newblock Transferable perturbations of deep feature distributions.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.

\bibitem[Inkawhich et~al.(2020{\natexlab{b}})Inkawhich, Liang, Wang, Inkawhich,
  Carin, and Chen]{inkawhich2020perturbing}
Nathan Inkawhich, Kevin Liang, Binghui Wang, Matthew Inkawhich, Lawrence Carin,
  and Yiran Chen.
\newblock Perturbing across the feature hierarchy to improve standard and
  strict blackbox attack transferability.
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{b}}.

\bibitem[Kurakin et~al.(2018)Kurakin, Goodfellow, and Bengio]{Kurakin_2018}
Alexey Kurakin, Ian~J. Goodfellow, and Samy Bengio.
\newblock Adversarial examples in the physical world.
\newblock \emph{Artificial Intelligence Safety and Security}, page 99â€“112,
  Jul 2018.
\newblock \doi{10.1201/9781351251389-8}.

\bibitem[Li et~al.(2020)Li, Bai, Zhou, Xie, Zhang, and Yuille]{li2020learning}
Yingwei Li, Song Bai, Yuyin Zhou, Cihang Xie, Zhishuai Zhang, and Alan Yuille.
\newblock Learning transferable adversarial examples via ghost networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 11458--11465, 2020.

\bibitem[Liang et~al.(2021)Liang, Wu, Fan, Wei, and Cao]{liang2021parallel}
Siyuan Liang, Baoyuan Wu, Yanbo Fan, Xingxing Wei, and Xiaochun Cao.
\newblock Parallel rectangle flip attack: A query-based black-box attack
  against object detection.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 7697--7707, 2021.

\bibitem[Lin et~al.(2020)Lin, Song, He, Wang, and Hopcroft]{Lin2020Nesterov}
Jiadong Lin, Chuanbiao Song, Kun He, Liwei Wang, and John~E. Hopcroft.
\newblock Nesterov accelerated gradient and scale invariance for adversarial
  attacks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Liu et~al.(2021)Liu, Gao, Zhang, Meng, and Lin]{liu2021investigating}
Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin.
\newblock Investigating bi-level optimization for learning and vision from a
  unified perspective: A survey and beyond.
\newblock \emph{arXiv preprint arXiv:2101.11517}, 2021.

\bibitem[Liu et~al.(2016)Liu, Chen, Liu, and Song]{liu2016delving}
Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song.
\newblock Delving into transferable adversarial examples and black-box attacks.
\newblock \emph{arXiv preprint arXiv:1611.02770}, 2016.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2018towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Naseer et~al.(2019)Naseer, Khan, Khan, Shahbaz~Khan, and
  Porikli]{naseer2019cross}
Muhammad~Muzammal Naseer, Salman~H Khan, Muhammad~Haris Khan, Fahad
  Shahbaz~Khan, and Fatih Porikli.
\newblock Cross-domain transferability of adversarial perturbations.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 12905--12915, 2019.

\bibitem[Naseer et~al.(2020)Naseer, Khan, Hayat, Khan, and
  Porikli]{naseer2020self}
Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad~Shahbaz Khan, and Fatih
  Porikli.
\newblock A self-supervised approach for adversarial robustness.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 262--271, 2020.

\bibitem[Naseer et~al.(2021)Naseer, Khan, Hayat, Khan, and
  Porikli]{naseer2021generating}
Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad~Shahbaz Khan, and Fatih
  Porikli.
\newblock On generating transferable targeted perturbations.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 7708--7717, 2021.

\bibitem[Poursaeed et~al.(2018)Poursaeed, Katsman, Gao, and
  Belongie]{poursaeed2018generative}
Omid Poursaeed, Isay Katsman, Bicheng Gao, and Serge Belongie.
\newblock Generative adversarial perturbations.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4422--4431, 2018.

\bibitem[Qin et~al.(2021)Qin, Fan, Zha, and Wu]{qin2021random}
Zeyu Qin, Yanbo Fan, Hongyuan Zha, and Baoyuan Wu.
\newblock Random noise defense against query-based black-box attacks.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan,
  editors, \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Salman et~al.(2020)Salman, Ilyas, Engstrom, Kapoor, and
  Madry]{NEURIPS2020_24357dd0}
Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry.
\newblock Do adversarially robust imagenet models transfer better?
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 3533--3545. Curran Associates, Inc., 2020.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6199}, 2013.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem[Szegedy et~al.(2017)Szegedy, Ioffe, Vanhoucke, and
  Alemi]{szegedy2017inception}
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander~A Alemi.
\newblock Inception-v4, inception-resnet and the impact of residual connections
  on learning.
\newblock In \emph{Thirty-first AAAI conference on artificial intelligence},
  2017.

\bibitem[TramÃ¨r et~al.(2018)TramÃ¨r, Kurakin, Papernot, Goodfellow, Boneh, and
  McDaniel]{tramer2018ensemble}
Florian TramÃ¨r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh,
  and Patrick McDaniel.
\newblock Ensemble adversarial training: Attacks and defenses.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Zhang, Liu, Chen, Xu, Fardad, and
  Li]{wang2021adversarial}
Jingkang Wang, Tianyun Zhang, Sijia Liu, Pin-Yu Chen, Jiacen Xu, Makan Fardad,
  and Bo~Li.
\newblock Adversarial attack generation empowered by min-max optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Wang and He(2021)]{Wang_2021_CVPR}
Xiaosen Wang and Kun He.
\newblock Enhancing the transferability of adversarial attacks through variance
  tuning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 1924--1933, June 2021.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, He, Wang, and He]{wang2021admix}
Xiaosen Wang, Xuanran He, Jingdong Wang, and Kun He.
\newblock Admix: Enhancing the transferability of adversarial attacks.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 16158--16167, 2021{\natexlab{b}}.

\bibitem[Wang et~al.(2021{\natexlab{c}})Wang, Lin, Hu, Wang, and
  He]{wang2021boosting}
Xiaosen Wang, Jiadong Lin, Han Hu, Jingdong Wang, and Kun He.
\newblock Boosting adversarial transferability through enhanced momentum.
\newblock \emph{arXiv preprint arXiv:2103.10609}, 2021{\natexlab{c}}.

\bibitem[Wu et~al.(2020)Wu, Wang, Xia, Bailey, and Ma]{Wu2020Skip}
Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun Ma.
\newblock Skip connections matter: On the transferability of adversarial
  examples generated with resnets.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Xiao et~al.(2022{\natexlab{a}})Xiao, Fan, Sun, and
  Luo]{xiao2022adversarial}
Jiancong Xiao, Yanbo Fan, Ruoyu Sun, and Zhi-Quan Luo.
\newblock Adversarial rademacher complexity of deep neural networks,
  2022{\natexlab{a}}.

\bibitem[Xiao et~al.(2022{\natexlab{b}})Xiao, Fan, Sun, Wang, and
  Luo]{xiao2022stability}
Jiancong Xiao, Yanbo Fan, Ruoyu Sun, Jue Wang, and Zhi-Quan Luo.
\newblock Stability analysis and generalization bounds of adversarial training.
\newblock \emph{arXiv preprint arXiv:2210.00960}, 2022{\natexlab{b}}.

\bibitem[Xie et~al.(2018)Xie, Wang, Zhang, Ren, and Yuille]{xie2018mitigating}
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille.
\newblock Mitigating adversarial effects through randomization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Xie et~al.(2019{\natexlab{a}})Xie, Wu, Maaten, Yuille, and
  He]{xie2019feature}
Cihang Xie, Yuxin Wu, Laurens van~der Maaten, Alan~L Yuille, and Kaiming He.
\newblock Feature denoising for improving adversarial robustness.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 501--509, 2019{\natexlab{a}}.

\bibitem[Xie et~al.(2019{\natexlab{b}})Xie, Zhang, Zhou, Bai, Wang, Ren, and
  Yuille]{xie2019improving}
Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and
  Alan~L Yuille.
\newblock Improving transferability of adversarial examples with input
  diversity.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 2730--2739, 2019{\natexlab{b}}.

\bibitem[Zhao et~al.(2020)Zhao, Liu, and Larson]{zhao2020success}
Zhengyu Zhao, Zhuoran Liu, and Martha Larson.
\newblock On success and simplicity: A second look at transferable targeted
  attacks.
\newblock \emph{arXiv preprint arXiv:2012.11207}, 2020.

\bibitem[Zheng et~al.(2022)Zheng, Fan, Wu, Zhang, Wang, and
  Pan]{zheng2022robust}
Xin Zheng, Yanbo Fan, Baoyuan Wu, Yong Zhang, Jue Wang, and Shirui Pan.
\newblock Robust physical-world attacks on face recognition.
\newblock \emph{Pattern Recognition}, page 109009, 2022.

\bibitem[Zoph et~al.(2018)Zoph, Vasudevan, Shlens, and Le]{zoph2018learning}
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc~V Le.
\newblock Learning transferable architectures for scalable image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 8697--8710, 2018.

\end{thebibliography}
