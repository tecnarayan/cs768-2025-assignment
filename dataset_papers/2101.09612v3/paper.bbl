\begin{thebibliography}{18}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{AllenZhuEtal2018}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Arora, S., Du, S., Hu, W., Li, Z., and Wang, R.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Brutzkus et~al.(2018)Brutzkus, Globerson, Malach, and
  Shalev-Shwartz]{BrutzkusEtal2018}
Brutzkus, A., Globerson, A., Malach, E., and Shalev-Shwartz, S.
\newblock Sgd learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Chen et~al.(2019)Chen, Cao, Zou, and Gu]{ChenCaoZouGu2019}
Chen, Z., Cao, Y., Zou, D., and Gu, Q.
\newblock How much over-parameterization is sufficient to learn deep relu
  networks?, 2019.
\newblock \texttt{arXiv:1911.12360}.

\bibitem[Daniely(2017)]{Amit2017}
Daniely, A.
\newblock Sgd learns the conjugate kernel class of the network.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Davidson \& Szarek(2001)Davidson and Szarek]{Davidson2001}
Davidson, K.~R. and Szarek, S.~J.
\newblock Local operator theory, random matrices and banach spaces.
\newblock \emph{Handbook of the geometry of Banach spaces}, 1\penalty0
  (140):\penalty0 317--366, 2001.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and Zhai]{DuEtal2019}
Du, S.~S., Lee, J.~D., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2019{\natexlab{a}}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos, and
  Singh]{DuEtal2018_ICLR}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019{\natexlab{b}}.

\bibitem[Huang \& Yau(2020)Huang and Yau]{HuangYau2020}
Huang, J. and Yau, H.-T.
\newblock Dynamics of deep neural networks and neural tangent hierarchy.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{JacotEtc2018}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem[Nguyen \& Mondelli(2020)Nguyen and Mondelli]{QuynhMarco2020}
Nguyen, Q. and Mondelli, M.
\newblock Global convergence of deep networks with one wide layer followed by
  pyramidal topology.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Nguyen et~al.(2021)Nguyen, Mondelli, and Montufar]{QuynhNTK2021}
Nguyen, Q., Mondelli, M., and Montufar, G.
\newblock Tight bounds on the smallest eigenvalue of the neural tangent kernel
  for deep relu networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Oymak \& Soltanolkotabi(2020)Oymak and Soltanolkotabi]{OymakMahdi2019}
Oymak, S. and Soltanolkotabi, M.
\newblock Towards moderate overparameterization: global convergence guarantees
  for training shallow neural networks.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory}, 2020.

\bibitem[Polyak(1963)]{Polyak1963}
Polyak, B.~T.
\newblock Gradient methods for minimizing functionals.
\newblock \emph{Zh. Vychisl. Mat. Mat. Fiz.}, 3\penalty0 (4), 1963.

\bibitem[Song \& Yang(2020)Song and Yang]{SongYang2020}
Song, Z. and Yang, X.
\newblock Quadratic suffices for over-parametrization via matrix chernoff
  bound, 2020.
\newblock \texttt{arXiv:1906.03593}.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{SoudryEtal2018}
Soudry, D., Hoffer, E., Nacson, M.~S., Gunasekar, S., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 19:\penalty0
  2822--2878, 2018.

\bibitem[Vershynin(2018)]{vershynin2018high}
Vershynin, R.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}.
\newblock Cambridge university press, 2018.

\bibitem[Zou \& Gu(2019)Zou and Gu]{ZouGu2019}
Zou, D. and Gu, Q.
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2019.

\end{thebibliography}
