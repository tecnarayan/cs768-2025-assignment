@article{batchmodeRL,
  author  = {Damien  Ernst and Pierre  Geurts and Louis  Wehenkel},
  title   = {Tree-Based Batch Mode Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {18},
  pages   = {503-556},
  url     = {http://jmlr.org/papers/v6/ernst05a.html}
}


@incollection{mitchell2020offline,
      title={Offline Meta-Reinforcement Learning with Advantage Weighting}, 
      author={Eric Mitchell and Rafael Rafailov and Xue Bin Peng and Sergey Levine and Chelsea Finn},
      year={2020},
      eprint={2008.06043},
      booktitle={arXiv},
      primaryClass={cs.LG}
}

@incollection{warmstart,
      title={On Warm-Starting Neural Network Training}, 
      author={Jordan T. Ash and Ryan P. Adams},
      year={2020},
      booktitle={Advances in Neural Information Processing Systems (NeurIPS)}
}

@article{transientnonstat,
  title={Transient non-stationarity and generalisation in deep reinforcement learning},
  author={Igl, Maximilian and Farquhar, Gregory and Luketina, Jelena and B{\"o}hmer, Wendelin and Whiteson, Shimon},
  journal={arXiv preprint arXiv:2006.05826},
  year={2020}
}

@incollection{
brac,
title={Behavior Regularized Offline Reinforcement Learning},
author={Yifan Wu and George Tucker and Ofir Nachum},
year={2021},
booktitle={To Appear: The International Conference on Learning Representations (ICLR)}

}

@incollection{cql,
  author       = {Aviral Kumar and Aurick Zhou and George Tucker and Sergey Levine},
  title        = {Conservative Q-Learning for Offline Reinforcement Learning},
  booktitle   = {Advances in Neural Information Processing Systems},
  year = {2020}
}

@inproceedings{bcq,
  title={Off-Policy Deep Reinforcement Learning without Exploration},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle={International Conference on Machine Learning},
  pages={2052--2062},
  year={2019}
}

@misc{offlinerl_survey,
      title={Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems}, 
      author={Sergey Levine and Aviral Kumar and George Tucker and Justin Fu},
      year={2020},
      eprint={2005.01643},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{tmcl,

  title={Trajectory-wise Multiple Choice Learning for Dynamics Generalization in Reinforcement Learning},

  author={Seo, Younggyo and Lee, Kimin and Clavera, Ignasi and Kurutach, Thanard and Shin, Jinwoo and Abbeel, Pieter},

  booktitle={Advances in Neural Information Processing Systems},

  year={2020}

}

@inproceedings{lompo,

  title={Offline Reinforcement Learning from Images with Latent Space Models},

  author={Rafael Rafailov and Tianhe Yu and	Aravind Rajeswaran and Chelsea Finn},

  booktitle={Offline Reinforcement Learning Workshop at Neural Information Processing Systems},

  year={2020}

}

@article{sac-v2,
  author    = {Tuomas Haarnoja and
               Aurick Zhou and
               Kristian Hartikainen and
               George Tucker and
               Sehoon Ha and
               Jie Tan and
               Vikash Kumar and
               Henry Zhu and
               Abhishek Gupta and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Soft Actor-Critic Algorithms and Applications},
  journal   = {CoRR},
  volume    = {abs/1812.05905},
  year      = {2018}
}


@INPROCEEDINGS{nixweigend, 
author={D. A. {Nix} and A. S. {Weigend}}, 
booktitle={Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)}, title={Estimating the mean and variance of the target probability distribution}, year={1994}, volume={1}, number={}, pages={55-60 vol.1},}


@incollection{mopo,
  title={MOPO: Model-based Offline Policy Optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}



@article{dexterity,
author = {OpenAI: Marcin Andrychowicz and Bowen Baker and Maciek Chociej and Rafal Józefowicz and Bob McGrew and Jakub Pachocki and Arthur Petron and Matthias Plappert and Glenn Powell and Alex Ray and Jonas Schneider and Szymon Sidor and Josh Tobin and Peter Welinder and Lilian Weng and Wojciech Zaremba},
title ={Learning dexterous in-hand manipulation},
journal = {The International Journal of Robotics Research},
volume = {39},
number = {1},
pages = {3-20},
year = {2020}
    }


@misc{antonova2017reinforcement,
      title={Reinforcement Learning for Pivoting Task}, 
      author={Rika Antonova and Silvia Cruciani and Christian Smith and Danica Kragic},
      year={2017},
      eprint={1703.00472},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}


@article{rubics_cube,
  author    = {OpenAI and
               Ilge Akkaya and
               Marcin Andrychowicz and
               Maciek Chociej and
               Mateusz Litwin and
               Bob McGrew and
               Arthur Petron and
               Alex Paino and
               Matthias Plappert and
               Glenn Powell and
               Raphael Ribas and
               Jonas Schneider and
               Nikolas Tezak and
               Jerry Tworek and
               Peter Welinder and
               Lilian Weng and
               Qiming Yuan and
               Wojciech Zaremba and
               Lei Zhang},
  title     = {Solving Rubik's Cube with a Robot Hand},
  journal   = {CoRR},
  volume    = {abs/1910.07113},
  year      = {2019},
}

@inproceedings{
Engstrom2020Implementation,
title={Implementation Matters in Deep RL: A Case Study on PPO and TRPO},
author={Logan Engstrom and Andrew Ilyas and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},
booktitle={International Conference on Learning Representations},
year={2020},
}

@inproceedings{pilco,
author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
title = {PILCO: A Model-Based and Data-Efficient Approach to Policy Search},
year = {2011},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {465–472},
numpages = {8},
}

@inproceedings{
andrychowicz2021what,
title={What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study},
author={Marcin Andrychowicz and Anton Raichuk and Piotr Sta{\'n}czyk and Manu Orsini and Sertan Girgin and Rapha{\"e}l Marinier and Leonard Hussenot and Matthieu Geist and Olivier Pietquin and Marcin Michalski and Sylvain Gelly and Olivier Bachem},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=nIAxjsniDzg}
}

@incollection{rajendran2020metalearning,
      title={Meta-Learning Requires Meta-Augmentation}, 
      author={Janarthanan Rajendran and Alex Irpan and Eric Jang},
      year={2020},
  booktitle={Advances in Neural Information Processing Systems},
}

@InProceedings{pbtbt,
  title = 	 { On the Importance of Hyperparameter Optimization for Model-based Reinforcement Learning },
  author =       {Zhang, Baohe and Rajan, Raghu and Pineda, Luis and Lambert, Nathan and Biedenkapp, Andr{\'e} and Chua, Kurtland and Hutter, Frank and Calandra, Roberto},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  year = 	 {2021},
  
}

@incollection{steve,
author = {Buckman, Jacob and Hafner, Danijar and Tucker, George and Brevdo, Eugene and Lee, Honglak},
year = {2018},
month = {07},
booktitle = {Advances in Neural Information Processing Systems},
title = {Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion}
}

@inproceedings{
dreamer,
title={Dream to Control: Learning Behaviors by Latent Imagination},
author={Danijar Hafner and Timothy Lillicrap and Jimmy Ba and Mohammad Norouzi},
booktitle={International Conference on Learning Representations},
year={2020},
}


@article{dyna,
 author = {Sutton, Richard S.},
 title = {Dyna, an Integrated Architecture for Learning, Planning, and Reacting},
 year = {1991},
 issue_date = {July 1991},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 volume = {2},
 number = {4},
 issn = {0163-5719},
 url = {https://doi.org/10.1145/122344.122377},
 doi = {10.1145/122344.122377},
 journal = {SIGART Bull.},
 month = jul,
 pages = {160–163},
 numpages = {4}
}


@inproceedings{
zhou2018environment,
title={Environment Probing Interaction Policies},
author={Wenxuan Zhou and Lerrel Pinto and Abhinav Gupta},
booktitle={International Conference on Learning Representations},
year={2019},
}

@inproceedings{dynamics_gen_sim2real,
  author    = {Wenhao Yu and
               Jie Tan and
               C. Karen Liu and
               Greg Turk},
  editor    = {Nancy M. Amato and
               Siddhartha S. Srinivasa and
               Nora Ayanian and
               Scott Kuindersma},
  title     = {Preparing for the Unknown: Learning a Universal Policy with Online
               System Identification},
  booktitle = {Robotics: Science and Systems XIII},
  year      = {2017},
}

@inproceedings{sim2realdynamics,
  author    = {Xue Bin Peng and
               Marcin Andrychowicz and
               Wojciech Zaremba and
               Pieter Abbeel},
  title     = {Sim-to-Real Transfer of Robotic Control with Dynamics Randomization},
  booktitle = {{IEEE} International Conference on Robotics and Automation, {ICRA}},
  year      = {2018},
}

@InProceedings{curl, 
title = {{CURL}: Contrastive Unsupervised Representations for Reinforcement Learning}, author = {Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter}, 
booktitle = {Proceedings of the 37th International Conference on Machine Learning},  
year = {2020}
}

@inproceedings{
Song2020Observational,
title={Observational Overfitting in Reinforcement Learning},
author={Xingyou Song and Yiding Jiang and Stephen Tu and Yilun Du and Behnam Neyshabur},
booktitle={International Conference on Learning Representations},
year={2020},
}

@INPROCEEDINGS{domain_randomization,
  author={J. {Tobin} and R. {Fong} and A. {Ray} and J. {Schneider} and W. {Zaremba} and P. {Abbeel}},
  booktitle={2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Domain randomization for transferring deep neural networks from simulation to the real world}, 
  year={2017},
  volume={},
  number={},
  pages={23-30},
  doi={10.1109/IROS.2017.8202133}}

@incollection{james2017transferring,
      title={Transferring End-to-End Visuomotor Control from Simulation to Real World for a Multi-Stage Task}, 
      author={Stephen James and Andrew J. Davison and Edward Johns},
      year={2017},
      booktitle={1st Conference on Robot Learning },
}

@article{gym,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@inproceedings{
varibad,
title={VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning},
author={Luisa Zintgraf and Kyriacos Shiarlis and Maximilian Igl and Sebastian Schulze and Yarin Gal and Katja Hofmann and Shimon Whiteson},
booktitle={International Conference on Learning Representations},
year={2020},
}

@inproceedings{pearl,
  author    = {Kate Rakelly and
               Aurick Zhou and
               Chelsea Finn and
               Sergey Levine and
               Deirdre Quillen},
  title     = {Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic
               Context Variables},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  volume    = {97},
  pages     = {5331--5340},
  publisher = {{PMLR}},
  year      = {2019}
}

@article{peng2021linear,
      title={Linear Representation Meta-Reinforcement Learning for Instant Adaptation}, 
      author={Matt Peng and Banghua Zhu and Jiantao Jiao},
      year={2021},
      eprint={2101.04750},
      archivePrefix={arXiv},
      journal={CoRR}
}

@incollection{drq,
      title={Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels}, 
      author={Ilya Kostrikov and Denis Yarats and Rob Fergus},
        year      = {2021},
        booktitle={International Conference on Learning Representations},
}

@article{ucb_drac,
  author    = {Roberta Raileanu and
               Max Goldstein and
               Denis Yarats and
               Ilya Kostrikov and
               Rob Fergus},
  title     = {Automatic Data Augmentation for Generalization in Deep Reinforcement
               Learning},
  journal   = {CoRR},
  volume    = {abs/2006.12862},
  year      = {2020},
}

@misc{
d4rl,
title={D4{\{}RL{\}}: Datasets for Deep Data-Driven Reinforcement Learning},
author={Justin Fu and Aviral Kumar and Ofir Nachum and George Tucker and Sergey Levine},
year={2021},
}

@incollection{pitis2020counterfactual,
      title={Counterfactual Data Augmentation using Locally Factored Dynamics}, 
      author={Silviu Pitis and Elliot Creager and Animesh Garg},
      year={2020},
    booktitle={Advances in Neural Information Processing Systems},

}

@incollection{rad,
  title={Reinforcement Learming with Augmented Data},
  author={Laskin, Michael and Lee, Kimin and Stooke, Adam and Pinto, Lerrel and Abbeel, Pieter and Srinivas, Aravind},
  year={2020},
  booktitle = {Advances in Neural Information Processing Systems 33},
}


@inproceedings{worldmodels,
 author = {Ha, David and Schmidhuber, J\"{u}rgen},
 title = {Recurrent World Models Facilitate Policy Evolution},
 booktitle = {Proceedings of the 32Nd International Conference on Neural Information Processing Systems},
 series = {NeurIPS'18},
 year = {2018},
 pages = {2455--2467},
} 


@incollection{rp1,
    title={Ready Policy One: World Building Through Active Learning},
    author={Philip Ball and Jack Parker-Holder and Aldo Pacchiano and Krzysztof Choromanski and Stephen Roberts},
    year={2020},
    booktitle = {Proceedings of the 37th International Conference on Machine Learning,
               {ICML}}
}

@misc{jaderberg2017population,
      title={Population Based Training of Neural Networks}, 
      author={Max Jaderberg and Valentin Dalibard and Simon Osindero and Wojciech M. Czarnecki and Jeff Donahue and Ali Razavi and Oriol Vinyals and Tim Green and Iain Dunning and Karen Simonyan and Chrisantha Fernando and Koray Kavukcuoglu},
      year={2017},
      eprint={1711.09846},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{pb2,
 author = {Parker-Holder, Jack and Nguyen, Vu and Roberts, Stephen J},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {17200--17211},
 publisher = {Curran Associates, Inc.},
 title = {Provably Efficient Online Hyperparameter Optimization with Population-Based Bandits},
 url = {https://proceedings.neurips.cc/paper/2020/file/c7af0926b294e47e52e46cfebe173f20-Paper.pdf},
 volume = {33},
 year = {2020}
}



@inproceedings{
METRPO,
title={Model-Ensemble Trust-Region Policy Optimization},
author={Thanard Kurutach and Ignasi Clavera and Yan Duan and Aviv Tamar and Pieter Abbeel},
booktitle={International Conference on Learning Representations},
year={2018},
}

@inproceedings{
hansen2021selfsupervised,
title={Self-Supervised Policy Adaptation during Deployment},
author={Nicklas Hansen and Rishabh Jangir and Yu Sun and Guillem Aleny{\`a} and Pieter Abbeel and Alexei A Efros and Lerrel Pinto and Xiaolong Wang},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=o_V-MjyyGV_}
}


@article{muzero,
  author    = {Julian Schrittwieser and
               Ioannis Antonoglou and
               Thomas Hubert and
               Karen Simonyan and
               Laurent Sifre and
               Simon Schmitt and
               Arthur Guez and
               Edward Lockhart and
               Demis Hassabis and
               Thore Graepel and
               Timothy P. Lillicrap and
               David Silver},
  title     = {{Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model}},
  journal   = {CoRR},
  volume    = {abs/1911.08265},
  year      = {2019}
}

@inproceedings{
simple,
title={Model Based Reinforcement Learning for {A}tari},
author={Lukasz Kaiser and Mohammad Babaeizadeh and Piotr Milos and Blazej Osiński and Roy H Campbell and Konrad Czechowski and Dumitru Erhan and Chelsea Finn and Piotr Kozakowski and Sergey Levine and Afroz Mohiuddin and Ryan Sepassi and George Tucker and Henryk Michalewski},
booktitle={International Conference on Learning Representations},
year={2020},
}


@incollection{pets,
title = {Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models},
author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems 31},
pages = {4754--4765},
year = {2018},
}


@incollection{mbpo,
  author = {Michael Janner and Justin Fu and Marvin Zhang and Sergey Levine},
  title = {When to Trust Your Model: Model-Based Policy Optimization},
    booktitle={Advances in Neural Information Processing Systems},
  year = {2019}
}

@incollection{morel,
      title={MOReL : Model-Based Offline Reinforcement Learning}, 
      author={Rahul Kidambi and Aravind Rajeswaran and Praneeth Netrapalli and Thorsten Joachims},
      year={2020},
        booktitle={Advances in Neural Information Processing Systems},

}

@inproceedings{mbmpo,
  author    = {Ignasi Clavera and
               Jonas Rothfuss and
               John Schulman and
               Yasuhiro Fujita and
               Tamim Asfour and
               Pieter Abbeel},
  title     = {Model-Based Reinforcement Learning via Meta-Policy Optimization},
  booktitle = {2nd Annual Conference on Robot Learning, CoRL 2018, Z{\"{u}}rich,
               Switzerland, 29-31 October 2018, Proceedings},
  series    = {Proceedings of Machine Learning Research},
  volume    = {87},
  pages     = {617--629},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v87/clavera18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:24 +0200},
  biburl    = {https://dblp.org/rec/conf/corl/ClaveraRS0AA18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
clavera2018learning,
title={Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning},
author={Ignasi Clavera and Anusha Nagabandi and Simin Liu and Ronald S. Fearing and Pieter Abbeel and Sergey Levine and Chelsea Finn},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HyztsoC5Y7},
}

@inproceedings{volpi2018unseen,
 author = {Volpi, Riccardo and Namkoong, Hongseok and Sener, Ozan and Duchi, John C and Murino, Vittorio and Savarese, Silvio},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {5334--5344},
 publisher = {Curran Associates, Inc.},
 title = {Generalizing to Unseen Domains via Adversarial Data Augmentation},
 url = {https://proceedings.neurips.cc/paper/2018/file/1d94108e907bb8311d8802b48fd54b4a-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{
ajay2021opal,
title={{OPAL}: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning},
author={Anurag Ajay and Aviral Kumar and Pulkit Agrawal and Sergey Levine and Ofir Nachum},
booktitle={International Conference on Learning Representations},
year={2021},
}

@article{dulac2021challenges,
  title={Challenges of real-world reinforcement learning: definitions, benchmarks and analysis},
  author={Dulac-Arnold, Gabriel and Levine, Nir and Mankowitz, Daniel J and Li, Jerry and Paduraru, Cosmin and Gowal, Sven and Hester, Todd},
  journal={Machine Learning},
  pages={1--50},
  year={2021},
  publisher={Springer}
}

@inproceedings{
argenson2021modelbased,
title={Model-Based Offline Planning},
author={Arthur Argenson and Gabriel Dulac-Arnold},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=OMNB1G5xzd4}
}

@inproceedings{
nagabandi2018deep,
title={Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based {RL}},
author={Anusha Nagabandi and Chelsea Finn and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HyxAfnA5tm},
}

@misc{anne2021metareinforcement,
      title={Meta-Reinforcement Learning for Adaptive Motor Control in Changing Robot Dynamics and Environments}, 
      author={Timothée Anne and Jack Wilkinson and Zhibin Li},
      year={2021},
      eprint={2101.07599},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@inproceedings{m2ac,
 author = {Pan, Feiyang and He, Jia and Tu, Dandan and He, Qing},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {10537--10546},
 publisher = {Curran Associates, Inc.},
 title = {Trust the Model When It Is Confident: Masked Model-based Actor-Critic},
 url = {https://proceedings.neurips.cc/paper/2020/file/77133be2e96a577bd4794928976d2ae2-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{cowenrivers2020samba,
      title={SAMBA: Safe Model-Based \& Active Reinforcement Learning}, 
      author={Alexander I. Cowen-Rivers and Daniel Palenicek and Vincent Moens and Mohammed Abdullah and Aivar Sootla and Jun Wang and Haitham Ammar},
      year={2020},
      eprint={2006.09436},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yu2021combo,
      title={COMBO: Conservative Offline Model-Based Policy Optimization}, 
      author={Tianhe Yu and Aviral Kumar and Rafael Rafailov and Aravind Rajeswaran and Sergey Levine and Chelsea Finn},
      year={2021},
      eprint={2102.08363},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{deep_ensembles,
author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
title = {Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6405–6416},
numpages = {12},
location = {Long Beach, California, USA},
series = {NIPS'17}
}


@inproceedings{ovadia2019uncertainty,
 author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D. and Nowozin, Sebastian and Dillon, Joshua and Lakshminarayanan, Balaji and Snoek, Jasper},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Can you trust your model\textquotesingle s uncertainty?  Evaluating predictive uncertainty under dataset shift},
 url = {https://proceedings.neurips.cc/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf},
 volume = {32},
 year = {2019}
}


@article{hparams_offline,
  author    = {Tom Le Paine and
               Cosmin Paduraru and
               Andrea Michi and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               Konrad Zolna and
               Alexander Novikov and
               Ziyu Wang and
               Nando de Freitas},
  title     = {Hyperparameter Selection for Offline Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/2007.09055},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.09055},
  archivePrefix = {arXiv},
  eprint    = {2007.09055},
  timestamp = {Wed, 20 Jan 2021 16:29:33 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-09055.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{vanamersfoort2021improving,
      title={Improving Deterministic Uncertainty Estimation in Deep Learning for Classification and Regression}, 
      author={Joost van Amersfoort and Lewis Smith and Andrew Jesson and Oscar Key and Yarin Gal},
      year={2021},
      eprint={2102.11409},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@InProceedings{ball2021augwm,
  title = 	 {Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single Offline Environment},
  author =       {Ball, Philip J and Lu, Cong and Parker-Holder, Jack and Roberts, Stephen},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {619--629},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/ball21a/ball21a.pdf},
  url = 	 {http://proceedings.mlr.press/v139/ball21a.html},
}


@InProceedings{wan2021casmopolitan,
  title = 	 {Think Global and Act Local: Bayesian Optimisation over High-Dimensional Categorical and Mixed Search Spaces},
  author =       {Wan, Xingchen and Nguyen, Vu and Ha, Huong and Ru, Binxin and Lu, Cong and Osborne, Michael A.},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10663--10674},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/wan21b/wan21b.pdf},
  url = 	 {http://proceedings.mlr.press/v139/wan21b.html},
}


@inproceedings{
matsushima2021deploymentefficient,
title={Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization},
author={Tatsuya Matsushima and Hiroki Furuta and Yutaka Matsuo and Ofir Nachum and Shixiang Gu},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=3hGNqpI4WS}
}

@inproceedings{maddox2019calibration,
 author = {Maddox, Wesley J and Izmailov, Pavel and Garipov, Timur and Vetrov, Dmitry P and Wilson, Andrew Gordon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Simple Baseline for Bayesian Uncertainty in Deep Learning},
 url = {https://proceedings.neurips.cc/paper/2019/file/118921efba23fc329e6560b27861f0c2-Paper.pdf},
 volume = {32},
 year = {2019}
}


@InProceedings{pmlr-v80-kuleshov18a,
  title = 	 {Accurate Uncertainties for Deep Learning Using Calibrated Regression},
  author =       {Kuleshov, Volodymyr and Fenner, Nathan and Ermon, Stefano},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2796--2804},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kuleshov18a/kuleshov18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/kuleshov18a.html},
  abstract = 	 {Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate {—} for example, a 90\% credible interval may not contain the true outcome 90\% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.}
}

 @InProceedings{pmlr-v70-guo17a, title = {On Calibration of Modern Neural Networks}, author = {Chuan Guo and Geoff Pleiss and Yu Sun and Kilian Q. Weinberger}, booktitle = {Proceedings of the 34th International Conference on Machine Learning}, pages = {1321--1330}, year = {2017}, editor = {Precup, Doina and Teh, Yee Whye}, volume = {70}, series = {Proceedings of Machine Learning Research}, month = {06--11 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v70/guo17a/guo17a.pdf}, url = { http://proceedings.mlr.press/v70/guo17a.html }, abstract = {Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a single-parameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.} } 
 
 @incollection{pacchiano2020optimism,
      title={Towards Tractable Optimism in Model-Based Reinforcement Learning}, 
      author={Aldo Pacchiano and Philip Ball and Jack Parker-Holder and Krzysztof Choromanski and Stephen Roberts},
      year={2021},
      booktitle={Uncertainty in Artificial Intelligence}
}

@INPROCEEDINGS{mopac,  author={Omer, Muhammad and Ahmed, Rami and Rosman, Benjamin and Babikir, Sharief F.},  booktitle={2020 International Conference on Computer, Control, Electrical, and Electronics Engineering (ICCCEEE)},   title={Model Predictive-Actor Critic Reinforcement Learning for Dexterous Manipulation},   year={2021},  volume={},  number={},  pages={1-6},  doi={10.1109/ICCCEEE49695.2021.9429677}}

@Article{Pineda2021MBRL,
  author  = {Luis Pineda and Brandon Amos and Amy Zhang and Nathan O. Lambert and Roberto Calandra},
  journal = {Arxiv},
  title   = {MBRL-Lib: A Modular Library for Model-based Reinforcement Learning},
  year    = {2021},
  url     = {https://arxiv.org/abs/2104.10159},
}

@inproceedings{ampo,
 author = {Shen, Jian and Zhao, Han and Zhang, Weinan and Yu, Yong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {2823--2834},
 publisher = {Curran Associates, Inc.},
 title = {Model-based Policy Optimization with Unsupervised Model Adaptation},
 url = {https://proceedings.neurips.cc/paper/2020/file/1dc3a89d0d440ba31729b0ba74b93a33-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{NIPS2001_4b86abe4,
 author = {Kakade, Sham M},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 pages = {},
 publisher = {MIT Press},
 title = {A Natural Policy Gradient},
 url = {https://proceedings.neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf},
 volume = {14},
 year = {2002}
}

@misc{sinha2021s4rl,
      title={S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement Learning}, 
      author={Samarth Sinha and Animesh Garg},
      year={2021},
      eprint={2103.06326},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{mouret2015illuminating,
      title={Illuminating search spaces by mapping elites}, 
      author={Jean-Baptiste Mouret and Jeff Clune},
      year={2015},
      eprint={1504.04909},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}


@phdthesis{10.5555/143221,
author = {Mackay, David John Cameron},
title = {Bayesian Methods for Adaptive Models},
year = {1992},
publisher = {California Institute of Technology},
address = {USA},
note = {UMI Order No. GAX92-32200}
}

@InProceedings{pmlr-v37-blundell15,
title = {Weight Uncertainty in Neural Network},
author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
pages = {1613--1622},
year = {2015},
editor = {Bach, Francis and Blei, David},
volume = {37},
series = {Proceedings of Machine Learning Research},
address = {Lille, France},
month = {07--09 Jul},
publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v37/blundell15.pdf}, url = { http://proceedings.mlr.press/v37/blundell15.html }, }

@InProceedings{pmlr-v119-van-amersfoort20a, title = {Uncertainty Estimation Using a Single Deep Deterministic Neural Network}, author = {Van Amersfoort, Joost and Smith, Lewis and Teh, Yee Whye and Gal, Yarin}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {9690--9700}, year = {2020}, editor = {Hal Daumé III and Aarti Singh}, volume = {119}, series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v119/van-amersfoort20a/van-amersfoort20a.pdf}, url = { http://proceedings.mlr.press/v119/van-amersfoort20a.html }, abstract = {We propose a method for training a deterministic deep model that can find and reject out of distribution data points at test time with a single forward pass. Our approach, deterministic uncertainty quantification (DUQ), builds upon ideas of RBF networks. We scale training in these with a novel loss function and centroid updating scheme and match the accuracy of softmax models. By enforcing detectability of changes in the input using a gradient penalty, we are able to reliably detect out of distribution data. Our uncertainty quantification scales well to large datasets, and using a single model, we improve upon or match Deep Ensembles in out of distribution detection on notable difficult dataset pairs such as FashionMNIST vs. MNIST, and CIFAR-10 vs. SVHN.} }

@inproceedings{
fu2021benchmarks,
title={Benchmarks for Deep Off-Policy Evaluation},
author={Justin Fu and Mohammad Norouzi and Ofir Nachum and George Tucker and ziyu wang and Alexander Novikov and Mengjiao Yang and Michael R Zhang and Yutian Chen and Aviral Kumar and Cosmin Paduraru and Sergey Levine and Thomas Paine},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=kWSeGEeHvF8}
}

@inproceedings{offlineevaluation,
author = {Mandel, Travis and Liu, Yun-En and Brunskill, Emma and Popovi\'{c}, Zoran},
title = {Offline Evaluation of Online Reinforcement Learning Algorithms},
year = {2016},
publisher = {AAAI Press},
abstract = {In many real-world reinforcement learning problems, we have access to an existing dataset and would like to use it to evaluate various learning approaches. Typically, one would prefer not to deploy a fixed policy, but rather an algorithm that learns to improve its behavior as it gains more experience. Therefore, we seek to evaluate how a proposed algorithm learns in our environment, meaning we need to evaluate how an algorithm would have gathered experience if it were run online. In this work, we develop three new evaluation approaches which guarantee that, given some history, algorithms are fed samples from the distribution that they would have encountered if they were run online. Additionally, we are the first to propose an approach that is provably unbiased given finite data, eliminating bias due to the length of the evaluation. Finally, we compare the sample-efficiency of these approaches on multiple datasets, including one from a real-world deployment of an educational game.},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {1926–1933},
numpages = {8},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@INPROCEEDINGS{mujoco,  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},   title={MuJoCo: A physics engine for model-based control},   year={2012},  volume={},  number={},  pages={5026-5033},  doi={10.1109/IROS.2012.6386109}}

@misc{dmcontrol,
    title={dm\_control: Software and Tasks for Continuous Control},
    author={Yuval Tassa and Saran Tunyasuvunakool and Alistair Muldal and
            Yotam Doron and Siqi Liu and Steven Bohez and Josh Merel and
            Tom Erez and Timothy Lillicrap and Nicolas Heess},
    year={2020},
    eprint={2006.12983},
    archivePrefix={arXiv},
    primaryClass={cs.RO}
}

@inproceedings{PrasadCCDE17,
  author    = {Niranjani Prasad and
               Li{-}Fang Cheng and
               Corey Chivers and
               Michael Draugelis and
               Barbara E. Engelhardt},
  editor    = {Gal Elidan and
               Kristian Kersting and
               Alexander T. Ihler},
  title     = {A Reinforcement Learning Approach to Weaning of Mechanical Ventilation
               in Intensive Care Units},
  booktitle = {Proceedings of the Thirty-Third Conference on Uncertainty in Artificial
               Intelligence, {UAI} 2017, Sydney, Australia, August 11-15, 2017},
  publisher = {{AUAI} Press},
  year      = {2017},
  url       = {http://auai.org/uai2017/proceedings/papers/209.pdf},
  timestamp = {Thu, 12 Mar 2020 11:31:10 +0100},
  biburl    = {https://dblp.org/rec/conf/uai/PrasadCCDE17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{ebert2018visual,
      title={Visual Foresight: Model-Based Deep Reinforcement Learning for Vision-Based Robotic Control}, 
      author={Frederik Ebert and Chelsea Finn and Sudeep Dasari and Annie Xie and Alex Lee and Sergey Levine},
      year={2018},
      eprint={1812.00568},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@misc{chen2021instrumental,
      title={On Instrumental Variable Regression for Deep Offline Policy Evaluation}, 
      author={Yutian Chen and Liyuan Xu and Caglar Gulcehre and Tom Le Paine and Arthur Gretton and Nando de Freitas and Arnaud Doucet},
      year={2021},
      eprint={2105.10148},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{cocabo,
  title={{B}ayesian optimisation over multiple continuous and categorical inputs},
  author={Ru, Binxin and Alvi, Ahsan and Nguyen, Vu and Osborne, Michael A and Roberts, Stephen},
  booktitle={International Conference on Machine Learning},
  pages={8276--8285},
  year={2020},
  organization={PMLR}
}

@inproceedings{CRR,
 author = {Wang, Ziyu and Novikov, Alexander and Zolna, Konrad and Merel, Josh S and Springenberg, Jost Tobias and Reed, Scott E and Shahriari, Bobak and Siegel, Noah and Gulcehre, Caglar and Heess, Nicolas and de Freitas, Nando},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {7768--7778},
 publisher = {Curran Associates, Inc.},
 title = {Critic Regularized Regression},
 url = {https://proceedings.neurips.cc/paper/2020/file/588cb956d6bbe67078f29f8de420a13d-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{schrittwieser2021online,
      title={Online and Offline Reinforcement Learning by Planning with a Learned Model}, 
      author={Julian Schrittwieser and Thomas Hubert and Amol Mandhane and Mohammadamin Barekatain and Ioannis Antonoglou and David Silver},
      year={2021},
      eprint={2104.06294},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
Chan2020Measuring,
title={Measuring the Reliability of Reinforcement Learning Algorithms},
author={Stephanie C.Y. Chan and Samuel Fishman and Anoop Korattikara and John Canny and Sergio Guadarrama},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJlpYJBKvH}
}

@misc{agarwal2021deep,
      title={Deep Reinforcement Learning at the Edge of the Statistical Precipice}, 
      author={Rishabh Agarwal and Max Schwarzer and Pablo Samuel Castro and Aaron Courville and Marc G. Bellemare},
      year={2021},
      eprint={2108.13264},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{suttonbarto,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {Reinforcement Learning: An Introduction},
year = {2018},
isbn = {0262039249},
publisher = {A Bradford Book},
address = {Cambridge, MA, USA},
}


@InProceedings{pmlr-v130-lyle21a,
  title = 	 { On the Effect of Auxiliary Tasks on Representation Dynamics },
  author =       {Lyle, Clare and Rowland, Mark and Ostrovski, Georg and Dabney, Will},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1--9},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/lyle21a/lyle21a.pdf},
  url = 	 {https://proceedings.mlr.press/v130/lyle21a.html},
  abstract = 	 { While auxiliary tasks play a key role in shaping the representations learnt by reinforcement learning agents, much is still unknown about the mechanisms through which this is achieved. This work develops our understanding of the relationship between auxiliary tasks, environment structure, and representations by analysing the dynamics of temporal difference algorithms. Through this approach, we establish a connection between the spectral decomposition of the transition operator and the representations induced by a variety of auxiliary tasks. We then leverage insights from these theoretical results to inform the selection of auxiliary tasks for deep reinforcement learning agents in sparse-reward environments. }
}

@inproceedings{
drqv2,
title={Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning},
author={Denis Yarats and Rob Fergus and Alessandro Lazaric and Lerrel Pinto},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=_SJ-_yyes8}
}

@inproceedings{td3,
  author={Scott Fujimoto and Herke van Hoof and David Meger},
  title={Addressing Function Approximation Error in Actor-Critic Methods},
  year={2018},
  cdate={1514764800000},
  pages={1582-1591},
  url={http://proceedings.mlr.press/v80/fujimoto18a.html},
  booktitle={ICML},
}

@inproceedings{
brandfonbrener2021offline,
title={Offline {RL} Without Off-Policy Evaluation},
author={David Brandfonbrener and William F Whitney and Rajesh Ranganath and Joan Bruna},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=LU687itn08w}
}

@techreport{sarsa,
  added-at = {2008-03-11T14:52:34.000+0100},
  address = {Cambridge, England},
  author = {Rummery, G. A. and Niranjan, M.},
  biburl = {https://www.bibsonomy.org/bibtex/220239f82583859588c96171ccc015e65/idsia},
  citeulike-article-id = {2378892},
  institution = {Cambridge University Engineering Department},
  interhash = {0c7cd3821ad0fe1b39a6ce1b35ec4bc0},
  intrahash = {20239f82583859588c96171ccc015e65},
  keywords = {nn},
  number = {TR 166},
  priority = {2},
  timestamp = {2008-03-11T14:59:39.000+0100},
  title = {On-Line {Q}-Learning Using Connectionist Systems},
  year = 1994
}

@inproceedings{
mohtashami2021reproducibility,
title={Reproducibility Report for ''On Warm-Starting Neural Network Training''},
author={Amirkeivan Mohtashami and Ehsan Pajouheshgar and Klim Kireev},
booktitle={ML Reproducibility Challenge 2020},
year={2021},
url={https://openreview.net/forum?id=N43DVxrjCw}
}

@inproceedings{
igl2021transient,
title={Transient Non-stationarity and Generalisation in Deep Reinforcement Learning},
author={Maximilian Igl and Gregory Farquhar and Jelena Luketina and Wendelin Boehmer and Shimon Whiteson},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Qun8fv4qSby}
}

@article{sacae, title={Improving Sample Efficiency in Model-Free Reinforcement Learning from Images}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/17276}, abstractNote={Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. A promising approach is to learn a latent representation together with the control policy. However, fitting a high-capacity encoder using a scarce reward signal is sample inefficient and leads to poor performance.
Prior work has shown that auxiliary losses, such as image reconstruction, can aid efficient representation learning. However, incorporating reconstruction loss into an off-policy learning algorithm often leads to training instability. We explore the underlying reasons and identify variational autoencoders, used by previous investigations, as the cause of the divergence. Following these findings, we propose effective techniques to improve training stability. This results in a simple approach capable of
matching state-of-the-art model-free and model-based algorithms on MuJoCo control tasks. Furthermore, our approach demonstrates robustness to observational noise, surpassing existing approaches in this setting. Code, results, and videos are anonymously available at https://sites.google.com/view/sac-ae/home.}, number={12}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Yarats, Denis and Zhang, Amy and Kostrikov, Ilya and Amos, Brandon and Pineau, Joelle and Fergus, Rob}, year={2021}, month={May}, pages={10674-10681} }

@inproceedings{
nachum2021provable,
title={Provable Representation Learning for Imitation with Contrastive Fourier Features},
author={Ofir Nachum and Mengjiao Yang},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=Xv7rBttjWFT}
}

@article{shorten2019augsurvey,
  title={A survey on image data augmentation for deep learning},
  author={Shorten, Connor and Khoshgoftaar, Taghi M},
  journal={Journal of Big Data},
  volume={6},
  number={1},
  pages={1--48},
  year={2019},
  publisher={Springer}
}

@misc{schaul2021returnbased,
      title={Return-based Scaling: Yet Another Normalisation Trick for Deep RL}, 
      author={Tom Schaul and Georg Ostrovski and Iurii Kemaev and Diana Borsa},
      year={2021},
      eprint={2105.05347},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{visualloss,
  title={Visualizing the Loss Landscape of Neural Nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={Neural Information Processing Systems},
  year={2018}
}

@inproceedings{drac,
      title={Automatic Data Augmentation for Generalization in Deep Reinforcement Learning}, 
      author={Roberta Raileanu and Max Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},
      year={2021},
      booktitle={Neural Information Processing Systems},
 
}

@inproceedings{randomlabels,
 author = {Maennel, Hartmut and Alabdulmohsin, Ibrahim M and Tolstikhin, Ilya O and Baldock, Robert and Bousquet, Olivier and Gelly, Sylvain and Keysers, Daniel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {19693--19704},
 publisher = {Curran Associates, Inc.},
 title = {What Do Neural Networks Learn When Trained With Random Labels?},
 url = {https://proceedings.neurips.cc/paper/2020/file/e4191d610537305de1d294adb121b513-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{pitiscounterfactualaug,
 author = {Pitis, Silviu and Creager, Elliot and Garg, Animesh},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {3976--3990},
 publisher = {Curran Associates, Inc.},
 title = {Counterfactual Data Augmentation using Locally Factored Dynamics},
 url = {https://proceedings.neurips.cc/paper/2020/file/294e09f267683c7ddc6cc5134a7e68a8-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{
kumar2021implicit,
title={Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning},
author={Aviral Kumar and Rishabh Agarwal and Dibya Ghosh and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=O9bnihsFfXU}
}

@inproceedings{mobahiselfdistillation,
 author = {Mobahi, Hossein and Farajtabar, Mehrdad and Bartlett, Peter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {3351--3361},
 publisher = {Curran Associates, Inc.},
 title = {Self-Distillation Amplifies Regularization in Hilbert Space},
 url = {https://proceedings.neurips.cc/paper/2020/file/2288f691b58edecadcc9a8691762b4fd-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{allenzhu2021feature,
      title={Feature Purification: How Adversarial Training Performs Robust Deep Learning}, 
      author={Zeyuan Allen-Zhu and Yuanzhi Li},
      year={2021},
      eprint={2005.10190},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@article{lecun-deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{dqn,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}


@article{alphazero,
  title={Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1712.01815},
  year={2017}
}


@article{dmc,
  title={Deepmind control suite},
  author={Tassa, Yuval and Doron, Yotam and Muldal, Alistair and Erez, Tom and Li, Yazhe and Casas, Diego de Las and Budden, David and Abdolmaleki, Abbas and Merel, Josh and Lefrancq, Andrew and others},
  journal={arXiv preprint arXiv:1801.00690},
  year={2018}
}

@article{maxentobj,
  title={Modeling purposeful adaptive behavior with the principle of maximum causal entropy},
  author={Ziebart, Brian D},
  year={2010}
}

@inproceedings{pg-thm,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}

@inproceedings{dpg,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  year={2014}
}

% From DisentanGAIL

@article{ppo,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06347},
  archivePrefix = {arXiv},
  eprint    = {1707.06347},
  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SchulmanWDRK17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ddpg,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}


@InProceedings{sac,
  title = 	 {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author =       {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1861--1870},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf},
  url = 	 {https://proceedings.mlr.press/v80/haarnoja18b.html},
  abstract = 	 {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.}
}


@article{maxentirl,
  title={Maximum entropy inverse reinforcement learning},
  author={Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K},
  year={2008},
  publisher={figshare}
}

@article{maxentdeepirl,
  title={Maximum entropy deep inverse reinforcement learning},
  author={Wulfmeier, Markus and Ondruska, Peter and Posner, Ingmar},
  journal={arXiv preprint arXiv:1507.04888},
  year={2015}
}

@inproceedings{gan,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}

@inproceedings{gail,
  title={Generative adversarial imitation learning},
  author={Ho, Jonathan and Ermon, Stefano},
  booktitle={Advances in neural information processing systems},
  pages={4565--4573},
  year={2016}
}

@article{airl,
  title={Learning robust rewards with adversarial inverse reinforcement learning},
  author={Fu, Justin and Luo, Katie and Levine, Sergey},
  journal={arXiv preprint arXiv:1710.11248},
  year={2017}
}

@article{dac,
  title={Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial imitation learning},
  author={Kostrikov, Ilya and Agrawal, Kumar Krishna and Dwibedi, Debidatta and Levine, Sergey and Tompson, Jonathan},
  year={2018}
}

@article{nice,
  title={Nice: Non-linear independent components estimation},
  author={Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1410.8516},
  year={2014}
}

@article{realnvp,
  title={Density estimation using real nvp},
  author={Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  journal={arXiv preprint arXiv:1605.08803},
  year={2016}
}

@inproceedings{glow,
  title={Glow: Generative flow with invertible 1x1 convolutions},
  author={Kingma, Durk P and Dhariwal, Prafulla},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10215--10224},
  year={2018}
}

@article{flow++,
  title={Flow++: Improving flow-based generative models with variational dequantization and architecture design},
  author={Ho, Jonathan and Chen, Xi and Srinivas, Aravind and Duan, Yan and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1902.00275},
  year={2019}
}

@inproceedings{imi1,
  title={Alvinn: An autonomous land vehicle in a neural network},
  author={Pomerleau, Dean A},
  booktitle={Advances in neural information processing systems},
  pages={305--313},
  year={1989}
}

@article{imi15,
  title={Efficient training of artificial neural networks for autonomous navigation},
  author={Pomerleau, Dean A},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={88--97},
  year={1991},
  publisher={MIT Press}
}

@inproceedings{imi2,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={627--635},
  year={2011}
}

@inproceedings{irl1,
  title={Algorithms for inverse reinforcement learning.},
  author={Ng, Andrew Y and Russell, Stuart J and others},
  booktitle={Icml},
  volume={1},
  pages={2},
  year={2000}
}

@inproceedings{irl2,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={1},
  year={2004},
  organization={ACM}
}

@inproceedings{irl3,
  title={Maximum margin planning},
  author={Ratliff, Nathan D and Bagnell, J Andrew and Zinkevich, Martin A},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={729--736},
  year={2006},
  organization={ACM}
}

@inproceedings{gcl,
  title={Guided cost learning: Deep inverse optimal control via policy optimization},
  author={Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={49--58},
  year={2016}
}

@article{connection,
  title={A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models},
  author={Finn, Chelsea and Christiano, Paul and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1611.03852},
  year={2016}
}

@article{engi1,
  title={An object-based approach to map human hand synergies onto robotic hands with dissimilar kinematics},
  author={Gioioso, Guido and Salvietti, Gionata and Malvezzi, Monica and Prattichizzo, Daniele},
  journal={Robotics: Science and Systems VIII},
  pages={97--104},
  year={2012},
  publisher={The MIT Press Sydney, NSW}
}

@inproceedings{engi2,
  title={Learning dexterous manipulation for a soft robotic hand from human demonstrations},
  author={Gupta, Abhishek and Eppner, Clemens and Levine, Sergey and Abbeel, Pieter},
  booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={3786--3793},
  year={2016},
  organization={IEEE}
}

@article{reli1,
  title={Learning invariant feature spaces to transfer skills with reinforcement learning},
  author={Gupta, Abhishek and Devin, Coline and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1703.02949},
  year={2017}
}

@inproceedings{reli2,
  title={Time-contrastive networks: Self-supervised learning from video},
  author={Sermanet, Pierre and Lynch, Corey and Chebotar, Yevgen and Hsu, Jasmine and Jang, Eric and Schaal, Stefan and Levine, Sergey and Brain, Google},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1134--1141},
  year={2018},
  organization={IEEE}
}

@inproceedings{reli3,
  title={Imitation from observation: Learning to imitate behaviors from raw video via context translation},
  author={Liu, YuXuan and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1118--1125},
  year={2018},
  organization={IEEE}
}

@article{reli4,
  title={AVID: Learning Multi-Stage Tasks via Pixel-Level Translation of Human Videos},
  author={Smith, Laura and Dhawan, Nikita and Zhang, Marvin and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1912.04443},
  year={2019}
}

@inproceedings{reli5,
  title={Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller},
  author={Sharma, Pratyusha and Pathak, Deepak and Gupta, Abhinav},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2593--2603},
  year={2019}
}

@article{cdil,
  title={Cross Domain Imitation Learning},
  author={Kim, Kun Ho and Gu, Yihong and Song, Jiaming and Zhao, Shengjia and Ermon, Stefano},
  journal={arXiv preprint arXiv:1910.00105},
  year={2019}
}

@article{tpil,
  title={Third-person imitation learning},
  author={Stadie, Bradly C and Abbeel, Pieter and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1703.01703},
  year={2017}
}


@article{vae,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{iwae,
  title={Importance weighted autoencoders},
  author={Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1509.00519},
  year={2015}
}

@inproceedings{infogan,
  title={Infogan: Interpretable representation learning by information maximizing generative adversarial nets},
  author={Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  booktitle={Advances in neural information processing systems},
  pages={2172--2180},
  year={2016}
}

@article{robonet,
  title={RoboNet: Large-Scale Multi-Robot Learning},
  author={Dasari, Sudeep and Ebert, Frederik and Tian, Stephen and Nair, Suraj and Bucher, Bernadette and Schmeckpeper, Karl and Singh, Siddharth and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:1910.11215},
  year={2019}
}

@article{wake,
  title={The" wake-sleep" algorithm for unsupervised neural networks},
  author={Hinton, Geoffrey E and Dayan, Peter and Frey, Brendan J and Neal, Radford M},
  journal={Science},
  volume={268},
  number={5214},
  pages={1158--1161},
  year={1995},
  publisher={American Association for the Advancement of Science}
}

@article{vdb,
  title={Variational discriminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining information flow},
  author={Peng, Xue Bin and Kanazawa, Angjoo and Toyer, Sam and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1810.00821},
  year={2018}
}

@article{adv-ae,
  title={Adversarial autoencoders},
  author={Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  journal={arXiv preprint arXiv:1511.05644},
  year={2015}
}

@article{rev-2019,
  title={Recent advances in imitation learning from observation},
  author={Torabi, Faraz and Warnell, Garrett and Stone, Peter},
  journal={arXiv preprint arXiv:1905.13566},
  year={2019}
}

@inproceedings{mine,
  title={Mutual information neural estimation},
  author={Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
  booktitle={International Conference on Machine Learning},
  pages={531--540},
  year={2018}
}

@article{mi-motivation,
  title={Equitability, mutual information, and the maximal information coefficient},
  author={Kinney, Justin B and Atwal, Gurinder S},
  journal={Proceedings of the National Academy of Sciences},
  volume={111},
  number={9},
  pages={3354--3359},
  year={2014},
  publisher={National Acad Sciences}
}

@article{mi-challenges,
  title={Estimation of entropy and mutual information},
  author={Paninski, Liam},
  journal={Neural computation},
  volume={15},
  number={6},
  pages={1191--1253},
  year={2003},
  publisher={MIT Press}
}

@inproceedings{uda1,
  title={A unified feature disentangler for multi-domain image translation and manipulation},
  author={Liu, Alexander H and Liu, Yen-Cheng and Yeh, Yu-Ying and Wang, Yu-Chiang Frank},
  booktitle={Advances in neural information processing systems},
  pages={2590--2599},
  year={2018}
}

@article{saturation,
  title={Towards Principled Methods for Training Generative Adversarial Networks. arXiv e-prints, art},
  author={Arjovsky, Martin and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1701.04862},
  year={2017}
}

@article{donsker,
  title={Asymptotic evaluation of certain Markov process expectations for large time, I},
  author={Donsker, Monroe D and Varadhan, SR Srinivasa},
  journal={Communications on Pure and Applied Mathematics},
  volume={28},
  number={1},
  pages={1--47},
  year={1975},
  publisher={Wiley Online Library}
}

@inproceedings{trpo,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015}
}

@article{sac-alg,
  title={Soft actor-critic algorithms and applications},
  author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}

@inproceedings{double-q,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Thirtieth AAAI conference on artificial intelligence},
  year={2016}
}


@article{sngan,
  title={Spectral normalization for generative adversarial networks},
  author={Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  journal={arXiv preprint arXiv:1802.05957},
  year={2018}
}

@article{wass,
  title={Wasserstein gan},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1701.07875},
  year={2017}
}

@inproceedings{wass-gp,
  title={Improved training of wasserstein gans},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  booktitle={Advances in neural information processing systems},
  pages={5767--5777},
  year={2017}
}

\@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@inproceedings{infogan,
  title={Infogan: Interpretable representation learning by information maximizing generative adversarial nets},
  author={Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  booktitle={Advances in neural information processing systems},
  pages={2172--2180},
  year={2016}
}

@article{mi1,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@article{mi2,
  title={Learning deep representations by mutual information estimation and maximization},
  author={Hjelm, R Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1808.06670},
  year={2018}
}

@article{gym,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{kl-div,
  title={On information and sufficiency},
  author={Kullback, Solomon and Leibler, Richard A},
  journal={The annals of mathematical statistics},
  volume={22},
  number={1},
  pages={79--86},
  year={1951},
  publisher={JSTOR}
}

@inproceedings{handful,
  title={Deep reinforcement learning in a handful of trials using probabilistic dynamics models},
  author={Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4754--4765},
  year={2018}
}

@inproceedings{maml, 
  title={Model-agnostic meta-learning for fast adaptation of deep networks}, 
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey}, 
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70}, 
  pages={1126--1135}, 
  year={2017}, 
  organization={JMLR. org} 
} 

@article{meta2, 
  title={Memory-based control with recurrent neural networks}, 
  author={Heess, Nicolas and Hunt, Jonathan J and Lillicrap, Timothy P and Silver, David}, 
  journal={arXiv preprint arXiv:1512.04455}, 
  year={2015} 
} 

@article{meta3, 
  title={RL $\^{} 2$: Fast Reinforcement Learning via Slow Reinforcement Learning}, 
  author={Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L and Sutskever, Ilya and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1611.02779}, 
  year={2016} 
} 

@book{convex,
  title={Convex optimization},
  author={Boyd, Stephen and Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@article{challenges-rl,
  title={Challenges of real-world reinforcement learning},
  author={Dulac-Arnold, Gabriel and Mankowitz, Daniel and Hester, Todd},
  journal={arXiv preprint arXiv:1904.12901},
  year={2019}
}

@article{rl-real-world-design,
  title={Benchmarking reinforcement learning algorithms on real-world robots},
  author={Mahmood, A Rupam and Korenkevych, Dmytro and Vasan, Gautham and Ma, William and Bergstra, James},
  journal={arXiv preprint arXiv:1809.07731},
  year={2018}
}


@book{sutton-reinforcement-n-step,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@inproceedings{A3c,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016}
}

@inproceedings{dynamic-action-rep,
  title={Dynamic action repetition for deep reinforcement learning},
  author={Lakshminarayanan, Aravind and Sharma, Sahil and Ravindran, Balaraman},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

@inproceedings{frameskip-importance,
  title={Frame skip is a powerful parameter for learning to play atari},
  author={Braylan, Alex and Hollenbeck, Mark and Meyerson, Elliot and Miikkulainen, Risto},
  booktitle={Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence},
  year={2015}
}

@article{d4pg,
  title={Distributed distributional deterministic policy gradients},
  author={Barth-Maron, Gabriel and Hoffman, Matthew W and Budden, David and Dabney, Will and Horgan, Dan and Tb, Dhruva and Muldal, Alistair and Heess, Nicolas and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:1804.08617},
  year={2018}
}

@article{STRAW,
  title={Strategic attentive writer for learning macro-actions},
  author={Vezhnevets, Alexander and Mnih, Volodymyr and Osindero, Simon and Graves, Alex and Vinyals, Oriol and Agapiou, John and others},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={3486--3494},
  year={2016}
}

% References for Related works

% Options intro

%paper proposing options

@article{opt-intro1,
  title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1-2},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}

@article{opt-intro2,
  title={Temporal abstraction in reinforcement learning.},
  author={Precup, Doina},
  year={2001}
}

% Options learnt from subgoals

@inproceedings{opt-sub-1,
 author = {Dayan, Peter and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 pages = {271--278},
 publisher = {Morgan-Kaufmann},
 title = {Feudal Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/1992/file/d14220ee66aeec73c49038385428ec4c-Paper.pdf},
 volume = {5},
 year = {1993}
}


% decomposing MDP in a hierarchy of smaller MDPs, decomposing value f'n into addittive combination of the smaller MDPs - hierarchical more than options

@article{opt-sub-2, 
  title={Hierarchical reinforcement learning with the MAXQ value function decomposition},
  author={Dietterich, Thomas G},
  journal={Journal of artificial intelligence research},
  volume={13},
  pages={227--303},
  year={2000}
}


% + opt-end-to-end

@inproceedings{option-critic,
  title={The option-critic architecture},
  author={Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

% Hierarchical RL through subgoal specification

% build a hierarchical policy by re-using knowledge from previous skills in a lifelong learning setting
@inproceedings{hierarchical-subgoals-mine, 
  title={A deep hierarchical approach to lifelong learning in minecraft},
  author={Tessler, Chen and Givony, Shahar and Zahavy, Tom and Mankowitz, Daniel and Mannor, Shie},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

% Learn low level skills through incentivizing particular simple behaviour in low-level env. (e.g. for locomotion simply movement of the agent) + diversity through MI. Re-use these skills for higher-level controller to acquire more complex behavior.
@article{hierarchical-subgoals-small-prior, 
  title={Stochastic neural networks for hierarchical reinforcement learning},
  author={Florensa, Carlos and Duan, Yan and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1704.03012},
  year={2017}
}

% learn embedding space which can be viewed as a higher-level hierarchical action space via learning many specified subtasks
@inproceedings{hierarchical-subgoals-robot,
  title={Learning an embedding space for transferable robot skills},
  author={Hausman, Karol and Springenberg, Jost Tobias and Wang, Ziyu and Heess, Nicolas and Riedmiller, Martin},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

% Intrinsic subgoals

% intrinsic objective used for low-level to learn options
@article{hierarchical-subgoals-intrinsic-1,
  title={Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation},
  author={Kulkarni, Tejas D and Narasimhan, Karthik and Saeedi, Ardavan and Tenenbaum, Josh},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={3675--3683},
  year={2016}
}

% learn set of intrinsic options by maximizing the num. of reachable states.
@article{hierarchical-subgoals-intrinsic-2,
  title={Variational intrinsic control},
  author={Gregor, Karol and Rezende, Danilo Jimenez and Wierstra, Daan},
  journal={arXiv preprint arXiv:1611.07507},
  year={2016}
}


% Learn better representations of single actions

% Learn to embed large discrete action spaces into a continuous param. with KNN
@article{single-action-rep-discrete-1,
  title={Deep reinforcement learning in large discrete action spaces},
  author={Dulac-Arnold, Gabriel and Evans, Richard and van Hasselt, Hado and Sunehag, Peter and Lillicrap, Timothy and Hunt, Jonathan and Mann, Timothy and Weber, Theophane and Degris, Thomas and Coppin, Ben},
  journal={arXiv preprint arXiv:1512.07679},
  year={2015}
}

% Bundle actions using a max. likelihood objective from state next state recover action (latent representation is used as new action space and decoder as the mapping from the rep. back to an executable action)
@article{single-action-rep-discrete-2,
  title={Learning action representations for reinforcement learning},
  author={Chandak, Yash and Theocharous, Georgios and Kostas, James and Jordan, Scott and Thomas, Philip S},
  journal={arXiv preprint arXiv:1902.00183},
  year={2019}
}

% Action-representations from demos

@article{demos-act-rep,
  title={The natural language of actions},
  author={Tennenholtz, Guy and Mannor, Shie},
  journal={arXiv preprint arXiv:1902.01119},
  year={2019}
}

% Learn to repeat actions - temporally extended behaviour - using a set of Macro-actions defined as repeating the same action multiple times


% Augments space of discrete actions by considering repetitions at 2 different time-scales 
@inproceedings{action-reps-discrete,
  title={Dynamic action repetition for deep reinforcement learning},
  author={Lakshminarayanan, Aravind and Sharma, Sahil and Ravindran, Balaraman},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

% factors policy in 2 components 1) for selecting action 2) for selecting reps
@article{action-reps-policy-factor,
  title={Learning to repeat: Fine grained action repetition for deep reinforcement learning},
  author={Sharma, Sahil and Lakshminarayanan, Aravind S and Ravindran, Balaraman},
  journal={arXiv preprint arXiv:1702.06054},
  year={2017}
}

% Macro actions - related concept representing uncertain higher level behavior abstractions

@inproceedings{macro-intro,
  title={Planning with closed-loop macro actions},
  author={Precup, Doina and Sutton, Richard S and Singh, Satinder P},
  booktitle={Working notes of the 1997 AAAI Fall Symposium on Model-directed Autonomous Systems},
  pages={70--76},
  year={1997}
}

% keeps a stored running plan to execute that is adaptively updated through attentive reading and writing operators.
@article{macro-STRAWS,
  title={Strategic attentive writer for learning macro-actions},
  author={Vezhnevets, Alexander and Mnih, Volodymyr and Osindero, Simon and Graves, Alex and Vinyals, Oriol and Agapiou, John and others},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={3486--3494},
  year={2016}
}


% Highly relevant, based on target-states to parameterize low level behaviour (e.g. intentions) - to review


% Hierarchical, framework, higher level controller sets goals to be reached by lower-level modules in latent space
@article{FuN,
  title={Feudal networks for hierarchical reinforcement learning},
  author={Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1703.01161},
  year={2017}
}

% based on next state - learn trajectory AE (only states) and policy (conditioned on state and latent dims of AE) to imitate the outputed trajs 
@article{traj-embeddings-auto-encoder,
  title={Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings},
  author={Co-Reyes, John D and Liu, YuXuan and Gupta, Abhishek and Eysenbach, Benjamin and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1806.02813},
  year={2018}
}

%HIRO - use target states as higher-level actions, low level policy needs to learn to reach with L2 loss
@inproceedings{HIRO,
  title={Data-efficient hierarchical reinforcement learning},
  author={Nachum, Ofir and Gu, Shixiang Shane and Lee, Honglak and Levine, Sergey},
  booktitle={Advances in neural information processing systems},
  pages={3303--3313},
  year={2018}
}

% Learn representations of action sequences - can combine with other macro-actions papers

% Learn lower-level representation of action sequences (length k - fixed?) (and of current states) that is maximally useful to recover next states through an AE, learn Q f'n after fixing these representations, which learns Q for the new embeddings + # of steps left in the k-len plan.
@article{actseq-rep-dynamics-aware-emb,
  title={Dynamics-aware Embeddings},
  author={Whitney, William and Agarwal, Rajat and Cho, Kyunghyun and Gupta, Abhinav},
  journal={arXiv preprint arXiv:1908.09357},
  year={2019}
}

% other relevant + recent

% reparameterizing action space utilizing a parameterized dynamical system with a fixed structure adding helpful inductive bias to the model.
@article{NDP,
  title={Neural Dynamic Policies for End-to-End Sensorimotor Learning},
  author={Bahl, Shikhar and Mukadam, Mustafa and Gupta, Abhinav and Pathak, Deepak},
  journal={arXiv preprint arXiv:2012.02788},
  year={2020}
}

% HUMAN MOTOR-LEARNING
@book{human-motor-learning,
  title={Mechanisms of memory},
  author={Sweatt, J David},
  year={2009},
  publisher={Academic Press}
}

% CAMERA-READY 4 REVIEWERS

% works showing that simpleaction repetitions learning  can speed up learning and improve exploration

@inproceedings{actreps_mot0,
  title={Speeding-up reinforcement learning with multi-step actions},
  author={Schoknecht, Ralf and Riedmiller, Martin},
  booktitle={International Conference on Artificial Neural Networks},
  pages={813--818},
  year={2002},
  organization={Springer}
}

@article{actreps_mot1,
  title={Reinforcement learning on explicitly specified time scales},
  author={Schoknecht, Ralf and Riedmiller, Martin},
  journal={Neural Computing \& Applications},
  volume={12},
  number={2},
  pages={61--80},
  year={2003},
  publisher={Springer}
}

@inproceedings{actreps_mot2,
  title={Continuous-discrete reinforcement learning for hybrid control in robotics},
  author={Neunert, Michael and Abdolmaleki, Abbas and Wulfmeier, Markus and Lampe, Thomas and Springenberg, Tobias and Hafner, Roland and Romano, Francesco and Buchli, Jonas and Heess, Nicolas and Riedmiller, Martin},
  booktitle={Conference on Robot Learning},
  pages={735--751},
  year={2020},
  organization={PMLR}
}

% Learn best 'action-persistent fixed throughout a problem' (incur in non-trivial increased learning cost)

@inproceedings{actpers0,
  title={Control frequency adaptation via action persistence in batch reinforcement learning},
  author={Metelli, Alberto Maria and Mazzolini, Flavio and Bisi, Lorenzo and Sabbioni, Luca and Restelli, Marcello},
  booktitle={International Conference on Machine Learning},
  pages={6862--6873},
  year={2020},
  organization={PMLR}
}

@inproceedings{actpers1,
  title={Reinforcement Learning for Control with Multiple Frequencies},
  author={Lee, Jongmin and Lee, Byung-Jun and Kim, Kee-Eung},
  booktitle={Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS 2020)},
  year={2020},
  organization={Neural information processing systems foundation}
}

%OTHER

% other work learning action repetitions
@inproceedings{act-rep-cr,
  title={Towards TempoRL: Learning When to Act},
  author={Biedenkapp, Andr{\'e} and Rajan, Raghu and Hutter, Frank and Lindauer, Marius},
  booktitle={Workshop on Inductive Biases, Invariances and Generalization in Reinforcement Learning (BIG@ ICML’20)},
  year={2020}
}

% temporally extended exploration

@article{tempExt0,
  title={Temporally-extended $\{\backslash$epsilon$\}$-greedy exploration},
  author={Dabney, Will and Ostrovski, Georg and Barreto, Andr{\'e}},
  journal={arXiv preprint arXiv:2006.01782},
  year={2020}
}


% NEW REFERENCES

@article{redq,
  title={Randomized ensembled double q-learning: Learning fast without a model},
  author={Chen, Xinyue and Wang, Che and Zhou, Zijian and Ross, Keith},
  journal={arXiv preprint arXiv:2101.05982},
  year={2021}
}

@article{mbpo,
  title={When to trust your model: Model-based policy optimization},
  author={Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  journal={arXiv preprint arXiv:1906.08253},
  year={2019}
}

@inproceedings{sunrise,
  title={Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning},
  author={Lee, Kimin and Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={6131--6141},
  year={2021},
  organization={PMLR}
}

@article{handful-of-trials,
  title={Deep reinforcement learning in a handful of trials using probabilistic dynamics models},
  author={Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  journal={arXiv preprint arXiv:1805.12114},
  year={2018}
}


% overestimation in model-free

@inproceedings{over-sem-first,
  title={Issues in using function approximation for reinforcement learning},
  author={Thrun, Sebastian and Schwartz, Anton},
  booktitle={Proceedings of the Fourth Connectionist Models Summer School},
  pages={255--263},
  year={1993},
  organization={Hillsdale, NJ}
}

@book{over-sem-0,
  title={Estimator variance in reinforcement learning: Theoretical problems and practical solutions},
  author={Pendrith, Mark D and Ryan, Malcolm RK and others},
  year={1997},
  publisher={University of New South Wales, School of Computer Science and Engineering}
}

@article{over-sem-1,
  title={Bias and variance approximation in value function estimates},
  author={Mannor, Shie and Simester, Duncan and Sun, Peng and Tsitsiklis, John N},
  journal={Management Science},
  volume={53},
  number={2},
  pages={308--322},
  year={2007},
  publisher={INFORMS}
}

% Counteracting overestimation

@article{double-q-sem,
  title={Double Q-learning},
  author={Hasselt, Hado},
  journal={Advances in neural information processing systems},
  volume={23},
  pages={2613--2621},
  year={2010}
}

% re-weight bellman backups
@article{discor,
  title={Discor: Corrective feedback in reinforcement learning via distribution correction},
  author={Kumar, Aviral and Gupta, Abhishek and Levine, Sergey},
  journal={arXiv preprint arXiv:2003.07305},
  year={2020}
}

% also proposes reparameterization + analysis of bias

@article{maxmin-q-learning,
  title={Maxmin q-learning: Controlling the estimation bias of q-learning},
  author={Lan, Qingfeng and Pan, Yangchen and Fyshe, Alona and White, Martha},
  journal={arXiv preprint arXiv:2002.06487},
  year={2020}
}

% proposes alternative bias correction term

@inproceedings{bias-corr-q-learn, 
  title={Bias-corrected q-learning to control max-operator bias in q-learning},
  author={Lee, Donghun and Defourny, Boris and Powell, Warren B},
  booktitle={2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)},
  pages={93--99},
  year={2013},
  organization={IEEE}
}
% proposes interpolation between double-q and dqn targets (over/under estimation bias)
@inproceedings{weighted-double-q,
  title={Weighted Double Q-learning.},
  author={Zhang, Zongzhang and Pan, Zhiyuan and Kochenderfer, Mykel J},
  booktitle={IJCAI},
  pages={3455--3461},
  year={2017}
}

% compute max actions from average q estimates from K prev. learnt models (could be considered ensembling) to reduce the bias mag.
@inproceedings{average-dqn, 
  title={Averaged-dqn: Variance reduction and stabilization for deep reinforcement learning},
  author={Anschel, Oron and Baram, Nir and Shimkin, Nahum},
  booktitle={International conference on machine learning},
  pages={176--185},
  year={2017},
  organization={PMLR}
}


% ensembling

@article{boot-dqn,
  title={Deep exploration via bootstrapped DQN},
  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={4026--4034},
  year={2016}
}

@article{boot-dqn-ucb,
  title={UCB exploration via Q-ensembles},
  author={Chen, Richard Y and Sidor, Szymon and Abbeel, Pieter and Schulman, John},
  journal={arXiv preprint arXiv:1706.01502},
  year={2017}
}


% model-based seminal

@inproceedings{mbrl-sem-comparison,
  title={A comparison of direct and model-based reinforcement learning},
  author={Atkeson, Christopher G and Santamaria, Juan Carlos},
  booktitle={Proceedings of international conference on robotics and automation},
  volume={4},
  pages={3557--3564},
  year={1997},
  organization={IEEE}
}

%model-based other ensembing

% like PETS but propose to perform CEM on parameter space of learnt policy
@article{POPLIN,
  title={Exploring model-based planning with policy networks},
  author={Wang, Tingwu and Ba, Jimmy},
  journal={arXiv preprint arXiv:1906.08649},
  year={2019}
}

% Optimistic advantages

% proposes justification for biasing towards optimism at initialization
@article{optadv-justOrig,
  title={R-max-a general polynomial time algorithm for near-optimal reinforcement learning},
  author={Brafman, Ronen I and Tennenholtz, Moshe},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Oct},
  pages={213--231},
  year={2002}
}

@article{optadv-OAC,
  title={Better exploration with optimistic actor-critic},
  author={Ciosek, Kamil and Vuong, Quan and Loftin, Robert and Hofmann, Katja},
  journal={arXiv preprint arXiv:1910.12807},
  year={2019}
}

% Adaptivity

@inproceedings{autoRL-1meta,
  title={Meta learning via learned loss},
  author={Bechtle, Sarah and Molchanov, Artem and Chebotar, Yevgen and Grefenstette, Edward and Righetti, Ludovic and Sukhatme, Gaurav and Meier, Franziska},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)},
  pages={4161--4168},
  year={2021},
  organization={IEEE}
}

@article{autoRL0meta,
  title={Discovering reinforcement learning algorithms},
  author={Oh, Junhyuk and Hessel, Matteo and Czarnecki, Wojciech M and Xu, Zhongwen and van Hasselt, Hado and Singh, Satinder and Silver, David},
  journal={arXiv preprint arXiv:2007.08794},
  year={2020}
}

@inproceedings{autoRLmeta,
 author = {Xu, Zhongwen and van Hasselt, Hado P and Silver, David},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Meta-Gradient Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2018/file/2715518c875999308842e3455eda2fe3-Paper.pdf},
 volume = {31},
 year = {2018}
}



@article{autoRL1meta,
  title={Meta-gradient reinforcement learning with an objective discovered online},
  author={Xu, Zhongwen and van Hasselt, Hado and Hessel, Matteo and Oh, Junhyuk and Singh, Satinder and Silver, David},
  journal={arXiv preprint arXiv:2007.08433},
  year={2020}
}

@article{autoRL2meta,
  title={Evolving reinforcement learning algorithms},
  author={Co-Reyes, John D and Miao, Yingjie and Peng, Daiyi and Real, Esteban and Levine, Sergey and Le, Quoc V and Lee, Honglak and Faust, Aleksandra},
  journal={arXiv preprint arXiv:2101.03958},
  year={2021}
}

@misc{top,
      title={Tactical Optimism and Pessimism for Deep Reinforcement Learning}, 
      author={Ted Moskovitz and Jack Parker-Holder and Aldo Pacchiano and Michael Arbel and Michael I. Jordan},
      year={2021},
      eprint={2102.03765},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% distributional 

@inproceedings{distributional-q-belle,
  title={A distributional perspective on reinforcement learning},
  author={Bellemare, Marc G and Dabney, Will and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={449--458},
  year={2017},
  organization={PMLR}
}

@article{quantreg,
  title={Quantile regression},
  author={Koenker, Roger and Hallock, Kevin F},
  journal={Journal of economic perspectives},
  volume={15},
  number={4},
  pages={143--156},
  year={2001}
}

@inproceedings{distributional-qrdqn,
  title={Distributional reinforcement learning with quantile regression},
  author={Dabney, Will and Rowland, Mark and Bellemare, Marc G and Munos, R{\'e}mi},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@inproceedings{distributional-iqn,
  title={Implicit quantile networks for distributional reinforcement learning},
  author={Dabney, Will and Ostrovski, Georg and Silver, David and Munos, R{\'e}mi},
  booktitle={International conference on machine learning},
  pages={1096--1105},
  year={2018},
  organization={PMLR}
}

@article{deeper-deep-RL,
  title={Towards Deeper Deep Reinforcement Learning},
  author={Bjorck, Johan and Gomes, Carla P and Weinberger, Kilian Q},
  journal={arXiv preprint arXiv:2106.01151},
  year={2021}
}

@inproceedings{scheduled-exploration-noise,
  title={On the model-based stochastic value gradient for continuous reinforcement learning},
  author={Amos, Brandon and Stanton, Samuel and Yarats, Denis and Wilson, Andrew Gordon},
  booktitle={Learning for Dynamics and Control},
  pages={6--20},
  year={2021},
  organization={PMLR}
}

@article{dyna,
  title={Dyna, an integrated architecture for learning, planning, and reacting},
  author={Sutton, Richard S},
  journal={ACM Sigart Bulletin},
  volume={2},
  number={4},
  pages={160--163},
  year={1991},
  publisher={ACM New York, NY, USA}
}

% structure

@article{layernorm,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

% math utils


@article{foldednormal,
  title={The folded normal distribution},
  author={Leone, Fred C and Nelson, Lloyd S and Nottingham, RB},
  journal={Technometrics},
  volume={3},
  number={4},
  pages={543--550},
  year={1961},
  publisher={Taylor \& Francis}
}

@book{chidist,
  title={Statistical distributions},
  author={Forbes, Catherine and Evans, Merran and Hastings, Nicholas and Peacock, Brian},
  year={2011},
  publisher={John Wiley \& Sons}
}

@inproceedings{preln-trans,
  title={On layer normalization in the transformer architecture},
  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle={International Conference on Machine Learning},
  pages={10524--10533},
  year={2020},
  organization={PMLR}
}

@ARTICLE{mdp,
    author = "Richard Bellman",
     title = "A Markovian Decision Process",
   journal = "Indiana Univ. Math. J.",
  fjournal = "Indiana University Mathematics Journal",
    volume = 6,
      year = 1957,
     issue = 4,
     pages = "679--684",
      issn = "0022-2518",
     coden = "IUMJAB",
   mrclass = "",
}

@inproceedings{nstepBias,
author = {Kearns, Michael J. and Singh, Satinder P.},
title = {Bias-Variance Error Bounds for Temporal Difference Updates},
year = {2000},
isbn = {155860703X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Thirteenth Annual Conference on Computational Learning Theory},
pages = {142–147},
numpages = {6},
series = {COLT '00}
}

@inproceedings{dropblock,
 author = {Ghiasi, Golnaz and Lin, Tsung-Yi and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {DropBlock: A regularization method for convolutional networks},
 url = {https://proceedings.neurips.cc/paper/2018/file/7edcfb2d8f6a659ef4cd1e6c9b6d7079-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@article{cutout,  
  title={Improved Regularization of Convolutional Neural Networks with Cutout},  
  author={DeVries, Terrance and Taylor, Graham W},  
  journal={arXiv preprint arXiv:1708.04552},  
  year={2017}  
}

@article{checkerboard,
  author = {Odena, Augustus and Dumoulin, Vincent and Olah, Chris},
  title = {Deconvolution and Checkerboard Artifacts},
  journal = {Distill},
  year = {2016},
  url = {http://distill.pub/2016/deconv-checkerboard},
  doi = {10.23915/distill.00003}
}

@misc{parkerholder2022automated,
      title={Automated Reinforcement Learning (AutoRL): A Survey and Open Problems}, 
      author={Jack Parker-Holder and Raghu Rajan and Xingyou Song and André Biedenkapp and Yingjie Miao and Theresa Eimer and Baohe Zhang and Vu Nguyen and Roberto Calandra and Aleksandra Faust and Frank Hutter and Marius Lindauer},
      year={2022},
      eprint={2201.03916},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
moskovitz2021tactical,
title={Tactical Optimism and Pessimism for Deep Reinforcement Learning},
author={Ted Moskovitz and Jack Parker-Holder and Aldo Pacchiano and Michael Arbel and Michael Jordan},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=a4WgjcLeZIn}
}

@article{schmidhuberaug,
    author = {Cireşan, Dan Claudiu and Meier, Ueli and Gambardella, Luca Maria and Schmidhuber, Jürgen},
    title = "{Deep, Big, Simple Neural Nets for Handwritten Digit Recognition}",
    journal = {Neural Computation},
    volume = {22},
    number = {12},
    pages = {3207-3220},
    year = {2010},
    month = {12},
    abstract = "{Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35\\% error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning.}",
    issn = {0899-7667},
    doi = {10.1162/NECO_a_00052},
    url = {https://doi.org/10.1162/NECO\_a\_00052},
    eprint = {https://direct.mit.edu/neco/article-pdf/22/12/3207/842857/neco\_a\_00052.pdf},
}

@article{coherentgrads,
  title={Coherent gradients: An approach to understanding generalization in gradient descent-based optimization},
  author={Chatterjee, Satrajit},
  journal={arXiv preprint arXiv:2002.10657},
  year={2020}
}

@inproceedings{cnnspectralbias,
  title={On the spectral bias of neural networks},
  author={Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={5301--5310},
  year={2019},
  organization={PMLR}
}

@article{atariALE,
  title={The arcade learning environment: An evaluation platform for general agents},
  author={Bellemare, Marc G and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={47},
  pages={253--279},
  year={2013}
}

@article{machadoALEprotocol,
  title={Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents},
  author={Machado, Marlos C and Bellemare, Marc G and Talvitie, Erik and Veness, Joel and Hausknecht, Matthew and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={523--562},
  year={2018}
}

@inproceedings{rainbowDQN,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  booktitle={Thirty-second AAAI conference on artificial intelligence},
  year={2018}
}

@article{der-rainbow,
  title={When to use parametric models in reinforcement learning?},
  author={van Hasselt, Hado P and Hessel, Matteo and Aslanides, John},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={14322--14333},
  year={2019}
}

@article{learningpessimism,
  title={Learning pessimism for robust and efficient off-policy reinforcement learning},
  author={Cetin, Edoardo and Celiktutan, Oya},
  journal={arXiv preprint arXiv:2110.03375},
  year={2021}
}

@inproceedings{shift-invariant-again,
  title={Making convolutional networks shift-invariant again},
  author={Zhang, Richard},
  booktitle={International conference on machine learning},
  pages={7324--7334},
  year={2019},
  organization={PMLR}
}

@article{suttonTDlearning,
author = {Sutton, Richard},
year = {1988},
month = {08},
pages = {9-44},
title = {Learning to Predict by the Method of Temporal Differences},
volume = {3},
journal = {Machine Learning},
doi = {10.1007/BF00115009}
}


@InProceedings{memorizationCNNs,
  title = 	 {A Closer Look at Memorization in Deep Networks},
  author =       {Devansh Arpit and Stanisław Jastrzebski and Nicolas Ballas and David Krueger and Emmanuel Bengio and Maxinder S. Kanwal and Tegan Maharaj and Asja Fischer and Aaron Courville and Yoshua Bengio and Simon Lacoste-Julien},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {233--242},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/arpit17a/arpit17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/arpit17a.html},
  abstract = 	 {We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs.~real data. We also demonstrate that for appropriately tuned explicit regularization (e.g.,~dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.}
}

@inproceedings{
mindThePad,
title={Mind the Pad -- {\{}CNN{\}}s Can Develop Blind Spots},
author={Bilal Alsallakh and Narine Kokhlikyan and Vivek Miglani and Jun Yuan and Orion Reblitz-Richardson},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=m1CD7tPubNy}
}

@inproceedings{sharpLossBadGeneralization,
  author    = {Nitish Shirish Keskar and
               Dheevatsa Mudigere and
               Jorge Nocedal and
               Mikhail Smelyanskiy and
               Ping Tak Peter Tang},
  title     = {On Large-Batch Training for Deep Learning: Generalization Gap and
               Sharp Minima},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=H1oyRlYgg},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/KeskarMNST17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{RL-that-matters,
  title={Deep reinforcement learning that matters},
  author={Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{deadly_triad,
  title={Deep reinforcement learning and the deadly triad},
  author={Van Hasselt, Hado and Doron, Yotam and Strub, Florian and Hessel, Matteo and Sonnerat, Nicolas and Modayil, Joseph},
  journal={arXiv preprint arXiv:1812.02648},
  year={2018}
}

@article{RL_stability_practical,
  title={Reinforcement learning for control: Performance, stability, and deep approximators},
  author={Bu{\c{s}}oniu, Lucian and de Bruin, Tim and Toli{\'c}, Domagoj and Kober, Jens and Palunko, Ivana},
  journal={Annual Reviews in Control},
  volume={46},
  pages={8--28},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{RL_online_transformers,
  title={Stabilizing transformers for reinforcement learning},
  author={Parisotto, Emilio and Song, Francis and Rae, Jack and Pascanu, Razvan and Gulcehre, Caglar and Jayakumar, Siddhant and Jaderberg, Max and Kaufman, Raphael Lopez and Clark, Aidan and Noury, Seb and others},
  booktitle={International Conference on Machine Learning},
  pages={7487--7498},
  year={2020},
  organization={PMLR}
}

@article{procgen_comp_bench,
  title={Measuring Sample Efficiency and Generalization in Reinforcement Learning Benchmarks: NeurIPS 2020 Procgen Benchmark},
  author={Mohanty, Sharada and Poonganam, Jyotish and Gaidon, Adrien and Kolobov, Andrey and Wulfe, Blake and Chakraborty, Dipam and {\v{S}}emetulskis, Gra{\v{z}}vydas and Schapke, Jo{\~a}o and Kubilius, Jonas and Pa{\v{s}}ukonis, Jurgis and others},
  journal={arXiv preprint arXiv:2103.15332},
  year={2021}
}

@article{impala,
  title={Measuring Sample Efficiency and Generalization in Reinforcement Learning Benchmarks: NeurIPS 2020 Procgen Benchmark},
  author={Mohanty, Sharada and Poonganam, Jyotish and Gaidon, Adrien and Kolobov, Andrey and Wulfe, Blake and Chakraborty, Dipam and {\v{S}}emetulskis, Gra{\v{z}}vydas and Schapke, Jo{\~a}o and Kubilius, Jonas and Pa{\v{s}}ukonis, Jurgis and others},
  journal={arXiv preprint arXiv:2103.15332},
  year={2021}
}

@article{gae,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1506.02438},
  year={2015}
}

@inproceedings{ppg,
  title={Phasic policy gradient},
  author={Cobbe, Karl W and Hilton, Jacob and Klimov, Oleg and Schulman, John},
  booktitle={International Conference on Machine Learning},
  pages={2020--2027},
  year={2021},
  organization={PMLR}
}

@article{alphastar,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{OpenAI5,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Przemysław Debiak and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

@article{seminal_td_ana,
  title={An analysis of temporal-difference learning with function approximation},
  author={Tsitsiklis, John N and Van Roy, Benjamin},
  journal={IEEE transactions on automatic control},
  volume={42},
  number={5},
  pages={674--690},
  year={1997},
  publisher={IEEE}
}

@inproceedings{unstable_off,
  title={Benchmarking deep reinforcement learning for continuous control},
  author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={1329--1338},
  year={2016},
  organization={PMLR}
}

@article{rl_real_world_ingredients,
  title={The ingredients of real-world robotic reinforcement learning},
  author={Zhu, Henry and Yu, Justin and Gupta, Abhishek and Shah, Dhruv and Hartikainen, Kristian and Singh, Avi and Kumar, Vikash and Levine, Sergey},
  journal={arXiv preprint arXiv:2004.12570},
  year={2020}
}

@article{qt-opt,
  title={Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation},
  author={Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and others},
  journal={arXiv preprint arXiv:1806.10293},
  year={2018}
}

@article{deepspatial-ae,
  title={Learning visual feature spaces for robotic manipulation with deep spatial autoencoders},
  author={Finn, Chelsea and Tan, Xin Yu and Duan, Yan and Darrell, Trevor and Levine, Sergey and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1509.06113},
  volume={25},
  year={2015}
}

@article{spr-atari,
  title={Data-efficient reinforcement learning with self-predictive representations},
  author={Schwarzer, Max and Anand, Ankesh and Goel, Rishab and Hjelm, R Devon and Courville, Aaron and Bachman, Philip},
  journal={arXiv preprint arXiv:2007.05929},
  year={2020}
}

@inproceedings{actionable-repr,
  title={Learning actionable representations from visual observations},
  author={Dwibedi, Debidatta and Tompson, Jonathan and Lynch, Corey and Sermanet, Pierre},
  booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={1577--1584},
  year={2018},
  organization={IEEE}
}

@article{slac,
  title={Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model},
  author={Lee, Alex X and Nagabandi, Anusha and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1907.00953},
  year={2019}
}

@inproceedings{
rosca2020a,
title={A case for new neural networks smoothness constraints},
author={Mihaela Rosca and Theophane Weber and Arthur Gretton and Shakir Mohamed},
booktitle={''I Can't Believe It's Not Better!'' NeurIPS 2020 workshop},
year={2020},
url={https://openreview.net/forum?id=_b-uT9wCI-7}
}

@inproceedings{exploringgeneralization,
author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
title = {Exploring Generalization in Deep Learning},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {5949–5958},
numpages = {10},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@article{badglobalminimaexist,
  title={Bad global minima exist and sgd can reach them},
  author={Liu, Shengchao and Papailiopoulos, Dimitris and Achlioptas, Dimitris},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{continuallearning,
title = {Continual lifelong learning with neural networks: A review},
journal = {Neural Networks},
volume = {113},
pages = {54-71},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300231},
author = {German I. Parisi and Ronald Kemker and Jose L. Part and Christopher Kanan and Stefan Wermter},
keywords = {Continual learning, Lifelong learning, Catastrophic forgetting, Developmental systems, Memory consolidation},
abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.}
}

@inproceedings{
spectralNorm,
title={Spectral Normalization for Generative Adversarial Networks},
author={Takeru Miyato and Toshiki Kataoka and Masanori Koyama and Yuichi Yoshida},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=B1QRgziT-},
}

@InProceedings{specnormDQN,
  title = 	 {Spectral Normalisation for Deep Reinforcement Learning: An Optimisation Perspective},
  author =       {Gogianu, Florin and Berariu, Tudor and Rosca, Mihaela C and Clopath, Claudia and Busoniu, Lucian and Pascanu, Razvan},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {3734--3744},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/gogianu21a/gogianu21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/gogianu21a.html},
  abstract = 	 {Most of the recent deep reinforcement learning advances take an RL-centric perspective and focus on refinements of the training objective. We diverge from this view and show we can recover the performance of these developments not by changing the objective, but by regularising the value-function estimator. Constraining the Lipschitz constant of a single layer using spectral normalisation is sufficient to elevate the performance of a Categorical-DQN agent to that of a more elaborated agent on the challenging Atari domain. We conduct ablation studies to disentangle the various effects normalisation has on the learning dynamics and show that is sufficient to modulate the parameter updates to recover most of the performance of spectral normalisation. These findings hint towards the need to also focus on the neural component and its learning dynamics to tackle the peculiarities of Deep Reinforcement Learning.}
}

@misc{hoffman2019robust,
      title={Robust Learning with Jacobian Regularization}, 
      author={Judy Hoffman and Daniel A. Roberts and Sho Yaida},
      year={2019},
      eprint={1908.02729},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{labelnoise,
    author = {An, Guozhong},
    title = "{The Effects of Adding Noise During Backpropagation Training on a Generalization Performance}",
    journal = {Neural Computation},
    volume = {8},
    number = {3},
    pages = {643-674},
    year = {1996},
    month = {04},
    abstract = "{We study the effects of adding noise to the inputs, outputs, weight connections, and weight changes of multilayer feedforward neural networks during backpropagation training. We rigorously derive and analyze the objective functions that are minimized by the noise-affected training processes. We show that input noise and weight noise encourage the neural-network output to be a smooth function of the input or its weights, respectively. In the weak-noise limit, noise added to the output of the neural networks only changes the objective function by a constant. Hence, it cannot improve generalization. Input noise introduces penalty terms in the objective function that are related to, but distinct from, those found in the regularization approaches. Simulations have been performed on a regression and a classification problem to further substantiate our analysis. Input noise is found to be effective in improving the generalization performance for both problems. However, weight noise is found to be effective in improving the generalization performance only for the classification problem. Other forms of noise have practically no effect on generalization.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1996.8.3.643},
    url = {https://doi.org/10.1162/neco.1996.8.3.643},
    eprint = {https://direct.mit.edu/neco/article-pdf/8/3/643/813312/neco.1996.8.3.643.pdf},
}

@misc{nstepStudy,
      title={Understanding Multi-Step Deep Reinforcement Learning: A Systematic Study of the DQN Target}, 
      author={J. Fernando Hernandez-Garcia and Richard S. Sutton},
      year={2019},
      eprint={1901.07510},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@inproceedings{planet,
  title={Learning latent dynamics for planning from pixels},
  author={Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  booktitle={International Conference on Machine Learning},
  pages={2555--2565},
  year={2019},
  organization={PMLR}
}

@article{machadosticky,
  title={Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents},
  author={Machado, Marlos C and Bellemare, Marc G and Talvitie, Erik and Veness, Joel and Hausknecht, Matthew and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={523--562},
  year={2018}
}

@article{otrainbow,
  title={Do recent advancements in model-based deep reinforcement learning really improve data efficiency?},
  author={Kielak, Kacper Piotr},
  year={2019}
}

@article{wilcoxon,
 ISSN = {00994987},
 URL = {http://www.jstor.org/stable/3001968},
 author = {Frank Wilcoxon},
 journal = {Biometrics Bulletin},
 number = {6},
 pages = {80--83},
 publisher = {[International Biometric Society, Wiley]},
 title = {Individual Comparisons by Ranking Methods},
 volume = {1},
 year = {1945}
}

@article{studentttest,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2331554},
 author = {Student},
 journal = {Biometrika},
 number = {1},
 pages = {1--25},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {The Probable Error of a Mean},
 volume = {6},
 year = {1908}
}

@article{mannwhitneyUstat,
author = {H. B. Mann and D. R. Whitney},
title = {{On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other}},
volume = {18},
journal = {The Annals of Mathematical Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {50 -- 60},
year = {1947},
doi = {10.1214/aoms/1177730491},
URL = {https://doi.org/10.1214/aoms/1177730491}
}

@article{perf-profile,
  title={Benchmarking optimization software with performance profiles},
  author={Dolan, Elizabeth D and Mor{\'e}, Jorge J},
  journal={Mathematical programming},
  volume={91},
  number={2},
  pages={201--213},
  year={2002},
  publisher={Springer}
}

@incollection{bootstrap-cis,
  title={Bootstrap methods: another look at the jackknife},
  author={Efron, Bradley},
  booktitle={Breakthroughs in statistics},
  pages={569--593},
  year={1992},
  publisher={Springer}
}

@inproceedings{stoch-dom,
  title={Deep dominance-how to properly compare deep neural models},
  author={Dror, Rotem and Shlomov, Segev and Reichart, Roi},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2773--2785},
  year={2019}
}

% GRADIENT-TD

@article{gradient-TD-Baird,
  title={Gradient descent for general reinforcement learning},
  author={Baird, Leemon and Moore, Andrew},
  journal={Advances in neural information processing systems},
  volume={11},
  year={1998}
}

@techreport{gradient-TD-Baird-2,
  title={Reinforcement learning through gradient descent},
  author={Baird, Leemon},
  year={1999},
  institution={Carnegie-Mellon University, Department of Computer Science}
}
