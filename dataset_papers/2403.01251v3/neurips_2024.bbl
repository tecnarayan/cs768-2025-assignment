\begin{thebibliography}{71}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen,
  DasSarma, Drain, Fort, Ganguli, Henighan et~al.}]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
  Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.
  2022{\natexlab{a}}.
\newblock Training a helpful and harmless assistant with reinforcement learning
  from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}.

\bibitem[{Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion,
  Jones, Chen, Goldie, Mirhoseini, McKinnon et~al.}]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
  Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
  et~al. 2022{\natexlab{b}}.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:1877--1901.

\bibitem[{Burns et~al.(2024)Burns, Izmailov, Kirchner, Baker, Gao,
  Aschenbrenner, Chen, Ecoffet, Joglekar, Leike et~al.}]{burns2023weak}
Collin Burns, Pavel Izmailov, Jan~Hendrik Kirchner, Bowen Baker, Leo Gao,
  Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan
  Leike, et~al. 2024.
\newblock Weak-to-strong generalization: Eliciting strong capabilities with
  weak supervision.
\newblock In \emph{Forty-first International Conference on Machine Learning}.

\bibitem[{Cai et~al.(2024)Cai, Li, Geng, Peng, Lee, Chen, and
  Dao}]{cai2024medusa}
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason~D Lee, Deming Chen,
  and Tri Dao. 2024.
\newblock Medusa: Simple llm inference acceleration framework with multiple
  decoding heads.
\newblock In \emph{Forty-first International Conference on Machine Learning}.

\bibitem[{Chao et~al.()Chao, Robey, Dobriban, Hassani, Pappas, and
  Wong}]{chao2023jailbreaking}
Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George~J Pappas,
  and Eric Wong.
\newblock Jailbreaking black box large language models in twenty queries.
\newblock In \emph{R0-FoMo: Robustness of Few-shot and Zero-shot Learning in
  Large Foundation Models}.

\bibitem[{Chen et~al.(2023)Chen, Borgeaud, Irving, Lespiau, Sifre, and
  Jumper}]{chen2023accelerating}
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau,
  Laurent Sifre, and John Jumper. 2023.
\newblock Accelerating large language model decoding with speculative sampling.
\newblock \emph{arXiv preprint arXiv:2302.01318}.

\bibitem[{Cheng et~al.(2023)Cheng, Yang, Li, Dai, and
  Du}]{cheng2023adversarial}
Pengyu Cheng, Yifan Yang, Jian Li, Yong Dai, and Nan Du. 2023.
\newblock Adversarial preference optimization.
\newblock \emph{arXiv preprint arXiv:2311.08045}.

\bibitem[{Cho et~al.(2023)Cho, Jeong, yeon Seo, and Park}]{cho2023discrete}
Sukmin Cho, Soyeong Jeong, Jeong yeon Seo, and Jong Park. 2023.
\newblock Discrete prompt optimization via constrained generation for zero-shot
  re-ranker.
\newblock In \emph{The 61st Annual Meeting Of The Association For Computational
  Linguistics}.

\bibitem[{Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann et~al.}]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al. 2023.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{Journal of Machine Learning Research}, 24(240):1--113.

\bibitem[{Chung et~al.(2024)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma et~al.}]{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al. 2024.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{Journal of Machine Learning Research}, 25(70):1--53.

\bibitem[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman}]{cobbe2021gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  Christopher Hesse, and John Schulman. 2021.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}.

\bibitem[{Dai et~al.(2021)Dai, Liu, Le, and Tan}]{dai2021coatnet}
Zihang Dai, Hanxiao Liu, Quoc~V Le, and Mingxing Tan. 2021.
\newblock Coatnet: Marrying convolution and attention for all data sizes.
\newblock \emph{Advances in neural information processing systems},
  34:3965--3977.

\bibitem[{Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and
  R{\'e}}]{dao2022flashattention}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}. 2022.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:16344--16359.

\bibitem[{Deng et~al.(2024)Deng, Zhang, Pan, and Bing}]{deng2023multilingual}
Yue Deng, Wenxuan Zhang, Sinno~Jialin Pan, and Lidong Bing. 2024.
\newblock Multilingual jailbreak challenges in large language models.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}.

\bibitem[{Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,
  He, Thite, Nabeshima et~al.}]{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al. 2020.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}.

\bibitem[{Geisler et~al.(2024)Geisler, Wollschl{\"a}ger, Abdalla, Gasteiger,
  and G{\"u}nnemann}]{geisler2024attacking}
Simon Geisler, Tom Wollschl{\"a}ger, MHI Abdalla, Johannes Gasteiger, and
  Stephan G{\"u}nnemann. 2024.
\newblock Attacking large language models with projected gradient descent.
\newblock \emph{arXiv preprint arXiv:2402.09154}.

\bibitem[{Goodman et~al.(1979)Goodman, Kruskal, Goodman, and
  Kruskal}]{goodman1979measures}
Leo~A Goodman, William~H Kruskal, Leo~A Goodman, and William~H Kruskal. 1979.
\newblock \emph{Measures of association for cross classifications}.
\newblock Springer.

\bibitem[{Gu et~al.(2020)Gu, Dao, Ermon, Rudra, and R{\'e}}]{gu2020hippo}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R{\'e}. 2020.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock \emph{Advances in neural information processing systems},
  33:1474--1487.

\bibitem[{Gu et~al.(2021)Gu, Goel, and R{\'e}}]{gu2021efficiently}
Albert Gu, Karan Goel, and Christopher R{\'e}. 2021.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock \emph{arXiv preprint arXiv:2111.00396}.

\bibitem[{Gulcehre et~al.(2023)Gulcehre, Paine, Srinivasan, Konyushkova,
  Weerts, Sharma, Siddhant, Ahern, Wang, Gu et~al.}]{gulcehre2023reinforced}
Caglar Gulcehre, Tom~Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte
  Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie
  Gu, et~al. 2023.
\newblock Reinforced self-training (rest) for language modeling.
\newblock \emph{arXiv preprint arXiv:2308.08998}.

\bibitem[{Guo et~al.(2021)Guo, Sablayrolles, J{\'e}gou, and
  Kiela}]{guo2021gradient}
Chuan Guo, Alexandre Sablayrolles, Herv{\'e} J{\'e}gou, and Douwe Kiela. 2021.
\newblock Gradient-based adversarial attacks against text transformers.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 5747--5757.

\bibitem[{Hartvigsen et~al.(2022)Hartvigsen, Gabriel, Palangi, Sap, Ray, and
  Kamar}]{hartvigsen2022toxigen}
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray,
  and Ece Kamar. 2022.
\newblock Toxigen: A large-scale machine-generated dataset for adversarial and
  implicit hate speech detection.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 3309--3326.

\bibitem[{He(2023)}]{gptfast}
Horace He. 2023.
\newblock {GPT-Fast}.
\newblock [Online]. Available:
  \url{https://github.com/pytorch-labs/gpt-fast/tree/main?tab=readme-ov-file}.
\newblock Accessed: Feb. 2, 2024.

\bibitem[{He et~al.(2024)He, Zhong, Cai, Lee, and He}]{he2023rest}
Zhenyu He, Zexuan Zhong, Tianle Cai, Jason Lee, and Di~He. 2024.
\newblock Rest: Retrieval-based speculative decoding.
\newblock In \emph{Proceedings of the 2024 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies (Volume 1: Long Papers)}, pages 1582--1595.

\bibitem[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt}]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
  Song, and Jacob Steinhardt. 2020.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Jang et~al.(2016)Jang, Gu, and Poole}]{jang2016categorical}
Eric Jang, Shixiang Gu, and Ben Poole. 2016.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot,
  Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
  Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel,
  Guillaume Lample, Lucile Saulnier, et~al. 2023.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}.

\bibitem[{Kaddour et~al.(2023)Kaddour, Harris, Mozes, Bradley, Raileanu, and
  McHardy}]{kaddour2023challenges}
Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta
  Raileanu, and Robert McHardy. 2023.
\newblock Challenges and applications of large language models.
\newblock \emph{arXiv preprint arXiv:2307.10169}.

\bibitem[{Kendall(1938)}]{kendall1938new}
Maurice~G Kendall. 1938.
\newblock A new measure of rank correlation.
\newblock \emph{Biometrika}, 30(1/2):81--93.

\bibitem[{Lermen and Rogers-Smith(2024)}]{lermen2023lora}
Simon Lermen and Charlie Rogers-Smith. 2024.
\newblock Lora fine-tuning efficiently undoes safety training in llama 2-chat
  70b.
\newblock In \emph{ICLR 2024 Workshop on Secure and Trustworthy Large Language
  Models}.

\bibitem[{Lester et~al.(2021)Lester, Al-Rfou, and Constant}]{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 3045--3059.

\bibitem[{Leviathan et~al.(2023)Leviathan, Kalman, and
  Matias}]{leviathan2023fast}
Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023.
\newblock Fast inference from transformers via speculative decoding.
\newblock In \emph{International Conference on Machine Learning}, pages
  19274--19286. PMLR.

\bibitem[{Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer}]{lewis2019bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock \emph{arXiv preprint arXiv:1910.13461}.

\bibitem[{Li et~al.(2023)Li, Bubeck, Eldan, Del~Giorno, Gunasekar, and
  Lee}]{textbooks2}
Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya
  Gunasekar, and Yin~Tat Lee. 2023.
\newblock Textbooks are all you need ii: \textbf{phi-1.5} technical report.
\newblock \emph{arXiv preprint arXiv:2309.05463}.

\bibitem[{Liu et~al.(2021)Liu, Dai, So, and Le}]{liu2021pay}
Hanxiao Liu, Zihang Dai, David So, and Quoc~V Le. 2021.
\newblock Pay attention to mlps.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:9204--9215.

\bibitem[{Liu et~al.(2024)Liu, Xu, Chen, and Xiao}]{liu2023autodan}
Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. 2024.
\newblock Autodan: Generating stealthy jailbreak prompts on aligned large
  language models.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}.

\bibitem[{Liu et~al.(2023)Liu, Yao, Ton, Zhang, Guo, Cheng, Klochkov, Taufiq,
  and Li}]{liu2023trustworthy}
Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao
  Cheng, Yegor Klochkov, Muhammad~Faaiz Taufiq, and Hang Li. 2023.
\newblock Trustworthy llms: a survey and guideline for evaluating large
  language models' alignment.
\newblock In \emph{Socially Responsible Language Modelling Research}.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov}]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}.

\bibitem[{Long et~al.(2024)Long, Zhao, Brown, Xie, Zhao, Chen, Kawaguchi,
  Shieh, and He}]{do2023prompt}
Do~Long, Yiran Zhao, Hannah Brown, Yuxi Xie, James Zhao, Nancy Chen, Kenji
  Kawaguchi, Michael Shieh, and Junxian He. 2024.
\newblock \href {https://doi.org/10.18653/v1/2024.acl-long.395} {Prompt
  optimization via adversarial in-context learning}.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 7308--7327,
  Bangkok, Thailand. Association for Computational Linguistics.

\bibitem[{Lu et~al.(2023)Lu, Qiu, Chang, Wu, Zhu, Rajpurohit, Clark, and
  Kalyan}]{ludynamic}
Pan Lu, Liang Qiu, Kai-Wei Chang, Ying~Nian Wu, Song-Chun Zhu, Tanmay
  Rajpurohit, Peter Clark, and Ashwin Kalyan. 2023.
\newblock Dynamic prompt learning via policy gradient for semi-structured
  mathematical reasoning.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[{Maddison et~al.(2022)Maddison, Mnih, and Teh}]{maddison2016concrete}
Chris~J Maddison, Andriy Mnih, and Yee~Whye Teh. 2022.
\newblock The concrete distribution: A continuous relaxation of discrete random
  variables.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Marelli et~al.(2014)Marelli, Menini, Baroni, Bentivogli, Bernardi,
  and Zamparelli}]{marelli-etal-2014-sick}
Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella
  Bernardi, and Roberto Zamparelli. 2014.
\newblock \href
  {http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf} {A {SICK}
  cure for the evaluation of compositional distributional semantic models}.
\newblock In \emph{Proceedings of the Ninth International Conference on
  Language Resources and Evaluation ({LREC}'14)}, pages 216--223, Reykjavik,
  Iceland. European Language Resources Association (ELRA).

\bibitem[{Mingkai and Jianyu(2022)}]{mingkai2022rlprompt}
Deng Mingkai and Wang Jianyu. 2022.
\newblock Rlprompt: Optimizing discrete text prompts with reinforcement
  learning.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}.

\bibitem[{M{\"o}kander et~al.(2023)M{\"o}kander, Schuett, Kirk, and
  Floridi}]{mokander2023auditing}
Jakob M{\"o}kander, Jonas Schuett, Hannah~Rose Kirk, and Luciano Floridi. 2023.
\newblock Auditing large language models: a three-layered approach.
\newblock \emph{AI and Ethics}, pages 1--31.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray et~al.}]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
  2022.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:27730--27744.

\bibitem[{Pearson(1900)}]{pearson1900x}
Karl Pearson. 1900.
\newblock X. on the criterion that a given system of deviations from the
  probable in the case of a correlated system of variables is such that it can
  be reasonably supposed to have arisen from random sampling.
\newblock \emph{The London, Edinburgh, and Dublin Philosophical Magazine and
  Journal of Science}, 50(302):157--175.

\bibitem[{Pincus(1970)}]{pincus1970monte}
Martin Pincus. 1970.
\newblock A monte carlo method for the approximate solution of certain types of
  constrained optimization problems.
\newblock \emph{Operations research}, 18(6):1225--1228.

\bibitem[{Pryzant et~al.(2023)Pryzant, Iter, Li, Lee, Zhu, and
  Zeng}]{pryzant2023automatic}
Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng.
  2023.
\newblock Automatic prompt optimization with “gradient descent” and beam
  search.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in
  Natural Language Processing}, pages 7957--7968.

\bibitem[{Qi et~al.(2024)Qi, Zeng, Xie, Chen, Jia, Mittal, and
  Henderson}]{qi2023fine}
Xiangyu Qi, Yi~Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and
  Peter Henderson. 2024.
\newblock Fine-tuning aligned language models compromises safety, even when
  users do not intend to!
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever
  et~al.}]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al. 2019.
\newblock Language models are unsupervised multitask learners.

\bibitem[{Rando and Tram{\`e}r(2023)}]{rando2023universal}
Javier Rando and Florian Tram{\`e}r. 2023.
\newblock Universal jailbreak backdoors from poisoned human feedback.
\newblock \emph{arXiv preprint arXiv:2311.14455}.

\bibitem[{Shayegani et~al.(2024)Shayegani, Dong, and
  Abu-Ghazaleh}]{shayegani2023jailbreak}
Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. 2024.
\newblock Jailbreak in pieces: Compositional adversarial attacks on multi-modal
  language models.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}.

\bibitem[{Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and
  Singh}]{shin2020autoprompt}
Taylor Shin, Yasaman Razeghi, Robert~L Logan~IV, Eric Wallace, and Sameer
  Singh. 2020.
\newblock Autoprompt: Eliciting knowledge from language models with
  automatically generated prompts.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 4222--4235.

\bibitem[{So et~al.(2019)So, Le, and Liang}]{so2019evolved}
David So, Quoc Le, and Chen Liang. 2019.
\newblock The evolved transformer.
\newblock In \emph{International conference on machine learning}, pages
  5877--5886. PMLR.

\bibitem[{Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts}]{socher2013recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D Manning,
  Andrew~Y Ng, and Christopher Potts. 2013.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pages 1631--1642.

\bibitem[{Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss,
  Radford, Amodei, and Christiano}]{stiennon2020learning}
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea
  Voss, Alec Radford, Dario Amodei, and Paul~F Christiano. 2020.
\newblock Learning to summarize with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:3008--3021.

\bibitem[{Suzgun et~al.(2023)Suzgun, Scales, Sch{\"a}rli, Gehrmann, Tay, Chung,
  Chowdhery, Le, Chi, Zhou et~al.}]{suzgun2023challenging}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"a}rli, Sebastian Gehrmann, Yi~Tay,
  Hyung~Won Chung, Aakanksha Chowdhery, Quoc Le, Ed~Chi, Denny Zhou, et~al.
  2023.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve
  them.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL 2023}, pages 13003--13051.

\bibitem[{Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi,
  Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al. 2023.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}.

\bibitem[{Toyer et~al.(2023)Toyer, Watkins, Mendes, Svegliato, Bailey, Wang,
  Ong, Elmaaroufi, Abbeel, Darrell et~al.}]{toyer2023tensor}
Sam Toyer, Olivia Watkins, Ethan Mendes, Justin Svegliato, Luke Bailey, Tiffany
  Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, et~al.
  2023.
\newblock Tensor trust: Interpretable prompt injection attacks from an online
  game.
\newblock In \emph{NeurIPS 2023 Workshop on Instruction Tuning and Instruction
  Following}.

\bibitem[{Wen et~al.(2024)Wen, Jain, Kirchenbauer, Goldblum, Geiping, and
  Goldstein}]{wen2023hard}
Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom
  Goldstein. 2024.
\newblock Hard prompts made easy: Gradient-based discrete optimization for
  prompt tuning and discovery.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Xia et~al.(2023)Xia, Gao, Zeng, and Chen}]{xia2023sheared}
Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2023.
\newblock Sheared llama: Accelerating language model pre-training via
  structured pruning.
\newblock In \emph{Workshop on Advancing Neural Network Training: Computational
  Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023)}.

\bibitem[{Xu et~al.(2023)Xu, Chern, Chern, Zhang, Wang, Liu, Li, Fu, and
  Liu}]{xu2023align}
Chunpu Xu, Steffi Chern, Ethan Chern, Ge~Zhang, Zekun Wang, Ruibo Liu, Jing Li,
  Jie Fu, and Pengfei Liu. 2023.
\newblock Align on the fly: Adapting chatbot behavior to established norms.
\newblock \emph{arXiv preprint arXiv:2312.15907}.

\bibitem[{Xu et~al.(2022)Xu, Chen, Du, Shao, Yanggang, Li, and
  Yang}]{xu2022gps}
Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Wang Yanggang, Haiyu Li, and Zhilin
  Yang. 2022.
\newblock Gps: Genetic prompt search for efficient few-shot learning.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 8162--8171.

\bibitem[{Yuan et~al.(2024{\natexlab{a}})Yuan, Pang, Cho, Li, Sukhbaatar, Xu,
  and Weston}]{yuan2024self}
Weizhe Yuan, Richard~Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar
  Sukhbaatar, Jing Xu, and Jason~E Weston. 2024{\natexlab{a}}.
\newblock Self-rewarding language models.
\newblock In \emph{Forty-first International Conference on Machine Learning}.

\bibitem[{Yuan et~al.(2024{\natexlab{b}})Yuan, Jiao, Wang, Huang, He, Shi, and
  Tu}]{yuan2023gpt}
Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming
  Shi, and Zhaopeng Tu. 2024{\natexlab{b}}.
\newblock Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}.

\bibitem[{Zar(2005)}]{zar2005spearman}
Jerrold~H Zar. 2005.
\newblock Spearman rank correlation.
\newblock \emph{Encyclopedia of biostatistics}, 7.

\bibitem[{Zhang et~al.(2024)Zhang, Zeng, Wang, and Lu}]{zhang2024tinyllama}
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024.
\newblock Tinyllama: An open-source small language model.
\newblock \emph{arXiv preprint arXiv:2401.02385}.

\bibitem[{Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li,
  Li, Xing et~al.}]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
  Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al. 2023.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock \emph{Advances in Neural Information Processing Systems},
  36:46595--46623.

\bibitem[{Zhou et~al.(2022)Zhou, Muresanu, Han, Paster, Pitis, Chan, and
  Ba}]{zhou2022large}
Yongchao Zhou, Andrei~Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis,
  Harris Chan, and Jimmy Ba. 2022.
\newblock Large language models are human-level prompt engineers.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[{Zou et~al.(2023)Zou, Wang, Kolter, and Fredrikson}]{zou2023universal}
Andy Zou, Zifan Wang, J~Zico Kolter, and Matt Fredrikson. 2023.
\newblock Universal and transferable adversarial attacks on aligned language
  models.
\newblock \emph{arXiv preprint arXiv:2307.15043}.

\end{thebibliography}
