@article{allen2018convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  journal={arXiv preprint arXiv:1811.03962},
  year={2018}
}
@inproceedings{allen2019learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in neural information processing systems},
  pages={6155--6166},
  year={2019}
}
@article{arora2020dropout,
  title={Dropout: Explicit Forms and Capacity Control},
  author={Arora, Raman and Bartlett, Peter and Mianjy, Poorya and Srebro, Nathan},
  journal={arXiv preprint arXiv:2003.03397},
  year={2020}
}
@article{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  journal={arXiv preprint arXiv:1901.08584},
  year={2019}
}
@inproceedings{baldi2013understanding,
  title={Understanding dropout},
  author={Baldi, Pierre and Sadowski, Peter J},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  pages={2814--2822},
  year={2013}
}
@inproceedings{beygelzimer2011contextual,
  title={Contextual bandit algorithms with supervised learning guarantees},
  author={Beygelzimer, Alina and Langford, John and Li, Lihong and Reyzin, Lev and Schapire, Robert},
  booktitle={Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages={19--26},
  year={2011}
}
@article{brutzkus2017sgd,
  title={{SGD} learns over-parameterized networks that provably generalize on linearly separable data},
  author={Brutzkus, Alon and Globerson, Amir and Malach, Eran and Shalev-Shwartz, Shai}, journal={arXiv preprint arXiv:1710.10174}, year={2017}
 }
@article{cao2019generalization1,
  title={Generalization error bounds of gradient descent for learning overparameterized deep relu networks},
  author={Cao, Yuan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1902.01384},
  year={2019}
}
@article{chizat2018lazy,
  title={On Lazy Training in Differentiable Programming. arXiv e-prints, page},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={arXiv preprint arXiv:1812.07956},
  year={2018}
}
@inproceedings{dahl2013improving,
  title={Improving deep neural networks for {LVCSR} using rectified linear units and dropout},
  author={Dahl, George E and Sainath, Tara N and Hinton, Geoffrey E},
  booktitle={2013 IEEE international conference on acoustics, speech and signal processing},
  pages={8609--8613},
  year={2013},
  organization={IEEE}
}
@inproceedings{daniely2017sgd,
  title={SGD learns the conjugate kernel class of the network},
  author={Daniely, Amit},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2422--2430},
  year={2017}
}
@article{dauber2020can,
  title={Can Implicit Bias Explain Generalization? Stochastic Convex Optimization as a Case Study},
  author={Dauber, Assaf and Feder, Meir and Koren, Tomer and Livni, Roi},
  journal={arXiv preprint arXiv:2003.06152},
  year={2020}
}
@article{du2018gradient1,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon S and Lee, Jason D and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  journal={arXiv preprint arXiv:1811.03804},
  year={2018}
}
@inproceedings{
du2018gradient,
title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
author={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1eK3i09YQ},
}
@inproceedings{
frankle2018the,
title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
author={Jonathan Frankle and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJl-b3RcF7},
}
@inproceedings{gal2016dropout,
  title={Dropout as a Bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={Int. Conf. Machine Learning (ICML)},
  year={2016}
}
@article{gao2016dropout,
  title={Dropout Rademacher complexity of deep neural networks},
  author={Gao, Wei and Zhou, Zhi-Hua},
  journal={Science China Information Sciences},
  volume={59},
  number={7},
  pages={072104},
  year={2016},
  publisher={Springer}
}
@article{gomez2019learning,
  title={Learning sparse networks using targeted dropout},
  author={Gomez, Aidan N and Zhang, Ivan and Swersky, Kevin and Gal, Yarin and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1905.13678},
  year={2019}
}
@article{helmbold2015inductive,
  title={On the inductive bias of dropout.},
  author={Helmbold, David P and Long, Philip M},
  journal={Journal of Machine Learning Research (JMLR)},
  volume={16},
  pages={3403--3454},
  year={2015}
}
@article{hinton2012improving,
  title={Improving neural networks by preventing co-adaptation of feature detectors},
  author={Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
  journal={arXiv preprint arXiv:1207.0580},
  year={2012}
}
@inproceedings{
Hu2020Simple,
title={Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee},
author={Wei Hu and Zhiyuan Li and Dingli Yu},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Hke3gyHYwH}
}
@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}
@article{ji2019polylogarithmic,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1909.12292},
  year={2019}
}
@inproceedings{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  booktitle={Advances in neural information processing systems},
  pages={8570--8581},
  year={2019}
}
@article{li2019gradient,
  title={Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks},
  author={Li, Mingchen and Soltanolkotabi, Mahdi and Oymak, Samet},
  journal={arXiv preprint arXiv:1903.11680},
  year={2019}
}
@inproceedings{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8157--8166},
  year={2018}
}
@article{mcallester2013pac,
  title={A {PAC}-Bayesian tutorial with a dropout bound},
  author={McAllester, David},
  journal={arXiv preprint arXiv:1307.2118},
  year={2013}
}
@article{merity2017regularizing,
  title={Regularizing and optimizing {LSTM} language models},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1708.02182},
  year={2017}
}
@inproceedings{mianjy2018implicit,
  title={On the Implicit Bias of Dropout},
  author={Mianjy, Poorya and Arora, Raman and Vidal, Rene},
  booktitle={International Conference on Machine Learning},
  pages={3540--3548},
  year={2018}
}
@inproceedings{mianjy2019dropout,
  title={On Dropout and Nuclear Norm Regularization},
  author={Mianjy, Poorya and Arora, Raman},
  booktitle={International Conference on Machine Learning},
  pages={4575--4584},
  year={2019}
}
@inproceedings{molchanov2017variational,
  title={Variational dropout sparsifies deep neural networks},
  author={Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2498--2507},
  year={2017},
  organization={JMLR. org}
}
@article{neyshabur2017geometry,
  title={Geometry of optimization and implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Salakhutdinov, Ruslan and Srebro, Nathan},
  journal={arXiv preprint arXiv:1705.03071},
  year={2017}
}
@article{nitanda2019refined,
  title={Refined Generalization Analysis of Gradient Descent for Over-parameterized Two-layer Neural Networks with Smooth Activations on Classification Problems},
  author={Nitanda, Atsushi and Suzuki, Taiji},
  journal={arXiv preprint arXiv:1905.09870},
  year={2019}
}
@article{oymak2020towards,
  title={Towards moderate overparameterization: global convergence guarantees for training shallow neural networks},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  journal={IEEE Journal on Selected Areas in Information Theory},
  year={2020},
  publisher={IEEE}
}
@article{senen2020almost,
  title={Almost sure convergence of dropout algorithms for neural networks},
  author={Senen-Cerda, Albert and Sanders, Jaron},
  journal={arXiv preprint arXiv:2002.02247},
  year={2020}
}
@article{song2019quadratic,
  title={Quadratic suffices for over-parametrization via matrix chernoff bound},
  author={Song, Zhao and Yang, Xin},
  journal={arXiv preprint arXiv:1906.03593},
  year={2019}
}
@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting.},
  author={Srivastava, Nitish and Hinton, Geoffrey E and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={Journal of Machine Learning Research (JMLR)},
  volume={15},
  number={1},
  year={2014}
}
@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}
@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge University Press}
}
@inproceedings{wager2013dropout,
  title={Dropout training as adaptive regularization},
  author={Wager, Stefan and Wang, Sida and Liang, Percy S},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2013}
}
@inproceedings{wager2014altitude,
  title={Altitude training: Strong bounds for single-layer dropout},
  author={Wager, Stefan and Fithian, William and Wang, Sida and Liang, Percy S},
  booktitle={Adv. Neural Information Processing Systems},
  year={2014}
}
@inproceedings{wan2013regularization,
  title={Regularization of neural networks using dropconnect},
  author={Wan, Li and Zeiler, Matthew and Zhang, Sixin and Le Cun, Yann and Fergus, Rob},
  booktitle={International conference on machine learning},
  pages={1058--1066},
  year={2013}
}
@inproceedings{wei2019regularization,
  title={Regularization matters: Generalization and optimization of neural nets vs their induced kernel},
  author={Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9709--9721},
  year={2019}
}
@article{wei2020implicit,
  title={The Implicit and Explicit Regularization Effects of Dropout},
  author={Wei, Colin and Kakade, Sham and Ma, Tengyu},
  journal={arXiv preprint arXiv:2002.12915},
  year={2020}
}
@inproceedings{
zhai2018adaptive,
title={Adaptive Dropout with Rademacher Complexity Regularization},
author={Ke Zhai and Huan Wang},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=S1uxsye0Z},
}
@article{zou2018stochastic,
  title={Stochastic gradient descent optimizes over-parameterized deep relu networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:1811.08888},
  year={2018}
}
@inproceedings{cao2019generalization,
  title={Generalization bounds of stochastic gradient descent for wide and deep neural networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10835--10845},
  year={2019}
}
