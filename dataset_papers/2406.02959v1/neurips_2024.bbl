\begin{thebibliography}{10}

\bibitem{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
  Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
  Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{agarwal2024policy}
Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela~Ramos
  Garea, Matthieu Geist, and Olivier Bachem.
\newblock On-policy distillation of language models: Learning from
  self-generated mistakes.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
  Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
  et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock {\em arXiv preprint arXiv:2212.08073}, 2022.

\bibitem{cettolo2017overview}
Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian
  St{\"u}ker, Katsuitho Sudoh, Koichiro Yoshino, and Christian Federmann.
\newblock Overview of the iwslt 2017 evaluation campaign.
\newblock In {\em Proceedings of the 14th International Workshop on Spoken
  Language Translation}, pages 2--14, 2017.

\bibitem{chiang2023vicuna}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E Gonzalez, et~al.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality.
\newblock {\em See https://vicuna. lmsys. org (accessed 14 April 2023)},
  2(3):6, 2023.

\bibitem{ciosek2021imitation}
Kamil Ciosek.
\newblock Imitation learning by reinforcement learning.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{cui2023ultrafeedback}
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie,
  Zhiyuan Liu, and Maosong Sun.
\newblock Ultrafeedback: Boosting language models with high-quality feedback.
\newblock {\em arXiv preprint arXiv:2310.01377}, 2023.

\bibitem{dolly2023introducing}
Free Dolly.
\newblock Introducing the worldâ€™s first truly open instruction-tuned llm.
  databricks. com, 2023.

\bibitem{geng2023openllama}
Xinyang Geng and Hao Liu.
\newblock Openllama: An open reproduction of llama.
\newblock {\em URL: https://github. com/openlm-research/open\_llama}, 2023.

\bibitem{geva2021did}
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan
  Berant.
\newblock Did aristotle use a laptop? a question answering benchmark with
  implicit reasoning strategies.
\newblock {\em Transactions of the Association for Computational Linguistics},
  9:346--361, 2021.

\bibitem{gliwa2019samsum}
Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer.
\newblock Samsum corpus: A human-annotated dialogue dataset for abstractive
  summarization.
\newblock {\em arXiv preprint arXiv:1911.12237}, 2019.

\bibitem{gokaslan}
Aaron Gokaslan, Vanya Aaron, Ellie Pavlick, and Stefanie Tellex.
\newblock Openwebtext corpus.
\newblock 2019.

\bibitem{gu2023minillm}
Yuxian Gu, Li~Dong, Furu Wei, and Minlie Huang.
\newblock Minillm: Knowledge distillation of large language models.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{hao2022teacher}
Yongchang Hao, Yuxin Liu, and Lili Mou.
\newblock Teacher forcing recovers reward functions for text generation.
\newblock {\em Advances in Neural Information Processing Systems},
  35:12594--12607, 2022.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{honovich2022unnatural}
Or~Honovich, Thomas Scialom, Omer Levy, and Timo Schick.
\newblock Unnatural instructions: Tuning language models with (almost) no human
  labor.
\newblock {\em arXiv preprint arXiv:2212.09689}, 2022.

\bibitem{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock {\em arXiv preprint arXiv:2001.08361}, 2020.

\bibitem{kim2016sequence}
Yoon Kim and Alexander~M Rush.
\newblock Sequence-level knowledge distillation.
\newblock In {\em Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}. Association for Computational Linguistics,
  2016.

\bibitem{ko2024distillm}
Jongwoo Ko, Sungnyun Kim, Tianyi Chen, and Se-Young Yun.
\newblock Distillm: Towards streamlined distillation for large language models.
\newblock {\em arXiv preprint arXiv:2402.03898}, 2024.

\bibitem{liang2023less}
Chen Liang, Simiao Zuo, Qingru Zhang, Pengcheng He, Weizhu Chen, and Tuo Zhao.
\newblock Less is more: Task-aware layer-wise distillation for language model
  compression.
\newblock In {\em International Conference on Machine Learning}, pages
  20852--20867. PMLR, 2023.

\bibitem{lin2020autoregressive}
Alexander Lin, Jeremy Wohlwend, Howard Chen, and Tao Lei.
\newblock Autoregressive knowledge distillation through imitation learning.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 6121--6133, 2020.

\bibitem{lin2004rouge}
Chin-Yew Lin.
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In {\em Text summarization branches out}, pages 74--81, 2004.

\bibitem{pang2021text}
Richard~Yuanzhe Pang and He~He.
\newblock Text generation by learning from demonstrations.
\newblock In {\em 9th International Conference on Learning Representations,
  ICLR 2021}, 2021.

\bibitem{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In {\em Proceedings of the 40th annual meeting of the Association for
  Computational Linguistics}, pages 311--318, 2002.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em Journal of machine learning research}, 21(140):1--67, 2020.

\bibitem{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem{sriperumbudur2009integral}
Bharath~K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard
  Sch{\"o}lkopf, and Gert~RG Lanckriet.
\newblock On integral probability metrics,$\backslash$phi-divergences and
  binary classification.
\newblock {\em arXiv preprint arXiv:0901.2698}, 2009.

\bibitem{swamy2022sequence}
Gokul Swamy, Sanjiban Choudhury, J~Bagnell, and Steven~Z Wu.
\newblock Sequence model imitation learning with unobserved contexts.
\newblock {\em Advances in Neural Information Processing Systems},
  35:17665--17676, 2022.

\bibitem{swamy2021moments}
Gokul Swamy, Sanjiban Choudhury, J~Andrew Bagnell, and Steven Wu.
\newblock Of moments and matching: A game-theoretic framework for closing the
  imitation gap.
\newblock In {\em International Conference on Machine Learning}, pages
  10022--10032. PMLR, 2021.

\bibitem{taori2023stanford}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model, 2023.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{wang2022self}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language models with self-generated
  instructions.
\newblock {\em arXiv preprint arXiv:2212.10560}, 2022.

\bibitem{wang2022super}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
  Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut~Selvan Dhanasekaran, Atharva
  Naik, David Stap, et~al.
\newblock Super-naturalinstructions: Generalization via declarative
  instructions on 1600+ nlp tasks.
\newblock {\em arXiv preprint arXiv:2204.07705}, 2022.

\bibitem{wen2023f}
Yuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou.
\newblock f-divergence minimization for sequence-level knowledge distillation.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 10817--10834, 2023.

\bibitem{wu2021textgail}
Qingyang Wu, Lei Li, and Zhou Yu.
\newblock Textgail: Generative adversarial imitation learning for text
  generation.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 14067--14075, 2021.

\bibitem{wu2024rethinking}
Taiqiang Wu, Chaofan Tao, Jiahao Wang, Zhe Zhao, and Ngai Wong.
\newblock Rethinking kullback-leibler divergence in knowledge distillation for
  large language models.
\newblock {\em arXiv preprint arXiv:2404.02657}, 2024.

\bibitem{xu2024survey}
Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can
  Xu, Dacheng Tao, and Tianyi Zhou.
\newblock A survey on knowledge distillation of large language models.
\newblock {\em arXiv preprint arXiv:2402.13116}, 2024.

\bibitem{xue2020mt5}
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
  Siddhant, Aditya Barua, and Colin Raffel.
\newblock mt5: A massively multilingual pre-trained text-to-text transformer.
\newblock {\em arXiv preprint arXiv:2010.11934}, 2020.

\bibitem{yu2017seqgan}
Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
\newblock Seqgan: Sequence generative adversarial nets with policy gradient.
\newblock In {\em Proceedings of the AAAI conference on artificial
  intelligence}, volume~31, 2017.

\end{thebibliography}
