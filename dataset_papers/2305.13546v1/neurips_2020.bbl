\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainsworth et~al.(2022)Ainsworth, Hayase, and
  Srinivasa]{ainsworth2022git}
S.~K. Ainsworth, J.~Hayase, and S.~Srinivasa.
\newblock Git re-basin: Merging models modulo permutation symmetries.
\newblock \emph{arXiv preprint arXiv:2209.04836}, 2022.

\bibitem[Andrychowicz et~al.(2016)Andrychowicz, Denil, Gomez, Hoffman, Pfau,
  Schaul, Shillingford, and De~Freitas]{andrychowicz2016learning}
M.~Andrychowicz, M.~Denil, S.~Gomez, M.~W. Hoffman, D.~Pfau, T.~Schaul,
  B.~Shillingford, and N.~De~Freitas.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Bauer et~al.(2023)Bauer, Dupont, Brock, Rosenbaum, Schwarz, and
  Kim]{bauer2023spatial}
M.~Bauer, E.~Dupont, A.~Brock, D.~Rosenbaum, J.~Schwarz, and H.~Kim.
\newblock Spatial functa: Scaling functa to imagenet classification and
  generation.
\newblock \emph{arXiv preprint arXiv:2302.03130}, 2023.

\bibitem[Brea et~al.(2019)Brea, Simsek, Illing, and Gerstner]{brea2019weight}
J.~Brea, B.~Simsek, B.~Illing, and W.~Gerstner.
\newblock Weight-space symmetry in deep networks gives rise to permutation
  saddles, connected by equal-loss valleys across the loss landscape.
\newblock \emph{arXiv preprint arXiv:1907.02911}, 2019.

\bibitem[Cohen and Welling(2016)]{cohen2016group}
T.~Cohen and M.~Welling.
\newblock Group equivariant convolutional networks.
\newblock In \emph{International conference on machine learning}, pages
  2990--2999. PMLR, 2016.

\bibitem[De~Luigi et~al.(2023)De~Luigi, Cardace, Spezialetti, Ramirez, Salti,
  and Di~Stefano]{de2023deep}
L.~De~Luigi, A.~Cardace, R.~Spezialetti, P.~Z. Ramirez, S.~Salti, and
  L.~Di~Stefano.
\newblock Deep learning on implicit neural representations of shapes.
\newblock \emph{arXiv preprint arXiv:2302.05438}, 2023.

\bibitem[Deutsch et~al.(2019)Deutsch, Nijkamp, and Yang]{deutsch2019generative}
L.~Deutsch, E.~Nijkamp, and Y.~Yang.
\newblock A generative model for sampling high-performance and diverse weights
  for neural networks.
\newblock \emph{arXiv preprint arXiv:1905.02898}, 2019.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Dupont et~al.(2021)Dupont, Teh, and Doucet]{dupont2021generative}
E.~Dupont, Y.~W. Teh, and A.~Doucet.
\newblock Generative models as distributions of functions.
\newblock \emph{arXiv preprint arXiv:2102.04776}, 2021.

\bibitem[Dupont et~al.(2022)Dupont, Kim, Eslami, Rezende, and
  Rosenbaum]{dupont2022data}
E.~Dupont, H.~Kim, S.~Eslami, D.~Rezende, and D.~Rosenbaum.
\newblock From data to functa: Your data point is a function and you should
  treat it like one.
\newblock \emph{arXiv preprint arXiv:2201.12204}, 2022.

\bibitem[Eilertsen et~al.(2020)Eilertsen, J{\"o}nsson, Ropinski, Unger, and
  Ynnerman]{eilertsen2020classifying}
G.~Eilertsen, D.~J{\"o}nsson, T.~Ropinski, J.~Unger, and A.~Ynnerman.
\newblock Classifying the classifier: dissecting the weight space of neural
  networks.
\newblock \emph{arXiv preprint arXiv:2002.05688}, 2020.

\bibitem[Entezari et~al.(2021)Entezari, Sedghi, Saukh, and
  Neyshabur]{entezari2021role}
R.~Entezari, H.~Sedghi, O.~Saukh, and B.~Neyshabur.
\newblock The role of permutation invariance in linear mode connectivity of
  neural networks.
\newblock \emph{arXiv preprint arXiv:2110.06296}, 2021.

\bibitem[Finzi et~al.(2021)Finzi, Welling, and Wilson]{finzi2021practical}
M.~Finzi, M.~Welling, and A.~G. Wilson.
\newblock A practical method for constructing equivariant multilayer
  perceptrons for arbitrary matrix groups.
\newblock In \emph{International Conference on Machine Learning}, pages
  3318--3328. PMLR, 2021.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{garipov2018loss}
T.~Garipov, P.~Izmailov, D.~Podoprikhin, D.~P. Vetrov, and A.~G. Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of {DNN}s.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Ha et~al.(2016)Ha, Dai, and Le]{ha2016hypernetworks}
D.~Ha, A.~Dai, and Q.~V. Le.
\newblock Hypernetworks.
\newblock \emph{arXiv preprint arXiv:1609.09106}, 2016.

\bibitem[Hartford et~al.(2018)Hartford, Graham, Leyton-Brown, and
  Ravanbakhsh]{hartford2018deep}
J.~Hartford, D.~Graham, K.~Leyton-Brown, and S.~Ravanbakhsh.
\newblock Deep models of interactions across sets.
\newblock In \emph{International Conference on Machine Learning}, pages
  1909--1918. PMLR, 2018.

\bibitem[Hecht-Nielsen(1990)]{hecht1990algebraic}
R.~Hecht-Nielsen.
\newblock On the algebraic structure of feedforward network weight spaces.
\newblock In \emph{Advanced Neural Computers}, pages 129--135. Elsevier, 1990.

\bibitem[Jiang et~al.(2019)Jiang, Krishnan, Mobahi, and
  Bengio]{jiang2018predicting}
Y.~Jiang, D.~Krishnan, H.~Mobahi, and S.~Bengio.
\newblock Predicting the generalization gap in deep networks with margin
  distributions.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HJlQfnCqKX}.

\bibitem[Jiang et~al.(2021)Jiang, Natekar, Sharma, Aithal, Kashyap,
  Subramanyam, Lassance, Roy, Dziugaite, Gunasekar, et~al.]{jiang2021methods}
Y.~Jiang, P.~Natekar, M.~Sharma, S.~K. Aithal, D.~Kashyap, N.~Subramanyam,
  C.~Lassance, D.~M. Roy, G.~K. Dziugaite, S.~Gunasekar, et~al.
\newblock Methods and analysis of the first competition in predicting
  generalization of deep learning.
\newblock In \emph{NeurIPS 2020 Competition and Demonstration Track}, pages
  170--190. PMLR, 2021.

\bibitem[Kendall(1938)]{kendall1938new}
M.~G. Kendall.
\newblock A new measure of rank correlation.
\newblock \emph{Biometrika}, 30\penalty0 (1/2):\penalty0 81--93, 1938.

\bibitem[Knyazev et~al.(2021)Knyazev, Drozdzal, Taylor, and
  Romero~Soriano]{knyazev2021parameter}
B.~Knyazev, M.~Drozdzal, G.~W. Taylor, and A.~Romero~Soriano.
\newblock Parameter prediction for unseen deep architectures.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 29433--29448, 2021.

\bibitem[Kondor and Trivedi(2018)]{kondor2018generalization}
R.~Kondor and S.~Trivedi.
\newblock On the generalization of equivariance and convolution in neural
  networks to the action of compact groups.
\newblock In \emph{International Conference on Machine Learning}, pages
  2747--2755. PMLR, 2018.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Krueger et~al.(2017)Krueger, Huang, Islam, Turner, Lacoste, and
  Courville]{krueger2017bayesian}
D.~Krueger, C.-W. Huang, R.~Islam, R.~Turner, A.~Lacoste, and A.~Courville.
\newblock Bayesian hypernetworks.
\newblock \emph{arXiv preprint arXiv:1710.04759}, 2017.

\bibitem[LeCun et~al.(1995)LeCun, Bengio, et~al.]{lecun1995convolutional}
Y.~LeCun, Y.~Bengio, et~al.
\newblock Convolutional networks for images, speech, and time series.
\newblock \emph{The handbook of brain theory and neural networks},
  3361\penalty0 (10):\penalty0 1995, 1995.

\bibitem[Li and Malik(2016)]{li2016learning}
K.~Li and J.~Malik.
\newblock Learning to optimize.
\newblock \emph{arXiv preprint arXiv:1606.01885}, 2016.

\bibitem[Maron et~al.(2020)Maron, Litany, Chechik, and
  Fetaya]{maron2020learning}
H.~Maron, O.~Litany, G.~Chechik, and E.~Fetaya.
\newblock On learning sets of symmetric elements.
\newblock In \emph{International conference on machine learning}, pages
  6734--6744. PMLR, 2020.

\bibitem[Martin and Mahoney(2021)]{martin2021implicit}
C.~H. Martin and M.~W. Mahoney.
\newblock Implicit self-regularization in deep neural networks: Evidence from
  random matrix theory and implications for learning.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0
  (1):\penalty0 7479--7551, 2021.

\bibitem[Metz et~al.(2022)Metz, Harrison, Freeman, Merchant, Beyer, Bradbury,
  Agrawal, Poole, Mordatch, Roberts, et~al.]{metz2022velo}
L.~Metz, J.~Harrison, C.~D. Freeman, A.~Merchant, L.~Beyer, J.~Bradbury,
  N.~Agrawal, B.~Poole, I.~Mordatch, A.~Roberts, et~al.
\newblock Velo: Training versatile learned optimizers by scaling up.
\newblock \emph{arXiv preprint arXiv:2211.09760}, 2022.

\bibitem[Mildenhall et~al.(2020)Mildenhall, Srinivasan, Tancik, Barron,
  Ramamoorthi, and Ng]{mildenhall2020nerf}
B.~Mildenhall, P.~P. Srinivasan, M.~Tancik, J.~T. Barron, R.~Ramamoorthi, and
  R.~Ng.
\newblock Nerf: representing scenes as neural radiance fields for view
  synthesis (2020).
\newblock \emph{arXiv preprint arXiv:2003.08934}, 2020.

\bibitem[Navon et~al.(2023)Navon, Shamsian, Achituve, Fetaya, Chechik, and
  Maron]{navon2023equivariant}
A.~Navon, A.~Shamsian, I.~Achituve, E.~Fetaya, G.~Chechik, and H.~Maron.
\newblock Equivariant architectures for learning in deep weight spaces.
\newblock \emph{arXiv preprint arXiv:2301.12780}, 2023.

\bibitem[Park et~al.(2019)Park, Florence, Straub, Newcombe, and
  Lovegrove]{park2019deepsdf}
J.~J. Park, P.~Florence, J.~Straub, R.~Newcombe, and S.~Lovegrove.
\newblock Deepsdf: Learning continuous signed distance functions for shape
  representation.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 165--174, 2019.

\bibitem[Peebles et~al.(2022)Peebles, Radosavovic, Brooks, Efros, and
  Malik]{peebles2022learning}
W.~Peebles, I.~Radosavovic, T.~Brooks, A.~A. Efros, and J.~Malik.
\newblock Learning to learn with generative models of neural network
  checkpoints.
\newblock \emph{arXiv preprint arXiv:2209.12892}, 2022.

\bibitem[Qi et~al.(2016)Qi, Su, Mo, and Guibas]{qi2016pointnet}
C.~Qi, H.~Su, K.~Mo, and L.~Guibas.
\newblock Pointnet: deep learning on point sets for 3d classification and
  segmentation. cvpr (2017).
\newblock \emph{arXiv preprint arXiv:1612.00593}, 2016.

\bibitem[Rahimi and Recht(2007)]{rahimi2007random}
A.~Rahimi and B.~Recht.
\newblock Random features for large-scale kernel machines.
\newblock \emph{Advances in neural information processing systems}, 20, 2007.

\bibitem[Ravanbakhsh et~al.(2017)Ravanbakhsh, Schneider, and
  Poczos]{ravanbakhsh2017equivariance}
S.~Ravanbakhsh, J.~Schneider, and B.~Poczos.
\newblock Equivariance through parameter-sharing.
\newblock In \emph{International conference on machine learning}, pages
  2892--2901. PMLR, 2017.

\bibitem[Sitzmann et~al.(2020)Sitzmann, Martel, Bergman, Lindell, and
  Wetzstein]{sitzmann2020implicit}
V.~Sitzmann, J.~Martel, A.~Bergman, D.~Lindell, and G.~Wetzstein.
\newblock Implicit neural representations with periodic activation functions.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7462--7473, 2020.

\bibitem[Tatro et~al.(2020)Tatro, Chen, Das, Melnyk, Sattigeri, and
  Lai]{tatro2020optimizing}
N.~Tatro, P.-Y. Chen, P.~Das, I.~Melnyk, P.~Sattigeri, and R.~Lai.
\newblock Optimizing mode connectivity via neuron alignment.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15300--15311, 2020.

\bibitem[Thiede et~al.(2020)Thiede, Hy, and Kondor]{thiede2020general}
E.~H. Thiede, T.~S. Hy, and R.~Kondor.
\newblock The general theory of permutation equivarant neural networks and
  higher order graph variational encoders.
\newblock \emph{arXiv preprint arXiv:2004.03990}, 2020.

\bibitem[Unterthiner et~al.(2020)Unterthiner, Keysers, Gelly, Bousquet, and
  Tolstikhin]{unterthiner2020predicting}
T.~Unterthiner, D.~Keysers, S.~Gelly, O.~Bousquet, and I.~Tolstikhin.
\newblock Predicting neural network accuracy from weights.
\newblock \emph{arXiv preprint arXiv:2002.11448}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017/online}
H.~Xiao, K.~Rasul, and R.~Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\bibitem[Yak et~al.(2019)Yak, Gonzalvo, and Mazzawi]{yak2019towards}
S.~Yak, J.~Gonzalvo, and H.~Mazzawi.
\newblock Towards task and architecture-independent generalization gap
  predictors.
\newblock \emph{arXiv preprint arXiv:1906.01550}, 2019.

\bibitem[Zaheer et~al.(2017)Zaheer, Kottur, Ravanbakhsh, Poczos, Salakhutdinov,
  and Smola]{zaheer2017deep}
M.~Zaheer, S.~Kottur, S.~Ravanbakhsh, B.~Poczos, R.~Salakhutdinov, and
  A.~Smola.
\newblock Deep sets. doi: 10.48550.
\newblock \emph{arXiv preprint ARXIV.1703.06114}, 2017.

\bibitem[Zhang et~al.(2018)Zhang, Ren, and Urtasun]{zhang2018graph}
C.~Zhang, M.~Ren, and R.~Urtasun.
\newblock Graph hypernetworks for neural architecture search.
\newblock \emph{arXiv preprint arXiv:1810.05749}, 2018.

\bibitem[Zhou et~al.(2023)Zhou, Yang, Burns, Jiang, Sokota, Kolter, and
  Finn]{zhou2023permutation}
A.~Zhou, K.~Yang, K.~Burns, Y.~Jiang, S.~Sokota, J.~Z. Kolter, and C.~Finn.
\newblock Permutation equivariant neural functionals.
\newblock \emph{arXiv preprint arXiv:2302.14040}, 2023.

\end{thebibliography}
