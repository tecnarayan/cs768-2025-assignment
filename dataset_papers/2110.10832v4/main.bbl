\begin{thebibliography}{10}

\bibitem{albuquerque2020improving}
Isabela Albuquerque, Nikhil Naik, Junnan Li, Nitish Keskar, and Richard Socher.
\newblock Improving out-of-distribution generalization via multi-task
  self-supervised pretraining.
\newblock {\em arXiv preprint arXiv:2003.13525}, 2020.

\bibitem{arjovsky2019invariant}
Martin Arjovsky, L{\'e}on Bottou, Ishaan Gulrajani, and David Lopez-Paz.
\newblock Invariant risk minimization.
\newblock {\em arXiv preprint arXiv:1907.02893}, 2019.

\bibitem{beery2018recognition}
Sara Beery, Grant Van~Horn, and Pietro Perona.
\newblock Recognition in terra incognita.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 456--473, 2018.

\bibitem{ben2006analysis}
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira.
\newblock Analysis of representations for domain adaptation.
\newblock {\em Advances in neural information processing systems}, 19, 2006.

\bibitem{blanchard2011generalizing}
Gilles Blanchard, Gyemin Lee, and Clayton Scott.
\newblock Generalizing from several related classification tasks to a new
  unlabeled sample.
\newblock {\em Advances in neural information processing systems},
  24:2178--2186, 2011.

\bibitem{breiman1996bagging}
Leo Breiman.
\newblock Bagging predictors.
\newblock {\em Machine learning}, 24(2):123--140, 1996.

\bibitem{bucci2021self}
Silvia Bucci, Antonio D'Innocente, Yujun Liao, Fabio~Maria Carlucci, Barbara
  Caputo, and Tatiana Tommasi.
\newblock Self-supervised learning across domains.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2021.

\bibitem{cha2021swad}
Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung
  Lee, and Sungrae Park.
\newblock Swad: Domain generalization by seeking flat minima.
\newblock {\em arXiv preprint arXiv:2102.08604}, 2021.

\bibitem{cha2022domain}
Junbum Cha, Kyungjae Lee, Sungrae Park, and Sanghyuk Chun.
\newblock Domain generalization by mutual-information regularization with
  pre-trained models.
\newblock {\em arXiv preprint arXiv:2203.10789}, 2022.

\bibitem{d2020underspecification}
Alexander D'Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi,
  Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew~D
  Hoffman, et~al.
\newblock Underspecification presents challenges for credibility in modern
  machine learning.
\newblock {\em arXiv preprint arXiv:2011.03395}, 2020.

\bibitem{dietterich2000ensemble}
Thomas~G Dietterich.
\newblock Ensemble methods in machine learning.
\newblock In {\em International workshop on multiple classifier systems}, pages
  1--15. Springer, 2000.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{fang2013unbiased}
Chen Fang, Ye~Xu, and Daniel~N Rockmore.
\newblock Unbiased metric learning: On the utilization of multiple datasets and
  web images for softening bias.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1657--1664, 2013.

\bibitem{fort2019deep}
Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan.
\newblock Deep ensembles: A loss landscape perspective.
\newblock {\em arXiv preprint arXiv:1912.02757}, 2019.

\bibitem{gal2016dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In {\em international conference on machine learning}, pages
  1050--1059. PMLR, 2016.

\bibitem{ganin2016domain}
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
  Larochelle, Fran{\c{c}}ois Laviolette, Mario Marchand, and Victor Lempitsky.
\newblock Domain-adversarial training of neural networks.
\newblock {\em The journal of machine learning research}, 17(1):2096--2030,
  2016.

\bibitem{geman1992neural}
Stuart Geman, Elie Bienenstock, and Ren{\'e} Doursat.
\newblock Neural networks and the bias/variance dilemma.
\newblock {\em Neural computation}, 4(1):1--58, 1992.

\bibitem{gulrajani2020search}
Ishaan Gulrajani and David Lopez-Paz.
\newblock In search of lost domain generalization.
\newblock {\em arXiv preprint arXiv:2007.01434}, 2020.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hendrycks2021many}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan
  Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et~al.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 8340--8349, 2021.

\bibitem{huang2020self}
Zeyi Huang, Haohan Wang, Eric~P Xing, and Dong Huang.
\newblock Self-challenging improves cross-domain generalization.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part II 16}, pages 124--140.
  Springer, 2020.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em International conference on machine learning}, pages
  448--456. PMLR, 2015.

\bibitem{izmailov2018averaging}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and
  Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock {\em arXiv preprint arXiv:1803.05407}, 2018.

\bibitem{jain2018parallelizing}
Prateek Jain, Sham Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron
  Sidford.
\newblock Parallelizing stochastic gradient descent for least squares
  regression: mini-batching, averaging, and model misspecification.
\newblock {\em Journal of Machine Learning Research}, 18, 2018.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{krueger2021out}
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan
  Binas, Dinghuai Zhang, Remi Le~Priol, and Aaron Courville.
\newblock Out-of-distribution generalization via risk extrapolation (rex).
\newblock In {\em International Conference on Machine Learning}, pages
  5815--5826. PMLR, 2021.

\bibitem{li2017deeper}
Da~Li, Yongxin Yang, Yi-Zhe Song, and Timothy~M Hospedales.
\newblock Deeper, broader and artier domain generalization.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 5542--5550, 2017.

\bibitem{li2018learning}
Da~Li, Yongxin Yang, Yi-Zhe Song, and Timothy~M Hospedales.
\newblock Learning to generalize: Meta-learning for domain generalization.
\newblock In {\em Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem{li2019episodic}
Da~Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy~M
  Hospedales.
\newblock Episodic training for domain generalization.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1446--1455, 2019.

\bibitem{li2018domain}
Haoliang Li, Sinno~Jialin Pan, Shiqi Wang, and Alex~C Kot.
\newblock Domain generalization with adversarial feature learning.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 5400--5409, 2018.

\bibitem{li2018deep}
Ya~Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and
  Dacheng Tao.
\newblock Deep domain generalization via conditional invariant adversarial
  networks.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 624--639, 2018.

\bibitem{mahajan2018exploring}
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
  Yixuan Li, Ashwin Bharambe, and Laurens Van Der~Maaten.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 181--196, 2018.

\bibitem{muandet2013domain}
Krikamol Muandet, David Balduzzi, and Bernhard Sch{\"o}lkopf.
\newblock Domain generalization via invariant feature representation.
\newblock In {\em International Conference on Machine Learning}, pages 10--18.
  PMLR, 2013.

\bibitem{neu2018iterate}
Gergely Neu and Lorenzo Rosasco.
\newblock Iterate averaging as regularization for stochastic gradient descent.
\newblock In {\em Conference On Learning Theory}, pages 3222--3242. PMLR, 2018.

\bibitem{peng2019moment}
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo~Wang.
\newblock Moment matching for multi-source domain adaptation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1406--1415, 2019.

\bibitem{polyak1992acceleration}
Boris~T Polyak and Anatoli~B Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock {\em SIAM journal on control and optimization}, 30(4):838--855, 1992.

\bibitem{rame2021fishr}
Alexandre Rame, Corentin Dancette, and Matthieu Cord.
\newblock Fishr: Invariant gradient variances for out-of-distribution
  generalization.
\newblock {\em arXiv preprint arXiv:2109.02934}, 2021.

\bibitem{sagawa2019distributionally}
Shiori Sagawa, Pang~Wei Koh, Tatsunori~B Hashimoto, and Percy Liang.
\newblock Distributionally robust neural networks for group shifts: On the
  importance of regularization for worst-case generalization.
\newblock {\em arXiv preprint arXiv:1911.08731}, 2019.

\bibitem{shi2021gradient}
Yuge Shi, Jeffrey Seely, Philip~HS Torr, N~Siddharth, Awni Hannun, Nicolas
  Usunier, and Gabriel Synnaeve.
\newblock Gradient matching for domain generalization.
\newblock {\em arXiv preprint arXiv:2104.09937}, 2021.

\bibitem{singh2022revisiting}
Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de~Freitas Reis, Bugra
  Gedik, Raj~Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Doll{\'a}r,
  and Laurens van~der Maaten.
\newblock Revisiting weakly supervised pre-training of visual perception
  models.
\newblock {\em arXiv preprint arXiv:2201.08371}, 2022.

\bibitem{sun2016deep}
Baochen Sun and Kate Saenko.
\newblock Deep coral: Correlation alignment for deep domain adaptation.
\newblock In {\em European conference on computer vision}, pages 443--450.
  Springer, 2016.

\bibitem{tikhonov1943stability}
Andrey~Nikolayevich Tikhonov.
\newblock On the stability of inverse problems.
\newblock In {\em Dokl. Akad. Nauk SSSR}, volume~39, pages 195--198, 1943.

\bibitem{vapnik1998statistical}
Vladimir Vapnik and Vlamimir Vapnik.
\newblock Statistical learning theory wiley.
\newblock {\em New York}, 1(624):2, 1998.

\bibitem{vedantam2021empirical}
Ramakrishna Vedantam, David Lopez-Paz, and David~J Schwab.
\newblock An empirical investigation of domain generalization with empirical
  risk minimizers.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{venkateswara2017deep}
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman
  Panchanathan.
\newblock Deep hashing network for unsupervised domain adaptation.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 5018--5027, 2017.

\bibitem{wang2020heterogeneous}
Yufei Wang, Haoliang Li, and Alex~C Kot.
\newblock Heterogeneous domain generalization via domain mixup.
\newblock In {\em ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 3622--3626. IEEE, 2020.

\bibitem{xu2020adversarial}
Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi~Tian, and
  Wenjun Zhang.
\newblock Adversarial domain adaptation with domain mixup.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 6502--6509, 2020.

\bibitem{yalniz2019billion}
I~Zeki Yalniz, Herv{\'e} J{\'e}gou, Kan Chen, Manohar Paluri, and Dhruv
  Mahajan.
\newblock Billion-scale semi-supervised learning for image classification.
\newblock {\em arXiv preprint arXiv:1905.00546}, 2019.

\bibitem{yang2020rethinking}
Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi~Ma.
\newblock Rethinking bias-variance trade-off for generalization of neural
  networks.
\newblock In {\em International Conference on Machine Learning}, pages
  10767--10777. PMLR, 2020.

\bibitem{yue2019domain}
Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni-Vincentelli, Kurt
  Keutzer, and Boqing Gong.
\newblock Domain randomization and pyramid consistency: Simulation-to-real
  generalization without accessing target domain data.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 2100--2110, 2019.

\bibitem{zhou2021domain}
Kaiyang Zhou, Ziwei Liu, Yu~Qiao, Tao Xiang, and Chen~Change Loy.
\newblock Domain generalization: A survey.
\newblock {\em arXiv preprint arXiv:2103.02503}, 2021.

\bibitem{zhou2021semi}
Kaiyang Zhou, Chen~Change Loy, and Ziwei Liu.
\newblock Semi-supervised domain generalization with stochastic stylematch.
\newblock {\em arXiv preprint arXiv:2106.00592}, 2021.

\end{thebibliography}
