\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Applegate et~al.(2021)Applegate, D{\'\i}az, Hinder, Lu, Lubin, O'Donoghue, and Schudy]{applegate2021practical}
Applegate, D., D{\'\i}az, M., Hinder, O., Lu, H., Lubin, M., O'Donoghue, B., and Schudy, W.
\newblock {Practical} large-scale linear programming using primal-dual hybrid gradient.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 20243--20257, 2021.

\bibitem[Applegate et~al.(2023)Applegate, Hinder, Lu, and Lubin]{applegate2023faster}
Applegate, D., Hinder, O., Lu, H., and Lubin, M.
\newblock {Faster} first-order primal-dual methods for linear programming using restarts and sharpness.
\newblock \emph{Mathematical Programming}, 201\penalty0 (1-2):\penalty0 133--184, 2023.

\bibitem[Basu et~al.(2020)Basu, Ghoting, Mazumder, and Pan]{basu2020eclipse}
Basu, K., Ghoting, A., Mazumder, R., and Pan, Y.
\newblock {Eclipse: An extreme-scale linear program solver for web-applications}.
\newblock In \emph{International Conference on Machine Learning}, pp.\  704--714. PMLR, 2020.

\bibitem[Beck(2017)]{beck2017first}
Beck, A.
\newblock \emph{{First-order} methods in optimization}.
\newblock SIAM, 2017.

\bibitem[Bengio et~al.(2021)Bengio, Lodi, and Prouvost]{bengio2021machine}
Bengio, Y., Lodi, A., and Prouvost, A.
\newblock {Machine} learning for combinatorial optimization: a methodological tour {d’horizon}.
\newblock \emph{European Journal of Operational Research}, 290\penalty0 (2):\penalty0 405--421, 2021.

\bibitem[Bixby et~al.(1992)Bixby, Gregory, Lustig, Marsten, and Shanno]{bixby1992very}
Bixby, R.~E., Gregory, J.~W., Lustig, I.~J., Marsten, R.~E., and Shanno, D.~F.
\newblock {Very} large-scale linear programming: {A} case study in combining interior point and simplex methods.
\newblock \emph{Operations Research}, 40\penalty0 (5):\penalty0 885--897, 1992.

\bibitem[Chambolle \& Pock(2011)Chambolle and Pock]{chambolle2011first}
Chambolle, A. and Pock, T.
\newblock {A} first-order primal-dual algorithm for convex problems with applications to imaging.
\newblock \emph{Journal of mathematical imaging and vision}, 40:\penalty0 120--145, 2011.

\bibitem[Chambolle \& Pock(2016)Chambolle and Pock]{chambolle2016ergodic}
Chambolle, A. and Pock, T.
\newblock {On} the ergodic convergence rates of a first-order primal--dual algorithm.
\newblock \emph{Mathematical Programming}, 159\penalty0 (1-2):\penalty0 253--287, 2016.

\bibitem[Charnes \& Cooper(1957)Charnes and Cooper]{charnes1957management}
Charnes, A. and Cooper, W.~W.
\newblock {Management} models and industrial applications of linear programming.
\newblock \emph{Management science}, 4\penalty0 (1):\penalty0 38--91, 1957.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Chen, Chen, Heaton, Liu, Wang, and Yin]{chen2022learning}
Chen, T., Chen, X., Chen, W., Heaton, H., Liu, J., Wang, Z., and Yin, W.
\newblock {Learning to optimize}: {A} primer and a benchmark.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0 (189):\penalty0 1--59, 2022{\natexlab{a}}.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Liu, Wang, Lu, and Yin]{chen2022representing}
Chen, Z., Liu, J., Wang, X., Lu, J., and Yin, W.
\newblock {On} representing linear programs by graph neural networks.
\newblock \emph{arXiv preprint arXiv:2209.12288}, 2022{\natexlab{b}}.

\bibitem[Dantzig(1990)]{dantzig1990origins}
Dantzig, G.~B.
\newblock {Origins} of the simplex method.
\newblock In \emph{A history of scientific computing}, pp.\  141--151. 1990.

\bibitem[Dantzig(1998)]{dantzig1998linear}
Dantzig, G.~B.
\newblock \emph{{Linear} programming and extensions}, volume~48.
\newblock Princeton university press, 1998.

\bibitem[Deng et~al.(2022)Deng, Feng, Gao, Ge, Jiang, Jiang, Liu, Liu, Xue, Ye, et~al.]{deng2022new}
Deng, Q., Feng, Q., Gao, W., Ge, D., Jiang, B., Jiang, Y., Liu, J., Liu, T., Xue, C., Ye, Y., et~al.
\newblock {New Developments of ADMM-based Interior Point Methods for Linear Programming and Conic Programming}.
\newblock \emph{arXiv preprint arXiv:2209.01793}, 2022.

\bibitem[Dorfman et~al.(1987)Dorfman, Samuelson, and Solow]{dorfman1987linear}
Dorfman, R., Samuelson, P.~A., and Solow, R.~M.
\newblock \emph{{Linear} programming and economic analysis}.
\newblock Courier Corporation, 1987.

\bibitem[Fan et~al.(2023)Fan, Wang, Yakovenko, Sivas, Ren, Zhang, and Zhou]{fan2023smart}
Fan, Z., Wang, X., Yakovenko, O., Sivas, A.~A., Ren, O., Zhang, Y., and Zhou, Z.
\newblock {Smart initial basis selection for linear programs}.
\newblock In \emph{International Conference on Machine Learning}, pp.\  9650--9664. PMLR, 2023.

\bibitem[Fercoq(2021)]{fercoq2021quadratic}
Fercoq, O.
\newblock {Quadratic} error bound of the smoothed gap and the restarted averaged primal-dual hybrid gradient.
\newblock 2021.

\bibitem[Garver(1970)]{garver1970transmission}
Garver, L.~L.
\newblock {Transmission} network estimation using linear programming.
\newblock \emph{IEEE Transactions on power apparatus and systems}, \penalty0 (7):\penalty0 1688--1697, 1970.

\bibitem[Gasse et~al.(2019{\natexlab{a}})Gasse, Ch{\'e}telat, Ferroni, Charlin, and Lodi]{gasse2019exact}
Gasse, M., Ch{\'e}telat, D., Ferroni, N., Charlin, L., and Lodi, A.
\newblock Exact combinatorial optimization with graph convolutional neural networks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019{\natexlab{a}}.

\bibitem[Gasse et~al.(2019{\natexlab{b}})Gasse, Chételat, Ferroni, Charlin, and Lodi]{conf/nips/GasseCFCL19}
Gasse, M., Chételat, D., Ferroni, N., Charlin, L., and Lodi, A.
\newblock {Exact Combinatorial Optimization with Graph Convolutional Neural Networks}.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, 2019{\natexlab{b}}.

\bibitem[Gasse et~al.(2022)Gasse, Bowly, Cappart, Charfreitag, Charlin, Ch{\'e}telat, Chmiela, Dumouchelle, Gleixner, Kazachkov, et~al.]{gasse2022machine}
Gasse, M., Bowly, S., Cappart, Q., Charfreitag, J., Charlin, L., Ch{\'e}telat, D., Chmiela, A., Dumouchelle, J., Gleixner, A., Kazachkov, A.~M., et~al.
\newblock The machine learning for combinatorial optimization competition (ml4co): Results and insights.
\newblock In \emph{NeurIPS 2021 Competitions and Demonstrations Track}, pp.\  220--231. PMLR, 2022.

\bibitem[Gondzio(2012)]{gondzio2012interior}
Gondzio, J.
\newblock {Interior} point methods 25 years later.
\newblock \emph{European Journal of Operational Research}, 218\penalty0 (3):\penalty0 587--601, 2012.

\bibitem[Gregor \& LeCun(2010)Gregor and LeCun]{gregor2010learning}
Gregor, K. and LeCun, Y.
\newblock {Learning} fast approximations of sparse coding.
\newblock In \emph{Proceedings of the 27th international conference on international conference on machine learning}, pp.\  399--406, 2010.

\bibitem[Gupta et~al.(2020)Gupta, Gasse, Khalil, Mudigonda, Lodi, and Bengio]{gupta2020hybrid}
Gupta, P., Gasse, M., Khalil, E., Mudigonda, P., Lodi, A., and Bengio, Y.
\newblock Hybrid models for learning to branch.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 18087--18097, 2020.

\bibitem[{Gurobi Optimization, LLC}(2023)]{gurobi}
{Gurobi Optimization, LLC}.
\newblock {Gurobi Optimizer Reference Manual}, 2023.
\newblock URL \url{https://www.gurobi.com}.

\bibitem[Hinder(2023)]{hinder2023worst}
Hinder, O.
\newblock {Worst-case analysis of restarted primal-dual hybrid gradient on totally unimodular linear programs}.
\newblock \emph{arXiv preprint arXiv:2309.03988}, 2023.

\bibitem[Kim \& Fessler(2016)Kim and Fessler]{kim2016optimized}
Kim, D. and Fessler, J.~A.
\newblock {Optimized} first-order methods for smooth convex minimization.
\newblock \emph{Mathematical programming}, 159:\penalty0 81--107, 2016.

\bibitem[Kuang et~al.(2023)Kuang, Li, Wang, Zhu, Lu, Wang, Zeng, Li, Zhang, and Wu]{kuang2023accelerate}
Kuang, Y., Li, X., Wang, J., Zhu, F., Lu, M., Wang, Z., Zeng, J., Li, H., Zhang, Y., and Wu, F.
\newblock {Accelerate Presolve in Large-Scale Linear Programming via Reinforcement Learning}, 2023.

\bibitem[Li et~al.(2022)Li, Qu, Zhu, Zeng, Yuan, Mao, and Wang]{li2022learning}
Li, X., Qu, Q., Zhu, F., Zeng, J., Yuan, M., Mao, K., and Wang, J.
\newblock {Learning to Reformulate for Linear Programming}, 2022.

\bibitem[Lin et~al.(2021)Lin, Ma, Ye, and Zhang]{lin2021admm}
Lin, T., Ma, S., Ye, Y., and Zhang, S.
\newblock {An ADMM-based} interior-point method for large-scale linear programming.
\newblock \emph{Optimization Methods and Software}, 36\penalty0 (2-3):\penalty0 389--424, 2021.

\bibitem[Liu \& Chen(2019)Liu and Chen]{liu2019alista}
Liu, J. and Chen, X.
\newblock {ALISTA: Analytic weights are as good as learned weights in LISTA}.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2019.

\bibitem[Liu et~al.(2023)Liu, Pu, Ge, and Ye]{liu2023learning}
Liu, T., Pu, S., Ge, D., and Ye, Y.
\newblock {Learning to Pivot as a Smart Expert}.
\newblock \emph{arXiv preprint arXiv:2308.08171}, 2023.

\bibitem[Lu \& Yang(2022)Lu and Yang]{lu2022infimal}
Lu, H. and Yang, J.
\newblock {On} the {Infimal Sub-differential Size }of {Primal-Dual Hybrid Gradient Method}.
\newblock \emph{arXiv preprint arXiv:2206.12061}, 2022.

\bibitem[Lu \& Yang(2023{\natexlab{a}})Lu and Yang]{lu2023cupdlp}
Lu, H. and Yang, J.
\newblock {cuPDLP. jl: A GPU implementation of restarted primal-dual hybrid gradient for linear programming in Julia}.
\newblock \emph{arXiv preprint arXiv:2311.12180}, 2023{\natexlab{a}}.

\bibitem[Lu \& Yang(2023{\natexlab{b}})Lu and Yang]{lu2023geometry}
Lu, H. and Yang, J.
\newblock {On the Geometry and Refined Rate of Primal-Dual Hybrid Gradient for Linear Programming}.
\newblock \emph{arXiv preprint arXiv:2307.03664}, 2023{\natexlab{b}}.

\bibitem[Lu \& Yang(2023{\natexlab{c}})Lu and Yang]{lu2023practical}
Lu, H. and Yang, J.
\newblock {A Practical and Optimal First-Order Method for Large-Scale Convex Quadratic Programming}.
\newblock \emph{arXiv preprint arXiv:2311.07710}, 2023{\natexlab{c}}.

\bibitem[Lu \& Yang(2023{\natexlab{d}})Lu and Yang]{lu2023unified}
Lu, H. and Yang, J.
\newblock {On} a {Unified} and {Simplified} {Proof} for the {Ergodic Convergence Rates} of {PPM}, {PDHG} and {ADMM}.
\newblock \emph{arXiv preprint arXiv:2305.02165}, 2023{\natexlab{d}}.

\bibitem[Lu et~al.(2023)Lu, Yang, Hu, Huangfu, Liu, Liu, Ye, Zhang, and Ge]{cuPDLP-C}
Lu, H., Yang, J., Hu, H., Huangfu, Q., Liu, J., Liu, T., Ye, Y., Zhang, C., and Ge, D.
\newblock {cuPDLP-C}: A strengthened implementation of {cuPDLP} for linear programming by {C} language.
\newblock \emph{arXiv preprint arXiv:2312.14832}, 2023.

\bibitem[Luenberger et~al.(1984)Luenberger, Ye, et~al.]{luenberger1984linear}
Luenberger, D.~G., Ye, Y., et~al.
\newblock \emph{{Linear} and nonlinear programming}, volume~2.
\newblock Springer, 1984.

\bibitem[Nair et~al.(2020)Nair, Bartunov, Gimeno, Von~Glehn, Lichocki, Lobov, O'Donoghue, Sonnerat, Tjandraatmadja, Wang, et~al.]{nair2020solving}
Nair, V., Bartunov, S., Gimeno, F., Von~Glehn, I., Lichocki, P., Lobov, I., O'Donoghue, B., Sonnerat, N., Tjandraatmadja, C., Wang, P., et~al.
\newblock {Solving} mixed integer programs using neural networks.
\newblock \emph{arXiv preprint arXiv:2012.13349}, 2020.

\bibitem[Necoara et~al.(2019)Necoara, Nesterov, and Glineur]{necoara2019linear}
Necoara, I., Nesterov, Y., and Glineur, F.
\newblock Linear convergence of first order methods for non-strongly convex optimization.
\newblock \emph{Mathematical Programming}, 175:\penalty0 69--107, 2019.

\bibitem[Nesterov(2014)]{nesterov2014subgradient}
Nesterov, Y.
\newblock Subgradient methods for huge-scale optimization problems.
\newblock \emph{Mathematical Programming}, 146\penalty0 (1-2):\penalty0 275--297, 2014.

\bibitem[Nesterov \& Nemirovskii(1994)Nesterov and Nemirovskii]{nesterov1994interior}
Nesterov, Y. and Nemirovskii, A.
\newblock \emph{{Interior-point} polynomial algorithms in convex programming}.
\newblock SIAM, 1994.

\bibitem[O'Donoghue(2021)]{o2021operator}
O'Donoghue, B.
\newblock {Operator} splitting for a homogeneous embedding of the linear complementarity problem.
\newblock \emph{SIAM Journal on Optimization}, 31\penalty0 (3):\penalty0 1999--2023, 2021.

\bibitem[O'Donoghue et~al.(2016)O'Donoghue, Chu, Parikh, and Boyd]{o2016conic}
O'Donoghue, B., Chu, E., Parikh, N., and Boyd, S.
\newblock {Conic optimization} via operator splitting and homogeneous self-dual embedding.
\newblock \emph{Journal of Optimization Theory and Applications}, 169\penalty0 (3):\penalty0 1042--1068, 2016.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Köpf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S.
\newblock {PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 2019.

\bibitem[Perron \& Furnon()Perron and Furnon]{ortools}
Perron, L. and Furnon, V.
\newblock {OR-Tools}.
\newblock URL \url{https://developers.google.com/optimization/}.

\bibitem[Qian et~al.(2023)Qian, Ch{\'e}telat, and Morris]{qian2023exploring}
Qian, C., Ch{\'e}telat, D., and Morris, C.
\newblock {Exploring the Power of Graph Neural Networks in Solving Linear Optimization Problems}.
\newblock \emph{arXiv preprint arXiv:2310.10603}, 2023.

\bibitem[Rabinowitz(1968)]{rabinowitz1968applications}
Rabinowitz, P.
\newblock {Applications} of linear programming to numerical analysis.
\newblock \emph{SIAM Review}, 10\penalty0 (2):\penalty0 121--159, 1968.

\bibitem[Renegar(2014)]{renegar2014efficient}
Renegar, J.
\newblock {Efficient} first-order methods for linear programming and semidefinite programming.
\newblock \emph{arXiv preprint arXiv:1409.5832}, 2014.

\bibitem[Schrijver(1998)]{schrijver1998theory}
Schrijver, A.
\newblock \emph{{Theory} of linear and integer programming}.
\newblock John Wiley \& Sons, 1998.

\bibitem[Todd(1983)]{todd1983large}
Todd, M.~J.
\newblock {Large-scale} linear programming: {Geometry}, working bases and factorizations.
\newblock \emph{Mathematical Programming}, 26:\penalty0 1--20, 1983.

\bibitem[Wang \& Shroff(2017)Wang and Shroff]{wang2017new}
Wang, S. and Shroff, N.
\newblock A new alternating direction method for linear programming.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Xu et~al.(2018)Xu, Hu, Leskovec, and Jegelka]{xu2018powerful}
Xu, K., Hu, W., Leskovec, J., and Jegelka, S.
\newblock {How Powerful are Graph Neural Networks?}
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Ye(2011)]{ye2011interior}
Ye, Y.
\newblock \emph{{Interior} point algorithms: theory and analysis}.
\newblock John Wiley \& Sons, 2011.

\bibitem[Zhang \& Sra(2016)Zhang and Sra]{zhang2016first}
Zhang, H. and Sra, S.
\newblock {First-order} methods for geodesically convex optimization.
\newblock In \emph{Conference on Learning Theory}, pp.\  1617--1638. PMLR, 2016.

\end{thebibliography}
