\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
J.~L. Ba, J.~R. Kiros, and G.~E. Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Chelba et~al.(2013)Chelba, Mikolov, Schuster, Ge, Brants, and
  Koehn]{lm1b}
C.~Chelba, T.~Mikolov, M.~Schuster, Q.~Ge, T.~Brants, and P.~Koehn.
\newblock One billion word benchmark for measuring progress in statistical
  language modeling.
\newblock \emph{CoRR}, abs/1312.3005, 2013.
\newblock URL \url{http://arxiv.org/abs/1312.3005}.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
R.~Child, S.~Gray, A.~Radford, and I.~Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Chiu et~al.(2018)Chiu, Sainath, Wu, Prabhavalkar, Nguyen, Chen,
  Kannan, Weiss, Rao, Gonina, et~al.]{chiu2018state}
C.-C. Chiu, T.~N. Sainath, Y.~Wu, R.~Prabhavalkar, P.~Nguyen, Z.~Chen,
  A.~Kannan, R.~J. Weiss, K.~Rao, E.~Gonina, et~al.
\newblock State-of-the-art speech recognition with sequence-to-sequence models.
\newblock In \emph{2018 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 4774--4778. IEEE, 2018.

\bibitem[Cho et~al.(2014)Cho, van Merrienboer, Bahdanau, and
  Bengio]{cho2014properties}
K.~Cho, B.~van Merrienboer, D.~Bahdanau, and Y.~Bengio.
\newblock On the properties of neural machine translation: Encoder--decoder
  approaches.
\newblock In \emph{Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics
  and Structure in Statistical Translation}, pages 103--111, 2014.

\bibitem[Correia et~al.(2019)Correia, Niculae, and
  Martins]{correia2019adaptively}
G.~M. Correia, V.~Niculae, and A.~F. Martins.
\newblock Adaptively sparse transformers.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 2174--2184, 2019.

\bibitem[Dehghani et~al.(2018)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser]{dehghani2018universal}
M.~Dehghani, S.~Gouws, O.~Vinyals, J.~Uszkoreit, and {\L}.~Kaiser.
\newblock Universal transformers.
\newblock \emph{arXiv preprint arXiv:1807.03819}, 2018.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Gehring et~al.(2017)Gehring, Auli, Grangier, Yarats, and
  Dauphin]{gehring2017convolutional}
J.~Gehring, M.~Auli, D.~Grangier, D.~Yarats, and Y.~N. Dauphin.
\newblock Convolutional sequence to sequence learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1243--1252. JMLR. org, 2017.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Li(2017)]{li2017deep}
Y.~Li.
\newblock Deep reinforcement learning: An overview.
\newblock \emph{arXiv preprint arXiv:1701.07274}, 2017.

\bibitem[Liu et~al.(2018)Liu, Saleh, Pot, Goodrich, Sepassi, Kaiser, and
  Shazeer]{liu2018generating}
P.~J. Liu, M.~Saleh, E.~Pot, B.~Goodrich, R.~Sepassi, L.~Kaiser, and
  N.~Shazeer.
\newblock Generating wikipedia by summarizing long sequences.
\newblock \emph{arXiv preprint arXiv:1801.10198}, 2018.

\bibitem[Michel et~al.(2019)Michel, Levy, and Neubig]{michel2019sixteen}
P.~Michel, O.~Levy, and G.~Neubig.
\newblock Are sixteen heads really better than one?
\newblock \emph{arXiv preprint arXiv:1905.10650}, 2019.

\bibitem[Ott et~al.(2018)Ott, Edunov, Grangier, and Auli]{ott2018scaling}
M.~Ott, S.~Edunov, D.~Grangier, and M.~Auli.
\newblock Scaling neural machine translation.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation:
  Research Papers}, pages 1--9, 2018.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
A.~Radford, K.~Narasimhan, T.~Salimans, and I.~Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock \emph{Technical report, OpenAI}, 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, and I.~Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{Technical report, OpenAI}, 2019.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang]{rajpurkar2016squad}
P.~Rajpurkar, J.~Zhang, K.~Lopyrev, and P.~Liang.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock \emph{arXiv preprint arXiv:1606.05250}, 2016.

\bibitem[Sun et~al.(2019)Sun, Tan, Gan, Liu, Zhao, Qin, and Liu]{sun2019token}
H.~Sun, X.~Tan, J.-W. Gan, H.~Liu, S.~Zhao, T.~Qin, and T.-Y. Liu.
\newblock Token-level ensemble distillation for grapheme-to-phoneme conversion.
\newblock \emph{arXiv preprint arXiv:1904.03446}, 2019.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and
  Le]{sutskever2014sequence}
I.~Sutskever, O.~Vinyals, and Q.~V. Le.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  3104--3112, 2014.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5998--6008, 2017.

\bibitem[Vaswani et~al.(2018)Vaswani, Bengio, Brevdo, Chollet, Gomez, Gouws,
  Jones, Kaiser, Kalchbrenner, Parmar, Sepassi, Shazeer, and
  Uszkoreit]{tensor2tensor}
A.~Vaswani, S.~Bengio, E.~Brevdo, F.~Chollet, A.~N. Gomez, S.~Gouws, L.~Jones,
  L.~Kaiser, N.~Kalchbrenner, N.~Parmar, R.~Sepassi, N.~Shazeer, and
  J.~Uszkoreit.
\newblock Tensor2tensor for neural machine translation.
\newblock \emph{CoRR}, abs/1803.07416, 2018.
\newblock URL \url{http://arxiv.org/abs/1803.07416}.

\bibitem[Voita et~al.(2019)Voita, Talbot, Moiseev, Sennrich, and
  Titov]{voita2019analyzing}
E.~Voita, D.~Talbot, F.~Moiseev, R.~Sennrich, and I.~Titov.
\newblock Analyzing multi-head self-attention: Specialized heads do the heavy
  lifting, the rest can be pruned.
\newblock \emph{arXiv preprint arXiv:1905.09418}, 2019.

\bibitem[Wang et~al.(2018)Wang, Girshick, Gupta, and He]{wang2018non}
X.~Wang, R.~Girshick, A.~Gupta, and K.~He.
\newblock Non-local neural networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 7794--7803, 2018.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{MNLI}
A.~Williams, N.~Nangia, and S.~Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pages 1112--1122. Association for
  Computational Linguistics, 2018.
\newblock URL \url{http://aclweb.org/anthology/N18-1101}.

\bibitem[Wu et~al.(2019)Wu, Fan, Baevski, Dauphin, and Auli]{wu2019pay}
F.~Wu, A.~Fan, A.~Baevski, Y.~N. Dauphin, and M.~Auli.
\newblock Pay less attention with lightweight and dynamic convolutions.
\newblock \emph{arXiv preprint arXiv:1901.10430}, 2019.

\bibitem[Yang et~al.(2017)Yang, Dai, Salakhutdinov, and
  Cohen]{yang2017breaking}
Z.~Yang, Z.~Dai, R.~Salakhutdinov, and W.~W. Cohen.
\newblock Breaking the softmax bottleneck: A high-rank rnn language model.
\newblock \emph{arXiv preprint arXiv:1711.03953}, 2017.

\bibitem[Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le]{yang2019xlnet}
Z.~Yang, Z.~Dai, Y.~Yang, J.~Carbonell, R.~Salakhutdinov, and Q.~V. Le.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1906.08237}, 2019.

\bibitem[You et~al.(2019)You, Li, Hseu, Song, Demmel, and
  Hsieh]{you2019reducing}
Y.~You, J.~Li, J.~Hseu, X.~Song, J.~Demmel, and C.-J. Hsieh.
\newblock Reducing bert pre-training time from 3 days to 76 minutes.
\newblock \emph{arXiv preprint arXiv:1904.00962}, 2019.

\bibitem[Yun et~al.(2019)Yun, Bhojanapalli, Rawat, Reddi, and
  Kumar]{yun2019transformers}
C.~Yun, S.~Bhojanapalli, A.~S. Rawat, S.~J. Reddi, and S.~Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence
  functions?
\newblock \emph{arXiv preprint arXiv:1912.10077}, 2019.

\bibitem[Zambaldi et~al.(2018)Zambaldi, Raposo, Santoro, Bapst, Li, Babuschkin,
  Tuyls, Reichert, Lillicrap, Lockhart, et~al.]{zambaldi2018relational}
V.~Zambaldi, D.~Raposo, A.~Santoro, V.~Bapst, Y.~Li, I.~Babuschkin, K.~Tuyls,
  D.~Reichert, T.~Lillicrap, E.~Lockhart, et~al.
\newblock Relational deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1806.01830}, 2018.

\bibitem[Zhang et~al.(2018)Zhang, Goodfellow, Metaxas, and
  Odena]{zhang2018self}
H.~Zhang, I.~Goodfellow, D.~Metaxas, and A.~Odena.
\newblock Self-attention generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1805.08318}, 2018.

\bibitem[Zhu et~al.(2015)Zhu, Kiros, Zemel, Salakhutdinov, Urtasun, Torralba,
  and Fidler]{zhu2015aligning}
Y.~Zhu, R.~Kiros, R.~Zemel, R.~Salakhutdinov, R.~Urtasun, A.~Torralba, and
  S.~Fidler.
\newblock Aligning books and movies: Towards story-like visual explanations by
  watching movies and reading books.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 19--27, 2015.

\end{thebibliography}
