\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Awasthi et~al.(2022{\natexlab{a}})Awasthi, Mao, Mohri, and
  Zhong]{awasthi2022Hconsistency}
Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock {${\mathscr H}$}-consistency bounds for surrogate loss minimizers.
\newblock In \emph{International Conference on Machine Learning},
  2022{\natexlab{a}}.

\bibitem[Awasthi et~al.(2022{\natexlab{b}})Awasthi, Mao, Mohri, and
  Zhong]{awasthi2022multi}
Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Multi-class {$ H $}-consistency bounds.
\newblock In \emph{Advances in neural information processing systems}, pages
  782--795, 2022{\natexlab{b}}.

\bibitem[Awasthi et~al.(2023{\natexlab{a}})Awasthi, Mao, Mohri, and
  Zhong]{AwasthiMaoMohriZhong2023theoretically}
Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Theoretically grounded loss functions and algorithms for adversarial
  robustness.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 10077--10094, 2023{\natexlab{a}}.

\bibitem[Awasthi et~al.(2023{\natexlab{b}})Awasthi, Mao, Mohri, and
  Zhong]{awasthi2023dc}
Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock {DC}-programming for neural network optimizations.
\newblock \emph{Journal of Global Optimization}, 2023{\natexlab{b}}.

\bibitem[Bartlett et~al.(2006)Bartlett, Jordan, and
  McAuliffe]{bartlett2006convexity}
Peter~L. Bartlett, Michael~I. Jordan, and Jon~D. McAuliffe.
\newblock Convexity, classification, and risk bounds.
\newblock \emph{Journal of the American Statistical Association}, 101\penalty0
  (473):\penalty0 138--156, 2006.

\bibitem[Berkson(1944)]{Berkson1944}
Joseph Berkson.
\newblock Application of the logistic function to bio-assay.
\newblock \emph{Journal of the American Statistical Association}, 39:\penalty0
  357–--365, 1944.

\bibitem[Berkson(1951)]{Berkson1951}
Joseph Berkson.
\newblock Why {I} prefer logits to probits.
\newblock \emph{Biometrics}, 7\penalty0 (4):\penalty0 327–--339, 1951.

\bibitem[Bogatinovski et~al.(2022)Bogatinovski, Todorovski, D{\v{z}}eroski, and
  Kocev]{bogatinovski2022comprehensive}
Jasmin Bogatinovski, Ljup{\v{c}}o Todorovski, Sa{\v{s}}o D{\v{z}}eroski, and
  Dragi Kocev.
\newblock Comprehensive comparative study of multi-label classification
  methods.
\newblock \emph{Expert Systems with Applications}, 203:\penalty0 117215, 2022.

\bibitem[Busa-Fekete et~al.(2022)Busa-Fekete, Choi, Dembczynski, Gentile,
  Reeve, and Szorenyi]{busa2022regret}
R{\'o}bert Busa-Fekete, Heejin Choi, Krzysztof Dembczynski, Claudio Gentile,
  Henry Reeve, and Balazs Szorenyi.
\newblock Regret bounds for multilabel classification in sparse label regimes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5404--5416, 2022.

\bibitem[Cheng et~al.(2010)Cheng, H{\"u}llermeier, and
  Dembczynski]{cheng2010bayes}
Weiwei Cheng, Eyke H{\"u}llermeier, and Krzysztof~J Dembczynski.
\newblock Bayes optimal multilabel classification via probabilistic classifier
  chains.
\newblock In \emph{international conference on machine learning}, pages
  279--286, 2010.

\bibitem[Cortes et~al.(2016)Cortes, Kuznetsov, Mohri, and
  Yang]{CortesKuznetsovMohriYang2016}
Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri, and Scott Yang.
\newblock Structured prediction theory based on factor graph complexity.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Cortes et~al.(2018)Cortes, Kuznetsov, Mohri, Storcheus, and
  Yang]{CortesKuznetsovMohriStorcheusYang2018}
Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri, Dmitry Storcheus, and Scott
  Yang.
\newblock Efficient gradient computation for structured output learning with
  rational and tropical losses.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Cortes et~al.(2024)Cortes, Mao, Mohri, Mohri, and
  Zhong]{cortes2024cardinality}
Corinna Cortes, Anqi Mao, Christopher Mohri, Mehryar Mohri, and Yutao Zhong.
\newblock Cardinality-aware set prediction and top-$ k $ classification.
\newblock \emph{arXiv preprint arXiv:2407.07140}, 2024.

\bibitem[Dembczynski et~al.(2011)Dembczynski, Waegeman, Cheng, and
  H{\"u}llermeier]{dembczynski2011exact}
Krzysztof Dembczynski, Willem Waegeman, Weiwei Cheng, and Eyke H{\"u}llermeier.
\newblock An exact algorithm for f-measure maximization.
\newblock In \emph{Advances in neural information processing systems}, 2011.

\bibitem[Dembczynski et~al.(2012)Dembczynski, Kot{\l}owski, and
  H{\"u}llermeier]{dembczynski2012consistent}
Krzysztof Dembczynski, Wojciech Kot{\l}owski, and Eyke H{\"u}llermeier.
\newblock Consistent multilabel ranking through univariate loss minimization.
\newblock In \emph{Proceedings of the 29th International Coference on
  International Conference on Machine Learning}, pages 1347--1354, 2012.

\bibitem[Dembczy{\'n}ski et~al.(2012)Dembczy{\'n}ski, Waegeman, Cheng, and
  H{\"u}llermeier]{dembczynski2012label}
Krzysztof Dembczy{\'n}ski, Willem Waegeman, Weiwei Cheng, and Eyke
  H{\"u}llermeier.
\newblock On label dependence and loss minimization in multi-label
  classification.
\newblock \emph{Machine Learning}, 88:\penalty0 5--45, 2012.

\bibitem[Dembczynski et~al.(2013)Dembczynski, Jachnik, Kotlowski, Waegeman, and
  H{\"u}llermeier]{dembczynski2013optimizing}
Krzysztof Dembczynski, Arkadiusz Jachnik, Wojciech Kotlowski, Willem Waegeman,
  and Eyke H{\"u}llermeier.
\newblock Optimizing the f-measure in multi-label classification: Plug-in rule
  approach versus structured loss minimization.
\newblock In \emph{International conference on machine learning}, pages
  1130--1138, 2013.

\bibitem[Deng et~al.(2011)Deng, Satheesh, Berg, and Li]{deng2011fast}
Jia Deng, Sanjeev Satheesh, Alexander Berg, and Fei Li.
\newblock Fast and balanced: Efficient label tree learning for large scale
  object recognition.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2011.

\bibitem[Elisseeff and Weston(2001)]{elisseeff2001kernel}
Andr{\'e} Elisseeff and Jason Weston.
\newblock A kernel method for multi-labelled classification.
\newblock In \emph{Advances in neural information processing systems}, 2001.

\bibitem[Gao and Zhou(2011)]{gao2011consistency}
Wei Gao and Zhi-Hua Zhou.
\newblock On the consistency of multi-label learning.
\newblock In \emph{Conference on learning theory}, pages 341--358, 2011.

\bibitem[Ghosh et~al.(2017)Ghosh, Kumar, and Sastry]{ghosh2017robust}
Aritra Ghosh, Himanshu Kumar, and P~Shanti Sastry.
\newblock Robust loss functions under label noise for deep neural networks.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, 2017.

\bibitem[Kapoor et~al.(2012)Kapoor, Viswanathan, and
  Jain]{kapoor2012multilabel}
Ashish Kapoor, Raajay Viswanathan, and Prateek Jain.
\newblock Multilabel classification using bayesian compressed sensing.
\newblock In \emph{Advances in neural information processing systems}, 2012.

\bibitem[Koyejo et~al.(2014)Koyejo, Natarajan, Ravikumar, and
  Dhillon]{koyejo2014consistent}
Oluwasanmi~O Koyejo, Nagarajan Natarajan, Pradeep~K Ravikumar, and Inderjit~S
  Dhillon.
\newblock Consistent binary classification with generalized performance
  metrics.
\newblock In \emph{Advances in neural information processing systems}, 2014.

\bibitem[Koyejo et~al.(2015)Koyejo, Natarajan, Ravikumar, and
  Dhillon]{koyejo2015consistent}
Oluwasanmi~O Koyejo, Nagarajan Natarajan, Pradeep~K Ravikumar, and Inderjit~S
  Dhillon.
\newblock Consistent multilabel classification.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[Lee et~al.(2004)Lee, Lin, and Wahba]{lee2004multicategory}
Yoonkyung Lee, Yi~Lin, and Grace Wahba.
\newblock Multicategory support vector machines: Theory and application to the
  classification of microarray data and satellite radiance data.
\newblock \emph{Journal of the American Statistical Association}, 99\penalty0
  (465):\penalty0 67--81, 2004.

\bibitem[Long and Servedio(2013)]{long2013consistency}
Phil Long and Rocco Servedio.
\newblock Consistency versus realizable {H}-consistency for multiclass
  classification.
\newblock In \emph{International Conference on Machine Learning}, pages
  801--809, 2013.

\bibitem[Manning and Schutze(1999)]{manning1999foundations}
Christopher Manning and Hinrich Schutze.
\newblock \emph{Foundations of statistical natural language processing}.
\newblock MIT press, 1999.

\bibitem[Mao et~al.(2023{\natexlab{a}})Mao, Mohri, Mohri, and
  Zhong]{MaoMohriMohriZhong2023twostage}
Anqi Mao, Christopher Mohri, Mehryar Mohri, and Yutao Zhong.
\newblock Two-stage learning to defer with multiple experts.
\newblock In \emph{Advances in neural information processing systems},
  2023{\natexlab{a}}.

\bibitem[Mao et~al.(2023{\natexlab{b}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2023characterization}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock {H}-consistency bounds: Characterization and extensions.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2023{\natexlab{b}}.

\bibitem[Mao et~al.(2023{\natexlab{c}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2023ranking}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock {H}-consistency bounds for pairwise misranking loss surrogates.
\newblock In \emph{International conference on Machine learning},
  2023{\natexlab{c}}.

\bibitem[Mao et~al.(2023{\natexlab{d}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2023rankingabs}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Ranking with abstention.
\newblock In \emph{ICML 2023 Workshop The Many Facets of Preference-Based
  Learning}, 2023{\natexlab{d}}.

\bibitem[Mao et~al.(2023{\natexlab{e}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2023structured}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Structured prediction with stronger consistency guarantees.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2023{\natexlab{e}}.

\bibitem[Mao et~al.(2023{\natexlab{f}})Mao, Mohri, and Zhong]{mao2023cross}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Cross-entropy loss functions: Theoretical analysis and applications.
\newblock In \emph{International Conference on Machine Learning}, pages
  23803--23828, 2023{\natexlab{f}}.

\bibitem[Mao et~al.(2024{\natexlab{a}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2024deferral}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Principled approaches for learning to defer with multiple experts.
\newblock In \emph{International Symposium on Artificial Intelligence and
  Mathematics}, 2024{\natexlab{a}}.

\bibitem[Mao et~al.(2024{\natexlab{b}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2024predictor}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Predictor-rejector multi-class abstention: Theoretical analysis and
  algorithms.
\newblock In \emph{Algorithmic Learning Theory}, 2024{\natexlab{b}}.

\bibitem[Mao et~al.(2024{\natexlab{c}})Mao, Mohri, and
  Zhong]{MaoMohriZhong2024score}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Theoretically grounded loss functions and algorithms for score-based
  multi-class abstention.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2024{\natexlab{c}}.

\bibitem[Mao et~al.(2024{\natexlab{d}})Mao, Mohri, and Zhong]{mao2024h}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock {$ H $}-consistency guarantees for regression.
\newblock \emph{arXiv preprint arXiv:2403.19480}, 2024{\natexlab{d}}.

\bibitem[Mao et~al.(2024{\natexlab{e}})Mao, Mohri, and
  Zhong]{mao2024regression}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Regression with multi-expert deferral.
\newblock \emph{arXiv preprint arXiv:2403.19494}, 2024{\natexlab{e}}.

\bibitem[Mao et~al.(2024{\natexlab{f}})Mao, Mohri, and Zhong]{mao2024top}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock Top-$ k $ classification and cardinality-aware prediction.
\newblock \emph{arXiv preprint arXiv:2403.19625}, 2024{\natexlab{f}}.

\bibitem[Mao et~al.(2024{\natexlab{g}})Mao, Mohri, and Zhong]{mao2024universal}
Anqi Mao, Mehryar Mohri, and Yutao Zhong.
\newblock A universal growth rate for learning with smooth surrogate losses.
\newblock \emph{arXiv preprint arXiv:2405.05968}, 2024{\natexlab{g}}.

\bibitem[McCallum(1999)]{mccallum1999multi}
Andrew~Kachites McCallum.
\newblock Multi-label text classification with a mixture model trained by em.
\newblock In \emph{AAAI'99 workshop on text learning}, 1999.

\bibitem[Menon et~al.(2019)Menon, Rawat, Reddi, and Kumar]{menon2019multilabel}
Aditya~K Menon, Ankit~Singh Rawat, Sashank Reddi, and Sanjiv Kumar.
\newblock Multilabel reductions: what is my loss optimising?
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Mohri et~al.(2024)Mohri, Andor, Choi, Collins, Mao, and
  Zhong]{MohriAndorChoiCollinsMaoZhong2023learning}
Christopher Mohri, Daniel Andor, Eunsol Choi, Michael Collins, Anqi Mao, and
  Yutao Zhong.
\newblock Learning to reject with a fixed predictor: Application to
  decontextualization.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Petterson and Caetano(2011)]{petterson2011submodular}
James Petterson and Tiberio Caetano.
\newblock Submodular multi-label learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2011.

\bibitem[Ramaswamy et~al.(2014)Ramaswamy, Babu, Agarwal, and
  Williamson]{ramaswamy2014consistency}
Harish~G Ramaswamy, Balaji~Srinivasan Babu, Shivani Agarwal, and Robert~C
  Williamson.
\newblock On the consistency of output code based learning algorithms for
  multiclass learning problems.
\newblock In \emph{Conference on Learning Theory}, pages 885--902, 2014.

\bibitem[Schapire and Singer(2000)]{schapire2000boostexter}
Robert~E Schapire and Yoram Singer.
\newblock Boostexter: A boosting-based system for text categorization.
\newblock \emph{Machine learning}, 39:\penalty0 135--168, 2000.

\bibitem[Steinwart(2007)]{steinwart2007compare}
Ingo Steinwart.
\newblock How to compare different loss functions and their risks.
\newblock \emph{Constructive Approximation}, 26\penalty0 (2):\penalty0
  225--287, 2007.

\bibitem[Tewari and Bartlett(2007)]{tewari2007consistency}
Ambuj Tewari and Peter~L. Bartlett.
\newblock On the consistency of multiclass classification methods.
\newblock \emph{Journal of Machine Learning Research}, 8\penalty0
  (36):\penalty0 1007--1025, 2007.

\bibitem[Verhulst(1838)]{Verhulst1838}
Pierre~François Verhulst.
\newblock Notice sur la loi que la population suit dans son accroissement.
\newblock \emph{Correspondance math\'ematique et physique}, 10:\penalty0
  113–--121, 1838.

\bibitem[Verhulst(1845)]{Verhulst1845}
Pierre~François Verhulst.
\newblock Recherches math\'ematiques sur la loi d'accroissement de la
  population.
\newblock \emph{Nouveaux M\'emoires de l'Acad\'emie Royale des Sciences et
  Belles-Lettres de Bruxelles}, 18:\penalty0 1–--42, 1845.

\bibitem[Waegeman et~al.(2014)Waegeman, Dembczy{\'n}ski, Jachnik, Cheng, and
  H{\"u}llermeier]{waegeman2014bayes}
Willem Waegeman, Krzysztof Dembczy{\'n}ski, Arkadiusz Jachnik, Weiwei Cheng,
  and Eyke H{\"u}llermeier.
\newblock On the bayes-optimality of f-measure maximizers.
\newblock \emph{Journal of Machine Learning Research}, 15:\penalty0 3333--3388,
  2014.

\bibitem[Weston and Watkins(1998)]{weston1998multi}
Jason Weston and Chris Watkins.
\newblock Multi-class support vector machines.
\newblock Technical report, Citeseer, 1998.

\bibitem[Wu and Zhu(2020)]{wu2020multi}
Guoqiang Wu and Jun Zhu.
\newblock Multi-label classification: do hamming loss and subset accuracy
  really conflict with each other?
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3130--3140, 2020.

\bibitem[Wu et~al.(2021)Wu, Li, Xu, and Zhu]{wu2021rethinking}
Guoqiang Wu, Chongxuan Li, Kun Xu, and Jun Zhu.
\newblock Rethinking and reweighting the univariate losses for multi-label
  ranking: Consistency and generalization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  14332--14344, 2021.

\bibitem[Wu et~al.(2023)Wu, Li, and Yin]{wu2023towards}
Guoqiang Wu, Chongxuan Li, and Yilong Yin.
\newblock Towards understanding generalization of macro-auc in multi-label
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  37540--37570, 2023.

\bibitem[Wydmuch et~al.(2018)Wydmuch, Jasinska, Kuznetsov, Busa-Fekete, and
  Dembczynski]{wydmuch2018no}
Marek Wydmuch, Kalina Jasinska, Mikhail Kuznetsov, R{\'o}bert Busa-Fekete, and
  Krzysztof Dembczynski.
\newblock A no-regret generalization of hierarchical softmax to extreme
  multi-label classification.
\newblock In \emph{Advances in neural information processing systems}, 2018.

\bibitem[Ye et~al.(2012)Ye, Chai, Lee, and Chieu]{ye2012optimizing}
Nan Ye, Kian~Ming Chai, Wee~Sun Lee, and Hai~Leong Chieu.
\newblock Optimizing f-measures: A tale of two approaches.
\newblock In \emph{International Conference on Machine Learning}, pages
  289--296, 2012.

\bibitem[Yu et~al.(2014)Yu, Jain, Kar, and Dhillon]{yu2014large}
Hsiang-Fu Yu, Prateek Jain, Purushottam Kar, and Inderjit Dhillon.
\newblock Large-scale multi-label learning with missing labels.
\newblock In \emph{International conference on machine learning}, pages
  593--601, 2014.

\bibitem[Zhang and Zhou(2013)]{zhang2013review}
Min-Ling Zhang and Zhi-Hua Zhou.
\newblock A review on multi-label learning algorithms.
\newblock \emph{IEEE transactions on knowledge and data engineering},
  26\penalty0 (8):\penalty0 1819--1837, 2013.

\bibitem[Zhang and Agarwal(2020)]{zhang2020bayes}
Mingyuan Zhang and Shivani Agarwal.
\newblock Bayes consistency vs. {H}-consistency: The interplay between
  surrogate loss functions and the scoring function class.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Zhang et~al.(2020)Zhang, Ramaswamy, and Agarwal]{zhang2020convex}
Mingyuan Zhang, Harish~Guruprasad Ramaswamy, and Shivani Agarwal.
\newblock Convex calibrated surrogates for the multi-label f-measure.
\newblock In \emph{International Conference on Machine Learning}, pages
  11246--11255, 2020.

\bibitem[Zhang(2004{\natexlab{a}})]{Zhang2003}
Tong Zhang.
\newblock Statistical behavior and consistency of classification methods based
  on convex risk minimization.
\newblock \emph{The Annals of Statistics}, 32\penalty0 (1):\penalty0 56--85,
  2004{\natexlab{a}}.

\bibitem[Zhang(2004{\natexlab{b}})]{zhang2004statistical}
Tong Zhang.
\newblock Statistical analysis of some multi-category large margin
  classification methods.
\newblock \emph{Journal of Machine Learning Research}, 5\penalty0
  (Oct):\penalty0 1225--1251, 2004{\natexlab{b}}.

\bibitem[Zhang and Sabuncu(2018)]{zhang2018generalized}
Zhilu Zhang and Mert Sabuncu.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock In \emph{Advances in neural information processing systems}, 2018.

\end{thebibliography}
