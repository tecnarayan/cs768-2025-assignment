@article{shwartz2017opening,
  title={Opening the black box of deep neural networks via information},
  author={Shwartz-Ziv, Ravid and Tishby, Naftali},
  journal={arXiv preprint arXiv:1703.00810},
  year={2017}
}

@incollection{von2011statistical,
  title={Statistical learning theory: Models, concepts, and results},
  author={Von Luxburg, Ulrike and Sch{\"o}lkopf, Bernhard},
  booktitle={Handbook of the History of Logic},
  volume={10},
  pages={651--706},
  year={2011},
  publisher={Elsevier}
}

@inproceedings{hinton93keeping,
  title={Keeping neural networks simple by minimising the description length of weights. 1993},
  author={Hinton, GE and van Camp, Drew},
  booktitle={Proceedings of COLT-93},
  pages={5--13}
}

@article{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1019--1028},
  year={2017},
  organization={PMLR}
}

@article{chaudhari2019entropy,
  title={Entropy-sgd: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124018},
  year={2019},
  publisher={IOP Publishing}
}

@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{achille2019information,
  title={Where is the information in a deep neural network?},
  author={Achille, Alessandro and Paolini, Giovanni and Soatto, Stefano},
  journal={arXiv preprint arXiv:1905.12213},
  year={2019}
}

@article{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  journal={arXiv preprint arXiv:1703.11008},
  year={2017}
}

@article{achille2018emergence,
  title={Emergence of invariance and disentanglement in deep representations},
  author={Achille, Alessandro and Soatto, Stefano},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={1947--1980},
  year={2018},
  publisher={JMLR. org}
}

@inproceedings{KhoslaYaoJayadevaprakashFeiFei_FGVC2011,
author = "Aditya Khosla and Nityananda Jayadevaprakash and Bangpeng Yao and Li Fei-Fei",
title = "Novel Dataset for Fine-Grained Image Categorization",
booktitle = "First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition",
year = "2011",
month = "June",
address = "Colorado Springs, CO",
}


@InProceedings{parkhi12a,
  author       = "Omkar M. Parkhi and Andrea Vedaldi and Andrew Zisserman and C. V. Jawahar",
  title        = "Cats and Dogs",
  booktitle    = "IEEE Conference on Computer Vision and Pattern Recognition",
  year         = "2012",
}

@inproceedings{quattoni2009recognizing,
  title={Recognizing indoor scenes},
  author={Quattoni, Ariadna and Torralba, Antonio},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={413--420},
  year={2009},
  organization={IEEE}
}

@techreport{maji13fine-grained,
   title         = {Fine-Grained Visual Classification of Aircraft},
   author        = {S. Maji and J. Kannala and E. Rahtu
                    and M. Blaschko and A. Vedaldi},
   year          = {2013},
   archivePrefix = {arXiv},
   eprint        = {1306.5151},
   primaryClass  = "cs-cv",
}


@InProceedings{Nilsback06,
  author       = "Maria-Elena Nilsback and Andrew Zisserman",
  title        = "A Visual Vocabulary for Flower Classification",
  booktitle    = "IEEE Conference on Computer Vision and Pattern Recognition",
  volume       = "2",
  pages        = "1447--1454",
  year         = "2006",
}


@inproceedings{hardt2016train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={International conference on machine learning},
  pages={1225--1234},
  year={2016},
  organization={PMLR}
}

@inproceedings{harutyunyan21,
 author = {Harutyunyan, Hrayr and Raginsky, Maxim and Ver Steeg, Greg and Galstyan, Aram},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {24670--24682},
 publisher = {Curran Associates, Inc.},
 title = {Information-theoretic generalization bounds for black-box learning algorithms},
 url = {https://proceedings.neurips.cc/paper/2021/file/cf0d02ec99e61a64137b8a2c3b03e030-Paper.pdf},
 volume = {34},
 year = {2021}
}


@article{entropy,
  title={Approximations of Shannon Mutual Information for Discrete Variables with Applications to Neural Population Coding},
  author={Wentao Huang and Kechen Zhang},
  journal={Entropy},
  year={2019},
  volume={21}
}
@misc{smooth_sample,
  doi = {10.48550/ARXIV.2101.06640},
  
  url = {https://arxiv.org/abs/2101.06640},
  
  author = {Harutyunyan, Hrayr and Achille, Alessandro and Paolini, Giovanni and Majumder, Orchid and Ravichandran, Avinash and Bhotika, Rahul and Soatto, Stefano},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Estimating informativeness of samples with Smooth Unique Information},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Bu_2020,
	doi = {10.1109/jsait.2020.2991139},
  
	url = {https://doi.org/10.1109%2Fjsait.2020.2991139},
  
	year = 2020,
	month = {may},
  
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  
	volume = {1},
  
	number = {1},
  
	pages = {121--130},
  
	author = {Yuheng Bu and Shaofeng Zou and Venugopal V. Veeravalli},
  
	title = {Tightening Mutual Information-Based Bounds on Generalization Error},
  
	journal = {{IEEE} Journal on Selected Areas in Information Theory}
}

@misc{https://doi.org/10.48550/arxiv.1710.05233,
  doi = {10.48550/ARXIV.1710.05233},
  
  url = {https://arxiv.org/abs/1710.05233},
  
  author = {Bassily, Raef and Moran, Shay and Nachum, Ido and Shafer, Jonathan and Yehudayoff, Amir},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Cryptography and Security (cs.CR), Information Theory (cs.IT), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learners that Use Little Information},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@INPROCEEDINGS{6289001,

  author={Durrieu, J.-L. and Thiran, J.-Ph. and Kelly, F.},

  booktitle={2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 

  title={Lower and upper bounds for approximation of the Kullback-Leibler divergence between Gaussian Mixture Models}, 

  year={2012},

  volume={},

  number={},

  pages={4833-4836},

  doi={10.1109/ICASSP.2012.6289001}}


@inproceedings{recht2015,
author = {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
title = {Train Faster, Generalize Better: Stability of Stochastic Gradient Descent},
year = {2016},
publisher = {JMLR.org},
abstract = {We show that parametric models trained by a stochastic gradient method (SGM) with few iterations have vanishing generalization error. We prove our results by arguing that SGM is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. We derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions.Applying our results to the convex case, we provide new insights for why multiple epochs of stochastic gradient methods generalize well in practice. In the non-convex case, we give a new interpretation of common practices in neural networks, and formally show that popular techniques for training large deep models are indeed stability-promoting. Our findings conceptually underscore the importance of reducing training time beyond its obvious benefit.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {1225–1234},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@inproceedings{xu2017,
	author = {Xu, Aolin and Raginsky, Maxim},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Information-theoretic analysis of generalization capability of learning algorithms},
	url = {https://proceedings.neurips.cc/paper/2017/file/ad71c82b22f4f65b9398f76d8be4c615-Paper.pdf},
	volume = {30},
	year = {2017},
	Bdsk-Url-1 = {https://proceedings.neurips.cc/paper/2017/file/ad71c82b22f4f65b9398f76d8be4c615-Paper.pdf}}

@InProceedings{russo16,
  title = 	 {Controlling Bias in Adaptive Data Analysis Using Information Theory},
  author = 	 {Russo, Daniel and Zou, James},
  booktitle = 	 {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1232--1240},
  year = 	 {2016},
  editor = 	 {Gretton, Arthur and Robert, Christian C.},
  volume = 	 {51},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Cadiz, Spain},
  month = 	 {09--11 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v51/russo16.pdf},
  url = 	 {https://proceedings.mlr.press/v51/russo16.html},
  abstract = 	 {Modern big data settings often involve messy, high-dimensional data, where it is not clear <em>a priori</em> what are the right questions to ask. To extract the most insights from a dataset, the analyst typically needs to engage in an iterative process of adaptive data analysis. The choice of analytics to be performed next depends on the results of the previous analyses on the same data. It is commonly recognized that such adaptivity (also called researcher degrees of freedom), even if well-intentioned, can lead to false discoveries, contributing to the crisis of reproducibility in science. In this paper, we propose a general information-theoretic framework to quantify and provably bound the bias of arbitrary adaptive analysis process. We prove that our mutual information based bound is tight in natural models. We show how this framework can give rigorous insights into when commonly used feature selection protocols (e.g. rank selection) do and do not lead to biased estimation. We also show how recent insights from differential privacy emerge from this framework when the analyst is assumed to be adversarial, though our bounds applies in more general settings. We illustrate our results with simple simulations.}
}
@INPROCEEDINGS{bu2019,

  author={Bu, Yuheng and Zou, Shaofeng and Veeravalli, Venugopal V.},

  booktitle={2019 IEEE International Symposium on Information Theory (ISIT)}, 

  title={Tightening Mutual Information Based Bounds on Generalization Error}, 

  year={2019},

  volume={},

  number={},

  pages={587-591},

  doi={10.1109/ISIT.2019.8849590}}

@inproceedings{haghifam2020,
	author = {Haghifam, Mahdi and Negrea, Jeffrey and Khisti, Ashish and Roy, Daniel M and Dziugaite, Gintare Karolina},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {9925--9935},
	publisher = {Curran Associates, Inc.},
	title = {Sharpened Generalization Bounds based on Conditional Mutual Information and an Application to Noisy, Iterative Algorithms},
	url = {https://proceedings.neurips.cc/paper/2020/file/712a3c9878efeae8ff06d57432016ceb-Paper.pdf},
	volume = {33},
	year = {2020},
	Bdsk-Url-1 = {https://proceedings.neurips.cc/paper/2020/file/712a3c9878efeae8ff06d57432016ceb-Paper.pdf}}

@inproceedings{negrea2019,
	author = {Negrea, Jeffrey and Haghifam, Mahdi and Dziugaite, Gintare Karolina and Khisti, Ashish and Roy, Daniel M},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Information-Theoretic Generalization Bounds for SGLD via Data-Dependent Estimates},
	url = {https://proceedings.neurips.cc/paper/2019/file/05ae14d7ae387b93370d142d82220f1b-Paper.pdf},
	volume = {32},
	year = {2019},
	Bdsk-Url-1 = {https://proceedings.neurips.cc/paper/2019/file/05ae14d7ae387b93370d142d82220f1b-Paper.pdf}}


@InProceedings{neu21,
  title = 	 {Information-Theoretic Generalization Bounds for Stochastic Gradient Descent},
  author =       {Neu, Gergely and Dziugaite, Gintare Karolina and Haghifam, Mahdi and Roy, Daniel M.},
  booktitle = 	 {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages = 	 {3526--3545},
  year = 	 {2021},
  editor = 	 {Belkin, Mikhail and Kpotufe, Samory},
  volume = 	 {134},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {15--19 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v134/neu21a/neu21a.pdf},
  url = 	 {https://proceedings.mlr.press/v134/neu21a.html},
  abstract = 	 {We study the generalization properties of the popular stochastic optimization method known as  stochastic gradient descent (SGD) for optimizing general non-convex loss functions. Our main contribution is providing upper bounds on the generalization error that depend on local statistics of the stochastic gradients evaluated along the path of iterates calculated by SGD. The key factors our bounds depend on are the variance of the gradients (with respect to the data distribution) and the local smoothness of the objective function along the SGD path, and the sensitivity of the loss function to perturbations to the final output. Our key technical tool is combining the information-theoretic generalization bounds previously used for analyzing randomized variants of SGD with a perturbation analysis of the iterates.}
}
@article{pensia2018,
  title={Generalization Error Bounds for Noisy, Iterative Algorithms},
  author={Ankit Pensia and Varun Jog and Po-Ling Loh},
  journal={2018 IEEE International Symposium on Information Theory (ISIT)},
  year={2018},
  pages={546-550}
}

@InProceedings{steinke2020,
  title = 	 {{R}easoning {A}bout {G}eneralization via {C}onditional {M}utual {I}nformation},
  author =       {Steinke, Thomas and Zakynthinou, Lydia},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {3437--3452},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v125/steinke20a/steinke20a.pdf},
  url = 	 {https://proceedings.mlr.press/v125/steinke20a.html},
  abstract = 	 { We provide an information-theoretic framework for studying the generalization properties of machine learning algorithms. Our framework ties together existing approaches, including uniform convergence bounds and recent methods for adaptive data analysis. Specifically, we use Conditional Mutual Information (CMI) to quantify how well the input (i.e., the training data) can be recognized given the output (i.e., the trained model) of the learning algorithm. We show that bounds on CMI can be obtained from VC dimension, compression schemes, differential privacy, and other methods. We then show that bounded CMI implies various forms of generalization.}
}
@inproceedings{
harutyunyan2021estimating,
title={Estimating informativeness of samples with Smooth Unique Information},
author={Hrayr Harutyunyan and Alessandro Achille and Giovanni Paolini and Orchid Majumder and Avinash Ravichandran and Rahul Bhotika and Stefano Soatto},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=kEnBH98BGs5}
}
@inproceedings{koh2017,
author = {Koh, Pang Wei and Liang, Percy},
title = {Understanding Black-Box Predictions via Influence Functions},
year = {2017},
publisher = {JMLR.org},
abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic technique from robust statistics — to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1885–1894},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}
@MISC{4412164,
    TITLE = {Obtaining a tight bound for an Expectation w.r.t a uniform random variable},
    AUTHOR = {user6247850 (https://math.stackexchange.com/users/472694/user6247850)},
    HOWPUBLISHED = {Mathematics Stack Exchange},
    NOTE = {URL:https://math.stackexchange.com/q/4412164 (version: 2022-03-24)},
    EPRINT = {https://math.stackexchange.com/q/4412164},
    URL = {https://math.stackexchange.com/q/4412164}
}
  

