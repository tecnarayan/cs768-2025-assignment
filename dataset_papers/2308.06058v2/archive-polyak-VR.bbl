\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acar et~al.(2021)Acar, Zhao, Navarro, Mattina, Whatmough, and
  Saligrama]{feddyn}
Durmus Alp~Emre Acar, Yue Zhao, Ramon~Matas Navarro, Matthew Mattina, Paul~N
  Whatmough, and Venkatesh Saligrama.
\newblock Federated learning based on dynamic regularization.
\newblock \emph{arXiv preprint arXiv:2111.04263}, 2021.

\bibitem[Armijo(1966)]{armijo-line-search}
Larry Armijo.
\newblock Minimization of functions having lipschitz continuous first partial
  derivatives.
\newblock \emph{Pacific Journal of Mathematics}, 16\penalty0 (1):\penalty0
  1--3, 1 1966.

\bibitem[Berrada et~al.(2020)Berrada, Zisserman, and Kumar]{alig}
Leonard Berrada, Andrew Zisserman, and M.~Pawan Kumar.
\newblock Training neural networks for and by interpolation.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, ICML'20. JMLR.org, 2020.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and Nocedal]{large_opt_book}
L\'{e}on Bottou, Frank~E. Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{SIAM Review}, 60\penalty0 (2):\penalty0 223--311, 2018.
\newblock \doi{10.1137/16M1080173}.
\newblock URL \url{https://doi.org/10.1137/16M1080173}.

\bibitem[Bubeck(2015)]{bubek_convex}
S\'{e}bastien Bubeck.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Found. Trends Mach. Learn.}, 8\penalty0 (3–4):\penalty0
  231–357, nov 2015.
\newblock ISSN 1935-8237.
\newblock \doi{10.1561/2200000050}.
\newblock URL \url{https://doi.org/10.1561/2200000050}.

\bibitem[Chang and Lin(2011)]{libsvm}
Chih-Chung Chang and Chih-Jen Lin.
\newblock Libsvm: A library for support vector machines.
\newblock \emph{ACM Trans. Intell. Syst. Technol.}, 2\penalty0 (3), may 2011.
\newblock ISSN 2157-6904.
\newblock \doi{10.1145/1961189.1961199}.
\newblock URL \url{https://doi.org/10.1145/1961189.1961199}.

\bibitem[Cutkosky and Boahen(2016)]{unbound_adp}
Ashok Cutkosky and Kwabena Boahen.
\newblock Online convex optimization with unconstrained domains and losses.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, NIPS'16, page 748–756, Red Hook, NY, USA,
  2016. Curran Associates Inc.
\newblock ISBN 9781510838819.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and Lacoste-Julien]{saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In Z.~Ghahramani, M.~Welling, C.~Cortes, N.~Lawrence, and K.Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~27. Curran Associates, Inc., 2014.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2014/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf}.

\bibitem[Dubois{-}Taine et~al.(2022)Dubois{-}Taine, Vaswani, Babanezhad,
  Schmidt, and Lacoste{-}Julien]{ada-svrg}
Benjamin Dubois{-}Taine, Sharan Vaswani, Reza Babanezhad, Mark Schmidt, and
  Simon Lacoste{-}Julien.
\newblock {SVRG} meets adagrad: painless variance reduction.
\newblock \emph{Mach. Learn.}, 111\penalty0 (12):\penalty0 4359--4409, 2022.
\newblock \doi{10.1007/s10994-022-06265-x}.
\newblock URL \url{https://doi.org/10.1007/s10994-022-06265-x}.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{AdaGrad}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{J. Mach. Learn. Res.}, 12\penalty0 (null):\penalty0
  2121–2159, jul 2011.
\newblock ISSN 1532-4435.

\bibitem[Galli et~al.(2023)Galli, Rauhut, and Schmidt]{relax_ls}
Leonardo Galli, Holger Rauhut, and Mark Schmidt.
\newblock Don't be so monotone: Relaxing stochastic line search in
  over-parameterized models, 2023.

\bibitem[Gower et~al.(2021)Gower, Defazio, and Rabbat]{spsmt}
Robert~M. Gower, Aaron Defazio, and Michael~G. Rabbat.
\newblock Stochastic polyak stepsize with a moving target.
\newblock \emph{CoRR}, abs/2106.11851, 2021.
\newblock URL \url{https://arxiv.org/abs/2106.11851}.

\bibitem[Gower et~al.(2022)Gower, Blondel, Gazagnadou, and
  Pedregosa]{gower2022cutting}
Robert~M Gower, Mathieu Blondel, Nidham Gazagnadou, and Fabian Pedregosa.
\newblock Cutting some slack for sgd with adaptive polyak stepsizes.
\newblock \emph{arXiv preprint arXiv:2202.12328}, 2022.

\bibitem[Hastie et~al.(2009)Hastie, Tibshirani, and Friedman]{hastie}
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
\newblock \emph{The elements of statistical learning: data mining, inference
  and prediction}.
\newblock Springer, 2 edition, 2009.
\newblock URL \url{http://www-stat.stanford.edu/~tibs/ElemStatLearn/}.

\bibitem[Hazan and Kakade(2019)]{hazan_polyak}
Elad Hazan and Sham Kakade.
\newblock Revisiting the polyak step size, 2019.
\newblock URL \url{https://arxiv.org/abs/1905.00313}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 770--778, 2016.
\newblock \doi{10.1109/CVPR.2016.90}.

\bibitem[Johnson and Zhang(2013)]{VR_zhang}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In C.J. Burges, L.~Bottou, M.~Welling, Z.~Ghahramani, and K.Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~26. Curran Associates, Inc., 2013.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2013/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Paper.pdf}.

\bibitem[Kairouz et~al.(2021)Kairouz, McMahan, Avent, Bellet, Bennis,
  Nitin~Bhagoji, Bonawitz, Charles, Cormode, Cummings, D’Oliveira, Eichner,
  El~Rouayheb, Evans, Gardner, Garrett, Gasc\'{o}n, Ghazi, Gibbons, Gruteser,
  Harchaoui, He, He, Huo, Hutchinson, Hsu, Jaggi, Javidi, Joshi, Khodak,
  Konecn\'{y}, Korolova, Koushanfar, Koyejo, Lepoint, Liu, Mittal, Mohri, Nock,
  \"{O}zg\"{u}r, Pagh, Qi, Ramage, Raskar, Raykova, Song, Song, Stich, Sun,
  Suresh, Tram\`{e}r, Vepakomma, Wang, Xiong, Xu, Yang, Yu, Yu, and Zhao]{FL}
Peter Kairouz, H.~Brendan McMahan, Brendan Avent, Aur\'{e}lien Bellet, Mehdi
  Bennis, Arjun Nitin~Bhagoji, Kallista Bonawitz, Zachary Charles, Graham
  Cormode, Rachel Cummings, Rafael G.~L. D’Oliveira, Hubert Eichner, Salim
  El~Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adri\`{a}
  Gasc\'{o}n, Badih Ghazi, Phillip~B. Gibbons, Marco Gruteser, Zaid Harchaoui,
  Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi,
  Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konecn\'{y}, Aleksandra
  Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancr\`{e}de Lepoint, Yang Liu,
  Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer \"{O}zg\"{u}r, Rasmus
  Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova, Dawn Song,
  Weikang Song, Sebastian~U. Stich, Ziteng Sun, Ananda~Theertha Suresh, Florian
  Tram\`{e}r, Praneeth Vepakomma, Jianyu Wang, Li~Xiong, Zheng Xu, Qiang Yang,
  Felix~X. Yu, Han Yu, and Sen Zhao.
\newblock Advances and open problems in federated learning.
\newblock \emph{Found. Trends Mach. Learn.}, 14\penalty0 (1–2):\penalty0
  1–210, jun 2021.
\newblock ISSN 1935-8237.
\newblock \doi{10.1561/2200000083}.
\newblock URL \url{https://doi.org/10.1561/2200000083}.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
  Stich, and Ananda~Theertha Suresh.
\newblock {SCAFFOLD}: Stochastic controlled averaging for federated learning.
\newblock In Hal~Daumé III and Aarti Singh, editors, \emph{Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 5132--5143. PMLR,
  13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/karimireddy20a.html}.

\bibitem[Kavis et~al.(2022)Kavis, Skoulakis, Antonakopoulos, Dadi, and
  Cevher]{svrg-nonconvex}
Ali Kavis, Stratis Skoulakis, Kimon Antonakopoulos, Leello~Tadesse Dadi, and
  Volkan Cevher.
\newblock Adaptive stochastic variance reduction for non-convex finite-sum
  minimization.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~35,
  pages 23524--23538. Curran Associates, Inc., 2022.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/94f625dcdec313cd432d65f96fcc51c8-Paper-Conference.pdf}.

\bibitem[Kingma and Ba(2015)]{adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Yoshua Bengio and Yann LeCun, editors, \emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6980}.

\bibitem[Kovalev et~al.(2020)Kovalev, Horv{\'a}th, and
  Richt{\'a}rik]{loopless_svrg}
Dmitry Kovalev, Samuel Horv{\'a}th, and Peter Richt{\'a}rik.
\newblock Don’t jump through hoops and remove those loops: Svrg and katyusha
  are better without the outer loop.
\newblock In Aryeh Kontorovich and Gergely Neu, editors, \emph{Proceedings of
  the 31st International Conference on Algorithmic Learning Theory}, volume 117
  of \emph{Proceedings of Machine Learning Research}, pages 451--467. PMLR, 08
  Feb--11 Feb 2020.
\newblock URL \url{https://proceedings.mlr.press/v117/kovalev20a.html}.

\bibitem[Krizhevsky et~al.({\natexlab{a}})Krizhevsky, Nair, and
  Hinton]{cifar10}
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
\newblock Cifar-10 (canadian institute for advanced research).
\newblock {\natexlab{a}}.
\newblock URL \url{http://www.cs.toronto.edu/~kriz/cifar.html}.

\bibitem[Krizhevsky et~al.({\natexlab{b}})Krizhevsky, Nair, and
  Hinton]{cifar100}
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
\newblock Cifar-100 (canadian institute for advanced research).
\newblock {\natexlab{b}}.
\newblock URL \url{http://www.cs.toronto.edu/~kriz/cifar.html}.

\bibitem[Kunstner et~al.(2023)Kunstner, Portella, Schmidt, and
  Harvey]{multidimls}
Frederik Kunstner, Victor~S. Portella, Mark Schmidt, and Nick Harvey.
\newblock Searching for optimal per-coordinate step-sizes with multidimensional
  backtracking, 2023.

\bibitem[Li et~al.(2022)Li, Swartworth, Tak{\'a}{\v{c}}, Needell, and
  Gower]{sp2}
Shuang Li, William~J Swartworth, Martin Tak{\'a}{\v{c}}, Deanna Needell, and
  Robert~M Gower.
\newblock Sp2: A second order stochastic polyak method.
\newblock \emph{arXiv preprint arXiv:2207.08171}, 2022.

\bibitem[Li et~al.(2020)Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{fedprox}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
  Virginia Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock In Inderjit~S. Dhillon, Dimitris~S. Papailiopoulos, and Vivienne Sze,
  editors, \emph{Proceedings of Machine Learning and Systems 2020, MLSys 2020,
  Austin, TX, USA, March 2-4, 2020}. mlsys.org, 2020.
\newblock URL \url{https://proceedings.mlsys.org/book/316.pdf}.

\bibitem[Li et~al.(2021)Li, Bao, Zhang, and Richtarik]{page}
Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richtarik.
\newblock Page: A simple and optimal probabilistic gradient estimator for
  nonconvex optimization.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 6286--6295. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/li21a.html}.

\bibitem[Loizou et~al.(2021)Loizou, Vaswani, Laradji, and
  Lacoste{-}Julien]{sps}
Nicolas Loizou, Sharan Vaswani, Issam~Hadj Laradji, and Simon Lacoste{-}Julien.
\newblock Stochastic polyak step-size for {SGD:} an adaptive learning rate for
  fast convergence.
\newblock In Arindam Banerjee and Kenji Fukumizu, editors, \emph{The 24th
  International Conference on Artificial Intelligence and Statistics, {AISTATS}
  2021, April 13-15, 2021, Virtual Event}, volume 130 of \emph{Proceedings of
  Machine Learning Research}, pages 1306--1314. {PMLR}, 2021.
\newblock URL \url{http://proceedings.mlr.press/v130/loizou21a.html}.

\bibitem[Loshchilov and Hutter(2017)]{warmupstepsize}
Ilya Loshchilov and Frank Hutter.
\newblock {SGDR}: Stochastic gradient descent with warm restarts.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=Skq89Scxx}.

\bibitem[Mishchenko et~al.(2022)Mishchenko, Malinovsky, Stich, and
  Richt{\'a}rik]{proxskip}
Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter
  Richt{\'a}rik.
\newblock Proxskip: Yes! local gradient steps provably lead to communication
  acceleration! finally!
\newblock In \emph{International Conference on Machine Learning}, pages
  15750--15769. PMLR, 2022.

\bibitem[Mukkamala and Hein(2017)]{log-AdaGrad}
Mahesh~Chandra Mukkamala and Matthias Hein.
\newblock Variants of {RMSP}rop and {A}dagrad with logarithmic regret bounds.
\newblock In Doina Precup and Yee~Whye Teh, editors, \emph{Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of
  \emph{Proceedings of Machine Learning Research}, pages 2545--2553. PMLR,
  06--11 Aug 2017.
\newblock URL \url{https://proceedings.mlr.press/v70/mukkamala17a.html}.

\bibitem[Nedi{\'c} and Bertsekas(2001)]{increment-polyak}
Angelia Nedi{\'c} and Dimitri Bertsekas.
\newblock \emph{Convergence Rate of Incremental Subgradient Algorithms}, pages
  223--264.
\newblock Springer US, Boston, MA, 2001.
\newblock ISBN 978-1-4757-6594-6.
\newblock \doi{10.1007/978-1-4757-6594-6{\_}11}.
\newblock URL \url{https://doi.org/10.1007/978-1-4757-6594-6_11}.

\bibitem[Nesterov(2014)]{nesterov_book}
Yurii Nesterov.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Springer Publishing Company, Incorporated, 1 edition, 2014.
\newblock ISBN 1461346916.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{sarah}
Lam~M. Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}{\v{c}}.
\newblock {SARAH}: A novel method for machine learning problems using
  stochastic recursive gradient.
\newblock In Doina Precup and Yee~Whye Teh, editors, \emph{Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of
  \emph{Proceedings of Machine Learning Research}, pages 2613--2621. PMLR,
  06--11 Aug 2017.
\newblock URL \url{https://proceedings.mlr.press/v70/nguyen17b.html}.

\bibitem[Nocedal and Wright(2006)]{num_opt_book}
Jorge Nocedal and Stephen~J. Wright.
\newblock \emph{Numerical Optimization}.
\newblock Springer, New York, NY, USA, 2e edition, 2006.

\bibitem[Oberman and Prazeres(2019)]{oberman2019}
Adam~M Oberman and Mariana Prazeres.
\newblock Stochastic gradient descent with polyak's learning rate.
\newblock \emph{arXiv preprint arXiv:1903.08688}, 2019.

\bibitem[Orabona and P{\'a}l(2015)]{adanorm-2015}
Francesco Orabona and D{\'a}vid P{\'a}l.
\newblock Scale-free algorithms for online linear optimization.
\newblock In Kamalika Chaudhuri, CLAUDIO GENTILE, and Sandra Zilles, editors,
  \emph{Algorithmic Learning Theory}, pages 287--301, Cham, 2015. Springer
  International Publishing.
\newblock ISBN 978-3-319-24486-0.

\bibitem[Orvieto et~al.(2022)Orvieto, Lacoste-Julien, and Loizou]{decsps}
Antonio Orvieto, Simon Lacoste-Julien, and Nicolas Loizou.
\newblock Dynamics of sgd with stochastic polyak stepsizes: Truly adaptive
  variants and convergence to exact solution.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~35,
  pages 26943--26954. Curran Associates, Inc., 2022.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/ac662d74829e4407ce1d126477f4a03a-Paper-Conference.pdf}.

\bibitem[Polyak(1987)]{polyak-book}
B.~T. Polyak.
\newblock \emph{Introduction to optimization}.
\newblock Translations series in mathematics and engineering. Optimization
  Software, Publications Division, New York, 1987.
\newblock ISBN 0911575146; 9780911575149.

\bibitem[Robbins and Monro(1951)]{sgd_robbin}
Herbert Robbins and Sutton Monro.
\newblock {A Stochastic Approximation Method}.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0
  (3):\penalty0 400 -- 407, 1951.
\newblock \doi{10.1214/aoms/1177729586}.
\newblock URL \url{https://doi.org/10.1214/aoms/1177729586}.

\bibitem[Rolinek and Martius(2018)]{L4}
Michal Rolinek and Georg Martius.
\newblock L4: Practical loss-based stepsize adaptation for deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems 31
  (NeurIPS 2018)}, pages 6434--6444. Curran Associates, Inc., 2018.
\newblock URL
  \url{http://papers.nips.cc/paper/7879-l4-practical-loss-based-stepsize-adaptation-for-deep-learning.pdf}.

\bibitem[Schmidt et~al.(2017)Schmidt, Le~Roux, and Bach]{sag}
Mark Schmidt, Nicolas Le~Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, 162\penalty0 (1):\penalty0 83--112,
  2017.
\newblock \doi{10.1007/s10107-016-1030-6}.
\newblock URL \url{https://doi.org/10.1007/s10107-016-1030-6}.

\bibitem[Streeter and McMahan(2010)]{adanorm-2010}
Matthew~J. Streeter and H.~Brendan McMahan.
\newblock Less regret via online conditioning.
\newblock \emph{CoRR}, abs/1002.4862, 2010.
\newblock URL \url{http://arxiv.org/abs/1002.4862}.

\bibitem[Vaswani et~al.(2019)Vaswani, Mishkin, Laradji, Schmidt, Gidel, and
  Lacoste{-}Julien]{sls}
Sharan Vaswani, Aaron Mishkin, Issam~H. Laradji, Mark Schmidt, Gauthier Gidel,
  and Simon Lacoste{-}Julien.
\newblock Painless stochastic gradient: Interpolation, line-search, and
  convergence rates.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
  d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett, editors, \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pages 3727--3740, 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/hash/2557911c1bf75c2b643afb4ecbfc8ec2-Abstract.html}.

\bibitem[Vaswani et~al.(2020)Vaswani, Laradji, Kunstner, Meng, Schmidt, and
  Lacoste-Julien]{adagrad+ls}
Sharan Vaswani, Issam Laradji, Frederik Kunstner, Si~Yi Meng, Mark Schmidt, and
  Simon Lacoste-Julien.
\newblock Adaptive gradient methods converge faster with over-parameterization
  (but you should do a line-search).
\newblock \emph{arXiv preprint arXiv:2006.06835}, 2020.

\bibitem[Ward et~al.(2019)Ward, Wu, and Bottou]{wald}
Rachel Ward, Xiaoxia Wu, and Leon Bottou.
\newblock {A}da{G}rad stepsizes: Sharp convergence over nonconvex landscapes.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  6677--6686. PMLR, 09--15 Jun 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/ward19a.html}.

\bibitem[Xie et~al.(2020)Xie, Wu, and Ward]{linear-AdaGrad}
Yuege Xie, Xiaoxia Wu, and Rachel Ward.
\newblock Linear convergence of adaptive stochastic gradient descent.
\newblock In Silvia Chiappa and Roberto Calandra, editors, \emph{Proceedings of
  the Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of \emph{Proceedings of Machine Learning Research},
  pages 1475--1485. PMLR, 26--28 Aug 2020.
\newblock URL \url{https://proceedings.mlr.press/v108/xie20a.html}.

\end{thebibliography}
