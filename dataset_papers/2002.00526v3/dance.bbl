\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adebayo et~al.(2018)Adebayo, Gilmer, Muelly, Goodfellow, Hardt, and
  Kim]{adebayo2018sanity}
Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., and Kim, B.
\newblock Sanity checks for saliency maps.
\newblock In \emph{Proc. of NeurIPS}, 2018.

\bibitem[Alvarez-Melis \& Jaakkola(2018)Alvarez-Melis and
  Jaakkola]{alvarez2018towards}
Alvarez-Melis, D. and Jaakkola, T.~S.
\newblock Towards robust interpretability with self-explaining neural networks.
\newblock In \emph{Proc. of NeurIPS}, 2018.

\bibitem[Ancona et~al.(2018)Ancona, Ceolini, {\"O}ztireli, and
  Gross]{ancona2017towards}
Ancona, M., Ceolini, E., {\"O}ztireli, C., and Gross, M.
\newblock Towards better understanding of gradient-based attribution methods
  for deep neural networks.
\newblock In \emph{Proc. of ICLR}, 2018.

\bibitem[Barber \& Cand{\`e}s(2015)Barber and
  Cand{\`e}s]{barber2015controlling}
Barber, R.~F. and Cand{\`e}s, E.~J.
\newblock Controlling the false discovery rate via knockoffs.
\newblock \emph{The Annals of Statistics}, 2015.

\bibitem[Binder et~al.(2016)Binder, Montavon, Lapuschkin, M{\"u}ller, and
  Samek]{binder2016layer}
Binder, A., Montavon, G., Lapuschkin, S., M{\"u}ller, K.-R., and Samek, W.
\newblock Layer-wise relevance propagation for neural networks with local
  renormalization layers.
\newblock In \emph{Proc. of ICANN}, 2016.

\bibitem[Burns et~al.(2019)Burns, Thomason, and Tansey]{burns2019hypothesis}
Burns, C., Thomason, J., and Tansey, W.
\newblock Interpreting black box models via hypothesis testing.
\newblock \emph{arXiv:1904.00045}, 2019.

\bibitem[Chalasani et~al.(2020)Chalasani, Chen, Chowdhury, Wu, and
  Jha]{chalasani2020concise}
Chalasani, P., Chen, J., Chowdhury, A.~R., Wu, X., and Jha, S.
\newblock Concise explanations of neural networks using adversarial training.
\newblock In \emph{Proc. of ICML}, 2020.

\bibitem[Chang et~al.(2019)Chang, Creager, Goldenberg, and
  Duvenaud]{change2019xplaining}
Chang, C.-H., Creager, E., Goldenberg, A., and Duvenaud, D.
\newblock Explaining image classifiers by counterfactual generation.
\newblock In \emph{Proc. of ICLR}, 2019.

\bibitem[Chen et~al.(2019{\natexlab{a}})Chen, Li, Tao, Barnett, Rudin, and
  Su]{chen2019looks}
Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., and Su, J.~K.
\newblock This looks like that: deep learning for interpretable image
  recognition.
\newblock In \emph{Proc. of NeurIPS}, 2019{\natexlab{a}}.

\bibitem[Chen et~al.(2018)Chen, Song, Wainwright, and Jordan]{chen2018learning}
Chen, J., Song, L., Wainwright, M.~J., and Jordan, M.~I.
\newblock Learning to explain: An information-theoretic perspective on model
  interpretation.
\newblock In \emph{Proc. of ICML}, 2018.

\bibitem[Chen et~al.(2019{\natexlab{b}})Chen, Song, Wainwright, and
  Jordan]{chen2018shapley}
Chen, J., Song, L., Wainwright, M.~J., and Jordan, M.~I.
\newblock L-shapley and c-shapley: Efficient model interpretation for
  structured data.
\newblock In \emph{Proc. of ICLR}, 2019{\natexlab{b}}.

\bibitem[Chen et~al.(2019{\natexlab{c}})Chen, Wu, Rastogi, Liang, and
  Jha]{chen2019robust}
Chen, J., Wu, X., Rastogi, V., Liang, Y., and Jha, S.
\newblock Robust attribution regularization.
\newblock In \emph{Proc. of NeurIPS}, 2019{\natexlab{c}}.

\bibitem[Dabkowski \& Gal(2017)Dabkowski and Gal]{dabkowski2017real}
Dabkowski, P. and Gal, Y.
\newblock Real time image saliency for black box classifiers.
\newblock In \emph{Proc. of NeurIPS}, 2017.

\bibitem[Etmann et~al.(2019)Etmann, Lunz, Maass, and
  Sch{\"o}nlieb]{etmann2019connection}
Etmann, C., Lunz, S., Maass, P., and Sch{\"o}nlieb, C.-B.
\newblock On the connection between adversarial robustness and saliency map
  interpretability.
\newblock \emph{arXiv preprint arXiv:1905.04172}, 2019.

\bibitem[Fan et~al.(2017)Fan, Zhao, and Ermon]{fan2017adversarial}
Fan, L., Zhao, S., and Ermon, S.
\newblock Adversarial localization network.
\newblock In \emph{Proc. of NeurIPS LLD Workshop}, 2017.

\bibitem[Fong \& Vedaldi(2017)Fong and Vedaldi]{fong2017interpretable}
Fong, R.~C. and Vedaldi, A.
\newblock Interpretable explanations of black boxes by meaningful perturbation.
\newblock In \emph{Proc. of ICCV}, 2017.

\bibitem[Ghorbani et~al.(2017)Ghorbani, Abid, and
  Zou]{ghorbani2017interpretation}
Ghorbani, A., Abid, A., and Zou, J.
\newblock Interpretation of neural networks is fragile.
\newblock \emph{arXiv:1710.10547}, 2017.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Wexler, Zou, and
  Kim]{ghorbani2019towards}
Ghorbani, A., Wexler, J., Zou, J.~Y., and Kim, B.
\newblock Towards automatic concept-based explanations.
\newblock In \emph{Proc. of NeurIPS}, 2019.

\bibitem[Goyal et~al.(2019)Goyal, Wu, Ernst, Batra, Parikh, and
  Lee]{goyal2019counterfactual}
Goyal, Y., Wu, Z., Ernst, J., Batra, D., Parikh, D., and Lee, S.
\newblock Counterfactual visual explanations.
\newblock \emph{Proc. of ICML}, 2019.

\bibitem[Guan et~al.(2019)Guan, Wang, Zhang, Chen, He, and
  Xie]{guan2019towards}
Guan, C., Wang, X., Zhang, Q., Chen, R., He, D., and Xie, X.
\newblock Towards a deep and unified understanding of deep neural models in
  nlp.
\newblock In \emph{Proc. of ICML}, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proc. of CVPR}, 2016.

\bibitem[Hendrycks \& Dietterich(2019)Hendrycks and
  Dietterich]{hendrycks2019benchmarking}
Hendrycks, D. and Dietterich, T.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In \emph{Proc. of ICLR}, 2019.

\bibitem[Hooker et~al.(2019)Hooker, Erhan, Kindermans, and
  Kim]{hooker2019benchmark}
Hooker, S., Erhan, D., Kindermans, P.-J., and Kim, B.
\newblock A benchmark for interpretability methods in deep neural networks.
\newblock In \emph{Proc. of NeurIPS}, 2019.

\bibitem[Ikeno \& Hara(2018)Ikeno and Hara]{ikeno2018maximizing}
Ikeno, K. and Hara, S.
\newblock Maximizing invariant data perturbation with stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1807.05077}, 2018.

\bibitem[Kim(2014)]{kim2014convolutional}
Kim, Y.
\newblock Convolutional neural networks for sentence classification.
\newblock \emph{Proc. of EMNLP}, 2014.

\bibitem[Kindermans et~al.(2017)Kindermans, Hooker, Adebayo, Alber, Sch{\"u}tt,
  D{\"a}hne, Erhan, and Kim]{kindermans2017reliability}
Kindermans, P.-J., Hooker, S., Adebayo, J., Alber, M., Sch{\"u}tt, K.~T.,
  D{\"a}hne, S., Erhan, D., and Kim, B.
\newblock The ({Un}) reliability of saliency methods.
\newblock \emph{arXiv:1711.00867}, 2017.

\bibitem[Koh \& Liang(2017)Koh and Liang]{koh2017understanding}
Koh, P.~W. and Liang, P.
\newblock Understanding black-box predictions via influence functions.
\newblock \emph{Proc. of ICML}, 2017.

\bibitem[Koh et~al.(2019)Koh, Ang, Teo, and Liang]{koh2019accuracy}
Koh, P. W.~W., Ang, K.-S., Teo, H., and Liang, P.~S.
\newblock On the accuracy of influence functions for measuring group effects.
\newblock In \emph{Proc. of NeurIPS}, 2019.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Proc. of NeurIPS}, 2012.

\bibitem[Levine et~al.(2019)Levine, Singla, and Feizi]{levine2019certifiably}
Levine, A., Singla, S., and Feizi, S.
\newblock Certifiably robust interpretation in deep learning.
\newblock \emph{arXiv preprint arXiv:1905.12105}, 2019.

\bibitem[Lipton(2016)]{lipton2016mythos}
Lipton, Z.~C.
\newblock The mythos of model interpretability.
\newblock \emph{arXiv:1606.03490}, 2016.

\bibitem[Lu et~al.(2018)Lu, Fan, Lv, and Noble]{lu2018deeppink}
Lu, Y., Fan, Y., Lv, J., and Noble, W.~S.
\newblock {DeepPINK}: reproducible feature selection in deep neural networks.
\newblock In \emph{Proc. of NeurIPS}, 2018.

\bibitem[Lundberg \& Lee(2017)Lundberg and Lee]{lundberg2017unified}
Lundberg, S.~M. and Lee, S.-I.
\newblock A unified approach to interpreting model predictions.
\newblock In \emph{Proc. of NeurIPS}, 2017.

\bibitem[Moosavi-Dezfooli et~al.(2017)Moosavi-Dezfooli, Fawzi, Fawzi, and
  Frossard]{moosavi2017universal}
Moosavi-Dezfooli, S.-M., Fawzi, A., Fawzi, O., and Frossard, P.
\newblock Universal adversarial perturbations.
\newblock In \emph{Proc. of CVPR}, 2017.

\bibitem[Nie et~al.(2018)Nie, Zhang, and Patel]{nie2018theoretical}
Nie, W., Zhang, Y., and Patel, A.
\newblock A theoretical explanation for perplexing behaviors of
  backpropagation-based visualizations.
\newblock In \emph{Proc. of ICML}, 2018.

\bibitem[Pennington et~al.(2014)Pennington, Socher, and
  Manning]{pennington2014glove}
Pennington, J., Socher, R., and Manning, C.~D.
\newblock Glove: Global vectors for word representation.
\newblock In \emph{Proc. of EMNLP}, 2014.

\bibitem[Ribeiro et~al.(2016)Ribeiro, Singh, and Guestrin]{ribeiro2016should}
Ribeiro, M.~T., Singh, S., and Guestrin, C.
\newblock Why should i trust you?: Explaining the predictions of any
  classifier.
\newblock In \emph{Proc. of KDD}, 2016.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International Journal of Computer Vision}, 2015.

\bibitem[Schulz et~al.(2020)Schulz, Sixt, Tombari, and
  Landgraf]{schulz2020restricting}
Schulz, K., Sixt, L., Tombari, F., and Landgraf, T.
\newblock Restricting the flow: Information bottlenecks for attribution.
\newblock 2020.

\bibitem[Selvaraju et~al.(2016)Selvaraju, Das, Vedantam, Cogswell, Parikh, and
  Batra]{selvaraju2016grad}
Selvaraju, R.~R., Das, A., Vedantam, R., Cogswell, M., Parikh, D., and Batra,
  D.
\newblock Grad-cam: Visual explanations from deep networks via gradient-based
  localization.
\newblock \emph{arXiv:1611.07450}, 2016.

\bibitem[Shrikumar et~al.(2017)Shrikumar, Greenside, and
  Kundaje]{shrikumar2017learning}
Shrikumar, A., Greenside, P., and Kundaje, A.
\newblock Learning important features through propagating activation
  differences.
\newblock In \emph{Proc. of ICML}, 2017.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv:1409.1556}, 2014.

\bibitem[Simonyan et~al.(2013)Simonyan, Vedaldi, and
  Zisserman]{simonyan2013deep}
Simonyan, K., Vedaldi, A., and Zisserman, A.
\newblock Deep inside convolutional networks: Visualising image classification
  models and saliency maps.
\newblock \emph{arXiv:1312.6034}, 2013.

\bibitem[Singla et~al.(2019)Singla, Wallace, Feng, and
  Feizi]{singla2019understanding}
Singla, S., Wallace, E., Feng, S., and Feizi, S.
\newblock Understanding impacts of high-order loss approximations and features
  in deep learning interpretation.
\newblock \emph{arXiv:1902.00407}, 2019.

\bibitem[Sixt et~al.(2020)Sixt, Granz, and Landgraf]{sixt2019explanations}
Sixt, L., Granz, M., and Landgraf, T.
\newblock When explanations lie: Why many modified bp attributions fail.
\newblock In \emph{Proc. of ICML}, 2020.

\bibitem[Smilkov et~al.(2017)Smilkov, Thorat, Kim, Vi{\'e}gas, and
  Wattenberg]{smilkov2017smoothgrad}
Smilkov, D., Thorat, N., Kim, B., Vi{\'e}gas, F., and Wattenberg, M.
\newblock Smoothgrad: removing noise by adding noise.
\newblock \emph{arXiv:1706.03825}, 2017.

\bibitem[Springenberg et~al.(2014)Springenberg, Dosovitskiy, Brox, and
  Riedmiller]{springenberg2014striving}
Springenberg, J.~T., Dosovitskiy, A., Brox, T., and Riedmiller, M.
\newblock Striving for simplicity: The all convolutional net.
\newblock \emph{arXiv preprint arXiv:1412.6806}, 2014.

\bibitem[Sturmfels et~al.(2020)Sturmfels, Lundberg, and
  Lee]{sturmfels2020visualizing}
Sturmfels, P., Lundberg, S., and Lee, S.-I.
\newblock Visualizing the impact of feature attribution baselines.
\newblock \emph{Distill}, 2020.

\bibitem[Sundararajan et~al.(2017)Sundararajan, Taly, and
  Yan]{sundararajan2017axiomatic}
Sundararajan, M., Taly, A., and Yan, Q.
\newblock Axiomatic attribution for deep networks.
\newblock In \emph{Proc. of ICML}, 2017.

\bibitem[Toneva \& Wehbe(2019)Toneva and Wehbe]{toneva2019interpreting}
Toneva, M. and Wehbe, L.
\newblock Interpreting and improving natural-language processing (in machines)
  with natural language-processing (in the brain).
\newblock In \emph{Proc. of NeurIPS}, 2019.

\bibitem[Yeh et~al.(2018)Yeh, Kim, Yen, and Ravikumar]{yeh2018representer}
Yeh, C.-K., Kim, J., Yen, I. E.-H., and Ravikumar, P.~K.
\newblock Representer point selection for explaining deep neural networks.
\newblock In \emph{Proc. of NeurIPS}, 2018.

\bibitem[Yousefzadeh \& O'Leary(2019)Yousefzadeh and
  O'Leary]{yousefzadeh2019interpreting}
Yousefzadeh, R. and O'Leary, D.~P.
\newblock Interpreting neural networks using flip points.
\newblock \emph{arXiv preprint arXiv:1903.08789}, 2019.

\bibitem[Zeiler \& Fergus(2014)Zeiler and Fergus]{zeiler2014visualizing}
Zeiler, M.~D. and Fergus, R.
\newblock Visualizing and understanding convolutional networks.
\newblock In \emph{Proc. of ECCV}, 2014.

\bibitem[Zo{\l}na et~al.(2019)Zo{\l}na, Geras, and Cho]{zolna2019classifier}
Zo{\l}na, K., Geras, K.~J., and Cho, K.
\newblock Classifier-agnostic saliency map extraction.
\newblock In \emph{Proceedings of AAAI}, 2019.

\end{thebibliography}
