\begin{thebibliography}{10}

\bibitem{thrun2012learning}
S.~Thrun and L.~Pratt, {\em Learning to learn}.
\newblock Springer Science \& Business Media, 2012.

\bibitem{schmidhuber1992learning}
J.~Schmidhuber, ``Learning to control fast-weight memories: An alternative to
  dynamic recurrent networks,'' {\em Neural Computation}, vol.~4, no.~1,
  pp.~131--139, 1992.

\bibitem{bengio1990learning}
Y.~Bengio, S.~Bengio, and J.~Cloutier, {\em Learning a synaptic learning rule}.
\newblock Citeseer.

\bibitem{vilalta2002perspective}
R.~Vilalta and Y.~Drissi, ``A perspective view and survey of meta-learning,''
  {\em Artificial intelligence review}, vol.~18, no.~2, pp.~77--95, 2002.

\bibitem{finn2017model}
C.~Finn, P.~Abbeel, and S.~Levine, ``Model-agnostic meta-learning for fast
  adaptation of deep networks,'' in {\em Proceedings of the 34th International
  Conference on Machine Learning-Volume 70}, pp.~1126--1135, JMLR. org, 2017.

\bibitem{gabillon2013adaptive}
V.~Gabillon, B.~Kveton, Z.~Wen, B.~Eriksson, and S.~Muthukrishnan, ``Adaptive
  submodular maximization in bandit setting,'' in {\em Advances in Neural
  Information Processing Systems}, pp.~2697--2705, 2013.

\bibitem{el2009turning}
K.~El-Arini, G.~Veda, D.~Shahaf, and C.~Guestrin, ``Turning down the noise in
  the blogosphere,'' in {\em Proceedings of the 15th ACM SIGKDD international
  conference on Knowledge discovery and data mining}, pp.~289--298, 2009.

\bibitem{yue2011linear}
Y.~Yue and C.~Guestrin, ``Linear submodular bandits and their application to
  diversified retrieval,'' in {\em Advances in Neural Information Processing
  Systems}, pp.~2483--2491, 2011.

\bibitem{vinyals2016matching}
O.~Vinyals, C.~Blundell, T.~Lillicrap, D.~Wierstra, {\em et~al.}, ``Matching
  networks for one shot learning,'' in {\em Advances in neural information
  processing systems}, pp.~3630--3638, 2016.

\bibitem{DBLP:conf/iclr/RaviL17}
S.~Ravi and H.~Larochelle, ``Optimization as a model for few-shot learning,''
  in {\em 5th International Conference on Learning Representations, {ICLR}},
  2017.

\bibitem{snell2017prototypical}
J.~Snell, K.~Swersky, and R.~Zemel, ``Prototypical networks for few-shot
  learning,'' in {\em Advances in neural information processing systems},
  pp.~4077--4087, 2017.

\bibitem{wang2019few}
Y.~Wang and Q.~Yao, ``Few-shot learning: A survey,'' {\em arXiv preprint
  arXiv:1904.05046}, 2019.

\bibitem{DBLP:journals/corr/DuanSCBSA16}
Y.~Duan, J.~Schulman, X.~Chen, P.~L. Bartlett, I.~Sutskever, and P.~Abbeel,
  ``Rl{\textdollar}{\^{}}2{\textdollar}: Fast reinforcement learning via slow
  reinforcement learning,'' {\em CoRR}, vol.~abs/1611.02779, 2016.

\bibitem{wang2016learning}
J.~X. Wang, Z.~Kurth-Nelson, D.~Tirumala, H.~Soyer, J.~Z. Leibo, R.~Munos,
  C.~Blundell, D.~Kumaran, and M.~Botvinick, ``Learning to reinforcement
  learn,'' {\em arXiv preprint arXiv:1611.05763}, 2016.

\bibitem{DBLP:conf/iclr/SongGYCPT20}
X.~Song, W.~Gao, Y.~Yang, K.~Choromanski, A.~Pacchiano, and Y.~Tang,
  ``{ES-MAML:} simple {Hessian}-free meta learning,'' in {\em 8th International
  Conference on Learning Representations, {ICLR}}, 2020.

\bibitem{fallah2019convergence}
A.~Fallah, A.~Mokhtari, and A.~Ozdaglar, ``On the convergence theory of
  gradient-based model-agnostic meta-learning algorithms,'' in {\em
  International Conference on Artificial Intelligence and Statistics},
  pp.~1082--1092, 2020.

\bibitem{nichol2018first}
A.~Nichol, J.~Achiam, and J.~Schulman, ``On first-order meta-learning
  algorithms,'' {\em arXiv preprint arXiv:1803.02999}, 2018.

\bibitem{finn2018probabilistic}
C.~Finn, K.~Xu, and S.~Levine, ``Probabilistic model-agnostic meta-learning,''
  in {\em Advances in Neural Information Processing Systems}, pp.~9516--9527,
  2018.

\bibitem{DBLP:conf/iclr/GrantFLDG18}
E.~Grant, C.~Finn, S.~Levine, T.~Darrell, and T.~L. Griffiths, ``Recasting
  gradient-based meta-learning as hierarchical bayes,'' in {\em 6th
  International Conference on Learning Representations, {ICLR}}, 2018.

\bibitem{yoon2018bayesian}
J.~Yoon, T.~Kim, O.~Dia, S.~Kim, Y.~Bengio, and S.~Ahn, ``Bayesian
  model-agnostic meta-learning,'' in {\em Advances in Neural Information
  Processing Systems}, pp.~7332--7342, 2018.

\bibitem{DBLP:conf/iclr/AntoniouES19}
A.~Antoniou, H.~Edwards, and A.~J. Storkey, ``How to train your {MAML},'' in
  {\em 7th International Conference on Learning Representations, {ICLR}}, 2019.

\bibitem{collins2020distribution}
L.~Collins, A.~Mokhtari, and S.~Shakkottai, ``Distribution-agnostic
  model-agnostic meta-learning,'' {\em arXiv preprint arXiv:2002.04766}, 2020.

\bibitem{lin2011class}
H.~Lin and J.~Bilmes, ``A class of submodular functions for document
  summarization,'' in {\em Proceedings of the 49th Annual Meeting of the
  Association for Computational Linguistics: Human Language Technologies-Volume
  1}, pp.~510--520, Association for Computational Linguistics, 2011.

\bibitem{wei2013using}
K.~Wei, Y.~Liu, K.~Kirchhoff, and J.~Bilmes, ``Using document summarization
  techniques for speech data subset selection,'' in {\em Proceedings of the
  2013 Conference of the North American Chapter of the Association for
  Computational Linguistics: Human Language Technologies}, pp.~721--726, 2013.

\bibitem{kirchhoff2014submodularity}
K.~Kirchhoff and J.~Bilmes, ``Submodularity for data selection in machine
  translation,'' in {\em Proceedings of the 2014 Conference on Empirical
  Methods in Natural Language Processing (EMNLP)}, pp.~131--141, 2014.

\bibitem{mirzasoleiman2016distributed}
B.~Mirzasoleiman, A.~Karbasi, R.~Sarkar, and A.~Krause, ``Distributed
  submodular maximization,'' {\em The Journal of Machine Learning Research},
  vol.~17, no.~1, pp.~8330--8373, 2016.

\bibitem{kempe2003maximizing}
D.~Kempe, J.~Kleinberg, and {\'E}.~Tardos, ``Maximizing the spread of influence
  through a social network,'' in {\em Proceedings of the ninth ACM SIGKDD
  international conference on Knowledge discovery and data mining},
  pp.~137--146, 2003.

\bibitem{krause2008near}
A.~Krause, A.~Singh, and C.~Guestrin, ``Near-optimal sensor placements in
  gaussian processes: Theory, efficient algorithms and empirical studies,''
  {\em Journal of Machine Learning Research}, vol.~9, no.~Feb, pp.~235--284,
  2008.

\bibitem{das2011submodular}
A.~Das and D.~Kempe, ``Submodular meets spectral: Greedy algorithms for subset
  selection, sparse approximation and dictionary selection,'' {\em arXiv
  preprint arXiv:1102.3975}, 2011.

\bibitem{krause2014submodular}
A.~Krause and D.~Golovin, ``Submodular function maximization..''

\bibitem{nemhauser1978best}
G.~L. Nemhauser and L.~A. Wolsey, ``Best algorithms for approximating the
  maximum of a submodular set function,'' {\em Mathematics of operations
  research}, vol.~3, no.~3, pp.~177--188, 1978.

\bibitem{wolsey1982analysis}
L.~A. Wolsey, ``An analysis of the greedy algorithm for the submodular set
  covering problem,'' {\em Combinatorica}, vol.~2, no.~4, pp.~385--393, 1982.

\bibitem{mirzasoleiman2015lazier}
B.~Mirzasoleiman, A.~Badanidiyuru, A.~Karbasi, J.~Vondr{\'a}k, and A.~Krause,
  ``Lazier than lazy greedy,'' in {\em Twenty-Ninth AAAI Conference on
  Artificial Intelligence}, 2015.

\bibitem{karimi2017stochastic}
M.~Karimi, M.~Lucic, H.~Hassani, and A.~Krause, ``Stochastic submodular
  maximization: The case of coverage functions,'' in {\em Advances in Neural
  Information Processing Systems}, pp.~6853--6863, 2017.

\bibitem{barbosa2015power}
R.~Barbosa, A.~Ene, H.~Nguyen, and J.~Ward, ``The power of randomization:
  Distributed submodular maximization on massive datasets,'' in {\em
  International Conference on Machine Learning}, pp.~1236--1244, 2015.

\bibitem{mirrokni2015randomized}
V.~Mirrokni and M.~Zadimoghaddam, ``Randomized composable core-sets for
  distributed submodular maximization,'' in {\em Proceedings of the
  forty-seventh annual ACM symposium on Theory of computing}, pp.~153--162,
  2015.

\bibitem{kumar2015fast}
R.~Kumar, B.~Moseley, S.~Vassilvitskii, and A.~Vattani, ``Fast greedy
  algorithms in mapreduce and streaming,'' {\em ACM Transactions on Parallel
  Computing (TOPC)}, vol.~2, no.~3, pp.~1--22, 2015.

\bibitem{balkanski2019exponential}
E.~Balkanski, A.~Rubinstein, and Y.~Singer, ``An exponential speedup in
  parallel running time for submodular maximization without loss in
  approximation,'' in {\em Proceedings of the Thirtieth Annual ACM-SIAM
  Symposium on Discrete Algorithms}, pp.~283--302, SIAM, 2019.

\bibitem{lin2012learning}
H.~Lin and J.~A. Bilmes, ``Learning mixtures of submodular shells with
  application to document summarization,'' {\em arXiv preprint
  arXiv:1210.4871}, 2012.

\bibitem{zhang2019online}
M.~Zhang, L.~Chen, H.~Hassani, and A.~Karbasi, ``Online continuous submodular
  maximization: From full-information to bandit feedback,'' in {\em Advances in
  Neural Information Processing Systems}, pp.~9210--9221, 2019.

\bibitem{jegelka2011online}
S.~Jegelka and J.~A. Bilmes, ``Online submodular minimization for combinatorial
  structures.,'' in {\em ICML}, pp.~345--352, Citeseer, 2011.

\bibitem{streeter2009online}
M.~Streeter and D.~Golovin, ``An online algorithm for maximizing submodular
  functions,'' in {\em Advances in Neural Information Processing Systems},
  pp.~1577--1584, 2009.

\bibitem{chen2018projection}
L.~Chen, C.~Harshaw, H.~Hassani, and A.~Karbasi, ``Projection-free online
  optimization with stochastic gradient: From convexity to submodularity,''
  {\em arXiv preprint arXiv:1802.08183}, 2018.

\bibitem{mirzasoleiman2016fast}
B.~Mirzasoleiman, M.~Zadimoghaddam, and A.~Karbasi, ``Fast distributed
  submodular cover: Public-private data summarization,'' in {\em Advances in
  Neural Information Processing Systems}, pp.~3594--3602, 2016.

\bibitem{balkanski2016learning}
E.~Balkanski, B.~Mirzasoleiman, A.~Krause, and Y.~Singer, ``Learning sparse
  combinatorial representations via two-stage submodular maximization,'' in
  {\em International Conference on Machine Learning}, pp.~2207--2216, 2016.

\bibitem{mitrovic2018data}
M.~Mitrovic, E.~Kazemi, M.~Zadimoghaddam, and A.~Karbasi, ``Data summarization
  at scale: A two-stage submodular approach,'' {\em arXiv preprint
  arXiv:1806.02815}, 2018.

\bibitem{stan2017probabilistic}
S.~Stan, M.~Zadimoghaddam, A.~Krause, and A.~Karbasi, ``Probabilistic
  submodular maximization in sub-linear time,'' in {\em Proceedings of the 34th
  International Conference on Machine Learning-Volume 70}, pp.~3241--3250,
  JMLR. org, 2017.

\bibitem{ohsaka2015monotone}
N.~Ohsaka and Y.~Yoshida, ``Monotone k-submodular function maximization with
  size constraints,'' in {\em Advances in Neural Information Processing
  Systems}, pp.~694--702, 2015.

\bibitem{uber_2019}
UberDataset, ``Uber pickups in new york city.''
  \url{https://www.kaggle.com/fivethirtyeight/uber-pickups-in-new-york-city}.

\bibitem{harper2015movielens}
F.~M. Harper and J.~A. Konstan, ``The movielens datasets: History and
  context,'' {\em Acm transactions on interactive intelligent systems (tiis)},
  vol.~5, no.~4, pp.~1--19, 2015.

\end{thebibliography}
