\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and
  Zheng]{tensorflow2015-whitepaper}
Mart\'{i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Greg~S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,
  Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
  Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
  Levenberg, Dandelion Man\'{e}, Rajat Monga, Sherry Moore, Derek Murray, Chris
  Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal
  Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\'{e}gas,
  Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and
  Xiaoqiang Zheng.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock URL \url{https://www.tensorflow.org/}.
\newblock Software available from tensorflow.org.

\bibitem[Akbari et~al.(2021)Akbari, Yuan, Qian, Chuang, Chang, Cui, and
  Gong]{akbari2021vatt}
Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin
  Cui, and Boqing Gong.
\newblock Vatt: Transformers for multimodal self-supervised learning from raw
  video, audio and text.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Baevski et~al.(2020)Baevski, Zhou, Mohamed, and
  Auli]{baevski2020wav2vec}
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.
\newblock wav2vec 2.0: A framework for self-supervised learning of speech
  representations.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander{P}las, Skye
  Wanderman-{M}ilne, and Qiao Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs.
\newblock 2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Calvert(2001)]{calvert2001crossmodal}
Gemma~A Calvert.
\newblock Crossmodal processing in the human brain: insights from functional
  neuroimaging studies.
\newblock \emph{Cerebral cortex}, 2001.

\bibitem[Carreira et~al.(2018)Carreira, Noland, Banki-Horvath, Hillier, and
  Zisserman]{carreira2018short}
Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew
  Zisserman.
\newblock A short note about kinetics-600.
\newblock \emph{arXiv preprint arXiv:1808.01340}, 2018.

\bibitem[Carreira et~al.(2019)Carreira, Noland, Hillier, and
  Zisserman]{carreira2019short}
Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman.
\newblock A short note on the kinetics-700 human action dataset.
\newblock \emph{arXiv preprint arXiv:1907.06987}, 2019.

\bibitem[Changpinyo et~al.(2021)Changpinyo, Sharma, Ding, and
  Soricut]{changpinyo2021conceptual}
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.
\newblock Conceptual 12m: Pushing web-scale image-text pre-training to
  recognize long-tail visual concepts.
\newblock In \emph{CVPR}, 2021.

\bibitem[Chen et~al.(2022)Chen, Wang, Changpinyo, Piergiovanni, Padlewski,
  Salz, Goodman, Grycner, Mustafa, Beyer, et~al.]{chen2022pali}
Xi~Chen, Xiao Wang, Soravit Changpinyo, AJ~Piergiovanni, Piotr Padlewski,
  Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer,
  et~al.
\newblock Pali: A jointly-scaled multilingual language-image model.
\newblock \emph{arXiv preprint arXiv:2209.06794}, 2022.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek,
  Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, et~al.]{dehghani2023scaling}
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan
  Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim
  Alabdulmohsin, et~al.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock \emph{arXiv preprint arXiv:2302.05442}, 2023.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Driver and Noesselt(2008)]{driver2008multisensory}
Jon Driver and Toemme Noesselt.
\newblock Multisensory interplay reveals crossmodal influences on
  ‘sensory-specific’brain regions, neural responses, and judgments.
\newblock \emph{Neuron}, 2008.

\bibitem[Gemmeke et~al.(2017)Gemmeke, Ellis, Freedman, Jansen, Lawrence, Moore,
  Plakal, and Ritter]{gemmeke2017audio}
Jort~F Gemmeke, Daniel~PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence,
  R~Channing Moore, Manoj Plakal, and Marvin Ritter.
\newblock Audio set: An ontology and human-labeled dataset for audio events.
\newblock In \emph{ICASSP}, 2017.

\bibitem[Gowda et~al.(2021)Gowda, Rohrbach, and Sevilla-Lara]{gowda2021smart}
Shreyank~N Gowda, Marcus Rohrbach, and Laura Sevilla-Lara.
\newblock Smart frame selection for action recognition.
\newblock In \emph{AAAI}, 2021.

\bibitem[Huang et~al.(2022)Huang, Xu, Li, Baevski, Auli, Galuba, Metze, and
  Feichtenhofer]{huang2022masked}
Po-Yao Huang, Hu~Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech
  Galuba, Florian Metze, and Christoph Feichtenhofer.
\newblock Masked autoencoders that listen.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Jain et~al.(2017)Jain, Kar, et~al.]{jain2017non}
Prateek Jain, Purushottam Kar, et~al.
\newblock Non-convex optimization for machine learning.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  2017.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and
  Duerig]{jia2021scaling}
Chao Jia, Yinfei Yang, Ye~Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
  Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In \emph{ICML}. PMLR, 2021.

\bibitem[Jouppi et~al.(2020)Jouppi, Yoon, Kurian, Li, Patil, Laudon, Young, and
  Patterson]{jouppi2020domain}
Norman~P Jouppi, Doe~Hyun Yoon, George Kurian, Sheng Li, Nishant Patil, James
  Laudon, Cliff Young, and David Patterson.
\newblock A domain-specific supercomputer for training deep neural networks.
\newblock \emph{Communications of the ACM}, 2020.

\bibitem[Kay et~al.(2017)Kay, Carreira, Simonyan, Zhang, Hillier,
  Vijayanarasimhan, Viola, Green, Back, Natsev, et~al.]{kay2017kinetics}
Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra
  Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et~al.
\newblock The kinetics human action video dataset.
\newblock \emph{arXiv preprint arXiv:1705.06950}, 2017.

\bibitem[Kuehne et~al.(2011)Kuehne, Jhuang, Garrote, Poggio, and
  Serre]{kuehne2011hmdb}
Hildegard Kuehne, Hueihan Jhuang, Est{\'\i}baliz Garrote, Tomaso Poggio, and
  Thomas Serre.
\newblock Hmdb: a large video database for human motion recognition.
\newblock In \emph{ICCV}, 2011.

\bibitem[Likhosherstov et~al.(2021)Likhosherstov, Arnab, Choromanski, Lucic,
  Tay, Weller, and Dehghani]{likhosherstov2021polyvit}
Valerii Likhosherstov, Anurag Arnab, Krzysztof Choromanski, Mario Lucic,
  Yi~Tay, Adrian Weller, and Mostafa Dehghani.
\newblock Polyvit: Co-training vision transformers on images, videos and audio.
\newblock \emph{arXiv preprint arXiv:2111.12993}, 2021.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
  Doll{\'a}r, and Zitnick]{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{ECCV}, 2014.

\bibitem[Miech et~al.(2019)Miech, Zhukov, Alayrac, Tapaswi, Laptev, and
  Sivic]{miech2019howto100m}
Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan
  Laptev, and Josef Sivic.
\newblock Howto100m: Learning a text-video embedding by watching hundred
  million narrated video clips.
\newblock In \emph{ICCV}, 2019.

\bibitem[Mindermann et~al.(2022)Mindermann, Brauner, Razzak, Sharma, Kirsch,
  Xu, H{\"o}ltgen, Gomez, Morisot, Farquhar, et~al.]{mindermann2022prioritized}
S{\"o}ren Mindermann, Jan~M Brauner, Muhammed~T Razzak, Mrinank Sharma, Andreas
  Kirsch, Winnie Xu, Benedikt H{\"o}ltgen, Aidan~N Gomez, Adrien Morisot,
  Sebastian Farquhar, et~al.
\newblock Prioritized training on points that are learnable, worth learning,
  and not yet learnt.
\newblock In \emph{ICML}. PMLR, 2022.

\bibitem[Mitchell et~al.(2019)Mitchell, Wu, Zaldivar, Barnes, Vasserman,
  Hutchinson, Spitzer, Raji, and Gebru]{mitchell2019model}
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman,
  Ben Hutchinson, Elena Spitzer, Inioluwa~Deborah Raji, and Timnit Gebru.
\newblock Model cards for model reporting.
\newblock In \emph{Proceedings of the conference on fairness, accountability,
  and transparency}, 2019.

\bibitem[Mustafa et~al.(2022)Mustafa, Riquelme, Puigcerver, Jenatton, and
  Houlsby]{mustafa2022multimodal}
Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil
  Houlsby.
\newblock Multimodal contrastive learning with limoe: the language-image
  mixture of experts.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Nagrani et~al.(2022)Nagrani, Seo, Seybold, Hauth, Manen, Sun, and
  Schmid]{nagrani2022learning}
Arsha Nagrani, Paul~Hongsuck Seo, Bryan Seybold, Anja Hauth, Santiago Manen,
  Chen Sun, and Cordelia Schmid.
\newblock Learning audio-video modalities from image captions.
\newblock \emph{arXiv preprint arXiv:2204.00679}, 2022.

\bibitem[Ni et~al.(2022)Ni, Peng, Chen, Zhang, Meng, Fu, Xiang, and
  Ling]{ni2022expanding}
Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu,
  Shiming Xiang, and Haibin Ling.
\newblock Expanding language-image pretrained models for general video
  recognition.
\newblock In \emph{ECCV}, 2022.

\bibitem[Pascal et~al.(2021)Pascal, Michiardi, Bost, Huet, and
  Zuluaga]{pascal2021improved}
Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet, and Maria~A Zuluaga.
\newblock Improved optimization strategies for deep multi-task networks.
\newblock \emph{arXiv preprint arXiv:2109.11678}, 2021.

\bibitem[Pham et~al.(2021)Pham, Dai, Ghiasi, Kawaguchi, Liu, Yu, Yu, Chen,
  Luong, Wu, et~al.]{pham2021combined}
Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams~Wei
  Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et~al.
\newblock Combined scaling for open-vocabulary image classification.
\newblock \emph{arXiv preprint arXiv: 2111.10050}, 2021.

\bibitem[Piczak(2015)]{piczak2015esc}
Karol~J Piczak.
\newblock Esc: Dataset for environmental sound classification.
\newblock In \emph{ACM MM}, 2015.

\bibitem[Piergiovanni et~al.(2023)Piergiovanni, Kuo, Li, and
  Angelova]{piergiovanni2023dynamic}
AJ~Piergiovanni, Weicheng Kuo, Wei Li, and Anelia Angelova.
\newblock Dynamic pretraining of vision-language models, 2023.
\newblock URL \url{https://openreview.net/forum?id=QcffIcjq8bl}.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{ICML}. PMLR, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, Liu, et~al.]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, Peter~J Liu, et~al.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{JMLR}, 2020.

\bibitem[Ridnik et~al.(2021)Ridnik, Ben-Baruch, Noy, and
  Zelnik-Manor]{ridnik2021imagenet}
Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor.
\newblock Imagenet-21k pretraining for the masses.
\newblock \emph{arXiv preprint arXiv:2104.10972}, 2021.

\bibitem[Riquelme et~al.(2021)Riquelme, Puigcerver, Mustafa, Neumann, Jenatton,
  Susano~Pinto, Keysers, and Houlsby]{riquelme2021scaling}
Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe
  Jenatton, Andr{\'e} Susano~Pinto, Daniel Keysers, and Neil Houlsby.
\newblock Scaling vision with sparse mixture of experts.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Roberts et~al.(2022)Roberts, Chung, Levskaya, Mishra, Bradbury, Andor,
  Narang, Lester, Gaffney, Mohiuddin, et~al.]{roberts2022scaling}
Adam Roberts, Hyung~Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury,
  Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin,
  et~al.
\newblock Scaling up models and data with t5x and seqio.
\newblock \emph{arXiv preprint arXiv:2203.17189}, 2022.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
  Alexander~C. Berg, and Li~Fei-Fei.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{IJCV}, 2015.

\bibitem[Schuhmann et~al.(2021)Schuhmann, Vencu, Beaumont, Kaczmarczyk, Mullis,
  Katta, Coombes, Jitsev, and Komatsuzaki]{schuhmann2021laion}
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk,
  Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran
  Komatsuzaki.
\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text
  pairs.
\newblock \emph{arXiv preprint arXiv:2111.02114}, 2021.

\bibitem[Shen et~al.(2023)Shen, Yao, Li, Darrell, Keutzer, and
  He]{shen2023scaling}
Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, and Yuxiong
  He.
\newblock Scaling vision-language models with sparse mixture of experts.
\newblock \emph{arXiv preprint arXiv:2303.07226}, 2023.

\bibitem[Smith and Gasser(2005)]{smith2005development}
Linda Smith and Michael Gasser.
\newblock The development of embodied cognition: Six lessons from babies.
\newblock \emph{Artificial life}, 2005.

\bibitem[Soomro et~al.(2012)Soomro, Zamir, and Shah]{soomro2012ucf101}
Khurram Soomro, Amir~Roshan Zamir, and Mubarak Shah.
\newblock Ucf101: A dataset of 101 human actions classes from videos in the
  wild.
\newblock \emph{arXiv preprint arXiv:1212.0402}, 2012.

\bibitem[Srinivasan et~al.(2021)Srinivasan, Raman, Chen, Bendersky, and
  Najork]{srinivasan2021wit}
Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc
  Najork.
\newblock Wit: Wikipedia-based image text dataset for multimodal multilingual
  machine learning.
\newblock In \emph{SIGIR}, 2021.

\bibitem[Stroud et~al.(2020)Stroud, Lu, Sun, Deng, Sukthankar, Schmid, and
  Ross]{stroud2020learning}
Jonathan~C Stroud, Zhichao Lu, Chen Sun, Jia Deng, Rahul Sukthankar, Cordelia
  Schmid, and David~A Ross.
\newblock Learning video representations from textual web supervision.
\newblock \emph{arXiv preprint arXiv:2007.14937}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{NeurIPS}, 2017.

\bibitem[Wang et~al.(2022)Wang, Bao, Dong, Bjorck, Peng, Liu, Aggarwal,
  Mohammed, Singhal, Som, et~al.]{wang2022image}
Wenhui Wang, Hangbo Bao, Li~Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti
  Aggarwal, Owais~Khan Mohammed, Saksham Singhal, Subhojit Som, et~al.
\newblock Image as a foreign language: Beit pretraining for all vision and
  vision-language tasks.
\newblock \emph{arXiv preprint arXiv:2208.10442}, 2022.

\bibitem[Wu et~al.(2020)Wu, Girshick, He, Feichtenhofer, and
  Krahenbuhl]{wu2020multigrid}
Chao-Yuan Wu, Ross Girshick, Kaiming He, Christoph Feichtenhofer, and Philipp
  Krahenbuhl.
\newblock A multigrid method for efficiently training video models.
\newblock In \emph{CVPR}, 2020.

\bibitem[Wu et~al.(2022{\natexlab{a}})Wu, Liang, Han, Akbari, Wang, and
  Yu]{wu2022scaling}
Junru Wu, Yi~Liang, Feng Han, Hassan Akbari, Zhangyang Wang, and Cong Yu.
\newblock Scaling multimodal pre-training via cross-modality gradient
  harmonization.
\newblock In \emph{NeurIPS}, 2022{\natexlab{a}}.

\bibitem[Wu et~al.(2022{\natexlab{b}})Wu, Sun, and Ouyang]{wu2022transferring}
Wenhao Wu, Zhun Sun, and Wanli Ouyang.
\newblock Transferring textual knowledge for visual recognition.
\newblock \emph{arXiv preprint arXiv:2207.01297}, 2022{\natexlab{b}}.

\bibitem[Wu et~al.(2022{\natexlab{c}})Wu, Wang, Luo, Wang, Yang, and
  Ouyang]{wu2022bidirectional}
Wenhao Wu, Xiaohan Wang, Haipeng Luo, Jingdong Wang, Yi~Yang, and Wanli Ouyang.
\newblock Bidirectional cross-modal knowledge exploration for video recognition
  with pre-trained vision-language models.
\newblock \emph{arXiv preprint arXiv:2301.00182}, 2022{\natexlab{c}}.

\bibitem[Yan et~al.(2022)Yan, Zhu, Wang, Cao, Zhang, Ghosh, Wu, and
  Yu]{yan2022video}
Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi~Zhang, Soham Ghosh, Yonghui Wu, and
  Jiahui Yu.
\newblock Video-text modeling with zero-shot transfer from contrastive
  captioners.
\newblock \emph{arXiv preprint arXiv:2212.04979}, 2022.

\bibitem[Young et~al.(2014)Young, Lai, Hodosh, and Hockenmaier]{young2014image}
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.
\newblock From image descriptions to visual denotations: New similarity metrics
  for semantic inference over event descriptions.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  2014.

\bibitem[Yu et~al.(2022)Yu, Wang, Vasudevan, Yeung, Seyedhosseini, and
  Wu]{yu2022coca}
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and
  Yonghui Wu.
\newblock Coca: Contrastive captioners are image-text foundation models.
\newblock \emph{arXiv preprint arXiv:2205.01917}, 2022.

\bibitem[Zhai et~al.(2022)Zhai, Kolesnikov, Houlsby, and
  Beyer]{zhai2022scaling}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
\newblock Scaling vision transformers.
\newblock In \emph{CVPR}, 2022.

\bibitem[Zhou et~al.(2022)Zhou, Lei, Liu, Du, Huang, Zhao, Dai, Chen, Le, and
  Laudon]{zhou2022mixture}
Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew
  Dai, Zhifeng Chen, Quoc Le, and James Laudon.
\newblock Mixture-of-experts with expert choice routing.
\newblock In \emph{NeurIPS}, 2022.

\end{thebibliography}
