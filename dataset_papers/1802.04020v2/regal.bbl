\begin{thebibliography}{21}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi{-}Yadkori and
  Szepesv{\'{a}}ri(2015)]{Abbasi-Yadkori2015bayesian}
Yasin Abbasi{-}Yadkori and Csaba Szepesv{\'{a}}ri.
\newblock Bayesian optimal control of smoothly parameterized systems.
\newblock In \emph{{UAI}}, pages 1--11. {AUAI} Press, 2015.

\bibitem[Agrawal and Jia(2017)]{Agrawal2017posterior}
Shipra Agrawal and Randy Jia.
\newblock Optimistic posterior sampling for reinforcement learning: worst-case
  regret bounds.
\newblock In \emph{{NIPS}}, pages 1184--1194, 2017.

\bibitem[Audibert et~al.(2007)Audibert, Munos, and
  Szepesv{\'a}ri]{audibert2007tuning}
Jean-Yves Audibert, R{\'e}mi Munos, and Csaba Szepesv{\'a}ri.
\newblock Tuning bandit algorithms in stochastic environments.
\newblock In \emph{Algorithmic Learning Theory}, pages 150--165, Berlin,
  Heidelberg, 2007. Springer Berlin Heidelberg.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{pmlr-v70-azar17a}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In Doina Precup and Yee~Whye Teh, editors, \emph{Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of
  \emph{Proceedings of Machine Learning Research}, pages 263--272,
  International Convention Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem[Bartlett and Tewari(2009)]{Bartlett2009regal}
Peter~L. Bartlett and Ambuj Tewari.
\newblock {REGAL:} {A} regularization based algorithm for reinforcement
  learning in weakly communicating {MDP}s.
\newblock In \emph{{UAI}}, pages 35--42. {AUAI} Press, 2009.

\bibitem[Bertsekas(1995)]{bertsekas1995dynamic}
Dimitri~P Bertsekas.
\newblock \emph{Dynamic programming and optimal control. Vol II}.
\newblock Number~2. Athena scientific Belmont, MA, 1995.

\bibitem[Brafman and Tennenholtz(2003)]{Brafman:2003:RGP:944919.944928}
Ronen~I. Brafman and Moshe Tennenholtz.
\newblock R-max - a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{J. Mach. Learn. Res.}, 3:\penalty0 213--231, March 2003.
\newblock ISSN 1532-4435.

\bibitem[Dann and Brunskill(2015)]{DannBrunskill15}
Christoph Dann and Emma Brunskill.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In \emph{Proceedings of the 28th International Conference on Neural
  Information Processing Systems}, NIPS 15, pages 2818--2826. MIT Press, 2015.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{Jaksch10}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11:\penalty0 1563--1600,
  2010.

\bibitem[Knuth(1997)]{knuth1997artvol2}
Donald~E. Knuth.
\newblock \emph{The Art of Computer Programming, Volume 2 (3rd Ed.):
  Seminumerical Algorithms}.
\newblock Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1997.
\newblock ISBN 0-201-89684-2.

\bibitem[Lakshmanan et~al.(2015)Lakshmanan, Ortner, and
  Ryabko]{pmlr-v37-lakshmanan15}
K.~Lakshmanan, Ronald Ortner, and Daniil Ryabko.
\newblock Improved regret bounds for undiscounted continuous reinforcement
  learning.
\newblock In Francis Bach and David Blei, editors, \emph{Proceedings of the
  32nd International Conference on Machine Learning}, volume~37 of
  \emph{Proceedings of Machine Learning Research}, pages 524--532, Lille,
  France, 07--09 Jul 2015. PMLR.

\bibitem[Maurer and Pontil(2009)]{Maurer2009empirical}
Andreas Maurer and Massimiliano Pontil.
\newblock Empirical bernstein bounds and sample-variance penalization.
\newblock In \emph{{COLT}}, 2009.

\bibitem[Ortner(2008)]{DBLP:journals/mima/Ortner08}
Ronald Ortner.
\newblock Optimism in the face of uncertainty should be refutable.
\newblock \emph{Minds and Machines}, 18\penalty0 (4):\penalty0 521--526, 2008.

\bibitem[Ortner and Ryabko(2013)]{DBLP:journals/corr/abs-1302-2550}
Ronald Ortner and Daniil Ryabko.
\newblock Online regret bounds for undiscounted continuous reinforcement
  learning.
\newblock \emph{CoRR}, abs/1302.2550, 2013.

\bibitem[{Osband} and {Van Roy}(2016)]{Osband2016}
I.~{Osband} and B.~{Van Roy}.
\newblock {On Lower Bounds for Regret in Reinforcement Learning}.
\newblock \emph{ArXiv e-prints}, August 2016.

\bibitem[Osband and Roy(2017)]{Osband2017posterior}
Ian Osband and Benjamin~Van Roy.
\newblock Why is posterior sampling better than optimism for reinforcement
  learning?
\newblock In \emph{{ICML}}, volume~70 of \emph{Proceedings of Machine Learning
  Research}, pages 2701--2710. {PMLR}, 2017.

\bibitem[Osband et~al.(2013)Osband, Russo, and Roy]{Osband2013more}
Ian Osband, Daniel Russo, and Benjamin~Van Roy.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock In \emph{{NIPS}}, pages 3003--3011, 2013.

\bibitem[Ouyang et~al.(2017)Ouyang, Gagrani, Nayyar, and
  Jain]{Ouyang2017learning}
Yi~Ouyang, Mukul Gagrani, Ashutosh Nayyar, and Rahul Jain.
\newblock Learning unknown markov decision processes: {A} thompson sampling
  approach.
\newblock In \emph{{NIPS}}, pages 1333--1342, 2017.

\bibitem[Puterman(1994)]{puterman1994markov}
Martin~L. Puterman.
\newblock \emph{Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, Inc., New York, NY, USA, 1994.
\newblock ISBN 0471619779.

\bibitem[Seneta(1993)]{Seneta1993}
E.~Seneta.
\newblock Sensitivity of finite markov chains under perturbation.
\newblock \emph{Statistics \& Probability Letters}, 17\penalty0 (2):\penalty0
  163--168, May 1993.

\bibitem[Thompson(1933)]{thompson1933likelihood}
William~R. Thompson.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock \emph{Biometrika}, 25\penalty0 (3-4):\penalty0 285--294, 1933.

\end{thebibliography}
