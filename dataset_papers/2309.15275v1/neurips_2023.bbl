\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Liu et~al.(2022)Liu, Hu, Lin, Yao, Xie, Wei, Ning, Cao, Zhang, Dong,
  et~al.]{liu2022swin}
Ze~Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue
  Cao, Zheng Zhang, Li~Dong, et~al.
\newblock {Swin Transformer v2: Scaling up Capacity and Resolution}.
\newblock In \emph{IEEE conference on computer vision and pattern recognition},
  2022.

\bibitem[Kirillov et~al.(2023)Kirillov, Mintun, Ravi, Mao, Rolland, Gustafson,
  Xiao, Whitehead, Berg, Lo, et~al.]{kirillov2023segment}
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
  Gustafson, Tete Xiao, Spencer Whitehead, Alexander~C Berg, Wan-Yen Lo, et~al.
\newblock Segment anything.
\newblock \emph{arXiv preprint arXiv:2304.02643}, 2023.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International conference on machine learning}, 2021.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J{\'e}gou]{touvron2021training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In \emph{International conference on machine learning}, 2021.

\bibitem[Zhao et~al.(2022)Zhao, Zhang, Poggi, Tosi, Guo, Zhu, Huang, Tang, and
  Mattoccia]{zhao2022monovit}
Chaoqiang Zhao, Youmin Zhang, Matteo Poggi, Fabio Tosi, Xianda Guo, Zheng Zhu,
  Guan Huang, Yang Tang, and Stefano Mattoccia.
\newblock Monovit: Self-supervised monocular depth estimation with a vision
  transformer.
\newblock In \emph{International Conference on 3D Vision}, 2022.

\bibitem[Agarwal and Arora(2022)]{agarwal2022depthformer}
Ashutosh Agarwal and Chetan Arora.
\newblock Depthformer: Multiscale vision transformer for monocular depth
  estimation with local global information fusion.
\newblock \emph{arXiv preprint arXiv:2207.04535}, 2022.

\bibitem[Lin et~al.(2023)Lin, Lin, Lai, Lin, Shih, and
  Ramamoorthi]{lin2023vision}
Kai-En Lin, Yen-Chen Lin, Wei-Sheng Lai, Tsung-Yi Lin, Yi-Chang Shih, and Ravi
  Ramamoorthi.
\newblock Vision transformer for nerf-based view synthesis from a single input
  image.
\newblock In \emph{IEEE Winter Conference on Applications of Computer Vision},
  pages 806--815, 2023.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock {Communication-Efficient Learning of Deep Networks from Decentralized
  Data}.
\newblock In \emph{Artificial intelligence and statistics}, 2017.

\bibitem[Zhang et~al.(2021)Zhang, Xie, Bai, Yu, Li, and Gao]{zhang2021survey}
Chen Zhang, Yu~Xie, Hang Bai, Bin Yu, Weihong Li, and Yuan Gao.
\newblock {A Survey on Federated Learning}.
\newblock \emph{Knowledge-Based Systems}, 2021.

\bibitem[Chen et~al.(2021)Chen, Bhardwaj, and Marculescu]{chen2021fedmax}
Wei Chen, Kartikeya Bhardwaj, and Radu Marculescu.
\newblock {Fedmax: Mitigating Activation Divergence for Accurate and
  Communication-Efficient Federated Learning}.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases:
  European Conference, ECML PKDD 2020}, 2021.

\bibitem[Yang et~al.(2022)Yang, Xue, and Marculescu]{yang2022anytime}
Yuedong Yang, Zihui Xue, and Radu Marculescu.
\newblock {Anytime Depth Estimation with Limited Sensing and Computation
  Capabilities on Mobile Devices}.
\newblock In \emph{Conference on Robot Learning}, 2022.

\bibitem[Li et~al.(2021)Li, Mandal, Ogras, and Marculescu]{li2021flash}
Guihong Li, Sumit~K Mandal, Umit~Y Ogras, and Radu Marculescu.
\newblock {FLASH: Fast Neural Architecture Search with Hardware Optimization}.
\newblock \emph{ACM Transactions on Embedded Computing Systems}, 2021.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{Goodfellow-et-al-2016}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock \emph{Deep Learning}.
\newblock MIT Press, 2016.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen]{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Ryser(1963)]{ryser_1963}
Herbert~John Ryser.
\newblock \emph{Combinatorial Mathematics}.
\newblock Carus Mathematical Monographs. Mathematical Association of America,
  1963.
\newblock \doi{10.5948/UPO9781614440147}.

\bibitem[Shanks(1969)]{1671278}
J.L. Shanks.
\newblock Computation of the fast walsh-fourier transform.
\newblock \emph{IEEE Transactions on Computers}, C-18\penalty0 (5):\penalty0
  457--459, 1969.
\newblock \doi{10.1109/T-C.1969.222685}.

\bibitem[Gonzales and Wintz(1987)]{10.5555/22881}
Rafael~C. Gonzales and Paul Wintz.
\newblock \emph{Digital Image Processing (2nd Ed.)}.
\newblock Addison-Wesley Longman Publishing Co., Inc., USA, 1987.
\newblock ISBN 0201110261.

\bibitem[Yang et~al.(2023)Yang, Li, and Marculescu]{yang2023efficient}
Yuedong Yang, Guihong Li, and Radu Marculescu.
\newblock {Efficient On-device Training via Gradient Filtering}.
\newblock In \emph{IEEE conference on computer vision and pattern recognition},
  2023.

\bibitem[Parseval(1806)]{parseval1806memoire}
Marc-Antoine Parseval.
\newblock M{\'e}moire sur les s{\'e}ries et sur l’int{\'e}gration
  compl{\`e}te d’une {\'e}quation aux diff{\'e}rences partielles
  lin{\'e}aires du second ordre, {\`a} coefficients constants.
\newblock \emph{M{\'e}m. pr{\'e}s. par divers savants, Acad. des Sciences,
  Paris,(1)}, 1:\penalty0 638--648, 1806.

\bibitem[Cai et~al.(2020)Cai, Gan, Zhu, and Han]{cai2020tinytl}
Han Cai, Chuang Gan, Ligeng Zhu, and Song Han.
\newblock {Tinytl: Reduce Memory, not Parameters for Efficient On-device
  Learning}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
  Alexander~C. Berg, and Li~Fei-Fei.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision}, 2015.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Krause et~al.(2013)Krause, Stark, Deng, and Fei-Fei]{krause20133d}
Jonathan Krause, Michael Stark, Jia Deng, and Li~Fei-Fei.
\newblock {3D Object Representations for Fine-grained Categorization}.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision workshops}, 2013.

\bibitem[Nilsback and Zisserman(2006)]{Nilsback06}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock {A Visual Vocabulary for Flower Classification}.
\newblock In \emph{IEEE conference on computer vision and pattern recognition},
  2006.

\bibitem[Bossard et~al.(2014)Bossard, Guillaumin, and Van~Gool]{bossard14}
Lukas Bossard, Matthieu Guillaumin, and Luc Van~Gool.
\newblock {Food-101 -- Mining Discriminative Components with Random Forests}.
\newblock In \emph{European Conference on Computer Vision}, 2014.

\bibitem[Parkhi et~al.(2012)Parkhi, Vedaldi, Zisserman, and Jawahar]{parkhi12a}
Omkar~M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C.~V. Jawahar.
\newblock {Cats and Dogs}.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  2012.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock {Decoupled Weight Decay Regularization}.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Yuan, Wen, Hu, Evangelidis, Tulyakov,
  Wang, and Ren]{li2022efficientformer}
Yanyu Li, Geng Yuan, Yang Wen, Ju~Hu, Georgios Evangelidis, Sergey Tulyakov,
  Yanzhi Wang, and Jian Ren.
\newblock Efficientformer: Vision transformers at mobilenet speed,
  2022{\natexlab{a}}.

\bibitem[Zhou et~al.(2017)Zhou, Zhao, Puig, Fidler, Barriuso, and
  Torralba]{zhou2017scene}
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio
  Torralba.
\newblock Scene parsing through ade20k dataset.
\newblock In \emph{IEEE conference on computer vision and pattern recognition},
  pages 633--641, 2017.

\bibitem[Xie et~al.(2021)Xie, Wang, Yu, Anandkumar, Alvarez, and
  Luo]{xie2021segformer}
Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose~M Alvarez, and Ping
  Luo.
\newblock Segformer: Simple and efficient design for semantic segmentation with
  transformers.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Cordts et~al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
  Franke, Roth, and Schiele]{Cordts2016Cityscapes}
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,
  Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.
\newblock The cityscapes dataset for semantic urban scene understanding.
\newblock In \emph{IEEE conference on computer vision and pattern recognition},
  pages 3213--3223, 2016.

\bibitem[Chen et~al.(2017)Chen, Papandreou, Schroff, and
  Adam]{chen2017rethinking}
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam.
\newblock Rethinking atrous convolution for semantic image segmentation.
\newblock \emph{arXiv preprint arXiv:1706.05587}, 2017.

\bibitem[Lin et~al.(2022)Lin, Zhu, Chen, Wang, Gan, and Han]{lin2022device}
Ji~Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and Song Han.
\newblock {On-device Training under 256kb Memory}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Guo et~al.(2019)Guo, Shi, Kumar, Grauman, Rosing, and
  Feris]{guo2019spottune}
Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, and
  Rogerio Feris.
\newblock {Spottune: Transfer Learning through Adaptive Fine-tuning}.
\newblock In \emph{IEEE conference on computer vision and pattern recognition},
  2019.

\bibitem[Long et~al.(2015)Long, Cao, Wang, and Jordan]{long2015learning}
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan.
\newblock {Learning Transferable Features with Deep Adaptation Networks}.
\newblock In \emph{International conference on machine learning}, 2015.

\bibitem[Yosinski et~al.(2014)Yosinski, Clune, Bengio, and
  Lipson]{yosinski2014transferable}
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
\newblock {How Transferable are Features in Deep Neural Networks?}
\newblock In \emph{Advances in neural information processing systems}, 2014.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Hu, Wen, Evangelidis, Salahi, Wang,
  Tulyakov, and Ren]{li2022rethinking}
Yanyu Li, Ju~Hu, Yang Wen, Georgios Evangelidis, Kamyar Salahi, Yanzhi Wang,
  Sergey Tulyakov, and Jian Ren.
\newblock {Rethinking Vision Transformers for MobileNet Size and Speed}.
\newblock \emph{arXiv preprint arXiv:2212.08059}, 2022{\natexlab{b}}.

\bibitem[Chen et~al.(2020)Chen, Gai, Yao, Mahoney, and
  Gonzalez]{chen2020statistical}
Jianfei Chen, Yu~Gai, Zhewei Yao, Michael~W Mahoney, and Joseph~E Gonzalez.
\newblock {A Statistical Framework for Low-bitwidth Training of Deep Neural
  Networks}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Banner et~al.(2018)Banner, Hubara, Hoffer, and
  Soudry]{banner2018scalable}
Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry.
\newblock Scalable methods for 8-bit training of neural networks.
\newblock In \emph{Advances in neural information processing systems}, 2018.

\bibitem[Sun et~al.(2020)Sun, Wang, Chen, Ni, Agrawal, Cui, Venkataramani,
  El~Maghraoui, Srinivasan, and Gopalakrishnan]{sun2020ultra}
Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui,
  Swagath Venkataramani, Kaoutar El~Maghraoui, Vijayalakshmi~(Viji) Srinivasan,
  and Kailash Gopalakrishnan.
\newblock {Ultra-Low Precision 4-bit Training of Deep Neural Networks}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Hubara et~al.(2017)Hubara, Courbariaux, Soudry, El-Yaniv, and
  Bengio]{hubara2017quantized}
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
  Bengio.
\newblock Quantized neural networks: Training neural networks with low
  precision weights and activations.
\newblock \emph{The Journal of Machine Learning Research}, 2017.

\bibitem[Zhao et~al.(2021)Zhao, Huang, Pan, Li, Zhang, Gu, and
  Xu]{zhao2021distribution}
Kang Zhao, Sida Huang, Pan Pan, Yinghan Li, Yingya Zhang, Zhenyu Gu, and
  Yinghui Xu.
\newblock {Distribution Adaptive INT8 Quantization for Training CNNs}.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2021.

\bibitem[Hong and Yue(2022)]{hong2022efficient}
Ziyang Hong and C~Patrick Yue.
\newblock Efficient-grad: Efficient training deep convolutional neural networks
  on edge devices with grad ient optimizations.
\newblock \emph{ACM Transactions on Embedded Computing Systems}, 2022.

\bibitem[Sung et~al.(2022)Sung, Cho, and Bansal]{sung2022lst}
Yi-Lin Sung, Jaemin Cho, and Mohit Bansal.
\newblock {Lst: Ladder Side-tuning for Parameter and Memory Efficient Transfer
  Learning}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\end{thebibliography}
