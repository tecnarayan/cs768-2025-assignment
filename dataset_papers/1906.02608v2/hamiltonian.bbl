\begin{thebibliography}{10}

\bibitem{hamilton1834general}
Sir William~Rowan Hamilton.
\newblock On a general method in dynamics.
\newblock {\em Philosophical Transactions of the Royal Society}, 2:247--308,
  1834.

\bibitem{peypouquet2009evolution}
Juan Peypouquet and Sylvain Sorin.
\newblock Evolution equations for maximal monotone operators: {A}symptotic
  analysis in continuous and discrete time.
\newblock {\em arXiv preprint arXiv:0905.1270}, 2009.

\bibitem{bianchi2017constant}
Pascal Bianchi, Walid Hachem, and Adil Salim.
\newblock A constant step forward-backward algorithm involving random maximal
  monotone operators.
\newblock {\em arXiv preprint arXiv:1702.04144}, 2017.

\bibitem{condat2013primal}
Laurent Condat.
\newblock A primal--dual splitting method for convex optimization involving
  {L}ipschitzian, proximable and linear composite terms.
\newblock {\em Journal of Optimization Theory and Applications},
  158(2):460--479, 2013.

\bibitem{su2016differential}
Weijie Su, Stephen Boyd, and Emmanuel~J Cand{\`e}s.
\newblock A differential equation for modeling {N}esterov's accelerated
  gradient method: theory and insights.
\newblock {\em Journal of Machine Learning Research}, 17(1):5312--5354, 2016.

\bibitem{nesterov1983method}
Yurii Nesterov.
\newblock A method of solving a convex programming problem with convergence
  rate $o(1/k^2)$.
\newblock {\em Soviet Mathematics Doklady}, 27(2):372--376, 1983.

\bibitem{attouch2019rate}
Hedy Attouch, Zaki Chbani, and Hassan Riahi.
\newblock Rate of convergence of the {N}esterov accelerated gradient method in
  the subcritical case $\alpha \leq 3$.
\newblock {\em ESAIM: Control, Optimisation and Calculus of Variations}, 25:2,
  2019.

\bibitem{wibisono2016variational}
Andre Wibisono, Ashia~C Wilson, and Michael~I Jordan.
\newblock A variational perspective on accelerated methods in optimization.
\newblock {\em Proceedings of the National Academy of Sciences},
  113(47):E7351--E7358, 2016.

\bibitem{wilson2019accelerating}
Ashia Wilson, Lester Mackey, and Andre Wibisono.
\newblock Accelerating rescaled gradient descent.
\newblock {\em arXiv preprint arXiv:1902.08825}, 2019.

\bibitem{wilson2016lyapunov}
Ashia~C Wilson, Benjamin Recht, and Michael~I Jordan.
\newblock A {L}yapunov analysis of momentum methods in optimization.
\newblock {\em arXiv preprint arXiv:1611.02635}, 2016.

\bibitem{betancourt2018symplectic}
Michael Betancourt, Michael~I Jordan, and Ashia~C Wilson.
\newblock On symplectic optimization.
\newblock {\em arXiv preprint arXiv:1802.03653}, 2018.

\bibitem{maddison2018hamiltonian}
Chris~J Maddison, Daniel Paulin, Yee~Whye Teh, Brendan O'Donoghue, and Arnaud
  Doucet.
\newblock Hamiltonian descent methods.
\newblock {\em arXiv preprint arXiv:1809.05042}, 2018.

\bibitem{francca2019conformal}
Guilherme Fran{\c{c}}a, Jeremias Sulam, Daniel~P Robinson, and Ren{\'e} Vidal.
\newblock Conformal symplectic and relativistic optimization.
\newblock {\em arXiv preprint arXiv:1903.04100}, 2019.

\bibitem{rockafellar1973saddle}
RT~Rockafellar.
\newblock Saddle points of {H}amiltonian systems in convex problems of
  lagrange.
\newblock {\em Journal of Optimization Theory and Applications},
  12(4):367--390, 1973.

\bibitem{neal2011mcmc}
Radford~M Neal.
\newblock {MCMC} using {H}amiltonian dynamics.
\newblock {\em Handbook of Markov Chain Monte Carlo}, pages 113--162, 2011.

\bibitem{balduzzi2018mechanics}
David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls,
  and Thore Graepel.
\newblock The mechanics of n-player differentiable games.
\newblock In {\em International Conference on Machine Learning}, pages
  363--372, 2018.

\bibitem{rockafellar1970convex}
Ralph~Tyrell Rockafellar.
\newblock {\em Convex analysis}.
\newblock Princeton university press, 1970.

\bibitem{gronwall1919note}
Thomas~Hakon Gronwall.
\newblock Note on the derivatives with respect to a parameter of the solutions
  of a system of differential equations.
\newblock {\em Annals of Mathematics}, pages 292--296, 1919.

\bibitem{slotine1991applied}
Jean-Jacques~E Slotine, Weiping Li, et~al.
\newblock {\em Applied nonlinear control}, volume 199.
\newblock Prentice hall Englewood Cliffs, NJ, 1991.

\bibitem{hairer2006geometric}
Ernst Hairer, Christian Lubich, and Gerhard Wanner.
\newblock {\em Geometric numerical integration: structure-preserving algorithms
  for ordinary differential equations}, volume~31.
\newblock Springer Science \& Business Media, 2006.

\bibitem{nesterov2013gradient}
Yu~Nesterov.
\newblock Gradient methods for minimizing composite functions.
\newblock {\em Mathematical Programming}, 140(1):125--161, 2013.

\bibitem{boyd2004convex}
Stephen Boyd and Lieven Vandenberghe.
\newblock {\em Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem{bregman1967relaxation}
Lev~M Bregman.
\newblock The relaxation method of finding the common point of convex sets and
  its application to the solution of problems in convex programming.
\newblock {\em USSR computational mathematics and mathematical physics},
  7(3):200--217, 1967.

\bibitem{bauschke1997legendre}
Heinz~H Bauschke and Jonathan~M Borwein.
\newblock Legendre functions and the method of random bregman projections.
\newblock {\em Journal of Convex Analysis}, 4(1):27--67, 1997.

\bibitem{boyd2011distributed}
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et~al.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock {\em Foundations and Trends{\textregistered} in Machine learning},
  3(1):1--122, 2011.

\bibitem{heyuan2012}
Bingsheng He and Xiaoming Yuan.
\newblock On the $o(1/n)$ convergence rate of the {D}ouglas--{R}achford
  alternating direction method.
\newblock {\em SIAM Journal on Numerical Analysis}, 50(2):700--709, 2012.

\bibitem{franca2018admm}
Guilherme Franca, Daniel Robinson, and Rene Vidal.
\newblock Admm and accelerated admm as continuous dynamical systems.
\newblock In {\em International Conference on Machine Learning}, pages
  1554--1562, 2018.

\bibitem{goldstein2014fast}
Tom Goldstein, Brendan O'Donoghue, Simon Setzer, and Richard Baraniuk.
\newblock Fast alternating direction optimization methods.
\newblock {\em SIAM Journal on Imaging Sciences}, 7(3):1588--1623, 2014.

\bibitem{zhu2008efficient}
Mingqiang Zhu and Tony Chan.
\newblock An efficient primal-dual hybrid gradient algorithm for total
  variation image restoration.
\newblock {\em {UCLA CAM} report}, 34, 2008.

\bibitem{esser2009general}
Ernie Esser, Xiaoqun Zhang, and Tony Chan.
\newblock A general framework for a class of first order primal-dual algorithms
  for tv minimization.
\newblock 2009.

\bibitem{chambolle2011first}
Antonin Chambolle and Thomas Pock.
\newblock A first-order primal-dual algorithm for convex problems with
  applications to imaging.
\newblock {\em Journal of mathematical imaging and vision}, 40(1):120--145,
  2011.

\bibitem{parikh2014proximal}
Neal Parikh, Stephen Boyd, et~al.
\newblock Proximal algorithms.
\newblock {\em Foundations and Trends{\textregistered} in Optimization},
  1(3):127--239, 2014.

\bibitem{o2015adaptive}
Brendan Oâ€™Donoghue and Emmanuel Candes.
\newblock Adaptive restart for accelerated gradient schemes.
\newblock {\em Foundations of computational mathematics}, 15(3):715--732, 2015.

\bibitem{zou2005regularization}
Hui Zou and Trevor Hastie.
\newblock Regularization and variable selection via the elastic net.
\newblock {\em Journal of the royal statistical society: series B (statistical
  methodology)}, 67(2):301--320, 2005.

\bibitem{ocpb:16}
B.~O'Donoghue, E.~Chu, N.~Parikh, and S.~Boyd.
\newblock Conic optimization via operator splitting and homogeneous self-dual
  embedding.
\newblock {\em Journal of Optimization Theory and Applications},
  169(3):1042--1068, June 2016.

\bibitem{scs}
B.~O'Donoghue, E.~Chu, N.~Parikh, and S.~Boyd.
\newblock {SCS}: Splitting conic solver, version 2.1.0.
\newblock \url{https://github.com/cvxgrp/scs}, November 2017.

\end{thebibliography}
