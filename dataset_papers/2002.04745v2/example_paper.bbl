\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Al-Rfou et~al.(2018)Al-Rfou, Choe, Constant, Guo, and
  Jones]{al2018character}
Al-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones, L.
\newblock Character-level language modeling with deeper self-attention.
\newblock \emph{arXiv preprint arXiv:1808.04444}, 2018.

\bibitem[Baevski \& Auli(2018)Baevski and Auli]{baevski2018adaptive}
Baevski, A. and Auli, M.
\newblock Adaptive input representations for neural language modeling.
\newblock \emph{arXiv preprint arXiv:1809.10853}, 2018.

\bibitem[Bahdanau et~al.(2017)Bahdanau, Cho, and Bengio]{bahdanau2017neural}
Bahdanau, D., Cho, K., and Bengio, Y.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock 2017.

\bibitem[Bentivogli et~al.(2009)Bentivogli, Dagan, Dang, Giampiccolo, and
  Magnini]{bentivogli2009fifth}
Bentivogli, L., Dagan, I., Dang, H.~T., Giampiccolo, D., and Magnini, B.
\newblock The fifth {PASCAL} recognizing textual entailment challenge.
\newblock 2009.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Cohen, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Dai, Z., Yang, Z., Yang, Y., Cohen, W.~W., Carbonell, J., Le, Q.~V., and
  Salakhutdinov, R.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock \emph{arXiv preprint arXiv:1901.02860}, 2019.

\bibitem[Dauphin et~al.(2017)Dauphin, Fan, Auli, and
  Grangier]{dauphin2017language}
Dauphin, Y.~N., Fan, A., Auli, M., and Grangier, D.
\newblock Language modeling with gated convolutional networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  933--941, 2017.

\bibitem[Dehghani et~al.(2018)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser]{dehghani2018universal}
Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, {\L}.
\newblock Universal transformers.
\newblock \emph{arXiv preprint arXiv:1807.03819}, 2018.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dolan \& Brockett(2005)Dolan and Brockett]{dolan2005automatically}
Dolan, W.~B. and Brockett, C.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \emph{Proceedings of the International Workshop on Paraphrasing.},
  2005.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Edunov et~al.(2018)Edunov, Ott, Auli, and
  Grangier]{edunov2018understanding}
Edunov, S., Ott, M., Auli, M., and Grangier, D.
\newblock Understanding back-translation at scale.
\newblock \emph{arXiv preprint arXiv:1808.09381}, 2018.

\bibitem[Gehring et~al.(2017)Gehring, Auli, Grangier, Yarats, and
  Dauphin]{gehring2017convolutional}
Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y.~N.
\newblock Convolutional sequence to sequence learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1243--1252, 2017.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pp.\  249--256, 2010.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Goyal, P., Doll{\'a}r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola,
  A., Tulloch, A., Jia, Y., and He, K.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2017)He, Gkioxari, Doll{\'a}r, and Girshick]{he2017mask}
He, K., Gkioxari, G., Doll{\'a}r, P., and Girshick, R.
\newblock Mask r-cnn.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  2961--2969, 2017.

\bibitem[He et~al.(2019)He, Zhang, Zhang, Zhang, Xie, and Li]{he2019bag}
He, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.
\newblock Bag of tricks for image classification with convolutional neural
  networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  558--567, 2019.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Klein et~al.(2018)Klein, Kim, Deng, Nguyen, Senellart, and
  Rush]{klein2018opennmt}
Klein, G., Kim, Y., Deng, Y., Nguyen, V., Senellart, J., and Rush, A.
\newblock Opennmt: Neural machine translation toolkit.
\newblock In \emph{Proceedings of the 13th Conference of the Association for
  Machine Translation in the Americas (Volume 1: Research Papers)}, volume~1,
  pp.\  177--184, 2018.

\bibitem[Koehn et~al.(2007)Koehn, Hoang, Birch, Callison-Burch, Federico,
  Bertoldi, Cowan, Shen, Moran, Zens, et~al.]{koehn2007moses}
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi,
  N., Cowan, B., Shen, W., Moran, C., Zens, R., et~al.
\newblock Moses: Open source toolkit for statistical machine translation.
\newblock In \emph{Proceedings of the 45th annual meeting of the association
  for computational linguistics companion volume proceedings of the demo and
  poster sessions}, pp.\  177--180, 2007.

\bibitem[Lee et~al.(2017)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl-Dickstein]{lee2017deep}
Lee, J., Bahri, Y., Novak, R., Schoenholz, S.~S., Pennington, J., and
  Sohl-Dickstein, J.
\newblock Deep neural networks as gaussian processes.
\newblock \emph{arXiv preprint arXiv:1711.00165}, 2017.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Sohl-Dickstein, and
  Pennington]{lee2019wide}
Lee, J., Xiao, L., Schoenholz, S.~S., Bahri, Y., Sohl-Dickstein, J., and
  Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{arXiv preprint arXiv:1902.06720}, 2019.

\bibitem[Lei~Ba et~al.(2016)Lei~Ba, Kiros, and Hinton]{lei2016layer}
Lei~Ba, J., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Liu et~al.(2019{\natexlab{a}})Liu, Jiang, He, Chen, Liu, Gao, and
  Han]{liu2019variance}
Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock \emph{arXiv preprint arXiv:1908.03265}, 2019{\natexlab{a}}.

\bibitem[Liu et~al.(2019{\natexlab{b}})Liu, Ott, Goyal, Du, Joshi, Chen, Levy,
  Lewis, Zettlemoyer, and Stoyanov]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019{\natexlab{b}}.

\bibitem[Lu et~al.(2019)Lu, Li, He, Sun, Dong, Qin, Wang, and
  Liu]{lu2019understanding}
Lu, Y., Li, Z., He, D., Sun, Z., Dong, B., Qin, T., Wang, L., and Liu, T.-Y.
\newblock Understanding and improving transformer from a multi-particle dynamic
  system point of view.
\newblock \emph{arXiv preprint arXiv:1906.02762}, 2019.

\bibitem[Nguyen \& Salazar(2019)Nguyen and Salazar]{nguyen2019transformers}
Nguyen, T.~Q. and Salazar, J.
\newblock Transformers without tears: Improving the normalization of
  self-attention.
\newblock \emph{arXiv preprint arXiv:1910.05895}, 2019.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu]{papineni2002bleu}
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th annual meeting on association for
  computational linguistics}, pp.\  311--318. Association for Computational
  Linguistics, 2002.

\bibitem[Popel \& Bojar(2018)Popel and Bojar]{popel2018training}
Popel, M. and Bojar, O.
\newblock Training tips for the transformer model.
\newblock \emph{The Prague Bulletin of Mathematical Linguistics}, 110\penalty0
  (1):\penalty0 43--70, 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Ruder(2016)]{ruder2016overview}
Ruder, S.
\newblock An overview of gradient descent optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1609.04747}, 2016.

\bibitem[Sennrich et~al.(2015)Sennrich, Haddow, and Birch]{sennrich2015neural}
Sennrich, R., Haddow, B., and Birch, A.
\newblock Neural machine translation of rare words with subword units.
\newblock \emph{arXiv preprint arXiv:1508.07909}, 2015.

\bibitem[Sennrich et~al.(2016)Sennrich, Haddow, and Birch]{BPE}
Sennrich, R., Haddow, B., and Birch, A.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{ACL}, 2016.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and
  Le]{sutskever2014sequence}
Sutskever, I., Vinyals, O., and Le, Q.~V.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3104--3112, 2014.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  2818--2826, 2016.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{tieleman2012lecture}
Tieleman, T. and Hinton, G.
\newblock Lecture 6.5-rmsprop, coursera: Neural networks for machine learning.
\newblock \emph{University of Toronto, Technical Report}, 2012.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5998--6008, 2017.

\bibitem[Vaswani et~al.(2018)Vaswani, Bengio, Brevdo, Chollet, Gomez, Gouws,
  Jones, Kaiser, Kalchbrenner, Parmar, Sepassi, Shazeer, and
  Uszkoreit]{tensor2tensor}
Vaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez, A.~N., Gouws, S.,
  Jones, L., Kaiser, L., Kalchbrenner, N., Parmar, N., Sepassi, R., Shazeer,
  N., and Uszkoreit, J.
\newblock Tensor2tensor for neural machine translation.
\newblock \emph{CoRR}, abs/1803.07416, 2018.
\newblock URL \url{http://arxiv.org/abs/1803.07416}.

\bibitem[Wainwright(2019)]{wainwright2019high}
Wainwright, M.~J.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wang et~al.(2019)Wang, Li, Xiao, Zhu, Li, Wong, and
  Chao]{wang2019learning}
Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D.~F., and Chao, L.~S.
\newblock Learning deep transformer models for machine translation.
\newblock \emph{arXiv preprint arXiv:1906.01787}, 2019.

\bibitem[Xiao et~al.(2018)Xiao, Bahri, Sohl-Dickstein, Schoenholz, and
  Pennington]{xiao2018dynamical}
Xiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S., and Pennington, J.
\newblock Dynamical isometry and a mean field theory of cnns: How to train
  10,000-layer vanilla convolutional neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5389--5398, 2018.

\bibitem[Yang(2019)]{yang2019scaling}
Yang, G.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian
  process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock \emph{arXiv preprint arXiv:1902.04760}, 2019.

\bibitem[Yang et~al.(2019{\natexlab{a}})Yang, Pennington, Rao, Sohl-Dickstein,
  and Schoenholz]{yang2019mean}
Yang, G., Pennington, J., Rao, V., Sohl-Dickstein, J., and Schoenholz, S.~S.
\newblock A mean field theory of batch normalization.
\newblock \emph{arXiv preprint arXiv:1902.08129}, 2019{\natexlab{a}}.

\bibitem[Yang et~al.(2019{\natexlab{b}})Yang, Dai, Yang, Carbonell,
  Salakhutdinov, and Le]{yang2019xlnet}
Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., and Le, Q.~V.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1906.08237}, 2019{\natexlab{b}}.

\bibitem[You et~al.(2018)You, Zhang, Hsieh, Demmel, and
  Keutzer]{you2018imagenet}
You, Y., Zhang, Z., Hsieh, C.-J., Demmel, J., and Keutzer, K.
\newblock Imagenet training in minutes.
\newblock In \emph{Proceedings of the 47th International Conference on Parallel
  Processing}, pp.\ ~1. ACM, 2018.

\bibitem[Zeiler(2012)]{zeiler2012adadelta}
Zeiler, M.~D.
\newblock Adadelta: an adaptive learning rate method.
\newblock \emph{arXiv preprint arXiv:1212.5701}, 2012.

\bibitem[Zhang et~al.(2019)Zhang, Dauphin, and Ma]{zhang2019fixup}
Zhang, H., Dauphin, Y.~N., and Ma, T.
\newblock Fixup initialization: Residual learning without normalization.
\newblock \emph{arXiv preprint arXiv:1901.09321}, 2019.

\bibitem[Zhu et~al.(2015)Zhu, Kiros, Zemel, Salakhutdinov, Urtasun, Torralba,
  and Fidler]{moviebook}
Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A.,
  and Fidler, S.
\newblock Aligning books and movies: Towards story-like visual explanations by
  watching movies and reading books.
\newblock In \emph{arXiv preprint arXiv:1506.06724}, 2015.

\end{thebibliography}
