\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{agarwal2021theory}
Agarwal, A., Kakade, S.M., Lee, J.D., Mahajan, G.: On the theory of policy
  gradient methods: Optimality, approximation, and distribution shift. Journal
  of Machine Learning Research  \textbf{22}(1),  4431--4506 (2021)

\bibitem{agarwal2021communication}
Agarwal, M., Ganguly, B., Aggarwal, V.: Communication efficient parallel
  reinforcement learning. In: Uncertainty in Artificial Intelligence (2021)

\bibitem{al2019deeppool}
Al-Abbasi, A.O., Ghosh, A., Aggarwal, V.: {DeepPool}: Distributed model-free
  algorithm for ride-sharing using deep reinforcement learning. IEEE
  Transactions on Intelligent Transportation Systems  \textbf{20}(12),
  4714--4727 (2019)

\bibitem{boyd2011distributed}
Boyd, S., Parikh, N., Chu, E., Peleato, B., Eckstein, J., et~al.: Distributed
  optimization and statistical learning via the alternating direction method of
  multipliers. Foundations and Trends{\textregistered} in Machine learning
  \textbf{3}(1),  1--122 (2011)

\bibitem{chen2018bandit}
Chen, T., Giannakis, G.B.: Bandit convex optimization for scalable and dynamic
  iot management. IEEE Internet of Things Journal  \textbf{6}(1),  1276--1286
  (2018)

\bibitem{NEURIPS2020_5f7695de}
Ding, D., Zhang, K., Basar, T., Jovanovic, M.: Natural policy gradient
  primal-dual method for constrained {Markov} decision processes. In: Advances
  in Neural Information Processing Systems (2020)

\bibitem{elgabli2022fednew}
Elgabli, A., Issaid, C.B., Bedi, A.S., Rajawat, K., Bennis, M., Aggarwal, V.:
  {FedNew}: A communication-efficient and privacy-preserving {Newton}-type
  method for federated learning. In: International Conference on Machine
  Learning (2022)

\bibitem{elgabli2020gadmm}
Elgabli, A., Park, J., Bedi, A.S., Bennis, M., Aggarwal, V.: {GADMM}: Fast and
  communication efficient framework for distributed machine learning. Journal
  of Machine Learning Research  \textbf{21}(76),  1--39 (2020)

\bibitem{elgabli2020q}
Elgabli, A., Park, J., Bedi, A.S., Issaid, C.B., Bennis, M., Aggarwal, V.:
  {Q-GADMM}: Quantized group {ADMM} for communication efficient decentralized
  machine learning. IEEE Transactions on Communications  \textbf{69}(1),
  164--181 (2020)

\bibitem{Engstrom2020Implementation}
Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L.,
  Madry, A.: Implementation matters in deep {RL}: A case study on {PPO} and
  {TRPO}. In: International Conference on Learning Representations (2020)

\bibitem{haliem2021distributed}
Haliem, M., Mani, G., Aggarwal, V., Bhargava, B.: A distributed model-free
  ride-sharing approach for joint matching, pricing, and dispatching using deep
  reinforcement learning. IEEE Transactions on Intelligent Transportation
  Systems  \textbf{22}(12),  7931--7942 (2021)

\bibitem{9311906}
Hosseinalipour, S., Brinton, C.G., Aggarwal, V., Dai, H., Chiang, M.: From
  federated to fog learning: Distributed machine learning over heterogeneous
  wireless networks. IEEE Communications Magazine  \textbf{58}(12),  41--47
  (2020)

\bibitem{pmlr-v139-islamov21a}
Islamov, R., Qian, X., Richtarik, P.: Distributed second order methods with
  fast rates and compressed communication. In: Proceedings of the 38th
  International Conference on Machine Learning (2021)

\bibitem{jin2022federated}
Jin, H., Peng, Y., Yang, W., Wang, S., Zhang, Z.: Federated reinforcement
  learning with environment heterogeneity. In: International Conference on
  Artificial Intelligence and Statistics (2022)

\bibitem{MAL-083}
Kairouz, P., McMahan, H.B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.N.,
  Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., D’Oliveira, R.G.L.,
  Eichner, H., Rouayheb, S.E., Evans, D., Gardner, J., Garrett, Z., Gascón,
  A., Ghazi, B., Gibbons, P.B., Gruteser, M., Harchaoui, Z., He, C., He, L.,
  Huo, Z., Hutchinson, B., Hsu, J., Jaggi, M., Javidi, T., Joshi, G., Khodak,
  M., Konecný, J., Korolova, A., Koushanfar, F., Koyejo, S., Lepoint, T., Liu,
  Y., Mittal, P., Mohri, M., Nock, R., Özgür, A., Pagh, R., Qi, H., Ramage,
  D., Raskar, R., Raykova, M., Song, D., Song, W., Stich, S.U., Sun, Z.,
  Suresh, A.T., Tramèr, F., Vepakomma, P., Wang, J., Xiong, L., Xu, Z., Yang,
  Q., Yu, F.X., Yu, H., Zhao, S.: Advances and open problems in federated
  learning. Foundations and Trends{®} in Machine Learning  \textbf{14}(1–2),
   1--210 (2021)

\bibitem{kakade2001natural}
Kakade, S.M.: A natural policy gradient. In: Advances in Neural Information
  Processing Systems (2001)

\bibitem{khodadadian2022federated}
Khodadadian, S., Sharma, P., Joshi, G., Maguluri, S.T.: Federated reinforcement
  learning: Linear speedup under {Markovian} sampling. In: International
  Conference on Machine Learning (2022)

\bibitem{kingma2014adam}
Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv
  preprint arXiv:1412.6980  (2014)

\bibitem{lan2023}
Lan, G., Liu, X.Y., Zhang, Y., Wang, X.: Communication-efficient federated
  learning for resource-constrained edge devices. IEEE Transactions on Machine
  Learning in Communications and Networking  \textbf{1},  210--224 (2023)

\bibitem{li2019optimistic}
Li, X., Li, Y., Zhan, Y., Liu, X.Y.: Optimistic bull or pessimistic bear:
  Adaptive deep reinforcement learning for stock portfolio allocation. In:
  International Conference on Machine Learning Workshop on Applications and
  Infrastructure for Multi-Agent Learning (2019)

\bibitem{NEURIPS2019_227e072d}
Liu, B., Cai, Q., Yang, Z., Wang, Z.: Neural trust region/proximal policy
  optimization attains globally optimal policy. In: Advances in Neural
  Information Processing Systems (2019)

\bibitem{liu2021elegantrl}
Liu, X.Y., Li, Z., Yang, Z., Zheng, J., Wang, Z., Walid, A., Guo, J., Jordan,
  M.I.: {ElegantRL-Podracer}: Scalable and elastic library for cloud-native
  deep reinforcement learning. In: Advances in Neural Information Processing
  Systems Workshop on Deep Reinforcement Learning (2021)

\bibitem{liu2020improved}
Liu, Y., Zhang, K., Basar, T., Yin, W.: An improved analysis of
  (variance-reduced) policy gradient and natural policy gradient methods. In:
  Advances in Neural Information Processing Systems (2020)

\bibitem{makhdoumi2017convergence}
Makhdoumi, A., Ozdaglar, A.: Convergence rate of distributed {ADMM} over
  networks. IEEE Transactions on Automatic Control  \textbf{62}(10),
  5082--5095 (2017)

\bibitem{mcmahan2017communication}
McMahan, B., Moore, E., Ramage, D., Hampson, S., y~Arcas, B.A.:
  Communication-efficient learning of deep networks from decentralized data.
  In: International Conference on Artificial Intelligence and Statistics (2017)

\bibitem{mothukuri2021survey}
Mothukuri, V., Parizi, R.M., Pouriyeh, S., Huang, Y., Dehghantanha, A.,
  Srivastava, G.: A survey on security and privacy of federated learning.
  Future Generation Computer Systems  \textbf{115},  619--640 (2021)

\bibitem{nguyen2021federated}
Nguyen, D.C., Ding, M., Pathirana, P.N., Seneviratne, A., Li, J., Poor, H.V.:
  Federated learning for internet of things: A comprehensive survey. IEEE
  Communications Surveys \& Tutorials  \textbf{23}(3),  1622--1658 (2021)

\bibitem{niknam2020federated}
Niknam, S., Dhillon, H.S., Reed, J.H.: Federated learning for wireless
  communications: Motivation, opportunities, and challenges. IEEE
  Communications Magazine  \textbf{58}(6),  46--51 (2020)

\bibitem{openai2023gpt4}
OpenAI: {GPT}-4 technical report. arXiv preprint arXiv:2303.08774  (2023)

\bibitem{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang,
  C., Agarwal, S., Slama, K., Gray, A., Schulman, J., Hilton, J., Kelton, F.,
  Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J.,
  Lowe, R.: Training language models to follow instructions with human
  feedback. In: Advances in Neural Information Processing Systems (2022)

\bibitem{pmlr-v80-papini18a}
Papini, M., Binaghi, D., Canonaco, G., Pirotta, M., Restelli, M.: Stochastic
  variance-reduced policy gradient. In: Proceedings of the 35th International
  Conference on Machine Learning (2018)

\bibitem{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.: {PyTorch}: An imperative
  style, high-performance deep learning library. In: Advances in Neural
  Information Processing Systems (2019)

\bibitem{provost2013data}
Provost, F., Fawcett, T.: Data science and its relationship to big data and
  data-driven decision making. Big Data  \textbf{1}(1),  51--59 (2013)

\bibitem{pmlr-v151-qian22a}
Qian, X., Islamov, R., Safaryan, M., Richtarik, P.: Basis matters: Better
  communication-efficient second order methods for federated learning. In:
  Proceedings of The 25th International Conference on Artificial Intelligence
  and Statistics (2022)

\bibitem{stable-baselines3}
Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., Dormann, N.:
  Stable-baselines3: Reliable reinforcement learning implementations. Journal
  of Machine Learning Research  \textbf{22}(268), ~1--8 (2021)

\bibitem{rajeswaran2017towards}
Rajeswaran, A., Lowrey, K., Todorov, E.V., Kakade, S.M.: Towards generalization
  and simplicity in continuous control. Advances in Neural Information
  Processing Systems  (2017)

\bibitem{safaryan2022fednl}
Safaryan, M., Islamov, R., Qian, X., Richtarik, P.: {FedNL}: Making
  {Newton}-type methods applicable to federated learning. In: International
  Conference on Machine Learning (2022)

\bibitem{schulman2017trust}
Schulman, J., Levine, S., Moritz, P., Jordan, M.I., Abbeel, P.: Trust region
  policy optimization. arXiv preprint arXiv:1502.05477  (2017)

\bibitem{schulman2018highdimensional}
Schulman, J., Moritz, P., Levine, S., Jordan, M., Abbeel, P.: High-dimensional
  continuous control using generalized advantage estimation. arXiv preprint
  arXiv:1506.02438  (2018)

\bibitem{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal
  policy optimization algorithms. arXiv preprint arXiv:1707.06347  (2017)

\bibitem{shalev2016safe}
Shalev-Shwartz, S., Shammah, S., Shashua, A.: Safe, multi-agent, reinforcement
  learning for autonomous driving. arXiv preprint arXiv:1610.03295  (2016)

\bibitem{shamir2014communication}
Shamir, O., Srebro, N., Zhang, T.: Communication-efficient distributed
  optimization using an approximate {Newton}-type method. In: International
  Conference on Machine Learning (2014)

\bibitem{speedtest}
speedtest.net: Speedtest market report in the {United States} (2023),
  \url{http://www.speedtest.net/reports/united-states}

\bibitem{todorov2012mujoco}
Todorov, E., Erez, T., Tassa, Y.: {MuJoCo}: A physics engine for model-based
  control. In: IEEE/RSJ International Conference on Intelligent Robots and
  Systems (2012)

\bibitem{wang2022fedadmm}
Wang, H., Marella, S., Anderson, J.: {FedADMM}: A federated primal-dual
  algorithm allowing partial participation. In: IEEE 61st Conference on
  Decision and Control (2022)

\bibitem{wang2023federated}
Wang, H., Mitra, A., Hassani, H., Pappas, G.J., Anderson, J.: Federated
  temporal difference learning with linear function approximation under
  environmental heterogeneity. arXiv preprint arXiv:2302.02212  (2023)

\bibitem{wang2023model}
Wang, H., Toso, L.F., Mitra, A., Anderson, J.: Model-free learning with
  heterogeneous dynamical systems: A federated {LQR} approach. arXiv preprint
  arXiv:2308.11743  (2023)

\bibitem{wang2023fedsysid}
Wang, H., Toso, L.F., Anderson, J.: {FedSysID}: A federated approach to
  sample-efficient system identification. In: Learning for Dynamics and Control
  Conference. pp. 1308--1320. PMLR (2023)

\bibitem{woodworth2020minibatch}
Woodworth, B.E., Patel, K.K., Srebro, N.: Minibatch vs local {SGD} for
  heterogeneous distributed learning. In: Advances in Neural Information
  Processing Systems (2020)

\bibitem{fedkl2023}
Xie, Z., Song, S.: {FedKL}: Tackling data heterogeneity in federated
  reinforcement learning by penalizing {KL} divergence. IEEE Journal on
  Selected Areas in Communications  \textbf{41}(4),  1227--1242 (2023)

\bibitem{xu2020Sample}
Xu, P., Gao, F., Gu, Q.: Sample efficient policy gradient methods with
  recursive variance reduction. In: International Conference on Learning
  Representations (2020)

\end{thebibliography}
