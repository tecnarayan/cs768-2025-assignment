\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Almeida et~al.(1999)Almeida, Langlois, Amaral, and
  Plakhov]{almeida1999parameter}
Almeida, L.~B., Langlois, T., Amaral, J.~D., and Plakhov, A.
\newblock \emph{Parameter Adaptation in Stochastic Optimization}, pp.\
  111–134.
\newblock Cambridge University Press, 1999.

\bibitem[Bach(2014)]{bach2014adaptivity}
Bach, F.
\newblock Adaptivity of averaged stochastic gradient descent to local strong
  convexity for logistic regression.
\newblock \emph{Journal of Machine Learning Research}, 15:\penalty0 595--627,
  2014.

\bibitem[Bach \& Moulines(2011)Bach and Moulines]{moulines2011non}
Bach, F. and Moulines, E.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  451--459, 2011.

\bibitem[Bach \& Moulines(2013)Bach and Moulines]{bach2013non}
Bach, F. and Moulines, E.
\newblock Non-strongly-convex smooth stochastic approximation with convergence
  rate o (1/n).
\newblock In \emph{Advances in neural information processing systems}, pp.\
  773--781, 2013.

\bibitem[Beck \& Teboulle(2009)Beck and Teboulle]{BecTeb09}
Beck, A. and Teboulle, M.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock \emph{SIAM J. Imaging Sci.}, 2\penalty0 (1):\penalty0 183--202, 2009.

\bibitem[Benveniste et~al.(1990)Benveniste, Priouret, and
  M\'{e}tivier]{benveniste1990adaptive}
Benveniste, A., Priouret, P., and M\'{e}tivier, M.
\newblock \emph{Adaptive Algorithms and Stochastic Approximations}.
\newblock Springer-Verlag, 1990.

\bibitem[Bottou(1998)]{bottou98}
Bottou, L.
\newblock Online algorithms and stochastic approximations.
\newblock In Saad, D. (ed.), \emph{Online Learning and Neural Networks}.
  Cambridge University Press, Cambridge, UK, 1998.
\newblock URL \url{http://leon.bottou.org/papers/bottou-98x}.
\newblock revised, oct 2012.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and
  Nocedal]{bottou2018optimization}
Bottou, L., Curtis, F.~E., and Nocedal, J.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{Siam Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Chee \& Toulis(2018)Chee and Toulis]{chee2018convergence}
Chee, J. and Toulis, P.
\newblock Convergence diagnostics for stochastic gradient descent with constant
  learning rate.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1476--1485, 2018.

\bibitem[Delyon \& Juditsky(1993)Delyon and Juditsky]{delyon1993accelerated}
Delyon, B. and Juditsky, A.
\newblock Accelerated stochastic approximation.
\newblock \emph{SIAM Journal on Optimization}, 3\penalty0 (4):\penalty0
  868--881, 1993.

\bibitem[Dieuleveut et~al.(2017)Dieuleveut, Durmus, and
  Bach]{dieuleveut2017bridging}
Dieuleveut, A., Durmus, A., and Bach, F.
\newblock Bridging the gap between constant step size stochastic gradient
  descent and markov chains.
\newblock \emph{arXiv preprint arXiv:1707.06386}, 2017.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of machine learning research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Graves et~al.(2013)Graves, Mohamed, and Hinton]{graves2013speech}
Graves, A., Mohamed, A., and Hinton, G.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In \emph{2013 IEEE International Conference on Acoustics, Speech and
  Signal Processing}, pp.\  6645--6649, 2013.

\bibitem[Hazan \& Kale(2014)Hazan and Kale]{hazan2014optimal}
Hazan, E. and Kale, S.
\newblock Beyond the regret minimization barrier: Optimal algorithms for
  stochastic strongly-convex optimization.
\newblock \emph{Journal of Machine Learning Research}, 15:\penalty0 2489--2512,
  2014.

\bibitem[{He} et~al.(2016){He}, {Zhang}, {Ren}, and {Sun}]{he2016deep}
{He}, K., {Zhang}, X., {Ren}, S., and {Sun}, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  770--778, 2016.

\bibitem[Jacobs(1988)]{jacob1988increased}
Jacobs, R.~A.
\newblock Increased rates of convergence through learning rate adaptation.
\newblock \emph{Neural Networks}, 1\penalty0 (4):\penalty0 295 -- 307, 1988.

\bibitem[Juditsky \& Nesterov(2014)Juditsky and
  Nesterov]{juditsky2014deterministic}
Juditsky, A. and Nesterov, Y.
\newblock Deterministic and stochastic primal-dual subgradient algorithms for
  uniformly convex minimization.
\newblock \emph{Stochastic Systems}, 4\penalty0 (1):\penalty0 44--80, 2014.

\bibitem[Kesten(1958)]{kesten1958accelerated}
Kesten, H.
\newblock Accelerated stochastic approximation.
\newblock \emph{Ann. Math. Statist.}, 29\penalty0 (1):\penalty0 41--59, 03
  1958.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krizhevsky(2009)]{Krizhevsky09learningmultiple}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1097--1105, 2012.

\bibitem[Kushner \& Huang(1981)Kushner and Huang]{kushner1981asymptotic}
Kushner, H.~J. and Huang, H.
\newblock Asymptotic properties of stochastic approximations with constant
  coefficients.
\newblock \emph{SIAM Journal on Control and Optimization}, 19\penalty0
  (1):\penalty0 87--105, 1981.

\bibitem[{Kushner} \& {Yang}(1995){Kushner} and {Yang}]{kushner1995analysis}
{Kushner}, H.~J. and {Yang}, J.
\newblock Analysis of adaptive step-size sa algorithms for parameter tracking.
\newblock \emph{IEEE Transactions on Automatic Control}, 40\penalty0
  (8):\penalty0 1403--1410, 1995.

\bibitem[Lacoste-Julien et~al.(2012)Lacoste-Julien, Schmidt, and
  Bach]{lacoste2012simpler}
Lacoste-Julien, S., Schmidt, M., and Bach, F.
\newblock A simpler approach to obtaining an o (1/t) convergence rate for the
  projected stochastic subgradient method.
\newblock \emph{arXiv preprint arXiv:1212.2002}, 2012.

\bibitem[Lang et~al.(2019)Lang, Xiao, and Zhang]{lang2019using}
Lang, H., Xiao, L., and Zhang, P.
\newblock Using statistics to automate stochastic optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9536--9546, 2019.

\bibitem[Loshchilov \& Hutter(2016)Loshchilov and Hutter]{Loshchilov2016}
Loshchilov, I. and Hutter, F.
\newblock {SGDR:} stochastic gradient descent with restarts.
\newblock \emph{CoRR}, abs/1608.03983, 2016.
\newblock URL \url{http://arxiv.org/abs/1608.03983}.

\bibitem[Needell et~al.(2014)Needell, Ward, and Srebro]{needell2014stochastic}
Needell, D., Ward, R., and Srebro, N.
\newblock Stochastic gradient descent, weighted sampling, and the randomized
  kaczmarz algorithm.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1017--1025, 2014.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski2009robust}
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.

\bibitem[Nemirovsky \& Yudin(1983)Nemirovsky and Yudin]{NemYud83}
Nemirovsky, A.~S. and Yudin, D.~B.
\newblock \emph{{Problem Complexity and Method Efficiency in Optimization}}.
\newblock Wiley-Interscience Series in Discrete Mathematics. John Wiley \&\
  Sons, 1983.

\bibitem[Nguyen et~al.(2019)Nguyen, Nguyen, and van Dijk]{nguyen2019tight}
Nguyen, P., Nguyen, L., and van Dijk, M.
\newblock Tight dimension independent lower bound on the expected convergence
  rate for diminishing step sizes in sgd.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3665--3674, 2019.

\bibitem[Paley \& Zygmund(1932)Paley and Zygmund]{paley1932series}
Paley, R. E. A.~C. and Zygmund, A.
\newblock On some series of functions, (3).
\newblock \emph{Mathematical Proceedings of the Cambridge Philosophical
  Society}, 28\penalty0 (2):\penalty0 190–205, 1932.

\bibitem[Pflug(1983)]{pflug1983determination}
Pflug, G.~C.
\newblock On the determination of the step size in stochastic quasigradient
  methods.
\newblock Technical report, IIASA Collaborative Paper, 1983.

\bibitem[Pflug(1986)]{pflug1986stochastic}
Pflug, G.~C.
\newblock Stochastic minimization with constant step-size: Asymptotic laws.
\newblock \emph{SIAM Journal on Control and Optimization}, 24\penalty0
  (4):\penalty0 655--666, 1986.

\bibitem[Pflug(1988{\natexlab{a}})]{pflug1988adaptive}
Pflug, G.~C.
\newblock Adaptive stepsize control in stochastic approximation algorithms.
\newblock \emph{IFAC Proceedings Volumes}, 21\penalty0 (9):\penalty0 787--792,
  1988{\natexlab{a}}.

\bibitem[Pflug(1988{\natexlab{b}})]{pflug1988stepsize}
Pflug, G.~C.
\newblock Stepsize rules, stopping times and their implementation in stochastic
  quasi-gradient algorithms.
\newblock \emph{numerical techniques for stochastic optimization}, pp.\
  353--372, 1988{\natexlab{b}}.

\bibitem[Polyak \& Juditsky(1992)Polyak and Juditsky]{polyak1992acceleration}
Polyak, B.~T. and Juditsky, A.~B.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM Journal on Control and Optimization}, 30\penalty0
  (4):\penalty0 838--855, 1992.

\bibitem[Rakhlin et~al.(2012)Rakhlin, Shamir, and Sridharan]{RakShaSri12}
Rakhlin, A., Shamir, O., and Sridharan, K.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock In \emph{{Proceedings of the Conference on Machine Learning (ICML)}},
  2012.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins1951stochastic}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pp.\  400--407, 1951.

\bibitem[Roulet \& d'Aspremont(2017)Roulet and
  d'Aspremont]{roulet2017sharpness}
Roulet, V. and d'Aspremont, A.
\newblock Sharpness, restart and acceleration.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1119--1129, 2017.

\bibitem[Schaul et~al.(2013)Schaul, Zhang, and LeCun]{schaul2013no}
Schaul, T., Zhang, S., and LeCun, Y.
\newblock No more pesky learning rates.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  343--351, 2013.

\bibitem[Schraudolph(1999)]{schraudolph1999local}
Schraudolph, N.~N.
\newblock Local gain adaptation in stochastic gradient descent.
\newblock In \emph{In Proc. Intl. Conf. Artificial Neural Networks}, pp.\
  569--574, 1999.

\bibitem[Shamir \& Zhang(2013)Shamir and Zhang]{shamir2013stochastic}
Shamir, O. and Zhang, T.
\newblock Stochastic gradient descent for non-smooth optimization: Convergence
  results and optimal averaging schemes.
\newblock In \emph{International Conference on Machine Learning}, pp.\  71--79,
  2013.

\bibitem[Smith(2017)]{smith2017cyclical}
Smith, L.~N.
\newblock Cyclical learning rates for training neural networks.
\newblock In \emph{2017 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pp.\  464--472. IEEE, 2017.

\bibitem[Sordello \& Su(2019)Sordello and Su]{sordello2019robust}
Sordello, M. and Su, W.
\newblock Robust learning rate selection for stochastic optimization via
  splitting diagnostic.
\newblock \emph{arXiv preprint arXiv:1910.08597}, 2019.

\bibitem[Sutton(1981)]{sutton1981adaption}
Sutton, R.
\newblock Adaptation of learning rate parameters.
\newblock In \emph{In: Goal Seeking Components for Adaptive Intelligence: An
  Initial Assessment, by A. G. Barto and R. S. Sutton.} Air Force Wright
  Aeronautical Laboratories Technical Report AFWAL-TR-81-1070. Wright-Patterson
  Air Force Base, Ohio 45433., 1981.

\bibitem[Sutton(1992)]{sutton1992adapting}
Sutton, R.~S.
\newblock Adapting bias by gradient descent: An incremental version of
  delta-bar-delta.
\newblock In \emph{AAAI}, 1992.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and
  Recht]{wilson2017marginal}
Wilson, A.~C., Roelofs, R., Stern, M., Srebro, N., and Recht, B.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4148--4158, 2017.

\bibitem[{Yaida}(2018)]{yaida18}
{Yaida}, S.
\newblock {Fluctuation-dissipation relations for stochastic gradient descent}.
\newblock \emph{arXiv e-prints}, art. arXiv:1810.00004, Sep 2018.

\bibitem[Zeiler(2012)]{matthew12}
Zeiler, M.~D.
\newblock {ADADELTA:} an adaptive learning rate method.
\newblock \emph{CoRR}, abs/1212.5701, 2012.
\newblock URL \url{http://arxiv.org/abs/1212.5701}.

\bibitem[Zhang(2004)]{zhang2004solving}
Zhang, T.
\newblock Solving large scale linear prediction problems using stochastic
  gradient descent algorithms.
\newblock In \emph{Proceedings of the Twenty-First International Conference on
  Machine Learning}, ICML ’04, pp.\  116, 2004.

\end{thebibliography}
