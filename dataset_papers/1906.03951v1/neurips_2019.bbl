\begin{thebibliography}{10}

\bibitem{attentionnlp}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock In {\em ICLR}, 2015.

\bibitem{distillationorigin}
Cristian Bucilu«é, Rich Caruana, and Alexandru Niculescu-Mizil.
\newblock Model compression.
\newblock In {\em SIGKDD}, pages 535--541, 2006.

\bibitem{binary}
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock In {\em NeurIPS}, pages 3123--3131, 2015.

\bibitem{imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em CVPR}, pages 248--255, 2009.

\bibitem{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em NAACL}, 2018.

\bibitem{lookcloser}
Jianlong Fu, Heliang Zheng, and Tao Mei.
\newblock Look closer to see better: Recurrent attention convolutional neural
  network for fine-grained image recognition.
\newblock In {\em CVPR}, volume~2, page~3, 2017.

\bibitem{deepcompression}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock In {\em ICLR}, 2016.

\bibitem{pruning}
Song Han, Jeff Pool, John Tran, and William Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In {\em NeurIPS}, pages 1135--1143, 2015.

\bibitem{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, pages 770--778, 2016.

\bibitem{distill_hinton}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock In {\em NeurIPS}, 2014.

\bibitem{mobilenets}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock In {\em CVPR}, 2017.

\bibitem{multiscaledensenet}
Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van~der Maaten, and
  Kilian~Q Weinberger.
\newblock Multi-scale dense networks for resource efficient image
  classification.
\newblock In {\em ICLR}, 2018.

\bibitem{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em CVPR}, pages 4700--4708, 2017.

\bibitem{droplayer}
Gao Huang, Yu~Sun, Zhuang Liu, Daniel Sedra, and Kilian~Q Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In {\em ECCV}, pages 646--661, 2016.

\bibitem{squeezenet}
Forrest~N Iandola, Song Han, Matthew~W Moskewicz, Khalid Ashraf, William~J
  Dally, and Kurt Keutzer.
\newblock Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5
  mb model size.
\newblock In {\em ICLR}, 2016.

\bibitem{cifar}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem{deeplysupervisednet}
Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu.
\newblock Deeply-supervised nets.
\newblock In {\em Artificial Intelligence and Statistics}, pages 562--570,
  2015.

\bibitem{pyramidfeature}
Tsung-Yi Lin, Piotr Doll{\'a}r, Ross Girshick, Kaiming He, Bharath Hariharan,
  and Serge Belongie.
\newblock Feature pyramid networks for object detection.
\newblock In {\em CVPR}, volume~1, page~4, 2017.

\bibitem{liu2016ssd}
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
  Cheng-Yang Fu, and Alexander~C Berg.
\newblock Ssd: Single shot multibox detector.
\newblock In {\em ECCV}, pages 21--37, 2016.

\bibitem{xnor}
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In {\em ECCV}, pages 525--542, 2016.

\bibitem{fitnets}
Adriana Romero, Nicolas Ballas, Samira~Ebrahimi Kahou, Antoine Chassang, Carlo
  Gatta, and Yoshua Bengio.
\newblock Fitnets: Hints for thin deep nets.
\newblock In {\em ICLR}, 2015.

\bibitem{vgg}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em ICLR}, 2015.

\bibitem{attentionisallyouneed}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em NeurIPS}, pages 5998--6008, 2017.

\bibitem{attentionresidual}
Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang,
  Xiaogang Wang, and Xiaoou Tang.
\newblock Residual attention network for image classification.
\newblock In {\em CVPR}, pages 3156--3164, 2017.

\bibitem{skipnet}
Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph~E Gonzalez.
\newblock Skipnet: Learning dynamic routing in convolutional networks.
\newblock In {\em ECCV}, pages 409--424, 2018.

\bibitem{wang2017dynamic}
Xuejian Wang, Lantao Yu, Kan Ren, Guanyu Tao, Weinan Zhang, Yong Yu, and Jun
  Wang.
\newblock Dynamic attention deep model for article recommendation by learning
  human editors' demonstration.
\newblock In {\em SIGKDD}, pages 2051--2059, 2017.

\bibitem{blockdrop}
Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry~S Davis,
  Kristen Grauman, and Rogerio Feris.
\newblock Blockdrop: Dynamic inference paths in residual networks.
\newblock In {\em CVPR}, pages 8817--8826, 2018.

\bibitem{ResNeXt}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In {\em CVPR}, pages 5987--5995, 2017.

\bibitem{xuk2015show}
BaJ XuK, CourvilleA KirosR, et~al.
\newblock Show, attend and tell: Neural image caption generation with visual
  attention.
\newblock In {\em ICML}, pages 20148--2057, 2015.

\bibitem{slimmable}
Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas Huang.
\newblock Slimmable neural networks.
\newblock In {\em ICLR}, 2019.

\bibitem{wideresnet}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In {\em BMVC}, 2016.

\bibitem{attentiondistillation}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Paying more attention to attention: Improving the performance of
  convolutional neural networks via attention transfer.
\newblock In {\em ICLR}, 2017.

\bibitem{selfdistillation}
Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng
  Ma.
\newblock Be your own teacher: Improve the performance of convolutional neural
  networks via self distillation.
\newblock In {\em arXiv preprint:1905.08094}, 2019.

\bibitem{deepmutuallearning}
Ying Zhang, Tao Xiang, Timothy~M Hospedales, and Huchuan Lu.
\newblock Deep mutual learning.
\newblock In {\em CVPR}, pages 4320--4328, 2018.

\end{thebibliography}
