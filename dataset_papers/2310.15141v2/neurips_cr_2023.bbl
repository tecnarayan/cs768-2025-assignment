\begin{thebibliography}{10}

\bibitem{ackley1985learning}
David~H Ackley, Geoffrey~E Hinton, and Terrence~J Sejnowski.
\newblock A learning algorithm for boltzmann machines.
\newblock {\em Cognitive science}, 9(1):147--169, 1985.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{chelba2013one}
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi~Ge, Thorsten Brants, Phillipp
  Koehn, and Tony Robinson.
\newblock One billion word benchmark for measuring progress in statistical
  language modeling.
\newblock {\em arXiv preprint arXiv:1312.3005}, 2013.

\bibitem{chen2023accelerating}
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau,
  Laurent Sifre, and John Jumper.
\newblock Accelerating large language model decoding with speculative sampling.
\newblock {\em arXiv preprint arXiv:2302.01318}, 2023.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{Cuturi13lightspeed}
Marco Cuturi.
\newblock Sinkhorn distances: Lightspeed computation of optimal transport.
\newblock In C.J. Burges, L.~Bottou, M.~Welling, Z.~Ghahramani, and K.Q.
  Weinberger, editors, {\em Advances in Neural Information Processing Systems},
  volume~26. Curran Associates, Inc., 2013.

\bibitem{dantzig2002linear}
George~B Dantzig.
\newblock Linear programming.
\newblock {\em Operations research}, 50(1):42--47, 2002.

\bibitem{den2012probability}
Frank Den~Hollander.
\newblock Probability theory: The coupling method.
\newblock {\em Lecture notes available online (http://websites. math.
  leidenuniv. nl/probability/lecturenotes/CouplingLectures. pdf)}, 2012.

\bibitem{fan2018hierarchical}
Angela Fan, Mike Lewis, and Yann Dauphin.
\newblock Hierarchical neural story generation.
\newblock {\em arXiv preprint arXiv:1805.04833}, 2018.

\bibitem{ficler2017controlling}
Jessica Ficler and Yoav Goldberg.
\newblock Controlling linguistic style aspects in neural language generation.
\newblock {\em arXiv preprint arXiv:1707.02633}, 2017.

\bibitem{ge2022lossless}
Tao Ge, Heming Xia, Xin Sun, Si-Qing Chen, and Furu Wei.
\newblock Lossless acceleration for seq2seq generation with aggressive
  decoding.
\newblock {\em arXiv preprint arXiv:2205.10350}, 2022.

\bibitem{palm2blog}
{Google AI}.
\newblock Introducing {PaLM} 2, 2023.
\newblock
  \url{https://blog.google/technology/ai/google-palm-2-ai-large-language-model/}.

\bibitem{palm2}
{Google PaLM-2 Team}.
\newblock {PaLM 2} technical report, 2023.

\bibitem{pmlr-v108-guo20a}
Wenshuo Guo, Nhat Ho, and Michael Jordan.
\newblock Fast algorithms for computational optimal transport and wasserstein
  barycenter.
\newblock In Silvia Chiappa and Roberto Calandra, editors, {\em Proceedings of
  the Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of {\em Proceedings of Machine Learning Research},
  pages 2088--2097. PMLR, 26--28 Aug 2020.

\bibitem{flax2020github}
Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand
  Rondepierre, Andreas Steiner, and Marc van {Z}ee.
\newblock {F}lax: A neural network library and ecosystem for {JAX}, 2023.

\bibitem{holtzman2019curious}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock {\em arXiv preprint arXiv:1904.09751}, 2019.

\bibitem{kantorovich1942translocation}
Leonid~V Kantorovich.
\newblock On the translocation of masses.
\newblock In {\em Dokl. Akad. Nauk. USSR (NS)}, volume~37, pages 199--201,
  1942.

\bibitem{lee2014path}
Yin~Tat Lee and Aaron Sidford.
\newblock Path finding methods for linear programming: Solving linear programs
  in o (vrank) iterations and faster algorithms for maximum flow.
\newblock In {\em 2014 IEEE 55th Annual Symposium on Foundations of Computer
  Science}, pages 424--433. IEEE, 2014.

\bibitem{leviathan2022fast}
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
\newblock Fast inference from transformers via speculative decoding.
\newblock In {\em International Conference on Machine Learning}, pages
  19274--19286. PMLR, 2023.

\bibitem{eagle2023}
Yuhui Li, Chao Zhang, and Hongyang Zhang.
\newblock Eagle: Lossless acceleration of llm decoding by feature
  extrapolation, 2023.
\newblock \url{https://sites.google.com/corp/view/eagle-llm}.

\bibitem{miao2023specinfer}
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae
  Ying~Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming
  Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia.
\newblock Specinfer: Accelerating generative large language model serving with
  speculative inference and token tree verification, 2023.

\bibitem{pele2009fast}
Ofir Pele and Michael Werman.
\newblock Fast and robust earth mover's distances.
\newblock In {\em 2009 IEEE 12th international conference on computer vision},
  pages 460--467. IEEE, 2009.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{stern2018blockwise}
Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit.
\newblock Blockwise parallel decoding for deep autoregressive models.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{tay2022efficient}
Yi~Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
\newblock Efficient transformers: A survey.
\newblock {\em ACM Computing Surveys}, 55(6):1--28, 2022.

\bibitem{thoppilan2022lamda}
Romal Thoppilan, Daniel De~Freitas, Jamie Hall, Noam Shazeer, Apoorv
  Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du,
  et~al.
\newblock Lamda: Language models for dialog applications.
\newblock {\em arXiv preprint arXiv:2201.08239}, 2022.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock {LLaMA}: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{villani2009optimal}
C{\'e}dric Villani et~al.
\newblock {\em Optimal transport: old and new}, volume 338.
\newblock Springer, 2009.

\bibitem{yang2023inference}
Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan
  Majumder, and Furu Wei.
\newblock Inference with reference: Lossless acceleration of large language
  models.
\newblock {\em arXiv preprint arXiv:2304.04487}, 2023.

\end{thebibliography}
