\begin{thebibliography}{10}

\bibitem{baker2017accelerating}
Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik.
\newblock Accelerating neural architecture search using performance prediction.
\newblock In {\em ICLR Workshop}, 2018.

\bibitem{bender2018understanding}
Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc
  Le.
\newblock Understanding and simplifying one-shot architecture search.
\newblock In {\em ICML}, 2018.

\bibitem{tpe}
James~S Bergstra, R{\'e}mi Bardenet, Yoshua Bengio, and Bal{\'a}zs K{\'e}gl.
\newblock Algorithms for hyper-parameter optimization.
\newblock In {\em NeurIPS}, 2011.

\bibitem{chandrashekaran2017speeding}
Akshay Chandrashekaran and Ian~R Lane.
\newblock Speeding up hyper-parameter optimization by extrapolation of learning
  curves using previous builds.
\newblock In {\em ECML-PKDD}, 2017.

\bibitem{chen2016xgboost}
Tianqi Chen and Carlos Guestrin.
\newblock Xgboost: A scalable tree boosting system.
\newblock In {\em Proceedings of the 22nd acm sigkdd international conference
  on knowledge discovery and data mining}, pages 785--794, 2016.

\bibitem{chen2020drnas}
Xiangning Chen, Ruochen Wang, Minhao Cheng, Xiaocheng Tang, and Cho-Jui Hsieh.
\newblock Drnas: Dirichlet neural architecture search.
\newblock In {\em ICLR}, 2021.

\bibitem{tinyimagenet17}
Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter.
\newblock A downsampled variant of imagenet as an alternative to the cifar
  datasets.
\newblock In {\em arXiv:1707.08819}, 2017.

\bibitem{domhan2015speeding}
Tobias Domhan, Jost~Tobias Springenberg, and Frank Hutter.
\newblock Speeding up automatic hyperparameter optimization of deep neural
  networks by extrapolation of learning curves.
\newblock In {\em IJCAI}, 2015.

\bibitem{natsbench}
Xuanyi Dong, Lu~Liu, Katarzyna Musial, and Bogdan Gabrys.
\newblock Nats-bench: Benchmarking nas algorithms for architecture topology and
  size.
\newblock In {\em PAMI}, 2021.

\bibitem{gdas}
Xuanyi Dong and Yi~Yang.
\newblock Searching for a robust neural architecture in four gpu hours.
\newblock In {\em CVPR}, 2019.

\bibitem{nasbench201}
Xuanyi Dong and Yi~Yang.
\newblock Nas-bench-201: Extending the scope of reproducible neural
  architecture search.
\newblock In {\em ICLR}, 2020.

\bibitem{eggensperger2015efficient}
Katharina Eggensperger, Frank Hutter, Holger Hoos, and Kevin Leyton-Brown.
\newblock Efficient benchmarking of hyperparameter optimizers via surrogates.
\newblock In {\em AAAI}, 2015.

\bibitem{nas-survey}
Thomas Elsken, Jan~Hendrik Metzen, and Frank Hutter.
\newblock Neural architecture search: A survey.
\newblock In {\em JMLR}, 2019.

\bibitem{bohb}
Stefan Falkner, Aaron Klein, and Frank Hutter.
\newblock Bohb: Robust and efficient hyperparameter optimization at scale.
\newblock In {\em ICML}, 2018.

\bibitem{gargiani2019probabilistic}
Matilde Gargiani, Aaron Klein, Stefan Falkner, and Frank Hutter.
\newblock Probabilistic rollouts for learning curve extrapolation across
  hyperparameter settings.
\newblock {\em arXiv preprint arXiv:1910.04522}, 2019.

\bibitem{golub1965calculating}
Gene Golub and William Kahan.
\newblock Calculating the singular values and pseudo-inverse of a matrix.
\newblock {\em Journal of the Society for Industrial and Applied Mathematics,
  Series B: Numerical Analysis}, 2(2):205--224, 1965.

\bibitem{hao2019training}
Karen Hao.
\newblock Training a single ai model can emit as much carbon as five cars in
  their lifetimes.
\newblock {\em MIT Technology Review}, 2019.

\bibitem{hu2019forwardnas}
Hanzhang Hu, John Langford, Rich Caruana, Saurajit Mukherjee, Eric Horvitz, and
  Debadeepta Dey.
\newblock Efficient forward architecture search.
\newblock In {\em NeurIPS}, 2019.

\bibitem{huang2020asymptotically}
Yimin Huang, Yujun Li, Hanrong Ye, Zhenguo Li, and Zhihua Zhang.
\newblock An asymptotically optimal multi-armed bandit algorithm and
  hyperparameter optimization.
\newblock {\em arXiv preprint arXiv:2007.05670}, 2020.

\bibitem{k2016multifidelity}
Kirthevasan Kandasamy, Gautam Dasarathy, Junier~B. Oliva, Jeff Schneider, and
  Barnabas Poczos.
\newblock Multi-fidelity gaussian process bandit optimisation.
\newblock In {\em NeurIPS}, 2016.

\bibitem{k2017multifidelity}
Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, and Barnabas Poczos.
\newblock Multi-fidelity bayesian optimisation with continuous approximations.
\newblock In {\em JMLR}, 2017.

\bibitem{nasbot}
Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and
  Eric~P Xing.
\newblock Neural architecture search with bayesian optimisation and optimal
  transport.
\newblock In {\em NeurIPS}, 2018.

\bibitem{ke2017lightgbm}
Guolin Ke, Qi~Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei
  Ye, and Tie-Yan Liu.
\newblock Lightgbm: A highly efficient gradient boosting decision tree.
\newblock In {\em NeurIPS}, 2017.

\bibitem{kendall1938new}
Maurice~G Kendall.
\newblock A new measure of rank correlation.
\newblock {\em Biometrika}, 30(1/2):81--93, 1938.

\bibitem{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock In {\em ICLR}, 2014.

\bibitem{kitano1990designing}
Hiroaki Kitano.
\newblock Designing neural networks using genetic algorithms with graph
  generation system.
\newblock {\em Complex systems}, 4(4):461--476, 1990.

\bibitem{klein17fast}
Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter.
\newblock {Fast Bayesian Optimization of Machine Learning Hyperparameters on
  Large Datasets}.
\newblock In {\em AISTATS}, 2017.

\bibitem{lcnet}
Aaron Klein, Stefan Falkner, Jost~Tobias Springenberg, and Frank Hutter.
\newblock Learning curve prediction with bayesian neural networks.
\newblock In {\em ICLR}, 2017.

\bibitem{abohb}
Aaron Klein, Louis Tiao, Thibaut Lienart, Cedric Archambeau, and Matthias
  Seeger.
\newblock Model-based asynchronous hyperparameter and neural architecture
  search.
\newblock {\em arXiv preprint arXiv:2003.10865}, 2020.

\bibitem{nasbenchnlp}
Nikita Klyuchnikov, Ilya Trofimov, Ekaterina Artemova, Mikhail Salnikov, Maxim
  Fedorov, and Evgeny Burnaev.
\newblock Nas-bench-nlp: neural architecture search benchmark for natural
  language processing.
\newblock {\em arXiv preprint arXiv:2006.07116}, 2020.

\bibitem{CIFAR10}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem{li2018massively}
Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Moritz Hardt,
  Benjamin Recht, and Ameet Talwalkar.
\newblock A system for massively parallel hyperparameter tuning.
\newblock In {\em MLSys Conference}, 2020.

\bibitem{li2020geometry}
Liam Li, Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar.
\newblock Geometry-aware gradient algorithms for neural architecture search.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2021.

\bibitem{randomnas}
Liam Li and Ameet Talwalkar.
\newblock Random search and reproducibility for neural architecture search.
\newblock In {\em UAI}, 2019.

\bibitem{hyperband}
Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet
  Talwalkar.
\newblock Hyperband: A novel bandit-based approach to hyperparameter
  optimization.
\newblock In {\em JMLR}, 2018.

\bibitem{lindauer2019best}
Marius Lindauer and Frank Hutter.
\newblock Best practices for scientific research on neural architecture search.
\newblock In {\em JMLR}, 2020.

\bibitem{liu2017progressive}
Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li,
  Li~Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy.
\newblock Progressive neural architecture search.
\newblock In {\em ECCV}, 2018.

\bibitem{darts}
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
\newblock Darts: Differentiable architecture search.
\newblock In {\em ICLR}, 2019.

\bibitem{seminas}
Renqian Luo, Xu~Tan, Rui Wang, Tao Qin, Enhong Chen, and Tie-Yan Liu.
\newblock Semi-supervised neural architecture search.
\newblock In {\em NeurIPS}, 2020.

\bibitem{dehb}
Neeratyoy Mallik and Noor Awad.
\newblock Dehb: Evolutionary hyperband for scalable, robust and efficient
  hyperparameter optimization.
\newblock In {\em IJCAI}, 2021.

\bibitem{nasbenchasr}
Abhinav Mehrotra, Alberto Gil C.~P. Ramos, Sourav Bhattacharya, {\L}ukasz
  Dudziak, Ravichander Vipperla, Thomas Chau, Mohamed~S Abdelfattah, Samin
  Ishtiaq, and Nicholas~Donald Lane.
\newblock Nas-bench-asr: Reproducible neural architecture search for speech
  recognition.
\newblock In {\em ICLR}, 2021.

\bibitem{penntreebank}
Tom{\'a}{\v{s}} Mikolov, Martin Karafi{\'a}t, Luk{\'a}{\v{s}} Burget, Jan
  {\v{C}}ernock{\`y}, and Sanjeev Khudanpur.
\newblock Recurrent neural network based language model.
\newblock In {\em Annual conference of the international speech communication
  association}, 2010.

\bibitem{miller1989designing}
Geoffrey~F Miller, Peter~M Todd, and Shailesh~U Hegde.
\newblock Designing neural networks using genetic algorithms.
\newblock In {\em ICGA}, volume~89, pages 379--384, 1989.

\bibitem{negrinho2017deeparchitect}
Renato Negrinho and Geoff Gordon.
\newblock Deeparchitect: Automatically designing and training deep
  architectures.
\newblock {\em arXiv preprint arXiv:1704.08792}, 2017.

\bibitem{vu2020bayesian}
Vu~Nguyen, Sebastian Schulze, and Michael Osborne.
\newblock Bayesian optimization for iterative learning.
\newblock In {\em NeurIPS}, 2020.

\bibitem{ning2020surgery}
Xuefei Ning, Wenshuo Li, Zixuan Zhou, Tianchen Zhao, Yin Zheng, Shuang Liang,
  Huazhong Yang, and Yu~Wang.
\newblock A surgery of the neural architecture evaluators.
\newblock {\em arXiv preprint arXiv:2008.03064}, 2020.

\bibitem{oh2019combinatorial}
Changyong Oh, Jakub~M Tomczak, Efstratios Gavves, and Max Welling.
\newblock Combinatorial bayesian optimization using the graph cartesian
  product.
\newblock {\em arXiv preprint arXiv:1902.00448}, 2019.

\bibitem{ottelander2020local}
T~Den Ottelander, Arkadiy Dushatskiy, Marco Virgolin, and Peter~AN Bosman.
\newblock Local search is a remarkably strong baseline for neural architecture
  search.
\newblock In {\em International Conference on Evolutionary Multi-Criterion
  Optimization}, 2021.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Proceedings of the Annual Conference on Neural Information
  Processing Systems (NeurIPS)}, 2019.

\bibitem{patterson2021carbon}
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia,
  Daniel Rothchild, David So, Maud Texier, and Jeff Dean.
\newblock Carbon emissions and large neural network training.
\newblock {\em arXiv preprint arXiv:2104.10350}, 2021.

\bibitem{peng2020cream}
Houwen Peng, Hao Du, Hongyuan Yu, Qi~Li, Jing Liao, and Jianlong Fu.
\newblock Cream of the crop: Distilling prioritized paths for one-shot neural
  architecture search.
\newblock In {\em NeurIPS}, 2020.

\bibitem{enas}
Hieu Pham, Melody~Y Guan, Barret Zoph, Quoc~V Le, and Jeff Dean.
\newblock Efficient neural architecture search via parameter sharing.
\newblock In {\em ICML}, 2018.

\bibitem{real2019regularized}
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc~V Le.
\newblock Regularized evolution for image classifier architecture search.
\newblock In {\em AAAI}, 2019.

\bibitem{ru2020revisiting}
Binxin Ru, Clare Lyle, Lisa Schut, Mark van~der Wilk, and Yarin Gal.
\newblock Revisiting the train loss: an efficient performance estimator for
  neural architecture search.
\newblock {\em arXiv preprint arXiv:2006.04492}, 2020.

\bibitem{nasbowl}
Binxin Ru, Xingchen Wan, Xiaowen Dong, and Michael Osborne.
\newblock Neural architecture search using bayesian optimisation with
  weisfeiler-lehman kernel.
\newblock In {\em ICLR}, 2021.

\bibitem{ruchte2020naslib}
Michael Ruchte, Arber Zela, Julien Siems, Josif Grabocka, and Frank Hutter.
\newblock Naslib: a modular and flexible neural architecture search library,
  2020.

\bibitem{sciuto2019evaluating}
Christian Sciuto, Kaicheng Yu, Martin Jaggi, Claudiu Musat, and Mathieu
  Salzmann.
\newblock Evaluating the search phase of neural architecture search.
\newblock In {\em ICLR}, 2020.

\bibitem{scott2015multivariate}
David~W Scott.
\newblock {\em Multivariate density estimation: theory, practice, and
  visualization}.
\newblock John Wiley \& Sons, 2015.

\bibitem{shi2020bonas}
Han Shi, Renjie Pi, Hang Xu, Zhenguo Li, James Kwok, and Tong Zhang.
\newblock Bridging the gap between sample-based and one-shot neural
  architecture search with bonas.
\newblock In {\em NeurIPS}, 2020.

\bibitem{nasbench301}
Julien Siems, Lucas Zimmer, Arber Zela, Jovita Lukasik, Margret Keuper, and
  Frank Hutter.
\newblock Nas-bench-301 and the case for surrogate benchmarks for neural
  architecture search.
\newblock {\em arXiv preprint arXiv:2008.09777}, 2020.

\bibitem{nb301response}
Julien Siems, Lucas Zimmer, Arber Zela, Jovita Lukasik, Margret Keuper, and
  Frank Hutter.
\newblock Nas-bench-301 and the case for surrogate benchmarks for neural
  architecture search: Openreview response, 2021.

\bibitem{stanley2002evolving}
Kenneth~O Stanley and Risto Miikkulainen.
\newblock Evolving neural networks through augmenting topologies.
\newblock {\em Evolutionary computation}, 10(2):99--127, 2002.

\bibitem{swersky2014freeze}
Kevin Swersky, Jasper Snoek, and Ryan~Prescott Adams.
\newblock Freeze-thaw bayesian optimization.
\newblock {\em arXiv preprint arXiv:1406.3896}, 2014.

\bibitem{npenas}
Chen Wei, Chuang Niu, Yiping Tang, and Jimin Liang.
\newblock Npenas: Neural predictor guided evolution for neural architecture
  search.
\newblock {\em arXiv preprint arXiv:2003.12857}, 2020.

\bibitem{wen2019neural}
Wei Wen, Hanxiao Liu, Hai Li, Yiran Chen, Gabriel Bender, and Pieter-Jan
  Kindermans.
\newblock Neural predictor for neural architecture search.
\newblock In {\em ECCV}, 2020.

\bibitem{white2020study}
Colin White, Willie Neiswanger, Sam Nolen, and Yash Savani.
\newblock A study on encodings for neural architecture search.
\newblock In {\em NeurIPS}, 2020.

\bibitem{bananas}
Colin White, Willie Neiswanger, and Yash Savani.
\newblock Bananas: Bayesian optimization with neural architectures for neural
  architecture search.
\newblock In {\em AAAI}, 2021.

\bibitem{white2020local}
Colin White, Sam Nolen, and Yash Savani.
\newblock Local search is state of the art for nas benchmarks.
\newblock In {\em UAI}, 2021.

\bibitem{white2021powerful}
Colin White, Arber Zela, Binxin Ru, Yang Liu, and Frank Hutter.
\newblock How powerful are performance predictors in neural architecture
  search?
\newblock {\em arXiv preprint arXiv:2104.01177}, 2021.

\bibitem{wright1921correlation}
Sewall Wright.
\newblock Correlation and causation.
\newblock {\em Journal of Agricultural Research}, 20:557--580, 1921.

\bibitem{xie2020weight}
Lingxi Xie, Xin Chen, Kaifeng Bi, Longhui Wei, Yuhui Xu, Zhengsu Chen, Lanfei
  Wang, An~Xiao, Jianlong Chang, Xiaopeng Zhang, et~al.
\newblock Weight-sharing neural architecture search: A battle to shrink the
  optimization gap.
\newblock {\em arXiv preprint arXiv:2008.01475}, 2020.

\bibitem{pcdarts}
Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi~Tian, and
  Hongkai Xiong.
\newblock Pc-darts: Partial channel connections for memory-efficient
  architecture search.
\newblock In {\em ICLR}, 2019.

\bibitem{yan2021cate}
Shen Yan, Kaiqiang Song, Fei Liu, and Mi~Zhang.
\newblock Cate: Computation-aware neural architecture encoding with
  transformers.
\newblock In {\em ICML}, 2021.

\bibitem{yan2020does}
Shen Yan, Yu~Zheng, Wei Ao, Xiao Zeng, and Mi~Zhang.
\newblock Does unsupervised architecture representation learning help neural
  architecture search?
\newblock In {\em NeurIPS}, 2020.

\bibitem{yang2019evaluation}
Antoine Yang, Pedro~M Esperan{\c{c}}a, and Fabio~M Carlucci.
\newblock Nas evaluation is frustratingly hard.
\newblock In {\em ICLR}, 2020.

\bibitem{nasbench}
Chris Ying, Aaron Klein, Esteban Real, Eric Christiansen, Kevin Murphy, and
  Frank Hutter.
\newblock Nas-bench-101: Towards reproducible neural architecture search.
\newblock In {\em ICML}, 2019.

\bibitem{you2020greedynas}
Shan You, Tao Huang, Mingmin Yang, Fei Wang, Chen Qian, and Changshui Zhang.
\newblock Greedynas: Towards fast one-shot nas with greedy supernet.
\newblock In {\em CVPR}, 2020.

\bibitem{yu2021landmark}
Kaicheng Yu, Rene Ranftl, and Mathieu Salzmann.
\newblock Landmark regularization: Ranking guided super-net training in neural
  architecture search.
\newblock In {\em CVPR}, 2021.

\bibitem{zela2020understanding}
Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and
  Frank Hutter.
\newblock Understanding and robustifying differentiable architecture search.
\newblock In {\em ICLR}, 2020.

\bibitem{zela2020bench}
Arber Zela, Julien Siems, and Frank Hutter.
\newblock Nas-bench-1shot1: Benchmarking and dissecting one-shot neural
  architecture search.
\newblock In {\em ICLR}, 2020.

\bibitem{zhang2020deeper}
Yuge Zhang, Zejun Lin, Junyang Jiang, Quanlu Zhang, Yujing Wang, Hui Xue, Chen
  Zhang, and Yaming Yang.
\newblock Deeper insights into weight sharing in neural architecture search.
\newblock {\em arXiv preprint arXiv:2001.01431}, 2020.

\bibitem{zoph2017neural}
Barret Zoph and Quoc~V. Le.
\newblock Neural architecture search with reinforcement learning.
\newblock In {\em ICLR}, 2017.

\bibitem{zoph2018learning}
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc~V Le.
\newblock Learning transferable architectures for scalable image recognition.
\newblock In {\em CVPR}, 2018.

\end{thebibliography}
