\begin{thebibliography}{135}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{AI@Meta(2024)}]{llama3modelcard}
AI@Meta. 2024.
\newblock \href {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md} {Llama 3 model card}.

\bibitem[{Amini et~al.(2019)Amini, Gabriel, Lin, Koncel-Kedziorski, Choi, and Hajishirzi}]{amini-etal-2019-mathqa}
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019.
\newblock \href {https://doi.org/10.18653/v1/N19-1245} {{M}ath{QA}: Towards interpretable math word problem solving with operation-based formalisms}.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 2357--2367, Minneapolis, Minnesota. Association for Computational Linguistics.

\bibitem[{Ando et~al.(2023)Ando, Morishita, Abe, Mineshima, and Okada}]{ando-etal-2023-evaluating}
Risako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, and Mitsuhiro Okada. 2023.
\newblock \href {https://aclanthology.org/2023.naloma-1.1} {Evaluating large language models with {N}eu{BAROCO}: Syllogistic reasoning ability and human-like biases}.
\newblock In \emph{Proceedings of the 4th Natural Logic Meets Machine Learning Workshop}, pages 1--11, Nancy, France. Association for Computational Linguistics.

\bibitem[{Aoki et~al.(2024)Aoki, Kudo, Kuribayashi, Sone, Taniguchi, Sakaguchi, and Inui}]{aoki2024heuristicrationaldynamicuse}
Yoichi Aoki, Keito Kudo, Tatsuki Kuribayashi, Shusaku Sone, Masaya Taniguchi, Keisuke Sakaguchi, and Kentaro Inui. 2024.
\newblock \href {http://arxiv.org/abs/2406.16078} {First heuristic then rational: Dynamic use of heuristics in language model reasoning}.

\bibitem[{Askell(2020)}]{askell2020gpt}
Amanda Askell. 2020.
\newblock Gpt-3: Towards renaissance models.
\newblock \emph{Daily Nous Blog: Philosophers On GPT-3}.

\bibitem[{Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le et~al.}]{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al. 2021.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}.

\bibitem[{Bao et~al.(2022)Bao, Peng, Hartill, Tan, Deng, Witbrock, and Liu}]{bao2022multi}
Qiming Bao, Alex~Yuxuan Peng, Tim Hartill, Neset Tan, Zhenyun Deng, Michael Witbrock, and Jiamou Liu. 2022.
\newblock Multi-step deductive reasoning over natural language: An empirical study on out-of-distribution generalisation.
\newblock In \emph{Proceedings of the 16th International Workshop on Neural-Symbolic Learning and Reasoning as part of the 2nd International Joint Conference on Learning \& Reasoning (IJCLR 2022)}, pages 202--217, Cumberland Lodge, Windsor Great Park, United Kingdom.

\bibitem[{Ben~Allal et~al.(2024)Ben~Allal, Lozhkov, Penedo, Wolf, and von Werra}]{benallal2024cosmopedia}
Loubna Ben~Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. 2024.
\newblock \href {https://huggingface.co/datasets/HuggingFaceTB/cosmopedia} {Cosmopedia}.

\bibitem[{Ben~Allal et~al.(2022)Ben~Allal, Muennighoff, Kumar~Umapathi, Lipkin, and von Werra}]{bigcode-evaluation-harness}
Loubna Ben~Allal, Niklas Muennighoff, Logesh Kumar~Umapathi, Ben Lipkin, and Leandro von Werra. 2022.
\newblock A framework for the evaluation of code generation models.
\newblock \url{https://github.com/bigcode-project/bigcode-evaluation-harness}.

\bibitem[{Bentivogli et~al.(2009)Bentivogli, Dagan, Dang, Giampiccolo, and Magnini}]{bentivogli2009fifth}
Luisa Bentivogli, Ido Dagan, Hoa~Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009.
\newblock The fifth pascal recognizing textual entailment challenge.
\newblock In \emph{Text Analysis Conference}.

\bibitem[{Bertolazzi et~al.(2024)Bertolazzi, Gatt, and Bernardi}]{bertolazzi2024systematicanalysislargelanguage}
Leonardo Bertolazzi, Albert Gatt, and Raffaella Bernardi. 2024.
\newblock \href {http://arxiv.org/abs/2406.11341} {A systematic analysis of large language models as soft reasoners: The case of syllogistic inferences}.

\bibitem[{Bertrand()}]{russel1946}
Russell Bertrand.
\newblock A history of western philosophy.

\bibitem[{Betz et~al.(2021)Betz, Voigt, and Richardson}]{betz-etal-2021-critical}
Gregor Betz, Christian Voigt, and Kyle Richardson. 2021.
\newblock \href {https://aclanthology.org/2021.iwcs-1.7} {Critical thinking for language models}.
\newblock In \emph{Proceedings of the 14th International Conference on Computational Semantics (IWCS)}, pages 63--75, Groningen, The Netherlands (online). Association for Computational Linguistics.

\bibitem[{Bhagavatula et~al.(2019)Bhagavatula, Bras, Malaviya, Sakaguchi, Holtzman, Rashkin, Downey, Yih, and Choi}]{bhagavatula2019abductive}
Chandra Bhagavatula, Ronan~Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Scott Wen-tau Yih, and Yejin Choi. 2019.
\newblock Abductive commonsense reasoning.
\newblock \emph{arXiv preprint arXiv:1908.05739}.

\bibitem[{Bhuiya et~al.(2024)Bhuiya, Schlegel, and Winkler}]{bhuiya2024seeminglyplausibledistractorsmultihop}
Neeladri Bhuiya, Viktor Schlegel, and Stefan Winkler. 2024.
\newblock \href {http://arxiv.org/abs/2409.05197} {Seemingly plausible distractors in multi-hop reasoning: Are large language models attentive readers?}

\bibitem[{Bostrom et~al.(2021)Bostrom, Zhao, Chaudhuri, and Durrett}]{bostrom-etal-2021-flexible}
Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and Greg Durrett. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.emnlp-main.506} {Flexible generation of natural language deductions}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 6266--6278, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

\bibitem[{Bowman et~al.()Bowman, Angeli, Potts, and Manning}]{bowmanlarge}
Samuel~R Bowman, Gabor Angeli, Christopher Potts, and Christopher~D Manning.
\newblock A large annotated corpus for learning natural language inference.

\bibitem[{Cassano et~al.(2023)Cassano, Gouwar, Nguyen, Nguyen, Phipps-Costin, Pinckney, Yee, Zi, Anderson, Feldman, Guha, Greenberg, and Jangda}]{multiple}
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn~Jane Anderson, Molly~Q Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. 2023.
\newblock \href {https://doi.org/10.1109/TSE.2023.3267446} {Multipl-e: A scalable and polyglot approach to benchmarking neural code generation}.
\newblock \emph{IEEE Transactions on Software Engineering}, 49(7):3675--3691.

\bibitem[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba}]{chen2021codex}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Ponde de~Oliveira~Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew~N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.
\newblock \href {http://arxiv.org/abs/2107.03374} {Evaluating large language models trained on code}.

\bibitem[{Chen et~al.(2020)Chen, Hou, Cui, Che, Liu, and Yu}]{recadam}
Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu. 2020.
\newblock \href {https://www.aclweb.org/anthology/2020.emnlp-main.634} {Recall and learn: Fine-tuning deep pretrained language models with less forgetting}.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 7870--7881, Online. Association for Computational Linguistics.

\bibitem[{Chen et~al.(2024)Chen, Chi, Wang, and Zhou}]{pmlr-v235-chen24i}
Xinyun Chen, Ryan~Andrew Chi, Xuezhi Wang, and Denny Zhou. 2024.
\newblock \href {https://proceedings.mlr.press/v235/chen24i.html} {Premise order matters in reasoning with large language models}.
\newblock In \emph{Proceedings of the 41st International Conference on Machine Learning}, volume 235 of \emph{Proceedings of Machine Learning Research}, pages 6596--6620. PMLR.

\bibitem[{Cheng et~al.(2017)Cheng, Bernstein, Danescu-Niculescu-Mizil, and Leskovec}]{Cheng:2017ud}
J.~Cheng, M.~Bernstein, C.~Danescu-Niculescu-Mizil, and J.~Leskovec. 2017.
\newblock \href {https://doi.org/10.1145/2998181.2998213} {Anyone can become a troll: Causes of trolling behavior in online discussions}.
\newblock \emph{CSCW: Proceedings of the Conference on Computer-Supported Cooperative Work. Conference on Computer-Supported Cooperative Work, 2017}.

\bibitem[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}.

\bibitem[{Clark et~al.(2021)Clark, Tafjord, and Richardson}]{clark2020transformers}
Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021.
\newblock Transformers as soft reasoners over language.
\newblock In \emph{Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence}, pages 3882--3890.

\bibitem[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman}]{cobbe2021gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}.

\bibitem[{Colmerauer and Roussel(1973)}]{colmerauer1973prolog}
A.~Colmerauer and P~Roussel. 1973.
\newblock The birth of prolog.
\newblock \emph{The ALP Newsletter}.

\bibitem[{Dagan et~al.(2005)Dagan, Glickman, and Magnini}]{dagan2005pascal}
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
\newblock The {PASCAL} recognising textual entailment challenge.
\newblock pages 177--190.

\bibitem[{Dalvi et~al.(2021)Dalvi, Jansen, Tafjord, Xie, Smith, Pipatanangkura, and Clark}]{dalvi-etal-2021-explaining}
Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.emnlp-main.585} {Explaining answers with entailment trees}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 7358--7370, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

\bibitem[{Dasgupta et~al.(2023)Dasgupta, Lampinen, Chan, Sheahan, Creswell, Kumaran, McClelland, and Hill}]{dasgupta2023language}
Ishita Dasgupta, Andrew~K. Lampinen, Stephanie C.~Y. Chan, Hannah~R. Sheahan, Antonia Creswell, Dharshan Kumaran, James~L. McClelland, and Felix Hill. 2023.
\newblock \href {http://arxiv.org/abs/2207.07051} {Language models show human-like content effects on reasoning tasks}.

\bibitem[{Dougrez-Lewis et~al.(2024)Dougrez-Lewis, Akhter, He, and Liakata}]{dougrezlewis2024assessingreasoningabilitieschatgpt}
John Dougrez-Lewis, Mahmud~Elahi Akhter, Yulan He, and Maria Liakata. 2024.
\newblock \href {http://arxiv.org/abs/2402.10735} {Assessing the reasoning abilities of chatgpt in the context of claim verification}.

\bibitem[{Dziri et~al.(2023)Dziri, Lu, Sclar, Li, Jiang, Lin, West, Bhagavatula, Bras, Hwang, Sanyal, Welleck, Ren, Ettinger, Harchaoui, and Choi}]{dziri2023faith}
Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang~Lorraine Li, Liwei Jiang, Bill~Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan~Le Bras, Jena~D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. 2023.
\newblock \href {http://arxiv.org/abs/2305.18654} {Faith and fate: Limits of transformers on compositionality}.

\bibitem[{Eisape et~al.(2024)Eisape, Tessler, Dasgupta, Sha, Steenkiste, and Linzen}]{eisape-etal-2024-systematic}
Tiwalayo Eisape, Michael Tessler, Ishita Dasgupta, Fei Sha, Sjoerd Steenkiste, and Tal Linzen. 2024.
\newblock \href {https://doi.org/10.18653/v1/2024.naacl-long.466} {A systematic comparison of syllogistic reasoning in humans and language models}.
\newblock In \emph{Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 8425--8444, Mexico City, Mexico. Association for Computational Linguistics.

\bibitem[{Elkan and Greiner(1993)}]{elkan1993building}
Charles Elkan and Russell Greiner. 1993.
\newblock Building large knowledge-based systems: Representation and inference in the cyc project: Db lenat and rv guha.

\bibitem[{Gao et~al.(2023)Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster, Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds, Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and Zou}]{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023.
\newblock \href {https://doi.org/10.5281/zenodo.10256836} {A framework for few-shot language model evaluation}.

\bibitem[{Giampiccolo et~al.(2007)Giampiccolo, Magnini, Dagan, and Dolan}]{giampiccolo2007third}
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and William~B Dolan. 2007.
\newblock The third {PASCAL} recognizing textual entailment challenge.
\newblock In \emph{ACL-PASCAL Workshop on Textual Entailment and Paraphrasing}, pages 1--9.

\bibitem[{G\"{o}del(1930)}]{godel1930uber}
Kurt G\"{o}del. 1930.
\newblock \emph{Uber die vollst{\"a}ndigkeit des logikkalk{\"u}ls}.
\newblock Ph.D. thesis, Ph. D. dissertation, University of Vienna.

\bibitem[{Gontier et~al.(2020)Gontier, Sinha, Reddy, and Pal}]{gontier2020measuring}
Nicolas Gontier, Koustuv Sinha, Siva Reddy, and Chris Pal. 2020.
\newblock Measuring systematic generalization in neural proof generation with transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:22231--22242.

\bibitem[{Goodman(1954)}]{goodman1954fact}
Nelson Goodman. 1954.
\newblock Fact, fiction, and forecast. london: University of london.

\bibitem[{Guia{\c{s}}u and Tindale(2018)}]{guiacsu2018logical}
Radu~Cornel Guia{\c{s}}u and Christopher~W Tindale. 2018.
\newblock Logical fallacies and invasion biology.
\newblock \emph{Biology \& philosophy}, 33(5-6):34.

\bibitem[{Habernal et~al.(2018)Habernal, Wachsmuth, Gurevych, and Stein}]{habernal-etal-2018-argument}
Ivan Habernal, Henning Wachsmuth, Iryna Gurevych, and Benno Stein. 2018.
\newblock \href {https://doi.org/10.18653/v1/N18-1175} {The argument reasoning comprehension task: Identification and reconstruction of implicit warrants}.
\newblock In \emph{Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)}, pages 1930--1940, New Orleans, Louisiana. Association for Computational Linguistics.

\bibitem[{Han et~al.(2024)Han, Song, Yu, and You}]{han2024incontextlearningelicittrustworthy}
Pengrui Han, Peiyang Song, Haofei Yu, and Jiaxuan You. 2024.
\newblock \href {http://arxiv.org/abs/2409.15454} {In-context learning may not elicit trustworthy reasoning: A-not-b errors in pretrained language models}.

\bibitem[{Han et~al.(2022)Han, Schoelkopf, Zhao, Qi, Riddell, Benson, Sun, Zubova, Qiao, Burtell et~al.}]{han2022folio}
Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, et~al. 2022.
\newblock Folio: Natural language reasoning with first-order logic.
\newblock \emph{arXiv e-prints}, pages arXiv--2209.

\bibitem[{Hansson(2004)}]{hansson2004fallacies}
Sven~Ove Hansson. 2004.
\newblock Fallacies of risk.
\newblock \emph{Journal of Risk Research}, 7(3):353--360.

\bibitem[{Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendryckstest2021}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021{\natexlab{a}}.
\newblock Measuring massive multitask language understanding.
\newblock \emph{Proceedings of the International Conference on Learning Representations (ICLR)}.

\bibitem[{Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt}]{hendrycksmath2021}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021{\natexlab{b}}.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock \emph{NeurIPS}.

\bibitem[{Ho et~al.(2023)Ho, Schmid, and Yun}]{ho2023large}
Namgyu Ho, Laura Schmid, and Se-Young Yun. 2023.
\newblock \href {http://arxiv.org/abs/2212.10071} {Large language models are reasoning teachers}.

\bibitem[{Hodel and West(2023)}]{hodel2023response}
Damian Hodel and Jevin West. 2023.
\newblock \href {http://arxiv.org/abs/2308.16118} {Response: Emergent analogical reasoning in large language models}.

\bibitem[{Hong et~al.(2024)Hong, Zhang, Pang, Yu, and Zhang}]{hong-etal-2024-closer}
Ruixin Hong, Hongming Zhang, Xinyu Pang, Dong Yu, and Changshui Zhang. 2024.
\newblock \href {https://doi.org/10.18653/v1/2024.naacl-long.52} {A closer look at the self-verification abilities of large language models in logical reasoning}.
\newblock In \emph{Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 900--925, Mexico City, Mexico. Association for Computational Linguistics.

\bibitem[{Hu et~al.(2024)Hu, Gao, Gao, Chen, and Huang}]{hu2024largelanguagemodelslimited}
Peng Hu, Changjiang Gao, Ruiqi Gao, Jiajun Chen, and Shujian Huang. 2024.
\newblock \href {http://arxiv.org/abs/2406.07393} {Large language models are limited in out-of-context knowledge reasoning}.

\bibitem[{Huang et~al.(2024)Huang, Chen, Mishra, Zheng, Yu, Song, and Zhou}]{huang2024large}
Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu~Steven Zheng, Adams~Wei Yu, Xinying Song, and Denny Zhou. 2024.
\newblock \href {https://openreview.net/forum?id=IkmD3fKBPQ} {Large language models cannot self-correct reasoning yet}.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Hume(1748)}]{hume1748enquiry}
David Hume. 1748.
\newblock An enquiry concerning human understanding (section iv).
\newblock \emph{Recuperado de http://www. clorenzano. com. ar}.

\bibitem[{Jiang et~al.(2024{\natexlab{a}})Jiang, Xie, Hao, Wang, Mallick, Su, Taylor, and Roth}]{jiang2024peektokenbiaslarge}
Bowen Jiang, Yangxinyu Xie, Zhuoqun Hao, Xiaomeng Wang, Tanwi Mallick, Weijie~J. Su, Camillo~J. Taylor, and Dan Roth. 2024{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2406.11050} {A peek into token bias: Large language models are not yet genuine reasoners}.

\bibitem[{Jiang et~al.(2024{\natexlab{b}})Jiang, Yan, Liu, Jin, Peng, Zhang, Cai, Cao, Gao, and Tang}]{jiang2024logicproimprovingcomplexlogical}
Jin Jiang, Yuchen Yan, Yang Liu, Yonggang Jin, Shuai Peng, Mengdi Zhang, Xunliang Cai, Yixin Cao, Liangcai Gao, and Zhi Tang. 2024{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2409.12929} {Logicpro: Improving complex logical reasoning via program-guided learning}.

\bibitem[{Kahneman(2011)}]{kahneman2011thinking}
Daniel Kahneman. 2011.
\newblock \emph{Thinking, fast and slow}.
\newblock Macmillan.

\bibitem[{Lanham et~al.(2023)Lanham, Chen, Radhakrishnan, Steiner, Denison, Hernandez, Li, Durmus, Hubinger, Kernion, Lukošiūtė, Nguyen, Cheng, Joseph, Schiefer, Rausch, Larson, McCandlish, Kundu, Kadavath, Yang, Henighan, Maxwell, Telleen-Lawton, Hume, Hatfield-Dodds, Kaplan, Brauner, Bowman, and Perez}]{lanham2023measuring}
Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamilė Lukošiūtė, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner, Samuel~R. Bowman, and Ethan Perez. 2023.
\newblock \href {http://arxiv.org/abs/2307.13702} {Measuring faithfulness in chain-of-thought reasoning}.

\bibitem[{Li et~al.(2023)Li, Hessel, Yu, Ren, Chang, and Choi}]{li-etal-2023-symbolic}
Liunian~Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.acl-long.150} {Symbolic chain-of-thought distillation: Small models can also {``}think{''} step-by-step}.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 2665--2679, Toronto, Canada. Association for Computational Linguistics.

\bibitem[{Li et~al.(2022)Li, Chen, Shen, Chen, Zhang, Li, Wang, Qian, Peng, Mao, Chen, and Yan}]{li2022explanations}
Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi~Mao, Wenhu Chen, and Xifeng Yan. 2022.
\newblock \href {http://arxiv.org/abs/2210.06726} {Explanations from large language models make small reasoners better}.

\bibitem[{Liu et~al.(2023{\natexlab{a}})Liu, Liu, Cui, Teng, Duan, Zhou, and Zhang}]{logiqa2}
Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, and Yue Zhang. 2023{\natexlab{a}}.
\newblock \href {https://doi.org/10.1109/TASLP.2023.3293046} {Logiqa 2.0—an improved dataset for logical reasoning in natural language understanding}.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 31:2947--2962.

\bibitem[{Liu et~al.(2023{\natexlab{b}})Liu, Ning, Teng, Liu, Zhou, and Zhang}]{liu2023evaluating}
Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. 2023{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2304.03439} {Evaluating the logical reasoning ability of chatgpt and gpt-4}.

\bibitem[{Liu et~al.(2023{\natexlab{c}})Liu, Teng, Cui, Zhang, Zhou, and Zhang}]{liu-etal-2023-logicot}
Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, and Yue Zhang. 2023{\natexlab{c}}.
\newblock \href {https://doi.org/10.18653/v1/2023.findings-emnlp.191} {{L}ogi{C}o{T}: Logical chain-of-thought instruction tuning}.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 2908--2921, Singapore. Association for Computational Linguistics.

\bibitem[{Liu et~al.(2020)Liu, Cui, Liu, Huang, Wang, and Zhang}]{ijcai2020p501}
Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020.
\newblock \href {https://doi.org/10.24963/ijcai.2020/501} {Logiqa: A challenge dataset for machine reading comprehension with logical reasoning}.
\newblock In \emph{Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, {IJCAI-20}}, pages 3622--3628. International Joint Conferences on Artificial Intelligence Organization.
\newblock Main track.

\bibitem[{Liu et~al.(2023{\natexlab{d}})Liu, Xia, Wang, and Zhang}]{evalplus}
Jiawei Liu, Chunqiu~Steven Xia, Yuyao Wang, and Lingming Zhang. 2023{\natexlab{d}}.
\newblock \href {https://openreview.net/forum?id=1qvx610Cu7} {Is your code generated by chat{GPT} really correct? rigorous evaluation of large language models for code generation}.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov}]{liu-et-al-2019-roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
\newblock {RoBERTa}: A robustly optimized {BERT} pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}.

\bibitem[{Liu et~al.(2024)Liu, Lee, Du, Sanyal, and Zhao}]{liu2024selfcontradictoryreasoningevaluationdetection}
Ziyi Liu, Isabelle Lee, Yongkang Du, Soumya Sanyal, and Jieyu Zhao. 2024.
\newblock \href {http://arxiv.org/abs/2311.09603} {Self-contradictory reasoning evaluation and detection}.

\bibitem[{Lu et~al.(2024)Lu, Zhou, Ren, Wang, Shi, Pan, Zhan, and Li}]{lu2024mathgenie}
Zimu Lu, Aojun Zhou, Houxing Ren, Ke~Wang, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. 2024.
\newblock \href {http://arxiv.org/abs/2402.16352} {Mathgenie: Generating synthetic data with question back-translation for enhancing mathematical reasoning of llms}.

\bibitem[{MA et~al.(2024)MA, Liu, Yu, Zhang, Jiang, Wang, and Li}]{ma2024at}
YINGWEI MA, Yue Liu, Yue Yu, Yuanliang Zhang, Yu~Jiang, Changjian Wang, and Shanshan Li. 2024.
\newblock \href {https://openreview.net/forum?id=KIPJKST4gw} {At which training stage does code data help {LLM}s reasoning?}
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Magister et~al.(2023)Magister, Mallinson, Adamek, Malmi, and Severyn}]{magister-etal-2023-teaching}
Lucie~Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.acl-short.151} {Teaching small language models to reason}.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pages 1773--1781, Toronto, Canada. Association for Computational Linguistics.

\bibitem[{McCarthy(1959)}]{Mccarthy1959ProgramsWC}
John~W. McCarthy. 1959.
\newblock Programs with common sense.
\newblock In \emph{Proc. Tedding Conf. on the Mechanization of Thought Processes}, pages 75--91.

\bibitem[{Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal}]{OpenBookQA2018}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering.
\newblock In \emph{EMNLP}.

\bibitem[{Miller(1995)}]{miller1995wordnet}
George~A Miller. 1995.
\newblock Wordnet: a lexical database for english.
\newblock \emph{Communications of the ACM}, 38(11):39--41.

\bibitem[{Mirzadeh et~al.(2024)Mirzadeh, Alizadeh, Shahrokhi, Tuzel, Bengio, and Farajtabar}]{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical}
Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. 2024.
\newblock \href {http://arxiv.org/abs/2410.05229} {Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models}.

\bibitem[{Mitchell(2023)}]{melanie2023blog}
Melanie Mitchell. 2023.
\newblock Can large language models reason?
\newblock \emph{blog}, pages https://aiguide.substack.com/p/can--large--language--models--reason.

\bibitem[{Mitra et~al.(2023)Mitra, Corro, Mahajan, Codas, Simoes, Agarwal, Chen, Razdaibiedina, Jones, Aggarwal, Palangi, Zheng, Rosset, Khanpour, and Awadallah}]{mitra2023orca}
Arindam Mitra, Luciano~Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah. 2023.
\newblock \href {http://arxiv.org/abs/2311.11045} {Orca 2: Teaching small language models how to reason}.

\bibitem[{Mondorf and Plank(2024)}]{mondorf2024liarliarlogicalmire}
Philipp Mondorf and Barbara Plank. 2024.
\newblock \href {http://arxiv.org/abs/2406.12546} {Liar, liar, logical mire: A benchmark for suppositional reasoning in large language models}.

\bibitem[{Morishita et~al.(2023)Morishita, Morio, Yamaguchi, and Sogawa}]{pmlr-v202-morishita23a}
Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, and Yasuhiro Sogawa. 2023.
\newblock \href {https://proceedings.mlr.press/v202/morishita23a.html} {Learning deductive reasoning from synthetic corpus based on formal logic}.
\newblock In \emph{Proceedings of the 40th International Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 25254--25274. PMLR.

\bibitem[{Morishita et~al.(2024)Morishita, Yamaguchi, Morio, Tomonari, Imaichi, and Sogawa}]{morishita-etal-2024-jfld}
Terufumi Morishita, Atsuki Yamaguchi, Gaku Morio, Hikaru Tomonari, Osamu Imaichi, and Yasuhiro Sogawa. 2024.
\newblock \href {https://aclanthology.org/2024.lrec-main.832} {{JFLD}: A {J}apanese benchmark for deductive reasoning based on formal logic}.
\newblock In \emph{Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)}, pages 9526--9535, Torino, Italia. ELRA and ICCL.

\bibitem[{Nafar et~al.(2024)Nafar, Venable, and Kordjamshidi}]{nafar-etal-2024-teaching}
Aliakbar Nafar, K.~Brent Venable, and Parisa Kordjamshidi. 2024.
\newblock \href {https://aclanthology.org/2024.findings-eacl.112} {Teaching probabilistic logical reasoning to transformers}.
\newblock In \emph{Findings of the Association for Computational Linguistics: EACL 2024}, pages 1615--1632, St. Julian{'}s, Malta. Association for Computational Linguistics.

\bibitem[{Ozeki et~al.(2024)Ozeki, Ando, Morishita, Abe, Mineshima, and Okada}]{ozeki-etal-2024-exploring}
Kentaro Ozeki, Risako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, and Mitsuhiro Okada. 2024.
\newblock \href {https://doi.org/10.18653/v1/2024.findings-acl.950} {Exploring reasoning biases in large language models through syllogism: Insights from the {N}eu{BAROCO} dataset}.
\newblock In \emph{Findings of the Association for Computational Linguistics ACL 2024}, pages 16063--16077, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics.

\bibitem[{Paglieri(2017)}]{Paglieri2017}
Fabio Paglieri. 2017.
\newblock \href {https://doi.org/10.1007/s13347-016-0222-6} {A plea for ecological argument technologies}.
\newblock \emph{Philosophy {\&} Technology}, 30(2):209--238.

\bibitem[{Parmar et~al.(2024)Parmar, Patel, Varshney, Nakamura, Luo, Mashetty, Mitra, and Baral}]{parmar-etal-2024-logicbench}
Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, and Chitta Baral. 2024.
\newblock \href {https://doi.org/10.18653/v1/2024.acl-long.739} {{L}ogic{B}ench: Towards systematic evaluation of logical reasoning ability of large language models}.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 13679--13707, Bangkok, Thailand. Association for Computational Linguistics.

\bibitem[{Patel et~al.(2024)Patel, Kulkarni, Parmar, Budhiraja, Nakamura, Varshney, and Baral}]{patel2024multilogievalevaluatingmultisteplogical}
Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, and Chitta Baral. 2024.
\newblock \href {http://arxiv.org/abs/2406.17169} {Multi-logieval: Towards evaluating multi-step logical reasoning ability of large language models}.

\bibitem[{Pi et~al.(2022)Pi, Liu, Chen, Ziyadi, Lin, Fu, Gao, Lou, and Chen}]{pi-etal-2022-reasoning}
Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Qiang Fu, Yan Gao, Jian-Guang Lou, and Weizhu Chen. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.emnlp-main.48} {Reasoning like program executors}.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 761--779, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

\bibitem[{Quine(1969)}]{quine1969epistemology}
Willard Van~Orman Quine. 1969.
\newblock Epistemology naturalized. ontological relativity and other essays.
\newblock \emph{New York: Columbia UP}.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever}]{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.
\newblock Language models are unsupervised multitask learners.

\bibitem[{Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young et~al.}]{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et~al. 2021.
\newblock Scaling language models: Methods, analysis \& insights from training gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}.

\bibitem[{Rajpurkar et~al.(2018)Rajpurkar, Jia, and Liang}]{rajpurkar-etal-2018-know}
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
\newblock \href {https://doi.org/10.18653/v1/P18-2124} {Know what you don{'}t know: Unanswerable questions for {SQ}u{AD}}.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pages 784--789, Melbourne, Australia. Association for Computational Linguistics.

\bibitem[{Razeghi et~al.(2022)Razeghi, Logan~IV, Gardner, and Singh}]{razeghi2022impact}
Yasaman Razeghi, Robert~L Logan~IV, Matt Gardner, and Sameer Singh. 2022.
\newblock Impact of pretraining term frequencies on few-shot numerical reasoning.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2022}, pages 840--854.

\bibitem[{Rein et~al.(2023)Rein, Hou, Stickland, Petty, Pang, Dirani, Michael, and Bowman}]{rein2023gpqa}
David Rein, Betty~Li Hou, Asa~Cooper Stickland, Jackson Petty, Richard~Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel~R Bowman. 2023.
\newblock Gpqa: A graduate-level google-proof q\&a benchmark.
\newblock \emph{arXiv preprint arXiv:2311.12022}.

\bibitem[{Robinson and Wingate(2023)}]{joshua2023mcq}
Joshua Robinson and David Wingate. 2023.
\newblock \href {https://openreview.net/forum?id=yKbprarjc5B} {Leveraging large language models for multiple choice question answering}.
\newblock In \emph{The Eleventh International Conference on Learning Representations}.

\bibitem[{Saeed et~al.(2021)Saeed, Ahmadi, Nakov, and Papotti}]{saeed-etal-2021-rulebert}
Mohammed Saeed, Naser Ahmadi, Preslav Nakov, and Paolo Papotti. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.emnlp-main.110} {{R}ule{BERT}: Teaching soft rules to pre-trained language models}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 1460--1476, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

\bibitem[{Saha et~al.(2020)Saha, Ghosh, Srivastava, and Bansal}]{saha-etal-2020-prover}
Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, and Mohit Bansal. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.emnlp-main.9} {{PR}over: Proof generation for interpretable reasoning over rules}.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 122--136, Online. Association for Computational Linguistics.

\bibitem[{Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and Choi}]{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64(9):99--106.

\bibitem[{Sanyal et~al.(2022{\natexlab{a}})Sanyal, Liao, and Ren}]{sanyal2022robustlr}
Soumya Sanyal, Zeyi Liao, and Xiang Ren. 2022{\natexlab{a}}.
\newblock Robustlr: Evaluating robustness to logical perturbation in deductive reasoning.
\newblock \emph{arXiv preprint arXiv:2205.12598}.

\bibitem[{Sanyal et~al.(2022{\natexlab{b}})Sanyal, Singh, and Ren}]{sanyal2022fairr}
Soumya Sanyal, Harman Singh, and Xiang Ren. 2022{\natexlab{b}}.
\newblock Fairr: Faithful and robust deductive reasoning over natural language.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 1075--1093.

\bibitem[{Shortliffe(1976)}]{shortliffe1976computer}
eh~Shortliffe. 1976.
\newblock Computer based medical consultations: Mycin.
\newblock \emph{Elsevier}.

\bibitem[{Shridhar et~al.(2023)Shridhar, Stolfo, and Sachan}]{shridhar-etal-2023-distilling}
Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.findings-acl.441} {Distilling reasoning capabilities into smaller language models}.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pages 7059--7073, Toronto, Canada. Association for Computational Linguistics.

\bibitem[{Sileo(2024)}]{sileo2024scalingsyntheticlogicalreasoning}
Damien Sileo. 2024.
\newblock \href {http://arxiv.org/abs/2406.11035} {Scaling synthetic logical reasoning datasets with context-sensitive declarative grammars}.

\bibitem[{Sprague et~al.(2024)Sprague, Ye, Bostrom, Chaudhuri, and Durrett}]{sprague2024musr}
Zayne~Rea Sprague, Xi~Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. 2024.
\newblock \href {https://openreview.net/forum?id=jenyYQzue1} {Mu{SR}: Testing the limits of chain-of-thought with multistep soft reasoning}.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Sunstein and Hastie(2015)}]{SunsteinHastie2015}
Cass~R Sunstein and Reid Hastie. 2015.
\newblock \emph{Wiser: getting beyond groupthink to make groups smarter}.
\newblock Harvard Business Review Press, Boston.

\bibitem[{Suzgun et~al.(2022)Suzgun, Scales, Sch{\"a}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou, , and Wei}]{suzgun2022challenging}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"a}rli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, , and Jason Wei. 2022.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve them.
\newblock \emph{arXiv preprint arXiv:2210.09261}.

\bibitem[{Tafjord et~al.(2021)Tafjord, Dalvi, and Clark}]{tafjord-etal-2021-proofwriter}
Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.findings-acl.317} {{P}roof{W}riter: Generating implications, proofs, and abductive statements over natural language}.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}, pages 3621--3634, Online. Association for Computational Linguistics.

\bibitem[{Talmor et~al.(2018)Talmor, Herzig, Lourie, and Berant}]{talmor2018commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018.
\newblock Commonsenseqa: A question answering challenge targeting commonsense knowledge.
\newblock \emph{arXiv preprint arXiv:1811.00937}.

\bibitem[{Tian et~al.(2021)Tian, Li, Chen, Xiao, He, and Jin}]{tian2021diagnosing}
Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, and Yaohui Jin. 2021.
\newblock Diagnosing the first-order logical reasoning ability through logicnli.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 3738--3747.

\bibitem[{Trinh et~al.(2024)Trinh, Wu, Le, He, and Luong}]{trinh2024solving}
Trieu~H Trinh, Yuhuai Wu, Quoc~V Le, He~He, and Thang Luong. 2024.
\newblock Solving olympiad geometry without human demonstrations.
\newblock \emph{Nature}, 625(7995):476--482.

\bibitem[{Turpin et~al.(2023)Turpin, Michael, Perez, and Bowman}]{turpin2023language}
Miles Turpin, Julian Michael, Ethan Perez, and Samuel~R. Bowman. 2023.
\newblock \href {http://arxiv.org/abs/2305.04388} {Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting}.

\bibitem[{Uchiyama et~al.(2024)Uchiyama, Kojima, Gambardella, Cao, Iwasawa, and Matsuo}]{uchiyama2024programminglanguagefeaturespretraining}
Fumiya Uchiyama, Takeshi Kojima, Andrew Gambardella, Qi~Cao, Yusuke Iwasawa, and Yutaka Matsuo. 2024.
\newblock \href {http://arxiv.org/abs/2410.06735} {Which programming language and what features at pre-training stage affect downstream logical inference performance?}

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~30.

\bibitem[{Wan et~al.(2024)Wan, Wang, Yang, Yuan, tse Huang, He, Jiao, and Lyu}]{wan2024logicaskerevaluatingimprovinglogical}
Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen tse Huang, Pinjia He, Wenxiang Jiao, and Michael~R. Lyu. 2024.
\newblock \href {http://arxiv.org/abs/2401.00757} {Logicasker: Evaluating and improving the logical reasoning ability of large language models}.

\bibitem[{Wang et~al.(2024{\natexlab{a}})Wang, Zhao, Qiang, Qin, and Liu}]{wang2024answers}
Haochun Wang, Sendong Zhao, Zewen Qiang, Bing Qin, and Ting Liu. 2024{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2402.01349} {Beyond the answers: Reviewing the rationality of multiple choice question answering for the evaluation of large language models}.

\bibitem[{Wang et~al.(2023)Wang, Wang, Li, Gao, Yin, and Ren}]{wang-etal-2023-scott}
Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.acl-long.304} {{SCOTT}: Self-consistent chain-of-thought distillation}.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 5546--5558, Toronto, Canada. Association for Computational Linguistics.

\bibitem[{Wang et~al.(2024{\natexlab{b}})Wang, Wei, Choi, and Ren}]{wang-etal-2024-llms}
Siyuan Wang, Zhongyu Wei, Yejin Choi, and Xiang Ren. 2024{\natexlab{b}}.
\newblock \href {https://doi.org/10.18653/v1/2024.acl-long.406} {Can {LLM}s reason with rules? logic scaffolding for stress-testing and improving {LLM}s}.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 7523--7543, Bangkok, Thailand. Association for Computational Linguistics.

\bibitem[{Wang et~al.(2024{\natexlab{c}})Wang, Ma, Zhang, Ni, Chandra, Guo, Ren, Arulraj, He, Jiang, Li, Ku, Wang, Zhuang, Fan, Yue, and Chen}]{wang2024mmluprorobustchallengingmultitask}
Yubo Wang, Xueguang Ma, Ge~Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024{\natexlab{c}}.
\newblock \href {http://arxiv.org/abs/2406.01574} {Mmlu-pro: A more robust and challenging multi-task language understanding benchmark (published at neurips 2024 track datasets and benchmarks)}.

\bibitem[{Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, brian ichter, Xia, Chi, Le, and Zhou}]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed~H. Chi, Quoc~V Le, and Denny Zhou. 2022.
\newblock \href {https://openreview.net/forum?id=_VjQlMeSB_J} {Chain of thought prompting elicits reasoning in large language models}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Weizenbaum(1966)}]{weizenbaum1966eliza}
Joseph Weizenbaum. 1966.
\newblock Eliza—a computer program for the study of natural language communication between man and machine.
\newblock \emph{Communications of the ACM}, 9(1):36--45.

\bibitem[{Welbl et~al.(2017)Welbl, Liu, and Gardner}]{welbl-etal-2017-crowdsourcing}
Johannes Welbl, Nelson~F. Liu, and Matt Gardner. 2017.
\newblock \href {https://doi.org/10.18653/v1/W17-4413} {Crowdsourcing multiple choice science questions}.
\newblock In \emph{Proceedings of the 3rd Workshop on Noisy User-generated Text}, pages 94--106, Copenhagen, Denmark. Association for Computational Linguistics.

\bibitem[{Weston et~al.(2015)Weston, Bordes, Chopra, Rush, Van~Merri{\"e}nboer, Joulin, and Mikolov}]{weston2015towards}
Jason Weston, Antoine Bordes, Sumit Chopra, Alexander~M Rush, Bart Van~Merri{\"e}nboer, Armand Joulin, and Tomas Mikolov. 2015.
\newblock Towards ai-complete question answering: A set of prerequisite toy tasks.
\newblock \emph{arXiv preprint arXiv:1502.05698}.

\bibitem[{Williams et~al.(2018)Williams, Nangia, and Bowman}]{williams2018broad}
Adina Williams, Nikita Nangia, and Samuel~R Bowman. 2018.
\newblock A broad-coverage challenge corpus for sentence understanding through inference.
\newblock In \emph{Proceedings of NAACL-HLT}, pages 1112--1122.

\bibitem[{Winograd(1971)}]{winograd1971procedures}
T~Winograd. 1971.
\newblock Procedures as a representation for data in a computer program for understanding natural language, mit ai technical report 235.

\bibitem[{Wittgenstein(1922)}]{wittgenstein1922tractatus}
Ludwig Wittgenstein. 1922.
\newblock \emph{Tractatus Logico Philosophicus: Logical-Philosophical Treatise}.
\newblock Really Simple Media.

\bibitem[{Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu, Le~Scao, Gugger, Drame, Lhoest, and Rush}]{wolf-et-al-2019-huggingface}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le~Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Empirical Methods in Natural Language Processing: System Demonstrations}, pages 38--45.

\bibitem[{Wu et~al.(2023)Wu, Qiu, Ross, Akyürek, Chen, Wang, Kim, Andreas, and Kim}]{wu2023reasoning}
Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. 2023.
\newblock \href {http://arxiv.org/abs/2307.02477} {Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks}.

\bibitem[{Yanaka et~al.(2019)Yanaka, Mineshima, Bekki, Inui, Sekine, Abzianidze, and Bos}]{yanaka2019help}
Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, Kentaro Inui, Satoshi Sekine, Lasha Abzianidze, and Johan Bos. 2019.
\newblock Help: A dataset for identifying shortcomings of neural models in monotonicity reasoning.
\newblock \emph{arXiv preprint arXiv:1904.12166}.

\bibitem[{Young et~al.(2022)Young, Bao, Bensemann, and Witbrock}]{young2022abductionrules}
Nathan Young, Qiming Bao, Joshua Bensemann, and Michael~J Witbrock. 2022.
\newblock Abductionrules: Training transformers to explain unexpected inputs.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2022}, pages 218--227.

\bibitem[{Yu et~al.(2020)Yu, Jiang, Dong, and Feng}]{yu2020reclor}
Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020.
\newblock Reclor: A reading comprehension dataset requiring logical reasoning.
\newblock In \emph{International Conference on Learning Representations (ICLR)}.

\bibitem[{Yuan et~al.(2023)Yuan, Hu, Vuli{\'c}, Korhonen, and Meng}]{yuan2023can}
Zhangdie Yuan, Songbo Hu, Ivan Vuli{\'c}, Anna Korhonen, and Zaiqiao Meng. 2023.
\newblock Can pretrained language models (yet) reason deductively?
\newblock In \emph{Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics}, pages 1439--1454.

\bibitem[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi}]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 4791--4800.

\bibitem[{Zhang et~al.(2022)Zhang, Li, Meng, Chang, and den Broeck}]{zhang2022paradox}
Honghua Zhang, Liunian~Harold Li, Tao Meng, Kai-Wei Chang, and Guy~Van den Broeck. 2022.
\newblock \href {http://arxiv.org/abs/2205.11502} {On the paradox of learning to reason from data}.

\bibitem[{Zhang et~al.(2024)Zhang, Da, Lee, Robinson, Wu, Song, Zhao, Raja, Slack, Lyu, Hendryx, Kaplan, Lunati, and Yue}]{zhang2024careful}
Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, and Summer Yue. 2024.
\newblock \href {http://arxiv.org/abs/2405.00332} {A careful examination of large language model performance on grade school arithmetic}.

\bibitem[{Zhao et~al.(2024{\natexlab{a}})Zhao, Tong, Mou, Zhang, Zhang, and Huang}]{zhao2024exploringcompositionaldeficiencylarge}
Jun Zhao, Jingqi Tong, Yurong Mou, Ming Zhang, Qi~Zhang, and Xuanjing Huang. 2024{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2405.06680} {Exploring the compositional deficiency of large language models in mathematical reasoning}.

\bibitem[{Zhao et~al.(2024{\natexlab{b}})Zhao, Chiu, Hwang, Brahman, Hessel, Choudhury, Choi, Li, and Suhr}]{zhao-etal-2024-uncommonsense}
Wenting Zhao, Justin Chiu, Jena Hwang, Faeze Brahman, Jack Hessel, Sanjiban Choudhury, Yejin Choi, Xiang Li, and Alane Suhr. 2024{\natexlab{b}}.
\newblock \href {https://doi.org/10.18653/v1/2024.naacl-long.469} {{UN}commonsense reasoning: Abductive reasoning about uncommon situations}.
\newblock In \emph{Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 8487--8505, Mexico City, Mexico. Association for Computational Linguistics.

\bibitem[{Zheng et~al.(2024)Zheng, Zhou, Meng, Zhou, and Huang}]{chuijie2024mcq}
Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2024.
\newblock \href {https://openreview.net/forum?id=shr9PXz7T0} {Large language models are not robust multiple choice selectors}.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Zhong et~al.(2021)Zhong, Wang, Tang, Xu, Guo, Wang, Yin, Zhou, and Duan}]{zhong2021ar}
Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming Zhou, and Nan Duan. 2021.
\newblock Ar-lsat: Investigating analytical reasoning of text.
\newblock \emph{arXiv preprint arXiv:2104.06598}.

\bibitem[{Zhou et~al.(2024{\natexlab{a}})Zhou, Staats, Li, Szegedy, Weinberger, and Wu}]{zhou2024dont}
Jin~Peng Zhou, Charles~E Staats, Wenda Li, Christian Szegedy, Kilian~Q Weinberger, and Yuhuai Wu. 2024{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=V5tdi14ple} {Don't trust: Verify -- grounding {LLM} quantitative reasoning with autoformalization}.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Zhou et~al.(2024{\natexlab{b}})Zhou, Zhu, Antognini, Kim, and Zhang}]{zhou-etal-2024-paraphrase}
Yue Zhou, Yada Zhu, Diego Antognini, Yoon Kim, and Yang Zhang. 2024{\natexlab{b}}.
\newblock \href {https://doi.org/10.18653/v1/2024.naacl-long.153} {Paraphrase and solve: Exploring and exploiting the impact of surface form on mathematical reasoning in large language models}.
\newblock In \emph{Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 2793--2804, Mexico City, Mexico. Association for Computational Linguistics.

\bibitem[{Zhu et~al.(2024)Zhu, Chen, Wang, Gong, Yang, and Xie}]{zhu2024dyval}
Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil~Zhenqiang Gong, Diyi Yang, and Xing Xie. 2024.
\newblock \href {https://openreview.net/forum?id=gjfOL9z5Xr} {Dyval: Dynamic evaluation of large language models for reasoning tasks}.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\end{thebibliography}
