@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020},
  organization={PMLR}
}

@inproceedings{shamir2013complexity,
  title={On the complexity of bandit and derivative-free stochastic convex optimization},
  author={Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={3--24},
  year={2013},
  organization={PMLR}
}

@article{agarwal2020flambe,
  title={Flambe: Structural complexity and representation learning of low rank mdps},
  author={Agarwal, Alekh and Kakade, Sham and Krishnamurthy, Akshay and Sun, Wen},
  journal={arXiv preprint arXiv:2006.10814},
  year={2020}
}

@article{modi2021model,
  title={Model-free representation learning and exploration in low-rank MDPs},
  author={Modi, Aditya and Chen, Jinglin and Krishnamurthy, Akshay and Jiang, Nan and Agarwal, Alekh},
  journal={arXiv preprint arXiv:2102.07035},
  year={2021}
}

@inproceedings{modi2020sample,
  title={Sample complexity of reinforcement learning using linearly combined model ensembles},
  author={Modi, Aditya and Jiang, Nan and Tewari, Ambuj and Singh, Satinder},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2010--2020},
  year={2020},
  organization={PMLR}
}

@article{huang2022towards,
  title={Towards Deployment-Efficient Reinforcement Learning: Lower Bound and Optimality},
  author={Huang, Jiawei and Chen, Jinglin and Zhao, Li and Qin, Tao and Jiang, Nan and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2202.06450},
  year={2022}
}

@article{wu2021gap,
  title={Gap-Dependent Unsupervised Exploration for Reinforcement Learning},
  author={Wu, Jingfeng and Braverman, Vladimir and Yang, Lin F},
  journal={arXiv preprint arXiv:2108.05439},
  year={2021}
}

@article{zhang2021reward,
  title={Reward-Free Model-Based Reinforcement Learning with Linear Function Approximation},
  author={Zhang, Weitong and Zhou, Dongruo and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@misc{tsybakov2009introduction,
  title={Introduction to Nonparametric Estimation.},
  author={Tsybakov, Alexandre B},
  journal={Springer series in statistics},
  pages={I--XII},
  year={2009},
  publisher={Springer}
}

@inproceedings{hao2021online,
  title={Online Sparse Reinforcement Learning},
  author={Hao, Botao and Lattimore, Tor and Szepesv{\'a}ri, Csaba and Wang, Mengdi},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={316--324},
  year={2021},
  organization={PMLR}
}

@article{agarwal2021online,
  title={Online target q-learning with reverse experience replay: Efficiently finding the optimal policy for linear mdps},
  author={Agarwal, Naman and Chaudhuri, Syomantak and Jain, Prateek and Nagaraj, Dheeraj and Netrapalli, Praneeth},
  journal={arXiv preprint arXiv:2110.08440},
  year={2021}
}

@article{vershynin2010introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Vershynin, Roman},
  journal={arXiv preprint arXiv:1011.3027},
  year={2010}
}


@article{wagenmaker2021first,
  title={First-Order Regret in Reinforcement Learning with Linear Function Approximation: A Robust Estimation Approach},
  author={Wagenmaker, Andrew and Chen, Yifang and Simchowitz, Max and Du, Simon S and Jamieson, Kevin},
  journal={arXiv preprint arXiv:2112.03432},
  year={2021}
}


@article{gao2021provably,
  title={A provably efficient algorithm for linear markov decision process with low switching cost},
  author={Gao, Minbo and Xie, Tianle and Du, Simon S and Yang, Lin F},
  journal={arXiv preprint arXiv:2101.00494},
  year={2021}
}


@article{ok2018exploration,
  title={Exploration in structured reinforcement learning},
  author={Ok, Jungseul and Proutiere, Alexandre and Tranos, Damianos},
  journal={arXiv preprint arXiv:1806.00775},
  year={2018}
}

@article{zhang2021variance,
  title={Variance-Aware Confidence Set: Variance-Dependent Bound for Linear Bandits and Horizon-Free Bound for Linear Mixture MDP},
  author={Zhang, Zihan and Yang, Jiaqi and Ji, Xiangyang and Du, Simon S},
  journal={arXiv preprint arXiv:2101.12745},
  year={2021}
}

@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}

@article{he2020logarithmic,
  title={Logarithmic regret for reinforcement learning with linear function approximation},
  author={He, Jiafan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:2011.11566},
  year={2020}
}

@inproceedings{tarbouriech2019active,
  title={Active exploration in markov decision processes},
  author={Tarbouriech, Jean and Lazaric, Alessandro},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={974--982},
  year={2019},
  organization={PMLR}
}

@article{abbasi2011improved,
  title={Improved algorithms for linear stochastic bandits},
  author={Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
  journal={Advances in neural information processing systems},
  volume={24},
  pages={2312--2320},
  year={2011}
}


@article{even2006action,
  title={Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems.},
  author={Even-Dar, Eyal and Mannor, Shie and Mansour, Yishay and Mahadevan, Sridhar},
  journal={Journal of machine learning research},
  volume={7},
  number={6},
  year={2006}
}


@article{zanette2019almost,
  title={Almost horizon-free structure-aware best policy identification with a generative model},
  author={Zanette, Andrea and Kochenderfer, Mykel and Brunskill, Emma},
  year={2019}
}


@article{simchowitz2019non,
  title={Non-asymptotic gap-dependent regret bounds for tabular MDPs},
  author={Simchowitz, Max and Jamieson, Kevin},
  journal={arXiv preprint arXiv:1905.03814},
  year={2019}
}

@article{khamaru2021instance,
  title={Instance-optimality in optimal value estimation: Adaptivity via variance-reduced Q-learning},
  author={Khamaru, Koulik and Xia, Eric and Wainwright, Martin J and Jordan, Michael I},
  journal={arXiv preprint arXiv:2106.14352},
  year={2021}
}

@article{tirinzoni2020asymptotically,
  title={An Asymptotically Optimal Primal-Dual Incremental Algorithm for Contextual Linear Bandits},
  author={Tirinzoni, Andrea and Pirotta, Matteo and Restelli, Marcello and Lazaric, Alessandro},
  journal={arXiv preprint arXiv:2010.12247},
  year={2020}
}

@article{agarwal2020flambe,
  title={Flambe: Structural complexity and representation learning of low rank mdps},
  author={Agarwal, Alekh and Kakade, Sham and Krishnamurthy, Akshay and Sun, Wen},
  journal={arXiv preprint arXiv:2006.10814},
  year={2020}
}

@article{wang2020reward,
  title={On reward-free reinforcement learning with linear function approximation},
  author={Wang, Ruosong and Du, Simon S and Yang, Lin F and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2006.11274},
  year={2020}
}

@article{zhang2020nearly,
  title={Nearly Minimax Optimal Reward-free Reinforcement Learning},
  author={Zhang, Zihan and Du, Simon S and Ji, Xiangyang},
  journal={arXiv preprint arXiv:2010.05901},
  year={2020}
}

@article{du2019provably,
  title={Provably Efficient $ Q $-learning with Function Approximation via Distribution Shift Error Checking Oracle},
  author={Du, Simon S and Luo, Yuping and Wang, Ruosong and Zhang, Hanrui},
  journal={arXiv preprint arXiv:1906.06321},
  year={2019}
}


@inproceedings{lattimore2012pac,
  title={PAC bounds for discounted MDPs},
  author={Lattimore, Tor and Hutter, Marcus},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={320--334},
  year={2012},
  organization={Springer}
}

@article{dann2015sample,
  title={Sample complexity of episodic fixed-horizon reinforcement learning},
  author={Dann, Christoph and Brunskill, Emma},
  journal={arXiv preprint arXiv:1510.08906},
  year={2015}
}

@article{jin2021bellman,
  title={Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms},
  author={Jin, Chi and Liu, Qinghua and Miryoosefi, Sobhan},
  journal={arXiv preprint arXiv:2102.00815},
  year={2021}
}


@article{zhou2020nearly,
  title={Nearly Minimax Optimal Reinforcement Learning for Linear Mixture Markov Decision Processes},
  author={Zhou, Dongruo and Gu, Quanquan and Szepesvari, Csaba},
  journal={arXiv preprint arXiv:2012.08507},
  year={2020}
}


@article{khamaru2020temporal,
  title={Is temporal difference learning optimal? an instance-dependent analysis},
  author={Khamaru, Koulik and Pananjady, Ashwin and Ruan, Feng and Wainwright, Martin J and Jordan, Michael I},
  journal={arXiv preprint arXiv:2003.07337},
  year={2020}
}


@inproceedings{agarwal2020model,
  title={Model-based reinforcement learning with a generative model is minimax optimal},
  author={Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
  booktitle={Conference on Learning Theory},
  pages={67--83},
  year={2020},
  organization={PMLR}
}


@article{kaufmann2016complexity,
  title={On the complexity of best-arm identification in multi-armed bandit models},
  author={Kaufmann, Emilie and Capp{\'e}, Olivier and Garivier, Aur{\'e}lien},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1--42},
  year={2016},
  publisher={JMLR. org}
}


@article{marjani2020best,
  title={Best Policy Identification in discounted MDPs: Problem-specific Sample Complexity},
  author={Marjani, Aymen Al and Proutiere, Alexandre},
  journal={arXiv preprint arXiv:2009.13405},
  year={2020}
}




@article{dann2017unifying,
  title={Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning},
  author={Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  journal={arXiv preprint arXiv:1703.07710},
  year={2017}
}

@inproceedings{yin2020asymptotically,
  title={Asymptotically efficient off-policy evaluation for tabular reinforcement learning},
  author={Yin, Ming and Wang, Yu-Xiang},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3948--3958},
  year={2020},
  organization={PMLR}
}

@article{xie2019towards,
  title={Towards optimal off-policy evaluation for reinforcement learning with marginalized importance sampling},
  author={Xie, Tengyang and Ma, Yifei and Wang, Yu-Xiang},
  journal={arXiv preprint arXiv:1906.03393},
  year={2019}
}

@inproceedings{jiang2016doubly,
  title={Doubly robust off-policy value evaluation for reinforcement learning},
  author={Jiang, Nan and Li, Lihong},
  booktitle={International Conference on Machine Learning},
  pages={652--661},
  year={2016},
  organization={PMLR}
}

@article{kallus2019efficiently,
  title={Efficiently breaking the curse of horizon in off-policy evaluation with double reinforcement learning},
  author={Kallus, Nathan and Uehara, Masatoshi},
  journal={arXiv preprint arXiv:1909.05850},
  year={2019}
}

@article{zhang2020reinforcement,
  title={Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon},
  author={Zhang, Zihan and Ji, Xiangyang and Du, Simon S},
  journal={arXiv preprint arXiv:2009.13503},
  year={2020}
}

@article{xu2021fine,
  title={Fine-Grained Gap-Dependent Bounds for Tabular MDPs via Adaptive Multi-Step Bootstrap},
  author={Xu, Haike and Ma, Tengyu and Du, Simon S},
  journal={arXiv preprint arXiv:2102.04692},
  year={2021}
}



@inproceedings{jin2020reward,
  title={Reward-free exploration for reinforcement learning},
  author={Jin, Chi and Krishnamurthy, Akshay and Simchowitz, Max and Yu, Tiancheng},
  booktitle={International Conference on Machine Learning},
  pages={4870--4879},
  year={2020},
  organization={PMLR}
}

@phdthesis{kakade2003sample,
  title={On the sample complexity of reinforcement learning},
  author={Kakade, Sham Machandranath},
  year={2003},
  school={UCL (University College London)}
}

@inproceedings{zanette2019tighter,
  title={Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds},
  author={Zanette, Andrea and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={7304--7312},
  year={2019},
  organization={PMLR}
}

@article{maurer2009empirical,
  title={Empirical Bernstein bounds and sample variance penalization},
  author={Maurer, Andreas and Pontil, Massimiliano},
  journal={arXiv preprint arXiv:0907.3740},
  year={2009}
}



%%% Potential duplicates start here

@article{kearns2002near,
  title={Near-optimal reinforcement learning in polynomial time},
  author={Kearns, Michael and Singh, Satinder},
  journal={Machine learning},
  volume={49},
  number={2},
  pages={209--232},
  year={2002},
  publisher={Springer}
}


@article{li2011knows,
  title={Knows what it knows: a framework for self-aware learning},
  author={Li, Lihong and Littman, Michael L and Walsh, Thomas J and Strehl, Alexander L},
  journal={Machine learning},
  volume={82},
  number={3},
  pages={399--443},
  year={2011},
  publisher={Springer}
}

@inproceedings{dann2019policy,
  title={Policy certificates: Towards accountable reinforcement learning},
  author={Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={1507--1516},
  year={2019},
  organization={PMLR}
}



@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}

@inproceedings{domingues2021episodic,
  title={Episodic reinforcement learning in finite MDPs: Minimax lower bounds revisited},
  author={Domingues, Omar Darwiche and M{\'e}nard, Pierre and Kaufmann, Emilie and Valko, Michal},
  booktitle={Algorithmic Learning Theory},
  pages={578--598},
  year={2021},
  organization={PMLR}
}

@article{menard2020fast,
  title={Fast active learning for pure exploration in reinforcement learning},
  author={M{\'e}nard, Pierre and Domingues, Omar Darwiche and Jonsson, Anders and Kaufmann, Emilie and Leurent, Edouard and Valko, Michal},
  journal={arXiv preprint arXiv:2007.13442},
  year={2020}
}

@inproceedings{jin2018q,
  title={Is Q-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={4868--4878},
  year={2018}
}



@inproceedings{jiang2018open,
  title={Open problem: The dependence of sample complexity lower bounds on planning horizon},
  author={Jiang, Nan and Agarwal, Alekh},
  booktitle={Conference On Learning Theory},
  pages={3395--3398},
  year={2018},
  organization={PMLR}
}

@article{wang2020long,
  title={Is Long Horizon Reinforcement Learning More Difficult Than Short Horizon Reinforcement Learning?},
  author={Wang, Ruosong and Du, Simon S and Yang, Lin F and Kakade, Sham M},
  journal={arXiv preprint arXiv:2005.00527},
  year={2020}
}



@article{azar2013minimax,
  title={Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model},
  author={Azar, Mohammad Gheshlaghi and Munos, R{\'e}mi and Kappen, Hilbert J},
  journal={Machine learning},
  volume={91},
  number={3},
  pages={325--349},
  year={2013},
  publisher={Springer}
}

@article{sidford2018near,
  title={Near-optimal time and sample complexities for solving discounted Markov decision process with a generative model},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin F and Ye, Yinyu},
  journal={arXiv preprint arXiv:1806.01492},
  year={2018}
}

@article{li2020breaking,
  title={Breaking the sample size barrier in model-based reinforcement learning with a generative model},
  author={Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}



@inproceedings{zanette2019generative,
 author = {Zanette, Andrea and Kochenderfer, Mykel J and Brunskill, Emma},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Almost Horizon-Free Structure-Aware Best Policy Identification with a Generative Model},
 volume = {32},
 year = {2019}
}



@article{jonsson2020planning,
  title={Planning in markov decision processes with gap-dependent sample complexity},
  author={Jonsson, Anders and Kaufmann, Emilie and M{\'e}nard, Pierre and Domingues, Omar Darwiche and Leurent, Edouard and Valko, Michal},
  journal={arXiv preprint arXiv:2006.05879},
  year={2020}
}


@article{marjani2021navigating,
  title={Navigating to the Best Policy in Markov Decision Processes},
  author={Marjani, Aymen Al and Garivier, Aur{\'e}lien and Proutiere, Alexandre},
  journal={arXiv preprint arXiv:2106.02847},
  year={2021}
}


@article{wagenmaker2021task,
  title={Task-Optimal Exploration in Linear Dynamical Systems},
  author={Wagenmaker, Andrew and Simchowitz, Max and Jamieson, Kevin},
  journal={arXiv preprint arXiv:2102.05214},
  year={2021}
}


@inproceedings{zimin2013online,
  title={Online learning in episodic Markovian decision processes by relative entropy policy search},
  author={Zimin, Alexander and Neu, Gergely},
  booktitle={Neural Information Processing Systems 26},
  year={2013}
}

@article{tarbouriech2020provably,
  title={A Provably Efficient Sample Collection Strategy for Reinforcement Learning},
  author={Tarbouriech, Jean and Pirotta, Matteo and Valko, Michal and Lazaric, Alessandro},
  journal={arXiv preprint arXiv:2007.06437},
  year={2020}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@article{foster2021statistical,
  title={The Statistical Complexity of Interactive Decision Making},
  author={Foster, Dylan J and Kakade, Sham M and Qian, Jian and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:2112.13487},
  year={2021}
}


@article{brafman2002r,
  title={R-max-a general polynomial time algorithm for near-optimal reinforcement learning},
  author={Brafman, Ronen I and Tennenholtz, Moshe},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Oct},
  pages={213--231},
  year={2002}
}


@inproceedings{yang2019sample,
  title={Sample-optimal parametric Q-learning using linearly additive features},
  author={Yang, Lin and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={6995--7004},
  year={2019},
  organization={PMLR}
}


@article{wang2019optimism,
  title={Optimism in reinforcement learning with generalized linear function approximation},
  author={Wang, Yining and Wang, Ruosong and Du, Simon S and Krishnamurthy, Akshay},
  journal={arXiv preprint arXiv:1912.04136},
  year={2019}
}

@article{du2019good,
  title={Is a good representation sufficient for sample efficient reinforcement learning?},
  author={Du, Simon S and Kakade, Sham M and Wang, Ruosong and Yang, Lin F},
  journal={arXiv preprint arXiv:1910.03016},
  year={2019}
}


@inproceedings{zanette2020frequentist,
  title={Frequentist regret bounds for randomized least-squares value iteration},
  author={Zanette, Andrea and Brandfonbrener, David and Brunskill, Emma and Pirotta, Matteo and Lazaric, Alessandro},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1954--1964},
  year={2020},
  organization={PMLR}
}

@inproceedings{zanette2020learning,
  title={Learning near optimal policies with low inherent bellman error},
  author={Zanette, Andrea and Lazaric, Alessandro and Kochenderfer, Mykel and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={10978--10989},
  year={2020},
  organization={PMLR}
}

@inproceedings{ayoub2020model,
  title={Model-based reinforcement learning with value-targeted regression},
  author={Ayoub, Alex and Jia, Zeyu and Szepesvari, Csaba and Wang, Mengdi and Yang, Lin},
  booktitle={International Conference on Machine Learning},
  pages={463--474},
  year={2020},
  organization={PMLR}
}

@inproceedings{weisz2021exponential,
  title={Exponential lower bounds for planning in mdps with linearly-realizable optimal action-value functions},
  author={Weisz, Gell{\'e}rt and Amortila, Philip and Szepesv{\'a}ri, Csaba},
  booktitle={Algorithmic Learning Theory},
  pages={1237--1264},
  year={2021},
  organization={PMLR}
}

@inproceedings{zhou2021provably,
  title={Provably efficient reinforcement learning for discounted mdps with feature mapping},
  author={Zhou, Dongruo and He, Jiafan and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={12793--12802},
  year={2021},
  organization={PMLR}
}

@article{du2021bilinear,
  title={Bilinear classes: A structural framework for provable generalization in rl},
  author={Du, Simon S and Kakade, Sham M and Lee, Jason D and Lovett, Shachar and Mahajan, Gaurav and Sun, Wen and Wang, Ruosong},
  journal={arXiv preprint arXiv:2103.10897},
  year={2021}
}

@inproceedings{jiang2017contextual,
  title={Contextual decision processes with low Bellman rank are PAC-learnable},
  author={Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
  booktitle={International Conference on Machine Learning},
  pages={1704--1713},
  year={2017},
  organization={PMLR}
}

@article{wang2021exponential,
  title={An exponential lower bound for linearly-realizable MDPs with constant suboptimality gap},
  author={Wang, Yuanhao and Wang, Ruosong and Kakade, Sham M},
  journal={arXiv preprint arXiv:2103.12690},
  year={2021}
}

@inproceedings{jia2020model,
  title={Model-based reinforcement learning with value-targeted regression},
  author={Jia, Zeyu and Yang, Lin and Szepesvari, Csaba and Wang, Mengdi},
  booktitle={Learning for Dynamics and Control},
  pages={666--686},
  year={2020},
  organization={PMLR}
}

@article{wagenmaker2021beyond,
  title={Beyond No Regret: Instance-Dependent PAC Reinforcement Learning},
  author={Wagenmaker, Andrew and Simchowitz, Max and Jamieson, Kevin},
  journal={arXiv preprint arXiv:2108.02717},
  year={2021}
}

@inproceedings{jin2020reward,
  title={Reward-free exploration for reinforcement learning},
  author={Jin, Chi and Krishnamurthy, Akshay and Simchowitz, Max and Yu, Tiancheng},
  booktitle={International Conference on Machine Learning},
  pages={4870--4879},
  year={2020},
  organization={PMLR}
}

@article{zanette2020provably,
  title={Provably efficient reward-agnostic navigation with linear value iteration},
  author={Zanette, Andrea and Lazaric, Alessandro and Kochenderfer, Mykel J and Brunskill, Emma},
  journal={arXiv preprint arXiv:2008.07737},
  year={2020}
}

@article{mannor2004sample,
  title={The sample complexity of exploration in the multi-armed bandit problem},
  author={Mannor, Shie and Tsitsiklis, John N},
  journal={Journal of Machine Learning Research},
  volume={5},
  number={Jun},
  pages={623--648},
  year={2004}
}


@article{freedman1975tail,
  title={On tail probabilities for martingales},
  author={Freedman, David A},
  journal={the Annals of Probability},
  pages={100--118},
  year={1975},
  publisher={JSTOR}
}


@article{lugosi2019mean,
  title={Mean estimation and regression under heavy-tailed distributions: A survey},
  author={Lugosi, G{\'a}bor and Mendelson, Shahar},
  journal={Foundations of Computational Mathematics},
  volume={19},
  number={5},
  pages={1145--1190},
  year={2019},
  publisher={Springer}
}


@inproceedings{wei2020taking,
  title={Taking a hint: How to leverage loss predictors in contextual bandits?},
  author={Wei, Chen-Yu and Luo, Haipeng and Agarwal, Alekh},
  booktitle={Conference on Learning Theory},
  pages={3583--3634},
  year={2020},
  organization={PMLR}
}

