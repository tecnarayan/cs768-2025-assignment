@misc{zanella2017informed,
	archiveprefix = {arXiv},
	author = {Giacomo Zanella},
	date-added = {2023-06-05 14:41:29 -0500},
	date-modified = {2023-06-05 14:41:37 -0500},
	eprint = {1711.07424},
	keywords = {locally balanced},
	primaryclass = {stat.CO},
	title = {Informed proposals for local MCMC in discrete spaces},
	year = {2017}}

@article{durmus2018efficient,
  title={Efficient bayesian computation by proximal markov chain monte carlo: when langevin meets moreau},
  author={Durmus, Alain and Moulines, Eric and Pereyra, Marcelo},
  journal={SIAM Journal on Imaging Sciences},
  volume={11},
  number={1},
  pages={473--506},
  year={2018},
  publisher={SIAM}
}

@article{dalalyan2017theoretical,
  title={Theoretical guarantees for approximate sampling from smooth and log-concave densities},
  author={Dalalyan, Arnak S},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={79},
  number={3},
  pages={651--676},
  year={2017},
  publisher={Oxford University Press}
}

@article{vats2019multivariate,
  title={Multivariate output analysis for Markov chain Monte Carlo},
  author={Vats, Dootika and Flegal, James M and Jones, Galin L},
  journal={Biometrika},
  volume={106},
  number={2},
  pages={321--337},
  year={2019},
  publisher={Oxford University Press}
}

@inproceedings{Zhang2020Cyclical,
	author = {Ruqi Zhang and Chunyuan Li and Jianyi Zhang and Changyou Chen and Andrew Gordon Wilson},
	booktitle = {International Conference on Learning Representations},
	date-added = {2023-06-05 14:40:34 -0500},
	date-modified = {2023-06-05 14:40:44 -0500},
	keywords = {cyclical, mcmc},
	title = {Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning},
	url = {https://openreview.net/forum?id=rkeS1RVtPS},
	year = {2020},
	bdsk-url-1 = {https://openreview.net/forum?id=rkeS1RVtPS}}

@inproceedings{sun2023anyscale,
	author = {Haoran Sun and Bo Dai and Charles Sutton and Dale Schuurmans and Hanjun Dai},
	booktitle = {The Eleventh International Conference on Learning Representations},
	date-added = {2023-06-05 14:39:58 -0500},
	date-modified = {2023-06-05 14:40:11 -0500},
	keywords = {anyscale},
	title = {Any-scale Balanced Samplers for Discrete Space},
	url = {https://openreview.net/forum?id=lEkl0jdSb7B},
	year = {2023},
	bdsk-url-1 = {https://openreview.net/forum?id=lEkl0jdSb7B}}

@misc{zhang2022langevinlike,
	archiveprefix = {arXiv},
	author = {Ruqi Zhang and Xingchao Liu and Qiang Liu},
	date-added = {2023-06-05 14:39:26 -0500},
	date-modified = {2023-06-05 14:39:34 -0500},
	eprint = {2206.09914},
	keywords = {DLP},
	primaryclass = {cs.LG},
	title = {A Langevin-like Sampler for Discrete Distributions},
	year = {2022}}

@misc{frazier2018tutorial,
      title={A Tutorial on Bayesian Optimization}, 
      author={Peter I. Frazier},
      year={2018},
      eprint={1807.02811},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@Misc{bayesOpt, 
    author = {Fernando Nogueira},
    title = {{Bayesian Optimization}: Open source constrained global optimization tool for {Python}},
    year = {2014--},
    url = " https://github.com/fmfn/BayesianOptimization"
}


@article{grathwohl2021gwg,
	author       = {Will Grathwohl and
	Kevin Swersky and
	Milad Hashemi and
	David Duvenaud and
	Chris J. Maddison},
	title        = {Oops {I} Took {A} Gradient: Scalable Sampling for Discrete Distributions},
	journal      = {CoRR},
	volume       = {abs/2102.04509},
	year         = {2021},
	url          = {https://arxiv.org/abs/2102.04509},
	eprinttype    = {arXiv},
	eprint       = {2102.04509},
	timestamp    = {Thu, 18 Feb 2021 15:26:00 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2102-04509.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@Article{robertsstramer2002langevinmh,
	author={Roberts, G. O.
	and Stramer, O.},
	title={Langevin Diffusions and Metropolis-Hastings Algorithms},
	journal={Methodology And Computing In Applied Probability},
	year={2002},
	month={Dec},
	day={01},
	volume={4},
	number={4},
	pages={337-357},
	abstract={We consider a class of Langevin diffusions with state-dependent volatility. The volatility of the diffusion is chosen so as to make the stationary distribution of the diffusion with respect to its natural clock, a heated version of the stationary density of interest. The motivation behind this construction is the desire to construct uniformly ergodic diffusions with required stationary densities. Discrete time algorithms constructed by Hastings accept reject mechanisms are constructed from discretisations of the algorithms, and the properties of these algorithms are investigated.},
	issn={1573-7713},
	doi={10.1023/A:1023562417138},
	url={https://doi.org/10.1023/A:1023562417138}
}

@misc{rhodes2022enhanced,
      title={Enhanced gradient-based MCMC in discrete spaces}, 
      author={Benjamin Rhodes and Michael Gutmann},
      year={2022},
      eprint={2208.00040},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{sun2022optimal,
      title={Optimal Scaling for Locally Balanced Proposals in Discrete Spaces}, 
      author={Haoran Sun and Hanjun Dai and Dale Schuurmans},
      year={2022},
      eprint={2209.08183},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{deng2020contour,
  title={A contour stochastic gradient langevin dynamics algorithm for simulations of multi-modal distributions},
  author={Deng, Wei and Lin, Guang and Liang, Faming},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={15725--15736},
  year={2020}
}

@inproceedings{deng2020non,
  title={Non-convex learning via replica exchange stochastic gradient mcmc},
  author={Deng, Wei and Feng, Qi and Gao, Liyao and Liang, Faming and Lin, Guang},
  booktitle={International Conference on Machine Learning},
  pages={2474--2483},
  year={2020},
  organization={PMLR}
}

@inproceedings{sun2021path,
  title={Path auxiliary proposal for MCMC in discrete space},
  author={Sun, Haoran and Dai, Hanjun and Xia, Wei and Ramamurthy, Arun},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{sun2023discrete,
  title={Discrete Langevin Samplers via Wasserstein Gradient Flow},
  author={Sun, Haoran and Dai, Hanjun and Dai, Bo and Zhou, Haomin and Schuurmans, Dale},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={6290--6313},
  year={2023},
  organization={PMLR}
}

@inproceedings{xiang2023efficient,
  title={Efficient Informed Proposals for Discrete Distributions via Newtonâ€™s Series Approximation},
  author={Xiang, Yue and Zhu, Dongyao and Lei, Bowen and Xu, Dongkuan and Zhang, Ruqi},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={7288--7310},
  year={2023},
  organization={PMLR}
}

@inproceedings{goshvadi2023discs,
  title={DISCS: A Benchmark for Discrete Sampling},
  author={Goshvadi, Katayoon and Sun, Haoran and Liu, Xingchao and Nova, Azade and Zhang, Ruqi and Grathwohl, Will Sussman and Schuurmans, Dale and Dai, Hanjun},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2023}
}

@inproceedings{tieleman2008training,
  title={Training restricted Boltzmann machines using approximations to the likelihood gradient},
  author={Tieleman, Tijmen},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1064--1071},
  year={2008}
}

@article{du2019implicit,
  title={Implicit generation and modeling with energy based models},
  author={Du, Yilun and Mordatch, Igor},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{neal2001annealed,
  title={Annealed importance sampling},
  author={Neal, Radford M},
  journal={Statistics and computing},
  volume={11},
  pages={125--139},
  year={2001},
  publisher={Springer}
}

@article{ruder2016overview,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1609.04747},
  year={2016}
}

@inproceedings{ziyin2021sgd,
  title={Sgd can converge to local maxima},
  author={Ziyin, Liu and Li, Botao and Simon, James B and Ueda, Masahito},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{swendsen1986replica,
  title={Replica Monte Carlo simulation of spin-glasses},
  author={Swendsen, Robert H and Wang, Jian-Sheng},
  journal={Physical review letters},
  volume={57},
  number={21},
  pages={2607},
  year={1986},
  publisher={APS}
}

@article{berg1991multicanonical,
  title={Multicanonical algorithms for first order phase transitions},
  author={Berg, Bernd A and Neuhaus, Thomas},
  journal={Physics Letters B},
  volume={267},
  number={2},
  pages={249--253},
  year={1991},
  publisher={Elsevier}
}

@article{marinari1992simulated,
  title={Simulated tempering: a new Monte Carlo scheme},
  author={Marinari, Enzo and Parisi, Giorgio},
  journal={Europhysics letters},
  volume={19},
  number={6},
  pages={451},
  year={1992},
  publisher={IOP Publishing}
}

@article{swendsen1987nonuniversal,
  title={Nonuniversal critical dynamics in Monte Carlo simulations},
  author={Swendsen, Robert H and Wang, Jian-Sheng},
  journal={Physical review letters},
  volume={58},
  number={2},
  pages={86},
  year={1987},
  publisher={APS}
}

@article{wolff1989collective,
  title={Collective Monte Carlo updating for spin systems},
  author={Wolff, Ulli},
  journal={Physical Review Letters},
  volume={62},
  number={4},
  pages={361},
  year={1989},
  publisher={APS}
}

@inproceedings{sansone2022lsb,
  title={Lsb: Local self-balancing mcmc in discrete spaces},
  author={Sansone, Emanuele},
  booktitle={International Conference on Machine Learning},
  pages={19205--19220},
  year={2022},
  organization={PMLR}
}



@article{hinton2002training,
  title={Training products of experts by minimizing contrastive divergence},
  author={Hinton, Geoffrey E},
  journal={Neural computation},
  volume={14},
  number={8},
  pages={1771--1800},
  year={2002},
  publisher={MIT Press}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}
%%%
@article{trench1999asymptotic,
  title={Asymptotic distribution of the spectra of a class of generalized Kac--Murdock--Szeg{\"o} matrices},
  author={Trench, William F},
  journal={Linear algebra and its applications},
  volume={294},
  number={1-3},
  pages={181--192},
  year={1999},
  publisher={Elsevier}
}
%%%
@article{jones2004markov,
  title={On the Markov chain central limit theorem},
  author={Jones, Galin L},
  year={2004}
}
%%%
@inproceedings{zhang2022langevin,
  title={A Langevin-like sampler for discrete distributions},
  author={Zhang, Ruqi and Liu, Xingchao and Liu, Qiang},
  booktitle={International Conference on Machine Learning},
  pages={26375--26396},
  year={2022},
  organization={PMLR}
}
%%%%%%
@misc{roy2022convergence,
      title={Convergence of position-dependent MALA with application to conditional simulation in GLMMs}, 
      author={Vivekananda Roy and Lijin Zhang},
      year={2022},
      eprint={2108.12662},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}
%%%
@article{MAL-070,
url = {http://dx.doi.org/10.1561/2200000070},
year = {2018},
volume = {11},
journal = {Foundations and TrendsÂ® in Machine Learning},
title = {A Tutorial on Thompson Sampling},
doi = {10.1561/2200000070},
issn = {1935-8237},
number = {1},
pages = {1-96},
author = {Daniel J. Russo and Benjamin Van Roy and Abbas Kazerouni and Ian Osband and Zheng Wen}
}
%%%
%%%
@article{WANG2021110134,
title = {Bayesian sparse learning with preconditioned stochastic gradient MCMC and its applications},
journal = {Journal of Computational Physics},
volume = {432},
pages = {110134},
year = {2021},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2021.110134},
url = {https://www.sciencedirect.com/science/article/pii/S0021999121000267},
author = {Yating Wang and Wei Deng and Guang Lin},
keywords = {Bayesian sparse learning, Preconditioned stochastic gradient MCMC, Deep learning, Deep neural network, Adaptive hierarchical posterior, Stochastic approximation},
abstract = {Deep neural networks have been successfully employed in an extensive variety of research areas, including solving partial differential equations. Despite its significant success, there are some challenges in effectively training DNN, such as avoiding overfitting in over-parameterized DNNs and accelerating the optimization in DNNs with pathological curvature. In this work, we propose a Bayesian type sparse deep learning algorithm. The algorithm utilizes a set of spike-and-slab priors for the parameters in the deep neural network. The hierarchical Bayesian mixture will be trained using an adaptive empirical method. That is, one will alternatively sample from the posterior using preconditioned stochastic gradient Langevin Dynamics (PSGLD), and optimize the latent variables via stochastic approximation. The sparsity of the network is achieved while optimizing the hyperparameters with adaptive searching and penalizing. A popular SG-MCMC approach is Stochastic gradient Langevin dynamics (SGLD). However, considering the complex geometry in the model parameter space in nonconvex learning, updating parameters using a universal step size in each component as in SGLD may cause slow mixing. To address this issue, we apply a computationally manageable preconditioner in the updating rule, which provides a step-size parameter to adapt to local geometric properties. Moreover, by smoothly optimizing the hyperparameter in the preconditioning matrix, our proposed algorithm ensures a decreasing bias, which is introduced by ignoring the correction term in the preconditioned SGLD. According to the existing theoretical framework, we show that the proposed algorithm can asymptotically converge to the correct distribution with a controllable bias under mild conditions. Numerical tests are performed on both synthetic regression problems and learning solutions of elliptic PDE, which demonstrate the accuracy and efficiency of the present work.}
}
%%%
%%%
@article{10.2307/41057430,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/41057430},
 abstract = {The paper proposes Metropolis adjusted Langevin and Hamiltonian Monte Carlo sampling methods defined on the Riemann manifold to resolve the shortcomings of existing Monte Carlo algorithms when sampling from target densities that may be high dimensional and exhibit strong correlations. The methods provide fully automated adaptation mechanisms that circumvent the costly pilot runs that are required to tune proposal densities for Metropolis-Hastings or indeed Hamiltonian Monte Carlo and Metropolis adjusted Langevin algorithms. This allows for highly efficient sampling even in very high dimensions where different scalings may be required for the transient and stationary phases of the Markov chain. The methodology proposed exploits the Riemann geometry of the parameter space of statistical models and thus automatically adapts to the local structure when simulating paths across this manifold, providing highly efficient convergence and exploration of the target density. The performance of these Riemann manifold Monte Carlo methods is rigorously assessed by performing inference on logistic regression models, log-Gaussian Cox point processes, stochastic volatility models and Bayesian estimation of dynamic systems described by non-linear differential equations. Substantial improvements in the time-normalized effective sample size are reported when compared with alternative sampling approaches. MATLAB code that is available from www.ucl.ac.uk/statistics/research/rmhmc allows replication of all the results reported.},
 author = {Mark Girolami and Ben Calderhead},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {2},
 pages = {123--214},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Riemann manifold Langevin and Hamiltonian Monte Carlo methods},
 urldate = {2022-05-27},
 volume = {73},
 year = {2011}
}
%%%
%%%
@book{aries2013statistics,
  title={Statistics of Random Processes I: General Theory},
  author={Aries, A.B. and Liptser, R.S. and Shiryaev, A.N.},
  isbn={9781475716658},
  lccn={76049817},
  series={Stochastic Modelling and Applied Probability},
  url={https://books.google.com/books?id=UavVBwAAQBAJ},
  year={2013},
  publisher={Springer New York}
}
%%%%
@article{li2017preconditioned,
  title={Preconditioned stochastic gradient descent},
  author={Li, Xi-Lin},
  journal={IEEE transactions on neural networks and learning systems},
  volume={29},
  number={5},
  pages={1454--1466},
  year={2017},
  publisher={IEEE}
}
%%%
@inproceedings{hendrikx2020statistically,
  title={Statistically preconditioned accelerated gradient method for distributed optimization},
  author={Hendrikx, Hadrien and Xiao, Lin and Bubeck, Sebastien and Bach, Francis and Massoulie, Laurent},
  booktitle={International Conference on Machine Learning},
  pages={4203--4227},
  year={2020},
  organization={PMLR}
}
%%%%
@inproceedings{bolley2005weighted,
  title={Weighted Csisz{\'a}r-Kullback-Pinsker inequalities and applications to transportation inequalities},
  author={Bolley, Fran{\c{c}}ois and Villani, C{\'e}dric},
  booktitle={Annales de la Facult{\'e} des sciences de Toulouse: Math{\'e}matiques},
  volume={14},
  number={3},
  pages={331--352},
  year={2005}
}
%%%
@inproceedings{NIPS2015_9a440050,
 author = {Ma, Yi-An and Chen, Tianqi and Fox, Emily},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Complete Recipe for Stochastic Gradient MCMC},
 url = {https://proceedings.neurips.cc/paper/2015/file/9a4400501febb2a95e79248486a5f6d3-Paper.pdf},
 volume = {28},
 year = {2015}
}
%%%
@article{durmus2019high,
  title={High-dimensional Bayesian inference via the unadjusted Langevin algorithm},
  author={Durmus, Alain and Moulines, Eric},
  journal={Bernoulli},
  volume={25},
  number={4A},
  pages={2854--2882},
  year={2019},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}
%%%
@article{roberts1996exponential,
  title={Exponential convergence of Langevin distributions and their discrete approximations},
  author={Roberts, Gareth O and Tweedie, Richard L},
  journal={Bernoulli},
  pages={341--363},
  year={1996},
  publisher={JSTOR}
}
%%%
@article{roberts2002langevin,
  title={Langevin diffusions and Metropolis-Hastings algorithms},
  author={Roberts, Gareth O and Stramer, Osnat},
  journal={Methodology and computing in applied probability},
  volume={4},
  number={4},
  pages={337--357},
  year={2002},
  publisher={Springer}
}
%%%
@article{durmus2016sampling,
  title={Sampling from strongly log-concave distributions with the Unadjusted Langevin Algorithm},
  author={Durmus, Alain and Moulines, Eric},
  journal={arXiv preprint arXiv:1605.01559},
  year={2016}
}
%%%
@inproceedings{brosse2017sampling,
  title={Sampling from a log-concave distribution with compact support with proximal Langevin Monte Carlo},
  author={Brosse, Nicolas and Durmus, Alain and Moulines, {\'E}ric and Pereyra, Marcelo},
  booktitle={Conference on learning theory},
  pages={319--342},
  year={2017},
  organization={PMLR}
}
%%%
@article{dalalyan2020sampling,
  title={On sampling from a log-concave density using kinetic Langevin diffusions},
  author={Dalalyan, Arnak S and Riou-Durand, Lionel},
  journal={Bernoulli},
  volume={26},
  number={3},
  pages={1956--1988},
  year={2020},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}
%%%
@article{polyak1992acceleration,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM journal on control and optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}
%%%%
x`
%%%%
@book{billingsley2013convergence,
  title={Convergence of probability measures},
  author={Billingsley, Patrick},
  year={2013},
  publisher={John Wiley \& Sons}
}
%%%%%
@book{oksendal2013stochastic,
  title={Stochastic differential equations: an introduction with applications},
  author={Oksendal, Bernt},
  year={2013},
  publisher={Springer Science \& Business Media}
}
%%%%%%
@book{liptser2013statistics,
  title={Statistics of random processes II: Applications},
  author={Liptser, Robert S and Shiryaev, Albert N},
  volume={6},
  year={2013},
  publisher={Springer Science \& Business Media}
}
%%%%%%%
@book{durrett2019probability,
  title={Probability: theory and examples},
  author={Durrett, Rick},
  volume={49},
  year={2019},
  publisher={Cambridge university press}
}
%%%%%%
@misc{wu2020noisy,
      title={On the Noisy Gradient Descent that Generalizes as SGD}, 
      author={Jingfeng Wu and Wenqing Hu and Haoyi Xiong and Jun Huan and Vladimir Braverman and Zhanxing Zhu},
      year={2020},
      eprint={1906.07405},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
%%%%%%
@misc{hu2018diffusion,
      title={On the diffusion approximation of nonconvex stochastic gradient descent}, 
      author={Wenqing Hu and Chris Junchi Li and Lei Li and Jian-Guo Liu},
      year={2018},
      eprint={1705.07562},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
%%%%%%
@article{bottou1991stochastic,
  title={Stochastic gradient learning in neural networks},
  author={Bottou, L{\'e}on and others},
  journal={Proceedings of Neuro-N{\i}mes},
  volume={91},
  number={8},
  pages={12},
  year={1991},
  publisher={Nimes}
}
%%%%%%
@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={Siam Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}
%%%%%%%%%%%
@article{cao2019generalization,
  title={Generalization bounds of stochastic gradient descent for wide and deep neural networks},
  author={Cao, Yuan and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={10836--10846},
  year={2019}
}
%%%%%%%%%%
@inproceedings{chaudhari2018stochastic,
  title={Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks},
  author={Chaudhari, Pratik and Soatto, Stefano},
  booktitle={2018 Information Theory and Applications Workshop (ITA)},
  pages={1--10},
  year={2018},
  organization={IEEE}
}
%%%%%%%%%%
@inproceedings{daneshmand2018escaping,
  title={Escaping saddles with stochastic gradients},
  author={Daneshmand, Hadi and Kohler, Jonas and Lucchi, Aurelien and Hofmann, Thomas},
  booktitle={International Conference on Machine Learning},
  pages={1155--1164},
  year={2018},
  organization={PMLR}
}
%%%%%%%%%%%%%
@inproceedings{defossez2015averaged,
  title={Averaged least-mean-squares: Bias-variance trade-offs and optimal sampling distributions},
  author={D{\'e}fossez, Alexandre and Bach, Francis},
  booktitle={Artificial Intelligence and Statistics},
  pages={205--213},
  year={2015},
  organization={PMLR}
}
%%%%%%%%%%
@misc{heaton2018ian,
  title={Ian goodfellow, yoshua bengio, and aaron courville: Deep learning},
  author={Heaton, Jeff},
  year={2018},
  publisher={Springer}
}

%%%%%%%%
@article{zhu2018anisotropic,
  title={The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  journal={arXiv preprint arXiv:1803.00195},
  year={2018}
}
%%%%%%%%%%%%%
@book{villani2009optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric},
  volume={338},
  year={2009},
  publisher={Springer}
}
%%%%%%%%%%%%%%%%
@article{dieuleveut2017bridging,
  title={Bridging the gap between constant step size stochastic gradient descent and markov chains},
  author={Dieuleveut, Aymeric and Durmus, Alain and Bach, Francis},
  journal={arXiv preprint arXiv:1707.06386},
  year={2017}
}
%%%%%%%%%%%%%%
@book{butcher2016numerical,
  title={Numerical methods for ordinary differential equations},
  author={Butcher, John Charles},
  year={2016},
  publisher={John Wiley \& Sons}
}
%%%%%%%%%%%%%
@article{leluc2020towards,
  title={Towards asymptotic optimality with conditioned stochastic gradient descent},
  author={Leluc, R{\'e}mi and Portier, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:2006.02745},
  year={2020}
}
%%%%%%%%%%%%%%%%
@book{meyn2012markov,
  title={Markov chains and stochastic stability},
  author={Meyn, Sean P and Tweedie, Richard L},
  year={2012},
  publisher={Springer Science \& Business Media}
}
%%%%%%%%%%%%%
@misc{zhou2019sgd,
      title={SGD Converges to Global Minimum in Deep Learning via Star-convex Path}, 
      author={Yi Zhou and Junjie Yang and Huishuai Zhang and Yingbin Liang and Vahid Tarokh},
      year={2019},
      eprint={1901.00451},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
%%%%%%%%%%%
@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}
%%%%%%%%%%%%%%%%%
@article{mertikopoulos2020almost,
  title={On the almost sure convergence of stochastic gradient descent in non-convex problems},
  author={Mertikopoulos, Panayotis and Hallak, Nadav and Kavis, Ali and Cevher, Volkan},
  journal={arXiv preprint arXiv:2006.11144},
  year={2020}
}
%%%%%%%%%%%%%%%%%
@article{rosenthal1995minorization,
  title={Minorization conditions and convergence rates for Markov chain Monte Carlo},
  author={Rosenthal, Jeffrey S},
  journal={Journal of the American Statistical Association},
  volume={90},
  number={430},
  pages={558--566},
  year={1995},
  publisher={Taylor \& Francis}
}
%%%%%%%%%%%%%%%%%
@misc{jin2019nonconvex,
      title={On Nonconvex Optimization for Machine Learning: Gradients, Stochasticity, and Saddle Points}, 
      author={Chi Jin and Praneeth Netrapalli and Rong Ge and Sham M. Kakade and Michael I. Jordan},
      year={2019},
      eprint={1902.04811},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
%%%%%%%%%%%
@article{golmant2018computational,
  title={On the computational inefficiency of large batch sizes for stochastic gradient descent},
  author={Golmant, Noah and Vemuri, Nikita and Yao, Zhewei and Feinberg, Vladimir and Gholami, Amir and Rothauge, Kai and Mahoney, Michael W and Gonzalez, Joseph},
  journal={arXiv preprint arXiv:1811.12941},
  year={2018}
}
%%%%%%%%%%%
@article{toulis2017asymptotic,
  title={Asymptotic and finite-sample properties of estimators based on stochastic gradients},
  author={Toulis, Panos and Airoldi, Edoardo M},
  journal={The Annals of Statistics},
  volume={45},
  number={4},
  pages={1694--1727},
  year={2017},
  publisher={Institute of Mathematical Statistics}
}
%%%%%%%%%%%%%%
@article{mattingly2002ergodicity,
  title={Ergodicity for SDEs and approximations: locally Lipschitz vector fields and degenerate noise},
  author={Mattingly, Jonathan C and Stuart, Andrew M and Higham, Desmond J},
  journal={Stochastic processes and their applications},
  volume={101},
  number={2},
  pages={185--232},
  year={2002},
  publisher={Elsevier}
}
%%%%%%%%%%%%
@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}
%%%%%%%%%%%%%%%%
@article{hoffer2017train,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  journal={arXiv preprint arXiv:1705.08741},
  year={2017}
}
%%%%%%%%%%%%%%%%
@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}
%%%%%%%%%%%%%%%%%
@article{yu2020analysis,
  title={An analysis of constant step size sgd in the non-convex regime: Asymptotic normality and bias},
  author={Yu, Lu and Balasubramanian, Krishnakumar and Volgushev, Stanislav and Erdogdu, Murat A},
  journal={arXiv preprint arXiv:2006.07904},
  year={2020}
}
%%%%%%%%%%%%%%%%%%%%%
@misc{gatmiry2022convergence,
      title={Convergence of the Riemannian Langevin Algorithm}, 
      author={Khashayar Gatmiry and Santosh S. Vempala},
      year={2022},
      eprint={2204.10818},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
%%%%%%%%%%%%%%%%%%%%%
@inproceedings{dalalyan2017further,
  title={Further and stronger analogy between sampling and optimization: Langevin Monte Carlo and gradient descent},
  author={Dalalyan, Arnak},
  booktitle={Conference on Learning Theory},
  pages={678--689},
  year={2017},
  organization={PMLR}
}
%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%
@inproceedings{cohen2017emnist,
  title={EMNIST: Extending MNIST to handwritten letters},
  author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Van Schaik, Andre},
  booktitle={2017 international joint conference on neural networks (IJCNN)},
  pages={2921--2926},
  year={2017},
  organization={IEEE}
}
%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%

@article{clanuwat2018deep,
  title={Deep learning for classical japanese literature},
  author={Clanuwat, Tarin and Bober-Irizar, Mikel and Kitamoto, Asanobu and Lamb, Alex and Yamamoto, Kazuaki and Ha, David},
  journal={arXiv preprint arXiv:1812.01718},
  year={2018}
}
%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%
@article{lake2015human,
  title={Human-level concept learning through probabilistic program induction},
  author={Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
  journal={Science},
  volume={350},
  number={6266},
  pages={1332--1338},
  year={2015},
  publisher={American Association for the Advancement of Science}
}
%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%
@inproceedings{tomczak2018vae,
  title={VAE with a VampPrior},
  author={Tomczak, Jakub and Welling, Max},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1214--1223},
  year={2018},
  organization={PMLR}
}
%%%%%%%%%%%%%%%%%%%%%


@article{wang2019bert,
  title={BERT has a mouth, and it must speak: BERT as a Markov random field language model},
  author={Wang, Alex and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:1902.04094},
  year={2019}
}

@inproceedings{zhu2018texygen,
  title={Texygen: A benchmarking platform for text generation models},
  author={Zhu, Yaoming and Lu, Sidi and Zheng, Lei and Guo, Jiaxian and Zhang, Weinan and Wang, Jun and Yu, Yong},
  booktitle={The 41st international ACM SIGIR conference on research \& development in information retrieval},
  pages={1097--1100},
  year={2018}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@online{clanuwat2018deep,
  author       = {Tarin Clanuwat and Mikel Bober-Irizar and Asanobu Kitamoto and Alex Lamb and Kazuaki Yamamoto and David Ha},
  title        = {Deep Learning for Classical Japanese Literature},
  date         = {2018-12-03},
  year         = {2018},
  eprintclass  = {cs.CV},
  eprinttype   = {arXiv},
  eprint       = {cs.CV/1812.01718},
}

@article{cohen2017emnist,
  title={EMNIST: an extension of MNIST to handwritten letters},
  author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and van Schaik, Andr{\'e}},
  journal={arXiv preprint arXiv:1702.05373},
  year={2017}
}

@article{deng2012mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}



@article{
doi:10.1126/science.aab3050,
author = {Brenden M. Lake  and Ruslan Salakhutdinov  and Joshua B. Tenenbaum },
title = {Human-level concept learning through probabilistic program induction},
journal = {Science},
volume = {350},
number = {6266},
pages = {1332-1338},
year = {2015},
doi = {10.1126/science.aab3050},
URL = {https://www.science.org/doi/abs/10.1126/science.aab3050},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aab3050},
}


@InProceedings{pmlr-v9-marlin10a,
  title = 	 {Inductive Principles for Restricted Boltzmann Machine Learning},
  author = 	 {Marlin, Benjamin and Swersky, Kevin and Chen, Bo and Freitas, Nando},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {509--516},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/marlin10a/marlin10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/marlin10a.html}
}

@article{neal2011mcmc,
  title={MCMC using Hamiltonian dynamics},
  author={Neal, Radford M and others},
  journal={Handbook of markov chain monte carlo},
  volume={2},
  number={11},
  pages={2},
  year={2011},
  publisher={Chapman and Hall/CRC}
}
