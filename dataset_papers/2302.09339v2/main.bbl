\begin{thebibliography}{70}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdolmaleki et~al.(2018)Abdolmaleki, Springenberg, Tassa, Heess, and
  Riedmiller]{map2018}
Abdolmaleki, A., Springenberg, J.~T., Tassa, Y., Heess, R. M.~N., and
  Riedmiller, M.
\newblock Maximum a posteriori policy optimisation.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Agarwal et~al.(2021)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2021theory}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G.
\newblock On the theory of policy gradient methods: Optimality, approximation,
  and distribution shift.
\newblock \emph{J. Mach. Learn. Res.}, 22\penalty0 (98):\penalty0 1--76, 2021.

\bibitem[Auer(2002)]{auer2002using}
Auer, P.
\newblock Using confidence bounds for exploitation-exploration trade-offs.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 397--422, 2002.

\bibitem[Auer et~al.(2008)Auer, Jaksch, and Ortner]{auer2008near}
Auer, P., Jaksch, T., and Ortner, R.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 21, 2008.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  263--272, 2017.

\bibitem[Barto(2013)]{barto2013intrinsic}
Barto, A.~G.
\newblock Intrinsic motivation and reinforcement learning.
\newblock In \emph{Intrinsically motivated learning in natural and artificial
  systems}, pp.\  17--47. Springer, 2013.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,
  Saxton, and Munos]{bellemare2016unifying}
Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and
  Munos, R.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1471--1479, 2016.

\bibitem[Bellemare et~al.(2012)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare-ale}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 2012.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, Debiak, Dennison,
  Farhi, Fischer, Hashme, Hesse, et~al.]{berner2019dota}
Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C.,
  Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.06680}, 2019.

\bibitem[Bhandari \& Russo(2019)Bhandari and Russo]{bhandari2019global}
Bhandari, J. and Russo, D.
\newblock Global optimality guarantees for policy gradient methods.
\newblock \emph{arXiv preprint arXiv:1906.01786}, 2019.

\bibitem[Bhandari \& Russo(2021)Bhandari and Russo]{bhandari2021linear}
Bhandari, J. and Russo, D.
\newblock On the linear convergence of policy gradient methods for finite mdps.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  2386--2394. PMLR, 2021.

\bibitem[Burda et~al.(2018)Burda, Edwards, Storkey, and
  Klimov]{burda2018exploration}
Burda, Y., Edwards, H., Storkey, A., and Klimov, O.
\newblock Exploration by random network distillation.
\newblock \emph{arXiv preprint arXiv:1810.12894}, 2018.

\bibitem[Cover \& Thomas(2012)Cover and Thomas]{cover2012elements}
Cover, T.~M. and Thomas, J.~A.
\newblock \emph{Elements of information theory}.
\newblock John Wiley \& Sons, 2012.

\bibitem[Dabney et~al.(2020)Dabney, Ostrovski, and
  Barreto]{dabney2020temporally}
Dabney, W., Ostrovski, G., and Barreto, A.
\newblock Temporally-extended $\epsilon$-greedy exploration.
\newblock \emph{arXiv preprint arXiv:2006.01782}, 2020.

\bibitem[Dayan \& Sejnowski(1996)Dayan and Sejnowski]{dayan1996exploration}
Dayan, P. and Sejnowski, T.~J.
\newblock Exploration bonuses and dual control.
\newblock \emph{Machine Learning}, 25\penalty0 (1):\penalty0 5--22, 1996.

\bibitem[Dimitrakakis \& Ortner(2018)Dimitrakakis and
  Ortner]{dimitrakakis2018decision}
Dimitrakakis, C. and Ortner, R.
\newblock Decision making under uncertainty and reinforcement learning, 2018.

\bibitem[Dwaracherla et~al.(2022)Dwaracherla, Wen, Osband, Lu, Asghari, and
  Van~Roy]{dwaracherla2022ensembles}
Dwaracherla, V., Wen, Z., Osband, I., Lu, X., Asghari, S.~M., and Van~Roy, B.
\newblock Ensembles for uncertainty estimation: Benefits of prior functions and
  bootstrapping.
\newblock \emph{arXiv preprint arXiv:2206.03633}, 2022.

\bibitem[Eriksson \& Dimitrakakis(2019)Eriksson and
  Dimitrakakis]{eriksson2019epistemic}
Eriksson, H. and Dimitrakakis, C.
\newblock Epistemic risk-sensitive reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1906.06273}, 2019.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, et~al.]{espeholt2018impala}
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron,
  Y., Firoiu, V., Harley, T., Dunning, I., et~al.
\newblock Impala: {S}calable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In \emph{International conference on machine learning}, pp.\
  1407--1416. PMLR, 2018.

\bibitem[Eysenbach \& Levine(2019)Eysenbach and Levine]{eysenbach2019if}
Eysenbach, B. and Levine, S.
\newblock If maxent {RL} is the answer, what is the question?
\newblock \emph{arXiv preprint arXiv:1910.01913}, 2019.

\bibitem[Fortunato et~al.(2017)Fortunato, Azar, Piot, Menick, Osband, Graves,
  Mnih, Munos, Hassabis, Pietquin, et~al.]{fortunato2017noisy}
Fortunato, M., Azar, M.~G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih,
  V., Munos, R., Hassabis, D., Pietquin, O., et~al.
\newblock Noisy networks for exploration.
\newblock \emph{arXiv preprint arXiv:1706.10295}, 2017.

\bibitem[Ghavamzadeh et~al.(2015)Ghavamzadeh, Mannor, Pineau, and
  Tamar]{ghavamzadeh2015bayesian}
Ghavamzadeh, M., Mannor, S., Pineau, J., and Tamar, A.
\newblock Bayesian reinforcement learning: {A} survey.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  8\penalty0 (5-6):\penalty0 359--483, 2015.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: {O}ff-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock \emph{arXiv preprint arXiv:1801.01290}, 2018.

\bibitem[Hessel et~al.(2018)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{hessel2018rainbow}
Hessel, M., Modayil, J., Van~Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
  W., Horgan, D., Piot, B., Azar, M., and Silver, D.
\newblock Rainbow: {C}ombining improvements in deep reinforcement learning.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~32, 2018.

\bibitem[Hessel et~al.(2021)Hessel, Kroiss, Clark, Kemaev, Quan, Keck, Viola,
  and van Hasselt]{hessel2021podracer}
Hessel, M., Kroiss, M., Clark, A., Kemaev, I., Quan, J., Keck, T., Viola, F.,
  and van Hasselt, H.
\newblock Podracer architectures for scalable reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2104.06272}, 2021.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Jaksch, T., Ortner, R., and Auer, P.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Apr):\penalty0 1563--1600, 2010.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I.
\newblock Is {Q}-learning provably efficient?
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Kakade(2001)]{kakade2001natural}
Kakade, S.
\newblock A natural policy gradient.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~14, pp.\  1531--1538, 2001.

\bibitem[Kearns \& Singh(2002)Kearns and Singh]{kearns2002near}
Kearns, M. and Singh, S.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine Learning}, 49\penalty0 (2-3):\penalty0 209--232, 2002.

\bibitem[Konda \& Tsitsiklis(2003)Konda and Tsitsiklis]{konda2003onactor}
Konda, V.~R. and Tsitsiklis, J.~N.
\newblock On actor-critic algorithms.
\newblock \emph{{SIAM} {J}ournal on Control and Optimization}, 42\penalty0
  (4):\penalty0 1143--1166, 2003.

\bibitem[Lu et~al.(2021)Lu, Van~Roy, Dwaracherla, Ibrahimi, Osband, and
  Wen]{lu2021reinforcement}
Lu, X., Van~Roy, B., Dwaracherla, V., Ibrahimi, M., Osband, I., and Wen, Z.
\newblock Reinforcement learning, bit by bit.
\newblock \emph{arXiv preprint arXiv:2103.04047}, 2021.

\bibitem[Mei et~al.(2020)Mei, Xiao, Szepesvari, and Schuurmans]{mei2020global}
Mei, J., Xiao, C., Szepesvari, C., and Schuurmans, D.
\newblock On the global convergence rates of softmax policy gradient methods.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6820--6829. PMLR, July 2020.

\bibitem[Meyn(2022)]{meyn2022control}
Meyn, S.
\newblock \emph{Control Systems and Reinforcement Learning}.
\newblock Cambridge University Press, 2022.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{mnih-dqn-2015}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., Petersen,
  S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,
  D., Legg, S., and Hassabis, D.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 02 2015.
\newblock URL \url{http://dx.doi.org/10.1038/nature14236}.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning ({ICML})}, pp.\  1928--1937, 2016.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{munos2016safe}
Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Nachum et~al.(2017)Nachum, Norouzi, Xu, and
  Schuurmans]{nachum2017bridging}
Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D.
\newblock Bridging the gap between value and policy based reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2772--2782, 2017.

\bibitem[Neu et~al.(2017)Neu, Jonsson, and G{\'o}mez]{neu2017unified}
Neu, G., Jonsson, A., and G{\'o}mez, V.
\newblock A unified view of entropy-regularized markov decision processes.
\newblock \emph{arXiv preprint arXiv:1705.07798}, 2017.

\bibitem[O'Donoghue(2021)]{o2021klearning}
O'Donoghue, B.
\newblock Variational {B}ayesian reinforcement learning with regret bounds.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 28208--28221, 2021.

\bibitem[O'Donoghue(2022)]{bregman_rl}
O'Donoghue, B.
\newblock On the connection between {B}regman divergence and value in
  regularized {M}arkov decision processes.
\newblock \emph{arXiv preprint arXiv:2210.12160}, 2022.

\bibitem[O'Donoghue \& Lattimore(2021)O'Donoghue and Lattimore]{o2021vbos}
O'Donoghue, B. and Lattimore, T.
\newblock Variational {B}ayesian optimistic sampling.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 12507--12519, 2021.

\bibitem[O'Donoghue et~al.(2017)O'Donoghue, Munos, Kavukcuoglu, and
  Mnih]{o2016pgq}
O'Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V.
\newblock Combining policy gradient and {Q}-learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[O'Donoghue et~al.(2018)O'Donoghue, Osband, Munos, and
  Mnih]{o2018uncertainty}
O'Donoghue, B., Osband, I., Munos, R., and Mnih, V.
\newblock The uncertainty {B}ellman equation and exploration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3836--3845, 2018.

\bibitem[O'Donoghue et~al.(2020)O'Donoghue, Lattimore, and
  Osband]{o2020stochastic}
O'Donoghue, B., Lattimore, T., and Osband, I.
\newblock Stochastic matrix games with bandit feedback.
\newblock \emph{arXiv preprint arXiv:2006.05145}, 2020.

\bibitem[Osband(2016)]{osband2016thesis}
Osband, I.
\newblock \emph{Deep Exploration via Randomized Value Functions}.
\newblock PhD thesis, Stanford University, 2016.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{osband2013more}
Osband, I., Russo, D., and Van~Roy, B.
\newblock ({M}ore) efficient reinforcement learning via posterior sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3003--3011, 2013.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and
  Van~Roy]{osband2016deep}
Osband, I., Blundell, C., Pritzel, A., and Van~Roy, B.
\newblock Deep exploration via bootstrapped {DQN}.
\newblock In \emph{Advances In Neural Information Processing Systems}, pp.\
  4026--4034, 2016.

\bibitem[Osband et~al.(2018)Osband, Aslanides, and
  Cassirer]{osband2018randomized}
Osband, I., Aslanides, J., and Cassirer, A.
\newblock Randomized prior functions for deep reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Osband et~al.(2019)Osband, Doron, Hessel, Aslanides, Sezener, Saraiva,
  McKinney, Lattimore, Szepesvari, Singh, Roy, Sutton, Silver, and
  Hasselt]{osband2019behaviour}
Osband, I., Doron, Y., Hessel, M., Aslanides, J., Sezener, E., Saraiva, A.,
  McKinney, K., Lattimore, T., Szepesvari, C., Singh, S., Roy, B.~V., Sutton,
  R., Silver, D., and Hasselt, H.~V.
\newblock Behaviour suite for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1908.03568}, 2019.

\bibitem[Osband et~al.(2021)Osband, Wen, Asghari, Dwaracherla, Hao, Ibrahimi,
  Lawson, Lu, O'Donoghue, and Van~Roy]{osband2021evaluating}
Osband, I., Wen, Z., Asghari, S.~M., Dwaracherla, V., Hao, B., Ibrahimi, M.,
  Lawson, D., Lu, X., O'Donoghue, B., and Van~Roy, B.
\newblock The neural testbed: {E}valuating joint predictions.
\newblock \emph{arXiv preprint arXiv:2110.04629}, 2021.

\bibitem[Ostrovski et~al.(2017)Ostrovski, Bellemare, Oord, and
  Munos]{ostrovski2017count}
Ostrovski, G., Bellemare, M.~G., Oord, A. v.~d., and Munos, R.
\newblock Count-based exploration with neural density models.
\newblock \emph{arXiv preprint arXiv:1703.01310}, 2017.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and
  Darrell]{pathak2017curiosity}
Pathak, D., Agrawal, P., Efros, A.~A., and Darrell, T.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{International conference on machine learning}, pp.\
  2778--2787. PMLR, 2017.

\bibitem[Plappert et~al.(2017)Plappert, Houthooft, Dhariwal, Sidor, Chen, Chen,
  Asfour, Abbeel, and Andrychowicz]{plappert2017parameter}
Plappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen, R.~Y., Chen, X.,
  Asfour, T., Abbeel, P., and Andrychowicz, M.
\newblock Parameter space noise for exploration.
\newblock \emph{arXiv preprint arXiv:1706.01905}, 2017.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: {D}iscrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Russo et~al.(2018)Russo, Van~Roy, Kazerouni, Osband, and
  Wen]{russo2018tutorial}
Russo, D.~J., Van~Roy, B., Kazerouni, A., Osband, I., and Wen, Z.
\newblock A tutorial on {T}hompson sampling.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  11\penalty0 (1):\penalty0 1--96, 2018.

\bibitem[Schaul et~al.(2015)Schaul, Quan, Antonoglou, and
  Silver]{schaul2015prioritized}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock arXiv preprint arXiv:1511.05952, 2015.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{Proceedings of The 32nd International Conference on Machine
  Learning}, pp.\  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, Dieleman,
  Grewe, Nham, Kalchbrenner, Sutskever, Lillicrap, Leach, Kavukcuoglu, Graepel,
  and Hassabis]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I.,
  Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Singh et~al.(2004)Singh, Barto, and Chentanez]{singh2004intrinsically}
Singh, S.~P., Barto, A.~G., and Chentanez, N.
\newblock Intrinsically motivated reinforcement learning.
\newblock In \emph{NIPS}, volume~17, pp.\  1281--1288, 2004.

\bibitem[Stadie et~al.(2015)Stadie, Levine, and
  Abbeel]{stadie2015incentivizing}
Stadie, B.~C., Levine, S., and Abbeel, P.
\newblock Incentivizing exploration in reinforcement learning with deep
  predictive models.
\newblock \emph{arXiv preprint arXiv:1507.00814}, 2015.

\bibitem[Strehl \& Littman(2008)Strehl and Littman]{strehl2008analysis}
Strehl, A.~L. and Littman, M.~L.
\newblock An analysis of model-based interval estimation for markov decision
  processes.
\newblock \emph{Journal of Computer and System Sciences}, 74\penalty0
  (8):\penalty0 1309--1331, 2008.

\bibitem[Strens(2000)]{strens2000bayesian}
Strens, M.
\newblock A {B}ayesian framework for reinforcement learning.
\newblock In \emph{ICML}, pp.\  943--950, 2000.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton:book}
Sutton, R. and Barto, A.
\newblock \emph{Reinforcement Learning: an Introduction}.
\newblock MIT Press, 1998.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and
  Mansour]{sutton1999policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~99, pp.\  1057--1063, 1999.

\bibitem[Tang et~al.(2017)Tang, Houthooft, Foote, Stooke, Xi~Chen, Duan,
  Schulman, DeTurck, and Abbeel]{tang2017exploration}
Tang, H., Houthooft, R., Foote, D., Stooke, A., Xi~Chen, O., Duan, Y.,
  Schulman, J., DeTurck, F., and Abbeel, P.
\newblock \# exploration: A study of count-based exploration for deep
  reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Thompson(1933)]{thompson1933likelihood}
Thompson, W.~R.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock \emph{Biometrika}, 25\penalty0 (3/4):\penalty0 285--294, 1933.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock Grandmaster level in {S}tarcraft {II} using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Zhang et~al.(2021)Zhang, Kim, O'Donoghue, and Boyd]{zhang2021sample}
Zhang, J., Kim, J., O'Donoghue, B., and Boyd, S.
\newblock Sample efficient reinforcement learning with {REINFORCE}.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  10887--10895, 2021.

\bibitem[Ziebart(2010)]{ziebart2010modeling}
Ziebart, B.~D.
\newblock \emph{Modeling purposeful adaptive behavior with the principle of
  maximum causal entropy}.
\newblock Carnegie Mellon University, 2010.

\end{thebibliography}
