@article{pour2022benefits,
  title={Benefits of Additive Noise in Composing Classes with Bounded Capacity},
  author={Fathollah Pour, Alireza and Ashtiani, Hassan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={32709--32722},
  year={2022}
}
%#################################################
%RNN

@article{koiran1998vapnik,
  title={Vapnik-Chervonenkis dimension of recurrent neural networks},
  author={Koiran, Pascal and Sontag, Eduardo D},
  journal={Discrete Applied Mathematics},
  volume={86},
  number={1},
  pages={63--79},
  year={1998},
  publisher={Elsevier}
}

@article{dasgupta1995sample,
  title={Sample complexity for learning recurrent perceptron mappings},
  author={Dasgupta, Bhaskar and Sontag, Eduardo},
  journal={Advances in Neural Information Processing Systems},
  volume={8},
  year={1995}
}

@inproceedings{
collins2017capacity,
title={Capacity and Trainability in Recurrent Neural Networks},
author={Jasmine Collins and Jascha Sohl-Dickstein and David Sussillo},
booktitle={International Conference on Learning Representations},
year={2017}
}

@inproceedings{chen2020generalization,
  title={On Generalization Bounds of a Family of Recurrent Neural Networks},
  author={Chen, Minshuo and Li, Xingguo and Zhao, Tuo},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1233--1243},
  year={2020},
  organization={PMLR}
}
@inproceedings{10.5555/646258.686055,
author = {Hammer, Barbara},
title = {On the Generalization Ability of Recurrent Networks},
year = {2001},
isbn = {3540424865},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
pages = {731–736},
numpages = {6},
series = {ICANN '01}
}


@article{akpinar2019sample,
  title={Sample complexity bounds for recurrent neural networks with application to combinatorial graph problems},
  author={Akpinar, Nil-Jana and Kratzwald, Bernhard and Feuerriegel, Stefan},
  journal={arXiv preprint arXiv:1901.10289},
  year={2019}
}

@inproceedings{zhang2018stabilizing,
  title={Stabilizing gradients for deep neural networks via efficient svd parameterization},
  author={Zhang, Jiong and Lei, Qi and Dhillon, Inderjit},
  booktitle={International Conference on Machine Learning},
  pages={5806--5814},
  year={2018},
  organization={PMLR}
}

@inproceedings{
Tu2020Understanding,
title={Understanding Generalization in Recurrent Neural Networks},
author={Zhuozhuo Tu and Fengxiang He and Dacheng Tao},
booktitle={International Conference on Learning Representations},
year={2020}
}

@article{allen2019can,
  title={Can sgd learn recurrent neural networks with provable generalization?},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{graves2013speech,
  title={Speech recognition with deep recurrent neural networks},
  author={Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  booktitle={2013 IEEE international conference on acoustics, speech and signal processing},
  pages={6645--6649},
  year={2013},
  organization={Ieee}
}

@article{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{hardt2016gradient,
  title={Gradient Descent Learns Linear Dynamical Systems},
  author={Hardt, Moritz and Ma, Tengyu and Recht, Benjamin},
  journal={Journal of Machine Learning Research},
  volume={19},
  pages={1--44},
  year={2018}
}

@inproceedings{karpathy2015deep,
  title={Deep visual-semantic alignments for generating image descriptions},
  author={Karpathy, Andrej and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3128--3137},
  year={2015}
}
@inproceedings{qin2017dual,
  title={A dual-stage attention-based recurrent neural network for time series prediction},
  author={Qin, Yao and Song, Dongjin and Cheng, Haifeng and Cheng, Wei and Jiang, Guofei and Cottrell, Garrison W},
  booktitle={Proceedings of the 26th International Joint Conference on Artificial Intelligence},
  pages={2627--2633},
  year={2017}
}


%#####################################################
@book{anthony1999neural,
  title={Neural network learning: Theoretical foundations},
  author={Anthony, Martin and Bartlett, Peter L and Bartlett, Peter L and others},
  volume={9},
  year={1999},
  publisher={cambridge university press Cambridge}
}


@article{bartlett1996valid,
  title={For valid generalization the size of the weights is more important than the size of the network},
  author={Bartlett, Peter},
  journal={Advances in neural information processing systems},
  volume={9},
  year={1996}
}
@inproceedings{golowich2018size,
  title={Size-independent sample complexity of neural networks},
  author={Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},
  booktitle={Conference On Learning Theory},
  pages={297--299},
  year={2018},
  organization={PMLR}
}

@article{laurent2000adaptive,
  title={Adaptive estimation of a quadratic functional by model selection},
  author={Laurent, Beatrice and Massart, Pascal},
  journal={Annals of Statistics},
  pages={1302--1338},
  year={2000},
  publisher={JSTOR}
}

%###################################
% Recently added for the related work part

@article{kakade2008complexity,
  title={On the complexity of linear prediction: Risk bounds, margin bounds, and regularization},
  author={Kakade, Sham M and Sridharan, Karthik and Tewari, Ambuj},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}

@article{xu2012robustness,
  title={Robustness and generalization},
  author={Xu, Huan and Mannor, Shie},
  journal={Machine learning},
  volume={86},
  number={3},
  pages={391--423},
  year={2012},
  publisher={Springer}
}

@article{bartlett2005local,
  title={Local rademacher complexities},
  author={Bartlett, Peter L and Bousquet, Olivier and Mendelson, Shahar},
  journal={The Annals of Statistics},
  volume={33},
  number={4},
  pages={1497--1537},
  year={2005},
  publisher={Institute of Mathematical Statistics}
}

@article{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{neyshabur2017pac,
  title={A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and Srebro, Nathan},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@article{bartlett2017spectrally,
  title={Spectrally-normalized margin bounds for neural networks},
  author={Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{zhang2002covering,
  title={Covering number bounds of certain regularized linear function classes},
  author={Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={2},
  number={Mar},
  pages={527--550},
  year={2002}
}

@article{chen2019generalization,
  title={On generalization bounds of a family of recurrent neural networks},
  author={Chen, Minshuo and Li, Xingguo and Zhao, Tuo},
  journal={arXiv preprint arXiv:1910.12947},
  year={2019}
}

@inproceedings{nagarajan2018deterministic,
  title={Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience},
  author={Nagarajan, Vaishnavh and Kolter, Zico},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{panigrahi2021learning,
  title={Learning and Generalization in RNNs},
  author={Panigrahi, Abhishek and Goyal, Navin},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

%%%%%%%%%%%%%%%%%%%%%%
%Implicit Bias
@article{jiang2019fantastic,
  title={Fantastic generalization measures and where to find them},
  author={Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  journal={arXiv preprint arXiv:1912.02178},
  year={2019}
}

@inproceedings{ji2021characterizing,
  title={Characterizing the implicit bias via a primal-dual analysis},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={Algorithmic Learning Theory},
  pages={772--804},
  year={2021},
  organization={PMLR}
}

@inproceedings{ji2020gradient,
  title={Gradient descent follows the regularization path for general losses},
  author={Ji, Ziwei and Dud{\'\i}k, Miroslav and Schapire, Robert E and Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={2109--2136},
  year={2020},
  organization={PMLR}
}

@inproceedings{chizat2020implicit,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Conference on Learning Theory},
  pages={1305--1338},
  year={2020},
  organization={PMLR}
}

@article{gunasekar2017implicit,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{jin2020implicit,
  title={Implicit bias of gradient descent for mean squared error regression with wide neural networks},
  author={Jin, Hui and Mont{\'u}far, Guido},
  journal={arXiv preprint arXiv:2006.07356},
  year={2020}
}


@inproceedings{DR17,
        title = {Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data},
       author = {Gintare Karolina Dziugaite and Daniel M. Roy},
         year = {2017},
    booktitle = {Proceedings of the 33rd Annual Conference on Uncertainty in Artificial Intelligence (UAI)},
archivePrefix = {arXiv},
       eprint = {1703.11008},
}


@InProceedings{pmlr-v40-Neyshabur15,
  title={Norm-based capacity control in neural networks},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={1376--1401},
  year={2015},
  organization={PMLR}
}


@article{WANG2019177,
  title={Rademacher dropout: An adaptive dropout for deep neural network via optimizing generalization gap},
  author={Wang, Haotian and Yang, Wenjing and Zhao, Zhenyu and Luo, Tingjin and Wang, Ji and Tang, Yuhua},
  journal={Neurocomputing},
  volume={357},
  pages={177--187},
  year={2019},
  publisher={Elsevier}
}

@article{gao2016dropout,
  title={Dropout Rademacher complexity of deep neural networks},
  author={Gao, Wei and Zhou, Zhi-Hua},
  journal={Science China Information Sciences},
  volume={59},
  number={7},
  pages={1--12},
  year={2016},
  publisher={Springer}
}

@article{SAF_1980-1981____A5_0,
     author = {Pisier, Gilles},
     title = {Remarques sur un r\'esultat non publi\'e de {B.} {Maurey}},
     journal = {S\'eminaire d'Analyse fonctionnelle (dit "Maurey-Schwartz")},
     note = {talk:5},
     publisher = {Ecole Polytechnique, Centre de Math\'ematiques},
     year = {1980-1981},
     zbl = {0491.46017},
     mrnumber = {659306},
     language = {fr},
     url = {http://www.numdam.org/item/SAF_1980-1981____A5_0/}
}


@article{baldi2013understanding,
  title={Understanding dropout},
  author={Baldi, Pierre and Sadowski, Peter J},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{JMLR:v15:srivastava14a,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@article{ba2013adaptive,
  title={Adaptive dropout for training deep neural networks},
  author={Ba, Jimmy and Frey, Brendan},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{jim1996analysis,
  title={An analysis of noise in recurrent neural networks: convergence and generalization},
  author={Jim, Kam-Chuen and Giles, C Lee and Horne, Bill G},
  journal={IEEE Transactions on neural networks},
  volume={7},
  number={6},
  pages={1424--1438},
  year={1996},
  publisher={IEEE}
}

@article{lim2021noisy,
  title={Noisy recurrent neural networks},
  author={Lim, Soon Hoe and Erichson, N Benjamin and Hodgkinson, Liam and Mahoney, Michael W},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@book{mohri2018foundations,
  title={Foundations of machine learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT press}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@article{vapnik1999overview,
  title={An overview of statistical learning theory},
  author={Vapnik, Vladimir N},
  journal={IEEE transactions on neural networks},
  volume={10},
  number={5},
  pages={988--999},
  year={1999},
  publisher={IEEE}
}

@article{sontag1998vc,
  title={VC dimension of neural networks},
  author={Sontag, Eduardo D and others},
  journal={NATO ASI Series F Computer and Systems Sciences},
  volume={168},
  pages={69--96},
  year={1998},
  publisher={Springer Verlag}
}

@article{bartlett2003vapnik,
  title={Vapnik-Chervonenkis dimension of neural nets},
  author={Bartlett, Peter L and Maass, Wolfgang},
  journal={The handbook of brain theory and neural networks},
  pages={1188--1192},
  year={2003},
  publisher={Citeseer}
}

@article{bartlett2019nearly,
  title={Nearly-tight VC-dimension and pseudodimension bounds for piecewise linear neural networks},
  author={Bartlett, Peter L and Harvey, Nick and Liaw, Christopher and Mehrabian, Abbas},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={2285--2301},
  year={2019},
  publisher={JMLR. org}
}

@inproceedings{arora2018stronger,
  title={Stronger generalization bounds for deep nets via a compression approach},
  author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  booktitle={International Conference on Machine Learning},
  pages={254--263},
  year={2018},
  organization={PMLR}
}
@article{nakkiran2020optimal,
  title={Optimal regularization can mitigate double descent},
  author={Nakkiran, Preetum and Venkat, Prayaag and Kakade, Sham and Ma, Tengyu},
  journal={arXiv preprint arXiv:2003.01897},
  year={2020}
}

@book{vapnik1999nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={1999},
  publisher={Springer science \& business media}
}

@inproceedings{wan2013regularization,
  title={Regularization of neural networks using dropconnect},
  author={Wan, Li and Zeiler, Matthew and Zhang, Sixin and Le Cun, Yann and Fergus, Rob},
  booktitle={International conference on machine learning},
  pages={1058--1066},
  year={2013},
  organization={PMLR}
}
@inproceedings{vincent2008extracting,
  title={Extracting and composing robust features with denoising autoencoders},
  author={Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1096--1103},
  year={2008}
}
@article{chae2020wasserstein,
  title={Wasserstein upper bounds of the total variation for smooth densities},
  author={Chae, Minwoo and Walker, Stephen G},
  journal={Statistics \& Probability Letters},
  volume={163},
  pages={108771},
  year={2020},
  publisher={Elsevier}
}

@article{bartlett1998almost,
  title={Almost linear VC dimension bounds for piecewise polynomial networks},
  author={Bartlett, Peter and Maiorov, Vitaly and Meir, Ron},
  journal={Advances in neural information processing systems},
  volume={11},
  year={1998}
}

@article{maass1994neural,
  title={Neural nets with superlinear VC-dimension},
  author={Maass, Wolfgang},
  journal={Neural Computation},
  volume={6},
  number={5},
  pages={877--884},
  year={1994},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{goldberg1995bounding,
  title={Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers},
  author={Goldberg, Paul W and Jerrum, Mark R},
  journal={Machine Learning},
  volume={18},
  number={2},
  pages={131--148},
  year={1995},
  publisher={Springer}
}

@article{baum1988size,
  title={What size net gives valid generalization?},
  author={Baum, Eric and Haussler, David},
  journal={Advances in neural information processing systems},
  volume={1},
  year={1988}
}


@book{vidyasagar1997theory,
  title={A theory of learning and generalization: with applications to neural networks and control systems},
  author={Vidyasagar, Mathukumalli},
  year={1997},
  publisher={Springer-Verlag}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{nagarajan2019uniform,
  title={Uniform convergence may be unable to explain generalization in deep learning},
  author={Nagarajan, Vaishnavh and Kolter, J Zico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}



@inproceedings{long2020size,
  title={Size-free generalization bounds for convolutional neural networks},
  author={Long, Philip M and Sedghi, Hanie},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{zhou2019non,
  title={Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach},
  author={Zhou, Wenda and Veitch, Victor and Austern, Morgane and Adams, Ryan P and Orbanz, Peter},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}


@article{bartlett2021deep,
  title={Deep learning: a statistical viewpoint},
  author={Bartlett, Peter L and Montanari, Andrea and Rakhlin, Alexander},
  journal={Acta numerica},
  volume={30},
  pages={87--201},
  year={2021},
  publisher={Cambridge University Press}
}

@article{bartlett2020benign,
  title={Benign overfitting in linear regression},
  author={Bartlett, Peter L and Long, Philip M and Lugosi, G{\'a}bor and Tsigler, Alexander},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30063--30070},
  year={2020},
  publisher={National Acad Sciences}
}

@article{belkin2018overfitting,
  title={Overfitting or perfect fitting? risk bounds for classification and regression rules that interpolate},
  author={Belkin, Mikhail and Hsu, Daniel J and Mitra, Partha},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{belkin2019does,
  title={Does data interpolation contradict statistical optimality?},
  author={Belkin, Mikhail and Rakhlin, Alexander and Tsybakov, Alexandre B},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1611--1619},
  year={2019},
  organization={PMLR}
}



%%%%%%%%%%%%%%%%%%
%Appendix

@article{bartlett2006convexity,
  title={Convexity, classification, and risk bounds},
  author={Bartlett, Peter L and Jordan, Michael I and McAuliffe, Jon D},
  journal={Journal of the American Statistical Association},
  volume={101},
  number={473},
  pages={138--156},
  year={2006},
  publisher={Taylor \& Francis}
}


@article{boucheron2005theory,
  title={Theory of classification: A survey of some recent advances},
  author={Boucheron, St{\'e}phane and Bousquet, Olivier and Lugosi, G{\'a}bor},
  journal={ESAIM: probability and statistics},
  volume={9},
  pages={323--375},
  year={2005},
  publisher={EDP Sciences}
}

@incollection{dudley2010universal,
  title={Universal Donsker classes and metric entropy},
  author={Dudley, Richard M},
  booktitle={Selected Works of RM Dudley},
  pages={345--365},
  year={2010},
  publisher={Springer}
}

@article{diakonikolas2019robust,
  title={Robust estimators in high-dimensions without the computational intractability},
  author={Diakonikolas, Ilias and Kamath, Gautam and Kane, Daniel and Li, Jerry and Moitra, Ankur and Stewart, Alistair},
  journal={SIAM Journal on Computing},
  volume={48},
  number={2},
  pages={742--864},
  year={2019},
  publisher={SIAM}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Information Theory
@article{haghifam2022limitations,
  title={Limitations of Information-Theoretic Generalization Bounds for Gradient Descent Methods in Stochastic Convex Optimization},
  author={Haghifam, Mahdi and Rodr{\'\i}guez-G{\'a}lvez, Borja and Thobaben, Ragnar and Skoglund, Mikael and Roy, Daniel M and Dziugaite, Gintare Karolina},
  journal={arXiv preprint arXiv:2212.13556},
  year={2022}
}
@inproceedings{russo2016controlling,
  title={Controlling bias in adaptive data analysis using information theory},
  author={Russo, Daniel and Zou, James},
  booktitle={Artificial Intelligence and Statistics},
  pages={1232--1240},
  year={2016},
  organization={PMLR}
}

@article{xu2017information,
  title={Information-theoretic analysis of generalization capability of learning algorithms},
  author={Xu, Aolin and Raginsky, Maxim},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{russo2019much,
  title={How much does your data exploration overfit? Controlling bias via information usage},
  author={Russo, Daniel and Zou, James},
  journal={IEEE Transactions on Information Theory},
  volume={66},
  number={1},
  pages={302--323},
  year={2019},
  publisher={IEEE}
}

@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={The Journal of Machine Learning Research},
  volume={2},
  pages={499--526},
  year={2002},
  publisher={JMLR. org}
}

@article{shalev2010learnability,
  title={Learnability, stability and uniform convergence},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
  journal={The Journal of Machine Learning Research},
  volume={11},
  pages={2635--2670},
  year={2010},
  publisher={JMLR. org}
}

@inproceedings{steinke2020reasoning,
  title={Reasoning about generalization via conditional mutual information},
  author={Steinke, Thomas and Zakynthinou, Lydia},
  booktitle={Conference on Learning Theory},
  pages={3437--3452},
  year={2020},
  organization={PMLR}
}

@article{haghifam2020sharpened,
  title={Sharpened generalization bounds based on conditional mutual information and an application to noisy, iterative algorithms},
  author={Haghifam, Mahdi and Negrea, Jeffrey and Khisti, Ashish and Roy, Daniel M and Dziugaite, Gintare Karolina},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9925--9935},
  year={2020}
}

@inproceedings{raginsky2017non,
  title={Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis},
  author={Raginsky, Maxim and Rakhlin, Alexander and Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={1674--1703},
  year={2017},
  organization={PMLR}
}


@inproceedings{neu2021information,
  title={Information-theoretic generalization bounds for stochastic gradient descent},
  author={Neu, Gergely and Dziugaite, Gintare Karolina and Haghifam, Mahdi and Roy, Daniel M},
  booktitle={Conference on Learning Theory},
  pages={3526--3545},
  year={2021},
  organization={PMLR}
}


@inproceedings{zhao2020rnn,
  title={Do RNN and LSTM have long memory?},
  author={Zhao, Jingyu and Huang, Feiqing and Lv, Jia and Duan, Yanjie and Qin, Zhen and Li, Guodong and Tian, Guangjian},
  booktitle={International Conference on Machine Learning},
  pages={11365--11375},
  year={2020},
  organization={PMLR}
}

@article{negrea2019information,
  title={Information-theoretic generalization bounds for SGLD via data-dependent estimates},
  author={Negrea, Jeffrey and Haghifam, Mahdi and Dziugaite, Gintare Karolina and Khisti, Ashish and Roy, Daniel M},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}