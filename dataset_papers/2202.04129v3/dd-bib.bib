



@article{altman1997contraction,
  title={Contraction conditions for average and $\alpha$-discount optimality in countable state {M}arkov games with unbounded rewards},
  author={Altman, Eitan and Hordijk, Arie and Spieksma, FM},
  journal={Mathematics of Operations Research},
  volume={22},
  number={3},
  pages={588--618},
  year={1997}
}



@article{takahashi1962stochastic,
  title={Stochastic games with infinitely many strategies},
  author={Takahashi, Masayuki},
  journal={Journal of Science of the Hiroshima University, Series AI (Mathematics)},
  volume={26},
  number={2},
  pages={123--134},
  year={1962},
  publisher={Hiroshima University, Mathematics Program}
}

@article{peng2021facmac,
  title={{FACMAC}: {F}actored multi-agent centralised policy gradients},
  author={Peng, Bei and Rashid, Tabish and Schroeder de Witt, Christian and Kamienny, Pierre-Alexandre and Torr, Philip and B{\"o}hmer, Wendelin and Whiteson, Shimon},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12208--12221},
  year={2021}
}



@article{maitra1970stochastic,
  title={On stochastic games},
  author={Maitra, A and Parthasarathy, T},
  journal={Journal of Optimization Theory and Applications},
  volume={5},
  number={4},
  pages={289--300},
  year={1970}
}


@article{maitra1971stochastic,
  title={On stochastic games, {II}},
  author={Maitra, A and Parthasarathy, T},
  journal={Journal of Optimization Theory and Applications},
  volume={8},
  number={2},
  pages={154--160},
  year={1971}
}


@article{nowak2002varepsilon,
  title={$\varepsilon$-equilibria for stochastic games with uncountable state space and unbounded costs},
  author={Nowak, Andrzej S and Altman, Eitan},
  journal={SIAM Journal on Control and Optimization},
  volume={40},
  number={6},
  pages={1821--1839},
  year={2002}
}


@inproceedings{
huang2022towards,
title={Towards General Function Approximation in Zero-Sum {M}arkov Games},
author={Baihe Huang and Jason D. Lee and Zhaoran Wang and Zhuoran Yang},
booktitle={International Conference on Learning Representations},
year={2022}
}

@article{huang2021towards,
	title={Towards general function approximation in zero-sum {M}arkov games},
	author={Huang, Baihe and Lee, Jason D and Wang, Zhaoran and Yang, Zhuoran},
	journal={arXiv preprint arXiv:2107.14702},
	year={2021}
}


@article{jin2021power,
	title={The Power of Exploiter: {P}rovable Multi-Agent {RL} in Large State Spaces},
	author={Jin, Chi and Liu, Qinghua and Yu, Tiancheng},
	journal={arXiv preprint arXiv:2106.03352},
	year={2021}
}

@inproceedings{xie2020learning,
	title={Learning zero-sum simultaneous-move {M}arkov games using function approximation and correlated equilibrium},
	author={Xie, Qiaomin and Chen, Yudong and Wang, Zhaoran and Yang, Zhuoran},
	booktitle={Conference on learning theory},
	pages={3674--3682},
	year={2020}
}

@inproceedings{zanette2021exponential,
	title={Exponential lower bounds for batch reinforcement learning: {B}atch {RL} can be exponentially harder than online {RL}},
	author={Zanette, Andrea},
	booktitle={International Conference on Machine Learning},
	pages={12287--12297},
	year={2021}
}

@article{wang2021exponential,
	title={An Exponential Lower Bound for Linearly Realizable {MDP} with Constant Suboptimality Gap},
	author={Wang, Yuanhao and Wang, Ruosong and Kakade, Sham},
	journal={Advances in Neural Information Processing Systems},
	volume={34},
	year={2021}
}

@inproceedings{weisz2021exponential,
	title={Exponential lower bounds for planning in {MDP}s with linearly-realizable optimal action-value functions},
	author={Weisz, Gell{\'e}rt and Amortila, Philip and Szepesv{\'a}ri, Csaba},
	booktitle={Algorithmic Learning Theory},
	pages={1237--1264},
	year={2021}
}

@inproceedings{abbasi2019politex,
	title={Politex: {R}egret bounds for policy iteration using expert prediction},
	author={Abbasi-Yadkori, Yasin and Bartlett, Peter and Bhatia, Kush and Lazic, Nevena and Szepesvari, Csaba and Weisz, Gell{\'e}rt},
	booktitle={International Conference on Machine Learning},
	pages={3692--3702},
	year={2019}
}

@article{sayin2022logit,
	title={Logit-{Q} Learning in {M}arkov Games},
	author={Sayin, Muhammed O and Unlu, Onur},
	journal={arXiv preprint arXiv:2205.13266},
	year={2022}
}

@article{maheshwari2022independent,
	title={Independent and Decentralized Learning in {M}arkov Potential Games},
	author={Maheshwari, Chinmay and Wu, Manxi and Pai, Druv and Sastry, Shankar},
	journal={arXiv preprint arXiv:2205.14590},
	year={2022}
}

@article{cen2022independent,
	title={Independent Natural Policy Gradient Methods for Potential Games: Finite-time Global Convergence with Entropy Regularization},
	author={Cen, Shicong and Chen, Fan and Chi, Yuejie},
	journal={arXiv preprint arXiv:2204.05466},
	year={2022}
}

@article{zhang2022effect,
	title={On the effect of log-barrier regularization in decentralized softmax gradient play in multiagent systems},
	author={Zhang, Runyu and Mei, Jincheng and Dai, Bo and Schuurmans, Dale and Li, Na},
	journal={arXiv preprint arXiv:2202.00872},
	year={2022}
}


@article{liu2019neural,
  title={Neural trust region/proximal policy optimization attains globally optimal policy},
  author={Liu, Boyi and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{leonardos2021exploration,
  title={Exploration-Exploitation in Multi-Agent Competition: Convergence with Bounded Rationality},
  author={Leonardos, Stefanos and Piliouras, Georgios and Spendlove, Kelly},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{leonardos2022exploration,
  title={Exploration-exploitation in multi-agent learning: Catastrophe theory meets game theory},
  author={Leonardos, Stefanos and Piliouras, Georgios},
  journal={Artificial Intelligence},
  volume={304},
  pages={103653},
  year={2022}
}

@inproceedings{mao2022on,
  author    = {Weichao Mao and
               Tamer Basar and
               Lin F. Yang and
               Kaiqing Zhang},
  title = {On Improving Model-Free Algorithms for Decentralized Multi-Agent Reinforcement Learning},
  booktitle = {International Conference on Machine Learning},
  year = {2022}
}

@inproceedings{kao2021decentralized,
	title={Decentralized cooperative reinforcement learning with hierarchical information structure},
	author={Kao, Hsu and Wei, Chen-Yu and Subramanian, Vijay},
	booktitle={International Conference on Algorithmic Learning Theory},
	pages={573--605},
	year={2022}
}

@inproceedings{bailey2018multiplicative,
  title={Multiplicative weights update in zero-sum games},
  author={Bailey, James P and Piliouras, Georgios},
  booktitle={Proceedings of the 2018 ACM Conference on Economics and Computation},
  pages={321--338},
  year={2018}
}

@inproceedings{mokhtari2020unified,
  title={A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: {P}roximal point approach},
  author={Mokhtari, Aryan and Ozdaglar, Asuman and Pattathil, Sarath},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1497--1507},
  year={2020},
  organization={PMLR}
}

@inproceedings{wei2020linear,
	title={Linear Last-iterate Convergence in Constrained Saddle-point Optimization},
	author={Wei, Chen-Yu and Lee, Chung-Wei and Zhang, Mengxiao and Luo, Haipeng},
	booktitle={International Conference on Learning Representations},
	year={2020}
}

@article{daskalakis2018limit,
  title={The limit points of (optimistic) gradient descent in min-max optimization},
  author={Daskalakis, Constantinos and Panageas, Ioannis},
  journal={arXiv preprint arXiv:1807.03907},
  year={2018}
}
@book{cesa2006prediction,
	author = {N. Cesa-Bianchi and G. Lugosi},
	publisher = {Cambridge University Press},
	title = {Prediction, Learning, and Games},
	year = {2006}}

@article{hofbauer2002global,
	title={On the global convergence of stochastic fictitious play},
	author={Hofbauer, Josef and Sandholm, William H},
	journal={Econometrica},
	volume={70},
	number={6},
	pages={2265--2294},
	year={2002},
	publisher={Wiley Online Library}
}
@article{ref:Miyasawa61,
	author	= "K. Miyasawa",
	title		= "On the convergence of the learning process in a 2x2 non-zero-sum game",
	journal	= "Economic Research Program, Princeton University, Research Memorandum",
	volume	= "33",
	year		= "1961",
}
@article{ref:Monderer96b,
	author	= "D. Monderer and L. Shapley",
	title		= "Fictitious play property for games with identical interests",
	journal	= "Journal of Economic Theory",
	volume	= "68",
	pages	= "258--265",
	year		= "1996",
}
@article{robinson1951iterative,
	title={An iterative method of solving a game},
	author={Robinson, Julia},
	journal={Annals of Mathematics},
	pages={296--301},
	year={1951},
	publisher={JSTOR}
}
@article{brown1951iterative,
	title={Iterative solution of games by fictitious play},
	author={Brown, George W},
	journal={Activity Analysis of Production and Allocation},
	volume={13},
	number={1},
	pages={374--376},
	year={1951},
	publisher={New York}
}
@article{kakade2001natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  journal={Advances in Neural Information Processing Systems},
  volume={14},
  year={2001}
}
@inproceedings{nagarajan2020chaos,
  title={From chaos to order: {S}ymmetry and conservation laws in game dynamics},
  author={Nagarajan, Sai Ganesh and Balduzzi, David and Piliouras, Georgios},
  booktitle={International Conference on Machine Learning},
  pages={7186--7196},
  year={2020}
}

@article{cheung2020chaos,
  title={Chaos of Learning Beyond Zero-sum and Coordination via Game Decompositions},
  author={Cheung, Yun Kuen and Tao, Yixin},
  journal={arXiv preprint arXiv:2008.00540},
  year={2020}
}

@inproceedings{cheung2019vortices,
  title={Vortices instead of equilibria in minmax optimization: {C}haos and butterfly effects of online learning in zero-sum games},
  author={Cheung, Yun Kuen and Piliouras, Georgios},
  booktitle={Conference on Learning Theory},
  pages={807--834},
  year={2019}
}

@article{palaiopanos2017multiplicative,
	title={Multiplicative weights update with constant step-size in congestion games: {C}onvergence, limit cycles and chaos},
	author={Palaiopanos, Gerasimos and Panageas, Ioannis and Piliouras, Georgios},
	journal={Advances in Neural Information Processing Systems},
	volume={30},
	year={2017}
}

@article{dechert2006stochastic,
  title={The stochastic lake game: {A} numerical solution},
  author={Dechert, W Davis and O’Donnell, SI},
  journal={Journal of Economic Dynamics and Control},
  volume={30},
  number={9-10},
  pages={1569--1587},
  year={2006},
  publisher={Elsevier}
}

@inproceedings{mguni2018decentralised,
  title={Decentralised learning in systems with many, many strategic agents},
  author={Mguni, David and Jennings, Joel and de Cote, Enrique Munoz},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{bistritz2020cooperative,
  title={Cooperative multi-player bandit optimization},
  author={Bistritz, Ilai and Bambos, Nicholas},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@book{gonzalez2013discrete,
  title={Discrete--time stochastic control and dynamic potential games: the {E}uler--Equation approach},
  author={Gonz{\'a}lez-S{\'a}nchez, David and Hern{\'a}ndez-Lerma, On{\'e}simo},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{mazalov2017linear,
  title={Linear-quadratic discrete-time dynamic potential games},
  author={Mazalov, Vladimir V and Rettieva, Anna N and Avrachenkov, Konstantin E},
  journal={Automation and Remote Control},
  volume={78},
  number={8},
  pages={1537--1544},
  year={2017},
  publisher={Springer}
}

@article{zazo2016dynamic,
  title={Dynamic potential games with constraints: Fundamentals and applications in communications},
  author={Zazo, Santiago and Macua, Sergio Valcarcel and S{\'a}nchez-Fern{\'a}ndez, Matilde and Zazo, Javier},
  journal={IEEE Transactions on Signal Processing},
  volume={64},
  number={14},
  pages={3806--3821},
  year={2016},
  publisher={IEEE}
}

@article{fink1964equilibrium,
  title={Equilibrium in a stochastic $n$-person game},
  author={Fink, Arlington M},
  journal={Journal of science of the hiroshima university, series ai (mathematics)},
  volume={28},
  number={1},
  pages={89--93},
  year={1964},
  publisher={Hiroshima University, Mathematics Program}
}
@article{busoniu2008comprehensive,
  title={A comprehensive survey of multiagent reinforcement learning},
  author={Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume={38},
  number={2},
  pages={156--172},
  year={2008},
  publisher={IEEE}
}
@article{jin2021v,
  title={{V}-Learning -- {A} Simple, Efficient, Decentralized Algorithm for Multiagent {RL}},
  author={Jin, Chi and Liu, Qinghua and Wang, Yuanhao and Yu, Tiancheng},
  journal={arXiv preprint arXiv:2110.14555},
  year={2021}
}

@inproceedings{
	sayin2021decentralized,
	title={Decentralized {Q}-learning in Zero-sum {M}arkov Games},
	author={Muhammed O. Sayin and Kaiqing Zhang and David S. Leslie and Tamer Basar and Asuman E. Ozdaglar},
	booktitle={Advances in Neural Information Processing Systems},
	editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
	year={2021}
}

@article{condon1990algorithms,
  title={On Algorithms for Simple Stochastic Games.},
  author={Condon, Anne},
  journal={Advances in computational complexity theory},
  volume={13},
  pages={51--72},
  year={1990}
}

@article{claus1998dynamics,
  title={The dynamics of reinforcement learning in cooperative multiagent systems},
  author={Claus, Caroline and Boutilier, Craig},
  journal={AAAI/IAAI},
  volume={1998},
  number={746-752},
  pages={2},
  year={1998}
}

@inproceedings{tan1993multi,
  title={Multi-agent reinforcement learning: {I}ndependent vs. cooperative agents},
  author={Tan, Ming},
  booktitle={Proceedings of the tenth international conference on machine learning},
  pages={330--337},
  year={1993}
}

@inproceedings{
	wang2020off,
	title={{DOP}: {O}ff-Policy Multi-Agent Decomposed Policy Gradients},
	author={Yihan Wang and Beining Han and Tonghan Wang and Heng Dong and Chongjie Zhang},
	booktitle={International Conference on Learning Representations},
	year={2021}
}

@article{xie2020semicentralized,
  title={Semicentralized deep deterministic policy gradient in cooperative {S}tar{C}raft games},
  author={Xie, Dong and Zhong, Xiangnan},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2020}
}

@inproceedings{kononen2004policy,
  title={Policy gradient method for team {M}arkov games},
  author={K{\"o}n{\"o}nen, Ville},
  booktitle={International Conference on Intelligent Data Engineering and Automated Learning},
  pages={733--739},
  year={2004}
}


@inproceedings{mguni2021learning,
	title={Learning in nonzero-sum stochastic games with potentials},
	author={Mguni, David H and Wu, Yutong and Du, Yali and Yang, Yaodong and Wang, Ziyi and Li, Minne and Wen, Ying and Jennings, Joel and Wang, Jun},
	booktitle={International Conference on Machine Learning},
	pages={7688--7699},
	year={2021}
}

@article{marden2012state,
  title={State based potential games},
  author={Marden, Jason R},
  journal={Automatica},
  volume={48},
  number={12},
  pages={3075--3088},
  year={2012}
}

@inproceedings{cohen2017learning,
  title={Learning with bandit feedback in potential games},
  author={Cohen, Johanne and H{\'e}liou, Am{\'e}lie and Mertikopoulos, Panayotis},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={6372--6381},
  year={2017}
}



@inproceedings{kleinberg2009multiplicative,
  title={Multiplicative updates outperform generic no-regret learning in congestion games},
  author={Kleinberg, Robert and Piliouras, Georgios and Tardos, {\'E}va},
  booktitle={Proceedings of the forty-first annual ACM symposium on Theory of computing},
  pages={533--542},
  year={2009}
}

@article{yu2021surprising,
  title={The Surprising Effectiveness of {PPO} in Cooperative, Multi-Agent Games},
  author={Yu, Chao and Velu, Akash and Vinitsky, Eugene and Wang, Yu and Bayen, Alexandre and Wu, Yi},
  journal={arXiv preprint arXiv:2103.01955},
  year={2021}
}

@article{lowe2017multi,
  title={Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
  author={Lowe, Ryan and WU, YI and Tamar, Aviv and Harb, Jean and Pieter Abbeel, OpenAI and Mordatch, Igor},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  pages={6379--6390},
  year={2017}
}

@article{wang2002reinforcement,
  title={Reinforcement learning to play an optimal {N}ash equilibrium in team {M}arkov games},
  author={Wang, Xiaofeng and Sandholm, Tuomas},
  journal={Advances in neural information processing systems},
  volume={15},
  pages={1603--1610},
  year={2002}
}

@inproceedings{liu2021sharp,
  title={A sharp analysis of model-based reinforcement learning with self-play},
  author={Liu, Qinghua and Yu, Tiancheng and Bai, Yu and Jin, Chi},
  booktitle={International Conference on Machine Learning},
  pages={7001--7010},
  year={2021}
}

@inproceedings{perolat2015approximate,
  title={Approximate dynamic programming for two-player zero-sum {M}arkov games},
  author={Perolat, Julien and Scherrer, Bruno and Piot, Bilal and Pietquin, Olivier},
  booktitle={International Conference on Machine Learning},
  pages={1321--1329},
  year={2015}
}

@inproceedings{mazumdar2019policy,
	title={Policy-Gradient Algorithms Have No Guarantees of Convergence in Linear Quadratic Games},
	author={Mazumdar, Eric and Ratliff, Lillian J and Jordan, Michael I and Sastry, S Shankar},
	booktitle={AAMAS},
	year={2020}
}

@article{hambly2021policy,
  title={Policy Gradient Methods Find the {N}ash Equilibrium in {N}-player General-sum Linear-quadratic Games},
  author={Hambly, Ben M and Xu, Renyuan and Yang, Huining},
  journal={arXiv preprint arXiv:2107.13090},
  year={2021}
}

@article{bu2019global,
  title={Global convergence of policy gradient for sequential zero-sum linear quadratic dynamic games},
  author={Bu, Jingjing and Ratliff, Lillian J and Mesbahi, Mehran},
  journal={arXiv preprint arXiv:1911.04672},
  year={2019}
}

@article{zhang2019policy,
  title={Policy Optimization Provably Converges to {N}ash Equilibria in Zero-Sum Linear Quadratic Games},
  author={Zhang, Kaiqing and Yang, Zhuoran and Basar, Tamer},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={11602--11614},
  year={2019}
}

@article{bailey2019fast,
	title={Fast and furious learning in zero-sum games: {V}anishing regret with non-vanishing step sizes},
	author={Bailey, James and Piliouras, Georgios},
	journal={Advances in Neural Information Processing Systems},
	volume={32},
	year={2019}
}

@article{zhang2020global,
  title={Global convergence of policy gradient methods to (almost) locally optimal policies},
  author={Zhang, Kaiqing and Koppel, Alec and Zhu, Hao and Basar, Tamer},
  journal={SIAM Journal on Control and Optimization},
  volume={58},
  number={6},
  pages={3586--3612},
  year={2020}
}

@misc{audibert09,
  author        = {Jean-Yves Audibert and  Olivier Catoni},
  title         = {Risk bounds for linear regression},
  year          = {2009},
  note={\url{http://imagine.enpc.fr/~audibert/Mes%20articles/Chevaleret09.pdf}}
}

@misc{Arora08,
  author        = {Arora, Sanjeev},
  title         = {Lecture 6 in Toward Theoretical Understanding of Deep Learning},
  month         = {October},
  year          = {2008},
  note={\url{https://www.cs.princeton.edu/courses/archive/fall18/cos597G/lecnotes/lecture6.pdf}}
}

@inproceedings{fox2021independent,
	title={Independent natural policy gradient always converges in {M}arkov potential games},
	author={Fox, Roy and Mcaleer, Stephen M and Overman, Will and Panageas, Ioannis},
	booktitle={International Conference on Artificial Intelligence and Statistics},
	pages={4414--4425},
	year={2022}
}

@article{cohen2017projected,
  title={On projected stochastic gradient descent algorithm with weighted averaging for least squares regression},
  author={Cohen, Kobi and Nedi{\'c}, Angelia and Srikant, R},
  journal={IEEE Transactions on Automatic Control},
  volume={62},
  number={11},
  pages={5974--5981},
  year={2017}
}

@article{dubey2021provably,
  title={Provably Efficient Cooperative Multi-Agent Reinforcement Learning with Function Approximation},
  author={Dubey, Abhimanyu and Pentland, Alex},
  journal={arXiv preprint arXiv:2103.04972},
  year={2021}
}

@article{ozdaglar2021independent,
  title={Independent learning in stochastic games},
  author={Ozdaglar, Asuman and Sayin, Muhammed O and Zhang, Kaiqing},
  journal={arXiv preprint arXiv:2111.11743},
  year={2021}
}

@article{xiao2022convergence,
	title={On the Convergence Rates of Policy Gradient Methods},
	author={Xiao, Lin},
	journal={arXiv preprint arXiv:2201.07443},
	year={2022}
}

@article{matignon2012independent,
  title={Independent reinforcement learners in cooperative {M}arkov games: {A} survey regarding coordination problems},
  author={Matignon, Laetitia and Laurent, Guillaume J and Le Fort-Piat, Nadine},
  journal={The Knowledge Engineering Review},
  volume={27},
  number={1},
  pages={1--31},
  year={2012}
}

@inproceedings{zinkevich2003online,
  title={Online convex programming and generalized infinitesimal gradient ascent},
  author={Zinkevich, Martin},
  booktitle={Proceedings of the 20th international conference on machine learning},
  pages={928--936},
  year={2003}
}

@inproceedings{bhandari2021linear,
  title={On the Linear Convergence of Policy Gradient Methods for Finite {MDP}s},
  author={Bhandari, Jalaj and Russo, Daniel},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2386--2394},
  year={2021}
}

@inproceedings{shani2020adaptive,
  title={Adaptive trust region policy optimization: {G}lobal convergence and faster rates for regularized {MDP}s},
  author={Shani, Lior and Efroni, Yonathan and Mannor, Shie},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5668--5675},
  year={2020}
}

@inproceedings{fazel2018global,
  title={Global convergence of policy gradient methods for the linear quadratic regulator},
  author={Fazel, Maryam and Ge, Rong and Kakade, Sham and Mesbahi, Mehran},
  booktitle={International Conference on Machine Learning},
  pages={1467--1476},
  year={2018}
}

@article{zhang2021multi,
  title={Multi-agent reinforcement learning: {A} selective overview of theories and algorithms},
  author={Zhang, Kaiqing and Yang, Zhuoran and Ba{\c{s}}ar, Tamer},
  journal={Handbook of Reinforcement Learning and Control},
  pages={321--384},
  year={2021}
}

@inproceedings{wei2021learning,
  title={Learning infinite-horizon average-reward {MDP}s with linear function approximation},
  author={Wei, Chen-Yu and Jahromi, Mehdi Jafarnia and Luo, Haipeng and Jain, Rahul},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3007--3015},
  year={2021},
  organization={PMLR}
}

@article{bhandari2019global,
  title={Global optimality guarantees for policy gradient methods},
  author={Bhandari, Jalaj and Russo, Daniel},
  journal={arXiv preprint arXiv:1906.01786},
  year={2019}
}

@article{zhao2021provably,
  title={Provably efficient policy gradient methods for two-player zero-sum {M}arkov games},
  author={Zhao, Yulai and Tian, Yuandong and Lee, Jason D and Du, Simon S},
  journal={arXiv preprint arXiv:2102.08903},
  year={2021}
}

@inproceedings{qu2020scalable,
  title={Scalable reinforcement learning of localized policies for multi-agent networked systems},
  author={Qu, Guannan and Wierman, Adam and Li, Na},
  booktitle={Learning for Dynamics and Control},
  pages={256--266},
  year={2020}
}

@inproceedings{zhang2018fully,
  title={Fully decentralized multi-agent reinforcement learning with networked agents},
  author={Zhang, Kaiqing and Yang, Zhuoran and Liu, Han and Zhang, Tong and Basar, Tamer},
  booktitle={International Conference on Machine Learning},
  pages={5872--5881},
  year={2018},
  organization={PMLR}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={229--256},
  year={1992}
}

@article{stastny2021normative,
  title={Normative Disagreement as a Challenge for Cooperative {AI}},
  author={Stastny, Julian and Rich{\'e}, Maxime and Lyzhov, Alexander and Treutlein, Johannes and Dafoe, Allan and Clifton, Jesse},
  journal={arXiv preprint arXiv:2111.13872},
  year={2021}
}

@article{dafoe2020open,
  title={Open problems in cooperative {AI}},
  author={Dafoe, Allan and Hughes, Edward and Bachrach, Yoram and Collins, Tantum and McKee, Kevin R and Leibo, Joel Z and Larson, Kate and Graepel, Thore},
  journal={arXiv preprint arXiv:2012.08630},
  year={2020}
}

@misc{dafoe2021cooperative,
  title={Cooperative {AI}: machines must learn to find common ground},
  author={Dafoe, Allan and Bachrach, Yoram and Hadfield, Gillian and Horvitz, Eric and Larson, Kate and Graepel, Thore},
  year={2021}
}

@article{cheung2020chaosvideo,
  title={Chaos, extremism and optimism: {V}olume analysis of learning in games},
  author={Cheung, Yun Kuen and Piliouras, Georgios},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9039--9049},
  year={2020}
}

@article{shapley1953stochastic,
  title={Stochastic games},
  author={Shapley, Lloyd S},
  journal={Proceedings of the national academy of sciences},
  volume={39},
  number={10},
  pages={1095--1100},
  year={1953}
}

@article{dieuleveut2017harder,
  title={Harder, better, faster, stronger convergence rates for least-squares regression},
  author={Dieuleveut, Aymeric and Flammarion, Nicolas and Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={3520--3570},
  year={2017}
}

@article{lan2021policy,
  title={Policy mirror descent for reinforcement learning: {L}inear convergence, new sampling complexity, and generalized problem classes},
  author={Lan, Guanghui},
  journal={Mathematical Programming},
  year={2022}
}


@article{zhan2021policy,
  title={Policy mirror descent for regularized reinforcement learning: {A} generalized framework with linear convergence},
  author={Zhan, Wenhao and Cen, Shicong and Huang, Baihe and Chen, Yuxin and Lee, Jason D and Chi, Yuejie},
  journal={arXiv preprint arXiv:2105.11066},
  year={2021}
}


@inproceedings{
	song2021can,
	title={When Can We Learn General-Sum {M}arkov Games with a Large Number of Players Sample-Efficiently?},
	author={Ziang Song and Song Mei and Yu Bai},
	booktitle={International Conference on Learning Representations},
	year={2022}
}

@inproceedings{macua2018learning,
  title={Learning Parametric Closed-Loop Policies for {M}arkov Potential Games},
  author={Macua, Sergio Valcarcel and Zazo, Javier and Zazo, Santiago},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{levine2016end,
  title={End-to-end training of deep visuomotor policies},
  author={Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1334--1373},
  year={2016}
}

@article{shalev2016safe,
  title={Safe, multi-agent, reinforcement learning for autonomous driving},
  author={Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
  journal={arXiv preprint arXiv:1610.03295},
  year={2016}
}

@article{trott2021building,
  title={Building a foundation for data-driven, interpretable, and robust policy design using the {AI} economist},
  author={Trott, Alexander and Srinivasa, Sunil and van der Wal, Douwe and Haneuse, Sebastien and Zheng, Stephan},
  journal={arXiv preprint arXiv:2108.02904},
  year={2021}
}

@article{zheng2020ai,
  title={The {AI} economist: {I}mproving equality and productivity with {AI}-driven tax policies},
  author={Zheng, Stephan and Trott, Alexander and Srinivasa, Sunil and Naik, Nikhil and Gruesbeck, Melvin and Parkes, David C and Socher, Richard},
  journal={arXiv preprint arXiv:2004.13332},
  year={2020}
}

@article{berner2019dota,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in {S}tar{C}raft {II} using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019}
}

@article{silver2018general,
  title={A general reinforcement learning algorithm that masters chess, shogi, and {G}o through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018}
}

@article{silver2016mastering,
  title={Mastering the game of {G}o with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016}
}

@inproceedings{Wei2021LastiterateCO,
  title={Last-iterate Convergence of Decentralized Optimistic Gradient Descent/Ascent in Infinite-horizon Competitive {M}arkov Games},
  author={Chen-Yu Wei and Chung-Wei Lee and Mengxiao Zhang and Haipeng Luo},
  booktitle={Conference on learning theory},
  year={2021}
}

@inproceedings{hsu2012random,
  title={Random design analysis of ridge regression},
  author={Hsu, Daniel and Kakade, Sham M and Zhang, Tong},
  booktitle={Conference on learning theory},
  pages={9--1},
  year={2012}
}


@book{shalev2014understanding,
  title={Understanding machine learning: {F}rom theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge University Press}
}

@article{daskalakis2020independent,
  title={Independent Policy Gradient Methods for Competitive Reinforcement Learning},
  author={Daskalakis, Constantinos and Foster, Dylan J and Golowich, Noah},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}


@article{hazan2016introduction,
  title={Introduction to Online Convex Optimization},
  author={Hazan, Elad},
  journal={Foundations and Trends in Optimization},
  volume={2},
  number={3-4},
  pages={157--325},
  year={2016},
  publisher={Now Publishers Inc. Hanover, MA, USA}
}


@inproceedings{leonardos2021global,
  title={Global Convergence of Multi-Agent Policy Gradient in {M}arkov Potential Games},
  author={Leonardos, Stefanos and Overman, Will and Panageas, Ioannis and Piliouras, Georgios},
  booktitle={International Conference on Learning Representations},
  year={2022}
}


@article{zhang2021gradient,
  title={Gradient Play in Multi-Agent {M}arkov Stochastic Games: {S}tationary Points and Convergence},
  author={Zhang, Runyu and Ren, Zhaolin and Li, Na},
  journal={arXiv preprint arXiv:2106.00198},
  year={2021}
}


@article{cen2021fast,
	title={Fast policy extragradient methods for competitive games with entropy regularization},
	author={Cen, Shicong and Wei, Yuting and Chi, Yuejie},
	journal={Advances in Neural Information Processing Systems},
	volume={34},
	year={2021}
}


@article{agarwal2021theory,
  title={On the theory of policy gradient methods: {O}ptimality, approximation, and distribution shift},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={98},
  pages={1--76},
  year={2021}
}

@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}

@inproceedings{cai2020provably,
  title={Provably efficient exploration in policy optimization},
  author={Cai, Qi and Yang, Zhuoran and Jin, Chi and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={1283--1294},
  year={2020},
  organization={PMLR}
}

@article{luo2021policy,
  title={Policy optimization in adversarial {MDP}s: {I}mproved exploration via dilated bonuses},
  author={Luo, Haipeng and Wei, Chen-Yu and Lee, Chung-Wei},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}