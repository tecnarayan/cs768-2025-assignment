\begin{thebibliography}{78}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and
  Zheng]{tensorflow2015-whitepaper}
Mart\'{i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Greg~S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,
  Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
  Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
  Levenberg, Dan Man\'{e}, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,
  Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar,
  Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\'{e}gas, Oriol
  Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang
  Zheng.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock URL \url{http://tensorflow.org/}.
\newblock Software available from tensorflow.org.

\bibitem[Anderson et~al.(1999)Anderson, Bai, Bischof, Blackford, Demmel,
  Dongarra, Du~Croz, Greenbaum, Hammarling, McKenney, and Sorensen]{lapack}
E.~Anderson, Z.~Bai, C.~Bischof, S.~Blackford, J.~Demmel, J.~Dongarra,
  J.~Du~Croz, A.~Greenbaum, S.~Hammarling, A.~McKenney, and D.~Sorensen.
\newblock \emph{{LAPACK} Users' Guide}.
\newblock SIAM, Philadelphia, PA, third edition, 1999.

\bibitem[Ballard et~al.(2014)Ballard, Demmel, Grigori, Jacquelin, Nguyen, and
  Solomonik]{ballard2014tsqr}
Grey Ballard, James Demmel, Laura Grigori, Mathias Jacquelin, Hong~Diep Nguyen,
  and Edgar Solomonik.
\newblock Reconstructing householder vectors from tall-skinny qr.
\newblock In \emph{2014 IEEE 28th International Parallel and Distributed
  Processing Symposium}. IEEE, 2014.

\bibitem[Barocas et~al.(2019)Barocas, Hardt, and
  Narayanan]{barocas-hardt-narayanan}
Solon Barocas, Moritz Hardt, and Arvind Narayanan.
\newblock \emph{Fairness and Machine Learning}.
\newblock fairmlbook.org, 2019.
\newblock \url{http://www.fairmlbook.org}.

\bibitem[Baykal et~al.(2019)Baykal, Liebenwein, Gilitschenski, Feldman, and
  Rus]{sippBayal}
Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman, and Daniela
  Rus.
\newblock Sipping neural networks: Sensitivity-informed provable pruning of
  neural networks.
\newblock \emph{CoRR}, abs/1910.05422, 2019.
\newblock URL \url{http://arxiv.org/abs/1910.05422}.

\bibitem[Blalock et~al.(2020)Blalock, Ortiz, Frankle, and
  Guttag]{blalock2020state}
Davis Blalock, Jose Javier~Gonzalez Ortiz, Jonathan Frankle, and John Guttag.
\newblock What is the state of neural network pruning?
\newblock \emph{arXiv preprint arXiv:2003.03033}, 2020.

\bibitem[Boutsidis et~al.(2009)Boutsidis, Mahoney, and
  Drineas]{boutsidis2009improved}
Christos Boutsidis, Michael~W Mahoney, and Petros Drineas.
\newblock An improved approximation algorithm for the column subset selection
  problem.
\newblock In \emph{Proceedings of the twentieth annual ACM-SIAM symposium on
  Discrete algorithms}, pages 968--977. SIAM, 2009.

\bibitem[Businger and Golub(1965)]{businger1965linear}
Peter Businger and Gene~H Golub.
\newblock Linear least squares solutions by householder transformations.
\newblock \emph{Numerische Mathematik}, 7\penalty0 (3):\penalty0 269--276,
  1965.

\bibitem[Carlini et~al.(2019)Carlini, Athalye, Papernot, Brendel, Rauber,
  Tsipras, Goodfellow, Madry, and Kurakin]{carlini2019evaluating}
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas
  Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey
  Kurakin.
\newblock On evaluating adversarial robustness.
\newblock \emph{arXiv preprint arXiv:1902.06705}, 2019.

\bibitem[Carmon et~al.(2019)Carmon, Raghunathan, Schmidt, Duchi, and
  Liang]{liang2019adversarial}
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John~C Duchi, and Percy~S
  Liang.
\newblock Unlabeled data improves adversarial robustness.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/32e0bd1497aa43e02a42f47d9d6515ad-Paper.pdf}.

\bibitem[Chan and Hansen(1992)]{chan1992some}
Tony~F Chan and Per~Christian Hansen.
\newblock Some applications of the rank revealing qr factorization.
\newblock \emph{SIAM Journal on Scientific and Statistical Computing},
  13\penalty0 (3):\penalty0 727--741, 1992.

\bibitem[Chandrasekaran and Ipsen(1994)]{chandrasekaran1994rank}
Shivkumar Chandrasekaran and Ilse~CF Ipsen.
\newblock On rank-revealing factorisations.
\newblock \emph{SIAM J. on Matrix Anal. and Apps.}, 15\penalty0 (2):\penalty0
  592--622, 1994.

\bibitem[Cheng et~al.(2005)Cheng, Gimbutas, Martinsson, and
  Rokhlin]{cheng2005compression}
H.~Cheng, Z.~Gimbutas, Per-Gunnar Martinsson, and V.~Rokhlin.
\newblock On the compression of low rank matrices.
\newblock \emph{SIAM Journal on Scientific Computing}, 26\penalty0
  (4):\penalty0 1389--1404, 2005.

\bibitem[Chouldechova and Roth(2020)]{rothfair}
Alexandra Chouldechova and Aaron Roth.
\newblock A snapshot of the frontiers of fairness in machine learning.
\newblock \emph{Communications of the ACM}, 63:\penalty0 82--89, 2020.

\bibitem[Civril and Magdon-Ismail(2009)]{civril2009selecting}
Ali Civril and Malik Magdon-Ismail.
\newblock On selecting a maximum volume sub-matrix of a matrix and related
  problems.
\newblock \emph{Theoretical Computer Science}, 410\penalty0 (47-49):\penalty0
  4801--4811, 2009.

\bibitem[Cohen et~al.(2019)Cohen, Rosenfeld, and Kolter]{kolter2019adversarial}
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock In \emph{International Conference on Machine Learning}, pages
  1310--1320. PMLR, 2019.

\bibitem[Corbett-Davies and Goel(2018)]{goel2018fair}
Sam Corbett-Davies and Sharad Goel.
\newblock The measure and mismeasure of fairness: A critical review of fair
  machine learning.
\newblock \emph{arXiv:1808.00023}, 2018.

\bibitem[Croce and Hein(2020)]{hein2020adversarial}
Francesco Croce and Matthias Hein.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock In \emph{International Conference on Machine Learning}, pages
  2206--2216. PMLR, 2020.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. IEEE, 2009.

\bibitem[Denton et~al.(2014)Denton, Zaremba, Bruna, LeCun, and
  Fergus]{denten2014svd}
Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus.
\newblock Exploiting linear structure within convolutional networks for
  efficient evaluation.
\newblock In \emph{Advances in neural information processing systems}, 2014.

\bibitem[Frankle and Carbin(2018)]{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Frankle et~al.(2020{\natexlab{a}})Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020linear}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel Roy, and Michael Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{International Conference on Machine Learning}, pages
  3259--3269. PMLR, 2020{\natexlab{a}}.

\bibitem[Frankle et~al.(2020{\natexlab{b}})Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020pruning}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel~M Roy, and Michael Carbin.
\newblock Pruning neural networks at initialization: Why are we missing the
  mark?
\newblock \emph{arXiv preprint arXiv:2009.08576}, 2020{\natexlab{b}}.

\bibitem[Friedler et~al.(2019)Friedler, Scheidegger, Venkatasubramanian,
  Choudhary, Hamilton, and Roth]{roth2019fair}
Sorelle~A. Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam
  Choudhary, Evan~P. Hamilton, and Derek Roth.
\newblock A comparative study of fairness-enhancing interventions in machine
  learning.
\newblock In \emph{Proceedings of the Conference on Fairness, Accountability,
  and Transparency}, FAT$\star$ '19, page 329â€“338. Association for Computing
  Machinery, 2019.
\newblock ISBN 9781450361255.

\bibitem[Gale et~al.(2019)Gale, Elsen, and Hooker]{gale2019state}
Trevor Gale, Erich Elsen, and Sara Hooker.
\newblock The state of sparsity in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1902.09574}, 2019.

\bibitem[Golub and Van~Loan(1996)]{GVL}
G.~H. Golub and C.~F. Van~Loan.
\newblock \emph{Matrix computations}.
\newblock Johns Hopkins Univ. Press, Baltimore, third edition, 1996.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock \emph{Deep learning}.
\newblock MIT press, 2016.

\bibitem[Gu and Eisenstat(1996)]{gu1996efficient}
M.~Gu and S.~Eisenstat.
\newblock Efficient algorithms for computing a strong rank-revealing {QR}
  factorization.
\newblock \emph{SIAM J. Sci. Comput.}, 17\penalty0 (4):\penalty0 848--869,
  1996.

\bibitem[Harvey et~al.(2017)Harvey, Liaw, and Mehrabian]{pmlr-v65-harvey17a}
Nick Harvey, Christopher Liaw, and Abbas Mehrabian.
\newblock Nearly-tight {VC}-dimension bounds for piecewise linear neural
  networks.
\newblock In Satyen Kale and Ohad Shamir, editors, \emph{Proceedings of the
  2017 Conference on Learning Theory}, volume~65 of \emph{Proceedings of
  Machine Learning Research}, pages 1064--1068. PMLR, 07--10 Jul 2017.
\newblock URL \url{http://proceedings.mlr.press/v65/harvey17a.html}.

\bibitem[He et~al.(2018{\natexlab{a}})He, Kang, Dong, Fu, and Yang]{softnetHe}
Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi~Yang.
\newblock Soft filter pruning for accelerating deep convolutional neural
  networks.
\newblock \emph{CoRR}, abs/1808.06866, 2018{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1808.06866}.

\bibitem[He et~al.(2019)He, Liu, Wang, Hu, and Yang]{he2019fpgm}
Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi~Yang.
\newblock Filter pruning via geometric median for deep convolutional neural
  networks acceleration.
\newblock In \emph{Conference on Computer Vision and Patter Recognition}, 2019.

\bibitem[He et~al.(2017)He, Zhang, and Sun]{he2017feat}
Yihui He, Xiangyu Zhang, and Jian Sun.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{International Conference on Computer Vision}, 2017.

\bibitem[He et~al.(2018{\natexlab{b}})He, Lin, Liu, Wang, and Li]{he2018amc}
Yihui He, Ji~Lin, Zhijian Liu, Hanrui Wang, and Song Li, Li-Jia nad~Han.
\newblock Amc: Automl for model compression and acceleration on mobile devices.
\newblock In \emph{European Conference on Computer Vision}, 2018{\natexlab{b}}.

\bibitem[Ho and Ying(2016)]{ho2016hierarchical}
Kenneth~L Ho and Lexing Ying.
\newblock Hierarchical interpolative factorization for elliptic operators:
  differential equations.
\newblock \emph{Communications on Pure and Applied Mathematics}, 69\penalty0
  (8):\penalty0 1415--1451, 2016.

\bibitem[Ho and Greengard(2012)]{ho2012fast}
K.L. Ho and L.~Greengard.
\newblock A fast direct solver for structured linear systems by recursive
  skeletonization.
\newblock \emph{SIAM Journal on Scientific Computing}, 34\penalty0
  (5):\penalty0 A2507--A2532, 2012.
\newblock \doi{10.1137/120866683}.

\bibitem[Ho and Ying(2015)]{ho2015hierarchical}
K.L. Ho and L.~Ying.
\newblock Hierarchical interpolative factorization for elliptic operators:
  Integral equations.
\newblock \emph{Communications on Pure and Applied Mathematics}, 2015.
\newblock ISSN 1097-0312.
\newblock \doi{10.1002/cpa.21577}.
\newblock URL \url{http://dx.doi.org/10.1002/cpa.21577}.

\bibitem[Hong and Pan(1992)]{hong1992rank}
Yoo~Pyo Hong and C-T Pan.
\newblock Rank-revealing qr factorizations and the singular value
  decomposition.
\newblock \emph{Mathematics of Computation}, 58\penalty0 (197):\penalty0
  213--232, 1992.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{MobilenetMainPaper}
Andrew~G. Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{CoRR}, abs/1704.04861, 2017.
\newblock URL \url{http://arxiv.org/abs/1704.04861}.

\bibitem[Huang and Wan(2018)]{huang2018sss}
Zehao Huang and Naiyan Wan.
\newblock Data-driven sparse structure selection for deep neural networks.
\newblock In \emph{European Conference on Computer Vision}, 2018.

\bibitem[Idelbayev and Carreira-Perpinan(2020)]{idel2020lrank}
Yerlan Idelbayev and Miguel~A. Carreira-Perpinan.
\newblock Low-rank compression of neural nets: Learning the rank of each layer.
\newblock In \emph{2020 IEEE conference on computer vision and pattern
  recognition}, pages 8049--8059. IEEE, 2020.

\bibitem[Ioffe and Szegedy(2015)]{batchnorm}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In Francis Bach and David Blei, editors, \emph{Proceedings of the
  32nd International Conference on Machine Learning}, volume~37 of
  \emph{Proceedings of Machine Learning Research}, pages 448--456, Lille,
  France, 07--09 Jul 2015. PMLR.

\bibitem[Jaderberg et~al.(2014)Jaderberg, Vedaldi, and
  Zisserman]{jaderberg2014speeding}
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman.
\newblock Speeding up convolutional neural networks with low rank expansions.
\newblock In \emph{Proceedings of the British Machine Vision Conference. BMVA
  Press}, 2014.

\bibitem[Krizhevsky and Hinton(2009)]{datacifar10}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Technical Report}, 2009.

\bibitem[Lebedev et~al.(2015)Lebedev, Ganin, Rakhuba, Oseledets, and
  Lempitsky]{lebedev2015cpdecomp}
Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor
  Lempitsky.
\newblock Speeding-up convolutional neural networks using fine-tuned
  cp-decomposition.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Lee et~al.(2019)Lee, Ajanthan, and Torr]{lee2019snip}
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip~H.S. Torr.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Li et~al.(2017)Li, Kadav, Durdanovic, Samet, and Graf]{li2017l1}
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans~Peter Graf.
\newblock Pruning filters for efficient convnets.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Liberty et~al.(2007)Liberty, Woolfe, Martinsson, Rokhlin, and
  Tygert]{liberty2007randomized}
Edo Liberty, Franco Woolfe, Per-Gunnar Martinsson, Vladimir Rokhlin, and Mark
  Tygert.
\newblock Randomized algorithms for the low-rank approximation of matrices.
\newblock \emph{Proceedings of the National Academy of Sciences}, 104\penalty0
  (51):\penalty0 20167--20172, 2007.

\bibitem[Liebenwein et~al.(2020)Liebenwein, Baykal, Lang, Feldman, and
  Rus]{liebenwein2020provable}
Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, and Daniela Rus.
\newblock Provable filter pruning for efficient neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=BJxkOlSYDH}.

\bibitem[Liebenwein et~al.(2021{\natexlab{a}})Liebenwein, Baykal, Carter,
  Gifford, and Rus]{liebenwein2021lost}
Lucas Liebenwein, Cenk Baykal, Brandon Carter, David Gifford, and Daniela Rus.
\newblock Lost in pruning: The effects of pruning neural networks beyond test
  accuracy.
\newblock In \emph{Proceedings of the 4th MLSys Conference},
  2021{\natexlab{a}}.

\bibitem[Liebenwein et~al.(2021{\natexlab{b}})Liebenwein, Maalouf, Feldman, and
  Rus]{liebenwein2021alds}
Lucas Liebenwein, Alaa Maalouf, Dan Feldman, and Daniela Rus.
\newblock Compressing neural networks: Towards determining the optimal
  layer-wise decomposition.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, 2021{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2107.11442}.

\bibitem[Lin et~al.(2020)Lin, Ji, Wang, Zhang, Zhang, Tian, and
  Shao]{lin2020hrank}
Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong
  Tian, and Ling Shao.
\newblock Hrank: Filter pruning using high-rank feature map.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 1529--1538, 2020.

\bibitem[Liu et~al.(2017)Liu, Li, Shen, haung, Yan, and Zhang]{liu2017netslim}
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao haung, Shoumeng Yan, and Changshui
  Zhang.
\newblock Learning efficient convolutional networks through network slimming.
\newblock In \emph{International Conference on Computer Vision}, 2017.

\bibitem[Liu et~al.(2019)Liu, Sun, Zhou, Huang, and Darrell]{liu2019rethink}
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell.
\newblock Rethinking the value of network pruning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Luo et~al.(2017)Luo, Wu, and Lin]{luo2017thinet}
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin.
\newblock Thinet: A filter level pruning method for deep neural network
  compression.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 5058--5066, 2017.

\bibitem[Maalouf et~al.(2021)Maalouf, Lang, Rus, and
  Feldman]{Maalouf2021DeepLM}
Alaa Maalouf, Harry Lang, Daniela Rus, and Dan Feldman.
\newblock Deep learning meets projective clustering.
\newblock \emph{ArXiv}, abs/2010.04290, 2021.

\bibitem[Mahoney and Drineas(2009)]{mahoney2009cur}
Michael~W Mahoney and Petros Drineas.
\newblock {CUR} matrix decompositions for improved data analysis.
\newblock \emph{Proceedings of the National Academy of Sciences}, 106\penalty0
  (3):\penalty0 697--702, 2009.

\bibitem[Martinsson(2019)]{martinsson2019fast}
Per-Gunnar. Martinsson.
\newblock \emph{Fast Direct Solvers for Elliptic PDEs}.
\newblock Society for Industrial and Applied Mathematics, Philadelphia, PA,
  2019.
\newblock \doi{10.1137/1.9781611976045}.
\newblock URL \url{https://epubs.siam.org/doi/abs/10.1137/1.9781611976045}.

\bibitem[Martinsson and Rokhlin(2005)]{martinsson2005fast}
Per-Gunnar Martinsson and V.~Rokhlin.
\newblock A fast direct solver for boundary integral equations in two
  dimensions.
\newblock \emph{J. Comput. Phys.}, 205\penalty0 (1):\penalty0 1--23, May 2005.
\newblock ISSN 0021-9991.
\newblock \doi{10.1016/j.jcp.2004.10.033}.
\newblock URL \url{http://dx.doi.org/10.1016/j.jcp.2004.10.033}.

\bibitem[Martinsson et~al.(2011)Martinsson, Rokhlin, and
  Tygert]{martinsson2011randomized}
Per-Gunnar Martinsson, Vladimir Rokhlin, and Mark Tygert.
\newblock A randomized algorithm for the decomposition of matrices.
\newblock \emph{Applied and Computational Harmonic Analysis}, 30\penalty0
  (1):\penalty0 47--68, 2011.

\bibitem[Marx et~al.(2020)Marx, Calmon, and Ustum]{marx2020multiplicity}
Charles~T. Marx, Flaviou du~Pin Calmon, and Berk Ustum.
\newblock Predicitive multiplicity in classification.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2020.

\bibitem[Mehrabi et~al.(2021)Mehrabi, Morstatter, Saxena, Lerman, and
  Galstyan]{aram2021fair}
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram
  Galstyan.
\newblock A survey on bias and fairness in machine learning.
\newblock \emph{ACM Comput. Surv.}, 54\penalty0 (6), jul 2021.
\newblock ISSN 0360-0300.
\newblock \doi{10.1145/3457607}.
\newblock URL \url{https://doi.org/10.1145/3457607}.

\bibitem[Minden et~al.(2017)Minden, Ho, Damle, and Ying]{minden2017recursive}
Victor Minden, Kenneth~L Ho, Anil Damle, and Lexing Ying.
\newblock A recursive skeletonization factorization based on strong
  admissibility.
\newblock \emph{Multiscale Modeling \& Simulation}, 15\penalty0 (2):\penalty0
  768--796, 2017.

\bibitem[Mohri et~al.(2018)Mohri, Rostamizadeh, and Talwalkar]{foundationsML}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock \emph{Foundations of Machine Learning}.
\newblock MIT press, 2 edition, 2018.

\bibitem[Mussay et~al.(2020)Mussay, Osadchy, Braverman, Zhou, and
  Feldman]{mussay2020coreset}
Ben Mussay, Margarita Osadchy, Vladimir Braverman, Samson Zhou, and Dan
  Feldman.
\newblock Data-independent neural pruning via coresets.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Peng et~al.(2018)Peng, Tan, Li, Zhang, Xie, and Pu]{peng2018group}
Bo~Peng, Wenming Tan, Zheyang Li, Shun Zhang, Di~Xie, and Shiliang Pu.
\newblock Extreme network compression via filter group approximation.
\newblock In \emph{European Conference on Computer Vision}, 2018.

\bibitem[Peng et~al.(2019)Peng, Wu, Chen, and Huang]{peng2019ccp}
Hanyu Peng, Jiaxiang Wu, Shifeng Chen, and Junzhou Huang.
\newblock Collaborative channel pruning for deep networks.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Quintana-Ort\'i et~al.(1998)Quintana-Ort\'i, Sun, and
  Bischof]{blas3QRCP}
Gregorio Quintana-Ort\'i, Xiaobai Sun, and Christian~H. Bischof.
\newblock A blas-3 version of the qr factorization with column pivoting.
\newblock \emph{SIAM J. on Sci. Comp.}, 19\penalty0 (5):\penalty0 1486--1494,
  1998.
\newblock \doi{10.1137/S1064827595296732}.

\bibitem[Silva and Najafirad(2020)]{silva2020adversarial}
Samuel~Henrique Silva and Peyman Najafirad.
\newblock Opportunities and challenges in deep learning adversarial robustness:
  A survey.
\newblock \emph{ArXiv}, abs/2007.00753, 2020.

\bibitem[Simonyan and Zisserman(2015)]{simoyan2015vgg}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Townshend et~al.(2020)Townshend, V{\"{o}}gele, Suriana, Derry, Powers,
  Laloudakis, Balachandar, Anderson, Eismann, Kondor, Altman, and Dror]{atom3d}
Raphael J.~L. Townshend, Martin V{\"{o}}gele, Patricia Suriana, Alexander
  Derry, Alexander Powers, Yianni Laloudakis, Sidhika Balachandar, Brandon~M.
  Anderson, Stephan Eismann, Risi Kondor, Russ~B. Altman, and Ron~O. Dror.
\newblock {ATOM3D:} tasks on molecules in three dimensions.
\newblock \emph{CoRR}, abs/2012.04035, 2020.
\newblock URL \url{https://arxiv.org/abs/2012.04035}.

\bibitem[Tropp(2009)]{tropp2009column}
Joel~A Tropp.
\newblock Column subset selection, matrix factorization, and eigenvalue
  optimization.
\newblock In \emph{Proceedings of the twentieth annual ACM-SIAM symposium on
  Discrete algorithms}, pages 978--986. SIAM, 2009.

\bibitem[Voronin and Martinsson(2017)]{voronin2017efficient}
Sergey Voronin and Per-Gunnar Martinsson.
\newblock Efficient algorithms for cur and interpolative matrix decompositions.
\newblock \emph{Advances in Computational Mathematics}, 43\penalty0
  (3):\penalty0 495--516, 2017.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{datafashionmnist}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv:1708.07747}, 2017.

\bibitem[Yang et~al.(2020)Yang, Wen, and Li]{yang2020hoyer}
Huanrui Yang, Wei Wen, and Hai Li.
\newblock Deephoyer: Learning sparser neural network with differentiable
  scale-invariant sparsity measures.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Zhang et~al.(2015)Zhang, Zou, He, and Sun]{zhang20153dfilter}
Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun.
\newblock Accelerating very deep convolutional networks for classification and
  detection.
\newblock In \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2015.

\bibitem[Zhuang et~al.(2020)Zhuang, Zhang, Huang, Zeng, Shuang, and
  Li]{zhuang2020polar}
Tao Zhuang, Zhixuan Zhang, Yuhen Huang, Xiaoyi Zeng, Kai Shuang, and Xiang Li.
\newblock Neuron-level structured pruning using polarization regularizer.
\newblock In \emph{Advances in neural information processing systems}, 2020.

\bibitem[Zhuang et~al.(2018)Zhuang, Tan, Zhuang, Liu, Guo, Wu1, Huang, and
  Zhu]{zhuang2018dcp}
Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu1,
  Junzhou Huang, and Jinhui Zhu.
\newblock Discrimination-aware channel pruning for deep neural networks.
\newblock In \emph{Advances in neural information processing systems}, 2018.

\end{thebibliography}
