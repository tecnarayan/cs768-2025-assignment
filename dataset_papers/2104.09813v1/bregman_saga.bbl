\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu(2017)]{allen2017katyusha}
Zeyuan Allen-Zhu.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock \emph{The Journal of Machine Learning Research}, 2017.

\bibitem[Antonakopoulos et~al.(2019)Antonakopoulos, Belmega, and
  Mertikopoulos]{antonakopoulos2019adaptive}
Kimon Antonakopoulos, Elena~Veronica Belmega, and Panayotis Mertikopoulos.
\newblock An adaptive mirror-prox algorithm for variational inequalities with
  singular operators.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Antonakopoulos et~al.(2020)Antonakopoulos, Belmega, and
  Mertikopoulos]{antonakopoulos2020online}
Kimon Antonakopoulos, Elena~Veronica Belmega, and Panayotis Mertikopoulos.
\newblock Online and stochastic optimization beyond lipschitz continuity: A
  riemannian approach.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Bach and Moulines(2011)]{moulines2011non}
Francis Bach and Eric Moulines.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In \emph{Advances in neural information processing systems}, 2011.

\bibitem[Barr{\'e} et~al.(2020)Barr{\'e}, Taylor, and
  d’Aspremont]{barre2020complexity}
Mathieu Barr{\'e}, Adrien Taylor, and Alexandre d’Aspremont.
\newblock Complexity guarantees for polyak steps with momentum.
\newblock In \emph{Conference on Learning Theory}, 2020.

\bibitem[Bauschke and Combettes(2011)]{Bauschke2011book}
Heinz Bauschke and Patrick Combettes.
\newblock \emph{Convex analysis and monotone operator theory in Hilbert
  spaces}.
\newblock 2011.

\bibitem[Bauschke and Borwein(1997)]{Bauschke1997}
Heinz~H. Bauschke and Jonathan~M. Borwein.
\newblock {Legendre Functions and the Method of Random Bregman Projections}.
\newblock \emph{Journal of Convex Analysis}, 4\penalty0 (1):\penalty0 27--67,
  1997.

\bibitem[Bauschke et~al.(2017)Bauschke, Bolte, and Teboulle]{Bauschke2017}
Heinz~H. Bauschke, J{\'{e}}r{\^{o}}me Bolte, and Marc Teboulle.
\newblock {A Descent Lemma Beyond Lipschitz Gradient Continuity: First-Order
  Methods Revisited and Applications}.
\newblock \emph{Mathematics of Operations Research}, 42\penalty0 (2):\penalty0
  330--348, 2017.

\bibitem[Beck and Teboulle(2003)]{beck2003mirror}
Amir Beck and Marc Teboulle.
\newblock Mirror descent and nonlinear projected subgradient methods for convex
  optimization.
\newblock \emph{Operations Research Letters}, 31\penalty0 (3):\penalty0
  167--175, 2003.

\bibitem[Bertero et~al.(2009)Bertero, Boccaci, Desidera, and
  Vicidomini]{Review2009}
M~Bertero, P~Boccaci, G~Desidera, and G~Vicidomini.
\newblock {Image deblurring with {P}oisson data: from cells to galaxies}.
\newblock \emph{Inverse Problems}, 25, 2009.

\bibitem[Bolte et~al.(2018)Bolte, Sabach, Teboulle, and
  Vaisbourd]{Bolte2018noncvx}
J{\'{e}}r{\^{o}}me Bolte, Shoham Sabach, Marc Teboulle, and Yakov Vaisbourd.
\newblock {First order methods beyond convexity and {L}ipschitz gradient
  continuity with applications to quadratic inverse problems}.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (3), 2018.

\bibitem[Bottou(2012)]{bottou2012stochastic}
L{\'e}on Bottou.
\newblock Stochastic gradient descent tricks.
\newblock In \emph{Neural networks: Tricks of the trade}. Springer, 2012.

\bibitem[Bubeck(2011)]{Bubeck2011}
S{\'{e}}bastien Bubeck.
\newblock {Introduction to online optimization}.
\newblock \emph{Lecture Notes}, 2011.

\bibitem[Davis et~al.(2018)Davis, Drusvyatskiy, and
  MacPhee]{Davis2018StochasticMM}
D.~Davis, D.~Drusvyatskiy, and Kellie~J. MacPhee.
\newblock Stochastic model-based minimization under high-order growth.
\newblock \emph{arXiv preprint arXiv:1807.00255}, 2018.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and Lacoste-Julien]{Defazio2014}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock {SAGA}: a fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Dragomir et~al.(2019)Dragomir, Taylor, d'Aspremont, and
  Bolte]{dragomir2019optimal}
Radu-Alexandru Dragomir, Adrien Taylor, Alexandre d'Aspremont, and
  J{\'e}r{\^o}me Bolte.
\newblock Optimal complexity and certification of {B}regman first-order
  methods.
\newblock \emph{arXiv preprint arXiv:1911.08510. To appear in Mathematical
  Programming}, 2019.

\bibitem[Even and Massoulié(2021)]{even2021concentration}
Mathieu Even and Laurent Massoulié.
\newblock Concentration of non-isotropic random tensors with applications to
  learning and empirical risk minimization, 2021.

\bibitem[Gao et~al.(2020)Gao, Lu, Liu, and Chu]{gao2020randomized}
Tianxiang Gao, Songtao Lu, Jia Liu, and Chris Chu.
\newblock Randomized {B}regman coordinate descent methods for non-{L}ipschitz
  optimization.
\newblock \emph{arXiv preprint arXiv:2001.05202}, 2020.

\bibitem[Gower et~al.(2019)Gower, Loizou, Qian, Sailanbayev, Shulgin, and
  Richt{\'a}rik]{gower2019sgd}
Robert~Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor
  Shulgin, and Peter Richt{\'a}rik.
\newblock Sgd: General analysis and improved rates.
\newblock In \emph{International Conference on Machine Learning}, pages
  5200--5209. PMLR, 2019.

\bibitem[Hanzely and Richt{\'a}rik(2018)]{hanzely2018fastest}
Filip Hanzely and Peter Richt{\'a}rik.
\newblock Fastest rates for stochastic mirror descent methods.
\newblock \emph{arXiv preprint arXiv:1803.07374}, 2018.

\bibitem[Hanzely et~al.(2018)Hanzely, Richt, and Xiao]{Hanzely2018}
Filip Hanzely, Peter Richt, and Lin Xiao.
\newblock {Accelerated Bregman proximal gradient methods for relatively smooth
  convex optimization}.
\newblock \emph{ArXiv preprint arXiv:1808.03045v1}, 2018.

\bibitem[Hendrikx et~al.(2020{\natexlab{a}})Hendrikx, Bach, and
  Massouli{\'e}]{hendrikx2020dual}
Hadrien Hendrikx, Francis Bach, and Laurent Massouli{\'e}.
\newblock Dual-free stochastic decentralized optimization with variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2020{\natexlab{a}}.

\bibitem[Hendrikx et~al.(2020{\natexlab{b}})Hendrikx, Xiao, Bubeck, Bach, and
  Massouli\'e]{hendrikx2020statistically}
Hadrien Hendrikx, Lin Xiao, S\'ebastien Bubeck, Francis Bach, and Laurent
  Massouli\'e.
\newblock Communication-efficient distributed optimization using an approximate
  {N}ewton-type method.
\newblock In \emph{International Conference on Machine Learning},
  2020{\natexlab{b}}.

\bibitem[Hofmann et~al.(2015)Hofmann, Lucchi, Lacoste-Julien, and
  Mcwilliams]{hofmann2015variance}
Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, and Brian Mcwilliams.
\newblock Variance reduced stochastic gradient descent with neighbors.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock \emph{Advances in Neural Information Processing Systems}, 2013.

\bibitem[Kak and Slaney(2001)]{kak2002principles}
Avinash~C. Kak and Malcolm Slaney.
\newblock \emph{Principles of computerized tomographic imaging}.
\newblock SIAM, 2001.

\bibitem[Kakade et~al.(2009)Kakade, Shalev-Shwartz, and
  Tewari]{kakade2009duality}
Sham Kakade, Shai Shalev-Shwartz, and Ambuj Tewari.
\newblock On the duality of strong convexity and strong smoothness: Learning
  applications and matrix regularization.
\newblock \emph{Unpublished Manuscript, \url{http://ttic.
  uchicago.edu/shai/papers/KakadeShalevTewari09.pdf}}, 2009.

\bibitem[Lewis et~al.(2004)Lewis, Yang, Rose, and Li]{lewis2004rcv1}
David~D Lewis, Yiming Yang, Tony~G Rose, and Fan Li.
\newblock {RCV1}: A new benchmark collection for text categorization research.
\newblock \emph{Journal of machine learning research}, 5\penalty0
  (Apr):\penalty0 361--397, 2004.

\bibitem[Lu(2019)]{lu2019relative}
Haihao Lu.
\newblock “{R}elative continuity” for non-{L}ipschitz nonsmooth convex
  optimization using stochastic (or deterministic) mirror descent.
\newblock \emph{INFORMS Journal on Optimization}, 1\penalty0 (4):\penalty0
  288--303, 2019.

\bibitem[Lu et~al.(2018)Lu, Freund, and Nesterov]{lu2018relatively}
Haihao Lu, Robert~M Freund, and Yurii Nesterov.
\newblock Relatively smooth convex optimization by first-order methods, and
  applications.
\newblock \emph{SIAM Journal on Optimization}, 2018.

\bibitem[Mishchenko(2019)]{mishchenko2019sinkhorn}
Konstantin Mishchenko.
\newblock Sinkhorn algorithm as a special case of stochastic mirror descent.
\newblock \emph{arXiv preprint arXiv:1909.06918}, 2019.

\bibitem[Nemirovsky and Yudin(1983)]{nemirovsky1983problem}
Arkadi{\u\i}~Semenovich Nemirovsky and David~Borisovich Yudin.
\newblock Problem complexity and method efficiency in optimization.
\newblock 1983.

\bibitem[Nesterov(2003)]{Nesterov2004}
Yuri Nesterov.
\newblock \emph{{Introductory lectures on convex optimization: A basic
  course}}.
\newblock Springer, 2003.

\bibitem[Nesterov(2019)]{Nesterov2019}
Yurii Nesterov.
\newblock {Implementable tensor methods in unconstrained convex optimization}.
\newblock \emph{Mathematical Programming}, 2019.

\bibitem[Pfau(2013)]{pfau2013generalized}
David Pfau.
\newblock A generalized bias-variance decomposition for bregman divergences.
\newblock \emph{Unpublished Manuscript,
  \url{http://davidpfau.com/assets/generalized_bvd_proof.pdf}}, 2013.

\bibitem[Reddi et~al.(2016)Reddi, Kone{\v{c}}n{\`y}, Richt{\'a}rik,
  P{\'o}cz{\'o}s, and Smola]{reddi2016aide}
Sashank~J. Reddi, Jakub Kone{\v{c}}n{\`y}, Peter Richt{\'a}rik, Barnab{\'a}s
  P{\'o}cz{\'o}s, and Alex Smola.
\newblock {AIDE}: Fast and communication efficient distributed optimization.
\newblock \emph{arXiv preprint arXiv:1608.06879}, 2016.

\bibitem[Schmidt et~al.(2013)Schmidt, Roux, and Bach]{SAG}
Mark Schmidt, Nicolas Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, 162, 09 2013.

\bibitem[Shalev-Shwartz(2016)]{shalev2016sdca}
Shai Shalev-Shwartz.
\newblock {SDCA} without duality, regularization, and individual convexity.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Shalev-Shwartz and Zhang(2013)]{shalev2013stochastic}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock \emph{Journal of Machine Learning Research}, 2013.

\bibitem[Shamir et~al.(2014)Shamir, Srebro, and Zhang]{shamir2014communication}
Ohad Shamir, Nati Srebro, and Tong Zhang.
\newblock Communication-efficient distributed optimization using an approximate
  {N}ewton-type method.
\newblock In \emph{International Conference on Machine Learning}, 2014.

\bibitem[{Shepp} and {Vardi}(1982)]{MU-poisson}
L.~A. {Shepp} and Y.~{Vardi}.
\newblock Maximum likelihood reconstruction for emission tomography.
\newblock \emph{IEEE Transactions on Medical Imaging}, 1\penalty0 (2), 1982.

\bibitem[Shi et~al.(2017)Shi, Zhang, and Yu]{shi2017bregman}
Zhan Shi, Xinhua Zhang, and Yaoliang Yu.
\newblock Bregman divergence for stochastic variance reduction: Saddle-point
  and adversarial prediction.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Yuan and Li(2020)]{yuan2020convergence}
Xiao-Tong Yuan and Ping Li.
\newblock On convergence of distributed approximate {N}ewton methods:
  Globalization, sharper bounds and beyond.
\newblock \emph{Journal of Machine Learning Research}, 2020.

\bibitem[Zhang and He(2018)]{zhang2018convergence}
Siqi Zhang and Niao He.
\newblock On the convergence rate of stochastic mirror descent for nonsmooth
  nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1806.04781}, 2018.

\bibitem[Zhou et~al.(2020)Zhou, Sanches~Portella, Schmidt, and
  Harvey]{zhou2020regret}
Yihan Zhou, Victor Sanches~Portella, Mark Schmidt, and Nicholas Harvey.
\newblock Regret bounds without {L}ipschitz continuity: Online learning with
  relative-{L}ipschitz losses.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\end{thebibliography}
