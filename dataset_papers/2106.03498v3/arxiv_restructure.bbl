\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel and Ng(2004)]{abbeel2004apprenticeship}
Pieter Abbeel and Andrew~Y Ng.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{Proceedings of the twenty-first international conference on
  Machine learning}, page~1, 2004.

\bibitem[Amin and Singh(2016)]{amin2016towards}
Kareem Amin and Satinder Singh.
\newblock Towards resolving unidentifiability in inverse reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1601.06569}, 2016.

\bibitem[Amin et~al.(2017)Amin, Jiang, and Singh]{amin2017repeated}
Kareem Amin, Nan Jiang, and Satinder Singh.
\newblock Repeated inverse reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  30:\penalty0 1815--1824, 2017.

\bibitem[Balakrishnan et~al.(2020)Balakrishnan, Nguyen, Low, and
  Soh]{balakrishnan2020efficient}
Sreejith Balakrishnan, Quoc~Phong Nguyen, Bryan Kian~Hsiang Low, and Harold
  Soh.
\newblock Efficient exploration of reward functions in inverse reinforcement
  learning via bayesian optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Bertsekas and Shreve(2004)]{bertsekas2004stochastic}
Dimitir~P Bertsekas and Steven Shreve.
\newblock \emph{Stochastic optimal control: the discrete-time case}.
\newblock 2004.

\bibitem[Boularias et~al.(2011)Boularias, Kober, and
  Peters]{boularias2011relative}
Abdeslam Boularias, Jens Kober, and Jan Peters.
\newblock Relative entropy inverse reinforcement learning.
\newblock In \emph{Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pages 182--189. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Boyd et~al.(1994)Boyd, El~Ghaoui, Feron, and
  Balakrishnan]{boyd1994linear}
Stephen Boyd, Laurent El~Ghaoui, Eric Feron, and Venkataramanan Balakrishnan.
\newblock \emph{Linear matrix inequalities in system and control theory}.
\newblock SIAM, 1994.

\bibitem[Dupuis and Ellis(2011)]{dupuis2011weak}
Paul Dupuis and Richard~S Ellis.
\newblock \emph{A weak convergence approach to the theory of large deviations},
  volume 902.
\newblock John Wiley \& Sons, 2011.

\bibitem[Dvijotham and Todorov(2010)]{dvijotham2010}
Krishnamurthy Dvijotham and Emanuel Todorov.
\newblock Inverse optimal control with linearly-solvable mdps.
\newblock In \emph{Proceedings of the 27th International Conference on
  International Conference on Machine Learning}, ICML'10, page 335–342,
  Madison, WI, USA, 2010. Omnipress.
\newblock ISBN 9781605589077.

\bibitem[Finn et~al.(2016{\natexlab{a}})Finn, Christiano, Abbeel, and
  Levine]{finn2016connection}
Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine.
\newblock A connection between generative adversarial networks, inverse
  reinforcement learning, and energy-based models.
\newblock \emph{arXiv preprint arXiv:1611.03852}, 2016{\natexlab{a}}.

\bibitem[Finn et~al.(2016{\natexlab{b}})Finn, Levine, and
  Abbeel]{finn2016guided}
Chelsea Finn, Sergey Levine, and Pieter Abbeel.
\newblock Guided cost learning: Deep inverse optimal control via policy
  optimization.
\newblock In \emph{International conference on machine learning}, pages 49--58,
  2016{\natexlab{b}}.

\bibitem[Fu et~al.(2018)Fu, Luo, and Levine]{fu2017learning}
Justin Fu, Katie Luo, and Sergey Levine.
\newblock Learning robust rewards with adverserial inverse reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International Conference on Machine Learning}, pages
  1352--1361. PMLR, 2017.

\bibitem[Kalman(1964)]{Kalman64}
R.~E. Kalman.
\newblock {When Is a Linear Control System Optimal?}
\newblock \emph{Journal of Basic Engineering}, 86\penalty0 (1):\penalty0
  51--60, 03 1964.

\bibitem[Keeney and Raiffa(1976)]{keeney_raiffa_1993}
Ralph~L. Keeney and Howard Raiffa.
\newblock \emph{Decisions with Multiple Objectives: Preferences and Value
  Trade-Offs}.
\newblock Wiley, 1976.

\bibitem[Kim et~al.(2021)Kim, Garg, Shiragur, and Ermon]{Kim2021}
Kuno Kim, Shivam Garg, Kirankumar Shiragur, and Stefano Ermon.
\newblock Reward identification in inverse reinforcement learning.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 5496--5505. PMLR,
  18--24 Jul 2021.
\newblock URL \url{http://proceedings.mlr.press/v139/kim21c.html}.

\bibitem[Levine(2018)]{levine2018reinforcement}
Sergey Levine.
\newblock Reinforcement learning and control as probabilistic inference:
  Tutorial and review.
\newblock \emph{arXiv preprint arXiv:1805.00909}, 2018.

\bibitem[Levine et~al.(2011)Levine, Popovic, and Koltun]{levine2011nonlinear}
Sergey Levine, Zoran Popovic, and Vladlen Koltun.
\newblock Nonlinear inverse reinforcement learning with gaussian processes.
\newblock \emph{Advances in neural information processing systems},
  24:\penalty0 19--27, 2011.

\bibitem[Lucas(1976)]{Lucas1976}
Robert~E. Lucas.
\newblock Econometric policy evaluation: A critique.
\newblock \emph{Carnegie-Rochester Conference Series on Public Policy},
  1:\penalty0 19--46, 1976.

\bibitem[Ng and Russell(2000)]{ng2000algorithms}
Andrew~Y Ng and Stuart Russell.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{Proceedings of Seventeenth International Conference on
  Machine Learning}. Citeseer, 2000.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{Ng99policyinvariance}
Andrew~Y. Ng, Daishi Harada, and Stuart Russell.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In \emph{In Proceedings of the Sixteenth International Conference on
  Machine Learning}, pages 278--287. Morgan Kaufmann, 1999.

\bibitem[Puterman(2014)]{puterman2014markov}
Martin~L Puterman.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Ratliff et~al.(2006)Ratliff, Bagnell, and Zinkevich]{ratliff2006}
Nathan~D. Ratliff, J.~Andrew Bagnell, and Martin~A. Zinkevich.
\newblock Maximum margin planning.
\newblock In \emph{Proceedings of the 23rd International Conference on Machine
  Learning}, ICML '06, page 729–736, New York, NY, USA, 2006. Association for
  Computing Machinery.
\newblock ISBN 1595933832.
\newblock \doi{10.1145/1143844.1143936}.
\newblock URL \url{https://doi.org/10.1145/1143844.1143936}.

\bibitem[Russell(1998)]{russell1998learning}
Stuart Russell.
\newblock Learning agents for uncertain environments.
\newblock In \emph{Proceedings of the Eleventh Annual Conference on
  Computational Learning Theory}, pages 101--103, 1998.

\bibitem[Sargent(1978)]{sargent1978estimation}
Thomas~J Sargent.
\newblock Estimation of dynamic labor demand schedules under rational
  expectations.
\newblock \emph{Journal of Political Economy}, 86\penalty0 (6):\penalty0
  1009--1044, 1978.

\bibitem[Seneta(2006)]{Seneta2006}
E.~Seneta.
\newblock \emph{Non-negative Matrices and {M}arkov chains}.
\newblock Springer, revised printing edition, 2006.

\bibitem[Ziebart(2010)]{ziebart2010modeling}
Brian~D Ziebart.
\newblock \emph{{Modeling Purposeful Adaptive Behavior with the Principle of
  Maximum Causal Entropy}}.
\newblock PhD thesis, Carnegie Mellon University, 2010.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and
  Dey]{ziebart2008maximum}
Brian~D Ziebart, Andrew~L Maas, J~Andrew Bagnell, and Anind~K Dey.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{Aaai}, volume~8, pages 1433--1438. Chicago, IL, USA, 2008.

\end{thebibliography}
