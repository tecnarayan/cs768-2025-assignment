\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adebayo et~al.(2018)Adebayo, Gilmer, Muelly, Goodfellow, Hardt, and
  Kim]{adebayo2018sanity}
Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., and Kim, B.
\newblock Sanity checks for saliency maps.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9505--9515, 2018.

\bibitem[Alvarez{-}Melis \& Jaakkola(2018)Alvarez{-}Melis and
  Jaakkola]{alvarez2018robustness}
Alvarez{-}Melis, D. and Jaakkola, T.
\newblock On the robustness of interpretability methods.
\newblock \emph{CoRR}, abs/1806.08049, 2018.

\bibitem[Ancona et~al.(2018)Ancona, Ceolini, {\"O}ztireli, and
  Gross]{ancona2018towards}
Ancona, M., Ceolini, E., {\"O}ztireli, C., and Gross, M.
\newblock Towards better understanding of gradient-based attribution methods
  for deep neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Barocas et~al.(2020)Barocas, Selbst, and Raghavan]{Barocas_2020}
Barocas, S., Selbst, A., and Raghavan, M.
\newblock The hidden assumptions behind counterfactual explanations and
  principal reasons.
\newblock In \emph{ACM Conference on Fairness, Accountability, and
  Transparency}, pp.\  80--89, 2020.

\bibitem[Bastani et~al.(2017)Bastani, Kim, and
  Bastani]{bastani2017interpretability}
Bastani, O., Kim, C., and Bastani, H.
\newblock Interpretability via model extraction.
\newblock \emph{CoRR, abs/1706.09773}, 2017.

\bibitem[Bien \& Tibshirani(2009)Bien and Tibshirani]{bien2009classification}
Bien, J. and Tibshirani, R.
\newblock Classification by set cover: The prototype vector machine.
\newblock \emph{CoRR, abs/0908.2284}, 2009.

\bibitem[Caruana et~al.(2015)Caruana, Lou, Gehrke, Koch, Sturm, and
  Elhadad]{caruana15:intelligible}
Caruana, R., Lou, Y., Gehrke, J., Koch, P., Sturm, M., and Elhadad, N.
\newblock Intelligible models for healthcare: Predicting pneumonia risk and
  hospital 30-day readmission.
\newblock In \emph{ACM SIGKDD Conference on Knowledge Discovery and Data
  Mining}, pp.\  1721--1730, 2015.

\bibitem[Chalasani et~al.(2020)Chalasani, Chen, Chowdhury, Wu, and
  Jha]{pmlr-v119-chalasani20a}
Chalasani, P., Chen, J., Chowdhury, A.~R., Wu, X., and Jha, S.
\newblock Concise explanations of neural networks using adversarial training.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1383--1391, 2020.

\bibitem[Dombrowski et~al.(2019)Dombrowski, Alber, Anders, Ackermann,
  M{\"u}ller, and Kessel]{dombrowski2019explanations}
Dombrowski, A.-K., Alber, M., Anders, C., Ackermann, M., M{\"u}ller, K.-R., and
  Kessel, P.
\newblock Explanations can be manipulated and geometry is to blame.
\newblock \emph{CoRR, abs/1906.07983}, 2019.

\bibitem[Doshi-Velez \& Kim(2017)Doshi-Velez and Kim]{doshi2017towards}
Doshi-Velez, F. and Kim, B.
\newblock Towards a rigorous science of interpretable machine learning.
\newblock \emph{CoRR, abs/1702.08608}, 2017.

\bibitem[Dua \& Graff(2017)Dua and Graff]{Dua:2019}
Dua, D. and Graff, C.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Garreau \& von Luxburg(2020)Garreau and von
  Luxburg]{garreau2020looking}
Garreau, D. and von Luxburg, U.
\newblock Looking deeper into {LIME}.
\newblock \emph{CoRR}, abs/2008.11092, 2020.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Abid, and
  Zou]{ghorbani2019interpretation}
Ghorbani, A., Abid, A., and Zou, J.
\newblock Interpretation of neural networks is fragile.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, volume~33, pp.\
   3681--3688, 2019.

\bibitem[Karimi et~al.(2019)Karimi, Barthe, Balle, and Valera]{MACE}
Karimi, A.-H., Barthe, G., Balle, B., and Valera, I.
\newblock Model-agnostic counterfactual explanations for consequential
  decisions, 2019.

\bibitem[Karimi et~al.(2020{\natexlab{a}})Karimi, Sch{\"o}lkopf, and
  Valera]{karimi2020algorithmic}
Karimi, A.-H., Sch{\"o}lkopf, B., and Valera, I.
\newblock Algorithmic recourse: from counterfactual explanations to
  interventions.
\newblock \emph{CoRR, abs/2002.06278}, 2020{\natexlab{a}}.

\bibitem[Karimi et~al.(2020{\natexlab{b}})Karimi, von K{\"u}gelgen,
  Sch{\"o}lkopf, and Valera]{karimi2020causal}
Karimi, A.-H., von K{\"u}gelgen, J., Sch{\"o}lkopf, B., and Valera, I.
\newblock Algorithmic recourse under imperfect causal knowledge: a
  probabilistic approach.
\newblock \emph{CoRR, abs/2006.06831}, 2020{\natexlab{b}}.

\bibitem[Kim et~al.(2014)Kim, Rudin, and Shah]{kim14:the-bayesian}
Kim, B., Rudin, C., and Shah, J.~A.
\newblock The bayesian case model: A generative approach for case-based
  reasoning and prototype classification.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1952--1960, 2014.

\bibitem[Kim et~al.(2018)Kim, Wattenberg, Gilmer, Cai, Wexler, Vi{\'e}gas, and
  Sayres]{kim2018interpretability}
Kim, B., Wattenberg, M., Gilmer, J., Cai, C.~J., Wexler, J., Vi{\'e}gas, F.~B.,
  and Sayres, R.
\newblock Interpretability beyond feature attribution: Quantitative testing
  with concept activation vectors ({TCAV}).
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Koh \& Liang(2017)Koh and Liang]{koh2017understanding}
Koh, P. and Liang, P.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1885--1894, 2017.

\bibitem[Koltchinskii \& Lounici(2017)Koltchinskii and
  Lounici]{koltchinskii2017}
Koltchinskii, V. and Lounici, K.
\newblock Concentration inequalities and moment bounds for sample covariance
  operators.
\newblock \emph{Bernoulli}, 23\penalty0 (1):\penalty0 110--133, 02 2017.

\bibitem[Lakkaraju \& Bastani(2020)Lakkaraju and Bastani]{lakkaraju2020how}
Lakkaraju, H. and Bastani, O.
\newblock ``{H}ow do {I} fool you?": Manipulating user trust via misleading
  black box explanations.
\newblock In \emph{AAAI Conference on Artificial Intelligence, Ethics,and
  Society}, pp.\  79--85, 2020.

\bibitem[Lakkaraju et~al.(2016)Lakkaraju, Bach, and
  Leskovec]{lakkaraju16:interpretable}
Lakkaraju, H., Bach, S.~H., and Leskovec, J.
\newblock Interpretable decision sets: A joint framework for description and
  prediction.
\newblock In \emph{ACM SIGKDD International Conference on Knowledge Discovery
  and Data Mining}, pp.\  1675--1684, 2016.

\bibitem[Lakkaraju et~al.(2019)Lakkaraju, Kamar, Caruana, and
  Leskovec]{lakkaraju19:faithful}
Lakkaraju, H., Kamar, E., Caruana, R., and Leskovec, J.
\newblock Faithful and customizable explanations of black box models.
\newblock In \emph{AAAI Conference on Artificial Intelligence, Ethics, and
  Society}, pp.\  131--138, 2019.

\bibitem[Landsman \& Nešlehová(2008)Landsman and
  Nešlehová]{LANDSMAN2008912}
Landsman, Z. and Nešlehová, J.
\newblock Stein's lemma for elliptical random vectors.
\newblock \emph{Journal of Multivariate Analysis}, 99\penalty0 (5):\penalty0
  912--927, 2008.

\bibitem[Letham et~al.(2015)Letham, Rudin, McCormick, and
  Madigan]{letham15:interpretable}
Letham, B., Rudin, C., McCormick, T., and Madigan, D.
\newblock Interpretable classifiers using rules and bayesian analysis: Building
  a better stroke prediction model.
\newblock \emph{Annals of Applied Statistics}, 2015.

\bibitem[Levine et~al.(2019)Levine, Singla, and Feizi]{levine2019certifiably}
Levine, A., Singla, S., and Feizi, S.
\newblock Certifiably robust interpretation in deep learning.
\newblock \emph{CoRR, abs/1905.12105}, 2019.

\bibitem[Liu(1994)]{liu1994siegel}
Liu, J.
\newblock Siegel's formula via {Stein's} identities.
\newblock \emph{Statistics \& Probability Letters}, 21\penalty0 (3):\penalty0
  247--251, 1994.

\bibitem[Looveren \& Klaise(2019)Looveren and
  Klaise]{looveren2019interpretable}
Looveren, A. and Klaise, J.
\newblock Interpretable counterfactual explanations guided by prototypes.
\newblock \emph{CoRR, abs/ 1907.02584}, 2019.

\bibitem[Lou et~al.(2012)Lou, Caruana, and Gehrke]{lou2012intelligible}
Lou, Y., Caruana, R., and Gehrke, J.
\newblock Intelligible models for classification and regression.
\newblock In \emph{ACM SIGKDD International Conference on Knowledge Discovery
  and Data Mining}, pp.\  150--158, 2012.

\bibitem[Lundberg \& Lee(2017)Lundberg and Lee]{lundberg2017unified}
Lundberg, S.~M. and Lee, S.-I.
\newblock A unified approach to interpreting model predictions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4765--4774, 2017.

\bibitem[Poyiadzi et~al.(2020)Poyiadzi, Sokol, Santos-Rodriguez, De~Bie, and
  Flach]{FACE}
Poyiadzi, R., Sokol, K., Santos-Rodriguez, R., De~Bie, T., and Flach, P.
\newblock {FACE}: Feasible and actionable counterfactual explanations.
\newblock In \emph{AAAI/ACM Conference on AI, Ethics, and Society}, pp.\
  344–350, 2020.

\bibitem[Ribeiro et~al.(2018)Ribeiro, Singh, and Guestrin]{ribeiro2018anchors}
Ribeiro, M., Singh, S., and Guestrin, C.
\newblock Anchors: High-precision model-agnostic explanations.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, pp.\
  1527--1535, 2018.

\bibitem[Ribeiro et~al.(2016)Ribeiro, Singh, and Guestrin]{ribeiro2016should}
Ribeiro, M.~T., Singh, S., and Guestrin, C.
\newblock ``{W}hy should {I} trust you?" {E}xplaining the predictions of any
  classifier.
\newblock In \emph{ACM SIGKDD International Conference on Knowledge Discovery
  and Data Mining}, pp.\  1135--1144, 2016.

\bibitem[Rudin(2019)]{rudin2019stop}
Rudin, C.
\newblock Stop explaining black box machine learning models for high stakes
  decisions and use interpretable models instead.
\newblock \emph{Nature Machine Intelligence}, 1\penalty0 (5):\penalty0 206,
  2019.

\bibitem[Saka et~al.(2019)Saka, Polat, Katircioglu, and Kastro]{sakar2018}
Saka, O., Polat, O., Katircioglu, M., and Kastro, Y.
\newblock Real-time prediction of online shoppers’ purchasing intention using
  multilayer perceptron and {LSTM} recurrent neural networks.
\newblock \emph{Neural Computing and Applications}, 31:\penalty0 6893--6908,
  2019.

\bibitem[Selvaraju et~al.(2017)Selvaraju, Cogswell, Das, Vedantam, Parikh, and
  Batra]{selvaraju2017grad}
Selvaraju, R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra, D.
\newblock Grad-cam: Visual explanations from deep networks via gradient-based
  localization.
\newblock In \emph{IEEE International Conference on Computer Vision}, pp.\
  618--626, 2017.

\bibitem[Simonyan et~al.(2014)Simonyan, Vedaldi, and
  Zisserman]{simonyan2013saliency}
Simonyan, K., Vedaldi, A., and Zisserman, A.
\newblock Deep inside convolutional networks: Visualising image classification
  models and saliency maps.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Slack et~al.(2020{\natexlab{a}})Slack, Hilgard, Jia, Singh, and
  Lakkaraju]{slack2019can}
Slack, D., Hilgard, S., Jia, E., Singh, S., and Lakkaraju, H.
\newblock How can we fool {LIME} and {SHAP}? adversarial attacks on post hoc
  explanation methods.
\newblock In \emph{AAAI/ACM Conference on AI, Ethics, and Society}, pp.\
  180--186, 2020{\natexlab{a}}.

\bibitem[Slack et~al.(2020{\natexlab{b}})Slack, Hilgard, Singh, and
  Lakkaraju]{slack2020i}
Slack, D., Hilgard, S., Singh, S., and Lakkaraju, H.
\newblock How much should {I} trust you? modeling uncertainty of black box
  explanations.
\newblock \emph{CoRR, abs/2008.05030}, 2020{\natexlab{b}}.

\bibitem[Smilkov et~al.(2017)Smilkov, Thorat, Kim, Vi{\'{e}}gas, and
  Wattenberg]{smilkov2017smoothgrad}
Smilkov, D., Thorat, N., Kim, B., Vi{\'{e}}gas, F., and Wattenberg, M.
\newblock Smoothgrad: {R}emoving noise by adding noise.
\newblock \emph{CoRR}, abs/1706.03825, 2017.

\bibitem[Sundararajan et~al.(2017)Sundararajan, Taly, and
  Yan]{sundararajan2017axiomatic}
Sundararajan, M., Taly, A., and Yan, Q.
\newblock Axiomatic attribution for deep networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3319--3328, 2017.

\bibitem[Tropp(2012)]{Tropp12}
Tropp, J.
\newblock User-friendly tail bounds for sums of random matrices.
\newblock \emph{Foundations of Computational Mathematics}, 12\penalty0
  (4):\penalty0 389--434, 2012.

\bibitem[Ustun et~al.(2019)Ustun, Spangher, and Liu]{ustun2019actionable}
Ustun, B., Spangher, A., and Liu, Y.
\newblock Actionable recourse in linear classification.
\newblock In \emph{ACM Conference on Fairness, Accountability, and
  Transparency}, pp.\  10--19, 2019.

\bibitem[{van Erven} \& {Harremos}(2014){van Erven} and {Harremos}]{divergence}
{van Erven}, T. and {Harremos}, P.
\newblock Rényi divergence and kullback-leibler divergence.
\newblock \emph{IEEE Transactions on Information Theory}, 60\penalty0
  (7):\penalty0 3797--3820, 2014.

\bibitem[Wachter et~al.(2017)Wachter, Mittelstadt, and
  Russell]{wachter2017counterfactual}
Wachter, S., Mittelstadt, B., and Russell, C.
\newblock Counterfactual explanations without opening the black box: Automated
  decisions and the {GDPR}.
\newblock \emph{Harvard Journal of Law \& Technology}, 31:\penalty0 841, 2017.

\bibitem[Zi{k{e}}ba et~al.(2016)Zi{k{e}}ba, Tomczak, and
  Tomczak]{zikeba2016ensemble}
Zi{k{e}}ba, M., Tomczak, S., and Tomczak, J.
\newblock Ensemble boosted trees with synthetic features generation in
  application to bankruptcy prediction.
\newblock \emph{Expert Systems with Applications}, 2016.

\end{thebibliography}
