\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ailon \& Chazelle(2009)Ailon and Chazelle]{ailon2009fast}
Nir Ailon and Bernard Chazelle.
\newblock The fast johnson--lindenstrauss transform and approximate nearest
  neighbors.
\newblock \emph{SIAM Journal on computing}, 39\penalty0 (1):\penalty0 302--322,
  2009.

\bibitem[Allen-Zhu \& Li(2019)Allen-Zhu and Li]{allen2019can}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock What can resnet learn efficiently, going beyond kernels?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9015--9025, 2019.

\bibitem[Allen-Zhu et~al.(2018)Allen-Zhu, Li, and Song]{allen2018convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock \emph{arXiv preprint arXiv:1811.03962}, 2018.

\bibitem[Aomoto(1977)]{aomoto1977analytic}
Kazuhiko Aomoto.
\newblock Analytic structure of schl{\"a}fli function.
\newblock \emph{Nagoya Mathematical Journal}, 68:\penalty0 1--16, 1977.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8139--8148, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Li, Salakhutdinov, Wang,
  and Yu]{arora2019harnessing}
Sanjeev Arora, Simon~S Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and
  Dingli Yu.
\newblock Harnessing the power of infinitely wide deep nets on small-data
  tasks.
\newblock \emph{arXiv preprint arXiv:1910.01663}, 2019{\natexlab{b}}.

\bibitem[Bai \& Lee(2019)Bai and Lee]{bai2019beyond}
Yu~Bai and Jason~D Lee.
\newblock Beyond linearization: On quadratic and higher-order approximation of
  wide neural networks.
\newblock \emph{arXiv preprint arXiv:1910.01619}, 2019.

\bibitem[Bai et~al.(2020)Bai, Krause, Wang, Xiong, and
  Socher]{bai2020taylorized}
Yu~Bai, Ben Krause, Huan Wang, Caiming Xiong, and Richard Socher.
\newblock Taylorized training: Towards better approximation of neural network
  training at finite width.
\newblock \emph{arXiv preprint arXiv:2002.04010}, 2020.

\bibitem[Bietti \& Mairal(2019)Bietti and Mairal]{bietti2019inductive}
Alberto Bietti and Julien Mairal.
\newblock On the inductive bias of neural tangent kernels.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  12873--12884, 2019.

\bibitem[Cao \& Gu(2019)Cao and Gu]{cao2019generalization}
Yuan Cao and Quanquan Gu.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10836--10846, 2019.

\bibitem[Chen et~al.(2019)Chen, Cao, Zou, and Gu]{chen2019much}
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu.
\newblock How much over-parameterization is sufficient to learn deep relu
  networks?
\newblock \emph{arXiv preprint arXiv:1911.12360}, 2019.

\bibitem[Chen et~al.(2020)Chen, Cao, Gu, and Zhang]{chen2020generalized}
Zixiang Chen, Yuan Cao, Quanquan Gu, and Tong Zhang.
\newblock A generalized neural tangent kernel analysis for two-layer neural
  networks.
\newblock \emph{arXiv preprint arXiv:2002.04026}, 2020.

\bibitem[Chizat \& Bach(2018)Chizat and Bach]{chizat2018note}
Lenaic Chizat and Francis Bach.
\newblock A note on lazy training in supervised differentiable programming.
\newblock 2018.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2019lazy}
Lenaic Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2933--2943, 2019.

\bibitem[Cho \& Saul(2010)Cho and Saul]{cho2010large}
Youngmin Cho and Lawrence~K Saul.
\newblock Large-margin classification in infinite neural networks.
\newblock \emph{Neural computation}, 22\penalty0 (10):\penalty0 2678--2697,
  2010.

\bibitem[Cortes et~al.(2012)Cortes, Mohri, and Rostamizadeh]{cortes2012l2}
Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh.
\newblock L2 regularization for learning kernels.
\newblock \emph{arXiv preprint arXiv:1205.2653}, 2012.

\bibitem[Cristianini et~al.(2002)Cristianini, Shawe-Taylor, Elisseeff, and
  Kandola]{cristianini2002kernel}
Nello Cristianini, John Shawe-Taylor, Andre Elisseeff, and Jaz~S Kandola.
\newblock On kernel-target alignment.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  367--373, 2002.

\bibitem[Doshi-Velez \& Kim(2017)Doshi-Velez and Kim]{doshi2017towards}
Finale Doshi-Velez and Been Kim.
\newblock Towards a rigorous science of interpretable machine learning.
\newblock \emph{arXiv preprint arXiv:1702.08608}, 2017.

\bibitem[Du et~al.(2018)Du, Lee, Li, Wang, and Zhai]{du2018gradient}
Simon~S Du, Jason~D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock \emph{arXiv preprint arXiv:1811.03804}, 2018.

\bibitem[Du et~al.(2019)Du, Hou, Salakhutdinov, Poczos, Wang, and
  Xu]{du2019graph}
Simon~S Du, Kangcheng Hou, Russ~R Salakhutdinov, Barnabas Poczos, Ruosong Wang,
  and Keyulu Xu.
\newblock Graph neural tangent kernel: Fusing graph neural networks with graph
  kernels.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5724--5734, 2019.

\bibitem[Fort et~al.(2019)Fort, Nowak, Jastrzebski, and
  Narayanan]{fort2019stiffness}
Stanislav Fort, Pawe{\l}~Krzysztof Nowak, Stanislaw Jastrzebski, and Srini
  Narayanan.
\newblock Stiffness: A new perspective on generalization in neural networks.
\newblock \emph{arXiv preprint arXiv:1901.09491}, 2019.

\bibitem[Garriga-Alonso et~al.(2018)Garriga-Alonso, Rasmussen, and
  Aitchison]{garriga2018deep}
Adri{\`a} Garriga-Alonso, Carl~Edward Rasmussen, and Laurence Aitchison.
\newblock Deep convolutional networks as shallow gaussian processes.
\newblock \emph{arXiv preprint arXiv:1808.05587}, 2018.

\bibitem[Ghorbani et~al.(2019{\natexlab{a}})Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2019limitations}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Limitations of lazy training of two-layers neural network.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9108--9118, 2019{\natexlab{a}}.

\bibitem[Ghorbani et~al.(2019{\natexlab{b}})Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2019linearized}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Linearized two-layers neural networks in high dimension.
\newblock \emph{arXiv preprint arXiv:1904.12191}, 2019{\natexlab{b}}.

\bibitem[G{\"o}nen \& Alpaydin(2011)G{\"o}nen and Alpaydin]{gonen2011multiple}
Mehmet G{\"o}nen and Ethem Alpaydin.
\newblock Multiple kernel learning algorithms.
\newblock \emph{Journal of machine learning research}, 12\penalty0
  (64):\penalty0 2211--2268, 2011.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1321--1330. JMLR. org, 2017.

\bibitem[Hanin \& Nica(2019)Hanin and Nica]{hanin2019finite}
Boris Hanin and Mihai Nica.
\newblock Finite depth and width corrections to the neural tangent kernel.
\newblock \emph{arXiv preprint arXiv:1909.05989}, 2019.

\bibitem[Hazan \& Jaakkola(2015)Hazan and Jaakkola]{hazan2015steps}
Tamir Hazan and Tommi Jaakkola.
\newblock Steps toward deep kernel methods from infinite neural networks.
\newblock \emph{arXiv preprint arXiv:1508.05133}, 2015.

\bibitem[He \& Su(2020)He and Su]{he2019local}
Hangfeng He and Weijie~J. Su.
\newblock The local elasticity of neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Hoeffding et~al.(1948)]{hoeffding1948class}
Wassily Hoeffding et~al.
\newblock A class of statistics with asymptotically normal distribution.
\newblock \emph{The Annals of Mathematical Statistics}, 19\penalty0
  (3):\penalty0 293--325, 1948.

\bibitem[Huang \& Yau(2019)Huang and Yau]{huang2019dynamics}
Jiaoyang Huang and Horng-Tzer Yau.
\newblock Dynamics of deep neural networks and neural tangent hierarchy.
\newblock \emph{arXiv preprint arXiv:1909.08156}, 2019.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  8571--8580, 2018.

\bibitem[Ji \& Telgarsky(2019)Ji and Telgarsky]{ji2019polylogarithmic}
Ziwei Ji and Matus Telgarsky.
\newblock Polylogarithmic width suffices for gradient descent to achieve
  arbitrarily small test error with shallow relu networks.
\newblock \emph{arXiv preprint arXiv:1909.12292}, 2019.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{ICLR}, 2015.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
A~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Masterâ€™s thesis, Department of Computer Science, University
  of Toronto}, 2009.

\bibitem[Lanckriet et~al.(2004)Lanckriet, Cristianini, Bartlett, Ghaoui, and
  Jordan]{lanckriet2004learning}
Gert~RG Lanckriet, Nello Cristianini, Peter Bartlett, Laurent~El Ghaoui, and
  Michael~I Jordan.
\newblock Learning the kernel matrix with semidefinite programming.
\newblock \emph{Journal of Machine learning research}, 5\penalty0
  (Jan):\penalty0 27--72, 2004.

\bibitem[Le~Roux \& Bengio(2007)Le~Roux and Bengio]{le2007continuous}
Nicolas Le~Roux and Yoshua Bengio.
\newblock Continuous neural networks.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  404--411,
  2007.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock \emph{nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Lee et~al.(2017)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl-Dickstein]{lee2017deep}
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel~S Schoenholz, Jeffrey
  Pennington, and Jascha Sohl-Dickstein.
\newblock Deep neural networks as gaussian processes.
\newblock \emph{arXiv preprint arXiv:1711.00165}, 2017.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  8570--8581, 2019.

\bibitem[Li \& Liang(2018)Li and Liang]{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8157--8166, 2018.

\bibitem[Li et~al.(2019)Li, Wang, Yu, Du, Hu, Salakhutdinov, and
  Arora]{li2019enhanced}
Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon~S Du, Wei Hu, Ruslan Salakhutdinov,
  and Sanjeev Arora.
\newblock Enhanced convolutional neural tangent kernels.
\newblock \emph{arXiv preprint arXiv:1911.00809}, 2019.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
  Doll{\'a}r, and Zitnick]{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{European conference on computer vision}, pp.\  740--755.
  Springer, 2014.

\bibitem[Matthews et~al.(2018)Matthews, Rowland, Hron, Turner, and
  Ghahramani]{matthews2018gaussian}
Alexander G de~G Matthews, Mark Rowland, Jiri Hron, Richard~E Turner, and
  Zoubin Ghahramani.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock \emph{arXiv preprint arXiv:1804.11271}, 2018.

\bibitem[Neal(1996)]{neal1996priors}
Radford~M Neal.
\newblock Priors for infinite networks.
\newblock In \emph{Bayesian Learning for Neural Networks}, pp.\  29--53.
  Springer, 1996.

\bibitem[Novak et~al.(2018)Novak, Xiao, Lee, Bahri, Yang, Hron, Abolafia,
  Pennington, and Sohl-Dickstein]{novak2018bayesian}
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron,
  Daniel~A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein.
\newblock Bayesian deep convolutional networks with many channels are gaussian
  processes.
\newblock \emph{arXiv preprint arXiv:1810.05148}, 2018.

\bibitem[Novak et~al.(2020)Novak, Xiao, Hron, Lee, Alemi, Sohl-Dickstein, and
  Schoenholz]{neuraltangents2020}
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander~A. Alemi, Jascha
  Sohl-Dickstein, and Samuel~S. Schoenholz.
\newblock Neural tangents: Fast and easy infinite neural networks in python.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://github.com/google/neural-tangents}.

\bibitem[Ribando(2006)]{ribando2006measuring}
Jason~M Ribando.
\newblock Measuring solid angles beyond dimension three.
\newblock \emph{Discrete \& Computational Geometry}, 36\penalty0 (3):\penalty0
  479--487, 2006.

\bibitem[Tishby \& Zaslavsky(2015)Tishby and Zaslavsky]{tishby2015deep}
Naftali Tishby and Noga Zaslavsky.
\newblock Deep learning and the information bottleneck principle.
\newblock In \emph{2015 IEEE Information Theory Workshop (ITW)}, pp.\  1--5.
  IEEE, 2015.

\bibitem[Tishby et~al.(2000)Tishby, Pereira, and Bialek]{tishby2000information}
Naftali Tishby, Fernando~C Pereira, and William Bialek.
\newblock The information bottleneck method.
\newblock \emph{arXiv preprint physics/0004057}, 2000.

\bibitem[Van~der Vaart(2000)]{van2000asymptotic}
Aad~W Van~der Vaart.
\newblock \emph{Asymptotic statistics}, volume~3.
\newblock Cambridge university press, 2000.

\bibitem[Wei et~al.(2019)Wei, Lee, Liu, and Ma]{wei2019regularization}
Colin Wei, Jason~D Lee, Qiang Liu, and Tengyu Ma.
\newblock Regularization matters: Generalization and optimization of neural
  nets vs their induced kernel.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9709--9721, 2019.

\bibitem[Williams(1997)]{williams1997computing}
Christopher~KI Williams.
\newblock Computing with infinite networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  295--301, 1997.

\bibitem[Yang(2019)]{yang2019scaling}
Greg Yang.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian
  process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock \emph{arXiv preprint arXiv:1902.04760}, 2019.

\bibitem[Yehudai \& Shamir(2019)Yehudai and Shamir]{yehudai2019power}
Gilad Yehudai and Ohad Shamir.
\newblock On the power and limitations of random features for understanding
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6594--6604, 2019.

\bibitem[Zou et~al.(2020)Zou, Cao, Zhou, and Gu]{zou2020gradient}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Gradient descent optimizes over-parameterized deep relu networks.
\newblock \emph{Machine Learning}, 109\penalty0 (3):\penalty0 467--492, 2020.

\end{thebibliography}
