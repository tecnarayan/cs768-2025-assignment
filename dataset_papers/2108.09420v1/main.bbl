\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{MOSW21}

\bibitem[AC06]{ac06}
Nir Ailon and Bernard Chazelle.
\newblock Approximate nearest neighbors and the fast johnson-lindenstrauss
  transform.
\newblock In {\em STOC}, pages 557–--563, 2006.

\bibitem[ACSS20]{acss20}
Josh Alman, Timothy Chu, Aaron Schild, and Zhao Song.
\newblock Algorithms and hardness for linear algebra on geometric graphs.
\newblock In {\em FOCS}, 2020.

\bibitem[ACW17a]{acw17b}
H.~Avron, K.~Clarkson, and D.~Woodruff.
\newblock Sharper bounds for regularized data fitting.
\newblock In {\em APPROX-RANDOM}, 2017.

\bibitem[ACW17b]{acw17}
Haim Avron, Kenneth~L Clarkson, and David~P Woodruff.
\newblock Faster kernel ridge regression using sketching and preconditioning.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  38(4):1116--1138, 2017.

\bibitem[AKK{\etalchar{+}}20]{akk+20}
Thomas~D Ahle, Michael Kapralov, Jakob~BT Knudsen, Rasmus Pagh, Ameya
  Velingker, David~P Woodruff, and Amir Zandieh.
\newblock Oblivious sketching of high-degree polynomial kernels.
\newblock In {\em Proceedings of the Fourteenth Annual ACM-SIAM Symposium on
  Discrete Algorithms (SODA)}, pages 141--160. SIAM, 2020.

\bibitem[AKM{\etalchar{+}}17]{akmmvz17}
Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya
  Velingker, and Amir Zandieh.
\newblock Random fourier features for kernel ridge regression: Approximation
  bounds and statistical guarantees.
\newblock In {\em ICML}, 2017.

\bibitem[ALS{\etalchar{+}}18]{alszz18}
Alexandr Andoni, Chengyu Lin, Ying Sheng, Peilin Zhong, and Ruiqi Zhong.
\newblock Subspace embedding and linear regression with orlicz norm.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  224--233. PMLR, 2018.

\bibitem[AM15]{am15}
Ahmed Alaoui and Michael~W Mahoney.
\newblock Fast randomized kernel ridge regression with statistical guarantees.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 775--783, 2015.

\bibitem[ANW14]{anw14}
Haim Avron, Huy Nguyen, and David Woodruff.
\newblock Subspace embeddings for the polynomial kernel.
\newblock In Z.~Ghahramani, M.~Welling, C.~Cortes, N.~D. Lawrence, and K.~Q.
  Weinberger, editors, {\em Advances in Neural Information Processing Systems
  27}, pages 2258--2266. 2014.

\bibitem[AZLS19a]{als19b}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In {\em ICML}, 2019.

\bibitem[AZLS19b]{als19a}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock On the convergence rate of training recurrent neural networks.
\newblock In {\em NeurIPS}, 2019.

\bibitem[Bac13]{b13}
Francis Bach.
\newblock Sharp analysis of low-rank kernel matrix approximations.
\newblock In {\em Conference on Learning Theory (COLT)}, pages 185--209, 2013.

\bibitem[BPSW21]{bpsw21}
Jan van~den Brand, Binghui Peng, Zhao Song, and Omri Weinstein.
\newblock Training (overparametrized) neural networks in near-linear time.
\newblock In {\em ITCS}, 2021.

\bibitem[BW14]{bw14}
Christos Boutsidis and David~P Woodruff.
\newblock Optimal cur matrix decompositions.
\newblock In {\em Proceedings of the 46th Annual ACM Symposium on Theory of
  Computing (STOC)}, pages 353--362. ACM, 2014.

\bibitem[BWZ16]{bwz16}
Christos Boutsidis, David~P Woodruff, and Peilin Zhong.
\newblock Optimal principal component analysis in distributed and streaming
  models.
\newblock In {\em Proceedings of the forty-eighth annual ACM symposium on
  Theory of Computing (STOC)}, pages 236--249, 2016.

\bibitem[CHC{\etalchar{+}}10]{chc+10}
Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, and Chih-Jen
  Lin.
\newblock Training and testing low-degree polynomial data mappings via linear
  svm.
\newblock {\em Journal of Machine Learning Research}, pages 1471--1490, 2010.

\bibitem[COCF16]{cocf16}
Kurt Cutajar, Michael~A. Osborne, John~P. Cunningham, and Maurizio Filippone.
\newblock Preconditioning kernel matrices, 2016.

\bibitem[CS09]{cs09}
Youngmin Cho and Lawrence~K Saul.
\newblock Kernel methods for deep learning.
\newblock In {\em Advances in neural information processing systems (NIPS)},
  pages 342--350, 2009.

\bibitem[CT65]{ct65}
James~W Cooley and John~W Tukey.
\newblock An algorithm for the machine calculation of complex fourier series.
\newblock {\em Mathematics of computation}, 19(90):297--301, 1965.

\bibitem[CW13]{cw13}
Kenneth~L. Clarkson and David~P. Woodruff.
\newblock Low rank approximation and regression in input sparsity time.
\newblock In {\em Symposium on Theory of Computing Conference (STOCå)}, pages
  81--90, 2013.

\bibitem[DDH07]{ddh07}
James Demmel, Ioana Dumitriu, and Olga Holtz.
\newblock Fast linear algebra is stable.
\newblock {\em Numerische Mathematik}, 108(1):59–91, Oct 2007.

\bibitem[DJS{\etalchar{+}}19]{djssw19}
Huaian Diao, Rajesh Jayaram, Zhao Song, Wen Sun, and David~P Woodruff.
\newblock Optimal sketching for kronecker product regression and low rank
  approximation.
\newblock In {\em NeurIPS}, 2019.

\bibitem[DL20]{dl20}
Reza Drikvandi and Olamide Lawal.
\newblock Sparse principal component analysis for natural language processing.
\newblock {\em Annals of data science.}, 2020.

\bibitem[DSSW18]{dssw18}
Huaian Diao, Zhao Song, Wen Sun, and David Woodruff.
\newblock Sketching for kronecker product regression and p-splines.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1299--1308. PMLR, 2018.

\bibitem[DZPS19]{dzps19}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In {\em ICLR}. arXiv preprint arXiv:1810.02054, 2019.

\bibitem[GE08]{ge08}
Yoav Goldberg and Michael Elhadad.
\newblock split{SVM}: Fast, space-efficient, non-heuristic, polynomial kernel
  computation for {NLP} applications.
\newblock In {\em Proceedings of ACL-08: HLT, Short Papers}, pages 237--240,
  Columbus, Ohio, June 2008. Association for Computational Linguistics.

\bibitem[HLSY21]{hlsy21}
Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang.
\newblock Fl-ntk: A neural tangent kernel-based framework for federated
  learning convergence analysis.
\newblock In {\em ICML}, 2021.

\bibitem[HLW17]{hlw17}
Jarvis Haupt, Xingguo Li, and David~P Woodruff.
\newblock Near optimal sketching of low-rank tensor regression.
\newblock In {\em NeurIPS}, 2017.

\bibitem[JGH18]{jgh18}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems (NeurIPS)},
  pages 8571--8580, 2018.

\bibitem[JLSW20]{jlsw20}
Haotian Jiang, Yin~Tat Lee, Zhao Song, and Sam Chiu-wai Wong.
\newblock An improved cutting plane method for convex optimization,
  convex-concave games and its applications.
\newblock In {\em STOC}, 2020.

\bibitem[JSWZ21]{jswz20}
Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang.
\newblock Faster dynamic matrix inverse for faster lps.
\newblock In {\em STOC}, 2021.

\bibitem[LDFU13]{ldfu13}
Yichao Lu, Paramveer Dhillon, Dean~P Foster, and Lyle Ungar.
\newblock Faster ridge regression via the subsampled randomized hadamard
  transform.
\newblock In {\em Advances in neural information processing systems}, pages
  369--377, 2013.

\bibitem[LG14]{l14}
Fran{\c{c}}ois Le~Gall.
\newblock Powers of tensors and fast matrix multiplication.
\newblock In {\em Proceedings of the 39th international symposium on symbolic
  and algebraic computation (ISSAC)}, pages 296--303. ACM, 2014.

\bibitem[LL18]{ll18}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In {\em NeurIPS}, 2018.

\bibitem[LSS{\etalchar{+}}20]{lsswy20}
Jason~D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, and Zheng Yu.
\newblock Generalized leverage score sampling for neural networks.
\newblock In {\em NeurIPS}, 2020.

\bibitem[LSZ19]{lsz19}
Yin~Tat Lee, Zhao Song, and Qiuyi Zhang.
\newblock Solving empirical risk minimization in the current matrix
  multiplication time.
\newblock In {\em COLT}, 2019.

\bibitem[MM13]{mm13}
Xiangrui Meng and Michael~W Mahoney.
\newblock Low-distortion subspace embeddings in input-sparsity time and
  applications to robust linear regression.
\newblock In {\em Proceedings of the forty-fifth annual ACM symposium on Theory
  of computing (STOC)}, pages 91--100, 2013.

\bibitem[MM17]{mm17}
Cameron Musco and Christopher Musco.
\newblock Recursive sampling for the nystrom method.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 3833--3845, 2017.

\bibitem[MOSW21]{mosw21}
Alexander Munteanu, Simon Omlor, Zhao Song, and David~P. Woodruff.
\newblock A new initialization technique for reducing the width of neural
  networks.
\newblock {\em manuscript}, 2021.

\bibitem[NJW01]{njw01}
Andrew Ng, Michael Jordan, and Yair Weiss.
\newblock On spectral clustering: Analysis and an algorithm.
\newblock {\em Advances in neural information processing systems}, 14:849--856,
  2001.

\bibitem[NN13]{nn13}
Jelani Nelson and Huy~L Nguy{\^e}n.
\newblock Osnap: Faster numerical linear algebra algorithms via sparser
  subspace embeddings.
\newblock In {\em 2013 IEEE 54th Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 117--126. IEEE, 2013.

\bibitem[RR07]{rr07}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In {\em NIPS}, volume~3, page~5. Citeseer, 2007.

\bibitem[Sar06]{s06}
Tamas Sarlos.
\newblock Improved approximation algorithms for large matrices via random
  projections.
\newblock In {\em 2006 47th Annual IEEE Symposium on Foundations of Computer
  Science (FOCS)}, pages 143--152. IEEE, 2006.

\bibitem[SGV98]{sgv98}
Craig Saunders, Alexander Gammerman, and Volodya Vovk.
\newblock Ridge regression learning algorithm in dual variables.
\newblock In {\em Proceedings of the Fifteenth International Conference on
  Machine Learning}, ICML '98, page 515–521, San Francisco, CA, USA, 1998.
  Morgan Kaufmann Publishers Inc.

\bibitem[She94]{s94}
Jonathan~Richard Shewchuk.
\newblock An introduction to the conjugate gradient method without the
  agonizing pain.
\newblock Technical report, Carnegie Mellon University, 1994.

\bibitem[SWZ17]{swz17}
Zhao Song, David~P Woodruff, and Peilin Zhong.
\newblock Low rank approximation with entrywise $\ell_1$-norm error.
\newblock In {\em Proceedings of the 49th Annual Symposium on the Theory of
  Computing (STOC)}, 2017.

\bibitem[SWZ19]{swz19_soda}
Zhao Song, David~P Woodruff, and Peilin Zhong.
\newblock Relative error tensor low rank approximation.
\newblock In {\em SODA}. arXiv preprint arXiv:1704.08246, 2019.

\bibitem[SY19]{sy19}
Zhao Song and Xin Yang.
\newblock Quadratic suffices for over-parametrization via matrix chernoff
  bound.
\newblock {\em arXiv preprint arXiv:1906.03593}, 2019.

\bibitem[SY21]{sy21}
Zhao Song and Zheng Yu.
\newblock Oblivious sketching-based central path method for solving linear
  programming problems.
\newblock In {\em 38th International Conference on Machine Learning (ICML)},
  2021.

\bibitem[TPK02]{tpk02}
Miguel~L. Teodoro, G.~Phillips, and L.~Kavraki.
\newblock A dimensionality reduction approach to modeling protein flexibility.
\newblock In {\em RECOMB '02}, 2002.

\bibitem[Tro15]{t15}
Joel~A Tropp.
\newblock An introduction to matrix concentration inequalities.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  8(1-2):1--230, 2015.

\bibitem[Wil12]{w12}
Virginia~Vassilevska Williams.
\newblock Multiplying matrices faster than coppersmith-winograd.
\newblock In {\em Proceedings of the forty-fourth annual ACM symposium on
  Theory of computing (STOC)}, pages 887--898. ACM, 2012.

\bibitem[Woo14]{w14}
David~P. Woodruff.
\newblock Sketching as a tool for numerical linear algebra.
\newblock {\em Foundations and Trends in Theoretical Computer Science},
  10(1--2):1--157, 2014.

\bibitem[WZ20]{wz20}
David~P Woodruff and Amir Zandieh.
\newblock Near input sparsity time kernel embeddings via adaptive sampling.
\newblock In {\em ICML}. arXiv preprint arXiv:2007.03927, 2020.

\bibitem[XLS17]{xls17}
Bo~Xie, Yingyu Liang, and Le~Song.
\newblock Diverse neural network learns true target functions.
\newblock In {\em Artificial Intelligence and Statistics (AISTATS)}, pages
  1216--1224, 2017.

\bibitem[ZDW15]{zdw15}
Yuchen Zhang, John Duchi, and Martin Wainwright.
\newblock Divide and conquer kernel ridge regression: A distributed algorithm
  with minimax optimal rates.
\newblock {\em The Journal of Machine Learning Research}, 16(1):3299--3340,
  2015.

\bibitem[ZNV{\etalchar{+}}20]{znvkr20}
Amir Zandieh, Navid Nouri, Ameya Velingker, Michael Kapralov, and Ilya
  Razenshteyn.
\newblock Scaling up kernel ridge regression via locality sensitive hashing.
\newblock In {\em AISTATS}, 2020.

\end{thebibliography}
