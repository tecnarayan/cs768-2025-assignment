\begin{thebibliography}{10}

\bibitem{Shwartz-Ziv2022}
Ravid Shwartz-Ziv and Amitai Armon.
\newblock Tabular data: Deep learning is not all you need.
\newblock {\em Information Fusion}, 81:84--90, 2022.

\bibitem{Abdar2021}
Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li~Liu,
  Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U.~Rajendra
  Acharya, Vladimir Makarenkov, and Saeid Nahavandi.
\newblock A review of uncertainty quantification in deep learning: Techniques,
  applications and challenges.
\newblock {\em Information Fusion}, 76:243--297, 2021.

\bibitem{Topol2019}
Eric Topol.
\newblock High-performance medicine: the convergence of human and artificial
  intelligence.
\newblock {\em Nature Medicine}, 25:44--56, 2019.

\bibitem{Sorin2020}
Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Macesanu.
\newblock A survey of deep learning techniques for autonomous driving.
\newblock {\em Journal of Field Robotics}, 37(3):362--386, 2020.

\bibitem{Richardson2007}
Matthew Richardson, Ewa Dominowska, and Robert Ragno.
\newblock Predicting clicks: estimating the click-through rate for new ads.
\newblock In {\em Proceedings of the 16th International Conference on World
  Wide Web}, page 521–530, 2007.

\bibitem{Burges2010}
Christopher Burges.
\newblock From ranknet to lambdarank to lambdamart: An overview.
\newblock {\em Learning}, 11, 2010.

\bibitem{Roe2005}
Byron~P. Roe, Hai-Jun Yang, Ji~Zhu, Yong Liu, Ion Stancu, and Gordon McGregor.
\newblock Boosted decision trees as an alternative to artificial neural
  networks for particle identification.
\newblock {\em Nuclear Instruments and Methods in Physics Research Section A:
  Accelerators, Spectrometers, Detectors and Associated Equipment},
  543(2):577--584, 2005.

\bibitem{Bennett2007}
James Bennett and Stan Lanning.
\newblock The {Netflix} prize.
\newblock In {\em Proceedings of the KDD Cup Workshop 2007}, pages 3--6, 2007.

\bibitem{Gal2016}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a {Bayesian} approximation: Representing model uncertainty
  in deep learning.
\newblock In {\em Proceedings of The 33rd International Conference on Machine
  Learning}, volume~48 of {\em Proceedings of Machine Learning Research}, pages
  1050--1059. PMLR, 2016.

\bibitem{Lakshminarayanan2017}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~30, 2017.

\bibitem{Sensoy2018}
Murat Sensoy, Lance Kaplan, and Melih Kandemir.
\newblock Evidential deep learning to quantify classification uncertainty.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~31, 2018.

\bibitem{Gawlikowski2023}
Jakob Gawlikowski, Cedrique Rovile~Njieutcheu Tassi, Mohsin Ali, Jongseok Lee,
  Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung,
  Ribana Roscher, Muhammad Shahzad, Wen Yang, Richard Bamler, and Xiao~Xiang
  Zhu.
\newblock A survey of uncertainty in deep neural networks.
\newblock {\em Artificial Intelligence Review}, 56:1513--1589, 2023.

\bibitem{Amini2020}
Alexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela Rus.
\newblock Deep evidential regression.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 14927--14937, 2020.

\bibitem{Charpentier2020}
Bertrand Charpentier, Daniel Z\"{u}gner, and Stephan G\"{u}nnemann.
\newblock Posterior network: Uncertainty estimation without {OOD} samples via
  density-based pseudo-counts.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 1356--1367. Curran Associates, Inc., 2020.

\bibitem{Ulmer2023}
Dennis~Thomas Ulmer, Christian Hardmeier, and Jes Frellsen.
\newblock Prior and posterior networks: A survey on evidential deep learning
  methods for uncertainty estimation.
\newblock {\em Transactions on Machine Learning Research}, 2023.

\bibitem{Capellier2019}
Edouard Capellier, Franck Davoine, Veronique Cherfaoui, and You Li.
\newblock Evidential deep learning for arbitrary lidar object classification in
  the context of autonomous driving.
\newblock In {\em 2019 IEEE Intelligent Vehicles Symposium (IV)}, pages
  1304--1311, 2019.

\bibitem{Hemmer2020}
Patrick Hemmer, Niklas Kühl, and Jakob Schöffer.
\newblock Deal: Deep evidential active learning for image classification.
\newblock In {\em 2020 19th IEEE International Conference on Machine Learning
  and Applications (ICMLA)}, pages 865--870, 2020.

\bibitem{Soleimany2021}
Ava~P. Soleimany, Alexander Amini, Samuel Goldman, Daniela Rus, Sangeeta~N.
  Bhatia, and Connor~W. Coley.
\newblock Evidential deep learning for guided molecular property prediction and
  discovery.
\newblock {\em ACS Central Science}, 7(8):1356--1367, 2021.

\bibitem{Gawlikowski2022}
Jakob Gawlikowski, Sudipan Saha, Anna Kruspe, and Xiao~Xiang Zhu.
\newblock An advanced {Dirichlet} prior network for out-of-distribution
  detection in remote sensing.
\newblock {\em IEEE Transactions on Geoscience and Remote Sensing}, 60:1--19,
  2022.

\bibitem{Gelman2013}
Andrew Gelman, John~B. Carlin, Hal~S. Stern, David~B. Dunson, Aki Vehtari, and
  Donald~B. Rubin.
\newblock {\em {Bayesian} Data Analysis}.
\newblock Chapman and Hall/CRC, 3rd ed. edition, 2013.

\bibitem{Chen2016}
Tianqi Chen and Carlos Guestrin.
\newblock {XGBoost}: A scalable tree boosting system.
\newblock In {\em Proceedings of the 22nd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, KDD '16, page 785–794, 2016.

\bibitem{Ke2017}
Guolin Ke, Qi~Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei
  Ye, and Tie-Yan Liu.
\newblock {LightGBM}: A highly efficient gradient boosting decision tree.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~30, 2017.

\bibitem{Villani2003}
C'{e}dric Villani.
\newblock {\em Topics in Optimal Transportation}.
\newblock Americal Mathematical Society, 2003.

\bibitem{Ambrosio2005}
Luigi Ambrosio, Nicola Gigli, and Giuseppe Savaré.
\newblock {\em Gradient Flows In Metric Spaces and in the Space of Probability
  Measures}.
\newblock Birkhäuser Basel, 2005.

\bibitem{Santambrogio2017}
Filippo Santambrogio.
\newblock \{Euclidean, metric, and Wasserstein\} gradient flows: an overview.
\newblock {\em Bulletin of Mathematical Sciences}, 7:87--154, 2017.

\bibitem{Liu2017}
Qiang Liu.
\newblock {Stein} variational gradient descent as gradient flow.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~30, 2017.

\bibitem{Carrillo2019}
Jos'{e}~Antonio Carrillo, Katy Craig, and Francesco~S. Patacchini.
\newblock A blob method for diffusion.
\newblock {\em Calculus of Variations and Partial Differential Equations},
  58(53), 2019.

\bibitem{Wang2022}
Yifei Wang, Peng Chen, and Wuchen Li.
\newblock Projected {Wasserstein} gradient descent for high-dimensional
  {Bayesian} inference.
\newblock {\em SIAM/ASA Journal on Uncertainty Quantification},
  10(4):1513--1532, 2022.

\bibitem{Maoutsa2020}
Dimitra Maoutsa, Sebastian Reich, and Manfred Opper.
\newblock Interacting particle solutions of {Fokker-Planck} equations through
  gradient-log-density estimation.
\newblock {\em Entropy (Basel)}, 22(8):802, 2020.

\bibitem{He20202}
Ye~He, Krishnakumar Balasubramanian, Bharath~K. Sriperumbudur, and Jianfeng Lu.
\newblock Regularized {Stein} variational gradient flow.
\newblock {\em arXiv:2211.07861}, 2022.

\bibitem{Friedman2001}
Jerome~H. Friedman.
\newblock Greedy function approximation: A gradient boosting machine.
\newblock {\em The Annals of Statistics}, 29(5):1189--1232, 2001.

\bibitem{Duan2020}
Tony Duan, Avati Anand, Daisy~Yi Ding, Khanh~K. Thai, Sanjay Basu, Andrew Ng,
  and Alejandro Schuler.
\newblock {NGB}oost: Natural gradient boosting for probabilistic prediction.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, volume 119 of {\em Proceedings of Machine Learning Research},
  pages 2690--2700. PMLR, 2020.

\bibitem{Buhlmann2007}
Peter B{\"u}hlmann and Torsten Hothorn.
\newblock {Boosting Algorithms: Regularization, Prediction and Model Fitting}.
\newblock {\em Statistical Science}, 22(4):477 -- 505, 2007.

\bibitem{Grinsztajn2022}
Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux.
\newblock Why do tree-based models still outperform deep learning on typical
  tabular data?
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~35, pages 507--520, 2022.

\bibitem{Florek2023}
Piotr Florek and Adam Zagdański.
\newblock Benchmarking state-of-the-art gradient boosting algorithms for
  classification.
\newblock {\em arXiv:2305.17094}, 2023.

\bibitem{Zhang2021}
Zhendong Zhang and Cheolkon Jung.
\newblock {GBDT-MO}: Gradient-boosted decision trees for multiple outputs.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  32(7):3156--3167, 2021.

\bibitem{Chen2015}
Tianqi Chen, Sameer Singh, Ben Taskar, and Carlos Guestrin.
\newblock {Efficient Second-Order Gradient Boosting for Conditional Random
  Fields}.
\newblock In {\em Proceedings of the Eighteenth International Conference on
  Artificial Intelligence and Statistics}, volume~38 of {\em Proceedings of
  Machine Learning Research}, pages 147--155. PMLR, 2015.

\bibitem{Friedman2002}
Jerome~H. Friedman.
\newblock Stochastic gradient boosting.
\newblock {\em Computational Statistics \& Data Analysis}, 38(4):367--378,
  2002.

\bibitem{Detommaso2018}
Gianluca Detommaso, Tiangang Cui, Youssef Marzouk, Alessio Spantini, and Robert
  Scheichl.
\newblock A {Stein} variational {Newton} method.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~31, 2018.

\bibitem{Wang2020}
Yifei Wang and Wuchen Li.
\newblock Information {Newton}'s flow: second-order optimization method in
  probability space.
\newblock {\em arXiv:2001.04341}, 2020.

\bibitem{Ghosh2011}
Malay Ghosh.
\newblock Objective priors: An introduction for frequentists.
\newblock {\em Statistical Science}, 26(2):187--202, 2011.

\bibitem{Aitchison1980}
J.~Aitchison and S.~M. Shen.
\newblock Logistic-normal distributions: Some properties and uses.
\newblock {\em Biometrika}, 67(2):261--272, 1980.

\bibitem{Liu2016}
Qiang Liu and Dilin Wang.
\newblock {Stein} variational gradient descent: A general purpose {Bayesian}
  inference algorithm.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~29, 2016.

\bibitem{Wang2018}
Dilin Wang, Zhe Zeng, and Qiang Liu.
\newblock {Stein} variational message passing for continuous graphical models.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning}, volume~80 of {\em Proceedings of Machine Learning Research}, pages
  5219--5227. PMLR, 2018.

\bibitem{Lambert2021}
Alexander Lambert, Fabio Ramos, Byron Boots, Dieter Fox, and Adam Fishman.
\newblock {Stein} variational model predictive control.
\newblock In {\em Proceedings of the 2020 Conference on Robot Learning}, volume
  155 of {\em Proceedings of Machine Learning Research}, pages 1278--1297.
  PMLR, 2021.

\bibitem{Korba2020}
Anna Korba, Adil Salim, Michael Arbel, Giulia Luise, and Arthur Gretton.
\newblock A non-asymptotic analysis for {Stein} variational gradient descent.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 4672--4682, 2020.

\bibitem{Chewi2020}
Sinho Chewi, Thibaut Le~Gouic, Chen Lu, Tyler Maunu, and Philippe Rigollet.
\newblock {SVGD} as a kernelized {Wasserstein} gradient flow of the chi-squared
  divergence.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 2098--2109, 2020.

\bibitem{Wibisono2018}
Andre Wibisono.
\newblock Sampling as optimization in the space of measures: The {Langevin}
  dynamics as a composite optimization problem.
\newblock In {\em Proceedings of the 31st Conference On Learning Theory},
  volume~75 of {\em Proceedings of Machine Learning Research}, pages
  2093--3027. PMLR, 2018.

\bibitem{Roberts1996}
Gareth~O. Roberts and Richard~L. Tweedie.
\newblock Exponential convergence of {Langevin} distributions and their
  discrete approximations.
\newblock {\em Bernoulli}, 2(4):341 -- 363, 1996.

\bibitem{Hernandez-Lobato2015}
Jose~Miguel Hernandez-Lobato and Ryan Adams.
\newblock Probabilistic backpropagation for scalable learning of {Bayesian}
  neural networks.
\newblock In {\em Proceedings of the 32nd International Conference on Machine
  Learning}, volume~37 of {\em Proceedings of Machine Learning Research}, pages
  1861--1869. PMLR, 2015.

\bibitem{Breiman1984}
Leo Breiman, Jerome Friedman, R.A. Olshen, and Charles~J. Stone.
\newblock {\em Classification and Regression Trees}.
\newblock Chapman and Hall/CRC, 1984.

\bibitem{Hastie2009}
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
\newblock {\em The Elements of Statistical Learning}.
\newblock Springer New York, 2009.

\bibitem{Weisberg1985}
Sanford Weisberg.
\newblock {\em Applied Linear Regression}.
\newblock John Wiley \& Sons, 1985.

\bibitem{Dua2017}
Dheeru Dua and Casey Graff.
\newblock {UCI} machine learning repository, 2017.

\bibitem{Gal2017}
Yarin Gal, Jiri Hron, and Alex Kendall.
\newblock Concrete dropout.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~30, 2017.

\bibitem{Yang2024}
Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu.
\newblock Generalized out-of-distribution detection: A survey.
\newblock {\em arXiv:2110.11334}, 2024.

\bibitem{Malinin2020}
Andrey Malinin, Bruno Mlodozeniec, and Mark Gales.
\newblock Ensemble distribution distillation.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{Santambrogio2015}
Filippo Santambrogio.
\newblock {\em Optimal Transport for Applied Mathematicians}.
\newblock Birkhäuser Cham, 2015.

\bibitem{Yi2023}
Mingxuan Yi and Song Liu.
\newblock Bridging the gap between variational inference and {Wasserstein}
  gradient flows, 2023.

\bibitem{Arbel2019}
Michael Arbel, Anna Korba, Adil Salim, and Arthur Gretton.
\newblock Maximum mean discrepancy gradient flow.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem{Jordan1998}
Richard Jordan, David Kinderlehrer, and Felix Otto.
\newblock The variational formulation of the {Fokker--Planck} equation.
\newblock {\em SIAM Journal on Mathematical Analysis}, 29(1):1--17, 1998.

\bibitem{Pavliotis2014}
Grigorios~A. Pavliotis.
\newblock {\em Stochastic Processes and Applications}.
\newblock Springer New York, 2014.

\bibitem{Paulsen2016}
Vern~I. Paulsen and Mrinal Raghupathi.
\newblock {\em An Introduction to the Theory of Reproducing Kernel {Hilbert}
  Spaces}.
\newblock Cambridge University Press, 2016.

\bibitem{Leviyev2022}
Alex Leviyev, Joshua Chen, Yifei Wang, Omar Ghattas, and Aaron Zimmerman.
\newblock A stochastic {Stein} variational {Newton} method.
\newblock {\em arXiv:2204.09039}, 2022.

\bibitem{Smola2007}
Alex Smola, Arthur Gretton, Le~Song, and Bernhard Sch{\"o}lkopf.
\newblock A {Hilbert} space embedding for distributions.

\bibitem{Amari2016}
Shun ichi Amari.
\newblock {\em Information Geometry and Its Applications}.
\newblock Springer Tokyo, 2016.

\end{thebibliography}
