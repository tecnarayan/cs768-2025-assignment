\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bach et~al.(2012)Bach, Jenatton, Mairal, and Obozinski]{bach:2012}
F.~Bach, R.~Jenatton, J.~Mairal, and G.~Obozinski.
\newblock Optimization with sparsity-inducing penalties.
\newblock \emph{Foundations and Trends in Machine Learning}, 4\penalty0
  (1):\penalty0 1--106, 2012.

\bibitem[Bengio et~al.(1995)Bengio, Bengio, and Cloutier]{bengio:1995}
S.~Bengio, Y.~Bengio, and J.~Cloutier.
\newblock On the search for new learning rules for {ANN}s.
\newblock \emph{Neural Processing Letters}, 2\penalty0 (4):\penalty0 26--30,
  1995.

\bibitem[Bengio et~al.(1990)Bengio, Bengio, and Cloutier]{bengio:1990}
Y.~Bengio, S.~Bengio, and J.~Cloutier.
\newblock \emph{Learning a synaptic learning rule}.
\newblock Universit{\'e} de Montr{\'e}al, D{\'e}partement d'informatique et de
  recherche op{\'e}rationnelle, 1990.

\bibitem[Bengio et~al.(2013)Bengio, Boulanger-Lewandowski, and
  Pascanu]{bengio:2013}
Y.~Bengio, N.~Boulanger-Lewandowski, and R.~Pascanu.
\newblock Advances in optimizing recurrent networks.
\newblock In \emph{International Conference on Acoustics, Speech and Signal
  Processing}, pages 8624--8628. IEEE, 2013.

\bibitem[Bobolas(2009)]{neuron1}
F.~Bobolas.
\newblock brain-neurons, 2009.
\newblock URL \url{https://www.flickr.com/photos/fbobolas/3822222947}.
\newblock Creative Commons Attribution-ShareAlike 2.0 Generic.

\bibitem[Cotter and Conwell(1990)]{cotter:1990}
N.~E. Cotter and P.~R. Conwell.
\newblock Fixed-weight networks can learn.
\newblock In \emph{International Joint Conference on Neural Networks}, pages
  553--559, 1990.

\bibitem[Daniel et~al.(2016)Daniel, Taylor, and Nowozin]{daniel:2016}
C.~Daniel, J.~Taylor, and S.~Nowozin.
\newblock Learning step size controllers for robust neural network training.
\newblock In \emph{Association for the Advancement of Artificial Intelligence},
  2016.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng:2009}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{Computer Vision and Pattern Recognition}, pages 248--255.
  IEEE, 2009.

\bibitem[Donoho(2006)]{donoho:2006}
D.~L. Donoho.
\newblock Compressed sensing.
\newblock \emph{Transactions on Information Theory}, 52\penalty0 (4):\penalty0
  1289--1306, 2006.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi:2011}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2121--2159,
  2011.

\bibitem[Feldkamp and Puskorius(1998)]{feldkamp:1998}
L.~A. Feldkamp and G.~V. Puskorius.
\newblock A signal processing framework based on dynamic neural networks with
  application to problems in adaptation, filtering, and classification.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2259--2277, 1998.

\bibitem[Gatys et~al.(2015)Gatys, Ecker, and Bethge]{gatys:2015}
L.~A. Gatys, A.~S. Ecker, and M.~Bethge.
\newblock A neural algorithm of artistic style.
\newblock arXiv Report 1508.06576, 2015.

\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihkela]{graves:2014}
A.~Graves, G.~Wayne, and I.~Danihkela.
\newblock Neural {T}uring machines.
\newblock arXiv Report 1410.5401, 2014.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter:1997}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Younger, and
  Conwell]{hochreiter:2001}
S.~Hochreiter, A.~S. Younger, and P.~R. Conwell.
\newblock Learning to learn using gradient descent.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pages 87--94. Springer, 2001.

\bibitem[Kingma and Ba(2015)]{kingma:2015}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Krizhevsky(2009)]{krizhevsky:2009}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Lake et~al.(2016)Lake, Ullman, Tenenbaum, and Gershman]{lake:2016}
B.~M. Lake, T.~D. Ullman, J.~B. Tenenbaum, and S.~J. Gershman.
\newblock Building machines that learn and think like people.
\newblock arXiv Report 1604.00289, 2016.

\bibitem[Maley(2011)]{neuron2}
T.~Maley.
\newblock neuron, 2011.
\newblock URL \url{https://www.flickr.com/photos/taylortotz101/6280077898}.
\newblock Creative Commons Attribution 2.0 Generic.

\bibitem[Martens and Grosse(2015)]{martens:2015}
J.~Martens and R.~Grosse.
\newblock Optimizing neural networks with {K}ronecker-factored approximate
  curvature.
\newblock In \emph{International Conference on Machine Learning}, pages
  2408--2417, 2015.

\bibitem[Nemhauser and Wolsey(1988)]{nemhauser:1988}
G.~L. Nemhauser and L.~A. Wolsey.
\newblock \emph{Integer and combinatorial optimization}.
\newblock John Wiley \& Sons, 1988.

\bibitem[Nesterov(1983)]{nesterov:1983}
Y.~Nesterov.
\newblock A method of solving a convex programming problem with convergence
  rate o (1/k2).
\newblock In \emph{Soviet Mathematics Doklady}, volume~27, pages 372--376,
  1983.

\bibitem[Nocedal and Wright(2006)]{nocedal:2006}
J.~Nocedal and S.~Wright.
\newblock \emph{Numerical optimization}.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Riedmiller and Braun(1993)]{riedmiller:1992}
M.~Riedmiller and H.~Braun.
\newblock A direct adaptive method for faster backpropagation learning: The
  {RPROP} algorithm.
\newblock In \emph{International Conference on Neural Networks}, pages
  586--591, 1993.

\bibitem[Runarsson and Jonsson(2000)]{runarsson:2000}
T.~P. Runarsson and M.~T. Jonsson.
\newblock Evolution and design of distributed learning rules.
\newblock In \emph{IEEE Symposium on Combinations of Evolutionary Computation
  and Neural Networks}, pages 59--63. IEEE, 2000.

\bibitem[Santoro et~al.(2016)Santoro, Bartunov, Botvinick, Wierstra, and
  Lillicrap]{santoro:2016}
A.~Santoro, S.~Bartunov, M.~Botvinick, D.~Wierstra, and T.~Lillicrap.
\newblock Meta-learning with memory-augmented neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Schmidhuber(1987)]{schmidhuber:1987}
J.~Schmidhuber.
\newblock \emph{Evolutionary principles in self-referential learning; On
  learning how to learn: The meta-meta-... hook.}
\newblock PhD thesis, Institut f. Informatik, Tech. Univ. Munich, 1987.

\bibitem[Schmidhuber(1992)]{schmidhuber:1992}
J.~Schmidhuber.
\newblock Learning to control fast-weight memories: An alternative to dynamic
  recurrent networks.
\newblock \emph{Neural Computation}, 4\penalty0 (1):\penalty0 131--139, 1992.

\bibitem[Schmidhuber(1993)]{schmidhuber:1993}
J.~Schmidhuber.
\newblock A neural network that embeds its own meta-levels.
\newblock In \emph{International Conference on Neural Networks}, pages
  407--412. IEEE, 1993.

\bibitem[Schmidhuber et~al.(1997)Schmidhuber, Zhao, and
  Wiering]{schmidhuber:1997}
J.~Schmidhuber, J.~Zhao, and M.~Wiering.
\newblock Shifting inductive bias with success-story algorithm, adaptive levin
  search, and incremental self-improvement.
\newblock \emph{Machine Learning}, 28\penalty0 (1):\penalty0 105--130, 1997.

\bibitem[Schraudolph(1999)]{schraudolph:1999}
N.~N. Schraudolph.
\newblock Local gain adaptation in stochastic gradient descent.
\newblock In \emph{International Conference on Artificial Neural Networks},
  volume~2, pages 569--574, 1999.

\bibitem[Sutton(1992)]{sutton:1992}
R.~S. Sutton.
\newblock Adapting bias by gradient descent: An incremental version of
  delta-bar-delta.
\newblock In \emph{Association for the Advancement of Artificial Intelligence},
  pages 171--176, 1992.

\bibitem[Thrun and Pratt(1998)]{thrun:1998}
S.~Thrun and L.~Pratt.
\newblock \emph{Learning to learn}.
\newblock Springer Science \& Business Media, 1998.

\bibitem[Tieleman and Hinton(2012)]{tieleman:2012}
T.~Tieleman and G.~Hinton.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural Networks for Machine Learning}, 4:\penalty0 2,
  2012.

\bibitem[Tseng(1998)]{tseng:1998}
P.~Tseng.
\newblock An incremental gradient (-projection) method with momentum term and
  adaptive stepsize rule.
\newblock \emph{Journal on Optimization}, 8\penalty0 (2):\penalty0 506--531,
  1998.

\bibitem[Wolpert and Macready(1997)]{wolpert:1997}
D.~H. Wolpert and W.~G. Macready.
\newblock No free lunch theorems for optimization.
\newblock \emph{Transactions on Evolutionary Computation}, 1\penalty0
  (1):\penalty0 67--82, 1997.

\bibitem[Younger et~al.(1999)Younger, Conwell, and Cotter]{younger:1999}
A.~S. Younger, P.~R. Conwell, and N.~E. Cotter.
\newblock Fixed-weight on-line learning.
\newblock \emph{Transactions on Neural Networks}, 10\penalty0 (2):\penalty0
  272--283, 1999.

\bibitem[Younger et~al.(2001)Younger, Hochreiter, and Conwell]{younger:2001}
A.~S. Younger, S.~Hochreiter, and P.~R. Conwell.
\newblock Meta-learning with backpropagation.
\newblock In \emph{International Joint Conference on Neural Networks}, 2001.

\end{thebibliography}
