\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Beltagy, I., Peters, M., and Cohan, A.
\newblock {Longformer: The Long-Document Transformer}.
\newblock Preprint arXiv:2004.05150, 2020.

\bibitem[Bentivogli et~al.(2009)Bentivogli, Dagan, Hoa, Giampiccolo, and
  Magnini]{bentivogli2009fifth}
Bentivogli, L., Dagan, I., Hoa, D., Giampiccolo, D., and Magnini, B.
\newblock {The Fifth PASCAL Recognizing Textual Entailment Challenge}.
\newblock In \emph{TAC 2009 Workshop}, 2009.

\bibitem[Bi et~al.(2020)Bi, Xie, Chen, Wei, and Tian]{bi2020gold}
Bi, K., Xie, L., Chen, X., Wei, L., and Tian, Q.
\newblock {GOLD-NAS: Gradual, One-Level, Differentiable}.
\newblock Preprint arXiv:2007.03331, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock {Language Models are Few-Shot Learners}.
\newblock Preprint arXiv:2005.14165, 2020.

\bibitem[Carion et~al.(2020)Carion, Massa, Synnaeve, Usunier, Kirillov, and
  Zagoruyko]{carion2020end}
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko,
  S.
\newblock {End-to-End Object Detection with Transformers}.
\newblock In \emph{European Conference on Computer Vision}, 2020.

\bibitem[Cer et~al.(2017)Cer, Diab, Agirre, Lopez-Gazpio, and
  Specia]{cer2017semeval}
Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., and Specia, L.
\newblock {SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and
  Crosslingual Focused Evaluation}.
\newblock In \emph{International Workshop on Semantic Evaluation}, 2017.

\bibitem[Chen et~al.(2018)Chen, Zhang, Zhang, and Zhao]{chen2018quora}
Chen, Z., Zhang, H., Zhang, X., and Zhao, L.
\newblock {Quora question pairs}.
\newblock \emph{University of Waterloo}, 2018.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock {Generating Long Sequences with Sparse Transformers}.
\newblock Preprint arXiv:1904.10509, 2019.

\bibitem[Clark et~al.(2019)Clark, Khandelwal, Levy, and Manning]{clark2019does}
Clark, K., Khandelwal, U., Levy, O., and Manning, C.
\newblock {What Does BERT Look at? An Analysis of BERT's Attention}.
\newblock In \emph{ACL Workshop BlackboxNLP}, 2019.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Devlin, J., Chang, M., Lee, K., and Toutanova, K.
\newblock {BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding}.
\newblock In \emph{North American Chapter of the Association for Computational
  Linguistics}, 2019.

\bibitem[Dolan \& Brockett(2005)Dolan and Brockett]{dolan2005automatically}
Dolan, W. and Brockett, C.
\newblock {Automatically Constructing a Corpus of Sentential Paraphrases}.
\newblock In \emph{International Workshop on Paraphrasing}, 2005.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock {An Image is Worth 16x16 Words: Transformers for Image Recognition at
  Scale}.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Elsken et~al.(2019)Elsken, Metzen, and Hutter]{elsken2019neural}
Elsken, T., Metzen, J., and Hutter, F.
\newblock {Neural Architecture Search: A Survey}.
\newblock \emph{Journal of Machine Learning Research}, 2019.

\bibitem[Gong et~al.(2019)Gong, He, Li, Qin, Wang, and Liu]{gong2019efficient}
Gong, L., He, D., Li, Z., Qin, T., Wang, L., and Liu, T.
\newblock {Efficient Training of BERT by Progressively Stacking}.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Guo et~al.(2019)Guo, Qiu, Liu, Shao, Xue, and Zhang]{guo2019star}
Guo, Q., Qiu, X., Liu, P., Shao, Y., Xue, X., and Zhang, Z.
\newblock Star-{T}ransformer.
\newblock In \emph{North American Chapter of the Association for Computational
  Linguistics}, 2019.

\bibitem[Kovaleva et~al.(2019)Kovaleva, Romanov, Rogers, and
  Rumshisky]{kovaleva2019revealing}
Kovaleva, O., Romanov, A., Rogers, A., and Rumshisky, A.
\newblock {Revealing the Dark Secrets of BERT}.
\newblock In \emph{Empirical Methods in Natural Language Processing}, 2019.

\bibitem[Li et~al.(2019)Li, Jin, Xuan, Zhou, Chen, Wang, and
  Yan]{li2019enhancing}
Li, S., Jin, X., Xuan, Y., Zhou, X., Chen, W., Wang, Y., and Yan, X.
\newblock {Enhancing the Locality and Breaking the Memory Bottleneck of
  Transformer on Time Series Forecasting}.
\newblock In \emph{Neural Information Processing Systems}, 2019.

\bibitem[Liu et~al.(2019)Liu, Simonyan, and Yang]{liu2018darts}
Liu, H., Simonyan, K., and Yang, Y.
\newblock {DARTS: Differentiable Architecture Search}.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Maddison et~al.(2017)Maddison, Mnih, and Teh]{maddison2017concrete}
Maddison, C., Mnih, A., and Teh, Y.
\newblock {The Concrete Distribution: A Continuous Relaxation of Discrete
  Random Variables}.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Michel et~al.(2019)Michel, Levy, and Neubig]{michel2019sixteen}
Michel, P., Levy, O., and Neubig, G.
\newblock {Are Sixteen Heads Really Better than One?}
\newblock In \emph{Neural Information Processing Systems}, 2019.

\bibitem[Ott et~al.(2018)Ott, Edunov, Grangier, and Auli]{ott2018scaling}
Ott, M., Edunov, S., Grangier, D., and Auli, M.
\newblock {Scaling Neural Machine Translation}.
\newblock In \emph{Machine Translation}, 2018.

\bibitem[Park et~al.(2019)Park, Na, Jo, Shin, Yoo, Kwon, Zhao, Noh, Lee, and
  Choo]{park2019sanvis}
Park, C., Na, I., Jo, Y., Shin, S., Yoo, J., Kwon, B., Zhao, J., Noh, H., Lee,
  Y., and Choo, J.
\newblock {SANVis: Visual Analytics for Understanding Self-Attention Networks}.
\newblock In \emph{IEEE Visualization Conference}, 2019.

\bibitem[Qiu et~al.(2020)Qiu, Ma, Levy, Yih, Wang, and Tang]{qiu2020blockwise}
Qiu, J., Ma, H., Levy, O., Yih, W., Wang, S., and Tang, J.
\newblock {Blockwise Self-Attention for Long Document Understanding}.
\newblock In \emph{Empirical Methods in Natural Language Processing}, 2020.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang]{rajpurkar2016squad}
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.
\newblock {SQuAD: 100,000+ Questions for Machine Comprehension of Text}.
\newblock In \emph{Empirical Methods in Natural Language Processing}, 2016.

\bibitem[Rajpurkar et~al.(2018)Rajpurkar, Jia, and Liang]{rajpurkar2018know}
Rajpurkar, P., Jia, R., and Liang, P.
\newblock {Know What You Don't Know: Unanswerable Questions for SQuAD}.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics}, 2018.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{socher2013recursive}
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C., Ng, A., and Potts,
  C.
\newblock {Recursive Deep Models for Semantic Compositionality Over a Sentiment
  Treebank}.
\newblock In \emph{Empirical Methods in Natural Language Processing}, 2013.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock {Attention Is All You Need}.
\newblock In \emph{Neural Information Processing Systems}, 2017.

\bibitem[Wang et~al.(2018{\natexlab{a}})Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2018glue}
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.
\newblock {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural
  Language Understanding}.
\newblock In \emph{EMNLP Workshop BlackboxNLP}, 2018{\natexlab{a}}.

\bibitem[Wang et~al.(2018{\natexlab{b}})Wang, Yan, and Wu]{wang2018multi}
Wang, W., Yan, M., and Wu, C.
\newblock {Multi-Granularity Hierarchical Attention Fusion Networks for Reading
  Comprehension and Question Answering}.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics}, 2018{\natexlab{b}}.

\bibitem[Warstadt et~al.(2019)Warstadt, Singh, and Bowman]{warstadt2019neural}
Warstadt, A., Singh, A., and Bowman, S.
\newblock {Neural Network Acceptability Judgments}.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  2019.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{williams2018broad}
Williams, A., Nangia, N., and Bowman, S.
\newblock {A Broad-Coverage Challenge Corpus for Sentence Understanding through
  Inference}.
\newblock In \emph{North American Chapter of the Association for Computational
  Linguistics}, 2018.

\bibitem[Wu et~al.(2016)Wu, Schuster, Chen, Le, Norouzi, Macherey, Krikun, Cao,
  Gao, Macherey, et~al.]{wu2016google}
Wu, Y., Schuster, M., Chen, Z., Le, Q., Norouzi, M., Macherey, W., Krikun, M.,
  Cao, Y., Gao, Q., Macherey, K., et~al.
\newblock {Google's Neural Machine Translation System: Bridging the Gap between
  Human and Machine Translation}.
\newblock Preprint arXiv:1609.08144, 2016.

\bibitem[Xie et~al.(2018)Xie, Zheng, Liu, and Lin]{xie2018snas}
Xie, S., Zheng, H., Liu, C., and Lin, L.
\newblock {SNAS: Stochastic Neural Architecture Search}.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Yun et~al.(2019)Yun, Bhojanapalli, Rawat, Reddi, and
  Kumar]{yun2019Transformers}
Yun, C., Bhojanapalli, S., Rawat, A., Reddi, S., and Kumar, S.
\newblock {Are Transformers universal approximators of sequence-to-sequence
  functions?}
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Yun et~al.(2020)Yun, Chang, Bhojanapalli, Rawat, Reddi, and
  Kumar]{yun2020n}
Yun, C., Chang, Y., Bhojanapalli, S., Rawat, A., Reddi, S., and Kumar, S.
\newblock {${O}(n)$ Connections are Expressive Enough: Universal
  Approximability of Sparse Transformers}.
\newblock In \emph{Neural Information Processing Systems}, 2020.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, et~al.]{zaheer2020big}
Zaheer, M., Guruganesh, G., Dubey, K., Ainslie, J., Alberti, C., Ontanon, S.,
  Pham, P., Ravula, A., Wang, Q., Yang, L., et~al.
\newblock {Big Bird: Transformers for Longer Sequences}.
\newblock In \emph{Neural Information Processing Systems}, 2020.

\bibitem[Zellers et~al.(2018)Zellers, Bisk, Schwartz, and
  Choi]{zellers2018swag}
Zellers, R., Bisk, Y., Schwartz, R., and Choi, Y.
\newblock {SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense
  Inference}.
\newblock In \emph{Empirical Methods in Natural Language Processing}, 2018.

\bibitem[Zhu et~al.(2015)Zhu, Kiros, Zemel, Salakhutdinov, Urtasun, Torralba,
  and Fidler]{zhu2015aligning}
Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A.,
  and Fidler, S.
\newblock {Aligning Books and Movies: Towards Story-like Visual Explanations by
  Watching Movies and Reading Books}.
\newblock In \emph{International Conference on Computer Vision}, 2015.

\end{thebibliography}
