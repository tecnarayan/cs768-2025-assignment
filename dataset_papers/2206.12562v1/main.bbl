\begin{thebibliography}{55}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{}\fi

\bibitem[{Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard et~al.}]{abadi2016tensorflow}
\textsc{Abadi, M.}, \textsc{Barham, P.}, \textsc{Chen, J.}, \textsc{Chen, Z.},
  \textsc{Davis, A.}, \textsc{Dean, J.}, \textsc{Devin, M.}, \textsc{Ghemawat,
  S.}, \textsc{Irving, G.}, \textsc{Isard, M.} \textsc{et~al.} (2016).
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \textit{12th $\{$USENIX$\}$ symposium on operating systems design
  and implementation ($\{$OSDI$\}$ 16)}.

\bibitem[{Bar-Haim et~al.(2006)Bar-Haim, Dagan, Dolan, Ferro and
  Giampiccolo}]{rte2}
\textsc{Bar-Haim, R.}, \textsc{Dagan, I.}, \textsc{Dolan, B.}, \textsc{Ferro,
  L.} and \textsc{Giampiccolo, D.} (2006).
\newblock The second {PASCAL} recognising textual entailment challenge.
\newblock In \textit{Proceedings of the Second {PASCAL} Challenges Workshop on
  Recognising Textual Entailment}.

\bibitem[{Behnke and Heafield(2021)}]{behnke2021pruning}
\textsc{Behnke, M.} and \textsc{Heafield, K.} (2021).
\newblock Pruning neural machine translation for speed using group lasso.
\newblock In \textit{Proceedings of the Sixth Conference on Machine
  Translation}.

\bibitem[{Bentivogli et~al.(2009)Bentivogli, Dagan, Dang, Giampiccolo and
  Magnini}]{rte5}
\textsc{Bentivogli, L.}, \textsc{Dagan, I.}, \textsc{Dang, H.~T.},
  \textsc{Giampiccolo, D.} and \textsc{Magnini, B.} (2009).
\newblock The fifth pascal recognizing textual entailment challenge.
\newblock In \textit{In Proc Text Analysis Conference (TACâ€™09}.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever and
  Amodei}]{brown2020language}
\textsc{Brown, T.~B.}, \textsc{Mann, B.}, \textsc{Ryder, N.}, \textsc{Subbiah,
  M.}, \textsc{Kaplan, J.}, \textsc{Dhariwal, P.}, \textsc{Neelakantan, A.},
  \textsc{Shyam, P.}, \textsc{Sastry, G.}, \textsc{Askell, A.},
  \textsc{Agarwal, S.}, \textsc{Herbert{-}Voss, A.}, \textsc{Krueger, G.},
  \textsc{Henighan, T.}, \textsc{Child, R.}, \textsc{Ramesh, A.},
  \textsc{Ziegler, D.~M.}, \textsc{Wu, J.}, \textsc{Winter, C.}, \textsc{Hesse,
  C.}, \textsc{Chen, M.}, \textsc{Sigler, E.}, \textsc{Litwin, M.},
  \textsc{Gray, S.}, \textsc{Chess, B.}, \textsc{Clark, J.}, \textsc{Berner,
  C.}, \textsc{McCandlish, S.}, \textsc{Radford, A.}, \textsc{Sutskever, I.}
  and \textsc{Amodei, D.} (2020).
\newblock Language models are few-shot learners.
\newblock In \textit{Advances in Neural Information Processing Systems 33:
  Annual Conference on Neural Information Processing Systems 2020, NeurIPS
  2020, December 6-12, 2020, virtual} (H.~Larochelle, M.~Ranzato, R.~Hadsell,
  M.~Balcan and H.~Lin, eds.).

\bibitem[{Cer et~al.(2017)Cer, Diab, Agirre, Lopez-Gazpio and
  Specia}]{sts-b2017}
\textsc{Cer, D.}, \textsc{Diab, M.}, \textsc{Agirre, E.}, \textsc{Lopez-Gazpio,
  I.} and \textsc{Specia, L.} (2017).
\newblock {S}em{E}val-2017 task 1: Semantic textual similarity multilingual and
  crosslingual focused evaluation.
\newblock In \textit{Proceedings of the 11th International Workshop on Semantic
  Evaluation ({S}em{E}val-2017)}. Association for Computational Linguistics,
  Vancouver, Canada.

\bibitem[{Chen et~al.(2020)Chen, Frankle, Chang, Liu, Zhang, Wang and
  Carbin}]{chen2020lottery}
\textsc{Chen, T.}, \textsc{Frankle, J.}, \textsc{Chang, S.}, \textsc{Liu, S.},
  \textsc{Zhang, Y.}, \textsc{Wang, Z.} and \textsc{Carbin, M.} (2020).
\newblock The lottery ticket hypothesis for pre-trained {BERT} networks.
\newblock In \textit{Advances in Neural Information Processing Systems 33:
  Annual Conference on Neural Information Processing Systems 2020, NeurIPS
  2020, December 6-12, 2020, virtual} (H.~Larochelle, M.~Ranzato, R.~Hadsell,
  M.~Balcan and H.~Lin, eds.).

\bibitem[{Dagan et~al.(2006)Dagan, Glickman and Magnini}]{rte1}
\textsc{Dagan, I.}, \textsc{Glickman, O.} and \textsc{Magnini, B.} (2006).
\newblock The pascal recognising textual entailment challenge.
\newblock In \textit{Proceedings of the First International Conference on
  Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object
  Classification, and Recognizing Textual Entailment}. MLCW'05,
  Springer-Verlag, Berlin, Heidelberg.

\bibitem[{Deng et~al.(2009)Deng, Dong, Socher, Li, Li and
  Li}]{deng2009imagenet}
\textsc{Deng, J.}, \textsc{Dong, W.}, \textsc{Socher, R.}, \textsc{Li, L.},
  \textsc{Li, K.} and \textsc{Li, F.} (2009).
\newblock Imagenet: {A} large-scale hierarchical image database.
\newblock In \textit{2009 {IEEE} Computer Society Conference on Computer Vision
  and Pattern Recognition {(CVPR} 2009), 20-25 June 2009, Miami, Florida,
  {USA}}. {IEEE} Computer Society.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee and Toutanova}]{devlin2018bert}
\textsc{Devlin, J.}, \textsc{Chang, M.-W.}, \textsc{Lee, K.} and
  \textsc{Toutanova, K.} (2019).
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \textit{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}. Association for
  Computational Linguistics, Minneapolis, Minnesota.

\bibitem[{Ding et~al.(2019)Ding, Ding, Zhou, Guo, Han and Liu}]{ding2019global}
\textsc{Ding, X.}, \textsc{Ding, G.}, \textsc{Zhou, X.}, \textsc{Guo, Y.},
  \textsc{Han, J.} and \textsc{Liu, J.} (2019).
\newblock Global sparse momentum {SGD} for pruning very deep neural networks.
\newblock In \textit{Advances in Neural Information Processing Systems 32:
  Annual Conference on Neural Information Processing Systems 2019, NeurIPS
  2019, December 8-14, 2019, Vancouver, BC, Canada} (H.~M. Wallach,
  H.~Larochelle, A.~Beygelzimer, F.~d'Alch{\'{e}}{-}Buc, E.~B. Fox and
  R.~Garnett, eds.).

\bibitem[{Dolan and Brockett(2005)}]{mrpc2005}
\textsc{Dolan, W.~B.} and \textsc{Brockett, C.} (2005).
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \textit{Proceedings of the Third International Workshop on
  Paraphrasing ({IWP}2005)}.

\bibitem[{Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly
  et~al.}]{dosovitskiy2020image}
\textsc{Dosovitskiy, A.}, \textsc{Beyer, L.}, \textsc{Kolesnikov, A.},
  \textsc{Weissenborn, D.}, \textsc{Zhai, X.}, \textsc{Unterthiner, T.},
  \textsc{Dehghani, M.}, \textsc{Minderer, M.}, \textsc{Heigold, G.},
  \textsc{Gelly, S.} \textsc{et~al.} (2020).
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \textit{arXiv preprint arXiv:2010.11929}.

\bibitem[{Fan et~al.(2020)Fan, Grave and Joulin}]{fan2019reducing}
\textsc{Fan, A.}, \textsc{Grave, E.} and \textsc{Joulin, A.} (2020).
\newblock Reducing transformer depth on demand with structured dropout.
\newblock In \textit{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net.

\bibitem[{Frankle and Carbin(2019)}]{frankle2018lottery}
\textsc{Frankle, J.} and \textsc{Carbin, M.} (2019).
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \textit{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net.

\bibitem[{Giampiccolo et~al.(2007)Giampiccolo, Magnini, Dagan and Dolan}]{rte3}
\textsc{Giampiccolo, D.}, \textsc{Magnini, B.}, \textsc{Dagan, I.} and
  \textsc{Dolan, B.} (2007).
\newblock The third {PASCAL} recognizing textual entailment challenge.
\newblock In \textit{Proceedings of the {ACL}-{PASCAL} Workshop on Textual
  Entailment and Paraphrasing}. Association for Computational Linguistics,
  Prague.

\bibitem[{Han et~al.(2016)Han, Liu, Mao, Pu, Pedram, Horowitz and
  Dally}]{han2016eie}
\textsc{Han, S.}, \textsc{Liu, X.}, \textsc{Mao, H.}, \textsc{Pu, J.},
  \textsc{Pedram, A.}, \textsc{Horowitz, M.~A.} and \textsc{Dally, W.~J.}
  (2016).
\newblock Eie: Efficient inference engine on compressed deep neural network.
\newblock \textit{ACM SIGARCH Computer Architecture News}, \textbf{44}
  243--254.

\bibitem[{Han et~al.(2015{\natexlab{a}})Han, Mao and Dally}]{han2015deep}
\textsc{Han, S.}, \textsc{Mao, H.} and \textsc{Dally, W.~J.}
  (2015{\natexlab{a}}).
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \textit{arXiv preprint arXiv:1510.00149}.

\bibitem[{Han et~al.(2015{\natexlab{b}})Han, Pool, Tran and
  Dally}]{han2015learning}
\textsc{Han, S.}, \textsc{Pool, J.}, \textsc{Tran, J.} and \textsc{Dally,
  W.~J.} (2015{\natexlab{b}}).
\newblock Learning both weights and connections for efficient neural network.
\newblock In \textit{Advances in Neural Information Processing Systems 28:
  Annual Conference on Neural Information Processing Systems 2015, December
  7-12, 2015, Montreal, Quebec, Canada} (C.~Cortes, N.~D. Lawrence, D.~D. Lee,
  M.~Sugiyama and R.~Garnett, eds.).

\bibitem[{He et~al.(2021{\natexlab{a}})He, Gao and Chen}]{he2021debertav3}
\textsc{He, P.}, \textsc{Gao, J.} and \textsc{Chen, W.} (2021{\natexlab{a}}).
\newblock Debertav3: Improving deberta using electra-style pre-training with
  gradient-disentangled embedding sharing.
\newblock \textit{arXiv preprint arXiv:2111.09543}.

\bibitem[{He et~al.(2021{\natexlab{b}})He, Liu, Gao and Chen}]{he2021deberta}
\textsc{He, P.}, \textsc{Liu, X.}, \textsc{Gao, J.} and \textsc{Chen, W.}
  (2021{\natexlab{b}}).
\newblock Deberta: Decoding-enhanced bert with disentangled attention.
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Krizhevsky(2009)}]{Krizhevsky09learningmultiple}
\textsc{Krizhevsky, A.} (2009).
\newblock Learning multiple layers of features from tiny images.
\newblock Tech. rep.

\bibitem[{Krizhevsky et~al.(2009)Krizhevsky, Hinton
  et~al.}]{krizhevsky2009learning}
\textsc{Krizhevsky, A.}, \textsc{Hinton, G.} \textsc{et~al.} (2009).
\newblock Learning multiple layers of features from tiny images.

\bibitem[{Lagunas et~al.(2021)Lagunas, Charlaix, Sanh and
  Rush}]{lagunas2021block}
\textsc{Lagunas, F.}, \textsc{Charlaix, E.}, \textsc{Sanh, V.} and
  \textsc{Rush, A.~M.} (2021).
\newblock Block pruning for faster transformers.
\newblock \textit{arXiv preprint arXiv:2109.04838}.

\bibitem[{Lai et~al.(1985)Lai, Robbins et~al.}]{lai1985asymptotically}
\textsc{Lai, T.~L.}, \textsc{Robbins, H.} \textsc{et~al.} (1985).
\newblock Asymptotically efficient adaptive allocation rules.
\newblock \textit{Advances in applied mathematics}, \textbf{6} 4--22.

\bibitem[{LeCun et~al.(1990)LeCun, Denker and Solla}]{lecun1990optimal}
\textsc{LeCun, Y.}, \textsc{Denker, J.~S.} and \textsc{Solla, S.~A.} (1990).
\newblock Optimal brain damage.
\newblock In \textit{Advances in neural information processing systems}.

\bibitem[{Lee et~al.(2019)Lee, Ajanthan and Torr}]{lee2018snip}
\textsc{Lee, N.}, \textsc{Ajanthan, T.} and \textsc{Torr, P. H.~S.} (2019).
\newblock Snip: single-shot network pruning based on connection sensitivity.
\newblock In \textit{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net.

\bibitem[{Levesque et~al.(2012)Levesque, Davis and Morgenstern}]{wnli2012}
\textsc{Levesque, H.}, \textsc{Davis, E.} and \textsc{Morgenstern, L.} (2012).
\newblock The winograd schema challenge.
\newblock In \textit{Thirteenth International Conference on the Principles of
  Knowledge Representation and Reasoning}.

\bibitem[{Liang et~al.(2021)Liang, Zuo, Chen, Jiang, Liu, He, Zhao and
  Chen}]{liang2021super}
\textsc{Liang, C.}, \textsc{Zuo, S.}, \textsc{Chen, M.}, \textsc{Jiang, H.},
  \textsc{Liu, X.}, \textsc{He, P.}, \textsc{Zhao, T.} and \textsc{Chen, W.}
  (2021).
\newblock Super tickets in pre-trained language models: From model compression
  to improving generalization.
\newblock In \textit{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}. Association for
  Computational Linguistics, Online.

\bibitem[{Liu et~al.(2019{\natexlab{a}})Liu, He, Chen and Gao}]{liu2019multi}
\textsc{Liu, X.}, \textsc{He, P.}, \textsc{Chen, W.} and \textsc{Gao, J.}
  (2019{\natexlab{a}}).
\newblock Multi-task deep neural networks for natural language understanding.
\newblock In \textit{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}. Association for Computational Linguistics,
  Florence, Italy.

\bibitem[{Liu et~al.(2020)Liu, Wang, Ji, Cheng, Zhu, Awa, He, Chen, Poon, Cao
  and Gao}]{mtdnn2020demo}
\textsc{Liu, X.}, \textsc{Wang, Y.}, \textsc{Ji, J.}, \textsc{Cheng, H.},
  \textsc{Zhu, X.}, \textsc{Awa, E.}, \textsc{He, P.}, \textsc{Chen, W.},
  \textsc{Poon, H.}, \textsc{Cao, G.} and \textsc{Gao, J.} (2020).
\newblock The {M}icrosoft toolkit of multi-task deep neural networks for
  natural language understanding.
\newblock In \textit{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics: System Demonstrations}. Association for
  Computational Linguistics, Online.

\bibitem[{Liu et~al.(2019{\natexlab{b}})Liu, Ott, Goyal, Du, Joshi, Chen, Levy,
  Lewis, Zettlemoyer and Stoyanov}]{liu2019roberta}
\textsc{Liu, Y.}, \textsc{Ott, M.}, \textsc{Goyal, N.}, \textsc{Du, J.},
  \textsc{Joshi, M.}, \textsc{Chen, D.}, \textsc{Levy, O.}, \textsc{Lewis, M.},
  \textsc{Zettlemoyer, L.} and \textsc{Stoyanov, V.} (2019{\natexlab{b}}).
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \textit{arXiv preprint arXiv:1907.11692}.

\bibitem[{Loshchilov and Hutter(2019)}]{loshchilov2017decoupled}
\textsc{Loshchilov, I.} and \textsc{Hutter, F.} (2019).
\newblock Decoupled weight decay regularization.
\newblock In \textit{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net.

\bibitem[{Louizos et~al.(2018)Louizos, Welling and
  Kingma}]{louizos2017learning}
\textsc{Louizos, C.}, \textsc{Welling, M.} and \textsc{Kingma, D.~P.} (2018).
\newblock Learning sparse neural networks through l{\_}0 regularization.
\newblock In \textit{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}. OpenReview.net.

\bibitem[{Mallya and Lazebnik(2018)}]{mallya2018piggyback}
\textsc{Mallya, A.} and \textsc{Lazebnik, S.} (2018).
\newblock Piggyback: Adding multiple tasks to a single, fixed network by
  learning to mask.
\newblock \textit{arXiv preprint arXiv:1801.06519}, \textbf{6}.

\bibitem[{McCarley et~al.(2019)McCarley, Chakravarti and
  Sil}]{mccarley2019structured}
\textsc{McCarley, J.}, \textsc{Chakravarti, R.} and \textsc{Sil, A.} (2019).
\newblock Structured pruning of a bert-based question answering model.
\newblock \textit{arXiv preprint arXiv:1910.06360}.

\bibitem[{Molchanov et~al.(2019)Molchanov, Mallya, Tyree, Frosio and
  Kautz}]{molchanov2019importance}
\textsc{Molchanov, P.}, \textsc{Mallya, A.}, \textsc{Tyree, S.},
  \textsc{Frosio, I.} and \textsc{Kautz, J.} (2019).
\newblock Importance estimation for neural network pruning.
\newblock In \textit{{IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2019, Long Beach, CA, USA, June 16-20, 2019}. Computer
  Vision Foundation / {IEEE}.

\bibitem[{Molchanov et~al.(2017)Molchanov, Tyree, Karras, Aila and
  Kautz}]{molchanov2016pruning}
\textsc{Molchanov, P.}, \textsc{Tyree, S.}, \textsc{Karras, T.}, \textsc{Aila,
  T.} and \textsc{Kautz, J.} (2017).
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock In \textit{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net.

\bibitem[{Paganini and Forde(2020)}]{paganini2020iterative}
\textsc{Paganini, M.} and \textsc{Forde, J.} (2020).
\newblock On iterative neural network pruning, reinitialization, and the
  similarity of masks.
\newblock \textit{arXiv preprint arXiv:2001.05050}.

\bibitem[{Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, K{\"{o}}pf, Yang, DeVito,
  Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai and
  Chintala}]{paszke2019pytorch}
\textsc{Paszke, A.}, \textsc{Gross, S.}, \textsc{Massa, F.}, \textsc{Lerer,
  A.}, \textsc{Bradbury, J.}, \textsc{Chanan, G.}, \textsc{Killeen, T.},
  \textsc{Lin, Z.}, \textsc{Gimelshein, N.}, \textsc{Antiga, L.},
  \textsc{Desmaison, A.}, \textsc{K{\"{o}}pf, A.}, \textsc{Yang, E.},
  \textsc{DeVito, Z.}, \textsc{Raison, M.}, \textsc{Tejani, A.},
  \textsc{Chilamkurthy, S.}, \textsc{Steiner, B.}, \textsc{Fang, L.},
  \textsc{Bai, J.} and \textsc{Chintala, S.} (2019).
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \textit{Advances in Neural Information Processing Systems 32:
  Annual Conference on Neural Information Processing Systems 2019, NeurIPS
  2019, December 8-14, 2019, Vancouver, BC, Canada} (H.~M. Wallach,
  H.~Larochelle, A.~Beygelzimer, F.~d'Alch{\'{e}}{-}Buc, E.~B. Fox and
  R.~Garnett, eds.).

\bibitem[{Qian(1999)}]{qian1999momentum}
\textsc{Qian, N.} (1999).
\newblock On the momentum term in gradient descent learning algorithms.
\newblock \textit{Neural networks}, \textbf{12} 145--151.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever
  et~al.}]{radford2019language}
\textsc{Radford, A.}, \textsc{Wu, J.}, \textsc{Child, R.}, \textsc{Luan, D.},
  \textsc{Amodei, D.}, \textsc{Sutskever, I.} \textsc{et~al.} (2019).
\newblock Language models are unsupervised multitask learners.
\newblock \textit{OpenAI blog}, \textbf{1} 9.

\bibitem[{Rajpurkar et~al.(2016{\natexlab{a}})Rajpurkar, Zhang, Lopyrev and
  Liang}]{squad1}
\textsc{Rajpurkar, P.}, \textsc{Zhang, J.}, \textsc{Lopyrev, K.} and
  \textsc{Liang, P.} (2016{\natexlab{a}}).
\newblock {SQ}u{AD}: 100,000+ questions for machine comprehension of text.
\newblock In \textit{Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}. Association for Computational Linguistics,
  Austin, Texas.

\bibitem[{Rajpurkar et~al.(2016{\natexlab{b}})Rajpurkar, Zhang, Lopyrev and
  Liang}]{rajpurkar2016squad}
\textsc{Rajpurkar, P.}, \textsc{Zhang, J.}, \textsc{Lopyrev, K.} and
  \textsc{Liang, P.} (2016{\natexlab{b}}).
\newblock {SQ}u{AD}: 100,000+ questions for machine comprehension of text.
\newblock In \textit{Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}. Association for Computational Linguistics,
  Austin, Texas.

\bibitem[{Renda et~al.(2020)Renda, Frankle and Carbin}]{renda2020comparing}
\textsc{Renda, A.}, \textsc{Frankle, J.} and \textsc{Carbin, M.} (2020).
\newblock Comparing rewinding and fine-tuning in neural network pruning.
\newblock In \textit{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net.

\bibitem[{Sajjad et~al.(2020)Sajjad, Dalvi, Durrani and Nakov}]{sajjad2020poor}
\textsc{Sajjad, H.}, \textsc{Dalvi, F.}, \textsc{Durrani, N.} and
  \textsc{Nakov, P.} (2020).
\newblock Poor man's bert: Smaller and faster transformer models.
\newblock \textit{arXiv e-prints} arXiv--2004.

\bibitem[{Sanh et~al.(2020)Sanh, Wolf and Rush}]{sanh2020movement}
\textsc{Sanh, V.}, \textsc{Wolf, T.} and \textsc{Rush, A.~M.} (2020).
\newblock Movement pruning: Adaptive sparsity by fine-tuning.

\bibitem[{Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng and
  Potts}]{sst2013}
\textsc{Socher, R.}, \textsc{Perelygin, A.}, \textsc{Wu, J.}, \textsc{Chuang,
  J.}, \textsc{Manning, C.~D.}, \textsc{Ng, A.} and \textsc{Potts, C.} (2013).
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \textit{Proceedings of the 2013 Conference on Empirical Methods in
  Natural Language Processing}. Association for Computational Linguistics,
  Seattle, Washington, USA.

\bibitem[{Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy and
  Bowman}]{wang2018glue}
\textsc{Wang, A.}, \textsc{Singh, A.}, \textsc{Michael, J.}, \textsc{Hill, F.},
  \textsc{Levy, O.} and \textsc{Bowman, S.~R.} (2019).
\newblock {GLUE:} {A} multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \textit{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net.

\bibitem[{Warstadt et~al.(2019)Warstadt, Singh and Bowman}]{cola2018}
\textsc{Warstadt, A.}, \textsc{Singh, A.} and \textsc{Bowman, S.~R.} (2019).
\newblock Neural network acceptability judgments.
\newblock \textit{Transactions of the Association for Computational
  Linguistics}, \textbf{7} 625--641.

\bibitem[{Williams et~al.(2018)Williams, Nangia and Bowman}]{mnli2018}
\textsc{Williams, A.}, \textsc{Nangia, N.} and \textsc{Bowman, S.} (2018).
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \textit{Proceedings of the 2018 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}. Association for Computational
  Linguistics, New Orleans, Louisiana.

\bibitem[{Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz et~al.}]{wolf2019huggingface}
\textsc{Wolf, T.}, \textsc{Debut, L.}, \textsc{Sanh, V.}, \textsc{Chaumond,
  J.}, \textsc{Delangue, C.}, \textsc{Moi, A.}, \textsc{Cistac, P.},
  \textsc{Rault, T.}, \textsc{Louf, R.}, \textsc{Funtowicz, M.} \textsc{et~al.}
  (2019).
\newblock Huggingface's transformers: State-of-the-art natural language
  processing.
\newblock \textit{ArXiv preprint}, \textbf{abs/1910.03771}.

\bibitem[{Zafrir et~al.(2021)Zafrir, Larey, Boudoukh, Shen and
  Wasserblat}]{zafrir2021prune}
\textsc{Zafrir, O.}, \textsc{Larey, A.}, \textsc{Boudoukh, G.}, \textsc{Shen,
  H.} and \textsc{Wasserblat, M.} (2021).
\newblock Prune once for all: Sparse pre-trained language models.
\newblock \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Zhang et~al.(2021)Zhang, Wipf, Gan and Song}]{zhang2021biased}
\textsc{Zhang, Q.}, \textsc{Wipf, D.}, \textsc{Gan, Q.} and \textsc{Song, L.}
  (2021).
\newblock A biased graph neural network sampler with near-optimal regret.
\newblock \textit{Advances in Neural Information Processing Systems},
  \textbf{34} 8833--8844.

\bibitem[{Zhu and Gupta(2018)}]{zhu2017prune}
\textsc{Zhu, M.} and \textsc{Gupta, S.} (2018).
\newblock To prune, or not to prune: Exploring the efficacy of pruning for
  model compression.

\end{thebibliography}
