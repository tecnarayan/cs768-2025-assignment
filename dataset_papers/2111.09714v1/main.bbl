\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andoni et~al.(2015)Andoni, Indyk, Laarhoven, Razenshteyn, and
  Schmidt]{alex2015practical}
Andoni, A., Indyk, P., Laarhoven, T., Razenshteyn, I., and Schmidt, L.
\newblock Practical and optimal lsh for angular distance.
\newblock In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~28.
  Curran Associates, Inc., 2015.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2015/file/2823f4797102ce1a1aec05359cc16dd9-Paper.pdf}.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Beltagy, I., Peters, M.~E., and Cohan, A.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Charikar \& Siminelakis(2017)Charikar and
  Siminelakis]{charikar2017hashing}
Charikar, M. and Siminelakis, P.
\newblock Hashing-based-estimators for kernel density in high dimensions.
\newblock In \emph{2017 IEEE 58th Annual Symposium on Foundations of Computer
  Science (FOCS)}, pp.\  1032--1043, 2017.
\newblock \doi{10.1109/FOCS.2017.99}.

\bibitem[Charikar(2002)]{charikar2002similarity}
Charikar, M.~S.
\newblock Similarity estimation techniques from rounding algorithms.
\newblock In \emph{Proceedings of the Thiry-Fourth Annual ACM Symposium on
  Theory of Computing}, STOC '02, pp.\  380â€“388, New York, NY, USA, 2002.
  Association for Computing Machinery.
\newblock ISBN 1581134959.
\newblock \doi{10.1145/509907.509965}.
\newblock URL \url{https://doi.org/10.1145/509907.509965}.

\bibitem[Chen et~al.(2018)Chen, Zhang, Zhang, and Zhao]{chen2018quora}
Chen, Z., Zhang, H., Zhang, X., and Zhao, L.
\newblock Quora question pairs, 2018.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{Child2019GeneratingLS}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Choromanski et~al.(2021)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and
  Weller]{choromanski2020rethinking}
Choromanski, K.~M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos,
  T., Hawkins, P., Davis, J.~Q., Mohiuddin, A., Kaiser, L., Belanger, D.~B.,
  Colwell, L.~J., and Weller, A.
\newblock Rethinking attention with performers.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Ua6zuk0WRH}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://www.aclweb.org/anthology/N19-1423}.

\bibitem[Dolan \& Brockett(2005)Dolan and Brockett]{dolan2005automatically}
Dolan, W.~B. and Brockett, C.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \emph{Proceedings of the Third International Workshop on
  Paraphrasing ({IWP}2005)}, 2005.
\newblock URL \url{https://www.aclweb.org/anthology/I05-5002}.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{Katharopoulos2020TransformersAR}
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
\newblock Transformers are {RNN}s: Fast autoregressive transformers with linear
  attention.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  5156--5165. PMLR,
  13--18 Jul 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/katharopoulos20a.html}.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{Kitaev2020ReformerTE}
Kitaev, N., Kaiser, L., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkgNKkHtvB}.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lan et~al.(2020)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{lan2019albert}
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=H1eA7AEtvS}.

\bibitem[Levy et~al.(2015)Levy, Goldberg, and Dagan]{levy-etal-2015-improving}
Levy, O., Goldberg, Y., and Dagan, I.
\newblock Improving distributional similarity with lessons learned from word
  embeddings.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  3:\penalty0 211--225, 2015.
\newblock \doi{10.1162/tacl_a_00134}.
\newblock URL \url{https://www.aclweb.org/anthology/Q15-1016}.

\bibitem[Linsley et~al.(2018)Linsley, Kim, Veerabadran, Windolf, and
  Serre]{linsley2018learning}
Linsley, D., Kim, J., Veerabadran, V., Windolf, C., and Serre, T.
\newblock Learning long-range spatial dependencies with horizontal gated
  recurrent units.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf}.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and
  Potts]{maas2011learning}
Maas, A.~L., Daly, R.~E., Pham, P.~T., Huang, D., Ng, A.~Y., and Potts, C.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proceedings of the 49th Annual Meeting of the Association
  for Computational Linguistics: Human Language Technologies}, pp.\  142--150,
  Portland, Oregon, USA, June 2011. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/P11-1015}.

\bibitem[Nangia \& Bowman(2018)Nangia and Bowman]{nangia2018listops}
Nangia, N. and Bowman, S.
\newblock {L}ist{O}ps: A diagnostic dataset for latent tree learning.
\newblock In \emph{Proceedings of the 2018 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Student Research
  Workshop}, pp.\  92--99, New Orleans, Louisiana, USA, June 2018. Association
  for Computational Linguistics.
\newblock \doi{10.18653/v1/N18-4013}.
\newblock URL \url{https://www.aclweb.org/anthology/N18-4013}.

\bibitem[Neyshabur \& Srebro(2015)Neyshabur and Srebro]{neyshabur2015symmetric}
Neyshabur, B. and Srebro, N.
\newblock On symmetric and asymmetric lshs for inner product search.
\newblock In Bach, F. and Blei, D. (eds.), \emph{Proceedings of the 32nd
  International Conference on Machine Learning}, volume~37 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1926--1934, Lille, France, 07--09 Jul
  2015. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v37/neyshabur15.html}.

\bibitem[Peng et~al.(2021)Peng, Pappas, Yogatama, Schwartz, Smith, and
  Kong]{peng2021rfa}
Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N., and Kong, L.
\newblock Random feature attention.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=QtTKTdVrFBB}.

\bibitem[Press et~al.(2007)Press, Teukolsky, Vetterling, and
  Flannery]{press2007numerical}
Press, W.~H., Teukolsky, S.~A., Vetterling, W.~T., and Flannery, B.~P.
\newblock \emph{Numerical recipes: the art of scientific computing, 3rd
  Edition}.
\newblock Cambridge University Press, 2007.
\newblock ISBN 9780521706858.

\bibitem[Radev et~al.(2013)Radev, Muthukrishnan, Qazvinian, and
  Abu{-}Jbara]{radev2013acl}
Radev, D.~R., Muthukrishnan, P., Qazvinian, V., and Abu{-}Jbara, A.
\newblock The {ACL} anthology network corpus.
\newblock \emph{Language Resources and Evaluation}, 47\penalty0 (4):\penalty0
  919--944, 2013.
\newblock \doi{10.1007/s10579-012-9211-2}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (140):\penalty0 1--67, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang]{rajpurkar2016squad}
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.
\newblock {SQ}u{AD}: 100,000+ questions for machine comprehension of text.
\newblock In \emph{Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  2383--2392, Austin, Texas, November 2016.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D16-1264}.
\newblock URL \url{https://www.aclweb.org/anthology/D16-1264}.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{socher2013recursive}
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.~D., Ng, A., and
  Potts, C.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Proceedings of the 2013 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  1631--1642, Seattle, Washington, USA,
  October 2013. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/D13-1170}.

\bibitem[Spring \& Shrivastava(2018)Spring and Shrivastava]{spring2017new}
Spring, R. and Shrivastava, A.
\newblock Scalable estimation via {LSH} samplers {(LSS)}.
\newblock In \emph{International Conference on Learning Representations,
  Workshop Track Proceedings}, 2018.
\newblock URL \url{https://openreview.net/forum?id=BJazbHkPG}.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (56):\penalty0 1929--1958, 2014.
\newblock URL \url{http://jmlr.org/papers/v15/srivastava14a.html}.

\bibitem[Tay et~al.(2021)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang,
  Ruder, and Metzler]{tay2020long}
Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang,
  L., Ruder, S., and Metzler, D.
\newblock Long range arena : A benchmark for efficient transformers.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=qVyeW-grC2k}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L.~u., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{Wang2020LinformerSW}
Wang, S., Li, B., Khabsa, M., Fang, H., and Ma, H.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{williams2018broad}
Williams, A., Nangia, N., and Bowman, S.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{Proceedings of the 2018 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pp.\  1112--1122, New Orleans,
  Louisiana, June 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N18-1101}.
\newblock URL \url{https://www.aclweb.org/anthology/N18-1101}.

\bibitem[Xiong et~al.(2021)Xiong, Zeng, Chakraborty, Tan, Fung, Li, and
  Singh]{xiong2021nystromformer}
Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., and Singh, V.
\newblock NystrÃ¶mformer: A nystrÃ¶m-based algorithm for approximating
  self-attention.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  35\penalty0 (16):\penalty0 14138--14148, May 2021.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/17664}.

\bibitem[Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le]{yang2020xlnet}
Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.~R., and Le, Q.~V.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf}.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, and Ahmed]{zaheer2020big}
Zaheer, M., Guruganesh, G., Dubey, K.~A., Ainslie, J., Alberti, C., Ontanon,
  S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed, A.
\newblock Big bird: Transformers for longer sequences.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  17283--17297. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf}.

\bibitem[Zhu et~al.(2015)Zhu, Kiros, Zemel, Salakhutdinov, Urtasun, Torralba,
  and Fidler]{zhu2015aligning}
Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A.,
  and Fidler, S.
\newblock Aligning books and movies: Towards story-like visual explanations by
  watching movies and reading books.
\newblock In \emph{2015 IEEE International Conference on Computer Vision
  (ICCV)}, pp.\  19--27, 2015.
\newblock \doi{10.1109/ICCV.2015.11}.

\end{thebibliography}
