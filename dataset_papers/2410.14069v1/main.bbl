\begin{thebibliography}{10}

\bibitem{arjovsky2017wasserstein}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock Wasserstein generative adversarial networks.
\newblock In {\em International conference on machine learning}, pages 214--223. PMLR, 2017.

\bibitem{asadulaev2022neural}
Arip Asadulaev, Alexander Korotin, Vage Egiazarian, and Evgeny Burnaev.
\newblock Neural optimal transport with general cost functionals.
\newblock {\em arXiv preprint arXiv:2205.15403}, 2022.

\bibitem{bacon2017option}
Pierre-Luc Bacon, Jean Harb, and Doina Precup.
\newblock The option-critic architecture.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~31, 2017.

\bibitem{bai2023sliced}
Yikun Bai, Bernhard Schmitzer, Matthew Thorpe, and Soheil Kolouri.
\newblock Sliced optimal partial transport.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 13681--13690, 2023.

\bibitem{wandb}
Lukas Biewald.
\newblock Experiment tracking with weights and biases, 2020.
\newblock Software available from wandb.com.

\bibitem{brandfonbrener2021offline}
David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna.
\newblock Offline rl without off-policy evaluation.
\newblock {\em Advances in neural information processing systems}, 34:4933--4946, 2021.

\bibitem{chen2022latent}
Xi~Chen, Ali Ghadirzadeh, Tianhe Yu, Yuan Gao, Jianhao Wang, Wenzhe Li, Bin Liang, Chelsea Finn, and Chongjie Zhang.
\newblock Latent-variable advantage-weighted policy optimization for offline rl.
\newblock {\em arXiv preprint arXiv:2203.08949}, 2022.

\bibitem{cheng2022adversarially}
Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal.
\newblock Adversarially trained actor critic for offline reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 3852--3878. PMLR, 2022.

\bibitem{cohen2021imitation}
Samuel Cohen, Brandon Amos, Marc~Peter Deisenroth, Mikael Henaff, Eugene Vinitsky, and Denis Yarats.
\newblock Imitation learning from pixel observations for continuous control.
\newblock 2021.

\bibitem{courty2016optimal}
Nicolas Courty, R{\'e}mi Flamary, Devis Tuia, and Alain Rakotomamonjy.
\newblock Optimal transport for domain adaptation.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence}, 39(9):1853--1865, 2016.

\bibitem{dadashi2020primal}
Robert Dadashi, L{\'e}onard Hussenot, Matthieu Geist, and Olivier Pietquin.
\newblock Primal wasserstein imitation learning.
\newblock {\em arXiv preprint arXiv:2006.04678}, 2020.

\bibitem{figalli2010optimal}
Alessio Figalli.
\newblock The optimal partial transport problem.
\newblock {\em Archive for rational mechanics and analysis}, 195(2):533--560, 2010.

\bibitem{fu2020d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock {\em arXiv preprint arXiv:2004.07219}, 2020.

\bibitem{fujimoto2021minimalist}
Scott Fujimoto and Shixiang~Shane Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 34:20132--20145, 2021.

\bibitem{fujimoto2018addressing}
Scott Fujimoto, Herke Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In {\em International conference on machine learning}, pages 1587--1596. PMLR, 2018.

\bibitem{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em International conference on machine learning}, pages 2052--2062. PMLR, 2019.

\bibitem{gazdieva2023extremal}
Milena Gazdieva, Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev.
\newblock Extremal domain translation with neural optimal transport.
\newblock {\em arXiv preprint arXiv:2301.12874}, 2023.

\bibitem{gulrajani2017improved}
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron~C Courville.
\newblock Improved training of {W}asserstein {GAN}s.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 5767--5777, 2017.

\bibitem{haarnoja2017reinforcement}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In {\em International conference on machine learning}, pages 1352--1361. PMLR, 2017.

\bibitem{haldar2022watch}
Siddhant Haldar, Vaibhav Mathur, Denis Yarats, and Lerrel Pinto.
\newblock Watch and match: Supercharging imitation with regularized optimal transport.
\newblock {\em arXiv preprint arXiv:2206.15469}, 2022.

\bibitem{kakade2002approximately}
Sham Kakade and John Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In {\em Proceedings of the Nineteenth International Conference on Machine Learning}, pages 267--274, 2002.

\bibitem{kantorovitch1958translocation}
Leonid Kantorovitch.
\newblock On the translocation of masses.
\newblock {\em Management Science}, 5(1):1--4, 1958.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{korotin2019wasserstein}
Alexander Korotin, Vage Egiazarian, Arip Asadulaev, Alexander Safin, and Evgeny Burnaev.
\newblock Wasserstein-2 generative networks.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{korotin2022kantorovich}
Alexander Korotin, Alexander Kolesov, and Evgeny Burnaev.
\newblock Kantorovich strikes back! wasserstein gans are not optimal transport?
\newblock {\em arXiv preprint arXiv:2206.07767}, 2022.

\bibitem{korotin2022kernel}
Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev.
\newblock Kernel neural optimal transport.
\newblock {\em arXiv preprint arXiv:2205.15269}, 2022.

\bibitem{korotin2022neural}
Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev.
\newblock Neural optimal transport.
\newblock {\em arXiv preprint arXiv:2201.12220}, 2022.

\bibitem{kostrikov2021offline}
Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum.
\newblock Offline reinforcement learning with fisher divergence critic regularization.
\newblock In {\em International Conference on Machine Learning}, pages 5774--5783. PMLR, 2021.

\bibitem{kumar2019stabilizing}
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{kumar2022should}
Aviral Kumar, Joey Hong, Anikait Singh, and Sergey Levine.
\newblock When should we prefer offline reinforcement learning over behavioral cloning?
\newblock {\em arXiv preprint arXiv:2204.05618}, 2022.

\bibitem{kumardasco}
Aviral Kumar, Sergey Levine, Yevgen Chebotar, et~al.
\newblock Dasco: Dual-generator adversarial support constrained offline reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 33:1179--1191, 2020.

\bibitem{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on open problems.
\newblock {\em arXiv preprint arXiv:2005.01643}, 2020.

\bibitem{li2021optimal}
Zhuo Li, Xu~Zhou, Taixin Li, and Yang Liu.
\newblock An optimal-transport-based reinforcement learning approach for computation offloading.
\newblock In {\em 2021 IEEE Wireless Communications and Networking Conference (WCNC)}, pages 1--6. IEEE, 2021.

\bibitem{luo2023optimal}
Yicheng Luo, Zhengyao Jiang, Samuel Cohen, Edward Grefenstette, and Marc~Peter Deisenroth.
\newblock Optimal transport for offline imitation learning.
\newblock {\em arXiv preprint arXiv:2303.13971}, 2023.

\bibitem{makkuva2019optimal}
Ashok~Vardhan Makkuva, Amirhossein Taghvaei, Sewoong Oh, and Jason~D Lee.
\newblock Optimal transport mapping via input convex neural networks.
\newblock {\em arXiv preprint arXiv:1908.10962}, 2019.

\bibitem{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness, Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em nature}, 518(7540):529--533, 2015.

\bibitem{nachum2019algaedice}
Ofir Nachum, Bo~Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans.
\newblock Algaedice: Policy gradient from arbitrary experience.
\newblock {\em arXiv preprint arXiv:1912.02074}, 2019.

\bibitem{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock {\em Advances in Neural Information Processing Systems}, 35:27730--27744, 2022.

\bibitem{papagiannis2020imitation}
Georgios Papagiannis and Yunpeng Li.
\newblock Imitation learning with sinkhorn distances.
\newblock {\em arXiv preprint arXiv:2008.09167}, 2020.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{paty2020regularized}
Fran{\c{c}}ois-Pierre Paty and Marco Cuturi.
\newblock Regularized optimal transport is ground cost adversarial.
\newblock In {\em International Conference on Machine Learning}, pages 7532--7542. PMLR, 2020.

\bibitem{rockafellar1976integral}
R~Tyrrell Rockafellar.
\newblock Integral functionals, normal integrands and measurable selections.
\newblock In {\em Nonlinear operators and the calculus of variations}, pages 157--207. Springer, 1976.

\bibitem{santambrogio2015optimal}
Filippo Santambrogio.
\newblock Optimal transport for applied mathematicians.
\newblock {\em Birk{\"a}user, NY}, 55(58-63):94, 2015.

\bibitem{silver2014deterministic}
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock In {\em International conference on machine learning}, pages 387--395. PMLR, 2014.

\bibitem{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{tarasov2023revisiting}
Denis Tarasov, Vladislav Kurenkov, Alexander Nikulin, and Sergey Kolesnikov.
\newblock Revisiting the minimalist approach to offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2305.09836}, 2023.

\bibitem{tarasov2022corl}
Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, and Sergey Kolesnikov.
\newblock {CORL}: Research-oriented deep offline reinforcement learning library.
\newblock In {\em 3rd Offline RL Workshop: Offline RL as a ''Launchpad''}, 2022.

\bibitem{villani2003topics}
C{\'e}dric Villani.
\newblock {\em Topics in optimal transportation}.
\newblock Number~58. American Mathematical Soc., 2003.

\bibitem{villani2008optimal}
C{\'e}dric Villani.
\newblock {\em Optimal transport: old and new}, volume 338.
\newblock Springer Science \& Business Media, 2008.

\bibitem{vinyals2019alphastar}
Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojciech~M Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, et~al.
\newblock Alphastar: Mastering the real-time strategy game starcraft ii.
\newblock {\em DeepMind blog}, 2, 2019.

\bibitem{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:1911.11361}, 2019.

\bibitem{zhou2021plas}
Wenxuan Zhou, Sujay Bajracharya, and David Held.
\newblock Plas: Latent action space for offline reinforcement learning.
\newblock In {\em Conference on Robot Learning}, pages 1719--1735. PMLR, 2021.

\end{thebibliography}
