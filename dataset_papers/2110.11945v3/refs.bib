@STRING{aaai	= {AAAI} }
@STRING{accv	= {ACCV} }
@STRING{bmvc	= {BMVC} }
@STRING{cviu	= {CVIU} }
@STRING{cvpr	= {CVPR} }
@STRING{cvprw	= {CVPR workshops} }
@STRING{eccv	= {ECCV} }
@STRING{eccvw	= {ECCV workshops} }
@STRING{iccv	= {ICCV} }
@STRING{iccvw	= {ICCV workshops} }
@STRING{icip	= {ICIP} }
@STRING{icml	= {ICML} }
@STRING{icmlw	= {ICML workshops} }
@STRING{icpr	= {ICPR} }
@STRING{icra	= {ICRA} }
@STRING{ijcv	= {IJCV} }
@STRING{ijcai	= {IJCAI} }
@STRING{jmlr	= {JMLR} }
@STRING{miccai	= {MICCAI} }
@STRING{mm	= {ACM MM} }
@STRING{nips	= {NeurIPS} }
@STRING{nipsw	= {NeurIPS workshops} }
@STRING{pr	= {PR} }
@STRING{tip	= {TIP} }
@STRING{tpami	= {TPAMI} }
@STRING{wacv	= {WACV} }
@STRING{iclr	= {ICLR} }


@article{tay2020efficient,
  title={Efficient transformers: A survey},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={arXiv preprint},
  year={2020}
}


@article{touvron2020training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint},
  year={2020}
}

@inproceedings{lin2017focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle=iccv,
  year={2017}
}

@inproceedings{he2017mask,
  title={Mask r-cnn},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle=iccv,
  year={2017}
}

@inproceedings{ren2015faster,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle=nips,
  year={2015}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle=eccv,
  year={2014}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint},
  year={2017}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle=cvpr,
  year={2016}
}

@article{chen2019mmdetection,
  title={MMDetection: Open mmlab detection toolbox and benchmark},
  author={Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and Cao, Yuhang and Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and Liu, Ziwei and Xu, Jiarui and others},
  journal={arXiv preprint},
  year={2019}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle=cvpr,
  year={2009}
}

@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint},
  year={2017}
}

@article{devries2017improved,
  title={Improved regularization of convolutional neural networks with cutout},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint},
  year={2017}
}

@inproceedings{yun2019cutmix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle=iccv,
  year={2019}
}

@inproceedings{szegedy2016rethinking,
	title={Rethinking the inception architecture for computer vision},
	author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
	booktitle=cvpr,
	year={2016}
}

@inproceedings{zhao2020exploring,
  title={Exploring self-attention for image recognition},
  author={Zhao, Hengshuang and Jia, Jiaya and Koltun, Vladlen},
  booktitle=cvpr,
  year={2020}
}

@inproceedings{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  booktitle=iclr,
  year={2020}
}

@article{wang2021pyramid,
  title={Pyramid vision transformer: A versatile backbone for dense prediction without convolutions},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  journal={arXiv preprint},
  year={2021}
}

@inproceedings{NIPS2000_nyst,
 author = {Williams, Christopher and Seeger, Matthias},
 title = {Using the Nystr\"{o}m Method to Speed Up Kernel Machines},
 booktitle = nips,
 year = {2001}
}

@article{ben1966iterative,
  title={On iterative computation of generalized inverses and associated projections},
  author={Ben-Israel, Adi and Cohen, Dan},
  journal={SIAM Journal on Numerical Analysis},
  year={1966}
}

@article{bbstep,
    author = {BARZILAI, JONATHAN and BORWEIN, JONATHAN M.},
    title = "{Two-Point Step Size Gradient Methods}",
    journal = {IMA Journal of Numerical Analysis},
    year = {1988}
}

@inproceedings{ham,
    title={Is Attention Better Than Matrix Decomposition?},
    author={Zhengyang Geng and Meng-Hao Guo and Hongxu Chen and Xia Li and Ke Wei and Zhouchen Lin},
    booktitle=iclr,
    year={2021},
}

@article{drineas2005nystrom,
  title={On the Nystr{\"o}m Method for Approximating a Gram Matrix for Improved Kernel-Based Learning},
  author={Drineas, Petros and Mahoney, Michael W and Cristianini, Nello},
  journal=jmlr,
  year={2005}
}
@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint},
  year={2020}
}

@inproceedings{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  booktitle=iclr,
  year={2021}
}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle=icml,
  year={2015}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint},
  year={2016}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle=icml,
  year={2020}
}

@inproceedings{tsai2019transformer,
  title={Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Yamada, Makoto and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={EMNLP},
  year={2019}
}



@article{DBLP:journals/corr/CordtsORREBFRS16,
  author    = {Marius Cordts and
               Mohamed Omran and
               Sebastian Ramos and
               Timo Rehfeld and
               Markus Enzweiler and
               Rodrigo Benenson and
               Uwe Franke and
               Stefan Roth and
               Bernt Schiele},
  title     = {The Cityscapes Dataset for Semantic Urban Scene Understanding},
  journal   = {CoRR},
  year      = {2016}
}

@article{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  journal={arXiv preprint},
  year={2021}
}



@inproceedings{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle=iclr,
  year={2021}
}


@inproceedings{xiong2021nystr,
  title={Nystr{\"o}mformer: A Nystr{\"o}m-Based Algorithm for Approximating Self-Attention},
  author={Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  booktitle=aaai,
  year={2021}
}

@article{jaegle2021perceiver,
  title={Perceiver: General Perception with Iterative Attention},
  author={Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
  journal={arXiv preprint},
  year={2021}
}

@inproceedings{d2021convit,
  title={Convit: Improving vision transformers with soft convolutional inductive biases},
  author={d'Ascoli, St{\'e}phane and Touvron, Hugo and Leavitt, Matthew and Morcos, Ari and Biroli, Giulio and Sagun, Levent},
  booktitle=icml,
  year={2021}
}

@article{kumar2012sampling,
  title={Sampling methods for the Nystr{\"o}m method},
  author={Kumar, Sanjiv and Mohri, Mehryar and Talwalkar, Ameet},
  journal=jmlr,
  year={2012},
}

@inproceedings{kumar2009sampling,
  title={Sampling techniques for the nystrom method},
  author={Kumar, Sanjiv and Mohri, Mehryar and Talwalkar, Ameet},
  booktitle={Artificial Intelligence and Statistics},
  year={2009}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle=nips,
  year={2017}
}



@article{yuan2021tokens,
  title={Tokens-to-token vit: Training vision transformers from scratch on imagenet},
  author={Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zihang and Tay, Francis EH and Feng, Jiashi and Yan, Shuicheng},
  journal={arXiv preprint},
  year={2021}
}

@inproceedings{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={ACL},
  year={2018}
}

@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle=nips,
  year={2020}
}

@inproceedings{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  booktitle=iclr,
  year={2020}
}

@inproceedings{zhu2020deformable,
  title={Deformable DETR: Deformable Transformers for End-to-End Object Detection},
  author={Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  booktitle=iclr,
  year={2021}
}

@inproceedings{tian2019fcos,
  title={Fcos: Fully convolutional one-stage object detection},
  author={Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},
  booktitle=iccv,
  year={2019}
}

@inproceedings{long2015fully,
  title={Fully convolutional networks for semantic segmentation},
  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle=cvpr,
  year={2015}
}

@inproceedings{zhao2017pyramid,
  title={Pyramid scene parsing network},
  author={Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  booktitle=cvpr,
  year={2017}
}

@inproceedings{zhang2018context,
  title={Context encoding for semantic segmentation},
  author={Zhang, Hang and Dana, Kristin and Shi, Jianping and Zhang, Zhongyue and Wang, Xiaogang and Tyagi, Ambrish and Agrawal, Amit},
  booktitle=cvpr,
  year={2018}
}

@inproceedings{he2019dynamic,
  title={Dynamic multi-scale filters for semantic segmentation},
  author={He, Junjun and Deng, Zhongying and Qiao, Yu},
  booktitle=iccv,
  year={2019}
}

@inproceedings{zheng2020rethinking,
  title={Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers},
  author={Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip HS and Zhang, Li},
  booktitle=cvpr,
  year={2021}
}

@article{dong2021attention,
  title={Attention is not all you need: Pure attention loses rank doubly exponentially with depth},
  author={Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
  journal={arXiv preprint},
  year={2021}
}

@article{fasshauer2011positive,
  title={Positive definite kernels: past, present and future},
  author={Fasshauer, Gregory E},
  journal={Dolomites Research Notes on Approximation},
  year={2011}
}



@inproceedings{radosavovic2020designing,
  title={Designing network design spaces},
  author={Radosavovic, Ilija and Kosaraju, Raj Prateek and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle=cvpr,
  year={2020}
}

@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}



@inproceedings{wang2018non,
  title={Non-local neural networks},
  author={Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
  booktitle=cvpr,
  year={2018}
}

@article{xu2021co,
  title={Co-Scale Conv-Attentional Image Transformers},
  author={Xu, Weijian and Xu, Yifan and Chang, Tyler and Tu, Zhuowen},
  journal={arXiv preprint},
  year={2021}
}

@article{chu2021conditional,
  title={Conditional Positional Encodings for Vision Transformers},
  author={Chu, Xiangxiang and Tian, Zhi and Zhang, Bo and Wang, Xinlong and Wei, Xiaolin and Xia, Huaxia and Shen, Chunhua},
  journal={arXiv preprint},
  year={2021}
}

@article{chu2021twins,
  title={Twins: Revisiting the Design of Spatial Attention in Vision Transformers},
  author={Chu, Xiangxiang and Tian, Zhi and Wang, Yuqing and Zhang, Bo and Ren, Haibing and Wei, Xiaolin and Xia, Huaxia and Shen, Chunhua},
  journal={arXiv preprint},
  year={2021}
}

@article{bello2021lambdanetworks,
  title={Lambdanetworks: Modeling long-range interactions without attention},
  author={Bello, Irwan},
  journal={arXiv preprint},
  year={2021}
}

@article{srinivas2021bottleneck,
  title={Bottleneck transformers for visual recognition},
  author={Srinivas, Aravind and Lin, Tsung-Yi and Parmar, Niki and Shlens, Jonathon and Abbeel, Pieter and Vaswani, Ashish},
  journal={arXiv preprint},
  year={2021}
}

@article{touvron2021going,
  title={Going deeper with image transformers},
  author={Touvron, Hugo and Cord, Matthieu and Sablayrolles, Alexandre and Synnaeve, Gabriel and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint},
  year={2021}
}

@article{tay2020long,
  title={Long range arena: A benchmark for efficient transformers},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  journal={arXiv preprint},
  year={2020}
}

@article{nangia2018listops,
  title={Listops: A diagnostic dataset for latent tree learning},
  author={Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint},
  year={2018}
}

@inproceedings{maas2011learning,
  title={Learning word vectors for sentiment analysis},
  author={Maas, Andrew and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies},
  year={2011}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  journal={Citeseer}
}

@article{radev2013acl,
  title={The ACL anthology network corpus},
  author={Radev, Dragomir R and Muthukrishnan, Pradeep and Qazvinian, Vahed and Abu-Jbara, Amjad},
  journal={Language Resources and Evaluation},
  year={2013},
}

@article{peng2021random,
  title={Random feature attention},
  author={Peng, Hao and Pappas, Nikolaos and Yogatama, Dani and Schwartz, Roy and Smith, Noah A and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2103.02143},
  year={2021}
}

@article{kasai2021finetuning,
  title={Finetuning Pretrained Transformers into RNNs},
  author={Kasai, Jungo and Peng, Hao and Zhang, Yizhe and Yogatama, Dani and Ilharco, Gabriel and Pappas, Nikolaos and Mao, Yi and Chen, Weizhu and Smith, Noah A},
  journal={arXiv preprint arXiv:2103.13076},
  year={2021}
}


@inproceedings{zhang2020dynamic,
  title={Dynamic graph message passing networks},
  author={Zhang, Li and Xu, Dan and Arnab, Anurag and Torr, Philip HS},
  booktitle=cvpr,
  year={2020}
}

@misc{mindspore,
  author = {Mindspore},
  title = {\url{https://www.mindspore.cn/}},
  year = {2020}
}