\begin{thebibliography}{79}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alizadeh et~al.(2020)Alizadeh, Farhadi, and
  Rastegari]{alizadeh2019butterfly}
Keivan Alizadeh, Ali Farhadi, and Mohammad Rastegari.
\newblock Butterfly transform: An efficient {FFT} based neural architecture
  design.
\newblock In \emph{The Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2020.

\bibitem[Arfken and Weber(2005)]{arfken2005mathematical}
George~B Arfken and Hans~J Weber.
\newblock \emph{Mathematical methods for physicists}.
\newblock Elsevier Academic Press, 2005.

\bibitem[Arjovsky et~al.(2016)Arjovsky, Shah, and Bengio]{arjovsky2016unitary}
Martin Arjovsky, Amar Shah, and Yoshua Bengio.
\newblock Unitary evolution recurrent neural networks.
\newblock In \emph{The International Conference on Machine Learning (ICML)},
  pages 1120--1128, 2016.

\bibitem[Bagnall et~al.(2018)Bagnall, Dau, Lines, Flynn, Large, Bostrom,
  Southam, and Keogh]{bagnall2018uea}
Anthony Bagnall, Hoang~Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron
  Bostrom, Paul Southam, and Eamonn Keogh.
\newblock The {UEA} multivariate time series classification archive, 2018.
\newblock \emph{arXiv preprint arXiv:1811.00075}, 2018.

\bibitem[Bai et~al.(2018)Bai, Kolter, and Koltun]{bai2018empirical}
Shaojie Bai, J~Zico Kolter, and Vladlen Koltun.
\newblock An empirical evaluation of generic convolutional and recurrent
  networks for sequence modeling.
\newblock \emph{arXiv preprint arXiv:1803.01271}, 2018.

\bibitem[Bai et~al.(2019)Bai, Kolter, and Koltun]{trellisnet}
Shaojie Bai, J~Zico Kolter, and Vladlen Koltun.
\newblock Trellis networks for sequence modeling.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2019.

\bibitem[Berthier et~al.(2020)Berthier, Bach, and
  Gaillard]{berthier2020accelerated}
Rapha{\"e}l Berthier, Francis Bach, and Pierre Gaillard.
\newblock Accelerated gossip in networks of given dimension using {J}acobi
  polynomial iterations.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 2\penalty0
  (1):\penalty0 24--47, 2020.

\bibitem[Boyd(2001)]{boyd2001chebyshev}
John~P Boyd.
\newblock \emph{Chebyshev and Fourier spectral methods}.
\newblock Courier Corporation, 2001.

\bibitem[Chandar et~al.(2019)Chandar, Sankar, Vorontsov, Kahou, and
  Bengio]{chandar2019towards}
Sarath Chandar, Chinnadhurai Sankar, Eugene Vorontsov, Samira~Ebrahimi Kahou,
  and Yoshua Bengio.
\newblock Towards non-saturating recurrent units for modelling long-term
  dependencies.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 3280--3287, 2019.

\bibitem[Chang et~al.(2017)Chang, Zhang, Han, Yu, Guo, Tan, Cui, Witbrock,
  Hasegawa-Johnson, and Huang]{chang2017dilated}
Shiyu Chang, Yang Zhang, Wei Han, Mo~Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui,
  Michael Witbrock, Mark~A Hasegawa-Johnson, and Thomas~S Huang.
\newblock Dilated recurrent neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  77--87, 2017.

\bibitem[Che et~al.(2018)Che, Purushotham, Cho, Sontag, and
  Liu]{che2018recurrent}
Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu.
\newblock Recurrent neural networks for multivariate time series with missing
  values.
\newblock \emph{Scientific reports}, 8\penalty0 (1):\penalty0 1--12, 2018.

\bibitem[Chen et~al.(2015)Chen, Coatrieux, Wu, Dong, Coatrieux, and
  Shu]{chen2015fast}
Beijing Chen, Gouenou Coatrieux, Jiasong Wu, Zhifang Dong, Jean~Louis
  Coatrieux, and Huazhong Shu.
\newblock Fast computation of sliding discrete {T}chebichef moments and its
  application in duplicated regions detection.
\newblock \emph{IEEE Transactions on Signal Processing}, 63\penalty0
  (20):\penalty0 5424--5436, 2015.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and
  Duvenaud]{chen2018neural}
Tian~Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David~K Duvenaud.
\newblock Neural ordinary differential equations.
\newblock In \emph{Advances in neural information processing systems}, pages
  6571--6583, 2018.

\bibitem[Chihara(2011)]{chihara}
T.~S. Chihara.
\newblock \emph{An introduction to orthogonal polynomials}.
\newblock Dover Books on Mathematics. Dover Publications, 2011.
\newblock ISBN 9780486479293.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Cho et~al.(2014)Cho, Van~Merri{\"e}nboer, Gulcehre, Bahdanau,
  Bougares, Schwenk, and Bengio]{cho2014learning}
Kyunghyun Cho, Bart Van~Merri{\"e}nboer, Caglar Gulcehre, Dzmitry Bahdanau,
  Fethi Bougares, Holger Schwenk, and Yoshua Bengio.
\newblock Learning phrase representations using {RNN} encoder-decoder for
  statistical machine translation.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing (EMNLP)}, 2014.

\bibitem[Chung et~al.(2014)Chung, Gulcehre, Cho, and
  Bengio]{chung2014empirical}
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio.
\newblock Empirical evaluation of gated recurrent neural networks on sequence
  modeling.
\newblock \emph{arXiv preprint arXiv:1412.3555}, 2014.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc~V Le, and Ruslan
  Salakhutdinov.
\newblock Transformer-{XL}: Attentive language models beyond a fixed-length
  context.
\newblock In \emph{Proceedings of the Annual Meeting of the Association for
  Computational Linguistics}, 2019.

\bibitem[Dao et~al.(2017)Dao, De~Sa, and R\'{e}]{dao2017gaussian}
Tri Dao, Christopher~M De~Sa, and Christopher R\'{e}.
\newblock {G}aussian quadrature for kernel features.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 6107--6117, 2017.

\bibitem[Dao et~al.(2019)Dao, Gu, Eichhorn, Rudra, and R{\'e}]{dao2019learning}
Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R{\'e}.
\newblock Learning fast algorithms for linear transforms using butterfly
  factorizations.
\newblock In \emph{The International Conference on Machine Learning ({ICML})},
  2019.

\bibitem[Dao et~al.(2020)Dao, Sohoni, Gu, Eichhorn, Blonder, Leszczynski,
  Rudra, and Ré]{dao2020kaleidoscope}
Tri Dao, Nimit Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan
  Leszczynski, Atri Rudra, and Christopher Ré.
\newblock Kaleidoscope: An efficient, learnable representation for all
  structured linear maps.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2020.

\bibitem[De~Sa et~al.(2018)De~Sa, Gu, Puttagunta, R{\'e}, and Rudra]{de2018two}
Christopher De~Sa, Albert Gu, Rohan Puttagunta, Christopher R{\'e}, and Atri
  Rudra.
\newblock A two-pronged progress in structured dense matrix vector
  multiplication.
\newblock In \emph{Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 1060--1079. SIAM, 2018.

\bibitem[DeCarlo(1989)]{decarlo1989linear}
Raymond~A DeCarlo.
\newblock \emph{Linear systems: A state variable approach with numerical
  implementation}.
\newblock Prentice-Hall, Inc., 1989.

\bibitem[Defferrard et~al.(2016)Defferrard, Bresson, and
  Vandergheynst]{defferrard2016convolutional}
Micha{\"e}l Defferrard, Xavier Bresson, and Pierre Vandergheynst.
\newblock Convolutional neural networks on graphs with fast localized spectral
  filtering.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 3844--3852, 2016.

\bibitem[Dua and Graff(2017)]{Dua:2019}
Dheeru Dua and Casey Graff.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Duda(2010)]{duda2010accurate}
Krzysztof Duda.
\newblock Accurate, guaranteed stable, sliding discrete {F}ourier transform
  [{DSP} tips \& tricks].
\newblock \emph{IEEE Signal Processing Magazine}, 27\penalty0 (6):\penalty0
  124--127, 2010.

\bibitem[Dupont et~al.(2019)Dupont, Doucet, and Teh]{dupont2019augmented}
Emilien Dupont, Arnaud Doucet, and Yee~Whye Teh.
\newblock Augmented neural {ODE}s.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3134--3144, 2019.

\bibitem[Farhang-Boroujeny and Gazor(1994)]{farhang1994generalized}
Behrouz Farhang-Boroujeny and Saeed Gazor.
\newblock Generalized sliding {FFT} and its application to implementation of
  block {LMS} adaptive filters.
\newblock \emph{IEEE Transactions on Signal Processing}, 42\penalty0
  (3):\penalty0 532--538, 1994.

\bibitem[Finlay et~al.(2020)Finlay, Jacobsen, Nurbekyan, and
  Oberman]{finlay2020train}
Chris Finlay, J{\"o}rn-Henrik Jacobsen, Levon Nurbekyan, and Adam~M Oberman.
\newblock How to train your neural {ODE}: the world of {J}acobian and kinetic
  regularization.
\newblock In \emph{The International Conference on Machine Learning ({ICML})},
  2020.

\bibitem[Greff et~al.(2016)Greff, Srivastava, Koutn{\'\i}k, Steunebrink, and
  Schmidhuber]{greff2016lstm}
Klaus Greff, Rupesh~K Srivastava, Jan Koutn{\'\i}k, Bas~R Steunebrink, and
  J{\"u}rgen Schmidhuber.
\newblock {LSTM}: A search space odyssey.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  28\penalty0 (10):\penalty0 2222--2232, 2016.

\bibitem[Gu et~al.(2020)Gu, Gulcehre, Paine, Hoffman, and
  Pascanu]{gu2020improving}
Albert Gu, Caglar Gulcehre, Tom~Le Paine, Matt Hoffman, and Razvan Pascanu.
\newblock Improving the gating mechanism of recurrent neural networks.
\newblock In \emph{The International Conference on Machine Learning ({ICML})},
  2020.

\bibitem[Gulcehre et~al.(2016)Gulcehre, Moczulski, Denil, and
  Bengio]{gulcehre2016noisy}
Caglar Gulcehre, Marcin Moczulski, Misha Denil, and Yoshua Bengio.
\newblock Noisy activation functions.
\newblock In \emph{The International Conference on Machine Learning (ICML)},
  pages 3059--3068, 2016.

\bibitem[Henaff et~al.(2016)Henaff, Szlam, and LeCun]{henaff2016recurrent}
Mikael Henaff, Arthur Szlam, and Yann LeCun.
\newblock Recurrent orthogonal networks and long-memory tasks.
\newblock In \emph{The International Conference on Machine Learning (ICML)},
  2016.

\bibitem[Hochreiter and Schmidhuber(1997)]{lstm}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Iserles(2009)]{iserles2009first}
Arieh Iserles.
\newblock \emph{A first course in the numerical analysis of differential
  equations}.
\newblock Number~44. Cambridge university press, 2009.

\bibitem[Jacobsen and Lyons(2003)]{jacobsen2003sliding}
Eric Jacobsen and Richard Lyons.
\newblock The sliding {DFT}.
\newblock \emph{IEEE Signal Processing Magazine}, 20\penalty0 (2):\penalty0
  74--80, 2003.

\bibitem[Jacobsen and Lyons(2004)]{jacobsen2004update}
Eric Jacobsen and Richard Lyons.
\newblock An update to the sliding {DFT}.
\newblock \emph{IEEE Signal Processing Magazine}, 21\penalty0 (1):\penalty0
  110--111, 2004.

\bibitem[Jaeger and Haas(2004)]{jaeger2004harnessing}
Herbert Jaeger and Harald Haas.
\newblock Harnessing nonlinearity: Predicting chaotic systems and saving energy
  in wireless communication.
\newblock \emph{Science}, 304\penalty0 (5667):\penalty0 78--80, 2004.

\bibitem[Jozefowicz et~al.(2015)Jozefowicz, Zaremba, and
  Sutskever]{jozefowicz2015empirical}
Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever.
\newblock An empirical exploration of recurrent network architectures.
\newblock In \emph{International Conference on Machine Learning}, pages
  2342--2350, 2015.

\bibitem[Kidger et~al.(2020)Kidger, Morrill, Foster, and
  Lyons]{kidger2020neural}
Patrick Kidger, James Morrill, James Foster, and Terry Lyons.
\newblock Neural controlled differential equations for irregular time series.
\newblock \emph{arXiv preprint arXiv:2005.08926}, 2020.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2015.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In \emph{The International Conference on Machine Learning ({ICML})},
  2020.

\bibitem[Kober(2004)]{kober2004fast}
Vitaly Kober.
\newblock Fast algorithms for the computation of sliding discrete sinusoidal
  transforms.
\newblock \emph{IEEE transactions on signal processing}, 52\penalty0
  (6):\penalty0 1704--1710, 2004.

\bibitem[Kober(2007)]{kober2007fast}
Vitaly Kober.
\newblock Fast algorithms for the computation of sliding discrete {H}artley
  transforms.
\newblock \emph{IEEE transactions on signal processing}, 55\penalty0
  (6):\penalty0 2937--2944, 2007.

\bibitem[K{\"o}rner(1989)]{korner1989fourier}
Thomas~William K{\"o}rner.
\newblock \emph{{F}ourier analysis}.
\newblock Cambridge university press, 1989.

\bibitem[Krueger et~al.(2016)Krueger, Maharaj, Kram{\'a}r, Pezeshki, Ballas,
  Ke, Goyal, Bengio, Courville, and Pal]{krueger2016zoneout}
David Krueger, Tegan Maharaj, J{\'a}nos Kram{\'a}r, Mohammad Pezeshki, Nicolas
  Ballas, Nan~Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Aaron Courville, and
  Chris Pal.
\newblock Zoneout: Regularizing {RNN}s by randomly preserving hidden
  activations.
\newblock \emph{arXiv preprint arXiv:1606.01305}, 2016.

\bibitem[Le et~al.(2015)Le, Jaitly, and Hinton]{le2015simple}
Quoc~V Le, Navdeep Jaitly, and Geoffrey~E Hinton.
\newblock A simple way to initialize recurrent networks of rectified linear
  units.
\newblock \emph{arXiv preprint arXiv:1504.00941}, 2015.

\bibitem[Lezcano-Casado and Mart{\'\i}nez-Rubio(2019)]{lezcano2019cheap}
Mario Lezcano-Casado and David Mart{\'\i}nez-Rubio.
\newblock Cheap orthogonal constraints in neural networks: A simple
  parametrization of the orthogonal and unitary group.
\newblock In \emph{The International Conference on Machine Learning (ICML)},
  2019.

\bibitem[Li et~al.(2018)Li, Li, Cook, Zhu, and Gao]{indrnn}
Shuai Li, Wanqing Li, Chris Cook, Ce~Zhu, and Yanbo Gao.
\newblock Independently recurrent neural network ({IndRNN}): Building a longer
  and deeper {RNN}.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 5457--5466, 2018.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and
  Potts]{maas-EtAl:2011:ACL-HLT2011}
Andrew~L. Maas, Raymond~E. Daly, Peter~T. Pham, Dan Huang, Andrew~Y. Ng, and
  Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proceedings of the 49th Annual Meeting of the Association
  for Computational Linguistics: Human Language Technologies}, pages 142--150,
  Portland, Oregon, USA, June 2011. Association for Computational Linguistics.
\newblock URL \url{http://www.aclweb.org/anthology/P11-1015}.

\bibitem[Macias and Exposito(2005)]{macias2005efficient}
Jose A~Rosendo Macias and Antonio~Gomez Exposito.
\newblock Efficient computation of the running discrete {H}aar transform.
\newblock \emph{IEEE transactions on power delivery}, 21\penalty0 (1):\penalty0
  504--505, 2005.

\bibitem[Mackey and Glass(1977)]{mackey1977oscillation}
Michael~C Mackey and Leon Glass.
\newblock Oscillation and chaos in physiological control systems.
\newblock \emph{Science}, 197\penalty0 (4300):\penalty0 287--289, 1977.

\bibitem[Massaroli et~al.(2020)Massaroli, Poli, Bin, Park, Yamashita, and
  Asama]{massaroli2020stable}
Stefano Massaroli, Michael Poli, Michelangelo Bin, Jinkyoo Park, Atsushi
  Yamashita, and Hajime Asama.
\newblock Stable neural flows.
\newblock \emph{arXiv preprint arXiv:2003.08063}, 2020.

\bibitem[Mozafari and Savoji(2007)]{mozafari2007efficient}
Barzan Mozafari and Mohammad~H Savoji.
\newblock An efficient recursive algorithm and an explicit formula for
  calculating update vectors of running {W}alsh-{H}adamard transform.
\newblock In \emph{2007 9th International Symposium on Signal Processing and
  Its Applications}, pages 1--4. IEEE, 2007.

\bibitem[Ouyang and Cham(2009)]{ouyang2009fast}
Wanli Ouyang and Wai-Kuen Cham.
\newblock Fast algorithm for {W}alsh {H}adamard transform on sliding windows.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 32\penalty0 (1):\penalty0 165--171, 2009.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{pascanu2013difficulty}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{International conference on machine learning}, pages
  1310--1318, 2013.

\bibitem[Proakis(2001)]{proakis2001digital}
John~G Proakis.
\newblock \emph{Digital signal processing: principles algorithms and
  applications}.
\newblock Pearson Education India, 2001.

\bibitem[Quaglino et~al.(2020)Quaglino, Gallieri, Masci, and
  Koutn{\'\i}k]{quaglino2020snode}
Alessio Quaglino, Marco Gallieri, Jonathan Masci, and Jan Koutn{\'\i}k.
\newblock {SNODE}: Spectral discretization of neural {ODE}s for system
  identification.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2020.

\bibitem[Rae et~al.(2020)Rae, Potapenko, Jayakumar, and
  Lillicrap]{rae2019compressive}
Jack~W Rae, Anna Potapenko, Siddhant~M Jayakumar, and Timothy~P Lillicrap.
\newblock Compressive transformers for long-range sequence modelling.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2020.

\bibitem[Roy et~al.(2020)Roy, Saffar, Vaswani, and Grangier]{roy2020efficient}
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock \emph{arXiv preprint arXiv:2003.05997}, 2020.

\bibitem[Rubanova et~al.(2019)Rubanova, Chen, and Duvenaud]{rubanova2019latent}
Yulia Rubanova, Tian~Qi Chen, and David~K Duvenaud.
\newblock Latent ordinary differential equations for irregularly-sampled time
  series.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5321--5331, 2019.

\bibitem[Saab et~al.(2020)Saab, Dunnmon, R{\'e}, Rubin, and
  Lee-Messer]{saab2020weak}
Khaled Saab, Jared Dunnmon, Christopher R{\'e}, Daniel Rubin, and Christopher
  Lee-Messer.
\newblock Weak supervision as an efficient approach for automated seizure
  detection in electroencephalography.
\newblock \emph{{NPJ} Digital Medicine}, 3\penalty0 (1):\penalty0 1--12, 2020.

\bibitem[Shah et~al.(2018)Shah, Von~Weltin, Lopez, McHugh, Veloso,
  Golmohammadi, Obeid, and Picone]{shah2018temple}
Vinit Shah, Eva Von~Weltin, Silvia Lopez, James~Riley McHugh, Lillian Veloso,
  Meysam Golmohammadi, Iyad Obeid, and Joseph Picone.
\newblock The {T}emple {U}niversity hospital seizure detection corpus.
\newblock \emph{Frontiers in neuroinformatics}, 12:\penalty0 83, 2018.

\bibitem[Sukhbaatar et~al.(2019)Sukhbaatar, Grave, Bojanowski, and
  Joulin]{sukhbaatar2019adaptive}
Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin.
\newblock Adaptive attention span in transformers.
\newblock In \emph{Proceedings of the Annual Meeting of the Association for
  Computational Linguistics}, 2019.

\bibitem[Szeg{\"o}(1967)]{szego}
G.~Szeg{\"o}.
\newblock \emph{Orthogonal Polynomials}.
\newblock Number v.23 in American Mathematical Society colloquium publications.
  American Mathematical Society, 1967.
\newblock ISBN 9780821889527.

\bibitem[Tallec and Ollivier(2018)]{tallec2018can}
Corentin Tallec and Yann Ollivier.
\newblock Can recurrent neural networks warp time?
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2018.

\bibitem[Thomas et~al.(2018)Thomas, Gu, Dao, Rudra, and
  R{\'e}]{thomas2018learning}
Anna Thomas, Albert Gu, Tri Dao, Atri Rudra, and Christopher R{\'e}.
\newblock Learning compressed transforms with low displacement rank.
\newblock In \emph{Advances in neural information processing systems}, pages
  9052--9060, 2018.

\bibitem[Trefethen(2019)]{trefethen2019approximation}
Lloyd~N Trefethen.
\newblock \emph{Approximation theory and approximation practice}, volume 164.
\newblock SIAM, 2019.

\bibitem[Trinh et~al.(2018)Trinh, Dai, Luong, and Le]{trinh2018learning}
Trieu~H Trinh, Andrew~M Dai, Minh-Thang Luong, and Quoc~V Le.
\newblock Learning longer-term dependencies in {RNNs} with auxiliary losses.
\newblock In \emph{The International Conference on Machine Learning ({ICML})},
  2018.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems
  ({NeurIPS})}, 2017.

\bibitem[Voelker et~al.(2019)Voelker, Kaji{\'c}, and
  Eliasmith]{voelker2019legendre}
Aaron Voelker, Ivana Kaji{\'c}, and Chris Eliasmith.
\newblock Legendre memory units: Continuous-time representation in recurrent
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  15544--15553, 2019.

\bibitem[Voelker and Eliasmith(2018)]{voelker2018improving}
Aaron~R Voelker and Chris Eliasmith.
\newblock Improving spiking dynamical networks: Accurate delays, higher-order
  synapses, and time cells.
\newblock \emph{Neural computation}, 30\penalty0 (3):\penalty0 569--609, 2018.

\bibitem[Voelker(2019)]{voelker2019dynamical}
Aaron~Russell Voelker.
\newblock \emph{Dynamical systems in spiking neuromorphic hardware}.
\newblock PhD thesis, University of Waterloo, 2019.

\bibitem[Wu et~al.(2019)Wu, Fan, Baevski, Dauphin, and Auli]{wu2019pay}
Felix Wu, Angela Fan, Alexei Baevski, Yann~N Dauphin, and Michael Auli.
\newblock Pay less attention with lightweight and dynamic convolutions.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2019.

\bibitem[Wu et~al.(2012)Wu, Wang, Yang, Senhadji, Luo, and Shu]{wu2012sliding}
Jiasong Wu, Lu~Wang, Guanyu Yang, Lotfi Senhadji, Limin Luo, and Huazhong Shu.
\newblock Sliding conjugate symmetric sequency-ordered complex {H}adamard
  transform: fast algorithm and applications.
\newblock \emph{IEEE Transactions on Circuits and Systems I: Regular Papers},
  59\penalty0 (6):\penalty0 1321--1334, 2012.

\bibitem[Yang et~al.(2019)Yang, Pennington, Rao, Sohl-Dickstein, and
  Schoenholz]{yang2019mean}
Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel~S
  Schoenholz.
\newblock A mean field theory of batch normalization.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2019.

\bibitem[Zhang et~al.(2007)Zhang, Chen, and Chen]{zhang2007performance}
Guofeng Zhang, Tongwen Chen, and Xiang Chen.
\newblock Performance recovery in digital implementation of analogue systems.
\newblock \emph{{SIAM} journal on control and optimization}, 45\penalty0
  (6):\penalty0 2207--2223, 2007.

\bibitem[Zhang et~al.(2020)Zhang, Gao, Unterman, and
  Arodz]{zhang2019approximation}
Han Zhang, Xi~Gao, Jacob Unterman, and Tom Arodz.
\newblock Approximation capabilities of neural ordinary differential equations.
\newblock In \emph{The International Conference on Machine Learning ({ICML})},
  2020.

\bibitem[Zhang et~al.(2018)Zhang, Lin, Song, and Dhillon]{zhang2018learning}
Jiong Zhang, Yibo Lin, Zhao Song, and Inderjit~S Dhillon.
\newblock Learning long term dependencies via {F}ourier recurrent units.
\newblock In \emph{The International Conference on Machine Learning (ICML)},
  2018.

\end{thebibliography}
