\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Boyd et~al.(2011)Boyd, Parikh, Chu, Peleato, and
  Eckstein]{boyd2011admm}
S.~Boyd, N.~Parikh, E.~Chu, B.~Peleato, and J.~Eckstein.
\newblock Distributed optimization and statistical learning via alternating
  direction method of multipliers.
\newblock \emph{Found. Trends Mach. Lear.}, 3\penalty0 (1):\penalty0 1--122,
  Jan. 2011.

\bibitem[Dominguez-Garcia et~al.(2012)Dominguez-Garcia, Cady, and
  Hadjicostis]{dominguez2012decentralized}
A.~D. Dominguez-Garcia, S.~T. Cady, and C.~N. Hadjicostis.
\newblock Decentralized optimal dispatch of distributed energy resources.
\newblock In \emph{51st IEEE Conference on Decision and Control (CDC)}, pages
  3688--3693, Maui, HI, USA, Dec. 2012.

\bibitem[Deng et~al.(2017)Deng, Lai, Peng, and Yin]{deng2017parallel}
W.~Deng, M.-J. Lai, Z.~Peng, and W.~Yin.
\newblock Parallel multi-block {ADMM} with o (1/k) convergence.
\newblock \emph{Journal of Scientific Computing}, 71\penalty0 (2):\penalty0
  712--736, 2017.

\bibitem[Zinkevich et~al.(2010)Zinkevich, Weimer, Li, and
  Smola]{zinkevich2010parallelized}
M.~Zinkevich, M.~Weimer, L.~Li, and A.~J. Smola.
\newblock Parallelized stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 2595--2603, Vancouver, Canada, 2010.

\bibitem[Agarwal and Duchi(2011)]{agarwal2011distributed}
A.~Agarwal and J.~C. Duchi.
\newblock Distributed delayed stochastic optimization.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 873--881, Granada Spain, 2011.

\bibitem[Shamir et~al.(2014)Shamir, Srebro, and Zhang]{shamir2014communication}
O.~Shamir, N.~Srebro, and T.~Zhang.
\newblock Communication-efficient distributed optimization using an approximate
  {N}ewton-type method.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  1000--1008, {B}eijing, China, 2014.

\bibitem[Zhang and Lin(2015)]{zhang2015disco}
Y.~Zhang and X.~Lin.
\newblock Disco: Distributed optimization for self-concordant empirical loss.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  362--370, Lille, France, 2015.

\bibitem[Lee et~al.(2018)Lee, Lim, and Wright]{lee2018distributed}
C.-P. Lee, C.~H. Lim, and S.~J. Wright.
\newblock A distributed quasi-newton algorithm for empirical risk minimization
  with nonsmooth regularization.
\newblock In \emph{Proc. {ACM SIGKDD}}, pages 1646--1655, {L}ondon, United
  Kingdom, 2018.

\bibitem[Smith et~al.(2018)Smith, Forte, Chenxin, Takac, Jordan, and
  Jaggi]{smith2018cocoa}
V.~Smith, S.~Forte, M.~Chenxin, M.~Takac, M.~I. Jordan, and M.~Jaggi.
\newblock {CoCoA}: A general framework for communication-efficient distributed
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (230):\penalty0 1--49, 2018.

\bibitem[Peng et~al.(2016)Peng, Xu, Yan, and Yin]{peng2016arock}
Z.~Peng, Y.~Xu, M.~Yan, and W.~Yin.
\newblock {ARock}: an algorithmic framework for asynchronous parallel
  coordinate updates.
\newblock \emph{SIAM Journal on Scientific Computing}, 38\penalty0
  (5):\penalty0 A2851--A2879, 2016.

\bibitem[Li et~al.(2014)Li, Andersen, Park, Smola, Ahmed, Josifovski, Long,
  Shekita, and Su]{li2014scaling}
M.~Li, D.~G. Andersen, J.~W. Park, A.~J. Smola, A.~Ahmed, V.~Josifovski,
  J.~Long, E.~J. Shekita, and B.-Y. Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In \emph{11th Symposium on Operating Systems Design and
  Implementation ($OSDI$)}, pages 583--598, {B}roomfield, Denver, Colorado,
  2014.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
X.~Lian, C.~Zhang, H.~Zhang, C.-J. Hsieh, W.~Zhang, and J.~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? {A}
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 5330--5340, {L}ong Beach, CA, USA, 2017.

\bibitem[Lian et~al.(2018)Lian, Zhang, Zhang, and Liu]{lian2018asynchronous}
X.~Lian, W.~Zhang, C.~Zhang, and J.~Liu.
\newblock Asynchronous decentralized parallel stochastic gradient descent.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  1--10, {S}tockholm, Sweden, 2018.

\bibitem[Sayed(2014{\natexlab{a}})]{sayed2014nowbook}
A.~H. Sayed.
\newblock Adaptation, learning, and optimization over neworks.
\newblock \emph{Foundations and Trends in Machine Learning}, 7\penalty0
  (4-5):\penalty0 311--801, 2014{\natexlab{a}}.

\bibitem[Shi et~al.(2015{\natexlab{a}})Shi, Ling, Wu, and Yin]{shi2015extra}
W.~Shi, Q.~Ling, G.~Wu, and W.~Yin.
\newblock {EXTRA}: An exact first-order algorithm for decentralized consensus
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0
  944--966, 2015{\natexlab{a}}.

\bibitem[Nedic and Ozdaglar(2009)]{nedic2009distributed}
A.~Nedic and A.~Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 54\penalty0
  (1):\penalty0 48--61, 2009.

\bibitem[Sayed(2014{\natexlab{b}})]{sayed2014adaptive}
A.~H. Sayed.
\newblock Adaptive networks.
\newblock \emph{Proceedings of the IEEE}, 102\penalty0 (4):\penalty0 460--497,
  Apr. 2014{\natexlab{b}}.

\bibitem[Shi et~al.(2014)Shi, Ling, Yuan, Wu, and Yin]{shi2014onthe}
W.~Shi, Q.~Ling, K.~Yuan, G.~Wu, and W.~Yin.
\newblock On the linear convergence of the {ADMM} in decentralized consensus
  optimization.
\newblock \emph{IEEE Trans. Signal Process.}, 62\penalty0 (7):\penalty0
  1750--1761, 2014.

\bibitem[Jakoveti{\'c} et~al.(2015)Jakoveti{\'c}, Moura, and
  Xavier]{jakovetic2014linear}
D.~Jakoveti{\'c}, J.~M. Moura, and J.~Xavier.
\newblock Linear convergence rate of a class of distributed augmented
  {L}agrangian algorithms.
\newblock \emph{IEEE Transactions on Automatic Control}, 60\penalty0
  (4):\penalty0 922--936, 2015.

\bibitem[Iutzeler et~al.(2016)Iutzeler, Bianchi, Ciblat, and
  Hachem]{iutzeler2015explicit}
F.~Iutzeler, P.~Bianchi, P.~Ciblat, and W.~Hachem.
\newblock Explicit convergence rate of a distributed alternating direction
  method of multipliers.
\newblock \emph{IEEE Transactions on Automatic Control}, 61\penalty0
  (4):\penalty0 892--904, 2016.

\bibitem[Ling et~al.(2015)Ling, Shi, Wu, and Ribeiro]{ling2015dlm}
Q.~Ling, W.~Shi, G.~Wu, and A.~Ribeiro.
\newblock {DLM}: Decentralized linearized alternating direction method of
  multipliers.
\newblock \emph{IEEE Transactions on Signal Processing}, 63:\penalty0
  4051--4064, 2015.

\bibitem[Chang et~al.(2015)Chang, Hong, and Wang]{chang2015multi}
T.-H. Chang, M.~Hong, and X.~Wang.
\newblock Multi-agent distributed optimization via inexact consensus {ADMM}.
\newblock \emph{IEEE Transactions on Signal Processing}, 63\penalty0
  (2):\penalty0 482--497, Jan. 2015.

\bibitem[Shi et~al.(2015{\natexlab{b}})Shi, Ling, Wu, and Yin]{shi2015proximal}
W.~Shi, Q.~Ling, G.~Wu, and W.~Yin.
\newblock A proximal gradient algorithm for decentralized composite
  optimization.
\newblock \emph{IEEE Transactions on Signal Processing}, 63\penalty0
  (22):\penalty0 6013--6023, 2015{\natexlab{b}}.

\bibitem[Di~Lorenzo and Scutari(2016)]{di2016next}
P.~Di~Lorenzo and G.~Scutari.
\newblock Next: In-network nonconvex optimization.
\newblock \emph{IEEE Transactions on Signal and Information Processing over
  Networks}, 2\penalty0 (2):\penalty0 120--136, 2016.

\bibitem[Xu et~al.(2015)Xu, Zhu, Soh, and Xie]{xu2015augmented}
J.~Xu, S.~Zhu, Y.~C. Soh, and L.~Xie.
\newblock Augmented distributed gradient methods for multi-agent optimization
  under uncoordinated constant stepsizes.
\newblock In \emph{{\em Proc. 54th} IEEE Conference on Decision and Control
  (CDC)}, pages 2055--2060, {O}saka, Japan, 2015.

\bibitem[Nedic et~al.(2017)Nedic, Olshevsky, and Shi]{nedic2017achieving}
A.~Nedic, A.~Olshevsky, and W.~Shi.
\newblock Achieving geometric convergence for distributed optimization over
  time-varying graphs.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0
  2597--2633, 2017.

\bibitem[Qu and Li(2018)]{qu2017harnessing}
G.~Qu and N.~Li.
\newblock Harnessing smoothness to accelerate distributed optimization.
\newblock \emph{IEEE Transactions on Control of Network Systems}, 5\penalty0
  (3):\penalty0 1245--1260, Sept. 2018.

\bibitem[Yuan et~al.(2019{\natexlab{a}})Yuan, Ying, Zhao, and
  Sayed]{yuan2019exactdiffI}
K.~Yuan, B.~Ying, X.~Zhao, and A.~H. Sayed.
\newblock Exact diffusion for distributed optimization and learning-{P}art {I}:
  {A}lgorithm development.
\newblock \emph{IEEE Transactions on Signal Processing}, 67\penalty0
  (3):\penalty0 708--723, Feb. 2019{\natexlab{a}}.

\bibitem[He et~al.(2018)He, Bian, and Jaggi]{he2018cola}
L.~He, A.~Bian, and M.~Jaggi.
\newblock {COLA}: Decentralized linear learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 4536--4546, {M}ontreal, Canada, 2018.

\bibitem[Scaman et~al.(2017)Scaman, Bach, Bubeck, Lee, and
  Massoulie]{seaman2017optimal}
K.~Scaman, F.~Bach, S.~Bubeck, Y.~T. Lee, and L.~Massoulie.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  3027--3036, {S}tockholm, Sweden, 2017.

\bibitem[Yuan et~al.(2019{\natexlab{b}})Yuan, Ying, Zhao, and
  Sayed]{yuan2019exactdiffII}
K.~Yuan, B.~Ying, X.~Zhao, and A.~H. Sayed.
\newblock Exact diffusion for distributed optimization and learning-{P}art
  {II}: {C}onvergence analysis.
\newblock \emph{IEEE Transactions on Signal Processing}, 67\penalty0
  (3):\penalty0 724--739, Feb. 2019{\natexlab{b}}.

\bibitem[Xiao and Zhang(2014)]{xiao2014proximal}
L.~Xiao and T.~Zhang.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock \emph{SIAM Journal on Optimization}, 24\penalty0 (4):\penalty0
  2057--2075, 2014.

\bibitem[Chazan and Miranker(1969)]{chazan1969chaotic}
D.~Chazan and W.~Miranker.
\newblock Chaotic relaxation.
\newblock \emph{Linear Algebra and its Applications}, 2\penalty0 (2):\penalty0
  199--222, 1969.

\bibitem[Baudet(1978)]{baudet1978asynchronous}
G.~M. Baudet.
\newblock Asynchronous iterative methods for multiprocessors.
\newblock \emph{Journal of the ACM (JACM)}, 25\penalty0 (2):\penalty0 226--244,
  1978.

\bibitem[Bertsekas(1983)]{bertsekas1983distributed}
D.~P. Bertsekas.
\newblock Distributed asynchronous computation of fixed points.
\newblock \emph{Mathematical Programming}, 27\penalty0 (1):\penalty0 107--120,
  1983.

\bibitem[Tsitsiklis et~al.(1986)Tsitsiklis, Bertsekas, and
  Athans]{tsitsiklis1986distributed}
J.~Tsitsiklis, D.~Bertsekas, and M.~Athans.
\newblock Distributed asynchronous deterministic and stochastic gradient
  optimization algorithms.
\newblock \emph{IEEE Transactions on Automatic Control}, 31\penalty0
  (9):\penalty0 803--812, 1986.

\bibitem[Duchi et~al.(2012)Duchi, Agarwal, and Wainwright]{duchi2012dual}
J.~C. Duchi, A.~Agarwal, and M.~J. Wainwright.
\newblock Dual averaging for distributed optimization: Convergence analysis and
  network scaling.
\newblock \emph{IEEE Transactions on Automatic Control}, 57\penalty0
  (3):\penalty0 592--606, 2012.

\bibitem[Yuan et~al.(2016)Yuan, Ling, and Yin]{yuan2016convergence}
K.~Yuan, Q.~Ling, and W.~Yin.
\newblock On the convergence of decentralized gradient descent.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (3):\penalty0
  1835--1854, 2016.

\bibitem[Chen and Sayed(2013)]{chen2013distributed}
J.~Chen and A.~H. Sayed.
\newblock Distributed {P}areto optimization via diffusion strategies.
\newblock \emph{IEEE J. Sel. Topics Signal Process.}, 7\penalty0 (2):\penalty0
  205--220, April 2013.

\bibitem[Li et~al.(2019)Li, Shi, and Yan]{li2017nids}
Z.~Li, W.~Shi, and M.~Yan.
\newblock A decentralized proximal-gradient method with network independent
  step-sizes and separated convergence rates.
\newblock \emph{IEEE Transactions on Signal Processing}, 67\penalty0
  (17):\penalty0 4494--4506, Sept. 2019.

\bibitem[Chen and Ozdaglar(2012)]{chen2012fast}
A.~I. Chen and A.~Ozdaglar.
\newblock A fast distributed proximal-gradient method.
\newblock In \emph{Annual Allerton Conference on Communication, Control, and
  Computing}, pages 601--608, {M}onticello, IL, USA, Oct. 2012.

\bibitem[Aybat et~al.(2018)Aybat, Wang, Lin, and Ma]{aybat2018distributed}
N.~S. Aybat, Z.~Wang, T.~Lin, and S.~Ma.
\newblock Distributed linearized alternating direction method of multipliers
  for composite convex consensus optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 63\penalty0
  (1):\penalty0 5--20, 2018.

\bibitem[Latafat et~al.(2019)Latafat, Freris, and Patrinos]{latafat2017new}
P.~Latafat, N.~M. Freris, and P.~Patrinos.
\newblock A new randomized block-coordinate primal-dual proximal algorithm for
  distributed optimization.
\newblock \emph{{\em IEEE} Transactions on Automatic Control}, 64\penalty0
  (10):\penalty0 4050--4065, Oct. 2019.

\bibitem[Bot et~al.(2015)Bot, Csetnek, Heinrich, and
  Hendrich]{boct2015convergence}
R.~I. Bot, E.~R. Csetnek, A.~Heinrich, and C.~Hendrich.
\newblock On the convergence rate improvement of a primal-dual splitting
  algorithm for solving monotone inclusion problems.
\newblock \emph{Mathematical Programming}, 150\penalty0 (2):\penalty0 251--279,
  2015.

\bibitem[Chambolle and Pock(2016)]{chambolle2016ergodic}
A.~Chambolle and T.~Pock.
\newblock On the ergodic convergence rates of a first-order primal--dual
  algorithm.
\newblock \emph{Mathematical Programming}, 159\penalty0 (1-2):\penalty0
  253--287, Sept. 2016.

\bibitem[Chen et~al.(2013)Chen, Huang, and Zhang]{chen2013aprimal}
P.~Chen, J.~Huang, and X.~Zhang.
\newblock A primal-dual fixed point algorithm for convex separable minimization
  with applications to image restoration.
\newblock \emph{Inverse Problems}, 29\penalty0 (2):\penalty0 025011, Jan. 2013.

\bibitem[Metropolis et~al.(1953)Metropolis, Rosenbluth, Rosenbluth, Teller, and
  Teller]{metropolis1953equation}
N.~Metropolis, A.~W. Rosenbluth, M.~N. Rosenbluth, A.~H. Teller, and E.~Teller.
\newblock Equation of state calculations by fast computing machines.
\newblock \emph{The Journal of Chemical Physics}, 21\penalty0 (6):\penalty0
  1087--1092, 1953.

\bibitem[Xiao and Boyd(2004)]{xiao2004fast}
L.~Xiao and S.~Boyd.
\newblock Fast linear iterations for distributed averaging.
\newblock \emph{Systems \& Control Letters}, 53\penalty0 (1):\penalty0 65--78,
  2004.

\bibitem[Pillai et~al.(2005)Pillai, Suel, and Cha]{pillai2005perron}
S.~U. Pillai, T.~Suel, and S.~Cha.
\newblock The {P}erron-{F}robenius theorem: {S}ome of its applications.
\newblock \emph{IEEE Signal Processing Magazine}, 22\penalty0 (2):\penalty0
  62--75, 2005.

\bibitem[Li and Yan(2017)]{li2017primal}
Z.~Li and M.~Yan.
\newblock A primal-dual algorithm with optimal stepsizes and its application in
  decentralized consensus optimization.
\newblock \emph{available on arXiv:1711.06785}, Nov. 2017.

\bibitem[Nesterov(2013)]{nesterov2013introductory}
Y.~Nesterov.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Volume 87, Springer, 2013.

\end{thebibliography}
