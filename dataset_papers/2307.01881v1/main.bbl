\begin{thebibliography}{10}

\bibitem{aws-pii-detection}
{Amazon Web Services}.
\newblock Detecting and redacting pii using amazon comprehend.
\newblock
  \url{https://aws.amazon.com/ko/blogs/machine-learning/detecting-and-redacting-pii-using-amazon-comprehend/},
  2023.

\bibitem{bird2009natural}
Steven Bird, Ewan Klein, and Edward Loper.
\newblock {\em Natural language processing with Python: analyzing text with the
  natural language toolkit}.
\newblock " O'Reilly Media, Inc.", 2009.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{carlini2022quantifying}
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian
  Tramer, and Chiyuan Zhang.
\newblock Quantifying memorization across neural language models.
\newblock {\em arXiv preprint arXiv:2202.07646}, 2022.

\bibitem{carlini2021extracting}
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel
  Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar
  Erlingsson, Alina Oprea, and Colin Raffel.
\newblock Extracting training data from large language models, 2021.

\bibitem{chen2020gan}
Dingfan Chen, Ning Yu, Yang Zhang, and Mario Fritz.
\newblock Gan-leaks: A taxonomy of membership inference attacks against
  generative models.
\newblock In {\em Proceedings of the 2020 ACM SIGSAC conference on computer and
  communications security}, pages 343--362, 2020.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{conover1999practical}
William~Jay Conover.
\newblock {\em Practical nonparametric statistics}, volume 350.
\newblock john wiley \& sons, 1999.

\bibitem{fredrikson2015model}
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.
\newblock Model inversion attacks that exploit confidence information and basic
  countermeasures.
\newblock In {\em Proceedings of the 22nd ACM SIGSAC conference on computer and
  communications security}, pages 1322--1333, 2015.

\bibitem{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The {P}ile: An 800{GB} dataset of diverse text for language modeling.
\newblock {\em arXiv preprint arXiv:2101.00027}, 2020.

\bibitem{he2019model}
Zecheng He, Tianwei Zhang, and Ruby~B Lee.
\newblock Model inversion attacks against collaborative inference.
\newblock In {\em Proceedings of the 35th Annual Computer Security Applications
  Conference}, pages 148--162, 2019.

\bibitem{hisamoto2020membership}
Sorami Hisamoto, Matt Post, and Kevin Duh.
\newblock Membership inference attacks on sequence-to-sequence models: Is my
  data in your machine translation system?
\newblock {\em Transactions of the Association for Computational Linguistics},
  8:49--63, 2020.

\bibitem{huang2022large}
Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang.
\newblock Are large pre-trained language models leaking your personal
  information?
\newblock {\em arXiv preprint arXiv:2205.12628}, 2022.

\bibitem{ippolito2022preventing}
Daphne Ippolito, Florian Tram{\`e}r, Milad Nasr, Chiyuan Zhang, Matthew
  Jagielski, Katherine Lee, Christopher~A Choquette-Choo, and Nicholas Carlini.
\newblock Preventing verbatim memorization in language models gives a false
  sense of privacy.
\newblock {\em arXiv preprint arXiv:2210.17546}, 2022.

\bibitem{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 3045--3059, 2021.

\bibitem{levenshtein1966binary}
Vladimir~I Levenshtein et~al.
\newblock Binary codes capable of correcting deletions, insertions, and
  reversals.
\newblock In {\em Soviet physics doklady}, volume~10, pages 707--710. Soviet
  Union, 1966.

\bibitem{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In {\em Proceedings of the 59th Annual Meeting of the Association for
  Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 4582--4597, 2021.

\bibitem{livemint-pii-leak}
{LiveMint}.
\newblock Chatgpt answer goes wrong, gives away journalist's number to join
  signal.
\newblock
  \url{https://www.livemint.com/news/chatgpt-answer-goes-wrong-gives-away-journalist-s-number-to-join-signal}\\
  \url{-11676625029542.html}, 2023.

\bibitem{loshchilovdecoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{lu2021fantastically}
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.
\newblock Fantastically ordered prompts and where to find them: Overcoming
  few-shot prompt order sensitivity.
\newblock {\em arXiv preprint arXiv:2104.08786}, 2021.

\bibitem{lukas2023analyzing}
Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, and
  Santiago Zanella-B{\'e}guelin.
\newblock Analyzing leakage of personally identifiable information in language
  models.
\newblock {\em arXiv preprint arXiv:2302.00539}, 2023.

\bibitem{maus2023adversarial}
Natalie Maus, Patrick Chao, Eric Wong, and Jacob Gardner.
\newblock Adversarial prompting for black box foundation models.
\newblock {\em arXiv preprint arXiv:2302.04237}, 2023.

\bibitem{MsPresidio}
Omri Mendels, Coby Peled, Nava Vaisman~Levy, Tomer Rosenthal, Limor Lahiani,
  et~al.
\newblock {Microsoft Presidio}: Context aware, pluggable and customizable pii
  anonymization service for text and images, 2018.

\bibitem{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem{Paris_2023}
Martine Paris.
\newblock Chatgpt hits 100 million users, google invests in ai bot and catgpt
  goes viral, Apr 2023.

\bibitem{pfitzmann2010terminology}
Andreas Pfitzmann and Marit Hansen.
\newblock A terminology for talking about privacy by data minimization:
  Anonymity, unlinkability, undetectability, unobservability, pseudonymity, and
  identity management, 2010.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{reynolds2021prompt}
Laria Reynolds and Kyle McDonell.
\newblock Prompt programming for large language models: Beyond the few-shot
  paradigm.
\newblock In {\em Extended Abstracts of the 2021 CHI Conference on Human
  Factors in Computing Systems}, pages 1--7, 2021.

\bibitem{rigaki2020survey}
Maria Rigaki and Sebastian Garcia.
\newblock A survey of privacy attacks in machine learning.
\newblock {\em arXiv preprint arXiv:2007.07646}, 2020.

\bibitem{scao2022bloom}
Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c},
  Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois
  Yvon, Matthias Gall{\'e}, et~al.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock {\em arXiv preprint arXiv:2211.05100}, 2022.

\bibitem{shokri2017membership}
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.
\newblock Membership inference attacks against machine learning models.
\newblock In {\em 2017 IEEE symposium on security and privacy (SP)}, pages
  3--18. IEEE, 2017.

\bibitem{song2019auditing}
Congzheng Song and Vitaly Shmatikov.
\newblock Auditing data provenance in text-generation models.
\newblock In {\em Proceedings of the 25th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 196--206, 2019.

\bibitem{thoppilan2022lamda}
Romal Thoppilan, Daniel De~Freitas, Jamie Hall, Noam Shazeer, Apoorv
  Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du,
  et~al.
\newblock Lamda: Language models for dialog applications.
\newblock {\em arXiv preprint arXiv:2201.08239}, 2022.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 38--45, Online,
  October 2020. Association for Computational Linguistics.

\bibitem{yang2019neural}
Ziqi Yang, Jiyi Zhang, Ee-Chien Chang, and Zhenkai Liang.
\newblock Neural network inversion in adversarial setting via background
  knowledge alignment.
\newblock In {\em Proceedings of the 2019 ACM SIGSAC Conference on Computer and
  Communications Security}, pages 225--240, 2019.

\bibitem{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock {\em arXiv preprint arXiv:2205.01068}, 2022.

\bibitem{zhang2020secret}
Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo~Li, and Dawn Song.
\newblock The secret revealer: Generative model-inversion attacks against deep
  neural networks.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 253--261, 2020.

\bibitem{zhu2019deep}
Ligeng Zhu, Zhijian Liu, and Song Han.
\newblock Deep leakage from gradients.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\end{thebibliography}
