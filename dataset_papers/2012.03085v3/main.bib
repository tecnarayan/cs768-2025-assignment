% SIR model
@article{kermack_contribution_1927,
  title={A contribution to the mathematical theory of epidemics},
  author={Kermack, William Ogilvy and McKendrick, Anderson G},
  journal={Proceedings of the royal society of london. Series A, Containing papers of a mathematical and physical character},
  volume={115},
  number={772},
  pages={700--721},
  year={1927},
  publisher={The Royal Society London}
}

@article{jeffreys_invariant_1946,
  title={An invariant form for the prior probability in estimation problems},
  author={Jeffreys, Harold},
  journal={Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences},
  volume={186},
  number={1007},
  pages={453--461},
  year={1946},
  publisher={The Royal Society London}
}

% EM
@article{dempster_maximum_1977,
  title={Maximum likelihood from incomplete data via the EM algorithm},
  author={Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={39},
  number={1},
  pages={1--22},
  year={1977},
  publisher={Wiley Online Library}
}

@article{jacobs_adaptive_1991,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}

% Main article: Mixture Density Networks
@techreport{bishop_mixture_1994,
     title = {{Mixture Density Networks}},
     author = {Bishop, Christopher M},
     year = {1994},
     institution = {Aston University},
}


% Hierarchical Mixture of Experts
@article{jordan_hierarchical_1994,
  title={Hierarchical mixtures of experts and the EM algorithm},
  author={Jordan, Michael I and Jacobs, Robert A},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={181--214},
  year={1994},
  publisher={MIT Press}
}

@article{lecun_convolutional_1995,
	title = {Convolutional networks for images, speech, and time series},
	volume = {3361},
	number = {10},
	journal = {The Handbook of Brain Theory and Neural Networks},
	author = {LeCun, Yann and Bengio, Yoshua and {others}},
	year = {1995},
	pages = {1995}
}

@article{sperduti_supervised_1997,
	title = {Supervised neural networks for the classification of structures},
	volume = {8},
	number = {3},
	journal = {IEEE Transactions on Neural Networks},
	author = {Sperduti, Alessandro and Starita, Antonina},
	year = {1997},
	note = {Publisher: IEEE},
	pages = {714--735}
}

@incollection{prechelt_early_1998,
	title = {Early stopping-but when?},
	booktitle = {Neural {Networks}: {Tricks} of the trade},
	publisher = {Springer},
	author = {Prechelt, Lutz},
	year = {1998},
	pages = {55--69}
}

% Application of LSTM + MDN to finance
@inproceedings{schittenkopf_volatility_1998,
  title={Volatility prediction with mixture density networks},
  author={Schittenkopf, Christian and Dorffner, Georg and Dockner, Engelbert J},
  booktitle={International Conference on Artificial Neural Networks (ICANN)},
  pages={929--934},
  year={1998},
  organization={Springer}
}

@article{frasconi_general_1998,
	title = {A general framework for adaptive processing of data structures},
	volume = {9},
	number = {5},
	journal = {IEEE Transactions on Neural Networks},
	author = {Frasconi, Paolo and Gori, Marco and Sperduti, Alessandro},
	year = {1998},
	note = {Publisher: IEEE},
	pages = {768--786}
}

% Barabasi-Albert Model
@article{barabasi_emergence_1999,
  title={Emergence of scaling in random networks},
  author={Barab{\'a}si, Albert-L{\'a}szl{\'o} and Albert, R{\'e}ka},
  journal={Science},
  volume={286},
  number={5439},
  pages={509--512},
  year={1999},
  publisher={American Association for the Advancement of Science}
}

@book{cover_elements_1999,
  title={Elements of information theory},
  author={Cover, Thomas M},
  year={1999},
  publisher={John Wiley \& Sons}
}

@book{bollobas_random_2001,
  title={Random graphs},
  author={Bollob{\'a}s, B{\'e}la and B{\'e}la, Bollob{\'a}s},
  number={73},
  year={2001},
  publisher={Cambridge university press}
}

% Variational Bayesian model selection for Mixture Model
@inproceedings{corduneanu_variational_2001,
  title={Variational Bayesian model selection for mixture distributions},
  author={Corduneanu, Adrian and Bishop, Christopher M},
  booktitle={Artificial intelligence and Statistics},
  volume={2001},
  pages={27--34},
  year={2001},
  organization={Morgan Kaufmann Waltham, MA}
}

@inproceedings{hinton_stochastic_2003,
  title={Stochastic neighbor embedding},
  author={Hinton, Geoffrey E and Roweis, Sam T},
  booktitle={Advances in neural information processing systems},
  pages={857--864},
  year={2002}
}

% Review article, useful to describe different kinds of networks, epidemic models and potential behaviors 
@article{keeling_networks_2005,
  title={Networks and epidemic models},
  author={Keeling, Matt J and Eames, Ken TD},
  journal={Journal of the Royal Society Interface},
  volume={2},
  number={4},
  pages={295--307},
  year={2005},
  publisher={The Royal Society London}
}

@article{ralaivola_graph_2005,
	title = {Graph kernels for chemical informatics},
	volume = {18},
	number = {8},
	journal = {Neural Networks},
	author = {Ralaivola, Liva and Swamidass, Sanjay J and Saigo, Hiroto and Baldi, Pierre},
	year = {2005},
	note = {Publisher: Elsevier},
	pages = {1093--1110}
}

% L2 distance between mixture of Gaussians
@inproceedings{helen_query_2007,
  title={Query by example of audio signals using Euclidean distance between Gaussian mixture models},
  author={Hel{\'e}n, Marko and Virtanen, Tuomas},
  booktitle={2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP'07},
  volume={1},
  pages={I--225},
  year={2007},
  organization={IEEE}
}


@article{maaten_visualizing_2008,
  title={Visualizing data using t-SNE},
  author={Maaten, Laurens van der and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={Nov},
  pages={2579--2605},
  year={2008}
}

@article{scarselli_graph_2009,
	title = {The graph neural network model},
	volume = {20},
	number = {1},
	journal = {IEEE Transactions on Neural Networks},
	author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
	year = {2009},
	note = {Publisher: IEEE},
	pages = {61--80}
}

@article{micheli_neural_2009,
	title = {Neural network for graphs: {A} contextual constructive approach},
	volume = {20},
	number = {3},
	journal = {IEEE Transactions on Neural Networks},
	author = {Micheli, Alessio},
	year = {2009},
	note = {Publisher: IEEE},
	pages = {498--511}
}

@article{vishwanathan_graph_2010,
	title = {Graph kernels},
	volume = {11},
	number = {Apr},
	journal = {Journal of Machine Learning Research},
	author = {Vishwanathan, S Vichy N and Schraudolph, Nicol N and Kondor, Risi and Borgwardt, Karsten M},
	year = {2010},
	pages = {1201--1242}
}

% TO READ
@article{lancic_phase_2011,
  title={Phase diagram of epidemic spreading---unimodal vs. bimodal probability distributions},
  author={Lancic, Alen and Antulov-Fantulin, Nino and Sikic, Mile and Stefancic, Hrvoje},
  journal={Physica A Statistical Mechanics and its Applications},
  volume={390},
  pages={65--76},
  year={2011}
}

% MoE survey
@article{yuksel_twenty_2012,
  title={Twenty years of mixture of experts},
  author={Yuksel, Seniha Esen and Wilson, Joseph N and Gader, Paul D},
  journal={IEEE transactions on neural networks and learning systems},
  volume={23},
  number={8},
  pages={1177--1193},
  year={2012},
  publisher={IEEE}
}

@article{irwin_zinc_2012,
  title={ZINC: a free tool to discover chemistry for biology},
  author={Irwin, John J and Sterling, Teague and Mysinger, Michael M and Bolstad, Erin S and Coleman, Ryan G},
  journal={Journal of chemical information and modeling},
  volume={52},
  number={7},
  pages={1757--1768},
  year={2012},
  publisher={ACS Publications}
}

% Studies how the network influences the SIR model 
@inproceedings{opuszko_impact_2013,
  title={Impact of the network structure on the {SIR} model spreading phenomena in online networks},
  author={Opuszko, Marek and Ruhland, Johannes},
  booktitle={Proceedings of the 8th International Multi-Conference on Computing in the Global Information Technology (ICCGIâ€™13)},
  year={2013}
}

% Use it as example of applications where ML accelerates simulations
@article{pilania_accelerating_2013,
  title={Accelerating materials property predictions using machine learning},
  author={Pilania, Ghanshyam and Wang, Chenchen and Jiang, Xun and Rajasekaran, Sanguthevar and Ramprasad, Ramamurthy},
  journal={Scientific reports},
  volume={3},
  number={1},
  pages={1--6},
  year={2013},
  publisher={Nature Publishing Group}
}

% Deep Mixture of Experts (different from Density Networks). It contains another strategy to avoid collapse of softmax (see pereyra_regularizing_2017).
@article{eigen_learning_2013,
  title={Learning factored representations in a deep mixture of experts},
  author={Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  journal={International Conference on Learning Representations (ICLR) Workshop},
  year={2013}
}

% MoE survey
@article{masoudnia_mixture_2014,
  title={Mixture of experts: a literature survey},
  author={Masoudnia, Saeed and Ebrahimpour, Reza},
  journal={Artificial Intelligence Review},
  volume={42},
  number={2},
  pages={275--293},
  year={2014},
  publisher={Springer}
}

% Generic analysis of unsupervised metrics for clustering. Not necessarily useful here
@article{emmons_analysis_2016,
  title={Analysis of network clustering algorithms and cluster quality metrics at scale},
  author={Emmons, Scott and Kobourov, Stephen and Gallant, Mike and B{\"o}rner, Katy},
  journal={PloS one},
  volume={11},
  number={7},
  year={2016},
  publisher={Public Library of Science}
}

@inproceedings{kipf_variational_2016,
	title = {Variational graph auto-encoders},
	booktitle = {Workshop on {Bayesian} {Deep} {Learning}, {Neural} {Information} {Processing} {System} ({NIPS})},
	author = {Kipf, Thomas N and Welling, Max},
	year = {2016}
}

@inproceedings{yang_revisiting_2016,
  title={Revisiting semi-supervised learning with graph embeddings},
  author={Yang, Zhilin and Cohen, William and Salakhudinov, Ruslan},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={40--48},
  year={2016}
}

% Use it as example of applications where ML accelerates simulations
@article{behler_perspective_2016,
  title={Perspective: Machine learning potentials for atomistic simulations},
  author={Behler, J{\"o}rg},
  journal={The Journal of chemical physics},
  volume={145},
  number={17},
  pages={170901},
  year={2016},
  publisher={AIP Publishing LLC}
}

@article{bronstein_geometric_2017,
	title = {Geometric deep learning: going beyond {Euclidean} data},
	volume = {34},
	number = {4},
	journal = {IEEE Signal Processing Magazine},
	author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
	year = {2017},
	pages = {25. 18--42}
}

@inproceedings{kipf_semi-supervised_2017,
	title = {Semi-supervised classification with graph convolutional networks},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Kipf, Thomas N and Welling, Max},
	year = {2017}
}

% Article that shows how to prevent collapse of softmax in a (not-so-principled) way. We achieve this via a prior (MAP learning).
@article{pereyra_regularizing_2017,
  title={Regularizing neural networks by penalizing confident output distributions},
  author={Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
  journal={International Conference on Learning Representations (ICLR) Workshop},
  year={2017}
}

@article{battaglia_relational_2018,
	title = {Relational inductive biases, deep learning, and graph networks},
	journal = {arXiv preprint arXiv:1806.01261},
	author = {Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and {others}},
	year = {2018}
}

@inproceedings{beck_graph--sequence_2018,
	title = {Graph-to-sequence {Learning} using {Gated} {Graph} {Neural} {Networks}},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({ACL}), {Volume} 1 ({Long} {Papers})},
	author = {Beck, Daniel and Haffari, Gholamreza and Cohn, Trevor},
	year = {2018},
	pages = {273--283}
}

@inproceedings{bacciu_contextual_2018,
	title = {Contextual {Graph} {Markov} {Model}: {A} deep and generative approach to graph processing},
	volume = {80},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning} ({ICML})},
	publisher = {PMLR},
	author = {Bacciu, Davide and Errica, Federico and Micheli, Alessio},
	year = {2018},
	pages = {294--303}
}

@inproceedings{choi_uncertainty_2018,
  title={Uncertainty-aware learning from demonstration using mixture density networks with sampling-free variance modeling},
  author={Choi, Sungjoon and Lee, Kyungjae and Lim, Sungbin and Oh, Songhwai},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={6915--6922},
  year={2018},
  organization={IEEE}
}

% Review on the use of ANN to predict an epidemic, with practical challenges in the data preprocessing etc.
@article{philemon_review_2019,
  title={A Review of Epidemic Forecasting Using Artificial Neural Networks},
  author={Philemon, Manliura Datilo and Ismail, Zuhaimy and Dare, Jayeola},
  journal={International Journal of Epidemiologic Research},
  volume={6},
  number={3},
  pages={132--143},
  year={2019},
  publisher={Shahrekord University of Medical Sciences}
}

@article{bresson_two_2019,
  title={A two-step graph convolutional decoder for molecule generation},
  author={Bresson, Xavier and Laurent, Thomas},
  journal={arXiv preprint arXiv:1906.03412},
  year={2019}
}

@inproceedings{xu_how_2019,
	title = {How powerful are graph neural networks?},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
	year = {2019}
}

@article{chen_alchemy_2019,
  title={Alchemy: A quantum chemistry dataset for benchmarking ai models},
  author={Chen, Guangyong and Chen, Pengfei and Hsieh, Chang-Yu and Lee, Chee-Kong and Liao, Benben and Liao, Renjie and Liu, Weiwen and Qiu, Jiezhong and Sun, Qiming and Tang, Jie and others},
  journal={arXiv preprint arXiv:1906.09427},
  year={2019}
}

% Second main article: Mixture Density Networks applied to SIR WITHOUT STRUCTURE.
@article{davis_use_2020,
  title={The use of mixture density networks in the emulation of complex epidemiological individual-based models},
  author={Davis, Christopher N and Hollingsworth, T Deirdre and Caudron, Quentin and Irvine, Michael A},
  journal={PLoS computational biology},
  volume={16},
  number={3},
  pages={e1006869},
  year={2020},
  publisher={Public Library of Science}
}

% Technical Review on Deep Graph Networks
@article{wu_comprehensive_2020,
  title={A comprehensive survey on graph neural networks},
  author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Philip, S Yu},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2020},
  publisher={IEEE}
}

%doi = {10.1016/j.neunet.2020.06.006},
@article{bacciu_gentle_2020,
  title={A Gentle Introduction to Deep Learning for Graphs},
  author={Bacciu, Davide and Errica, Federico and Micheli, Alessio and Podda, Marco},
  journal={Neural Networks},
  volume={129},
  month={9},
  pages={203--221},
  year={2020},
  publisher={Elsevier}
}

% Application of LSTM + MDN to trajectory prediction
@article{chen_predicting_2020,
  title={Predicting Future Locations of Moving Objects by Recurrent Mixture Density Network},
  author={Chen, Rui and Chen, Mingjian and Li, Wanli and Guo, Naikun},
  journal={ISPRS International Journal of Geo-Information},
  volume={9},
  number={2},
  pages={116},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}

 @inproceedings{podda_deep_2020, 
 title = {A Deep Generative Model for Fragment-Based Molecule Generation}, author = {Podda, Marco and Bacciu, Davide and Micheli, Alessio},
 booktitle = {Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics (AISTATS)},
 pages = {2240-2250},
 year = {2020},
 volume = {108},
 publisher = {PMLR}
} 


@inproceedings{errica_fair_2020,
	title = {A fair comparison of graph neural networks for graph classification},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Errica, Federico and Podda, Marco and Bacciu, Davide and Micheli, Alessio},
	year = {2020}
}

@article{bacciu_probabilistic_2020,
  title={Probabilistic Learning on Graphs via Contextual Architectures},
  author={Bacciu, Davide and Errica, Federico and Micheli, Alessio},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={134},
  pages={1--39},
  year={2020}
}

@article{nowicki_estimation_2001,
  title={Estimation and prediction for stochastic blockstructures},
  author={Nowicki, Krzysztof and Snijders, Tom A B},
  journal={Journal of the American statistical association},
  volume={96},
  number={455},
  pages={1077--1087},
  year={2001},
  publisher={Taylor \& Francis}
}


@article{seshadhri_community_2012,
  title={Community structure and scale-free collections of Erd{\H{o}}s-R{\'e}nyi graphs},
  author={Seshadhri, Comandur and Kolda, Tamara G and Pinar, Ali},
  journal={Physical Review E},
  volume={85},
  number={5},
  pages={056109},
  year={2012},
  publisher={APS}
}
