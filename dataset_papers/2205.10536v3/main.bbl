\begin{thebibliography}{10}

\bibitem{ahn2019variational}
S.~Ahn, S.~X. Hu, A.~Damianou, N.~D. Lawrence, and Z.~Dai.
\newblock Variational information distillation for knowledge transfer.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 9163--9171, 2019.

\bibitem{Cai_2019}
Z.~Cai and N.~Vasconcelos.
\newblock Cascade r-cnn: High quality object detection and instance
  segmentation.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  page 1–1, 2019.

\bibitem{chandrasegaran2022to}
K.~Chandrasegaran, N.-T. Tran, Y.~ZHAO, and N.~man Cheung.
\newblock To smooth or not to smooth? on compatibility between label smoothing
  and knowledge distillation, 2022.

\bibitem{chen2020learning}
H.~Chen, Y.~Wang, C.~Xu, C.~Xu, and D.~Tao.
\newblock Learning student networks via feature embedding.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  32(1):25--35, 2020.

\bibitem{chen2017rethinking}
L.-C. Chen, G.~Papandreou, F.~Schroff, and H.~Adam.
\newblock Rethinking atrous convolution for semantic image segmentation.
\newblock {\em arXiv preprint arXiv:1706.05587}, 2017.

\bibitem{chen2018encoder}
L.-C. Chen, Y.~Zhu, G.~Papandreou, F.~Schroff, and H.~Adam.
\newblock Encoder-decoder with atrous separable convolution for semantic image
  segmentation.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 801--818, 2018.

\bibitem{chen2021distilling}
P.~Chen, S.~Liu, H.~Zhao, and J.~Jia.
\newblock Distilling knowledge via knowledge review.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 5008--5017, 2021.

\bibitem{cho2019efficacy}
J.~H. Cho and B.~Hariharan.
\newblock On the efficacy of knowledge distillation.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 4794--4802, 2019.

\bibitem{cubuk2020randaugment}
E.~D. Cubuk, B.~Zoph, J.~Shlens, and Q.~V. Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 702--703, 2020.

\bibitem{dodge2008concise}
Y.~Dodge.
\newblock {\em The concise encyclopedia of statistics}.
\newblock Springer Science \& Business Media, 2008.

\bibitem{du2020agree}
S.~Du, S.~You, X.~Li, J.~Wu, F.~Wang, C.~Qian, and C.~Zhang.
\newblock Agree to disagree: Adaptive ensemble knowledge distillation in
  gradient space.
\newblock {\em advances in neural information processing systems},
  33:12345--12355, 2020.

\bibitem{gou2021knowledge}
J.~Gou, B.~Yu, S.~J. Maybank, and D.~Tao.
\newblock Knowledge distillation: A survey.
\newblock {\em International Journal of Computer Vision}, 129(6):1789--1819,
  2021.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{heo2019comprehensive}
B.~Heo, J.~Kim, S.~Yun, H.~Park, N.~Kwak, and J.~Y. Choi.
\newblock A comprehensive overhaul of feature distillation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1921--1930, 2019.

\bibitem{heo2019knowledge}
B.~Heo, M.~Lee, S.~Yun, and J.~Y. Choi.
\newblock Knowledge transfer via distillation of activation boundaries formed
  by hidden neurons.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 3779--3787, 2019.

\bibitem{hinton2015distilling}
G.~Hinton, O.~Vinyals, and J.~Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{howard2017mobilenets}
A.~G. Howard, M.~Zhu, B.~Chen, D.~Kalenichenko, W.~Wang, T.~Weyand,
  M.~Andreetto, and H.~Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{huang2021relational}
T.~Huang, Z.~Li, H.~Lu, Y.~Shan, S.~Yang, Y.~Feng, F.~Wang, S.~You, and C.~Xu.
\newblock Relational surrogate loss learning.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{huang2022dyrep}
T.~Huang, S.~You, B.~Zhang, Y.~Du, F.~Wang, C.~Qian, and C.~Xu.
\newblock Dyrep: Bootstrapping training with dynamic re-parameterization.
\newblock {\em arXiv preprint arXiv:2203.12868}, 2022.

\bibitem{kendall1938new}
M.~G. Kendall.
\newblock A new measure of rank correlation.
\newblock {\em Biometrika}, 30(1/2):81--93, 1938.

\bibitem{kim2018paraphrasing}
J.~Kim, S.~Park, and N.~Kwak.
\newblock Paraphrasing complex network: Network compression via factor
  transfer.
\newblock {\em arXiv preprint arXiv:1802.04977}, 2018.

\bibitem{kong2020learning}
S.~Kong, T.~Guo, S.~You, and C.~Xu.
\newblock Learning student networks with few data.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 4469--4476, 2020.

\bibitem{lin2017feature}
T.-Y. Lin, P.~Doll{\'a}r, R.~Girshick, K.~He, B.~Hariharan, and S.~Belongie.
\newblock Feature pyramid networks for object detection.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2117--2125, 2017.

\bibitem{lin2017focal}
T.-Y. Lin, P.~Goyal, R.~Girshick, K.~He, and P.~Doll{\'a}r.
\newblock Focal loss for dense object detection.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2980--2988, 2017.

\bibitem{lin2014microsoft}
T.-Y. Lin, M.~Maire, S.~Belongie, J.~Hays, P.~Perona, D.~Ramanan,
  P.~Doll{\'a}r, and C.~L. Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em European conference on computer vision}, pages 740--755.
  Springer, 2014.

\bibitem{liu2020structured}
Y.~Liu, C.~Shu, J.~Wang, and C.~Shen.
\newblock Structured knowledge distillation for dense prediction.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  2020.

\bibitem{liu2021swin}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock {\em arXiv preprint arXiv:2103.14030}, 2021.

\bibitem{marcel2010torchvision}
S.~Marcel and Y.~Rodriguez.
\newblock Torchvision the machine-vision package of torch.
\newblock In {\em Proceedings of the 18th ACM international conference on
  Multimedia}, pages 1485--1488, 2010.

\bibitem{mirzadeh2020improved}
S.~I. Mirzadeh, M.~Farajtabar, A.~Li, N.~Levine, A.~Matsukawa, and
  H.~Ghasemzadeh.
\newblock Improved knowledge distillation via teacher assistant.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 5191--5198, 2020.

\bibitem{park2019relational}
W.~Park, D.~Kim, Y.~Lu, and M.~Cho.
\newblock Relational knowledge distillation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 3967--3976, 2019.

\bibitem{passalis2020probabilistic}
N.~Passalis, M.~Tzelepi, and A.~Tefas.
\newblock Probabilistic knowledge transfer for lightweight deep representation
  learning.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  32(5):2030--2039, 2020.

\bibitem{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Advances in neural information processing systems},
  32:8026--8037, 2019.

\bibitem{pearson1896vii}
K.~Pearson.
\newblock Vii. mathematical contributions to the theory of evolution.—iii.
  regression, heredity, and panmixia.
\newblock {\em Philosophical Transactions of the Royal Society of London.
  Series A, containing papers of a mathematical or physical character},
  (187):253--318, 1896.

\bibitem{peng2019correlation}
B.~Peng, X.~Jin, J.~Liu, D.~Li, Y.~Wu, Y.~Liu, S.~Zhou, and Z.~Zhang.
\newblock Correlation congruence for knowledge distillation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 5007--5016, 2019.

\bibitem{romero2014fitnets}
A.~Romero, N.~Ballas, S.~E. Kahou, A.~Chassang, C.~Gatta, and Y.~Bengio.
\newblock Fitnets: Hints for thin deep nets.
\newblock {\em arXiv preprint arXiv:1412.6550}, 2014.

\bibitem{shen2020label}
Z.~Shen, Z.~Liu, D.~Xu, Z.~Chen, K.-T. Cheng, and M.~Savvides.
\newblock Is label smoothing truly incompatible with knowledge distillation: An
  empirical study.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{shu2021channel}
C.~Shu, Y.~Liu, J.~Gao, Z.~Yan, and C.~Shen.
\newblock Channel-wise knowledge distillation for dense prediction.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 5311--5320, 2021.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{son2021densely}
W.~Son, J.~Na, J.~Choi, and W.~Hwang.
\newblock Densely guided knowledge distillation using multiple teacher
  assistants.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9395--9404, 2021.

\bibitem{tan2019efficientnet}
M.~Tan and Q.~Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In {\em International Conference on Machine Learning}, pages
  6105--6114. PMLR, 2019.

\bibitem{tian2019contrastive}
Y.~Tian, D.~Krishnan, and P.~Isola.
\newblock Contrastive representation distillation.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{tung2019similarity}
F.~Tung and G.~Mori.
\newblock Similarity-preserving knowledge distillation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1365--1374, 2019.

\bibitem{wang2020intra}
Y.~Wang, W.~Zhou, T.~Jiang, X.~Bai, and Y.~Xu.
\newblock Intra-class feature variation distillation for semantic segmentation.
\newblock In {\em European Conference on Computer Vision}, pages 346--362.
  Springer, 2020.

\bibitem{wightman2021resnet}
R.~Wightman, H.~Touvron, and H.~J{\'e}gou.
\newblock Resnet strikes back: An improved training procedure in timm.
\newblock {\em arXiv preprint arXiv:2110.00476}, 2021.

\bibitem{xie2017aggregated}
S.~Xie, R.~Girshick, P.~Doll{\'a}r, Z.~Tu, and K.~He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1492--1500, 2017.

\bibitem{yang2022cross}
C.~Yang, H.~Zhou, Z.~An, X.~Jiang, Y.~Xu, and Q.~Zhang.
\newblock Cross-image relational knowledge distillation for semantic
  segmentation.
\newblock {\em arXiv preprint arXiv:2204.06986}, 2022.

\bibitem{yang2020knowledge}
J.~Yang, B.~Martinez, A.~Bulat, and G.~Tzimiropoulos.
\newblock Knowledge distillation via softmax regression representation
  learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{you2020greedynas}
S.~You, T.~Huang, M.~Yang, F.~Wang, C.~Qian, and C.~Zhang.
\newblock Greedynas: Towards fast one-shot nas with greedy supernet.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 1999--2008, 2020.

\bibitem{you2017learning}
S.~You, C.~Xu, C.~Xu, and D.~Tao.
\newblock Learning from multiple teacher networks.
\newblock In {\em Proceedings of the 23rd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pages 1285--1294, 2017.

\bibitem{zagoruyko2016paying}
S.~Zagoruyko and N.~Komodakis.
\newblock Paying more attention to attention: Improving the performance of
  convolutional neural networks via attention transfer.
\newblock {\em arXiv preprint arXiv:1612.03928}, 2016.

\bibitem{zhang2018mixup}
H.~Zhang, M.~Cisse, Y.~N. Dauphin, and D.~Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{zhang2020improve}
L.~Zhang and K.~Ma.
\newblock Improve object detection with feature-based knowledge distillation:
  Towards accurate and efficient detectors.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{zhang2018shufflenet}
X.~Zhang, X.~Zhou, M.~Lin, and J.~Sun.
\newblock Shufflenet: An extremely efficient convolutional neural network for
  mobile devices.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 6848--6856, 2018.

\bibitem{zhao2017pyramid}
H.~Zhao, J.~Shi, X.~Qi, X.~Wang, and J.~Jia.
\newblock Pyramid scene parsing network.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2881--2890, 2017.

\end{thebibliography}
