\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Song et~al.(2021)Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and
  Poole]{song2021diffusionsde}
Y.~Song, S.~Sohl-Dickstein, D.P. Kingma, A.~Kumar, S.~Ermon, and B.~Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Hyv{{\"a}}rinen(2005)]{hyvarinen2005scorematching}
A.~Hyv{{\"a}}rinen.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (24):\penalty0 695--709, 2005.

\bibitem[Gutmann and Hyv{\"a}rinen(2012)]{gutmann2012nce}
M.~Gutmann and A.~Hyv{\"a}rinen.
\newblock Noise-contrastive estimation of unnormalized statistical models, with
  applications to natural image statistics.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (11):\penalty0 307--361, 2012.

\bibitem[Hinton(2002)]{hinton2002contrastivedivergence}
G.E. Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural Computation}, 14\penalty0 (8):\penalty0 1771--1800,
  2002.

\bibitem[Gao et~al.(2020)Gao, Nijkamp, Kingma, Xu, Dai, and Wu]{gao2020fce}
R.~Gao, E.~Nijkamp, D.P. Kingma, Z.~Xu, A.M. Dai, and Y.~Nian Wu.
\newblock Flow contrastive estimation of energy-based models.
\newblock \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 7515--7525, 2020.

\bibitem[Owen(2013)]{owenmontecarlobook}
A.B. Owen.
\newblock \emph{{Monte Carlo} theory, methods and examples}.
\newblock \url{https://artowen.su.domains/mc/}, 2013.

\bibitem[Liu et~al.(2022)Liu, Rosenfeld, Ravikumar, and
  Risteski]{liu2021nceoptim}
B.~Liu, E.~Rosenfeld, P.~Ravikumar, and A.~Risteski.
\newblock Analyzing and improving the optimization landscape of
  noise-contrastive estimation.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Lee et~al.(2023)Lee, Pabbaraju, Sevekari, and
  Risteski]{lee2022ncegaussiannoise}
H.~Lee, C.~Pabbaraju, A.P. Sevekari, and A.~Risteski.
\newblock Pitfalls of gaussians as a noise distribution in {NCE}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2023.

\bibitem[Chehab et~al.(2022)Chehab, Gramfort, and
  Hyv{\"a}rinen]{chehab2022nceoptimal}
O.~Chehab, A.~Gramfort, and A.~Hyv{\"a}rinen.
\newblock The optimal noise in noise-contrastive learning is not what you
  think.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence (UAI)},
  volume 180, pages 307--316. PMLR, 2022.

\bibitem[Neal(1998)]{neal1998annealing}
R.M. Neal.
\newblock Annealed importance sampling.
\newblock \emph{Statistics and Computing}, 11:\penalty0 125--139, 1998.

\bibitem[Jarzynski(1996)]{jarzynski1996annealing}
C.~Jarzynski.
\newblock Nonequilibrium equality for free energy differences.
\newblock \emph{Physical Review Letters}, 78:\penalty0 2690--2693, 1996.

\bibitem[Jarzynski(1997)]{jarzynski1997annealing}
C.~Jarzynski.
\newblock Equilibrium free-energy differences from nonequilibrium measurements:
  A master-equation approach.
\newblock \emph{Physical Review E}, 56:\penalty0 5018--5035, 1997.

\bibitem[Salakhutdinov and Hinton(2009)]{salakhutdinov2009ais}
R.~Salakhutdinov and G.~Hinton.
\newblock Replicated softmax: an undirected topic model.
\newblock In \emph{Neural Information Processing Systems (NIPS)}, 2009.

\bibitem[Dauphin and Bengio(2013)]{dauphin2013ais}
Y.~Dauphin and Y.~Bengio.
\newblock Stochastic ratio matching of rbms for sparse high-dimensional inputs.
\newblock In \emph{Neural Information Processing Systems (NIPS)}, volume~26.
  Curran Associates, Inc., 2013.

\bibitem[Masrani et~al.(2019)Masrani, Le, and Wood]{masrani2019thermodynamic}
V.~Masrani, T.A. Le, and F.D. Wood.
\newblock The thermodynamic variational objective.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Brekelmans et~al.(2022)Brekelmans, Huang, Ghassemi, Steeg, Grosse, and
  Makhzani]{brekelmans2022ebmbounds}
R.~Brekelmans, S.~Huang, M.~Ghassemi, G.~Ver Steeg, R.B. Grosse, and
  A.~Makhzani.
\newblock Improving mutual information estimation with annealed and
  energy-based bounds.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Gelman and Meng(1998)]{gelman1998importancesamplingext}
A.~Gelman and X.-L. Meng.
\newblock Simulating normalizing constants: From importance sampling to bridge
  sampling to path sampling.
\newblock \emph{Statistical Science}, 13:\penalty0 163--185, 1998.

\bibitem[Grosse et~al.(2013)Grosse, Maddison, and
  Salakhutdinov]{grosse2013annealing}
R.~Grosse, C.~Maddison, and R.~Salakhutdinov.
\newblock Annealing between distributions by averaging moments.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  volume~26. Curran Associates, Inc., 2013.

\bibitem[Torrie and Valleau(1977)]{torrie1977isumbrella}
G.M. Torrie and J.P. Valleau.
\newblock Nonphysical sampling distributions in monte carlo free-energy
  estimation: Umbrella sampling.
\newblock \emph{Journal of Computational Physics}, 23\penalty0 (2):\penalty0
  187--199, 1977.

\bibitem[Meng and Wong(1996)]{meng1996importancesamplingext}
X.-L. Meng and W.H. Wong.
\newblock Simulating ratios of normalizing constants via a simple identity: a
  theoretical exploration.
\newblock \emph{Statistica Sinica}, pages 831--860, 1996.

\bibitem[Masrani et~al.(2021)Masrani, Brekelmans, Bui, Nielsen, Galstyan,
  Ver~Steeg, and Wood]{masrani2021annealedisqpath}
V.~Masrani, R.~Brekelmans, T.~Bui, F.~Nielsen, A.~Galstyan, G.~Ver~Steeg, and
  F.~Wood.
\newblock q-paths: Generalizing the geometric annealing path using power means.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence (UAI)},
  volume 161, pages 1938--1947. PMLR, 27--30 Jul 2021.

\bibitem[Chehab et~al.(2023)Chehab, Gramfort, and
  Hyv{\"a}rinen]{chehab2022nceoptimaljmlr}
O.~Chehab, A.~Gramfort, and A.~Hyv{\"a}rinen.
\newblock {Optimizing the Noise in Self-Supervised Learning: from Importance
  Sampling to Noise-Contrastive Estimation}.
\newblock \emph{ArXiv}, 2023.
\newblock \doi{10.48550/arXiv.2301.09696}.

\bibitem[Newton and Raftery(1994)]{newton1994revis}
M.A. Newton and A.E. Raftery.
\newblock {Approximate Bayesian Inference with the Weighted Likelihood
  Bootstrap}.
\newblock \emph{Journal of the Royal Statistical Society. Series B
  (Methodological)}, 56\penalty0 (1):\penalty0 3--48, 1994.

\bibitem[{van der Vaart}(2000)]{vandervaart2000asympstats}
A.W. {van der Vaart}.
\newblock \emph{Asymptotic Statistics}.
\newblock Asymptotic Statistics. Cambridge University Press, 2000.

\bibitem[Chen and Shao(1997)]{chen1997isratio}
M.-H. Chen and Q.-M. Shao.
\newblock On monte carlo methods for estimating ratios of normalizing
  constants.
\newblock \emph{The Annals of Statistics}, 25\penalty0 (4):\penalty0
  1563--1594, 1997.

\bibitem[Uehara et~al.(2018)Uehara, Matsuda, and Komaki]{uehara2018nce}
M.~Uehara, T.~Matsuda, and F.~Komaki.
\newblock Analysis of noise contrastive estimation from the perspective of
  asymptotic variance.
\newblock \emph{ArXiv}, 2018.
\newblock \doi{10.48550/ARXIV.1808.07983}.

\bibitem[Liu et~al.(2015)Liu, Peng, Ihler, and III]{liu2015ais}
Q.~Liu, J.~Peng, A.T. Ihler, and J.W.~Fisher III.
\newblock Estimating the partition function by discriminance sampling.
\newblock In \emph{Uncertainty in Artificial Intelligence (UAI)}, 2015.

\bibitem[Hyv\"arinen et~al.(2009)Hyv\"arinen, Hurri, and
  Hoyer]{hyvarinen2009naturalimages}
A.~Hyv\"arinen, J.~Hurri, and P.~O. Hoyer.
\newblock \emph{Natural Image Statistics}.
\newblock Springer-Verlag, 2009.

\bibitem[Krause et~al.(2020)Krause, Fischer, and Igel]{krause2020ais}
O.~Krause, A.~Fischer, and C.~Igel.
\newblock Algorithms for estimating the partition function of restricted
  boltzmann machines.
\newblock \emph{Artificial Intelligence}, 278:\penalty0 103195, 2020.

\bibitem[Chatterjee and Diaconis(2015)]{chatterjee2015ISsamplesize}
S.~Chatterjee and P.~Diaconis.
\newblock The sample size required in importance sampling.
\newblock \emph{arXiv: Probability}, 2015.

\bibitem[Efron(2022)]{efron2022exponentialfamilybook}
B.~Efron.
\newblock \emph{Exponential Families in Theory and Practice}.
\newblock Institute of Mathematical Statistics Textbooks. Cambridge University
  Press, 2022.
\newblock ISBN 9781108488907.

\bibitem[Sriperumbudur et~al.(2017)Sriperumbudur, Fukumizu, Gretton,
  Hyv\"{a}rinen, and Kumar]{sriperumbudur2017exponentialfamily}
B.~Sriperumbudur, K.~Fukumizu, A.~Gretton, A.~Hyv\"{a}rinen, and R.~Kumar.
\newblock Density estimation in infinite dimensional exponential families.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (57):\penalty0 1--59, 2017.

\bibitem[Krause et~al.(2013)Krause, Fischer, Glasmachers, and
  Igel]{krause2013dbnapproxerror}
O.~Krause, A.~Fischer, T.~Glasmachers, and C.~Igel.
\newblock Approximation properties of dbns with binary hidden units and
  real-valued visible units.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2013.

\bibitem[Nielsen and Garcia(2011)]{nielsen2011exponentialfamilyreview}
F.~Nielsen and V.~Garcia.
\newblock Statistical exponential families: A digest with flash cards.
\newblock \emph{ArXiv}, 2011.
\newblock \doi{10.48550/arXiv.0911.4863}.

\bibitem[Meng and Schilling(1996)]{meng1996isbridgeempirical}
X.-L. Meng and S.~Schilling.
\newblock Fitting full-information item factor models and an empirical
  investigation of bridge sampling.
\newblock \emph{Journal of the American Statistical Association}, 91\penalty0
  (435):\penalty0 1254--1267, 1996.

\bibitem[Uehara et~al.(2020)Uehara, Kanamori, Takenouchi, and
  Matsuda]{uehara2020nceefficiency}
M.~Uehara, T.~Kanamori, T.~Takenouchi, and T.~Matsuda.
\newblock A unified statistically efficient estimation framework for
  unnormalized models.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, volume 108, pages 809--819. PMLR, 2020.

\bibitem[Ellam et~al.(2017)Ellam, Strathmann, Girolami, and
  Murray]{ellam2017determinant}
L.~Ellam, H.~Strathmann, M.A. Girolami, and I.~Murray.
\newblock A determinant‚Äêfree method to simulate the parameters of large
  gaussian fields.
\newblock \emph{Stat}, 6:\penalty0 271--281, 2017.

\bibitem[Virtanen et~al.(2020)Virtanen, Gommers, Oliphant, Haberland, Reddy,
  Cournapeau, Burovski, Peterson, Weckesser, Bright, {van der Walt}, Brett,
  Wilson, Millman, Mayorov, Nelson, Jones, Kern, Larson, Carey, Polat, Feng,
  Moore, {VanderPlas}, Laxalde, Perktold, Cimrman, Henriksen, Quintero, Harris,
  Archibald, Ribeiro, Pedregosa, {van Mulbregt}, and {SciPy 1.0
  Contributors}]{scipy}
P.~Virtanen, R.~Gommers, T.E. Oliphant, M.~Haberland, T.~Reddy, D.~Cournapeau,
  E.~Burovski, P.~Peterson, W.~Weckesser, J.~Bright, S.J. {van der Walt},
  M.~Brett, J.~Wilson, J.K. Millman, N.~Mayorov, A.R.J. Nelson, E.~Jones,
  R.~Kern, E.~Larson, C.J. Carey, I.~Polat, Y.~Feng, E.W. Moore,
  J.~{VanderPlas}, D.~Laxalde, J.~Perktold, R.~Cimrman, I.~Henriksen, E.A.
  Quintero, C.R. Harris, A.M. Archibald, A.H. Ribeiro, F.~Pedregosa, P.~{van
  Mulbregt}, and {SciPy 1.0 Contributors}.
\newblock {{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in
  Python}.
\newblock \emph{Nature Methods}, 17:\penalty0 261--272, 2020.

\bibitem[Kiwaki(2015)]{kiwaki2015annealing}
T.~Kiwaki.
\newblock Variational optimization of annealing schedules.
\newblock \emph{ArXiv}, 2015.
\newblock \doi{10.48550/arXiv.1502.05313}.

\bibitem[Brekelmans et~al.(2020)Brekelmans, Masrani, Wood, Steeg, and
  Galstyan]{brekelmans2020aisvariationalinference}
R.~Brekelmans, V.~Masrani, F.D. Wood, G.~Ver Steeg, and A.G. Galstyan.
\newblock All in the exponential family: Bregman duality in thermodynamic
  variational inference.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Goshtasbpour et~al.(2023)Goshtasbpour, Cohen, and
  Perez-Cruz]{goshtasbpour2033adaptiveais}
S.~Goshtasbpour, V.~Cohen, and F.~Perez-Cruz.
\newblock Adaptive annealed importance sampling with constant rate progress.
\newblock In \emph{International Conference on Machine Learning (ICML)}, volume
  202 of \emph{Proceedings of Machine Learning Research}, pages 11642--11658.
  PMLR, 2023.

\bibitem[Chopin and Papaspiliopoulos(2020)]{chopin2020smcbook}
N.~Chopin and O.~Papaspiliopoulos.
\newblock \emph{An Introduction to Sequential Monte Carlo}.
\newblock Springer Series in Statistics. Springer International Publishing,
  2020.
\newblock ISBN 9783030478452.

\bibitem[Moral and Doucet(2002)]{delmoral2002smcreview}
P.~Del Moral and A.~Doucet.
\newblock Sequential monte carlo samplers.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 68, 2002.

\bibitem[Dai et~al.(2020)Dai, Heng, Jacob, and Whiteley]{dai2020smcreview}
C.~Dai, J.~Heng, P.E. Jacob, and N.~Whiteley.
\newblock An invitation to sequential monte carlo samplers.
\newblock \emph{Journal of the American Statistical Association}, 117:\penalty0
  1587--1600, 2020.

\bibitem[Syed et~al.(2021)Syed, Romaniello, Campbell, and
  Bouchard-Cote]{syed2021paralleltemperingcost}
S.~Syed, V.~Romaniello, T.~Campbell, and A.~Bouchard-Cote.
\newblock Parallel tempering on optimized paths.
\newblock In \emph{International Conference on Machine Learning (ICML)}, volume
  139 of \emph{Proceedings of Machine Learning Research}, pages 10033--10042.
  PMLR, 2021.

\bibitem[Behrens et~al.(2010)Behrens, Friel, and
  Hurn]{behrens2010temperedtransitionscost}
G.~Behrens, N.~Friel, and M.~Hurn.
\newblock Tuning tempered transitions.
\newblock \emph{Statistics and Computing}, 22:\penalty0 65--78, 2010.

\bibitem[Pihlaja et~al.(2010)Pihlaja, Gutmann, and
  Hyv{\"a}rinen]{pihlaja2010nce}
M.~Pihlaja, M.~Gutmann, and A.~Hyv{\"a}rinen.
\newblock A family of computationally efficient and simple estimators for
  unnormalized statistical models.
\newblock In \emph{Uncertainty in Artificial Intelligence (UAI)}, 2010.

\bibitem[Wang et~al.(2022)Wang, Jones, and Meng]{wang2022bridge}
L.~Wang, D.E. Jones, and X.-L. Meng.
\newblock Warp bridge sampling: The next generation.
\newblock \emph{Journal of the American Statistical Association}, 117\penalty0
  (538):\penalty0 835--851, 2022.

\bibitem[Xing(2022)]{xing2022adaptiveIS}
H.~Xing.
\newblock Improving bridge estimators via {f-GAN}.
\newblock \emph{Statistics and Computing}, 32, 2022.

\bibitem[Polyanskiy and Wu(2022)]{fdivnotespolyanskiy}
Y.~Polyanskiy and Y.~Wu.
\newblock \emph{Information Theory: From Coding to Learning}.
\newblock Cambridge University Press, 2022.

\bibitem[Amari and Nagaoka(2000)]{amari2000infogeometry}
S.-I. Amari and H.~Nagaoka.
\newblock \emph{Methods of information geometry}.
\newblock AMS, 2000.

\end{thebibliography}
