\begin{thebibliography}{10}

\bibitem{liu2021cmix}
Liu, C., N.~Geng, et~al.
\newblock Cmix: Deep multi-agent reinforcement learning with peak and average constraints.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13--17, 2021, Proceedings, Part I 21}, pages 157--173. Springer, 2021.

\bibitem{al2019deeppool}
Al-Abbasi, A.~O., A.~Ghosh, V.~Aggarwal.
\newblock Deeppool: Distributed model-free algorithm for ride-sharing using deep reinforcement learning.
\newblock \emph{IEEE Transactions on Intelligent Transportation Systems}, 20(12):4714--4727, 2019.

\bibitem{ling2023cooperating}
Ling, L., W.~U. Mondal, S.~V. Ukkusuri.
\newblock Cooperating graph neural networks with deep reinforcement learning for vaccine prioritization.
\newblock \emph{arXiv preprint arXiv:2305.05163}, 2023.

\bibitem{bai2023achieving}
Bai, Q., A.~S. Bedi, V.~Aggarwal.
\newblock Achieving zero constraint violation for constrained reinforcement learning via conservative natural policy gradient primal-dual algorithm.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, pages 6737--6744. 2023.

\bibitem{agarwal2022concave}
Agarwal, M., Q.~Bai, V.~Aggarwal.
\newblock Concave utility reinforcement learning with zero-constraint violations.
\newblock \emph{Transactions on Machine Learning Research}, 2022.

\bibitem{chen2022learning}
Chen, L., R.~Jain, H.~Luo.
\newblock Learning infinite-horizon average-reward markov decision process with constraints.
\newblock In \emph{International Conference on Machine Learning}, pages 3246--3270. PMLR, 2022.

\bibitem{agarwal2022regret}
Agarwal, M., Q.~Bai, V.~Aggarwal.
\newblock Regret guarantees for model-based reinforcement learning with long-term average constraints.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 22--31. PMLR, 2022.

\bibitem{wei2022provably}
Wei, H., X.~Liu, L.~Ying.
\newblock A provably-efficient model-free algorithm for infinite-horizon average-reward constrained markov decision processes.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, pages 3868--3876. 2022.

\bibitem{ghosh2022achieving}
Ghosh, A., X.~Zhou, N.~Shroff.
\newblock Achieving sub-linear regret in infinite horizon average reward constrained mdp with linear function approximation.
\newblock In \emph{The Eleventh International Conference on Learning Representations}. 2023.

\bibitem{agarwal2021theory}
Agarwal, A., S.~M. Kakade, J.~D. Lee, G.~Mahajan.
\newblock On the theory of policy gradient methods: Optimality, approximation, and distribution shift.
\newblock \emph{The Journal of Machine Learning Research}, 22(1):4431--4506, 2021.

\bibitem{mondal2023improved}
Mondal, W.~U., V.~Aggarwal.
\newblock Improved sample complexity analysis of natural policy gradient algorithm with general parameterization for infinite horizon discounted reward markov decision processes.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics (AISTATS)}. 2024.

\bibitem{mondal2024sample}
---.
\newblock Sample-efficient constrained reinforcement learning with general parameterization.
\newblock \emph{arXiv preprint arXiv:2405.10624}, 2024.

\bibitem{bai2023regret}
Bai, Q., W.~U. Mondal, V.~Aggarwal.
\newblock Regret analysis of policy gradient algorithm for infinite horizon average reward markov decision processes.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}. 2024.

\bibitem{ding2020natural}
Ding, D., K.~Zhang, T.~Basar, M.~Jovanovic.
\newblock Natural policy gradient primal-dual method for constrained markov decision processes.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:8378--8390, 2020.

\bibitem{agrawal2017optimistic}
Agrawal, S., R.~Jia.
\newblock Optimistic posterior sampling for reinforcement learning: worst-case regret bounds.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{auer2008near}
Auer, P., T.~Jaksch, R.~Ortner.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 21, 2008.

\bibitem{wei2020model}
Wei, C.-Y., M.~J. Jahromi, H.~Luo, H.~Sharma, R.~Jain.
\newblock Model-free reinforcement learning in infinite-horizon average-reward markov decision processes.
\newblock In \emph{International conference on machine learning}, pages 10170--10180. PMLR, 2020.

\bibitem{bai2022achieving}
Bai, Q., A.~S. Bedi, M.~Agarwal, A.~Koppel, V.~Aggarwal.
\newblock Achieving zero constraint violation for constrained reinforcement learning via primal-dual approach.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, pages 3682--3689. 2022.

\bibitem{xu2021crpo}
Xu, T., Y.~Liang, G.~Lan.
\newblock Crpo: A new approach for safe reinforcement learning with convergence guarantee.
\newblock In \emph{International Conference on Machine Learning}, pages 11480--11491. PMLR, 2021.

\bibitem{efroni2020exploration}
Efroni, Y., S.~Mannor, M.~Pirotta.
\newblock Exploration-exploitation in constrained mdps.
\newblock \emph{arXiv preprint arXiv:2003.02189}, 2020.

\bibitem{qiu2020upper}
Qiu, S., X.~Wei, Z.~Yang, J.~Ye, Z.~Wang.
\newblock Upper confidence primal-dual reinforcement learning for cmdp with adversarial loss.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:15277--15287, 2020.

\bibitem{germano2023best}
Germano, J., F.~E. Stradi, G.~Genalti, M.~Castiglioni, A.~Marchesi, N.~Gatti.
\newblock A best-of-both-worlds algorithm for constrained mdps with long-term constraints.
\newblock \emph{arXiv preprint arXiv:2304.14326}, 2023.

\bibitem{pesquerel2022imed}
Pesquerel, F., O.-A. Maillard.
\newblock Imed-rl: Regret optimal learning of ergodic markov decision processes.
\newblock In \emph{NeurIPS 2022-Thirty-sixth Conference on Neural Information Processing Systems}. 2022.

\bibitem{gong2020duality}
Gong, H., M.~Wang.
\newblock A duality approach for regret minimization in average-award ergodic markov decision processes.
\newblock In \emph{Learning for Dynamics and Control}, pages 862--883. PMLR, 2020.

\bibitem{sutton1999policy}
Sutton, R.~S., D.~McAllester, S.~Singh, Y.~Mansour.
\newblock Policy gradient methods for reinforcement learning with function approximation.
\newblock \emph{Advances in neural information processing systems}, 12, 1999.

\bibitem{lattimore2020bandit}
Lattimore, T., C.~Szepesv{\'a}ri.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem{Alekh2020}
Agarwal, A., S.~M. Kakade, J.~D. Lee, G.~Mahajan.
\newblock Optimality and approximation with policy gradient methods in markov decision processes.
\newblock In \emph{Conference on Learning Theory}, pages 64--66. PMLR, 2020.

\bibitem{Mengdi2021}
Zhang, J., C.~Ni, C.~Szepesvari, M.~Wang.
\newblock On the convergence and sample efficiency of variance-reduced policy gradient method.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:2228--2240, 2021.

\bibitem{liu2020improved}
Liu, Y., K.~Zhang, T.~Basar, W.~Yin.
\newblock An improved analysis of (variance-reduced) policy gradient and natural policy gradient methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:7624--7636, 2020.

\bibitem{Chi2019}
Jin, C., Z.~Yang, Z.~Wang, M.~I. Jordan.
\newblock Provably efficient reinforcement learning with linear function approximation.
\newblock In J.~Abernethy, S.~Agarwal, eds., \emph{Proceedings of Thirty Third Conference on Learning Theory}, vol. 125 of \emph{Proceedings of Machine Learning Research}, pages 2137--2143. PMLR, 2020.

\bibitem{Lingxiao2019}
Wang, L., Q.~Cai, Z.~Yang, Z.~Wang.
\newblock Neural policy gradient methods: Global optimality and rates of convergence.
\newblock In \emph{International Conference on Learning Representations}. 2019.

\bibitem{yuan2022general}
Yuan, R., R.~M. Gower, A.~Lazaric.
\newblock A general sample complexity analysis of vanilla policy gradient.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 3332--3380. PMLR, 2022.

\bibitem{fatkhullin2023stochastic}
Fatkhullin, I., A.~Barakat, A.~Kireeva, N.~He.
\newblock Stochastic policy gradient methods: Improved sample complexity for fisher-non-degenerate policies.
\newblock In \emph{International Conference on Machine Learning}, pages 9827--9869. PMLR, 2023.

\bibitem{mondal2023mean}
Mondal, W.~U., V.~Aggarwal, S.~V. Ukkusuri.
\newblock Mean-field control based approximation of multi-agent reinforcement learning in presence of a non-decomposable shared global state.
\newblock \emph{Transactions on Machine Learning Research}, 2023.

\bibitem{monograph}
Aggarwal, V., W.~U. Mondal, Q.~Bai.
\newblock Constrained reinforcement learning with average reward objective: Model-based and model-free algorithms.
\newblock \emph{Found. Trends Optim.}, 6(4):193â€“298, 2024.

\bibitem{dorfman2022adapting}
Dorfman, R., K.~Y. Levy.
\newblock Adapting to mixing time in stochastic optimization with markovian data.
\newblock In \emph{International Conference on Machine Learning}, pages 5429--5446. PMLR, 2022.

\bibitem{ding2023convergence}
Ding, D., K.~Zhang, J.~Duan, T.~Ba{\c{s}}ar, M.~R. Jovanovi{\'c}.
\newblock Convergence and sample complexity of natural policy gradient primal-dual methods for constrained mdps.
\newblock \emph{arXiv preprint arXiv:2206.02346}, 2022.

\bibitem{bai2023provably}
Bai, Q., V.~Aggarwal, A.~Gattami.
\newblock Provably sample-efficient model-free algorithm for mdps with peak constraints.
\newblock \emph{Journal of Machine Learning Research}, 24(60):1--25, 2023.

\end{thebibliography}
