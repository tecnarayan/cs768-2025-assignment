\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alon et~al.(2022)Alon, Xu, He, Sengupta, Roth, and
  Neubig]{alon2022neuro}
Uri Alon, Frank~F Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham
  Neubig.
\newblock Neuro-symbolic language modeling with automaton-augmented retrieval.
\newblock \emph{arXiv preprint arXiv:2201.12431}, 2022.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Baevski and Auli(2018)]{baevski2018adaptive}
Alexei Baevski and Michael Auli.
\newblock Adaptive input representations for neural language modeling.
\newblock \emph{arXiv preprint arXiv:1809.10853}, 2018.

\bibitem[Bengio et~al.(2003)Bengio, Ducharme, Vincent, and
  Jauvin]{bengio2003neural}
Yoshua Bengio, R{\'e}jean Ducharme, Pascal Vincent, and Christian Jauvin.
\newblock A neural probabilistic language model.
\newblock \emph{Journal of machine learning research}, 3\penalty0
  (Feb):\penalty0 1137--1155, 2003.

\bibitem[Borgeaud et~al.(2022)Borgeaud, Mensch, Hoffmann, Cai, Rutherford,
  Millican, Van Den~Driessche, Lespiau, Damoc, Clark,
  et~al.]{borgeaud2022improving}
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza
  Rutherford, Katie Millican, George~Bm Van Den~Driessche, Jean-Baptiste
  Lespiau, Bogdan Damoc, Aidan Clark, et~al.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock In \emph{International conference on machine learning}, pages
  2206--2240. PMLR, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Clauset et~al.(2009)Clauset, Shalizi, and Newman]{clauset2009power}
Aaron Clauset, Cosma~Rohilla Shalizi, and Mark~EJ Newman.
\newblock Power-law distributions in empirical data.
\newblock \emph{SIAM review}, 51\penalty0 (4):\penalty0 661--703, 2009.

\bibitem[Demeter et~al.(2020)Demeter, Kimmel, and Downey]{demeter2020stolen}
David Demeter, Gregory Kimmel, and Doug Downey.
\newblock Stolen probability: A structural weakness of neural language models.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 2191--2197, 2020.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock B{ERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Grave et~al.(2017)Grave, Ciss{\'e}, and Joulin]{grave2017unbounded}
Edouard Grave, Moustapha Ciss{\'e}, and Armand Joulin.
\newblock Unbounded cache model for online language modeling with open
  vocabulary.
\newblock \emph{arXiv preprint arXiv:1711.02604}, 2017.

\bibitem[Grivas et~al.(2022)Grivas, Bogoychev, and Lopez]{grivas2022low}
Andreas Grivas, Nikolay Bogoychev, and Adam Lopez.
\newblock Low-rank softmax can have unargmaxable classes in theory but rarely
  in practice.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 6738--6758,
  2022.

\bibitem[Guu et~al.(2018)Guu, Hashimoto, Oren, and Liang]{guu2018generating}
Kelvin Guu, Tatsunori~B Hashimoto, Yonatan Oren, and Percy Liang.
\newblock Generating sentences by editing prototypes.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  6:\penalty0 437--450, 2018.

\bibitem[He et~al.(2020)He, Berg-Kirkpatrick, and Neubig]{he2020learning}
Junxian He, Taylor Berg-Kirkpatrick, and Graham Neubig.
\newblock Learning sparse prototypes for text generation.
\newblock \emph{arXiv preprint arXiv:2006.16336}, 2020.

\bibitem[He et~al.(2021)He, Neubig, and Berg-Kirkpatrick]{he2021efficient}
Junxian He, Graham Neubig, and Taylor Berg-Kirkpatrick.
\newblock Efficient nearest neighbor language models.
\newblock \emph{arXiv preprint arXiv:2109.04212}, 2021.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, Dean,
  et~al.]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et~al.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2\penalty0 (7), 2015.

\bibitem[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and
  Choi]{holtzman2019curious}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock \emph{arXiv preprint arXiv:1904.09751}, 2019.

\bibitem[Johnson et~al.(2019)Johnson, Douze, and J{\'e}gou]{johnson2019billion}
Jeff Johnson, Matthijs Douze, and Herv{\'e} J{\'e}gou.
\newblock Billion-scale similarity search with {GPUs}.
\newblock \emph{IEEE Transactions on Big Data}, 7\penalty0 (3):\penalty0
  535--547, 2019.

\bibitem[Joulin et~al.(2017)Joulin, Ciss{\'e}, Grangier, J{\'e}gou,
  et~al.]{joulin2017efficient}
Armand Joulin, Moustapha Ciss{\'e}, David Grangier, Herv{\'e} J{\'e}gou, et~al.
\newblock Efficient softmax approximation for gpus.
\newblock In \emph{International conference on machine learning}, pages
  1302--1310. PMLR, 2017.

\bibitem[Khandelwal et~al.(2020{\natexlab{a}})Khandelwal, Fan, Jurafsky,
  Zettlemoyer, and Lewis]{khandelwal2020nearest}
Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.
\newblock Nearest neighbor machine translation.
\newblock \emph{arXiv preprint arXiv:2010.00710}, 2020{\natexlab{a}}.

\bibitem[Khandelwal et~al.(2020{\natexlab{b}})Khandelwal, Levy, Jurafsky,
  Zettlemoyer, and Lewis]{khandelwal20generalization}
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.
\newblock {Generalization through Memorization: Nearest Neighbor Language
  Models}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020{\natexlab{b}}.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Meister et~al.(2020{\natexlab{a}})Meister, Salesky, and
  Cotterell]{meister2020generalized}
Clara Meister, Elizabeth Salesky, and Ryan Cotterell.
\newblock Generalized entropy regularization or: There's nothing special about
  label smoothing.
\newblock \emph{arXiv preprint arXiv:2005.00820}, 2020{\natexlab{a}}.

\bibitem[Meister et~al.(2020{\natexlab{b}})Meister, Vieira, and
  Cotterell]{meister2020best}
Clara Meister, Tim Vieira, and Ryan Cotterell.
\newblock Best-first beam search.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:\penalty0 795--809, 2020{\natexlab{b}}.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and
  Socher]{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}, 2016.

\bibitem[Merity et~al.(2018)Merity, Keskar, and Socher]{merity2018regularizing}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher.
\newblock Regularizing and optimizing {LSTM} language models.
\newblock In \emph{Proceedings of ICLR}, 2018.

\bibitem[Ney et~al.(1994)Ney, Essen, and Kneser]{ney1994structuring}
Hermann Ney, Ute Essen, and Reinhard Kneser.
\newblock On structuring probabilistic dependences in stochastic language
  modelling.
\newblock \emph{Computer Speech \& Language}, 8\penalty0 (1):\penalty0 1--38,
  1994.

\bibitem[Pereyra et~al.(2017)Pereyra, Tucker, Chorowski, Kaiser, and
  Hinton]{pereyra2017regularizing}
Gabriel Pereyra, George Tucker, Jan Chorowski, {\L}ukasz Kaiser, and Geoffrey
  Hinton.
\newblock Regularizing neural networks by penalizing confident output
  distributions.
\newblock \emph{arXiv preprint arXiv:1701.06548}, 2017.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}, 2019.

\bibitem[Sennrich et~al.(2015)Sennrich, Haddow, and Birch]{sennrich2015neural}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock \emph{arXiv preprint arXiv:1508.07909}, 2015.

\bibitem[Stahlberg and Byrne(2019)]{stahlberg2019nmt}
Felix Stahlberg and Bill Byrne.
\newblock On nmt search errors and model errors: Cat got your tongue?
\newblock \emph{arXiv preprint arXiv:1908.10090}, 2019.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem[Wang et~al.(2022)Wang, Fan, Chen, and Xiong]{Wang2022EfficientCK}
Dexin Wang, Kai Fan, Boxing Chen, and Deyi Xiong.
\newblock Efficient cluster-based k-nearest-neighbor machine translation.
\newblock \emph{ArXiv}, abs/2204.06175, 2022.

\bibitem[Yang et~al.(2017)Yang, Dai, Salakhutdinov, and
  Cohen]{yang2017breaking}
Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William~W Cohen.
\newblock Breaking the softmax bottleneck: A high-rank rnn language model.
\newblock \emph{arXiv preprint arXiv:1711.03953}, 2017.

\bibitem[Yang et~al.(2022)Yang, Sun, and Wan]{yang2022nearest}
Zhixian Yang, Renliang Sun, and Xiaojun Wan.
\newblock Nearest neighbor knowledge distillation for neural machine
  translation.
\newblock In \emph{Proceedings of the 2022 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 5546--5556, Seattle, United States, July 2022.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.406}.
\newblock URL \url{https://aclanthology.org/2022.naacl-main.406}.

\end{thebibliography}
