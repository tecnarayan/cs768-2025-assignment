@inproceedings{syed2008apprenticeship,
  title={Apprenticeship learning using linear programming},
  author={Syed, Umar and Bowling, Michael and Schapire, Robert E},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1032--1039},
  year={2008}
}

@inproceedings{ratliff2006maximum,
  title={Maximum margin planning},
  author={Ratliff, Nathan D and Bagnell, J Andrew and Zinkevich, Martin A},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={729--736},
  year={2006}
}

@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K},
  booktitle={Aaai},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA}
}

@article{torabi2018behavioral,
  title={Behavioral cloning from observation},
  author={Torabi, Faraz and Warnell, Garrett and Stone, Peter},
  journal={arXiv preprint arXiv:1805.01954},
  year={2018}
}
@inproceedings{nair2017combining,
  title={Combining self-supervised learning and imitation for vision-based rope manipulation},
  author={Nair, Ashvin and Chen, Dian and Agrawal, Pulkit and Isola, Phillip and Abbeel, Pieter and Malik, Jitendra and Levine, Sergey},
  booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={2146--2153},
  year={2017},
  organization={IEEE}
}

@article{pan2017agile,
  title={Agile autonomous driving using end-to-end deep imitation learning},
  author={Pan, Yunpeng and Cheng, Ching-An and Saigol, Kamil and Lee, Keuntaek and Yan, Xinyan and Theodorou, Evangelos and Boots, Byron},
  journal={arXiv preprint arXiv:1709.07174},
  year={2017}
}

@inproceedings{sun2017deeply,
  title={Deeply aggrevated: Differentiable imitation learning for sequential prediction},
  author={Sun, Wen and Venkatraman, Arun and Gordon, Geoffrey J and Boots, Byron and Bagnell, J Andrew},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={3309--3318},
  year={2017},
  organization={JMLR. org}
}

@article{arora2020provable,
  title={Provable Representation Learning for Imitation Learning via Bi-level Optimization},
  author={Arora, Sanjeev and Du, Simon S and Kakade, Sham and Luo, Yuping and Saunshi, Nikunj},
  journal={arXiv preprint arXiv:2002.10544},
  year={2020}
}

@inproceedings{finn2016guided,
  title={Guided cost learning: Deep inverse optimal control via policy optimization},
  author={Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={49--58},
  year={2016}
}

@inproceedings{ho2016generative,
  title={Generative adversarial imitation learning},
  author={Ho, Jonathan and Ermon, Stefano},
  booktitle={Advances in neural information processing systems},
  pages={4565--4573},
  year={2016}
}

@article{schaal1999imitation,
  title={Is imitation learning the route to humanoid robots?},
  author={Schaal, Stefan},
  journal={Trends in cognitive sciences},
  volume={3},
  number={6},
  pages={233--242},
  year={1999},
  publisher={Elsevier}
}

@article{ke2019imitation,
  title={Imitation Learning as $ f $-Divergence Minimization},
  author={Ke, Liyiming and Barnes, Matt and Sun, Wen and Lee, Gilwoo and Choudhury, Sanjiban and Srinivasa, Siddhartha},
  journal={arXiv preprint arXiv:1905.12888},
  year={2019}
}

@Article{GheshlaghiAzar2013,
author="Gheshlaghi Azar, Mohammad
and Munos, R{\'e}mi
and Kappen, Hilbert J.",
title="Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model",
journal="Machine Learning",
year="2013",
month="Jun",
day="01",
volume="91",
number="3",
pages="325--349",
abstract="We consider the problems of learning the optimal action-value function and the optimal policy in discounted-reward Markov decision processes (MDPs). We prove new PAC bounds on the sample-complexity of two well-known model-based reinforcement learning (RL) algorithms in the presence of a generative model of the MDP: value iteration and policy iteration. The first result indicates that for an MDP with N state-action pairs and the discount factor $\gamma$∈[0,1) only O(Nlog(N/$\delta$)/((1−$\gamma$)3$\epsilon$2)) state-transition samples are required to find an $\epsilon$-optimal estimation of the action-value function with the probability (w.p.) 1−$\delta$. Further, we prove that, for small values of $\epsilon$, an order of O(Nlog(N/$\delta$)/((1−$\gamma$)3$\epsilon$2)) samples is required to find an $\epsilon$-optimal policy w.p. 1−$\delta$. We also prove a matching lower bound of $\Theta$(Nlog(N/$\delta$)/((1−$\gamma$)3$\epsilon$2)) on the sample complexity of estimating the optimal action-value function with $\epsilon$ accuracy. To the best of our knowledge, this is the first minimax result on the sample complexity of RL: the upper bounds match the lower bound in terms of N, $\epsilon$, $\delta$ and 1/(1−$\gamma$) up to a constant factor. Also, both our lower bound and upper bound improve on the state-of-the-art in terms of their dependence on 1/(1−$\gamma$).",
issn="1573-0565",
doi="10.1007/s10994-013-5368-1",
url="https://doi.org/10.1007/s10994-013-5368-1"
}


@inproceedings{Sidford:2018:VRV:3174304.3175320,
 author = {Sidford, Aaron and Wang, Mengdi and Wu, Xian and Ye, Yinyu},
 title = {Variance Reduced Value Iteration and Faster Algorithms for Solving Markov Decision Processes},
 booktitle = {Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms},
 series = {SODA '18},
 year = {2018},
 isbn = {978-1-6119-7503-1},
 location = {New Orleans, Louisiana},
 pages = {770--787},
 numpages = {18},
 url = {http://dl.acm.org/citation.cfm?id=3174304.3175320},
 acmid = {3175320},
 publisher = {Society for Industrial and Applied Mathematics},
 address = {Philadelphia, PA, USA},
} 


@incollection{NIPS1998_1531,
title = {Finite-Sample Convergence Rates for Q-Learning and Indirect Algorithms},
author = {Michael J. Kearns and Satinder P. Singh},
booktitle = {Advances in Neural Information Processing Systems 11},
editor = {M. J. Kearns and S. A. Solla and D. A. Cohn},
pages = {996--1002},
year = {1999},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/1531-finite-sample-convergence-rates-for-q-learning-and-indirect-algorithms.pdf}
}


@article{10.2307/2627210,
 ISSN = {00251909, 15265501},
 URL = {http://www.jstor.org/stable/2627210},
 author = {F. d'Epenoux},
 journal = {Management Science},
 number = {1},
 pages = {98--108},
 publisher = {INFORMS},
 title = {A Probabilistic Production and Inventory Problem},
 volume = {10},
 year = {1963}
}


@phdthesis{Littman:1996:ASD:924039,
 author = {Littman, Michael Lederman},
 title = {Algorithms for Sequential Decision-making},
 year = {1996},
 isbn = {0-591-16350-0},
 note = {AAI9709069},
 publisher = {Brown University},
 address = {Providence, RI, USA},
} 

@InProceedings{Ross-AIstats10,
  title = 	 {Efficient Reductions for Imitation Learning},
  author = 	 {Stephane Ross and Drew Bagnell},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {661--668},
  year = 	 {2010},
  editor = 	 {Yee Whye Teh and Mike Titterington},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/ross10a/ross10a.pdf},
  url = 	 {http://proceedings.mlr.press/v9/ross10a.html},
  abstract = 	 {Imitation Learning, while applied successfully on many large real-world problems, is typically addressed as a standard supervised learning problem, where it is assumed the training and testing data are i.i.d..  This is not true in imitation learning as the learned policy influences the future test inputs (states) upon which it will be tested. We show that this leads to compounding errors and a regret bound that grows quadratically in the time horizon of the task. We propose two alternative algorithms for imitation learning where training occurs over several episodes of interaction. These two approaches share in common that the learner’s policy is slowly modified from executing the expert’s policy to the learned policy. We show that this leads to stronger performance guarantees and demonstrate the improved performance on two challenging problems: training a learner to play 1) a 3D racing game (Super Tux Kart) and 2) Mario Bros.; given input images from the games and corresponding actions taken by a human expert and near-optimal planner respectively.}
}

@inproceedings{Beygelzimer05,
  author={Alina Beygelzimer and Varsha Dani and Thomas P. Hayes and John Langford and Bianca Zadrozny},
  title={Error limiting reductions between classification tasks},
  year={2005},
  cdate={1104537600000},
  pages={49-56},
  url={https://doi.org/10.1145/1102351.1102358},
  booktitle={ICML},
}

@inproceedings{kaariainen2006lower,
  title={Lower bounds for reductions},
  author={K{\"a}{\"a}ri{\"a}inen, Matti},
  booktitle={Atomic Learning Workshop},
  year={2006}
}

@inproceedings{Abbeel-Ng-ILviaIRL,
author = {Abbeel, Pieter and Ng, Andrew Y.},
title = {Apprenticeship Learning via Inverse Reinforcement Learning},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015430},
doi = {10.1145/1015330.1015430},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {1},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML ’04}
}

@incollection{Syed-Schapire-08,
title = {A Game-Theoretic Approach to Apprenticeship Learning},
author = {Syed, Umar and Schapire, Robert E},
booktitle = {Advances in Neural Information Processing Systems 20},
editor = {J. C. Platt and D. Koller and Y. Singer and S. T. Roweis},
pages = {1449--1456},
year = {2008},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3293-a-game-theoretic-approach-to-apprenticeship-learning.pdf}
}

@incollection{Abbeel-Ng-helicopter,
title = {An Application of Reinforcement Learning to Aerobatic Helicopter Flight},
author = {Abbeel, Pieter and Adam Coates and Morgan Quigley and Andrew Y. Ng},
booktitle = {Advances in Neural Information Processing Systems 19},
editor = {B. Sch\"{o}lkopf and J. C. Platt and T. Hoffman},
pages = {1--8},
year = {2007},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/3151-an-application-of-reinforcement-learning-to-aerobatic-helicopter-flight.pdf}
}


@incollection{syed-schapire-10,
title = {A Reduction from Apprenticeship Learning to Classification},
author = {Syed, Umar and Schapire, Robert E},
booktitle = {Advances in Neural Information Processing Systems 23},
editor = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
pages = {2253--2261},
year = {2010},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4180-a-reduction-from-apprenticeship-learning-to-classification.pdf}
}


@article{berend2013,
author = "Berend, Daniel and Kontorovich, Aryeh",
doi = "10.1214/ECP.v18-2359",
fjournal = "Electronic Communications in Probability",
journal = "Electron. Commun. Probab.",
pages = "7 pp.",
pno = "3",
publisher = "The Institute of Mathematical Statistics and the Bernoulli Society",
title = "On the concentration of the missing mass",
url = "https://doi.org/10.1214/ECP.v18-2359",
volume = "18",
year = "2013"
}


@inproceedings{GT-MM-conc,
author = {McAllester, David A. and Schapire, Robert E.},
title = {On the Convergence Rate of Good-Turing Estimators},
year = {2000},
isbn = {155860703X},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Thirteenth Annual Conference on Computational Learning Theory},
pages = {1–6},
numpages = {6},
series = {COLT ’00}
}

@article{GT,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2333344},
 author = {I. J. Good},
 journal = {Biometrika},
 number = {3/4},
 pages = {237--264},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {The Population Frequencies of Species and the Estimation of Population Parameters},
 volume = {40},
 year = {1953}
}



@article{billingsley1961,
author = "Billingsley, Patrick",
doi = "10.1214/aoms/1177705136",
fjournal = "Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "03",
number = "1",
pages = "12--40",
publisher = "The Institute of Mathematical Statistics",
title = "Statistical Methods in Markov Chains",
url = "https://doi.org/10.1214/aoms/1177705136",
volume = "32",
year = "1961"
}


@article{ANGLUIN1979155,
title = "Fast probabilistic algorithms for hamiltonian circuits and matchings",
journal = "Journal of Computer and System Sciences",
volume = "18",
number = "2",
pages = "155 - 193",
year = "1979",
issn = "0022-0000",
doi = "https://doi.org/10.1016/0022-0000(79)90045-X",
url = "http://www.sciencedirect.com/science/article/pii/002200007990045X",
author = "D. Angluin and L.G. Valiant",
abstract = "We describe and analyse three simple efficient algorithms with good probabilistic behaviour; two algorithms with run times of O(n(log n)2) which almost certainly find directed (undirected) Hamiltonian circuits in random graphs of at least cn log n edges, and an algorithm with a run time of O(n log n) which almost certainly finds a perfect matching in a random graph of at least cn log n edges. Auxiliary propositions regarding conversion between input distributions and the “de-randomization” of randomized algorithms are proved. A new model, the random access computer (RAC), is introduced specifically to treat run times in low-level complexity."
}


@InProceedings{wen-sun-ILFO,
  title = 	 {Provably Efficient Imitation Learning from Observation Alone},
  author = 	 {Sun, Wen and Vemula, Anirudh and Boots, Byron and Bagnell, Drew},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6036--6045},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/sun19b/sun19b.pdf},
  url = 	 {http://proceedings.mlr.press/v97/sun19b.html},
  abstract = 	 {We study Imitation Learning (IL) from Observations alone (ILFO) in large-scale MDPs. While most IL algorithms rely on an expert to directly provide actions to the learner, in this setting the expert only supplies sequences of observations. We design a new model-free algorithm for ILFO, Forward Adversarial Imitation Learning (FAIL), which learns a sequence of time-dependent policies by minimizing an Integral Probability Metric between the observation distributions of the expert policy and the learner. FAIL provably learns a near-optimal policy with a number of samples that is polynomial in all relevant parameters but independent of the number of unique observations. The resulting theory extends the domain of provably sample efficient learning algorithms beyond existing results that typically only consider tabular RL settings or settings that require access to a near-optimal reset distribution. We also demonstrate the efficacy ofFAIL on multiple OpenAI Gym control tasks.}
}

@article{Bojarski2016EndTE,
  title={End to End Learning for Self-Driving Cars},
  author={Mariusz Bojarski and Davide Del Testa and Daniel Dworakowski and Bernhard Firner and Beat Flepp and Prasoon Goyal and Lawrence D. Jackel and Mathew Monfort and Urs Muller and Jiakai Zhang and Xin Zhang and Jake Zhao and Karol Zieba},
  journal={ArXiv},
  year={2016},
  volume={abs/1604.07316}
}


@article{Merel2017LearningHB,
  title={Learning human behaviors from motion capture by adversarial imitation},
  author={Josh Merel and Yuval Tassa and TB Dhruva and Sriram Srinivasan and Jay Lemmon and Ziyu Wang and Greg Wayne and Nicolas Manfred Otto Heess},
  journal={ArXiv},
  year={2017},
  volume={abs/1707.02201}
}


@article{ARGALL2009469,
title = "A survey of robot learning from demonstration",
journal = "Robotics and Autonomous Systems",
volume = "57",
number = "5",
pages = "469 - 483",
year = "2009",
issn = "0921-8890",
doi = "https://doi.org/10.1016/j.robot.2008.10.024",
url = "http://www.sciencedirect.com/science/article/pii/S0921889008001772",
author = "Brenna D. Argall and Sonia Chernova and Manuela Veloso and Brett Browning",
keywords = "Learning from demonstration, Robotics, Machine learning, Autonomous systems",
abstract = "We present a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, and contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research."
}


@inproceedings{DeepQlearning-demonstrations,
  title={Deep Q-learning From Demonstrations},
  author={Todd Hester and Matej Vecer{\'i}k and Olivier Pietquin and Marc Lanctot and Tom Schaul and Bilal Piot and Dan Horgan and John Quan and Andrew Sendonaris and Ian Osband and Gabriel Dulac-Arnold and John Agapiou and Joel Z. Leibo and Audrunas Gruslys},
  booktitle={AAAI},
  year={2018}
}


@article{IL4agile-driving,
author = {Yunpeng Pan and Ching-An Cheng and Kamil Saigol and Keuntaek Lee and Xinyan Yan and Evangelos A Theodorou and Byron Boots},
title ={Imitation learning for agile autonomous driving},
journal = {The International Journal of Robotics Research},
volume = {39},
number = {2-3},
pages = {286-302},
year = {2020},
doi = {10.1177/0278364919880273},

URL = { 
        https://doi.org/10.1177/0278364919880273
    
},
eprint = { 
        https://doi.org/10.1177/0278364919880273
    
}
,
    abstract = { We present an end-to-end imitation learning system for agile, off-road autonomous driving using only low-cost on-board sensors. By imitating a model predictive controller equipped with advanced sensors, we train a deep neural network control policy to map raw, high-dimensional observations to continuous steering and throttle commands. Compared with recent approaches to similar tasks, our method requires neither state estimation nor on-the-fly planning to navigate the vehicle. Our approach relies on, and experimentally validates, recent imitation learning theory. Empirically, we show that policies trained with online imitation learning overcome well-known challenges related to covariate shift and generalize better than policies trained with batch imitation learning. Built on these insights, our autonomous driving system demonstrates successful high-speed off-road driving, matching the state-of-the-art performance. }
}



@article{MoM,
author = {Blair, Charles},
title = {Problem Complexity and Method Efficiency in Optimization (A. S. Nemirovsky and D. B. Yudin)},
journal = {SIAM Review},
volume = {27},
number = {2},
pages = {264-265},
year = {1985},
doi = {10.1137/1027074},
URL = {https://doi.org/10.1137/1027074},
eprint = {https://doi.org/10.1137/1027074}
}

@inproceedings{Boucheron2013ConcentrationI,
  title={Concentration Inequalities - A Nonasymptotic Theory of Independence},
  author={St{\'e}phane Boucheron and G{\'a}bor Lugosi and Pascal Massart},
  booktitle={Concentration Inequalities},
  year={2013}
}


@article{Vinyals2019,
doi = {10.1038/s41586-019-1724-z},
issn = {1476-4687},
journal = {Nature},
number = {7782},
pages = {350--354},
title = {{Grandmaster level in StarCraft II using multi-agent reinforcement learning}},
url = {https://doi.org/10.1038/s41586-019-1724-z},
volume = {575},
year = {2019}
}


@inproceedings{Brantley2020Disagreement-Regularized,
title={Disagreement-Regularized Imitation Learning},
author={Kiante Brantley and Wen Sun and Mikael Henaff},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgbYyHtwB}
}

@article{Ross2014ReinforcementAI,
  title={Reinforcement and Imitation Learning via Interactive No-Regret Learning},
  author={St{\'e}phane Ross and J. Andrew Bagnell},
  journal={ArXiv},
  year={2014},
  volume={abs/1406.5979}
}

@incollection{NIPS2018_8025,
title = {Reward learning from human preferences and demonstrations in Atari},
author = {Ibarz, Borja and Leike, Jan and Pohlen, Tobias and Irving, Geoffrey and Legg, Shane and Amodei, Dario},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {8011--8023},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8025-reward-learning-from-human-preferences-and-demonstrations-in-atari.pdf}
}


@misc{reddy2019what,
title={What Would pi* Do?: Imitation Learning via Off-Policy Reinforcement Learning},
author={Siddharth Reddy and Anca D. Dragan and Sergey Levine},
year={2019},
url={https://openreview.net/forum?id=B1excoAqKQ},
}

@incollection{NIPS1988_95,
title = {ALVINN: An Autonomous Land Vehicle in a Neural Network},
author = {Pomerleau, Dean A.},
booktitle = {Advances in Neural Information Processing Systems 1},
editor = {D. S. Touretzky},
pages = {305--313},
year = {1989},
publisher = {Morgan-Kaufmann},
url = {http://papers.nips.cc/paper/95-alvinn-an-autonomous-land-vehicle-in-a-neural-network.pdf}
}


@article{JMLR:v15:judah14a,
  author  = {Kshitij Judah and Alan P. Fern and Thomas G. Dietterich and Prasad Tadepalli},
  title   = {Active Imitation Learning: Formal and Practical Reductions to I.I.D. Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {120},
  pages   = {4105-4143},
  url     = {http://jmlr.org/papers/v15/judah14a.html}
}

@article{donoho1988,
author = "Donoho, David L. and Liu, Richard C.",
doi = "10.1214/aos/1176350820",
fjournal = "Annals of Statistics",
journal = "Ann. Statist.",
month = "06",
number = "2",
pages = "552--586",
publisher = "The Institute of Mathematical Statistics",
title = "The ``Automatic'' Robustness of Minimum Distance Functionals",
url = "https://doi.org/10.1214/aos/1176350820",
volume = "16",
year = "1988"
}

@article{yatracos1985,
author = "Yatracos, Yannis G.",
doi = "10.1214/aos/1176349553",
fjournal = "Annals of Statistics",
journal = "Ann. Statist.",
month = "06",
number = "2",
pages = "768--774",
publisher = "The Institute of Mathematical Statistics",
title = "Rates of Convergence of Minimum Distance Estimators and Kolmogorov's Entropy",
url = "https://doi.org/10.1214/aos/1176349553",
volume = "13",
year = "1985"
}


@inproceedings{journals/jmlr/RossGB11,
  added-at = {2019-05-29T00:00:00.000+0200},
  author = {Ross, Stéphane and Gordon, Geoffrey J. and Bagnell, Drew},
  biburl = {https://www.bibsonomy.org/bibtex/2ac498c87961d968024282b8d340bac48/dblp},
  booktitle = {AISTATS},
  editor = {Gordon, Geoffrey J. and Dunson, David B. and Dudík, Miroslav},
  ee = {http://proceedings.mlr.press/v15/ross11a/ross11a.pdf},
  interhash = {60995151caa6bc50419bd5e98250ff96},
  intrahash = {ac498c87961d968024282b8d340bac48},
  keywords = {dblp},
  pages = {627-635},
  publisher = {JMLR.org},
  series = {JMLR Proceedings},
  timestamp = {2019-05-30T11:49:35.000+0200},
  title = {A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning.},
  url = {http://dblp.uni-trier.de/db/journals/jmlr/jmlrp15.html#RossGB11},
  volume = 15,
  year = 2011
}

@inproceedings{Luo2020Learning,
title={Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling},
author={Yuping Luo and Huazhe Xu and Tengyu Ma},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rke-f6NKvS}
}

@misc{zhu2018reinforcement,
title={Reinforcement and Imitation Learning for Diverse Visuomotor Skills},
author={Yuke Zhu and Ziyu Wang and Josh Merel and Andrei Rusu and Tom Erez and Serkan Cabi and Saran Tunyasuvunakool and János Kramár and Raia Hadsell and Nando de Freitas and Nicolas Heess},
year={2018},
url={https://openreview.net/forum?id=HJWGdbbCW},
}

@INPROCEEDINGS{McAllesterOrtiz,
    author = {David Mcallester and Luis Ortiz and Ralf Herbrich and Thore Graepel},
    title = {Concentration Inequalities for the Missing Mass and for Histogram Rule Error},
    booktitle = {Journal of Machine Learning Research},
    year = {2003},
    pages = {895--911}
}