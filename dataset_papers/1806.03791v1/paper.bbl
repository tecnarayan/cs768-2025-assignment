\begin{thebibliography}{10}

\bibitem{Tensorflow2016}
Mart{\'{\i}}n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis,
  Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael
  Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore,
  Derek~Gordon Murray, Benoit Steiner, Paul~A. Tucker, Vijay Vasudevan, Pete
  Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.
\newblock Tensorflow: {A} system for large-scale machine learning.
\newblock In {\em {OSDI} 2016}, pages 265--283, 2016.

\bibitem{ScaleParameterServer2014}
Mu~Li, David~G. Andersen, Jun~Woo Park, Alexander~J. Smola, Amr Ahmed, Vanja
  Josifovski, James Long, Eugene~J. Shekita, and Bor{-}Yiing Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In {\em {OSDI} 2014}, pages 583--598, 2014.

\bibitem{DistributedDual2010}
John~C. Duchi, Alekh Agarwal, and Martin~J. Wainwright.
\newblock Distributed dual averaging in networks.
\newblock In {\em NIPS 2010}, pages 550--558, 2010.

\bibitem{MXNet15}
Tianqi Chen, Mu~Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao,
  Bing Xu, Chiyuan Zhang, and Zheng Zhang.
\newblock Mxnet: {A} flexible and efficient machine learning library for
  heterogeneous distributed systems.
\newblock {\em CoRR}, abs/1512.01274, 2015.

\bibitem{RevisitDML2016}
Jianmin Chen, Rajat Monga, Samy Bengio, and Rafal J{\'{o}}zefowicz.
\newblock Revisiting distributed synchronous {SGD}.
\newblock {\em CoRR}, abs/1604.00981, 2016.

\bibitem{DistBelief12}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc~V. Le,
  Mark~Z. Mao, Marc'Aurelio Ranzato, Andrew~W. Senior, Paul~A. Tucker, Ke~Yang,
  and Andrew~Y. Ng.
\newblock Large scale distributed deep networks.
\newblock In {\em {NIPS} 2012}, pages 1232--1240, 2012.

\bibitem{caffe2}
Caffe2: A new lightweight, modular, and scalable deep learning framework.

\bibitem{Draco2018}
Lingjiao Chen, Hongyi Wang, Zachary~B. Charles, and Dimitris~S. Papailiopoulos.
\newblock {DRACO:} robust distributed training via redundant gradients.
\newblock {\em CoRR}, abs/1803.09877, 2018.

\bibitem{DracoSysML2018}
Lingjiao Chen, Hongyi Wang, and Dimitris~S. Papailiopoulos.
\newblock Draco: Robust distributed training against adversaries.
\newblock {\em SysML}, 2018.

\bibitem{paleo17}
Hang Qi, Evan~R. Sparks, and Ameet Talwalkar.
\newblock Paleo: A performance model for deep neural networks.
\newblock In {\em {ICLR}}, 2017.

\bibitem{CeZhang_DecentralizedTraining18}
Hanlin Tang, Xiangru Lian, Ming Yan, Ce~Zhang, and Ji~Liu.
\newblock D\({}_{\mbox{2}}\): Decentralized training over decentralized data.
\newblock {\em CoRR}, abs/1803.07068, 2018.

\bibitem{qsgd17}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock {QSGD:} communication-efficient {SGD} via gradient quantization and
  encoding.
\newblock In {\em {NIPS} 2017}, pages 1707--1718, 2017.

\bibitem{terngrad17}
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In {\em {NIPS} 2017}, pages 1508--1518, 2017.

\bibitem{deep_gradient_compression17}
Yujun Lin, Song Han, Huizi Mao, Yu~Wang, and William~J. Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock {\em CoRR}, abs/1712.01887, 2017.

\bibitem{Goyal_LargeBatchFacebook17}
Priya Goyal, Piotr Doll{\'{a}}r, Ross~B. Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch {SGD:} training imagenet in 1 hour.
\newblock {\em CoRR}, abs/1706.02677, 2017.

\bibitem{Hoffer_LargeBatchBetter_17}
Elad Hoffer, Itay Hubara, and Daniel Soudry.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock In {\em {NIPS} 2017}, pages 1729--1739, 2017.

\bibitem{You_LargeBatch32KImageNet_2017}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Scaling {SGD} batch size to 32k for imagenet training.
\newblock {\em CoRR}, abs/1708.03888, 2017.

\bibitem{Keskar_LargeBatchSharpMinima_16}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock {\em CoRR}, abs/1609.04836, 2016.

\bibitem{Kakade2017}
Prateek Jain, Sham~M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron
  Sidford.
\newblock Parallelizing stochastic gradient descent for least squares
  regression: mini-batching, averaging, and model misspecification.
\newblock {\em CoRR}, abs/1610.03774, 2018.

\bibitem{Misha2017}
Siyuan Ma, Raef Bassily, and Mikhail Belkin.
\newblock The power of interpolation: Understanding the effectiveness of {SGD}
  in modern over-parametrized learning.
\newblock {\em CoRR}, abs/1712.06559, 2017.

\bibitem{GradientDiversity2018AISTAT}
Dong Yin, Ashwin Pananjady, Maximilian Lam, Dimitris~S. Papailiopoulos, Kannan
  Ramchandran, and Peter Bartlett.
\newblock Gradient diversity: a key ingredient for scalable distributed
  learning.
\newblock In {\em AISTATS 2018}, pages 1998--2007, 2018.

\bibitem{Masters_LargeBatchSensitive_2017}
Dominic Masters and Carlo Luschi.
\newblock Revisiting small batch training for deep neural networks.
\newblock {\em CoRR}, abs/1804.07612, 2018.

\bibitem{MiniBatch_OptimalSize2012}
Ofer Dekel, Ran Gilad{-}Bachrach, Ohad Shamir, and Lin Xiao.
\newblock Optimal distributed online prediction using mini-batches.
\newblock {\em Journal of Machine Learning Research}, 13:165--202, 2012.

\bibitem{MiniBatch_LS2016}
Prateek Jain, Sham~M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron
  Sidford.
\newblock Parallelizing stochastic approximation through mini-batching and
  tail-averaging.
\newblock {\em CoRR}, abs/1610.03774, 2016.

\bibitem{DeepComplexity14}
Monica Bianchini and Franco Scarselli.
\newblock On the complexity of neural network classifiers: {A} comparison
  between shallow and deep architectures.
\newblock {\em {IEEE} Trans. Neural Netw. Learning Syst.}, 25(8):1553--1565,
  2014.

\bibitem{Barron_ANNExpressivePower_91}
Andrew~R. Barron.
\newblock Approximation and estimation bounds for artificial neural networks.
\newblock In {\em {COLT} 1991}, pages 243--249, 1991.

\bibitem{Lu_ExpressivePower_Wide17}
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang.
\newblock The expressive power of neural networks: {A} view from the width.
\newblock In {\em {NIPS} 2017}, pages 6232--6240, 2017.

\bibitem{DeepwideSumproduct11}
Olivier Delalleau and Yoshua Bengio.
\newblock Shallow vs. deep sum-product networks.
\newblock In {\em NIPS 2011}, pages 666--674, 2011.

\bibitem{DenseNet17}
Gao Huang, Zhuang Liu, Laurens van~der Maaten, and Kilian~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em CVPR 2017}, pages 2261--2269, 2017.

\bibitem{ResNetCVPR16}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR 2016}, pages 770--778, 2016.

\bibitem{MiniBatch_SVM2013}
Martin Tak{\'{a}}c, Avleen~Singh Bijral, Peter Richt{\'{a}}rik, and Nati
  Srebro.
\newblock Mini-batch primal and dual methods for svms.
\newblock In {\em ICML 2013}, pages 1022--1030, 2013.

\bibitem{MiniBatch_AdaptSize2013}
Michael~P. Friedlander and Mark~W. Schmidt.
\newblock Erratum: Hybrid deterministic-stochastic methods for data fitting.
\newblock {\em {SIAM} J. Scientific Computing}, 35(4), 2013.

\bibitem{MiniBatch_AutoSize2016}
Soham De, Abhay~Kumar Yadav, David~W. Jacobs, and Tom Goldstein.
\newblock Big batch {SGD:} automated inference using adaptive batch sizes.
\newblock {\em CoRR}, abs/1610.05792, 2016.

\bibitem{MiniBatch_Accelerate2011}
Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan.
\newblock Better mini-batch algorithms via accelerated gradient methods.
\newblock In {\em NIPS 2011}, pages 1647--1655, 2011.

\bibitem{MiniBatch_SDCA2013}
Shai Shalev{-}Shwartz and Tong Zhang.
\newblock Accelerated mini-batch stochastic dual coordinate ascent.
\newblock In {\em NIPS 2013}, pages 378--385, 2013.

\bibitem{MiniBatch_SDCA2_2015}
Martin Tak{\'{a}}c, Peter Richt{\'{a}}rik, and Nathan Srebro.
\newblock Distributed mini-batch {SDCA}.
\newblock {\em CoRR}, abs/1507.08322, 2015.

\bibitem{MiniBatch_Prox_2017}
Jialei Wang, Weiran Wang, and Nathan Srebro.
\newblock Memory and communication efficient distributed stochastic
  optimization with minibatch prox.
\newblock In {\em COLT 2017}, pages 1882--1919, 2017.

\bibitem{SGDSampling_2014}
Peilin Zhao and Tong Zhang.
\newblock Accelerating minibatch stochastic gradient descent using stratified
  sampling.
\newblock {\em CoRR}, abs/1405.3080, 2014.

\bibitem{Minibatch_DPP2017}
Cheng Zhang, Hedvig Kjellstr{\"{o}}m, and Stephan Mandt.
\newblock Stochastic learning on imbalanced data: Determinantal point processes
  for mini-batch diversification.
\newblock {\em CoRR}, abs/1705.00607, 2017.

\bibitem{MiniBatch_Sampling2018}
Cheng Zhang, Cengiz {\"{O}}ztireli, Stephan Mandt, and Giampiero Salvi.
\newblock Active mini-batch sampling using repulsive point processes.
\newblock {\em CoRR}, abs/1804.02772, 2018.

\bibitem{WideResNet16}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In {\em BMVC 2016}, 2016.

\bibitem{ShallowDeep_SaliencyPrediction16}
Junting Pan, Elisa Sayrol, Xavier {Gir{\'{o}} i Nieto}, Kevin McGuinness, and
  Noel~E. O'Connor.
\newblock Shallow and deep convolutional networks for saliency prediction.
\newblock In {\em {CVPR} 2016}, pages 598--606, 2016.

\bibitem{ResNet_W_vs_D2016}
Zifeng Wu, Chunhua Shen, and Anton van~den Hengel.
\newblock Wider or deeper: Revisiting the resnet model for visual recognition.
\newblock {\em CoRR}, abs/1611.10080, 2016.

\bibitem{Keras2015}
Fran√ßois Chollet.
\newblock keras.
\newblock \url{https://github.com/fchollet/keras}, 2015.

\bibitem{lecun2010mnist}
Yann LeCun, Corinna Cortes, and CJ~Burges.
\newblock Mnist handwritten digit database.
\newblock {\em AT\&T Labs [Online]. Available: http://yann. lecun.
  com/exdb/mnist}, 2, 2010.

\bibitem{cohen2017emnist}
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr{\'e} van Schaik.
\newblock Emnist: an extension of mnist to handwritten letters.
\newblock {\em arXiv preprint arXiv:1702.05373}, 2017.

\bibitem{chang2011libsvm}
Chih-Chung Chang and Chih-Jen Lin.
\newblock Libsvm: a library for support vector machines.
\newblock {\em ACM transactions on intelligent systems and technology (TIST)},
  2(3):27, 2011.

\bibitem{CIRFA10}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{IsserlisThoerem1918}
L.~Isserlis.
\newblock On a formula for the product-moment coefficient of any order of a
  normal frequency distribution in any number of variables.
\newblock {\em Biometrika}, 12(1/2):134--139, 1918.

\end{thebibliography}
