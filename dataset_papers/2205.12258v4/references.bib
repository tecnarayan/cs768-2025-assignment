
@article{radford_language_2019,
	title = {Language models are unsupervised multitask learners},
	volume = {1},
	number = {8},
	journal = {OpenAI blog},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year = {2019},
	pages = {9},
	file = {Full Text:/home/user/Zotero/storage/U2P59DAA/Radford et al. - 2019 - Language models are unsupervised multitask learner.pdf:application/pdf},
}

@inproceedings{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	year = {2021},
}

@inproceedings{beck_amrl_2020,
	title = {{AMRL}: {Aggregated} {Memory} {For} {Reinforcement} {Learning}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Beck, Jacob and Ciosek, Kamil and Devlin, Sam and Tschiatschek, Sebastian and Zhang, Cheng and Hofmann, Katja},
	year = {2020},
}

@inproceedings{mishra_simple_2018,
	title = {A {Simple} {Neural} {Attentive} {Meta}-{Learner}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
	year = {2018},
}

@inproceedings{hill_grounded_2021,
	title = {Grounded {Language} {Learning} {Fast} and {Slow}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Hill, Felix and Tieleman, Olivier and Glehn, Tamara von and Wong, Nathaniel and Merzic, Hamza and Clark, Stephen},
	year = {2021},
}

@inproceedings{ramsauer_hopfield_2021,
	title = {Hopfield {Networks} is {All} {You} {Need}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Gruber, Lukas and Holzleitner, Markus and Adler, Thomas and Kreil, David and Kopp, Michael K. and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
	year = {2021},
}

@article{jumper_highly_2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2021-09-08},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 7873
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Computational biophysics;Machine learning;Protein structure predictions;Structural biology
Subject\_term\_id: computational-biophysics;machine-learning;protein-structure-predictions;structural-biology},
	pages = {583--589},
	file = {Full Text PDF:/home/user/Zotero/storage/B7MW886Z/Jumper et al. - 2021 - Highly accurate protein structure prediction with .pdf:application/pdf;Snapshot:/home/user/Zotero/storage/7K7ZFVIU/s41586-021-03819-2.html:text/html},
}

@inproceedings{dong_speech-transformer_2018,
	title = {Speech-{Transformer}: {A} {No}-{Recurrence} {Sequence}-to-{Sequence} {Model} for {Speech} {Recognition}},
	shorttitle = {Speech-{Transformer}},
	doi = {10.1109/ICASSP.2018.8462506},
	abstract = {Recurrent sequence-to-sequence models using encoder-decoder architecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanisms to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanism, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive representations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9\%, while the whole training process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Dong, Linhao and Xu, Shuang and Xu, Bo},
	month = apr,
	year = {2018},
	note = {ISSN: 2379-190X},
	keywords = {Attention, Decoding, Encoding, Hidden Markov models, Sequence-to-Sequence, Spectrogram, Speech recognition, Speech Recognition, Time-frequency analysis, Training, Transformer},
	pages = {5884--5888},
	file = {IEEE Xplore Full Text PDF:/home/user/Zotero/storage/ZU3ABSGV/Dong et al. - 2018 - Speech-Transformer A No-Recurrence Sequence-to-Se.pdf:application/pdf},
}

@inproceedings{yang_xlnet_2019,
	title = {{XLNet}: {Generalized} {Autoregressive} {Pretraining} for {Language} {Understanding}},
	volume = {32},
	shorttitle = {{XLNet}},
	urldate = {2021-09-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
	year = {2019},
	file = {Full Text PDF:/home/user/Zotero/storage/V45LVQRS/Yang et al. - 2019 - XLNet Generalized Autoregressive Pretraining for .pdf:application/pdf},
}

@inproceedings{fang_scene_2019,
	title = {Scene {Memory} {Transformer} for {Embodied} {Agents} in {Long}-{Horizon} {Tasks}},
	urldate = {2021-09-08},
	author = {Fang, Kuan and Toshev, Alexander and Fei-Fei, Li and Savarese, Silvio},
	year = {2019},
	pages = {538--547},
	file = {Full Text PDF:/home/user/Zotero/storage/SWLZZSKM/Fang et al. - 2019 - Scene Memory Transformer for Embodied Agents in Lo.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/QEBX8NX5/Fang_Scene_Memory_Transformer_for_Embodied_Agents_in_Long-Horizon_Tasks_CVPR_2019_paper.html:text/html},
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	issn = {1573-0565},
	doi = {10.1007/BF00992698},
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
	language = {en},
	number = {3},
	urldate = {2021-09-08},
	journal = {Machine Learning},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	month = may,
	year = {1992},
	pages = {279--292},
	file = {Springer Full Text PDF:/home/user/Zotero/storage/2J45L9DZ/Watkins and Dayan - 1992 - Q-learning.pdf:application/pdf},
}

@inproceedings{cho_learning_2014,
	address = {Doha, Qatar},
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}–{Decoder} for {Statistical} {Machine} {Translation}},
	doi = {10.3115/v1/D14-1179},
	urldate = {2021-09-08},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Cho, Kyunghyun and van Merriënboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = oct,
	year = {2014},
	pages = {1724--1734},
	file = {Full Text PDF:/home/user/Zotero/storage/DW2QGE9P/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder–.pdf:application/pdf},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2021-09-08},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
	file = {Full Text PDF:/home/user/Zotero/storage/EGFK59B7/Hochreiter and Schmidhuber - 1997 - Long Short-Term Memory.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/RNBIP64K/Long-Short-Term-Memory.html:text/html},
}

@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	urldate = {2021-09-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	pages = {1877--1901},
	file = {Full Text PDF:/home/user/Zotero/storage/UXZB53Q4/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2021-09-08},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
	file = {Full Text PDF:/home/user/Zotero/storage/PSYWN6WC/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	urldate = {2021-09-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:/home/user/Zotero/storage/IYB5HWP4/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@inproceedings{dai_transformer-xl_2019,
	address = {Florence, Italy},
	title = {Transformer-{XL}: {Attentive} {Language} {Models} beyond a {Fixed}-{Length} {Context}},
	shorttitle = {Transformer-{XL}},
	doi = {10.18653/v1/P19-1285},
	abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
	urldate = {2021-09-08},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc and Salakhutdinov, Ruslan},
	month = jul,
	year = {2019},
	pages = {2978--2988},
	file = {Full Text PDF:/home/user/Zotero/storage/EP6DAHR4/Dai et al. - 2019 - Transformer-XL Attentive Language Models beyond a.pdf:application/pdf},
}

@article{hill_human_2020,
	title = {Human {Instruction}-{Following} with {Deep} {Reinforcement} {Learning} via {Transfer}-{Learning} from {Text}},
	volume = {abs/2005.09382},
	urldate = {2021-09-08},
	journal = {CoRR},
	author = {Hill, Felix and Mokra, Sona and Wong, Nathaniel and Harley, Tim},
	year = {2020},
	note = {arXiv: 2005.09382},
}

@inproceedings{jiang_language_2019,
	title = {Language as an {Abstraction} for {Hierarchical} {Deep} {Reinforcement} {Learning}},
	volume = {32},
	urldate = {2021-09-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Jiang, YiDing and Gu, Shixiang (Shane) and Murphy, Kevin P and Finn, Chelsea},
	year = {2019},
	file = {Full Text PDF:/home/user/Zotero/storage/EXTPN4YZ/Jiang et al. - 2019 - Language as an Abstraction for Hierarchical Deep R.pdf:application/pdf},
}

@article{jaeger_echo_2001,
	title = {The “echo state” approach to analysing and training recurrent neural networks-with an erratum note},
	volume = {148},
	number = {34},
	journal = {Bonn, Germany: German National Research Center for Information Technology GMD Technical Report},
	author = {Jaeger, Herbert},
	year = {2001},
	note = {Publisher: Bonn},
	pages = {13},
	file = {Full Text:/home/user/Zotero/storage/HMBJ7FSI/Jaeger - 2001 - The “echo state” approach to analysing and trainin.pdf:application/pdf},
}

@article{johnson_extensions_1984,
	title = {Extensions of {Lipschitz} mappings into a {Hilbert} space},
	volume = {26},
	journal = {Contemporary mathematics},
	author = {Johnson, William B. and Lindenstrauss, Joram},
	year = {1984},
}

@inproceedings{sukhbaatar_not_2021,
	title = {Not {All} {Memories} are {Created} {Equal}: {Learning} to {Forget} by {Expiring}},
	shorttitle = {Not {All} {Memories} are {Created} {Equal}},
	language = {en},
	urldate = {2021-09-07},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sukhbaatar, Sainbayar and Ju, Da and Poff, Spencer and Roller, Stephen and Szlam, Arthur and Weston, Jason and Fan, Angela},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {9902--9912},
	file = {Full Text PDF:/home/user/Zotero/storage/ERSBL5LM/Sukhbaatar et al. - 2021 - Not All Memories are Created Equal Learning to Fo.pdf:application/pdf;Supplementary PDF:/home/user/Zotero/storage/JE3LWYQ7/Sukhbaatar et al. - 2021 - Not All Memories are Created Equal Learning to Fo.pdf:application/pdf},
}

@inproceedings{parisotto_stabilizing_2020,
	title = {Stabilizing {Transformers} for {Reinforcement} {Learning}},
	language = {en},
	urldate = {2021-09-07},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Parisotto, Emilio and Song, Francis and Rae, Jack and Pascanu, Razvan and Gulcehre, Caglar and Jayakumar, Siddhant and Jaderberg, Max and Kaufman, Raphaël Lopez and Clark, Aidan and Noury, Seb and Botvinick, Matthew and Heess, Nicolas and Hadsell, Raia},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {7487--7498},
	file = {Full Text PDF:/home/user/Zotero/storage/3DN9PXG7/Parisotto et al. - 2020 - Stabilizing Transformers for Reinforcement Learnin.pdf:application/pdf;Supplementary PDF:/home/user/Zotero/storage/A62QHGQR/Parisotto et al. - 2020 - Stabilizing Transformers for Reinforcement Learnin.pdf:application/pdf},
}

@inproceedings{tieck_learning_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning {Continuous} {Muscle} {Control} for a {Multi}-joint {Arm} by {Extending} {Proximal} {Policy} {Optimization} with a {Liquid} {State} {Machine}},
	doi = {10.1007/978-3-030-01418-6_21},
	abstract = {There have been many advances in the field of reinforcement learning in continuous control problems. Usually, these approaches use deep learning with artificial neural networks for approximation of policies and value functions. In addition, there have been interesting advances in spiking neural networks, towards a more biologically plausible model of the neurons and the learning mechanisms. We present an approach to learn continuous muscle control of a multi joint arm. We use reinforcement learning for a target reaching task, which can be modeled as partially observable markov decision processes. We extend proximal policy optimization with a liquid state machine (LSM) for state representation to achieve better performance in the target reaching task. The results show that we are able to learn to control the arm after training the readout of the LSM with reinforcement learning. The input current encoding used for encoding the state is enough to have a good projection into a higher dimensional space of the LSM. The results also show that we are able to learn a linear readout, which is equivalent to a one-layer neural network to learn to control the arm. We show that there are clear benefits of training the readouts of a LSM with reinforcement learning. These results can lead to demonstrate the benefits of using a LSM as a drop-in state transformation in general.},
	language = {en},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2018},
	publisher = {Springer International Publishing},
	author = {Tieck, Juan Camilo Vasquez and Pogančić, Marin Vlastelica and Kaiser, Jacques and Roennau, Arne and Gewaltig, Marc-Oliver and Dillmann, Rüdiger},
	editor = {Kůrková, Věra and Manolopoulos, Yannis and Hammer, Barbara and Iliadis, Lazaros and Maglogiannis, Ilias},
	year = {2018},
	keywords = {Muscle control, Neurorobotics, Reinforcement learning, Reservoir computing, Spiking networks},
	pages = {211--221},
	file = {Springer Full Text PDF:/home/user/Zotero/storage/FRQINHPQ/Tieck et al. - 2018 - Learning Continuous Muscle Control for a Multi-joi.pdf:application/pdf},
}

@inproceedings{bellec_long_2018,
	title = {Long short-term memory and {Learning}-to-learn in networks of spiking neurons},
	volume = {31},
	urldate = {2021-09-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bellec, Guillaume and Salaj, Darjan and Subramoney, Anand and Legenstein, Robert and Maass, Wolfgang},
	year = {2018},
	file = {Full Text PDF:/home/user/Zotero/storage/RPGCQAI3/Bellec et al. - 2018 - Long short-term memory and Learning-to-learn in ne.pdf:application/pdf},
}

@inproceedings{xie_composed_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Composed {Fine}-{Tuning}: {Freezing} {Pre}-{Trained} {Denoising} {Autoencoders} for {Improved} {Generalization}},
	volume = {139},
	shorttitle = {Composed {Fine}-{Tuning}},
	urldate = {2021-09-07},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}, {ICML} 2021, 18-24 {July} 2021, {Virtual} {Event}},
	publisher = {PMLR},
	author = {Xie, Sang Michael and Ma, Tengyu and Liang, Percy},
	editor = {Meila, Marina and Zhang, Tong},
	year = {2021},
	pages = {11424--11435},
}

@article{chang_reinforcement_2020,
	title = {Reinforcement learning with convolutional reservoir computing},
	volume = {50},
	issn = {1573-7497},
	doi = {10.1007/s10489-020-01679-3},
	abstract = {Recently, reinforcement learning models have achieved great success, mastering complex tasks such as Go and other games with higher scores than human players. Many of these models store considerable data on the tasks and achieve high performance by extracting visual and time-series features using convolutional neural networks (CNNs) and recurrent neural networks respectively. However, these networks have very high computational costs because they need to be trained by repeatedly using the stored data. In this study, we propose a novel practical approach called reinforcement learning with a convolutional reservoir computing (RCRC) model. The RCRC model uses a fixed random-weight CNN and a reservoir computing model to extract visual and time-series features. Using these extracted features, it decides actions with an evolution strategy method. Thereby, the RCRC model has several desirable features: (1) there is no need to train the feature extractor, (2) there is no need to store training data, (3) it can take a wide range of actions, and (4) there is only a single task-dependent weight matrix to be trained. Furthermore, we show the RCRC model can solve multiple reinforcement learning tasks with a completely identical feature extractor.},
	language = {en},
	number = {8},
	urldate = {2021-09-07},
	journal = {Applied Intelligence},
	author = {Chang, Hanten and Futagami, Katsuya},
	month = aug,
	year = {2020},
	pages = {2400--2410},
	file = {Springer Full Text PDF:/home/user/Zotero/storage/RFQDKDRW/Chang and Futagami - 2020 - Reinforcement learning with convolutional reservoi.pdf:application/pdf},
}

@article{maass_real-time_2002,
	title = {Real-time computing without stable states: a new framework for neural computation based on perturbations},
	volume = {14},
	issn = {0899-7667},
	shorttitle = {Real-time computing without stable states},
	doi = {10.1162/089976602760407955},
	abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
	language = {eng},
	number = {11},
	journal = {Neural Computation},
	author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
	month = nov,
	year = {2002},
	pmid = {12433288},
	keywords = {Action Potentials, Computer Simulation, Computer Systems, Computers, Models, Neurological, Neural Networks, Computer, Neurons},
	pages = {2531--2560},
}

@inproceedings{maass_model_2002,
	title = {A {Model} for {Real}-{Time} {Computation} in {Generic} {Neural} {Microcircuits}},
	urldate = {2021-09-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 15 [{Neural} {Information} {Processing} {Systems}, {NIPS} 2002, {December} 9-14, 2002, {Vancouver}, {British} {Columbia}, {Canada}]},
	publisher = {MIT Press},
	author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
	editor = {Becker, Suzanna and Thrun, Sebastian and Obermayer, Klaus},
	year = {2002},
	pages = {213--220},
}

@inproceedings{espeholt_impala_2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{IMPALA}: {Scalable} {Distributed} {Deep}-{RL} with {Importance} {Weighted} {Actor}-{Learner} {Architectures}},
	volume = {80},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}, {ICML} 2018, {Stockholmsmässan}, {Stockholm}, {Sweden}, {July} 10-15, 2018},
	publisher = {PMLR},
	author = {Espeholt, Lasse and Soyer, Hubert and Munos, Rémi and Simonyan, Karen and Mnih, Volodymyr and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	year = {2018},
	pages = {1406--1415},
}

@inproceedings{bellemare_arcade_2015,
	title = {The {Arcade} {Learning} {Environment}: {An} {Evaluation} {Platform} for {General} {Agents} ({Extended} {Abstract})},
	booktitle = {Proceedings of the {Twenty}-{Fourth} {International} {Joint} {Conference} on {Artificial} {Intelligence}, {IJCAI} 2015, {Buenos} {Aires}, {Argentina}, {July} 25-31, 2015},
	publisher = {AAAI Press},
	author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
	editor = {Yang, Qiang and Wooldridge, Michael J.},
	year = {2015},
	pages = {4148--4152},
}

@article{christodoulou_soft_2019,
	title = {Soft {Actor}-{Critic} for {Discrete} {Action} {Settings}},
	volume = {abs/1910.07207},
	journal = {CoRR},
	author = {Christodoulou, Petros},
	year = {2019},
	note = {arXiv: 1910.07207},
}

@inproceedings{haarnoja_soft_2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
	volume = {80},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}, {ICML} 2018, {Stockholmsmässan}, {Stockholm}, {Sweden}, {July} 10-15, 2018},
	publisher = {PMLR},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	year = {2018},
	pages = {1856--1865},
}

@inproceedings{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	booktitle = {7th {International} {Conference} on {Learning} {Representations}, {ICLR} 2019, {New} {Orleans}, {LA}, {USA}, {May} 6-9, 2019},
	publisher = {OpenReview.net},
	author = {Loshchilov, Ilya and Hutter, Frank},
	year = {2019},
}

@inproceedings{raileanu_ride_2020,
	title = {{RIDE}: {Rewarding} {Impact}-{Driven} {Exploration} for {Procedurally}-{Generated} {Environments}},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Raileanu, Roberta and Rocktäschel, Tim},
	year = {2020},
}

@article{cover_geometrical_1965,
	title = {Geometrical and {Statistical} {Properties} of {Systems} of {Linear} {Inequalities} with {Applications} in {Pattern} {Recognition}},
	volume = {14},
	doi = {10.1109/PGEC.1965.264137},
	number = {3},
	journal = {IEEE Trans. Electron. Comput.},
	author = {Cover, Thomas M.},
	year = {1965},
	pages = {326--334},
}

@inproceedings{li_prefix-tuning_2021,
	title = {Prefix-{Tuning}: {Optimizing} {Continuous} {Prompts} for {Generation}},
	doi = {10.18653/v1/2021.acl-long.353},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing}, {ACL}/{IJCNLP} 2021, ({Volume} 1: {Long} {Papers}), {Virtual} {Event}, {August} 1-6, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Li, Xiang Lisa and Liang, Percy},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	year = {2021},
	pages = {4582--4597},
}

@article{lester_power_2021,
	title = {The {Power} of {Scale} for {Parameter}-{Efficient} {Prompt} {Tuning}},
	volume = {abs/2104.08691},
	journal = {CoRR},
	author = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
	year = {2021},
	note = {arXiv: 2104.08691},
}

@article{tsimpoukelli_multimodal_2021,
	title = {Multimodal {Few}-{Shot} {Learning} with {Frozen} {Language} {Models}},
	volume = {abs/2106.13884},
	journal = {CoRR},
	author = {Tsimpoukelli, Maria and Menick, Jacob and Cabi, Serkan and Eslami, S. M. Ali and Vinyals, Oriol and Hill, Felix},
	year = {2021},
	note = {arXiv: 2106.13884},
	annote = {NIPS2021},
}

@inproceedings{wolf_transformers_2020,
	title = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
	doi = {10.18653/v1/2020.emnlp-demos.6},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}, {EMNLP} 2020 - {Demos}, {Online}, {November} 16-20, 2020},
	publisher = {Association for Computational Linguistics},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and Platen, Patrick von and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
	editor = {Liu, Qun and Schlangen, David},
	year = {2020},
	pages = {38--45},
}

@inproceedings{hafner_dream_2020,
	title = {Dream to {Control}: {Learning} {Behaviors} by {Latent} {Imagination}},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Hafner, Danijar and Lillicrap, Timothy P. and Ba, Jimmy and Norouzi, Mohammad},
	year = {2020},
}

@inproceedings{merity_pointer_2017,
	title = {Pointer {Sentinel} {Mixture} {Models}},
	booktitle = {5th {International} {Conference} on {Learning} {Representations}, {ICLR} 2017, {Toulon}, {France}, {April} 24-26, 2017, {Conference} {Track} {Proceedings}},
	publisher = {OpenReview.net},
	author = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
	year = {2017},
}

@inproceedings{howard_universal_2018,
	title = {Universal {Language} {Model} {Fine}-tuning for {Text} {Classification}},
	doi = {10.18653/v1/P18-1031},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}, {ACL} 2018, {Melbourne}, {Australia}, {July} 15-20, 2018, {Volume} 1: {Long} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Howard, Jeremy and Ruder, Sebastian},
	editor = {Gurevych, Iryna and Miyao, Yusuke},
	year = {2018},
	pages = {328--339},
}

@inproceedings{merity_regularizing_2018,
	title = {Regularizing and {Optimizing} {LSTM} {Language} {Models}},
	booktitle = {6th {International} {Conference} on {Learning} {Representations}, {ICLR} 2018, {Vancouver}, {BC}, {Canada}, {April} 30 - {May} 3, 2018, {Conference} {Track} {Proceedings}},
	publisher = {OpenReview.net},
	author = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
	year = {2018},
}

@article{chen_decision_2021,
	title = {Decision {Transformer}: {Reinforcement} {Learning} via {Sequence} {Modeling}},
	volume = {abs/2106.01345},
	journal = {CoRR},
	author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
	year = {2021},
	note = {arXiv: 2106.01345},
}

@inproceedings{loynd_working_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Working {Memory} {Graphs}},
	volume = {119},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}, {ICML} 2020, 13-18 {July} 2020, {Virtual} {Event}},
	publisher = {PMLR},
	author = {Loynd, Ricky and Fernandez, Roland and Celikyilmaz, Asli and Swaminathan, Adith and Hausknecht, Matthew J.},
	year = {2020},
	pages = {6404--6414},
}

@article{janner_reinforcement_2021,
	title = {Reinforcement {Learning} as {One} {Big} {Sequence} {Modeling} {Problem}},
	volume = {abs/2106.02039},
	journal = {CoRR},
	author = {Janner, Michael and Li, Qiyang and Levine, Sergey},
	year = {2021},
	note = {arXiv: 2106.02039},
}

@article{hu_updet_2021,
	title = {{UPDeT}: {Universal} {Multi}-agent {Reinforcement} {Learning} via {Policy} {Decoupling} with {Transformers}},
	volume = {abs/2101.08001},
	journal = {CoRR},
	author = {Hu, Siyi and Zhu, Fengda and Chang, Xiaojun and Liang, Xiaodan},
	year = {2021},
	note = {arXiv: 2101.08001},
}

@article{wayne_unsupervised_2018,
	title = {Unsupervised {Predictive} {Memory} in a {Goal}-{Directed} {Agent}},
	volume = {abs/1803.10760},
	journal = {CoRR},
	author = {Wayne, Greg and Hung, Chia-Chun and Amos, David and Mirza, Mehdi and Ahuja, Arun and Grabska-Barwinska, Agnieszka and Rae, Jack W. and Mirowski, Piotr and Leibo, Joel Z. and Santoro, Adam and Gemici, Mevlana and Reynolds, Malcolm and Harley, Tim and Abramson, Josh and Mohamed, Shakir and Rezende, Danilo Jimenez and Saxton, David and Cain, Adam and Hillier, Chloe and Silver, David and Kavukcuoglu, Koray and Botvinick, Matthew and Hassabis, Demis and Lillicrap, Timothy P.},
	year = {2018},
	note = {arXiv: 1803.10760},
}

@inproceedings{weiss_thinking_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Thinking {Like} {Transformers}},
	volume = {139},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}, {ICML} 2021, 18-24 {July} 2021, {Virtual} {Event}},
	publisher = {PMLR},
	author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
	editor = {Meila, Marina and Zhang, Tong},
	year = {2021},
	pages = {11080--11090},
}

@article{love_task_2021,
	title = {Task {Agnostic} {Metrics} for {Reservoir} {Computing}},
	volume = {abs/2108.01512},
	journal = {CoRR},
	author = {Love, Jake and Mulkers, Jeroen and Bourianoff, George and Leliaert, Jonathan and Everschor-Sitte, Karin},
	year = {2021},
	note = {arXiv: 2108.01512},
}

@article{lukosevicius_reservoir_2009,
	title = {Reservoir computing approaches to recurrent neural network training},
	volume = {3},
	doi = {10.1016/j.cosrev.2009.03.005},
	number = {3},
	journal = {Comput. Sci. Rev.},
	author = {Lukosevicius, Mantas and Jaeger, Herbert},
	year = {2009},
	pages = {127--149},
}

@article{ponghiran_reinforcement_2019,
	title = {Reinforcement {Learning} with {Low}-{Complexity} {Liquid} {State} {Machines}},
	volume = {abs/1906.01695},
	journal = {CoRR},
	author = {Ponghiran, Wachirawit and Srinivasan, Gopalakrishnan and Roy, Kaushik},
	year = {2019},
	note = {arXiv: 1906.01695},
}

@article{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	volume = {abs/1707.06347},
	journal = {CoRR},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	year = {2017},
	note = {arXiv: 1707.06347},
}

@article{lu_pretrained_2021,
	title = {Pretrained {Transformers} as {Universal} {Computation} {Engines}},
	volume = {abs/2103.05247},
	journal = {CoRR},
	author = {Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
	year = {2021},
	note = {arXiv: 2103.05247},
}

@article{wang_linformer_2020,
	title = {Linformer: {Self}-{Attention} with {Linear} {Complexity}},
	volume = {abs/2006.04768},
	journal = {CoRR},
	author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
	year = {2020},
	note = {arXiv: 2006.04768},
}

@inproceedings{radford_improving_2018,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	author = {Radford, Alec and Narasimhan, Karthik},
	year = {2018},
}

@article{abramson_imitating_2020,
	title = {Imitating {Interactive} {Intelligence}},
	volume = {abs/2012.05672},
	journal = {CoRR},
	author = {Abramson, Josh and Ahuja, Arun and Brussee, Arthur and Carnevale, Federico and Cassin, Mary and Clark, Stephen and Dudzik, Andrew and Georgiev, Petko and Guy, Aurelia and Harley, Tim and Hill, Felix and Hung, Alden and Kenton, Zachary and Landon, Jessica and Lillicrap, Timothy P. and Mathewson, Kory and Muldal, Alistair and Santoro, Adam and Savinov, Nikolay and Varma, Vikrant and Wayne, Greg and Wong, Nathaniel and Yan, Chen and Zhu, Rui},
	year = {2020},
	note = {arXiv: 2012.05672},
}

@article{child_generating_2019,
	title = {Generating {Long} {Sequences} with {Sparse} {Transformers}},
	volume = {abs/1904.10509},
	journal = {CoRR},
	author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
	year = {2019},
	note = {arXiv: 1904.10509},
}

@article{ziegler_fine-tuning_2019,
	title = {Fine-{Tuning} {Language} {Models} from {Human} {Preferences}},
	volume = {abs/1909.08593},
	journal = {CoRR},
	author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul F. and Irving, Geoffrey},
	year = {2019},
	note = {arXiv: 1909.08593},
}

@article{adler_cross-domain_2020,
	title = {Cross-{Domain} {Few}-{Shot} {Learning} by {Representation} {Fusion}},
	volume = {abs/2010.06498},
	journal = {CoRR},
	author = {Adler, Thomas and Brandstetter, Johannes and Widrich, Michael and Mayr, Andreas and Kreil, David P. and Kopp, Michael and Klambauer, Günter and Hochreiter, Sepp},
	year = {2020},
	note = {arXiv: 2010.06498},
}

@misc{chevalier-boisvert_minimalistic_2018,
	title = {Minimalistic {Gridworld} {Environment} for {OpenAI} {Gym}},
	publisher = {GitHub},
	author = {Chevalier-Boisvert, Maxime and Willems, Lucas and Pal, Suman},
	year = {2018},
	note = {Publication Title: GitHub repository},
}

@inproceedings{cobbe_leveraging_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Leveraging {Procedural} {Generation} to {Benchmark} {Reinforcement} {Learning}},
	volume = {119},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}, {ICML} 2020, 13-18 {July} 2020, {Virtual} {Event}},
	publisher = {PMLR},
	author = {Cobbe, Karl and Hesse, Christopher and Hilton, Jacob and Schulman, John},
	year = {2020},
	pages = {2048--2056},
}

@inproceedings{houlsby_parameter-efficient_2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Parameter-{Efficient} {Transfer} {Learning} for {NLP}},
	volume = {97},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}, {ICML} 2019, 9-15 {June} 2019, {Long} {Beach}, {California}, {USA}},
	publisher = {PMLR},
	author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and Laroussilhe, Quentin de and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	year = {2019},
	pages = {2790--2799},
}

@article{rothermel_dont_2021,
	title = {Don't {Sweep} your {Learning} {Rate} under the {Rug}: {A} {Closer} {Look} at {Cross}-modal {Transfer} of {Pretrained} {Transformers}},
	volume = {abs/2107.12460},
	journal = {CoRR},
	author = {Rothermel, Danielle and Li, Margaret and Rocktäschel, Tim and Foerster, Jakob N.},
	year = {2021},
	note = {arXiv: 2107.12460},
}

@inproceedings{jiang_prioritized_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Prioritized {Level} {Replay}},
	volume = {139},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}, {ICML} 2021, 18-24 {July} 2021, {Virtual} {Event}},
	publisher = {PMLR},
	author = {Jiang, Minqi and Grefenstette, Edward and Rocktäschel, Tim},
	editor = {Meila, Marina and Zhang, Tong},
	year = {2021},
	pages = {4940--4950},
}

@article{ba_layer_2016,
	title = {Layer {Normalization}},
	volume = {abs/1607.06450},
	journal = {CoRR},
	author = {Ba, Lei Jimmy and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	year = {2016},
	note = {arXiv: 1607.06450},
}

@inproceedings{schulman_high-dimensional_2016,
	title = {High-{Dimensional} {Continuous} {Control} {Using} {Generalized} {Advantage} {Estimation}},
	booktitle = {4th {International} {Conference} on {Learning} {Representations}, {ICLR} 2016, {San} {Juan}, {Puerto} {Rico}, {May} 2-4, 2016, {Conference} {Track} {Proceedings}},
	author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael I. and Abbeel, Pieter},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2016},
}

@inproceedings{schaul_prioritized_2016,
	title = {Prioritized {Experience} {Replay}},
	booktitle = {4th {International} {Conference} on {Learning} {Representations}, {ICLR} 2016, {San} {Juan}, {Puerto} {Rico}, {May} 2-4, 2016, {Conference} {Track} {Proceedings}},
	author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2016},
}

@inproceedings{colas_hitchhikers_2019,
	title = {A {Hitchhiker}'s {Guide} to {Statistical} {Comparisons} of {Reinforcement} {Learning} {Algorithms}},
	booktitle = {Reproducibility in {Machine} {Learning}, {ICLR} 2019 {Workshop}, {New} {Orleans}, {Louisiana}, {United} {States}, {May} 6, 2019},
	publisher = {OpenReview.net},
	author = {Colas, Cédric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
	year = {2019},
}

@article{welch_generalization_1947,
	title = {The {Generalization} of `{Student}'s' {Problem} when {Several} {Different} {Population} {Variances} are {Involved}},
	volume = {34},
	issn = {00063444},
	doi = {10.2307/2332510},
	number = {1/2},
	journal = {Biometrika},
	author = {Welch, B L},
	year = {1947},
	keywords = {1st-yr-report},
	pages = {28--35},
}

@article{wilcoxon_individual_1945,
	title = {Individual {Comparisons} by {Ranking} {Methods}},
	volume = {1},
	issn = {00994987},
	number = {6},
	journal = {Biometrics Bulletin},
	author = {Wilcoxon, Frank},
	year = {1945},
	note = {Publisher: [International Biometric Society, Wiley]},
	pages = {80--83},
}

@inproceedings{shen_reservoir_2021,
	title = {Reservoir {Transformers}},
	doi = {10.18653/v1/2021.acl-long.331},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing}, {ACL}/{IJCNLP} 2021, ({Volume} 1: {Long} {Papers}), {Virtual} {Event}, {August} 1-6, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Shen, Sheng and Baevski, Alexei and Morcos, Ari S. and Keutzer, Kurt and Auli, Michael and Kiela, Douwe},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	year = {2021},
	pages = {4294--4309},
}

@article{goyal_inductive_2020,
	title = {Inductive {Biases} for {Deep} {Learning} of {Higher}-{Level} {Cognition}},
	volume = {abs/2011.15091},
	journal = {CoRR},
	author = {Goyal, Anirudh and Bengio, Yoshua},
	year = {2020},
	note = {arXiv: 2011.15091},
}

@inproceedings{andreas_learning_2018,
	title = {Learning with {Latent} {Language}},
	doi = {10.18653/v1/n18-1197},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL}-{HLT} 2018, {New} {Orleans}, {Louisiana}, {USA}, {June} 1-6, 2018, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Andreas, Jacob and Klein, Dan and Levine, Sergey},
	editor = {Walker, Marilyn A. and Ji, Heng and Stent, Amanda},
	year = {2018},
	pages = {2166--2179},
}

@inproceedings{artetxe_cross-lingual_2020,
	title = {On the {Cross}-lingual {Transferability} of {Monolingual} {Representations}},
	doi = {10.18653/v1/2020.acl-main.421},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}, {ACL} 2020, {Online}, {July} 5-10, 2020},
	publisher = {Association for Computational Linguistics},
	author = {Artetxe, Mikel and Ruder, Sebastian and Yogatama, Dani},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel R.},
	year = {2020},
	pages = {4623--4637},
}

@inproceedings{sennrich_neural_2016,
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
	doi = {10.18653/v1/p16-1162},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}, {ACL} 2016, {August} 7-12, 2016, {Berlin}, {Germany}, {Volume} 1: {Long} {Papers}},
	publisher = {The Association for Computer Linguistics},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	year = {2016},
}

@article{wu_googles_2016,
	title = {Google's {Neural} {Machine} {Translation} {System}: {Bridging} the {Gap} between {Human} and {Machine} {Translation}},
	volume = {abs/1609.08144},
	journal = {CoRR},
	author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, Lukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
	year = {2016},
	note = {arXiv: 1609.08144},
}

@inproceedings{mikolov_linguistic_2013,
	title = {Linguistic {Regularities} in {Continuous} {Space} {Word} {Representations}},
	booktitle = {Human {Language} {Technologies}: {Conference} of the {North} {American} {Chapter} of the {Association} of {Computational} {Linguistics}, {Proceedings}, {June} 9-14, 2013, {Westin} {Peachtree} {Plaza} {Hotel}, {Atlanta}, {Georgia}, {USA}},
	publisher = {The Association for Computational Linguistics},
	author = {Mikolov, Tomás and Yih, Wen-tau and Zweig, Geoffrey},
	editor = {Vanderwende, Lucy and III, Hal Daumé and Kirchhoff, Katrin},
	year = {2013},
	pages = {746--751},
}

@article{subramoney_reservoirs_2019,
	title = {Reservoirs learn to learn},
	volume = {abs/1909.07486},
	journal = {CoRR},
	author = {Subramoney, Anand and Scherr, Franz and Maass, Wolfgang},
	year = {2019},
	note = {arXiv: 1909.07486},
}

@article{steiner_cluster-based_2021,
	title = {Cluster-based {Input} {Weight} {Initialization} for {Echo} {State} {Networks}},
	volume = {abs/2103.04710},
	journal = {CoRR},
	author = {Steiner, Peter and Jalalvand, Azarakhsh and Birkholz, Peter},
	year = {2021},
	note = {arXiv: 2103.04710},
}

@inproceedings{rebuffi_learning_2017,
	title = {Learning multiple visual domains with residual adapters},
	volume = {30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@inproceedings{papadimitriou_learning_2020,
	title = {Learning {Music} {Helps} {You} {Read}: {Using} {Transfer} to {Study} {Linguistic} {Structure} in {Language} {Models}},
	doi = {10.18653/v1/2020.emnlp-main.554},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2020, {Online}, {November} 16-20, 2020},
	publisher = {Association for Computational Linguistics},
	author = {Papadimitriou, Isabel and Jurafsky, Dan},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	year = {2020},
	pages = {6829--6839},
}

@article{orhan_compositional_2021,
	title = {Compositional generalization in semantic parsing with pretrained transformers},
	volume = {abs/2109.15101},
	journal = {CoRR},
	author = {Orhan, A. Emin},
	year = {2021},
	note = {arXiv: 2109.15101},
}

@inproceedings{sinha_masked_2021,
	title = {Masked {Language} {Modeling} and the {Distributional} {Hypothesis}: {Order} {Word} {Matters} {Pre}-training for {Little}},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2021, {Virtual} {Event} / {Punta} {Cana}, {Dominican} {Republic}, 7-11 {November}, 2021},
	publisher = {Association for Computational Linguistics},
	author = {Sinha, Koustuv and Jia, Robin and Hupkes, Dieuwke and Pineau, Joelle and Williams, Adina and Kiela, Douwe},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	year = {2021},
	pages = {2888--2913},
}

@misc{raffin_stable_2019,
	title = {Stable {Baselines3}},
	publisher = {GitHub},
	author = {Raffin, Antonin and Hill, Ashley and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Dormann, Noah},
	year = {2019},
	note = {Publication Title: GitHub repository},
}

@article{zhang_are_2019,
	title = {Are {All} {Layers} {Created} {Equal}?},
	volume = {abs/1902.01996},
	journal = {CoRR},
	author = {Zhang, Chiyuan and Bengio, Samy and Singer, Yoram},
	year = {2019},
	note = {arXiv: 1902.01996},
}

@inproceedings{madan_fast_2021,
	title = {Fast {And} {Slow} {Learning} {Of} {Recurrent} {Independent} {Mechanisms}},
	booktitle = {9th {International} {Conference} on {Learning} {Representations}, {ICLR} 2021, {Virtual} {Event}, {Austria}, {May} 3-7, 2021},
	publisher = {OpenReview.net},
	author = {Madan, Kanika and Ke, Nan Rosemary and Goyal, Anirudh and Schölkopf, Bernhard and Bengio, Yoshua},
	year = {2021},
}

@inproceedings{goyal_recurrent_2021,
	title = {Recurrent {Independent} {Mechanisms}},
	booktitle = {9th {International} {Conference} on {Learning} {Representations}, {ICLR} 2021, {Virtual} {Event}, {Austria}, {May} 3-7, 2021},
	publisher = {OpenReview.net},
	author = {Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani, Shagun and Levine, Sergey and Bengio, Yoshua and Schölkopf, Bernhard},
	year = {2021},
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	doi = {10.1038/nature16961},
	number = {7587},
	journal = {Nat.},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Driessche, George van den and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Vedavyas and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy P. and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	year = {2016},
	pages = {484--489},
}

@article{vinyals_grandmaster_2019,
	title = {Grandmaster level in {StarCraft} {II} using multi-agent reinforcement learning},
	volume = {575},
	doi = {10.1038/s41586-019-1724-z},
	number = {7782},
	journal = {Nat.},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander Sasha and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom Le and Gülçehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy P. and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	year = {2019},
	pages = {350--354},
}

@inproceedings{hafner_mastering_2021,
	title = {Mastering {Atari} with {Discrete} {World} {Models}},
	booktitle = {9th {International} {Conference} on {Learning} {Representations}, {ICLR} 2021, {Virtual} {Event}, {Austria}, {May} 3-7, 2021},
	publisher = {OpenReview.net},
	author = {Hafner, Danijar and Lillicrap, Timothy P. and Norouzi, Mohammad and Ba, Jimmy},
	year = {2021},
}

@article{sutton_dyna_1991,
	title = {{DYNA}, an integrated architecture for learning, planning, and reacting},
	journal = {Working Notes of the 1991 AAAI Spring Symposium on Integrated Intelligent Architectures},
	author = {Sutton, Richard S.},
	year = {1991},
	note = {Place: Palo Alto, CA
Publisher: Stanford University},
	keywords = {imported},
	pages = {151--155},
}

@article{kaelbling_planning_1998,
	title = {Planning and {Acting} in {Partially} {Observable} {Stochastic} {Domains}},
	volume = {101},
	journal = {Artificial Intelligence},
	author = {Kaelbling, Leslie P. and Littman, Michael L. and Cassandra, Anthony R.},
	year = {1998},
	keywords = {nn},
	pages = {99--134},
}

@article{wei_vir_2021,
	title = {{ViR}: the {Vision} {Reservoir}},
	volume = {abs/2112.13545},
	journal = {CoRR},
	author = {Wei, Xian and Wang, Bin and Chen, Mingsong and Yuan, Ji and Lan, Hai and Shi, Jiehuang and Tang, Xuan and Jin, Bo and Chen, Guozhang and Yang, Dongping},
	year = {2021},
	note = {arXiv: 2112.13545},
}

@inproceedings{kassner_are_2020,
	title = {Are {Pretrained} {Language} {Models} {Symbolic} {Reasoners} over {Knowledge}?},
	doi = {10.18653/v1/2020.conll-1.45},
	booktitle = {Proceedings of the 24th {Conference} on {Computational} {Natural} {Language} {Learning}, {CoNLL} 2020, {Online}, {November} 19-20, 2020},
	publisher = {Association for Computational Linguistics},
	author = {Kassner, Nora and Krojer, Benno and Schütze, Hinrich},
	editor = {Fernández, Raquel and Linzen, Tal},
	year = {2020},
	pages = {552--564},
	annote = {Transformers can learn simple symbolic reasoning like equlvalence, symmtetry, inversion, implication (not: negation, composition). This basic understanding of logic might be beneficial in reinforcement learning and make language transformers more suitable than e.g. video transformers.},
}

@inproceedings{roy_effective_2007,
	title = {The effective rank: {A} measure of effective dimensionality},
	booktitle = {2007 15th {European} {Signal} {Processing} {Conference}},
	author = {Roy, Olivier and Vetterli, Martin},
	year = {2007},
	pages = {606--610},
}

@inproceedings{zhang_revisiting_2021,
	title = {Revisiting {Few}-sample {BERT} {Fine}-tuning},
	booktitle = {9th {International} {Conference} on {Learning} {Representations}, {ICLR} 2021, {Virtual} {Event}, {Austria}, {May} 3-7, 2021},
	publisher = {OpenReview.net},
	author = {Zhang, Tianyi and Wu, Felix and Katiyar, Arzoo and Weinberger, Kilian Q. and Artzi, Yoav},
	year = {2021},
}

@inproceedings{mosbach_stability_2021,
	title = {On the {Stability} of {Fine}-tuning {BERT}: {Misconceptions}, {Explanations}, and {Strong} {Baselines}},
	booktitle = {9th {International} {Conference} on {Learning} {Representations}, {ICLR} 2021, {Virtual} {Event}, {Austria}, {May} 3-7, 2021},
	publisher = {OpenReview.net},
	author = {Mosbach, Marius and Andriushchenko, Maksym and Klakow, Dietrich},
	year = {2021},
}

@inproceedings{petroni_language_2019,
	title = {Language {Models} as {Knowledge} {Bases}?},
	doi = {10.18653/v1/D19-1250},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing}, {EMNLP}-{IJCNLP} 2019, {Hong} {Kong}, {China}, {November} 3-7, 2019},
	publisher = {Association for Computational Linguistics},
	author = {Petroni, Fabio and Rocktäschel, Tim and Riedel, Sebastian and Lewis, Patrick S. H. and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H.},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	year = {2019},
	pages = {2463--2473},
}

@article{talmor_olmpics_2020,
	title = {{oLMpics} - {On} what {Language} {Model} {Pre}-training {Captures}},
	volume = {8},
	journal = {Trans. Assoc. Comput. Linguistics},
	author = {Talmor, Alon and Elazar, Yanai and Goldberg, Yoav and Berant, Jonathan},
	year = {2020},
	pages = {743--758},
}

@article{berner_dota_2019,
	title = {Dota 2 with {Large} {Scale} {Deep} {Reinforcement} {Learning}},
	volume = {abs/1912.06680},
	journal = {CoRR},
	author = {Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Debiak, Przemyslaw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Christopher and Józefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique Pondé de Oliveira and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
	year = {2019},
	note = {arXiv: 1912.06680},
}

@article{quian_invariant_2005,
	title = {Invariant {Visual} {Representation} by {Single} {Neurons} in the {Human} {Brain}},
	volume = {435},
	doi = {10.1038/nature03687},
	journal = {Nature},
	author = {Quian, Rodrigo and Reddy, Leila and Kreiman, Gabriel and Koch, Christof and Fried, Itzhak},
	month = jul,
	year = {2005},
	pages = {1102--7},
}

@article{schmidhuber_learning_1992,
	title = {Learning {Complex}, {Extended} {Sequences} {Using} the {Principle} of {History} {Compression}},
	volume = {4},
	doi = {10.1162/neco.1992.4.2.234},
	number = {2},
	journal = {Neural Comput.},
	author = {Schmidhuber, Jürgen},
	year = {1992},
	pages = {234--242},
}

@article{botvinick_reinforcement_2015,
	title = {Reinforcement learning, efficient coding, and the statistics of natural tasks},
	volume = {5},
	issn = {2352-1546},
	doi = {https://doi.org/10.1016/j.cobeha.2015.08.009},
	abstract = {The application of ideas from computational reinforcement learning has recently enabled dramatic advances in behavioral and neuroscientific research. For the most part, these advances have involved insights concerning the algorithms underlying learning and decision making. In the present article, we call attention to the equally important but relatively neglected question of how problems in learning and decision making are internally represented. To articulate the significance of representation for reinforcement learning we draw on the concept of efficient coding, as developed in perception research. The resulting perspective exposes a range of novel goals for behavioral and neuroscientific research, highlighting in particular the need for research into the statistical structure of naturalistic tasks.},
	journal = {Current Opinion in Behavioral Sciences},
	author = {Botvinick, Matthew and Weinstein, Ari and Solway, Alec and Barto, Andrew},
	year = {2015},
	pages = {71--77},
	annote = {Neuroeconomics},
}

@inproceedings{ha_recurrent_2018,
	title = {Recurrent {World} {Models} {Facilitate} {Policy} {Evolution}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2018, {NeurIPS} 2018, {December} 3-8, 2018, {Montréal}, {Canada}},
	author = {Ha, David and Schmidhuber, Jürgen},
	editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	year = {2018},
	pages = {2455--2467},
}

@inproceedings{maguire_compressionism_2015,
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Compressionism: {A} {Theory} of {Mind} {Based} on {Data} {Compression}},
	volume = {1419},
	booktitle = {Proceedings of the {EuroAsianPacific} {Joint} {Conference} on {Cognitive} {Science} / 4th {European} {Conference} on {Cognitive} {Science} / 11th {International} {Conference} on {Cognitive} {Science}, {Torino}, {Italy}, {September} 25-27, 2015},
	publisher = {CEUR-WS.org},
	author = {Maguire, Phil and Mulhall, Oisin and Maguire, Rebecca and Taylor, Jessica},
	editor = {Airenti, Gabriella and Bara, Bruno G. and Sandini, Giulio and Cruciani, Marco},
	year = {2015},
}

@inproceedings{abel_state_2019,
	title = {State {Abstraction} as {Compression} in {Apprenticeship} {Learning}},
	doi = {10.1609/aaai.v33i01.33013134},
	booktitle = {The {Thirty}-{Third} {AAAI} {Conference} on {Artificial} {Intelligence}, {AAAI} 2019, {The} {Thirty}-{First} {Innovative} {Applications} of {Artificial} {Intelligence} {Conference}, {IAAI} 2019, {The} {Ninth} {AAAI} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}, {EAAI} 2019, {Honolulu}, {Hawaii}, {USA}, {January} 27 - {February} 1, 2019},
	publisher = {AAAI Press},
	author = {Abel, David and Arumugam, Dilip and Asadi, Kavosh and Jinnai, Yuu and Littman, Michael L. and Wong, Lawson L. S.},
	year = {2019},
	pages = {3134--3142},
}

@article{noorbakhsh_pretrained_2021,
	title = {Pretrained {Language} {Models} are {Symbolic} {Mathematics} {Solvers} too!},
	volume = {abs/2110.03501},
	journal = {CoRR},
	author = {Noorbakhsh, Kimia and Sulaiman, Modar and Sharifi, Mahdi and Roy, Kallol and Jamshidi, Pooyan},
	year = {2021},
	note = {arXiv: 2110.03501},
}

@article{griffith_solving_2019,
	title = {Solving {Arithmetic} {Word} {Problems} {Automatically} {Using} {Transformer} and {Unambiguous} {Representations}},
	volume = {abs/1912.00871},
	journal = {CoRR},
	author = {Griffith, Kaden and Kalita, Jugal},
	year = {2019},
	note = {arXiv: 1912.00871},
}

@article{drori_neural_2021,
	title = {A {Neural} {Network} {Solves} and {Generates} {Mathematics} {Problems} by {Program} {Synthesis}: {Calculus}, {Differential} {Equations}, {Linear} {Algebra}, and {More}},
	volume = {abs/2112.15594},
	journal = {CoRR},
	author = {Drori, Iddo and Tran, Sunny and Wang, Roman and Cheng, Newman and Liu, Kevin and Tang, Leonard and Ke, Elizabeth and Singh, Nikhil and Patti, Taylor L. and Lynch, Jayson and Shporer, Avi and Verma, Nakul and Wu, Eugene and Strang, Gilbert},
	year = {2021},
	note = {arXiv: 2112.15594},
}

@article{chaitin_limits_2006,
	title = {The limits of reason},
	volume = {294},
	number = {3},
	journal = {Sci Am},
	author = {Chaitin, G.},
	year = {2006},
	pages = {74--81},
}

@article{gell-mann_what_1995,
	title = {What is complexity? {Remarks} on simplicity and complexity by the {Nobel} {Prize}-winning author of {The} {Quark} and the {Jaguar}.},
	volume = {1},
	number = {1},
	journal = {Complex.},
	author = {Gell-Mann, Murray},
	year = {1995},
	keywords = {dblp},
	pages = {16--19},
}

@article{taiga_approximate_2018,
	title = {Approximate {Exploration} through {State} {Abstraction}},
	volume = {abs/1808.09819},
	journal = {CoRR},
	author = {Taïga, Adrien Ali and Courville, Aaron C. and Bellemare, Marc G.},
	year = {2018},
	note = {arXiv: 1808.09819},
}

@inproceedings{abel_near_2016,
	series = {{JMLR} {Workshop} and {Conference} {Proceedings}},
	title = {Near {Optimal} {Behavior} via {Approximate} {State} {Abstraction}},
	volume = {48},
	booktitle = {Proceedings of the 33nd {International} {Conference} on {Machine} {Learning}, {ICML} 2016, {New} {York} {City}, {NY}, {USA}, {June} 19-24, 2016},
	publisher = {JMLR.org},
	author = {Abel, David and Hershkowitz, D. Ellis and Littman, Michael L.},
	editor = {Balcan, Maria-Florina and Weinberger, Kilian Q.},
	year = {2016},
	pages = {2915--2923},
}

@inproceedings{hostetler_state_2014,
	title = {State {Aggregation} in {Monte} {Carlo} {Tree} {Search}},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {AAAI} {Conference} on {Artificial} {Intelligence}, {July} 27 -31, 2014, {Québec} {City}, {Québec}, {Canada}},
	publisher = {AAAI Press},
	author = {Hostetler, Jesse and Fern, Alan and Dietterich, Tom},
	editor = {Brodley, Carla E. and Stone, Peter},
	year = {2014},
	pages = {2446--2452},
}

@inproceedings{jiang_improving_2014,
	title = {Improving {UCT} planning via approximate homomorphisms},
	booktitle = {International conference on {Autonomous} {Agents} and {Multi}-{Agent} {Systems}, {AAMAS} '14, {Paris}, {France}, {May} 5-9, 2014},
	publisher = {IFAAMAS/ACM},
	author = {Jiang, Nan and Singh, Satinder P. and Lewis, Richard L.},
	editor = {Bazzan, Ana L. C. and Huhns, Michael N. and Lomuscio, Alessio and Scerri, Paul},
	year = {2014},
	pages = {1289--1296},
}

@inproceedings{li_towards_2006,
	title = {Towards a {Unified} {Theory} of {State} {Abstraction} for {MDPs}},
	booktitle = {International {Symposium} on {Artificial} {Intelligence} and {Mathematics}, {ISAIM} 2006, {Fort} {Lauderdale}, {Florida}, {USA}, {January} 4-6, 2006},
	author = {Li, Lihong and Walsh, Thomas J. and Littman, Michael L.},
	year = {2006},
}

@inproceedings{ferns_metrics_2004,
	title = {Metrics for {Finite} {Markov} {Decision} {Processes}},
	booktitle = {Proceedings of the {Nineteenth} {National} {Conference} on {Artificial} {Intelligence}, {Sixteenth} {Conference} on {Innovative} {Applications} of {Artificial} {Intelligence}, {July} 25-29, 2004, {San} {Jose}, {California}, {USA}},
	publisher = {AAAI Press / The MIT Press},
	author = {Ferns, Norm and Panangaden, Prakash and Precup, Doina},
	editor = {McGuinness, Deborah L. and Ferguson, George},
	year = {2004},
	pages = {950--951},
}

@article{givan_equivalence_2003,
	title = {Equivalence notions and model minimization in {Markov} decision processes},
	volume = {147},
	doi = {10.1016/S0004-3702(02)00376-4},
	number = {1-2},
	journal = {Artif. Intell.},
	author = {Givan, Robert and Dean, Thomas L. and Greig, Matthew},
	year = {2003},
	pages = {163--223},
}

@inproceedings{andre_state_2002,
	title = {State {Abstraction} for {Programmable} {Reinforcement} {Learning} {Agents}},
	booktitle = {Proceedings of the {Eighteenth} {National} {Conference} on {Artificial} {Intelligence} and {Fourteenth} {Conference} on {Innovative} {Applications} of {Artificial} {Intelligence}, {July} 28 - {August} 1, 2002, {Edmonton}, {Alberta}, {Canada}},
	publisher = {AAAI Press / The MIT Press},
	author = {Andre, David and Russell, Stuart J.},
	editor = {Dechter, Rina and Kearns, Michael J. and Sutton, Richard S.},
	year = {2002},
	pages = {119--125},
}

@article{dietterich_hierarchical_2000,
	title = {Hierarchical {Reinforcement} {Learning} with the {MAXQ} {Value} {Function} {Decomposition}},
	volume = {13},
	doi = {10.1613/jair.639},
	journal = {J. Artif. Intell. Res.},
	author = {Dietterich, Thomas G.},
	year = {2000},
	pages = {227--303},
}

@inproceedings{dean_model_1997,
	title = {Model {Reduction} {Techniques} for {Computing} {Approximately} {Optimal} {Solutions} for {Markov} {Decision} {Processes}},
	booktitle = {{UAI} '97: {Proceedings} of the {Thirteenth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}, {Brown} {University}, {Providence}, {Rhode} {Island}, {USA}, {August} 1-3, 1997},
	publisher = {Morgan Kaufmann},
	author = {Dean, Thomas L. and Givan, Robert and Leach, Sonia M.},
	editor = {Geiger, Dan and Shenoy, Prakash P.},
	year = {1997},
	pages = {124--131},
}

@phdthesis{mccallum_reinforcement_1995,
	type = {{PhD} {Thesis}},
	title = {Reinforcement {Learning} with {Selective} {Perception} and {Hidden} {States}},
	school = {Department of Computer Science, University of Rochester},
	author = {McCallum, A. K.},
	year = {1995},
	keywords = {ml},
}

@inproceedings{singh_reinforcement_1994,
	title = {Reinforcement {Learning} with {Soft} {State} {Aggregation}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 7, [{NIPS} {Conference}, {Denver}, {Colorado}, {USA}, 1994]},
	publisher = {MIT Press},
	author = {Singh, Satinder P. and Jaakkola, Tommi S. and Jordan, Michael I.},
	editor = {Tesauro, Gerald and Touretzky, David S. and Leen, Todd K.},
	year = {1994},
	pages = {361--368},
}

@article{bertsekas_adaptive_1989,
	title = {Adaptive aggregation methods for infinite horizon dynamic programming},
	volume = {34},
	doi = {10.1109/9.24227},
	number = {6},
	journal = {IEEE Transactions on Automatic Control},
	author = {Bertsekas, D.P. and Castanon, D.A.},
	year = {1989},
	pages = {589--598},
}

@article{whitt_approximations_1978,
	title = {Approximations of {Dynamic} {Programs}, {I}},
	volume = {3},
	doi = {10.1287/moor.3.3.231},
	number = {3},
	journal = {Math. Oper. Res.},
	author = {Whitt, Ward},
	year = {1978},
	pages = {231--243},
}

@inproceedings{hamilton_modelling_2013,
	address = {Atlanta, Georgia, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Modelling {Sparse} {Dynamical} {Systems} with {Compressed} {Predictive} {State} {Representations}},
	volume = {28},
	abstract = {Efficiently learning accurate models of dynamical systems is of central importance for developing rational agents that can succeed in a wide range of challenging domains. The difficulty of this learning problem is particularly acute in settings with large observation spaces and partial observability. We present a new algorithm, called Compressed Predictive State Representation (CPSR), for learning models of high-dimensional partially observable uncontrolled dynamical systems from small sample sets. The algorithm, which extends previous work on Predictive State Representations, exploits a particular sparse structure present in many domains. This sparse structure is used to compress information during learning, allowing for an increase in both the efficiency and predictive power. The compression technique also relieves the burden of domain specific feature selection and allows for domains with extremely large discrete observation spaces to be efficiently modelled. We present empirical results showing that the algorithm is able to build accurate models more efficiently than its uncompressed counterparts, and provide theoretical results on the accuracy of the learned compressed model.},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hamilton, William L. and Fard, Mahdi Milani and Pineau, Joelle},
	editor = {Dasgupta, Sanjoy and McAllester, David},
	month = jun,
	year = {2013},
	note = {Issue: 1},
	pages = {178--186},
}

@inproceedings{dayan_feudal_1992,
	title = {Feudal {Reinforcement} {Learning}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 5, [{NIPS} {Conference}, {Denver}, {Colorado}, {USA}, {November} 30 - {December} 3, 1992]},
	publisher = {Morgan Kaufmann},
	author = {Dayan, Peter and Hinton, Geoffrey E.},
	editor = {Hanson, Stephen Jose and Cowan, Jack D. and Giles, C. Lee},
	year = {1992},
	pages = {271--278},
}

@article{sutton_between_1999,
	title = {Between {MDPs} and {Semi}-{MDPs}: {A} {Framework} for {Temporal} {Abstraction} in {Reinforcement} {Learning}},
	volume = {112},
	doi = {10.1016/S0004-3702(99)00052-1},
	number = {1-2},
	journal = {Artif. Intell.},
	author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder P.},
	year = {1999},
	pages = {181--211},
}

@inproceedings{hauskrecht_hierarchical_1998,
	title = {Hierarchical {Solution} of {Markov} {Decision} {Processes} using {Macro}-actions},
	booktitle = {{UAI} '98: {Proceedings} of the {Fourteenth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}, {University} of {Wisconsin} {Business} {School}, {Madison}, {Wisconsin}, {USA}, {July} 24-26, 1998},
	publisher = {Morgan Kaufmann},
	author = {Hauskrecht, Milos and Meuleau, Nicolas and Kaelbling, Leslie Pack and Dean, Thomas L. and Boutilier, Craig},
	editor = {Cooper, Gregory F. and Moral, Serafín},
	year = {1998},
	pages = {220--229},
}

@inproceedings{dietterich_state_1999,
	title = {State {Abstraction} in {MAXQ} {Hierarchical} {Reinforcement} {Learning}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 12, [{NIPS} {Conference}, {Denver}, {Colorado}, {USA}, {November} 29 - {December} 4, 1999]},
	publisher = {The MIT Press},
	author = {Dietterich, Thomas G.},
	editor = {Solla, Sara A. and Leen, Todd K. and Müller, Klaus-Robert},
	year = {1999},
	pages = {994--1000},
}

@article{barlow_possible_1961,
	title = {Possible {Principles} {Underlying} the {Transformations} of {Sensory} {Messages}},
	volume = {1},
	doi = {10.7551/mitpress/9780262518420.003.0013},
	journal = {Sensory Communication},
	author = {Barlow, Horace},
	month = jan,
	year = {1961},
	note = {ISBN: 9780262518420},
}

@article{attneave_informational_1954,
	title = {Some informational aspects of visual perception.},
	volume = {61 3},
	journal = {Psychological review},
	author = {Attneave, Fred},
	year = {1954},
	pages = {183--93},
}

@article{minixhofer_wechsel_2021,
	title = {{WECHSEL}: {Effective} initialization of subword embeddings for cross-lingual transfer of monolingual language models},
	volume = {abs/2112.06598},
	journal = {CoRR},
	author = {Minixhofer, Benjamin and Paischer, Fabian and Rekabsaz, Navid},
	year = {2021},
	note = {arXiv: 2112.06598},
}

@article{astrom_optimal_1964,
	title = {Optimal control of {Markov} processes with incomplete state information},
	volume = {10},
	journal = {Journal of Mathematical Analysis and Applications},
	author = {Åström, Karl Johan},
	year = {1964},
	pages = {174--205},
}

@inproceedings{chen_high_1987,
	address = {USA},
	title = {High {Order} {Correlation} {Model} for {Associative} {Memory}},
	booktitle = {{AIP} {Conference} {Proceedings} 151 on {Neural} {Networks} for {Computing}},
	publisher = {American Institute of Physics Inc.},
	author = {Chen, H H and Lee, Y C and Sun, G Z and Lee, H Y and Maxwell, T and Giles, C L},
	year = {1987},
	note = {event-place: Snowbird, Utah, USA},
	pages = {86--99},
}

@inproceedings{psaltis_nonlinear_1986,
	title = {Nonlinear discriminant functions and associative memories},
	volume = {151},
	booktitle = {{AIP} conference {Proceedings}},
	publisher = {American Institute of Physics},
	author = {Psaltis, Demetri and Park, Cheol Hoon},
	year = {1986},
	note = {Issue: 1},
	pages = {370--375},
}

@article{baldi_number_1987,
	title = {Number of stable points for spin-glasses and neural networks of higher orders},
	volume = {58},
	doi = {10.1103/PhysRevLett.58.913},
	number = {9},
	journal = {Phys. Rev. Lett.},
	author = {Baldi, Pierre and Venkatesh, Santosh S.},
	month = mar,
	year = {1987},
	note = {Publisher: American Physical Society},
	pages = {913--916},
}

@article{gardner_multiconnected_1987,
	title = {Multiconnected neural network models},
	volume = {20},
	doi = {10.1088/0305-4470/20/11/046},
	abstract = {A generalisation of the Hopfield model which includes interactions between p()2) Ising spins is considered. The exact storage capacity behaves as Np-1/2(p-1)! ln N when the number of nodes, N, is large. In the limit p to infinity , the thermodynamics of the model can be solved exactly without using the replica method; at zero temperature, a solution which is completely correlated with the input pattern exists for alpha {\textless} alpha c where alpha c to infinity as p to infinity and this solution has lower energy than the spin-glass solution if alpha {\textless} alpha 1=1/4 ln 2 where the number of patterns n=(2 alpha /p!)Np-1. For finite values of p, the correlation with the input pattern is not complete; for p=3 and 4, approximate values of alpha c and alpha 1 are obtained and for p to infinity the replica symmetric approximation gives alpha c approximately p/4 ln p.},
	number = {11},
	journal = {Journal of Physics A: Mathematical and General},
	author = {Gardner, E.},
	month = aug,
	year = {1987},
	note = {Publisher: IOP Publishing},
	pages = {3453--3464},
}

@article{abbott_storage_1987,
	title = {Storage capacity of generalized networks},
	volume = {36},
	doi = {10.1103/PhysRevA.36.5091},
	number = {10},
	journal = {Phys. Rev. A},
	author = {Abbott, L. F. and Arian, Yair},
	month = nov,
	year = {1987},
	note = {Publisher: American Physical Society},
	pages = {5091--5094},
}

@article{horn_capacities_1988,
	title = {Capacities of multiconnected memory models},
	volume = {49},
	number = {3},
	journal = {Journal de Physique},
	author = {Horn, D and Usher, M},
	year = {1988},
	note = {Publisher: Société Française de Physique},
	pages = {389--395},
}

@inproceedings{caputo_storage_2002,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Storage {Capacity} of {Kernel} {Associative} {Memories}},
	volume = {2415},
	doi = {10.1007/3-540-46084-5_9},
	booktitle = {Artificial {Neural} {Networks} - {ICANN} 2002, {International} {Conference}, {Madrid}, {Spain}, {August} 28-30, 2002, {Proceedings}},
	publisher = {Springer},
	author = {Caputo, Barbara and Niemann, Heinrich},
	editor = {Dorronsoro, José R.},
	year = {2002},
	pages = {51--56},
}

@article{hopfield_neurons_1984,
	title = {Neurons with graded response have collective computational properties like those of two-state neurons},
	volume = {81},
	issn = {0027-8424},
	doi = {10.1073/pnas.81.10.3088},
	abstract = {A model for a large network of "neurons" with a graded response (or sigmoid input-output relation) is studied. This deterministic system has collective properties in very close correspondence with the earlier stochastic model based on McCulloch - Pitts neurons. The content- addressable memory and other emergent collective properties of the original model also are present in the graded response model. The idea that such collective properties are used in biological systems is given added credence by the continued presence of such properties for more nearly biological "neurons." Collective analog electrical circuits of the kind described will certainly function. The collective states of the two models have a simple correspondence. The original model will continue to be useful for simulations, because its connection to graded response systems is established. Equations that include the effect of action potentials in the graded response system are also developed.},
	number = {10},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Hopfield, J J},
	year = {1984},
	note = {Publisher: National Academy of Sciences
\_eprint: https://www.pnas.org/content/81/10/3088.full.pdf},
	pages = {3088--3092},
}

@article{hopfield_neural_1982,
	title = {Neural networks and physical systems with emergent collective computational abilities},
	volume = {79},
	issn = {0027-8424},
	doi = {10.1073/pnas.79.8.2554},
	abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
	number = {8},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Hopfield, J J},
	year = {1982},
	note = {Publisher: National Academy of Sciences
\_eprint: https://www.pnas.org/content/79/8/2554.full.pdf},
	pages = {2554--2558},
}

@inproceedings{krotov_dense_2016,
	title = {Dense {Associative} {Memory} for {Pattern} {Recognition}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2016, {December} 5-10, 2016, {Barcelona}, {Spain}},
	author = {Krotov, Dmitry and Hopfield, John J.},
	editor = {Lee, Daniel D. and Sugiyama, Masashi and Luxburg, Ulrike von and Guyon, Isabelle and Garnett, Roman},
	year = {2016},
	pages = {1172--1180},
}

@article{krotov_dense_2018,
	title = {Dense {Associative} {Memory} {Is} {Robust} to {Adversarial} {Inputs}},
	volume = {30},
	doi = {10.1162/neco_a_01143},
	number = {12},
	journal = {Neural Comput.},
	author = {Krotov, Dmitry and Hopfield, John J.},
	year = {2018},
}

@inproceedings{krotov_dense_2016-1,
	title = {Dense {Associative} {Memory} for {Pattern} {Recognition}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2016, {December} 5-10, 2016, {Barcelona}, {Spain}},
	author = {Krotov, Dmitry and Hopfield, John J.},
	editor = {Lee, Daniel D. and Sugiyama, Masashi and Luxburg, Ulrike von and Guyon, Isabelle and Garnett, Roman},
	year = {2016},
	pages = {1172--1180},
}

@article{krotov_dense_2018-1,
	title = {Dense {Associative} {Memory} {Is} {Robust} to {Adversarial} {Inputs}},
	volume = {30},
	doi = {10.1162/neco_a_01143},
	number = {12},
	journal = {Neural Comput.},
	author = {Krotov, Dmitry and Hopfield, John J.},
	year = {2018},
}

@inproceedings{krotov_large_2021,
	title = {Large {Associative} {Memory} {Problem} in {Neurobiology} and {Machine} {Learning}},
	booktitle = {9th {International} {Conference} on {Learning} {Representations}, {ICLR} 2021, {Virtual} {Event}, {Austria}, {May} 3-7, 2021},
	publisher = {OpenReview.net},
	author = {Krotov, Dmitry and Hopfield, John J.},
	year = {2021},
}

@article{demircigil_model_2017,
	title = {On a {Model} of {Associative} {Memory} with {Huge} {Storage} {Capacity}},
	volume = {168},
	doi = {10.1007/s10955-017-1806-y},
	number = {2},
	journal = {Journal of Statistical Physics},
	author = {Demircigil, Mete and Heusel, Judith and Löwe, Matthias and Upgang, Sven and Vermet, Franck},
	month = jul,
	year = {2017},
	note = {\_eprint: 1702.01929},
	keywords = {60K35, 82C32, 92B20, Associative memory, Exponential inequalities, Hopfield model, Mathematics - Probability, Neural networks, Secondary: 68T05},
	pages = {288--299},
}

@inproceedings{widrich_modern_2020,
	title = {Modern {Hopfield} {Networks} and {Attention} for {Immune} {Repertoire} {Classification}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 33: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2020, {NeurIPS} 2020, {December} 6-12, 2020, virtual},
	author = {Widrich, Michael and Schäfl, Bernhard and Pavlovic, Milena and Ramsauer, Hubert and Gruber, Lukas and Holzleitner, Markus and Brandstetter, Johannes and Sandve, Geir Kjetil and Greiff, Victor and Hochreiter, Sepp and Klambauer, Günter},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
	year = {2020},
}

@book{vershynin_high-dimensional_2018,
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {High-{Dimensional} {Probability}: {An} {Introduction} with {Applications} in {Data} {Science}},
	publisher = {Cambridge University Press},
	author = {Vershynin, Roman},
	year = {2018},
	doi = {10.1017/9781108231596},
}

@book{olver_nist_2010,
	address = {USA},
	edition = {1st},
	title = {{NIST} {Handbook} of {Mathematical} {Functions}},
	abstract = {Modern developments in theoretical and applied science depend on knowledge of the properties of mathematical functions, from elementary trigonometric functions to the multitude of special functions. These functions appear whenever natural phenomena are studied, engineering problems are formulated, and numerical simulations are performed. They also crop up in statistics, financial models, and economic analysis. Using them effectively requires practitioners to have ready access to a reliable collection of their properties. This handbook results from a 10-year project conducted by the National Institute of Standards and Technology with an international group of expert authors and validators. Printed in full color, it is destined to replace its predecessor, the classic but long-outdated Handbook of Mathematical Functions, edited by Abramowitz and Stegun. Included with every copy of the book is a CD with a searchable PDF of each chapter.},
	publisher = {Cambridge University Press},
	author = {Olver, Frank W. and Lozier, Daniel W. and Boisvert, Ronald F. and Clark, Charles W.},
	year = {2010},
}

@article{dasgupta_elementary_2003,
	title = {An elementary proof of a theorem of {Johnson} and {Lindenstrauss}},
	volume = {22},
	doi = {10.1002/rsa.10073},
	number = {1},
	journal = {Random Struct. Algorithms},
	author = {Dasgupta, Sanjoy and Gupta, Anupam},
	year = {2003},
	pages = {60--65},
}

@inproceedings{arjona-medina_rudder_2019,
	title = {{RUDDER}: {Return} {Decomposition} for {Delayed} {Rewards}},
	volume = {32},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Arjona-Medina, Jose A. and Gillhofer, Michael and Widrich, Michael and Unterthiner, Thomas and Brandstetter, Johannes and Hochreiter, Sepp},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
}

@article{patil_align-rudder_2020,
	title = {Align-{RUDDER}: {Learning} {From} {Few} {Demonstrations} by {Reward} {Redistribution}},
	volume = {abs/2009.14108},
	journal = {CoRR},
	author = {Patil, Vihang P. and Hofmarcher, Markus and Dinu, Marius-Constantin and Dorfer, Matthias and Blies, Patrick M. and Brandstetter, Johannes and Arjona-Medina, Jose A. and Hochreiter, Sepp},
	year = {2020},
	note = {arXiv: 2009.14108},
}

@inproceedings{widrich_modern_2021,
	title = {Modern {Hopfield} {Networks} for {Return} {Decomposition} for {Delayed} {Rewards}},
	booktitle = {Deep {RL} {Workshop} {NeurIPS} 2021},
	author = {Widrich, Michael and Hofmarcher, Markus and Patil, Vihang Prakash and Bitto-Nemling, Angela and Hochreiter, Sepp},
	year = {2021},
}

@article{furst_cloob_2021,
	title = {{CLOOB}: {Modern} {Hopfield} {Networks} with {InfoLOOB} {Outperform} {CLIP}},
	volume = {abs/2110.11316},
	journal = {CoRR},
	author = {Fürst, Andreas and Rumetshofer, Elisabeth and Tran, Viet and Ramsauer, Hubert and Tang, Fei and Lehner, Johannes and Kreil, David P. and Kopp, Michael and Klambauer, Günter and Bitto-Nemling, Angela and Hochreiter, Sepp},
	year = {2021},
	note = {arXiv: 2110.11316},
}

@article{seidl_improving_2022,
	title = {Improving {Few}-and {Zero}-{Shot} {Reaction} {Template} {Prediction} {Using} {Modern} {Hopfield} {Networks}},
	journal = {Journal of Chemical Information and Modeling},
	author = {Seidl, Philipp and Renz, Philipp and Dyubankova, Natalia and Neves, Paulo and Verhoeven, Jonas and Wegner, Jörg K and Segler, Marwin and Hochreiter, Sepp and Klambauer, Günter},
	year = {2022},
	note = {Publisher: ACS Publications},
}

@misc{zuo_mazelab_2018,
	title = {mazelab: {A} customizable framework to create maze and gridworld environments.},
	publisher = {GitHub},
	author = {Zuo, Xingdong},
	year = {2018},
	note = {Publication Title: GitHub repository},
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation {Applied} to {Handwritten} {Zip} {Code} {Recognition}},
	volume = {1},
	doi = {10.1162/neco.1989.1.4.541},
	number = {4},
	journal = {Neural Comput.},
	author = {LeCun, Yann and Boser, Bernhard E. and Denker, John S. and Henderson, Donnie and Howard, Richard E. and Hubbard, Wayne E. and Jackel, Lawrence D.},
	year = {1989},
	pages = {541--551},
}

@article{fukushima_neocognitron_1980,
	title = {Neocognitron: {A} {Self}-{Organizing} {Neural} {Network} {Model} for a {Mechanism} of {Pattern} {Recognition} {Unaffected} by {Shift} in {Position}},
	volume = {36},
	journal = {Biological Cybernetics},
	author = {Fukushima, Kunihiko},
	year = {1980},
	keywords = {nn},
	pages = {193--202},
}

@inproceedings{parisotto_efficient_2021,
	title = {Efficient {Transformers} in {Reinforcement} {Learning} using {Actor}-{Learner} {Distillation}},
	booktitle = {9th {International} {Conference} on {Learning} {Representations}, {ICLR} 2021, {Virtual} {Event}, {Austria}, {May} 3-7, 2021},
	publisher = {OpenReview.net},
	author = {Parisotto, Emilio and Salakhutdinov, Russ R.},
	year = {2021},
}

@inproceedings{ruvolo_ella_2013,
	series = {{JMLR} {Workshop} and {Conference} {Proceedings}},
	title = {{ELLA}: {An} {Efficient} {Lifelong} {Learning} {Algorithm}},
	volume = {28},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}, {ICML} 2013, {Atlanta}, {GA}, {USA}, 16-21 {June} 2013},
	publisher = {JMLR.org},
	author = {Ruvolo, Paul and Eaton, Eric},
	year = {2013},
	pages = {507--515},
}

@inproceedings{zenke_continual_2017,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Continual {Learning} {Through} {Synaptic} {Intelligence}},
	volume = {70},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}, {ICML} 2017, {Sydney}, {NSW}, {Australia}, 6-11 {August} 2017},
	publisher = {PMLR},
	author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
	editor = {Precup, Doina and Teh, Yee Whye},
	year = {2017},
	pages = {3987--3995},
}

@article{kirkpatrick_overcoming_2016,
	title = {Overcoming catastrophic forgetting in neural networks},
	volume = {abs/1612.00796},
	journal = {CoRR},
	author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil C. and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
	year = {2016},
	note = {arXiv: 1612.00796},
}

@inproceedings{schwarz_progress_2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Progress \& {Compress}: {A} scalable framework for continual learning},
	volume = {80},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}, {ICML} 2018, {Stockholmsmässan}, {Stockholm}, {Sweden}, {July} 10-15, 2018},
	publisher = {PMLR},
	author = {Schwarz, Jonathan and Czarnecki, Wojciech and Luketina, Jelena and Grabska-Barwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	year = {2018},
	pages = {4535--4544},
}

@phdthesis{sutton_temporal_1984,
	type = {{PhD} {Thesis}},
	title = {Temporal {Credit} {Assignment} in {Reinforcement} {Learning}},
	school = {University of Massachusetts, Dept. of Comp. and Inf. Sci.},
	author = {Sutton, R. S.},
	year = {1984},
	keywords = {juergen},
}

@book{puterman_markov_1994,
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Markov {Decision} {Processes}: {Discrete} {Stochastic} {Dynamic} {Programming}},
	publisher = {Wiley},
	author = {Puterman, Martin L.},
	year = {1994},
	doi = {10.1002/9780470316887},
}

@book{sutton_reinforcement_2018,
	edition = {Second},
	title = {Reinforcement {Learning}: {An} {Introduction}},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
}

@inproceedings{hausknecht_deep_2015,
	title = {Deep {Recurrent} {Q}-{Learning} for {Partially} {Observable} {MDPs}},
	booktitle = {2015 {AAAI} {Fall} {Symposia}, {Arlington}, {Virginia}, {USA}, {November} 12-14, 2015},
	publisher = {AAAI Press},
	author = {Hausknecht, Matthew J. and Stone, Peter},
	year = {2015},
	pages = {29--37},
}

@inproceedings{cassandra_acting_1994,
	title = {Acting {Optimally} in {Partially} {Observable} {Stochastic} {Domains}},
	booktitle = {Proceedings of the 12th {National} {Conference} on {Artificial} {Intelligence}, {Seattle}, {WA}, {USA}, {July} 31 - {August} 4, 1994, {Volume} 2},
	publisher = {AAAI Press / The MIT Press},
	author = {Cassandra, Anthony R. and Kaelbling, Leslie Pack and Littman, Michael L.},
	editor = {Hayes-Roth, Barbara and Korf, Richard E.},
	year = {1994},
	pages = {1023--1028},
}

@article{lin_self-improving_1992,
	title = {Self-{Improving} {Reactive} {Agents} {Based} {On} {Reinforcement} {Learning}, {Planning} and {Teaching}},
	volume = {8},
	doi = {10.1007/BF00992699},
	journal = {Mach. Learn.},
	author = {Lin, Long Ji},
	year = {1992},
	pages = {293--321},
}

@inproceedings{rae_compressive_2020,
	title = {Compressive {Transformers} for {Long}-{Range} {Sequence} {Modelling}},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Rae, Jack W. and Potapenko, Anna and Jayakumar, Siddhant M. and Hillier, Chloe and Lillicrap, Timothy P.},
	year = {2020},
}

@article{reid_can_2022,
	title = {Can {Wikipedia} {Help} {Offline} {Reinforcement} {Learning}?},
	volume = {abs/2201.12122},
	journal = {CoRR},
	author = {Reid, Machel and Yamada, Yutaro and Gu, Shixiang Shane},
	year = {2022},
	note = {arXiv: 2201.12122},
}

@article{li_pre-trained_2022,
	title = {Pre-{Trained} {Language} {Models} for {Interactive} {Decision}-{Making}},
	volume = {abs/2202.01771},
	journal = {CoRR},
	author = {Li, Shuang and Puig, Xavier and Paxton, Chris and Du, Yilun and Wang, Clinton and Fan, Linxi and Chen, Tao and Huang, De-An and Akyürek, Ekin and Anandkumar, Anima and Andreas, Jacob and Mordatch, Igor and Torralba, Antonio and Zhu, Yuke},
	year = {2022},
	note = {arXiv: 2202.01771},
}

@article{huang_language_2022,
	title = {Language {Models} as {Zero}-{Shot} {Planners}: {Extracting} {Actionable} {Knowledge} for {Embodied} {Agents}},
	volume = {abs/2201.07207},
	journal = {CoRR},
	author = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
	year = {2022},
	note = {arXiv: 2201.07207},
}

@article{gupta_metamorph_2022,
	title = {{MetaMorph}: {Learning} {Universal} {Controllers} with {Transformers}},
	volume = {abs/2203.11931},
	doi = {10.48550/arXiv.2203.11931},
	journal = {CoRR},
	author = {Gupta, Agrim and Fan, Linxi and Ganguli, Surya and Fei-Fei, Li},
	year = {2022},
	note = {arXiv: 2203.11931},
}

@article{mu_improving_2022,
	title = {Improving {Intrinsic} {Exploration} with {Language} {Abstractions}},
	volume = {abs/2202.08938},
	journal = {CoRR},
	author = {Mu, Jesse and Zhong, Victor and Raileanu, Roberta and Jiang, Minqi and Goodman, Noah D. and Rocktäschel, Tim and Grefenstette, Edward},
	year = {2022},
	note = {arXiv: 2202.08938},
}

@article{hill_human_2020-1,
	title = {Human {Instruction}-{Following} with {Deep} {Reinforcement} {Learning} via {Transfer}-{Learning} from {Text}},
	volume = {abs/2005.09382},
	journal = {CoRR},
	author = {Hill, Felix and Mokra, Sona and Wong, Nathaniel and Harley, Tim},
	year = {2020},
	note = {arXiv: 2005.09382},
}

@article{agarwal_deep_2021,
	title = {Deep reinforcement learning at the edge of the statistical precipice},
	volume = {34},
	journal = {Advances in Neural Information Processing Systems},
	author = {Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
	year = {2021},
}

@article{nowak_evolution_1999,
	title = {The evolution of language},
	volume = {96},
	doi = {10.1073/pnas.96.14.8028},
	abstract = {The emergence of language was a defining moment in the evolution of modern humans. It was an innovation that changed radically the character of human society. Here, we provide an approach to language evolution based on evolutionary game theory. We explore the ways in which protolanguages can evolve in a nonlinguistic society and how specific signals can become associated with specific objects. We assume that early in the evolution of language, errors in signaling and perception would be common. We model the probability of misunderstanding a signal and show that this limits the number of objects that can be described by a protolanguage. This “error limit” is not overcome by employing more sounds but by combining a small set of more easily distinguishable sounds into words. The process of “word formation” enables a language to encode an essentially unlimited number of objects. Next, we analyze how words can be combined into sentences and specify the conditions for the evolution of very simple grammatical rules. We argue that grammar originated as a simplified rule system that evolved by natural selection to reduce mistakes in communication. Our theory provides a systematic approach for thinking about the origin and evolution of human language.},
	number = {14},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Nowak, Martin A. and Krakauer, David C.},
	year = {1999},
	note = {\_eprint: https://www.pnas.org/doi/pdf/10.1073/pnas.96.14.8028},
	pages = {8028--8033},
}

@article{amari_learning_1972,
	title = {Learning {Patterns} and {Pattern} {Sequences} by {Self}-{Organizing} {Nets} of {Threshold} {Elements}},
	volume = {21},
	doi = {10.1109/T-C.1972.223477},
	number = {11},
	journal = {IEEE Trans. Computers},
	author = {Amari, Shun-ichi},
	year = {1972},
	pages = {1197--1206},
}

@inproceedings{yao_keep_2020,
	title = {Keep {CALM} and {Explore}: {Language} {Models} for {Action} {Generation} in {Text}-based {Games}},
	doi = {10.18653/v1/2020.emnlp-main.704},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}, {EMNLP} 2020, {Online}, {November} 16-20, 2020},
	publisher = {Association for Computational Linguistics},
	author = {Yao, Shunyu and Rao, Rohan and Hausknecht, Matthew J. and Narasimhan, Karthik},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	year = {2020},
	pages = {8736--8754},
}

@inproceedings{mnih_asynchronous_2016,
	series = {{JMLR} {Workshop} and {Conference} {Proceedings}},
	title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
	volume = {48},
	booktitle = {Proceedings of the 33nd {International} {Conference} on {Machine} {Learning}, {ICML} 2016, {New} {York} {City}, {NY}, {USA}, {June} 19-24, 2016},
	publisher = {JMLR.org},
	author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	editor = {Balcan, Maria-Florina and Weinberger, Kilian Q.},
	year = {2016},
	pages = {1928--1937},
}

@inproceedings{cobbe_phasic_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Phasic {Policy} {Gradient}},
	volume = {139},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}, {ICML} 2021, 18-24 {July} 2021, {Virtual} {Event}},
	publisher = {PMLR},
	author = {Cobbe, Karl and Hilton, Jacob and Klimov, Oleg and Schulman, John},
	editor = {Meila, Marina and Zhang, Tong},
	year = {2021},
	pages = {2020--2027},
}

@article{tam_semantic_2022,
	title = {Semantic {Exploration} from {Language} {Abstractions} and {Pretrained} {Representations}},
	volume = {abs/2204.05080},
	doi = {10.48550/arXiv.2204.05080},
	journal = {CoRR},
	author = {Tam, Allison C. and Rabinowitz, Neil C. and Lampinen, Andrew K. and Roy, Nicholas A. and Chan, Stephanie C. Y. and Strouse, D. J. and Wang, Jane X. and Banino, Andrea and Hill, Felix},
	year = {2022},
	note = {arXiv: 2204.05080},
}

@article{holzleitner_convergence_2020,
	title = {Convergence {Proof} for {Actor}-{Critic} {Methods} {Applied} to {PPO} and {RUDDER}},
	volume = {abs/2012.01399},
	journal = {CoRR},
	author = {Holzleitner, Markus and Gruber, Lukas and Arjona-Medina, Jose A. and Brandstetter, Johannes and Hochreiter, Sepp},
	year = {2020},
	note = {arXiv: 2012.01399},
}

@article{schafl_hopular_2022,
	title = {Hopular: {Modern} {Hopfield} {Networks} for {Tabular} {Data}},
	volume = {abs/2206.00664},
	doi = {10.48550/arXiv.2206.00664},
	journal = {CoRR},
	author = {Schäfl, Bernhard and Gruber, Lukas and Bitto-Nemling, Angela and Hochreiter, Sepp},
	year = {2022},
	note = {arXiv: 2206.00664},
}

@inproceedings{sharma_skill_2022,
  author    = {Pratyusha Sharma and
               Antonio Torralba and
               Jacob Andreas},
  editor    = {Smaranda Muresan and
               Preslav Nakov and
               Aline Villavicencio},
  title     = {Skill Induction and Planning with Latent Language},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational
               Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland,
               May 22-27, 2022},
  pages     = {1713--1726},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  doi       = {10.18653/v1/2022.acl-long.120},
  timestamp = {Mon, 01 Aug 2022 16:27:51 +0200}
}

@inproceedings{jacob_multitasking_2021,
  author    = {Athul Paul Jacob and
               Mike Lewis and
               Jacob Andreas},
  editor    = {Kristina Toutanova and
               Anna Rumshisky and
               Luke Zettlemoyer and
               Dilek Hakkani{-}T{\"{u}}r and
               Iz Beltagy and
               Steven Bethard and
               Ryan Cotterell and
               Tanmoy Chakraborty and
               Yichao Zhou},
  title     = {Multitasking Inhibits Semantic Drift},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2021, Online, June 6-11, 2021},
  pages     = {5351--5366},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  doi       = {10.18653/v1/2021.naacl-main.421},
  timestamp = {Fri, 06 Aug 2021 00:41:31 +0200}
}