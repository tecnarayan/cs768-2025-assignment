\begin{thebibliography}{10}

\bibitem{frenay2013classification}
Beno{\^\i}t Fr{\'e}nay and Michel Verleysen.
\newblock Classification in the presence of label noise: a survey.
\newblock {\em IEEE transactions on neural networks and learning systems},
  25(5):845--869, 2013.

\bibitem{xu2020adversarial}
Han Xu, Yao Ma, Hao-Chen Liu, Debayan Deb, Hui Liu, Ji-Liang Tang, and Anil~K
  Jain.
\newblock Adversarial attacks and defenses in images, graphs and text: A
  review.
\newblock {\em International Journal of Automation and Computing},
  17(2):151--178, 2020.

\bibitem{koh2021wilds}
Pang~Wei Koh, Shiori Sagawa, Sang~Michael Xie, Marvin Zhang, Akshay
  Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard~Lanas Phillips, Irena
  Gao, Tony Lee, et~al.
\newblock Wilds: A benchmark of in-the-wild distribution shifts.
\newblock In {\em International Conference on Machine Learning}, pages
  5637--5664. PMLR, 2021.

\bibitem{torralba2011unbiased}
Antonio Torralba and Alexei~A Efros.
\newblock Unbiased look at dataset bias.
\newblock In {\em CVPR 2011}, pages 1521--1528. IEEE, 2011.

\bibitem{chen2018my}
Irene Chen, Fredrik~D Johansson, and David Sontag.
\newblock Why is my classifier discriminatory?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3539--3550, 2018.

\bibitem{sagawa2019distributionally}
Shiori Sagawa, Pang~Wei Koh, Tatsunori~B Hashimoto, and Percy Liang.
\newblock Distributionally robust neural networks for group shifts: On the
  importance of regularization for worst-case generalization.
\newblock {\em arXiv preprint arXiv:1911.08731}, 2019.

\bibitem{nguyen2017variational}
Cuong~V Nguyen, Yingzhen Li, Thang~D Bui, and Richard~E Turner.
\newblock Variational continual learning.
\newblock {\em arXiv preprint arXiv:1710.10628}, 2017.

\bibitem{koh2017understanding}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In {\em ICML}, 2017.

\bibitem{guo2019certified}
Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens van~der Maaten.
\newblock Certified data removal from machine learning models.
\newblock {\em arXiv preprint arXiv:1911.03030}, 2019.

\bibitem{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proceedings of the national academy of sciences},
  114(13):3521--3526, 2017.

\bibitem{henn2021principled}
Thomas Henn, Yasukazu Sakamoto, Cl{\'e}ment Jacquet, Shunsuke Yoshizawa,
  Masamichi Andou, Stephen Tchen, Ryosuke Saga, Hiroyuki Ishihara, Katsuhiko
  Shimizu, Yingzhen Li, et~al.
\newblock A principled approach to failure analysis and model repairment:
  Demonstration in medical imaging.
\newblock In {\em International Conference on Medical Image Computing and
  Computer-Assisted Intervention}, pages 509--518. Springer, 2021.

\bibitem{parisi2019continual}
German~I Parisi, Ronald Kemker, Jose~L Part, Christopher Kanan, and Stefan
  Wermter.
\newblock Continual lifelong learning with neural networks: A review.
\newblock {\em Neural Networks}, 113:54--71, 2019.

\bibitem{mackay1992bayesian}
David~JC MacKay.
\newblock {\em Bayesian methods for adaptive models}.
\newblock PhD thesis, California Institute of Technology, 1992.

\bibitem{jordan1998introduction}
Michael~I Jordan, Zoubin Ghahramani, Tommi~S Jaakkola, and Lawrence~K Saul.
\newblock An introduction to variational methods for graphical models.
\newblock In {\em Learning in graphical models}, pages 105--161. Springer,
  1998.

\bibitem{blundell2015weight}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural networks.
\newblock In {\em International conference on machine learning}, pages
  1613--1622. PMLR, 2015.

\bibitem{amari1998natural}
Shun-Ichi Amari.
\newblock Natural gradient works efficiently in learning.
\newblock {\em Neural computation}, 10(2):251--276, 1998.

\bibitem{kschischang2001factor}
Frank~R Kschischang, Brendan~J Frey, and H-A Loeliger.
\newblock Factor graphs and the sum-product algorithm.
\newblock {\em IEEE Transactions on information theory}, 47(2):498--519, 2001.

\bibitem{zhu2020modifyingmemories}
Chen Zhu, Ankit Singh~Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li,
  Felix Yu, and Sanjiv Kumar.
\newblock Modifying memories in transformer models.
\newblock {\em arXiv preprint arXiv:2012.00363}, 2020.

\bibitem{sinitsin2020editable}
Anton Sinitsin, Vsevolod Plokhotnyuk, Sergei Popov, and Artem Babenko.
\newblock Editable neural networks.
\newblock {\em arXiv preprint arXiv:2004.00345}, 2020.

\bibitem{cao2021editing}
Nicola~De Cao, Wilker Aziz, and Ivan Titov.
\newblock Editing factual knowledge in language models.
\newblock 2021.

\bibitem{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1126--1135. PMLR, 2017.

\bibitem{mitchell2021fast}
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher~D.
  Manning.
\newblock Fast model editing at scale.
\newblock {\em CoRR}, 2021.

\bibitem{santurkar2021editing}
Shibani Santurkar, Dimitris Tsipras, Mahalaxmi Elango, David Bau, Antonio
  Torralba, and Aleksander Madry.
\newblock Editing a classifier by rewriting its prediction rules.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem{schwarz2018progress}
Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka
  Grabska-Barwinska, Yee~Whye Teh, Razvan Pascanu, and Raia Hadsell.
\newblock Progress \& compress: A scalable framework for continual learning.
\newblock In {\em International Conference on Machine Learning}, pages
  4528--4537. PMLR, 2018.

\bibitem{pan2020continual}
Pingbo Pan, Siddharth Swaroop, Alexander Immer, Runa Eschenhagen, Richard~E
  Turner, and Mohammad~Emtiyaz Khan.
\newblock Continual deep learning by functional regularisation of memorable
  past.
\newblock In {\em Advances in Neural Processing Information Systems}, 2020.

\bibitem{loo2021generalized}
Noel Loo, Siddharth Swaroop, and Richard~E Turner.
\newblock Generalized variational continual learning.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{zenke2017continual}
Friedemann Zenke, Ben Poole, and Surya Ganguli.
\newblock Continual learning through synaptic intelligence.
\newblock In {\em International Conference on Machine Learning}, pages
  3987--3995. PMLR, 2017.

\bibitem{farajtabar2020orthogonal}
Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li.
\newblock Orthogonal gradient descent for continual learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 3762--3773. PMLR, 2020.

\bibitem{koh2019accuracy}
Pang Wei~W Koh, Kai-Siang Ang, Hubert Teo, and Percy~S Liang.
\newblock On the accuracy of influence functions for measuring group effects.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5254--5264, 2019.

\bibitem{barshan2020relatif}
Elnaz Barshan, Marc-Etienne Brunet, and Gintare~Karolina Dziugaite.
\newblock Relatif: Identifying explanatory training examples via relative
  influence.
\newblock {\em arXiv preprint arXiv:2003.11630}, 2020.

\bibitem{giordano2019swiss}
Ryan Giordano, William Stephenson, Runjing Liu, Michael Jordan, and Tamara
  Broderick.
\newblock A swiss army infinitesimal jackknife.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1139--1147, 2019.

\bibitem{hara2019data}
Satoshi Hara, Atsushi Nitanda, and Takanori Maehara.
\newblock Data cleansing for models trained with sgd.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4213--4222, 2019.

\bibitem{khanna2019interpreting}
Rajiv Khanna, Been Kim, Joydeep Ghosh, and Sanmi Koyejo.
\newblock Interpreting black box predictions using fisher kernels.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 3382--3390, 2019.

\bibitem{warnecke2021machine}
Alexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck.
\newblock Machine unlearning of features and labels.
\newblock {\em arXiv preprint arXiv:2108.11577}, 2021.

\bibitem{ghorbani2019data}
Amirata Ghorbani and James Zou.
\newblock Data shapley: Equitable valuation of data for machine learning.
\newblock In {\em ICML}, 2019.

\bibitem{ancona2019explaining}
Marco Ancona, Cengiz Oztireli, and Markus Gross.
\newblock Explaining deep neural networks with a polynomial time algorithm for
  shapley value approximation.
\newblock In {\em International Conference on Machine Learning}, pages
  272--281. PMLR, 2019.

\bibitem{jia2019towards}
Ruoxi Jia, David Dao, Boxin Wang, Frances~Ann Hubis, Nick Hynes, Nezihe~Merve
  G{\"u}rel, Bo~Li, Ce~Zhang, Dawn Song, and Costas~J Spanos.
\newblock Towards efficient data valuation based on the shapley value.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1167--1176. PMLR, 2019.

\bibitem{chakarov2016debugging}
Aleksandar Chakarov, Aditya Nori, Sriram Rajamani, Shayak Sen, and Deepak
  Vijaykeerthy.
\newblock Debugging machine learning tasks.
\newblock {\em arXiv preprint arXiv:1603.07292}, 2016.

\bibitem{bourtoule2019machine}
Lucas Bourtoule, Varun Chandrasekaran, Christopher Choquette-Choo, Hengrui Jia,
  Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot.
\newblock Machine unlearning.
\newblock {\em arXiv preprint arXiv:1912.03817}, 2019.

\bibitem{ginart2019making}
Antonio Ginart, Melody Guan, Gregory Valiant, and James~Y Zou.
\newblock Making ai forget you: Data deletion in machine learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3518--3531, 2019.

\bibitem{izzo2020approximate}
Zachary Izzo, Mary~Anne Smart, Kamalika Chaudhuri, and James Zou.
\newblock Approximate data deletion from machine learning models: Algorithms
  and evaluations.
\newblock {\em arXiv preprint arXiv:2002.10077}, 2020.

\bibitem{neel2020descent}
Seth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi.
\newblock Descent-to-delete: Gradient-based methods for machine unlearning.
\newblock {\em arXiv preprint arXiv:2007.02923}, 2020.

\bibitem{gupta2021adaptive}
Varun Gupta, Christopher Jung, Seth Neel, Aaron Roth, Saeed Sharifi-Malvajerdi,
  and Chris Waites.
\newblock Adaptive machine unlearning.
\newblock {\em arXiv preprint arXiv:2106.04378}, 2021.

\bibitem{nguyen2020variational}
Quoc~Phong Nguyen, Bryan Kian~Hsiang Low, and Patrick Jaillet.
\newblock Variational bayesian unlearning.
\newblock {\em arXiv preprint arXiv:2010.12883}, 2020.

\bibitem{agarwal2016second}
Naman Agarwal, Brian Bullins, and Elad Hazan.
\newblock Second-order stochastic optimization in linear time.
\newblock {\em stat}, 1050:15, 2016.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{northcutt2021confidentlearning}
Curtis~G. Northcutt, Lu~Jiang, and Isaac~L. Chuang.
\newblock Confident learning: Estimating uncertainty in dataset labels.
\newblock {\em Journal of Artificial Intelligence Research (JAIR)},
  70:1373--1411, 2021.

\bibitem{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em arXiv preprint arXiv:1412.6572}, 2014.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\end{thebibliography}
