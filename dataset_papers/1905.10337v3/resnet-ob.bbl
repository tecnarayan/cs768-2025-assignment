\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[{Allen-Zhu} and Li(2019)]{al19-rnngen}
Zeyuan {Allen-Zhu} and Yuanzhi Li.
\newblock Can {SGD} learn recurrent neural networks with provable
  generalization?
\newblock In \emph{NeurIPS}, 2019.
\newblock Full version available at \url{http://arxiv.org/abs/1902.01028}.

\bibitem[{Allen-Zhu} and Li(2020{\natexlab{a}})]{AL2020-densenet}
Zeyuan {Allen-Zhu} and Yuanzhi Li.
\newblock {Backward Feature Correction: How Deep Learning Performs Deep
  Learning}.
\newblock \emph{arXiv preprint}, January 2020{\natexlab{a}}.
\newblock Full version available at \url{http://arxiv.org/abs/2001.04413}.

\bibitem[{Allen-Zhu} and Li(2020{\natexlab{b}})]{allen2020feature}
Zeyuan {Allen-Zhu} and Yuanzhi Li.
\newblock Feature purification: How adversarial training performs robust deep
  learning.
\newblock \emph{arXiv preprint arXiv:2005.10190}, 2020{\natexlab{b}}.

\bibitem[{Allen-Zhu} et~al.(2019{\natexlab{a}}){Allen-Zhu}, Li, and
  Liang]{all18}
Zeyuan {Allen-Zhu}, Yuanzhi Li, and Yingyu Liang.
\newblock {Learning and Generalization in Overparameterized Neural Networks,
  Going Beyond Two Layers}.
\newblock In \emph{NeurIPS}, 2019{\natexlab{a}}.
\newblock Full version available at \url{http://arxiv.org/abs/1811.04918}.

\bibitem[{Allen-Zhu} et~al.(2019{\natexlab{b}}){Allen-Zhu}, Li, and
  Song]{als18}
Zeyuan {Allen-Zhu}, Yuanzhi Li, and Zhao Song.
\newblock On the convergence rate of training recurrent neural networks.
\newblock In \emph{NeurIPS}, 2019{\natexlab{b}}.
\newblock Full version available at \url{http://arxiv.org/abs/1810.12065}.

\bibitem[{Allen-Zhu} et~al.(2019{\natexlab{c}}){Allen-Zhu}, Li, and
  Song]{als18dnn}
Zeyuan {Allen-Zhu}, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{ICML}, 2019{\natexlab{c}}.
\newblock Full version available at \url{http://arxiv.org/abs/1811.03962}.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock \emph{arXiv preprint arXiv:1904.11955}, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, and
  Wang]{arora2019finegrained}
Sanjeev Arora, Simon~S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock \emph{CoRR}, abs/1901.08584, 2019{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/1901.08584}.

\bibitem[Bakshi et~al.(2018)Bakshi, Jayaram, and Woodruff]{bakshi2018learning}
Ainesh Bakshi, Rajesh Jayaram, and David~P Woodruff.
\newblock Learning two layer rectified neural networks in polynomial time.
\newblock \emph{arXiv preprint arXiv:1811.01885}, 2018.

\bibitem[Boob and Lan(2017)]{boob2017theoretical}
Digvijay Boob and Guanghui Lan.
\newblock Theoretical properties of the global optimizer of two layer neural
  network.
\newblock \emph{arXiv preprint arXiv:1710.11241}, 2017.

\bibitem[Brutzkus and Globerson(2017)]{brutzkus2017globally}
Alon Brutzkus and Amir Globerson.
\newblock Globally optimal gradient descent for a convnet with gaussian inputs.
\newblock \emph{arXiv preprint arXiv:1702.07966}, 2017.

\bibitem[Daniely(2017)]{daniely2017sgd}
Amit Daniely.
\newblock Sgd learns the conjugate kernel class of the network.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2422--2430, 2017.

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and Singer]{dfs16}
Amit Daniely, Roy Frostig, and Yoram Singer.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 2253--2261, 2016.

\bibitem[Du et~al.(2018{\natexlab{a}})Du, Lee, Li, Wang, and
  Zhai]{du2018gradient2}
Simon~S Du, Jason~D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock \emph{arXiv preprint arXiv:1811.03804}, 2018{\natexlab{a}}.

\bibitem[Du et~al.(2018{\natexlab{b}})Du, Zhai, Poczos, and
  Singh]{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018{\natexlab{b}}.

\bibitem[Ge et~al.(2017)Ge, Lee, and Ma]{ge2017learning}
Rong Ge, Jason~D Lee, and Tengyu Ma.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock \emph{arXiv preprint arXiv:1711.00501}, 2017.

\bibitem[Ge et~al.(2019)Ge, Kuditipudi, Li, and Wang]{ge2019learning}
Rong Ge, Rohith Kuditipudi, Zhize Li, and Xiang Wang.
\newblock Learning two-layer neural networks with symmetric inputs.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Golowich et~al.(2018)Golowich, Rakhlin, and Shamir]{golowich2018size}
Noah Golowich, Alexander Rakhlin, and Ohad Shamir.
\newblock Size-independent sample complexity of neural networks.
\newblock In \emph{Proceedings of the Conference on Learning Theory}, 2018.

\bibitem[Graves et~al.(2013)Graves, Mohamed, and Hinton]{graves2013speech}
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In \emph{Acoustics, speech and signal processing (icassp), 2013 ieee
  international conference on}, pages 6645--6649. IEEE, 2013.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem[Kawaguchi(2016)]{kawaguchi2016deep}
Kenji Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  586--594, 2016.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem[Li and Dou(2020)]{li2020can}
Yuanzhi Li and Zehao Dou.
\newblock When can wasserstein gans minimize wasserstein distance?
\newblock \emph{arXiv preprint arXiv:2003.04033}, 2020.

\bibitem[Li and Liang(2017)]{li2017provable}
Yuanzhi Li and Yingyu Liang.
\newblock Provable alternating gradient descent for non-negative matrix
  factorization with strong correlations.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 2062--2070. JMLR. org, 2017.

\bibitem[Li and Liang(2018)]{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Li and Yuan(2017)]{li2017convergence}
Yuanzhi Li and Yang Yuan.
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  597--607, 2017.

\bibitem[Li et~al.(2016)Li, Liang, and Risteski]{li2016recovery}
Yuanzhi Li, Yingyu Liang, and Andrej Risteski.
\newblock Recovery guarantee of non-negative matrix factorization via
  alternating updates.
\newblock In \emph{Advances in neural information processing systems}, pages
  4987--4995, 2016.

\bibitem[Li et~al.(2018)Li, Ma, and Zhang]{li2017algorithmic}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In \emph{COLT}, 2018.

\bibitem[Ma(2017)]{macs229t}
Tengyu Ma.
\newblock {CS229T/STAT231: Statistical Learning Theory (Fall 2017)}.
\newblock
  \url{https://web.stanford.edu/class/cs229t/scribe_notes/10_17_final.pdf},
  October 2017.
\newblock accessed May 2019.

\bibitem[{Martin J. Wainwright}(2015)]{wainwright2015}
{Martin J. Wainwright}.
\newblock {Basic tail and concentration bounds}.
\newblock
  \url{https://www.stat.berkeley.edu/~mjwain/stat210b/Chap2_TailBounds_Jan22_2015.pdf},
  2015.
\newblock Online; accessed Oct 2018.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and
  Srebro]{neyshabur2015norm}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{Conference on Learning Theory}, pages 1376--1401, 2015.

\bibitem[Recht et~al.(2018)Recht, Roelofs, Schmidt, and
  Shankar]{recht2018cifar}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock {Do CIFAR-10 Classifiers Generalize to CIFAR-10?}
\newblock \emph{arXiv preprint arXiv:1806.00451}, 2018.

\bibitem[Rudelson and Vershynin(2010)]{rudelson2010non}
Mark Rudelson and Roman Vershynin.
\newblock Non-asymptotic theory of random matrices: extreme singular values.
\newblock In \emph{Proceedings of the International Congress of Mathematicians
  2010 (ICM 2010) (In 4 Volumes) Vol. I: Plenary Lectures and Ceremonies Vols.
  II--IV: Invited Lectures}, pages 1576--1602. World Scientific, 2010.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484, 2016.

\bibitem[Soltanolkotabi et~al.(2017)Soltanolkotabi, Javanmard, and
  Lee]{soltanolkotabi2017theoretical}
Mahdi Soltanolkotabi, Adel Javanmard, and Jason~D Lee.
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock \emph{arXiv preprint arXiv:1707.04926}, 2017.

\bibitem[Soudry and Carmon(2016)]{soudry2016no}
Daniel Soudry and Yair Carmon.
\newblock No bad local minima: Data independent training error guarantees for
  multilayer neural networks.
\newblock \emph{arXiv preprint arXiv:1605.08361}, 2016.

\bibitem[Tian(2017)]{tian2017analytical}
Yuandong Tian.
\newblock An analytical formula of population gradient for two-layered relu
  network and its applications in convergence and critical point analysis.
\newblock \emph{arXiv preprint arXiv:1703.00560}, 2017.

\bibitem[Vempala and Wilmes(2018)]{vempala2018polynomial}
Santosh Vempala and John Wilmes.
\newblock Polynomial convergence of gradient descent for training
  one-hidden-layer neural networks.
\newblock \emph{arXiv preprint arXiv:1805.02677}, 2018.

\bibitem[Wei et~al.(2018)Wei, Lee, Liu, and Ma]{wei2018margin}
Colin Wei, Jason~D Lee, Qiang Liu, and Tengyu Ma.
\newblock On the margin theory of feedforward neural networks.
\newblock \emph{arXiv preprint arXiv:1810.05369}, 2018.

\bibitem[Xie et~al.(2016)Xie, Liang, and Song]{xie2016diversity}
Bo~Xie, Yingyu Liang, and Le~Song.
\newblock Diversity leads to generalization in neural networks.
\newblock \emph{arXiv preprint Arxiv:1611.03131}, 2016.

\bibitem[Yang(2019)]{yang2019scaling}
Greg Yang.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian
  process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock \emph{arXiv preprint arXiv:1902.04760}, 2019.

\bibitem[Zhang et~al.(2016)Zhang, Lee, and Jordan]{zhang2016l1}
Yuchen Zhang, Jason~D Lee, and Michael~I Jordan.
\newblock l1-regularized neural networks are improperly learnable in polynomial
  time.
\newblock In \emph{International Conference on Machine Learning}, pages
  993--1001, 2016.

\bibitem[Zhong et~al.(2017)Zhong, Song, Jain, Bartlett, and
  Dhillon]{zhong2017recovery}
Kai Zhong, Zhao Song, Prateek Jain, Peter~L Bartlett, and Inderjit~S Dhillon.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock \emph{arXiv preprint arXiv:1706.03175}, 2017.

\bibitem[Zou et~al.(2018)Zou, Cao, Zhou, and Gu]{zou2018stochastic}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu
  networks.
\newblock \emph{arXiv preprint arXiv:1811.08888}, 2018.

\end{thebibliography}
