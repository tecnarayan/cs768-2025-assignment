\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Azizian \& Lelarge(2021)Azizian and Lelarge]{azizian2021expressive}
Azizian, W. and Lelarge, M.
\newblock Expressive power of invariant and equivariant graph neural networks.
\newblock In \emph{ICLR 2021-International Conference on Learning Representations}, 2021.

\bibitem[B{\"o}ker et~al.(2024)B{\"o}ker, Levie, Huang, Villar, and Morris]{boker2024fine}
B{\"o}ker, J., Levie, R., Huang, N., Villar, S., and Morris, C.
\newblock Fine-grained expressivity of graph neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Chen et~al.(2022)Chen, Lim, M{\'e}moli, Wan, and Wang]{chen2022weisfeiler}
Chen, S., Lim, S., M{\'e}moli, F., Wan, Z., and Wang, Y.
\newblock Weisfeiler-{L}ehman meets {G}romov-{W}asserstein.
\newblock In \emph{International Conference on Machine Learning}, pp.\  3371--3416. PMLR, 2022.

\bibitem[Chen et~al.(2023)Chen, Lim, M{\'e}moli, Wan, and Wang]{chen2023weisfeiler}
Chen, S., Lim, S., M{\'e}moli, F., Wan, Z., and Wang, Y.
\newblock The {W}eisfeiler-{L}ehman distance: Reinterpretation and connection with {GNN}s.
\newblock In \emph{Topological, Algebraic and Geometric Learning Workshops 2023}, pp.\  404--425. PMLR, 2023.

\bibitem[Chen et~al.(2020)Chen, Chen, Villar, and Bruna]{chen2020can}
Chen, Z., Chen, L., Villar, S., and Bruna, J.
\newblock Can graph neural networks count substructures?
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 10383--10395, 2020.

\bibitem[Choromanski et~al.(2022)Choromanski, Lin, Chen, Zhang, Sehanobish, Likhosherstov, Parker-Holder, Sarlos, Weller, and Weingarten]{choromanski22blocktoeplitz}
Choromanski, K., Lin, H., Chen, H., Zhang, T., Sehanobish, A., Likhosherstov, V., Parker-Holder, J., Sarlos, T., Weller, A., and Weingarten, T.
\newblock From {block-Toeplitz} matrices to differential equations on graphs: towards a general theory for scalable masked transformers.
\newblock In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), \emph{Proceedings of the 39th International Conference on Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  3962--3983. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/choromanski22a.html}.

\bibitem[Coifman \& Lafon(2006)Coifman and Lafon]{coifman2006diffusion}
Coifman, R.~R. and Lafon, S.
\newblock Diffusion maps.
\newblock \emph{Applied and computational harmonic analysis}, 21\penalty0 (1):\penalty0 5--30, 2006.

\bibitem[delle Rose et~al.(2023)delle Rose, Kozachinskiy, Rojas, Petrache, and Barcelo]{rose2023iterations}
delle Rose, V., Kozachinskiy, A., Rojas, C., Petrache, M., and Barcelo, P.
\newblock Three iterations of (d \ensuremath{-} 1)-{WL} test distinguish non isometric clouds of d-dimensional points.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=9yhYcjsdab}.

\bibitem[Dwivedi \& Bresson(2021)Dwivedi and Bresson]{dwivedi2020generalization}
Dwivedi, V.~P. and Bresson, X.
\newblock A generalization of transformer networks to graphs.
\newblock \emph{AAAI Workshop on Deep Learning on Graphs: Methods and Applications}, 2021.

\bibitem[Dwivedi et~al.(2023)Dwivedi, Joshi, Luu, Laurent, Bengio, and Bresson]{dwivedi2023benchmarking}
Dwivedi, V.~P., Joshi, C.~K., Luu, A.~T., Laurent, T., Bengio, Y., and Bresson, X.
\newblock Benchmarking graph neural networks.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (43):\penalty0 1--48, 2023.

\bibitem[Fanuel et~al.(2018)Fanuel, Ala{\'\i}z, Fern{\'a}ndez, and Suykens]{fanuel2018magnetic}
Fanuel, M., Ala{\'\i}z, C.~M., Fern{\'a}ndez, {\'A}., and Suykens, J.~A.
\newblock Magnetic eigenmaps for the visualization of directed networks.
\newblock \emph{Applied and Computational Harmonic Analysis}, 44\penalty0 (1):\penalty0 189--199, 2018.

\bibitem[Geisler et~al.(2023)Geisler, Li, Mankowitz, Cemgil, G{\"u}nnemann, and Paduraru]{geisler2023transformers}
Geisler, S., Li, Y., Mankowitz, D.~J., Cemgil, A.~T., G{\"u}nnemann, S., and Paduraru, C.
\newblock Transformers meet directed graphs.
\newblock In \emph{International Conference on Machine Learning}, pp.\  11144--11172. PMLR, 2023.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and Dahl]{gilmer2017neural}
Gilmer, J., Schoenholz, S.~S., Riley, P.~F., Vinyals, O., and Dahl, G.~E.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{International conference on machine learning}, pp.\  1263--1272. PMLR, 2017.

\bibitem[Hammond et~al.(2011)Hammond, Vandergheynst, and Gribonval]{hammond2011wavelets}
Hammond, D.~K., Vandergheynst, P., and Gribonval, R.
\newblock Wavelets on graphs via spectral graph theory.
\newblock \emph{Applied and Computational Harmonic Analysis}, 30\penalty0 (2):\penalty0 129--150, 2011.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016gaussian}
Hendrycks, D. and Gimpel, K.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Huang \& Villar(2021)Huang and Villar]{huang2021short}
Huang, N.~T. and Villar, S.
\newblock A short tutorial on the {Weisfeiler-Lehman} test and its variants.
\newblock In \emph{ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pp.\  8533--8537. IEEE, 2021.

\bibitem[Huang et~al.(2024)Huang, Lu, Robinson, Yang, Zhang, Jegelka, and Li]{huang2023stability}
Huang, Y., Lu, W., Robinson, J., Yang, Y., Zhang, M., Jegelka, S., and Li, P.
\newblock On the stability of expressive positional encodings for graph neural networks.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Jin et~al.(2023)Jin, Zhang, Meng, and Han]{jin2023edgeformers}
Jin, B., Zhang, Y., Meng, Y., and Han, J.
\newblock Edgeformers: Graph-empowered transformers for representation learning on textual-edge networks.
\newblock \emph{arXiv preprint arXiv:2302.11050}, 2023.

\bibitem[Kreuzer et~al.(2021)Kreuzer, Beaini, Hamilton, L{\'e}tourneau, and Tossou]{kreuzer2021rethinking}
Kreuzer, D., Beaini, D., Hamilton, W., L{\'e}tourneau, V., and Tossou, P.
\newblock Rethinking graph transformers with spectral attention.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 21618--21629, 2021.

\bibitem[Lim et~al.(2022)Lim, Robinson, Zhao, Smidt, Sra, Maron, and Jegelka]{lim2022sign}
Lim, D., Robinson, J.~D., Zhao, L., Smidt, T., Sra, S., Maron, H., and Jegelka, S.
\newblock Sign and basis invariant networks for spectral graph representation learning.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Ma et~al.(2023)Ma, Lin, Lim, Romero-Soriano, Dokania, Coates, Torr, and Lim]{ma2023inductive}
Ma, L., Lin, C., Lim, D., Romero-Soriano, A., Dokania, P.~K., Coates, M., Torr, P., and Lim, S.-N.
\newblock Graph inductive biases in transformers without message passing.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), \emph{Proceedings of the 40th International Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  23321--23337. PMLR, 23--29 Jul 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/ma23c.html}.

\bibitem[Maron et~al.(2019{\natexlab{a}})Maron, Ben-Hamu, Serviansky, and Lipman]{maron2019provably}
Maron, H., Ben-Hamu, H., Serviansky, H., and Lipman, Y.
\newblock Provably powerful graph networks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019{\natexlab{a}}.

\bibitem[Maron et~al.(2019{\natexlab{b}})Maron, Ben-Hamu, Shamir, and Lipman]{maron2019invariant}
Maron, H., Ben-Hamu, H., Shamir, N., and Lipman, Y.
\newblock Invariant and equivariant graph networks.
\newblock In \emph{International Conference on Learning Representations}, 2019{\natexlab{b}}.

\bibitem[Mialon et~al.(2021)Mialon, Chen, Selosse, and Mairal]{mialon2021graphit}
Mialon, G., Chen, D., Selosse, M., and Mairal, J.
\newblock Graphit: Encoding graph structure in transformers, 2021.

\bibitem[M{\"u}ller et~al.(2024)M{\"u}ller, Galkin, Morris, and Ramp{\'a}{\v{s}}ek]{muller2023attending}
M{\"u}ller, L., Galkin, M., Morris, C., and Ramp{\'a}{\v{s}}ek, L.
\newblock Attending to graph transformers.
\newblock \emph{Transactions on Machine Learning Research}, 2024.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=HhbqHBBrfZ}.

\bibitem[Ramp{\'a}{\v{s}}ek et~al.(2022)Ramp{\'a}{\v{s}}ek, Galkin, Dwivedi, Luu, Wolf, and Beaini]{rampavsek2022recipe}
Ramp{\'a}{\v{s}}ek, L., Galkin, M., Dwivedi, V.~P., Luu, A.~T., Wolf, G., and Beaini, D.
\newblock Recipe for a general, powerful, scalable graph transformer.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 14501--14515, 2022.

\bibitem[Shaw et~al.(2018)Shaw, Uszkoreit, and Vaswani]{shaw2018self}
Shaw, P., Uszkoreit, J., and Vaswani, A.
\newblock Self-attention with relative position representations.
\newblock In \emph{Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)}, pp.\  464--468, 2018.

\bibitem[Spielman(2019)]{spielman2019sagt}
Spielman, D.
\newblock Spectral and algebraic graph theory.
\newblock Available at http://cs-www.cs.yale.edu/homes/spielman/sagt/sagt.pdf (2021/12/01), 2019.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang \& Zhang(2024)Wang and Zhang]{wang2024brec}
Wang, Y. and Zhang, M.
\newblock An empirical study of realized {GNN} expressiveness.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2024.

\bibitem[Weisfeiler \& Leman(1968)Weisfeiler and Leman]{weisfeiler1968reduction}
Weisfeiler, B. and Leman, A.
\newblock The reduction of a graph to canonical form and the algebra which appears therein.
\newblock \emph{nti, Series}, 2\penalty0 (9):\penalty0 12--16, 1968.

\bibitem[Xu et~al.(2018)Xu, Hu, Leskovec, and Jegelka]{xu2018powerful}
Xu, K., Hu, W., Leskovec, J., and Jegelka, S.
\newblock How powerful are graph neural networks?
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Ying et~al.(2021)Ying, Cai, Luo, Zheng, Ke, He, Shen, and Liu]{ying2021transformers}
Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., and Liu, T.-Y.
\newblock Do transformers really perform badly for graph representation?
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 28877--28888, 2021.

\bibitem[Zaheer et~al.(2017)Zaheer, Kottur, Ravanbakhsh, Poczos, Salakhutdinov, and Smola]{zaheer2017deep}
Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R.~R., and Smola, A.~J.
\newblock Deep sets.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Zhang et~al.(2023)Zhang, Luo, Wang, and He]{zhang2023rethinking}
Zhang, B., Luo, S., Wang, L., and He, D.
\newblock Rethinking the expressive power of {GNN}s via graph biconnectivity.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Zhu et~al.(2023)Zhu, Wen, Song, Wang, and Zheng]{zhu2023structural}
Zhu, W., Wen, T., Song, G., Wang, L., and Zheng, B.
\newblock On structural expressive power of graph transformers.
\newblock In \emph{Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, KDD '23, pp.\  3628–3637, New York, NY, USA, 2023. Association for Computing Machinery.
\newblock ISBN 9798400701030.
\newblock \doi{10.1145/3580305.3599451}.
\newblock URL \url{https://doi.org/10.1145/3580305.3599451}.

\end{thebibliography}
