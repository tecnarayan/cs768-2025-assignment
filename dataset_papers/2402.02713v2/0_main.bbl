\begin{thebibliography}{111}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.~L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Alghamdi et~al.(2019)Alghamdi, Elgazzar, Bayoumi, Sharaf, and Shah]{alghamdi2019forecasting}
Alghamdi, T., Elgazzar, K., Bayoumi, M., Sharaf, T., and Shah, S.
\newblock Forecasting traffic congestion using arima modeling.
\newblock In \emph{2019 15th international wireless communications \& mobile computing conference (IWCMC)}, pp.\  1227--1232. IEEE, 2019.

\bibitem[Anguita et~al.(2013)Anguita, Ghio, Oneto, Parra, Reyes-Ortiz, et~al.]{anguita2013public}
Anguita, D., Ghio, A., Oneto, L., Parra, X., Reyes-Ortiz, J.~L., et~al.
\newblock A public domain dataset for human activity recognition using smartphones.
\newblock In \emph{Esann}, volume~3, pp.\ ~3, 2013.

\bibitem[Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022.

\bibitem[Besta et~al.(2023)Besta, Blach, Kubicek, Gerstenberger, Gianinazzi, Gajda, Lehmann, Podstawski, Niewiadomski, Nyczyk, et~al.]{besta2023graph}
Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gianinazzi, L., Gajda, J., Lehmann, T., Podstawski, M., Niewiadomski, H., Nyczyk, P., et~al.
\newblock Graph of thoughts: Solving elaborate problems with large language models.
\newblock \emph{arXiv preprint arXiv:2308.09687}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks}
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y.~T., Li, Y., Lundberg, S., et~al.
\newblock Sparks of artificial general intelligence: Early experiments with gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.

\bibitem[Cao et~al.(2023)Cao, Jia, Arik, Pfister, Zheng, Ye, and Liu]{cao2023tempo}
Cao, D., Jia, F., Arik, S.~O., Pfister, T., Zheng, Y., Ye, W., and Liu, Y.
\newblock Tempo: Prompt-based generative pre-trained transformer for time series forecasting.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Chang et~al.(2023)Chang, Peng, and Chen]{chang2023llm4ts}
Chang, C., Peng, W.-C., and Chen, T.-F.
\newblock Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms.
\newblock \emph{arXiv preprint arXiv:2308.08469}, 2023.

\bibitem[Chatterjee et~al.(2023)Chatterjee, Mitra, and Chakraborty]{chatterjee2023amicron}
Chatterjee, S., Mitra, B., and Chakraborty, S.
\newblock Amicron: A framework for generating annotations for human activity recognition with granular micro-activities.
\newblock \emph{arXiv preprint arXiv:2306.13149}, 2023.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Long, Shen, and Jiang]{chen2023prompt}
Chen, S., Long, G., Shen, T., and Jiang, J.
\newblock Prompt federated learning for weather forecasting: toward foundation models on meteorological data.
\newblock In \emph{Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence}, pp.\  3532--3540, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Wang, and Xu]{chen2023gatgpt}
Chen, Y., Wang, X., and Xu, G.
\newblock Gatgpt: A pre-trained large language model with graph attention network for spatiotemporal imputation.
\newblock \emph{arXiv preprint arXiv:2311.14332}, 2023{\natexlab{b}}.

\bibitem[Chen et~al.(2024)Chen, Mao, Li, Jin, Wen, Wei, Wang, Yin, Fan, Liu, et~al.]{chen2023exploring}
Chen, Z., Mao, H., Li, H., Jin, W., Wen, H., Wei, X., Wang, S., Yin, D., Fan, W., Liu, H., et~al.
\newblock Exploring the potential of large language models (llms) in learning on graphs.
\newblock \emph{ACM SIGKDD Explorations Newsletter}, 25\penalty0 (2):\penalty0 42--61, 2024.

\bibitem[Cheng \& Chin(2024)Cheng and Chin]{anonymous2024sociodojo}
Cheng, J. and Chin, P.
\newblock Sociodojo: Building lifelong analytical agents with real-world text and time series.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=s9z0HzWJJp}.

\bibitem[Cheng et~al.(2024)Cheng, Zhang, Zhang, Meng, Hong, Li, Wang, Wang, Yin, Zhao, et~al.]{cheng2024exploring}
Cheng, Y., Zhang, C., Zhang, Z., Meng, X., Hong, S., Li, W., Wang, Z., Wang, Z., Yin, F., Zhao, J., et~al.
\newblock Exploring large language model based intelligent agents: Definitions, methods, and prospects.
\newblock \emph{arXiv preprint arXiv:2401.03428}, 2024.

\bibitem[Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2023palm}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (240):\penalty0 1--113, 2023.

\bibitem[Da et~al.(2023)Da, Gao, Mei, and Wei]{da2023llm}
Da, L., Gao, M., Mei, H., and Wei, H.
\newblock Llm powered sim-to-real transfer for traffic signal control.
\newblock \emph{arXiv preprint arXiv:2308.14284}, 2023.

\bibitem[Da et~al.(2024)Da, Liou, Chen, Zhou, Luo, Yang, and Wei]{da2023open}
Da, L., Liou, K., Chen, T., Zhou, X., Luo, X., Yang, Y., and Wei, H.
\newblock Open-ti: Open traffic intelligence with augmented language model.
\newblock \emph{International Journal of Machine Learning and Cybernetics}, pp.\  1--26, 2024.

\bibitem[Das et~al.(2024)Das, Kong, Sen, and Zhou]{das2023decoder}
Das, A., Kong, W., Sen, R., and Zhou, Y.
\newblock A decoder-only foundation model for time-series forecasting.
\newblock In \emph{the 41st International Conference on Machine Learning}, 2024.

\bibitem[Ekambaram et~al.(2024)Ekambaram, Jati, Nguyen, Dayama, Reddy, Gifford, and Kalagnanam]{ekambaram2024ttms}
Ekambaram, V., Jati, A., Nguyen, N.~H., Dayama, P., Reddy, C., Gifford, W.~M., and Kalagnanam, J.
\newblock Ttms: Fast multi-level tiny time mixers for improved zero-shot and few-shot forecasting of multivariate time series.
\newblock \emph{arXiv preprint arXiv:2401.03955}, 2024.

\bibitem[Fatouros et~al.(2024)Fatouros, Metaxas, Soldatos, and Kyriazis]{fatouros2024can}
Fatouros, G., Metaxas, K., Soldatos, J., and Kyriazis, D.
\newblock Can large language models beat wall street? unveiling the potential of ai in stock selection.
\newblock \emph{arXiv preprint arXiv:2401.03737}, 2024.

\bibitem[Fuller(2009)]{fuller2009introduction}
Fuller, W.~A.
\newblock \emph{Introduction to statistical time series}.
\newblock John Wiley \& Sons, 2009.

\bibitem[Gamboa(2017)]{gamboa2017deep}
Gamboa, J. C.~B.
\newblock Deep learning for time-series analysis.
\newblock \emph{arXiv preprint arXiv:1701.01887}, 2017.

\bibitem[Garg et~al.(2023)Garg, Farajtabar, Pouransari, Vemulapalli, Mehta, Tuzel, Shankar, and Faghri]{garg2023tic}
Garg, S., Farajtabar, M., Pouransari, H., Vemulapalli, R., Mehta, S., Tuzel, O., Shankar, V., and Faghri, F.
\newblock Tic-clip: Continual training of clip models.
\newblock In \emph{NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models}, 2023.

\bibitem[Garza \& Mergenthaler-Canseco(2023)Garza and Mergenthaler-Canseco]{garza2023timegpt}
Garza, A. and Mergenthaler-Canseco, M.
\newblock Timegpt-1.
\newblock \emph{arXiv preprint arXiv:2310.03589}, 2023.

\bibitem[Ge et~al.(2023)Ge, Hua, Mei, Tan, Xu, Li, Zhang, et~al.]{ge2023openagi}
Ge, Y., Hua, W., Mei, K., Tan, J., Xu, S., Li, Z., Zhang, Y., et~al.
\newblock Openagi: When llm meets domain experts.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Ghosh et~al.(2023)Ghosh, Sengupta, and Mitra]{ghosh2023spatio}
Ghosh, S., Sengupta, S., and Mitra, P.
\newblock Spatio-temporal storytelling? leveraging generative models for semantic trajectory analysis.
\newblock \emph{arXiv preprint arXiv:2306.13905}, 2023.

\bibitem[Gruver et~al.(2023)Gruver, Finzi, Qiu, and Wilson]{gruver2023large}
Gruver, N., Finzi, M., Qiu, S., and Wilson, A.~G.
\newblock Large language models are zero-shot time series forecasters.
\newblock \emph{Advances in neural information processing systems}, 2023.

\bibitem[Gu et~al.(2024)Gu, Zhu, Zhu, Chen, Tang, and Wang]{gu2023anomalygpt}
Gu, Z., Zhu, B., Zhu, G., Chen, Y., Tang, M., and Wang, J.
\newblock Anomalygpt: Detecting industrial anomalies using large vision-language models.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pp.\  1932--1940, 2024.

\bibitem[Gudibande et~al.(2023)Gudibande, Wallace, Snell, Geng, Liu, Abbeel, Levine, and Song]{gudibande2023false}
Gudibande, A., Wallace, E., Snell, C., Geng, X., Liu, H., Abbeel, P., Levine, S., and Song, D.
\newblock The false promise of imitating proprietary llms.
\newblock \emph{arXiv preprint arXiv:2305.15717}, 2023.

\bibitem[Hamilton(2020)]{hamilton2020time}
Hamilton, J.~D.
\newblock \emph{Time series analysis}.
\newblock Princeton university press, 2020.

\bibitem[Huang et~al.(2022)Huang, Abbeel, Pathak, and Mordatch]{huang2022language}
Huang, W., Abbeel, P., Pathak, D., and Mordatch, I.
\newblock Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.
\newblock In \emph{International Conference on Machine Learning}, pp.\  9118--9147. PMLR, 2022.

\bibitem[Jin et~al.(2023{\natexlab{a}})Jin, Koh, Wen, Zambon, Alippi, Webb, King, and Pan]{jin2023survey}
Jin, M., Koh, H.~Y., Wen, Q., Zambon, D., Alippi, C., Webb, G.~I., King, I., and Pan, S.
\newblock A survey on graph neural networks for time series: Forecasting, classification, imputation, and anomaly detection.
\newblock \emph{arXiv preprint arXiv:2307.03759}, 2023{\natexlab{a}}.

\bibitem[Jin et~al.(2023{\natexlab{b}})Jin, Wen, Liang, Zhang, Xue, Wang, Zhang, Wang, Chen, Li, et~al.]{jin2023large}
Jin, M., Wen, Q., Liang, Y., Zhang, C., Xue, S., Wang, X., Zhang, J., Wang, Y., Chen, H., Li, X., et~al.
\newblock Large models for time series and spatio-temporal data: A survey and outlook.
\newblock \emph{arXiv preprint arXiv:2310.10196}, 2023{\natexlab{b}}.

\bibitem[Jin et~al.(2024)Jin, Wang, Ma, Chu, Zhang, Shi, Chen, Liang, Li, Pan, et~al.]{jin2023time}
Jin, M., Wang, S., Ma, L., Chu, Z., Zhang, J.~Y., Shi, X., Chen, P.-Y., Liang, Y., Li, Y.-F., Pan, S., et~al.
\newblock Time-llm: Time series forecasting by reprogramming large language models.
\newblock In \emph{International Conference on Machine Learning}, 2024.

\bibitem[Kalekar et~al.(2004)]{kalekar2004time}
Kalekar, P.~S. et~al.
\newblock Time series forecasting using holt-winters exponential smoothing.
\newblock \emph{Kanwal Rekhi school of information Technology}, 4329008\penalty0 (13):\penalty0 1--13, 2004.

\bibitem[Kamarthi \& Prakash(2023)Kamarthi and Prakash]{kamarthi2023pems}
Kamarthi, H. and Prakash, B.~A.
\newblock Pems: Pre-trained epidmic time-series models.
\newblock \emph{arXiv preprint arXiv:2311.07841}, 2023.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kim et~al.(2024)Kim, Xu, McDuff, Breazeal, and Park]{kim2024health}
Kim, Y., Xu, X., McDuff, D., Breazeal, C., and Park, H.~W.
\newblock Health-llm: Large language models for health prediction via wearable sensor data.
\newblock \emph{arXiv preprint arXiv:2401.06866}, 2024.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and Iwasawa]{kojima2022large}
Kojima, T., Gu, S.~S., Reid, M., Matsuo, Y., and Iwasawa, Y.
\newblock Large language models are zero-shot reasoners.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 22199--22213, 2022.

\bibitem[Lai et~al.(2023)Lai, Xu, Zhang, Liu, and Xiong]{lai2023large}
Lai, S., Xu, Z., Zhang, W., Liu, H., and Xiong, H.
\newblock Large language models as traffic signal control agents: Capacity and opportunity.
\newblock \emph{arXiv preprint arXiv:2312.16044}, 2023.

\bibitem[Lee et~al.(2023)Lee, Liu, Ryu, Watkins, Du, Boutilier, Abbeel, Ghavamzadeh, and Gu]{lee2023aligning}
Lee, K., Liu, H., Ryu, M., Watkins, O., Du, Y., Boutilier, C., Abbeel, P., Ghavamzadeh, M., and Gu, S.~S.
\newblock Aligning text-to-image models using human feedback.
\newblock \emph{arXiv preprint arXiv:2302.12192}, 2023.

\bibitem[Li et~al.(2023)Li, Cheng, Zhao, Nie, and Wen]{li2023helma}
Li, J., Cheng, X., Zhao, W.~X., Nie, J.-Y., and Wen, J.-R.
\newblock Helma: A large-scale hallucination evaluation benchmark for large language models.
\newblock \emph{arXiv preprint arXiv:2305.11747}, 2023.

\bibitem[Li et~al.(2024)Li, Liu, Cheng, Arcucci, and Hong]{li2023frozen}
Li, J., Liu, C., Cheng, S., Arcucci, R., and Hong, S.
\newblock Frozen language model helps ecg zero-shot learning.
\newblock In \emph{Medical Imaging with Deep Learning}, pp.\  402--415. PMLR, 2024.

\bibitem[Liang et~al.(2023)Liang, Liu, Wang, and Zhao]{liang2023exploring}
Liang, Y., Liu, Y., Wang, X., and Zhao, Z.
\newblock Exploring large language models for human mobility prediction under public events.
\newblock \emph{arXiv preprint arXiv:2311.17351}, 2023.

\bibitem[Liang et~al.(2024)Liang, Wen, Nie, Jiang, Jin, Song, Pan, and Wen]{liang2024foundation}
Liang, Y., Wen, H., Nie, Y., Jiang, Y., Jin, M., Song, D., Pan, S., and Wen, Q.
\newblock Foundation models for time series analysis: A tutorial and survey.
\newblock In \emph{ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'24)}, 2024.

\bibitem[Liao \& Vaughan(2024)Liao and Vaughan]{liao2023ai}
Liao, Q.~V. and Vaughan, J.~W.
\newblock Ai transparency in the age of llms: A human-centered research roadmap.
\newblock \emph{Harvard Data Science Review}, 2024.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Ma, Kothur, Nikpour, and Kavehei]{liu2023biosignal}
Liu, C., Ma, Y., Kothur, K., Nikpour, A., and Kavehei, O.
\newblock Biosignal copilot: Leveraging the power of llms in drafting reports for biomedical signals.
\newblock \emph{medRxiv}, pp.\  2023--06, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Yang, Xu, Li, Long, Li, and Zhao]{liu2024spatial}
Liu, C., Yang, S., Xu, Q., Li, Z., Long, C., Li, Z., and Zhao, R.
\newblock Spatial-temporal large language model for traffic prediction.
\newblock \emph{arXiv preprint arXiv:2401.10134}, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Li, Wu, and Lee]{liu2023llava}
Liu, H., Li, C., Wu, Q., and Lee, Y.~J.
\newblock Visual instruction tuning.
\newblock \emph{Advances in neural information processing systems}, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Hu, Li, Diao, Liang, Hooi, and Zimmermann]{liu2023unitime}
Liu, X., Hu, J., Li, Y., Diao, S., Liang, Y., Hooi, B., and Zimmermann, R.
\newblock Unitime: A language-empowered unified model for cross-domain time series forecasting.
\newblock In \emph{The Web Conference 2024 (WWW)}, 2024{\natexlab{b}}.

\bibitem[Lopez-Lira \& Tang(2023)Lopez-Lira and Tang]{lopez2023can}
Lopez-Lira, A. and Tang, Y.
\newblock Can chatgpt forecast stock price movements? return predictability and large language models.
\newblock \emph{Return Predictability and Large Language Models (April 6, 2023)}, 2023.

\bibitem[Madaan et~al.(2024)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe, Alon, Dziri, Prabhumoye, Yang, et~al.]{madaan2023self}
Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et~al.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Mirchandani et~al.(2023)Mirchandani, Xia, Florence, Driess, Arenas, Rao, Sadigh, Zeng, et~al.]{mirchandani2023large}
Mirchandani, S., Xia, F., Florence, P., Driess, D., Arenas, M.~G., Rao, K., Sadigh, D., Zeng, A., et~al.
\newblock Large language models as general pattern machines.
\newblock In \emph{7th Annual Conference on Robot Learning}, 2023.

\bibitem[Moon et~al.(2023)Moon, Madotto, Lin, Saraf, Bearman, and Damavandi]{moon2023imu2clip}
Moon, S., Madotto, A., Lin, Z., Saraf, A., Bearman, A., and Damavandi, B.
\newblock Imu2clip: Language-grounded motion sensor translation with multimodal contrastive learning.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pp.\  13246--13253, 2023.

\bibitem[Nie et~al.(2022)Nie, Nguyen, Sinthong, and Kalagnanam]{nie2022time}
Nie, Y., Nguyen, N.~H., Sinthong, P., and Kalagnanam, J.
\newblock A time series is worth 64 words: Long-term forecasting with transformers.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Oh et~al.(2024)Oh, Lee, Bae, Kwon, and Choi]{oh2023ecg}
Oh, J., Lee, G., Bae, S., Kwon, J.-m., and Choi, E.
\newblock Ecg-qa: A comprehensive question answering dataset combined with electrocardiogram.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Peris et~al.(2023)Peris, Dupuy, Majmudar, Parikh, Smaili, Zemel, and Gupta]{peris2023privacy}
Peris, C., Dupuy, C., Majmudar, J., Parikh, R., Smaili, S., Zemel, R., and Gupta, R.
\newblock Privacy in the time of language models.
\newblock In \emph{Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining}, pp.\  1291–1292, 2023.

\bibitem[Qiu et~al.(2023{\natexlab{a}})Qiu, Han, Zhu, Xu, Rosenberg, Liu, Weber, and Zhao]{qiu2023transfer}
Qiu, J., Han, W., Zhu, J., Xu, M., Rosenberg, M., Liu, E., Weber, D., and Zhao, D.
\newblock Transfer knowledge from natural language to electrocardiography: Can we detect cardiovascular disease through language models?
\newblock \emph{arXiv preprint arXiv:2301.09017}, 2023{\natexlab{a}}.

\bibitem[Qiu et~al.(2023{\natexlab{b}})Qiu, Zhu, Liu, Han, Zhang, Duan, Rosenberg, Liu, Weber, and Zhao]{qiu2023automated}
Qiu, J., Zhu, J., Liu, S., Han, W., Zhang, J., Duan, C., Rosenberg, M.~A., Liu, E., Weber, D., and Zhao, D.
\newblock Automated cardiovascular record retrieval by multimodal learning between electrocardiogram and clinical report.
\newblock In \emph{Machine Learning for Health (ML4H)}, pp.\  480--497. PMLR, 2023{\natexlab{b}}.

\bibitem[Rasul et~al.(2023)Rasul, Ashok, Williams, Khorasani, Adamopoulos, Bhagwatkar, Bilo{\v{s}}, Ghonia, Hassen, Schneider, et~al.]{rasul2023lag}
Rasul, K., Ashok, A., Williams, A.~R., Khorasani, A., Adamopoulos, G., Bhagwatkar, R., Bilo{\v{s}}, M., Ghonia, H., Hassen, N.~V., Schneider, A., et~al.
\newblock Lag-llama: Towards foundation models for time series forecasting.
\newblock \emph{arXiv preprint arXiv:2310.08278}, 2023.

\bibitem[Rawte et~al.(2023)Rawte, Sheth, and Das]{rawte2023survey}
Rawte, V., Sheth, A., and Das, A.
\newblock A survey of hallucination in large foundation models.
\newblock \emph{arXiv preprint arXiv:2309.05922}, 2023.

\bibitem[Schick et~al.(2024)Schick, Dwivedi-Yu, Dess{\`\i}, Raileanu, Lomeli, Hambro, Zettlemoyer, Cancedda, and Scialom]{schick2023toolformer}
Schick, T., Dwivedi-Yu, J., Dess{\`\i}, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., and Scialom, T.
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Shumway et~al.(2017)Shumway, Stoffer, Shumway, and Stoffer]{shumway2017arima}
Shumway, R.~H., Stoffer, D.~S., Shumway, R.~H., and Stoffer, D.~S.
\newblock Arima models.
\newblock \emph{Time series analysis and its applications: with R examples}, pp.\  75--163, 2017.

\bibitem[Singh et~al.(2023)Singh, Blukis, Mousavian, Goyal, Xu, Tremblay, Fox, Thomason, and Garg]{singh2023progprompt}
Singh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D., Tremblay, J., Fox, D., Thomason, J., and Garg, A.
\newblock Progprompt: Generating situated robot task plans using large language models.
\newblock In \emph{2023 IEEE International Conference on Robotics and Automation (ICRA)}, pp.\  11523--11530. IEEE, 2023.

\bibitem[Spathis \& Kawsar(2023)Spathis and Kawsar]{spathis2023first}
Spathis, D. and Kawsar, F.
\newblock The first step is the hardest: Pitfalls of representing and tokenizing temporal data for large language models.
\newblock \emph{arXiv preprint arXiv:2309.06236}, 2023.

\bibitem[Sun et~al.(2024)Sun, Li, Li, and Hong]{sun2023test}
Sun, C., Li, Y., Li, H., and Hong, S.
\newblock Test: Text prototype aligned embedding to activate llm's ability for time series.
\newblock In \emph{International Conference on Machine Learning}, 2024.

\bibitem[Sun et~al.(2023)Sun, Zhang, Ma, Shi, Li, Luo, Wang, Xu, Cao, and Zhao]{sun2023large}
Sun, Q., Zhang, S., Ma, D., Shi, J., Li, D., Luo, S., Wang, Y., Xu, N., Cao, G., and Zhao, H.
\newblock Large trajectory models are scalable motion predictors and planners.
\newblock \emph{arXiv preprint arXiv:2310.19620}, 2023.

\bibitem[Tian et~al.(2023)Tian, Mitchell, Yao, Manning, and Finn]{tian2023fine}
Tian, K., Mitchell, E., Yao, H., Manning, C.~D., and Finn, C.
\newblock Fine-tuning language models for factuality.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Tsay(2005)]{tsay2005analysis}
Tsay, R.~S.
\newblock \emph{Analysis of financial time series}.
\newblock John wiley \& sons, 2005.

\bibitem[Tsymbal(2004)]{tsymbal2004problem}
Tsymbal, A.
\newblock The problem of concept drift: definitions and related work.
\newblock \emph{Computer Science Department, Trinity College Dublin}, 106\penalty0 (2):\penalty0 58, 2004.

\bibitem[Victor et~al.(2022)Victor, Albert, Colin, Stephen, Lintang, Zaid, Antoine, Arnaud, Arun, Manan, et~al.]{sanh2021multitask}
Victor, S., Albert, W., Colin, R., Stephen, B., Lintang, S., Zaid, A., Antoine, C., Arnaud, S., Arun, R., Manan, D., et~al.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Vu et~al.(2023)Vu, Iyyer, Wang, Constant, Wei, Wei, Tar, Sung, Zhou, Le, et~al.]{vu2023freshllms}
Vu, T., Iyyer, M., Wang, X., Constant, N., Wei, J., Wei, J., Tar, C., Sung, Y.-H., Zhou, D., Le, Q., et~al.
\newblock Freshllms: Refreshing large language models with search engine augmentation.
\newblock \emph{arXiv preprint arXiv:2310.03214}, 2023.

\bibitem[Wang et~al.(2024)Wang, Ma, Feng, Zhang, Yang, Zhang, Chen, Tang, Chen, Lin, et~al.]{wang2023survey}
Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., et~al.
\newblock A survey on large language model based autonomous agents.
\newblock \emph{Frontiers of Computer Science}, 18\penalty0 (6):\penalty0 1--26, 2024.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Bao, Dong, Bjorck, Peng, Liu, Aggarwal, Mohammed, Singhal, Som, and Wei]{beit3}
Wang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal, K., Mohammed, O.~K., Singhal, S., Som, S., and Wei, F.
\newblock Image as a foreign language: {BEiT} pretraining for vision and vision-language tasks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Fang, Zeng, and Cheng]{wang2023would}
Wang, X., Fang, M., Zeng, Z., and Cheng, T.
\newblock Where would i go next? large language models as human mobility predictors.
\newblock \emph{arXiv preprint arXiv:2308.15197}, 2023{\natexlab{b}}.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, et~al.]{wei2022emergent}
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et~al.
\newblock Emergent abilities of large language models.
\newblock \emph{Transactions on Machine Learning Research}, 2022{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.~V., Zhou, D., et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24824--24837, 2022{\natexlab{b}}.

\bibitem[Wen et~al.(2021)Wen, Sun, Yang, Song, Gao, Wang, and Xu]{wen2020time}
Wen, Q., Sun, L., Yang, F., Song, X., Gao, J., Wang, X., and Xu, H.
\newblock Time series data augmentation for deep learning: A survey.
\newblock In \emph{IJCAI}, pp.\  4653--4660, 2021.

\bibitem[Wen et~al.(2022)Wen, Yang, Zhou, and Sun]{wen2022robust}
Wen, Q., Yang, L., Zhou, T., and Sun, L.
\newblock Robust time series analysis and applications: An industrial perspective.
\newblock In \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'22)}, pp.\  4836--4837, 2022.

\bibitem[Wen et~al.(2023)Wen, Zhou, Zhang, Chen, Ma, Yan, and Sun]{wen2023transformers}
Wen, Q., Zhou, T., Zhang, C., Chen, W., Ma, Z., Yan, J., and Sun, L.
\newblock Transformers in time series: A survey.
\newblock In \emph{International Joint Conference on Artificial Intelligence(IJCAI)}, 2023.

\bibitem[Wenzek et~al.(2020)Wenzek, Lachaux, Conneau, Chaudhary, Guzm{\'a}n, Joulin, and Grave]{wenzek2019ccnet}
Wenzek, G., Lachaux, M.-A., Conneau, A., Chaudhary, V., Guzm{\'a}n, F., Joulin, A., and Grave, {\'E}.
\newblock Ccnet: Extracting high quality monolingual datasets from web crawl data.
\newblock In \emph{Proceedings of the Twelfth Language Resources and Evaluation Conference}, pp.\  4003--4012, 2020.

\bibitem[Woo et~al.(2023)Woo, Liu, Kumar, and Sahoo]{woo2023pushing}
Woo, G., Liu, C., Kumar, A., and Sahoo, D.
\newblock Pushing the limits of pre-training for time series forecasting in the cloudops domain.
\newblock \emph{arXiv preprint arXiv:2310.05063}, 2023.

\bibitem[Wu et~al.(2023)Wu, Yin, Qi, Wang, Tang, and Duan]{wu2023visual}
Wu, C., Yin, S., Qi, W., Wang, X., Tang, Z., and Duan, N.
\newblock Visual chatgpt: Talking, drawing and editing with visual foundation models.
\newblock \emph{arXiv preprint arXiv:2303.04671}, 2023.

\bibitem[Xie et~al.(2023)Xie, Han, Zhang, Lai, Peng, Lopez-Lira, and Huang]{xie2023pixiu}
Xie, Q., Han, W., Zhang, X., Lai, Y., Peng, M., Lopez-Lira, A., and Huang, J.
\newblock Pixiu: A large language model, instruction data and evaluation benchmark for finance.
\newblock \emph{arXiv preprint arXiv:2306.05443}, 2023.

\bibitem[Xue \& Salim(2023)Xue and Salim]{xue2023promptcast}
Xue, H. and Salim, F.~D.
\newblock Promptcast: A new prompt-based learning paradigm for time series forecasting.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 2023.

\bibitem[Xue et~al.(2023)Xue, Zhou, Xu, Zhao, Xie, Jiang, Zhang, Zhou, Xu, Xiu, et~al.]{xue2023weaverbird}
Xue, S., Zhou, F., Xu, Y., Zhao, H., Xie, S., Jiang, C., Zhang, J., Zhou, J., Xu, P., Xiu, D., et~al.
\newblock Weaverbird: Empowering financial decision-making with large language model, knowledge base, and search engine.
\newblock \emph{arXiv preprint arXiv:2308.05361}, 2023.

\bibitem[Yan et~al.(2023)Yan, Wen, Zhong, Chen, Chen, Wen, Zimmermann, and Liang]{yan2023urban}
Yan, Y., Wen, H., Zhong, S., Chen, W., Chen, H., Wen, Q., Zimmermann, R., and Liang, Y.
\newblock When urban region profiling meets large language models.
\newblock \emph{arXiv preprint arXiv:2310.18340}, 2023.

\bibitem[Yang et~al.(2022{\natexlab{a}})Yang, Miech, Sivic, Laptev, and Schmid]{yang2022zero}
Yang, A., Miech, A., Sivic, J., Laptev, I., and Schmid, C.
\newblock Zero-shot video question answering via frozen bidirectional language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 124--141, 2022{\natexlab{a}}.

\bibitem[Yang et~al.(2022{\natexlab{b}})Yang, Gan, Wang, Hu, Lu, Liu, and Wang]{yang2022empirical}
Yang, Z., Gan, Z., Wang, J., Hu, X., Lu, Y., Liu, Z., and Wang, L.
\newblock An empirical study of gpt-3 for few-shot knowledge-based vqa.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~36, pp.\  3081--3089, 2022{\natexlab{b}}.

\bibitem[Yao et~al.(2024)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and Narasimhan]{yao2023tree}
Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., and Narasimhan, K.
\newblock Tree of thoughts: Deliberate problem solving with large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Yeh et~al.(2023)Yeh, Dai, Chen, Zheng, Fan, Der, Lai, Zhuang, Wang, Wang, et~al.]{yeh2023toward}
Yeh, C.-C.~M., Dai, X., Chen, H., Zheng, Y., Fan, Y., Der, A., Lai, V., Zhuang, Z., Wang, J., Wang, L., et~al.
\newblock Toward a foundation model for time series data.
\newblock In \emph{Proceedings of the 32nd ACM International Conference on Information and Knowledge Management}, pp.\  4400--4404, 2023.

\bibitem[Yin et~al.(2024)Yin, Wang, Cao, Shi, Liu, Li, Huang, Wang, Sheng, Bai, et~al.]{yin2023lamm}
Yin, Z., Wang, J., Cao, J., Shi, Z., Liu, D., Li, M., Huang, X., Wang, Z., Sheng, L., Bai, L., et~al.
\newblock Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Yu et~al.(2023{\natexlab{a}})Yu, Guo, and Sano]{yu2023zero}
Yu, H., Guo, P., and Sano, A.
\newblock Zero-shot ecg diagnosis with large language models and retrieval-augmented generation.
\newblock In \emph{Machine Learning for Health (ML4H)}, pp.\  650--663. PMLR, 2023{\natexlab{a}}.

\bibitem[Yu et~al.(2023{\natexlab{b}})Yu, Chen, Ling, Dong, Liu, and Lu]{yu2023temporal}
Yu, X., Chen, Z., Ling, Y., Dong, S., Liu, Z., and Lu, Y.
\newblock Temporal data meets llm--explainable financial time series forecasting.
\newblock \emph{arXiv preprint arXiv:2306.11025}, 2023{\natexlab{b}}.

\bibitem[Yu et~al.(2023{\natexlab{c}})Yu, Chen, and Lu]{yu-etal-2023-harnessing}
Yu, X., Chen, Z., and Lu, Y.
\newblock Harnessing {LLM}s for temporal data - a study on explainable financial time series forecasting.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track}, pp.\  739--753, Singapore, December 2023{\natexlab{c}}.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Diao, Lin, Fung, Lian, Wang, Chen, Ji, and Zhang]{zhang2023r}
Zhang, H., Diao, S., Lin, Y., Fung, Y.~R., Lian, Q., Wang, X., Chen, Y., Ji, H., and Zhang, T.
\newblock R-tuning: Teaching large language models to refuse unknown questions.
\newblock \emph{arXiv preprint arXiv:2311.09677}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Huang, Jin, and Lu]{zhang2023vision}
Zhang, J., Huang, J., Jin, S., and Lu, S.
\newblock Vision-language models for vision tasks: A survey.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 2024{\natexlab{a}}.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Wen, Zhang, Cai, Jin, Liu, Zhang, Liang, Pang, Song, et~al.]{zhang2023self}
Zhang, K., Wen, Q., Zhang, C., Cai, R., Jin, M., Liu, Y., Zhang, J.~Y., Liang, Y., Pang, G., Song, D., et~al.
\newblock Self-supervised learning for time series analysis: Taxonomy, progress, and prospects.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 2024{\natexlab{b}}.

\bibitem[Zhang et~al.(2024{\natexlab{c}})Zhang, Ren, Xia, Yiu, and Huang]{anonymous2024st}
Zhang, Q., Ren, X., Xia, L., Yiu, S.~M., and Huang, C.
\newblock Spatio-temporal graph learning with large language model.
\newblock 2024{\natexlab{c}}.
\newblock URL \url{https://openreview.net/forum?id=QUkcfqa6GX}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Dong, Li, Zhang, Sun, Wang, Li, Hu, Zhang, Wu, et~al.]{zhang2023instruction}
Zhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R., Zhang, T., Wu, F., et~al.
\newblock Instruction tuning for large language models: A survey.
\newblock \emph{arXiv preprint arXiv:2308.10792}, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2024{\natexlab{d}})Zhang, Fu, Liang, Zhang, Yu, Cai, and Yao]{zhang2023trafficgpt}
Zhang, S., Fu, D., Liang, W., Zhang, Z., Yu, B., Cai, P., and Yao, B.
\newblock Trafficgpt: Viewing, processing and interacting with traffic foundation models.
\newblock \emph{Transport Policy}, 150:\penalty0 95--105, 2024{\natexlab{d}}.

\bibitem[Zhang et~al.(2024{\natexlab{e}})Zhang, Chowdhury, Gupta, and Shang]{zhang2024large}
Zhang, X., Chowdhury, R.~R., Gupta, R.~K., and Shang, J.
\newblock Large language models for time series: A survey.
\newblock \emph{arXiv preprint arXiv:2402.01801}, 2024{\natexlab{e}}.

\bibitem[Zhang et~al.(2023{\natexlab{c}})Zhang, Zhang, Zheng, Chen, Gao, Ge, Teng, Jelloul, Rao, Guo, et~al.]{zhang2023insight}
Zhang, Y., Zhang, Y., Zheng, M., Chen, K., Gao, C., Ge, R., Teng, S., Jelloul, A., Rao, J., Guo, X., et~al.
\newblock Insight miner: A time series analysis dataset for cross-domain alignment with natural language.
\newblock In \emph{NeurIPS 2023 AI for Science Workshop}, 2023{\natexlab{c}}.

\bibitem[Zhang et~al.(2023{\natexlab{d}})Zhang, Amiri, Liu, Z{\"u}fle, and Zhao]{zhang2023large}
Zhang, Z., Amiri, H., Liu, Z., Z{\"u}fle, A., and Zhao, L.
\newblock Large language models for spatial trajectory patterns mining.
\newblock \emph{arXiv preprint arXiv:2310.04942}, 2023{\natexlab{d}}.

\bibitem[Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong, et~al.]{zhao2023survey}
Zhao, W.~X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et~al.
\newblock A survey of large language models.
\newblock \emph{arXiv preprint arXiv:2303.18223}, 2023.

\bibitem[Zhou et~al.(2023{\natexlab{a}})Zhou, Niu, Wang, Sun, and Jin]{zhou2023one1}
Zhou, T., Niu, P., Wang, X., Sun, L., and Jin, R.
\newblock One fits all: Power general time series analysis by pretrained lm.
\newblock \emph{Advances in neural information processing systems}, 2023{\natexlab{a}}.

\bibitem[Zhou et~al.(2023{\natexlab{b}})Zhou, Cui, Yoon, Zhang, Deng, Finn, Bansal, and Yao]{zhou2023analyzing}
Zhou, Y., Cui, C., Yoon, J., Zhang, L., Deng, Z., Finn, C., Bansal, M., and Yao, H.
\newblock Analyzing and mitigating object hallucination in large vision-language models.
\newblock \emph{arXiv preprint arXiv:2310.00754}, 2023{\natexlab{b}}.

\bibitem[Zhuo et~al.(2023)Zhuo, Huang, Chen, and Xing]{zhuo2023exploring}
Zhuo, T.~Y., Huang, Y., Chen, C., and Xing, Z.
\newblock Exploring ai ethics of chatgpt: A diagnostic analysis.
\newblock \emph{arXiv preprint arXiv:2301.12867}, 2023.

\end{thebibliography}
