\begin{thebibliography}{10}

\bibitem{2023YiVL}
01-ai.
\newblock Yi-vl.
\newblock https://huggingface.co/01-ai/Yi-VL-34B, 2023.

\bibitem{nocaps}
Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark
  Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson.
\newblock Nocaps: Novel object captioning at scale.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 8948--8957, 2019.

\bibitem{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
  Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
  et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock {\em Advances in Neural Information Processing Systems},
  35:23716--23736, 2022.

\bibitem{Claude3}
Anthropic.
\newblock The claude 3 model family: Opus, sonnet, haiku.
\newblock 2024.

\bibitem{qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,
  Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji
  Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,
  Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
  Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang,
  Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan,
  Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou,
  Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.
\newblock Qwen technical report.
\newblock {\em arXiv preprint arXiv:2309.16609}, 2023.

\bibitem{bai2023qwen}
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang
  Lin, Chang Zhou, and Jingren Zhou.
\newblock Qwen-vl: A frontier large vision-language model with versatile
  abilities.
\newblock {\em arXiv preprint arXiv:2308.12966}, 2023.

\bibitem{cai2024internlm2}
Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen,
  Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi~Fan, Zhaoye Fei,
  Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo,
  Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin,
  Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining
  Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran
  Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li~Ma, Runyuan Ma, Zerun Ma, Wenchang
  Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin
  Song, Zifan Song, Zhihao Sui, Peng Sun, Yu~Sun, Huanze Tang, Bin Wang,
  Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang,
  Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang
  Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu,
  Yuhang Zang, Chuyu Zhang, Li~Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo
  Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue
  Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming
  Zhuo, Yicheng Zou, Xipeng Qiu, Yu~Qiao, and Dahua Lin.
\newblock Internlm2 technical report, 2024.

\bibitem{changpinyo2021conceptual}
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.
\newblock Conceptual 12m: Pushing web-scale image-text pre-training to
  recognize long-tail visual concepts.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 3558--3568, 2021.

\bibitem{chen2024allava}
Guiming~Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi
  Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang.
\newblock Allava: Harnessing gpt4v-synthesized data for a lite vision-language
  model, 2024.

\bibitem{chen2024we}
Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong
  Duan, Jiaqi Wang, Yu~Qiao, Dahua Lin, et~al.
\newblock Are we on the right way for evaluating large vision-language models?
\newblock {\em arXiv preprint arXiv:2403.20330}, 2024.

\bibitem{chen2023sharegpt4v}
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao,
  and Dahua Lin.
\newblock Sharegpt4v: Improving large multi-modal models with better captions.
\newblock {\em arXiv preprint arXiv:2311.12793}, 2023.

\bibitem{chen2024far}
Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen
  Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji~Ma, Jiaqi Wang, Xiaoyi Dong, Hang
  Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang,
  Xingjian Wei, Wei Li, Wenjian Zhang, Bo~Zhang, Pinlong Cai, Licheng Wen,
  Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu~Qiao,
  Jifeng Dai, and Wenhai Wang.
\newblock How far are we to gpt-4v? closing the gap to commercial multimodal
  models with open-source suites, 2024.

\bibitem{chen2023internvl}
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong,
  Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu~Qiao, and
  Jifeng Dai.
\newblock Internvl: Scaling up vision foundation models and aligning for
  generic visual-linguistic tasks.
\newblock {\em arXiv preprint arXiv:2312.14238}, 2023.

\bibitem{2023opencompass}
OpenCompass Contributors.
\newblock Opencompass: A universal evaluation platform for foundation models.
\newblock \url{https://github.com/open-compass/opencompass}, 2023.

\bibitem{2023xtuner}
XTuner Contributors.
\newblock Xtuner: A toolkit for efficiently fine-tuning llm.
\newblock \url{https://github.com/InternLM/xtuner}, 2023.

\bibitem{dai2023instructblip}
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng~Huat Tiong, Junqi Zhao,
  Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.
\newblock Instructblip: Towards general-purpose vision-language models with
  instruction tuning.
\newblock {\em arXiv preprint arXiv:2305.06500}, 2023.

\bibitem{dettmers2023qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms, 2023.

\bibitem{internlmxcomposer2}
Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin
  Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang
  Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He,
  Xingcheng Zhang, Yu~Qiao, Dahua Lin, and Jiaqi Wang.
\newblock Internlm-xcomposer2: Mastering free-form text-image composition and
  comprehension in vision-language large model.
\newblock {\em arXiv preprint arXiv:2401.16420}, 2024.

\bibitem{du2022glm}
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and
  Jie Tang.
\newblock Glm: General language model pretraining with autoregressive blank
  infilling.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 320--335, 2022.

\bibitem{Fu2023MMEAC}
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu~Lin,
  Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke~Li, Xing Sun, and Rongrong
  Ji.
\newblock Mme: A comprehensive evaluation benchmark for multimodal large
  language models.
\newblock {\em ArXiv}, abs/2306.13394, 2023.

\bibitem{fu2024blink}
Xingyu Fu, Yushi Hu, Bangzheng Li, Yu~Feng, Haoyu Wang, Xudong Lin, Dan Roth,
  Noah~A Smith, Wei-Chiu Ma, and Ranjay Krishna.
\newblock Blink: Multimodal large language models can see but not perceive.
\newblock {\em arXiv preprint arXiv:2404.12390}, 2024.

\bibitem{vqav2}
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
\newblock Making the v in vqa matter: Elevating the role of image understanding
  in visual question answering.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 6904--6913, 2017.

\bibitem{han2023infimm}
Xiaotian Han, Quanzeng You, Yongfei Liu, Wentao Chen, Huangjie Zheng, Khalil
  Mrini, Xudong Lin, Yiqi Wang, Bohan Zhai, Jianbo Yuan, et~al.
\newblock Infimm-eval: Complex open-ended reasoning evaluation for multi-modal
  large language models.
\newblock {\em arXiv e-prints}, pages arXiv--2311, 2023.

\bibitem{hudson2019gqa}
Drew~A Hudson and Christopher~D Manning.
\newblock Gqa: A new dataset for real-world visual reasoning and compositional
  question answering.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 6700--6709, 2019.

\bibitem{kembhavi2016diagram}
Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi,
  and Ali Farhadi.
\newblock A diagram is worth a dozen images.
\newblock In {\em Computer Vision--ECCV 2016: 14th European Conference,
  Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14},
  pages 235--251. Springer, 2016.

\bibitem{laurencon2023obelics}
Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh,
  Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander~M. Rush, Douwe
  Kiela, Matthieu Cord, and Victor Sanh.
\newblock Obelics: An open web-scale filtered dataset of interleaved image-text
  documents, 2023.

\bibitem{li2023seed}
Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan.
\newblock Seed-bench: Benchmarking multimodal llms with generative
  comprehension.
\newblock {\em arXiv preprint arXiv:2307.16125}, 2023.

\bibitem{li2023blip}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock {\em arXiv preprint arXiv:2301.12597}, 2023.

\bibitem{li2023evaluating}
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne~Xin Zhao, and Ji-Rong Wen.
\newblock Evaluating object hallucination in large vision-language models.
\newblock {\em arXiv preprint arXiv:2305.10355}, 2023.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em Computer Vision--ECCV 2014: 13th European Conference, Zurich,
  Switzerland, September 6-12, 2014, Proceedings, Part V 13}, pages 740--755.
  Springer, 2014.

\bibitem{Liu2022VisualSR}
Fangyu Liu, Guy Edward~Toh Emerson, and Nigel Collier.
\newblock Visual spatial reasoning.
\newblock {\em Transactions of the Association for Computational Linguistics},
  2023.

\bibitem{liu2023hallusionbench}
Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh
  Manocha, and Tianyi Zhou.
\newblock Hallusionbench: You see what you think? or you think what you see? an
  image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5,
  and other multi-modality models.
\newblock {\em arXiv preprint arXiv:2310.14566}, 2023.

\bibitem{liu2023improved}
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee.
\newblock Improved baselines with visual instruction tuning.
\newblock {\em arXiv preprint arXiv:2310.03744}, 2023.

\bibitem{liu2024llavanext}
Haotian Liu, Chunyuan Li, Yuheng Li, Bo~Li, Yuanhan Zhang, Sheng Shen, and
  Yong~Jae Lee.
\newblock Llava-next: Improved reasoning, ocr, and world knowledge, January
  2024.

\bibitem{liu2023visual}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock {\em arXiv preprint arXiv:2304.08485}, 2023.

\bibitem{liu2023mmbench}
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo~Li, Songyang Zhang, Wangbo Zhao, Yike
  Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et~al.
\newblock Mmbench: Is your multi-modal model an all-around player?
\newblock {\em arXiv preprint arXiv:2307.06281}, 2023.

\bibitem{liu2023hidden}
Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng,
  Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et~al.
\newblock On the hidden mystery of ocr in large multimodal models.
\newblock {\em arXiv preprint arXiv:2305.07895}, 2023.

\bibitem{lu2024deepseek}
Haoyu Lu, Wen Liu, Bo~Zhang, Bingxuan Wang, Kai Dong, Bo~Liu, Jingxiang Sun,
  Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et~al.
\newblock Deepseek-vl: towards real-world vision-language understanding.
\newblock {\em arXiv preprint arXiv:2403.05525}, 2024.

\bibitem{lu2023mathvista}
Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh
  Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao.
\newblock Mathvista: Evaluating mathematical reasoning of foundation models in
  visual contexts.
\newblock {\em arXiv preprint arXiv:2310.02255}, 2023.

\bibitem{ok-vqa}
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
\newblock Ok-vqa: A visual question answering benchmark requiring external
  knowledge.
\newblock In {\em Proceedings of the IEEE/cvf conference on computer vision and
  pattern recognition}, pages 3195--3204, 2019.

\bibitem{masry2022chartqa}
Ahmed Masry, Do~Xuan Long, Jia~Qing Tan, Shafiq Joty, and Enamul Hoque.
\newblock Chartqa: A benchmark for question answering about charts with visual
  and logical reasoning.
\newblock {\em arXiv preprint arXiv:2203.10244}, 2022.

\bibitem{mathew2021docvqa}
Minesh Mathew, Dimosthenis Karatzas, and CV~Jawahar.
\newblock Docvqa: A dataset for vqa on document images.
\newblock In {\em Proceedings of the IEEE/CVF winter conference on applications
  of computer vision}, pages 2200--2209, 2021.

\bibitem{mishra2019ocr}
Anand Mishra, Shashank Shekhar, Ajeet~Kumar Singh, and Anirban Chakraborty.
\newblock Ocr-vqa: Visual question answering by reading text in images.
\newblock In {\em 2019 international conference on document analysis and
  recognition (ICDAR)}, pages 947--952. IEEE, 2019.

\bibitem{2022chatgpt}
OpenAI.
\newblock Chatgpt.
\newblock \url{https://openai.com/blog/chatgpt}, 2023.

\bibitem{OpenAI2023GPT4TR}
OpenAI.
\newblock Gpt-4 technical report.
\newblock {\em ArXiv}, abs/2303.08774, 2023.

\bibitem{minicpm2024}
OpenBMB.
\newblock Minicpm: Unveiling the potential of end-side large language models,
  2024.

\bibitem{ormazabal2024reka}
Aitor Ormazabal, Che Zheng, Cyprien de~Masson d'Autume, Dani Yogatama, Deyu Fu,
  Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac Ong, et~al.
\newblock Reka core, flash, and edge: A series of powerful multimodal language
  models.
\newblock {\em arXiv preprint arXiv:2404.12387}, 2024.

\bibitem{peng2023kosmos}
Zhiliang Peng, Wenhui Wang, Li~Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and
  Furu Wei.
\newblock Kosmos-2: Grounding multimodal large language models to the world.
\newblock {\em arXiv preprint arXiv:2306.14824}, 2023.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International conference on machine learning}, pages
  8748--8763. PMLR, 2021.

\bibitem{schuhmann2021laion}
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk,
  Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran
  Komatsuzaki.
\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text
  pairs.
\newblock {\em arXiv preprint arXiv:2111.02114}, 2021.

\bibitem{textvqa}
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu~Jiang, Xinlei Chen, Dhruv
  Batra, Devi Parikh, and Marcus Rohrbach.
\newblock Towards vqa models that can read.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 8317--8326, 2019.

\bibitem{sun2023emu2chat}
Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo,
  Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et~al.
\newblock Generative multimodal models are in-context learners.
\newblock {\em arXiv preprint arXiv:2312.13286}, 2023.

\bibitem{sun2023eva}
Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao.
\newblock Eva-clip: Improved training techniques for clip at scale.
\newblock {\em arXiv preprint arXiv:2303.15389}, 2023.

\bibitem{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac,
  Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{2023internlm}
InternLM Team.
\newblock Internlm: A multilingual language model with progressively enhanced
  capabilities.
\newblock \url{https://github.com/InternLM/InternLM-techreport}, 2023.

\bibitem{tong2024eyes}
Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi~Ma, Yann LeCun, and Saining Xie.
\newblock Eyes wide shut? exploring the visual shortcomings of multimodal llms.
\newblock {\em arXiv preprint arXiv:2401.06209}, 2024.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{wei2023chainofthought}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,
  Ed~Chi, Quoc Le, and Denny Zhou.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models, 2023.

\bibitem{RealWorldQA}
XAI.
\newblock Grok-1.5 vision preview.
\newblock 2024.

\bibitem{Xu2023LVLMeHubAC}
Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng,
  Siyuan Huang, Yu~Jiao Qiao, and Ping Luo.
\newblock Lvlm-ehub: A comprehensive evaluation benchmark for large
  vision-language models.
\newblock 2023.

\bibitem{ye2023mplug}
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang
  Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et~al.
\newblock mplug-owl: Modularization empowers large language models with
  multimodality.
\newblock {\em arXiv preprint arXiv:2304.14178}, 2023.

\bibitem{ye2023mplugowl2}
Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi~Qian,
  Ji~Zhang, Fei Huang, and Jingren Zhou.
\newblock mplug-owl2: Revolutionizing multi-modal large language model with
  modality collaboration, 2023.

\bibitem{yue2023mmmu}
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge~Zhang, Samuel
  Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et~al.
\newblock Mmmu: A massive multi-discipline multimodal understanding and
  reasoning benchmark for expert agi.
\newblock {\em arXiv preprint arXiv:2311.16502}, 2023.

\bibitem{zhai2023sigmoid}
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.
\newblock Sigmoid loss for language image pre-training.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 11975--11986, 2023.

\bibitem{zhang2024far}
Yizhe Zhang, He~Bai, Ruixiang Zhang, Jiatao Gu, Shuangfei Zhai, Josh Susskind,
  and Navdeep Jaitly.
\newblock How far are we from intelligent visual deductive reasoning?
\newblock {\em arXiv preprint arXiv:2403.04732}, 2024.

\bibitem{zhu2023minigpt}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced
  large language models.
\newblock {\em arXiv preprint arXiv:2304.10592}, 2023.

\end{thebibliography}
