\begin{thebibliography}{10}

\bibitem{brown2020gpt3}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal, A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877--1901, 2020.

\bibitem{chauhan2023post}
A.~Chauhan, U.~Tiwari, et~al.
\newblock Post training mixed precision quantization of neural networks using first-order information.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 1343--1352, 2023.

\bibitem{chen2021codex}
M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. d.~O. Pinto, J.~Kaplan, H.~Edwards, Y.~Burda, N.~Joseph, G.~Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock {\em arXiv preprint arXiv:2107.03374}, 2021.

\bibitem{chen2021humaneval}
M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. d.~O. Pinto, J.~Kaplan, H.~Edwards, Y.~Burda, N.~Joseph, G.~Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock {\em arXiv preprint arXiv:2107.03374}, 2021.

\bibitem{chiang2023vicuna}
W.-L. Chiang, Z.~Li, Z.~Lin, Y.~Sheng, Z.~Wu, H.~Zhang, L.~Zheng, S.~Zhuang, Y.~Zhuang, J.~E. Gonzalez, et~al.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, march 2023.
\newblock {\em URL https://lmsys. org/blog/2023-03-30-vicuna}, 3(5), 2023.

\bibitem{cobbe2021gsm8k}
K.~Cobbe, V.~Kosaraju, M.~Bavarian, M.~Chen, H.~Jun, L.~Kaiser, M.~Plappert, J.~Tworek, J.~Hilton, R.~Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock {\em arXiv preprint arXiv:2110.14168}, 2021.

\bibitem{dao2023flashattention2}
T.~Dao.
\newblock Flash{A}ttention-2: Faster attention with better parallelism and work partitioning.
\newblock 2023.

\bibitem{dao2022flashattention}
T.~Dao, D.~Y. Fu, S.~Ermon, A.~Rudra, and C.~R{\'e}.
\newblock Flash{A}ttention: Fast and memory-efficient exact attention with {IO}-awareness.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{dao2023flashdecoding}
T.~Dao, D.~Haziza, F.~Massa, and G.~Sizov.
\newblock Flash-decoding for long-context inference, 2023.

\bibitem{de2023can}
J.~C. de~Winter.
\newblock Can chatgpt pass high school exams on english language comprehension?
\newblock {\em International Journal of Artificial Intelligence in Education}, pages 1--16, 2023.

\bibitem{dettmers2022gpt3}
T.~Dettmers, M.~Lewis, Y.~Belkada, and L.~Zettlemoyer.
\newblock Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock {\em Advances in Neural Information Processing Systems}, 35:30318--30332, 2022.

\bibitem{dong2019hawq}
Z.~Dong, Z.~Yao, A.~Gholami, M.~W. Mahoney, and K.~Keutzer.
\newblock Hawq: Hessian aware quantization of neural networks with mixed-precision.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 293--302, 2019.

\bibitem{du2023shortcut}
M.~Du, F.~He, N.~Zou, D.~Tao, and X.~Hu.
\newblock Shortcut learning of large language models in natural language understanding.
\newblock {\em Communications of the ACM}, 67(1):110--120, 2023.

\bibitem{frantar2022gptq}
E.~Frantar, S.~Ashkboos, T.~Hoefler, and D.~Alistarh.
\newblock Gptq: Accurate post-training quantization for generative pre-trained transformers.
\newblock {\em arXiv preprint arXiv:2210.17323}, 2022.

\bibitem{eval-harness}
L.~Gao, J.~Tow, B.~Abbasi, S.~Biderman, S.~Black, A.~DiPofi, C.~Foster, L.~Golding, J.~Hsu, A.~Le~Noac'h, H.~Li, K.~McDonell, N.~Muennighoff, C.~Ociepa, J.~Phang, L.~Reynolds, H.~Schoelkopf, A.~Skowron, L.~Sutawika, E.~Tang, A.~Thite, B.~Wang, K.~Wang, and A.~Zou.
\newblock A framework for few-shot language model evaluation, 12 2023.

\bibitem{ge2023modeltells}
S.~Ge, Y.~Zhang, L.~Liu, M.~Zhang, J.~Han, and J.~Gao.
\newblock Model tells you what to discard: Adaptive kv cache compression for llms.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{he2023ptqd}
Y.~He, L.~Liu, J.~Liu, W.~Wu, H.~Zhou, and B.~Zhuang.
\newblock Ptqd: Accurate post-training quantization for diffusion models.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{hou2022tokendrop}
L.~Hou, R.~Y. Pang, T.~Zhou, Y.~Wu, X.~Song, X.~Song, and D.~Zhou.
\newblock Token dropping for efficient bert pretraining.
\newblock {\em arXiv preprint arXiv:2203.13240}, 2022.

\bibitem{howard2017mobilenets}
A.~G. Howard, M.~Zhu, B.~Chen, D.~Kalenichenko, W.~Wang, T.~Weyand, M.~Andreetto, and H.~Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision applications.
\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{jiang2023mistral}
A.~Q. Jiang, A.~Sablayrolles, A.~Mensch, C.~Bamford, D.~S. Chaplot, D.~d.~l. Casas, F.~Bressand, G.~Lengyel, G.~Lample, L.~Saulnier, et~al.
\newblock Mistral 7b.
\newblock {\em arXiv preprint arXiv:2310.06825}, 2023.

\bibitem{kang2024gear}
H.~Kang, Q.~Zhang, S.~Kundu, G.~Jeong, Z.~Liu, T.~Krishna, and T.~Zhao.
\newblock Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm.
\newblock {\em arXiv preprint arXiv:2403.05527}, 2024.

\bibitem{kim2023finequant}
Y.~J. Kim, R.~Henry, R.~Fahim, and H.~H. Awadalla.
\newblock Finequant: Unlocking efficiency with fine-grained weight-only quantization for llms.
\newblock {\em arXiv preprint arXiv:2308.09723}, 2023.

\bibitem{li2024common7bmath}
C.~Li, W.~Wang, J.~Hu, Y.~Wei, N.~Zheng, H.~Hu, Z.~Zhang, and H.~Peng.
\newblock Common 7b language models already possess strong math capabilities.
\newblock {\em arXiv preprint arXiv:2403.04706}, 2024.

\bibitem{longeval2023}
D.~Li, R.~Shao, A.~Xie, Y.~Sheng, L.~Zheng, J.~Gonzalez, I.~Stoica, X.~Ma, , and H.~Zhang.
\newblock How long can open-source llms truly promise on context length?, June 2023.

\bibitem{li2023lineretrival}
D.~Li, R.~Shao, A.~Xie, Y.~Sheng, L.~Zheng, J.~Gonzalez, I.~Stoica, X.~Ma, and H.~Zhang.
\newblock How long can context length of open-source llms truly promise?
\newblock In {\em NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following}, 2023.

\bibitem{li2020brecq}
Y.~Li, R.~Gong, X.~Tan, Y.~Yang, P.~Hu, Q.~Zhang, F.~Yu, W.~Wang, and S.~Gu.
\newblock Brecq: Pushing the limit of post-training quantization by block reconstruction.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{lin2023awq}
J.~Lin, J.~Tang, H.~Tang, S.~Yang, X.~Dang, and S.~Han.
\newblock Awq: Activation-aware weight quantization for llm compression and acceleration.
\newblock {\em arXiv preprint arXiv:2306.00978}, 2023.

\bibitem{liu2023qllm}
J.~Liu, R.~Gong, X.~Wei, Z.~Dong, J.~Cai, and B.~Zhuang.
\newblock Qllm: Accurate and efficient low-bitwidth quantization for large language models.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{liu2024yourcode}
J.~Liu, C.~S. Xia, Y.~Wang, and L.~Zhang.
\newblock Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{liu2024scissorhands}
Z.~Liu, A.~Desai, F.~Liao, W.~Wang, V.~Xie, Z.~Xu, A.~Kyrillidis, and A.~Shrivastava.
\newblock Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{liu2023llmqat}
Z.~Liu, B.~Oguz, C.~Zhao, E.~Chang, P.~Stock, Y.~Mehdad, Y.~Shi, R.~Krishnamoorthi, and V.~Chandra.
\newblock Llm-qat: Data-free quantization aware training for large language models.
\newblock {\em arXiv preprint arXiv:2305.17888}, 2023.

\bibitem{liu2024kivi}
Z.~Liu, J.~Yuan, H.~Jin, S.~Zhong, Z.~Xu, V.~Braverman, B.~Chen, and X.~Hu.
\newblock Kivi: A tuning-free asymmetric 2bit quantization for kv cache.
\newblock {\em arXiv preprint arXiv:2402.02750}, 2024.

\bibitem{luo2023wizardmath}
H.~Luo, Q.~Sun, C.~Xu, P.~Zhao, J.~Lou, C.~Tao, X.~Geng, Q.~Lin, S.~Chen, and D.~Zhang.
\newblock Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.
\newblock {\em arXiv preprint arXiv:2308.09583}, 2023.

\bibitem{radford2018improving}
A.~Radford, K.~Narasimhan, T.~Salimans, I.~Sutskever, et~al.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem{tan2021investigatingmath}
M.~Tan, L.~Wang, L.~Jiang, and J.~Jiang.
\newblock Investigating math word problems using pretrained multilingual language models.
\newblock {\em arXiv preprint arXiv:2105.08928}, 2021.

\bibitem{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix, B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{touvron2023llama2}
H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei, N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2019haq}
K.~Wang, Z.~Liu, Y.~Lin, J.~Lin, and S.~Han.
\newblock Haq: Hardware-aware automated quantization with mixed precision.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 8612--8620, 2019.

\bibitem{xiao2023smoothquant}
G.~Xiao, J.~Lin, M.~Seznec, H.~Wu, J.~Demouth, and S.~Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for large language models.
\newblock In {\em International Conference on Machine Learning}, pages 38087--38099. PMLR, 2023.

\bibitem{xiao2023attentionsinks}
G.~Xiao, Y.~Tian, B.~Chen, S.~Han, and M.~Lewis.
\newblock Efficient streaming language models with attention sinks.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{xu2022systematiccode}
F.~F. Xu, U.~Alon, G.~Neubig, and V.~J. Hellendoorn.
\newblock A systematic evaluation of large language models of code.
\newblock In {\em Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming}, pages 1--10, 2022.

\bibitem{yang2024notoken}
J.~Y. Yang, B.~Kim, J.~Bae, B.~Kwon, G.~Park, E.~Yang, S.~J. Kwon, and D.~Lee.
\newblock No token left behind: Reliable kv cache compression via importance-aware mixed precision quantization.
\newblock {\em arXiv preprint arXiv:2402.18096}, 2024.

\bibitem{yao2021hawq}
Z.~Yao, Z.~Dong, Z.~Zheng, A.~Gholami, J.~Yu, E.~Tan, L.~Wang, Q.~Huang, Y.~Wang, M.~Mahoney, et~al.
\newblock Hawq-v3: Dyadic neural network quantization.
\newblock In {\em International Conference on Machine Learning}, pages 11875--11886. PMLR, 2021.

\bibitem{yao2022zeroquant}
Z.~Yao, R.~Yazdani~Aminabadi, M.~Zhang, X.~Wu, C.~Li, and Y.~He.
\newblock Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.
\newblock {\em Advances in Neural Information Processing Systems}, 35:27168--27183, 2022.

\bibitem{zhang2024h2o}
Z.~Zhang, Y.~Sheng, T.~Zhou, T.~Chen, L.~Zheng, R.~Cai, Z.~Song, Y.~Tian, C.~R{\'e}, C.~Barrett, et~al.
\newblock H2o: Heavy-hitter oracle for efficient generative inference of large language models.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\end{thebibliography}
