\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Anil, Koren, Talwar, and
  Zhang]{agarwalAKTZ20}
N.~Agarwal, R.~Anil, T.~Koren, K.~Talwar, and C.~Zhang.
\newblock Stochastic optimization with laggard data pipelines.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020},
  2020.

\bibitem[Allen-Zhu and Orecchia(2017)]{allen2017linear}
Z.~Allen-Zhu and L.~Orecchia.
\newblock Linear coupling: An ultimate unification of gradient and mirror
  descent.
\newblock In \emph{8th Innovations in Theoretical Computer Science Conference
  (ITCS 2017)}. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2017.

\bibitem[Bassily et~al.(2019)Bassily, Feldman, Talwar, and
  Guha~Thakurta]{bassily2019private}
R.~Bassily, V.~Feldman, K.~Talwar, and A.~Guha~Thakurta.
\newblock Private stochastic convex optimization with optimal rates.
\newblock \emph{Advances in neural information processing systems}, 2019.

\bibitem[Bassily et~al.(2020)Bassily, Feldman, Guzm{\'a}n, and
  Talwar]{bassily2020stability}
R.~Bassily, V.~Feldman, C.~Guzm{\'a}n, and K.~Talwar.
\newblock Stability of stochastic gradient descent on nonsmooth convex losses.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet2002stability}
O.~Bousquet and A.~Elisseeff.
\newblock Stability and generalization.
\newblock \emph{Journal of machine learning research}, 2\penalty0
  (Mar):\penalty0 499--526, 2002.

\bibitem[Bousquet et~al.(2020)Bousquet, Klochkov, and
  Zhivotovskiy]{bousquet2020sharper}
O.~Bousquet, Y.~Klochkov, and N.~Zhivotovskiy.
\newblock Sharper bounds for uniformly stable algorithms.
\newblock In \emph{Conference on Learning Theory}, pages 610--626. PMLR, 2020.

\bibitem[Bubeck et~al.(2015)Bubeck, Lee, and Singh]{bubeck2015geometric}
S.~Bubeck, Y.~T. Lee, and M.~Singh.
\newblock A geometric alternative to nesterov's accelerated gradient descent.
\newblock \emph{arXiv preprint arXiv:1506.08187}, 2015.

\bibitem[Charles and Papailiopoulos(2018)]{charles2018stability}
Z.~Charles and D.~Papailiopoulos.
\newblock Stability and generalization of learning algorithms that converge to
  global optima.
\newblock In \emph{International Conference on Machine Learning}, pages
  745--754. PMLR, 2018.

\bibitem[Chen et~al.(2018)Chen, Jin, and Yu]{chen2018stability}
Y.~Chen, C.~Jin, and B.~Yu.
\newblock Stability and convergence trade-off of iterative optimization
  algorithms.
\newblock \emph{arXiv preprint arXiv:1804.01619}, 2018.

\bibitem[Devolder et~al.(2014)Devolder, Glineur, and
  Nesterov]{devolder2014first}
O.~Devolder, F.~Glineur, and Y.~Nesterov.
\newblock First-order methods of smooth convex optimization with inexact
  oracle.
\newblock \emph{Mathematical Programming}, 146\penalty0 (1-2):\penalty0 37--75,
  2014.

\bibitem[Feldman(2016)]{feldman2016generalization}
V.~Feldman.
\newblock Generalization of erm in stochastic convex optimization: The
  dimension strikes back.
\newblock \emph{Advances in Neural Information Processing Systems},
  29:\penalty0 3576--3584, 2016.

\bibitem[Feldman and Vondrak(2018)]{feldman2018generalization}
V.~Feldman and J.~Vondrak.
\newblock Generalization bounds for uniformly stable algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9747--9757, 2018.

\bibitem[Feldman and Vondrak(2019)]{feldman2019high}
V.~Feldman and J.~Vondrak.
\newblock High probability generalization bounds for uniformly stable
  algorithms with nearly optimal rate.
\newblock In \emph{Conference on Learning Theory}, pages 1270--1279, 2019.

\bibitem[Feldman et~al.(2020)Feldman, Koren, and Talwar]{feldman2020private}
V.~Feldman, T.~Koren, and K.~Talwar.
\newblock Private stochastic convex optimization: optimal rates in linear time.
\newblock In \emph{Proceedings of the 52nd Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 439--449, 2020.

\bibitem[Gonen and Shalev-Shwartz(2017)]{gonen2017fast}
A.~Gonen and S.~Shalev-Shwartz.
\newblock Fast rates for empirical risk minimization of strict saddle problems.
\newblock In \emph{Conference on Learning Theory}, pages 1043--1063. PMLR,
  2017.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
M.~Hardt, B.~Recht, and Y.~Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  1225--1234. PMLR, 2016.

\bibitem[Koren and Levy(2015)]{koren2015fast}
T.~Koren and K.~Y. Levy.
\newblock Fast rates for exp-concave empirical risk minimization.
\newblock In \emph{Proceedings of the 28th International Conference on Neural
  Information Processing Systems-Volume 1}, pages 1477--1485, 2015.

\bibitem[Lan(2012)]{lan2012optimal}
G.~Lan.
\newblock An optimal method for stochastic composite optimization.
\newblock \emph{Mathematical Programming}, 133\penalty0 (1-2):\penalty0
  365--397, 2012.

\bibitem[Lessard et~al.(2016)Lessard, Recht, and Packard]{lessard2016analysis}
L.~Lessard, B.~Recht, and A.~Packard.
\newblock Analysis and design of optimization algorithms via integral quadratic
  constraints.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (1):\penalty0
  57--95, 2016.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Y.~Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Nesterov(1983)]{nesterov1983method}
Y.~E. Nesterov.
\newblock A method for solving the convex programming problem with convergence
  rate o (1/k\^{} 2).
\newblock In \emph{Dokl. akad. nauk Sssr}, volume 269, pages 543--547, 1983.

\bibitem[Polyak(1964)]{polyak1964some}
B.~T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Shalev-Shwartz et~al.(2009)Shalev-Shwartz, Shamir, Srebro, and
  Sridharan]{shalev2009stochastic}
S.~Shalev-Shwartz, O.~Shamir, N.~Srebro, and K.~Sridharan.
\newblock Stochastic convex optimization.
\newblock In \emph{COLT}, 2009.

\bibitem[Wang et~al.(2017)Wang, Wang, and Srebro]{wang2017memory}
J.~Wang, W.~Wang, and N.~Srebro.
\newblock Memory and communication efficient distributed stochastic
  optimization with minibatch prox.
\newblock In \emph{Conference on Learning Theory}, pages 1882--1919. PMLR,
  2017.

\bibitem[Wu et~al.(2017)Wu, Li, Kumar, Chaudhuri, Jha, and
  Naughton]{wu2017bolt}
X.~Wu, F.~Li, A.~Kumar, K.~Chaudhuri, S.~Jha, and J.~Naughton.
\newblock Bolt-on differential privacy for scalable stochastic gradient
  descent-based analytics.
\newblock In \emph{Proceedings of the 2017 ACM International Conference on
  Management of Data}, pages 1307--1322, 2017.

\end{thebibliography}
