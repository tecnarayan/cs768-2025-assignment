\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahuja et~al.(1988)Ahuja, Magnanti, and Orlin]{ahuja1988network}
Ravindra~K Ahuja, Thomas~L Magnanti, and James~B Orlin.
\newblock Network flows.
\newblock 1988.

\bibitem[Banner et~al.(2018{\natexlab{a}})Banner, Hubara, Hoffer, and
  Soudry]{Banner2018ScalableMF}
Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry.
\newblock Scalable methods for 8-bit training of neural networks.
\newblock In \emph{NeurIPS}, 2018{\natexlab{a}}.

\bibitem[Banner et~al.(2018{\natexlab{b}})Banner, Nahshan, Hoffer, and
  Soudry]{banner2018post}
Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry.
\newblock Post-training 4-bit quantization of convolution networks for
  rapid-deployment.
\newblock \emph{arXiv preprint arXiv:1810.05723}, 2018{\natexlab{b}}.

\bibitem[Bellec et~al.(2017)Bellec, Kappel, Maass, and
  Legenstein]{bellec2017deep}
Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein.
\newblock Deep rewiring: Training very sparse deep networks.
\newblock \emph{arXiv preprint arXiv:1711.05136}, 2017.

\bibitem[Bengio et~al.(2013)Bengio, L{\'e}onard, and
  Courville]{bengio2013estimating}
Yoshua Bengio, Nicholas L{\'e}onard, and Aaron Courville.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock \emph{arXiv preprint arXiv:1308.3432}, 2013.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Kr{\"u}ger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3}
T.~Brown, B.~Mann, Nick Ryder, Melanie Subbiah, J.~Kaplan, Prafulla Dhariwal,
  Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini
  Agarwal, Ariel Herbert-Voss, G.~Kr{\"u}ger, T.~Henighan, R.~Child, Aditya
  Ramesh, D.~Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,
  E.~Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, J.~Clark, Christopher
  Berner, Sam McCandlish, A.~Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock \emph{ArXiv}, abs/2005.14165, 2020.

\bibitem[Dettmers \& Zettlemoyer(2019)Dettmers and
  Zettlemoyer]{dettmers2019sparse}
Tim Dettmers and Luke Zettlemoyer.
\newblock Sparse networks from scratch: Faster training without losing
  performance.
\newblock \emph{arXiv preprint arXiv:1907.04840}, 2019.

\bibitem[Elsen et~al.(2020)Elsen, Dukhan, Gale, and Simonyan]{Elsen2020FastSC}
Erich Elsen, Marat Dukhan, Trevor Gale, and Karen Simonyan.
\newblock Fast sparse convnets.
\newblock \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  14617--14626, 2020.

\bibitem[Evci et~al.(2020)Evci, Gale, Menick, Castro, and
  Elsen]{evci2020rigging}
Utku Evci, Trevor Gale, Jacob Menick, Pablo~Samuel Castro, and Erich Elsen.
\newblock Rigging the lottery: Making all tickets winners.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2943--2952. PMLR, 2020.

\bibitem[Fedus et~al.(2021)Fedus, Zoph, and Shazeer]{Fedus2021SwitchTS}
W.~Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock \emph{ArXiv}, abs/2101.03961, 2021.

\bibitem[Finkelstein et~al.(2019)Finkelstein, Almog, and
  Grobman]{finkelstein2019fighting}
Alexander Finkelstein, Uri Almog, and Mark Grobman.
\newblock Fighting quantization bias with bias.
\newblock \emph{arXiv preprint arXiv:1906.03193}, 2019.

\bibitem[Frankle \& Carbin(2018)Frankle and Carbin]{lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Gale et~al.(2020)Gale, Zaharia, Young, and Elsen]{Gale2020SparseGK}
Trevor Gale, Matei~A. Zaharia, Cliff Young, and Erich Elsen.
\newblock Sparse gpu kernels for deep learning.
\newblock In \emph{SC}, 2020.

\bibitem[Gray et~al.(2017)Gray, Radford, and Kingma]{gray2017gpu}
Scott Gray, Alec Radford, and Diederik~P Kingma.
\newblock Gpu kernels for block-sparse weights.
\newblock \emph{arXiv preprint arXiv:1711.09224}, 3, 2017.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{Han2015LearningBW}
Song Han, J.~Pool, John Tran, and W.~Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock \emph{ArXiv}, abs/1506.02626, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
Kaiming He, X.~Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  770--778, 2016.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{Hinton2015DistillingTK}
Geoffrey~E. Hinton, Oriol Vinyals, and J.~Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{ArXiv}, abs/1503.02531, 2015.

\bibitem[Hubara et~al.(2017)Hubara, Courbariaux, Soudry, El-Yaniv, and
  Bengio]{hubara2017quantized}
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
  Bengio.
\newblock Quantized neural networks: Training neural networks with low
  precision weights and activations.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 6869--6898, 2017.

\bibitem[Hubara et~al.(2020)Hubara, Nahshan, Hanani, Banner, and
  Soudry]{Hubara2020ImprovingPT}
Itay Hubara, Yury Nahshan, Yair Hanani, R.~Banner, and Daniel Soudry.
\newblock Improving post training neural quantization: Layer-wise calibration
  and integer programming.
\newblock \emph{ArXiv}, abs/2006.10518, 2020.

\bibitem[Janowsky(1989)]{Janowsky1989}
S.~A. Janowsky.
\newblock Pruning versus clipping in neural networks.
\newblock \emph{Physical Review A, 39(12):6600–6603}, 1989.
\newblock URL \url{https://link.aps.org/doi/10.1103/PhysRevA.39.6600}.

\bibitem[Karnin(1990)]{Karin1990}
E.~D. Karnin.
\newblock A simple procedure for pruning backpropagation trained neural
  networks.
\newblock \emph{IEEE transactions on neural networks, 1(2):239–242}, 1990.

\bibitem[Lee et~al.(2019)Lee, Ajanthan, and Torr]{snip}
N.~Lee, Thalaiyasingam Ajanthan, and P.~Torr.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock In \emph{ICLR}, 2019.

\bibitem[Li et~al.(2017)Li, Kadav, Durdanovic, Samet, and Graf]{l1Filter}
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans~Peter Graf.
\newblock Pruning filters for efficient convnets.
\newblock In \emph{ICLR}, 2017.

\bibitem[Liu et~al.(2020)Liu, Whatmough, and Mattina]{liu2020sparse}
Zhi-Gang Liu, Paul~N Whatmough, and Matthew Mattina.
\newblock Sparse systolic tensor array for efficient cnn hardware acceleration.
\newblock \emph{arXiv preprint arXiv:2009.02381}, 2020.

\bibitem[Liu et~al.(2018)Liu, Sun, Zhou, Huang, and Darrell]{liu2018rethinking}
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell.
\newblock Rethinking the value of network pruning.
\newblock \emph{arXiv preprint arXiv:1810.05270}, 2018.

\bibitem[Louizos et~al.(2018)Louizos, Welling, and Kingma]{l0regul}
Christos Louizos, M.~Welling, and Diederik~P. Kingma.
\newblock Learning sparse neural networks through l0 regularization.
\newblock In \emph{ICLR}, 2018.

\bibitem[Luo et~al.(2017)Luo, Wu, and Lin]{thinet}
Jian-Hao Luo, Jianxin Wu, and W.~Lin.
\newblock Thinet: A filter level pruning method for deep neural network
  compression.
\newblock \emph{2017 IEEE International Conference on Computer Vision (ICCV)},
  pp.\  5068--5076, 2017.

\bibitem[Marcel \& Rodriguez(2010)Marcel and Rodriguez]{torchvision}
S\'{e}bastien Marcel and Yann Rodriguez.
\newblock Torchvision the machine-vision package of torch.
\newblock In \emph{Proceedings of the 18th ACM International Conference on
  Multimedia}, MM '10, pp.\  1485–1488, New York, NY, USA, 2010. Association
  for Computing Machinery.
\newblock ISBN 9781605589336.
\newblock \doi{10.1145/1873951.1874254}.
\newblock URL \url{https://doi.org/10.1145/1873951.1874254}.

\bibitem[Mocanu et~al.(2018)Mocanu, Mocanu, Stone, Nguyen, Gibescu, and
  Liotta]{mocanu2018scalable}
Decebal~Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong~H Nguyen,
  Madeleine Gibescu, and Antonio Liotta.
\newblock Scalable training of artificial neural networks with adaptive sparse
  connectivity inspired by network science.
\newblock \emph{Nature communications}, 9\penalty0 (1):\penalty0 1--12, 2018.

\bibitem[Mostafa \& Wang(2019)Mostafa and Wang]{mostafa2019parameter}
Hesham Mostafa and Xin Wang.
\newblock Parameter efficient training of deep convolutional neural networks by
  dynamic sparse reparameterization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4646--4655. PMLR, 2019.

\bibitem[Mozer \& Smolensky(1989a)Mozer and Smolensky]{Mozer1989a}
M.~C. Mozer and P.~Smolensky.
\newblock Skeletonization: A technique for trimming the fat from a network via
  relevance assessment.
\newblock \emph{Advances in neural information processing systems, pp.
  107–115}, 1989a.

\bibitem[Mozer \& Smolensky(1989b)Mozer and Smolensky]{Mozer1989b}
M.~C. Mozer and P.~Smolensky.
\newblock Using relevance to reduce network size automatically.
\newblock \emph{Connection Science, 1(1):3–16}, 1989b.

\bibitem[Nagel et~al.(2020)Nagel, Amjad, van Baalen, Louizos, and
  Blankevoort]{markus2020}
Markus Nagel, Rana~Ali Amjad, Mart van Baalen, Christos Louizos, and Tijmen
  Blankevoort.
\newblock Up or down? adaptive rounding for post-training quantization.
\newblock In \emph{ICML}, 2020.

\bibitem[Nahshan et~al.(2019)Nahshan, Chmiel, Baskin, Zheltonozhskii, Banner,
  Bronstein, and Mendelson]{Nahshan2019LossAP}
Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner,
  Alex~M. Bronstein, and Avi Mendelson.
\newblock Loss aware post-training quantization.
\newblock \emph{ArXiv}, abs/1911.07190, 2019.

\bibitem[Nvidia(2018)]{NvidiaTensorCores}
Nvidia.
\newblock Nvidia deep learning examples for tensor cores.
\newblock 2018.
\newblock URL
  \url{https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch}.

\bibitem[Nvidia(2020)]{Nvidia}
Nvidia.
\newblock a100 tensor core gpu architecture.
\newblock 2020.
\newblock URL
  \url{http://https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf}.

\bibitem[Renda et~al.(2020{\natexlab{a}})Renda, Frankle, and
  Carbin]{Renda2020ComparingRA}
A.~Renda, Jonathan Frankle, and Michael Carbin.
\newblock Comparing rewinding and fine-tuning in neural network pruning.
\newblock \emph{ArXiv}, abs/2003.02389, 2020{\natexlab{a}}.

\bibitem[Renda et~al.(2020{\natexlab{b}})Renda, Frankle, and Carbin]{rewinding}
A.~Renda, Jonathan Frankle, and Michael Carbin.
\newblock Comparing rewinding and fine-tuning in neural network pruning.
\newblock In \emph{ICLR}, 2020{\natexlab{b}}.

\bibitem[Stosic \& Stosic(2021)Stosic and Stosic]{Stosic2021SearchSF}
Darko Stosic and Dusan Stosic.
\newblock Search spaces for neural model training.
\newblock \emph{ArXiv}, abs/2105.12920, 2021.

\bibitem[Tan et~al.(2019)Tan, Chen, Pang, Vasudevan, Sandler, Howard, and
  Le]{tan2019mnasnet}
Mingxing Tan, Bo~Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew
  Howard, and Quoc~V Le.
\newblock Mnasnet: Platform-aware neural architecture search for mobile.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  2820--2828, 2019.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li.]{blockSparsity}
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Learning structured sparsity in deep neural networks.
\newblock In \emph{In Advances in neural information processing systems, pp.
  2074–2082}, 2016.

\bibitem[Wu et~al.(2019)Wu, Dai, Zhang, Wang, Sun, Wu, Tian, Vajda, Jia, and
  Keutzer]{wu2019fbnet}
Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu,
  Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer.
\newblock Fbnet: Hardware-aware efficient convnet design via differentiable
  neural architecture search.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  10734--10742, 2019.

\bibitem[Zhou et~al.(2021)Zhou, Ma, Zhu, Liu, Zhang, Yuan, Sun, and
  Li]{n:mStructured}
Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu
  Sun, and Hongsheng Li.
\newblock Learning n:m fine-grained structures sparse neural networks from
  scratch.
\newblock In \emph{ICLR}, 2021.

\end{thebibliography}
