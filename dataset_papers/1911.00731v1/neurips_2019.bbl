\begin{thebibliography}{18}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bottou(2010)]{bottou2010large}
L{\'e}on Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In \emph{Proceedings of COMPSTAT}, pages 177--186. Springer, 2010.

\bibitem[Braverman et~al.(2016)Braverman, Garg, Ma, Nguyen, and
  Woodruff]{braverman2016communication}
Mark Braverman, Ankit Garg, Tengyu Ma, Huy~L Nguyen, and David~P Woodruff.
\newblock Communication lower bounds for statistical estimation problems via a
  distributed data processing inequality.
\newblock In \emph{Proceedings of the forty-eighth annual ACM symposium on
  Theory of Computing}, pages 1011--1020. ACM, 2016.

\bibitem[Chang et~al.(2017)Chang, Lin, and Zhou]{chang2017distributed}
Xiangyu Chang, Shao-Bo Lin, and Ding-Xuan Zhou.
\newblock Distributed semi-supervised learning with kernel ridge regression.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 1493--1514, 2017.

\bibitem[Chen et~al.(2017)Chen, Su, and Xu]{chen2017distributed}
Yudong Chen, Lili Su, and Jiaming Xu.
\newblock Distributed statistical machine learning in adversarial settings:
  Byzantine gradient descent.
\newblock \emph{Proceedings of the ACM on Measurement and Analysis of Computing
  Systems}, 1\penalty0 (2):\penalty0 44, 2017.

\bibitem[Diakonikolas et~al.(2017)Diakonikolas, Grigorescu, Li, Natarajan,
  Onak, and Schmidt]{diakonikolas2017communication}
Ilias Diakonikolas, Elena Grigorescu, Jerry Li, Abhiram Natarajan, Krzysztof
  Onak, and Ludwig Schmidt.
\newblock Communication-efficient distributed learning of discrete
  distributions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6391--6401, 2017.

\bibitem[Duchi et~al.(2012)Duchi, Agarwal, and Wainwright]{duchi2012dual}
John~C Duchi, Alekh Agarwal, and Martin~J Wainwright.
\newblock Dual averaging for distributed optimization: convergence analysis and
  network scaling.
\newblock \emph{IEEE Transactions on Automatic control}, 57\penalty0
  (3):\penalty0 592--606, 2012.

\bibitem[Jordan et~al.(2018)Jordan, Lee, and Yang]{jordan2018communication}
Michael~I Jordan, Jason~D Lee, and Yun Yang.
\newblock Communication-efficient distributed statistical inference.
\newblock \emph{Journal of the American Statistical Association}, pages 1--14,
  2018.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2015)Kone{\v{c}}n{\`y}, McMahan, and
  Ramage]{konevcny2015federated}
Jakub Kone{\v{c}}n{\`y}, Brendan McMahan, and Daniel Ramage.
\newblock Federated optimization: distributed optimization beyond the
  datacenter.
\newblock \emph{arXiv preprint arXiv:1511.03575}, 2015.

\bibitem[Lee et~al.(2017)Lee, Liu, Sun, and Taylor]{lee2017communication}
Jason~D Lee, Qiang Liu, Yuekai Sun, and Jonathan~E Taylor.
\newblock Communication-efficient sparse regression.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 115--144, 2017.

\bibitem[Lehmann and Casella(2006)]{lehmann2006theory}
Erich~L Lehmann and George Casella.
\newblock \emph{Theory of point estimation}.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Lian et~al.(2015)Lian, Huang, Li, and Liu]{lian2015asynchronous}
Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji~Liu.
\newblock Asynchronous parallel stochastic gradient for nonconvex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2737--2745, 2015.

\bibitem[McMahan and Ramage(2017)]{mcmahan2017federated}
Brendan McMahan and Daniel Ramage.
\newblock Federated learning: Collaborative machine learning without
  centralized training data.
\newblock \emph{Google Research Blog}, 3, 2017.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 1273--1282,
  2017.

\bibitem[Motwani and Raghavan(1995)]{motwani1995randomized}
Rajeev Motwani and Prabhakar Raghavan.
\newblock \emph{Randomized algorithms}.
\newblock Cambridge university press, 1995.

\bibitem[Salehkaleybar et~al.(2019)Salehkaleybar, Sharifnassab, and
  Golestani]{saleh2019}
Saber Salehkaleybar, Arsalan Sharifnassab, and S.~Jamaloddin Golestani.
\newblock One-shot distributed learning: theoretical limits and algorithms to
  achieve them.
\newblock \emph{arXiv preprint arXiv:1905.04634v1}, 2019.

\bibitem[Zhang et~al.(2015)Zhang, Choromanska, and LeCun]{zhang2015deep}
Sixin Zhang, Anna~E Choromanska, and Yann LeCun.
\newblock Deep learning with elastic averaging {SGD}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  685--693, 2015.

\bibitem[Zhang et~al.(2012)Zhang, Wainwright, and
  Duchi]{zhang2012communication}
Yuchen Zhang, Martin~J Wainwright, and John~C Duchi.
\newblock Communication-efficient algorithms for statistical optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1502--1510, 2012.

\bibitem[Zhang et~al.(2013)Zhang, Duchi, Jordan, and
  Wainwright]{zhang2013information}
Yuchen Zhang, John Duchi, Michael~I Jordan, and Martin~J Wainwright.
\newblock Information-theoretic lower bounds for distributed statistical
  estimation with communication constraints.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2328--2336, 2013.

\end{thebibliography}
