\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andriushchenko et~al.(2023)Andriushchenko, D'Angelo, Varre, and
  Flammarion]{andriushchenko2023wd}
Andriushchenko, M., D'Angelo, F., Varre, A., and Flammarion, N.
\newblock Why do we need weight decay in modern deep learning?
\newblock \emph{arXiv preprint arXiv:2310.04415}, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.04415}.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.
\newblock URL \url{https://arxiv.org/abs/1607.06450}.

\bibitem[Brock et~al.(2021{\natexlab{a}})Brock, De, and
  Smith]{brock2021signalpropagation}
Brock, A., De, S., and Smith, S.~L.
\newblock Characterizing signal propagation to close the performance gap in
  unnormalized resnets.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR}}, 2021{\natexlab{a}}.
\newblock \href{https://arxiv.org/abs/2101.08692}{arXiv:2101.08692}.

\bibitem[Brock et~al.(2021{\natexlab{b}})Brock, De, Smith, and
  Simonyan]{brock2021high}
Brock, A., De, S., Smith, S.~L., and Simonyan, K.
\newblock High-performance large-scale image recognition without normalization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1059--1071. PMLR, 2021{\natexlab{b}}.
\newblock \href{https://arxiv.org/abs/2102.06171}{arXiv:2102.06171}.

\bibitem[Cettolo et~al.(2014)Cettolo, Niehues, St{\"u}ker, Bentivogli, and
  Federico]{cettolo14iwslt}
Cettolo, M., Niehues, J., St{\"u}ker, S., Bentivogli, L., and Federico, M.
\newblock Report on the 11th {IWSLT} evaluation campaign.
\newblock In \emph{Proceedings of the 11th International Workshop on Spoken
  Language Translation: Evaluation Campaign}, pp.\  2--17, Lake Tahoe,
  California, December 4-5 2014.
\newblock URL \url{https://aclanthology.org/2014.iwslt-evaluation.1}.

\bibitem[Chen et~al.(2023)Chen, Liang, Huang, Real, Wang, Pham, Dong, Luong,
  Hsieh, Lu, and Le]{chen2023symbolic}
Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Pham, H., Dong, X., Luong,
  T., Hsieh, C.-J., Lu, Y., and Le, Q.~V.
\newblock Symbolic discovery of optimization algorithms.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=ne6zeqLFCZ}.
\newblock \href{https://arxiv.org/abs/2302.06675}{arXiv:2302.06675}.

\bibitem[Chiley et~al.(2019)Chiley, Sharapov, Kosson, Koster, Reece, Samaniego
  de~la Fuente, Subbiah, and James]{chiley2019online}
Chiley, V., Sharapov, I., Kosson, A., Koster, U., Reece, R., Samaniego de~la
  Fuente, S., Subbiah, V., and James, M.
\newblock Online normalization for training neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.
\newblock \href{https://arxiv.org/abs/1905.05894}{arXiv:1905.05894}.

\bibitem[Fu et~al.(2023)Fu, Wang, Zhang, Zhang, Chen, and
  Zheng]{fu2023momentum}
Fu, J., Wang, B., Zhang, H., Zhang, Z., Chen, W., and Zheng, N.
\newblock When and why momentum accelerates sgd: An empirical study.
\newblock \emph{arXiv preprint arXiv:2306.09000}, 2023.
\newblock URL \url{https://arxiv.org/abs/2306.09000}.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{Goodfellow-et-al-2016}
Goodfellow, I., Bengio, Y., and Courville, A.
\newblock \emph{Deep Learning}.
\newblock MIT Press, 2016.
\newblock \url{http://www.deeplearningbook.org}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2016.
\newblock \href{https://arxiv.org/abs/1512.03385}{arXiv:1512.03385}.

\bibitem[Heo et~al.(2021)Heo, Chun, Oh, Han, Yun, Kim, Uh, and
  Ha]{heo2021adamp}
Heo, B., Chun, S., Oh, S.~J., Han, D., Yun, S., Kim, G., Uh, Y., and Ha, J.-W.
\newblock Adamp: Slowing down the slowdown for momentum optimizers on
  scale-invariant weights.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Iz3zU3M316D}.
\newblock \href{https://arxiv.org/abs/2006.08217}{arXiv:2006.08217}.

\bibitem[Hoffer et~al.(2018)Hoffer, Banner, Golan, and Soudry]{hoffer2018norm}
Hoffer, E., Banner, R., Golan, I., and Soudry, D.
\newblock Norm matters: efficient and accurate normalization schemes in deep
  networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.
\newblock \href{https://arxiv.org/abs/1803.01814}{arXiv:1803.01814}.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford,
  E., Casas, D. d.~L., Hendricks, L.~A., Welbl, J., Clark, A., et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.15556}.

\bibitem[Huang et~al.(2017{\natexlab{a}})Huang, Liu, Lang, and
  Li]{huang2017projection}
Huang, L., Liu, X., Lang, B., and Li, B.
\newblock Projection based weight normalization for deep neural networks.
\newblock \emph{arXiv preprint arXiv:1710.02338}, 2017{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/1710.02338}.

\bibitem[Huang et~al.(2017{\natexlab{b}})Huang, Liu, Liu, Lang, and
  Tao]{huang2017centered}
Huang, L., Liu, X., Liu, Y., Lang, B., and Tao, D.
\newblock Centered weight normalization in accelerating training of deep neural
  networks.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, pp.\  2803--2811, 2017{\natexlab{b}}.
\newblock URL
  \url{https://openaccess.thecvf.com/content_iccv_2017/html/Huang_Centered_Weight_Normalization_ICCV_2017_paper.html}.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pp.\
  448--456. pmlr, 2015.
\newblock \href{https://arxiv.org/abs/1502.03167}{arXiv:1502.03167}.

\bibitem[Jia et~al.(2018)Jia, Song, He, Wang, Rong, Zhou, Xie, Guo, Yang, Yu,
  et~al.]{jia2018highly}
Jia, X., Song, S., He, W., Wang, Y., Rong, H., Zhou, F., Xie, L., Guo, Z.,
  Yang, Y., Yu, L., et~al.
\newblock Highly scalable deep learning training system with mixed-precision:
  Training imagenet in four minutes.
\newblock \emph{arXiv preprint arXiv:1807.11205}, 2018.
\newblock \href{https://arxiv.org/abs/1807.11205}{arXiv:1807.11205}.

\bibitem[Karpathy(2023)]{nanoGPT}
Karpathy, A.
\newblock nanogpt.
\newblock \url{https://github.com/karpathy/nanoGPT/}, 2023.

\bibitem[Karras et~al.(2023)Karras, Aittala, Lehtinen, Hellsten, Aila, and
  Laine]{karras2023analyzing}
Karras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila, T., and Laine, S.
\newblock Analyzing and improving the training dynamics of diffusion models.
\newblock \emph{arXiv preprint
  \href{https://arxiv.org/abs/2312.02696}{arXiv:2312.02696}}, 2023.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma15adam}
Kingma, D. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, San Diega, CA, USA, 2015.
\newblock \href{https://arxiv.org/abs/1412.6980}{arXiv:1412.6980}.

\bibitem[Kodryan et~al.(2022)Kodryan, Lobacheva, Nakhodnov, and
  Vetrov]{kodryan2022training}
Kodryan, M., Lobacheva, E., Nakhodnov, M., and Vetrov, D.~P.
\newblock Training scale-invariant neural networks on the sphere can happen in
  three regimes.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 14058--14070, 2022.
\newblock \href{https://arxiv.org/abs/2209.03695}{arXiv:2209.03695}.

\bibitem[Kosson et~al.(2024)Kosson, Fan, and Jaggi]{kosson2024ghost}
Kosson, A., Fan, D., and Jaggi, M.
\newblock Ghost noise for regularizing deep neural networks.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  38\penalty0 (12):\penalty0 13274--13282, Mar. 2024.
\newblock \doi{10.1609/aaai.v38i12.29228}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/29228}.
\newblock \href{https://arxiv.org/abs/2305.17205}{arXiv:2305.17205}.

\bibitem[Krizhevsky(2009)]{Krizhevsky2009LearningML}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{self-published}, 2009.
\newblock URL
  \url{https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}.

\bibitem[Li \& Arora(2020)Li and Arora]{li2020exponential}
Li, Z. and Arora, S.
\newblock An exponential learning rate schedule for deep learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rJg8TeSFDH}.
\newblock \href{https://arxiv.org/abs/1910.07454}{arXiv:1910.07454}.

\bibitem[Li et~al.(2020)Li, Lyu, and Arora]{li2020reconciling}
Li, Z., Lyu, K., and Arora, S.
\newblock Reconciling modern deep learning with traditional optimization
  analyses: The intrinsic learning rate.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 14544--14555, 2020.
\newblock \href{https://arxiv.org/abs/2010.02916}{arXiv:2010.02916}.

\bibitem[Li et~al.(2021)Li, Malladi, and Arora]{li2021validity}
Li, Z., Malladi, S., and Arora, S.
\newblock On the validity of modeling {SGD} with stochastic differential
  equations ({SDE}s).
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W.
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=goEdyJ_nVQI}.
\newblock \href{https://arxiv.org/abs/2102.12470}{arXiv:2102.12470}.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Bhojanapalli, Zaheer, Reddi, and
  Kumar]{li2022robust}
Li, Z., Bhojanapalli, S., Zaheer, M., Reddi, S., and Kumar, S.
\newblock Robust training of neural networks using scale invariant
  architectures.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12656--12684. PMLR, 2022{\natexlab{a}}.
\newblock \href{https://arxiv.org/abs/2202.00980}{arXiv:2202.00980}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Wang, and Yu]{li2022fast}
Li, Z., Wang, T., and Yu, D.
\newblock Fast mixing of stochastic gradient descent with normalization and
  weight decay.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 9233--9248, 2022{\natexlab{b}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2022/hash/3c215225324f9988858602dc92219615-Abstract-Conference.html}.

\bibitem[Liu et~al.(2021)Liu, Bernstein, Meister, and Yue]{liu2021nero}
Liu, Y., Bernstein, J., Meister, M., and Yue, Y.
\newblock Learning by turning: Neural architecture aware optimisation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6748--6758. PMLR, 2021.
\newblock \href{https://arxiv.org/abs/2102.07227}{arXiv:2102.07227}.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and
  Hutter]{loshchilov2018decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.
\newblock \href{https://arxiv.org/abs/1711.05101}{arXiv:1711.05101}.

\bibitem[Malladi et~al.(2022)Malladi, Lyu, Panigrahi, and Arora]{malladi2022on}
Malladi, S., Lyu, K., Panigrahi, A., and Arora, S.
\newblock On the {SDE}s and scaling rules for adaptive gradient algorithms.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=F2mhzjHkQP}.
\newblock \href{https://arxiv.org/abs/2205.10287}{arXiv:2205.10287}.

\bibitem[McCandlish et~al.(2018)McCandlish, Kaplan, Amodei, and
  Team]{mccandlish2018empirical}
McCandlish, S., Kaplan, J., Amodei, D., and Team, O.~D.
\newblock An empirical model of large-batch training.
\newblock \emph{arXiv preprint
  \href{https://arxiv.org/abs/1812.06162}{arXiv:1812.06162}}, 2018.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and
  Socher]{merity2017pointer}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=Byj72udxe}.
\newblock \href{https://arxiv.org/abs/1609.07843}{arXiv:1609.07843}.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Salakhutdinov, and
  Srebro]{neyshabur2015path}
Neyshabur, B., Salakhutdinov, R.~R., and Srebro, N.
\newblock Path-sgd: Path-normalized optimization in deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.
\newblock \href{https://arxiv.org/abs/1506.02617}{arXiv:1506.02617}.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{ott2019fairseq}
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and
  Auli, M.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock In \emph{Proceedings of NAACL-HLT 2019: Demonstrations}, 2019.
\newblock \href{https://arxiv.org/abs/1904.01038}{arXiv:1904.01038}.

\bibitem[Pagliardini(2023)]{llm_baseline}
Pagliardini, M.
\newblock llm-baseline.
\newblock \url{https://github.com/epfml/llm-baselines}, 2023.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.
\newblock \href{https://arxiv.org/abs/1912.01703}{arXiv:1912.01703}.

\bibitem[Qiao et~al.(2019)Qiao, Wang, Liu, Shen, and Yuille]{qiao2019ws}
Qiao, S., Wang, H., Liu, C., Shen, W., and Yuille, A.~L.
\newblock Weight standardization.
\newblock \emph{CoRR}, abs/1903.10520, 2019.
\newblock URL \url{http://arxiv.org/abs/1903.10520}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{self-published}, 2019.
\newblock URL
  \url{https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}.

\bibitem[Roburin et~al.(2020)Roburin, de~Mont-Marin, Bursuc, Marlet, Perez, and
  Aubry]{roburin2020spherical}
Roburin, S., de~Mont-Marin, Y., Bursuc, A., Marlet, R., Perez, P., and Aubry,
  M.
\newblock A spherical analysis of adam with batch normalization.
\newblock \emph{arXiv preprint arXiv:2006.13382}, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.13382}.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{imagenet15russakovsky}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 115\penalty0
  (3):\penalty0 211--252, 2015.
\newblock \doi{10.1007/s11263-015-0816-y}.
\newblock \href{https://arxiv.org/abs/1409.0575}{arXiv:1409.0575}.

\bibitem[Salimans \& Kingma(2016)Salimans and Kingma]{salimans2016weight}
Salimans, T. and Kingma, D.~P.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.
\newblock \href{https://arxiv.org/abs/1602.07868}{arXiv:1602.07868}.

\bibitem[Shallue et~al.(2019)Shallue, Lee, Antognini, Sohl-Dickstein, Frostig,
  and Dahl]{shallue2019measuring}
Shallue, C.~J., Lee, J., Antognini, J., Sohl-Dickstein, J., Frostig, R., and
  Dahl, G.~E.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (112):\penalty0 1--49, 2019.
\newblock \href{https://arxiv.org/abs/1811.03600}{arXiv:1811.03600}.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{3rd International Conference on Learning Representations,
  {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
  Proceedings}, 2015.
\newblock URL \url{http://arxiv.org/abs/1409.1556}.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and
  Jegou]{deit_tiny}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  10347--10357. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/touvron21a.html}.
\newblock \href{https://arxiv.org/abs/2012.12877}{arXiv:2012.12877}.

\bibitem[Van~Laarhoven(2017)]{van2017l2}
Van~Laarhoven, T.
\newblock L2 regularization versus batch and weight normalization.
\newblock \emph{arXiv preprint arXiv:1706.05350}, 2017.
\newblock URL \url{https://arxiv.org/abs/1706.05350}.

\bibitem[Wan et~al.(2021)Wan, Zhu, Zhang, and Sun]{wan2021spherical}
Wan, R., Zhu, Z., Zhang, X., and Sun, J.
\newblock Spherical motion dynamics: Learning dynamics of normalized neural
  network using sgd and weight decay.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,
  J.~W. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  6380--6391. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/326a8c055c0d04f5b06544665d8bb3ea-Paper.pdf}.
\newblock \href{https://arxiv.org/abs/2006.08419}{arXiv:2006.08419}.

\bibitem[Wightman(2019)]{rw2019timm}
Wightman, R.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem[Wu \& He(2018)Wu and He]{wu2018group}
Wu, Y. and He, K.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pp.\  3--19, 2018.
\newblock \href{https://arxiv.org/abs/1803.08494}{arXiv:1803.08494}.

\bibitem[Xie et~al.(2023)Xie, zhiqiang xu, Zhang, Sato, and
  Sugiyama]{xie2023on}
Xie, Z., zhiqiang xu, Zhang, J., Sato, I., and Sugiyama, M.
\newblock On the overlooked pitfalls of weight decay and how to mitigate them:
  A gradient-norm perspective.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=vnGcubtzR1}.
\newblock \href{https://arxiv.org/abs/2011.11152}{arXiv:2011.11152}.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you2017lars}
You, Y., Gitman, I., and Ginsburg, B.
\newblock Large batch training of convolutional networks.
\newblock \emph{arXiv preprint arXiv:1708.03888}, 2017.
\newblock URL \url{https://arxiv.org/abs/1708.03888}.

\bibitem[You et~al.(2020)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{you2019lamb}
You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X.,
  Demmel, J., Keutzer, K., and Hsieh, C.-J.
\newblock Large batch optimization for deep learning: Training bert in 76
  minutes.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=Syx4wnEtvH}.
\newblock \href{https://arxiv.org/abs/1904.00962}{arXiv:1904.00962}.

\bibitem[Zhang et~al.(2019)Zhang, Wang, Xu, and Grosse]{zhang2018three}
Zhang, G., Wang, C., Xu, B., and Grosse, R.
\newblock Three mechanisms of weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=B1lz-3Rct7}.
\newblock \href{https://arxiv.org/abs/1810.12281}{arXiv:1810.12281}.

\bibitem[Zhou et~al.(2021)Zhou, Sun, and Zhong]{zhou2021fixnorm}
Zhou, Y., Sun, Y., and Zhong, Z.
\newblock Fixnorm: Dissecting weight decay for training deep neural networks.
\newblock \emph{arXiv preprint arXiv:2103.15345}, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.15345}.

\end{thebibliography}
