@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{modelsim2019,
  title={Model Similarity Mitigates Test Set Overuse},
  author={Horia Mania and John Miller and Ludwig Schmidt and Moritz Hardt and Benjamin Recht},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.12580}
}
@PhdThesis{Gal2016Uncertainty,
  title={Uncertainty in Deep Learning},
  author={Gal, Yarin},
  year={2016},
  school={University of Cambridge}
}
@article{wilson2020case,
  title={The case for Bayesian deep learning},
  author={Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2001.10995},
  year={2020}
}

@article{jft,
  title={Revisiting Unreasonable Effectiveness of Data in Deep Learning Era},
  author={Chen Sun and Abhinav Shrivastava and Saurabh Singh and Abhinav Kumar Gupta},
  journal={2017 IEEE International Conference on Computer Vision (ICCV)},
  year={2017},
  pages={843-852}
}

@article{vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.11929}
}

@article{kumar2022fine,
  title={Fine-tuning can distort pretrained features and underperform out-of-distribution},
  author={Kumar, Ananya and Raghunathan, Aditi and Jones, Robbie and Ma, Tengyu and Liang, Percy},
  journal={arXiv preprint arXiv:2202.10054},
  year={2022}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@article{transfer2019,
  title={Do Better ImageNet Models Transfer Better?},
  author={Simon Kornblith and Jonathon Shlens and Quoc V. Le},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018},
  pages={2656-2666}
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}


@ARTICLE{nn_ensem,
  author={Hansen, L.K. and Salamon, P.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Neural network ensembles}, 
  year={1990},
  volume={12},
  number={10},
  pages={993-1001},
  doi={10.1109/34.58871}}
  
@ARTICLE{bagging,
author={Breiman, L.},
journal={Mach Learn}, 
title={Bagging predictors}, 
year={1996},
volume={24},
pages={123-140},
doi={https://doi.org/10.1007/BF00058655}}

@article{no_one_rep_rules,
  author    = {Raphael Gontijo Lopes and
               Yann Dauphin and
               Ekin D. Cubuk},
  title     = {No One Representation to Rule Them All: Overlapping Features of Training
               Methods},
  journal   = {CoRR},
  volume    = {abs/2110.12899},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.12899},
  eprinttype = {arXiv},
  eprint    = {2110.12899},
  timestamp = {Thu, 28 Oct 2021 15:25:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-12899.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{DE,
  doi = {10.48550/ARXIV.1612.01474},
  url = {https://arxiv.org/abs/1612.01474},
  author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{simclr,
  doi = {10.48550/ARXIV.2002.05709},
  url = {https://arxiv.org/abs/2002.05709},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A Simple Framework for Contrastive Learning of Visual Representations},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{DE_losslandscape,
  doi = {10.48550/ARXIV.1912.02757},
  url = {https://arxiv.org/abs/1912.02757},
  author = {Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep Ensembles: A Loss Landscape Perspective},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{byol,
  doi = {10.48550/ARXIV.2006.07733},
  url = {https://arxiv.org/abs/2006.07733},
  author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Bootstrap your own latent: A new approach to self-supervised Learning},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{cpc,
  title={Representation Learning with Contrastive Predictive Coding},
  author={A{\"a}ron van den Oord and Yazhe Li and Oriol Vinyals},
  journal={ArXiv},
  year={2018},
  volume={abs/1807.03748}
}

@inproceedings{stl10,
  title={An Analysis of Single-Layer Networks in Unsupervised Feature Learning},
  author={Adam Coates and A. Ng and Honglak Lee},
  booktitle={AISTATS},
  year={2011}
}

@misc{lpft,
  doi = {10.48550/ARXIV.2202.10054},
  url = {https://arxiv.org/abs/2202.10054},
  author = {Kumar, Ananya and Raghunathan, Aditi and Jones, Robbie and Ma, Tengyu and Liang, Percy},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{Kornblith2019DoBI,
  title={Do Better ImageNet Models Transfer Better?},
  author={Simon Kornblith and Jonathon Shlens and Quoc V. Le},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={2656-2666}
}

@inproceedings{Bachman2019LearningRB,
  title={Learning Representations by Maximizing Mutual Information Across Views},
  author={Philip Bachman and R. Devon Hjelm and William Buchwalter},
  booktitle={NeurIPS},
  year={2019}
}

@article{Wu2018UnsupervisedFL,
  title={Unsupervised Feature Learning via Non-parametric Instance Discrimination},
  author={Zhirong Wu and Yuanjun Xiong and Stella X. Yu and Dahua Lin},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
  pages={3733-3742}
}

@misc{moco,
  doi = {10.48550/ARXIV.1911.05722},
  url = {https://arxiv.org/abs/1911.05722},
  
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Momentum Contrast for Unsupervised Visual Representation Learning},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{uq_ovadia,
  doi = {10.48550/ARXIV.1906.02530},
  
  url = {https://arxiv.org/abs/1906.02530},
  
  author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D and Nowozin, Sebastian and Dillon, Joshua V. and Lakshminarayanan, Balaji and Snoek, Jasper},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{yannlecun_da_classdep,
  doi = {10.48550/ARXIV.2204.03632},
  
  url = {https://arxiv.org/abs/2204.03632},
  
  author = {Balestriero, Randall and Bottou, Leon and LeCun, Yann},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The Effects of Regularization and Data Augmentation are Class Dependent},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}


@InProceedings{group_eqnn,
  title = 	 {Group Equivariant Convolutional Networks},
  author = 	 {Cohen, Taco and Welling, Max},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2990--2999},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/cohenc16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/cohenc16.html},
  abstract = 	 {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.}
}

@inproceedings{nilsback2008automated,
  title={Automated flower classification over a large number of classes},
  author={Nilsback, Maria-Elena and Zisserman, Andrew},
  booktitle={2008 Sixth Indian Conference on Computer Vision, Graphics \& Image Processing},
  pages={722--729},
  year={2008},
  organization={IEEE}
}

@article{e2cnn,
  author    = {Maurice Weiler and
               Gabriele Cesa},
  title     = {General E(2)-Equivariant Steerable CNNs},
  journal   = {CoRR},
  volume    = {abs/1911.08251},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.08251},
  eprinttype = {arXiv},
  eprint    = {1911.08251},
  timestamp = {Mon, 02 Dec 2019 17:48:37 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-08251.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{essl,
  doi = {10.48550/ARXIV.2111.00899},
  
  url = {https://arxiv.org/abs/2111.00899},
  
  author = {Dangovski, Rumen and Jing, Li and Loh, Charlotte and Han, Seungwook and Srivastava, Akash and Cheung, Brian and Agrawal, Pulkit and Soljačić, Marin},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Image and Video Processing (eess.IV), Applied Physics (physics.app-ph), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Physical sciences, FOS: Physical sciences},
  
  title = {Equivariant Contrastive Learning},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{bronstein2021geometric,
      title={Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges}, 
      author={Michael M. Bronstein and Joan Bruna and Taco Cohen and Petar Veličković},
      year={2021},
      eprint={2104.13478},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{cohen2016group,
  title={Group equivariant convolutional networks},
  author={Cohen, Taco and Welling, Max},
  booktitle={International conference on machine learning},
  pages={2990--2999},
  year={2016},
  organization={PMLR}
}

@inproceedings{reed2021selfaug,
  author    = {Colorado Reed and
               Sean Metzger and
               Aravind Srinivas and
               Trevor Darrell and
               Kurt Keutzer},
  title     = {Evaluating Self-Supervised Pretraining Without Using Labels},
  booktitle   = {CVPR},
  year = {2021}
}

@article{tian2020makes,
  title={What makes for good views for contrastive learning?},
  author={Tian, Yonglong and Sun, Chen and Poole, Ben and Krishnan, Dilip and Schmid, Cordelia and Isola, Phillip},
  journal={arXiv preprint arXiv:2005.10243},
  year={2020}
}

@misc{equimod,
  doi = {10.48550/ARXIV.2211.01244},
  
  url = {https://arxiv.org/abs/2211.01244},
  
  author = {Devillers, Alexandre and Lefort, Mathieu},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {EquiMod: An Equivariance Module to Improve Self-Supervised Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{looc,
  author    = {Tete Xiao and
               Xiaolong Wang and
               Alexei A. Efros and
               Trevor Darrell},
  title     = {What Should Not Be Contrastive in Contrastive Learning},
  journal   = {CoRR},
  volume    = {abs/2008.05659},
  year      = {2020},
  url       = {https://arxiv.org/abs/2008.05659},
  eprinttype = {arXiv},
  eprint    = {2008.05659},
  timestamp = {Sun, 07 Feb 2021 10:38:45 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2008-05659.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{imagenet-r,
  author    = {Dan Hendrycks and
               Steven Basart and
               Norman Mu and
               Saurav Kadavath and
               Frank Wang and
               Evan Dorundo and
               Rahul Desai and
               Tyler Zhu and
               Samyak Parajuli and
               Mike Guo and
               Dawn Song and
               Jacob Steinhardt and
               Justin Gilmer},
  title     = {The Many Faces of Robustness: {A} Critical Analysis of Out-of-Distribution
               Generalization},
  journal   = {CoRR},
  volume    = {abs/2006.16241},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.16241},
  eprinttype = {arXiv},
  eprint    = {2006.16241},
  timestamp = {Wed, 01 Jul 2020 15:21:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-16241.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{imagenet-sketch,
  author    = {Haohan Wang and
               Songwei Ge and
               Eric P. Xing and
               Zachary C. Lipton},
  title     = {Learning Robust Global Representations by Penalizing Local Predictive
               Power},
  journal   = {CoRR},
  volume    = {abs/1905.13549},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.13549},
  eprinttype = {arXiv},
  eprint    = {1905.13549},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-13549.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{he2020momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9729--9738},
  year={2020}
}

@article{diversity_prediction,
  author    = {Nikita Dvornik and
               Cordelia Schmid and
               Julien Mairal},
  title     = {Diversity with Cooperation: Ensemble Methods for Few-Shot Classification},
  journal   = {CoRR},
  volume    = {abs/1903.11341},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.11341},
  eprinttype = {arXiv},
  eprint    = {1903.11341},
  timestamp = {Tue, 02 Apr 2019 11:16:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1903-11341.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{pang_diversity,
  doi = {10.48550/ARXIV.1901.08846},
  
  url = {https://arxiv.org/abs/1901.08846},
  
  author = {Pang, Tianyu and Xu, Kun and Du, Chao and Chen, Ning and Zhu, Jun},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Improving Adversarial Robustness via Promoting Ensemble Diversity},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{lee_diversity,
  author    = {Stefan Lee and
               Senthil Purushwalkam and
               Michael Cogswell and
               Viresh Ranjan and
               David J. Crandall and
               Dhruv Batra},
  title     = {Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles},
  journal   = {CoRR},
  volume    = {abs/1606.07839},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.07839},
  eprinttype = {arXiv},
  eprint    = {1606.07839},
  timestamp = {Mon, 13 Aug 2018 16:49:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LeePCRCB16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{augmix,
  doi = {10.48550/ARXIV.1912.02781},
  
  url = {https://arxiv.org/abs/1912.02781},
  
  author = {Hendrycks, Dan and Mu, Norman and Cubuk, Ekin D. and Zoph, Barret and Gilmer, Justin and Lakshminarayanan, Balaji},
  
  keywords = {Machine Learning (stat.ML), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{modelsoups,
  doi = {10.48550/ARXIV.2203.05482},
  
  url = {https://arxiv.org/abs/2203.05482},
  
  author = {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Yitzhak and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S. and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{divensem_improve_calib,
  doi = {10.48550/ARXIV.2007.04206},
  
  url = {https://arxiv.org/abs/2007.04206},
  
  author = {Stickland, Asa Cooper and Murray, Iain},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Diverse Ensembles Improve Calibration},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{mcdropout,
  doi = {10.48550/ARXIV.1506.02142},
  
  url = {https://arxiv.org/abs/1506.02142},
  
  author = {Gal, Yarin and Ghahramani, Zoubin},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{batchensemble,
  doi = {10.48550/ARXIV.2002.06715},
  
  url = {https://arxiv.org/abs/2002.06715},
  
  author = {Wen, Yeming and Tran, Dustin and Ba, Jimmy},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BatchEnsemble: An Alternative Approach to Efficient Ensemble and Lifelong Learning},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{mimo,
  doi = {10.48550/ARXIV.2010.06610},
  
  url = {https://arxiv.org/abs/2010.06610},
  
  author = {Havasi, Marton and Jenatton, Rodolphe and Fort, Stanislav and Liu, Jeremiah Zhe and Snoek, Jasper and Lakshminarayanan, Balaji and Dai, Andrew M. and Tran, Dustin},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Training independent subnetworks for robust prediction},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{thomas_tensor_2018,
	title = {Tensor field networks: {Rotation}- and translation-equivariant neural networks for {3D} point clouds},
	shorttitle = {Tensor field networks},
	url = {http://arxiv.org/abs/1802.08219},
	abstract = {We introduce tensor field neural networks, which are locally equivariant to 3D rotations, translations, and permutations of points at every layer. 3D rotation equivariance removes the need for data augmentation to identify features in arbitrary orientations. Our network uses filters built from spherical harmonics; due to the mathematical consequences of this filter choice, each layer accepts as input (and guarantees as output) scalars, vectors, and higher-order tensors, in the geometric sense of these terms. We demonstrate the capabilities of tensor field networks with tasks in geometry, physics, and chemistry.},
	urldate = {2021-06-09},
	journal = {arXiv:1802.08219},
	author = {Thomas, Nathaniel and Smidt, Tess and Kearnes, Steven and Yang, Lusann and Li, Li and Kohlhoff, Kai and Riley, Patrick},
	month = may,
	year = {2018},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{weiler_3d_2018,
	title = {{3D} {Steerable} {CNNs}: {Learning} {Rotationally} {Equivariant} {Features} in {Volumetric} {Data}},
	shorttitle = {{3D} {Steerable} {CNNs}},
	url = {http://arxiv.org/abs/1807.02547},
	abstract = {We present a convolutional network that is equivariant to rigid body motions. The model uses scalar-, vector-, and tensor fields over 3D Euclidean space to represent data, and equivariant convolutions to map between such representations. These SE(3)-equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived analytically in this paper. We prove that equivariant convolutions are the most general equivariant linear maps between fields over R{\textasciicircum}3. Our experimental results confirm the effectiveness of 3D Steerable CNNs for the problem of amino acid propensity prediction and protein structure classification, both of which have inherent SE(3) symmetry.},
	urldate = {2021-06-09},
	journal = {arXiv:1807.02547},
	author = {Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco},
	month = oct,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{weiler_general_2019,
	title = {General \${E}(2)\$-{Equivariant} {Steerable} {CNNs}},
	url = {http://arxiv.org/abs/1911.08251},
	abstract = {The big empirical success of group equivariant networks has led in recent years to the sprouting of a great variety of equivariant network architectures. A particular focus has thereby been on rotation and reflection equivariant CNNs for planar images. Here we give a general description of \$E(2)\$-equivariant convolutions in the framework of Steerable CNNs. The theory of Steerable CNNs thereby yields constraints on the convolution kernels which depend on group representations describing the transformation laws of feature spaces. We show that these constraints for arbitrary group representations can be reduced to constraints under irreducible representations. A general solution of the kernel space constraint is given for arbitrary representations of the Euclidean group \$E(2)\$ and its subgroups. We implement a wide range of previously proposed and entirely new equivariant network architectures and extensively compare their performances. \$E(2)\$-steerable convolutions are further shown to yield remarkable gains on CIFAR-10, CIFAR-100 and STL-10 when used as a drop-in replacement for non-equivariant convolutions.},
	urldate = {2021-03-02},
	journal = {arXiv:1911.08251},
	author = {Weiler, Maurice and Cesa, Gabriele},
	month = nov,
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{learning_to_see_moving,
  doi = {10.48550/ARXIV.1505.01596},
  
  url = {https://arxiv.org/abs/1505.01596},
  
  author = {Agrawal, Pulkit and Carreira, Joao and Malik, Jitendra},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning to See by Moving},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{colorful,
  doi = {10.48550/ARXIV.1603.08511},
  
  url = {https://arxiv.org/abs/1603.08511},
  
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Colorful Image Colorization},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{aet,
  doi = {10.48550/ARXIV.1901.04596},
  
  url = {https://arxiv.org/abs/1901.04596},
  
  author = {Zhang, Liheng and Qi, Guo-Jun and Wang, Liqiang and Luo, Jiebo},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {AET vs. AED: Unsupervised Representation Learning by Auto-Encoding Transformations rather than Data},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{rotnet,
  doi = {10.48550/ARXIV.1803.07728},
  
  url = {https://arxiv.org/abs/1803.07728},
  
  author = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Unsupervised Representation Learning by Predicting Image Rotations},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{jigsaw,
  doi = {10.48550/ARXIV.1603.09246},
  
  url = {https://arxiv.org/abs/1603.09246},
  
  author = {Noroozi, Mehdi and Favaro, Paolo},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{dice,
  doi = {10.48550/ARXIV.2101.05544},
  
  url = {https://arxiv.org/abs/2101.05544},
  
  author = {Rame, Alexandre and Cord, Matthieu},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Information Theory (cs.IT), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {DICE: Diversity in Deep Ensembles via Conditional Redundancy Adversarial Estimation},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@InProceedings{pred_uncertainty_challenge,
author="Qui{\~{n}}onero-Candela, Joaquin
and Rasmussen, Carl Edward
and Sinz, Fabian
and Bousquet, Olivier
and Sch{\"o}lkopf, Bernhard",
editor="Qui{\~{n}}onero-Candela, Joaquin
and Dagan, Ido
and Magnini, Bernardo
and d'Alch{\'e}-Buc, Florence",
title="Evaluating Predictive Uncertainty Challenge",
booktitle="Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--27",
abstract="This Chapter presents the PASCAL Evaluating Predictive Uncertainty Challenge, introduces the contributed Chapters by the participants who obtained outstanding results, and provides a discussion with some lessons to be learnt. The Challenge was set up to evaluate the ability of Machine Learning algorithms to provide good ``probabilistic predictions'', rather than just the usual ``point predictions'' with no measure of uncertainty, in regression and classification problems. Parti-cipants had to compete on a number of regression and classification tasks, and were evaluated by both traditional losses that only take into account point predictions and losses we proposed that evaluate the quality of the probabilistic predictions.",
isbn="978-3-540-33428-6"}

@misc{yarin_uq,
  doi = {10.48550/ARXIV.1703.02910},
  
  url = {https://arxiv.org/abs/1703.02910},
  
  author = {Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deep Bayesian Active Learning with Image Data},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{bald_uq,
  doi = {10.48550/ARXIV.1112.5745},
  
  url = {https://arxiv.org/abs/1112.5745},
  
  author = {Houlsby, Neil and Huszár, Ferenc and Ghahramani, Zoubin and Lengyel, Máté},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Bayesian Active Learning for Classification and Preference Learning},
  
  publisher = {arXiv},
  
  year = {2011},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{hyperparameter_ens,
  doi = {10.48550/ARXIV.2006.13570},
  
  url = {https://arxiv.org/abs/2006.13570},
  
  author = {Wenzel, Florian and Snoek, Jasper and Tran, Dustin and Jenatton, Rodolphe},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Hyperparameter Ensembles for Robustness and Uncertainty Quantification},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{kendall_diversity,
  doi = {10.48550/ARXIV.1703.04977},
  
  url = {https://arxiv.org/abs/1703.04977},
  
  author = {Kendall, Alex and Gal, Yarin},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Wenzel2022AssayingOG,
  title={Assaying Out-Of-Distribution Generalization in Transfer Learning},
  author={Florian Wenzel and Andrea Dittadi and Peter V. Gehler and Carl-Johann Simon-Gabriel and Max Horn and Dominik Zietlow and David Kernert and Chris Russell and Thomas Brox and Bernt Schiele and Bernhard Sch\"olkopf and Francesco Locatello},
  booktitle={Neural Information Processing Systems},
  year={2022},
}

@InProceedings{inat,
author = {
Van Horn, Grant and Mac Aodha, Oisin and Song, Yang and Cui, Yin and Sun, Chen
and Shepard, Alex and Adam, Hartwig and Perona, Pietro and Belongie, Serge},
title = {The INaturalist Species Classification and Detection Dataset},
booktitle = {
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@misc{moe,
      title={Scaling Vision with Sparse Mixture of Experts}, 
      author={Carlos Riquelme and Joan Puigcerver and Basil Mustafa and Maxim Neumann and Rodolphe Jenatton and André Susano Pinto and Daniel Keysers and Neil Houlsby},
      year={2021},
      eprint={2106.05974},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{moe2,
  doi = {10.48550/ARXIV.2206.02770},
  
  url = {https://arxiv.org/abs/2206.02770},
  
  author = {Mustafa, Basil and Riquelme, Carlos and Puigcerver, Joan and Jenatton, Rodolphe and Houlsby, Neil},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{moe_ensemble,
  author    = {James Urquhart Allingham and
               Florian Wenzel and
               Zelda E. Mariet and
               Basil Mustafa and
               Joan Puigcerver and
               Neil Houlsby and
               Ghassen Jerfel and
               Vincent Fortuin and
               Balaji Lakshminarayanan and
               Jasper Snoek and
               Dustin Tran and
               Carlos Riquelme Ruiz and
               Rodolphe Jenatton},
  title     = {Sparse MoEs meet Efficient Ensembles},
  booktitle = {Transactions on Machine Learning Research},
  year      = {2022},
}


@article{WOLPERT1992241,
title = {Stacked generalization},
journal = {Neural Networks},
volume = {5},
number = {2},
pages = {241-259},
year = {1992},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(05)80023-1},
url = {https://www.sciencedirect.com/science/article/pii/S0893608005800231},
author = {David H. Wolpert},
keywords = {Generalization and induction, Combining generalizers, Learning set preprocessing, cross-validation, Error estimation and correction},
abstract = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.}
}