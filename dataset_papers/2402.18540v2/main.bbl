\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Askell et~al.(2021)Askell, Bai, Chen, Drain, Ganguli, Henighan, Jones, Joseph, Mann, DasSarma, et~al.]{askell2021general}
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et~al.
\newblock A general language assistant as a laboratory for alignment.
\newblock \emph{arXiv preprint arXiv:2112.00861}, 2021.

\bibitem[Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}, 2022{\natexlab{a}}.

\bibitem[Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022{\natexlab{b}}.

\bibitem[Bianchi et~al.(2023)Bianchi, Suzgun, Attanasio, Rottger, Jurafsky, Hashimoto, and Zou]{bianchi2023safety}
Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Rottger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou.
\newblock Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Chao et~al.(2023)Chao, Robey, Dobriban, Hassani, Pappas, and Wong]{chao2023jailbreaking}
Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George~J Pappas, and Eric Wong.
\newblock Jailbreaking black box large language models in twenty queries.
\newblock \emph{arXiv preprint arXiv:2310.08419}, 2023.

\bibitem[Chao et~al.(2024)Chao, Debenedetti, Robey, Andriushchenko, Croce, Sehwag, Dobriban, Flammarion, Pappas, Tramèr, Hassani, and Wong]{chao2024jailbreakbench}
Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George~J. Pappas, Florian Tramèr, Hamed Hassani, and Eric Wong.
\newblock Jailbreakbench: An open robustness benchmark for jailbreaking large language models, 2024.

\bibitem[Chen et~al.(2023)Chen, Shu, Shareghi, Collier, Narasimhan, and Yao]{chen2023fireact}
Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao.
\newblock Fireact: Toward language agent fine-tuning.
\newblock \emph{arXiv preprint arXiv:2310.05915}, 2023.

\bibitem[Chen et~al.(2024)Chen, Piet, Sitawarin, and Wagner]{chen2024struq}
Sizhe Chen, Julien Piet, Chawin Sitawarin, and David Wagner.
\newblock Struq: Defending against prompt injection with structured queries.
\newblock \emph{arXiv preprint arXiv:2402.06363}, 2024.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{allenai:arc}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv:1803.05457v1}, 2018.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Deng et~al.(2023)Deng, Zhang, Pan, and Bing]{deng2023multilingual}
Yue Deng, Wenxuan Zhang, Sinno~Jialin Pan, and Lidong Bing.
\newblock Multilingual jailbreak challenges in large language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[facebookresearch(2023)]{facebookresearch_2023}
facebookresearch.
\newblock Llama 2 post launch updates, Aug 2023.
\newblock URL \url{https://github.com/facebookresearch/llama/blob/008385a65aecfe5c14b5abc9e47c558c0fbe18ec/UPDATES.md}.

\bibitem[Ganguli et~al.(2023)Ganguli, Askell, Schiefer, Liao, Luko{\v{s}}i{\=u}t{\.e}, Chen, Goldie, Mirhoseini, Olsson, Hernandez, et~al.]{ganguli2023capacity}
Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamil{\.e} Luko{\v{s}}i{\=u}t{\.e}, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, et~al.
\newblock The capacity for moral self-correction in large language models.
\newblock \emph{arXiv preprint arXiv:2302.07459}, 2023.

\bibitem[Gao et~al.(2021)Gao, Tow, Biderman, Black, DiPofi, Foster, Golding, Hsu, McDonell, Muennighoff, Phang, Reynolds, Tang, Thite, Wang, Wang, and Zou]{eval-harness}
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
\newblock A framework for few-shot language model evaluation, September 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5371628}.

\bibitem[Hartford(2023)]{wizard-vicuna-30b-uncensored}
Eric Hartford.
\newblock Wizard vicuna 30b uncensored.
\newblock \url{https://huggingface.co/cognitivecomputations/Wizard-Vicuna-30B-Uncensored}, 2023.

\bibitem[He et~al.(2024)He, Xia, and Henderson]{he2024what}
Luxi He, Mengzhou Xia, and Peter Henderson.
\newblock What's in your" safe" data?: Identifying benign data that breaks safety.
\newblock \emph{arXiv preprint arXiv:2404.01099}, 2024.

\bibitem[Hsu et~al.(2024)Hsu, Tsai, Lin, Chen, Yu, and Huang]{hsu2024safe}
Chia-Yi Hsu, Yu-Lin Tsai, Chih-Hsun Lin, Pin-Yu Chen, Chia-Mu Yu, and Chun-Ying Huang.
\newblock Safe {LoRA}: The silver lining of reducing safety risks when finetuning large language models.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information Processing Systems}, 2024.
\newblock URL \url{https://openreview.net/forum?id=HcifdQZFZV}.

\bibitem[Huang et~al.(2024{\natexlab{a}})Huang, Hu, Ilhan, Tekin, and Liu]{huang2024lisa}
Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim~Furkan Tekin, and Ling Liu.
\newblock Lisa: Lazy safety alignment for large language models against harmful fine-tuning attack.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information Processing Systems}, 2024{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=RPChapuXlC}.

\bibitem[Huang et~al.(2024{\natexlab{b}})Huang, Hu, and Liu]{huang2024vaccine}
Tiansheng Huang, Sihao Hu, and Ling Liu.
\newblock Vaccine: Perturbation-aware alignment for large language model.
\newblock \emph{arXiv preprint arXiv:2402.01109}, 2024{\natexlab{b}}.

\bibitem[Huang et~al.(2023)Huang, Gupta, Xia, Li, and Chen]{huang2023catastrophic}
Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen.
\newblock Catastrophic jailbreak of open-source llms via exploiting generation.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Inan et~al.(2023)Inan, Upasani, Chi, Rungta, Iyer, Mao, Tontchev, Hu, Fuller, Testuggine, et~al.]{inan2023llamaguard}
Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et~al.
\newblock Llama guard: Llm-based input-output safeguard for human-ai conversations.
\newblock \emph{arXiv preprint arXiv:2312.06674}, 2023.

\bibitem[Jain et~al.(2023)Jain, Schwarzschild, Wen, Somepalli, Kirchenbauer, Chiang, Goldblum, Saha, Geiping, and Goldstein]{jain2023baseline}
Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein.
\newblock Baseline defenses for adversarial attacks against aligned language models.
\newblock \emph{arXiv preprint arXiv:2309.00614}, 2023.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness, Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska, Hassabis, Clopath, Kumaran, and Hadsell]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei~A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 114\penalty0 (13):\penalty0 3521--3526, 2017.
\newblock \doi{10.1073/pnas.1611835114}.
\newblock URL \url{https://www.pnas.org/doi/abs/10.1073/pnas.1611835114}.

\bibitem[Lermen et~al.(2023{\natexlab{a}})Lermen, Rogers-Smith, and Ladish]{70blora}
Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish.
\newblock Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b.
\newblock \emph{arXiv preprint arXiv:2310.20624}, 2023{\natexlab{a}}.

\bibitem[Lermen et~al.(2023{\natexlab{b}})Lermen, Rogers-Smith, and Ladish]{lermen2023lora}
Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish.
\newblock {LoRA} fine-tuning efficiently undoes safety training in llama 2-chat 70b.
\newblock \emph{arXiv preprint arXiv:2310.20624}, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Zhang, Dubois, Taori, Gulrajani, Guestrin, Liang, and Hashimoto]{alpaca_eval}
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock {AlpacaEval}: An automatic evaluator of instruction-following models.
\newblock \url{https://github.com/tatsu-lab/alpaca_eval}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Li, Zhang, Dan, Jiang, and Zhang]{li2023chatdoctor}
Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, and You Zhang.
\newblock Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge.
\newblock \emph{Cureus}, 15\penalty0 (6), 2023{\natexlab{b}}.

\bibitem[Lian et~al.(2023)Lian, Goodson, Pentland, Cook, Vong, and "Teknium"]{OpenOrca}
Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and "Teknium".
\newblock Openorca: An open dataset of gpt augmented flan reasoning traces.
\newblock \url{https://https://huggingface.co/Open-Orca/OpenOrca}, 2023.

\bibitem[Lin et~al.(2023)Lin, Ravichander, Lu, Dziri, Sclar, Chandu, Bhagavatula, and Choi]{lin2023unlocking}
Bill~Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi.
\newblock The unlocking spell on base llms: Rethinking alignment via in-context learning.
\newblock \emph{arXiv preprint arXiv:2312.01552}, 2023.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le, Zoph, Wei, and Roberts]{longpre2023flan}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny Zhou, Quoc~V. Le, Barret Zoph, Jason Wei, and Adam Roberts.
\newblock The flan collection: Designing data and methods for effective instruction tuning, 2023.

\bibitem[Luo et~al.(2023)Luo, Yang, Meng, Li, Zhou, and Zhang]{luo2023empirical}
Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang.
\newblock An empirical study of catastrophic forgetting in large language models during continual fine-tuning.
\newblock \emph{arXiv preprint arXiv:2308.08747}, 2023.

\bibitem[Mehrotra et~al.(2023)Mehrotra, Zampetakis, Kassianik, Nelson, Anderson, Singer, and Karbasi]{mehrotra2023tree}
Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi.
\newblock Tree of attacks: Jailbreaking black-box {LLMs} automatically.
\newblock \emph{arXiv preprint arXiv:2312.02119}, 2023.

\bibitem[{Mistral AI}(2024)]{mistralai_guardrailing}
{Mistral AI}.
\newblock Guardrailing.
\newblock \url{https://docs.mistral.ai/platform/guardrailing/}, 2024.
\newblock Accessed: 2024-02-16.

\bibitem[Mitra et~al.(2024)Mitra, Khanpour, Rosset, and Awadallah]{mitra2024orcamath}
Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah.
\newblock Orca-math: Unlocking the potential of slms in grade school math, 2024.

\bibitem[MosaicML(2023)]{MosaicML2023Introducing}
MosaicML.
\newblock Introducing {MPT}-30b: Raising the bar for open-source foundation models, 2023.
\newblock URL \url{https://www.mosaicml.com/blog/mpt-30b}.
\newblock Accessed: 2023-06-22.

\bibitem[Mukherjee et~al.(2023)Mukherjee, Mitra, Jawahar, Agarwal, Palangi, and Awadallah]{mukherjee2023orca}
Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah.
\newblock Orca: Progressive learning from complex explanation traces of gpt-4, 2023.

\bibitem[Mukhoti et~al.(2024)Mukhoti, Gal, Torr, and Dokania]{mukhoti2024finetuning}
Jishnu Mukhoti, Yarin Gal, Philip Torr, and Puneet~K. Dokania.
\newblock Fine-tuning can cripple your foundation model; preserving features may be the solution.
\newblock \emph{Transactions on Machine Learning Research}, 2024.
\newblock ISSN 2835-8856.
\newblock Featured Certification.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Pelrine et~al.(2023)Pelrine, Taufeeque, Zaj{\k{a}}c, McLean, and Gleave]{novelgpt}
Kellin Pelrine, Mohammad Taufeeque, Micha{\l} Zaj{\k{a}}c, Euan McLean, and Adam Gleave.
\newblock Exploiting novel {GPT}-4 {APIs}.
\newblock \emph{arXiv preprint arXiv:2312.14302}, 2023.

\bibitem[Peng et~al.(2023{\natexlab{a}})Peng, Wu, Allard, Kilpatrick, and Heidel]{peng2023gpt}
Andrew Peng, Michael Wu, John Allard, Logan Kilpatrick, and Steven Heidel.
\newblock {GPT-3.5 Turbo fine-tuning and API updates}.
\newblock \url{https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates}, 2023{\natexlab{a}}.

\bibitem[Peng et~al.(2023{\natexlab{b}})Peng, Li, He, Galley, and Gao]{peng2023instruction}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
\newblock Instruction tuning with gpt-4.
\newblock \emph{arXiv preprint arXiv:2304.03277}, 2023{\natexlab{b}}.

\bibitem[Qi et~al.(2024)Qi, Zeng, Xie, Chen, Jia, Mittal, and Henderson]{qi2024finetuning}
Xiangyu Qi, Yi~Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson.
\newblock Fine-tuning aligned language models compromises safety, even when users do not intend to!
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=hTEGyKf0dZ}.

\bibitem[Rosati et~al.(2024{\natexlab{a}})Rosati, Wehner, Williams, Bartoszcze, Gonzales, carsten maple, Majumdar, Sajjad, and Rudzicz]{rosati2024representation}
Domenic Rosati, Jan Wehner, Kai Williams, Lukasz Bartoszcze, Robie Gonzales, carsten maple, Subhabrata Majumdar, Hassan Sajjad, and Frank Rudzicz.
\newblock Representation noising: A defence mechanism against harmful finetuning.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information Processing Systems}, 2024{\natexlab{a}}.

\bibitem[Rosati et~al.(2024{\natexlab{b}})Rosati, Wehner, Williams, Bartoszcze, Sajjad, and Rudzicz]{rosati2024immunization}
Domenic Rosati, Jan Wehner, Kai Williams, Lukasz Bartoszcze, Hassan Sajjad, and Frank Rudzicz.
\newblock Immunization against harmful fine-tuning attacks.
\newblock In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, \emph{Findings of the Association for Computational Linguistics: EMNLP 2024}, pages 5234--5247, Miami, Florida, USA, November 2024{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.findings-emnlp.301}.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Wallace et~al.(2024)Wallace, Xiao, Leike, Weng, Heidecke, and Beutel]{wallace2024instruction}
Eric Wallace, Kai Xiao, Reimar Leike, Lilian Weng, Johannes Heidecke, and Alex Beutel.
\newblock The instruction hierarchy: Training llms to prioritize privileged instructions.
\newblock \emph{arXiv preprint arXiv:2404.13208}, 2024.

\bibitem[Wang et~al.(2024)Wang, Li, Li, Qi, Chen, Hu, Li, Li, and Xiao]{wang2024mitigating}
Jiongxiao Wang, Jiazhao Li, Yiquan Li, Xiangyu Qi, Muhao Chen, Junjie Hu, Yixuan Li, Bo~Li, and Chaowei Xiao.
\newblock Mitigating fine-tuning jailbreak attack with backdoor enhanced alignment.
\newblock \emph{arXiv preprint arXiv:2402.14968}, 2024.

\bibitem[Wei et~al.(2023)Wei, Wang, Li, Mo, and Wang]{wei2023jailbreak}
Zeming Wei, Yifei Wang, Ang Li, Yichuan Mo, and Yisen Wang.
\newblock Jailbreak and guard aligned language models with only few in-context demonstrations.
\newblock \emph{arXiv preprint arXiv:2310.06387}, 2023.

\bibitem[Wu et~al.(2023)Wu, Xie, Yi, Shao, Curl, Lyu, Chen, and Xie]{wu2023defending}
Fangzhao Wu, Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, and Xing Xie.
\newblock Defending chatgpt against jailbreak attack via self-reminder.
\newblock 2023.

\bibitem[Xie et~al.(2023)Xie, Yi, Shao, Curl, Lyu, Chen, Xie, and Wu]{xie2023defending}
Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, Xing Xie, and Fangzhao Wu.
\newblock Defending {ChatGPT} against jailbreak attack via self-reminders.
\newblock \emph{Nature Machine Intelligence}, pages 1--11, 2023.

\bibitem[Yang et~al.(2023)Yang, Wang, Zhang, Petzold, Wang, Zhao, and Lin]{yang2023shadow}
Xianjun Yang, Xiao Wang, Qi~Zhang, Linda Petzold, William~Yang Wang, Xun Zhao, and Dahua Lin.
\newblock Shadow alignment: The ease of subverting safely-aligned language models.
\newblock \emph{arXiv preprint arXiv:2310.02949}, 2023.

\bibitem[Yong et~al.(2023)Yong, Menghini, and Bach]{yong2023low}
Zheng~Xin Yong, Cristina Menghini, and Stephen Bach.
\newblock Low-resource languages jailbreak gpt-4.
\newblock In \emph{Socially Responsible Language Modelling Research}, 2023.

\bibitem[Zeng et~al.(2024)Zeng, Lin, Zhang, Yang, Jia, and Shi]{zeng2024johnny}
Yi~Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi.
\newblock How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.
\newblock \emph{arXiv preprint arXiv:2401.06373}, 2024.

\bibitem[Zhan et~al.(2023)Zhan, Fang, Bindu, Gupta, Hashimoto, and Kang]{zhan2023removing}
Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, and Daniel Kang.
\newblock Removing {RLHF} protections in {GPT-4} via fine-tuning.
\newblock \emph{arXiv preprint arXiv:2311.05553}, 2023.

\bibitem[Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and Artzi]{zhang2019bertscore}
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi.
\newblock Bertscore: Evaluating text generation with bert.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Yang, Ke, and Huang]{zhang2023defending}
Zhexin Zhang, Junxiao Yang, Pei Ke, and Minlie Huang.
\newblock Defending large language models against jailbreaking attacks through goal prioritization.
\newblock \emph{arXiv preprint arXiv:2311.09096}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Shen, Tao, Cheng, and Zhang]{zhang2023make}
Zhuo Zhang, Guangyu Shen, Guanhong Tao, Siyuan Cheng, and Xiangyu Zhang.
\newblock Make them spill the beans! coercive knowledge extraction from (production) llms.
\newblock \emph{arXiv preprint arXiv:2312.04782}, 2023{\natexlab{b}}.

\bibitem[Zhao et~al.(2023)Zhao, Deng, Madras, Zou, and Ren]{zhao2023learningandforgetting}
Jiachen Zhao, Zhun Deng, David Madras, James Zou, and Mengye Ren.
\newblock Learning and forgetting unsafe examples in large language models.
\newblock \emph{arXiv preprint arXiv:2312.12736}, 2023.

\bibitem[Zheng et~al.(2024)Zheng, Yin, Zhou, Meng, Zhou, Chang, Huang, and Peng]{zheng2024prompt}
Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, and Nanyun Peng.
\newblock On prompt-driven safeguarding for large language models.
\newblock \emph{arXiv preprint arXiv:2401.18018}, 2024.

\bibitem[Zhu et~al.(2015)Zhu, Kiros, Zemel, Salakhutdinov, Urtasun, Torralba, and Fidler]{Zhu_2015_ICCVbookcorpus}
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.
\newblock Aligning books and movies: Towards story-like visual explanations by watching movies and reading books.
\newblock In \emph{The IEEE International Conference on Computer Vision (ICCV)}, December 2015.

\bibitem[Zong et~al.(2024)Zong, Bohdal, Yu, Yang, and Hospedales]{zong2024safety}
Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, and Timothy Hospedales.
\newblock Safety fine-tuning at (almost) no cost: A baseline for vision large language models.
\newblock \emph{arXiv preprint arXiv:2402.02207}, 2024.

\bibitem[Zou et~al.(2023)Zou, Wang, Kolter, and Fredrikson]{zou2023universal}
Andy Zou, Zifan Wang, J~Zico Kolter, and Matt Fredrikson.
\newblock Universal and transferable adversarial attacks on aligned language models.
\newblock \emph{arXiv preprint arXiv:2307.15043}, 2023.

\end{thebibliography}
