\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{FeatKD-VID}
Sungsoo Ahn, Shell~Xu Hu, Andreas~C. Damianou, Neil~D. Lawrence, and Zhenwen
  Dai.
\newblock Variational information distillation for knowledge transfer.
\newblock In {\em {IEEE} Conference on Computer Vision and Pattern
  Recognition}, pages 9163--9171, 2019.

\bibitem{OnlineKD}
Rohan Anil, Gabriel Pereyra, Alexandre Passos, R{\'{o}}bert Orm{\'{a}}ndi,
  George~E. Dahl, and Geoffrey~E. Hinton.
\newblock Large scale distributed neural network training through online
  distillation.
\newblock In {\em The 6th International Conference on Learning
  Representations}, 2018.

\bibitem{ModelCompression}
Cristian Bucila, Rich Caruana, and Alexandru Niculescu{-}Mizil.
\newblock Model compression.
\newblock In {\em Proceedings of the Twelfth {ACM} {SIGKDD} International
  Conference on Knowledge Discovery and Data Mining}, pages 535--541, 2006.

\bibitem{ReviewKD}
Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia.
\newblock Distilling knowledge via knowledge review.
\newblock In {\em {IEEE} Conference on Computer Vision and Pattern
  Recognition}, pages 5008--5017, 2021.

\bibitem{KDConcept}
Xu Cheng, Zhefan Rao, Yilan Chen, and Quanshi Zhang.
\newblock Explaining knowledge distillation by quantifying the knowledge.
\newblock In {\em 2020 {IEEE/CVF} Conference on Computer Vision and Pattern
  Recognition}, pages 12922--12932, 2020.

\bibitem{KDEfficacy}
Jang~Hyun Cho and Bharath Hariharan.
\newblock On the efficacy of knowledge distillation.
\newblock In {\em {IEEE/CVF} International Conference on Computer Vision},
  pages 4793--4801, 2019.

\bibitem{KDSemiParametric}
Tri Dao, Govinda~M. Kamath, Vasilis Syrgkanis, and Lester Mackey.
\newblock Knowledge distillation as semiparametric inference.
\newblock In {\em The 9th International Conference on Learning
  Representations}, 2021.

\bibitem{KendallTau-Int}
Ronald Fagin, Ravi Kumar, and D. Sivakumar.
\newblock Comparing top k lists.
\newblock {\em SIAM Journal on Discrete Mathematics}, 17(1):134--160, 2003.

\bibitem{BAN}
Tommaso Furlanello, Zachary~Chase Lipton, Michael Tschannen, Laurent Itti, and
  Anima Anandkumar.
\newblock Born-again neural networks.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning}, volume~80, pages 1602--1611, 2018.

\bibitem{ResKD-NC}
Mengya Gao, Yujun Wang, and Liang Wan.
\newblock Residual error based knowledge distillation.
\newblock {\em Neurocomputing}, 433:154--161, 2021.

\bibitem{ResNet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em {IEEE} Conference on Computer Vision and Pattern
  Recognition}, pages 770--778, 2016.

\bibitem{FeatKD-AB}
Byeongho Heo, Minsik Lee, Sangdoo Yun, and Jin~Young Choi.
\newblock Knowledge transfer via distillation of activation boundaries formed
  by hidden neurons.
\newblock In {\em The Thirty-Third {AAAI} Conference on Artificial
  Intelligence}, pages 3779--3787, 2019.

\bibitem{KD}
Geoffrey~E. Hinton, Oriol Vinyals, and Jeffrey Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em CoRR}, abs/1503.02531, 2015.

\bibitem{MobileNet}
Andrew~G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock {\em CoRR}, abs/1704.04861, 2017.

\bibitem{GeneralizeBound}
Daniel Hsu, Ziwei Ji, Matus Telgarsky, and Lan Wang.
\newblock Generalization bounds via distillation.
\newblock In {\em The 9th International Conference on Learning
  Representations}, 2021.

\bibitem{RevisitKD}
Zhen Huang, Xu Shen, Jun Xing, Tongliang Liu, Xinmei Tian, Houqiang Li, Bing
  Deng, Jianqiang Huang, and Xian{-}Sheng Hua.
\newblock Revisiting knowledge distillation: An inheritance and exploration
  framework.
\newblock In {\em {IEEE} Conference on Computer Vision and Pattern
  Recognition}, pages 3579--3588, 2021.

\bibitem{FeatKD-NST}
Zehao Huang and Naiyan Wang.
\newblock Like what you like: Knowledge distill via neuron selectivity
  transfer.
\newblock {\em CoRR}, abs/1707.01219, 2017.

\bibitem{KDWide}
Guangda Ji and Zhanxing Zhu.
\newblock Knowledge distillation in wide neural networks: Risk bound, data
  efficiency and imperfect teacher.
\newblock In {\em Advances in Neural Information Processing Systems 33}, 2020.

\bibitem{dogs}
Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei fei Li.
\newblock Novel dataset for fine-grained image categorization.
\newblock In {\em First Workshop on Fine-Grained Visual Categorization, CVPR
  (2011)}.

\bibitem{SelfKD-PSKD}
Kyungyul Kim, ByeongMoon Ji, Doyoung Yoon, and Sangheum Hwang.
\newblock Self-knowledge distillation with progressive refinement of targets.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pages 6567--6576, 2021.

\bibitem{cifar}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock 2012.

\bibitem{AlexNet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in Neural Information Processing Systems 25}, pages
  1106--1114, 2012.

\bibitem{ParamKD-L2SP}
Xuhong Li, Yves Grandvalet, and Franck Davoine.
\newblock Explicit inductive bias for transfer learning with convolutional
  networks.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning}, pages 2830--2839, 2018.

\bibitem{ResKD-TIP}
Xuewei Li, Songyuan Li, Bourahla Omar, Fei Wu, and Xi Li.
\newblock Reskd: Residual-guided knowledge distillation.
\newblock {\em IEEE Transactions on Image Processing}, 30:4735--4746, 2021.

\bibitem{KDConsistency}
Ruofan Liang, Tianlin Li, Longfei Li, Jing Wang, and Quanshi Zhang.
\newblock Knowledge consistency between neural networks and beyond.
\newblock In {\em The 8th International Conference on Learning
  Representations}, 2020.

\bibitem{ParamKD-KR}
Junjie Liu, Dongchao Wen, Hongxing Gao, Wei Tao, Tse{-}Wei Chen, Kinya Osa, and
  Masami Kato.
\newblock Knowledge representing: Efficient, sparse representation of prior
  knowledge for knowledge distillation.
\newblock In {\em {IEEE} Conference on Computer Vision and Pattern Recognition
  Workshops}, pages 638--646, 2019.

\bibitem{ReKD-Graph}
Yufan Liu, Jiajiong Cao, Bing Li, Chunfeng Yuan, Weiming Hu, Yangxi Li, and
  Yunqiang Duan.
\newblock Knowledge distillation via instance relationship graph.
\newblock In {\em {IEEE} Conference on Computer Vision and Pattern
  Recognition}, pages 7096--7104, 2019.

\bibitem{KDPrivileged}
David Lopez{-}Paz, L{\'{e}}on Bottou, Bernhard Sch{\"{o}}lkopf, and Vladimir
  Vapnik.
\newblock Unifying distillation and privileged information.
\newblock In {\em The 4th International Conference on Learning
  Representations}, 2016.

\bibitem{ShuffleNetV2}
Ningning Ma, Xiangyu Zhang, Hai{-}Tao Zheng, and Jian Sun.
\newblock Shufflenet {V2:} practical guidelines for efficient {CNN}
  architecture design.
\newblock In {\em Computer Vision - {ECCV} 2018 - 15th European Conference},
  pages 122--138, 2018.

\bibitem{StasticalKD}
Aditya~Krishna Menon, Ankit~Singh Rawat, Sashank~J. Reddi, Seungyeon Kim, and
  Sanjiv Kumar.
\newblock A statistical perspective on distillation.
\newblock In {\em Proceedings of the 38th International Conference on Machine
  Learning}, pages 7632--7642, 2021.

\bibitem{TAKD}
Seyed{-}Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro
  Matsukawa, and Hassan Ghasemzadeh.
\newblock Improved knowledge distillation via teacher assistant.
\newblock In {\em The Thirty-Fourth {AAAI} Conference on Artificial
  Intelligence}, pages 5191--5198, 2020.

\bibitem{WhenLSHelp}
Rafael M{\"{u}}ller, Simon Kornblith, and Geoffrey~E. Hinton.
\newblock When does label smoothing help?
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  4696--4705, 2019.

\bibitem{FeatKD-PKT}
Nikolaos Passalis and Anastasios Tefas.
\newblock Learning deep representations with probabilistic knowledge transfer.
\newblock In {\em Computer Vision - {ECCV} 2018 - 15th European Conference},
  volume 11215, pages 283--299, 2018.

\bibitem{ReKD-CC}
Baoyun Peng, Xiao Jin, Dongsheng Li, Shunfeng Zhou, Yichao Wu, Jiaheng Liu,
  Zhaoning Zhang, and Yu Liu.
\newblock Correlation congruence for knowledge distillation.
\newblock In {\em 2019 {IEEE/CVF} International Conference on Computer Vision},
  pages 5006--5015, 2019.

\bibitem{TowardsKD}
Mary Phuong and Christoph Lampert.
\newblock Towards understanding knowledge distillation.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, pages 5142--5151, 2019.

\bibitem{FeatKD-FitNet}
Adriana Romero, Nicolas Ballas, Samira~Ebrahimi Kahou, Antoine Chassang, Carlo
  Gatta, and Yoshua Bengio.
\newblock Fitnets: Hints for thin deep nets.
\newblock In {\em The 3rd International Conference on Learning
  Representations}, 2015.

\bibitem{MobileNetV2}
Mark Sandler, Andrew~G. Howard, Menglong Zhu, Andrey Zhmoginov, and
  Liang{-}Chieh Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In {\em 2018 {IEEE} Conference on Computer Vision and Pattern
  Recognition}, pages 4510--4520, 2018.

\bibitem{ISLS}
Zhiqiang Shen, Zechun Liu, Dejia Xu, Zitian Chen, Kwang{-}Ting Cheng, and
  Marios Savvides.
\newblock Is label smoothing truly incompatible with knowledge distillation: An
  empirical study.
\newblock In {\em The 9th International Conference on Learning
  Representations}, 2021.

\bibitem{VGG}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em 3rd International Conference on Learning Representations},
  2015.

\bibitem{ImproveKD}
Jiaxi Tang, Rakesh Shivanna, Zhe Zhao, Dong Lin, Anima Singh, Ed~H. Chi, and
  Sagar Jain.
\newblock Understanding and improving knowledge distillation.
\newblock {\em CoRR}, abs/2002.03532, 2020.

\bibitem{KWS-ResNet}
Raphael Tang and Jimmy Lin.
\newblock Deep residual learning for small-footprint keyword spotting.
\newblock In {\em {IEEE} International Conference on Acoustics, Speech and
  Signal Processing}, pages 5484--5488, 2018.

\bibitem{MeanTeacher}
Antti Tarvainen and Harri Valpola.
\newblock Mean teachers are better role models: Weight-averaged consistency
  targets improve semi-supervised deep learning results.
\newblock In {\em The 5th International Conference on Learning
  Representations}, 2017.

\bibitem{TinyImageNet}
Amirhossein Tavanaei.
\newblock Embedded encoder-decoder in convolutional networks towards
  explainable {AI}.
\newblock {\em CoRR}, abs/2007.06712, 2020.

\bibitem{ReKD-CRD}
Yonglong Tian, Dilip Krishnan, and Phillip Isola.
\newblock Contrastive representation distillation.
\newblock In {\em The 8th International Conference on Learning
  Representations}, 2020.

\bibitem{ReKD-SP}
Frederick Tung and Greg Mori.
\newblock Similarity-preserving knowledge distillation.
\newblock In {\em 2019 {IEEE/CVF} International Conference on Computer Vision},
  pages 1365--1374, 2019.

\bibitem{DoDeep}
Gregor Urban, Krzysztof~J. Geras, Samira~Ebrahimi Kahou, {\"{O}}zlem Aslan,
  Shengjie Wang, Abdelrahman Mohamed, Matthai Philipose, Matthew Richardson,
  and Rich Caruana.
\newblock Do deep convolutional nets really need to be deep and convolutional?
\newblock In {\em The 5th International Conference on Learning
  Representations}, 2017.

\bibitem{CUB}
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
\newblock {The Caltech-UCSD Birds-200-2011 Dataset}.
\newblock Technical report, 2011.

\bibitem{SpeechCommands}
Pete Warden.
\newblock Speech commands: {A} dataset for limited-vocabulary speech
  recognition.
\newblock {\em CoRR}, abs/1804.03209, 2018.

\bibitem{ResNeXt}
Saining Xie, Ross~B. Girshick, Piotr Doll{\'{a}}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In {\em {IEEE} Conference on Computer Vision and Pattern
  Recognition}, pages 5987--5995, 2017.

\bibitem{ReKD-Refilled}
Han{-}Jia Ye, Su Lu, and De{-}Chuan Zhan.
\newblock Distilling cross-task knowledge via relationship matching.
\newblock In {\em 2020 {IEEE/CVF} Conference on Computer Vision and Pattern
  Recognition}, pages 12393--12402, 2020.

\bibitem{SCKD}
Yi~Wang Yichen~Zhu.
\newblock Student customized knowledge distillation: Bridging the gap between
  student and teacher.
\newblock In {\em {IEEE/CVF} International Conference on Computer Vision},
  2021.

\bibitem{ReKD-Gift}
Junho Yim, Donggyu Joo, Ji{-}Hoon Bae, and Junmo Kim.
\newblock A gift from knowledge distillation: Fast optimization, network
  minimization and transfer learning.
\newblock In {\em 2017 {IEEE} Conference on Computer Vision and Pattern
  Recognition}, pages 7130--7138, 2017.

\bibitem{LogME}
Kaichao You, Yong Liu, Jianmin Wang, and Mingsheng Long.
\newblock Logme: Practical assessment of pre-trained models for transfer
  learning.
\newblock In {\em Proceedings of the 38th International Conference on Machine
  Learning}, pages 12133--12143, 2021.

\bibitem{ReKD-Metric}
Lu Yu, Vacit~Oguz Yazici, Xialei Liu, Joost van~de Weijer, Yongmei Cheng, and
  Arnau Ramisa.
\newblock Learning metrics from teachers: Compact networks for image embedding.
\newblock In {\em {IEEE} Conference on Computer Vision and Pattern
  Recognition}, pages 2907--2916, 2019.

\bibitem{KDLSR}
Li Yuan, Francis E.~H. Tay, Guilin Li, Tao Wang, and Jiashi Feng.
\newblock Revisiting knowledge distillation via label smoothing regularization.
\newblock In {\em 2020 {IEEE/CVF} Conference on Computer Vision and Pattern
  Recognition}, pages 3902--3910, 2020.

\bibitem{WideResNet}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In {\em Proceedings of the British Machine Vision Conference}, 2016.

\bibitem{FeatKD-AT}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Paying more attention to attention: Improving the performance of
  convolutional neural networks via attention transfer.
\newblock In {\em The 5th International Conference on Learning
  Representations}, 2017.

\bibitem{SelfKD-BYOT}
Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng
  Ma.
\newblock Be your own teacher: Improve the performance of convolutional neural
  networks via self distillation.
\newblock In {\em 2019 {IEEE/CVF} International Conference on Computer Vision},
  pages 3712--3721, 2019.

\bibitem{ShuffleNet}
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
\newblock Shufflenet: An extremely efficient convolutional neural network for
  mobile devices.
\newblock In {\em 2018 {IEEE} Conference on Computer Vision and Pattern
  Recognition}, pages 6848--6856, 2018.

\bibitem{KWS-DSCNN}
Yundong Zhang, Naveen Suda, Liangzhen Lai, and Vikas Chandra.
\newblock Hello edge: Keyword spotting on microcontrollers.
\newblock {\em CoRR}, abs/1711.07128, 2017.

\bibitem{DML}
Ying Zhang, Tao Xiang, Timothy~M. Hospedales, and Huchuan Lu.
\newblock Deep mutual learning.
\newblock In {\em 2018 {IEEE} Conference on Computer Vision and Pattern
  Recognition}, pages 4320--4328, 2018.

\bibitem{SelfKD-LS}
Zhilu Zhang and Mert~R. Sabuncu.
\newblock Self-distillation as instance-specific label smoothing.
\newblock In {\em Advances in Neural Information Processing Systems 33}, 2020.

\bibitem{KDBiasVar}
Helong Zhou, Liangchen Song, Jiajie Chen, Ye Zhou, Guoli Wang, Junsong Yuan,
  and Qian Zhang.
\newblock Rethinking soft labels for knowledge distillation: {A} bias-variance
  tradeoff perspective.
\newblock In {\em The 9th International Conference on Learning
  Representations}, 2021.

\end{thebibliography}
