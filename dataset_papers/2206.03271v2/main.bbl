\begin{thebibliography}{10}

\bibitem{vinyals2016matching}
Vinyals, O., C.~Blundell, T.~Lillicrap, et~al.
\newblock Matching networks for one shot learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  29:3630--3638, 2016.

\bibitem{ravi2017optimization}
Ravi, S., H.~Larochelle.
\newblock Optimization as a model for few-shot learning.
\newblock \emph{Intl. Conference on Learning Representations}, 2017.

\bibitem{finn2017model}
Finn, C., P.~Abbeel, S.~Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Intl. Conference on Machine Learning}, pages 1126--1135.
  PMLR, 2017.

\bibitem{chen2019closer}
Chen, W.-Y., Y.-C. Liu, Z.~Kira, et~al.
\newblock A closer look at few-shot classification.
\newblock \emph{Intl. Conference on Learning Representations}, 2019.

\bibitem{dhillon2020baseline}
Dhillon, G.~S., P.~Chaudhari, A.~Ravichandran, et~al.
\newblock A baseline for few-shot image classification.
\newblock \emph{Intl. Conference on Learning Representations}, 2020.

\bibitem{tian2020rethinking}
Tian, Y., Y.~Wang, D.~Krishnan, et~al.
\newblock Rethinking few-shot image classification: a good embedding is all you
  need?
\newblock In \emph{European Conference on Computer Vision}, pages 266--282.
  Springer, 2020.

\bibitem{chen2021meta}
Chen, Y., Z.~Liu, H.~Xu, et~al.
\newblock Meta-baseline: exploring simple meta-learning for few-shot learning.
\newblock In \emph{Intl. Conference on Computer Vision}, pages 9062--9071.
  2021.

\bibitem{james2019rlbench}
James, S., Z.~Ma, D.~Rovick~Arrojo, et~al.
\newblock {RLB}ench: The robot learning benchmark \& learning environment.
\newblock \emph{IEEE Robotics and Automation Letters}, 2020.

\bibitem{mandi2021towards}
Mandi, Z., F.~Liu, K.~Lee, et~al.
\newblock Towards more generalizable one-shot visual imitation learning.
\newblock \emph{arXiv preprint arXiv:2110.13423}, 2021.

\bibitem{finn2017one}
Finn, C., T.~Yu, T.~Zhang, et~al.
\newblock One-shot visual imitation learning via meta-learning.
\newblock In \emph{Conference on Robot Learning}, pages 357--368. PMLR, 2017.

\bibitem{nichol2018first}
Nichol, A., J.~Achiam, J.~Schulman.
\newblock On first-order meta-learning algorithms.
\newblock \emph{arXiv preprint arXiv:1803.02999}, 2018.

\bibitem{rakelly2019efficient}
Rakelly, K., A.~Zhou, D.~Quillen, et~al.
\newblock Efficient off-policy meta-reinforcement learning via probabilistic
  context variables, 2019.

\bibitem{cobbe2020leveraging}
Cobbe, K., C.~Hesse, J.~Hilton, et~al.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  2048--2056. PMLR, 2020.

\bibitem{bellemare13arcade}
{Bellemare}, M.~G., Y.~{Naddaf}, J.~{Veness}, et~al.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:253--279,
  2013.

\bibitem{machado18arcade}
Machado, M.~C., M.~G. Bellemare, E.~Talvitie, et~al.
\newblock Revisiting the arcade learning environment: Evaluation protocols and
  open problems for general agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 61:523--562,
  2018.

\bibitem{duan2016rl}
Duan, Y., J.~Schulman, X.~Chen, et~al.
\newblock $rl^2$: Fast reinforcement learning via slow reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1611.02779}, 2016.

\bibitem{Cachet2020TransformerbasedML}
Cachet, T., J.~Perez.
\newblock Transformer-based meta-imitation learning for robotic manipulation.
\newblock 2020.

\bibitem{schulman2017proximal}
Schulman, J., F.~Wolski, P.~Dhariwal, et~al.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem{james2021coarsetofine}
James, S., K.~Wada, T.~Laidlow, et~al.
\newblock Coarse-to-{F}ine {Q}-attention: {E}fficient {L}earning for {V}isual
  {R}obotic {M}anipulation via {D}iscretisation.
\newblock \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  2022.

\bibitem{hessel2018rainbow}
Hessel, M., J.~Modayil, H.~Van~Hasselt, et~al.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \emph{Thirty-second AAAI conference on artificial intelligence}.
  2018.

\bibitem{wang2020improving}
Wang, K., B.~Kang, J.~Shao, et~al.
\newblock Improving generalization in reinforcement learning with mixture
  regularization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:7968--7978, 2020.

\bibitem{yarats2021mastering}
Yarats, D., R.~Fergus, A.~Lazaric, et~al.
\newblock Mastering visual continuous control: Improved data-augmented
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2107.09645}, 2021.

\bibitem{transient}
Igl, M., G.~Farquhar, J.~Luketina, et~al.
\newblock Transient non-stationarity and generalisation in deep reinforcement
  learning, 2020.

\bibitem{anand2021procedural}
Anand, A., J.~Walker, Y.~Li, et~al.
\newblock Procedural generalization by planning with self-supervised world
  models.
\newblock \emph{arXiv preprint arXiv:2111.01587}, 2021.

\bibitem{alver2020brief}
Alver, S., D.~Precup.
\newblock A brief look at generalization in visual meta-reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.07262}, 2020.

\bibitem{espeholt2018impala}
Espeholt, L., H.~Soyer, R.~Munos, et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In \emph{International Conference on Machine Learning}, pages
  1407--1416. PMLR, 2018.

\bibitem{Lee2021ImprovingGI}
Lee, S., S.-Y. Chung.
\newblock Improving generalization in meta-rl with imaginary tasks from latent
  dynamics mixture.
\newblock \emph{ArXiv}, abs/2105.13524, 2021.

\bibitem{lillicrap2015continuous}
Lillicrap, T.~P., J.~J. Hunt, A.~Pritzel, et~al.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{Intl. Conference on Learning Representations}, 2015.

\bibitem{fujimoto2018addressing}
Fujimoto, S., H.~Van~Hoof, D.~Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock \emph{Intl. Conference on Machine Learning}, 2018.

\bibitem{haarnoja2018soft}
Haarnoja, T., A.~Zhou, K.~Hartikainen, et~al.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{arXiv preprint arXiv:1812.05905}, 2018.

\bibitem{james2021qattention}
James, S., A.~J. Davison.
\newblock Q-attention: {E}nabling {E}fficient {L}earning for {V}ision-based
  {R}obotic {M}anipulation.
\newblock \emph{IEEE Robotics and Automation Letters}, 2022.

\bibitem{sobol2018visual}
Sobol, D., L.~Wolf, Y.~Taigman.
\newblock Visual analogies between atari games for studying transfer learning
  in rl.
\newblock \emph{arXiv preprint arXiv:1807.11074}, 2018.

\bibitem{du2016initial}
Du, Y., V.~Gabriel, J.~Irwin, et~al.
\newblock Initial progress in transfer for deep reinforcement learning
  algorithms.
\newblock In \emph{Proceedings of deep reinforcement learning: frontiers and
  challenges workshop, New York City, NY, USA}. 2016.

\bibitem{oh2020discovering}
Oh, J., M.~Hessel, W.~M. Czarnecki, et~al.
\newblock Discovering reinforcement learning algorithms.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:1060--1070, 2020.

\bibitem{van2019use}
van Hasselt, H.~P., M.~Hessel, J.~Aslanides.
\newblock When to use parametric models in reinforcement learning?
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{rusu2016progressive}
Rusu, A.~A., N.~C. Rabinowitz, G.~Desjardins, et~al.
\newblock Progressive neural networks.
\newblock \emph{arXiv preprint arXiv:1606.04671}, 2016.

\bibitem{mittel2019visual}
Mittel, A., P.~Sowmya~Munukutla.
\newblock Visual transfer between atari games using competitive reinforcement
  learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition Workshops}, pages 0--0. 2019.

\bibitem{wang2016learning}
Wang, J.~X., Z.~Kurth-Nelson, D.~Tirumala, et~al.
\newblock Learning to reinforcement learn.
\newblock \emph{arXiv preprint arXiv:1611.05763}, 2016.

\bibitem{fakoor2020metaqlearning}
Fakoor, R., P.~Chaudhari, S.~Soatto, et~al.
\newblock Meta-q-learning.
\newblock In \emph{International Conference on Learning Representations}. 2020.

\bibitem{xu2018meta}
Xu, Z., H.~van Hasselt, D.~Silver.
\newblock Meta-gradient reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1805.09801}, 2018.

\bibitem{sung2017learning}
Sung, F., L.~Zhang, T.~Xiang, et~al.
\newblock Learning to learn: Meta-critic networks for sample efficient
  learning.
\newblock \emph{arXiv preprint arXiv:1706.09529}, 2017.

\bibitem{houthooft2018evolved}
Houthooft, R., R.~Y. Chen, P.~Isola, et~al.
\newblock Evolved policy gradients.
\newblock \emph{arXiv preprint arXiv:1802.04821}, 2018.

\bibitem{rothfuss2018promp}
Rothfuss, J., D.~Lee, I.~Clavera, et~al.
\newblock Promp: Proximal meta-policy search.
\newblock \emph{Intl. Conference on Learning Representations}, 2019.

\bibitem{raghu2020rapid}
Raghu, A., M.~Raghu, S.~Bengio, et~al.
\newblock Rapid learning or feature reuse? towards understanding the
  effectiveness of maml, 2020.

\bibitem{packer2021hindsight}
Packer, C., P.~Abbeel, J.~E. Gonzalez.
\newblock Hindsight task relabelling: Experience replay for sparse reward
  meta-rl.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{zintgraf2021exploration}
Zintgraf, L.~M., L.~Feng, C.~Lu, et~al.
\newblock Exploration in approximate hyper-state space for meta reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  12991--13001. PMLR, 2021.

\bibitem{liu2020decoupling}
Liu, E.~Z., A.~Raghunathan, P.~Liang, et~al.
\newblock Decoupling exploration and exploitation for meta-reinforcement
  learning without sacrifices.
\newblock \emph{arXiv preprint arXiv:2008.02790}, 2020.

\bibitem{zhang2021metacure}
Zhang, J., J.~Wang, H.~Hu, et~al.
\newblock Metacure: Meta reinforcement learning with empowerment-driven
  exploration.
\newblock In \emph{International Conference on Machine Learning}, pages
  12600--12610. PMLR, 2021.

\bibitem{mendonca2020meta}
Mendonca, R., X.~Geng, C.~Finn, et~al.
\newblock Meta-reinforcement learning robust to distributional shift via model
  identification and experience relabeling.
\newblock \emph{arXiv preprint arXiv:2006.07178}, 2020.

\bibitem{kirsch2021introducing}
Kirsch, L., S.~Flennerhag, H.~van Hasselt, et~al.
\newblock Introducing symmetries to black box meta reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2109.10781}, 2021.

\bibitem{nagabandi2018learning}
Nagabandi, A., I.~Clavera, S.~Liu, et~al.
\newblock Learning to adapt in dynamic, real-world environments through
  meta-reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1803.11347}, 2018.

\bibitem{james2018taskembedded}
James, S., M.~Bloesch, A.~J. Davison.
\newblock Task-embedded control networks for few-shot imitation learning.
\newblock In \emph{Conference on Robot Learning}, pages 783--795. PMLR, 2018.

\bibitem{bonardi2020learning}
Bonardi, A., S.~James, A.~J. Davison.
\newblock Learning one-shot imitation from humans without humans.
\newblock \emph{IEEE Robotics and Automation Letters}, 5(2):3533--3539, 2020.

\bibitem{kirsch2019improving}
Kirsch, L., S.~van Steenkiste, J.~Schmidhuber.
\newblock Improving generalization in meta reinforcement learning using learned
  objectives.
\newblock \emph{arXiv preprint arXiv:1910.04098}, 2019.

\bibitem{xu2020meta}
Xu, Z., H.~P. van Hasselt, M.~Hessel, et~al.
\newblock Meta-gradient reinforcement learning with an objective discovered
  online.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:15254--15264, 2020.

\bibitem{co2021evolving}
Co-Reyes, J.~D., Y.~Miao, D.~Peng, et~al.
\newblock Evolving reinforcement learning algorithms.
\newblock \emph{arXiv preprint arXiv:2101.03958}, 2021.

\bibitem{EPG}
Houthooft, R., Y.~Chen, P.~Isola, et~al.
\newblock Evolved policy gradients.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  R.~Garnett, eds., \emph{Advances in Neural Information Processing Systems},
  vol.~31. Curran Associates, Inc., 2018.

\bibitem{Xiong2021OnTP}
Xiong, Z., L.~M. Zintgraf, J.~Beck, et~al.
\newblock On the practical consistency of meta-reinforcement learning
  algorithms.
\newblock \emph{ArXiv}, abs/2112.00478, 2021.

\bibitem{Yang2020MultiTaskRL}
Yang, R., H.~Xu, Y.~Wu, et~al.
\newblock Multi-task reinforcement learning with soft modularization.
\newblock \emph{ArXiv}, abs/2003.13661, 2020.

\bibitem{sodhani2021multitask}
Sodhani, S., A.~Zhang, J.~Pineau.
\newblock Multi-task reinforcement learning with context-based representations,
  2021.

\bibitem{kalashnikov2021mtopt}
Kalashnikov, D., J.~Varley, Y.~Chebotar, et~al.
\newblock Mt-opt: Continuous multi-task robotic reinforcement learning at
  scale, 2021.

\bibitem{kurin2022defense}
Kurin, V., A.~De~Palma, I.~Kostrikov, et~al.
\newblock In defense of the unitary scalarization for deep multi-task learning.
\newblock \emph{arXiv preprint arXiv:2201.04122}, 2022.

\bibitem{gao2020modeling}
Gao, K., O.~Sener.
\newblock Modeling and optimization trade-off in meta-learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:11154--11165, 2020.

\bibitem{yu2021metaworld}
Yu, T., D.~Quillen, Z.~He, et~al.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta
  reinforcement learning, 2021.

\bibitem{stable-baselines3}
Raffin, A., A.~Hill, A.~Gleave, et~al.
\newblock Stable-baselines3: Reliable reinforcement learning implementations.
\newblock \emph{Journal of Machine Learning Research}, 22(268):1--8, 2021.

\bibitem{rainbowcode}
Arulkumaran, K.
\newblock Pytorch implementation of rainbowdqn.
\newblock \url{https://github.com/Kaixhin/Rainbow}, 2022.

\bibitem{james2022lpr}
James, S., P.~Abbeel.
\newblock {C}oarse-to-{F}ine {Q}-attention with {L}earned {P}ath {R}anking.
\newblock \emph{arXiv preprint arXiv:2204.01571}, 2022.

\bibitem{james2022tree}
James, S., P.~Abbeel.
\newblock {C}oarse-to-{F}ine {Q}-attention with {T}ree {E}xpansion.
\newblock \emph{arXiv preprint arXiv:2204.12471}, 2022.

\end{thebibliography}
