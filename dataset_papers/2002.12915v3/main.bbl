\begin{thebibliography}{71}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2020)Arora, Bartlett, Mianjy, and
  Srebro]{arora2020dropout}
Arora, R., Bartlett, P.~L., Mianjy, P., and Srebro, N.
\newblock Dropout: Explicit forms and capacity control, 2020.
\newblock URL \url{https://openreview.net/forum?id=Bylthp4Yvr}.

\bibitem[Arora et~al.(2018)Arora, Ge, Neyshabur, and Zhang]{arora2018stronger}
Arora, S., Ge, R., Neyshabur, B., and Zhang, Y.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock \emph{arXiv preprint arXiv:1802.05296}, 2018.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{arora2019implicit}
Arora, S., Cohen, N., Hu, W., and Luo, Y.
\newblock Implicit regularization in deep matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  7411--7422, 2019.

\bibitem[Bach et~al.(2010)]{bach2010self}
Bach, F. et~al.
\newblock Self-concordant analysis for logistic regression.
\newblock \emph{Electronic Journal of Statistics}, 4:\penalty0 384--414, 2010.

\bibitem[Baldi \& Sadowski(2013)Baldi and Sadowski]{baldi2013understanding}
Baldi, P. and Sadowski, P.~J.
\newblock Understanding dropout.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2814--2822, 2013.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
Bartlett, P.~L., Foster, D.~J., and Telgarsky, M.~J.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6240--6249, 2017.

\bibitem[Bousquet(2002)]{bousquet2002concentration}
Bousquet, O.
\newblock Concentration inequalities and empirical processes theory applied to
  the analysis of learning algorithms.
\newblock 2002.

\bibitem[Bradbury et~al.(2016)Bradbury, Merity, Xiong, and
  Socher]{bradbury2016quasi}
Bradbury, J., Merity, S., Xiong, C., and Socher, R.
\newblock Quasi-recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1611.01576}, 2016.

\bibitem[Cavazza et~al.(2017)Cavazza, Morerio, Haeffele, Lane, Murino, and
  Vidal]{cavazza2017dropout}
Cavazza, J., Morerio, P., Haeffele, B., Lane, C., Murino, V., and Vidal, R.
\newblock Dropout as a low-rank regularizer for matrix factorization.
\newblock \emph{arXiv preprint arXiv:1710.05092}, 2017.

\bibitem[Chaudhari \& Soatto(2018)Chaudhari and
  Soatto]{chaudhari2018stochastic}
Chaudhari, P. and Soatto, S.
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock In \emph{2018 Information Theory and Applications Workshop (ITA)},
  pp.\  1--10. IEEE, 2018.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.~V., and Salakhutdinov, R.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock \emph{arXiv preprint arXiv:1901.02860}, 2019.

\bibitem[Dziugaite \& Roy(2017)Dziugaite and Roy]{dziugaite2017computing}
Dziugaite, G.~K. and Roy, D.~M.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{arXiv preprint arXiv:1703.11008}, 2017.

\bibitem[Gal \& Ghahramani(2016{\natexlab{a}})Gal and
  Ghahramani]{gal2016dropout}
Gal, Y. and Ghahramani, Z.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{international conference on machine learning}, pp.\
  1050--1059, 2016{\natexlab{a}}.

\bibitem[Gal \& Ghahramani(2016{\natexlab{b}})Gal and
  Ghahramani]{gal2016theoretically}
Gal, Y. and Ghahramani, Z.
\newblock A theoretically grounded application of dropout in recurrent neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1019--1027, 2016{\natexlab{b}}.

\bibitem[Gao et~al.(2019)Gao, Pei, and Huang]{gao2019demystifying}
Gao, H., Pei, J., and Huang, H.
\newblock Demystifying dropout.
\newblock In \emph{The 36th International Conference on Machine Learning (ICML
  2019)}, 2019.

\bibitem[Golowich et~al.(2017)Golowich, Rakhlin, and Shamir]{golowich2017size}
Golowich, N., Rakhlin, A., and Shamir, O.
\newblock Size-independent sample complexity of neural networks.
\newblock \emph{arXiv preprint arXiv:1712.06541}, 2017.

\bibitem[Grave et~al.(2017)Grave, Joulin, Ciss{\'e}, J{\'e}gou,
  et~al.]{grave2017efficient}
Grave, E., Joulin, A., Ciss{\'e}, M., J{\'e}gou, H., et~al.
\newblock Efficient softmax approximation for gpus.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1302--1310. JMLR. org, 2017.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Gunasekar, S., Woodworth, B.~E., Bhojanapalli, S., Neyshabur, B., and Srebro,
  N.
\newblock Implicit regularization in matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6151--6159, 2017.

\bibitem[Gunasekar et~al.(2018{\natexlab{a}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018implicit}
Gunasekar, S., Lee, J.~D., Soudry, D., and Srebro, N.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9461--9471, 2018{\natexlab{a}}.

\bibitem[Gunasekar et~al.(2018{\natexlab{b}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018characterizing}
Gunasekar, S., Lee, t., Soudry, D., and Srebro, N.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock \emph{arXiv preprint arXiv:1802.08246}, 2018{\natexlab{b}}.

\bibitem[Helmbold \& Long(2015)Helmbold and Long]{helmbold2015inductive}
Helmbold, D.~P. and Long, P.~M.
\newblock On the inductive bias of dropout.
\newblock \emph{The Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 3403--3454, 2015.

\bibitem[Helmbold \& Long(2017)Helmbold and Long]{helmbold2017surprising}
Helmbold, D.~P. and Long, P.~M.
\newblock Surprising properties of dropout in deep networks.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 7284--7311, 2017.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, Krizhevsky, Sutskever, and
  Salakhutdinov]{hinton2012improving}
Hinton, G.~E., Srivastava, N., Krizhevsky, A., Sutskever, I., and
  Salakhutdinov, R.~R.
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock \emph{arXiv preprint arXiv:1207.0580}, 2012.

\bibitem[Hoffman et~al.(2019)Hoffman, Roberts, and Yaida]{hoffman2019robust}
Hoffman, J., Roberts, D.~A., and Yaida, S.
\newblock Robust learning with jacobian regularization.
\newblock \emph{arXiv preprint arXiv:1908.02729}, 2019.

\bibitem[Jastrz{\k{e}}bski et~al.(2017)Jastrz{\k{e}}bski, Kenton, Arpit,
  Ballas, Fischer, Bengio, and Storkey]{jastrzkebski2017three}
Jastrz{\k{e}}bski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio,
  Y., and Storkey, A.
\newblock Three factors influencing minima in sgd.
\newblock \emph{arXiv preprint arXiv:1711.04623}, 2017.

\bibitem[Jastrzebski et~al.(2018)Jastrzebski, Kenton, Ballas, Fischer, Bengio,
  and Storkey]{jastrzebski2018relation}
Jastrzebski, S., Kenton, Z., Ballas, N., Fischer, A., Bengio, Y., and Storkey,
  A.
\newblock On the relation between the sharpest directions of dnn loss and the
  sgd step length.
\newblock \emph{arXiv preprint arXiv:1807.05031}, 2018.

\bibitem[Kakade et~al.(2009)Kakade, Sridharan, and
  Tewari]{kakade2009complexity}
Kakade, S.~M., Sridharan, K., and Tewari, A.
\newblock On the complexity of linear prediction: Risk bounds, margin bounds,
  and regularization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  793--800, 2009.

\bibitem[Keskar \& Socher(2017)Keskar and Socher]{keskar2017improving}
Keskar, N.~S. and Socher, R.
\newblock Improving generalization performance by switching from adam to sgd.
\newblock \emph{arXiv preprint arXiv:1712.07628}, 2017.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Krueger \& Memisevic(2015)Krueger and
  Memisevic]{krueger2015regularizing}
Krueger, D. and Memisevic, R.
\newblock Regularizing rnns by stabilizing activations.
\newblock \emph{arXiv preprint arXiv:1511.08400}, 2015.

\bibitem[LeCun et~al.(2012)LeCun, Bottou, Orr, and
  M{\"u}ller]{lecun2012efficient}
LeCun, Y.~A., Bottou, L., Orr, G.~B., and M{\"u}ller, K.-R.
\newblock Efficient backprop.
\newblock In \emph{Neural networks: Tricks of the trade}, pp.\  9--48.
  Springer, 2012.

\bibitem[Li et~al.(2017)Li, Ma, and Zhang]{li2017algorithmic}
Li, Y., Ma, T., and Zhang, H.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock \emph{arXiv preprint arXiv:1712.09203}, 2017.

\bibitem[Li et~al.(2019)Li, Wei, and Ma]{li2019towards}
Li, Y., Wei, C., and Ma, T.
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  11669--11680, 2019.

\bibitem[Ma et~al.(2016)Ma, Gao, Hu, Yu, Deng, and Hovy]{ma2016dropout}
Ma, X., Gao, Y., Hu, Z., Yu, Y., Deng, Y., and Hovy, E.
\newblock Dropout with expectation-linear regularization.
\newblock \emph{arXiv preprint arXiv:1609.08017}, 2016.

\bibitem[Maeda(2014)]{maeda2014bayesian}
Maeda, S.-i.
\newblock A bayesian encourages dropout.
\newblock \emph{arXiv preprint arXiv:1412.7003}, 2014.

\bibitem[Marcus et~al.(1994)Marcus, Kim, Marcinkiewicz, MacIntyre, Bies,
  Ferguson, Katz, and Schasberger]{marcus1994penn}
Marcus, M., Kim, G., Marcinkiewicz, M.~A., MacIntyre, R., Bies, A., Ferguson,
  M., Katz, K., and Schasberger, B.
\newblock The penn treebank: annotating predicate argument structure.
\newblock In \emph{Proceedings of the workshop on Human Language Technology},
  pp.\  114--119. Association for Computational Linguistics, 1994.

\bibitem[Melis et~al.(2017)Melis, Dyer, and Blunsom]{melis2017state}
Melis, G., Dyer, C., and Blunsom, P.
\newblock On the state of the art of evaluation in neural language models.
\newblock \emph{arXiv preprint arXiv:1707.05589}, 2017.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and
  Socher]{merity2016pointer}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}, 2016.

\bibitem[Merity et~al.(2017{\natexlab{a}})Merity, Keskar, and
  Socher]{merity2017regularizing}
Merity, S., Keskar, N.~S., and Socher, R.
\newblock Regularizing and optimizing lstm language models.
\newblock \emph{arXiv preprint arXiv:1708.02182}, 2017{\natexlab{a}}.

\bibitem[Merity et~al.(2017{\natexlab{b}})Merity, McCann, and
  Socher]{merity2017revisiting}
Merity, S., McCann, B., and Socher, R.
\newblock Revisiting activation regularization for language rnns.
\newblock \emph{arXiv preprint arXiv:1708.01009}, 2017{\natexlab{b}}.

\bibitem[Merity et~al.(2018)Merity, Keskar, and Socher]{merity2018analysis}
Merity, S., Keskar, N.~S., and Socher, R.
\newblock {An Analysis of Neural Language Modeling at Multiple Scales}.
\newblock \emph{arXiv preprint arXiv:1803.08240}, 2018.

\bibitem[Mianjy \& Arora(2019)Mianjy and Arora]{mianjy2019dropout}
Mianjy, P. and Arora, R.
\newblock On dropout and nuclear norm regularization.
\newblock \emph{arXiv preprint arXiv:1905.11887}, 2019.

\bibitem[Mianjy et~al.(2018)Mianjy, Arora, and Vidal]{mianjy2018implicit}
Mianjy, P., Arora, R., and Vidal, R.
\newblock On the implicit bias of dropout.
\newblock \emph{arXiv preprint arXiv:1806.09777}, 2018.

\bibitem[Nagarajan \& Kolter(2019)Nagarajan and
  Kolter]{nagarajan2019deterministic}
Nagarajan, V. and Kolter, J.~Z.
\newblock Deterministic pac-bayesian generalization bounds for deep networks
  via generalizing noise-resilience.
\newblock \emph{arXiv preprint arXiv:1905.13344}, 2019.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, and
  Srebro]{neyshabur2017pac}
Neyshabur, B., Bhojanapalli, S., and Srebro, N.
\newblock A pac-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock \emph{arXiv preprint arXiv:1707.09564}, 2017.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Li, Bhojanapalli, LeCun, and
  Srebro]{neyshabur2018towards}
Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N.
\newblock Towards understanding the role of over-parametrization in
  generalization of neural networks.
\newblock \emph{arXiv preprint arXiv:1805.12076}, 2018.

\bibitem[Novak et~al.(2018)Novak, Bahri, Abolafia, Pennington, and
  Sohl-Dickstein]{novak2018sensitivity}
Novak, R., Bahri, Y., Abolafia, D.~A., Pennington, J., and Sohl-Dickstein, J.
\newblock Sensitivity and generalization in neural networks: an empirical
  study.
\newblock \emph{arXiv preprint arXiv:1802.08760}, 2018.

\bibitem[Sagun et~al.(2017)Sagun, Evci, Guney, Dauphin, and
  Bottou]{sagun2017empirical}
Sagun, L., Evci, U., Guney, V.~U., Dauphin, Y., and Bottou, L.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1706.04454}, 2017.

\bibitem[Semeniuta et~al.(2016)Semeniuta, Severyn, and
  Barth]{semeniuta2016recurrent}
Semeniuta, S., Severyn, A., and Barth, E.
\newblock Recurrent dropout without memory loss.
\newblock \emph{arXiv preprint arXiv:1603.05118}, 2016.

\bibitem[Smith \& Le(2017)Smith and Le]{smith2017bayesian}
Smith, S.~L. and Le, Q.~V.
\newblock A bayesian perspective on generalization and stochastic gradient
  descent.
\newblock \emph{arXiv preprint arXiv:1710.06451}, 2017.

\bibitem[Sokoli{\'c} et~al.(2017)Sokoli{\'c}, Giryes, Sapiro, and
  Rodrigues]{sokolic2017robust}
Sokoli{\'c}, J., Giryes, R., Sapiro, G., and Rodrigues, M.~R.
\newblock Robust large margin deep neural networks.
\newblock \emph{IEEE Transactions on Signal Processing}, 65\penalty0
  (16):\penalty0 4265--4280, 2017.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Soudry, D., Hoffer, E., Nacson, M.~S., Gunasekar, S., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Srebro et~al.(2010)Srebro, Sridharan, and
  Tewari]{srebro2010smoothness}
Srebro, N., Sridharan, K., and Tewari, A.
\newblock Smoothness, low noise and fast rates.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2199--2207, 2010.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Tan \& Le(2019)Tan and Le]{tan2019efficientnet}
Tan, M. and Le, Q.~V.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock \emph{arXiv preprint arXiv:1905.11946}, 2019.

\bibitem[Wager et~al.(2013)Wager, Wang, and Liang]{wager2013dropout}
Wager, S., Wang, S., and Liang, P.~S.
\newblock Dropout training as adaptive regularization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  351--359, 2013.

\bibitem[Wager et~al.(2014)Wager, Fithian, Wang, and Liang]{wager2014altitude}
Wager, S., Fithian, W., Wang, S., and Liang, P.~S.
\newblock Altitude training: Strong bounds for single-layer dropout.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  100--108, 2014.

\bibitem[Wan et~al.(2013)Wan, Zeiler, Zhang, Le~Cun, and
  Fergus]{wan2013regularization}
Wan, L., Zeiler, M., Zhang, S., Le~Cun, Y., and Fergus, R.
\newblock Regularization of neural networks using dropconnect.
\newblock In \emph{International conference on machine learning}, pp.\
  1058--1066, 2013.

\bibitem[Wang \& Manning(2013)Wang and Manning]{wang2013fast}
Wang, S. and Manning, C.
\newblock Fast dropout training.
\newblock In \emph{international conference on machine learning}, pp.\
  118--126, 2013.

\bibitem[Wei \& Ma(2019{\natexlab{a}})Wei and Ma]{wei2019data}
Wei, C. and Ma, T.
\newblock Data-dependent sample complexity of deep neural networks via
  lipschitz augmentation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9722--9733, 2019{\natexlab{a}}.

\bibitem[Wei \& Ma(2019{\natexlab{b}})Wei and Ma]{wei2019improved}
Wei, C. and Ma, T.
\newblock Improved sample complexities for deep networks and robust
  classification via an all-layer margin.
\newblock \emph{arXiv preprint arXiv:1910.04284}, 2019{\natexlab{b}}.

\bibitem[Wei \& Schwab(2019)Wei and Schwab]{Wei2019HowNA}
Wei, M. and Schwab, D.~J.
\newblock How noise affects the hessian spectrum in overparameterized neural
  networks.
\newblock \emph{ArXiv}, abs/1910.00195, 2019.

\bibitem[Wen et~al.(2019)Wen, Luk, Gazeau, Zhang, Chan, and
  Ba]{wen2019interplay}
Wen, Y., Luk, K., Gazeau, M., Zhang, G., Chan, H., and Ba, J.
\newblock Interplay between optimization and generalization of stochastic
  gradient descent with covariance noise.
\newblock \emph{arXiv preprint arXiv:1902.08234}, 2019.

\bibitem[Woodworth et~al.(2019)Woodworth, Gunasekar, Lee, Soudry, and
  Srebro]{woodworth2019kernel}
Woodworth, B., Gunasekar, S., Lee, J., Soudry, D., and Srebro, N.
\newblock Kernel and deep regimes in overparametrized models.
\newblock \emph{arXiv preprint arXiv:1906.05827}, 2019.

\bibitem[Xing et~al.(2018)Xing, Arpit, Tsirigotis, and Bengio]{xing2018walk}
Xing, C., Arpit, D., Tsirigotis, C., and Bengio, Y.
\newblock A walk with sgd.
\newblock \emph{arXiv preprint arXiv:1802.08770}, 2018.

\bibitem[Yaida(2018)]{yaida2018fluctuation}
Yaida, S.
\newblock Fluctuation-dissipation relations for stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:1810.00004}, 2018.

\bibitem[Yao et~al.(2018)Yao, Gholami, Lei, Keutzer, and
  Mahoney]{yao2018hessian}
Yao, Z., Gholami, A., Lei, Q., Keutzer, K., and Mahoney, M.~W.
\newblock Hessian-based analysis of large batch training and robustness to
  adversaries.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4949--4959, 2018.

\bibitem[Zaremba et~al.(2014)Zaremba, Sutskever, and
  Vinyals]{zaremba2014recurrent}
Zaremba, W., Sutskever, I., and Vinyals, O.
\newblock Recurrent neural network regularization.
\newblock \emph{arXiv preprint arXiv:1409.2329}, 2014.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\bibitem[Zhang(2002)]{zhang2002covering}
Zhang, T.
\newblock Covering number bounds of certain regularized linear function
  classes.
\newblock \emph{Journal of Machine Learning Research}, 2\penalty0
  (Mar):\penalty0 527--550, 2002.

\bibitem[Zhu et~al.(2018)Zhu, Wu, Yu, Wu, and Ma]{zhu2018anisotropic}
Zhu, Z., Wu, J., Yu, B., Wu, L., and Ma, J.
\newblock The anisotropic noise in stochastic gradient descent: Its behavior of
  escaping from minima and regularization effects.
\newblock \emph{arXiv preprint arXiv:1803.00195}, 2018.

\end{thebibliography}
