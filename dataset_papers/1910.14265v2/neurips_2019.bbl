\begin{thebibliography}{75}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agakov and Barber(2004)]{agakov2004auxiliary}
Felix~V Agakov and David Barber.
\newblock An auxiliary variational method.
\newblock In \emph{International Conference on Neural Information Processing},
  pages 561--566. Springer, 2004.

\bibitem[Babaeizadeh et~al.(2017)Babaeizadeh, Finn, Erhan, Campbell, and
  Levine]{babaeizadeh2017stochastic}
Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy~H Campbell, and Sergey
  Levine.
\newblock Stochastic variational video prediction.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Bachman and Precup(2015)]{bachman2015training}
Philip Bachman and Doina Precup.
\newblock Training deep generative models: Variations on a theme.
\newblock In \emph{NIPS Approximate Inference Workshop}, 2015.

\bibitem[Barber and Agakov(2003)]{barber2003algorithm}
David Barber and Felix Agakov.
\newblock The im algorithm: a variational approach to information maximization.
\newblock In \emph{Proceedings of the 16th International Conference on Neural
  Information Processing Systems}, pages 201--208. MIT Press, 2003.

\bibitem[Bauer and Mnih(2018)]{bauer2018resampled}
Matthias Bauer and Andriy Mnih.
\newblock Resampled priors for variational autoencoders.
\newblock \emph{arXiv preprint arXiv:1810.11428}, 2018.

\bibitem[Besag(1975)]{besag1975statistical}
Julian Besag.
\newblock Statistical analysis of non-lattice data.
\newblock \emph{Journal of the Royal Statistical Society: Series D (The
  Statistician)}, 24\penalty0 (3):\penalty0 179--195, 1975.

\bibitem[Blei et~al.(2017)Blei, Kucukelbir, and McAuliffe]{blei2017variational}
David~M Blei, Alp Kucukelbir, and Jon~D McAuliffe.
\newblock Variational inference: A review for statisticians.
\newblock \emph{Journal of the American Statistical Association}, 2017.

\bibitem[Brown(1986)]{brown1986fundamentals}
Lawrence~D Brown.
\newblock Fundamentals of statistical exponential families: with applications
  in statistical decision theory.
\newblock Ims, 1986.

\bibitem[Burda et~al.(2015)Burda, Grosse, and
  Salakhutdinov]{burda2015importance}
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov.
\newblock Importance weighted autoencoders.
\newblock \emph{nternational Conference on Learning Representations}, 2015.

\bibitem[Caterini et~al.(2018)Caterini, Doucet, and
  Sejdinovic]{caterini2018hamiltonian}
Anthony~L Caterini, Arnaud Doucet, and Dino Sejdinovic.
\newblock Hamiltonian variational auto-encoder.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8167--8177, 2018.

\bibitem[Chen et~al.(2016)Chen, Kingma, Salimans, Duan, Dhariwal, Schulman,
  Sutskever, and Abbeel]{chen2016variational}
Xi~Chen, Diederik~P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John
  Schulman, Ilya Sutskever, and Pieter Abbeel.
\newblock Variational lossy autoencoder.
\newblock \emph{International Conference on Learning Representations}, 2016.

\bibitem[Chung et~al.(2015)Chung, Kastner, Dinh, Goel, Courville, and
  Bengio]{chung2015recurrent}
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron~C Courville,
  and Yoshua Bengio.
\newblock A recurrent latent variable model for sequential data.
\newblock In \emph{Advances in neural information processing systems}, pages
  2980--2988, 2015.

\bibitem[Cremer et~al.(2017)Cremer, Morris, and
  Duvenaud]{cremer2017reinterpreting}
Chris Cremer, Quaid Morris, and David Duvenaud.
\newblock Reinterpreting importance-weighted autoencoders.
\newblock \emph{arXiv preprint arXiv:1704.02916}, 2017.

\bibitem[Dai et~al.(2018)Dai, Dai, Gretton, Song, Schuurmans, and
  He]{dai2018kernel}
Bo~Dai, Hanjun Dai, Arthur Gretton, Le~Song, Dale Schuurmans, and Niao He.
\newblock Kernel exponential family estimation via doubly dual embedding.
\newblock \emph{arXiv preprint arXiv:1811.02228}, 2018.

\bibitem[Dai et~al.(2017)Dai, Almahairi, Bachman, Hovy, and
  Courville]{dai2017calibrating}
Zihang Dai, Amjad Almahairi, Philip Bachman, Eduard Hovy, and Aaron Courville.
\newblock Calibrating energy-based generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1702.01691}, 2017.

\bibitem[Dayan et~al.(1995)Dayan, Hinton, Neal, and Zemel]{dayan1995helmholtz}
Peter Dayan, Geoffrey~E Hinton, Radford~M Neal, and Richard~S Zemel.
\newblock The helmholtz machine.
\newblock \emph{Neural computation}, 7\penalty0 (5):\penalty0 889--904, 1995.

\bibitem[Denton and Fergus(2018)]{denton2018stochastic}
Emily Denton and Rob Fergus.
\newblock Stochastic video generation with a learned prior.
\newblock \emph{International Conference on Machine Learning}, 2018.

\bibitem[Dinh et~al.(2016)Dinh, Sohl-Dickstein, and Bengio]{dinh2016density}
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
\newblock Density estimation using real nvp.
\newblock \emph{arXiv preprint arXiv:1605.08803}, 2016.

\bibitem[Domke and Sheldon(2019)]{domke2019divide}
Justin Domke and Daniel Sheldon.
\newblock Divide and couple: Using monte carlo variational objectives for
  posterior approximation.
\newblock \emph{arXiv preprint arXiv:1906.10115}, 2019.

\bibitem[Domke and Sheldon(2018)]{domke2018importance}
Justin Domke and Daniel~R Sheldon.
\newblock Importance weighting and variational inference.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4471--4480, 2018.

\bibitem[Doucet et~al.(2001)Doucet, De~Freitas, and
  Gordon]{doucet2001introduction}
Arnaud Doucet, Nando De~Freitas, and Neil Gordon.
\newblock An introduction to sequential monte carlo methods.
\newblock In \emph{Sequential Monte Carlo methods in practice}, pages 3--14.
  Springer, 2001.

\bibitem[Du and Mordatch(2019)]{du2019implicit}
Yilun Du and Igor Mordatch.
\newblock Implicit generation and generalization in energy-based models.
\newblock \emph{arXiv preprint arXiv:1903.08689}, 2019.

\bibitem[Fraccaro et~al.(2016)Fraccaro, S{\o}nderby, Paquet, and
  Winther]{fraccaro2016sequential}
Marco Fraccaro, S{\o}ren~Kaae S{\o}nderby, Ulrich Paquet, and Ole Winther.
\newblock Sequential neural models with stochastic layers.
\newblock In \emph{Advances in neural information processing systems}, pages
  2199--2207, 2016.

\bibitem[Freund and Haussler(1992)]{freund1992fast}
Yoav Freund and David Haussler.
\newblock A fast and exact learning rule for a restricted class of boltzmann
  machines.
\newblock \emph{Advances in Neural Information Processing Systems}, 4:\penalty0
  912--919, 1992.

\bibitem[Gao et~al.(2018)Gao, Lu, Zhou, Zhu, and Nian~Wu]{gao2018learning}
Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian~Wu.
\newblock Learning generative convnets via multi-grid modeling and sampling.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 9155--9164, 2018.

\bibitem[Goyal et~al.(2017)Goyal, Ke, Ganguli, and
  Bengio]{goyal2017variational}
Anirudh Goyal Alias~Parth Goyal, Nan~Rosemary Ke, Surya Ganguli, and Yoshua
  Bengio.
\newblock Variational walkback: Learning a transition operator as a stochastic
  recurrent net.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4392--4402, 2017.

\bibitem[Gulrajani et~al.(2016)Gulrajani, Kumar, Ahmed, Taiga, Visin, Vazquez,
  and Courville]{gulrajani2016pixelvae}
Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien~Ali Taiga, Francesco Visin,
  David Vazquez, and Aaron Courville.
\newblock Pixelvae: A latent variable model for natural images.
\newblock \emph{International Conference on Learning Representations}, 2016.

\bibitem[Gutmann and Hyv{\"a}rinen(2010)]{gutmann2010noise}
Michael Gutmann and Aapo Hyv{\"a}rinen.
\newblock Noise-contrastive estimation: A new estimation principle for
  unnormalized statistical models.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, pages 297--304, 2010.

\bibitem[Ha and Schmidhuber(2018)]{ha2018world}
David Ha and J{\"u}rgen Schmidhuber.
\newblock World models.
\newblock \emph{Advances in neural information processing systems}, 2018.

\bibitem[Hinton(2002)]{hinton2002training}
Geoffrey~E Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural computation}, 14\penalty0 (8):\penalty0 1771--1800,
  2002.

\bibitem[Hyv{\"a}rinen(2005)]{hyvarinen2005estimation}
Aapo Hyv{\"a}rinen.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (Apr):\penalty0 695--709, 2005.

\bibitem[Jang et~al.(2016)Jang, Gu, and Poole]{jang2016categorical}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock \emph{arXiv preprint arXiv:1611.01144}, 2016.

\bibitem[Jordan et~al.(1999)Jordan, Ghahramani, Jaakkola, and
  Saul]{jordan1999introduction}
Michael~I Jordan, Zoubin Ghahramani, Tommi~S Jaakkola, and Lawrence~K Saul.
\newblock An introduction to variational methods for graphical models.
\newblock \emph{Machine learning}, 37\penalty0 (2):\penalty0 183--233, 1999.

\bibitem[Jozefowicz et~al.(2016)Jozefowicz, Vinyals, Schuster, Shazeer, and
  Wu]{jozefowicz2016exploring}
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.
\newblock Exploring the limits of language modeling.
\newblock \emph{arXiv preprint arXiv:1602.02410}, 2016.

\bibitem[Kim and Bengio(2016)]{kim2016deep}
Taesup Kim and Yoshua Bengio.
\newblock Deep directed generative models with energy-based probability
  estimation.
\newblock \emph{arXiv preprint arXiv:1606.03439}, 2016.

\bibitem[Kinderman and Snell(1980)]{kinderman1980markov}
R.~Kinderman and S.L. Snell.
\newblock \emph{Markov random fields and their applications}.
\newblock American mathematical society, 1980.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma and Welling(2013)]{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{nternational Conference on Learning Representations}, 2013.

\bibitem[Kingma et~al.(2016)Kingma, Salimans, Jozefowicz, Chen, Sutskever, and
  Welling]{kingma2016improved}
Diederik~P Kingma, Tim Salimans, Rafal Jozefowicz, Xi~Chen, Ilya Sutskever, and
  Max Welling.
\newblock Improved variational inference with inverse autoregressive flow.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4743--4751, 2016.

\bibitem[Krishnan et~al.(2015)Krishnan, Shalit, and Sontag]{krishnan2015deep}
Rahul~G Krishnan, Uri Shalit, and David Sontag.
\newblock Deep kalman filters.
\newblock \emph{arXiv preprint arXiv:1511.05121}, 2015.

\bibitem[Lafferty et~al.(2001)Lafferty, McCallum, and
  Pereira]{lafferty2001conditional}
John~D Lafferty, Andrew McCallum, and Fernando~CN Pereira.
\newblock Conditional random fields: Probabilistic models for segmenting and
  labeling sequence data.
\newblock In \emph{Proceedings of the Eighteenth International Conference on
  Machine Learning}, pages 282--289. Morgan Kaufmann Publishers Inc., 2001.

\bibitem[Le et~al.(2017)Le, Igl, Rainforth, Jin, and Wood]{le2017auto}
Tuan~Anh Le, Maximilian Igl, Tom Rainforth, Tom Jin, and Frank Wood.
\newblock Auto-encoding sequential monte carlo.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[LeCun(1998)]{lecun1998mnist}
Yann LeCun.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[LeCun et~al.(2006)LeCun, Chopra, and Hadsell]{lecun2006tutorial}
Yann LeCun, Sumit Chopra, and Raia Hadsell.
\newblock A tutorial on energy-based learning.
\newblock 2006.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015faceattributes}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of International Conference on Computer Vision
  (ICCV)}, 2015.

\bibitem[Ma and Collins(2018)]{ma2018noise}
Zhuang Ma and Michael Collins.
\newblock Noise contrastive estimation and negative sampling for conditional
  models: Consistency and statistical efficiency.
\newblock \emph{arXiv preprint arXiv:1809.01812}, 2018.

\bibitem[Maal{\o}e et~al.(2016)Maal{\o}e, S{\o}nderby, S{\o}nderby, and
  Winther]{maaloe2016auxiliary}
Lars Maal{\o}e, Casper~Kaae S{\o}nderby, S{\o}ren~Kaae S{\o}nderby, and Ole
  Winther.
\newblock Auxiliary deep generative models.
\newblock \emph{arXiv preprint arXiv:1602.05473}, 2016.

\bibitem[Maddison et~al.(2016)Maddison, Mnih, and Teh]{maddison2016concrete}
Chris~J Maddison, Andriy Mnih, and Yee~Whye Teh.
\newblock The concrete distribution: A continuous relaxation of discrete random
  variables.
\newblock \emph{arXiv preprint arXiv:1611.00712}, 2016.

\bibitem[Maddison et~al.(2017)Maddison, Lawson, Tucker, Heess, Norouzi, Mnih,
  Doucet, and Teh]{maddison2017filtering}
Chris~J Maddison, Dieterich Lawson, George Tucker, Nicolas Heess, Mohammad
  Norouzi, Andriy Mnih, Arnaud Doucet, and Yee Teh.
\newblock Filtering variational objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6573--6583, 2017.

\bibitem[Mnih and Rezende(2016)]{mnih2016variational}
Andriy Mnih and Danilo~J Rezende.
\newblock Variational inference for monte carlo objectives.
\newblock \emph{International Conference on Machine Learning}, 2016.

\bibitem[Molchanov et~al.(2018)Molchanov, Kharitonov, Sobolev, and
  Vetrov]{molchanov2018doubly}
Dmitry Molchanov, Valery Kharitonov, Artem Sobolev, and Dmitry Vetrov.
\newblock Doubly semi-implicit variational inference.
\newblock \emph{arXiv preprint arXiv:1810.02789}, 2018.

\bibitem[Naesseth et~al.(2018)Naesseth, Linderman, Ranganath, and
  Blei]{naesseth2018variational}
Christian Naesseth, Scott Linderman, Rajesh Ranganath, and David Blei.
\newblock Variational sequential monte carlo.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 968--977, 2018.

\bibitem[Neal(2001)]{neal2001annealed}
Radford~M Neal.
\newblock Annealed importance sampling.
\newblock \emph{Statistics and computing}, 11\penalty0 (2):\penalty0 125--139,
  2001.

\bibitem[Neal(2005)]{neal2005hamiltonian}
Radford~M Neal.
\newblock Hamiltonian importance sampling.
\newblock In \emph{In talk presented at the Banff International Research
  Station (BIRS) workshop on Mathematical Issues in Molecular Dynamics}, 2005.

\bibitem[Neal et~al.(2011)]{neal2011mcmc}
Radford~M Neal et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock \emph{Handbook of Markov Chain Monte Carlo}, 2\penalty0
  (11):\penalty0 2, 2011.

\bibitem[Nijkamp et~al.(2019)Nijkamp, Hill, Zhu, and Wu]{nijkamp2019learning}
Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Learning non-convergent non-persistent short-run mcmc toward
  energy-based model.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5233--5243, 2019.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Papamakarios et~al.(2017)Papamakarios, Pavlakou, and
  Murray]{papamakarios2017masked}
George Papamakarios, Theo Pavlakou, and Iain Murray.
\newblock Masked autoregressive flow for density estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2338--2347, 2017.

\bibitem[Ranganath et~al.(2016)Ranganath, Tran, and
  Blei]{ranganath2016hierarchical}
Rajesh Ranganath, Dustin Tran, and David Blei.
\newblock Hierarchical variational models.
\newblock In \emph{International Conference on Machine Learning}, pages
  324--333, 2016.

\bibitem[Rezende and Mohamed(2015)]{rezende2015variational}
Danilo Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In \emph{International Conference on Machine Learning}, pages
  1530--1538, 2015.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{International Conference on Machine Learning}, pages
  1278--1286, 2014.

\bibitem[Salakhutdinov and Murray(2008)]{salakhutdinov2008quantitative}
Ruslan Salakhutdinov and Iain Murray.
\newblock On the quantitative analysis of deep belief networks.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pages 872--879. ACM, 2008.

\bibitem[Salimans et~al.(2015)Salimans, Kingma, and
  Welling]{salimans2015markov}
Tim Salimans, Diederik Kingma, and Max Welling.
\newblock Markov chain monte carlo and variational inference: Bridging the gap.
\newblock In \emph{International Conference on Machine Learning}, pages
  1218--1226, 2015.

\bibitem[Smolensky(1986)]{smolensky1986information}
Paul Smolensky.
\newblock Information processing in dynamical systems: Foundations of harmony
  theory.
\newblock Technical report, Colorado Univ at Boulder Dept of Computer Science,
  1986.

\bibitem[Sobolev and Vetrov(2018)]{sobolev2018importance}
Artem Sobolev and Dmitry Vetrov.
\newblock Importance weighted hierarchical variational inference.
\newblock In \emph{Bayesian Deep Learning Workshop}, 2018.

\bibitem[Sohl-Dickstein et~al.(2011)Sohl-Dickstein, Battaglino, and
  DeWeese]{sohl2011minimum}
Jascha Sohl-Dickstein, Peter Battaglino, and Michael~R DeWeese.
\newblock Minimum probability flow learning.
\newblock In \emph{Proceedings of the 28th International Conference on
  International Conference on Machine Learning}, pages 905--912. Omnipress,
  2011.

\bibitem[Tieleman(2008)]{tieleman2008training}
Tijmen Tieleman.
\newblock Training restricted boltzmann machines using approximations to the
  likelihood gradient.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pages 1064--1071. ACM, 2008.

\bibitem[Williams(1992)]{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017/online}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\bibitem[Xie et~al.(2016)Xie, Lu, Zhu, and Wu]{xie2016theory}
Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu.
\newblock A theory of generative convnet.
\newblock In \emph{International Conference on Machine Learning}, pages
  2635--2644, 2016.

\bibitem[Xie et~al.(2017)Xie, Zhu, and Nian~Wu]{xie2017synthesizing}
Jianwen Xie, Song-Chun Zhu, and Ying Nian~Wu.
\newblock Synthesizing dynamic patterns by spatial-temporal generative convnet.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 7093--7101, 2017.

\bibitem[Xie et~al.(2018)Xie, Zheng, Gao, Wang, Zhu, and
  Nian~Wu]{xie2018learning}
Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and Ying
  Nian~Wu.
\newblock Learning descriptor networks for 3d shape synthesis and analysis.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 8629--8638, 2018.

\bibitem[Yin and Zhou(2018)]{yin2018semi}
Mingzhang Yin and Mingyuan Zhou.
\newblock Semi-implicit variational inference.
\newblock \emph{arXiv preprint arXiv:1805.11183}, 2018.

\bibitem[Zellner(1988)]{zellner1988optimal}
Arnold Zellner.
\newblock Optimal information processing and bayes's theorem.
\newblock \emph{The American Statistician}, 42\penalty0 (4):\penalty0 278--280,
  1988.

\bibitem[Zhu et~al.(1998)Zhu, Wu, and Mumford]{zhu1998filters}
Song~Chun Zhu, Yingnian Wu, and David Mumford.
\newblock Filters, random fields and maximum entropy (frame): Towards a unified
  theory for texture modeling.
\newblock \emph{International Journal of Computer Vision}, 27\penalty0
  (2):\penalty0 107--126, 1998.

\end{thebibliography}
