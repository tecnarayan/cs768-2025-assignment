\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, and Yang]{agarwal2020model}
Alekh Agarwal, Sham Kakade, and Lin~F Yang.
\newblock Model-based reinforcement learning with a generative model is minimax
  optimal.
\newblock In \emph{Conference on Learning Theory}, pages 67--83, 2020.

\bibitem[Antos et~al.(2008{\natexlab{a}})Antos, Munos, and
  Szepesvari]{antos2008fitted}
Andras Antos, Remi Munos, and Csaba Szepesvari.
\newblock Fitted q-iteration in continuous action-space mdps.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9--16, 2008{\natexlab{a}}.

\bibitem[Antos et~al.(2008{\natexlab{b}})Antos, Szepesv{\'a}ri, and
  Munos]{antos2008learning}
Andr{\'a}s Antos, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Learning near-optimal policies with bellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock \emph{Machine Learning}, 71\penalty0 (1):\penalty0 89--129,
  2008{\natexlab{b}}.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 263--272. JMLR. org, 2017.

\bibitem[Bai et~al.(2019)Bai, Xie, Jiang, and Wang]{bai2019provably}
Yu~Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang.
\newblock Provably efficient q-learning with low switching cost.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Brafman and Tennenholtz(2002)]{brafman2002r}
Ronen~I Brafman and Moshe Tennenholtz.
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Oct):\penalty0 213--231, 2002.

\bibitem[Bubeck and Cesa-Bianchi(2012)]{bubeck2012regret}
S{\'e}bastien Bubeck and Nicolo Cesa-Bianchi.
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit
  problems.
\newblock \emph{Foundations and Trends in Machine Learning}, 2012.

\bibitem[Buckman et~al.(2021)Buckman, Gelada, and
  Bellemare]{buckman2020importance}
Jacob Buckman, Carles Gelada, and Marc~G Bellemare.
\newblock The importance of pessimism in fixed-dataset policy optimization.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Cai et~al.(2020)Cai, Yang, Jin, and Wang]{cai2020provably}
Qi~Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang.
\newblock Provably efficient exploration in policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1283--1294. PMLR, 2020.

\bibitem[Cai and Low(2004)]{cai2004adaptation}
T~Tony Cai and Mark~G Low.
\newblock An adaptation theory for nonparametric confidence intervals.
\newblock \emph{The Annals of statistics}, 32\penalty0 (5):\penalty0
  1805--1840, 2004.

\bibitem[Chang et~al.(2021)Chang, Uehara, Sreenivas, Kidambi, and
  Sun]{chang2021mitigating}
Jonathan~D Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, and Wen
  Sun.
\newblock Mitigating covariate shift in imitation learning via offline data
  without great coverage.
\newblock \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Chen and Jiang(2019)]{chen2019information}
Jinglin Chen and Nan Jiang.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1042--1051, 2019.

\bibitem[Chernoff et~al.(1952)]{chernoff1952measure}
Herman Chernoff et~al.
\newblock A measure of asymptotic efficiency for tests of a hypothesis based on
  the sum of observations.
\newblock \emph{The Annals of Mathematical Statistics}, 23\penalty0
  (4):\penalty0 493--507, 1952.

\bibitem[Cheung et~al.(2020)Cheung, Simchi-Levi, and
  Zhu]{cheung2020reinforcement}
Wang~Chi Cheung, David Simchi-Levi, and Ruihao Zhu.
\newblock Reinforcement learning for non-stationary markov decision processes:
  The blessing of (more) optimism.
\newblock In \emph{International Conference on Machine Learning}, pages
  1843--1854. PMLR, 2020.

\bibitem[Duan et~al.(2020)Duan, Jia, and Wang]{duan2020minimax}
Yaqi Duan, Zeyu Jia, and Mengdi Wang.
\newblock Minimax-optimal off-policy evaluation with linear function
  approximation.
\newblock In \emph{International Conference on Machine Learning}, pages
  8334--8342, 2020.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Fu et~al.(2021)Fu, Norouzi, Nachum, Tucker, Wang, Novikov, Yang,
  Zhang, Chen, Kumar, et~al.]{fu2021benchmarks}
Justin Fu, Mohammad Norouzi, Ofir Nachum, George Tucker, Ziyu Wang, Alexander
  Novikov, Mengjiao Yang, Michael~R Zhang, Yutian Chen, Aviral Kumar, et~al.
\newblock Benchmarks for deep off-policy evaluation.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Gulcehre et~al.(2020)Gulcehre, Wang, Novikov, Paine, Colmenarejo,
  Zolna, Agarwal, Merel, Mankowitz, Paduraru, et~al.]{gulcehre2020rl}
Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom~Le Paine, Sergio~G{\'o}mez
  Colmenarejo, Konrad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz,
  Cosmin Paduraru, et~al.
\newblock Rl unplugged: Benchmarks for offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 2020.

\bibitem[Janner et~al.(2021)Janner, Li, and Levine]{janner2021reinforcement}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock Reinforcement learning as one big sequence modeling problem.
\newblock \emph{Advances in neural information processing systems}, 2021.

\bibitem[Jiang and Agarwal(2018)]{jiang2018open}
Nan Jiang and Alekh Agarwal.
\newblock Open problem: The dependence of sample complexity lower bounds on
  planning horizon.
\newblock In \emph{Conference On Learning Theory}, pages 3395--3398, 2018.

\bibitem[Jiang and Li(2016)]{jiang2016doubly}
Nan Jiang and Lihong Li.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \emph{Proceedings of the 33rd International Conference on
  International Conference on Machine Learning-Volume 48}, pages 652--661.
  JMLR. org, 2016.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E
  Schapire.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \emph{International Conference on Machine Learning-Volume 70},
  pages 1704--1713, 2017.

\bibitem[Jin et~al.(2020)Jin, Yang, and Wang]{jin2020pessimism}
Ying Jin, Zhuoran Yang, and Zhaoran Wang.
\newblock Is pessimism provably efficient for offline rl?
\newblock \emph{International Conference on Machine Learning}, 2020.

\bibitem[Jung and Stone(2010)]{jung2010gaussian}
Tobias Jung and Peter Stone.
\newblock Gaussian processes for sample efficient reinforcement learning with
  rmax-like exploration.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 601--616. Springer, 2010.

\bibitem[Khamaru et~al.(2021{\natexlab{a}})Khamaru, Pananjady, Ruan,
  Wainwright, and Jordan]{khamaru2020temporal}
Koulik Khamaru, Ashwin Pananjady, Feng Ruan, Martin~J Wainwright, and Michael~I
  Jordan.
\newblock Is temporal difference learning optimal? an instance-dependent
  analysis.
\newblock \emph{SIAM Journal on Mathematics of Data Science},
  2021{\natexlab{a}}.

\bibitem[Khamaru et~al.(2021{\natexlab{b}})Khamaru, Xia, Wainwright, and
  Jordan]{khamaru2021instance}
Koulik Khamaru, Eric Xia, Martin~J Wainwright, and Michael~I Jordan.
\newblock Instance-optimality in optimal value estimation: Adaptivity via
  variance-reduced q-learning.
\newblock \emph{arXiv preprint arXiv:2106.14352}, 2021{\natexlab{b}}.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{kidambi2020morel}
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
\newblock Morel: Model-based offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 2020.

\bibitem[Krishnamurthy et~al.(2016)Krishnamurthy, Agarwal, and
  Langford]{krishnamurthy2016pac}
Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock {PAC} reinforcement learning with rich observations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1840--1848, 2016.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{lange2012batch}
Sascha Lange, Thomas Gabel, and Martin Riedmiller.
\newblock Batch reinforcement learning.
\newblock In \emph{Reinforcement learning}, pages 45--73. Springer, 2012.

\bibitem[Le et~al.(2019)Le, Voloshin, and Yue]{le2019batch}
Hoang Le, Cameron Voloshin, and Yisong Yue.
\newblock Batch policy learning under constraints.
\newblock In \emph{International Conference on Machine Learning}, pages
  3703--3712, 2019.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Liu et~al.(2019)Liu, Swaminathan, Agarwal, and Brunskill]{liu2019off}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Off-policy policy gradient with state distribution correction.
\newblock In \emph{Uncertainty in Artificial Intelligence}, 2019.

\bibitem[Liu et~al.(2020)Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020provably}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Provably good batch reinforcement learning without great exploration.
\newblock \emph{Advances in neural information processing systems}, 2020.

\bibitem[Maillard et~al.(2014)Maillard, Mann, and Mannor]{maillard2014hard}
Odalric-Ambrym Maillard, Timothy~A Mann, and Shie Mannor.
\newblock How hard is my mdp?" the distribution-norm to the rescue".
\newblock \emph{Advances in Neural Information Processing Systems},
  27:\penalty0 1835--1843, 2014.

\bibitem[Maurer and Pontil(2009)]{maurer2009empirical}
Andreas Maurer and Massimiliano Pontil.
\newblock Empirical bernstein bounds and sample variance penalization.
\newblock \emph{Conference on Learning Theory}, 2009.

\bibitem[Rashidinejad et~al.(2021)Rashidinejad, Zhu, Ma, Jiao, and
  Russell]{rashidinejad2021bridging}
Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell.
\newblock Bridging offline reinforcement learning and imitation learning: A
  tale of pessimism.
\newblock \emph{arXiv preprint arXiv:2103.12021}, 2021.

\bibitem[Ren et~al.(2021)Ren, Li, Dai, Du, and Sanghavi]{ren2021nearly}
Tongzheng Ren, Jialian Li, Bo~Dai, Simon~S Du, and Sujay Sanghavi.
\newblock Nearly horizon-free offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 2021.

\bibitem[Sridharan(2002)]{sridharan2002gentle}
Karthik Sridharan.
\newblock A gentle introduction to concentration inequalities.
\newblock \emph{Dept. Comput. Sci., Cornell Univ., Tech. Rep}, 2002.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Szepesv{\'a}ri and Munos(2005)]{szepesvari2005finite}
Csaba Szepesv{\'a}ri and R{\'e}mi Munos.
\newblock Finite time bounds for sampling based fitted value iteration.
\newblock In \emph{Proceedings of the 22nd international conference on Machine
  learning}, pages 880--887, 2005.

\bibitem[Tropp et~al.(2011)]{tropp2011freedman}
Joel Tropp et~al.
\newblock Freedman's inequality for matrix martingales.
\newblock \emph{Electronic Communications in Probability}, 16:\penalty0
  262--270, 2011.

\bibitem[Uehara and Sun(2021)]{uehara2021pessimistic}
Masatoshi Uehara and Wen Sun.
\newblock Pessimistic model-based offline rl: Pac bounds and posterior sampling
  under partial coverage.
\newblock \emph{arXiv preprint arXiv:2107.06226}, 2021.

\bibitem[Wang et~al.(2021)Wang, Foster, and Kakade]{wang2020statistical}
Ruosong Wang, Dean~P Foster, and Sham~M Kakade.
\newblock What are the statistical limits of offline rl with linear function
  approximation?
\newblock \emph{International Conference on Machine Learning}, 2021.

\bibitem[Wen and Van~Roy(2013)]{wen2013efficient}
Zheng Wen and Benjamin Van~Roy.
\newblock Efficient exploration and value function generalization in
  deterministic systems.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Xiao et~al.(2021)Xiao, Wu, Mei, Dai, Lattimore, Li, Szepesvari, and
  Schuurmans]{xiao2021optimality}
Chenjun Xiao, Yifan Wu, Jincheng Mei, Bo~Dai, Tor Lattimore, Lihong Li, Csaba
  Szepesvari, and Dale Schuurmans.
\newblock On the optimality of batch policy optimization algorithms.
\newblock In \emph{International Conference on Machine Learning}, pages
  11362--11371. PMLR, 2021.

\bibitem[Xie and Jiang(2020)]{xie2020q}
Tengyang Xie and Nan Jiang.
\newblock Q* approximation schemes for batch reinforcement learning: A
  theoretical comparison.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 550--559,
  2020.

\bibitem[Xie and Jiang(2021)]{xie2020batch}
Tengyang Xie and Nan Jiang.
\newblock Batch value-function approximation with only realizability.
\newblock \emph{International Conference on Machine Learning}, 2021.

\bibitem[Xie et~al.(2021{\natexlab{a}})Xie, Cheng, Jiang, Mineiro, and
  Agarwal]{xie2021bellman}
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal.
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  2021{\natexlab{a}}.

\bibitem[Xie et~al.(2021{\natexlab{b}})Xie, Jiang, Wang, Xiong, and
  Bai]{xie2021policy}
Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu~Bai.
\newblock Policy finetuning: Bridging sample-efficient offline and online
  reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  2021{\natexlab{b}}.

\bibitem[Yin and Wang(2020)]{yin2020asymptotically}
Ming Yin and Yu-Xiang Wang.
\newblock Asymptotically efficient off-policy evaluation for tabular
  reinforcement learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3948--3958. PMLR, 2020.

\bibitem[Yin and Wang(2021)]{yin2021optimal}
Ming Yin and Yu-Xiang Wang.
\newblock Optimal uniform ope and model-based offline reinforcement learning in
  time-homogeneous, reward-free and task-agnostic settings.
\newblock \emph{Advances in neural information processing systems}, 2021.

\bibitem[Yin et~al.(2021{\natexlab{a}})Yin, Bai, and Wang]{yin2021near}
Ming Yin, Yu~Bai, and Yu-Xiang Wang.
\newblock Near-optimal provable uniform convergence in offline policy
  evaluation for reinforcement learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1567--1575. PMLR, 2021{\natexlab{a}}.

\bibitem[Yin et~al.(2021{\natexlab{b}})Yin, Bai, and Wang]{yin2021nearoptimal}
Ming Yin, Yu~Bai, and Yu-Xiang Wang.
\newblock Near-optimal offline reinforcement learning via double variance
  reduction.
\newblock \emph{Advances in neural information processing systems},
  2021{\natexlab{b}}.

\bibitem[Zanette(2021)]{zanette2020exponential}
Andrea Zanette.
\newblock Exponential lower bounds for batch reinforcement learning: Batch rl
  can be exponentially harder than online rl.
\newblock \emph{International Conference on Machine Learning}, 2021.

\bibitem[Zanette and Brunskill(2018)]{zanette2018problem}
Andrea Zanette and Emma Brunskill.
\newblock Problem dependent reinforcement learning bounds which can identify
  bandit structure in mdps.
\newblock In \emph{International Conference on Machine Learning}, pages
  5747--5755. PMLR, 2018.

\bibitem[Zanette and Brunskill(2019)]{zanette2019tighter}
Andrea Zanette and Emma Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{International Conference on Machine Learning}, pages
  7304--7312. PMLR, 2019.

\bibitem[Zanette et~al.(2021)Zanette, Wainwright, and
  Brunskill]{zanette2021provable}
Andrea Zanette, Martin~J Wainwright, and Emma Brunskill.
\newblock Provable benefits of actor-critic methods for offline reinforcement
  learning.
\newblock \emph{Advances in neural information processing systems}, 2021.

\bibitem[Zhang et~al.(2021)Zhang, Ji, and Du]{zhang2020reinforcement}
Zihan Zhang, Xiangyang Ji, and Simon~S Du.
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock \emph{Conference of Learning Theory}, 2021.

\end{thebibliography}
