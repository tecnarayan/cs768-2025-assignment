\begin{thebibliography}{78}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2018)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Abbeel, and Zaremba]{andrychowicz_hindsight_2018}
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong,
  Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba.
\newblock Hindsight {Experience} {Replay}.
\newblock \emph{arXiv:1707.01495 [cs]}, February 2018.
\newblock URL \url{http://arxiv.org/abs/1707.01495}.
\newblock arXiv: 1707.01495.

\bibitem[Arjona-Medina et~al.(2019)Arjona-Medina, Gillhofer, Widrich,
  Unterthiner, Brandstetter, and Hochreiter]{arjona2019rudder}
Jose~A Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner,
  Johannes Brandstetter, and Sepp Hochreiter.
\newblock Rudder: Return decomposition for delayed rewards.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  13566--13577, 2019.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{arjovsky_wasserstein_2017}
Martin Arjovsky, Soumith Chintala, and Léon Bottou.
\newblock Wasserstein {Generative} {Adversarial} {Networks}.
\newblock In \emph{International {Conference} on {Machine} {Learning}}, pages
  214--223, July 2017.
\newblock URL \url{http://proceedings.mlr.press/v70/arjovsky17a.html}.
\newblock ISSN: 2640-3498 Section: Machine Learning.

\bibitem[Arumugam et~al.(2021)Arumugam, Henderson, and Bacon]{Arumugam2021AnIP}
Dilip Arumugam, P.~Henderson, and P.~Bacon.
\newblock An information-theoretic perspective on credit assignment in
  reinforcement learning.
\newblock \emph{ArXiv}, abs/2103.06224, 2021.

\bibitem[Baldassarre(2011)]{baldassarre2011intrinsic}
Gianluca Baldassarre.
\newblock What are intrinsic motivations? a biological perspective.
\newblock In \emph{2011 IEEE international conference on development and
  learning (ICDL)}, volume~2, pages 1--8. IEEE, 2011.

\bibitem[Baldassarre et~al.(2014)Baldassarre, Stafford, Mirolli, Redgrave,
  Ryan, and Barto]{baldassarre2014intrinsic}
Gianluca Baldassarre, Tom Stafford, Marco Mirolli, Peter Redgrave, Richard~M
  Ryan, and Andrew Barto.
\newblock Intrinsic motivations and open-ended development in animals, humans,
  and robots: an overview.
\newblock \emph{Frontiers in psychology}, 5:\penalty0 985, 2014.

\bibitem[Baranes and Oudeyer(2009)]{baranes2009r}
Adrien Baranes and Pierre-Yves Oudeyer.
\newblock R-iac: Robust intrinsically motivated exploration and active
  learning.
\newblock \emph{IEEE Transactions on Autonomous Mental Development}, 1\penalty0
  (3):\penalty0 155--169, 2009.

\bibitem[Barto(2013)]{baldassarre_intrinsic_2013}
Andrew~G. Barto.
\newblock Intrinsic {Motivation} and {Reinforcement} {Learning}.
\newblock In Gianluca Baldassarre and Marco Mirolli, editors,
  \emph{Intrinsically {Motivated} {Learning} in {Natural} and {Artificial}
  {Systems}}, pages 17--47. Springer Berlin Heidelberg, Berlin, Heidelberg,
  2013.
\newblock ISBN 978-3-642-32374-4 978-3-642-32375-1.
\newblock \doi{10.1007/978-3-642-32375-1_2}.
\newblock URL \url{http://link.springer.com/10.1007/978-3-642-32375-1_2}.

\bibitem[Barto and Simsek(2005)]{barto2005intrinsic}
Andrew~G Barto and Ozg{\"u}r Simsek.
\newblock Intrinsic motivation for reinforcement learning systems.
\newblock In \emph{Proceedings of the Thirteenth Yale Workshop on Adaptive and
  Learning Systems}, pages 113--118, 2005.

\bibitem[Barto et~al.(2004)Barto, Singh, and Chentanez]{barto2004intrinsically}
Andrew~G Barto, Satinder Singh, and Nuttapong Chentanez.
\newblock Intrinsically motivated learning of hierarchical collections of
  skills.
\newblock In \emph{Proceedings of the 3rd International Conference on
  Development and Learning}, pages 112--19. Cambridge, MA, 2004.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,
  Saxton, and Munos]{bellemare2016unifying}
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,
  and Remi Munos.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In \emph{Advances in neural information processing systems}, pages
  1471--1479, 2016.

\bibitem[Bellemare et~al.(2017)Bellemare, Danihelka, Dabney, Mohamed,
  Lakshminarayanan, Hoyer, and Munos]{Bellemare2017CramerD}
Marc~G. Bellemare, Ivo Danihelka, Will Dabney, S.~Mohamed, Balaji
  Lakshminarayanan, Stephan Hoyer, and R.~Munos.
\newblock The cramer distance as a solution to biased wasserstein gradients.
\newblock \emph{ArXiv}, abs/1705.10743, 2017.
\newblock URL \url{https://arxiv.org/abs/1705.10743}.

\bibitem[Berseth et~al.(2019)Berseth, Geng, Devin, Rhinehart, Finn, Jayaraman,
  and Levine]{berseth2019smirl}
Glen Berseth, Daniel Geng, Coline Devin, Nicholas Rhinehart, Chelsea Finn,
  Dinesh Jayaraman, and Sergey Levine.
\newblock Smirl: Surprise minimizing reinforcement learning in unstable
  environments.
\newblock \emph{arXiv preprint arXiv:1912.05510}, 2019.

\bibitem[Bousquet et~al.(2017)Bousquet, Gelly, Tolstikhin, Simon-Gabriel, and
  Schoelkopf]{bousquet_optimal_2017}
Olivier Bousquet, Sylvain Gelly, Ilya Tolstikhin, Carl-Johann Simon-Gabriel,
  and Bernhard Schoelkopf.
\newblock From optimal transport to generative modeling: the {VEGAN} cookbook.
\newblock \emph{arXiv:1705.07642 [stat]}, May 2017.
\newblock URL \url{http://arxiv.org/abs/1705.07642}.
\newblock arXiv: 1705.07642.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Burda et~al.(2019)Burda, Edwards, Storkey, and Klimov]{burda2018rnd}
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov.
\newblock Exploration by random network distillation.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Caruana(1997)]{caruana1997multitask}
Rich Caruana.
\newblock Multitask learning.
\newblock \emph{Machine learning}, 28\penalty0 (1):\penalty0 41--75, 1997.

\bibitem[Clark and Amodei(2016)]{clark_faulty_2016}
Jack Clark and Dario Amodei.
\newblock Faulty reward functions in the wild, Dec 2016.
\newblock URL \url{https://openai.com/blog/faulty-reward-functions/}.

\bibitem[Dadashi et~al.(2020)Dadashi, Hussenot, Geist, and
  Pietquin]{dadashi_primal_2020}
Robert Dadashi, Léonard Hussenot, Matthieu Geist, and Olivier Pietquin.
\newblock Primal {Wasserstein} {Imitation} {Learning}.
\newblock \emph{arXiv:2006.04678 [cs, stat]}, June 2020.
\newblock URL \url{http://arxiv.org/abs/2006.04678}.
\newblock arXiv: 2006.04678.

\bibitem[Ding et~al.(2019)Ding, Florensa, Phielipp, and
  Abbeel]{Ding2019GoalconditionedIL}
Y.~Ding, Carlos Florensa, Mariano Phielipp, and P.~Abbeel.
\newblock Goal-conditioned imitation learning.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Eysenbach et~al.(2021{\natexlab{a}})Eysenbach, Levine, and
  Salakhutdinov]{eysenbach2021replacing}
Benjamin Eysenbach, Sergey Levine, and Ruslan Salakhutdinov.
\newblock Replacing rewards with examples: Example-based policy search via
  recursive classification.
\newblock \emph{arXiv preprint arXiv:2103.12656}, 2021{\natexlab{a}}.

\bibitem[Eysenbach et~al.(2021{\natexlab{b}})Eysenbach, Salakhutdinov, and
  Levine]{eysenbach2021clearning}
Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine.
\newblock C-learning: Learning to achieve goals via recursive classification.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=tc5qisoB-C}.

\bibitem[Forestier et~al.(2017)Forestier, Portelas, Mollard, and
  Oudeyer]{forestier2017intrinsically}
S{\'e}bastien Forestier, R{\'e}my Portelas, Yoan Mollard, and Pierre-Yves
  Oudeyer.
\newblock Intrinsically motivated goal exploration processes with automatic
  curriculum learning.
\newblock \emph{arXiv preprint arXiv:1708.02190}, 2017.

\bibitem[Fu et~al.(2018)Fu, Luo, and Levine]{fu_learning_2018}
Justin Fu, Katie Luo, and Sergey Levine.
\newblock Learning {Robust} {Rewards} with {Adversarial} {Inverse}
  {Reinforcement} {Learning}.
\newblock \emph{arXiv:1710.11248 [cs]}, August 2018.
\newblock URL \url{http://arxiv.org/abs/1710.11248}.
\newblock arXiv: 1710.11248.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and Meger]{fujimoto2018TD3}
Scott Fujimoto, Herke Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning}, pages
  1587--1596. PMLR, 2018.

\bibitem[Ganin et~al.(2018)Ganin, Kulkarni, Babuschkin, Eslami, and
  Vinyals]{ganin2018synthesizing}
Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, SM~Ali Eslami, and Oriol
  Vinyals.
\newblock Synthesizing programs for images using reinforced adversarial
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1666--1675. PMLR, 2018.

\bibitem[Ghasemipour et~al.(2020)Ghasemipour, Zemel, and
  Gu]{ghasemipour2020divergence}
Seyed Kamyar~Seyed Ghasemipour, Richard Zemel, and Shixiang Gu.
\newblock A divergence minimization perspective on imitation learning methods.
\newblock In \emph{Conference on Robot Learning}, pages 1259--1277. PMLR, 2020.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow_generative_2014}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative {Adversarial} {Nets}.
\newblock In Z.~Ghahramani, M.~Welling, C.~Cortes, N.~D. Lawrence, and K.~Q.
  Weinberger, editors, \emph{Advances in {Neural} {Information} {Processing}
  {Systems} 27}, pages 2672--2680. Curran Associates, Inc., 2014.
\newblock URL
  \url{http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf}.

\bibitem[Gottlieb et~al.(2013)Gottlieb, Oudeyer, Lopes, and
  Baranes]{gottlieb2013information}
Jacqueline Gottlieb, Pierre-Yves Oudeyer, Manuel Lopes, and Adrien Baranes.
\newblock Information-seeking, curiosity, and attention: computational and
  neural mechanisms.
\newblock \emph{Trends in cognitive sciences}, 17\penalty0 (11):\penalty0
  585--593, 2013.

\bibitem[Guillot and Stauffer(2020)]{guillot2020stochastic}
Matthieu Guillot and Gautier Stauffer.
\newblock The stochastic shortest path problem: a polyhedral combinatorics
  perspective.
\newblock \emph{European Journal of Operational Research}, 285\penalty0
  (1):\penalty0 148--158, 2020.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{gulrajani_improved_2017}
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron~C
  Courville.
\newblock Improved {Training} of {Wasserstein} {GANs}.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in {Neural}
  {Information} {Processing} {Systems} 30}, pages 5767--5777. Curran
  Associates, Inc., 2017.
\newblock URL
  \url{http://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans.pdf}.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1352--1361, 2017.

\bibitem[Hart et~al.(1968)Hart, Nilsson, and Raphael]{hart1968formal}
Peter~E Hart, Nils~J Nilsson, and Bertram Raphael.
\newblock A formal basis for the heuristic determination of minimum cost paths.
\newblock \emph{IEEE transactions on Systems Science and Cybernetics},
  4\penalty0 (2):\penalty0 100--107, 1968.

\bibitem[Hartikainen et~al.(2020)Hartikainen, Geng, Haarnoja, and
  Levine]{hartikainen2019ddl}
Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, and Sergey Levine.
\newblock Dynamical distance learning for semi-supervised and unsupervised
  skill discovery.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Hausknecht and Stone(2016)]{hausknecht2016deep}
Matthew Hausknecht and Peter Stone.
\newblock Deep reinforcement learning in parameterized action space.
\newblock In \emph{International Conference on Learning Representations}, 2016.
\newblock URL \url{https://arxiv.org/abs/1511.04143}.

\bibitem[Hazan et~al.(2019)Hazan, Kakade, Singh, and
  Van~Soest]{hazan2019provably}
Elad Hazan, Sham Kakade, Karan Singh, and Abby Van~Soest.
\newblock Provably efficient maximum entropy exploration.
\newblock In \emph{International Conference on Machine Learning}, pages
  2681--2691. PMLR, 2019.

\bibitem[Hill et~al.(2018)Hill, Raffin, Ernestus, Gleave, Kanervisto, Traore,
  Dhariwal, Hesse, Klimov, Nichol, Plappert, Radford, Schulman, Sidor, and
  Wu]{stable-baselines}
Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi
  Kanervisto, Rene Traore, Prafulla Dhariwal, Christopher Hesse, Oleg Klimov,
  Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor,
  and Yuhuai Wu.
\newblock Stable baselines.
\newblock \url{https://github.com/hill-a/stable-baselines}, 2018.

\bibitem[Ho and Ermon(2016)]{ho2016generative}
Jonathan Ho and Stefano Ermon.
\newblock Generative adversarial imitation learning.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 4565--4573, 2016.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch_near-optimal_2010}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal {Regret} {Bounds} for {Reinforcement} {Learning}.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (51):\penalty0 1563--1600, 2010.
\newblock ISSN 1533-7928.
\newblock URL \url{http://jmlr.org/papers/v11/jaksch10a.html}.

\bibitem[Jevti{\'c}(2018)]{jevtic2018combinatorial}
Filip Jevti{\'c}.
\newblock \emph{Combinatorial Structure of Finite Metric Spaces}.
\newblock PhD thesis, The University of Texas at Dallas, August 2018.

\bibitem[Jinnai et~al.(2020)Jinnai, Park, Machado, and
  Konidaris]{Jinnai2020Exploration}
Yuu Jinnai, Jee~Won Park, Marlos~C. Machado, and George Konidaris.
\newblock Exploration in reinforcement learning with deep covering options.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SkeIyaVtwB}.

\bibitem[Kaelbling(1993)]{kaelbling1993learning}
Leslie~Pack Kaelbling.
\newblock Learning to achieve goals.
\newblock In \emph{IJCAI}, pages 1094--1099. Citeseer, 1993.

\bibitem[Kearns and Singh(2002)]{kearns2002near}
Michael Kearns and Satinder Singh.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine learning}, 49\penalty0 (2):\penalty0 209--232, 2002.

\bibitem[Knox et~al.(2021)Knox, Allievi, Banzhaf, Schmitt, and
  Stone]{knox2021reward}
W~Bradley Knox, Alessandro Allievi, Holger Banzhaf, Felix Schmitt, and Peter
  Stone.
\newblock Reward (mis) design for autonomous driving.
\newblock \emph{arXiv preprint arXiv:2104.13906}, 2021.

\bibitem[Lee et~al.(2019)Lee, Eysenbach, Parisotto, Xing, Levine, and
  Salakhutdinov]{lee2019efficient}
Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and
  Ruslan Salakhutdinov.
\newblock Efficient exploration via state marginal matching.
\newblock \emph{arXiv preprint arXiv:1906.05274}, 2019.

\bibitem[Liao and Berg(2018)]{liao2018sharpening}
JG~Liao and Arthur Berg.
\newblock Sharpening jensen's inequality.
\newblock \emph{The American Statistician}, 2018.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Liu and Qiu(2020)]{liu2020lipschitz}
Kanglin Liu and Guoping Qiu.
\newblock Lipschitz constrained gans via boundedness and continuity.
\newblock \emph{Neural Computing and Applications}, pages 1--13, 2020.

\bibitem[Montgomery(2017)]{montgomery2017design}
Douglas~C Montgomery.
\newblock \emph{Design and analysis of experiments}.
\newblock John wiley \& sons, 2017.

\bibitem[Nasiriany(2020)]{Nasiriany:EECS-2020-151}
Soroush Nasiriany.
\newblock Disco rl: Distribution-conditioned reinforcement learning for
  general-purpose policies.
\newblock Master's thesis, EECS Department, University of California, Berkeley,
  Aug 2020.
\newblock URL
  \url{http://www2.eecs.berkeley.edu/Pubs/TechRpts/2020/EECS-2020-151.html}.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999policy}
Andrew~Y Ng, Daishi Harada, and Stuart Russell.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In \emph{ICML}, volume~99, pages 278--287, 1999.

\bibitem[Ni et~al.(2020)Ni, Sikchi, Wang, Gupta, Lee, and
  Eysenbach]{ni2020fIRL}
Tianwei Ni, Harshit Sikchi, Yufei Wang, Tejus Gupta, Lisa Lee, and Benjamin
  Eysenbach.
\newblock F-irl: Inverse reinforcement learning via state marginal matching.
\newblock In \emph{Conference on Robot Learning}, 2020.

\bibitem[Niekum(2010)]{niekum2010evolved}
Scott Niekum.
\newblock Evolved intrinsic reward functions for reinforcement learning.
\newblock In \emph{Proceedings of the Twenty-Fourth AAAI Conference on
  Artificial Intelligence}, pages 1955--1956, 2010.

\bibitem[Nota and Thomas(2020)]{nota2020PolicyGradient}
Chris Nota and Philip~S Thomas.
\newblock Is the policy gradient a gradient?
\newblock In \emph{International Conference on Autonomous Agents and
  Multi-Agent Systems}, pages 939--947, 2020.

\bibitem[Oudeyer and Kaplan(2008)]{oudeyer2008can}
Pierre-Yves Oudeyer and Frederic Kaplan.
\newblock How can we define intrinsic motivation?
\newblock In \emph{the 8th International Conference on Epigenetic Robotics:
  Modeling Cognitive Development in Robotic Systems}. Lund University Cognitive
  Studies, Lund: LUCS, Brighton, 2008.

\bibitem[Oudeyer and Kaplan(2009)]{oudeyer2009intrinsic}
Pierre-Yves Oudeyer and Frederic Kaplan.
\newblock What is intrinsic motivation? a typology of computational approaches.
\newblock \emph{Frontiers in neurorobotics}, 1:\penalty0 6, 2009.

\bibitem[Peyré and Cuturi(2019)]{peyre_computational_2020}
Gabriel Peyré and Marco Cuturi.
\newblock Computational optimal transport.
\newblock \emph{Foundations and Trends in Machine Learning}, 11\penalty0
  (5-6):\penalty0 355--607, 2019.
\newblock URL \url{http://arxiv.org/abs/1803.00567}.

\bibitem[Puterman(1990)]{puterman1990markov}
Martin~L Puterman.
\newblock Markov decision processes.
\newblock \emph{Handbooks in operations research and management science},
  2:\penalty0 331--434, 1990.

\bibitem[Raffin(2018)]{rl-zoo}
Antonin Raffin.
\newblock Rl baselines zoo.
\newblock \url{https://github.com/araffin/rl-baselines-zoo}, 2018.

\bibitem[Santucci et~al.(2013)Santucci, Baldassarre, and
  Mirolli]{santucci2013best}
Vieri~Giuliano Santucci, Gianluca Baldassarre, and Marco Mirolli.
\newblock Which is the best intrinsic motivation signal for learning multiple
  skills?
\newblock \emph{Frontiers in neurorobotics}, 7:\penalty0 22, 2013.

\bibitem[Schaul et~al.(2015)Schaul, Horgan, Gregor, and
  Silver]{Schaul2015UniversalVF}
T.~Schaul, Daniel Horgan, K.~Gregor, and D.~Silver.
\newblock Universal value function approximators.
\newblock In \emph{ICML}, 2015.

\bibitem[Schembri et~al.(2007)Schembri, Mirolli, and
  Baldassarre]{schembri2007evolving}
Massimiliano Schembri, Marco Mirolli, and Gianluca Baldassarre.
\newblock Evolving internal reinforcers for an intrinsically motivated
  reinforcement-learning robot.
\newblock In \emph{2007 IEEE 6th International Conference on Development and
  Learning}, pages 282--287. IEEE, 2007.

\bibitem[{\c{S}}im{\c{s}}ek and Barto(2006)]{csimcsek2006intrinsic}
{\"O}zg{\"u}r {\c{S}}im{\c{s}}ek and Andrew~G Barto.
\newblock An intrinsic reward mechanism for efficient exploration.
\newblock In \emph{Proceedings of the 23rd international conference on Machine
  learning}, pages 833--840, 2006.

\bibitem[Singh et~al.(2005)Singh, Barto, and Chentanez]{singh2005intrinsically}
Satinder Singh, Andrew~G Barto, and Nuttapong Chentanez.
\newblock Intrinsically motivated reinforcement learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  1281--1288, 2005.

\bibitem[Singh et~al.(2010)Singh, Lewis, Barto, and
  Sorg]{singh_intrinsically_2010}
Satinder Singh, Richard~L. Lewis, Andrew~G. Barto, and Jonathan Sorg.
\newblock Intrinsically {Motivated} {Reinforcement} {Learning}: {An}
  {Evolutionary} {Perspective}.
\newblock \emph{IEEE Transactions on Autonomous Mental Development}, 2\penalty0
  (2):\penalty0 70--82, June 2010.
\newblock ISSN 1943-0612.
\newblock \doi{10.1109/TAMD.2010.2051031}.
\newblock Conference Name: IEEE Transactions on Autonomous Mental Development.

\bibitem[Smyth(1987)]{Smyth1987QuasiUR}
M.~Smyth.
\newblock Quasi uniformities: Reconciling domains with metric spaces.
\newblock In \emph{MFPS}, 1987.

\bibitem[Sorg et~al.(2010{\natexlab{a}})Sorg, Lewis, and Singh]{sorg2010reward}
Jonathan Sorg, Richard~L Lewis, and Satinder~P Singh.
\newblock Reward design via online gradient ascent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2190--2198, 2010{\natexlab{a}}.

\bibitem[Sorg et~al.(2010{\natexlab{b}})Sorg, Singh, and
  Lewis]{sorg2010internal}
Jonathan Sorg, Satinder~P Singh, and Richard~L Lewis.
\newblock Internal rewards mitigate agent boundedness.
\newblock In \emph{Proceedings of the 27th international conference on machine
  learning (ICML-10)}, pages 1007--1014, 2010{\natexlab{b}}.

\bibitem[Stanczuk et~al.(2021)Stanczuk, Etmann, Kreusser, and
  Sch{\"o}nlieb]{Stanczuk2021WassersteinGW}
Jan Stanczuk, Christian Etmann, L.~Kreusser, and C.~Sch{\"o}nlieb.
\newblock Wasserstein gans work because they fail (to approximate the
  wasserstein distance).
\newblock \emph{ArXiv}, abs/2103.01678, 2021.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pages 5026--5033. IEEE, 2012.

\bibitem[Torabi et~al.(2019)Torabi, Warnell, and Stone]{torabi_generative_2019}
Faraz Torabi, Garrett Warnell, and Peter Stone.
\newblock Generative {Adversarial} {Imitation} from {Observation}.
\newblock \emph{arXiv:1807.06158 [cs, stat]}, June 2019.
\newblock URL \url{http://arxiv.org/abs/1807.06158}.
\newblock arXiv: 1807.06158.

\bibitem[Venkattaramanujam et~al.(2019)Venkattaramanujam, Crawford, Doan, and
  Precup]{Venkattaramanujam2019SelfsupervisedLO}
Srinivas Venkattaramanujam, E.~Crawford, T.~Doan, and Doina Precup.
\newblock Self-supervised learning of distance functions for goal-conditioned
  reinforcement learning.
\newblock \emph{ArXiv}, abs/1907.02998, 2019.

\bibitem[Villani(2008)]{villani2008optimal}
C{\'e}dric Villani.
\newblock \emph{Optimal transport: old and new}, volume 338.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Xiao et~al.(2019)Xiao, Herman, Wagner, Ziesche, Etesami, and
  Linh]{xiao_wasserstein_2019}
Huang Xiao, Michael Herman, Joerg Wagner, Sebastian Ziesche, Jalal Etesami, and
  Thai~Hong Linh.
\newblock Wasserstein {Adversarial} {Imitation} {Learning}.
\newblock \emph{arXiv:1906.08113 [cs, stat]}, June 2019.
\newblock URL \url{http://arxiv.org/abs/1906.08113}.
\newblock arXiv: 1906.08113.

\bibitem[Zhang et~al.(2020)Zhang, Abbeel, and Pinto]{zhang_automatic_2020}
Yunzhi Zhang, Pieter Abbeel, and Lerrel Pinto.
\newblock Automatic {Curriculum} {Learning} through {Value} {Disagreement}.
\newblock \emph{arXiv:2006.09641 [cs, stat]}, June 2020.
\newblock URL \url{http://arxiv.org/abs/2006.09641}.
\newblock arXiv: 2006.09641.

\bibitem[Zheng et~al.(2018)Zheng, Oh, and Singh]{zheng_learning_2018}
Zeyu Zheng, Junhyuk Oh, and Satinder Singh.
\newblock On {Learning} {Intrinsic} {Rewards} for {Policy} {Gradient}
  {Methods}.
\newblock \emph{arXiv:1804.06459 [cs, stat]}, June 2018.
\newblock URL \url{http://arxiv.org/abs/1804.06459}.
\newblock arXiv: 1804.06459.

\bibitem[Zheng et~al.(2020)Zheng, Oh, Hessel, Xu, Kroiss, van Hasselt, Silver,
  and Singh]{zheng_what_2020}
Zeyu Zheng, Junhyuk Oh, Matteo Hessel, Zhongwen Xu, Manuel Kroiss, Hado van
  Hasselt, David Silver, and Satinder Singh.
\newblock What {Can} {Learned} {Intrinsic} {Rewards} {Capture}?
\newblock \emph{arXiv:1912.05500 [cs]}, July 2020.
\newblock URL \url{http://arxiv.org/abs/1912.05500}.
\newblock arXiv: 1912.05500.

\end{thebibliography}
