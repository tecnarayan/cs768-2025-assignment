\begin{thebibliography}{10}

\bibitem{he2016deep}
He, K., X.~Zhang, S.~Ren, et~al.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778. 2016.

\bibitem{liu2018darts}
Liu, H., K.~Simonyan, Y.~Yang.
\newblock Darts: Differentiable architecture search.
\newblock \emph{arXiv preprint arXiv:1806.09055}, 2018.

\bibitem{he2020milenas}
He, C., H.~Ye, L.~Shen, et~al.
\newblock Milenas: Efficient neural architecture search via mixed-level
  reformulation.
\newblock \emph{arXiv preprint arXiv:2003.12238}, 2020.

\bibitem{tan2019efficientnet}
Tan, M., Q.~V. Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock \emph{arXiv preprint arXiv:1905.11946}, 2019.

\bibitem{kairouz2019advances}
Kairouz, P., H.~B. McMahan, B.~Avent, et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{arXiv preprint arXiv:1912.04977}, 2019.

\bibitem{mcmahan2016communication}
McMahan, H.~B., E.~Moore, D.~Ramage, et~al.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock \emph{arXiv preprint arXiv:1602.05629}, 2016.

\bibitem{he2019central}
He, C., C.~Tan, H.~Tang, et~al.
\newblock Central server free federated learning over single-sided trust social
  networks.
\newblock \emph{arXiv preprint arXiv:1910.04956}, 2019.

\bibitem{wang2020federated}
Wang, H., M.~Yurochkin, Y.~Sun, et~al.
\newblock Federated learning with matched averaging.
\newblock \emph{arXiv preprint arXiv:2002.06440}, 2020.

\bibitem{bonawitz2019towards}
Bonawitz, K., H.~Eichner, W.~Grieskamp, et~al.
\newblock Towards federated learning at scale: System design.
\newblock \emph{arXiv preprint arXiv:1902.01046}, 2019.

\bibitem{yu2019parallel}
Yu, H., S.~Yang, S.~Zhu.
\newblock Parallel restarted sgd with faster convergence and less
  communication: Demystifying why model averaging works for deep learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, vol.~33, pages 5693--5700. 2019.

\bibitem{gupta2018distributed}
Gupta, O., R.~Raskar.
\newblock Distributed learning of deep neural network over multiple agents.
\newblock \emph{Journal of Network and Computer Applications}, 116:1--8, 2018.

\bibitem{vepakomma2018split}
Vepakomma, P., O.~Gupta, T.~Swedish, et~al.
\newblock Split learning for health: Distributed deep learning without sharing
  raw patient data.
\newblock \emph{arXiv preprint arXiv:1812.00564}, 2018.

\bibitem{ortega1970iterative}
Ortega, J.~M., W.~C. Rheinboldt.
\newblock \emph{Iterative solution of nonlinear equations in several
  variables}, vol.~30.
\newblock Siam, 1970.

\bibitem{bertsekas1989parallel}
Bertsekas, D.~P., J.~N. Tsitsiklis.
\newblock \emph{Parallel and distributed computation: numerical methods},
  vol.~23.
\newblock Prentice hall Englewood Cliffs, NJ, 1989.

\bibitem{bolte2014proximal}
Bolte, J., S.~Sabach, M.~Teboulle.
\newblock Proximal alternating linearized minimization for nonconvex and
  nonsmooth problems.
\newblock \emph{Mathematical Programming}, 146(1-2):459--494, 2014.

\bibitem{attouch2010proximal}
Attouch, H., J.~Bolte, P.~Redont, et~al.
\newblock Proximal alternating minimization and projection methods for
  nonconvex problems: An approach based on the kurdyka-{\l}ojasiewicz
  inequality.
\newblock \emph{Mathematics of Operations Research}, 35(2):438--457, 2010.

\bibitem{wright2015coordinate}
Wright, S.~J.
\newblock Coordinate descent algorithms.
\newblock \emph{Mathematical Programming}, 151(1):3--34, 2015.

\bibitem{razaviyayn2013unified}
Razaviyayn, M., M.~Hong, Z.-Q. Luo.
\newblock A unified convergence analysis of block successive minimization
  methods for nonsmooth optimization.
\newblock \emph{SIAM Journal on Optimization}, 23(2):1126--1153, 2013.

\bibitem{hinton2015distilling}
Hinton, G., O.~Vinyals, J.~Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{bucilu2006model}
Bucilu«é, C., R.~Caruana, A.~Niculescu-Mizil.
\newblock Model compression.
\newblock In \emph{Proceedings of the 12th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 535--541. 2006.

\bibitem{ba2014deep}
Ba, J., R.~Caruana.
\newblock Do deep nets really need to be deep?
\newblock In \emph{Advances in neural information processing systems}, pages
  2654--2662. 2014.

\bibitem{romero2014fitnets}
Romero, A., N.~Ballas, S.~E. Kahou, et~al.
\newblock Fitnets: Hints for thin deep nets.
\newblock \emph{arXiv preprint arXiv:1412.6550}, 2014.

\bibitem{chaoyanghe2020fedml}
He, C., S.~Li, J.~So, et~al.
\newblock Fedml: A research library and benchmark for federated machine
  learning.
\newblock \emph{arXiv preprint arXiv:2007.13518}, 2020.

\bibitem{krizhevsky2009learning}
Krizhevsky, A., G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{darlow2018cinic}
Darlow, L.~N., E.~J. Crowley, A.~Antoniou, et~al.
\newblock Cinic-10 is not imagenet or cifar-10.
\newblock \emph{arXiv preprint arXiv:1810.03505}, 2018.

\bibitem{reddi2020adaptive}
Reddi, S., Z.~Charles, M.~Zaheer, et~al.
\newblock Adaptive federated optimization.
\newblock \emph{arXiv preprint arXiv:2003.00295}, 2020.

\bibitem{he2020fednas}
He, C., M.~Annavaram, S.~Avestimehr.
\newblock Fednas: Federated deep learning via neural architecture search.
\newblock \emph{arXiv preprint arXiv:2004.08546}, 2020.

\bibitem{hsieh2019non}
Hsieh, K., A.~Phanishayee, O.~Mutlu, et~al.
\newblock The non-iid data quagmire of decentralized machine learning.
\newblock \emph{arXiv preprint arXiv:1910.00189}, 2019.

\bibitem{bernstein2018signsgd}
Bernstein, J., Y.-X. Wang, K.~Azizzadenesheli, et~al.
\newblock signsgd: Compressed optimisation for non-convex problems.
\newblock \emph{arXiv preprint arXiv:1802.04434}, 2018.

\bibitem{wangni2018gradient}
Wangni, J., J.~Wang, J.~Liu, et~al.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1299--1309. 2018.

\bibitem{tang2018communication}
Tang, H., S.~Gan, C.~Zhang, et~al.
\newblock Communication compression for decentralized training.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7652--7662. 2018.

\bibitem{alistarh2017qsgd}
Alistarh, D., D.~Grubic, J.~Li, et~al.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1709--1720. 2017.

\bibitem{lin2017deep}
Lin, Y., S.~Han, H.~Mao, et~al.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock \emph{arXiv preprint arXiv:1712.01887}, 2017.

\bibitem{so2020turboaggregate}
So, J., B.~Guler, A.~S. Avestimehr.
\newblock Turbo-aggregate: Breaking the quadratic aggregation barrier in secure
  federated learning.
\newblock \emph{arXiv preprint arXiv:2002.04156}, 2020.

\bibitem{wang2018atomo}
Wang, H., S.~Sievert, S.~Liu, et~al.
\newblock Atomo: Communication-efficient learning via atomic sparsification.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9850--9861. 2018.

\bibitem{so2020byzantineresilient}
So, J., B.~Guler, A.~S. Avestimehr.
\newblock Byzantine-resilient secure federated learning.
\newblock \emph{arXiv preprint arXiv:2007.11115}, 2020.

\bibitem{elkordy2020secure}
Elkordy, A.~R., A.~S. Avestimehr.
\newblock Secure aggregation with heterogeneous quantization in federated
  learning.
\newblock \emph{arXiv preprint arXiv:2009.14388}, 2020.

\bibitem{lin2020ensemble}
Lin, T., L.~Kong, S.~U. Stich, et~al.
\newblock Ensemble distillation for robust model fusion in federated learning.
\newblock \emph{arXiv preprint arXiv:2006.07242}, 2020.

\bibitem{bistritz2020}
Bistritz, I., A.~Mann, N.~Bambos.
\newblock Distributed distillation for on-device learning.
\newblock In \emph{Advances in Neural Information Processing Systems 33}. 2020.

\bibitem{lee2020asynchronous}
Lee, S.-h., K.~Yoo, N.~Kwak.
\newblock Asynchronous edge learning using cloned knowledge distillation.
\newblock \emph{arXiv preprint arXiv:2010.10338}, 2020.

\bibitem{zhou2020distilled}
Zhou, Y., G.~Pu, X.~Ma, et~al.
\newblock Distilled one-shot federated learning.
\newblock \emph{arXiv preprint arXiv:2009.07999}, 2020.

\bibitem{sun2020federated}
Sun, L., L.~Lyu.
\newblock Federated model distillation with noise-free differential privacy.
\newblock \emph{arXiv preprint arXiv:2009.05537}, 2020.

\bibitem{ma2020adaptive}
Ma, J., R.~Yonetani, Z.~Iqbal.
\newblock Adaptive distillation for decentralized learning from heterogeneous
  clients.
\newblock \emph{arXiv preprint arXiv:2008.07948}, 2020.

\bibitem{itahara2020distillation}
Itahara, S., T.~Nishio, Y.~Koda, et~al.
\newblock Distillation-based semi-supervised federated learning for
  communication-efficient collaborative training with non-iid private data.
\newblock \emph{arXiv preprint arXiv:2008.06180}, 2020.

\bibitem{li2020model}
Li, Q., B.~He, D.~Song.
\newblock Model-agnostic round-optimal federated learning via knowledge
  transfer.
\newblock \emph{arXiv preprint arXiv:2010.01017}, 2020.

\bibitem{zhang2018deep}
Zhang, Y., T.~Xiang, T.~M. Hospedales, et~al.
\newblock Deep mutual learning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4320--4328. 2018.

\bibitem{anil2018large}
Anil, R., G.~Pereyra, A.~Passos, et~al.
\newblock Large scale distributed neural network training through online
  distillation.
\newblock \emph{arXiv preprint arXiv:1804.03235}, 2018.

\bibitem{song2018collaborative}
Song, G., W.~Chai.
\newblock Collaborative learning for deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1832--1841. 2018.

\bibitem{jeong2018communication}
Jeong, E., S.~Oh, H.~Kim, et~al.
\newblock Communication-efficient on-device machine learning: Federated
  distillation and augmentation under non-iid private data.
\newblock \emph{arXiv preprint arXiv:1811.11479}, 2018.

\bibitem{chen2019online}
Chen, D., J.-P. Mei, C.~Wang, et~al.
\newblock Online knowledge distillation with diverse peers.
\newblock \emph{arXiv preprint arXiv:1912.00350}, 2019.

\bibitem{park2019distilling}
Park, J., S.~Wang, A.~Elgabli, et~al.
\newblock Distilling on-device intelligence at the network edge.
\newblock \emph{arXiv preprint arXiv:1908.05895}, 2019.

\bibitem{tran2020hydra}
Tran, L., B.~S. Veeling, K.~Roth, et~al.
\newblock Hydra: Preserving ensemble diversity for model distillation.
\newblock \emph{arXiv preprint arXiv:2001.04694}, 2020.

\bibitem{zhu2018knowledge}
Zhu, X., S.~Gong, et~al.
\newblock Knowledge distillation by on-the-fly native ensemble.
\newblock In \emph{Advances in neural information processing systems}, pages
  7517--7527. 2018.

\bibitem{vongkulbhisal2019unifying}
Vongkulbhisal, J., P.~Vinayavekhin, M.~Visentini-Scarzanella.
\newblock Unifying heterogeneous classifiers with distillation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3175--3184. 2019.

\bibitem{han2015deep}
Han, S., H.~Mao, W.~J. Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015.

\bibitem{he2018amc}
He, Y., J.~Lin, Z.~Liu, et~al.
\newblock Amc: Automl for model compression and acceleration on mobile devices.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 784--800. 2018.

\bibitem{yang2018netadapt}
Yang, T.-J., A.~Howard, B.~Chen, et~al.
\newblock Netadapt: Platform-aware neural network adaptation for mobile
  applications.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 285--300. 2018.

\bibitem{howard2017mobilenets}
Howard, A.~G., M.~Zhu, B.~Chen, et~al.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{zhang2018shufflenet}
Zhang, X., X.~Zhou, M.~Lin, et~al.
\newblock Shufflenet: An extremely efficient convolutional neural network for
  mobile devices.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 6848--6856. 2018.

\bibitem{iandola2016squeezenet}
Iandola, F.~N., S.~Han, M.~W. Moskewicz, et~al.
\newblock Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5
  mb model size.
\newblock \emph{arXiv preprint arXiv:1602.07360}, 2016.

\bibitem{wu2019fbnet}
Wu, B., X.~Dai, P.~Zhang, et~al.
\newblock Fbnet: Hardware-aware efficient convnet design via differentiable
  neural architecture search.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 10734--10742. 2019.

\bibitem{li2018federated}
Li, T., A.~K. Sahu, M.~Zaheer, et~al.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{arXiv preprint arXiv:1812.06127}, 2018.

\bibitem{wang2020tackling}
Wang, J., Q.~Liu, H.~Liang, et~al.
\newblock Tackling the objective inconsistency problem in heterogeneous
  federated optimization.
\newblock \emph{arXiv preprint arXiv:2007.07481}, 2020.

\bibitem{qian1999momentum}
Qian, N.
\newblock On the momentum term in gradient descent learning algorithms.
\newblock \emph{Neural networks}, 12(1):145--151, 1999.

\bibitem{kingma2014adam}
Kingma, D.~P., J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{zou2019sufficient}
Zou, F., L.~Shen, Z.~Jie, et~al.
\newblock A sufficient condition for convergences of adam and rmsprop.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 11127--11135. 2019.

\bibitem{caldas2018leaf}
Caldas, S., P.~Wu, T.~Li, et~al.
\newblock Leaf: A benchmark for federated settings.
\newblock \emph{arXiv preprint arXiv:1812.01097}, 2018.

\bibitem{li2019exponential}
Li, Z., S.~Arora.
\newblock An exponential learning rate schedule for deep learning.
\newblock \emph{arXiv preprint arXiv:1910.07454}, 2019.

\bibitem{shi2020learning}
Shi, B., W.~J. Su, M.~I. Jordan.
\newblock On learning rates and schr$\backslash$" odinger operators.
\newblock \emph{arXiv preprint arXiv:2004.06977}, 2020.

\bibitem{AI_and_Compute}
OpenAI.
\newblock Ai and compute, 2018.
\newblock Https://openai.com/blog/ai-and-compute.

\bibitem{hernandez2020measuring}
Hernandez, D., T.~B. Brown.
\newblock Measuring the algorithmic efficiency of neural networks.
\newblock \emph{arXiv preprint arXiv:2005.04305}, 2020.

\bibitem{wang2020attack}
Wang, H., K.~Sreenivasan, S.~Rajput, et~al.
\newblock Attack of the tails: Yes, you really can backdoor federated learning.
\newblock \emph{arXiv preprint arXiv:2007.05084}, 2020.

\end{thebibliography}
