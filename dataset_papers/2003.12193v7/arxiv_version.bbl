\begin{thebibliography}{10}

\bibitem{conf/nips/AroraDH0SW19}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In {\em Advances in Neural Information Processing Systems 32}. Curran
  Associates, Inc., 2019.

\bibitem{Bai2020BeyondLO}
Yu~Bai and Jason~D. Lee.
\newblock Beyond linearization: On quadratic and higher-order approximation of
  wide neural networks.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{Bai2020TaylorizedTT}
Yunru Bai, Ben Krause, Haiquan Wang, Caiming Xiong, and Richard Socher.
\newblock Taylorized training: Towards better approximation of neural network
  training at finite width.
\newblock {\em ArXiv}, 2020.

\bibitem{brock2018smash}
Andrew Brock, Theo Lim, J.M. Ritchie, and Nick Weston.
\newblock {SMASH}: One-shot model architecture search through hypernetworks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{chang2020principled}
Oscar Chang, Lampros Flokas, and Hod Lipson.
\newblock Principled weight initialization for hypernetworks.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{Dyer2020Asymptotics}
Ethan Dyer and Guy Gur-Ari.
\newblock Asymptotics of wide networks from feynman diagrams.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{galanti2020modularity}
Tomer Galanti and Lior Wolf.
\newblock On the modularity of hypernetworks.
\newblock In {\em Advances in Neural Information Processing Systems 33}. Curran
  Associates, Inc., 2020.

\bibitem{Ha2017HyperNetworks}
David Ha, Andrew~M. Dai, and Quoc~V. Le.
\newblock Hypernetworks.
\newblock In {\em International Conference on Learning Representations}, 2016.

\bibitem{Hanin2020Finite}
Boris Hanin and Mihai Nica.
\newblock Finite depth and width corrections to the neural tangent kernel.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{He2016DeepRL}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2016.

\bibitem{jacotNTK}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in Neural Information Processing Systems 31}. Curran
  Associates, Inc., 2018.

\bibitem{jayakumar2020multiplicative}
Siddhant~M. Jayakumar, Jacob Menick, Wojciech~M. Czarnecki, Jonathan Schwarz,
  Jack Rae, Simon Osindero, Yee~Whye Teh, Tim Harley, and Razvan Pascanu.
\newblock Multiplicative interactions and where to find them.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{jia2016dynamic}
Xu~Jia, Bert De~Brabandere, Tinne Tuytelaars, and Luc~V Gool.
\newblock Dynamic filter networks.
\newblock In {\em Advances in Neural Information Processing Systems 29}. Curran
  Associates, Inc., 2016.

\bibitem{karras2018progressive}
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
\newblock Progressive growing of {GAN}s for improved quality, stability, and
  variation.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{klein2015dynamic}
Benjamin Klein, Lior Wolf, and Yehuda Afek.
\newblock A dynamic convolutional layer for short range weather prediction.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2015.

\bibitem{Klocek_2019}
Sylwester Klocek, Lukasz Maziarka, Maciej Wolczyk, Jacek Tabor, Jakub Nowak,
  and Marek Śmieja.
\newblock Hypernetwork functional image representation.
\newblock {\em Lecture Notes in Computer Science}, 2019.

\bibitem{Krizhevsky10convolutionaldeep}
Alex Krizhevsky.
\newblock Convolutional deep belief networks on cifar-10, 2010.

\bibitem{lecun-mnisthandwrittendigit-2010}
Yann LeCun and Corinna Cortes.
\newblock {MNIST} handwritten digit database.
\newblock 2010.

\bibitem{lee2018deep}
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel~S. Schoenholz, Jeffrey
  Pennington, and Jascha Sohl-Dickstein.
\newblock Deep neural networks as gaussian processes.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{NIPS2019_9063}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In {\em Advances in Neural Information Processing Systems 32}. Curran
  Associates, Inc., 2019.

\bibitem{littwin2020random}
Etai Littwin, Tomer Galanti, and Lior Wolf.
\newblock On random kernels of residual architectures.
\newblock {\em Arxiv}, 2020.

\bibitem{Littwin_2019_ICCV}
Gidi Littwin and Lior Wolf.
\newblock Deep meta functionals for shape representation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, 2019.

\bibitem{lorraine2018stochastic}
Jonathan Lorraine and David Duvenaud.
\newblock Stochastic hyperparameter optimization through hypernetworks, 2018.

\bibitem{mann1943}
Henry~B. Mann and Abraham Wald.
\newblock On stochastic limit and order relationships.
\newblock {\em Annals of Mathematical Statistics}, 14(3):217--226, 09 1943.

\bibitem{NIPS2019_8504}
Eliya Nachmani and Lior Wolf.
\newblock Hyper-graph-network decoders for block codes.
\newblock In {\em Advances in Neural Information Processing Systems 32}. Curran
  Associates, Inc., 2019.

\bibitem{Novak2018BayesianCN}
Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel~A.
  Abolafia, Jeffrey Pennington, and Jascha Sohl-dickstein.
\newblock Bayesian deep convolutional networks with many channels are gaussian
  processes.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{48223}
Daniel Park, Jascha Sohl-Dickstein, Quoc Le, and Samuel Smith.
\newblock The effect of network width on stochastic gradient descent and
  generalization: an empirical study.
\newblock In {\em Proceedings of Machine Learning Research}, volume~97, pages
  5042--5051. PMLR, 2019.

\bibitem{NIPS2007_3182}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In {\em Advances in Neural Information Processing Systems 20}. Curran
  Associates, Inc., 2008.

\bibitem{7410424}
G.~{Riegler}, S.~{Schulter}, M.~{Rüther}, and H.~{Bischof}.
\newblock Conditioned regression models for non-blind single image
  super-resolution.
\newblock In {\em IEEE International Conference on Computer Vision (ICCV)},
  pages 522--530, 2015.

\bibitem{9054042}
M.~{Rotman} and L.~{Wolf}.
\newblock Electric analog circuit design with hypernetworks and a differential
  simulator.
\newblock In {\em IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}, 2020.

\bibitem{Rudner2018OnTC}
Tim G.~J. Rudner.
\newblock On the connection between neural processes and gaussian processes
  with deep kernels.
\newblock In {\em Workshop on Bayesian Deep Learning (NeurIPS)}, 2018.

\bibitem{Schoenholz2016DeepIP}
Samuel~S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein.
\newblock Deep information propagation.
\newblock {\em ArXiv}, 2016.

\bibitem{sitzmann2020implicit}
Vincent Sitzmann, Julien N.~P. Martel, Alexander~W. Bergman, David~B. Lindell,
  and Gordon Wetzstein.
\newblock Implicit neural representations with periodic activation functions.
\newblock {\em Arxiv}, 2020.

\bibitem{tancik2020Fourier}
Matthew Tancik, Pratul~P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil,
  Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan~T. Barron, and
  Ren Ng.
\newblock Fourier features let networks learn high frequency functions in low
  dimensional domains.
\newblock {\em Arxiv}, 2020.

\bibitem{DBLP:journals/corr/Laarhoven17b}
Twan van Laarhoven.
\newblock L2 regularization versus batch and weight normalization.
\newblock {\em Arxiv}, 2017.

\bibitem{Oswald2020Continual}
Johannes von Oswald, Christian Henning, João Sacramento, and Benjamin~F.
  Grewe.
\newblock Continual learning with hypernetworks.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{Wei2018OnTM}
Colin Wei, Jason~D. Lee, Qiang Liu, and Tengyu Ma.
\newblock On the margin theory of feedforward neural networks.
\newblock {\em ArXiv}, 2018.

\bibitem{Woodworth2020KernelAR}
Blake Woodworth, Suriya Gunasekar, Jason~D. Lee, Edward Moroshko, Pedro
  Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In {\em Proceedings of Machine Learning Research}, volume 125, pages
  3635--3673. PMLR, 2020.

\bibitem{wu2018pay}
Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli.
\newblock Pay less attention with lightweight and dynamic convolutions.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{Yang2019ScalingLO}
Greg Yang.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian
  process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock {\em ArXiv}, 2019.

\bibitem{Yang2019TensorPI}
Greg Yang.
\newblock Tensor programs {I}: Wide feedforward or recurrent neural networks of
  any architecture are gaussian processes.
\newblock In {\em Advances in Neural Information Processing Systems 32}. Curran
  Associates, Inc., 2019.

\bibitem{NIPS2017_6879}
Greg Yang and Samuel Schoenholz.
\newblock Mean field residual networks: On the edge of chaos.
\newblock In {\em Advances in Neural Information Processing Systems 30}. Curran
  Associates, Inc., 2017.

\bibitem{li2019enhanced}
Dingli Yu, Ruosong Wang, Zhiyuan Li, Wei Hu, Ruslan Salakhutdinov, Sanjeev
  Arora, and Simon~S. Du.
\newblock Enhanced convolutional neural tangent kernels, 2020.

\bibitem{zhang2018graph}
Chris Zhang, Mengye Ren, and Raquel Urtasun.
\newblock Graph hypernetworks for neural architecture search.
\newblock In {\em International Conference on Learning Representations}, 2019.

\end{thebibliography}
