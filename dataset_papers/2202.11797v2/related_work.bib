@article{goodman2017european,
  title={European Union regulations on algorithmic decision-making and a “right to explanation”},
  author={Goodman, Bryce and Flaxman, Seth},
  journal={AI magazine},
  volume={38},
  number={3},
  pages={50--57},
  year={2017}
}
% What is Interpretability
@article{lipton2018mythos,
  title={The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery.},
  author={Lipton, Zachary C},
  journal={Queue},
  volume={16},
  number={3},
  pages={31--57},
  year={2018},
  publisher={ACM New York, NY, USA}
}
% Good overview over XAI
@article{adadi2018peeking,
  title={Peeking inside the black-box: a survey on explainable artificial intelligence (XAI)},
  author={Adadi, Amina and Berrada, Mohammed},
  journal={IEEE access},
  volume={6},
  pages={52138--52160},
  year={2018},
  publisher={IEEE}
}
% XAI for medicine
@article{holzinger2017we,
  title={What do we need to build explainable AI systems for the medical domain?},
  author={Holzinger, Andreas and Biemann, Chris and Pattichis, Constantinos S and Kell, Douglas B},
  journal={arXiv preprint arXiv:1712.09923},
  year={2017}
}
% XAI for criminal justice
@article{rudin2018optimized,
  title={Optimized scoring systems: Toward trust in machine learning for healthcare and criminal justice},
  author={Rudin, Cynthia and Ustun, Berk},
  journal={Interfaces},
  volume={48},
  number={5},
  pages={449--466},
  year={2018},
  publisher={INFORMS}
}
% For autonomous driving
@inproceedings{schraagen2020trusting,
  title={Trusting the X in XAI: Effects of different types of explanations by a self-driving car on trust, explanation satisfaction and mental models},
  author={Schraagen, Jan Maarten and Elsasser, Pia and Fricke, Hanna and Hof, Marleen and Ragalmuto, Fabyen},
  booktitle={Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  volume={64},
  number={1},
  pages={339--343},
  year={2020},
  organization={SAGE Publications Sage CA: Los Angeles, CA}
}


%% XAI models in general

% Clever Hans success
@article{lapuschkin2019unmasking,
  title={Unmasking Clever Hans predictors and assessing what machines really learn},
  author={Lapuschkin, Sebastian and W{\"a}ldchen, Stephan and Binder, Alexander and Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  journal={Nature communications},
  volume={10},
  number={1},
  pages={1--8},
  year={2019},
  publisher={Nature Publishing Group}
}
% SLIM: Sparse linear models
@article{ustun2016supersparse,
  title={Supersparse linear integer models for optimized medical scoring systems},
  author={Ustun, Berk and Rudin, Cynthia},
  journal={Machine Learning},
  volume={102},
  number={3},
  pages={349--391},
  year={2016},
  publisher={Springer}
}
% Inherently interpretable models, overview
@article{arrieta2020explainable,
  title={Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
  author={Arrieta, Alejandro Barredo and D{\'\i}az-Rodr{\'\i}guez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garc{\'\i}a, Salvador and Gil-L{\'o}pez, Sergio and Molina, Daniel and Benjamins, Richard and others},
  journal={Information Fusion},
  volume={58},
  pages={82--115},
  year={2020},
  publisher={Elsevier}
}
% Stop using Black-boxes
@article{rudin2019stop,
  title={Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
  author={Rudin, Cynthia},
  journal={Nature Machine Intelligence},
  volume={1},
  number={5},
  pages={206--215},
  year={2019},
  publisher={Nature Publishing Group}
}
@article{olah2018building,
  title={The building blocks of interpretability},
  author={Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  journal={Distill},
  volume={3},
  number={3},
  pages={e10},
  year={2018}
}




%%%%%% Evaluation Methods:

% Short Taxonomie of evaluation methods: Application grounded, human grounded, proxy grounded
@article{doshi2017towards,
  title={Towards a rigorous science of interpretable machine learning},
  author={Doshi-Velez, Finale and Kim, Been},
  journal={arXiv preprint arXiv:1702.08608},
  year={2017}
}
% Good overview over some quantitative Evals
@article{mohseni2018human,
  title={A human-grounded evaluation benchmark for local explanations of machine learning},
  author={Mohseni, Sina and Block, Jeremy E and Ragan, Eric D},
  journal={arXiv preprint arXiv:1801.05075},
  year={2018}
}
%% Sanity checks
@article{adebayo2018sanity,
  title={Sanity checks for saliency maps},
  author={Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  journal={arXiv preprint arXiv:1810.03292},
  year={2018}
}
%% Benchmarkign Explanations on Atari Games
@article{huber2021benchmarking,
  title={Benchmarking Perturbation-based Saliency Maps for Explaining Deep Reinforcement Learning Agents},
  author={Huber, Tobias and Limmer, Benedikt and Andr{\'e}, Elisabeth},
  journal={arXiv preprint arXiv:2101.07312},
  year={2021}
}
%% Comparing Hidden Info Humans vs method itself
@article{biessmann2021quality,
  title={Quality metrics for transparent machine learning with and without humans in the loop are not correlated},
  author={Biessmann, Felix and Refiano, Dionysius},
  journal={arXiv preprint arXiv:2107.02033},
  year={2021}
}

%Qunatitative Evaluation with bounding box annotations
@article{qin2021informative,
  title={Informative Class Activation Maps},
  author={Qin, Zhenyue and Kim, Dongwoo and Gedeon, Tom},
  journal={arXiv preprint arXiv:2106.10472},
  year={2021}
}
% Human-based evaluation of lasiency methods
@article{samuel2021evaluation,
  title={Evaluation of Saliency-based Explainability Method},
  author={Samuel, Sam Zabdiel Sunder and Kamakshi, Vidhya and Lodhi, Namrata and Krishnan, Narayanan C},
  journal={arXiv preprint arXiv:2106.12773},
  year={2021}
}
% Ours!!!
@inproceedings{macdonald2020explaining,
  title={Explaining neural network decisions is hard},
  author={Macdonald, Jan and W{\"a}ldchen, Stephan and Hauch, Sascha and Kutyniok, Gitta},
  booktitle={XXAI Workshop, 37th ICML},
  year={2020}
}
@article{macdonald2019rate,
  title={A rate-distortion framework for explaining neural network decisions},
  author={Macdonald, Jan and W{\"a}ldchen, Stephan and Hauch, Sascha and Kutyniok, Gitta},
  journal={arXiv preprint arXiv:1905.11092},
  year={2019}
}
@article{macdonald2021interpretable,
  title={Interpretable Neural Networks with Frank-Wolfe: Sparse Relevance Maps and Relevance Orderings},
  author={Macdonald, Jan and Besan{\c{c}}on, Mathieu and Pokutta, Sebastian},
  journal={arXiv preprint arXiv:2110.08105},
  year={2021}
}
@inproceedings{sundararajan2020many,
  title={The many Shapley values for model explanation},
  author={Sundararajan, Mukund and Najmi, Amir},
  booktitle={International Conference on Machine Learning},
  pages={9269--9278},
  year={2020},
  organization={PMLR}
}




%%%%% Manipulation
% For counterfactual models
@article{slack2021counterfactual,
  title={Counterfactual Explanations Can Be Manipulated},
  author={Slack, Dylan and Hilgard, Sophie and Lakkaraju, Himabindu and Singh, Sameer},
  journal={arXiv preprint arXiv:2106.02666},
  year={2021}
}
% For LIME and SHAP
@inproceedings{slack2020fooling,
  title={Fooling lime and shap: Adversarial attacks on post hoc explanation methods},
  author={Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu},
  booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  pages={180--186},
  year={2020}
}
@inproceedings{anders2020fairwashing,
  title={Fairwashing explanations with off-manifold detergent},
  author={Anders, Christopher and Pasliev, Plamen and Dombrowski, Ann-Kathrin and M{\"u}ller, Klaus-Robert and Kessel, Pan},
  booktitle={International Conference on Machine Learning},
  pages={314--323},
  year={2020},
  organization={PMLR}
}
% Grd, Lime, Shap, IG, GI
@inproceedings{dimanov2020you,
  title={You Shouldn't Trust Me: Learning Models Which Conceal Unfairness From Multiple Explanation Methods.},
  author={Dimanov, Botty and Bhatt, Umang and Jamnik, Mateja and Weller, Adrian},
  booktitle={SafeAI@ AAAI},
  year={2020}
}
%Grd, IG, LRP
@article{dombrowski2019explanations,
  title={Explanations can be manipulated and geometry is to blame},
  author={Dombrowski, Ann-Kathrin and Alber, Maximilian and Anders, Christopher J and Ackermann, Marcel and M{\"u}ller, Klaus-Robert and Kessel, Pan},
  journal={arXiv preprint arXiv:1906.07983},
  year={2019}
}
% LRP, Grad-Cam
@article{heo2019fooling,
  title={Fooling neural network interpretations via adversarial model manipulation},
  author={Heo, Juyeon and Joo, Sunghwan and Moon, Taesup},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={2925--2936},
  year={2019}
}



%%%%%%% Obfuscation Evaluation
@inproceedings{fong2017interpretable,
  title={Interpretable explanations of black boxes by meaningful perturbation},
  author={Fong, Ruth C and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={3429--3437},
  year={2017}
}
% Pixel flipping
@article {WojBGM2017pixelflip,
    author={Samek, Wojciech and Binder, Alexander and Montavon, Grégoire and Lapuschkin, Sebastian and Müller, Klaus-Robert}, 
    journal={IEEE Transactions on Neural Networks and Learning Systems}, 
    title={Evaluating the Visualization of What a Deep Neural Network Has Learned}, 
    year={2017}, 
    month={11},
    volume={28}, 
    number={11}, 
    pages={2660--2673}, 
    doi={10.1109/TNNLS.2016.2599820}, 
    issn={2162-237X}, 
}



%%%%% Other XAI_methods

%Gradient/saliency
@article{simonyan2013deep,
  title={Deep inside convolutional networks: Visualising image classification models and saliency maps},
  author={Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1312.6034},
  year={2013}
}
%SmoothGrad
@article{smilkov2017smoothgrad,
  title={Smoothgrad: removing noise by adding noise},
  author={Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  journal={arXiv preprint arXiv:1706.03825},
  year={2017}
}
%LIme
@inproceedings{ribeiro2016should,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}
%LRP
@article{bach-plos15,
    author = {Bach, Sebastian and Binder, Alexander and Montavon, Grégoire and Klauschen, Frederick and Müller, Klaus-Robert and Samek, Wojciech},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
    year = {2015},
    month = {07},
    volume = {10},
    pages = {1--46},
    number = {7},
    doi = {10.1371/journal.pone.0130140},
    }
% DeepLift
@inproceedings{shrikumar2017learning,
  title={Learning important features through propagating activation differences},
  author={Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  booktitle={International Conference on Machine Learning},
  pages={3145--3153},
  year={2017},
  organization={PMLR}
}
%SHAP
@incollection{NIPS2017_7062shap,
  title = {A Unified Approach to Interpreting Model Predictions},
  author = {Lundberg, Scott M. and Lee, Su-In},
  booktitle = {Advances in Neural Information Processing Systems 30},
  editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages = {4765--4774},
  year = {2017},
  publisher = {Curran Associates, Inc.},
}
@inproceedings{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  booktitle={Proceedings of the 31st international conference on neural information processing systems},
  pages={4768--4777},
  year={2017}
}
%Anchors
@inproceedings{ribeiro2018anchors,
  title={Anchors: High-precision model-agnostic explanations},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}




%%%%%% Applications

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}
