@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2020}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}


@article{brazdil_1,
  author    = {Pavel Brazdil and
               Christophe G. Giraud{-}Carrier},
  title     = {Metalearning and Algorithm Selection: progress, state of the art and
               introduction to the 2018 Special Issue},
  journal   = {Mach. Learn.},
  volume    = {107},
  number    = {1},
  pages     = {1--14},
  year      = {2018},
  url       = {https://doi.org/10.1007/s10994-017-5692-y},
  doi       = {10.1007/s10994-017-5692-y},
  timestamp = {Mon, 02 Mar 2020 16:28:58 +0100},
  biburl    = {https://dblp.org/rec/journals/ml/BrazdilG18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{brazdil_2,
  author    = {Pavel Brazdil and
               Ricardo Vilalta and
               Christophe G. Giraud{-}Carrier and
               Carlos Soares},
  editor    = {Claude Sammut and
               Geoffrey I. Webb},
  title     = {Metalearning},
  booktitle = {Encyclopedia of Machine Learning and Data Mining},
  pages     = {818--823},
  publisher = {Springer},
  year      = {2017},
  url       = {https://doi.org/10.1007/978-1-4899-7687-1\_543},
  doi       = {10.1007/978-1-4899-7687-1\_543},
  timestamp = {Fri, 02 Nov 2018 09:33:41 +0100},
  biburl    = {https://dblp.org/rec/reference/ml/BrazdilVGS17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{brazdil_3,
  author    = {Ricardo Vilalta and
               Christophe G. Giraud{-}Carrier and
               Pavel Brazdil},
  editor    = {Oded Maimon and
               Lior Rokach},
  title     = {Meta-Learning - Concepts and Techniques},
  booktitle = {Data Mining and Knowledge Discovery Handbook, 2nd ed},
  pages     = {717--731},
  publisher = {Springer},
  year      = {2010},
  url       = {https://doi.org/10.1007/978-0-387-09823-4\_36},
  doi       = {10.1007/978-0-387-09823-4\_36},
  timestamp = {Tue, 25 Feb 2020 11:33:46 +0100},
  biburl    = {https://dblp.org/rec/reference/dmkdh/VilaltaGB10.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{brazdil_4,
  author    = {Pavel Brazdil and
               Christophe G. Giraud{-}Carrier and
               Carlos Soares and
               Ricardo Vilalta},
  title     = {Metalearning - Applications to Data Mining},
  series    = {Cognitive Technologies},
  publisher = {Springer},
  year      = {2009},
  url       = {https://doi.org/10.1007/978-3-540-73263-1},
  doi       = {10.1007/978-3-540-73263-1},
  isbn      = {978-3-540-73262-4},
  timestamp = {Fri, 02 Nov 2018 09:27:04 +0100},
  biburl    = {https://dblp.org/rec/books/daglib/0022052.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{nature,
  author    = {Yann LeCun and
               Yoshua Bengio and
               Geoffrey E. Hinton},
  title     = {Deep learning},
  journal   = {Nature},
  volume    = {521},
  number    = {7553},
  pages     = {436--444},
  year      = {2015},
  url       = {https://doi.org/10.1038/nature14539},
  doi       = {10.1038/nature14539},
  timestamp = {Wed, 14 Nov 2018 10:30:42 +0100},
  biburl    = {https://dblp.org/rec/journals/nature/LeCunBH15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sejnowski,
  author    = {Terrence J. Sejnowski},
  title     = {The Unreasonable Effectiveness of Deep Learning in Artificial Intelligence},
  journal   = {CoRR},
  volume    = {abs/2002.04806},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.04806},
  archivePrefix = {arXiv},
  eprint    = {2002.04806},
  timestamp = {Fri, 14 Feb 2020 12:07:41 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-04806.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{bengio_0,
  author    = {Ian J. Goodfellow and
               Yoshua Bengio and
               Aaron C. Courville},
  title     = {Deep Learning},
  series    = {Adaptive computation and machine learning},
  publisher = {{MIT} Press},
  year      = {2016},
  url       = {http://www.deeplearningbook.org/},
  isbn      = {978-0-262-03561-3},
  timestamp = {Sat, 25 Mar 2017 20:16:59 +0100},
  biburl    = {https://dblp.org/rec/books/daglib/0040158.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bengio_1,
  author    = {Kenji Kawaguchi and
               Leslie Pack Kaelbling and
               Yoshua Bengio},
  title     = {Generalization in Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1710.05468},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.05468},
  archivePrefix = {arXiv},
  eprint    = {1710.05468},
  timestamp = {Mon, 13 Aug 2018 16:47:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-05468.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{esmaml,
title={{ES-MAML}: Simple Hessian-Free Meta Learning},
author={Xingyou Song and Wenbo Gao and Yuxiang Yang and Krzysztof Choromanski and Aldo Pacchiano and Yunhao Tang},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=S1exA2NtDB}
}


@inproceedings{maml,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1126--1135},
  year={2017},
  organization={JMLR. org}
}

@article{richard,
  author    = {Xingyou Song and
               Yuxiang Yang and
               Krzysztof Choromanski and
               Ken Caluwaerts and
               Wenbo Gao and
               Chelsea Finn and
               Jie Tan},
  title     = {Rapidly Adaptable Legged Robots via Evolutionary Meta-Learning},
  journal   = {CoRR},
  volume    = {abs/2003.01239},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.01239},
  archivePrefix = {arXiv},
  eprint    = {2003.01239},
  timestamp = {Tue, 10 Mar 2020 13:33:48 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-01239.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{reptile,
  author    = {Alex Nichol and
               Joshua Achiam and
               John Schulman},
  title     = {On First-Order Meta-Learning Algorithms},
  journal   = {CoRR},
  volume    = {abs/1803.02999},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.02999},
  archivePrefix = {arXiv},
  eprint    = {1803.02999},
  timestamp = {Mon, 13 Aug 2018 16:48:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-02999.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{gckpt,
  author    = {Tianqi Chen and
               Bing Xu and
               Chiyuan Zhang and
               Carlos Guestrin},
  title     = {Training Deep Nets with Sublinear Memory Cost},
  journal   = {CoRR},
  volume    = {abs/1604.06174},
  year      = {2016},
  url       = {http://arxiv.org/abs/1604.06174},
  archivePrefix = {arXiv},
  eprint    = {1604.06174},
  timestamp = {Mon, 13 Aug 2018 16:48:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChenXZG16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{imaml,
title = {Meta-Learning with Implicit Gradients},
author = {Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham M and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {113--124},
year = {2019},
}

@book{butcher,
  title={Numerical Methods for Ordinary Differential Equations},
  author={Butcher, J.C.},
  isbn={9780470753750},
  url={https://books.google.co.uk/books?id=opd2NkBmMxsC},
  year={2008},
  publisher={Wiley}
}

@incollection{neuralode,
title = {Neural Ordinary Differential Equations},
author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {6571--6583},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf}
}

@article{hessian,
    author = {Christianson, Bruce},
    title = "{Automatic Hessians by reverse accumulation}",
    journal = {IMA Journal of Numerical Analysis},
    volume = {12},
    number = {2},
    pages = {135-150},
    year = {1992},
    month = {04},
    abstract = "{Let n be the number of independent variables of a function f, and let W and S respectively be the time and space bounds for the joint evaluation of \\{f, ▽f\\} using automatic differentiation with reverse accumulation. In this note, we examine an extension of the technique of reverse accumulation which allows the automatic extraction of the Hessian of f. The method allows the parallel evaluation of all rows of the Hessian matrix in about 2W time units and 3S space units on each of n processors, or sequential row-by-row evaluation in about 2nW time units and 3S space units on a single processor. The approach described here is intended for use with operator overloading (for example in Ada) and allows the conventional coding of the target function f.}",
    issn = {0272-4979},
    doi = {10.1093/imanum/12.2.135},
    url = {https://doi.org/10.1093/imanum/12.2.135},
    eprint = {https://academic.oup.com/imajna/article-pdf/12/2/135/2091539/12-2-135.pdf},
}

@article{rproj,
  title={Literature survey on low rank approximation of matrices},
  author={Kishore Kumar, N and Schneider, Jan},
  journal={Linear and Multilinear Algebra},
  volume={65},
  number={11},
  pages={2212--2244},
  year={2017},
  publisher={Taylor \& Francis}
}

@book{calculus,
  title={Advanced calculus},
  author={James, R.C.},
  lccn={66014725},
  url={https://books.google.co.uk/books?id=d5kpAQAAMAAJ},
  year={1966},
  publisher={Wadsworth Pub. Co.}
}

@article{bottou,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={Siam Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@book{autograd,
  title={Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation, Second Edition},
  author={Griewank, A. and Walther, A.},
  isbn={9780898717761},
  lccn={2008021064},
  series={Other Titles in Applied Mathematics},
  url={https://books.google.co.uk/books?id=xoiiLaRxcbEC},
  year={2008},
  publisher={Society for Industrial and Applied Mathematics (SIAM, 3600 Market Street, Floor 6, Philadelphia, PA 19104)}
}

@misc{tensorflow,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@article{pytorch,
  title={Automatic differentiation in pytorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}

@article{gcsmaml,
  title={Multi-Step Model-Agnostic Meta-Learning: Convergence and Improved Algorithms},
  author={Ji, Kaiyi and Yang, Junjie and Liang, Yingbin},
  journal={arXiv preprint arXiv:2002.07836},
  year={2020}
}

@inproceedings{esthes,
author = {Martens, James and Sutskever, Ilya and Swersky, Kevin},
title = {Estimating the {H}essian by Back-Propagating Curvature},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {963–970},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML’12}
}

@article{iteig,
  title={Iterative methods for computing eigenvalues and eigenvectors},
  author={Panju, Maysum},
  journal={arXiv preprint arXiv:1105.1185},
  year={2011}
}

@article{ckpt,
  title={Enabling user-driven Checkpointing strategies in Reverse-mode Automatic Differentiation},
  author={Hascoet, Laurent and Araya-Polo, Mauricio},
  journal={arXiv preprint cs/0606042},
  year={2006}
}

@InProceedings{esblo,
  title = 	 {Guided evolutionary strategies: augmenting random search with surrogate gradients},
  author =       {Maheswaranathan, Niru and Metz, Luke and Tucker, George and Choi, Dami and Sohl-Dickstein, Jascha},
  pages = 	 {4264--4273},
  year = 	 {2019},
  editor = 	 {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/maheswaranathan19a/maheswaranathan19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/maheswaranathan19a.html},
  abstract = 	 {Many applications in machine learning require optimizing a function whose true gradient is unknown or computationally expensive, but where surrogate gradient information, directions that may be correlated with the true gradient, is cheaply available. For example, this occurs when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in reinforcement learning or training networks with discrete variables). We propose Guided Evolutionary Strategies (GES), a method for optimally using surrogate gradient directions to accelerate random search. GES defines a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients and estimates a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace and use this to derive a setting of the hyperparameters that works well across problems. We evaluate GES on several example problems, demonstrating an improvement over both standard evolutionary strategies and first-order methods that directly follow the surrogate gradient.}
}

@article{logckpt,
author = { Andreas   Griewank },
title = {Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation},
journal = {Optimization Methods and Software},
volume = {1},
number = {1},
pages = {35-54},
year  = {1992},
publisher = {Taylor & Francis},
doi = {10.1080/10556789208805505},

URL = { 
        https://doi.org/10.1080/10556789208805505
    
},
eprint = { 
        https://doi.org/10.1080/10556789208805505
    
}

}


@article{truncated,
  title={Truncated back-propagation for bilevel optimization},
  author={Shaban, Amirreza and Cheng, Ching-An and Hatch, Nathan and Boots, Byron},
  journal={arXiv preprint arXiv:1810.10667},
  year={2018}
}

@article{convergence_theory,
  author    = {Alireza Fallah and
               Aryan Mokhtari and
               Asuman E. Ozdaglar},
  title     = {On the Convergence Theory of Gradient-Based Model-Agnostic Meta-Learning
               Algorithms},
  journal   = {CoRR},
  volume    = {abs/1908.10400},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.10400},
  archivePrefix = {arXiv},
  eprint    = {1908.10400},
  timestamp = {Thu, 16 Apr 2020 08:14:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-10400.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={5754--5764},
  year={2019}
}

@article{megatron,
  title={Megatron-lm: Training multi-billion parameter language models using gpu model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@inproceedings{revopt,
  title={Gradient-based hyperparameter optimization through reversible learning},
  author={Maclaurin, Dougal and Duvenaud, David and Adams, Ryan},
  booktitle={International Conference on Machine Learning},
  pages={2113--2122},
  year={2015}
}

@inproceedings{revnet,
  title={The reversible residual network: Backpropagation without storing activations},
  author={Gomez, Aidan N and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B},
  booktitle={Advances in neural information processing systems},
  pages={2214--2224},
  year={2017}
}

@inproceedings{reformer,
title={Reformer: The Efficient Transformer},
author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgNKkHtvB}
}

@inproceedings{graphnf,
  title={Graph normalizing flows},
  author={Liu, Jenny and Kumar, Aviral and Ba, Jimmy and Kiros, Jamie and Swersky, Kevin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={13556--13566},
  year={2019}
}

@ARTICLE{nf,
  author={I. {Kobyzev} and S. {Prince} and M. {Brubaker}},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Normalizing Flows: An Introduction and Review of Current Methods}, 
  year={2020},
  volume={},
  number={},
  pages={1-1},}

@misc{utbptt,
title={Unbiasing Truncated Backpropagation Through Time},
author={Corentin Tallec and Yann Ollivier},
year={2018},
url={https://openreview.net/forum?id=rkrWCJWAW},
}

@article{tbptt,
author = {Jaeger, Herbert},
year = {2002},
month = {01},
pages = {},
title = {Tutorial on training recurrent neural networks, covering {BPPT, RTRL, EKF} and the echo state network approach},
volume = {5},
journal = {GMD-Forschungszentrum Informationstechnik, 2002.}
}

@article{transformerxl,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@inproceedings{online_metalearning,
  author    = {Chelsea Finn and
               Aravind Rajeswaran and
               Sham M. Kakade and
               Sergey Levine},
  title     = {Online Meta-Learning},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  pages     = {1920--1930},
  year      = {2019},
}
@inproceedings{provable_guarantees,
  author    = {Maria{-}Florina Balcan and
               Mikhail Khodak and
               Ameet Talwalkar},
  title     = {Provable Guarantees for Gradient-Based Meta-Learning},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  pages     = {424--433},
  year      = {2019},
}

@article{oco_book,
  author    = {Elad Hazan},
  title     = {Introduction to Online Convex Optimization},
  journal   = {CoRR},
  volume    = {abs/1909.05207},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.05207},
  archivePrefix = {arXiv},
  eprint    = {1909.05207},
  timestamp = {Tue, 17 Sep 2019 11:23:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-05207.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{reinforce,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@inproceedings{rebar,
  title={Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models},
  author={Tucker, George and Mnih, Andriy and Maddison, Chris J and Lawson, John and Sohl-Dickstein, Jascha},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2627--2636},
  year={2017}
}

@inproceedings{muprop,
  author={Shixiang Gu and Sergey Levine and Ilya Sutskever and Andriy Mnih},
  title={MuProp: Unbiased Backpropagation for Stochastic Neural Networks.},
  year={2016},
  cdate={1451606400000},
  url={http://arxiv.org/abs/1511.05176},
  booktitle={ICLR (Poster)},
}

@article{nes,
  title={Natural evolution strategies},
  author={Wierstra, Daan and Schaul, Tom and Glasmachers, Tobias and Sun, Yi and Peters, Jan and Schmidhuber, J{\"u}rgen},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={949--980},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{vardrop,
  title={Variational dropout and the local reparameterization trick},
  author={Kingma, Durk P and Salimans, Tim and Welling, Max},
  booktitle={Advances in neural information processing systems},
  pages={2575--2583},
  year={2015}
}

@inproceedings{autoencoder,
  author    = {Diederik P. Kingma and
               Max Welling},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Auto-Encoding Variational {B}ayes},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
  url       = {http://arxiv.org/abs/1312.6114},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{gumbel,
  title={Categorical reparameterization with {G}umbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}

@inproceedings{concrete,
  title={Concrete dropout},
  author={Gal, Yarin and Hron, Jiri and Kendall, Alex},
  booktitle={Advances in neural information processing systems},
  pages={3581--3590},
  year={2017}
}

@article{unbtruncated,
  author    = {Corentin Tallec and
               Yann Ollivier},
  title     = {Unbiasing Truncated Backpropagation Through Time},
  journal   = {CoRR},
  volume    = {abs/1705.08209},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.08209},
  archivePrefix = {arXiv},
  eprint    = {1705.08209},
  timestamp = {Mon, 13 Aug 2018 16:48:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/TallecO17a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{gilm,
  author    = {Edward Grefenstette and
               Brandon Amos and
               Denis Yarats and
               Phu Mon Htut and
               Artem Molchanov and
               Franziska Meier and
               Douwe Kiela and
               Kyunghyun Cho and
               Soumith Chintala},
  title     = {Generalized Inner Loop Meta-Learning},
  journal   = {CoRR},
  volume    = {abs/1910.01727},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.01727},
  archivePrefix = {arXiv},
  eprint    = {1910.01727},
  timestamp = {Wed, 09 Oct 2019 14:07:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-01727.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{imagenet,
title = {Matching Networks for One Shot Learning},
author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and kavukcuoglu, koray and Wierstra, Daan},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {3630--3638},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf}
}

@inproceedings{omniglot,
  title={One shot learning of simple visual concepts},
  author={Lake, Brenden and Salakhutdinov, Ruslan and Gross, Jason and Tenenbaum, Joshua},
  booktitle={Proceedings of the annual meeting of the cognitive science society},
  volume={33},
  number={33},
  year={2011}
}

@inproceedings{co2,
    title = "Energy and Policy Considerations for Deep Learning in {NLP}",
    author = "Strubell, Emma  and
      Ganesh, Ananya  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1355",
    doi = "10.18653/v1/P19-1355",
    pages = "3645--3650",
    abstract = "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
}

@inproceedings{
energy,
title={Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks},
author={Haoran You and Chaojian Li and Pengfei Xu and Yonggan Fu and Yue Wang and Xiaohan Chen and Richard G. Baraniuk and Zhangyang Wang and Yingyan Lin},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJxsrgStvr}
}

@InProceedings{hyperopt,
  title = 	 {Bilevel Programming for Hyperparameter Optimization and Meta-Learning},
  author = 	 {Franceschi, Luca and Frasconi, Paolo and Salzo, Saverio and Grazzi, Riccardo and Pontil, Massimiliano},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1568--1577},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/franceschi18a/franceschi18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/franceschi18a.html},
  abstract = 	 {We introduce a framework based on bilevel programming that unifies gradient-based hyperparameter optimization and meta-learning. We show that an approximate version of the bilevel problem can be solved by taking into explicit account the optimization dynamics for the inner objective. Depending on the specific setting, the outer variables take either the meaning of hyperparameters in a supervised learning problem or parameters of a meta-learner. We provide sufficient conditions under which solutions of the approximate problem converge to those of the exact problem. We instantiate our approach for meta-learning in the case of deep learning where representation layers are treated as hyperparameters shared across a set of training episodes. In experiments, we confirm our theoretical findings, present encouraging results for few-shot learning and contrast the bilevel approach against classical approaches for learning-to-learn.}
}

@article{hyperoptimpl,
  title={Optimizing Millions of Hyperparameters by Implicit Differentiation},
  author={Lorraine, Jonathan and Vicol, Paul and Duvenaud, David},
  journal={arXiv preprint arXiv:1911.02590},
  year={2019}
}


@inproceedings{
darts,
title={{DARTS}: Differentiable Architecture Search},
author={Hanxiao Liu and Karen Simonyan and Yiming Yang},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1eYHoC5FX},
}

@article{advrobbilevel,
  author    = {Zhehui Chen and
               Haoming Jiang and
               Bo Dai and
               Tuo Zhao},
  title     = {Learning to Defense by Learning to Attack},
  journal   = {CoRR},
  volume    = {abs/1811.01213},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.01213},
  archivePrefix = {arXiv},
  eprint    = {1811.01213},
  timestamp = {Thu, 22 Nov 2018 17:58:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-01213.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{advrobgraphs,
title={Adversarial Attacks on Graph Neural Networks via Meta Learning},
author={Daniel Zügner and Stephan Günnemann},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bylnx209YX},
}

@InProceedings{fordiff,
  title = 	 {Forward and Reverse Gradient-Based Hyperparameter Optimization},
  author = 	 {Luca Franceschi and Michele Donini and Paolo Frasconi and Massimiliano Pontil},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1165--1173},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/franceschi17a/franceschi17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/franceschi17a.html},
  abstract = 	 {We study two procedures (reverse-mode and forward-mode) for computing the gradient of the validation error with respect to the hyperparameters of any iterative learning algorithm such as stochastic gradient descent. These procedures mirror two ways of computing gradients for recurrent neural networks and have different trade-offs in terms of running time and space requirements. Our formulation of the reverse-mode procedure is linked to previous work by Maclaurin et al (2015) but does not require reversible dynamics. Additionally, we explore the use of constraints on the hyperparameters. The forward-mode procedure is suitable for real-time hyperparameter updates, which may significantly speedup hyperparameter optimization on large datasets. We present a series of experiments on image and phone classification tasks. In the second task, previous gradient-based approaches are prohibitive. We show that our real-time algorithm yields state-of-the-art results in affordable time.}
}

@inproceedings{impl1,
author = {Pedregosa, Fabian},
title = {Hyperparameter Optimization with Approximate Gradient},
year = {2016},
publisher = {JMLR.org},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {737–746},
numpages = {10},
location = {New York, NY, USA},
series = {ICML’16}
}



@article{impl2,
  author    = {Stephen Gould and
               Basura Fernando and
               Anoop Cherian and
               Peter Anderson and
               Rodrigo Santa Cruz and
               Edison Guo},
  title     = {On Differentiating Parameterized Argmin and Argmax Problems with Application
               to Bi-level Optimization},
  journal   = {CoRR},
  volume    = {abs/1607.05447},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.05447},
  archivePrefix = {arXiv},
  eprint    = {1607.05447},
  timestamp = {Tue, 20 Nov 2018 12:24:39 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/GouldFCACG16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@book{prob,
  title={Probability and statistics: The science of uncertainty},
  author={Evans, Michael J and Rosenthal, Jeffrey S},
  year={2004},
  publisher={Macmillan}
}


@InProceedings{randtelsums,
  title = 	 {Efficient optimization of loops and limits with randomized telescoping sums},
  author =       {Beatson, Alex and Adams, Ryan P},
  pages = 	 {534--543},
  year = 	 {2019},
  editor = 	 {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/beatson19a/beatson19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/beatson19a.html},
  abstract = 	 {We consider optimization problems in which the objective requires an inner loop with many steps or is the limit of a sequence of increasingly costly approximations. Meta-learning, training recurrent neural networks, and optimization of the solutions to differential equations are all examples of optimization problems with this character. In such problems, it can be expensive to compute the objective function value and its gradient, but truncating the loop or using less accurate approximations can induce biases that damage the overall solution. We propose <em>randomized telescope</em> (RT) gradient estimators, which represent the objective as the sum of a telescoping series and sample linear combinations of terms to provide cheap unbiased gradient estimates. We identify conditions under which RT estimators achieve optimization convergence rates independent of the length of the loop or the required accuracy of the approximation. We also derive a method for tuning RT estimators online to maximize a lower bound on the expected decrease in loss per unit of computation. We evaluate our adaptive RT estimators on a range of applications including meta-optimization of learning rates, variational inference of ODE parameters, and training an LSTM to model long sequences.}
}

@article{mnist,
  title={MNIST handwritten digit database. 2010},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  journal={URL http://yann. lecun. com/exdb/mnist},
  volume={7},
  pages={23},
  year={2010}
}

@article{cifar100,
title= {CIFAR-100 (Canadian Institute for Advanced Research)},
journal= {},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {2009},
url= {http://www.cs.toronto.edu/~kriz/cifar.html},
abstract= {This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a "fine" label (the class to which it belongs) and a "coarse" label (the superclass to which it belongs).
Here is the list of classes in the CIFAR-100:

Superclass	Classes
aquatic mammals	beaver, dolphin, otter, seal, whale
fish	aquarium fish, flatfish, ray, shark, trout
flowers	orchids, poppies, roses, sunflowers, tulips
food containers	bottles, bowls, cans, cups, plates
fruit and vegetables	apples, mushrooms, oranges, pears, sweet peppers
household electrical devices	clock, computer keyboard, lamp, telephone, television
household furniture	bed, chair, couch, table, wardrobe
insects	bee, beetle, butterfly, caterpillar, cockroach
large carnivores	bear, leopard, lion, tiger, wolf
large man-made outdoor things	bridge, castle, house, road, skyscraper
large natural outdoor scenes	cloud, forest, mountain, plain, sea
large omnivores and herbivores	camel, cattle, chimpanzee, elephant, kangaroo
medium-sized mammals	fox, porcupine, possum, raccoon, skunk
non-insect invertebrates	crab, lobster, snail, spider, worm
people	baby, boy, girl, man, woman
reptiles	crocodile, dinosaur, lizard, snake, turtle
small mammals	hamster, mouse, rabbit, shrew, squirrel
trees	maple, oak, palm, pine, willow
vehicles 1	bicycle, bus, motorcycle, pickup truck, train
vehicles 2	lawn-mower, rocket, streetcar, tank, tractor

Yes, I know mushrooms aren't really fruit or vegetables and bears aren't really carnivores. },
keywords= {Dataset},
terms= {}
}

@misc{seff,
      title={Super-efficiency of automatic differentiation for functions defined as a minimum}, 
      author={Pierre Ablin and Gabriel Peyré and Thomas Moreau},
      year={2020},
      eprint={2002.03722},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}