
@inproceedings{nguyen2017,
  title={{SARAH}: A novel method for machine learning problems using stochastic recursive gradient},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  booktitle={Proc. Intl. Conf. Machine Learning},
  address = {Sydney, Australia},
  year={2017}
}

@book{nesterov2004,
  title={Introductory Lectures on Convex Optimization: A Basic Course},
  author={Nesterov, Yurii},
  volume={87},
  year={2004},
  publisher={Springer Science \& Business Media}
}

@inproceedings{allen2017,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  booktitle={Proc. of ACM SIGACT Symposium on Theory of Computing},
  pages={1200--1205},
  year={2017},
  address = {Montreal, Canada}
}

@article{xiao2014,
  title={A proximal stochastic gradient method with progressive variance reduction},
  author={Xiao, Lin and Zhang, Tong},
  journal={SIAM Journal on Optimization},
  volume={24},
  number={4},
  pages={2057--2075},
  year={2014},
  publisher={SIAM}
}

@inproceedings{johnson2013,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={315--323},
  address = {Lake Tahoe, Nevada},
  year={2013}
}

@inproceedings{defazio2014,
  title={{SAGA}: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  address = {Montreal, Canada},
  pages={1646--1654},
  year={2014}
}

@article{zhou2018,
  title={A simple stochastic variance reduced algorithm with fast convergence rates},
  author={Zhou, Kaiwen and Shang, Fanhua and Cheng, James},
  journal={arXiv preprint arXiv:1806.11027},
  year={2018}
}

@inproceedings{roux2012,
  title={A stochastic gradient method with an exponential convergence rate for finite training sets},
  author={Roux, Nicolas L and Schmidt, Mark and Bach, Francis R},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  address = {Lake Tahoe, Nevada},
  pages={2663--2671},
  year={2012}
}

@inproceedings{mairal2013,
  title={Optimization with first-order surrogate functions},
  author={Mairal, Julien},
  booktitle = {Proc. Intl. Conf. on Machine Learning},
  pages={783--791},
  address = {Atlanta},
  year={2013}
}

@article{konecny2013,
  title={Semi-stochastic gradient descent methods},
  author={Konecn{\`y}, Jakub and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1312.1666},
  year={2013}
}

@article{shalev2013,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={vol. 14},
  pages={pp. 567--599},
  year={2013},
  month={Feb.}
}


@inproceedings{lei2017,
  title={Non-convex finite-sum optimization via {SCSG} methods},
  author={Lei, Lihua and Ju, Cheng and Chen, Jianbo and Jordan, Michael I},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={2348--2358},
  year={2017},
  address={Long Beach, CA}
}


@inproceedings{reddi2016,
  title={Stochastic variance reduction for nonconvex optimization},
  author={Reddi, Sashank J and Hefny, Ahmed and Sra, Suvrit and Poczos, Barnabas and Smola, Alex},
  booktitle={Proc. Intl. Conf. on Machine Learning},
  pages={314--323},
  address = {New York City, NY},
  year={2016}
}


@inproceedings{lin2015,
  title={A universal catalyst for first-order optimization},
  author={Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  address = {Montreal, Canada},
  pages={3384--3392},
  year={2015}
}

@inproceedings{nitanda2014,
  title={Stochastic proximal gradient descent with acceleration techniques},
  author={Nitanda, Atsushi},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  address = {Montreal, Canada},
  pages={1574--1582},
  year={2014}
}

@inproceedings{agarwal2014,
  title={A lower bound for the optimization of finite sums},
  author={Agarwal, Alekh and Bottou, Leon},
  booktitle={Proc. Intl. Conf. on Machine Learning},
  pages={78--86},
  address = {Lille, France},
  year={2015}
}

@inproceedings{allen2018,
  title={How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex SGD},
  author={Allen-Zhu, Zeyuan},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={1165--1175},
  address = {Montreal, Canada},
  year={2018}
}

@inproceedings{allen2016,
  title={Variance reduction for faster non-convex optimization},
  author={Allen-Zhu, Zeyuan and Hazan, Elad},
  booktitle={Proc. Intl. Conf. on Machine Learning},
  pages={699--707},
  address = {New York City, NY},
  year={2016}
}

@article{bottou2018,
  title={Optimization Methods for Large-Scale Machine Learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={arXiv preprint arXiv:1606.04838},
  year={2016}
}

@article{robbins1951,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The Annals of Mathematical Statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@inproceedings{fang2018,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={687--697},
  address = {Montreal, Canada},
  year={2018}
}

@article{zhang2018,
  title={R-spider: A fast riemannian stochastic optimization algorithm with curvature independent rate},
  author={Zhang, Jingzhao and Zhang, Hongyi and Sra, Suvrit},
  journal={arXiv preprint arXiv:1811.04194},
  year={2018}
}

@article{wang2018,
  title={{SpiderBoost}: A class of faster variance-reduced algorithms for nonconvex optimization},
  author={Wang, Zhe and Ji, Kaiyi and Zhou, Yi and Liang, Yingbin and Tarokh, Vahid},
  journal={arXiv preprint arXiv:1810.10690},
  year={2018}
}

@article{nguyen2019,
  title={Optimal Finite-Sum Smooth Non-Convex Optimization with {SARAH}},
  author={Nguyen, Lam M and van Dijk, Marten and Phan, Dzung T and Nguyen, Phuong Ha and Weng, Tsui-Wei and Kalagnanam, Jayant R},
  journal={arXiv preprint arXiv:1901.07648},
  year={2019}
}

@book{gubner2006,
  title={Probability and random processes for electrical and computer engineers},
  author={Gubner, John A},
  year={2006},
  publisher={Cambridge University Press}
}

@inproceedings{tan2016,
  title={Barzilai-{B}orwein step size for stochastic gradient descent},
  author={Tan, Conghui and Ma, Shiqian and Dai, Yu-Hong and Qian, Yuqiu},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={685--693},
  year={2016},
  address={Barcelona, Spain}
}

@article{barzilai1988,
  title={Two-point step size gradient methods},
  author={Barzilai, Jonathan and Borwein, Jonathan M},
  journal={IMA Journal of Numerical Analysis},
  volume={8},
  number={1},
  pages={141--148},
  year={1988},
  publisher={Oxford University Press}
}


@article{reddi2016saga,
  title={Fast incremental method for nonconvex optimization},
  author={Reddi, Sashank J and Sra, Suvrit and P{\'o}czos, Barnab{\'a}s and Smola, Alex},
  journal={arXiv preprint arXiv:1603.06159},
  year={2016}
}

@article{pham2019,
  title={Prox{SARAH}: An Efficient Algorithmic Framework for Stochastic Composite Nonconvex Optimization},
  author={Pham, Nhan H and Nguyen, Lam M and Phan, Dzung T and Tran-Dinh, Quoc},
  journal={arXiv preprint arXiv:1902.05679},
  year={2019}
}

@article{ghadimi2013,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

@inproceedings{kulunchakov2019,
  title={Estimate Sequences for Variance-Reduced Stochastic Composite Optimization},
  author={Kulunchakov, Andrei and Mairal, Julien},
  booktitle={Proc. Intl. Conf. Machine Learning},
  pages={3541--3550},
  year={2019},
  address={Long Beach, CA}
}

@inproceedings{nitanda2014,
  title={Stochastic proximal gradient descent with acceleration techniques},
  author={Nitanda, Atsushi},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={1574--1582},
  year={2014}
}

@article{allen2017,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={8194--8244},
  year={2017},
  publisher={JMLR}
}

@article{zhou2018,
  title={A simple stochastic variance reduced algorithm with fast convergence rates},
  author={Zhou, Kaiwen and Shang, Fanhua and Cheng, James},
  journal={arXiv preprint arXiv:1806.11027},
  year={2018}
}

@inproceedings{lin2015catalyst,
  title={A universal catalyst for first-order optimization},
  author={Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={3384--3392},
  year={2015}
}

@inproceedings{defazio2016,
  title={A simple practical accelerated method for finite sums},
  author={Defazio, Aaron},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={676--684},
  year={2016}
}

@article{yang2016,
  title={Unified convergence analysis of stochastic momentum methods for convex and non-convex optimization},
  author={Yang, Tianbao and Lin, Qihang and Li, Zhe},
  journal={arXiv preprint arXiv:1604.03257},
  year={2016}
}

@inproceedings{li2019l2s,
  title={On the Convergence of {SARAH} and Beyond},
  author={Li, Bingcong and Ma, Meng, and Giannakis, Georgios B},
  booktitle={Proc. Intl. Conf. on Artificial Intelligence and Statistics},
  year={2020},
  address={Palermo, Italy}
}

@article{kovalev2019,
  title={Don't Jump Through Hoops and Remove Those Loops: {SVRG} and {K}atyusha are Better Without the Outer Loop},
  author={Kovalev, Dmitry and Horvath, Samuel and Richtarik, Peter},
  journal={arXiv preprint arXiv:1901.08689},
  year={2019}
}


@article{liuclass,
  title={A Class of Stochastic Variance Reduced Methods with an Adaptive Stepsize},
  author={Liu, Yan and Han, Congying and Guo, Tiande},
  url={http://www.optimization-online.org/DB_FILE/2019/04/7170.pdf}
}

@article{yang2019,
  title={Accelerating Mini-batch SARAH by Step Size Rules},
  author={Yang, Zhuang and Chen, Zengping and Wang, Cheng},
  journal={arXiv preprint arXiv:1906.08496},
  year={2019}
}


@article{hu2018diss,
  title={Dissipativity theory for accelerating stochastic variance reduction: A unified analysis of {SVRG} and {K}atyusha using semidefinite programs},
  author={Hu, Bin and Wright, Stephen and Lessard, Laurent},
  journal={arXiv preprint arXiv:1806.03677},
  year={2018}
}

@article{li2019adaptive,
  title={Adaptive Step Sizes in Variance Reduction via Regularization},
  author={Li, Bingcong and Giannakis, Georgios B},
  journal={arXiv preprint arXiv:1910.06532},
  year={2019}
}

@inproceedings{cutkosky2019,
  title={Momentum-based variance reduction in non-convex SGD},
  author={Cutkosky, Ashok and Orabona, Francesco},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={15210--15219},
  year={2019},
  address={Vancouver, Canada}
}

