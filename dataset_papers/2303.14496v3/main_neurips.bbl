\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Balcan and Blum(2005)]{balcan2005pac}
M.-F. Balcan and A.~Blum.
\newblock A pac-style model for learning from labeled and unlabeled data.
\newblock In \emph{International Conference on Computational Learning Theory}, pages 111--126. Springer, 2005.

\bibitem[Balcan and Blum(2010)]{balcan2010discriminative}
M.-F. Balcan and A.~Blum.
\newblock A discriminative model for semi-supervised learning.
\newblock \emph{Journal of the ACM (JACM)}, 57\penalty0 (3):\penalty0 1--46, 2010.

\bibitem[Bilodeau et~al.(2022)Bilodeau, Jaques, Koh, and Kim]{bilodeau2022impossibility}
B.~Bilodeau, N.~Jaques, P.~W. Koh, and B.~Kim.
\newblock Impossibility theorems for feature attribution.
\newblock \emph{arXiv preprint arXiv:2212.11870}, 2022.

\bibitem[Birge and Louveaux(2011)]{birge2011introduction}
J.~R. Birge and F.~Louveaux.
\newblock \emph{Introduction to stochastic programming}.
\newblock Springer Science \& Business Media, 2011.

\bibitem[Chapelle et~al.(2009)Chapelle, Scholkopf, and Zien]{chapelle2009semi}
O.~Chapelle, B.~Scholkopf, and A.~Zien.
\newblock Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews].
\newblock \emph{IEEE Transactions on Neural Networks}, 20\penalty0 (3):\penalty0 542--542, 2009.

\bibitem[Czarnecki et~al.(2017)Czarnecki, Osindero, Jaderberg, Swirszcz, and Pascanu]{czarnecki2017sobolev}
W.~M. Czarnecki, S.~Osindero, M.~Jaderberg, G.~Swirszcz, and R.~Pascanu.
\newblock Sobolev training for neural networks.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Fioretto et~al.(2021)Fioretto, Van~Hentenryck, Mak, Tran, Baldo, and Lombardi]{fioretto2021lagrangian}
F.~Fioretto, P.~Van~Hentenryck, T.~W. Mak, C.~Tran, F.~Baldo, and M.~Lombardi.
\newblock Lagrangian duality for constrained deep learning.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge Discovery in Databases}, pages 118--135. Springer, 2021.

\bibitem[Frei et~al.(2022)Frei, Zou, Chen, and Gu]{frei2022self}
S.~Frei, D.~Zou, Z.~Chen, and Q.~Gu.
\newblock Self-training converts weak learners to strong learners in mixture models.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 8003--8021. PMLR, 2022.

\bibitem[Ganchev et~al.(2010)Ganchev, Gra{\c{c}}a, Gillenwater, and Taskar]{ganchev2010posterior}
K.~Ganchev, J.~Gra{\c{c}}a, J.~Gillenwater, and B.~Taskar.
\newblock Posterior regularization for structured latent variable models.
\newblock \emph{The Journal of Machine Learning Research}, 11:\penalty0 2001--2049, 2010.

\bibitem[Garg and Liang(2020)]{garg2020functional}
S.~Garg and Y.~Liang.
\newblock Functional regularization for representation learning: A unified theoretical perspective.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 17187--17199, 2020.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and Courville]{goodfellow2016deep}
I.~Goodfellow, Y.~Bengio, and A.~Courville.
\newblock \emph{Deep learning}.
\newblock MIT press, 2016.

\bibitem[Goyal et~al.(2019)Goyal, Wu, Ernst, Batra, Parikh, and Lee]{goyal2019counterfactual}
Y.~Goyal, Z.~Wu, J.~Ernst, D.~Batra, D.~Parikh, and S.~Lee.
\newblock Counterfactual visual explanations.
\newblock In \emph{International Conference on Machine Learning}, pages 2376--2384. PMLR, 2019.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
G.~Hinton, O.~Vinyals, and J.~Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hu et~al.(2016)Hu, Ma, Liu, Hovy, and Xing]{hu2016harnessing}
Z.~Hu, X.~Ma, Z.~Liu, E.~Hovy, and E.~Xing.
\newblock Harnessing deep neural networks with logic rules.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 2410--2420, 2016.

\bibitem[Ismail et~al.(2021)Ismail, Corrada~Bravo, and Feizi]{ismail2021improving}
A.~A. Ismail, H.~Corrada~Bravo, and S.~Feizi.
\newblock Improving deep learning interpretability by saliency guided training.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 26726--26739, 2021.

\bibitem[Kall et~al.(1994)Kall, Wallace, and Kall]{kall1994stochastic}
P.~Kall, S.~W. Wallace, and P.~Kall.
\newblock \emph{Stochastic programming}, volume~6.
\newblock Springer, 1994.

\bibitem[Kim et~al.(2018)Kim, Wattenberg, Gilmer, Cai, Wexler, Viegas, et~al.]{kim2018interpretability}
B.~Kim, M.~Wattenberg, J.~Gilmer, C.~Cai, J.~Wexler, F.~Viegas, et~al.
\newblock Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav).
\newblock In \emph{International conference on machine learning}, pages 2668--2677. PMLR, 2018.

\bibitem[Koh and Liang(2017)]{koh2017understanding}
P.~W. Koh and P.~Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{International conference on machine learning}, pages 1885--1894. PMLR, 2017.

\bibitem[Koh et~al.(2020)Koh, Nguyen, Tang, Mussmann, Pierson, Kim, and Liang]{koh2020concept}
P.~W. Koh, T.~Nguyen, Y.~S. Tang, S.~Mussmann, E.~Pierson, B.~Kim, and P.~Liang.
\newblock Concept bottleneck models.
\newblock In \emph{International Conference on Machine Learning}, pages 5338--5348. PMLR, 2020.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
Y.~LeCun, Y.~Bengio, and G.~Hinton.
\newblock Deep learning.
\newblock \emph{nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Ledoux and Talagrand(1991)]{ledoux1991probability}
M.~Ledoux and M.~Talagrand.
\newblock \emph{Probability in Banach Spaces: isoperimetry and processes}, volume~23.
\newblock Springer Science \& Business Media, 1991.

\bibitem[Li et~al.(2020)Li, Nagarajan, Plumb, and Talwalkar]{li2020learning}
J.~Li, V.~Nagarajan, G.~Plumb, and A.~Talwalkar.
\newblock A learning theoretic perspective on local explainability.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Lundberg and Lee(2017)]{lundberg2017unified}
S.~M. Lundberg and S.-I. Lee.
\newblock A unified approach to interpreting model predictions.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Ma(2022)]{ma2022notes}
T.~Ma.
\newblock Lecture notes from machine learning theory, 2022.
\newblock URL \url{http://web.stanford.edu/class/stats214/}.

\bibitem[Mothilal et~al.(2020)Mothilal, Sharma, and Tan]{mothilal2020explaining}
R.~K. Mothilal, A.~Sharma, and C.~Tan.
\newblock Explaining machine learning classifiers through diverse counterfactual explanations.
\newblock In \emph{Proceedings of the 2020 conference on fairness, accountability, and transparency}, pages 607--617, 2020.

\bibitem[Murdoch et~al.(2018)Murdoch, Liu, and Yu]{murdoch2018beyond}
W.~J. Murdoch, P.~J. Liu, and B.~Yu.
\newblock Beyond word importance: Contextual decomposition to extract interactions from lstms.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Natarajan et~al.(2013)Natarajan, Dhillon, Ravikumar, and Tewari]{natarajan2013learning}
N.~Natarajan, I.~S. Dhillon, P.~K. Ravikumar, and A.~Tewari.
\newblock Learning with noisy labels.
\newblock \emph{Advances in neural information processing systems}, 26, 2013.

\bibitem[Pukdee et~al.(2022)Pukdee, Sam, Ravikumar, and Balcan]{pukdee2022label}
R.~Pukdee, D.~Sam, P.~K. Ravikumar, and N.~Balcan.
\newblock Label propagation with weak supervision.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Ratner et~al.(2017)Ratner, Bach, Ehrenberg, Fries, Wu, and R{\'e}]{ratner2017snorkel}
A.~Ratner, S.~H. Bach, H.~Ehrenberg, J.~Fries, S.~Wu, and C.~R{\'e}.
\newblock Snorkel: Rapid training data creation with weak supervision.
\newblock In \emph{Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases}, volume~11, page 269. NIH Public Access, 2017.

\bibitem[Ratner et~al.(2016)Ratner, De~Sa, Wu, Selsam, and R{\'e}]{ratner2016data}
A.~J. Ratner, C.~M. De~Sa, S.~Wu, D.~Selsam, and C.~R{\'e}.
\newblock Data programming: Creating large training sets, quickly.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Ribeiro et~al.(2016)Ribeiro, Singh, and Guestrin]{ribeiro2016should}
M.~T. Ribeiro, S.~Singh, and C.~Guestrin.
\newblock " why should i trust you?" explaining the predictions of any classifier.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining}, pages 1135--1144, 2016.

\bibitem[Rieger et~al.(2020)Rieger, Singh, Murdoch, and Yu]{rieger2020interpretations}
L.~Rieger, C.~Singh, W.~Murdoch, and B.~Yu.
\newblock Interpretations are useful: penalizing explanations to align neural networks with prior knowledge.
\newblock In \emph{International conference on machine learning}, pages 8116--8126. PMLR, 2020.

\bibitem[Ross et~al.(2017)Ross, Hughes, and Doshi-Velez]{ross2017right}
A.~S. Ross, M.~C. Hughes, and F.~Doshi-Velez.
\newblock Right for the right reasons: training differentiable models by constraining their explanations.
\newblock In \emph{Proceedings of the 26th International Joint Conference on Artificial Intelligence}, pages 2662--2670, 2017.

\bibitem[Shao et~al.(2022)Shao, Montasser, and Blum]{shao2022a}
H.~Shao, O.~Montasser, and A.~Blum.
\newblock A theory of {PAC} learnability under transformation invariances.
\newblock In A.~H. Oh, A.~Agarwal, D.~Belgrave, and K.~Cho, editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=l1WlfNaRkKw}.

\bibitem[Smilkov et~al.(2017)Smilkov, Thorat, Kim, Vi{\'e}gas, and Wattenberg]{smilkov2017smoothgrad}
D.~Smilkov, N.~Thorat, B.~Kim, F.~Vi{\'e}gas, and M.~Wattenberg.
\newblock Smoothgrad: removing noise by adding noise.
\newblock \emph{ICML Workshop on Visualization for Deep Learning, 2017}, 2017.

\bibitem[Stacey et~al.(2022)Stacey, Belinkov, and Rei]{stacey2022supervising}
J.~Stacey, Y.~Belinkov, and M.~Rei.
\newblock Supervising model attention with human explanations for robust natural language inference.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~36, pages 11349--11357, 2022.

\bibitem[Sundararajan et~al.(2017)Sundararajan, Taly, and Yan]{sundararajan2017axiomatic}
M.~Sundararajan, A.~Taly, and Q.~Yan.
\newblock Axiomatic attribution for deep networks.
\newblock In \emph{International conference on machine learning}, pages 3319--3328. PMLR, 2017.

\bibitem[Valiant(1984)]{valiant1984theory}
L.~G. Valiant.
\newblock A theory of the learnable.
\newblock \emph{Communications of the ACM}, 27\penalty0 (11):\penalty0 1134--1142, 1984.

\bibitem[Wachter et~al.(2017)Wachter, Mittelstadt, and Russell]{wachter2017counterfactual}
S.~Wachter, B.~Mittelstadt, and C.~Russell.
\newblock Counterfactual explanations without opening the black box: Automated decisions and the gdpr.
\newblock \emph{Harv. JL \& Tech.}, 31:\penalty0 841, 2017.

\bibitem[Wei et~al.(2020)Wei, Shen, Chen, and Ma]{wei2020theoretical}
C.~Wei, K.~Shen, Y.~Chen, and T.~Ma.
\newblock Theoretical analysis of self-training with deep networks on unlabeled data.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Wicker et~al.(2022)Wicker, Heo, Costabello, and Weller]{wicker2022robust}
M.~R. Wicker, J.~Heo, L.~Costabello, and A.~Weller.
\newblock Robust explanation constraints for neural networks.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Xie et~al.(2020)Xie, Luong, Hovy, and Le]{xie2020self}
Q.~Xie, M.-T. Luong, E.~Hovy, and Q.~V. Le.
\newblock Self-training with noisy student improves imagenet classification.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10687--10698, 2020.

\bibitem[Yeh et~al.(2018)Yeh, Kim, Yen, and Ravikumar]{yeh2018representer}
C.-K. Yeh, J.~Kim, I.~E.-H. Yen, and P.~K. Ravikumar.
\newblock Representer point selection for explaining deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Yeh et~al.(2020)Yeh, Kim, Arik, Li, Pfister, and Ravikumar]{yeh2020completeness}
C.-K. Yeh, B.~Kim, S.~Arik, C.-L. Li, T.~Pfister, and P.~Ravikumar.
\newblock On completeness-aware concept-based explanations in deep neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 20554--20565, 2020.

\bibitem[Zarlenga et~al.(2022)Zarlenga, Barbiero, Ciravegna, Marra, Giannini, Diligenti, Precioso, Melacci, Weller, Lio, et~al.]{zarlenga2022concept}
M.~E. Zarlenga, P.~Barbiero, G.~Ciravegna, G.~Marra, F.~Giannini, M.~Diligenti, F.~Precioso, S.~Melacci, A.~Weller, P.~Lio, et~al.
\newblock Concept embedding models.
\newblock In \emph{NeurIPS 2022-36th Conference on Neural Information Processing Systems}, 2022.

\bibitem[Zhang et~al.(2021)Zhang, Yu, Li, Wang, Yang, Yang, and Ratner]{zhang2021wrench}
J.~Zhang, Y.~Yu, Y.~Li, Y.~Wang, Y.~Yang, M.~Yang, and A.~Ratner.
\newblock Wrench: A comprehensive benchmark for weak supervision.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)}, 2021.

\end{thebibliography}
