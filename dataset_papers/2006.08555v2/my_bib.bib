
@inproceedings{double_oracle,
  title={Planning in the presence of cost functions controlled by an adversary},
  author={McMahan, H Brendan and Gordon, Geoffrey J and Blum, Avrim},
  booktitle={Proceedings of the 20th International Conference on Machine Learning (ICML-03)},
  pages={536--543},
  year={2003}
}

@article{muller2019generalized,
  title={A Generalized Training Approach for Multiagent Learning},
  author={Muller, Paul and Omidshafiei, Shayegan and Rowland, Mark and Tuyls, Karl and Perolat, Julien and Liu, Siqi and Hennes, Daniel and Marris, Luke and Lanctot, Marc and Hughes, Edward and others},
  journal={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@misc{barrage_bots,
  author = {Sam Moore},
  title = {Stratego AI Evaluator},
  year = {2014},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/braathwaate/strategoevaluator}}
}

@book{shoham2008multiagent,
  title={Multiagent systems: Algorithmic, game-theoretic, and logical foundations},
  author={Shoham, Yoav and Leyton-Brown, Kevin},
  year={2008},
  publisher={Cambridge University Press}
}

@article{gruslys2020advantage,
  title={The Advantage Regret-Matching Actor-Critic},
  author={Gruslys, Audr{\=u}nas and Lanctot, Marc and Munos, R{\'e}mi and Timbers, Finbarr and Schmid, Martin and Perolat, Julien and Morrill, Dustin and Zambaldi, Vinicius and Lespiau, Jean-Baptiste and Schultz, John and others},
  journal={arXiv preprint arXiv:2008.12234},
  year={2020}
}

@article{steinberger2020dream,
  title={DREAM: Deep Regret minimization with Advantage baselines and Model-free learning},
  author={Steinberger, Eric and Lerer, Adam and Brown, Noam},
  journal={arXiv preprint arXiv:2006.10410},
  year={2020}
}

@article{bowling2002multiagent,
  title={Multiagent learning using a variable learning rate},
  author={Bowling, Michael and Veloso, Manuela},
  journal={Artificial Intelligence},
  volume={136},
  number={2},
  pages={215--250},
  year={2002},
  publisher={Elsevier}
}

@inproceedings{qpg,
  title={Actor-critic policy optimization in partially observable multiagent environments},
  author={Srinivasan, Sriram and Lanctot, Marc and Zambaldi, Vinicius and P{\'e}rolat, Julien and Tuyls, Karl and Munos, R{\'e}mi and Bowling, Michael},
  booktitle={Advances in neural information processing systems},
  pages={3422--3435},
  year={2018}
}

@article{replicator,
  title={Evolutionary stable strategies and game dynamics},
  author={Taylor, Peter D and Jonker, Leo B},
  journal={Mathematical biosciences},
  volume={40},
  number={1-2},
  pages={145--156},
  year={1978},
  publisher={Citeseer}
}

@book{game_tree_complexity,
  title={Searching for solutions in games and artificial intelligence},
  author={Allis, Louis Victor}
}

@inproceedings{deep_cfr,
  title={Deep Counterfactual Regret Minimization},
  author={Brown, Noam and Lerer, Adam and Gross, Sam and Sandholm, Tuomas},
  booktitle={International Conference on Machine Learning},
  pages={793--802},
  year={2019}
}

@article{silver2017mastering1,
	title={Mastering the game of go without human knowledge},
	author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
	journal={Nature},
	volume={550},
	number={7676},
	pages={354},
	year={2017},
	publisher={Nature Publishing Group}
}

@article{silver2017mastering2,
	title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
	author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
	journal={arXiv preprint arXiv:1712.01815},
	year={2017}
}

@article{dota,
  title={Dota 2 with Large Scale Deep Reinforcement Learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

@inproceedings{psro,
  title={A unified game-theoretic approach to multiagent reinforcement learning},
  author={Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and P{\'e}rolat, Julien and Silver, David and Graepel, Thore},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4190--4203},
  year={2017}
}

@book{agt,
  title={Algorithmic Game Theory},
  author={Nisan, Noam and Roughgarden, Tim and Tardos, Eva and Vazirani, Vijay V},
  year={2007},
  publisher={Cambridge University Press}
}

@inproceedings{xfp,
  title={Fictitious self-play in extensive-form games},
  author={Heinrich, Johannes and Lanctot, Marc and Silver, David},
  booktitle={International Conference on Machine Learning},
  pages={805--813},
  year={2015}
}

@article{neurd,
  title={Neural Replicator Dynamics},
  author={Omidshafiei, Shayegan and Hennes, Daniel and Morrill, Dustin and Munos, Remi and Perolat, Julien and Lanctot, Marc and Gruslys, Audrunas and Lespiau, Jean-Baptiste and Tuyls, Karl},
  journal={arXiv preprint arXiv:1906.00190},
  year={2019}
}

@inproceedings{cfr,
  title={Regret minimization in games with incomplete information},
  author={Zinkevich, Martin and Johanson, Michael and Bowling, Michael and Piccione, Carmelo},
  booktitle={Advances in neural information processing systems},
  pages={1729--1736},
  year={2008}
}

@inproceedings{ray,
  title={Ray: A distributed framework for emerging $\{$AI$\}$ applications},
  author={Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I and others},
  booktitle={13th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 18)},
  pages={561--577},
  year={2018}
}

@inproceedings{rllib,
  title={RLlib: Abstractions for Distributed Reinforcement Learning},
  author={Liang, Eric and Liaw, Richard and Nishihara, Robert and Moritz, Philipp and Fox, Roy and Goldberg, Ken and Gonzalez, Joseph and Jordan, Michael and Stoica, Ion},
  booktitle={International Conference on Machine Learning},
  pages={3053--3062},
  year={2018}
}

@article{discrete_sac,
  title={Soft Actor-Critic for Discrete Action Settings},
  author={Christodoulou, Petros},
  journal={arXiv preprint arXiv:1910.07207},
  year={2019}
}

@article{pbt,
  title={Human-level performance in 3D multiplayer games with population-based reinforcement learning},
  author={Jaderberg, Max and Czarnecki, Wojciech M and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C and Morcos, Ari S and Ruderman, Avraham and others},
  journal={Science},
  volume={364},
  number={6443},
  pages={859--865},
  year={2019},
  publisher={American Association for the Advancement of Science}
}

@article{deep_rl,
  title={Deep reinforcement learning: An overview},
  author={Li, Yuxi},
  journal={arXiv preprint arXiv:1701.07274},
  year={2017}
}

@inproceedings{stratego_bot,
  title={Quiescence search for stratego},
  author={Schadd, Maarten and Winands, Mark},
  booktitle={Proceedings of the 21st Benelux Conference on Artificial Intelligence. Eindhoven, the Netherlands},
  year={2009}
}

@article{stratego_bots,
  title={The 3rd stratego computer world championship},
  author={Jug, Sven and Schadd, Maarten},
  journal={Icga Journal},
  volume={32},
  number={4},
  pages={233},
  year={2009}
}

@article{brown2018superhuman,
  title={Superhuman AI for heads-up no-limit poker: Libratus beats top professionals},
  author={Brown, Noam and Sandholm, Tuomas},
  journal={Science},
  volume={359},
  number={6374},
  pages={418--424},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{sac,
  title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1861--1870},
  year={2018}
}

@book{theory_of_learning_in_games,
  title={The theory of learning in games},
  author={Fudenberg, Drew and Drew, Fudenberg and Levine, David K and Levine, David K},
  year={1998},
  publisher={The MIT Press}
}

@article{nfsp,
  title={Deep reinforcement learning from self-play in imperfect-information games},
  author={Heinrich, Johannes and Silver, David},
  journal={arXiv preprint arXiv:1603.01121},
  year={2016}
}

@inproceedings{rectified_psro,
  title={Open-ended learning in symmetric zero-sum games},
  author={Balduzzi, David and Garnelo, Marta and Bachrach, Yoram and Czarnecki, Wojciech and Perolat, Julien and Jaderberg, Max and Graepel, Thore},
  booktitle={International Conference on Machine Learning},
  pages={434--443},
  year={2019}
}

@article{alphastar,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{silver_2017,
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and Driessche, George van den and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  title = {Mastering the game of Go without human knowledge},
  journal = {Nature},
  publisher = {Nature Publishing Group},
  issn = {0028-0836},
  doi = {10.1038/nature24270},
  volume = {550},
  month = {10},
  pages = {354--359},
  number = {7676},
  url = {http:https://doi.org/10.1038/nature24270},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.}
}
