\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alvarez \& Salzmann(2016)Alvarez and Salzmann]{AlvarezSalzmann2016}
Alvarez, Jose~M and Salzmann, Mathieu.
\newblock Learning the number of neurons in deep networks.
\newblock In \emph{NIPS}, 2016.

\bibitem[Arora et~al.(2014)Arora, Bhaskara, Ge, and Ma]{AroraBhaskaraGeMa2014}
Arora, Sanjeev, Bhaskara, Aditya, Ge, Rong, and Ma, Tengyu.
\newblock Provable bounds for learning some deep representations.
\newblock In \emph{ICML}, pp.\  584--592, 2014.

\bibitem[Arora et~al.(2015)Arora, Liang, and Ma]{AroraLiangMa2015}
Arora, Sanjeev, Liang, Yingyu, and Ma, Tengyu.
\newblock Why are deep nets reversible: A simple theory, with implications for
  training.
\newblock \emph{arXiv:1511.05653}, 2015.

\bibitem[Baker et~al.(2016)Baker, Gupta, Naik, and
  Raskar]{BakerGuptaNaikRaskar2016}
Baker, Bowen, Gupta, Otkrist, Naik, Nikhil, and Raskar, Ramesh.
\newblock Designing neural network architectures using reinforcement learning.
\newblock \emph{CoRR}, 2016.

\bibitem[Bartlett(1998)]{Bartlett1998}
Bartlett, Peter~L.
\newblock The sample complexity of pattern classification with neural networks:
  the size of the weights is more important than the size of the network.
\newblock \emph{Information Theory, IEEE Transactions on}, 44\penalty0 (2),
  1998.

\bibitem[Bartlett \& Mendelson(2002)Bartlett and
  Mendelson]{BartlettMendelson2002}
Bartlett, Peter~L. and Mendelson, Shahar.
\newblock Rademacher and {G}aussian complexities: Risk bounds and structural
  results.
\newblock \emph{JMLR}, 3, 2002.

\bibitem[Bergstra et~al.(2011)Bergstra, Bardenet, Bengio, and
  K{\'e}gl]{BergstraBardenetBengioKegl2011}
Bergstra, James~S, Bardenet, R{\'e}mi, Bengio, Yoshua, and K{\'e}gl,
  Bal{\'a}zs.
\newblock Algorithms for hyper-parameter optimization.
\newblock In \emph{NIPS}, pp.\  2546--2554, 2011.

\bibitem[Chen et~al.(2015)Chen, Goodfellow, and
  Shlens]{ChenGoodfellowShlens2015}
Chen, Tianqi, Goodfellow, Ian~J., and Shlens, Jonathon.
\newblock Net2net: Accelerating learning via knowledge transfer.
\newblock \emph{CoRR}, 2015.

\bibitem[Choromanska et~al.(2014)Choromanska, Henaff, Mathieu, Arous, and
  LeCun]{ChoromanskaHenaffMathieuArousLeCun2014}
Choromanska, Anna, Henaff, Mikael, Mathieu, Michael, Arous, G{\'e}rard~Ben, and
  LeCun, Yann.
\newblock The loss surfaces of multilayer networks.
\newblock \emph{arXiv:1412.0233}, 2014.

\bibitem[Cohen et~al.(2015)Cohen, Sharir, and Shashua]{CohenSharirShashua2015}
Cohen, Nadav, Sharir, Or, and Shashua, Amnon.
\newblock On the expressive power of deep learning: a tensor analysis.
\newblock \emph{arXiv}, 2015.

\bibitem[Cortes et~al.(2014)Cortes, Mohri, and Syed]{CortesMohriSyed2014}
Cortes, Corinna, Mohri, Mehryar, and Syed, Umar.
\newblock Deep boosting.
\newblock In \emph{ICML}, pp.\  1179 -- 1187, 2014.

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and
  Singer]{DanielyFrostigSinger2016}
Daniely, Amit, Frostig, Roy, and Singer, Yoram.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In \emph{NIPS}, 2016.

\bibitem[Eldan \& Shamir(2015)Eldan and Shamir]{EldanShamir2015}
Eldan, Ronen and Shamir, Ohad.
\newblock The power of depth for feedforward neural networks.
\newblock \emph{arXiv:1512.03965}, 2015.

\bibitem[Freund \& Schapire(1997)Freund and Schapire]{FreundSchapire97}
Freund, Yoav and Schapire, Robert~E.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock \emph{Journal of Computer System Sciences}, 55\penalty0 (1):\penalty0
  119--139, 1997.

\bibitem[Ha et~al.(2016)Ha, Dai, and Le]{HaDaiLe16}
Ha, David, Dai, Andrew~M., and Le, Quoc~V.
\newblock Hypernetworks.
\newblock \emph{CoRR}, 2016.

\bibitem[Han \& Qiao(2013)Han and Qiao]{HanQiao2013}
Han, Hong-Gui and Qiao, Jun-Fei.
\newblock A structure optimisation algorithm for feedforward neural network
  construction.
\newblock \emph{Neurocomputing}, 99:\penalty0 347--357, 2013.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{HanPoolTranDally}
Han, Song, Pool, Jeff, Tran, John, and Dally, William~J.
\newblock Learning both weights and connections for efficient neural networks.
\newblock In \emph{NIPS}, 2015.

\bibitem[Hardt et~al.(2015)Hardt, Recht, and Singer]{HardtRechtSinger2015}
Hardt, Moritz, Recht, Benjamin, and Singer, Yoram.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock \emph{arXiv:1509.01240}, 2015.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{HeZhangRenSun2015}
He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian.
\newblock Deep residual learning for image recognition.
\newblock \emph{CoRR}, abs/1512.03385, 2015.

\bibitem[Huang et~al.(2016)Huang, Liu, and Weinberger]{HuangLiuWeinberger2016}
Huang, Gao, Liu, Zhuang, and Weinberger, Kilian~Q.
\newblock Densely connected convolutional networks.
\newblock \emph{CoRR}, 2016.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{IoffeSzegedy15}
Ioffe, Sergey and Szegedy, Christian.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{{ICML}}, 2015.

\bibitem[Islam et~al.(2003)Islam, Yao, and Murase]{IslamYaoMurase2003}
Islam, Md~Monirul, Yao, Xin, and Murase, Kazuyuki.
\newblock A constructive algorithm for training cooperative neural network
  ensembles.
\newblock \emph{IEEE Transactions on Neural Networks}, 14\penalty0
  (4):\penalty0 820--834, 2003.

\bibitem[Islam et~al.(2009)Islam, Sattar, Amin, Yao, and
  Murase]{IslamSattarAminYaoMurase2009}
Islam, MohamOBmad, Sattar, Abdul, Amin, Farnaz, Yao, Xin, and Murase, Kazuyuki.
\newblock A new adaptive merging and growing algorithm for designing artificial
  neural networks.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics},
  39\penalty0 (3):\penalty0 705--722, 2009.

\bibitem[Janzamin et~al.(2015)Janzamin, Sedghi, and
  Anandkumar]{JanzaminSedghiAnandkumar2015}
Janzamin, Majid, Sedghi, Hanie, and Anandkumar, Anima.
\newblock Generalization bounds for neural networks through tensor
  factorization.
\newblock \emph{arXiv:1506.08473}, 2015.

\bibitem[Kawaguchi(2016)]{Kawaguchi2016}
Kawaguchi, Kenji.
\newblock Deep learning without poor local minima.
\newblock In \emph{NIPS}, 2016.

\bibitem[Koltchinskii \& Panchenko(2002)Koltchinskii and
  Panchenko]{KoltchinskiiPanchenko2002}
Koltchinskii, Vladmir and Panchenko, Dmitry.
\newblock Empirical margin distributions and bounding the generalization error
  of combined classifiers.
\newblock \emph{Annals of Statistics}, 30, 2002.

\bibitem[Kotani et~al.(1997)Kotani, Kajiki, and
  Akazawa]{KotaniKajikiAkazawa1997}
Kotani, Manabu, Kajiki, Akihiro, and Akazawa, Kenzo.
\newblock A structural learning algorithm for multi-layered neural networks.
\newblock In \emph{International Conference on Neural Networks}, volume~2, pp.\
   1105--1110. IEEE, 1997.

\bibitem[Krizhevsky(2009)]{Krizhevsky09learningmultiple}
Krizhevsky, Alex.
\newblock Learning multiple layers of features from tiny images.
\newblock Master's thesis, University of Toronto, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{KrizhevskySutskeverHinton2012}
Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{NIPS}, pp.\  1097--1105, 2012.

\bibitem[Kuznetsov et~al.(2014)Kuznetsov, Mohri, and
  Syed]{KuznetsovMohriSyed2014}
Kuznetsov, Vitaly, Mohri, Mehryar, and Syed, Umar.
\newblock Multi-class deep boosting.
\newblock In \emph{NIPS}, 2014.

\bibitem[Kwok \& Yeung(1997)Kwok and Yeung]{KwokYeung1997}
Kwok, Tin-Yau and Yeung, Dit-Yan.
\newblock Constructive algorithms for structure learning in feedforward neural
  networks for regression problems.
\newblock \emph{IEEE Transactions on Neural Networks}, 8\penalty0 (3):\penalty0
  630--645, 1997.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{LeCunDenkerSolla}
LeCun, Yann, Denker, John~S., and Solla, Sara~A.
\newblock Optimal brain damage.
\newblock In \emph{NIPS}, 1990.

\bibitem[Lehtokangas(1999)]{Lehtokangas1999}
Lehtokangas, Mikko.
\newblock Modelling with constructive backpropagation.
\newblock \emph{Neural Networks}, 12\penalty0 (4):\penalty0 707--716, 1999.

\bibitem[Leung et~al.(2003)Leung, Lam, Ling, and Tam]{LeungLamLingTam2003}
Leung, Frank~HF, Lam, Hak-Keung, Ling, Sai-Ho, and Tam, Peter~KS.
\newblock Tuning of the structure and parameters of a neural network using an
  improved genetic algorithm.
\newblock \emph{IEEE Transactions on Neural Networks}, 14\penalty0
  (1):\penalty0 79--88, 2003.

\bibitem[Lian et~al.(2015)Lian, Huang, Li, and Liu]{LianHuangLiLiu2015}
Lian, Xiangru, Huang, Yijun, Li, Yuncheng, and Liu, Ji.
\newblock Asynchronous parallel stochastic gradient for nonconvex optimization.
\newblock In \emph{NIPS}, pp.\  2719--2727, 2015.

\bibitem[Livni et~al.(2014)Livni, Shalev-Shwartz, and
  Shamir]{LivniShalevShwartzShamir2014}
Livni, Roi, Shalev-Shwartz, Shai, and Shamir, Ohad.
\newblock On the computational efficiency of training neural networks.
\newblock In \emph{NIPS}, pp.\  855--863, 2014.

\bibitem[Luo \& Tseng(1992)Luo and Tseng]{LuoTseng1992}
Luo, Zhi-Quan and Tseng, Paul.
\newblock On the convergence of coordinate descent method for convex
  differentiable minimization.
\newblock \emph{Journal of Optimization Theory and Applications}, 72\penalty0
  (1):\penalty0 7 -- 35, 1992.

\bibitem[Ma \& Khorasani(2003)Ma and Khorasani]{MaKhorasani2003}
Ma, Liying and Khorasani, Khashayar.
\newblock A new strategy for adaptively constructing multilayer feedforward
  neural networks.
\newblock \emph{Neurocomputing}, 51:\penalty0 361--385, 2003.

\bibitem[Narasimha et~al.(2008)Narasimha, Delashmit, Manry, Li, and
  Maldonado]{NarasimhaDelashmitManryLiMaldonado2008}
Narasimha, Pramod~L, Delashmit, Walter~H, Manry, Michael~T, Li, Jiang, and
  Maldonado, Francisco.
\newblock An integrated growing-pruning method for feedforward network
  training.
\newblock \emph{Neurocomputing}, 71\penalty0 (13):\penalty0 2831--2847, 2008.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and
  Srebro]{NeyshaburTomiokaSrebro2015}
Neyshabur, Behnam, Tomioka, Ryota, and Srebro, Nathan.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{{COLT}}, 2015.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{PascanuMikolovBengio2013}
Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{{ICML}}, 2013.

\bibitem[R{\"a}tsch et~al.(2001)R{\"a}tsch, Mika, and
  Warmuth]{RatschMikaWarmuth2001}
R{\"a}tsch, Gunnar, Mika, Sebastian, and Warmuth, Manfred~K.
\newblock On the convergence of leveraging.
\newblock In \emph{NIPS}, pp.\  487--494, 2001.

\bibitem[Sagun et~al.(2014)Sagun, Guney, Arous, and
  LeCun]{SagunGuneyArousLeCun2014}
Sagun, Levent, Guney, V~Ugur, Arous, Gerard~Ben, and LeCun, Yann.
\newblock Explorations on high dimensional landscapes.
\newblock \emph{arXiv:1412.6615}, 2014.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{Snoek12bandit}
Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan~P.
\newblock {Practical Bayesian Optimization of Machine Learning Algorithms}.
\newblock In Pereira, F., Burges, C. J.~C., Bottou, L., and Weinberger, K.~Q.
  (eds.), \emph{NIPS}, pp.\  2951--2959. Curran Associates, Inc., 2012.

\bibitem[Sun et~al.(2016)Sun, Chen, Wang, Liu, and Liu]{SunChenWangLiu2016}
Sun, Shizhao, Chen, Wei, Wang, Liwei, Liu, Xiaoguang, and Liu, Tie{-}Yan.
\newblock On the depth of deep neural networks: {A} theoretical view.
\newblock In \emph{{AAAI}}, 2016.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and
  Le]{SutskeverVinyalsLe2014}
Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc~V.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{NIPS}, 2014.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{SzegedyEtal2015}
Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott~E.,
  Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich,
  Andrew.
\newblock Going deeper with convolutions.
\newblock In \emph{{CVPR}}, 2015.

\bibitem[Telgarsky(2016)]{Telgarsky2016}
Telgarsky, Matus.
\newblock Benefits of depth in neural networks.
\newblock In \emph{COLT}, 2016.

\bibitem[Zhang et~al.(2016)Zhang, Wu, Che, Lin, Memisevic, Salakhutdinov, and
  Bengio]{ZhangeEtAl2016}
Zhang, Saizheng, Wu, Yuhuai, Che, Tong, Lin, Zhouhan, Memisevic, Roland,
  Salakhutdinov, Ruslan, and Bengio, Yoshua.
\newblock Architectural complexity measures of recurrent neural networks.
\newblock \emph{CoRR}, 2016.

\bibitem[Zhang et~al.(2015)Zhang, Lee, and Jordan]{ZhangLeeJordan2015}
Zhang, Yuchen, Lee, Jason~D, and Jordan, Michael~I.
\newblock $\ell\_1$-regularized neural networks are improperly learnable in
  polynomial time.
\newblock \emph{arXiv:1510.03528}, 2015.

\bibitem[Zoph \& Le(2016)Zoph and Le]{ZophLe2016}
Zoph, Barret and Le, Quoc~V.
\newblock Neural architecture search with reinforcement learning.
\newblock \emph{CoRR}, 2016.

\end{thebibliography}
