\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ansari et~al.(2024)Ansari, Stella, Turkmen, Zhang, Mercado, Shen, Shchur, Rangapuram, Arango, Kapoor, Zschiegner, Maddix, Wang, Mahoney, Torkkola, Wilson, Bohlke-Schneider, and Wang]{ansari2024chronos}
A.~F. Ansari, L.~Stella, C.~Turkmen, X.~Zhang, P.~Mercado, H.~Shen, O.~Shchur, S.~S. Rangapuram, S.~P. Arango, S.~Kapoor, J.~Zschiegner, D.~C. Maddix, H.~Wang, M.~W. Mahoney, K.~Torkkola, A.~G. Wilson, M.~Bohlke-Schneider, and Y.~Wang.
\newblock Chronos: Learning the language of time series.
\newblock \emph{arXiv preprint arXiv:2403.07815}, 2024.

\bibitem[Borsos et~al.(2023)Borsos, Marinier, Vincent, Kharitonov, Pietquin, Sharifi, Roblek, Teboul, Grangier, Tagliasacchi, et~al.]{borsos2023audiolm}
Z.~Borsos, R.~Marinier, D.~Vincent, E.~Kharitonov, O.~Pietquin, M.~Sharifi, D.~Roblek, O.~Teboul, D.~Grangier, M.~Tagliasacchi, et~al.
\newblock Audiolm: a language modeling approach to audio generation.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 2023.

\bibitem[B{\"o}se et~al.(2017)B{\"o}se, Flunkert, Gasthaus, Januschowski, Lange, Salinas, Schelter, Seeger, and Wang]{bose2017probabilistic}
J.-H. B{\"o}se, V.~Flunkert, J.~Gasthaus, T.~Januschowski, D.~Lange, D.~Salinas, S.~Schelter, M.~Seeger, and Y.~Wang.
\newblock Probabilistic demand forecasting at scale.
\newblock \emph{VLDB}, 2017.

\bibitem[Cao et~al.(2024)Cao, Jia, Arik, Pfister, Zheng, Ye, and Liu]{cao2023tempo}
D.~Cao, F.~Jia, S.~O. Arik, T.~Pfister, Y.~Zheng, W.~Ye, and Y.~Liu.
\newblock Tempo: Prompt-based generative pre-trained transformer for time series forecasting.
\newblock In \emph{ICLR}, 2024.

\bibitem[Chang et~al.(2023)Chang, Peng, and Chen]{chang2024llm4ts}
C.~Chang, W.-C. Peng, and T.-F. Chen.
\newblock Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms.
\newblock \emph{arXiv preprint arXiv:2308.08469}, 2023.

\bibitem[Cheng and Chin(2024)]{cheng2024sociodojo}
J.~Cheng and P.~Chin.
\newblock Sociodojo: Building lifelong analytical agents with real-world text and time series.
\newblock In \emph{ICLR}, 2024.

\bibitem[Chow et~al.(2024)Chow, Gardiner, Hallgr{\'\i}msson, Xu, and Ren]{chow2024towards}
W.~Chow, L.~Gardiner, H.~T. Hallgr{\'\i}msson, M.~A. Xu, and S.~Y. Ren.
\newblock Towards time series reasoning with llms.
\newblock \emph{arXiv preprint arXiv:2409.11376}, 2024.

\bibitem[Chung et~al.(2014)Chung, Gulcehre, Cho, and Bengio]{chung2014empirical}
J.~Chung, C.~Gulcehre, K.~Cho, and Y.~Bengio.
\newblock Empirical evaluation of gated recurrent neural networks on sequence modeling.
\newblock \emph{arXiv preprint arXiv:1412.3555}, 2014.

\bibitem[Das et~al.(2024)Das, Kong, Sen, and Zhou]{das2023decoder}
A.~Das, W.~Kong, R.~Sen, and Y.~Zhou.
\newblock A decoder-only foundation model for time-series forecasting.
\newblock 2024.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Girdhar et~al.(2023)Girdhar, El-Nouby, Liu, Singh, Alwala, Joulin, and Misra]{girdhar2023imagebind}
R.~Girdhar, A.~El-Nouby, Z.~Liu, M.~Singh, K.~V. Alwala, A.~Joulin, and I.~Misra.
\newblock Imagebind: One embedding space to bind them all.
\newblock In \emph{CVPR}, 2023.

\bibitem[Godahewa et~al.(2021)Godahewa, Bergmeir, Webb, Hyndman, and Montero-Manso]{godahewa2021monash}
R.~Godahewa, C.~Bergmeir, G.~I. Webb, R.~J. Hyndman, and P.~Montero-Manso.
\newblock Monash time series forecasting archive.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Gruver et~al.(2023)Gruver, Finzi, Qiu, and Wilson]{gruver2024large}
N.~Gruver, M.~Finzi, S.~Qiu, and A.~G. Wilson.
\newblock Large language models are zero-shot time series forecasters.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Jin et~al.(2024{\natexlab{a}})Jin, Wang, Ma, Chu, Zhang, Shi, Chen, Liang, Li, Pan, et~al.]{jin2023time}
M.~Jin, S.~Wang, L.~Ma, Z.~Chu, J.~Y. Zhang, X.~Shi, P.-Y. Chen, Y.~Liang, Y.-F. Li, S.~Pan, et~al.
\newblock Time-llm: Time series forecasting by reprogramming large language models.
\newblock In \emph{ICLR}, 2024{\natexlab{a}}.

\bibitem[Jin et~al.(2024{\natexlab{b}})Jin, Zhang, Chen, Zhang, Liang, Yang, Wang, Pan, and Wen]{jin2024position}
M.~Jin, Y.~Zhang, W.~Chen, K.~Zhang, Y.~Liang, B.~Yang, J.~Wang, S.~Pan, and Q.~Wen.
\newblock Position paper: What can large language models tell us about time series analysis.
\newblock In \emph{ICML}, 2024{\natexlab{b}}.

\bibitem[J{\"o}rke et~al.(2024)J{\"o}rke, Sapkota, Warkenthien, Vainio, Schmiedmayer, Brunskill, and Landay]{jorke2024supporting}
M.~J{\"o}rke, S.~Sapkota, L.~Warkenthien, N.~Vainio, P.~Schmiedmayer, E.~Brunskill, and J.~Landay.
\newblock Supporting physical activity behavior change with llm-based conversational agents.
\newblock \emph{arXiv preprint arXiv:2405.06061}, 2024.

\bibitem[Lai et~al.(2018)Lai, Chang, Yang, and Liu]{lai2018modeling}
G.~Lai, W.-C. Chang, Y.~Yang, and H.~Liu.
\newblock Modeling long-and short-term temporal patterns with deep neural networks.
\newblock In \emph{SIGIR}, 2018.

\bibitem[Lee et~al.(2024)Lee, Park, and Lee]{lee2023learning}
S.~Lee, T.~Park, and K.~Lee.
\newblock Learning to embed time series patches independently.
\newblock In \emph{ICLR}, 2024.

\bibitem[Li et~al.(2024)Li, Luo, Wang, Chen, Liu, and He]{li2024reflective}
Y.~Li, B.~Luo, Q.~Wang, N.~Chen, X.~Liu, and B.~He.
\newblock A reflective llm-based agent to guide zero-shot cryptocurrency trading.
\newblock In \emph{EMNLP}, 2024.

\bibitem[Li et~al.(2023)Li, Qi, Li, and Xu]{li2023revisiting}
Z.~Li, S.~Qi, Y.~Li, and Z.~Xu.
\newblock Revisiting long-term time series forecasting: An investigation on linear mapping.
\newblock \emph{arXiv preprint arXiv:2305.10721}, 2023.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Guo, Dai, Li, Bao, Ren, Jiang, and Xia]{liu2024taming}
P.~Liu, H.~Guo, T.~Dai, N.~Li, J.~Bao, X.~Ren, Y.~Jiang, and S.-T. Xia.
\newblock Calf: Aligning llms for time series forecasting via cross-modal fine-tuning.
\newblock \emph{arXiv preprint arXiv:2403.07300}, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2023)Liu, McDuff, Kovacs, Galatzer-Levy, Sunshine, Zhan, Poh, Liao, Di~Achille, and Patel]{liuLargeLanguageModels2023}
X.~Liu, D.~McDuff, G.~Kovacs, I.~Galatzer-Levy, J.~Sunshine, J.~Zhan, M.-Z. Poh, S.~Liao, P.~Di~Achille, and S.~Patel.
\newblock Large language models are few-shot health learners.
\newblock \emph{arXiv preprint arXiv:2305.15525}, 2023.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Hu, Zhang, Wu, Wang, Ma, and Long]{liu2023itransformer}
Y.~Liu, T.~Hu, H.~Zhang, H.~Wu, S.~Wang, L.~Ma, and M.~Long.
\newblock {iTransformer}: Inverted transformers are effective for time series forecasting.
\newblock In \emph{ICLR}, 2024{\natexlab{b}}.

\bibitem[Merrill et~al.(2024)Merrill, Tan, Gupta, Hartvigsen, and Althoff]{merrill2024language}
M.~A. Merrill, M.~Tan, V.~Gupta, T.~Hartvigsen, and T.~Althoff.
\newblock Language models still struggle to zero-shot reason about time series.
\newblock In \emph{EMNLP}, 2024.

\bibitem[Morid et~al.(2023)Morid, Sheng, and Dunbar]{moridTimeSeriesPrediction2023}
M.~A. Morid, O.~R.~L. Sheng, and J.~Dunbar.
\newblock Time series prediction using deep learning methods in healthcare.
\newblock \emph{ACM Trans. Manage. Inf. Syst.}, 14\penalty0 (1), 2023.

\bibitem[Nie et~al.(2023)Nie, H.~Nguyen, Sinthong, and Kalagnanam]{Yuqietal-2023-PatchTST}
Y.~Nie, N.~H.~Nguyen, P.~Sinthong, and J.~Kalagnanam.
\newblock A time series is worth 64 words: Long-term forecasting with transformers.
\newblock In \emph{ICLR}, 2023.

\bibitem[Pan et~al.(2024)Pan, Jiang, Garg, Schneider, Nevmyvaka, and Song]{pan2024textbf}
Z.~Pan, Y.~Jiang, S.~Garg, A.~Schneider, Y.~Nevmyvaka, and D.~Song.
\newblock $s^2$ip-llm: Semantic space informed prompt learning with llm for time series forecasting.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rasul et~al.(2023)Rasul, Ashok, Williams, Khorasani, Adamopoulos, Bhagwatkar, Bilo{\v{s}}, Ghonia, Hassen, Schneider, et~al.]{rasul2024lagllama}
K.~Rasul, A.~Ashok, A.~R. Williams, A.~Khorasani, G.~Adamopoulos, R.~Bhagwatkar, M.~Bilo{\v{s}}, H.~Ghonia, N.~Hassen, A.~Schneider, et~al.
\newblock Lag-llama: Towards foundation models for time series forecasting.
\newblock In \emph{R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models}, 2023.

\bibitem[Sezer et~al.(2020)Sezer, Gudelek, and Ozbayoglu]{sezer2020financial}
O.~B. Sezer, M.~U. Gudelek, and A.~M. Ozbayoglu.
\newblock Financial time series forecasting with deep learning: A systematic literature review: 2005--2019.
\newblock \emph{Applied soft computing}, 90, 2020.

\bibitem[Sun et~al.(2024)Sun, Li, Li, and Hong]{sun2023test}
C.~Sun, Y.~Li, H.~Li, and S.~Hong.
\newblock Test: Text prototype aligned embedding to activate llm's ability for time series.
\newblock In \emph{ICLR}, 2024.

\bibitem[Talukder and Gkioxari(2023)]{talukder2023time}
S.~J. Talukder and G.~Gkioxari.
\newblock Time series modeling at scale: A universal representation across tasks and domains.
\newblock 2023.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix, B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar, et~al.
\newblock Llama: Open and efficient foundation language models (2023).
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Trindade(2015)]{misc_electricityloaddiagrams20112014_321}
A.~Trindade.
\newblock {ElectricityLoadDiagrams20112014}.
\newblock UCI Machine Learning Repository, 2015.
\newblock {DOI}: https://doi.org/10.24432/C58C86.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Ji, Wang, Sun, Liu, Kumar, and Lu]{wang2024stocktime}
S.~Wang, T.~Ji, L.~Wang, Y.~Sun, S.-C. Liu, A.~Kumar, and C.-T. Lu.
\newblock Stocktime: A time series specialized large language model architecture for stock price prediction.
\newblock \emph{arXiv preprint arXiv:2409.08281}, 2024{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Feng, Qiu, Gu, and Zhao]{wang2024news}
X.~Wang, M.~Feng, J.~Qiu, J.~Gu, and J.~Zhao.
\newblock From news to forecast: Iterative event reasoning in llm-based time series forecasting.
\newblock In \emph{Neural Information Processing Systems}, 2024{\natexlab{b}}.

\bibitem[Wimmer and Rekabsaz(2023)]{wimmer2023leveraging}
C.~Wimmer and N.~Rekabsaz.
\newblock Leveraging vision-language models for granular market change prediction.
\newblock \emph{arXiv preprint arXiv:2301.10166}, 2023.

\bibitem[Woo et~al.(2024)Woo, Liu, Kumar, Xiong, Savarese, and Sahoo]{woo2024unified}
G.~Woo, C.~Liu, A.~Kumar, C.~Xiong, S.~Savarese, and D.~Sahoo.
\newblock Unified training of universal time series forecasting transformers.
\newblock 2024.

\bibitem[Wu et~al.(2021)Wu, Xu, Wang, and Long]{wu2021autoformer}
H.~Wu, J.~Xu, J.~Wang, and M.~Long.
\newblock Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Wu et~al.(2023)Wu, Hu, Liu, Zhou, Wang, and Long]{wu2023timesnet}
H.~Wu, T.~Hu, Y.~Liu, H.~Zhou, J.~Wang, and M.~Long.
\newblock Timesnet: Temporal 2d-variation modeling for general time series analysis.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Xu et~al.(2024{\natexlab{a}})Xu, Bian, Zhong, Wen, and Xu]{xu2024beyond}
Z.~Xu, Y.~Bian, J.~Zhong, X.~Wen, and Q.~Xu.
\newblock Beyond trend and periodicity: Guiding time series forecasting with textual cues.
\newblock \emph{arXiv preprint arXiv:2405.13522}, 2024{\natexlab{a}}.

\bibitem[Xu et~al.(2024{\natexlab{b}})Xu, Zeng, and Xu]{xu2024fits}
Z.~Xu, A.~Zeng, and Q.~Xu.
\newblock Fits: Modeling time series with $10k$ parameters.
\newblock In \emph{ICLR}, 2024{\natexlab{b}}.

\bibitem[Xue and Salim(2023)]{xue2023promptcast}
H.~Xue and F.~D. Salim.
\newblock Promptcast: A new prompt-based learning paradigm for time series forecasting.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 2023.

\bibitem[Ye et~al.(2024)Ye, Zhang, Yang, Tang, Cao, Cai, and Liu]{beyondforecast}
W.~Ye, Y.~Zhang, W.~Yang, L.~Tang, D.~Cao, J.~Cai, and Y.~Liu.
\newblock Beyond forecasting: Compositional time series reasoning for end-to-end task execution, 2024.
\newblock URL \url{https://arxiv.org/abs/2410.04047}.

\bibitem[Zeng et~al.(2023)Zeng, Chen, Zhang, and Xu]{zeng2023transformers}
A.~Zeng, M.~Chen, L.~Zhang, and Q.~Xu.
\newblock Are transformers effective for time series forecasting?
\newblock In \emph{AAAI}, 2023.

\bibitem[Zhao and Shen(2024)]{LIFT}
L.~Zhao and Y.~Shen.
\newblock Rethinking channel dependence for multivariate time series forecasting: Learning from leading indicators.
\newblock In \emph{ICLR}, 2024.

\bibitem[Zhou et~al.(2021)Zhou, Zhang, Peng, Zhang, Li, Xiong, and Zhang]{zhou2021informer}
H.~Zhou, S.~Zhang, J.~Peng, S.~Zhang, J.~Li, H.~Xiong, and W.~Zhang.
\newblock Informer: Beyond efficient transformer for long sequence time-series forecasting.
\newblock In \emph{AAAI}, 2021.

\bibitem[Zhou et~al.(2022)Zhou, Ma, Wen, Wang, Sun, and Jin]{zhou2022fedformer}
T.~Zhou, Z.~Ma, Q.~Wen, X.~Wang, L.~Sun, and R.~Jin.
\newblock Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting.
\newblock In \emph{ICML}, 2022.

\bibitem[Zhou et~al.(2023)Zhou, Niu, Sun, Jin, et~al.]{zhou2024one}
T.~Zhou, P.~Niu, L.~Sun, R.~Jin, et~al.
\newblock One fits all: Power general time series analysis by pretrained lm.
\newblock In \emph{NeurIPS}, 2023.

\end{thebibliography}
