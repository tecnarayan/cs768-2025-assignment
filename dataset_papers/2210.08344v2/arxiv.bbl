\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bao et~al.(2022)Bao, Dong, Piao, and Wei]{beit}
Hangbo Bao, Li~Dong, Songhao Piao, and Furu Wei.
\newblock {BE}it: {BERT} pre-training of image transformers.
\newblock In \emph{ICLR}, 2022.

\bibitem[Cao et~al.(2022)Cao, Xu, and
  Clifton]{DBLP:journals/corr/abs-2202-03670}
Shuhao Cao, Peng Xu, and David~A. Clifton.
\newblock How to understand masked autoencoders.
\newblock \emph{CoRR}, 2022.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{swav}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
  Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Kornblith, Norouzi, and
  Hinton]{simclr}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{ICML}, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Kornblith, Swersky, Norouzi, and
  Hinton]{simclrv2}
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey
  Hinton.
\newblock Big self-supervised models are strong semi-supervised learners.
\newblock In \emph{NeurIPS}, 2020{\natexlab{b}}.

\bibitem[Chen and He(2021)]{simsiam}
Xinlei Chen and Kaiming He.
\newblock Exploring simple siamese representation learning.
\newblock In \emph{CVPR}, 2021.

\bibitem[Chung(1997)]{chung1997spectral}
Fan~RK Chung.
\newblock \emph{Spectral graph theory}, volume~92.
\newblock American Mathematical Soc., 1997.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL-HLT}, 2019.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Eckart and Young(1936)]{eckart1936approximation}
Carl Eckart and Gale Young.
\newblock The approximation of one matrix by another of lower rank.
\newblock \emph{Psychometrika}, 1\penalty0 (3):\penalty0 211--218, 1936.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond,
  Buchatskaya, Doersch, Pires, Guo, Azar, Piot, Kavukcuoglu, Munos, and
  Valko]{BYOL}
Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, C.~Tallec, Pierre~H.
  Richemond, Elena Buchatskaya, C.~Doersch, Bernardo~Avila Pires,
  Zhaohan~Daniel Guo, Mohammad~Gheshlaghi Azar, B.~Piot, K.~Kavukcuoglu,
  R{\'e}mi Munos, and Michal Valko.
\newblock Bootstrap your own latent: a new approach to self-supervised
  learning.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[HaoChen et~al.(2021)HaoChen, Wei, Gaidon, and Ma]{haochen}
Jeff~Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma.
\newblock Provable guarantees for self-supervised deep learning with spectral
  contrastive loss.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{moco}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{CVPR}, 2020.

\bibitem[He et~al.(2021)He, Chen, Xie, Li, Doll{\'a}r, and Girshick]{mae}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock \emph{arXiv preprint arXiv:2111.06377}, 2021.

\bibitem[Hua et~al.(2021)Hua, Wang, Xue, Ren, Wang, and Zhao]{hua2021feature}
Tianyu Hua, Wenxiao Wang, Zihui Xue, Sucheng Ren, Yue Wang, and Hang Zhao.
\newblock On feature decorrelation in self-supervised learning.
\newblock In \emph{CVPR}, 2021.

\bibitem[Jing et~al.(2020)Jing, Zbontar, et~al.]{jing2020implicit}
Li~Jing, Jure Zbontar, et~al.
\newblock Implicit rank-minimizing autoencoder.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Jing et~al.(2022)Jing, Vincent, LeCun, and
  Tian]{jing2021understanding}
Li~Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian.
\newblock Understanding dimensional collapse in contrastive self-supervised
  learning.
\newblock In \emph{ICLR}, 2022.

\bibitem[Kingma and Welling(2014)]{kingma2014auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational {Bayes}.
\newblock In \emph{ICLR}, 2014.

\bibitem[Kong et~al.(2020)Kong, d'Autume, Ling, Yu, Dai, and
  Yogatama]{kong2019mutual}
Lingpeng Kong, Cyprien de~Masson d'Autume, Wang Ling, Lei Yu, Zihang Dai, and
  Dani Yogatama.
\newblock A mutual information maximization perspective of language
  representation learning.
\newblock In \emph{ICLR}, 2020.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{cifar}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{InfoNCE}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Pan et~al.(2022)Pan, Zhou, and Yan]{pan2022towards}
Jiachun Pan, Pan Zhou, and Shuicheng Yan.
\newblock Towards understanding why mask-reconstruction pretraining helps in
  downstream tasks.
\newblock \emph{arXiv preprint arXiv:2206.03826}, 2022.

\bibitem[Roy and Vetterli(2007)]{roy2007effective}
Olivier Roy and Martin Vetterli.
\newblock The effective rank: A measure of effective dimensionality.
\newblock In \emph{2007 15th European signal processing conference}, pages
  606--610. IEEE, 2007.

\bibitem[Saunshi et~al.(2019)Saunshi, Plevrakis, Arora, Khodak, and
  Khandeparkar]{arora}
Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and
  Hrishikesh Khandeparkar.
\newblock A theoretical analysis of contrastive unsupervised representation
  learning.
\newblock In \emph{ICML}, 2019.

\bibitem[van~der Maaten and Hinton(2008)]{vandermaaten08a}
Laurens van~der Maaten and Geoffrey Hinton.
\newblock Visualizing data using t-sne.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0
  (86):\penalty0 2579--2605, 2008.

\bibitem[Wang et~al.(2021)Wang, Geng, Jiang, Li, Wang, Yang, and
  Lin]{wang2021residual}
Yifei Wang, Zhengyang Geng, Feng Jiang, Chuming Li, Yisen Wang, Jiansheng Yang,
  and Zhouchen Lin.
\newblock Residual relaxation for multi-view representation learning.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Wang et~al.(2022)Wang, Zhang, Wang, Yang, and Lin]{wang2022chaos}
Yifei Wang, Qi~Zhang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin.
\newblock Chaos is a ladder: A new theoretical understanding of contrastive
  learning via augmentation overlap.
\newblock In \emph{ICLR}, 2022.

\bibitem[Xie et~al.(2022)Xie, Zhang, Cao, Lin, Bao, Yao, Dai, and
  Hu]{xie2022simmim}
Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao,
  Qi~Dai, and Han Hu.
\newblock Simmim: A simple framework for masked image modeling.
\newblock In \emph{CVPR}, 2022.

\bibitem[Yun et~al.(2020)Yun, Bhojanapalli, Rawat, Reddi, and
  Kumar]{Yun2020Are}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank Reddi, and Sanjiv
  Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence
  functions?
\newblock In \emph{ICLR}, 2020.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{barlowtwins}
Jure Zbontar, Li~Jing, Ishan Misra, Yann LeCun, and St{\'e}phane Deny.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock In \emph{ICML}, 2021.

\bibitem[Zhou et~al.(2022)Zhou, Wei, Wang, Shen, Xie, Yuille, and Kong]{ibot}
Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao
  Kong.
\newblock Image {BERT} pre-training with online tokenizer.
\newblock In \emph{ICLR}, 2022.

\end{thebibliography}
