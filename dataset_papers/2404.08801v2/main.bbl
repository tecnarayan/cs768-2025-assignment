\begin{thebibliography}{80}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Baevski and Auli(2018)]{baevski2018adaptive}
Alexei Baevski and Michael Auli.
\newblock Adaptive input representations for neural language modeling.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, pages 7432--7439, 2020.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023.
\newblock URL \url{https://lmsys.org/blog/2023-03-30-vicuna/}.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova]{clark2019boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no questions.
\newblock \emph{arXiv preprint arXiv:1905.10044}, 2019.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem[Cooley and Tukey(1965)]{cooley1965algorithm}
James~W Cooley and John~W Tukey.
\newblock An algorithm for the machine calculation of complex fourier series.
\newblock \emph{Mathematics of computation}, 19\penalty0 (90):\penalty0 297--301, 1965.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and Salakhutdinov]{dai2019transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime~G Carbonell, Quoc Le, and Ruslan Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length context.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 2978--2988, 2019.

\bibitem[Dao(2024)]{dao2024flashattention}
Tri Dao.
\newblock Flashattention-2: Faster attention with better parallelism and work partitioning.
\newblock In \emph{International Conference on Learning Representations (ICLR-2024)}, 2024.

\bibitem[Dasigi et~al.(2021)Dasigi, Lo, Beltagy, Cohan, Smith, and Gardner]{dasigi-etal-2021-dataset}
Pradeep Dasigi, Kyle Lo, Iz~Beltagy, Arman Cohan, Noah~A. Smith, and Matt Gardner.
\newblock A dataset of information-seeking questions and answers anchored in research papers.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-2021)}, pages 4599--4610, Online, June 2021. Association for Computational Linguistics.

\bibitem[Davis et~al.(2021)Davis, Gu, Choromanski, Dao, Re, Finn, and Liang]{davis2021catformer}
Jared~Q Davis, Albert Gu, Krzysztof Choromanski, Tri Dao, Christopher Re, Chelsea Finn, and Percy Liang.
\newblock Catformer: Designing stable transformers via sensitivity analysis.
\newblock In \emph{International Conference on Machine Learning}, pages 2489--2499. PMLR, 2021.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern recognition}, pages 248--255. Ieee, 2009.

\bibitem[Fu et~al.(2023)Fu, Dao, Saab, Thomas, Rudra, and Re]{fu2023hungry}
Daniel~Y Fu, Tri Dao, Khaled~Kamal Saab, Armin~W Thomas, Atri Rudra, and Christopher Re.
\newblock Hungry hungry hippos: Towards language modeling with state space models.
\newblock In \emph{The Eleventh International Conference on Learning Representations (ICLR-2023)}, 2023.

\bibitem[Gu and Dao(2023)]{gu2023mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces, 2023.

\bibitem[Gu et~al.(2022{\natexlab{a}})Gu, Goel, and R{\'e}]{gu2022efficiently}
Albert Gu, Karan Goel, and Christopher R{\'e}.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In \emph{International Conference on Learning Representations (ICLR-2022)}, 2022{\natexlab{a}}.

\bibitem[Gu et~al.(2022{\natexlab{b}})Gu, Gupta, Goel, and R{\'e}]{gu2022parameterization}
Albert Gu, Ankit Gupta, Karan Goel, and Christopher R{\'e}.
\newblock On the parameterization and initialization of diagonal state space models.
\newblock \emph{arXiv preprint arXiv:2206.11893}, 2022{\natexlab{b}}.

\bibitem[Hanson and Pratt(1988)]{hanson1988comparing}
Stephen Hanson and Lorien Pratt.
\newblock Comparing biases for minimal network construction with back-propagation.
\newblock \emph{Advances in neural information processing systems}, 1, 1988.

\bibitem[Hawthorne et~al.(2022)Hawthorne, Jaegle, Cangea, Borgeaud, Nash, Malinowski, Dieleman, Vinyals, Botvinick, Simon, et~al.]{hawthorne2022general}
Curtis Hawthorne, Andrew Jaegle, C{\u{a}}t{\u{a}}lina Cangea, Sebastian Borgeaud, Charlie Nash, Mateusz Malinowski, Sander Dieleman, Oriol Vinyals, Matthew Botvinick, Ian Simon, et~al.
\newblock General-purpose, long-context autoregressive modeling with perceiver ar.
\newblock In \emph{International Conference on Machine Learning}, pages 8535--8558. PMLR, 2022.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Henry et~al.(2020)Henry, Dachapally, Pawar, and Chen]{henry2020query}
Alex Henry, Prudhvi~Raj Dachapally, Shubham~Shantaram Pawar, and Yuxuan Chen.
\newblock Query-key normalization for transformers.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2020}, pages 4246--4253, 2020.

\bibitem[Hua et~al.(2022)Hua, Dai, Liu, and Le]{hua2022transformer}
Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le.
\newblock Transformer quality in linear time.
\newblock In \emph{International Conference on Machine Learning (ICML-2022)}, pages 9099--9117. PMLR, 2022.

\bibitem[Huang et~al.(2019)Huang, Cheng, Bapna, Firat, Chen, Chen, Lee, Ngiam, Le, Wu, and Chen]{NEURIPS2019_093f65e0}
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc~V Le, Yonghui Wu, and zhifeng Chen.
\newblock Gpipe: Efficient training of giant neural networks using pipeline parallelism.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Hunter(1986)]{hunter1986exponentially}
J~Stuart Hunter.
\newblock The exponentially weighted moving average.
\newblock \emph{Journal of quality technology}, 18\penalty0 (4):\penalty0 203--210, 1986.

\bibitem[Hutchins et~al.(2022)Hutchins, Schlag, Wu, Dyer, and Neyshabur]{hutchins2022block}
DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur.
\newblock Block-recurrent transformers.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 33248--33261, 2022.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing internal covariate shift.
\newblock In \emph{International conference on machine learning (ICML-2015)}, pages 448--456. pmlr, 2015.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer]{joshi2017triviaqa}
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer.
\newblock Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}. Association for Computational Linguistics, 2017.

\bibitem[Kahan(1965)]{kahan1965pracniques}
William Kahan.
\newblock Pracniques: further remarks on reducing truncation errors.
\newblock \emph{Communications of the ACM}, 8\penalty0 (1):\penalty0 40, 1965.

\bibitem[Ko{\v{c}}isk{\'y} et~al.(2018)Ko{\v{c}}isk{\'y}, Schwarz, Blunsom, Dyer, Hermann, Melis, and Grefenstette]{kocisky-etal-2018-narrativeqa}
Tom{\'a}{\v{s}} Ko{\v{c}}isk{\'y}, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl~Moritz Hermann, G{\'a}bor Melis, and Edward Grefenstette.
\newblock The {N}arrative{QA} reading comprehension challenge.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 6:\penalty0 317--328, 2018.

\bibitem[Krizhevsky et~al.(2009)]{krizhevsky2009learning}
Alex Krizhevsky et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Technical Report. University of Toronto}, 2009.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins, Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee, et~al.]{kwiatkowski2019natural}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et~al.
\newblock Natural questions: a benchmark for question answering research.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 7:\penalty0 453--466, 2019.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Hu, Nie, Han, Jiang, Guo, and Liu]{Li_2023_CVPR}
Bonan Li, Yinhan Hu, Xuecheng Nie, Congying Han, Xiangjian Jiang, Tiande Guo, and Luoqi Liu.
\newblock Dropkey for vision transformer.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 22700--22709, June 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Shao, Xie, Xing, Gonzalez, Stoica, Ma, and Zhang]{li2023lightseq}
Dacheng Li, Rulin Shao, Anze Xie, Eric~P Xing, Joseph~E Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang.
\newblock Lightseq:: Sequence level parallelism for distributed training of long context transformers.
\newblock In \emph{Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023)}, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2023{\natexlab{c}})Li, Cai, Zhang, Chen, and Dey]{li2023makes}
Yuhong Li, Tianle Cai, Yi~Zhang, Deming Chen, and Debadeepta Dey.
\newblock What makes convolutional models great on long sequence modeling?
\newblock In \emph{International Conference on Learning Representations (ICLR-2023)}, 2023{\natexlab{c}}.

\bibitem[Linsley et~al.(2018)Linsley, Kim, Veerabadran, Windolf, and Serre]{drew2018advances}
Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, and Thomas Serre.
\newblock Learning long-range spatial dependencies with horizontal gated recurrent units.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi, and R.~Garnett, editors, \emph{Advances in Neural Information Processing Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem[Liu et~al.(2024)Liu, Zaharia, and Abbeel]{liu2024ring}
Hao Liu, Matei Zaharia, and Pieter Abbeel.
\newblock Ring attention with blockwise transformers for near-infinite context.
\newblock In \emph{International Conference on Learning Representations (ICLR-2024)}, 2024.

\bibitem[Liu et~al.(2022)Liu, Hu, Lin, Yao, Xie, Wei, Ning, Cao, Zhang, Dong, et~al.]{liu2022swin}
Ze~Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li~Dong, et~al.
\newblock Swin transformer v2: Scaling up capacity and resolution.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 12009--12019, 2022.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2019decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Luo et~al.(2018)Luo, Zhan, Xue, Wang, Ren, and Yang]{luo2018cosine}
Chunjie Luo, Jianfeng Zhan, Xiaohe Xue, Lei Wang, Rui Ren, and Qiang Yang.
\newblock Cosine normalization: Using cosine similarity instead of dot product in neural networks.
\newblock In \emph{27th International Conference on Artificial Neural Networks (ICANN-2018)}, pages 382--391. Springer, 2018.

\bibitem[Ma et~al.(2021)Ma, Kong, Wang, Zhou, May, Ma, and Zettlemoyer]{ma2021luna}
Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer.
\newblock Luna: Linear unified nested attention.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 2441--2453, 2021.

\bibitem[Ma et~al.(2023)Ma, Zhou, Kong, He, Gui, Neubig, May, and Zettlemoyer]{ma2023mega}
Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.
\newblock Mega: Moving average equipped gated attention.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and Potts]{maas2011learning}
Andrew Maas, Raymond~E Daly, Peter~T Pham, Dan Huang, Andrew~Y Ng, and Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies}, pages 142--150, 2011.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and Socher]{merity2017pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock In \emph{International Conference on Learning Representations (ICLR-2017)}, 2017.

\bibitem[Mesnard et~al.(2024)Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak, Sifre, Rivière, Kale, Love, Tafti, Hussenot, Chowdhery, Roberts, Barua, Botev, Castro-Ros, Slone, Héliou, Tacchetti, Bulanova, Paterson, Tsai, Shahriari, Lan, Choquette-Choo, Crepy, Cer, Ippolito, Reid, Buchatskaya, Ni, Noland, Yan, Tucker, Muraru, Rozhdestvenskiy, Michalewski, Tenney, Grishchenko, Austin, Keeling, Labanowski, Lespiau, Stanway, Brennan, Chen, Ferret, Chiu, Mao-Jones, Lee, Yu, Millican, Sjoesund, Lee, Dixon, Reid, Mikuła, Wirth, Sharman, Chinaev, Thain, Bachem, Chang, Wahltinez, Bailey, Michel, Yotov, Sessa, Chaabouni, Comanescu, Jana, Anil, McIlroy, Liu, Mullins, Smith, Borgeaud, Girgin, Douglas, Pandya, Shakeri, De, Klimenko, Hennigan, Feinberg, Stokowiec, hui Chen, Ahmed, Gong, Warkentin, Peran, Giang, Farabet, Vinyals, Dean, Kavukcuoglu, Hassabis, Ghahramani, Eck, Barral, Pereira, Collins, Joulin, Fiedel, Senter, Andreev, and Kenealy]{team2024gemma}
Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir~Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline~Le Lan, Christopher~A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars~Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige
  Bailey, Paul Michel, Petko Yotov, Pier~Giuseppe Sessa, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel~L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu~hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy.
\newblock Gemma: Open models based on gemini research and technology, 2024.

\bibitem[MosaicML(2023)]{mpt7b}
MosaicML.
\newblock Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023.

\bibitem[Nangia and Bowman(2018)]{nangia2018listops}
Nikita Nangia and Samuel Bowman.
\newblock Listops: A diagnostic dataset for latent tree learning.
\newblock In \emph{Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop}, pages 92--99, 2018.

\bibitem[Nijkamp et~al.(2023)Nijkamp, Xie, Hayashi, Pang, Xia, Xing, Vig, Yavuz, Laban, Krause, Purushwalkam, Niu, Kryściński, Murakhovs'ka, Choubey, Fabbri, Liu, Meng, Tu, Bhat, Wu, Savarese, Zhou, Joty, and Xiong]{nijkamp2023xgen7b}
Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo~Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryściński, Lidiya Murakhovs'ka, Prafulla~Kumar Choubey, Alex Fabbri, Ye~Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Joty, and Caiming Xiong.
\newblock Xgen-7b technical report, 2023.

\bibitem[Parisotto et~al.(2020)Parisotto, Song, Rae, Pascanu, Gulcehre, Jayakumar, Jaderberg, Kaufman, Clark, Noury, et~al.]{parisotto2020stabilizing}
Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg, Raphael~Lopez Kaufman, Aidan Clark, Seb Noury, et~al.
\newblock Stabilizing transformers for reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages 7487--7498. PMLR, 2020.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Peng et~al.(2023)Peng, Alcaide, Anthony, Albalak, Arcadinho, Cao, Cheng, Chung, Grella, GV, et~al.]{peng2023rwkv}
Bo~Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi~Kiran GV, et~al.
\newblock Rwkv: Reinventing rnns for the transformer era.
\newblock \emph{arXiv preprint arXiv:2305.13048}, 2023.

\bibitem[Peng et~al.(2024)Peng, Quesnelle, Fan, and Shippole]{peng2024yarn}
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
\newblock Ya{RN}: Efficient context window extension of large language models.
\newblock In \emph{International Conference on Learning Representations (ICLR-2024)}, 2024.

\bibitem[Poli et~al.(2023)Poli, Massaroli, Nguyen, Fu, Dao, Baccus, Bengio, Ermon, and R{\'e}]{poli2023hyena}
Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel~Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R{\'e}.
\newblock Hyena hierarchy: Towards larger convolutional language models.
\newblock In \emph{International conference on machine learning (ICML-2023)}. PMLR, 2023.

\bibitem[Radev et~al.(2013)Radev, Muthukrishnan, Qazvinian, and Abu-Jbara]{radev2013acl}
Dragomir~R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara.
\newblock The acl anthology network corpus.
\newblock \emph{Language Resources and Evaluation}, 47\penalty0 (4):\penalty0 919--944, 2013.

\bibitem[Rae et~al.(2019)Rae, Potapenko, Jayakumar, Hillier, and Lillicrap]{raecompressive2019}
Jack~W Rae, Anna Potapenko, Siddhant~M Jayakumar, Chloe Hillier, and Timothy~P Lillicrap.
\newblock Compressive transformers for long-range sequence modelling.
\newblock \emph{arXiv preprint}, 2019.
\newblock URL \url{https://arxiv.org/abs/1911.05507}.

\bibitem[Rae et~al.(2020)Rae, Potapenko, Jayakumar, Hillier, and Lillicrap]{rae2020compressive}
Jack~W Rae, Anna Potapenko, Siddhant~M Jayakumar, Chloe Hillier, and Timothy~P Lillicrap.
\newblock Compressive transformers for long-range sequence modeling.
\newblock In \emph{International Conference on Learning Representations (ICLR-2020)}, 2020.

\bibitem[Ramachandran et~al.(2017)Ramachandran, Zoph, and Le]{ramachandran2017swish}
Prajit Ramachandran, Barret Zoph, and Quoc~V Le.
\newblock Swish: a self-gated activation function.
\newblock \emph{arXiv preprint arXiv:1710.05941}, 7\penalty0 (1):\penalty0 5, 2017.

\bibitem[Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and Choi]{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64\penalty0 (9):\penalty0 99--106, 2021.

\bibitem[Sap et~al.(2019)Sap, Rashkin, Chen, LeBras, and Choi]{sap2019socialiqa}
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.
\newblock Socialiqa: Commonsense reasoning about social interactions.
\newblock \emph{arXiv preprint arXiv:1904.09728}, 2019.

\bibitem[Shaham et~al.(2022)Shaham, Segal, Ivgi, Efrat, Yoran, Haviv, Gupta, Xiong, Geva, Berant, and Levy]{shaham-etal-2022-scrolls}
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy.
\newblock {SCROLLS}: Standardized {C}ompa{R}ison over long language sequences.
\newblock In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP-2022)}, pages 12007--12021, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.

\bibitem[Shazeer(2020)]{shazeer2020glu}
Noam Shazeer.
\newblock Glu variants improve transformer.
\newblock \emph{arXiv preprint arXiv:2002.05202}, 2020.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro]{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Su et~al.(2021)Su, Lu, Pan, Wen, and Liu]{su2021roformer}
Jianlin Su, Yu~Lu, Shengfeng Pan, Bo~Wen, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{arXiv preprint arXiv:2104.09864}, 2021.

\bibitem[Tay et~al.(2020)Tay, Dehghani, Bahri, and Metzler]{tay2020efficient}
Yi~Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
\newblock Efficient transformers: A survey.
\newblock \emph{arXiv preprint arXiv:2009.06732}, 2020.

\bibitem[Tay et~al.(2021)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang, Ruder, and Metzler]{tay2021long}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena : A benchmark for efficient transformers.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=qVyeW-grC2k}.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Abnar, Chung, Fedus, Rao, Narang, Tran, Yogatama, and Metzler]{tay2022scaling}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Hyung~Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh~Q. Tran, Dani Yogatama, and Donald Metzler.
\newblock Scaling laws vs model architectures: How does inductive bias influence scaling?, 2022.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and J{\'e}gou]{touvron2021training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through attention.
\newblock In \emph{International Conference on Machine Learning}, pages 10347--10357. PMLR, 2021.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{Vaswani+2017}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Wang et~al.(2024)Wang, Salmani, Omidi, Ren, Rezagholizadeh, and Eshaghi]{wang2024limits}
Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, and Armaghan Eshaghi.
\newblock Beyond the limits: A survey of techniques to extend the context length in large language models, 2024.

\bibitem[Warden(2018)]{warden2018speech}
Pete Warden.
\newblock Speech commands: A dataset for limited-vocabulary speech recognition.
\newblock \emph{arXiv preprint arXiv:1804.03209}, 2018.

\bibitem[Welford(1962)]{welford1962note}
B.~P. Welford.
\newblock Note on a method for calculating corrected sums of squares and products.
\newblock \emph{Technometrics}, 4\penalty0 (3):\penalty0 419--420, 1962.

\bibitem[Wu and He(2018)]{wu2018group}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European conference on computer vision (ECCV-2018)}, pages 3--19, 2018.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan, Wang, and Liu]{xiong2020layer}
Ruibin Xiong, Yunchang Yang, Di~He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{International Conference on Machine Learning}, pages 10524--10533. PMLR, 2020.

\bibitem[Xiong et~al.(2023)Xiong, Liu, Molybog, Zhang, Bhargava, Hou, Martin, Rungta, Sankararaman, Oguz, et~al.]{xiong2023effective}
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik~Abinav Sankararaman, Barlas Oguz, et~al.
\newblock Effective long-context scaling of foundation models.
\newblock \emph{arXiv preprint arXiv:2309.16039}, 2023.

\bibitem[Xu et~al.(2020)Xu, Liu, Xiong, and van Genabith]{xu2020transformer}
Hongfei Xu, Qiuhui Liu, Deyi Xiong, and Josef van Genabith.
\newblock Transformer with depth-wise lstm.
\newblock \emph{arXiv preprint arXiv:2007.06257}, 2020.

\bibitem[Yu et~al.(2024)Yu, Simig, Flaherty, Aghajanyan, Zettlemoyer, and Lewis]{yu2024megabyte}
Lili Yu, D{\'a}niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis.
\newblock Megabyte: Predicting million-byte sequences with multiscale transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL-2019)}. Association for Computational Linguistics, 2019.

\bibitem[Zhang and Sennrich(2019)]{zhang2019root}
Biao Zhang and Rico Sennrich.
\newblock Root mean square layer normalization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Zhong et~al.(2021)Zhong, Yin, Yu, Zaidi, Mutuma, Jha, Awadallah, Celikyilmaz, Liu, Qiu, and Radev]{zhong-etal-2021-qmsum}
Ming Zhong, Da~Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed~Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev.
\newblock {QMS}um: A new benchmark for query-based multi-domain meeting summarization.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-2021)}, pages 5905--5921, Online, June 2021. Association for Computational Linguistics.

\bibitem[Zhou et~al.(2024)Zhou, Alon, Chen, Wang, Agarwal, and Zhou]{zhou2024transformers}
Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, and Denny Zhou.
\newblock Transformers can achieve length generalization but not robustly, 2024.

\end{thebibliography}
