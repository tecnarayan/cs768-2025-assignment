\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bishop(2006)]{bishop2006pattern}
Bishop, C.~M.
\newblock \emph{Pattern recognition and machine learning}.
\newblock Information Science and Statistics. Springer, 2006.

\bibitem[Blumer et~al.(1987)Blumer, Ehrenfeucht, Haussler, and
  Warmuth]{blumer1987occam}
Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M.~K.
\newblock Occam's razor.
\newblock \emph{Information processing letters}, 24\penalty0 (6):\penalty0
  377--380, 1987.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell2015weight}
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D.
\newblock Weight uncertainty in neural networks.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning}, pp.\  1613--1622, 2015.

\bibitem[Botev et~al.(2017)Botev, Ritter, and Barber]{botev2017practical}
Botev, A., Ritter, H., and Barber, D.
\newblock Practical {G}auss-{N}ewton optimisation for deep learning.
\newblock In \emph{International Conference on Machine Learning}, International
  Convention Centre, Sydney, Australia, 2017. PMLR.

\bibitem[Bottou(2010)]{bottou2010large}
Bottou, L.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In \emph{Proceedings of COMPSTAT'2010}, pp.\  177--186. Springer,
  2010.

\bibitem[Buntine \& Weigend(1991)Buntine and Weigend]{buntine1991bayesian}
Buntine, W.~L. and Weigend, A.~S.
\newblock Bayesian back-propagation.
\newblock \emph{Complex systems}, 5\penalty0 (6):\penalty0 603--643, 1991.

\bibitem[Damianou \& Lawrence(2013)Damianou and Lawrence]{deepgp2013}
Damianou, A. and Lawrence, N.~D.
\newblock Deep {G}aussian processes.
\newblock In \emph{Proceedings of the Sixteenth International Conference on
  Artificial Intelligence and Statistics}. PMLR, 2013.

\bibitem[Dangel et~al.(2019)Dangel, Kunstner, and Hennig]{dangel2019backpack}
Dangel, F., Kunstner, F., and Hennig, P.
\newblock Backpack: Packing more into backprop.
\newblock In \emph{Proceedings of 7th International Conference on Learning
  Representations}, 2019.

\bibitem[Dua \& Graff(2017)Dua and Graff]{ucidata}
Dua, D. and Graff, C.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Dutordoir et~al.(2020)Dutordoir, van~der Wilk, Artemev, and
  Hensman]{deep_gp_image_classification_2020}
Dutordoir, V., van~der Wilk, M., Artemev, A., and Hensman, J.
\newblock Bayesian image classification with deep convolutional gaussian
  processes.
\newblock In \emph{Proceedings of the Twenty Third International Conference on
  Artificial Intelligence and Statistics}, 2020.

\bibitem[Dziugaite \& Roy(2017)Dziugaite and Roy]{dziugaite2017computing}
Dziugaite, G.~K. and Roy, D.~M.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{arXiv preprint arXiv:1703.11008}, 2017.

\bibitem[Fong \& Holmes(2020)Fong and Holmes]{fong2020marginal}
Fong, E. and Holmes, C.
\newblock On the marginal likelihood and cross-validation.
\newblock \emph{Biometrika}, 107\penalty0 (2):\penalty0 489--496, 2020.

\bibitem[Foong et~al.(2019)Foong, Li, Hern{\'a}ndez-Lobato, and
  Turner]{foong2019between}
Foong, A.~Y., Li, Y., Hern{\'a}ndez-Lobato, J.~M., and Turner, R.~E.
\newblock 'in-between'uncertainty in bayesian neural networks.
\newblock \emph{arXiv preprint arXiv:1906.11537}, 2019.

\bibitem[Foresee \& Hagan(1997)Foresee and Hagan]{foresee1997gauss}
Foresee, F.~D. and Hagan, M.~T.
\newblock Gauss-newton approximation to bayesian learning.
\newblock In \emph{International Conference on Neural Networks (ICNN'97)},
  volume~3, pp.\  1930--1935. IEEE, 1997.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1321--1330. PMLR, 2017.

\bibitem[Harville(1998)]{harville1998matrix}
Harville, D.~A.
\newblock Matrix algebra from a statistician's perspective, 1998.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016residual}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016}, pp.\
  770--778. {IEEE} Computer Society, 2016.

\bibitem[Hern{\'a}ndez-Lobato \& Adams(2015)Hern{\'a}ndez-Lobato and
  Adams]{hernandez2015probabilistic}
Hern{\'a}ndez-Lobato, J.~M. and Adams, R.
\newblock Probabilistic backpropagation for scalable learning of bayesian
  neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1861--1869. PMLR, 2015.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997flat}
Hochreiter, S. and Schmidhuber, J.
\newblock Flat minima.
\newblock \emph{Neural Computation}, 9\penalty0 (1):\penalty0 1--42, 1997.

\bibitem[Immer et~al.(2021)Immer, Korzepa, and Bauer]{immer2020improving}
Immer, A., Korzepa, M., and Bauer, M.
\newblock Improving predictions of bayesian neural nets via local
  linearization.
\newblock In \emph{Proceedings of The 24th International Conference on
  Artificial Intelligence and Statistics}, pp.\  703--711, 2021.

\bibitem[Jefferys \& Berger(1992)Jefferys and
  Berger]{Jefferys1992_occams_razor}
Jefferys, W.~H. and Berger, J.~O.
\newblock Ockham's razor and bayesian analysis.
\newblock \emph{American Scientist}, 80\penalty0 (1):\penalty0 64--72, 1992.
\newblock ISSN 00030996.

\bibitem[Jiang et~al.(2019)Jiang, Neyshabur, Mobahi, Krishnan, and
  Bengio]{jiang2019fantastic}
Jiang, Y., Neyshabur, B., Mobahi, H., Krishnan, D., and Bengio, S.
\newblock Fantastic generalization measures and where to find them.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Kendall(1948)]{kendall1948rank}
Kendall, M.~G.
\newblock Rank correlation methods.
\newblock 1948.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Khan et~al.(2018)Khan, Nielsen, Tangkaratt, Lin, Gal, and
  Srivastava]{khan2018fast}
Khan, M., Nielsen, D., Tangkaratt, V., Lin, W., Gal, Y., and Srivastava, A.
\newblock Fast and scalable bayesian deep learning by weight-perturbation in
  adam.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2611--2620, 2018.

\bibitem[Khan et~al.(2019)Khan, Immer, Abedi, and Korzepa]{khan2019approximate}
Khan, M. E.~E., Immer, A., Abedi, E., and Korzepa, M.
\newblock Approximate inference turns deep networks into gaussian processes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3088--3098, 2019.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kristiadi et~al.(2020)Kristiadi, Hein, and Hennig]{kristiadi2020being}
Kristiadi, A., Hein, M., and Hennig, P.
\newblock Being bayesian, even just a bit, fixes overconfidence in relu
  networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5436--5446. PMLR, 2020.

\bibitem[Kunstner et~al.(2019)Kunstner, Hennig, and
  Balles]{kunstner2019limitations}
Kunstner, F., Hennig, P., and Balles, L.
\newblock Limitations of the empirical fisher approximation for natural
  gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4158--4169, 2019.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2017simple}
Lakshminarayanan, B., Pritzel, A., and Blundell, C.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  6402--6413, 2017.

\bibitem[Llorente et~al.(2020)Llorente, Martino, Delgado, and
  Lopez-Santiago]{llorente2020marginal}
Llorente, F., Martino, L., Delgado, D., and Lopez-Santiago, J.
\newblock Marginal likelihood computation for model selection and hypothesis
  testing: an extensive review.
\newblock \emph{arXiv preprint arXiv:2005.08334}, 2020.

\bibitem[Lyle et~al.(2020)Lyle, Schut, Ru, Gal, and van~der
  Wilk]{lyle2020bayesian}
Lyle, C., Schut, L., Ru, R., Gal, Y., and van~der Wilk, M.
\newblock A bayesian perspective on training speed and model selection.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[MacKay(1992)]{mackay1992practical}
MacKay, D.~J.
\newblock A practical bayesian framework for backpropagation networks.
\newblock \emph{Neural computation}, 4\penalty0 (3):\penalty0 448--472, 1992.

\bibitem[MacKay(1995)]{mackay1995probable}
MacKay, D.~J.
\newblock Probable networks and plausible predictionsâ€”a review of practical
  bayesian methods for supervised neural networks.
\newblock \emph{Network: computation in neural systems}, 6\penalty0
  (3):\penalty0 469--505, 1995.

\bibitem[MacKay(2003)]{mackay2003information}
MacKay, D.~J.
\newblock \emph{Information theory, inference and learning algorithms}.
\newblock Cambridge university press, 2003.

\bibitem[Maddox et~al.(2019)Maddox, Izmailov, Garipov, Vetrov, and
  Wilson]{maddox2019simple}
Maddox, W.~J., Izmailov, P., Garipov, T., Vetrov, D.~P., and Wilson, A.~G.
\newblock A simple baseline for bayesian uncertainty in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  13132--13143, 2019.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{martens2015optimizing}
Martens, J. and Grosse, R.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pp.\
  2408--2417, 2015.

\bibitem[Mascarenhas(2014)]{mascarenhas2014divergence}
Mascarenhas, W.~F.
\newblock The divergence of the bfgs and gauss newton methods.
\newblock \emph{Mathematical Programming}, 147\penalty0 (1):\penalty0 253--276,
  2014.

\bibitem[Neal(1995)]{neal1995bayesian}
Neal, R.~M.
\newblock \emph{Bayesian Learning for Neural Networks}.
\newblock PhD thesis, University of Toronto, 1995.

\bibitem[Neyman \& Pearson(1933)Neyman and Pearson]{neyman1933ix}
Neyman, J. and Pearson, E.~S.
\newblock Ix. on the problem of the most efficient tests of statistical
  hypotheses.
\newblock \emph{Philosophical Transactions of the Royal Society of London.
  Series A, Containing Papers of a Mathematical or Physical Character},
  231\penalty0 (694-706):\penalty0 289--337, 1933.

\bibitem[Osawa et~al.(2019)Osawa, Swaroop, Khan, Jain, Eschenhagen, Turner, and
  Yokota]{osawa2019practical}
Osawa, K., Swaroop, S., Khan, M. E.~E., Jain, A., Eschenhagen, R., Turner,
  R.~E., and Yokota, R.
\newblock Practical deep learning with bayesian principles.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4289--4301, 2019.

\bibitem[Rasmussen \& Ghahramani(2001)Rasmussen and
  Ghahramani]{rasmussen2001occam}
Rasmussen, C.~E. and Ghahramani, Z.
\newblock Occam's razor.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  294--300, 2001.

\bibitem[Rasmussen \& Williams(2006)Rasmussen and
  Williams]{rasmussen2006gaussian}
Rasmussen, C.~E. and Williams, C.~K.
\newblock \emph{Gaussian processes for machine learning}.
\newblock MIT press Cambridge, MA, 2006.

\bibitem[R{\"a}tsch et~al.(2001)R{\"a}tsch, Onoda, and
  M{\"u}ller]{ratsch2001soft}
R{\"a}tsch, G., Onoda, T., and M{\"u}ller, K.-R.
\newblock Soft margins for adaboost.
\newblock \emph{Machine learning}, 42\penalty0 (3):\penalty0 287--320, 2001.

\bibitem[Ritter et~al.(2018)Ritter, Botev, and Barber]{ritter2018scalable}
Ritter, H., Botev, A., and Barber, D.
\newblock A scalable laplace approximation for neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Robbins(1955)]{robbins1955empirical}
Robbins, H.
\newblock \emph{An empirical Bayes approach to statistics}.
\newblock Office of Scientific Research, US Air Force, 1955.

\bibitem[Schneider et~al.(2018)Schneider, Balles, and
  Hennig]{schneider2018deepobs}
Schneider, F., Balles, L., and Hennig, P.
\newblock Deepobs: A deep learning optimizer benchmark suite.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Snelson(2007)]{snelson2007flexible}
Snelson, E.~L.
\newblock \emph{Flexible and efficient Gaussian process models for machine
  learning}.
\newblock PhD thesis, UCL (University College London), 2007.

\bibitem[van~der Wilk et~al.(2018)van~der Wilk, Bauer, John, and
  Hensman]{van2018learning}
van~der Wilk, M., Bauer, M., John, S., and Hensman, J.
\newblock Learning invariances using the marginal likelihood.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9938--9948, 2018.

\bibitem[Wenzel et~al.(2020)Wenzel, Roth, Veeling, {\'S}wiatkowski, Tran,
  Mandt, Snoek, Salimans, Jenatton, and Nowozin]{wenzel2020good}
Wenzel, F., Roth, K., Veeling, B.~S., {\'S}wiatkowski, J., Tran, L., Mandt, S.,
  Snoek, J., Salimans, T., Jenatton, R., and Nowozin, S.
\newblock How good is the bayes posterior in deep neural networks really?
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoryuko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock In \emph{{BMVC}}. {BMVA} Press, 2016.

\bibitem[Zhang et~al.(2018)Zhang, Sun, Duvenaud, and Grosse]{zhang2018noisy}
Zhang, G., Sun, S., Duvenaud, D., and Grosse, R.
\newblock Noisy natural gradient as variational inference.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5852--5861, 2018.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Wang, Xu, and
  Grosse]{zhang2019weight}
Zhang, G., Wang, C., Xu, B., and Grosse, R.~B.
\newblock Three mechanisms of weight decay regularization.
\newblock In \emph{{ICLR} (Poster)}. OpenReview.net, 2019{\natexlab{a}}.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Dauphin, and
  Ma]{zhang2019fixup}
Zhang, H., Dauphin, Y.~N., and Ma, T.
\newblock Fixup initialization: Residual learning without normalization.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.

\end{thebibliography}
