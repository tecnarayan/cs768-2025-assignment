\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu and Orecchia(2014)]{allen2014linear}
Z.~Allen-Zhu and L.~Orecchia.
\newblock Linear coupling: An ultimate unification of gradient and mirror
  descent.
\newblock \emph{arXiv preprint arXiv:1407.1537}, 2014.

\bibitem[Anil et~al.(2019)Anil, Gupta, Koren, and Singer]{anil2019memory}
R.~Anil, V.~Gupta, T.~Koren, and Y.~Singer.
\newblock Memory-efficient adaptive optimization for large-scale learning.
\newblock \emph{arXiv preprint arXiv:1901.11150}, 2019.

\bibitem[Arora et~al.(2018)Arora, Li, and Lyu]{arora2018theoretical}
S.~Arora, Z.~Li, and K.~Lyu.
\newblock Theoretical analysis of auto rate-tuning by batch normalization.
\newblock \emph{arXiv preprint arXiv:1812.03981}, 2018.

\bibitem[Bapna et~al.(2019)Bapna, Cherry, Lepikhin, Foster, Krikun, Johnson,
  Chen, Ari, Firat, Macherey, Wu, Cao, and Chen]{bapna2019massively}
A.~Bapna, C.~A. Cherry, D.~D. Lepikhin, G.~Foster, M.~Krikun, M.~Johnson,
  M.~Chen, N.~Ari, O.~Firat, W.~Macherey, Y.~Wu, Y.~Cao, and Z.~Chen.
\newblock Massively multilingual neural machine translation in the wild:
  Findings and challenges, 2019.

\bibitem[Ben-Nun and Hoefler(2019)]{ben2019demystifying}
T.~Ben-Nun and T.~Hoefler.
\newblock Demystifying parallel and distributed deep learning: An in-depth
  concurrency analysis.
\newblock \emph{ACM Computing Surveys (CSUR)}, 52\penalty0 (4):\penalty0 1--43,
  2019.

\bibitem[Ben-Tal and Nemirovski(2001)]{ben2001lectures}
A.~Ben-Tal and A.~Nemirovski.
\newblock \emph{Lectures on modern convex optimization: analysis, algorithms,
  and engineering applications}.
\newblock SIAM, 2001.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet2002stability}
O.~Bousquet and A.~Elisseeff.
\newblock Stability and generalization.
\newblock \emph{Journal of machine learning research}, 2\penalty0
  (Mar):\penalty0 499--526, 2002.

\bibitem[Boyd and Vandenberghe(2004)]{boyd2004convex}
S.~Boyd and L.~Vandenberghe.
\newblock \emph{Convex Optimization}.
\newblock Cambridge University Press, 2004.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
T.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Chen et~al.(2019)Chen, Agarwal, Hazan, Zhang, and
  Zhang]{chen2019extreme}
X.~Chen, N.~Agarwal, E.~Hazan, C.~Zhang, and Y.~Zhang.
\newblock Extreme tensoring for low-memory preconditioning.
\newblock \emph{arXiv preprint arXiv:1902.04620}, 2019.

\bibitem[Chen et~al.(2018)Chen, Jin, and Yu]{chen2018stability}
Y.~Chen, C.~Jin, and B.~Yu.
\newblock Stability and convergence trade-off of iterative optimization
  algorithms.
\newblock \emph{arXiv preprint arXiv:1804.01619}, 2018.

\bibitem[Chien et~al.(2018)Chien, Markidis, Sishtla, Santos, Herman,
  Narasimhamurthy, and Laure]{chien2018characterizing}
S.~W. Chien, S.~Markidis, C.~P. Sishtla, L.~Santos, P.~Herman,
  S.~Narasimhamurthy, and E.~Laure.
\newblock Characterizing deep-learning i/o workloads in tensorflow.
\newblock In \emph{2018 IEEE/ACM 3rd International Workshop on Parallel Data
  Storage \& Data Intensive Scalable Computing Systems (PDSW-DISCS)}, pages
  54--63. IEEE, 2018.

\bibitem[Choi et~al.(2019)Choi, Passos, Shallue, and Dahl]{choi2019faster}
D.~Choi, A.~Passos, C.~J. Shallue, and G.~E. Dahl.
\newblock Faster neural network training with data echoing.
\newblock \emph{arXiv preprint arXiv:1907.05550}, 2019.

\bibitem[Ciregan et~al.(2012)Ciregan, Meier, and Schmidhuber]{ciregan2012multi}
D.~Ciregan, U.~Meier, and J.~Schmidhuber.
\newblock Multi-column deep neural networks for image classification.
\newblock In \emph{2012 IEEE conference on computer vision and pattern
  recognition}, pages 3642--3649. IEEE, 2012.

\bibitem[Cubuk et~al.(2019)Cubuk, Zoph, Mane, Vasudevan, and
  Le]{cubuk2019autoaugment}
E.~D. Cubuk, B.~Zoph, D.~Mane, V.~Vasudevan, and Q.~V. Le.
\newblock Autoaugment: Learning augmentation strategies from data.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 113--123, 2019.

\bibitem[Dalton et~al.(2019)Dalton, Frosio, and Garland]{dalton2019gpu}
S.~Dalton, I.~Frosio, and M.~Garland.
\newblock Gpu-accelerated atari emulation for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1907.08467}, 2019.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In \emph{CVPR09}, 2009.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dua and Graff(2017)]{dua2019uci}
D.~Dua and C.~Graff.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Guirao et~al.(2019)Guirao, Łęcki, Lisiecki, Panev, Szołucha,
  Wolant, and Zientkiewicz]{guirao2019fast}
J.~A. Guirao, K.~Łęcki, J.~Lisiecki, S.~Panev, M.~Szołucha, A.~Wolant, and
  M.~Zientkiewicz.
\newblock {Fast AI Data Preprocessing with NVIDIA DALI}, 2019.
\newblock URL
  \url{https://developer.nvidia.com/blog/fast-ai-data-preprocessing-with-nvidia-dali/}.

\bibitem[Hardt et~al.(2015)Hardt, Recht, and Singer]{hardt2015train}
M.~Hardt, B.~Recht, and Y.~Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock \emph{arXiv preprint arXiv:1509.01240}, 2015.

\bibitem[Hoffer et~al.(2019)Hoffer, Ben-Nun, Hubara, Giladi, Hoefler, and
  Soudry]{hoffer2019augment}
E.~Hoffer, T.~Ben-Nun, I.~Hubara, N.~Giladi, T.~Hoefler, and D.~Soudry.
\newblock Augment your batch: better training with larger batches.
\newblock \emph{arXiv preprint arXiv:1901.09335}, 2019.

\bibitem[Jiang et~al.(2020)Jiang, Zhang, Talwar, and Mozer]{jiang2020exploring}
Z.~Jiang, C.~Zhang, K.~Talwar, and M.~C. Mozer.
\newblock Exploring the memorization-generalization continuum in deep learning.
\newblock \emph{arXiv preprint arXiv:2002.03206}, 2020.

\bibitem[Jouppi et~al.(2017)Jouppi, Young, Patil, Patterson, Agrawal, Bajwa,
  Bates, Bhatia, Boden, Borchers, et~al.]{jouppi2017datacenter}
N.~P. Jouppi, C.~Young, N.~Patil, D.~Patterson, G.~Agrawal, R.~Bajwa, S.~Bates,
  S.~Bhatia, N.~Boden, A.~Borchers, et~al.
\newblock In-datacenter performance analysis of a tensor processing unit.
\newblock In \emph{Proceedings of the 44th Annual International Symposium on
  Computer Architecture}, pages 1--12, 2017.

\bibitem[Kairouz et~al.(2019)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, et~al.]{kairouz2019advances}
P.~Kairouz, H.~B. McMahan, B.~Avent, A.~Bellet, M.~Bennis, A.~N. Bhagoji,
  K.~Bonawitz, Z.~Charles, G.~Cormode, R.~Cummings, et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{arXiv preprint arXiv:1912.04977}, 2019.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
J.~Kaplan, S.~McCandlish, T.~Henighan, T.~B. Brown, B.~Chess, R.~Child,
  S.~Gray, A.~Radford, J.~Wu, and D.~Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Khadatare et~al.(2020)Khadatare, Khan, and
  Bayraktar]{khadatare2020leveraging}
M.~Khadatare, Z.~Khan, and H.~Bayraktar.
\newblock {Leveraging the Hardware JPEG Decoder and NVIDIA nvJPEG Library on
  NVIDIA A100 GPUs}, 2020.
\newblock URL
  \url{https://developer.nvidia.com/blog/leveraging-hardware-jpeg-decoder-and-nvjpeg-on-a100/}.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem[Lan(2012)]{lan2012optimal}
G.~Lan.
\newblock An optimal method for stochastic composite optimization.
\newblock \emph{Mathematical Programming}, 133\penalty0 (1-2):\penalty0
  365--397, 2012.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{lecun2010mnist}
Y.~LeCun, C.~Cortes, and C.~Burges.
\newblock Mnist handwritten digit database.
\newblock \emph{ATT Labs [Online]. Available:
  http://yann.lecun.com/exdb/mnist}, 2, 2010.

\bibitem[Li and Arora(2019)]{li2019exponential}
Z.~Li and S.~Arora.
\newblock An exponential learning rate schedule for deep learning.
\newblock \emph{arXiv preprint arXiv:1910.07454}, 2019.

\bibitem[Liang et~al.(2018)Liang, Makoviychuk, Handa, Chentanez, Macklin, and
  Fox]{liang2018gpu}
J.~Liang, V.~Makoviychuk, A.~Handa, N.~Chentanez, M.~Macklin, and D.~Fox.
\newblock Gpu-accelerated robotic simulation for distributed reinforcement
  learning.
\newblock In \emph{Conference on Robot Learning}, pages 270--282, 2018.

\bibitem[Mayer and Jacobsen(2020)]{mayer2020scalable}
R.~Mayer and H.-A. Jacobsen.
\newblock Scalable deep learning on distributed infrastructures: Challenges,
  techniques, and tools.
\newblock \emph{ACM Computing Surveys (CSUR)}, 53\penalty0 (1):\penalty0 1--37,
  2020.

\bibitem[McCandlish et~al.(2018)McCandlish, Kaplan, Amodei, and
  Team]{mccandlish2018empirical}
S.~McCandlish, J.~Kaplan, D.~Amodei, and O.~D. Team.
\newblock An empirical model of large-batch training.
\newblock \emph{arXiv preprint arXiv:1812.06162}, 2018.

\bibitem[Nesterov(2013)]{nesterov2013introductory}
Y.~Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Nesterov(1983)]{nesterov1983method}
Y.~E. Nesterov.
\newblock A method of solving a convex programming problem with convergence
  rate o(k\^{}2).
\newblock In \emph{Doklady Akademii Nauk}, volume 269, pages 543--547. Russian
  Academy of Sciences, 1983.

\bibitem[Raina et~al.(2009)Raina, Madhavan, and Ng]{raina2009large}
R.~Raina, A.~Madhavan, and A.~Y. Ng.
\newblock Large-scale deep unsupervised learning using graphics processors.
\newblock In \emph{Proceedings of the 26th annual international conference on
  machine learning}, pages 873--880, 2009.

\bibitem[Shallue et~al.(2018)Shallue, Lee, Antognini, Sohl-Dickstein, Frostig,
  and Dahl]{shallue2018measuring}
C.~J. Shallue, J.~Lee, J.~Antognini, J.~Sohl-Dickstein, R.~Frostig, and G.~E.
  Dahl.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock \emph{arXiv preprint arXiv:1811.03600}, 2018.

\bibitem[Shazeer and Stern(2018)]{shazeer2018adafactor}
N.~Shazeer and M.~Stern.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock \emph{arXiv preprint arXiv:1804.04235}, 2018.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean]{shazeer2017outrageously}
N.~Shazeer, A.~Mirhoseini, K.~Maziarz, A.~Davis, Q.~Le, G.~Hinton, and J.~Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock \emph{arXiv preprint arXiv:1701.06538}, 2017.

\bibitem[Shorten and Khoshgoftaar(2019)]{shorten2019survey}
C.~Shorten and T.~M. Khoshgoftaar.
\newblock A survey on image data augmentation for deep learning.
\newblock \emph{Journal of Big Data}, 6\penalty0 (1):\penalty0 60, 2019.

\bibitem[Smith et~al.(2017)Smith, Kindermans, Ying, and Le]{smith2017don}
S.~L. Smith, P.-J. Kindermans, C.~Ying, and Q.~V. Le.
\newblock Don't decay the learning rate, increase the batch size.
\newblock \emph{arXiv preprint arXiv:1711.00489}, 2017.

\bibitem[Wang et~al.(2017)Wang, Wang, and Srebro]{wang2017memory}
J.~Wang, W.~Wang, and N.~Srebro.
\newblock Memory and communication efficient distributed stochastic
  optimization with minibatch-prox.
\newblock \emph{arXiv preprint arXiv:1702.06269}, 2017.

\bibitem[Woodworth et~al.(2020)Woodworth, Patel, Stich, Dai, Bullins, McMahan,
  Shamir, and Srebro]{woodworth2020local}
B.~Woodworth, K.~K. Patel, S.~U. Stich, Z.~Dai, B.~Bullins, H.~B. McMahan,
  O.~Shamir, and N.~Srebro.
\newblock Is local sgd better than minibatch sgd?
\newblock \emph{arXiv preprint arXiv:2002.07839}, 2020.

\bibitem[Ying et~al.(2018)Ying, Kumar, Chen, Wang, and Cheng]{ying2018image}
C.~Ying, S.~Kumar, D.~Chen, T.~Wang, and Y.~Cheng.
\newblock Image classification at supercomputer scale.
\newblock \emph{arXiv preprint arXiv:1811.06992}, 2018.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you2017large}
Y.~You, I.~Gitman, and B.~Ginsburg.
\newblock Large batch training of convolutional networks.
\newblock \emph{arXiv preprint arXiv:1708.03888}, 2017.

\end{thebibliography}
