\begin{thebibliography}{66}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[ntk(2023)]{ntk}
Ntk-aware scaled rope.
\newblock \url{https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/}, 2023.

\bibitem[Arora et~al.(2023)Arora, Eyuboglu, Timalsina, Johnson, Poli, Zou, Rudra, and R{\'e}]{arora2023zoology}
Arora, S., Eyuboglu, S., Timalsina, A., Johnson, I., Poli, M., Zou, J., Rudra, A., and R{\'e}, C.
\newblock Zoology: Measuring and improving recall in efficient language models.
\newblock \emph{arXiv preprint arXiv:2312.04927}, 2023.

\bibitem[Azerbayev et~al.(2023)Azerbayev, Schoelkopf, Paster, Santos, McAleer, Jiang, Deng, Biderman, and Welleck]{azerbayev2023llemma}
Azerbayev, Z., Schoelkopf, H., Paster, K., Santos, M.~D., McAleer, S., Jiang, A.~Q., Deng, J., Biderman, S., and Welleck, S.
\newblock Llemma: An open language model for mathematics, 2023.

\bibitem[Bai et~al.(2023)Bai, Lv, Zhang, Lyu, Tang, Huang, Du, Liu, Zeng, Hou, et~al.]{bai2023longbench}
Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., et~al.
\newblock Longbench: A bilingual, multitask benchmark for long context understanding.
\newblock \emph{arXiv preprint arXiv:2308.14508}, 2023.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Beltagy, I., Peters, M.~E., and Cohan, A.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d.~O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Wong, Chen, and Tian]{chen2023extending}
Chen, S., Wong, S., Chen, L., and Tian, Y.
\newblock Extending context window of large language models via positional interpolation.
\newblock \emph{arXiv preprint arXiv:2306.15595}, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Qian, Tang, Lai, Liu, Han, and Jia]{chen2023longlora}
Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J.
\newblock Longlora: Efficient fine-tuning of long-context large language models.
\newblock \emph{arXiv preprint arXiv:2309.12307}, 2023{\natexlab{b}}.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and Sutskever]{child2019generating}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song, Gane, Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, et~al.]{choromanski2020rethinking}
Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et~al.
\newblock Rethinking attention with performers.
\newblock \emph{arXiv preprint arXiv:2009.14794}, 2020.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think}
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem[Computer(2023)]{together2023redpajama}
Computer, T.
\newblock Redpajama: an open dataset for training large language models, 2023.
\newblock URL \url{https://github.com/togethercomputer/RedPajama-Data}.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and Salakhutdinov]{dai2019transformer}
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.~V., and Salakhutdinov, R.
\newblock Transformer-xl: Attentive language models beyond a fixed-length context.
\newblock \emph{arXiv preprint arXiv:1901.02860}, 2019.

\bibitem[Dao(2023)]{dao2023flashattention}
Dao, T.
\newblock Flashattention-2: Faster attention with better parallelism and work partitioning.
\newblock \emph{arXiv preprint arXiv:2307.08691}, 2023.

\bibitem[Dauphin et~al.(2017)Dauphin, Fan, Auli, and Grangier]{dauphin2017language}
Dauphin, Y.~N., Fan, A., Auli, M., and Grangier, D.
\newblock Language modeling with gated convolutional networks.
\newblock In \emph{International conference on machine learning}, pp.\  933--941. PMLR, 2017.

\bibitem[Du et~al.(2021)Du, Qian, Liu, Ding, Qiu, Yang, and Tang]{du2021glm}
Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., and Tang, J.
\newblock Glm: General language model pretraining with autoregressive blank infilling.
\newblock \emph{arXiv preprint arXiv:2103.10360}, 2021.

\bibitem[Fu et~al.(2022)Fu, Dao, Saab, Thomas, Rudra, and R{\'e}]{fu2022hungry}
Fu, D.~Y., Dao, T., Saab, K.~K., Thomas, A.~W., Rudra, A., and R{\'e}, C.
\newblock Hungry hungry hippos: Towards language modeling with state space models.
\newblock \emph{arXiv preprint arXiv:2212.14052}, 2022.

\bibitem[Goyal \& Durrett(2020)Goyal and Durrett]{goyal2020evaluating}
Goyal, T. and Durrett, G.
\newblock Evaluating factuality in generation with dependency-level entailment.
\newblock \emph{arXiv preprint arXiv:2010.05478}, 2020.

\bibitem[Gu \& Dao(2023)Gu and Dao]{gu2023mamba}
Gu, A. and Dao, T.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock \emph{arXiv preprint arXiv:2312.00752}, 2023.

\bibitem[Gu et~al.(2020)Gu, Dao, Ermon, Rudra, and R{\'e}]{gu2020hippo}
Gu, A., Dao, T., Ermon, S., Rudra, A., and R{\'e}, C.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1474--1487, 2020.

\bibitem[Gu et~al.(2021{\natexlab{a}})Gu, Goel, and R{\'e}]{gu2021efficiently}
Gu, A., Goel, K., and R{\'e}, C.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock \emph{arXiv preprint arXiv:2111.00396}, 2021{\natexlab{a}}.

\bibitem[Gu et~al.(2021{\natexlab{b}})Gu, Johnson, Goel, Saab, Dao, Rudra, and R{\'e}]{gu2021combining}
Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., and R{\'e}, C.
\newblock Combining recurrent, convolutional, and continuous-time models with linear state space layers.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 572--585, 2021{\natexlab{b}}.

\bibitem[Gu et~al.(2022)Gu, Goel, Gupta, and R{\'e}]{gu2022parameterization}
Gu, A., Goel, K., Gupta, A., and R{\'e}, C.
\newblock On the parameterization and initialization of diagonal state space models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 35971--35983, 2022.

\bibitem[Gupta et~al.(2022)Gupta, Gu, and Berant]{gupta2022diagonal}
Gupta, A., Gu, A., and Berant, J.
\newblock Diagonal state spaces are as effective as structured state spaces.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 22982--22994, 2022.

\bibitem[Han et~al.(2023)Han, Wang, Xiong, Chen, Ji, and Wang]{han2023lm}
Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S.
\newblock Lm-infinite: Simple on-the-fly length generalization for large language models.
\newblock \emph{arXiv preprint arXiv:2308.16137}, 2023.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Jiang et~al.(2023)Jiang, Wu, Luo, Li, Lin, Yang, and Qiu]{jiang2023longllmlingua}
Jiang, H., Wu, Q., Luo, X., Li, D., Lin, C.-Y., Yang, Y., and Qiu, L.
\newblock Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression.
\newblock \emph{arXiv preprint arXiv:2310.06839}, 2023.

\bibitem[Jin et~al.(2024)Jin, Han, Yang, Jiang, Liu, Chang, Chen, and Hu]{jin2024llm}
Jin, H., Han, X., Yang, J., Jiang, Z., Liu, Z., Chang, C.-Y., Chen, H., and Hu, X.
\newblock Llm maybe longlm: Self-extend llm context window without tuning.
\newblock \emph{arXiv preprint arXiv:2401.01325}, 2024.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer]{joshi2017triviaqa}
Joshi, M., Choi, E., Weld, D.~S., and Zettlemoyer, L.
\newblock Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.
\newblock \emph{arXiv preprint arXiv:1705.03551}, 2017.

\bibitem[Kamalloo et~al.(2023)Kamalloo, Dziri, Clarke, and Rafiei]{kamalloo2023evaluating}
Kamalloo, E., Dziri, N., Clarke, C.~L., and Rafiei, D.
\newblock Evaluating open-domain question answering in the era of large language models.
\newblock \emph{arXiv preprint arXiv:2305.06984}, 2023.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and Fleuret]{katharopoulos2020transformers}
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
\newblock Transformers are rnns: Fast autoregressive transformers with linear attention.
\newblock In \emph{International conference on machine learning}, pp.\  5156--5165. PMLR, 2020.

\bibitem[Kim(2014)]{kim2014convolutional}
Kim, Y.
\newblock Convolutional neural networks for sentence classification.
\newblock \emph{arXiv preprint arXiv:1408.5882}, 2014.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
Kitaev, N., Kaiser, {\L}., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock \emph{arXiv preprint arXiv:2001.04451}, 2020.

\bibitem[Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy]{lai2017race}
Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E.
\newblock Race: Large-scale reading comprehension dataset from examinations.
\newblock \emph{arXiv preprint arXiv:1704.04683}, 2017.

\bibitem[Lewkowycz et~al.(2022)Lewkowycz, Andreassen, Dohan, Dyer, Michalewski, Ramasesh, Slone, Anil, Schlag, Gutman-Solo, et~al.]{lewkowycz2022solving}
Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., et~al.
\newblock Solving quantitative reasoning problems with language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 3843--3857, 2022.

\bibitem[Li et~al.(2022)Li, Choi, Chung, Kushman, Schrittwieser, Leblond, Eccles, Keeling, Gimeno, Dal~Lago, et~al.]{li2022competition}
Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal~Lago, A., et~al.
\newblock Competition-level code generation with alphacode.
\newblock \emph{Science}, 378\penalty0 (6624):\penalty0 1092--1097, 2022.

\bibitem[Liu et~al.(2023)Liu, Lin, Hewitt, Paranjape, Bevilacqua, Petroni, and Liang]{liu2023lost}
Liu, N.~F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P.
\newblock Lost in the middle: How language models use long contexts.
\newblock \emph{arXiv preprint arXiv:2307.03172}, 2023.

\bibitem[Massaroli et~al.(2023)Massaroli, Poli, Fu, Kumbong, Parnichkun, Timalsina, Romero, McIntyre, Chen, Rudra, et~al.]{massaroli2023laughing}
Massaroli, S., Poli, M., Fu, D.~Y., Kumbong, H., Parnichkun, R.~N., Timalsina, A., Romero, D.~W., McIntyre, Q., Chen, B., Rudra, A., et~al.
\newblock Laughing hyena distillery: Extracting compact recurrences from convolutions.
\newblock \emph{arXiv preprint arXiv:2310.18780}, 2023.

\bibitem[Mohtashami \& Jaggi(2023{\natexlab{a}})Mohtashami and Jaggi]{mohtashami2023landmark}
Mohtashami, A. and Jaggi, M.
\newblock Landmark attention: Random-access infinite context length for transformers, 2023{\natexlab{a}}.

\bibitem[Mohtashami \& Jaggi(2023{\natexlab{b}})Mohtashami and Jaggi]{mohtashami2023random}
Mohtashami, A. and Jaggi, M.
\newblock Random-access infinite context length for transformers.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023{\natexlab{b}}.

\bibitem[Peng et~al.(2023)Peng, Quesnelle, Fan, and Shippole]{peng2023yarn}
Peng, B., Quesnelle, J., Fan, H., and Shippole, E.
\newblock Yarn: Efficient context window extension of large language models.
\newblock \emph{arXiv preprint arXiv:2309.00071}, 2023.

\bibitem[Poli et~al.(2023)Poli, Massaroli, Nguyen, Fu, Dao, Baccus, Bengio, Ermon, and R{\'e}]{poli2023hyena}
Poli, M., Massaroli, S., Nguyen, E., Fu, D.~Y., Dao, T., Baccus, S., Bengio, Y., Ermon, S., and R{\'e}, C.
\newblock Hyena hierarchy: Towards larger convolutional language models.
\newblock \emph{arXiv preprint arXiv:2302.10866}, 2023.

\bibitem[Press et~al.(2021)Press, Smith, and Lewis]{press2021train}
Press, O., Smith, N.~A., and Lewis, M.
\newblock Train short, test long: Attention with linear biases enables input length extrapolation.
\newblock \emph{arXiv preprint arXiv:2108.12409}, 2021.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever, et~al.]{radford2018improving}
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et~al.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Ribar et~al.(2023)Ribar, Chelombiev, Hudlass-Galley, Blake, Luschi, and Orr]{ribar2023sparq}
Ribar, L., Chelombiev, I., Hudlass-Galley, L., Blake, C., Luschi, C., and Orr, D.
\newblock Sparq attention: Bandwidth-efficient llm inference.
\newblock \emph{arXiv preprint arXiv:2312.04985}, 2023.

\bibitem[Roy et~al.(2021)Roy, Saffar, Vaswani, and Grangier]{roy2021efficient}
Roy, A., Saffar, M., Vaswani, A., and Grangier, D.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 9:\penalty0 53--68, 2021.

\bibitem[Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and Choi]{sakaguchi2021winogrande}
Sakaguchi, K., Bras, R.~L., Bhagavatula, C., and Choi, Y.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64\penalty0 (9):\penalty0 99--106, 2021.

\bibitem[Shaham et~al.(2022)Shaham, Segal, Ivgi, Efrat, Yoran, Haviv, Gupta, Xiong, Geva, Berant, et~al.]{shaham2022scrolls}
Shaham, U., Segal, E., Ivgi, M., Efrat, A., Yoran, O., Haviv, A., Gupta, A., Xiong, W., Geva, M., Berant, J., et~al.
\newblock Scrolls: Standardized comparison over long language sequences.
\newblock \emph{arXiv preprint arXiv:2201.03533}, 2022.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Bahri, and Metzler]{tay2022efficient}
Tay, Y., Dehghani, M., Bahri, D., and Metzler, D.
\newblock Efficient transformers: A survey.
\newblock \emph{ACM Computing Surveys}, 55\penalty0 (6):\penalty0 1--28, 2022.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Tworkowski et~al.(2023)Tworkowski, Staniszewski, Pacek, Wu, Michalewski, and Mi{\l}o{\'s}]{tworkowski2023focused}
Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and Mi{\l}o{\'s}, P.
\newblock Focused transformer: Contrastive training for context scaling.
\newblock \emph{arXiv preprint arXiv:2307.03170}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Wang, S., Li, B.~Z., Khabsa, M., Fang, H., and Ma, H.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Wu et~al.(2022)Wu, Rabe, Hutchins, and Szegedy]{wu2022memorizing}
Wu, Y., Rabe, M.~N., Hutchins, D., and Szegedy, C.
\newblock Memorizing transformers.
\newblock \emph{arXiv preprint arXiv:2203.08913}, 2022.

\bibitem[Xiao et~al.(2023)Xiao, Tian, Chen, Han, and Lewis]{xiao2023efficient}
Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M.
\newblock Efficient streaming language models with attention sinks.
\newblock \emph{arXiv preprint arXiv:2309.17453}, 2023.

\bibitem[Xiong et~al.(2023)Xiong, Liu, Molybog, Zhang, Bhargava, Hou, Martin, Rungta, Sankararaman, Oguz, et~al.]{xiong2023effective}
Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K.~A., Oguz, B., et~al.
\newblock Effective long-context scaling of foundation models.
\newblock \emph{arXiv preprint arXiv:2309.16039}, 2023.

\bibitem[Xiong et~al.(2021)Xiong, Zeng, Chakraborty, Tan, Fung, Li, and Singh]{xiong2021nystromformer}
Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., and Singh, V.
\newblock Nystr{\"o}mformer: A nystr{\"o}m-based algorithm for approximating self-attention.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, pp.\  14138--14148, 2021.

\bibitem[Yuan et~al.(2022)Yuan, Coenen, Reif, and Ippolito]{yuan2022wordcraft}
Yuan, A., Coenen, A., Reif, E., and Ippolito, D.
\newblock Wordcraft: story writing with large language models.
\newblock In \emph{27th International Conference on Intelligent User Interfaces}, pp.\  841--852, 2022.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti, Ontanon, Pham, Ravula, Wang, Yang, et~al.]{zaheer2020big}
Zaheer, M., Guruganesh, G., Dubey, K.~A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 17283--17297, 2020.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag}
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock \emph{arXiv preprint arXiv:1905.07830}, 2019.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Ram, Hawkins, Zha, and Zhao]{zhang2023efficient}
Zhang, Q., Ram, D., Hawkins, C., Zha, S., and Zhao, T.
\newblock Efficient long-range transformers: You need to attend more, but not necessarily at every layer.
\newblock \emph{arXiv preprint arXiv:2310.12442}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2024)Zhang, Li, and Liu]{zhang2024extending}
Zhang, Y., Li, J., and Liu, P.
\newblock Extending llms' context window with 100 samples.
\newblock \emph{arXiv preprint arXiv:2401.07004}, 2024.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Sheng, Zhou, Chen, Zheng, Cai, Song, Tian, R{\'e}, Barrett, et~al.]{zhang2023h}
Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., R{\'e}, C., Barrett, C., et~al.
\newblock H $ \_2 $ o: Heavy-hitter oracle for efficient generative inference of large language models.
\newblock \emph{arXiv preprint arXiv:2306.14048}, 2023{\natexlab{b}}.

\bibitem[Zhu et~al.(2023)Zhu, Yang, Wang, Song, Wu, Wei, and Li]{zhu2023pose}
Zhu, D., Yang, N., Wang, L., Song, Y., Wu, W., Wei, F., and Li, S.
\newblock Pose: Efficient context window extension of llms via positional skip-wise training.
\newblock \emph{arXiv preprint arXiv:2309.10400}, 2023.

\end{thebibliography}
