\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amos and Yarats(2020)]{amos2020differentiable}
B.~Amos and D.~Yarats.
\newblock The differentiable cross-entropy method.
\newblock In \emph{International Conference on Machine Learning}, pages
  291--302. PMLR, 2020.

\bibitem[Anthony et~al.(2017)Anthony, Tian, and Barber]{anthony2017thinking}
T.~Anthony, Z.~Tian, and D.~Barber.
\newblock Thinking fast and slow with deep learning and tree search.
\newblock \emph{arXiv preprint arXiv:1705.08439}, 2017.

\bibitem[Anthony et~al.(2019)Anthony, Nishihara, Moritz, Salimans, and
  Schulman]{anthony2019policy}
T.~Anthony, R.~Nishihara, P.~Moritz, T.~Salimans, and J.~Schulman.
\newblock Policy gradient search: Online planning and expert iteration without
  search trees.
\newblock \emph{arXiv preprint arXiv:1904.03646}, 2019.

\bibitem[Badia et~al.(2020)Badia, Piot, Kapturowski, Sprechmann, Vitvitskyi,
  Guo, and Blundell]{badia2020agent57}
A.~P. Badia, B.~Piot, S.~Kapturowski, P.~Sprechmann, A.~Vitvitskyi, Z.~D. Guo,
  and C.~Blundell.
\newblock Agent57: Outperforming the atari human benchmark.
\newblock In \emph{International Conference on Machine Learning}, pages
  507--517. PMLR, 2020.

\bibitem[Bard et~al.(2020)Bard, Foerster, Chandar, Burch, Lanctot, Song,
  Parisotto, Dumoulin, Moitra, Hughes, et~al.]{bard2020hanabi}
N.~Bard, J.~N. Foerster, S.~Chandar, N.~Burch, M.~Lanctot, H.~F. Song,
  E.~Parisotto, V.~Dumoulin, S.~Moitra, E.~Hughes, et~al.
\newblock The hanabi challenge: A new frontier for ai research.
\newblock \emph{Artificial Intelligence}, 280:\penalty0 103216, 2020.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare2013arcade}
M.~G. Bellemare, Y.~Naddaf, J.~Veness, and M.~Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Brown and Sandholm(2017)]{brown2017superhuman}
N.~Brown and T.~Sandholm.
\newblock Superhuman {A}{I} for heads-up no-limit poker: Libratus beats top
  professionals.
\newblock \emph{Science}, page eaao1733, 2017.

\bibitem[Brown and Sandholm(2019)]{brown2019superhuman}
N.~Brown and T.~Sandholm.
\newblock Superhuman {A}{I} for multiplayer poker.
\newblock \emph{Science}, page eaay2400, 2019.

\bibitem[Brown et~al.(2020)Brown, Bakhtin, Lerer, and Gong]{brown2020combining}
N.~Brown, A.~Bakhtin, A.~Lerer, and Q.~Gong.
\newblock Combining deep reinforcement learning and search for
  imperfect-information games.
\newblock \emph{arXiv preprint arXiv:2007.13544}, 2020.

\bibitem[Campbell et~al.(2002)Campbell, Hoane~Jr, and Hsu]{campbell2002deep}
M.~Campbell, A.~J. Hoane~Jr, and F.-h. Hsu.
\newblock Deep {B}lue.
\newblock \emph{Artificial intelligence}, 134\penalty0 (1-2):\penalty0 57--83,
  2002.

\bibitem[Cowling et~al.(2012)Cowling, Powley, and
  Whitehouse]{cowling2012information}
P.~I. Cowling, E.~J. Powley, and D.~Whitehouse.
\newblock Information set monte carlo tree search.
\newblock \emph{IEEE Transactions on Computational Intelligence and AI in
  Games}, 4\penalty0 (2):\penalty0 120--143, 2012.

\bibitem[Efroni et~al.(2018)Efroni, Dalal, Scherrer, and
  Mannor]{efroni2018multiple}
Y.~Efroni, G.~Dalal, B.~Scherrer, and S.~Mannor.
\newblock Multiple-step greedy policies in approximate and online reinforcement
  learning.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Foerster et~al.(2019)Foerster, Song, Hughes, Burch, Dunning, Whiteson,
  Botvinick, and Bowling]{foerster2019bayesian}
J.~Foerster, F.~Song, E.~Hughes, N.~Burch, I.~Dunning, S.~Whiteson,
  M.~Botvinick, and M.~Bowling.
\newblock Bayesian action decoder for deep multi-agent reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1942--1951, 2019.

\bibitem[Grill et~al.(2020)Grill, Altch{\'e}, Tang, Hubert, Valko, Antonoglou,
  and Munos]{grill2020monte}
J.-B. Grill, F.~Altch{\'e}, Y.~Tang, T.~Hubert, M.~Valko, I.~Antonoglou, and
  R.~Munos.
\newblock Monte-carlo tree search as regularized policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  3769--3778. PMLR, 2020.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pages
  1861--1870. PMLR, 2018.

\bibitem[Hamrick et~al.(2019)Hamrick, Bapst, Sanchez-Gonzalez, Pfaff, Weber,
  Buesing, and Battaglia]{hamrick2019combining}
J.~B. Hamrick, V.~Bapst, A.~Sanchez-Gonzalez, T.~Pfaff, T.~Weber, L.~Buesing,
  and P.~W. Battaglia.
\newblock Combining q-learning and search with amortized value estimates.
\newblock \emph{arXiv preprint arXiv:1912.02807}, 2019.

\bibitem[Horgan et~al.(2018)Horgan, Quan, Budden, Barth{-}Maron, Hessel, van
  Hasselt, and Silver]{apex}
D.~Horgan, J.~Quan, D.~Budden, G.~Barth{-}Maron, M.~Hessel, H.~van Hasselt, and
  D.~Silver.
\newblock Distributed prioritized experience replay.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}. OpenReview.net, 2018.
\newblock URL \url{https://openreview.net/forum?id=H1Dy---0Z}.

\bibitem[Hu and Foerster(2019)]{hu2019simplified}
H.~Hu and J.~N. Foerster.
\newblock Simplified action decoder for deep multi-agent reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Hu et~al.(2020)Hu, Lerer, Peysakhovich, and Foerster]{hu2020other}
H.~Hu, A.~Lerer, A.~Peysakhovich, and J.~Foerster.
\newblock “other-play” for zero-shot coordination.
\newblock In \emph{International Conference on Machine Learning}, pages
  4399--4410. PMLR, 2020.

\bibitem[Hu et~al.(2021)Hu, Lerer, Brown, and Foerster]{hu2021learned}
H.~Hu, A.~Lerer, N.~Brown, and J.~N. Foerster.
\newblock Learned belief search: Efficiently improving policies in partially
  observable settings, 2021.
\newblock URL \url{https://openreview.net/forum?id=xP37gkVKa_0}.

\bibitem[Hubert et~al.(2021)Hubert, Schrittwieser, Antonoglou, Barekatain,
  Schmitt, and Silver]{hubert2021learning}
T.~Hubert, J.~Schrittwieser, I.~Antonoglou, M.~Barekatain, S.~Schmitt, and
  D.~Silver.
\newblock Learning and planning in complex action spaces.
\newblock \emph{arXiv preprint arXiv:2104.06303}, 2021.

\bibitem[Lerer et~al.(2020)Lerer, Hu, Foerster, and Brown]{lerer2019improving}
A.~Lerer, H.~Hu, J.~N. Foerster, and N.~Brown.
\newblock Improving policies via search in cooperative partially observable
  games.
\newblock In \emph{The Thirty-Fourth {AAAI} Conference on Artificial
  Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of
  Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium
  on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York,
  NY, USA, February 7-12, 2020}, pages 7187--7194. {AAAI} Press, 2020.
\newblock URL \url{https://aaai.org/ojs/index.php/AAAI/article/view/6208}.

\bibitem[Levine(2018)]{levine2018reinforcement}
S.~Levine.
\newblock Reinforcement learning and control as probabilistic inference:
  Tutorial and review.
\newblock \emph{arXiv preprint arXiv:1805.00909}, 2018.

\bibitem[Li et~al.(2017)Li, Xu, Taylor, Studer, and
  Goldstein]{li2017visualizing}
H.~Li, Z.~Xu, G.~Taylor, C.~Studer, and T.~Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{arXiv preprint arXiv:1712.09913}, 2017.

\bibitem[Marino et~al.(2020)Marino, Pich{\'e}, Ialongo, and
  Yue]{marino2020iterative}
J.~Marino, A.~Pich{\'e}, A.~D. Ialongo, and Y.~Yue.
\newblock Iterative amortized policy optimization.
\newblock \emph{arXiv preprint arXiv:2010.10670}, 2020.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Morav{\v{c}}{\'\i}k et~al.(2017)Morav{\v{c}}{\'\i}k, Schmid, Burch,
  Lis{\`y}, Morrill, Bard, Davis, Waugh, Johanson, and
  Bowling]{moravvcik2017deepstack}
M.~Morav{\v{c}}{\'\i}k, M.~Schmid, N.~Burch, V.~Lis{\`y}, D.~Morrill, N.~Bard,
  T.~Davis, K.~Waugh, M.~Johanson, and M.~Bowling.
\newblock Deepstack: Expert-level artificial intelligence in heads-up no-limit
  poker.
\newblock \emph{Science}, 356\penalty0 (6337):\penalty0 508--513, 2017.

\bibitem[Munos(2014)]{munos2014bandits}
R.~Munos.
\newblock From bandits to monte-carlo tree search: The optimistic principle
  applied to optimization and planning.
\newblock 2014.

\bibitem[Oliehoek et~al.(2008)Oliehoek, Spaan, and
  Vlassis]{oliehoek2008optimal}
F.~A. Oliehoek, M.~T. Spaan, and N.~Vlassis.
\newblock Optimal and approximate q-value functions for decentralized pomdps.
\newblock \emph{Journal of Artificial Intelligence Research}, 32:\penalty0
  289--353, 2008.

\bibitem[Oquab et~al.(2014)Oquab, Bottou, Laptev, and Sivic]{oquab2014learning}
M.~Oquab, L.~Bottou, I.~Laptev, and J.~Sivic.
\newblock Learning and transferring mid-level image representations using
  convolutional neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1717--1724, 2014.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and
  Silver]{prioritized-replay}
T.~Schaul, J.~Quan, I.~Antonoglou, and D.~Silver.
\newblock Prioritized experience replay.
\newblock In Y.~Bengio and Y.~LeCun, editors, \emph{4th International
  Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico,
  May 2-4, 2016, Conference Track Proceedings}, 2016.
\newblock URL \url{http://arxiv.org/abs/1511.05952}.

\bibitem[Schrittwieser et~al.(2019)Schrittwieser, Antonoglou, Hubert, Simonyan,
  Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel,
  et~al.]{schrittwieser2019mastering}
J.~Schrittwieser, I.~Antonoglou, T.~Hubert, K.~Simonyan, L.~Sifre, S.~Schmitt,
  A.~Guez, E.~Lockhart, D.~Hassabis, T.~Graepel, et~al.
\newblock Mastering atari, go, chess and shogi by planning with a learned
  model.
\newblock \emph{arXiv preprint arXiv:1911.08265}, 2019.

\bibitem[Schrittwieser et~al.(2021)Schrittwieser, Hubert, Mandhane, Barekatain,
  Antonoglou, and Silver]{schrittwieser2021online}
J.~Schrittwieser, T.~Hubert, A.~Mandhane, M.~Barekatain, I.~Antonoglou, and
  D.~Silver.
\newblock Online and offline reinforcement learning by planning with a learned
  model.
\newblock \emph{arXiv preprint arXiv:2104.06294}, 2021.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~Van Den~Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484, 2016.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
D.~Silver, J.~Schrittwieser, K.~Simonyan, I.~Antonoglou, A.~Huang, A.~Guez,
  T.~Hubert, L.~Baker, M.~Lai, A.~Bolton, et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354, 2017.

\bibitem[Silver et~al.(2018)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2018general}
D.~Silver, T.~Hubert, J.~Schrittwieser, I.~Antonoglou, M.~Lai, A.~Guez,
  M.~Lanctot, L.~Sifre, D.~Kumaran, T.~Graepel, et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and go through self-play.
\newblock \emph{Science}, 362\penalty0 (6419):\penalty0 1140--1144, 2018.

\bibitem[Sondik(1971)]{sondik1971optimal}
E.~J. Sondik.
\newblock The optimal control of partially observable markov decision
  processes.
\newblock \emph{PhD the sis, Stanford University}, 1971.

\bibitem[Springenberg et~al.(2020)Springenberg, Heess, Mankowitz, Merel,
  Byravan, Abdolmaleki, Kay, Degrave, Schrittwieser, Tassa,
  et~al.]{springenberg2020local}
J.~T. Springenberg, N.~Heess, D.~Mankowitz, J.~Merel, A.~Byravan,
  A.~Abdolmaleki, J.~Kay, J.~Degrave, J.~Schrittwieser, Y.~Tassa, et~al.
\newblock Local search for policy iteration in continuous control.
\newblock \emph{arXiv preprint arXiv:2010.05545}, 2020.

\bibitem[Sutton()]{sutton2019bitter}
R.~Sutton.
\newblock The bitter lesson.
\newblock \url{http://www.incompleteideas.net/IncIdeas/BitterLesson.html}.
\newblock Accessed: 2021-05-27.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tan et~al.(2018)Tan, Sun, Kong, Zhang, Yang, and Liu]{tan2018survey}
C.~Tan, F.~Sun, T.~Kong, W.~Zhang, C.~Yang, and C.~Liu.
\newblock A survey on deep transfer learning.
\newblock In \emph{International conference on artificial neural networks},
  pages 270--279. Springer, 2018.

\bibitem[Tan(1993)]{tan93multi}
M.~Tan.
\newblock Multi-agent reinforcement learning: Independent vs. cooperative
  agents.
\newblock In \emph{Proceedings of the tenth international conference on machine
  learning}, pages 330--337, 1993.

\bibitem[Tesauro(1994)]{tesauro1994td}
G.~Tesauro.
\newblock T{D}-{G}ammon, a self-teaching backgammon program, achieves
  master-level play.
\newblock \emph{Neural computation}, 6\penalty0 (2):\penalty0 215--219, 1994.

\bibitem[van Hasselt et~al.(2016)van Hasselt, Guez, and Silver]{double-dqn}
H.~van Hasselt, A.~Guez, and D.~Silver.
\newblock Deep reinforcement learning with double q-learning.
\newblock In D.~Schuurmans and M.~P. Wellman, editors, \emph{Proceedings of the
  Thirtieth {AAAI} Conference on Artificial Intelligence, February 12-17, 2016,
  Phoenix, Arizona, {USA}}, pages 2094--2100. {AAAI} Press, 2016.
\newblock URL
  \url{http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12389}.

\bibitem[Wang and Ba(2019)]{wang2019exploring}
T.~Wang and J.~Ba.
\newblock Exploring model-based planning with policy networks.
\newblock \emph{arXiv preprint arXiv:1906.08649}, 2019.

\bibitem[Wang et~al.(2016)Wang, Schaul, Hessel, van Hasselt, Lanctot, and
  de~Freitas]{dueling-dqn}
Z.~Wang, T.~Schaul, M.~Hessel, H.~van Hasselt, M.~Lanctot, and N.~de~Freitas.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock In M.~Balcan and K.~Q. Weinberger, editors, \emph{Proceedings of the
  33nd International Conference on Machine Learning, {ICML} 2016, New York
  City, NY, USA, June 19-24, 2016}, volume~48 of \emph{{JMLR} Workshop and
  Conference Proceedings}, pages 1995--2003. JMLR.org, 2016.
\newblock URL \url{http://proceedings.mlr.press/v48/wangf16.html}.

\bibitem[Yee et~al.(2016)Yee, Lis{\`y}, and Bowling]{yee2016monte}
T.~Yee, V.~Lis{\`y}, and M.~H. Bowling.
\newblock Monte carlo tree search in continuous action spaces with execution
  uncertainty.
\newblock In \emph{IJCAI}, pages 690--697, 2016.

\end{thebibliography}
