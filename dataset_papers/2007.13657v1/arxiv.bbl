\begin{thebibliography}{}

\bibitem[Chan et~al., 2016]{chan2016listen}
Chan, W., Jaitly, N., Le, Q., and Vinyals, O. (2016).
\newblock Listen, attend and spell: A neural network for large vocabulary
  conversational speech recognition.
\newblock In {\em 2016 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 4960--4964. IEEE.

\bibitem[Chen et~al., 2020]{chengenerative}
Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Dhariwal, P., Luan, D., and
  Sutskever, I. (2020).
\newblock Generative pretraining from pixels.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}.

\bibitem[Cohen and Shashua, 2017]{cohen2016inductive}
Cohen, N. and Shashua, A. (2017).
\newblock Inductive bias of deep convolutional networks through pooling
  geometry.
\newblock In {\em Proceeding of the International Conference on Learning
  Representations}.

\bibitem[Cordonnier et~al., 2020]{cordonnier2019relationship}
Cordonnier, J.-B., Loukas, A., and Jaggi, M. (2020).
\newblock On the relationship between self-attention and convolutional layers.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[d'Ascoli et~al., 2019]{d2019finding}
d'Ascoli, S., Sagun, L., Biroli, G., and Bruna, J. (2019).
\newblock Finding the needle in the haystack with convolutions: on the benefits
  of architectural bias.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9330--9340.

\bibitem[Dettmers and Zettlemoyer, 2019]{dettmers2019sparse}
Dettmers, T. and Zettlemoyer, L. (2019).
\newblock Sparse networks from scratch: Faster training without losing
  performance.
\newblock {\em arXiv preprint arXiv:1907.04840}.

\bibitem[Dziugaite and Roy, 2017]{dziugaite2017computing}
Dziugaite, G.~K. and Roy, D.~M. (2017).
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock In {\em Conference on Uncertainty in Artificial Intelligence}.

\bibitem[Evci et~al., 2020]{evci2019rigging}
Evci, U., Gale, T., Menick, J., Castro, P.~S., and Elsen, E. (2020).
\newblock Rigging the lottery: Making all tickets winners.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}.

\bibitem[Felzenszwalb et~al., 2009]{felzenszwalb2009object}
Felzenszwalb, P.~F., Girshick, R.~B., McAllester, D., and Ramanan, D. (2009).
\newblock Object detection with discriminatively trained part-based models.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  32(9):1627--1645.

\bibitem[Fernando et~al., 2016]{fernando2016convolution}
Fernando, C., Banarse, D., Reynolds, M., Besse, F., Pfau, D., Jaderberg, M.,
  Lanctot, M., and Wierstra, D. (2016).
\newblock Convolution by evolution: Differentiable pattern producing networks.
\newblock In {\em Proceedings of the Genetic and Evolutionary Computation
  Conference 2016}, pages 109--116.

\bibitem[Frankle and Carbin, 2019]{frankle2018lottery}
Frankle, J. and Carbin, M. (2019).
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In {\em Proceeding of the International Conference on Learning
  Representations}.

\bibitem[Gunasekar et~al., 2018]{gunasekar2018implicit}
Gunasekar, S., Lee, J.~D., Soudry, D., and Srebro, N. (2018).
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9461--9471.

\bibitem[He et~al., 2016a]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J. (2016a).
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778.

\bibitem[He et~al., 2016b]{he2016identity}
He, K., Zhang, X., Ren, S., and Sun, J. (2016b).
\newblock Identity mappings in deep residual networks.
\newblock In {\em European conference on computer vision}, pages 630--645.
  Springer.

\bibitem[Hinton et~al., 2012]{hinton2012improving}
Hinton, G.~E., Srivastava, N., Krizhevsky, A., Sutskever, I., and
  Salakhutdinov, R.~R. (2012).
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock {\em arXiv preprint arXiv:1207.0580}.

\bibitem[Krizhevsky et~al., 2012]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E. (2012).
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1097--1105.

\bibitem[Lawrence et~al., 1998]{lawrence1998size}
Lawrence, S., Giles, C.~L., and Tsoi, A.~C. (1998).
\newblock What size neural network gives optimal generalization? convergence
  properties of backpropagation.
\newblock Technical report.

\bibitem[Lee et~al., 2019]{lee2018snip}
Lee, N., Ajanthan, T., and Torr, P.~H. (2019).
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock In {\em Proceeding of the International Conference on Learning
  Representations}.

\bibitem[Lim et~al., 2019]{lim2019fast}
Lim, S., Kim, I., Kim, T., Kim, C., and Kim, S. (2019).
\newblock Fast autoaugment.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6665--6675.

\bibitem[Lin et~al., 2016]{lin2015far}
Lin, Z., Memisevic, R., and Konda, K. (2016).
\newblock How far can we go without convolution: Improving fully-connected
  networks.
\newblock {\em Proceeding of the International Conference on Learning
  Representations workshop track}.

\bibitem[Lowe, 1999]{lowe1999object}
Lowe, D.~G. (1999).
\newblock Object recognition from local scale-invariant features.
\newblock In {\em Proceedings of the seventh IEEE international conference on
  computer vision}, volume~2, pages 1150--1157. Ieee.

\bibitem[Mehrotra et~al., 1992]{mehrotra1992gabor}
Mehrotra, R., Namuduri, K.~R., and Ranganathan, N. (1992).
\newblock Gabor filter-based edge detection.
\newblock {\em Pattern recognition}, 25(12):1479--1494.

\bibitem[Mocanu et~al., 2018]{mocanu2018scalable}
Mocanu, D.~C., Mocanu, E., Stone, P., Nguyen, P.~H., Gibescu, M., and Liotta,
  A. (2018).
\newblock Scalable training of artificial neural networks with adaptive sparse
  connectivity inspired by network science.
\newblock {\em Nature communications}, 9(1):1--12.

\bibitem[Mukkamala and Hein, 2017]{mukkamala2017variants}
Mukkamala, M.~C. and Hein, M. (2017).
\newblock Variants of rmsprop and adagrad with logarithmic regret bounds.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning}, pages 2545--2553. JMLR. org.

\bibitem[Neyshabur et~al., 2017]{neyshabur2017exploring}
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N. (2017).
\newblock Exploring generalization in deep learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5947--5956.

\bibitem[Neyshabur et~al., 2019]{neyshabur2018towards}
Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N. (2019).
\newblock Towards understanding the role of over-parametrization in
  generalization of neural networks.
\newblock In {\em Proceeding of the International Conference on Learning
  Representations}.

\bibitem[Neyshabur et~al., 2015]{neyshabur2014search}
Neyshabur, B., Tomioka, R., and Srebro, N. (2015).
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock {\em International Conference on Learning Representations workshop
  track}.

\bibitem[Novak et~al., 2019]{novak2018bayesian}
Novak, R., Xiao, L., Lee, J., Bahri, Y., Yang, G., Hron, J., Abolafia, D.~A.,
  Pennington, J., and Sohl-Dickstein, J. (2019).
\newblock Bayesian deep convolutional networks with many channels are gaussian
  processes.
\newblock {\em Proceeding of the International Conference on Learning
  Representations}.

\bibitem[Real et~al., 2019]{real2019regularized}
Real, E., Aggarwal, A., Huang, Y., and Le, Q.~V. (2019).
\newblock Regularized evolution for image classifier architecture search.
\newblock In {\em Proceedings of the aaai conference on artificial
  intelligence}, volume~33, pages 4780--4789.

\bibitem[Ritchie et~al., 2020]{caliban2020github}
Ritchie, S., Slone, A., and Ramasesh, V. (2020).
\newblock {Caliban}: Docker-based job manager for reproducible workflows.

\bibitem[Senior et~al., 2020]{senior2020improved}
Senior, A.~W., Evans, R., Jumper, J., Kirkpatrick, J., Sifre, L., Green, T.,
  Qin, C., {\v{Z}}{\'\i}dek, A., Nelson, A.~W., Bridgland, A., et~al. (2020).
\newblock Improved protein structure prediction using potentials from deep
  learning.
\newblock {\em Nature}, pages 1--5.

\bibitem[Shalev-Shwartz and Ben-David, 2014]{shalev2014understanding}
Shalev-Shwartz, S. and Ben-David, S. (2014).
\newblock {\em Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press.

\bibitem[Springenberg et~al., 2015]{springenberg2014striving}
Springenberg, J.~T., Dosovitskiy, A., Brox, T., and Riedmiller, M. (2015).
\newblock Striving for simplicity: The all convolutional net.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Tan and Le, 2019]{tan2019efficientnet}
Tan, M. and Le, Q.~V. (2019).
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}.

\bibitem[Tibshirani, 1996]{tibshirani1996regression}
Tibshirani, R. (1996).
\newblock Regression shrinkage and selection via the lasso.
\newblock {\em Journal of the Royal Statistical Society: Series B
  (Methodological)}, 58(1):267--288.

\bibitem[Urban et~al., 2017]{urban2016deep}
Urban, G., Geras, K.~J., Kahou, S.~E., Aslan, O., Wang, S., Caruana, R.,
  Mohamed, A., Philipose, M., and Richardson, M. (2017).
\newblock Do deep convolutional nets really need to be deep and convolutional?
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Zeiler and Fergus, 2013]{zeiler2013stochastic}
Zeiler, M.~D. and Fergus, R. (2013).
\newblock Stochastic pooling for regularization of deep convolutional neural
  networks.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Zeiler and Fergus, 2014]{zeiler2014visualizing}
Zeiler, M.~D. and Fergus, R. (2014).
\newblock Visualizing and understanding convolutional networks.
\newblock In {\em European conference on computer vision}, pages 818--833.
  Springer.

\bibitem[Zhang et~al., 2017]{zhang2016understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2017).
\newblock Understanding deep learning requires rethinking generalization.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Zhou et~al., 2020]{zhou2020meta}
Zhou, A., Knowles, T., and Finn, C. (2020).
\newblock Meta-learning symmetries by reparameterization.
\newblock {\em arXiv preprint arXiv:2007.02933}.

\bibitem[Zoph and Le, 2017]{zoph2016neural}
Zoph, B. and Le, Q.~V. (2017).
\newblock Neural architecture search with reinforcement learning.
\newblock {\em International Conference on Learning Representations}.

\end{thebibliography}
