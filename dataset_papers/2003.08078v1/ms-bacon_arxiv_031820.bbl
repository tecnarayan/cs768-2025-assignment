\begin{thebibliography}{26}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adil and Sachdeva(2020)]{AS20}
Deeksha Adil and Sushant Sachdeva.
\newblock Faster \emph{p}-norm minimizing flows, via smoothed \emph{q}-norm
  problems.
\newblock In \emph{Symposium on Discrete Algorithms, {SODA}}, pages 892--910,
  2020.

\bibitem[Adil et~al.(2019{\natexlab{a}})Adil, Kyng, Peng, and
  Sachdeva]{AdilKPS19}
Deeksha Adil, Rasmus Kyng, Richard Peng, and Sushant Sachdeva.
\newblock Iterative refinement for $\ell_p$-norm regression.
\newblock In \emph{Symposium on Discrete Algorithms, {SODA}}, pages 1405--1424,
  2019{\natexlab{a}}.

\bibitem[Adil et~al.(2019{\natexlab{b}})Adil, Peng, and Sachdeva]{AdilPS19}
Deeksha Adil, Richard Peng, and Sushant Sachdeva.
\newblock Fast, provably convergent {IRLS} algorithm for p-norm linear
  regression.
\newblock In \emph{Advances in Neural Information Processing Systems, NeurIPS},
  pages 14166--14177, 2019{\natexlab{b}}.

\bibitem[Agarwal et~al.(2017)Agarwal, Bullins, and Hazan]{agarwal2017second}
Naman Agarwal, Brian Bullins, and Elad Hazan.
\newblock Second-order stochastic optimization for machine learning in linear
  time.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 4148--4187, 2017.

\bibitem[Bach et~al.(2010)]{bach2010}
Francis Bach et~al.
\newblock Self-concordant analysis for logistic regression.
\newblock \emph{Electronic Journal of Statistics}, 4:\penalty0 384--414, 2010.

\bibitem[Ball et~al.(1997)]{ball1997elementary}
Keith Ball et~al.
\newblock An elementary introduction to modern convex geometry.
\newblock \emph{Flavors of geometry}, 31:\penalty0 1--58, 1997.

\bibitem[Bubeck et~al.(2018)Bubeck, Cohen, Lee, and Li]{BubeckCLL18}
S{\'{e}}bastien Bubeck, Michael~B. Cohen, Yin~Tat Lee, and Yuanzhi Li.
\newblock An homotopy method for $\ell_p$ regression provably beyond
  self-concordance and in input-sparsity time.
\newblock In \emph{Symposium on Theory of Computing, {STOC}}, pages 1130--1137,
  2018.

\bibitem[Bubeck et~al.(2019)Bubeck, Jiang, Lee, Li, and
  Sidford]{bubeck2019complexity}
S{\'e}bastien Bubeck, Qijia Jiang, Yin-Tat Lee, Yuanzhi Li, and Aaron Sidford.
\newblock Complexity of highly parallel non-smooth convex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  13900--13909, 2019.

\bibitem[Bullins and Peng(2019)]{bullins2019higher}
Brian Bullins and Richard Peng.
\newblock Higher-order accelerated methods for faster non-smooth optimization.
\newblock \emph{arXiv preprint arXiv:1906.01621}, 2019.

\bibitem[Carmon et~al.(2019)Carmon, Duchi, Hinder, and
  Sidford]{carmon2019lower_i}
Yair Carmon, John~C Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Lower bounds for finding stationary points {I}.
\newblock \emph{Mathematical Programming}, May 2019.

\bibitem[Cohen et~al.(2018)Cohen, Diakonikolas, and Orecchia]{CohenDO18}
Michael Cohen, Jelena Diakonikolas, and Lorenzo Orecchia.
\newblock On acceleration with noise-corrupted gradients.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, pages 1018--1027, 2018.

\bibitem[Conn et~al.(2000)Conn, Gould, and Toint]{conn2000trust}
Andrew~R. Conn, Nicholas I.~M. Gould, and Philippe~L. Toint.
\newblock \emph{Trust Region Methods}.
\newblock {MOS-SIAM} Series on Optimization. {SIAM}, 2000.

\bibitem[Devolder et~al.(2014)Devolder, Glineur, and Nesterov]{DevolderGN14}
Olivier Devolder, Fran{\c{c}}ois Glineur, and Yurii~E. Nesterov.
\newblock First-order methods of smooth convex optimization with inexact
  oracle.
\newblock \emph{Math. Program.}, 146\penalty0 (1-2):\penalty0 37--75, 2014.

\bibitem[Diakonikolas and Guzm{\'{a}}n(2019)]{diakonikolas2018lower}
Jelena Diakonikolas and Crist{\'{o}}bal Guzm{\'{a}}n.
\newblock Lower bounds for parallel and randomized convex optimization.
\newblock In \emph{Conference on Learning Theory, {COLT}}, pages 1132--1157,
  2019.

\bibitem[Gasnikov et~al.(2019)Gasnikov, Dvurechensky, Gorbunov, Vorontsova,
  Selikhanovych, Uribe, Jiang, Wang, Zhang, Bubeck, Jiang, Lee, Li, and
  Sidford]{GasnikovDGVSU0W19}
Alexander Gasnikov, Pavel~E. Dvurechensky, Eduard~A. Gorbunov, Evgeniya~A.
  Vorontsova, Daniil Selikhanovych, C{\'{e}}sar~A. Uribe, Bo~Jiang, Haoyue
  Wang, Shuzhong Zhang, S{\'{e}}bastien Bubeck, Qijia Jiang, Yin~Tat Lee,
  Yuanzhi Li, and Aaron Sidford.
\newblock Near optimal methods for minimizing convex functions with lipschitz
  $p$-th derivatives.
\newblock In \emph{Conference on Learning Theory, {COLT} 2019}, pages
  1392--1393, 2019.

\bibitem[Guzm{\'a}n and Nemirovski(2015)]{guzman2015lower}
Crist{\'o}bal Guzm{\'a}n and Arkadi Nemirovski.
\newblock On lower complexity bounds for large-scale smooth convex
  optimization.
\newblock \emph{Journal of Complexity}, 31\penalty0 (1):\penalty0 1--14, 2015.

\bibitem[Hager(2001)]{Hager01}
William~W. Hager.
\newblock Minimizing a quadratic over a sphere.
\newblock \emph{{SIAM} Journal on Optimization}, 12\penalty0 (1):\penalty0
  188--208, 2001.

\bibitem[Karimireddy et~al.(2018)Karimireddy, Stich, and
  Jaggi]{karimireddy2018global}
Sai~Praneeth Karimireddy, Sebastian~U Stich, and Martin Jaggi.
\newblock Global linear convergence of {N}ewton's method without
  strong-convexity or lipschitz gradients.
\newblock \emph{arXiv preprint arXiv:1806.00413}, 2018.

\bibitem[Lin et~al.(2008)Lin, Weng, and Keerthi]{LinWK08}
Chih{-}Jen Lin, Ruby~C. Weng, and S.~Sathiya Keerthi.
\newblock Trust region {N}ewton method for large-scale logistic regression.
\newblock \emph{Journal of Machine Learning Research}, 9:\penalty0 627--650,
  2008.

\bibitem[Monteiro and Svaiter(2013)]{MonteiroS13a}
Renato D.~C. Monteiro and Benar~Fux Svaiter.
\newblock An accelerated hybrid proximal extragradient method for convex
  optimization and its implications to second-order methods.
\newblock \emph{{SIAM} Journal on Optimization}, 23\penalty0 (2):\penalty0
  1092--1125, 2013.

\bibitem[Nemirovski and Yudin(1983)]{nemirovski1983problem}
Arkadi Nemirovski and David~Borisovich Yudin.
\newblock \emph{Problem Complexity and Method Efficiency in Optimization}.
\newblock Wiley, 1983.

\bibitem[Nesterov(1983)]{Nesterov83}
Yurii Nesterov.
\newblock A method for solving a convex programming problem with convergence
  rate $o(1/k^2)$.
\newblock \emph{Doklady AN SSSR}, 269:\penalty0 543--547, 1983.

\bibitem[Schmidt et~al.(2011)Schmidt, Kim, and Sra]{SchmidtKS11}
Mark Schmidt, Dongmin Kim, and Suvrit Sra.
\newblock Projected {N}ewton-type methods in machine learning.
\newblock In Suvrit Sra, Sebastian Nowozin, and Stephen~J Wright, editors,
  \emph{Optimization for Machine Learning}, chapter~11. MIT Press, 2011.

\bibitem[Woodworth and Srebro(2016)]{woodworth2016tight}
Blake Woodworth and Nathan Srebro.
\newblock Tight complexity bounds for optimizing composite objectives.
\newblock In \emph{Advances in neural information processing systems}, pages
  3639--3647, 2016.

\bibitem[Woodworth and Srebro(2017)]{woodworth2017lower}
Blake Woodworth and Nathan Srebro.
\newblock Lower bound for randomized first order convex optimization.
\newblock \emph{arXiv preprint arXiv:1709.03594}, 2017.

\bibitem[Yao(1977)]{yao1977probabilistic}
Andrew Chi-Chin Yao.
\newblock Probabilistic computations: Toward a unified measure of complexity.
\newblock In \emph{18th Annual Symposium on Foundations of Computer Science},
  pages 222--227, 1977.

\end{thebibliography}
