\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasnejad et~al.(2015)Abbasnejad, Domke, and Sanner]{abbasnejad2015losscalibrated}
Ehsan Abbasnejad, Justin Domke, and Scott Sanner.
\newblock Loss-calibrated {{Monte Carlo}} action selection.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~29 of \emph{AAAI}. AAAI Press, March 2015.

\bibitem[Abbasnejad et~al.(2013)Abbasnejad, Bonilla, and Sanner]{abbasnejad2013decisiontheoretic}
M.~Ehsan Abbasnejad, Edwin~V. Bonilla, and Scott Sanner.
\newblock Decision-theoretic sparsification for {{Gaussian}} process preference learning.
\newblock In \emph{Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}}, volume 13717 of \emph{{LNCS}}, pages 515--530, {Berlin, Heidelberg}, 2013. {Springer}.

\bibitem[Aglietti et~al.(2020)Aglietti, Lu, Paleyes, and Gonz{\'a}lez]{aglietti2020causal}
Virginia Aglietti, Xiaoyu Lu, Andrei Paleyes, and Javier Gonz{\'a}lez.
\newblock Causal {{Bayesian}} optimization.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}, volume 108 of \emph{{{PMLR}}}, pages 3155--3164. {JMLR}, June 2020.

\bibitem[Balandat et~al.(2020)Balandat, Karrer, Jiang, Daulton, Letham, Wilson, and Bakshy]{balandat2020botorch}
Maximilian Balandat, Brian Karrer, Daniel~R. Jiang, Samuel Daulton, Benjamin Letham, Andrew~Gordon Wilson, and Eytan Bakshy.
\newblock {BoTorch}: A framework for efficient {{Monte-Carlo Bayesian}} optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pages 21524--21538. Curran Associates, Inc., 2020.

\bibitem[Bissiri et~al.(2016)Bissiri, Holmes, and Walker]{bissiri2016general}
P.~G. Bissiri, C.~C. Holmes, and S.~G. Walker.
\newblock A general framework for updating belief distributions.
\newblock \emph{Journal of the Royal Statistical Society Series B: Statistical Methodology}, 78\penalty0 (5):\penalty0 1103--1130, 2016.

\bibitem[Blei et~al.(2017)Blei, Kucukelbir, and McAuliffe]{blei2017variational}
David~M. Blei, Alp Kucukelbir, and Jon~D. McAuliffe.
\newblock Variational inference: {{A}} review for statisticians.
\newblock \emph{Journal of the American Statistical Association}, 112\penalty0 (518):\penalty0 859--877, April 2017.

\bibitem[Brown et~al.(2019)Brown, Fiscato, Segler, and Vaucher]{GuacaMol}
Nathan Brown, Marco Fiscato, Marwin~H.S. Segler, and Alain~C. Vaucher.
\newblock Guacamol: Benchmarking models for de novo molecular design.
\newblock \emph{Journal of Chemical Information and Modeling}, 59\penalty0 (3):\penalty0 1096–1108, Mar 2019.

\bibitem[Cobb et~al.(2018)Cobb, Roberts, and Gal]{cobb2018losscalibrated}
Adam~D. Cobb, Stephen~J. Roberts, and Yarin Gal.
\newblock Loss-{{Calibrated Approximate Inference}} in {{Bayesian Neural Networks}}.
\newblock {{arXiv}} Preprint arXiv:1805.03901, {arXiv}, May 2018.

\bibitem[Dempster et~al.(1977)Dempster, Laird, and Rubin]{dempster1977maximum}
A.~P. Dempster, N.~M. Laird, and D.~B. Rubin.
\newblock Maximum likelihood from incomplete data via the {{EM}} algorithm.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Methodological)}, 39\penalty0 (1):\penalty0 1--22, September 1977.

\bibitem[Eriksson et~al.(2019)Eriksson, Pearce, Gardner, Turner, and Poloczek]{turbo}
David Eriksson, Michael Pearce, Jacob Gardner, Ryan~D Turner, and Matthias Poloczek.
\newblock Scalable global optimization via local {{Bayesian}} optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~32, pages 5496--5507. Curran Associates, Inc., 2019.

\bibitem[Frazier(2009)]{frazier2009knowledge}
Peter~I Frazier.
\newblock \emph{Knowledge-gradient methods for statistical learning}.
\newblock PhD thesis, Princeton University Princeton, 2009.

\bibitem[Frazier(2018)]{frazier2018tutorial}
Peter~I Frazier.
\newblock A tutorial on {Bayesian} optimization.
\newblock {{arXiv}} Preprint arXiv:1807.02811, ArXiv, 2018.

\bibitem[{Galy-Fajou} and Opper(2021)]{galy-fajou2021adaptive}
Th{\'e}o {Galy-Fajou} and Manfred Opper.
\newblock Adaptive inducing points selection for {{Gaussian}} processes.
\newblock {{arXiv}} Preprint arXiv:2107.10066, {arXiv}, 2021.

\bibitem[Gardner et~al.(2018)Gardner, Pleiss, Weinberger, Bindel, and Wilson]{gardner2018gpytorch}
Jacob Gardner, Geoff Pleiss, Kilian~Q. Weinberger, David Bindel, and Andrew~G. Wilson.
\newblock {{GPyTorch}}: {{Blackbox}} matrix-matrix {{Gaussian}} process inference with {{GPU}} acceleration.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, volume~31, pages 7576--7586. {Curran Associates, Inc.}, 2018.

\bibitem[Garnett(2023)]{garnett2023bayesian}
Roman Garnett.
\newblock \emph{Bayesian Optimization}.
\newblock {Cambridge University Press}, {Cambridge, United Kingdom ; New York, NY}, 2023.

\bibitem[Gasso et~al.(2009)Gasso, Rakotomamonjy, and Canu]{gasso2009recovering}
Gilles Gasso, Alain Rakotomamonjy, and Stéphane Canu.
\newblock Recovering sparse signals with a certain family of nonconvex penalties and {{DC}} programming.
\newblock \emph{IEEE Transactions on Signal Processing}, 57\penalty0 (12):\penalty0 4686--4698, 2009.

\bibitem[Griffiths and {Hern{\'a}ndez-Lobato}(2020)]{griffiths2020constrained}
Ryan-Rhys Griffiths and Jos{\'e}~Miguel {Hern{\'a}ndez-Lobato}.
\newblock Constrained {{Bayesian}} optimization for automatic chemical design using variational autoencoders.
\newblock \emph{Chemical Science}, 11\penalty0 (2):\penalty0 577--586, 2020.

\bibitem[Hensman et~al.(2013)Hensman, Fusi, and Lawrence]{hensman2013gaussian}
James Hensman, Nicolo Fusi, and Neil~D. Lawrence.
\newblock Gaussian processes for big data.
\newblock In \emph{Proceedings of the Conference on Uncertainty in Artificial Intelligence}, pages 282--290. {AUAI Press}, 2013.

\bibitem[{Hern{\'a}ndez-Lobato} et~al.(2017){Hern{\'a}ndez-Lobato}, Requeima, {Pyzer-Knapp}, and {Aspuru-Guzik}]{hernandez-lobato2017parallel}
Jos{\'e}~Miguel {Hern{\'a}ndez-Lobato}, James Requeima, Edward~O. {Pyzer-Knapp}, and Al{\'a}n {Aspuru-Guzik}.
\newblock Parallel and distributed {{Thompson}} sampling for large-scale accelerated exploration of chemical space.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine Learning}}}, volume~70 of \emph{{{PMLR}}}, pages 1470--1479. {JMLR}, July 2017.

\bibitem[Jaiswal et~al.(2020)Jaiswal, Honnappa, and Rao]{jaiswal2020asymptotic}
Prateek Jaiswal, Harsha Honnappa, and Vinayak~A. Rao.
\newblock Asymptotic consistency of loss-calibrated variational {{Bayes}}.
\newblock \emph{Stat}, 9\penalty0 (1):\penalty0 e258, 2020.

\bibitem[Jaiswal et~al.(2023)Jaiswal, Honnappa, and Rao]{jaiswal2023statistical}
Prateek Jaiswal, Harsha Honnappa, and Vinayak Rao.
\newblock On the statistical consistency of risk-sensitive bayesian decision-making.
\newblock In \emph{{Advances} in {Neural Information Processing Systems}}, volume~36, pages 53158--53200. {Curran Associates, Inc.}, December 2023.

\bibitem[Jones et~al.(1998)Jones, Schonlau, and Welch]{jones1998efficient}
Donald~R. Jones, Matthias Schonlau, and William~J. Welch.
\newblock Efficient global optimization of expensive black-box functions.
\newblock \emph{Journal of Global Optimization}, 13\penalty0 (4):\penalty0 455--492, 1998.

\bibitem[Jordan et~al.(1999)Jordan, Ghahramani, Jaakkola, and Saul]{jordan1999introduction}
Michael~I. Jordan, Zoubin Ghahramani, Tommi~S. Jaakkola, and Lawrence~K. Saul.
\newblock An introduction to variational methods for graphical models.
\newblock \emph{Machine Learning}, 37\penalty0 (2):\penalty0 183--233, 1999.

\bibitem[Kingma and Ba(2015)]{kingma2015adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {{A Method}} for {{Stochastic Optimization}}.
\newblock In \emph{Proceedings of the {{International}} Conference on Learning Representations}, {San Diego, California, USA}, 2015.

\bibitem[Kingma and Welling(2014)]{kingma2013auto}
Diederik~P. Kingma and Max Welling.
\newblock Auto-encoding variational {{Bayes}}.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Learning Representations}}}, {Banff, AB, Canada}, April 2014.

\bibitem[Knoblauch et~al.(2022)Knoblauch, Jewson, and Damoulas]{knoblauch2022optimizationcentric}
Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas.
\newblock An optimization-centric view on {{Bayes}}' rule: Reviewing and generalizing variational inference.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0 (132):\penalty0 1--109, 2022.

\bibitem[Kulesza and Taskar(2012)]{kulesza2012determinantal}
Alex Kulesza and Ben Taskar.
\newblock Determinantal point processes for machine learning.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 5\penalty0 (2{\textendash}3):\penalty0 123--286, 2012.

\bibitem[Ku{\'s}mierczyk et~al.(2019)Ku{\'s}mierczyk, Sakaya, and Klami]{kusmierczyk2019variational}
Tomasz Ku{\'s}mierczyk, Joseph Sakaya, and Arto Klami.
\newblock Variational {{Bayesian}} decision-making for continuous utilities.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, volume~32, pages 6395--6405. {Curran Associates, Inc.}, 2019.

\bibitem[{Lacoste{\textendash}Julien} et~al.(2011){Lacoste{\textendash}Julien}, Husz{\'a}r, and Ghahramani]{lacostejulien2011approximate}
Simon {Lacoste{\textendash}Julien}, Ferenc Husz{\'a}r, and Zoubin Ghahramani.
\newblock Approximate inference for the loss-calibrated {{Bayesian}}.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}, volume~15 of \emph{{{PMLR}}}, pages 416--424. {JMLR}, June 2011.

\bibitem[Lange(2016)]{lange2016mm}
Kenneth Lange.
\newblock \emph{{{MM}} Optimization Algorithms}.
\newblock {Society for Industrial and Applied Mathematics}, {Philadelphia}, 2016.

\bibitem[Li and Zhang(2023)]{li2023longtailed}
Bolian Li and Ruqi Zhang.
\newblock Long-tailed {{Classification}} from a {{Bayesian-decision-theory Perspective}}.
\newblock {{arXiv}} Preprint arXiv:2303.06075, {arXiv}, 2023.

\bibitem[Maddox et~al.(2021)Maddox, Stanton, and Wilson]{maddox2021conditioning}
Wesley~J Maddox, Samuel Stanton, and Andrew~G Wilson.
\newblock Conditioning sparse variational {{Gaussian}} processes for online decision-making.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, volume~34, pages 6365--6379. {Curran Associates, Inc.}, 2021.

\bibitem[Matthews et~al.(2016)Matthews, Hensman, Turner, and Ghahramani]{matthews2016sparse}
Alexander G. de~G. Matthews, James Hensman, Richard Turner, and Zoubin Ghahramani.
\newblock On sparse variational methods and the {{Kullback-Leibler}} divergence between stochastic processes.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}, volume~51 of \emph{{{PMLR}}}, pages 231--239. {JMLR}, May 2016.

\bibitem[Maus et~al.(2022)Maus, Jones, Moore, Kusner, Bradshaw, and Gardner]{lolbo}
Natalie Maus, Haydn Jones, Juston Moore, Matt~J. Kusner, John Bradshaw, and Jacob Gardner.
\newblock Local latent space {{Bayesian}} optimization over structured inputs.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~35, pages 34505--34518, December 2022.

\bibitem[Maus et~al.(2023)Maus, Wu, Eriksson, and Gardner]{robot}
Natalie Maus, Kaiwen Wu, David Eriksson, and Jacob Gardner.
\newblock Discovering many diverse solutions with {{Bayesian}} optimization.
\newblock In \emph{Proceedings of {{the International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}, volume 206, pages 1779--1798. {PMLR}, April 2023.

\bibitem[McIntire et~al.(2016)McIntire, Ratner, and Ermon]{mcintire2016sparse}
Mitchell McIntire, Daniel Ratner, and Stefano Ermon.
\newblock Sparse {{Gaussian Processes}} for {{Bayesian Optimization}}.
\newblock In \emph{Proceedings of the {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}}, {Jersey City, New Jersey, USA}, 2016. {AUAI Press}.

\bibitem[Minka(2001)]{minka2001expectation}
Thomas~P. Minka.
\newblock Expectation propagation for approximate bayesian inference.
\newblock In \emph{Proceedings of the Conference on Uncertainty in Artificial Intelligence}, pages 362--369, {San Francisco, CA, USA}, 2001. {Morgan Kaufmann Publishers Inc.}

\bibitem[Mockus(1982)]{mockus1982bayesian}
Jonas Mockus.
\newblock The {Bayesian} approach to global optimization.
\newblock In \emph{System Modeling and Optimization}, pages 473--481. Springer, 1982.

\bibitem[Morais and Pillow(2022)]{morais022losscalibrated}
Michael~J. Morais and Jonathan~W. Pillow.
\newblock Loss-calibrated expectation propagation for approximate {{Bayesian}} decision-making.
\newblock Technical Report arXiv:2201.03128, {arXiv}, January 2022.

\bibitem[Moss et~al.(2023)Moss, Ober, and Picheny]{moss2023inducing}
Henry~B. Moss, Sebastian~W. Ober, and Victor Picheny.
\newblock Inducing point allocation for sparse {{Gaussian}} processes in high-throughput {{Bayesian}} optimisation.
\newblock In \emph{Proceedings of {{the}} {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}, volume 206 of \emph{{PMLR}}, pages 5213--5230. {JMLR}, April 2023.

\bibitem[Neal(1996)]{neal1996bayesian}
Radford~M. Neal.
\newblock \emph{Bayesian {{Learning}} for {{Neural Networks}}}, volume 118 of \emph{Lecture {{Notes}} in {{Statistics}}}.
\newblock {Springer New York}, {New York, NY}, 1996.

\bibitem[{Qui{\~n}onero-Candela} and Rasmussen(2005)]{quinonero-candela2005unifying}
Joaquin {Qui{\~n}onero-Candela} and Carl~Edward Rasmussen.
\newblock A unifying view of sparse approximate {{Gaussian}} process regression.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0 (65):\penalty0 1939--1959, 2005.

\bibitem[Ranganath et~al.(2014)Ranganath, Gerrish, and Blei]{ranganath2014black}
Rajesh Ranganath, Sean Gerrish, and David Blei.
\newblock Black box variational inference.
\newblock In \emph{Proceedings of the International Conference on Artificial Intelligence and Statistics}, volume~33 of \emph{{{PMLR}}}, pages 814--822. {JMLR}, April 2014.

\bibitem[Rasmussen and Williams(2005)]{rasmussen2005gaussian}
Carl~Edward Rasmussen and Christopher K.~I. Williams.
\newblock \emph{Gaussian Processes for Machine Learning}.
\newblock {The MIT Press}, November 2005.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and Wierstra]{rezende2014stochastic}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep generative models.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine Learning}}}, volume~32 of \emph{{{PMLR}}}, pages 1278--1286. {JMLR}, June 2014.

\bibitem[Robert(2001)]{robert2001bayesian}
Christian~P. Robert.
\newblock \emph{The {{Bayesian}} Choice: From Decision-Theoretic Foundations to Computational Implementation}.
\newblock Springer Texts in Statistics. {Springer}, {New York Berlin Heidelberg}, 2. ed edition, 2001.

\bibitem[Shahriari et~al.(2015)Shahriari, Swersky, Wang, Adams, and De~Freitas]{shahriari2015taking}
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan~P Adams, and Nando De~Freitas.
\newblock Taking the human out of the loop: {A} review of {Bayesian} optimization.
\newblock \emph{Proceedings of the IEEE}, 104\penalty0 (1):\penalty0 148--175, 2015.

\bibitem[Snelson and Ghahramani(2005)]{snelson2005sparse}
Edward Snelson and Zoubin Ghahramani.
\newblock Sparse {{Gaussian}} processes using pseudo-inputs.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~18, pages 1257--1264. {MIT Press}, 2005.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{snoek2012practical}
Jasper Snoek, Hugo Larochelle, and Ryan~P Adams.
\newblock Practical {Bayesian} optimization of machine learning algorithms.
\newblock \emph{Advances in neural information processing systems}, 25:\penalty0 2951--2959, 2012.

\bibitem[Snoek et~al.(2015)Snoek, Rippel, Swersky, Kiros, Satish, Sundaram, Patwary, Prabhat, and Adams]{snoek2015scalable}
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Mostofa Patwary, Mr~Prabhat, and Ryan Adams.
\newblock Scalable {{Bayesian}} optimization using deep neural networks.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine Learning}}}, volume~37 of \emph{{{PMLR}}}, pages 2171--2180. {JMLR}, June 2015.

\bibitem[Springenberg et~al.(2016)Springenberg, Klein, Falkner, and Hutter]{springenberg2016bayesian}
Jost~Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter.
\newblock Bayesian {{Optimization}} with {{Robust Bayesian Neural Networks}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, volume~29, pages 4134--4142. {Curran Associates, Inc.}, 2016.

\bibitem[Stanton et~al.(2022)Stanton, Maddox, Gruver, Maffettone, Delaney, Greenside, and Wilson]{stanton2022accelerating}
Samuel Stanton, Wesley Maddox, Nate Gruver, Phillip Maffettone, Emily Delaney, Peyton Greenside, and Andrew~Gordon Wilson.
\newblock Accelerating {{Bayesian}} optimization for biological sequence design with denoising autoencoders.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine Learning}}}, volume 162 of \emph{{{PMLR}}}, pages 20459--20478. {JMLR}, June 2022.

\bibitem[Surjanovic and Bingham(2013)]{surjanovic2013virtual}
Sonja Surjanovic and Derek Bingham.
\newblock Virtual library of simulation experiments: Test functions and datasets, 2013.

\bibitem[Terenin et~al.(2024)Terenin, Burt, Artemev, Flaxman, van~der Wilk, Rasmussen, and Ge]{terenin2024numerically}
Alexander Terenin, David~R. Burt, Artem Artemev, Seth Flaxman, Mark van~der Wilk, Carl~Edward Rasmussen, and Hong Ge.
\newblock Numerically stable sparse {{Gaussian}} processes via minimum separation using cover trees.
\newblock \emph{Journal of Machine Learning Research}, 25\penalty0 (26):\penalty0 1--36, 2024.

\bibitem[Titsias(2009)]{titsias2009variational}
Michalis Titsias.
\newblock Variational learning of inducing variables in sparse gaussian processes.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}, volume~5 of \emph{{{PMLR}}}, pages 567--574. {JMLR}, April 2009.

\bibitem[Titsias and {L{\'a}zaro-Gredilla}(2014)]{titsias2014doubly}
Michalis Titsias and Miguel {L{\'a}zaro-Gredilla}.
\newblock Doubly stochastic variational {{Bayes}} for non-conjugate inference.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine Learning}}}, volume~32 of \emph{{{PMLR}}}, pages 1971--1979. {JMLR}, June 2014.

\bibitem[Tripp et~al.(2020)Tripp, Daxberger, and {Hern{\'a}ndez-Lobato}]{tripp2020sampleefficient}
Austin Tripp, Erik Daxberger, and Jos{\'e}~Miguel {Hern{\'a}ndez-Lobato}.
\newblock Sample-efficient optimization in the latent space of deep generative models via weighted retraining.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, volume~33, pages 11259--11272. {Curran Associates, Inc.}, 2020.

\bibitem[Vakili et~al.(2021)Vakili, Moss, Artemev, Dutordoir, and Picheny]{vakili2021scalable}
Sattar Vakili, Henry Moss, Artem Artemev, Vincent Dutordoir, and Victor Picheny.
\newblock Scalable {{Thompson}} sampling using sparse {{Gaussian}} process models.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, volume~34, pages 5631--5643, 2021.

\bibitem[\v{S}ehi\'c et~al.(2022)\v{S}ehi\'c, Gramfort, Salmon, and Nardi]{lassobench}
Kenan \v{S}ehi\'c, Alexandre Gramfort, Joseph Salmon, and Luigi Nardi.
\newblock Lassobench: A high-dimensional hyperparameter optimization benchmark suite for {{LASSO}}.
\newblock In \emph{Proceedings of the International Conference on Automated Machine Learning}, volume 188 of \emph{PMLR}, pages 2/1--24. JMLR, 25--27 Jul 2022.

\bibitem[Wang and Blei(2019)]{wang2019frequentist}
Yixin Wang and David~M. Blei.
\newblock Frequentist consistency of variational {{Bayes}}.
\newblock \emph{Journal of the American Statistical Association}, 114\penalty0 (527):\penalty0 1147--1161, July 2019.

\bibitem[Wang et~al.(2018)Wang, Gehring, Kohli, and Jegelka]{ebo}
Zi~Wang, Clement Gehring, Pushmeet Kohli, and Stefanie Jegelka.
\newblock Batched large-scale bayesian optimization in high-dimensional spaces.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}, volume~84 of \emph{{{PMLR}}}, pages 745--754. {JMLR}, March 2018.

\bibitem[Wasserman(2013)]{wasserman2013all}
Larry Wasserman.
\newblock \emph{All of statistics: a concise course in statistical inference}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Wei et~al.(2021)Wei, Sheth, and Khardon]{wei2021direct}
Yadi Wei, Rishit Sheth, and Roni Khardon.
\newblock Direct loss minimization for sparse {{Gaussian}} processes.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}, volume 130 of \emph{{PMLR}}, pages 2566--2574. {JMLR}, March 2021.

\bibitem[Wilson et~al.(2018)Wilson, Hutter, and Deisenroth]{wilson2018maximizing}
James Wilson, Frank Hutter, and Marc Deisenroth.
\newblock Maximizing acquisition functions for {{Bayesian}} optimization.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, pages 9884--9895. {Curran Associates, Inc.}, 2018.

\bibitem[Wu et~al.(2017)Wu, Poloczek, Wilson, and Frazier]{wu2017bayesian}
Jian Wu, Matthias Poloczek, Andrew~G Wilson, and Peter Frazier.
\newblock Bayesian optimization with gradients.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}}, volume~30, pages 5267--5278. {Curran Associates, Inc.}, 2017.

\end{thebibliography}
