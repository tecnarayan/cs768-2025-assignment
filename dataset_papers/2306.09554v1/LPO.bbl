\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.()Abbasi-Yadkori, Bartle, Bhatia, Lazi{\'c},
  Szepesv{\'a}ri, and Weisz]{abbasip}
Abbasi-Yadkori, Y., Bartle, P.~L., Bhatia, K., Lazi{\'c}, N., Szepesv{\'a}ri,
  C., and Weisz, G.
\newblock P: Regret bounds for policy iteration using expert prediction.

\bibitem[Agarwal et~al.(2020{\natexlab{a}})Agarwal, Henaff, Kakade, and
  Sun]{agarwal2020pc}
Agarwal, A., Henaff, M., Kakade, S., and Sun, W.
\newblock Pc-pg: Policy cover directed exploration for provable policy gradient
  learning.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 13399--13412, 2020{\natexlab{a}}.

\bibitem[Agarwal et~al.(2020{\natexlab{b}})Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2020optimality}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G.
\newblock Optimality and approximation with policy gradient methods in markov
  decision processes.
\newblock In \emph{Conference on Learning Theory}, pp.\  64--66. PMLR,
  2020{\natexlab{b}}.

\bibitem[Bai et~al.(2019)Bai, Xie, Jiang, and Wang]{bai2019provably}
Bai, Y., Xie, T., Jiang, N., and Wang, Y.-X.
\newblock Provably efficient q-learning with low switching cost.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[{Bellemare} et~al.(2013){Bellemare}, {Naddaf}, {Veness}, and
  {Bowling}]{bellemare13arcade}
{Bellemare}, M.~G., {Naddaf}, Y., {Veness}, J., and {Bowling}, M.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, jun 2013.

\bibitem[Beygelzimer et~al.(2011)Beygelzimer, Langford, Li, Reyzin, and
  Schapire]{beygelzimer2011contextual}
Beygelzimer, A., Langford, J., Li, L., Reyzin, L., and Schapire, R.
\newblock Contextual bandit algorithms with supervised learning guarantees.
\newblock In \emph{Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  19--26. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Bhandari \& Russo(2019)Bhandari and Russo]{bhandari2019global}
Bhandari, J. and Russo, D.
\newblock Global optimality guarantees for policy gradient methods.
\newblock \emph{arXiv preprint arXiv:1906.01786}, 2019.

\bibitem[Brafman \& Tennenholtz(2002)Brafman and Tennenholtz]{brafman2002r}
Brafman, R.~I. and Tennenholtz, M.
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Oct):\penalty0 213--231, 2002.

\bibitem[Burda et~al.(2018)Burda, Edwards, Storkey, and
  Klimov]{burda2018exploration}
Burda, Y., Edwards, H., Storkey, A., and Klimov, O.
\newblock Exploration by random network distillation.
\newblock \emph{arXiv preprint arXiv:1810.12894}, 2018.

\bibitem[Cai et~al.(2020)Cai, Yang, Jin, and Wang]{cai2020provably}
Cai, Q., Yang, Z., Jin, C., and Wang, Z.
\newblock Provably efficient exploration in policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1283--1294. PMLR, 2020.

\bibitem[Chen et~al.(2022)Chen, Li, Yuan, Gu, and Jordan]{chen2022general}
Chen, Z., Li, C.~J., Yuan, A., Gu, Q., and Jordan, M.~I.
\newblock A general framework for sample-efficient function approximation in
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2209.15634}, 2022.

\bibitem[Feng et~al.(2021)Feng, Yin, Agarwal, and Yang]{feng2021provably}
Feng, F., Yin, W., Agarwal, A., and Yang, L.
\newblock Provably correct optimization and exploration with non-linear
  policies.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3263--3273. PMLR, 2021.

\bibitem[Foster et~al.(2021)Foster, Kakade, Qian, and
  Rakhlin]{foster2021statistical}
Foster, D.~J., Kakade, S.~M., Qian, J., and Rakhlin, A.
\newblock The statistical complexity of interactive decision making.
\newblock \emph{arXiv preprint arXiv:2112.13487}, 2021.

\bibitem[Freedman(1975)]{freedman1975tail}
Freedman, D.~A.
\newblock On tail probabilities for martingales.
\newblock \emph{the Annals of Probability}, pp.\  100--118, 1975.

\bibitem[Gao et~al.(2021)Gao, Xie, Du, and Yang]{gao2021provably}
Gao, M., Xie, T., Du, S.~S., and Yang, L.~F.
\newblock A provably efficient algorithm for linear markov decision process
  with low switching cost.
\newblock \emph{arXiv preprint arXiv:2101.00494}, 2021.

\bibitem[Geist et~al.(2019)Geist, Scherrer, and Pietquin]{geist2019theory}
Geist, M., Scherrer, B., and Pietquin, O.
\newblock A theory of regularized markov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2160--2169. PMLR, 2019.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pp.\
  1861--1870. PMLR, 2018.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I.
\newblock Is q-learning provably efficient?
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pp.\  2137--2143. PMLR,
  2020.

\bibitem[Jin et~al.(2021)Jin, Liu, and Miryoosefi]{jin2021bellman}
Jin, C., Liu, Q., and Miryoosefi, S.
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 13406--13418, 2021.

\bibitem[Kakade(2001)]{kakade2001natural}
Kakade, S.~M.
\newblock A natural policy gradient.
\newblock \emph{Advances in neural information processing systems}, 14, 2001.

\bibitem[Kearns \& Singh(2002)Kearns and Singh]{kearns2002near}
Kearns, M. and Singh, S.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine learning}, 49\penalty0 (2):\penalty0 209--232, 2002.

\bibitem[Kearns(1989)]{kearns89}
Kearns, M.~J.
\newblock \emph{Computational Complexity of Machine Learning}.
\newblock PhD thesis, Department of Computer Science, Harvard University, 1989.

\bibitem[Konda \& Tsitsiklis(1999)Konda and Tsitsiklis]{konda1999actor}
Konda, V. and Tsitsiklis, J.
\newblock Actor-critic algorithms.
\newblock \emph{Advances in neural information processing systems}, 12, 1999.

\bibitem[Kong et~al.(2021)Kong, Salakhutdinov, Wang, and Yang]{kong2021online}
Kong, D., Salakhutdinov, R., Wang, R., and Yang, L.~F.
\newblock Online sub-sampling for reinforcement learning with general function
  approximation.
\newblock \emph{arXiv preprint arXiv:2106.07203}, 2021.

\bibitem[Li et~al.(2022)Li, Zhai, Ma, and Levine]{li2022understanding}
Li, Q., Zhai, Y., Ma, Y., and Levine, S.
\newblock Understanding the complexity gains of single-task rl with a
  curriculum.
\newblock \emph{arXiv preprint arXiv:2212.12809}, 2022.

\bibitem[Precup(2000)]{precup2000eligibility}
Precup, D.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock \emph{Computer Science Department Faculty Publication Series}, pp.\
  ~80, 2000.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Raffin et~al.(2021)Raffin, Hill, Gleave, Kanervisto, Ernestus, and
  Dormann]{stable-baselines3}
Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., and Dormann, N.
\newblock Stable-baselines3: Reliable reinforcement learning implementations.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (268):\penalty0 1--8, 2021.
\newblock URL \url{http://jmlr.org/papers/v22/20-1364.html}.

\bibitem[Russo \& Van~Roy(2013)Russo and Van~Roy]{russo2013eluder}
Russo, D. and Van~Roy, B.
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Schulman et~al.(2015{\natexlab{a}})Schulman, Levine, Abbeel, Jordan,
  and Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897. PMLR, 2015{\natexlab{a}}.

\bibitem[Schulman et~al.(2015{\natexlab{b}})Schulman, Moritz, Levine, Jordan,
  and Abbeel]{schulman2015high}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock \emph{arXiv preprint arXiv:1506.02438}, 2015{\natexlab{b}}.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Shani et~al.(2020)Shani, Efroni, Rosenberg, and
  Mannor]{shani2020optimistic}
Shani, L., Efroni, Y., Rosenberg, A., and Mannor, S.
\newblock Optimistic policy optimization with bandit feedback.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8604--8613. PMLR, 2020.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{6386109}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pp.\  5026--5033, 2012.
\newblock \doi{10.1109/IROS.2012.6386109}.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Du, Yang, and
  Salakhutdinov]{wang2020reward}
Wang, R., Du, S.~S., Yang, L., and Salakhutdinov, R.~R.
\newblock On reward-free reinforcement learning with linear function
  approximation.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 17816--17826, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Salakhutdinov, and
  Yang]{wang2020reinforcement}
Wang, R., Salakhutdinov, R.~R., and Yang, L.
\newblock Reinforcement learning with general value function approximation:
  Provably efficient approach via bounded eluder dimension.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6123--6135, 2020{\natexlab{b}}.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3):\penalty0 229--256, 1992.

\bibitem[Yang \& Wang(2019)Yang and Wang]{yang2019sample}
Yang, L. and Wang, M.
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6995--7004. PMLR, 2019.

\bibitem[Yang \& Wang(2020)Yang and Wang]{yang2020reinforcement}
Yang, L. and Wang, M.
\newblock Reinforcement learning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10746--10756. PMLR, 2020.

\bibitem[Zanette et~al.(2021)Zanette, Cheng, and
  Agarwal]{zanette2021cautiously}
Zanette, A., Cheng, C.-A., and Agarwal, A.
\newblock Cautiously optimistic policy optimization and exploration with linear
  function approximation.
\newblock In \emph{Conference on Learning Theory}, pp.\  4473--4525. PMLR,
  2021.

\end{thebibliography}
