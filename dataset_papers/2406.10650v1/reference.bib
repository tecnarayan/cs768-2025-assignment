@inproceedings{chen2019convergence,
  title={On the convergence of a class of Adam-type algorithms for non-convex optimization},
  author={Chen, Xiangyi and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
  booktitle={7th International Conference on Learning Representations, ICLR 2019},
  year={2019}
}


@article{liu2020adam,
  title={Adam+: A Stochastic Method with Adaptive Variance Reduction},
  author={Liu, Mingrui and Zhang, Wei and Orabona, Francesco and Yang, Tianbao},
  journal={arXiv preprint arXiv:2011.11985},
  year={2020}
}


@article{huang2021super,
  title={Super-adam: faster and universal framework of adaptive gradients},
  author={Huang, Feihu and Li, Junyi and Huang, Heng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9074--9085},
  year={2021}
}


@inproceedings{gunasekar2017implicit,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nathan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6151--6159},
  year={2017}
}



@inproceedings{li2018algorithmic,
  title={Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  booktitle={Conference On Learning Theory},
  pages={2--47},
  year={2018}
}


@article{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@InProceedings{pmlr-v99-ji19a,
  title = 	 {The implicit bias of gradient descent on nonseparable data},
  author =       {Ji, Ziwei and Telgarsky, Matus},
  booktitle = 	 {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages = 	 {1772--1798},
  year = 	 {2019},
  volume = 	 {99},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--28 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v99/ji19a/ji19a.pdf},
  url = 	 {https://proceedings.mlr.press/v99/ji19a.html},
  abstract = 	 {Gradient descent, when applied to the task of logistic regression, outputs iterates which are biased to follow a unique ray defined by the data. The direction of this ray is the maximum margin predictor of a maximal linearly separable subset of the data; the gradient descent iterates converge to this ray in direction at the rate $\cO(\nicefrac{\ln\ln t }{\ln t})$. The ray does not pass through the origin in general, and its offset is the bounded global optimum of the risk over the remaining data; gradient descent recovers this offset at a rate $\cO(\nicefrac{(\ln t)^2}{\sqrt{t}})$.}
}

@inproceedings{NEURIPS2022_ab3f6bbe,
 author = {Wang, Bohan and Meng, Qi and Zhang, Huishuai and Sun, Ruoyu and Chen, Wei and Ma, Zhi-Ming and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {26764--26776},
 publisher = {Curran Associates, Inc.},
 title = {Does Momentum Change the Implicit Regularization on Separable Data?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/ab3f6bbe121a8f7a0263a9b393000741-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@misc{xie2024implicit,
      title={Implicit Bias of AdamW: $\ell_\infty$ Norm Constrained Optimization}, 
      author={Shuo Xie and Zhiyuan Li},
      year={2024},
      eprint={2404.04454},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@InProceedings{pmlr-v80-gunasekar18a,
  title = 	 {Characterizing Implicit Bias in Terms of Optimization Geometry},
  author =       {Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1832--1841},
  year = 	 {2018},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/gunasekar18a/gunasekar18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/gunasekar18a.html},
  abstract = 	 {We study the bias of generic optimization methods, including Mirror Descent, Natural Gradient Descent and Steepest Descent with respect to different potentials and norms, when optimizing underdetermined linear models or separable linear classification problems. We ask the question of whether the global minimum (among the many possible global minima) reached by optimization can be characterized in terms of the potential or norm, and indecently of hyper-parameter choices, such as stepsize and momentum.}
}

@inproceedings{DBLP:conf/iclr/Zou0LG23,
  author       = {Difan Zou and
                  Yuan Cao and
                  Yuanzhi Li and
                  Quanquan Gu},
  title        = {Understanding the Generalization of Adam in Learning Neural Networks
                  with Proper Regularization},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=iUYpN14qjTF},
  timestamp    = {Fri, 30 Jun 2023 14:55:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/Zou0LG23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/icml/Telgarsky13,
  author       = {Matus Telgarsky},
  title        = {Margins, Shrinkage, and Boosting},
  booktitle    = {Proceedings of the 30th International Conference on Machine Learning,
                  {ICML} 2013},
  series       = {{JMLR} Workshop and Conference Proceedings},
  volume       = {28},
  pages        = {307--315},
  publisher    = {JMLR.org},
  year         = {2013},
  url          = {http://proceedings.mlr.press/v28/telgarsky13.html},
  timestamp    = {Wed, 29 May 2019 08:41:46 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/Telgarsky13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/alt/JiT21,
  author       = {Ziwei Ji and
                  Matus Telgarsky},
  title        = {Characterizing the implicit bias via a primal-dual analysis},
  booktitle    = {Algorithmic Learning Theory, 16-19 March 2021, Virtual Conference,
                  Worldwide},
  series       = {Proceedings of Machine Learning Research},
  volume       = {132},
  pages        = {772--804},
  publisher    = {{PMLR}},
  year         = {2021},
  url          = {http://proceedings.mlr.press/v132/ji21a.html},
  timestamp    = {Fri, 26 Mar 2021 15:45:50 +0100},
  biburl       = {https://dblp.org/rec/conf/alt/JiT21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{10.7551/mitpress/8291.001.0001,
    author = {Schapire, Robert E. and Freund, Yoav},
    title = "{Boosting: Foundations and Algorithms}",
    publisher = {The MIT Press},
    year = {2012},
    month = {05},
    abstract = "{An accessible introduction and essential reference for an approach to machine learning that creates highly accurate prediction rules by combining many weak and inaccurate ones.Boosting is an approach to machine learning based on the idea of creating a highly accurate predictor by combining many weak and inaccurate “rules of thumb.” A remarkably rich theory has evolved around boosting, with connections to a range of topics, including statistics, game theory, convex optimization, and information geometry. Boosting algorithms have also enjoyed practical success in such fields as biology, vision, and speech processing. At various times in its history, boosting has been perceived as mysterious, controversial, even paradoxical.This book, written by the inventors of the method, brings together, organizes, simplifies, and substantially extends two decades of research on boosting, presenting both theory and applications in a way that is accessible to readers from diverse backgrounds while also providing an authoritative reference for advanced researchers. With its introductory treatment of all material and its inclusion of exercises in every chapter, the book is appropriate for course use as well.The book begins with a general introduction to machine learning algorithms and their analysis; then explores the core theory of boosting, especially its ability to generalize; examines some of the myriad other theoretical viewpoints that help to explain and understand boosting; provides practical extensions of boosting for more complex learning problems; and finally presents a number of advanced theoretical topics. Numerous applications and practical illustrations are offered throughout.}",
    isbn = {9780262301183},
    doi = {10.7551/mitpress/8291.001.0001},
    url = {https://doi.org/10.7551/mitpress/8291.001.0001},
    eprint = {https://direct.mit.edu/book-pdf/2280056/book\_9780262301183.pdf},
}


@inproceedings{DBLP:conf/iclr/KunstnerCL023,
  author       = {Frederik Kunstner and
                  Jacques Chen and
                  Jonathan Wilder Lavington and
                  Mark Schmidt},
  title        = {Noise Is Not the Main Factor Behind the Gap Between Sgd and Adam on
                  Transformers, But Sign Descent Might Be},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=a65YK0cqH8g},
  timestamp    = {Fri, 30 Jun 2023 14:55:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/KunstnerCL023.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2002-08056,
  author       = {Lukas Balles and
                  Fabian Pedregosa and
                  Nicolas Le Roux},
  title        = {The Geometry of Sign Gradient Descent},
  journal      = {CoRR},
  volume       = {abs/2002.08056},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.08056},
  eprinttype    = {arXiv},
  eprint       = {2002.08056},
  timestamp    = {Mon, 02 Mar 2020 16:46:06 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-08056.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icml/BallesH18,
  author       = {Lukas Balles and
                  Philipp Hennig},
  editor       = {Jennifer G. Dy and
                  Andreas Krause},
  title        = {Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients},
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning,
                  {ICML} 2018},
  series       = {Proceedings of Machine Learning Research},
  volume       = {80},
  pages        = {413--422},
  publisher    = {{PMLR}},
  year         = {2018},
  url          = {http://proceedings.mlr.press/v80/balles18a.html},
  timestamp    = {Wed, 03 Apr 2019 18:17:30 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/BallesH18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iclr/BernsteinZAA19,
  author       = {Jeremy Bernstein and
                  Jiawei Zhao and
                  Kamyar Azizzadenesheli and
                  Anima Anandkumar},
  title        = {signSGD with Majority Vote is Communication Efficient and Fault Tolerant},
  booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019},
  publisher    = {OpenReview.net},
  year         = {2019},
  url          = {https://openreview.net/forum?id=BJxhijAcY7},
  timestamp    = {Thu, 25 Jul 2019 14:25:47 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/BernsteinZAA19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icml/BernsteinWAA18,
  author       = {Jeremy Bernstein and
                  Yu{-}Xiang Wang and
                  Kamyar Azizzadenesheli and
                  Animashree Anandkumar},
  title        = {{SIGNSGD:} Compressed Optimisation for Non-Convex Problems},
  booktitle    = {Proceedings of the 35th International Conference on Machine Learning,
                  {ICML} 2018},
  series       = {Proceedings of Machine Learning Research},
  volume       = {80},
  pages        = {559--568},
  publisher    = {{PMLR}},
  year         = {2018},
  url          = {http://proceedings.mlr.press/v80/bernstein18a.html},
  timestamp    = {Thu, 30 Sep 2021 17:07:36 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/BernsteinWAA18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/WuBL23,
  author       = {Jingfeng Wu and
                  Vladimir Braverman and
                  Jason D. Lee},
  title        = {Implicit Bias of Gradient Descent for Logistic Regression at the Edge
                  of Stability},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/eb189151ced0ff808abafd16a51fec92-Abstract-Conference.html},
  timestamp    = {Fri, 01 Mar 2024 16:26:21 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/WuBL23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/aistats/NacsonSS19,
  author       = {Mor Shpigel Nacson and
                  Nathan Srebro and
                  Daniel Soudry},
  title        = {Stochastic Gradient Descent on Separable Data: Exact Convergence with
                  a Fixed Learning Rate},
  booktitle    = {The 22nd International Conference on Artificial Intelligence and Statistics,
                  {AISTATS} 2019},
  series       = {Proceedings of Machine Learning Research},
  volume       = {89},
  pages        = {3051--3059},
  publisher    = {{PMLR}},
  year         = {2019},
  url          = {http://proceedings.mlr.press/v89/nacson19a.html},
  timestamp    = {Fri, 07 Jun 2019 09:03:47 +0200},
  biburl       = {https://dblp.org/rec/conf/aistats/NacsonSS19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/jmlr/SoudryHNGS18,
  author       = {Daniel Soudry and
                  Elad Hoffer and
                  Mor Shpigel Nacson and
                  Suriya Gunasekar and
                  Nathan Srebro},
  title        = {The Implicit Bias of Gradient Descent on Separable Data},
  journal      = {J. Mach. Learn. Res.},
  volume       = {19},
  pages        = {70:1--70:57},
  year         = {2018},
  url          = {http://jmlr.org/papers/v19/18-188.html},
  timestamp    = {Wed, 10 Jul 2019 15:28:19 +0200},
  biburl       = {https://dblp.org/rec/journals/jmlr/SoudryHNGS18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/ml/Shalev-ShwartzS10,
  author       = {Shai Shalev{-}Shwartz and
                  Yoram Singer},
  title        = {On the equivalence of weak learnability and linear separability: new
                  relaxations and efficient boosting algorithms},
  journal      = {Mach. Learn.},
  volume       = {80},
  number       = {2-3},
  pages        = {141--163},
  year         = {2010},
  url          = {https://doi.org/10.1007/s10994-010-5173-z},
  doi          = {10.1007/S10994-010-5173-Z},
  timestamp    = {Mon, 02 Mar 2020 16:30:01 +0100},
  biburl       = {https://dblp.org/rec/journals/ml/Shalev-ShwartzS10.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/nips/QianQ19,
  author       = {Qian Qian and
                  Xiaoyuan Qian},
  title        = {The Implicit Bias of AdaGrad on Separable Data},
  booktitle    = {Advances in Neural Information Processing Systems 32: Annual Conference
                  on Neural Information Processing Systems 2019, NeurIPS 2019},
  pages        = {7759--7767},
  year         = {2019},
  url          = {https://proceedings.neurips.cc/paper/2019/hash/3335881e06d4d23091389226225e17c7-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/QianQ19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:journals/corr/KingmaB14,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015},
  year         = {2015},
  url          = {http://arxiv.org/abs/1412.6980},
  timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{neyshabur2014search,
  title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={arXiv preprint arXiv:1412.6614},
  year={2014}
}

@inproceedings{wang2021implicit,
  title={The implicit bias for adaptive optimization algorithms on homogeneous neural networks},
  author={Wang, Bohan and Meng, Qi and Chen, Wei and Liu, Tie-Yan},
  booktitle={International Conference on Machine Learning},
  pages={10849--10858},
  year={2021},
  organization={PMLR}
}

@inproceedings{reddi2018convergence,
  title={On the Convergence of Adam and Beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  booktitle={6th International Conference on Learning Representations, {ICLR} 2018},
  year={2018}
}

@article{zhou2018convergence,
  title={On the convergence of adaptive gradient methods for nonconvex optimization},
  author={Zhou, Dongruo and Chen, Jinghui and Cao, Yuan and Tang, Yiqi and Yang, Ziyan and Gu, Quanquan},
  journal={Transactions on Machine Learning Research},
  year={2024}
}

@article{guo2021novel,
  title={A novel convergence analysis for algorithms of the adam family},
  author={Guo, Zhishuai and Xu, Yi and Yin, Wotao and Jin, Rong and Yang, Tianbao},
  journal={arXiv preprint arXiv:2112.03459},
  year={2021}
}

@article{zhou2020towards,
  title={Towards theoretically understanding why sgd generalizes better than adam in deep learning},
  author={Zhou, Pan and Feng, Jiashi and Ma, Chao and Xiong, Caiming and Hoi, Steven Chu Hong and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21285--21296},
  year={2020}
}

@inproceedings{pan2022toward,
  title={Toward Understanding Why Adam Converges Faster Than SGD for Transformers},
  author={Pan, Yan and Li, Yuanzhi},
  booktitle={OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)},
  year={2022}
}

@article{zhang2020adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15383--15393},
  year={2020}
}

@article{wilson2017marginal,
  title={The marginal value of adaptive gradient methods in machine learning},
  author={Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{cattaneo2023implicit,
  title={On the implicit bias of adam},
  author={Cattaneo, Matias D and Klusowski, Jason M and Shigida, Boris},
  journal={arXiv preprint arXiv:2309.00079},
  year={2023}
}

@inproceedings{lyu2019gradient,
  title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},
  author={Lyu, Kaifeng and Li, Jian},
  booktitle={7th International Conference on Learning Representations, {ICLR} 2019},
  year={2019}
}

@article{ji2020directional,
  title={Directional convergence and alignment in deep learning},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17176--17186},
  year={2020}
}

@article{de2018convergence,
  title={Convergence guarantees for RMSProp and ADAM in non-convex optimization and an empirical comparison to Nesterov acceleration},
  author={De, Soham and Mukherjee, Anirbit and Ullah, Enayat},
  journal={arXiv preprint arXiv:1807.06766},
  year={2018}
}

@article{chen2024symbolic,
  title={Symbolic discovery of optimization algorithms},
  author={Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu, Yifeng and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{chen2023lion,
  title={Lion Secretly Solves a Constrained Optimization: As Lyapunov Predicts},
  author={Chen, Lizhang and Liu, Bo and Liang, Kaizhao and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{rosasco2015learning,
  title={Learning with incremental iterative regularization},
  author={Rosasco, Lorenzo and Villa, Silvia},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{ali2020implicit,
  title={The implicit regularization of stochastic gradient flow for least squares},
  author={Ali, Alnur and Dobriban, Edgar and Tibshirani, Ryan},
  booktitle={8th International Conference on Learning Representations, {ICLR} 2020},
  pages={233--244},
  year={2020},
  organization={PMLR}
}

@inproceedings{cao2023implicit,
  title={The implicit bias of batch normalization in linear models and two-layer linear convolutional neural networks},
  author={Cao, Yuan and Zou, Difan and Li, Yuanzhi and Gu, Quanquan},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={5699--5753},
  year={2023},
  organization={PMLR}
}

@article{wu2020implicit,
  title={Implicit regularization and convergence for weight normalization},
  author={Wu, Xiaoxia and Dobriban, Edgar and Ren, Tongzheng and Wu, Shanshan and Li, Zhiyuan and Gunasekar, Suriya and Ward, Rachel and Liu, Qiang},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2835--2847},
  year={2020}
}

@inproceedings{chizat2020implicit,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Conference on learning theory},
  pages={1305--1338},
  year={2020},
  organization={PMLR}
}

@inproceedings{ji2019gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={7th International Conference on Learning Representations, ICLR 2019},
  year={2019}
}

@article{gunasekar2018implicit,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{phuong2020inductive,
  title={The inductive bias of ReLU networks on orthogonally separable data},
  author={Phuong, Mary and Lampert, Christoph H},
  booktitle={8th International Conference on Learning Representations, {ICLR} 2020},
  year={2020}
}

@inproceedings{frei2022implicit,
  title={Implicit Bias in Leaky ReLU Networks Trained on High-Dimensional Data},
  author={Frei, Spencer and Vardi, Gal and Bartlett, Peter and Srebro, Nathan and Hu, Wei},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{vardi2022margin,
  title={On margin maximization in linear and relu networks},
  author={Vardi, Gal and Shamir, Ohad and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={37024--37036},
  year={2022}
}

@article{vardi2022gradient,
  title={Gradient methods provably converge to non-robust networks},
  author={Vardi, Gal and Yehudai, Gilad and Shamir, Ohad},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={20921--20932},
  year={2022}
}

@article{razin2020implicit,
  title={Implicit regularization in deep learning may not be explainable by norms},
  author={Razin, Noam and Cohen, Nadav},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21174--21187},
  year={2020}
}

@article{kou2024implicit,
  title={Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data},
  author={Kou, Yiwen and Chen, Zixiang and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{jin2023implicit,
  title={Implicit bias of gradient descent for mean squared error regression with two-layer wide neural networks},
  author={Jin, Hui and Mont{\'u}far, Guido},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={137},
  pages={1--97},
  year={2023}
}