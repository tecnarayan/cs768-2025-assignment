\begin{thebibliography}{10}

\bibitem{agrawal2012analysis}
S.~Agrawal and N.~Goyal.
\newblock Analysis of {T}hompson sampling for the multi-armed bandit problem.
\newblock In {\em Conference on Learning Theory}, pages 39--1, 2012.

\bibitem{protein}
C.~Angermueller, D.~Dohan, D.~Belanger, R.~Deshpande, K.~Murphy, and
  L.~Colwell.
\newblock Model-based reinforcement learning for biological sequence design.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{agent57}
A.~P. Badia, B.~Piot, S.~Kapturowski, P.~Sprechmann, A.~Vitvitskyi, D.~Guo, and
  C.~Blundell.
\newblock Agent57: Outperforming the atari human benchmark.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}. 2020.

\bibitem{rp1}
P.~Ball, J.~Parker{-}Holder, A.~Pacchiano, K.~Choromanski, and S.~Roberts.
\newblock Ready policy one: World building through active learning.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}. 2020.

\bibitem{gym}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba.
\newblock Open{AI} {G}ym, 2016.

\bibitem{rbo}
K.~Choromanski, A.~Pacchiano, J.~Parker{-}Holder, Y.~Tang, D.~Jain, Y.~Yang,
  A.~Iscen, J.~Hsu, and V.~Sindhwani.
\newblock Provably robust blackbox optimization for reinforcement learning.
\newblock In {\em The Conference on Robot Learning (CoRL)}, 2019.

\bibitem{choromanski_orthogonal}
K.~Choromanski, M.~Rowland, V.~Sindhwani, R.~E. Turner, and A.~Weller.
\newblock Structured evolution with compact architectures for scalable policy
  optimization.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, pages 969--977, 2018.

\bibitem{oac}
K.~Ciosek, Q.~Vuong, R.~Loftin, and K.~Hofmann.
\newblock Better exploration with optimistic actor critic.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  1787--1798. Curran Associates, Inc., 2019.

\bibitem{novelty}
E.~Conti, V.~Madhavan, F.~P. Such, J.~Lehman, K.~O. Stanley, and J.~Clune.
\newblock Improving exploration in {E}volution {S}trategies for deep
  reinforcement learning via a population of novelty-seeking agents.
\newblock In {\em Proceedings of the 32Nd International Conference on Neural
  Information Processing Systems}, pages 5032--5043, USA, 2018. Curran
  Associates Inc.

\bibitem{Cully2015RobotsTC}
A.~Cully, J.~Clune, D.~Tarapore, and J.-B. Mouret.
\newblock Robots that can adapt like animals.
\newblock {\em Nature}, 521:503--507, 2015.

\bibitem{doan2019attractionrepulsion}
T.~Doan, B.~Mazoure, A.~Durand, J.~Pineau, and R.~D. Hjelm.
\newblock Attraction-repulsion actor-critic for continuous control
  reinforcement learning.
\newblock {\em arXiv}, 2019.

\bibitem{eysenbach2018diversity}
B.~Eysenbach, A.~Gupta, J.~Ibarz, and S.~Levine.
\newblock Diversity is all you need: Learning skills without a reward function.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{maml}
C.~Finn, P.~Abbeel, and S.~Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning}, volume~70 of {\em Proceedings of Machine Learning Research}, pages
  1126--1135, 2017.

\bibitem{fortunato2018noisy}
M.~Fortunato, M.~G. Azar, B.~Piot, J.~Menick, M.~Hessel, I.~Osband, A.~Graves,
  V.~Mnih, R.~Munos, D.~Hassabis, O.~Pietquin, C.~Blundell, and S.~Legg.
\newblock Noisy networks for exploration.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{td3}
S.~Fujimoto, H.~van Hoof, and D.~Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In J.~Dy and A.~Krause, editors, {\em Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of {\em Proceedings
  of Machine Learning Research}, pages 1587--1596, Stockholmsmässan, Stockholm
  Sweden, 10--15 Jul 2018. PMLR.

\bibitem{gaier2020automating}
A.~Gaier, A.~Asteroth, and J.~Mouret.
\newblock Automating representation discovery with {MAP}-{E}lites.
\newblock {\em CoRR}, abs/2003.04389, 2020.

\bibitem{evolvabilityes}
A.~Gajewski, J.~Clune, K.~O. Stanley, and J.~Lehman.
\newblock Evolvability {ES}: Scalable and direct optimization of evolvability.
\newblock In {\em Proceedings of the Genetic and Evolutionary Computation
  Conference}, GECCO '19, pages 107--115, New York, NY, USA, 2019. ACM.

\bibitem{maesn}
A.~Gupta, R.~Mendonca, Y.~Liu, P.~Abbeel, and S.~Levine.
\newblock Meta-reinforcement learning of structured exploration strategies.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, {\em Advances in Neural Information Processing
  Systems 31}, pages 5302--5311. Curran Associates, Inc., 2018.

\bibitem{sac}
T.~Haarnoja, A.~Zhou, K.~Hartikainen, G.~Tucker, S.~Ha, J.~Tan, V.~Kumar,
  H.~Zhu, A.~Gupta, P.~Abbeel, and S.~Levine.
\newblock Soft actor-critic algorithms and applications.
\newblock {\em CoRR}, abs/1812.05905, 2018.

\bibitem{hartikainen2020dynamical}
K.~Hartikainen, X.~Geng, T.~Haarnoja, and S.~Levine.
\newblock Dynamical distance learning for semi-supervised and unsupervised
  skill discovery.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{diversitydriven}
Z.-W. Hong, T.-Y. Shann, S.-Y. Su, Y.-H. Chang, T.-J. Fu, and C.-Y. Lee.
\newblock Diversity-driven exploration strategy for deep reinforcement
  learning.
\newblock In {\em Advances in Neural Information Processing Systems 31}. 2018.

\bibitem{genetic_actionembedding}
E.~C. Jackson and M.~Daley.
\newblock Novelty search for deep reinforcement learning policy network weights
  by action sequence edit metric distance.
\newblock In {\em Proceedings of the Genetic and Evolutionary Computation
  Conference Companion}, GECCO ’19, page 173–174, New York, NY, USA, 2019.
  Association for Computing Machinery.

\bibitem{p3s}
W.~Jung, G.~Park, and Y.~Sung.
\newblock Population-guided parallel policy search for reinforcement learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{evorl_multi}
S.~Khadka, S.~Majumdar, and K.~Tumer.
\newblock Evolutionary reinforcement learning for sample-efficient multiagent
  coordination.
\newblock {\em CoRR}, abs/1906.07315, 2019.

\bibitem{egpg}
S.~Khadka and K.~Tumer.
\newblock Evolution-guided policy gradient in reinforcement learning.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, {\em Advances in Neural Information Processing
  Systems 31}, pages 1188--1200. Curran Associates, Inc., 2018.

\bibitem{whitesonBC}
S.~Kistemaker and S.~Whiteson.
\newblock Critical factors in the performance of novelty search.
\newblock In {\em Proceedings of the 13th Annual Conference on Genetic and
  Evolutionary Computation}, GECCO ’11, page 965–972, New York, NY, USA,
  2011. Association for Computing Machinery.

\bibitem{kdpp}
A.~Kulesza and B.~Taskar.
\newblock k-dpps: Fixed-size determinantal point processes.
\newblock In {\em Proceedings of the 28th International Conference on Machine
  Learning, {ICML} 2011, Bellevue, Washington, USA, June 28 - July 2, 2011},
  pages 1193--1200, 2011.

\bibitem{Kulesza}
A.~Kulesza and B.~Taskar.
\newblock {\em Determinantal Point Processes for Machine Learning}.
\newblock Now Publishers Inc., Hanover, MA, USA, 2012.

\bibitem{ME-TRPO}
T.~Kurutach, I.~Clavera, Y.~Duan, A.~Tamar, and P.~Abbeel.
\newblock Model-ensemble trust-region policy optimization.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{Lehman08exploitingopen}
J.~Lehman and K.~O. Stanley.
\newblock Exploiting open-endedness to solve problems through the search for
  novelty.
\newblock In {\em Proceedings of the Eleventh International Conference on
  Artificial Life (Alife XI}. MIT Press, 2008.

\bibitem{lehmannovelty}
J.~Lehman and K.~O. Stanley.
\newblock Abandoning objectives: Evolution through the search for novelty
  alone.
\newblock {\em Evolutionary Computation}, 19(2):189--223, 2011.

\bibitem{PBT}
A.~Li, O.~Spyra, S.~Perel, V.~Dalibard, M.~Jaderberg, C.~Gu, D.~Budden,
  T.~Harley, and P.~Gupta.
\newblock A generalized framework for population based training.
\newblock {\em CoRR}, abs/1902.01894, 2019.

\bibitem{liu2018emergent}
S.~Liu, G.~Lever, N.~Heess, J.~Merel, S.~Tunyasuvunakool, and T.~Graepel.
\newblock Emergent coordination through competition.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{horia}
H.~Mania, A.~Guy, and B.~Recht.
\newblock Simple random search provides a competitive approach to reinforcement
  learning.
\newblock {\em CoRR}, abs/1803.07055, 2018.

\bibitem{dqn2013}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~Graves, I.~Antonoglou, D.~Wierstra, and
  M.~A. Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock {\em ArXiv}, abs/1312.5602, 2013.

\bibitem{ray}
P.~Moritz, R.~Nishihara, S.~Wang, A.~Tumanov, R.~Liaw, E.~Liang, W.~Paul, M.~I.
  Jordan, and I.~Stoica.
\newblock Ray: {A} distributed framework for emerging {AI} applications.
\newblock {\em CoRR}, abs/1712.05889, 2017.

\bibitem{Mouret2015IlluminatingSS}
J.-B. Mouret and J.~Clune.
\newblock Illuminating search spaces by mapping elites.
\newblock {\em ArXiv}, abs/1504.04909, 2015.

\bibitem{nesterov}
Y.~Nesterov and V.~Spokoiny.
\newblock Random gradient-free minimization of convex functions.
\newblock {\em Found. Comput. Math.}, 17(2):527--566, Apr. 2017.

\bibitem{bootstrapped_dqn}
I.~Osband, C.~Blundell, A.~Pritzel, and B.~Van~Roy.
\newblock Deep exploration via bootstrapped {DQN}.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems 29}, pages
  4026--4034. Curran Associates, Inc., 2016.

\bibitem{wass}
A.~Pacchiano, J.~Parker-Holder, Y.~Tang, A.~Choromanska, K.~Choromanski, and
  M.~I. Jordan.
\newblock Learning to score behaviors for guided policy optimization.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}. 2020.

\bibitem{plappert2018parameter}
M.~Plappert, R.~Houthooft, P.~Dhariwal, S.~Sidor, R.~Y. Chen, X.~Chen,
  T.~Asfour, P.~Abbeel, and M.~Andrychowicz.
\newblock Parameter space noise for exploration.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{cemrl}
Pourchot and Sigaud.
\newblock {CEM}-{RL}: Combining evolutionary and gradient-based methods for
  policy search.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{qdnature}
J.~K. Pugh, L.~B. Soros, and K.~O. Stanley.
\newblock Quality diversity: A new frontier for evolutionary computation.
\newblock {\em Frontiers in Robotics and AI}, 3:40, 2016.

\bibitem{recht2011hogwild}
B.~Recht, C.~Re, S.~Wright, and F.~Niu.
\newblock Hogwild: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In {\em Advances in neural information processing systems}, pages
  693--701, 2011.

\bibitem{promp}
J.~Rothfuss, D.~Lee, I.~Clavera, T.~Asfour, and P.~Abbeel.
\newblock Pro{MP}: Proximal meta-policy search.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{ts_tutorial}
D.~Russo, B.~Roy, A.~Kazerouni, and I.~Osband.
\newblock A tutorial on {T}hompson sampling.
\newblock {\em Foundations and Trends® in Machine Learning}, 11, 07 2017.

\bibitem{ts_russovanroy}
D.~Russo and B.~Van~Roy.
\newblock An information-theoretic analysis of {T}hompson sampling.
\newblock {\em J. Mach. Learn. Res.}, 17(1):2442–2471, Jan. 2016.

\bibitem{leo}
A.~A. Rusu, D.~Rao, J.~Sygnowski, O.~Vinyals, R.~Pascanu, S.~Osindero, and
  R.~Hadsell.
\newblock Meta-learning with latent embedding optimization.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{ES}
T.~Salimans, J.~Ho, X.~Chen, S.~Sidor, and I.~Sutskever.
\newblock Evolution {S}trategies as a scalable alternative to reinforcement
  learning.
\newblock {\em arXiv}, abs/1703.03864, 2017.

\bibitem{schaul2019adapting}
T.~Schaul, D.~Borsa, D.~Ding, D.~Szepesvari, G.~Ostrovski, W.~Dabney, and
  S.~Osindero.
\newblock Adapting behaviour for learning progress.
\newblock {\em CoRR}, abs/1912.06910, 2019.

\bibitem{schmidhuber_fun}
J.~{Schmidhuber}.
\newblock Formal theory of creativity, fun, and intrinsic motivation
  (1990–2010).
\newblock {\em IEEE Transactions on Autonomous Mental Development},
  2(3):230--247, Sep. 2010.

\bibitem{schulman2015trust}
J.~Schulman, S.~Levine, P.~Abbeel, M.~Jordan, and P.~Moritz.
\newblock Trust region policy optimization.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2015.

\bibitem{schulman2017proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}, 2016-2018.

\bibitem{slivkins}
A.~Slivkins.
\newblock Introduction to multi-armed bandits.
\newblock {\em Foundations and Trends in Machine Learning}, 12(1-2):1--286,
  2019.

\bibitem{gpucb}
N.~Srinivas, A.~Krause, S.~Kakade, and M.~Seeger.
\newblock Gaussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock In {\em Proceedings of the 27th International Conference on
  International Conference on Machine Learning}, ICML’10, page 1015–1022,
  Madison, WI, USA, 2010. Omnipress.

\bibitem{neuronature}
K.~Stanley, J.~Clune, J.~Lehman, and R.~Miikkulainen.
\newblock Designing neural networks through neuroevolution.
\newblock {\em Nature Machine Intelligence}, 1, 01 2019.

\bibitem{stanley2011enumerative}
R.~P. Stanley.
\newblock Enumerative combinatorics volume 1 second edition.
\newblock {\em Cambridge studies in advanced mathematics}, 2011.

\bibitem{deepga}
F.~P. Such, V.~Madhavan, E.~Conti, J.~Lehman, K.~O. Stanley, and J.~Clune.
\newblock Deep neuroevolution: Genetic algorithms are a competitive alternative
  for training deep neural networks for reinforcement learning.
\newblock {\em CoRR}, abs/1712.06567, 2017.

\bibitem{Sutton1998}
R.~S. Sutton and A.~G. Barto.
\newblock {\em Introduction to Reinforcement Learning}.
\newblock MIT Press, Cambridge, MA, USA, 1st edition, 1998.

\bibitem{thompson1933likelihood}
W.~R. Thompson.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock {\em Biometrika}, 25(3/4):285--294, 1933.

\bibitem{svgd_diverse}
D.~Wang and Q.~Liu.
\newblock Nonlinear stein variational gradient descent for learning diversified
  mixture models.
\newblock In K.~Chaudhuri and R.~Salakhutdinov, editors, {\em Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of {\em
  Proceedings of Machine Learning Research}, pages 6576--6585, Long Beach,
  California, USA, 09--15 Jun 2019. PMLR.

\bibitem{yang2019morl}
R.~Yang, X.~Sun, and K.~Narasimhan.
\newblock A generalized algorithm for multi-objective reinforcement learning
  and policy adaptation.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  14610--14621. Curran Associates, Inc., 2019.

\end{thebibliography}
