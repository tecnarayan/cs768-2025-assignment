\begin{thebibliography}{95}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020{\natexlab{a}})Agarwal, Kakade, and
  Yang]{agarwal2020model}
Alekh Agarwal, Sham Kakade, and Lin~F Yang.
\newblock Model-based reinforcement learning with a generative model is minimax
  optimal.
\newblock In \emph{Conference on Learning Theory}, pages 67--83. PMLR,
  2020{\natexlab{a}}.

\bibitem[Agarwal et~al.(2020{\natexlab{b}})Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2020optimality}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock Optimality and approximation with policy gradient methods in {M}arkov
  decision processes.
\newblock In \emph{Conference on Learning Theory}, pages 64--66. PMLR,
  2020{\natexlab{b}}.

\bibitem[Agarwal et~al.(2020{\natexlab{c}})Agarwal, Schuurmans, and
  Norouzi]{agarwal2020optimistic}
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  104--114. PMLR, 2020{\natexlab{c}}.

\bibitem[Antos et~al.(2007)Antos, Munos, and Szepesvari]{antos2007fitted}
Andras Antos, R{\'e}mi Munos, and Csaba Szepesvari.
\newblock Fitted {Q}-iteration in continuous action-space mdps.
\newblock In \emph{Neural Information Processing Systems}, 2007.

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'a}ri, and Munos]{antos2008learning}
Andr{\'a}s Antos, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Learning near-optimal policies with {B}ellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock \emph{Machine Learning}, 71\penalty0 (1):\penalty0 89--129, 2008.

\bibitem[Azar et~al.(2013)Azar, Munos, and Kappen]{azar2013minimax}
Mohammad~Gheshlaghi Azar, R{\'e}mi Munos, and Hilbert~J Kappen.
\newblock Minimax {PAC} bounds on the sample complexity of reinforcement
  learning with a generative model.
\newblock \emph{Machine learning}, 91\penalty0 (3):\penalty0 325--349, 2013.

\bibitem[Bojarski et~al.(2016)Bojarski, Del~Testa, Dworakowski, Firner, Flepp,
  Goyal, Jackel, Monfort, Muller, Zhang, et~al.]{bojarski2016end}
Mariusz Bojarski, Davide Del~Testa, Daniel Dworakowski, Bernhard Firner, Beat
  Flepp, Prasoon Goyal, Lawrence~D Jackel, Mathew Monfort, Urs Muller, Jiakai
  Zhang, et~al.
\newblock End to end learning for self-driving cars.
\newblock \emph{arXiv preprint arXiv:1604.07316}, 2016.

\bibitem[Bubeck et~al.(2011)Bubeck, Munos, and Stoltz]{bubeck2011pure}
S{\'e}bastien Bubeck, R{\'e}mi Munos, and Gilles Stoltz.
\newblock Pure exploration in finitely-armed and continuous-armed bandits.
\newblock \emph{Theoretical Computer Science}, 412\penalty0 (19):\penalty0
  1832--1852, 2011.

\bibitem[Buckman et~al.(2020)Buckman, Gelada, and
  Bellemare]{buckman2020importance}
Jacob Buckman, Carles Gelada, and Marc~G Bellemare.
\newblock The importance of pessimism in fixed-dataset policy optimization.
\newblock \emph{arXiv preprint arXiv:2009.06799}, 2020.

\bibitem[Chen and Jiang(2019)]{chen2019information}
Jinglin Chen and Nan Jiang.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1905.00360}, 2019.

\bibitem[Dann and Brunskill(2015)]{dann2015sample}
Christoph Dann and Emma Brunskill.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In \emph{Proceedings of the 28th International Conference on Neural
  Information Processing Systems-Volume 2}, pages 2818--2826, 2015.

\bibitem[Domingues et~al.(2020)Domingues, M{\'e}nard, Kaufmann, and
  Valko]{domingues2020episodic}
Omar~Darwiche Domingues, Pierre M{\'e}nard, Emilie Kaufmann, and Michal Valko.
\newblock Episodic reinforcement learning in finite {MDPs: M}inimax lower
  bounds revisited.
\newblock \emph{arXiv preprint arXiv:2010.03531}, 2020.

\bibitem[Du et~al.(2020)Du, Kakade, Wang, and Yang]{du2020good}
Simon~S Du, Sham~M Kakade, Ruosong Wang, and Lin~F Yang.
\newblock Is a good representation sufficient for sample efficient
  reinforcement learning?
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Duan et~al.(2020)Duan, Jia, and Wang]{duan2020minimax}
Yaqi Duan, Zeyu Jia, and Mengdi Wang.
\newblock Minimax-optimal off-policy evaluation with linear function
  approximation.
\newblock In \emph{International Conference on Machine Learning}, pages
  2701--2709. PMLR, 2020.

\bibitem[Farahmand et~al.(2010)Farahmand, Munos, and
  Szepesv{\'a}ri]{farahmand2010error}
Amir~Massoud Farahmand, R{\'e}mi Munos, and Csaba Szepesv{\'a}ri.
\newblock Error propagation for approximate policy and value iteration.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2010.

\bibitem[Feng et~al.(2019)Feng, Li, and Liu]{feng2019kernel}
Yihao Feng, Lihong Li, and Qiang Liu.
\newblock A kernel loss for solving the {B}ellman equation.
\newblock \emph{arXiv preprint arXiv:1905.10506}, 2019.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock {D4RL: D}atasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Fujimoto et~al.(2019{\natexlab{a}})Fujimoto, Conti, Ghavamzadeh, and
  Pineau]{fujimoto2019benchmarking}
Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau.
\newblock Benchmarking batch deep reinforcement learning algorithms.
\newblock \emph{arXiv preprint arXiv:1910.01708}, 2019{\natexlab{a}}.

\bibitem[Fujimoto et~al.(2019{\natexlab{b}})Fujimoto, Meger, and
  Precup]{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning}, pages
  2052--2062. PMLR, 2019{\natexlab{b}}.

\bibitem[Garcin et~al.(2014)Garcin, Faltings, Donatsch, Alazzawi, Bruttin, and
  Huber]{garcin2014offline}
Florent Garcin, Boi Faltings, Olivier Donatsch, Ayar Alazzawi, Christophe
  Bruttin, and Amr Huber.
\newblock Offline and online evaluation of news recommender systems at
  swissinfo.ch.
\newblock In \emph{Proceedings of the 8th ACM Conference on Recommender
  systems}, pages 169--176, 2014.

\bibitem[Geist et~al.(2017)Geist, Piot, and Pietquin]{geist2017bellman}
Matthieu Geist, Bilal Piot, and Olivier Pietquin.
\newblock Is the {B}ellman residual a bad proxy?
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3205--3214, 2017.

\bibitem[Ghasemipour et~al.(2020)Ghasemipour, Schuurmans, and
  Gu]{ghasemipour2020emaq}
Seyed Kamyar~Seyed Ghasemipour, Dale Schuurmans, and Shixiang~Shane Gu.
\newblock {EMaQ}: {E}xpected-max {Q}-learning operator for simple yet effective
  offline and online {RL}.
\newblock \emph{arXiv preprint arXiv:2007.11091}, 2020.

\bibitem[Gilbert(1952)]{gilbert1952comparison}
Edgar~N Gilbert.
\newblock A comparison of signalling alphabets.
\newblock \emph{The Bell system technical journal}, 31\penalty0 (3):\penalty0
  504--522, 1952.

\bibitem[Gottesman et~al.(2019)Gottesman, Johansson, Komorowski, Faisal,
  Sontag, Doshi-Velez, and Celi]{gottesman2019guidelines}
Omer Gottesman, Fredrik Johansson, Matthieu Komorowski, Aldo Faisal, David
  Sontag, Finale Doshi-Velez, and Leo~Anthony Celi.
\newblock Guidelines for reinforcement learning in healthcare.
\newblock \emph{Nature medicine}, 25\penalty0 (1):\penalty0 16--18, 2019.

\bibitem[Gulcehre et~al.(2020)Gulcehre, Wang, Novikov, Paine, Colmenarejo,
  Zolna, Agarwal, Merel, Mankowitz, Paduraru, et~al.]{gulcehre2020rl}
Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom~Le Paine, Sergio~G{\'o}mez
  Colmenarejo, Konrad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz,
  Cosmin Paduraru, et~al.
\newblock {RL} unplugged: {B}enchmarks for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.13888}, 2020.

\bibitem[Hao et~al.(2020)Hao, Duan, Lattimore, Szepesv{\'a}ri, and
  Wang]{hao2020sparse}
Botao Hao, Yaqi Duan, Tor Lattimore, Csaba Szepesv{\'a}ri, and Mengdi Wang.
\newblock Sparse feature selection makes batch reinforcement learning more
  sample efficient.
\newblock \emph{arXiv preprint arXiv:2011.04019}, 2020.

\bibitem[Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{jaques2019way}
Natasha Jaques, Asma Ghandeharioun, Judy~Hanwen Shen, Craig Ferguson, Agata
  Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock \emph{arXiv preprint arXiv:1907.00456}, 2019.

\bibitem[Jiang(2019)]{jiang2019value}
Nan Jiang.
\newblock On value functions and the agent-environment boundary.
\newblock \emph{arXiv preprint arXiv:1905.13341}, 2019.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E
  Schapire.
\newblock Contextual decision processes with low {B}ellman rank are
  {PAC}-learnable.
\newblock In \emph{International Conference on Machine Learning}, pages
  1704--1713. PMLR, 2017.

\bibitem[Jiao et~al.(2018)Jiao, Han, and Weissman]{jiao2018minimax}
Jiantao Jiao, Yanjun Han, and Tsachy Weissman.
\newblock Minimax estimation of the $\ell_1$ distance.
\newblock \emph{IEEE Transactions on Information Theory}, 64\penalty0
  (10):\penalty0 6672--6706, 2018.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is {Q}-learning provably efficient?
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 4868--4878, 2018.

\bibitem[Jin et~al.(2020)Jin, Yang, and Wang]{jin2020pessimism}
Ying Jin, Zhuoran Yang, and Zhaoran Wang.
\newblock Is pessimism provably efficient for offline {RL}?
\newblock \emph{arXiv preprint arXiv:2012.15085}, 2020.

\bibitem[Kakade and Langford(2002)]{kakade2002approximately}
Sham Kakade and John Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{ICML}, volume~2, pages 267--274, 2002.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{kidambi2020morel}
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
\newblock {MOReL}: {M}odel-based offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2005.05951}, 2020.

\bibitem[Koh et~al.(2020)Koh, Sagawa, Marklund, Xie, Zhang, Balsubramani, Hu,
  Yasunaga, Phillips, Beery, et~al.]{koh2020wilds}
Pang~Wei Koh, Shiori Sagawa, Henrik Marklund, Sang~Michael Xie, Marvin Zhang,
  Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard~Lanas Phillips,
  Sara Beery, et~al.
\newblock {WILDS: A} benchmark of in-the-wild distribution shifts.
\newblock \emph{arXiv preprint arXiv:2012.07421}, 2020.

\bibitem[Krishnamurthy et~al.(2016)Krishnamurthy, Agarwal, and
  Langford]{krishnamurthy2016pac}
Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock {PAC} reinforcement learning with rich observations.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pages 1848--1856, 2016.

\bibitem[Kumar and Levine(2020)]{neurips_tutorial}
Aviral Kumar and Sergey Levine.
\newblock Offline reinforcement learning: {F}rom algorithms to practical
  challenges.
\newblock
  \url{https://sites.google.com/view/offlinerltutorial-neurips2020/home}, 2020.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Tucker, and Levine]{kumar2019stabilizing}
Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy {Q}-learning via bootstrapping error
  reduction.
\newblock \emph{arXiv preprint arXiv:1906.00949}, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative {Q}-learning for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.04779}, 2020.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{lange2012batch}
Sascha Lange, Thomas Gabel, and Martin Riedmiller.
\newblock Batch reinforcement learning.
\newblock In \emph{Reinforcement learning}, pages 45--73. Springer, 2012.

\bibitem[Laroche et~al.(2019)Laroche, Trichelair, and
  Des~Combes]{laroche2019safe}
Romain Laroche, Paul Trichelair, and Remi~Tachet Des~Combes.
\newblock Safe policy improvement with baseline bootstrapping.
\newblock In \emph{International Conference on Machine Learning}, pages
  3652--3661. PMLR, 2019.

\bibitem[Lattimore and Hutter(2012)]{lattimore2012pac}
Tor Lattimore and Marcus Hutter.
\newblock {PAC} bounds for discounted {MDP}s.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 320--334. Springer, 2012.

\bibitem[Le~Cam(2012)]{le2012asymptotic}
Lucien Le~Cam.
\newblock \emph{{Asymptotic Methods in Statistical Decision Theory}}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: {T}utorial, review, and perspectives
  on open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Li et~al.(2020)Li, Wei, Chi, Gu, and Chen]{li2020breaking}
Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen.
\newblock Breaking the sample size barrier in model-based reinforcement
  learning with a generative model.
\newblock \emph{arXiv preprint arXiv:2005.12900}, 2020.

\bibitem[Liao et~al.(2020)Liao, Qi, and Murphy]{liao2020batch}
Peng Liao, Zhengling Qi, and Susan Murphy.
\newblock Batch policy learning in average reward {M}arkov decision processes.
\newblock \emph{arXiv preprint arXiv:2007.11771}, 2020.

\bibitem[Liu et~al.(2019)Liu, Cai, Yang, and Wang]{liu2019neural}
Boyi Liu, Qi~Cai, Zhuoran Yang, and Zhaoran Wang.
\newblock Neural trust region/proximal policy optimization attains globally
  optimal policy.
\newblock In \emph{Neural Information Processing Systems}, 2019.

\bibitem[Liu et~al.(2020)Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020provably}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Provably good batch reinforcement learning without great exploration.
\newblock \emph{arXiv preprint arXiv:2007.08202}, 2020.

\bibitem[Ma et~al.(2021)Ma, Zhu, Jiao, and Wainwright]{ma2021minimax}
Cong Ma, Banghua Zhu, Jiantao Jiao, and Martin~J Wainwright.
\newblock Minimax off-policy evaluation for multi-armed bandits.
\newblock \emph{arXiv preprint arXiv:2101.07781}, 2021.

\bibitem[Mitzenmacher and Upfal(2017)]{mitzenmacher2017probability}
Michael Mitzenmacher and Eli Upfal.
\newblock \emph{Probability and computing: {R}andomization and probabilistic
  techniques in algorithms and data analysis}.
\newblock Cambridge university press, 2017.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing {Atari} with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Munos(2003)]{munos2003error}
R{\'e}mi Munos.
\newblock Error bounds for approximate policy iteration.
\newblock In \emph{Proceedings of the Twentieth International Conference on
  International Conference on Machine Learning}, pages 560--567, 2003.

\bibitem[Munos(2007)]{munos2007performance}
R{\'e}mi Munos.
\newblock Performance bounds in $\ell_p$-norm for approximate value iteration.
\newblock \emph{SIAM journal on control and optimization}, 46\penalty0
  (2):\penalty0 541--561, 2007.

\bibitem[Nachum and Dai(2020)]{nachum2020reinforcement}
Ofir Nachum and Bo~Dai.
\newblock Reinforcement learning via {F}enchel-{R}ockafellar duality.
\newblock \emph{arXiv preprint arXiv:2001.01866}, 2020.

\bibitem[Nachum et~al.(2019{\natexlab{a}})Nachum, Chow, Dai, and
  Li]{nachum2019dualdice}
Ofir Nachum, Yinlam Chow, Bo~Dai, and Lihong Li.
\newblock Dualdice: {B}ehavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2315--2325, 2019{\natexlab{a}}.

\bibitem[Nachum et~al.(2019{\natexlab{b}})Nachum, Dai, Kostrikov, Chow, Li, and
  Schuurmans]{nachum2019algaedice}
Ofir Nachum, Bo~Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale
  Schuurmans.
\newblock {AlgaeDICE: P}olicy gradient from arbitrary experience.
\newblock \emph{arXiv preprint arXiv:1912.02074}, 2019{\natexlab{b}}.

\bibitem[Nadjahi et~al.(2019)Nadjahi, Laroche, and des Combes]{nadjahi2019safe}
Kimia Nadjahi, Romain Laroche, and R{\'e}mi~Tachet des Combes.
\newblock Safe policy improvement with soft baseline bootstrapping.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 53--68. Springer, 2019.

\bibitem[Nie et~al.(2020)Nie, Brunskill, and Wager]{nie2020learning}
Xinkun Nie, Emma Brunskill, and Stefan Wager.
\newblock Learning when-to-treat policies.
\newblock \emph{Journal of the American Statistical Association}, pages 1--18,
  2020.

\bibitem[Pan et~al.(2017)Pan, Cheng, Saigol, Lee, Yan, Theodorou, and
  Boots]{pan2017agile}
Yunpeng Pan, Ching-An Cheng, Kamil Saigol, Keuntaek Lee, Xinyan Yan, Evangelos
  Theodorou, and Byron Boots.
\newblock Agile autonomous driving using end-to-end deep imitation learning.
\newblock \emph{arXiv preprint arXiv:1709.07174}, 2017.

\bibitem[Peng et~al.(2019)Peng, Kumar, Zhang, and Levine]{peng2019advantage}
Xue~Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
\newblock Advantage-weighted regression: {S}imple and scalable off-policy
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.00177}, 2019.

\bibitem[Puterman(1990)]{puterman1990markov}
Martin~L Puterman.
\newblock Markov decision processes.
\newblock \emph{Handbooks in operations research and management science},
  2:\penalty0 331--434, 1990.

\bibitem[Rajaraman et~al.(2020)Rajaraman, Yang, Jiao, and
  Ramachandran]{rajaraman2020toward}
Nived Rajaraman, Lin~F Yang, Jiantao Jiao, and Kannan Ramachandran.
\newblock Toward the fundamental limits of imitation learning.
\newblock \emph{arXiv preprint arXiv:2009.05990}, 2020.

\bibitem[Robert(1990)]{robert1990ash}
B~Ash. Robert.
\newblock Information theory, 1990.

\bibitem[Ross and Bagnell(2010)]{ross2010efficient}
St{\'e}phane Ross and Drew Bagnell.
\newblock Efficient reductions for imitation learning.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 661--668, 2010.

\bibitem[Scherrer(2014)]{scherrer2014approximate}
Bruno Scherrer.
\newblock Approximate policy iteration schemes: {A} comparison.
\newblock In \emph{International Conference on Machine Learning}, pages
  1314--1322, 2014.

\bibitem[Sidford et~al.(2018{\natexlab{a}})Sidford, Wang, Wu, Yang, and
  Ye]{sidford2018near}
Aaron Sidford, Mengdi Wang, Xian Wu, Lin~F Yang, and Yinyu Ye.
\newblock Near-optimal time and sample complexities for solving discounted
  {M}arkov decision process with a generative model.
\newblock \emph{arXiv preprint arXiv:1806.01492}, 2018{\natexlab{a}}.

\bibitem[Sidford et~al.(2018{\natexlab{b}})Sidford, Wang, Wu, and
  Ye]{sidford2018variance}
Aaron Sidford, Mengdi Wang, Xian Wu, and Yinyu Ye.
\newblock Variance reduced value iteration and faster algorithms for solving
  {M}arkov decision processes.
\newblock In \emph{Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 770--787. SIAM, 2018{\natexlab{b}}.

\bibitem[Siegel et~al.(2020)Siegel, Springenberg, Berkenkamp, Abdolmaleki,
  Neunert, Lampe, Hafner, and Riedmiller]{siegel2020keep}
Noah~Y Siegel, Jost~Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki,
  Michael Neunert, Thomas Lampe, Roland Hafner, and Martin Riedmiller.
\newblock Keep doing what worked: {B}ehavioral modelling priors for offline
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2002.08396}, 2020.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of {Go} with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  et~al.
\newblock Mastering the game of {G}o without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Strehl et~al.(2010)Strehl, Langford, Kakade, and
  Li]{strehl2010learning}
Alex Strehl, John Langford, Sham Kakade, and Lihong Li.
\newblock Learning from logged implicit exploration data.
\newblock \emph{arXiv preprint arXiv:1003.0120}, 2010.

\bibitem[Szepesv{\'a}ri(2010)]{szepesvari2010algorithms}
Csaba Szepesv{\'a}ri.
\newblock Algorithms for reinforcement learning.
\newblock \emph{Synthesis lectures on artificial intelligence and machine
  learning}, 4\penalty0 (1):\penalty0 1--103, 2010.

\bibitem[Szepesv{\'a}ri and Munos(2005)]{szepesvari2005finite}
Csaba Szepesv{\'a}ri and R{\'e}mi Munos.
\newblock Finite time bounds for sampling based fitted value iteration.
\newblock In \emph{Proceedings of the 22nd international conference on Machine
  learning}, pages 880--887, 2005.

\bibitem[Thomas et~al.(2017)Thomas, Theocharous, Ghavamzadeh, Durugkar, and
  Brunskill]{thomas2017predictive}
Philip~S Thomas, Georgios Theocharous, Mohammad Ghavamzadeh, Ishan Durugkar,
  and Emma Brunskill.
\newblock Predictive off-policy policy evaluation for nonstationary decision
  problems, with applications to digital marketing.
\newblock In \emph{AAAI}, pages 4740--4745, 2017.

\bibitem[Uehara et~al.(2020)Uehara, Huang, and Jiang]{uehara2020minimax}
Masatoshi Uehara, Jiawei Huang, and Nan Jiang.
\newblock Minimax weight and {Q}-function learning for off-policy evaluation.
\newblock In \emph{International Conference on Machine Learning}, pages
  9659--9668. PMLR, 2020.

\bibitem[Uehara et~al.(2021)Uehara, Imaizumi, Jiang, Kallus, Sun, and
  Xie]{uehara2021finite}
Masatoshi Uehara, Masaaki Imaizumi, Nan Jiang, Nathan Kallus, Wen Sun, and
  Tengyang Xie.
\newblock Finite sample analysis of minimax offline reinforcement learning:
  {C}ompleteness, fast rates and first-order efficiency.
\newblock \emph{arXiv preprint arXiv:2102.02981}, 2021.

\bibitem[Varshamov(1957)]{varshamov1957estimate}
Rom~Rubenovich Varshamov.
\newblock Estimate of the number of signals in error correcting codes.
\newblock \emph{Docklady Akad. Nauk, SSSR}, 117:\penalty0 739--741, 1957.

\bibitem[Wang et~al.(2019)Wang, Cai, Yang, and Wang]{wang2019neural}
Lingxiao Wang, Qi~Cai, Zhuoran Yang, and Zhaoran Wang.
\newblock Neural policy gradient methods: {G}lobal optimality and rates of
  convergence.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Wang et~al.(2018)Wang, Zhang, He, and Zha]{wang2018supervised}
Lu~Wang, Wei Zhang, Xiaofeng He, and Hongyuan Zha.
\newblock Supervised reinforcement learning with recurrent neural network for
  dynamic treatment recommendation.
\newblock In \emph{Proceedings of the 24th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 2447--2456, 2018.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Foster, and
  Kakade]{wang2020statistical}
Ruosong Wang, Dean~P Foster, and Sham~M Kakade.
\newblock What are the statistical limits of offline rl with linear function
  approximation?
\newblock \emph{arXiv preprint arXiv:2010.11895}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Novikov, Zolna, Merel,
  Springenberg, Reed, Shahriari, Siegel, Gulcehre, Heess,
  et~al.]{wang2020critic}
Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh~S Merel, Jost~Tobias
  Springenberg, Scott~E Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre,
  Nicolas Heess, et~al.
\newblock Critic regularized regression.
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{b}}.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Xie and Jiang(2020)]{xie2020batch}
Tengyang Xie and Nan Jiang.
\newblock Batch value-function approximation with only realizability.
\newblock \emph{arXiv preprint arXiv:2008.04990}, 2020.

\bibitem[Xu et~al.(2020)Xu, Li, and Yu]{xu2020error}
Tian Xu, Ziniu Li, and Yang Yu.
\newblock Error bounds of imitating policies and environments.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Yin et~al.(2020)Yin, Bai, and Wang]{yin2020near}
Ming Yin, Yu~Bai, and Yu-Xiang Wang.
\newblock Near optimal provable uniform convergence in off-policy evaluation
  for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2007.03760}, 2020.

\bibitem[Yin et~al.(2021)Yin, Bai, and Wang]{yin2021near}
Ming Yin, Yu~Bai, and Yu-Xiang Wang.
\newblock Near-optimal offline reinforcement learning via double variance
  reduction.
\newblock \emph{arXiv preprint arXiv:2102.01748}, 2021.

\bibitem[Yu(1997)]{yu1997assouad}
Bin Yu.
\newblock Assouad, fano, and le cam.
\newblock In \emph{{Festschrift for Lucien Le Cam}}, pages 423--435. Springer,
  1997.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{yu2020mopo}
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine,
  Chelsea Finn, and Tengyu Ma.
\newblock {MOPO: M}odel-based offline policy optimization.
\newblock \emph{arXiv preprint arXiv:2005.13239}, 2020.

\bibitem[Yu et~al.(2021)Yu, Kumar, Rafailov, Rajeswaran, Levine, and
  Finn]{yu2021combo}
Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine,
  and Chelsea Finn.
\newblock {COMBO: C}onservative offline model-based policy optimization.
\newblock \emph{arXiv preprint arXiv:2102.08363}, 2021.

\bibitem[Yurtsever et~al.(2020)Yurtsever, Lambert, Carballo, and
  Takeda]{yurtsever2020survey}
Ekim Yurtsever, Jacob Lambert, Alexander Carballo, and Kazuya Takeda.
\newblock A survey of autonomous driving: {C}ommon practices and emerging
  technologies.
\newblock \emph{IEEE Access}, 8:\penalty0 58443--58469, 2020.

\bibitem[Zanette(2020)]{zanette2020exponential}
Andrea Zanette.
\newblock Exponential lower bounds for batch reinforcement learning: {Batch RL}
  can be exponentially harder than online {RL}.
\newblock \emph{arXiv preprint arXiv:2012.08005}, 2020.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Koppel, Bedi, Szepesvari, and
  Wang]{zhang2020variational}
Junyu Zhang, Alec Koppel, Amrit~Singh Bedi, Csaba Szepesvari, and Mengdi Wang.
\newblock Variational policy gradient method for reinforcement learning with
  general utilities.
\newblock \emph{arXiv preprint arXiv:2007.02151}, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Dai, Li, and
  Schuurmans]{Zhang2020GenDICE:}
Ruiyi Zhang, Bo~Dai, Lihong Li, and Dale Schuurmans.
\newblock {GenDICE: G}eneralized offline estimation of stationary values.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2020{\natexlab{c}})Zhang, Liu, and
  Whiteson]{zhang2020gradientdice}
Shantong Zhang, Bo~Liu, and Shimon Whiteson.
\newblock {GradientDICE: R}ethinking generalized offline estimation of
  stationary values.
\newblock \emph{arXiv preprint arXiv:2001.11113}, 2020{\natexlab{c}}.

\end{thebibliography}
