@inproceedings{butakov2024lossy_compression,
    title={Information Bottleneck Analysis of Deep Neural Networks via Lossy Compression},
    author={Ivan Butakov and Alexander Tolmachev and Sofia Malanchuk and Anna Neopryatnaya and Alexey Frolov and Kirill Andreev},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=huGECz8dPp}
}

@inproceedings{anonymous2024lossy_compression,
    title={Information Bottleneck Analysis of Deep Neural Networks via Lossy Compression},
    author={Anonymous},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=huGECz8dPp}
}

@article{fefferman2013testing_manifold_hypothesis,
    author = {Fefferman, Charles and Mitter, Sanjoy and Narayanan, Hariharan},
    year = {2013},
    month = {10},
    pages = {},
    title = {Testing the Manifold Hypothesis},
    volume = {29},
    journal = {Journal of the American Mathematical Society},
    doi = {10.1090/jams/852}
}

@inproceedings{xu2017IT_analysis,
    author = {Xu, Aolin and Raginsky, Maxim},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Information-theoretic analysis of generalization capability of learning algorithms},
    url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ad71c82b22f4f65b9398f76d8be4c615-Paper.pdf},
    volume = {30},
    year = {2017}
}

@inproceedings{goldfeld2019estimating_information_flow,
    title={Estimating Information Flow in Deep Neural Networks},
    author={Ziv Goldfeld and Ewout van den Berg and Kristjan H. Greenewald and Igor V. Melnyk and Nam H. Nguyen and Brian Kingsbury and Yury Polyanskiy},
    booktitle={ICML},
    year={2019}
}

@InProceedings{abernethy2020reasoning_conditional_MI,
    title = 	 {{R}easoning {A}bout {G}eneralization via {C}onditional {M}utual {I}nformation},
    author =       {Steinke, Thomas and Zakynthinou, Lydia},
    booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
    pages = 	 {3437--3452},
    year = 	 {2020},
    editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
    volume = 	 {125},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {09--12 Jul},
    publisher =    {PMLR},
    pdf = 	 {http://proceedings.mlr.press/v125/steinke20a/steinke20a.pdf},
    url = 	 {https://proceedings.mlr.press/v125/steinke20a.html},
    abstract = 	 { We provide an information-theoretic framework for studying the generalization properties of machine learning algorithms. Our framework ties together existing approaches, including uniform convergence bounds and recent methods for adaptive data analysis. Specifically, we use Conditional Mutual Information (CMI) to quantify how well the input (i.e., the training data) can be recognized given the output (i.e., the trained model) of the learning algorithm. We show that bounds on CMI can be obtained from VC dimension, compression schemes, differential privacy, and other methods. We then show that bounded CMI implies various forms of generalization.}
}

@article{kairen2018individual_neurons,
    author={Amjad, Rana Ali and Liu, Kairen and Geiger, Bernhard C.},
    journal={IEEE Transactions on Neural Networks and Learning Systems}, 
    title={Understanding Neural Networks and Individual Neuron Importance via Information-Ordered Cumulative Ablation}, 
    year={2022},
    volume={33},
    number={12},
    pages={7842-7852},
    doi={10.1109/TNNLS.2021.3088685}
}

@article{tishby2015bottleneck_principle,
    title={Deep learning and the information bottleneck principle},
    author={Naftali Tishby and Noga Zaslavsky},
    journal={2015 IEEE Information Theory Workshop (ITW)},
    year={2015},
    pages={1-5}
}

@inproceedings{chen2016infogan,
    author       = {Xi Chen and
                    Yan Duan and
                    Rein Houthooft and
                    John Schulman and
                    Ilya Sutskever and
                    Pieter Abbeel},
    editor       = {Daniel D. Lee and
                    Masashi Sugiyama and
                    Ulrike von Luxburg and
                    Isabelle Guyon and
                    Roman Garnett},
    title        = {InfoGAN: Interpretable Representation Learning by Information Maximizing
                    Generative Adversarial Nets},
    booktitle    = {Advances in Neural Information Processing Systems 29: Annual Conference
                    on Neural Information Processing Systems 2016, December 5-10, 2016,
                    Barcelona, Spain},
    pages        = {2172--2180},
    year         = {2016},
    url          = {https://proceedings.neurips.cc/paper/2016/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html},
    timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
    biburl       = {https://dblp.org/rec/conf/nips/ChenCDHSSA16.bib},
    bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{donsker1983dv,
    title = "Asymptotic evaluation of certain markov process expectations for large time. IV",
    author = "Donsker, {M. D.} and Varadhan, {S. R.S.}",
    year = "1983",
    month = mar,
    doi = "10.1002/cpa.3160360204",
    language = "English (US)",
    volume = "36",
    pages = "183--212",
    journal = "Communications on Pure and Applied Mathematics",
    issn = "0010-3640",
    publisher = "Wiley-Liss Inc.",
    number = "2",
}

@inproceedings{nguyen2007nwj,
    author = {Nguyen, XuanLong and Wainwright, Martin J and Jordan, Michael},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization},
    url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/72da7fd6d1302c0a159f6436d01e9eb0-Paper.pdf},
    volume = {20},
    year = {2007}
}

@inproceedings{belghazi2018mine,
    title = {Mutual Information Neural Estimation},
    author = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
    booktitle = {Proceedings of the 35th International Conference on Machine Learning},
    pages = {531--540},
    year = {2018},
    editor = {Dy, Jennifer and Krause, Andreas},
    volume = {80},
    series = {Proceedings of Machine Learning Research},
    month = {07},
    publisher = {PMLR},
    pdf = {http://proceedings.mlr.press/v80/belghazi18a/belghazi18a.pdf},  url = {https://proceedings.mlr.press/v80/belghazi18a.html},
    abstract = {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement the Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.}
}

@misc{nishiyama2019new_lower_bound_on_kld,
    title={A New Lower Bound for Kullback-Leibler Divergence Based on Hammersley-Chapman-Robbins Bound}, 
    author={Tomohiro Nishiyama},
    year={2019},
    eprint={1907.00288},
    archivePrefix={arXiv},
    primaryClass={math.ST}
}

@inproceedings{ardizzone2020training_normflows,
    author = {Ardizzone, Lynton and Mackowiak, Radek and Rother, Carsten and K\"{o}the, Ullrich},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
    pages = {7828--7840},
    publisher = {Curran Associates, Inc.},
    title = {Training Normalizing Flows with the Information Bottleneck for Competitive Generative Classification},
    url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/593906af0d138e69f49d251d3e7cbed0-Paper.pdf},
    volume = {33},
    year = {2020}
}


@article{berrett2017independence_testing,
    author = {Berrett, Thomas and Samworth, Richard},
    year = {2017},
    month = {11},
    pages = {},
    title = {Nonparametric independence testing via mutual information},
    volume = {106},
    journal = {Biometrika},
    doi = {10.1093/biomet/asz024}
}

@inproceedings{sen2017conditional_independence_test,
    author = {Sen, Rajat and Suresh, Ananda Theertha and Shanmugam, Karthikeyan and Dimakis, Alexandros G and Shakkottai, Sanjay},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Model-Powered Conditional Independence Test},
    url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/02f039058bd48307e6f653a2005c9dd2-Paper.pdf},
    volume = {30},
    year = {2017}
}

@article{duong2023normflows_for_conditional_independence_testing,
    author = {Duong, Bao and Nguyen, Thin},
    year = {2023},
    month = {08},
    pages = {},
    title = {Normalizing flows for conditional independence testing},
    volume = {66},
    journal = {Knowledge and Information Systems},
    doi = {10.1007/s10115-023-01964-w}
}


@article{duong2023dine,
    title={Diffeomorphic Information Neural Estimation},
    volume={37},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/25908},
    DOI={10.1609/aaai.v37i6.25908},
    abstractNote={Mutual Information (MI) and Conditional Mutual Information (CMI) are multi-purpose tools from information theory that are able to naturally measure the statistical dependencies between random variables, thus they are usually of central interest in several statistical and machine learning tasks, such as conditional independence testing and representation learning. However, estimating CMI, or even MI, is infamously challenging due the intractable formulation. In this study, we introduce DINE (Diffeomorphic Information Neural Estimator)–a novel approach for estimating CMI of continuous random variables, inspired by the invariance of CMI over diffeomorphic maps. We show that the variables of interest can be replaced with appropriate surrogates that follow simpler distributions, allowing the CMI to be efficiently evaluated via analytical solutions. Additionally, we demonstrate the quality of the proposed estimator in comparison with state-of-the-arts in three important tasks, including estimating MI, CMI, as well as its application in conditional independence testing. The empirical evaluations show that DINE consistently outperforms competitors in all tasks and is able to adapt very well to complex and high-dimensional relationships.},
    number={6},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    author={Duong, Bao and Nguyen, Thin},
    year={2023},
    month={Jun.},
    pages={7468-7475}
}

@article{cai2015logdet,
    title = {Law of log determinant of sample covariance matrix and optimal estimation of differential entropy for high-dimensional Gaussian distributions},
    journal = {Journal of Multivariate Analysis},
    volume = {137},
    pages = {161-172},
    year = {2015},
    issn = {0047-259X},
    doi = {https://doi.org/10.1016/j.jmva.2015.02.003},
    url = {https://www.sciencedirect.com/science/article/pii/S0047259X1500038X},
    author = {T. Tony Cai and Tengyuan Liang and Harrison H. Zhou},
    keywords = {Asymptotic optimality, Central limit theorem, Covariance matrix, Determinant, Differential entropy, Minimax lower bound, Sharp minimaxity},
    abstract = {Differential entropy and log determinant of the covariance matrix of a multivariate Gaussian distribution have many applications in coding, communications, signal processing and statistical inference. In this paper we consider in the high-dimensional setting optimal estimation of the differential entropy and the log-determinant of the covariance matrix. We first establish a central limit theorem for the log determinant of the sample covariance matrix in the high-dimensional setting where the dimension p(n) can grow with the sample size n. An estimator of the differential entropy and the log determinant is then considered. Optimal rate of convergence is obtained. It is shown that in the case p(n)/n→0 the estimator is asymptotically sharp minimax. The ultra-high-dimensional setting where p(n)>n is also discussed.}
}


@inproceedings{tishby1999information,
    added-at = {2016-05-01T18:03:54.000+0200},
    author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
    biburl = {https://www.bibsonomy.org/bibtex/2c61af806ab3a8fe92154753e84736818/nosebrain},
    booktitle = {Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing},
    comment = {cte: information bottleneck},
    interhash = {15bd5efbf394791da00b09839b9a5757},
    intrahash = {c61af806ab3a8fe92154753e84736818},
    keywords = {bottleneck information method todo:data-mining},
    pages = {368-377},
    timestamp = {2016-05-01T18:05:41.000+0200},
    title = {The information bottleneck method},
    url = {/brokenurl#citeseer.nj.nec.com/tishby99information.html},
    year = 1999
}

@misc{shwartz_ziv2017opening_black_box,
    title={Opening the Black Box of Deep Neural Networks via Information}, 
    author={Ravid Shwartz-Ziv and Naftali Tishby},
    year={2017},
    eprint={1703.00810},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{he2023generalization_bounds,
    title={Information-Theoretic Generalization Bounds for Deep Neural Networks},
    author={Haiyun He and Christina Yu and Ziv Goldfeld},
    booktitle={NeurIPS 2023 workshop: Information-Theoretic Principles in Cognitive Systems},
    year={2023},
    url={https://openreview.net/forum?id=udEjq72DFO}
}


@article{goldfeld2020convergence_of_SEM_entropy_estimation,
    author={Z. {Goldfeld} and K. {Greenewald} and J. {Niles-Weed} and Y. {Polyanskiy}},
    journal={IEEE Transactions on Information Theory}, 
    title={Convergence of Smoothed Empirical Measures With Applications to Entropy Estimation}, 
    year={2020},
    volume={66},
    number={7},
    pages={4368-4391},
    doi={10.1109/TIT.2020.2975480}
}

@InProceedings{mcallester2020limitations_MI,
    title = 	 {Formal Limitations on the Measurement of Mutual Information},
    author =       {McAllester, David and Stratos, Karl},
    booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
    pages = 	 {875--884},
    year = 	 {2020},
    editor = 	 {Chiappa, Silvia and Calandra, Roberto},
    volume = 	 {108},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {08},
    publisher =    {PMLR},
    pdf = 	 {http://proceedings.mlr.press/v108/mcallester20a/mcallester20a.pdf},
    url = 	 {https://proceedings.mlr.press/v108/mcallester20a.html},
    abstract = 	 {Measuring mutual information from finite data is difficult. Recent work has considered variational methods maximizing a lower bound. In this paper, we prove that serious statistical limitations are inherent to any method of measuring mutual information. More specifically, we show that any distribution-free high-confidence lower bound on mutual information estimated from N samples cannot be larger than O(ln N).}
}

@InProceedings{poole2019on_variational_bounds_MI,
    title = 	 {On Variational Bounds of Mutual Information},
    author =       {Poole, Ben and Ozair, Sherjil and Van Den Oord, Aaron and Alemi, Alex and Tucker, George},
    booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
    pages = 	 {5171--5180},
    year = 	 {2019},
    editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
    volume = 	 {97},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {06},
    publisher =    {PMLR},
    pdf = 	 {http://proceedings.mlr.press/v97/poole19a/poole19a.pdf},
    url = 	 {https://proceedings.mlr.press/v97/poole19a.html},
    abstract = 	 {Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning, but bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks. However, the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of these new bounds for estimation and representation learning.}
}

@inproceedings{song2020understanding_limitations,
    title={xtanding the Limitations of Variational Mutual Information Estimators},
    author={Jiaming Song and Stefano Ermon},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=B1x62TNtDS}
}

@article{arellano_valle2013MI_for_skew_distributions,
    author = {Arellano-Valle, R. and Contreras-Reyes, Javier and Genton, Marc},
    year = {2013},
    month = {03},
    pages = {42-62},
    title = {Shannon Entropy and Mutual Information for Multivariate Skew-Elliptical Distributions},
    volume = {40},
    journal = {Scandinavian Journal of Statistics},
    doi = {10.1111/j.1467-9469.2011.}
}

@inproceedings{czyz2023beyond_normal,
    title={Beyond Normal: On the Evaluation of Mutual Information Estimators},
    author={Pawe{\l} Czy{\.z} and Frederic Grabowski and Julia E Vogt and Niko Beerenwinkel and Alexander Marx},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=25vRtG56YH}
}

@misc{czyz2023pointwise_MI,
      title={The Mixtures and the Neural Critics: On the Pointwise Mutual Information Profiles of Fine Distributions}, 
      author={Paweł Czyż and Frederic Grabowski and Julia E. Vogt and Niko Beerenwinkel and Alexander Marx},
      year={2023},
      eprint={2310.10240},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{geiger2022IP_overview,
	doi = {10.1109/tnnls.2021.3089037},
	url = {https://doi.org/10.1109\%2Ftnnls.2021.3089037},
	year = 2022,
	month = {12},
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume = {33},
	number = {12},
	pages = {7039--7051},
	author = {Bernhard C. Geiger},
	title = {On Information Plane Analyses of Neural Network Classifiers{\textemdash}A Review},
	journal = {{IEEE} Transactions on Neural Networks and Learning Systems}
}

@inproceedings{chen2000gaussianization,
    author = {Chen, Scott and Gopinath, Ramesh},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {T. Leen and T. Dietterich and V. Tresp},
    pages = {},
    publisher = {MIT Press},
    title = {Gaussianization},
    url = {https://proceedings.neurips.cc/paper_files/paper/2000/file/3c947bc2f7ff007b86a9428b74654de5-Paper.pdf},
    volume = {13},
    year = {2000}
}


@article{tabak2010PDF_estimators_LL_ascend,
author = {Esteban G. Tabak and Eric Vanden-Eijnden},
title = {{Density estimation by dual ascent of the log-likelihood}},
volume = {8},
journal = {Communications in Mathematical Sciences},
number = {1},
publisher = {International Press of Boston},
pages = {217 -- 233},
keywords = {Density estimation, machine learning, maximum likelihood},
year = {2010},
}


@article{tabak2013nonparametric_PDF_estimators,
    author = {Tabak, E. G. and Turner, Cristina V.},
    title = {A Family of Nonparametric Density Estimation Algorithms},
    journal = {Communications on Pure and Applied Mathematics},
    volume = {66},
    number = {2},
    pages = {145-164},
    doi = {https://doi.org/10.1002/cpa.21423},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21423},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.21423},
    abstract = {Abstract A new methodology for density estimation is proposed. The methodology, which builds on the one developed by Tabak and Vanden-Eijnden, normalizes the data points through the composition of simple maps. The parameters of each map are determined through the maximization of a local quadratic approximation to the log-likelihood. Various candidates for the elementary maps of each step are proposed; criteria for choosing one includes robustness, computational simplicity, and good behavior in high-dimensional settings. A good choice is that of localized radial expansions, which depend on a single parameter: all the complexity of arbitrary, possibly convoluted probability densities can be built through the composition of such simple maps. © 2012 Wiley Periodicals, Inc.},
    year = {2013}
}



@inproceedings{dinh2015NICE,
    author       = {Laurent Dinh and
                    David Krueger and
                    Yoshua Bengio},
    editor       = {Yoshua Bengio and
                    Yann LeCun},
    title        = {{NICE:} Non-linear Independent Components Estimation},
    booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                    San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings},
    year         = {2015},
    url          = {http://arxiv.org/abs/1410.8516},
    timestamp    = {Wed, 17 Jul 2019 10:40:54 +0200},
    biburl       = {https://dblp.org/rec/journals/corr/DinhKB14.bib},
    bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{rezende2015variational_inference_normflows,
    author       = {Danilo Jimenez Rezende and
                    Shakir Mohamed},
    editor       = {Francis R. Bach and
                    David M. Blei},
    title        = {Variational Inference with Normalizing Flows},
    booktitle    = {Proceedings of the 32nd International Conference on Machine Learning,
                    {ICML} 2015, Lille, France, 6-11 July 2015},
    series       = {{JMLR} Workshop and Conference Proceedings},
    volume       = {37},
    pages        = {1530--1538},
    publisher    = {JMLR.org},
    year         = {2015},
    url          = {http://proceedings.mlr.press/v37/rezende15.html},
    timestamp    = {Wed, 29 May 2019 08:41:45 +0200},
    biburl       = {https://dblp.org/rec/conf/icml/RezendeM15.bib},
    bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Ao_Li_2022entropy_estimation_normflows,
    title={Entropy Estimation via Normalizing Flow},
    volume={36},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/21237},
    DOI={10.1609/aaai.v36i9.21237},
    abstractNote={Entropy estimation is an important problem in information theory and statistical science. Many popular entropy estimators suffer from fast growing estimation bias with respect to dimensionality, rendering them unsuitable for high dimensional problems. In this work we propose a transformbased method for high dimensional entropy estimation, which consists of the following two main ingredients. First by modifying the k-NN based entropy estimator, we propose a new estimator which enjoys small estimation bias for samples that are close to a uniform distribution. Second we design a normalizing flow based mapping that pushes samples toward a uniform distribution, and the relation between the entropy of the original samples and the transformed ones is also derived. As a result the entropy of a given set of samples is estimated by first transforming them toward a uniform distribution and then applying the proposed estimator to the transformed samples. Numerical experiments demonstrate the effectiveness of the method for high dimensional entropy estimation problems.},
    number={9},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    author={Ao, Ziqiao and Li, Jinglai},
    year={2022},
    month={Jun.},
    pages={9990-9998}
}

@inproceedings{franzese2024minde,
    title={{MINDE}: Mutual Information Neural Diffusion Estimation},
    author={Giulio Franzese and Mustapha BOUNOUA and Pietro Michiardi},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=0kWd8SJq8d}
}

@inproceedings{anonymous2024minde,
    title={{MINDE}: Mutual Information Neural Diffusion Estimation},
    author={Anonymous},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=0kWd8SJq8d}
}


@book{spivak1965calculus,
    title={Calculus On Manifolds: A Modern Approach To Classical Theorems Of Advanced Calculus},
    author={Spivak, M.},
    isbn={9780805390216},
    lccn={lc66010910},
    year={1965},
    publisher={Avalon Publishing}
}

@article{loaiza2022manifold_overfitting,
    title={Diagnosing and Fixing Manifold Overfitting in Deep Generative Models},
    author={Gabriel Loaiza-Ganem and Brendan Leigh Ross and Jesse C Cresswell and Anthony L. Caterini},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2022},
    url={https://openreview.net/forum?id=0nEZCVshxS},
    note={Expert Certification}
}


@book{cover2006information_theory,
    author = {Cover, Thomas M. and Thomas, Joy A.},
    title = {Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)},
    year = {2006},
    publisher = {Wiley-Interscience},
    address = {USA}
}


@book{polyanskiy2024information_theory,
    title={Information Theory: From Coding to Learning},
    author={Polyanskiy, Y. and Wu, Y.},
    isbn={9781108832908},
    url={https://books.google.ru/books?id=CySo0AEACAAJ},
    year={2024},
    publisher={Cambridge University Press}
}


@article{butakov2021high_dimensional_entropy_estimation,
    author={Butakov, I. D. and Malanchuk, S. V. and Neopryatnaya, A. M. and Tolmachev, A. D. and Andreev, K. V. and Kruglik, S. A. and Marshakov, E. A. and Frolov, A. A.},
    title={High-Dimensional Dataset Entropy Estimation via Lossy Compression},
    journal={Journal of Communications Technology and Electronics},
    year={2021},
    month={7},
    day={01},
    volume={66},
    number={6},
    pages={764-768},
    abstract={This paper proposes a method for estimating a high-dimensional dataset entropy based on compression via autoencoder. We give theoretical proof of such estimation relevance and illustrate it by experiments with synthetic data and the MNIST digits dataset.},
    issn={1555-6557},
    doi={10.1134/S1064226921060061},
    url={https://doi.org/10.1134/S1064226921060061}
}

@misc{adilova2023IP_dropout,
    title={Information Plane Analysis for Dropout Neural Networks}, 
    author={Linara Adilova and Bernhard C. Geiger and Asja Fischer},
    year={2023},
    eprint={2303.00596},
    archivePrefix={arXiv},
    primaryClass={cs.IT}
}

@misc{oord2019representation_learning_CPC,
      title={Representation Learning with Contrastive Predictive Coding}, 
      author={Aaron van den Oord and Yazhe Li and Oriol Vinyals},
      year={2019},
      eprint={1807.03748},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{rhodes2020telescoping,
 author = {Rhodes, Benjamin and Xu, Kai and Gutmann, Michael U.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {4905--4916},
 publisher = {Curran Associates, Inc.},
 title = {Telescoping Density-Ratio Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/33d3b157ddc0896addfb22fa2a519097-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{letizia2022copula_NN_estimation,
      title={Copula Density Neural Estimation}, 
      author={Nunzio A. Letizia and Andrea M. Tonello},
      year={2022},
      eprint={2211.15353},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{kristjan2023smoothed_entropy_PCA,
  author       = {Kristjan H. Greenewald and
                  Brian Kingsbury and
                  Yuancheng Yu},
  title        = {High-Dimensional Smoothed Entropy Estimation via Dimensionality Reduction},
  booktitle    = {{IEEE} International Symposium on Information Theory, {ISIT} 2023,
                  Taipei, Taiwan, June 25-30, 2023},
  pages        = {2613--2618},
  publisher    = {{IEEE}},
  year         = {2023},
  url          = {https://doi.org/10.1109/ISIT54713.2023.10206641},
  doi          = {10.1109/ISIT54713.2023.10206641},
  timestamp    = {Mon, 28 Aug 2023 17:20:14 +0200},
  biburl       = {https://dblp.org/rec/conf/isit/GreenewaldKY23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{kozachenko1987entropy_of_random_vector,
    title = {Sample Estimate of the Entropy of a Random Vector},
    author = {L. F. Kozachenko and N. N. Leonenko},
    journal = {Problems Inform. Transmission},
    pages = {95--101},
    year = {1987},
    volume = {23},
    issue = {2},
}

@article{kraskov2004KSG,
    title = {Estimating mutual information},
    author = {Kraskov, Alexander and St\"ogbauer, Harald and Grassberger, Peter},
    journal = {Phys. Rev. E},
    volume = {69},
    issue = {6},
    pages = {066138},
    numpages = {16},
    year = {2004},
    month = {Jun},
    publisher = {American Physical Society},
    doi = {10.1103/PhysRevE.69.066138},
    url = {https://link.aps.org/doi/10.1103/PhysRevE.69.066138}
}


@article{berrett2019efficient_knn_entropy_estimation,
    author = {Berrett, Thomas B. and Samworth, Richard J. and Yuan, Ming},
    doi = {10.1214/18-AOS1688},
    fjournal = {Annals of Statistics},
    journal = {Ann. Statist.},
    month = {02},
    number = {1},
    pages = {288--318},
    publisher = {The Institute of Mathematical Statistics},
    title = {Efficient multivariate entropy estimation via $k$-nearest neighbour distances},
    url = {https://doi.org/10.1214/18-AOS1688},
    volume = {47},
    year = {2019}
}


@inproceedings{kingma2018GLOW,
    author = {Kingma, Durk P and Dhariwal, Prafulla},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
    url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf},
    volume = {31},
    year = {2018}
}

@inproceedings{dinh2017real_NVP,
title={Density estimation using Real {NVP}},
author={Laurent Dinh and Jascha Sohl-Dickstein and Samy Bengio},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=HkpbnH9lx}
}


@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{kingma2021density_estimation_with_diffusion_models,
    title={On Density Estimation with Diffusion Models},
    author={Diederik P Kingma and Tim Salimans and Ben Poole and Jonathan Ho},
    booktitle={Advances in Neural Information Processing Systems},
    editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
    year={2021},
    url={https://openreview.net/forum?id=2LdBqxc1Yv}
}


@article{kobyzev2021normflows_overview,
    author = {I. Kobyzev and S. D. Prince and M. A. Brubaker},
    journal = {IEEE Transactions on Pattern Analysis \& Machine Intelligence},
    title = {Normalizing Flows: An Introduction and Review of Current Methods},
    year = {2021},
    volume = {43},
    number = {11},
    issn = {1939-3539},
    pages = {3964-3979},
    abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
    keywords = {estimation;jacobian matrices;mathematical model;training;computational modeling;context modeling;random variables},
    doi = {10.1109/TPAMI.2020.2992934},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month = {nov}
}


@inproceedings{zhang2021diffusion_normalizing_flows,
     author = {Zhang, Qinsheng and Chen, Yongxin},
     booktitle = {Advances in Neural Information Processing Systems},
     editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
     pages = {16280--16291},
     publisher = {Curran Associates, Inc.},
     title = {Diffusion Normalizing Flow},
     url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/876f1f9954de0aa402d91bb988d12cd4-Paper.pdf},
     volume = {34},
     year = {2021}
}

@inproceedings{brehmer2020flows_manifold_learning,
    author = {Brehmer, Johann and Cranmer, Kyle},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
    pages = {442--453},
    publisher = {Curran Associates, Inc.},
    title = {Flows for simultaneous manifold learning and density estimation},
    url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/051928341be67dcba03f0e04104d9047-Paper.pdf},
    volume = {33},
    year = {2020}
}


@article{teng2019invertible_AE,
    AUTHOR = {Teng, Yunfei and Choromanska, Anna},
    TITLE = {Invertible Autoencoder for Domain Adaptation},
    JOURNAL = {Computation},
    VOLUME = {7},
    YEAR = {2019},
    NUMBER = {2},
    ARTICLE-NUMBER = {20},
    URL = {https://www.mdpi.com/2079-3197/7/2/20},
    ISSN = {2079-3197},
    ABSTRACT = {The unsupervised image-to-image translation aims at finding a mapping between the source ( A ) and target ( B ) image domains, where in many applications aligned image pairs are not available at training. This is an ill-posed learning problem since it requires inferring the joint probability distribution from marginals. Joint learning of coupled mappings F A B : A &rarr; B and F B A : B &rarr; A is commonly used by the state-of-the-art methods, like CycleGAN to learn this translation by introducing cycle consistency requirement to the learning problem, i.e., F A B ( F B A ( B ) ) &asymp; B and F B A ( F A B ( A ) ) &asymp; A . Cycle consistency enforces the preservation of the mutual information between input and translated images. However, it does not explicitly enforce F B A to be an inverse operation to F A B . We propose a new deep architecture that we call invertible autoencoder (InvAuto) to explicitly enforce this relation. This is done by forcing an encoder to be an inverted version of the decoder, where corresponding layers perform opposite mappings and share parameters. The mappings are constrained to be orthonormal. The resulting architecture leads to the reduction of the number of trainable parameters (up to 2 times). We present image translation results on benchmark datasets and demonstrate state-of-the art performance of our approach. Finally, we test the proposed domain adaptation method on the task of road video conversion. We demonstrate that the videos converted with InvAuto have high quality and show that the NVIDIA neural-network-based end-to-end learning system for autonomous driving, known as PilotNet, trained on real road videos performs well when tested on the converted ones.},
    DOI = {10.3390/computation7020020}
}


@article{lancaster1958bivariate_structure,
    ISSN = {00034851},
    URL = {http://www.jstor.org/stable/2237259},
    author = {H. O. Lancaster},
    journal = {The Annals of Mathematical Statistics},
    number = {3},
    pages = {719--736},
    publisher = {Institute of Mathematical Statistics},
    title = {The Structure of Bivariate Distributions},
    urldate = {2024-05-21},
    volume = {29},
    year = {1958}
}

@article{hannan1961cca,
    title={The general theory of canonical correlation and its relation to functional analysis},
    volume={2},
    DOI={10.1017/S1446788700026707},
    number={2},
    journal={Journal of the Australian Mathematical Society},
    author={Hannan, E. J.},
    year={1961},
    pages={229–242}
}


@article{stimper2023normflows, 
  author = {Vincent Stimper and David Liu and Andrew Campbell and Vincent Berenz and Lukas Ryll and Bernhard Schölkopf and José Miguel Hernández-Lobato}, 
  title = {normflows: A PyTorch Package for Normalizing Flows}, 
  journal = {Journal of Open Source Software}, 
  volume = {8},
  number = {86}, 
  pages = {5361}, 
  publisher = {The Open Journal}, 
  doi = {10.21105/joss.05361}, 
  url = {https://doi.org/10.21105/joss.05361}, 
  year = {2023}
} 