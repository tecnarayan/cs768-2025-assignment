@article{nedic2020review,
  title     = {Distributed Gradient Methods for Convex Machine Learning Problems in Networks: Distributed Optimization},
  author    = {Nedic, Angelia},
  journal   = {{IEEE} Signal Processing Magazine},
  url       = {https://ieeexplore.ieee.org/abstract/document/9084356},
  volume    = {37},
  number    = {3},
  pages     = {92--101},
  year      = {2020},
  publisher = {{IEEE}}
}

@inproceedings{singh2019model,
  title     = {Model Fusion via Optimal Transport},
  author    = {Singh, Sidak Pal and Jaggi, Martin},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2020}
}

@inproceedings{he2020group,
  title     = {Group Knowledge Transfer: Collaborative Training of Large CNNs on the Edge},
  author    = {He, Chaoyang and Avestimehr, Salman and Annavaram, Murali},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2020}
}

@article{hsu2019measuring,
  title   = {Measuring the effects of non-identical data distribution for federated visual classification},
  author  = {Hsu, Tzu-Ming Harry and Qi, Hang and Brown, Matthew},
  journal = {arXiv preprint arXiv:1909.06335},
  year    = {2019}
}

@article{caldas2018leaf,
  title   = {Leaf: A benchmark for federated settings},
  author  = {Caldas, Sebastian and Wu, Peter and Li, Tian and Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Smith, Virginia and Talwalkar, Ameet},
  journal = {arXiv preprint arXiv:1812.01097},
  year    = {2018}
}

@article{yurochkin2019bayesian,
  title   = {Bayesian nonparametric federated learning of neural networks},
  author  = {Yurochkin, Mikhail and Agarwal, Mayank and Ghosh, Soumya and Greenewald, Kristjan and Hoang, Trong Nghia and Khazaeni, Yasaman},
  journal = {arXiv preprint arXiv:1905.12022},
  year    = {2019}
}

@article{mcmahan2016communication,
  title   = {Communication-efficient learning of deep networks from decentralized data},
  author  = {McMahan, H Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and others},
  journal = {arXiv preprint arXiv:1602.05629},
  year    = {2016}
}

@inproceedings{Wang2020Federated,
  title     = {Federated Learning with Matched Averaging},
  author    = {Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},
  booktitle = {International Conference on Learning Representations},
  year      = {2020},
  url       = {https://openreview.net/forum?id=BkluqlSFDS}
}

@inproceedings{micaelli2019zero,
  title     = {Zero-shot knowledge transfer via adversarial belief matching},
  author    = {Micaelli, Paul and Storkey, Amos J},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {9547--9557},
  year      = {2019}
}

@inproceedings{chavdarova2018sgan,
  title     = {Sgan: An alternative training of generative adversarial networks},
  author    = {Chavdarova, Tatjana and Fleuret, Fran{\c{c}}ois},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {9407--9415},
  year      = {2018}
}

@inproceedings{sun2017ensemble,
  title        = {Ensemble-compression: A new method for parallel training of deep neural networks},
  author       = {Sun, Shizhao and Chen, Wei and Bian, Jiang and Liu, Xiaoguang and Liu, Tie-Yan},
  booktitle    = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages        = {187--202},
  year         = {2017},
  organization = {Springer}
}

@article{gao2019multistructure,
  title     = {Multistructure-Based Collaborative Online Distillation},
  author    = {Gao, Liang and Lan, Xu and Mi, Haibo and Feng, Dawei and Xu, Kele and Peng, Yuxing},
  journal   = {Entropy},
  volume    = {21},
  number    = {4},
  pages     = {357},
  year      = {2019},
  publisher = {Multidisciplinary Digital Publishing Institute}
}

@article{li2019fedmd,
  title   = {FedMD: Heterogenous Federated Learning via Model Distillation},
  author  = {Li, Daliang and Wang, Junpu},
  journal = {arXiv preprint arXiv:1910.03581},
  year    = {2019}
}

@article{shoham2019overcoming,
  title   = {Overcoming Forgetting in Federated Learning on Non-IID Data},
  author  = {Shoham, Neta and Avidor, Tomer and Keren, Aviv and Israel, Nadav and Benditkis, Daniel and Mor-Yosef, Liron and Zeitak, Itai},
  journal = {arXiv preprint arXiv:1910.07796},
  year    = {2019}
}

@article{karimireddy2019scaffold,
  title   = {SCAFFOLD: Stochastic controlled averaging for on-device federated learning},
  author  = {Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank J and Stich, Sebastian U and Suresh, Ananda Theertha},
  journal = {arXiv preprint arXiv:1910.06378},
  year    = {2019}
}

@inproceedings{hsu2020federated,
  title     = {Federated Visual Classification with Real-World Data Distribution},
  author    = {Hsu, Tzu-Ming Harry and Qi, Hang and Brown, Matthew},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year      = {2020}
}

@article{deng2020adaptive,
  title   = {Adaptive Personalized Federated Learning},
  author  = {Deng, Yuyang and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad},
  journal = {arXiv preprint arXiv:2003.13461},
  year    = {2020}
}

@article{ben2010theory,
  title     = {A theory of learning from different domains},
  author    = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  journal   = {Machine learning},
  volume    = {79},
  number    = {1-2},
  pages     = {151--175},
  year      = {2010},
  publisher = {Springer}
}

@book{shalev2014understanding,
  title     = {Understanding machine learning: From theory to algorithms},
  author    = {Shalev-Shwartz, Shai and Ben-David, Shai},
  year      = {2014},
  publisher = {Cambridge university press}
}

@inproceedings{hoffman2018algorithms,
  title     = {Algorithms and theory for multiple-source adaptation},
  author    = {Hoffman, Judy and Mohri, Mehryar and Zhang, Ningshan},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {8246--8256},
  year      = {2018}
}

@inproceedings{mansour2009domain,
  title     = {Domain adaptation with multiple sources},
  author    = {Mansour, Yishay and Mohri, Mehryar and Rostamizadeh, Afshin},
  booktitle = {Advances in neural information processing systems},
  pages     = {1041--1048},
  year      = {2009}
}

@article{mansour2012multiple,
  title   = {Multiple source adaptation and the R{\'e}nyi divergence},
  author  = {Mansour, Yishay and Mohri, Mehryar and Rostamizadeh, Afshin},
  journal = {arXiv preprint arXiv:1205.2628},
  year    = {2012}
}

@article{mohri2019agnostic,
  title   = {Agnostic federated learning},
  author  = {Mohri, Mehryar and Sivek, Gary and Suresh, Ananda Theertha},
  journal = {arXiv preprint arXiv:1902.00146},
  year    = {2019}
}

@article{tian2019contrastive,
  title   = {Contrastive representation distillation},
  author  = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  journal = {arXiv preprint arXiv:1910.10699},
  year    = {2019}
}

@article{zagoruyko2016paying,
  title   = {Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer},
  author  = {Zagoruyko, Sergey and Komodakis, Nikos},
  journal = {arXiv preprint arXiv:1612.03928},
  year    = {2016}
}

@article{li2019federated,
  title   = {Federated learning: Challenges, methods, and future directions},
  author  = {Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  journal = {arXiv preprint arXiv:1908.07873},
  year    = {2019}
}

@misc{kairouz2019advances,
  title         = {Advances and Open Problems in Federated Learning},
  author        = {Peter Kairouz and H. Brendan McMahan and Brendan Avent and Aurélien Bellet and Mehdi Bennis and Arjun Nitin Bhagoji and Keith Bonawitz and Zachary Charles and Graham Cormode and Rachel Cummings and Rafael G. L. D'Oliveira and Salim El Rouayheb and David Evans and Josh Gardner and Zachary Garrett and Adrià Gascón and Badih Ghazi and Phillip B. Gibbons and Marco Gruteser and Zaid Harchaoui and Chaoyang He and Lie He and Zhouyuan Huo and Ben Hutchinson and Justin Hsu and Martin Jaggi and Tara Javidi and Gauri Joshi and Mikhail Khodak and Jakub Konečný and Aleksandra Korolova and Farinaz Koushanfar and Sanmi Koyejo and Tancrède Lepoint and Yang Liu and Prateek Mittal and Mehryar Mohri and Richard Nock and Ayfer Özgür and Rasmus Pagh and Mariana Raykova and Hang Qi and Daniel Ramage and Ramesh Raskar and Dawn Song and Weikang Song and Sebastian U. Stich and Ziteng Sun and Ananda Theertha Suresh and Florian Tramèr and Praneeth Vepakomma and Jianyu Wang and Li Xiong and Zheng Xu and Qiang Yang and Felix X. Yu and Han Yu and Sen Zhao},
  year          = {2019},
  eprint        = {1912.04977},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@inproceedings{smith2017federated,
  title     = {Federated multi-task learning},
  author    = {Smith, Virginia and Chiang, Chao-Kai and Sanjabi, Maziar and Talwalkar, Ameet S},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {4424--4434},
  year      = {2017}
}

@misc{bonawitz2019federated,
  title         = {Towards Federated Learning at Scale: System Design},
  author        = {Keith Bonawitz and Hubert Eichner and Wolfgang Grieskamp and Dzmitry Huba and Alex Ingerman and Vladimir Ivanov and Chloe Kiddon and Jakub Konečný and Stefano Mazzocchi and H. Brendan McMahan and Timon Van Overveldt and David Petrou and Daniel Ramage and Jason Roselander},
  year          = {2019},
  eprint        = {1902.01046},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@inproceedings{shokri2015privacy,
  title     = {Privacy-preserving deep learning},
  author    = {Shokri, Reza and Shmatikov, Vitaly},
  booktitle = {Proceedings of the 22nd ACM SIGSAC conference on computer and communications security},
  pages     = {1310--1321},
  year      = {2015}
}

@article{li2018federated,
  title   = {Federated optimization in heterogeneous networks},
  author  = {Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  journal = {arXiv preprint arXiv:1812.06127},
  year    = {2018}
}

@article{reddi2020adaptive,
  title   = {Adaptive Federated Optimization},
  author  = {Reddi, Sashank and Charles, Zachary and Zaheer, Manzil and Garrett, Zachary and Rush, Keith and Kone{\v{c}}n{\`y}, Jakub and Kumar, Sanjiv and McMahan, H Brendan},
  journal = {arXiv preprint arXiv:2003.00295},
  year    = {2020}
}

@inproceedings{kingma2014adam,
  title     = {Adam: A method for stochastic optimization},
  author    = {Kingma, Diederik P and Ba, Jimmy},
  booktitle = {International Conference on Learning Representations},
  year      = {2015}
}

@article{duchi2011adaptive,
  title   = {Adaptive subgradient methods for online learning and stochastic optimization},
  author  = {Duchi, John and Hazan, Elad and Singer, Yoram},
  journal = {Journal of machine learning research},
  volume  = {12},
  number  = {Jul},
  pages   = {2121--2159},
  year    = {2011}
}

@inproceedings{reddi2018on,
  title     = {On the Convergence of Adam and Beyond},
  author    = {Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
  booktitle = {International Conference on Learning Representations},
  year      = {2018},
  url       = {https://openreview.net/forum?id=ryQu7f-RZ}
}

@article{hinton2015distilling,
  title   = {Distilling the knowledge in a neural network},
  author  = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal = {arXiv preprint arXiv:1503.02531},
  year    = {2015}
}

@misc{mansour2020approaches,
  title         = {Three Approaches for Personalization with Applications to Federated Learning},
  author        = {Yishay Mansour and Mehryar Mohri and Jae Ro and Ananda Theertha Suresh},
  year          = {2020},
  eprint        = {2002.10619},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@inproceedings{peng2019federated,
  title     = {Federated Adversarial Domain Adaptation},
  author    = {Peng, Xingchao and Huang, Zijun and Zhu, Yizhe and Saenko, Kate},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}

@article{liu2018secure,
  title   = {Secure federated transfer learning},
  author  = {Liu, Yang and Chen, Tianjian and Yang, Qiang},
  journal = {arXiv preprint arXiv:1812.03337},
  year    = {2018}
}

@inproceedings{li2020fair,
  title     = {Fair Resource Allocation in Federated Learning},
  author    = {Tian Li and Maziar Sanjabi and Ahmad Beirami and Virginia Smith},
  booktitle = {International Conference on Learning Representations},
  year      = {2020},
  url       = {https://openreview.net/forum?id=ByexElSYDr}
}

@inproceedings{bucilua2006model,
  title     = {Model compression},
  author    = {Buciluǎ, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
  booktitle = {Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages     = {535--541},
  year      = {2006}
}

@inproceedings{romero2014fitnets,
  title     = {Fitnets: Hints for thin deep nets},
  author    = {Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  booktitle = {International Conference on Learning Representations},
  year      = {2015}
}

@inproceedings{yim2017gift,
  title     = {A gift from knowledge distillation: Fast optimization, network minimization and transfer learning},
  author    = {Yim, Junho and Joo, Donggyu and Bae, Jihoon and Kim, Junmo},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {4133--4141},
  year      = {2017}
}


@article{huang2017like,
  title   = {Like what you like: Knowledge distill via neuron selectivity transfer},
  author  = {Huang, Zehao and Wang, Naiyan},
  journal = {arXiv preprint arXiv:1707.01219},
  year    = {2017}
}

@inproceedings{kim2018paraphrasing,
  title     = {Paraphrasing complex network: Network compression via factor transfer},
  author    = {Kim, Jangho and Park, SeongUk and Kwak, Nojun},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {2760--2769},
  year      = {2018}
}


@inproceedings{koratana19a,
  title     = {{LIT}: Learned Intermediate Representation Training for Model Compression},
  author    = {Koratana, Animesh and Kang, Daniel and Bailis, Peter and Zaharia, Matei},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  pages     = {3509--3518},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  address   = {Long Beach, California, USA},
  month     = {09--15 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v97/koratana19a/koratana19a.pdf},
  url       = {http://proceedings.mlr.press/v97/koratana19a.html},
  abstract  = {Researchers have proposed a range of model compression techniques to reduce the computational and memory footprint of deep neural networks (DNNs). In this work, we introduce Learned Intermediate representation Training (LIT), a novel model compression technique that outperforms a range of recent model compression techniques by leveraging the highly repetitive structure of modern DNNs (e.g., ResNet). LIT uses a teacher DNN to train a student DNN of reduced depth by leveraging two key ideas: 1) LIT directly compares intermediate representations of the teacher and student model and 2) LIT uses the intermediate representation from the teacher model’s previous block as input to the current student block during training, improving stability of intermediate representations in the student network. We show that LIT can substantially reduce network size without loss in accuracy on a range of DNN architectures and datasets. For example, LIT can compress ResNet on CIFAR10 by 3.4$\times$ outperforming network slimming and FitNets. Furthermore, LIT can compress, by depth, ResNeXt 5.5$\times$ on CIFAR10 (image classification), VDCNN by 1.7$\times$ on Amazon Reviews (sentiment analysis), and StarGAN by 1.8$\times$ on CelebA (style transfer, i.e., GANs).}
}

@inproceedings{ahn2019variational,
  title     = {Variational information distillation for knowledge transfer},
  author    = {Ahn, Sungsoo and Hu, Shell Xu and Damianou, Andreas and Lawrence, Neil D and Dai, Zhenwen},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {9163--9171},
  year      = {2019}
}

@inproceedings{tung2019similarity,
  title     = {Similarity-preserving knowledge distillation},
  author    = {Tung, Frederick and Mori, Greg},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  pages     = {1365--1374},
  year      = {2019}
}

@misc{mller2020subclass,
  title         = {Subclass Distillation},
  author        = {Rafael Müller and Simon Kornblith and Geoffrey Hinton},
  year          = {2020},
  eprint        = {2002.03936},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@article{nayak2019zero,
  title   = {Zero-shot knowledge distillation in deep networks},
  author  = {Nayak, Gaurav Kumar and Mopuri, Konda Reddy and Shaj, Vaisakh and Babu, R Venkatesh and Chakraborty, Anirban},
  journal = {arXiv preprint arXiv:1905.08114},
  year    = {2019}
}

@article{hanzely2020federated,
  title   = {Federated learning of a mixture of global and local models},
  author  = {Hanzely, Filip and Richt{\'a}rik, Peter},
  journal = {arXiv preprint arXiv:2002.05516},
  year    = {2020}
}

@article{furlanello2018born,
  title   = {Born again neural networks},
  author  = {Furlanello, Tommaso and Lipton, Zachary C and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
  journal = {arXiv preprint arXiv:1805.04770},
  year    = {2018}
}

@article{rahbar2020unreasonable,
  title   = {On the Unreasonable Effectiveness of Knowledge Distillation: Analysis in the Kernel Regime},
  author  = {Rahbar, Arman and Panahi, Ashkan and Bhattacharyya, Chiranjib and Dubhashi, Devdatt and Chehreghani, Morteza Haghir},
  journal = {arXiv preprint arXiv:2003.13438},
  year    = {2020}
}

@article{tang2020understanding,
  title   = {Understanding and Improving Knowledge Distillation},
  author  = {Tang, Jiaxi and Shivanna, Rakesh and Zhao, Zhe and Lin, Dong and Singh, Anima and Chi, Ed H and Jain, Sagar},
  journal = {arXiv preprint arXiv:2002.03532},
  year    = {2020}
}

@inproceedings{phuong2019towards,
  title     = {Towards understanding knowledge distillation},
  author    = {Phuong, Mary and Lampert, Christoph},
  booktitle = {International Conference on Machine Learning},
  pages     = {5142--5151},
  year      = {2019}
}

@article{xie2019self,
  title   = {Self-training with Noisy Student improves ImageNet classification},
  author  = {Xie, Qizhe and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V},
  journal = {arXiv preprint arXiv:1911.04252},
  year    = {2019}
}

@inproceedings{Zhang_2019_ICCV,
  author    = {Zhang, Linfeng and Song, Jiebo and Gao, Anni and Chen, Jingwei and Bao, Chenglong and Ma, Kaisheng},
  title     = {Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation},
  booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2019}
}

@article{bagherinezhad2018label,
  title   = {Label refinery: Improving imagenet classification through label progression},
  author  = {Bagherinezhad, Hessam and Horton, Maxwell and Rastegari, Mohammad and Farhadi, Ali},
  journal = {arXiv preprint arXiv:1805.02641},
  year    = {2018}
}

@inproceedings{yang2019snapshot,
  title     = {Snapshot distillation: Teacher-student optimization in one generation},
  author    = {Yang, Chenglin and Xie, Lingxi and Su, Chi and Yuille, Alan L},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {2859--2868},
  year      = {2019}
}

@inproceedings{zhu2018knowledge,
  title     = {Knowledge distillation by on-the-fly native ensemble},
  author    = {Zhu, Xiatian and Gong, Shaogang and others},
  booktitle = {Advances in neural information processing systems},
  pages     = {7517--7527},
  year      = {2018}
}

@article{mobahi2020self,
  title   = {Self-distillation amplifies regularization in hilbert space},
  author  = {Mobahi, Hossein and Farajtabar, Mehrdad and Bartlett, Peter L},
  journal = {arXiv preprint arXiv:2002.05715},
  year    = {2020}
}

@misc{dong2019distillation,
  title         = {Distillation $\approx$ Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network},
  author        = {Bin Dong and Jikai Hou and Yiping Lu and Zhihua Zhang},
  year          = {2019},
  eprint        = {1910.01255},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}

@article{stanforth2019labels,
  title   = {Are labels required for improving adversarial robustness?},
  author  = {Stanforth, Robert and Fawzi, Alhussein and Kohli, Pushmeet and others},
  journal = {arXiv preprint arXiv:1905.13725},
  year    = {2019}
}

@inproceedings{carmon2019unlabeled,
  title     = {Unlabeled data improves adversarial robustness},
  author    = {Carmon, Yair and Raghunathan, Aditi and Schmidt, Ludwig and Duchi, John C and Liang, Percy S},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {11190--11201},
  year      = {2019}
}

@inproceedings{najafi2019robustness,
  title     = {Robustness to adversarial perturbations in learning from incomplete data},
  author    = {Najafi, Amir and Maeda, Shin-ichi and Koyama, Masanori and Miyato, Takeru},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {5542--5552},
  year      = {2019}
}

@article{liu2020labels,
  title   = {Are Labels Necessary for Neural Architecture Search?},
  author  = {Liu, Chenxi and Doll{\'a}r, Piotr and He, Kaiming and Girshick, Ross and Yuille, Alan and Xie, Saining},
  journal = {arXiv preprint arXiv:2003.12056},
  year    = {2020}
}

@inproceedings{dvornik2019diversity,
  title     = {Diversity with cooperation: Ensemble methods for few-shot classification},
  author    = {Dvornik, Nikita and Schmid, Cordelia and Mairal, Julien},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  pages     = {3723--3731},
  year      = {2019}
}

@inproceedings{Shan2017learning,
  author    = {You, Shan and Xu, Chang and Xu, Chao and Tao, Dacheng},
  title     = {Learning from Multiple Teacher Networks},
  year      = {2017},
  isbn      = {9781450348874},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3097983.3098135},
  doi       = {10.1145/3097983.3098135},
  booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages     = {1285–1294},
  numpages  = {10},
  keywords  = {multiple teacher networks, deep learning, triplet loss, knowledge transfer},
  location  = {Halifax, NS, Canada},
  series    = {KDD ’17}
}

@inproceedings{Dvornik_2019_ICCV,
  author    = {Dvornik, Nikita and Schmid, Cordelia and Mairal, Julien},
  title     = {Diversity With Cooperation: Ensemble Methods for Few-Shot Classification},
  booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2019}
}

@article{anil2018large,
  title   = {Large scale distributed neural network training through online distillation},
  author  = {Anil, Rohan and Pereyra, Gabriel and Passos, Alexandre and Ormandi, Robert and Dahl, George E and Hinton, Geoffrey E},
  journal = {arXiv preprint arXiv:1804.03235},
  year    = {2018}
}

@article{park2019feed,
  title   = {FEED: Feature-level Ensemble for Knowledge Distillation},
  author  = {Park, SeongUk and Kwak, Nojun},
  journal = {arXiv preprint arXiv:1909.10754},
  year    = {2019}
}

@article{liu2019knowledge,
  title   = {Knowledge flow: Improve upon your teachers},
  author  = {Liu, Iou-Jen and Peng, Jian and Schwing, Alexander G},
  journal = {arXiv preprint arXiv:1904.05878},
  year    = {2019}
}

@inproceedings{wu2019personid,
  author    = {A. {Wu} and W. {Zheng} and X. {Guo} and J. {Lai}},
  booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Distilled Person Re-Identification: Towards a More Scalable System},
  year      = {2019}
}

@inproceedings{vongkulbhisal2019unifying,
  title     = {Unifying heterogeneous classifiers with distillation},
  author    = {Vongkulbhisal, Jayakorn and Vinayavekhin, Phongtharin and Visentini-Scarzanella, Marco},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {3175--3184},
  year      = {2019}
}

@inproceedings{brock2018large,
  title     = {Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},
  author    = {Andrew Brock and Jeff Donahue and Karen Simonyan},
  booktitle = {International Conference on Learning Representations},
  year      = {2019},
  url       = {https://openreview.net/forum?id=B1xsqj09Fm}
}

@inproceedings{zhang2015character,
  title     = {Character-level convolutional networks for text classification},
  author    = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  booktitle = {Advances in neural information processing systems},
  pages     = {649--657},
  year      = {2015}
}

@article{sanh2019distilbert,
  title   = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author  = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal = {arXiv preprint arXiv:1910.01108},
  year    = {2019}
}

@article{guha2019one,
  title   = {One-shot federated learning},
  author  = {Guha, Neel and Talwlkar, Ameet and Smith, Virginia},
  journal = {arXiv preprint arXiv:1902.11175},
  year    = {2019}
}

@inproceedings{socher-etal-2013-recursive,
  title     = {Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},
  author    = {Socher, Richard  and
	Perelygin, Alex  and
	Wu, Jean  and
	Chuang, Jason  and
	Manning, Christopher D.  and
	Ng, Andrew  and
	Potts, Christopher},
  booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  month     = oct,
  year      = {2013},
  address   = {Seattle, Washington, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/D13-1170},
  pages     = {1631--1642}
}

@article{krizhevsky2009learning,
  author    = {Krizhevsky, Alex and Hinton, Geoffrey},
  publisher = {Technical report, University of Toronto},
  title     = {Learning multiple layers of features from tiny images},
  year      = {2009}
}

@inproceedings{krizhevsky2012imagenet,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in neural information processing systems},
  pages     = {1097--1105},
  title     = {Imagenet classification with deep convolutional neural networks},
  year      = {2012}
}

@article{chrabaszcz2017downsampled,
  title   = {A downsampled variant of imagenet as an alternative to the cifar datasets},
  author  = {Chrabaszcz, Patryk and Loshchilov, Ilya and Hutter, Frank},
  journal = {arXiv preprint arXiv:1707.08819},
  year    = {2017}
}

@inproceedings{ma2018shufflenet,
  title     = {Shufflenet v2: Practical guidelines for efficient cnn architecture design},
  author    = {Ma, Ningning and Zhang, Xiangyu and Zheng, Hai-Tao and Sun, Jian},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  pages     = {116--131},
  year      = {2018}
}

@inproceedings{sandler2018mobilenetv2,
  title     = {Mobilenetv2: Inverted residuals and linear bottlenecks},
  author    = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {4510--4520},
  year      = {2018}
}

@article{simonyan2014very,
  title   = {Very deep convolutional networks for large-scale image recognition},
  author  = {Simonyan, Karen and Zisserman, Andrew},
  journal = {arXiv preprint arXiv:1409.1556},
  year    = {2014}
}

@inproceedings{he2016deep,
  title     = {Deep residual learning for image recognition},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {770--778},
  year      = {2016}
}

@article{hsieh2019non,
  title   = {The Non-IID Data Quagmire of Decentralized Machine Learning},
  author  = {Hsieh, Kevin and Phanishayee, Amar and Mutlu, Onur and Gibbons, Phillip B},
  journal = {arXiv preprint arXiv:1910.00189},
  year    = {2019}
}

@article{tan2019efficientnet,
  title   = {Efficientnet: Rethinking model scaling for convolutional neural networks},
  author  = {Tan, Mingxing and Le, Quoc V},
  journal = {arXiv preprint arXiv:1905.11946},
  year    = {2019}
}

@article{ioffe2015batch,
  title   = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author  = {Ioffe, Sergey and Szegedy, Christian},
  journal = {arXiv preprint arXiv:1502.03167},
  year    = {2015}
}

@inproceedings{huang2017densely,
  title     = {Densely connected convolutional networks},
  author    = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {4700--4708},
  year      = {2017}
}

@inproceedings{wu2018group,
  title     = {Group normalization},
  author    = {Wu, Yuxin and He, Kaiming},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  pages     = {3--19},
  year      = {2018}
}

@article{kuncheva2003measures,
  title     = {Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy},
  author    = {Kuncheva, Ludmila I and Whitaker, Christopher J},
  journal   = {Machine learning},
  volume    = {51},
  number    = {2},
  pages     = {181--207},
  year      = {2003},
  publisher = {Springer}
}

@inproceedings{sollich1996learning,
  title     = {Learning with ensembles: How overfitting can be useful},
  author    = {Sollich, Peter and Krogh, Anders},
  booktitle = {Advances in neural information processing systems},
  pages     = {190--196},
  year      = {1996}
}

@article{devlin2018bert,
  title   = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal = {arXiv preprint arXiv:1810.04805},
  year    = {2018}
}

@inproceedings{wang2018glue,
  title     = {{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author    = {Wang, Alex  and
Singh, Amanpreet  and
Michael, Julian  and
Hill, Felix  and
Levy, Omer  and
Bowman, Samuel},
  booktitle = {Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}},
  month     = nov,
  year      = {2018},
  address   = {Brussels, Belgium},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/W18-5446},
  doi       = {10.18653/v1/W18-5446},
  pages     = {353--355},
  abstract  = {Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.}
}

@inproceedings{wang2019superglue,
  title     = {Superglue: A stickier benchmark for general-purpose language understanding systems},
  author    = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {3261--3275},
  year      = {2019}
}

@article{geyer2017differentially,
  title   = {Differentially private federated learning: A client level perspective},
  author  = {Geyer, Robin C and Klein, Tassilo and Nabi, Moin},
  journal = {arXiv preprint arXiv:1712.07557},
  year    = {2017}
}

@article{jeong2018communication,
  title   = {Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data},
  author  = {Jeong, Eunjeong and Oh, Seungeun and Kim, Hyesung and Park, Jihong and Bennis, Mehdi and Kim, Seong-Lyun},
  journal = {arXiv preprint arXiv:1811.11479},
  year    = {2018}
}

@inproceedings{rastegari2016xnor,
  title        = {Xnor-net: Imagenet classification using binary convolutional neural networks},
  author       = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  booktitle    = {European conference on computer vision},
  pages        = {525--542},
  year         = {2016},
  organization = {Springer}
}


@article{hubara2017quantized,
  title     = {Quantized neural networks: Training neural networks with low precision weights and activations},
  author    = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal   = {The Journal of Machine Learning Research},
  volume    = {18},
  number    = {1},
  pages     = {6869--6898},
  year      = {2017},
  publisher = {JMLR. org}
}

# FD related, not cited yet
@article{Ahn_2019,
  title     = {Wireless Federated Distillation for Distributed Edge Learning with Heterogeneous Data},
  isbn      = {9781538681107},
  url       = {http://dx.doi.org/10.1109/PIMRC.2019.8904164},
  doi       = {10.1109/pimrc.2019.8904164},
  journal   = {2019 IEEE 30th Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)},
  publisher = {IEEE},
  author    = {Ahn, Jin-Hyun and Simeone, Osvaldo and Kang, Joonhyuk},
  year      = {2019},
  month     = {Sep}
}
@inproceedings{lin2020dont,
  title     = {Don't Use Large Mini-batches, Use Local {SGD}},
  author    = {Tao Lin and Sebastian U. Stich and Kumar Kshitij Patel and Martin Jaggi},
  booktitle = {ICLR - International Conference on Learning Representations},
  year      = {2020},
  url       = {https://openreview.net/forum?id=B1eyO1BFPr}
}

@article{chang2019cronus,
  title   = {Cronus: Robust and Heterogeneous Collaborative Learning with Black-Box Knowledge Transfer},
  author  = {Chang, Hongyan and Shejwalkar, Virat and Shokri, Reza and Houmansadr, Amir},
  journal = {arXiv preprint arXiv:1912.11279},
  year    = {2019}
}


@inproceedings{goodfellow2014generative,
  title     = {Generative adversarial nets},
  author    = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle = {Advances in neural information processing systems},
  pages     = {2672--2680},
  year      = {2014}
}


@inproceedings{arjovsky2017wasserstein,
  title     = {{W}asserstein Generative Adversarial Networks},
  author    = {Martin Arjovsky and Soumith Chintala and L{\'e}on Bottou},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  pages     = {214--223},
  year      = {2017},
  editor    = {Doina Precup and Yee Whye Teh},
  volume    = {70},
  series    = {Proceedings of Machine Learning Research},
  address   = {International Convention Centre, Sydney, Australia},
  month     = {06--11 Aug},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf},
  url       = {http://proceedings.mlr.press/v70/arjovsky17a.html},
  abstract  = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.}
}

@misc{bengio2013estimating,
  title         = {Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation},
  author        = {Yoshua Bengio and Nicholas Léonard and Aaron Courville},
  year          = {2013},
  eprint        = {1308.3432},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{hintonestimator,
  title     = {Neural networks for machine learning},
  author    = {Geoffrey Hinton},
  year      = {2012},
  booktitle = {Coursera, video lectures}
}

@inproceedings{hubara2016binarized,
  title     = {Binarized neural networks},
  author    = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  booktitle = {Advances in neural information processing systems},
  pages     = {4107--4115},
  year      = {2016}
}

@inproceedings{Lin2020Dynamic,
  title     = {Dynamic Model Pruning with Feedback},
  author    = {Tao Lin and Sebastian U. Stich and Luis Barba and Daniil Dmitriev and Martin Jaggi},
  booktitle = {International Conference on Learning Representations},
  year      = {2020},
  url       = {https://openreview.net/forum?id=SJem8lSFwB}
}

@article{zhou2020distilled,
  title   = {Distilled One-Shot Federated Learning},
  author  = {Yanlin Zhou and George Pu and Xiyao Ma and Xiaolin Li and Dapeng Wu},
  journal = {2009.07999},
  year    = {2020}
}

@article{shin2020xor,
  title   = {XOR Mixup: Privacy-Preserving Data Augmentation for One-Shot Federated Learning},
  author  = {Shin, MyungJae and Hwang, Chihoon and Kim, Joongheon and Park, Jihong and Bennis, Mehdi and Kim, Seong-Lyun},
  journal = {arXiv preprint arXiv:2006.05148},
  year    = {2020}
}

@article{chen2020feddistill,
  title   = {FedDistill: Making Bayesian Model Ensemble Applicable to Federated Learning},
  author  = {Chen, Hong-You and Chao, Wei-Lun},
  journal = {arXiv preprint arXiv:2009.01974},
  year    = {2020}
}

@article{sun2020federated,
  title   = {Federated Model Distillation with Noise-Free Differential Privacy},
  author  = {Sun, Lichao and Lyu, Lingjuan},
  journal = {arXiv preprint arXiv:2009.05537},
  year    = {2020}
}

@inproceedings{izmailov2018averaging,
  title     = {Averaging weights leads to wider optima and better generalization},
  author    = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  booktitle = {Appears at the Conference on Uncertainty in Artificial Intelligence (UAI)},
  year      = {2018}
}

@inproceedings{maddox2019simple,
  title     = {A simple baseline for bayesian uncertainty in deep learning},
  author    = {Maddox, Wesley J and Izmailov, Pavel and Garipov, Timur and Vetrov, Dmitry P and Wilson, Andrew Gordon},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {13153--13164},
  year      = {2019}
}
