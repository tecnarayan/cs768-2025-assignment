\begin{thebibliography}{}

\bibitem[Cao, 1993]{cao1993bootstrapping}
Cao, R. (1993).
\newblock Bootstrapping the mean integrated squared error.
\newblock {\em Journal of Multivariate Analysis}, 45(1):137--160.

\bibitem[Chang et~al., 2022]{chang2022learning}
Chang, J., Wang, K., Kallus, N., and Sun, W. (2022).
\newblock Learning bellman complete representations for offline policy
  evaluation.
\newblock In {\em International Conference on Machine Learning}, pages
  2938--2971. PMLR.

\bibitem[Chen, 2017a]{yenchi}
Chen, Y.-C. (2017a).
\newblock Introduction to resampling methods. lecture 5: Bootstrap.
\newblock
  \url{https://faculty.washington.edu/yenchic/17Sp_403/Lec5-bootstrap.pdf}.

\bibitem[Chen, 2017b]{yenchivar}
Chen, Y.-C. (2017b).
\newblock Introduction to resampling methods. lecture 9: Introduction to the
  bootstrap theory.
\newblock
  \url{https://faculty.washington.edu/yenchic/17Sp_403/Lec9_theory.pdf}.

\bibitem[Chernozhukov et~al., 2016]{chernozhukov2016double}
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey,
  W., and Robins, J. (2016).
\newblock Double/debiased machine learning for treatment and causal parameters.
\newblock {\em arXiv preprint arXiv:1608.00060}.

\bibitem[Danielsson et~al., 2001]{danielsson2001using}
Danielsson, J., de~Haan, L., Peng, L., and de~Vries, C.~G. (2001).
\newblock Using a bootstrap method to choose the sample fraction in tail index
  estimation.
\newblock {\em Journal of Multivariate analysis}, 76(2):226--248.

\bibitem[Delaigle and Gijbels, 2004]{delaigle2004bootstrap}
Delaigle, A. and Gijbels, I. (2004).
\newblock Bootstrap bandwidth selection in kernel density estimation from a
  contaminated sample.
\newblock {\em Annals of the Institute of Statistical Mathematics},
  56(1):19--47.

\bibitem[Diamond and Boyd, 2016]{diamond2016cvxpy}
Diamond, S. and Boyd, S. (2016).
\newblock Cvxpy: A python-embedded modeling language for convex optimization.
\newblock {\em The Journal of Machine Learning Research}, 17(1):2909--2913.

\bibitem[Dong et~al., 2023]{dong2023model}
Dong, K., Flet-Berliac, Y., Nie, A., and Brunskill, E. (2023).
\newblock Model-based offline reinforcement learning with local
  misspecification.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~37, pages 7423--7431.

\bibitem[dos Santos and Franco, 2019]{dos2019bootstrap}
dos Santos, T.~R. and Franco, G.~C. (2019).
\newblock Bootstrap for correcting the mean square error of prediction and
  smoothed estimates in structural models.
\newblock {\em Brazilian Journal of Probability and Statistics}.

\bibitem[Efron, 1990]{efron1990more}
Efron, B. (1990).
\newblock More efficient bootstrap computations.
\newblock {\em Journal of the American Statistical Association},
  85(409):79--89.

\bibitem[Efron, 1992]{efron1992bootstrap}
Efron, B. (1992).
\newblock {\em Bootstrap methods: another look at the jackknife}.
\newblock Springer.

\bibitem[Efron and Tibshirani, 1994]{efron1994introduction}
Efron, B. and Tibshirani, R.~J. (1994).
\newblock {\em An introduction to the bootstrap}.
\newblock CRC press.

\bibitem[Farajtabar et~al., 2018]{farajtabar2018more}
Farajtabar, M., Chow, Y., and Ghavamzadeh, M. (2018).
\newblock More robust doubly robust off-policy evaluation.
\newblock In {\em International Conference on Machine Learning}, pages
  1447--1456. PMLR.

\bibitem[Fu et~al., 2020]{fu2020d4rl}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. (2020).
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock {\em arXiv preprint arXiv:2004.07219}.

\bibitem[Fu et~al., 2021]{fu2021benchmarks}
Fu, J., Norouzi, M., Nachum, O., Tucker, G., Wang, Z., Novikov, A., Yang, M.,
  Zhang, M.~R., Chen, Y., Kumar, A., et~al. (2021).
\newblock Benchmarks for deep off-policy evaluation.
\newblock {\em arXiv preprint arXiv:2103.16596}.

\bibitem[Fujimoto et~al., 2018]{fujimoto2018addressing}
Fujimoto, S., Hoof, H., and Meger, D. (2018).
\newblock Addressing function approximation error in actor-critic methods.
\newblock In {\em International conference on machine learning}, pages
  1587--1596. PMLR.

\bibitem[Gamero et~al., 1998]{gamero1998bootstrapping}
Gamero, M.~J., Garc{\'\i}a, J.~M., and Reyes, A.~M. (1998).
\newblock Bootstrapping statistical functionals.
\newblock {\em Statistics \& probability letters}, 39(3):229--236.

\bibitem[Gao et~al., 2023]{gao2023variational}
Gao, Q., Gao, G., Chi, M., and Pajic, M. (2023).
\newblock Variational latent branching model for off-policy evaluation.
\newblock {\em arXiv preprint arXiv:2301.12056}.

\bibitem[Ghosh et~al., 1984]{ghosh1984note}
Ghosh, M., Parr, W.~C., Singh, K., and Babu, G.~J. (1984).
\newblock A note on bootstrapping the sample median.
\newblock {\em The Annals of Statistics}, 12(3):1130--1135.

\bibitem[Gottesman et~al., 2019]{gottesman2019combining}
Gottesman, O., Liu, Y., Sussex, S., Brunskill, E., and Doshi-Velez, F. (2019).
\newblock Combining parametric and nonparametric models for off-policy
  evaluation.
\newblock In {\em International Conference on Machine Learning}, pages
  2366--2375. PMLR.

\bibitem[Hall, 1990]{hall1990using}
Hall, P. (1990).
\newblock Using the bootstrap to estimate mean squared error and select
  smoothing parameter in nonparametric problems.
\newblock {\em Journal of multivariate analysis}, 32(2):177--203.

\bibitem[Hong, 1999]{hong}
Hong, H. (1999).
\newblock Lecture 11: Bootstrap.

\bibitem[Jiang and Li, 2016]{jiang2016doubly}
Jiang, N. and Li, L. (2016).
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  652--661. PMLR.

\bibitem[Kallus and Zhou, 2020]{kallus2020confounding}
Kallus, N. and Zhou, A. (2020).
\newblock Confounding-robust policy evaluation in infinite-horizon
  reinforcement learning.
\newblock {\em Advances in neural information processing systems},
  33:22293--22304.

\bibitem[Komorowski et~al., 2018]{komorowski2018artificial}
Komorowski, M., Celi, L.~A., Badawi, O., Gordon, A.~C., and Faisal, A.~A.
  (2018).
\newblock The artificial intelligence clinician learns optimal treatment
  strategies for sepsis in intensive care.
\newblock {\em Nature medicine}, 24(11):1716--1720.

\bibitem[Kostrikov et~al., 2021]{kostrikov2021offline}
Kostrikov, I., Nair, A., and Levine, S. (2021).
\newblock Offline reinforcement learning with implicit q-learning.
\newblock {\em arXiv preprint arXiv:2110.06169}.

\bibitem[Kumar et~al., 2020]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S. (2020).
\newblock Conservative q-learning for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:1179--1191.

\bibitem[Le et~al., 2019]{le2019batch}
Le, H., Voloshin, C., and Yue, Y. (2019).
\newblock Batch policy learning under constraints.
\newblock In {\em International Conference on Machine Learning}, pages
  3703--3712. PMLR.

\bibitem[Lee et~al., 2022]{lee2022oracle}
Lee, J.~N., Tucker, G., Nachum, O., Dai, B., and Brunskill, E. (2022).
\newblock Oracle inequalities for model selection in offline reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:2211.02016}.

\bibitem[Li and Maddala, 1999]{li1999bootstrap}
Li, H. and Maddala, G. (1999).
\newblock Bootstrap variance estimation of nonlinear functions of parameters:
  an application to long-run elasticities of energy demand.
\newblock {\em Review of Economics and Statistics}, 81(4):728--733.

\bibitem[Liu et~al., 2018a]{liu2018breaking}
Liu, Q., Li, L., Tang, Z., and Zhou, D. (2018a).
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock {\em Advances in Neural Information Processing Systems}, 31.

\bibitem[Liu et~al., 2018b]{liu2018representation}
Liu, Y., Gottesman, O., Raghu, A., Komorowski, M., Faisal, A.~A., Doshi-Velez,
  F., and Brunskill, E. (2018b).
\newblock Representation balancing mdps for off-policy policy evaluation.
\newblock {\em Advances in neural information processing systems}, 31.

\bibitem[Luo et~al., 2024]{luo2024position}
Luo, Z., Pan, Y., Watkinson, P., and Zhu, T. (2024).
\newblock Position: reinforcement learning in dynamic treatment regimes needs
  critical reexamination.
\newblock {\em Journal of Machine Learning Research}.

\bibitem[Mandel et~al., 2014]{mandel2014offline}
Mandel, T., Liu, Y.-E., Levine, S., Brunskill, E., and Popovic, Z. (2014).
\newblock Offline policy evaluation across representations with applications to
  educational games.
\newblock In {\em AAMAS}, volume 1077.

\bibitem[Marchetti et~al., 2012]{marchettinon}
Marchetti, S., Tzavidis, N., and Pratesi, M. (2012).
\newblock Non-parametric bootstrap mean squared error estimation for m-quantile
  estimators of small area averages, quantiles and poverty indicators.
\newblock {\em Computational Statistics \& Data Analysis}, 56(10):2889--2902.

\bibitem[Mikusheva, 2013]{Mikusheva}
Mikusheva, A. (2013).
\newblock Time series analysis. lecture 9: Bootstrap.
\newblock
  \url{https://ocw.mit.edu/courses/14-384-time-series-analysis-fall-2013/resources/mit14_384f13_lec9/}.

\bibitem[Nachum et~al., 2019]{nachum2019dualdice}
Nachum, O., Chow, Y., Dai, B., and Li, L. (2019).
\newblock Dualdice: Behavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Nair and Jiang, 2021]{nair2021spectral}
Nair, Y. and Jiang, N. (2021).
\newblock A spectral approach to off-policy evaluation for pomdps.
\newblock {\em arXiv preprint arXiv:2109.10502}.

\bibitem[Namkoong et~al., 2020]{namkoong2020off}
Namkoong, H., Keramati, R., Yadlowsky, S., and Brunskill, E. (2020).
\newblock Off-policy policy evaluation for sequential decisions under
  unobserved confounding.
\newblock {\em Advances in Neural Information Processing Systems},
  33:18819--18831.

\bibitem[Nie and Wager, 2021]{nie2021quasi}
Nie, X. and Wager, S. (2021).
\newblock Quasi-oracle estimation of heterogeneous treatment effects.
\newblock {\em Biometrika}, 108(2):299--319.

\bibitem[Oberst and Sontag, 2019]{oberst2019counterfactual}
Oberst, M. and Sontag, D. (2019).
\newblock Counterfactual off-policy evaluation with gumbel-max structural
  causal models.
\newblock In {\em International Conference on Machine Learning}, pages
  4881--4890. PMLR.

\bibitem[P{\u{a}}duraru, 2007]{puaduraru2007planning}
P{\u{a}}duraru, C. (2007).
\newblock {\em Planning with approximate and learned models of markov decision
  processes}.
\newblock PhD thesis, University of Alberta.

\bibitem[Paduraru, 2013]{paduraru2013off}
Paduraru, C. (2013).
\newblock {\em Off-policy evaluation in Markov decision processes}.
\newblock PhD thesis, McGill University.

\bibitem[Precup, 2000]{precup2000eligibility}
Precup, D. (2000).
\newblock Eligibility traces for off-policy policy evaluation.
\newblock {\em Computer Science Department Faculty Publication Series},
  page~80.

\bibitem[Ruan et~al., 2024]{ruan2024reinforcement}
Ruan, S., Nie, A., Steenbergen, W., He, J., Zhang, J., Guo, M., Liu, Y.,
  Dang~Nguyen, K., Wang, C.~Y., Ying, R., Landay, J., and Brunskill, E. (2024).
\newblock Reinforcement learning tutor better supported lower performers in a
  math task.
\newblock {\em Machine Learning}, pages 1--26.

\bibitem[Shao, 1990]{shao1990bootstrap}
Shao, J. (1990).
\newblock Bootstrap estimation of the asymptotic variances of statistical
  functionals.
\newblock {\em Annals of the Institute of Statistical Mathematics},
  42:737--752.

\bibitem[Shi et~al., 2022]{shi2022minimax}
Shi, C., Uehara, M., Huang, J., and Jiang, N. (2022).
\newblock A minimax learning approach to off-policy evaluation in confounded
  partially observable markov decision processes.
\newblock In {\em International Conference on Machine Learning}, pages
  20057--20094. PMLR.

\bibitem[Shi, 2012]{shi}
Shi, X. (2012).
\newblock Econ 715. lecture 10: Bootstrap.
\newblock
  \url{https://www.ssc.wisc.edu/~xshi/econ715/Lecture_10_bootstrap.pdf}.

\bibitem[Su et~al., 2020]{su2020adaptive}
Su, Y., Srinath, P., and Krishnamurthy, A. (2020).
\newblock Adaptive estimator selection for off-policy evaluation.
\newblock In {\em International Conference on Machine Learning}, pages
  9196--9205. PMLR.

\bibitem[Tennenholtz et~al., 2020]{tennenholtz2020off}
Tennenholtz, G., Shalit, U., and Mannor, S. (2020).
\newblock Off-policy evaluation in partially observable environments.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 10276--10283.

\bibitem[Thomas and Brunskill, 2016]{thomas2016data}
Thomas, P. and Brunskill, E. (2016).
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In {\em International Conference on Machine Learning}, pages
  2139--2148. PMLR.

\bibitem[Thomas et~al., 2015]{thomas2015high}
Thomas, P., Theocharous, G., and Ghavamzadeh, M. (2015).
\newblock High confidence policy improvement.
\newblock In {\em International Conference on Machine Learning}, pages
  2380--2388. PMLR.

\bibitem[Tucker and Lee, 2021]{tuckerimproved}
Tucker, G. and Lee, J. (2021).
\newblock Improved estimator selection for off-policy evaluation.
\newblock {\em Workshop on Reinforcement Learning Theory at the 38th
  International Conference on Machine Learning}.

\bibitem[Voloshin et~al., 2019]{voloshin2019empirical}
Voloshin, C., Le, H.~M., Jiang, N., and Yue, Y. (2019).
\newblock Empirical study of off-policy policy evaluation for reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:1911.06854}.

\bibitem[Voloshin et~al., 2021]{voloshin2021empirical}
Voloshin, C., Le, H.~M., Jiang, N., and Yue, Y. (2021).
\newblock Empirical study of off-policy policy evaluation for reinforcement
  learning.
\newblock In {\em Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 1)}.

\bibitem[Wang et~al., 2017]{wang2017optimal}
Wang, Y.-X., Agarwal, A., and Dud{\i}k, M. (2017).
\newblock Optimal and adaptive off-policy evaluation in contextual bandits.
\newblock In {\em International Conference on Machine Learning}, pages
  3589--3597. PMLR.

\bibitem[Williams, 2010]{williams}
Williams, C.~J. (2010).
\newblock The bootstrap method for estimating mse.
\newblock \url{https://www.webpages.uidaho.edu/~chrisw/stat514/bootstrap1.pdf}.

\bibitem[Wolpert, 1992]{wolpert1992stacked}
Wolpert, D.~H. (1992).
\newblock Stacked generalization.
\newblock {\em Neural networks}, 5(2):241--259.

\bibitem[Xie and Jiang, 2021]{xie2021batch}
Xie, T. and Jiang, N. (2021).
\newblock Batch value-function approximation with only realizability.
\newblock In {\em International Conference on Machine Learning}, pages
  11404--11413. PMLR.

\bibitem[Yang et~al., 2020]{yang2020off}
Yang, M., Nachum, O., Dai, B., Li, L., and Schuurmans, D. (2020).
\newblock Off-policy evaluation via the regularized lagrangian.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6551--6561.

\bibitem[Yu et~al., 2020]{yu2020mopo}
Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J.~Y., Levine, S., Finn, C., and
  Ma, T. (2020).
\newblock Mopo: Model-based offline policy optimization.
\newblock {\em Advances in Neural Information Processing Systems},
  33:14129--14142.

\bibitem[Yuan et~al., 2021]{yuan2021sope}
Yuan, C., Chandak, Y., Giguere, S., Thomas, P.~S., and Niekum, S. (2021).
\newblock Sope: Spectrum of off-policy estimators.
\newblock {\em Advances in Neural Information Processing Systems},
  34:18958--18969.

\bibitem[Zhang and Bareinboim, 2021]{zhang2021non}
Zhang, J. and Bareinboim, E. (2021).
\newblock Non-parametric methods for partial identification of causal effects.
\newblock {\em Columbia CausalAI Laboratory Technical Report}.

\bibitem[Zhang and Jiang, 2021]{zhang2021towards}
Zhang, S. and Jiang, N. (2021).
\newblock Towards hyperparameter-free policy selection for offline
  reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\end{thebibliography}
