\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2021)Agarwal, Kakade, Lee, and Mahajan]{agarwal2021theory}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G.
\newblock On the theory of policy gradient methods: Optimality, approximation, and distribution shift.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 22:\penalty0 98:1--98:76, 2021.

\bibitem[Ahmed et~al.(2019)Ahmed, Roux, Norouzi, and Schuurmans]{ahmed2019understanding}
Ahmed, Z., Roux, N.~L., Norouzi, M., and Schuurmans, D.
\newblock Understanding the impact of entropy on policy optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\  151--160. {PMLR}, 2019.

\bibitem[Allgower \& Georg(1990)Allgower and Georg]{allgower1990numerical}
Allgower, E.~L. and Georg, K.
\newblock \emph{Numerical continuation methods - an introduction}, volume~13 of \emph{Springer series in computational mathematics}.
\newblock Springer, 1990.

\bibitem[Arjevani et~al.(2020)Arjevani, Carmon, Duchi, Foster, Sekhari, and Sridharan]{yossi2020second}
Arjevani, Y., Carmon, Y., Duchi, J.~C., Foster, D.~J., Sekhari, A., and Sridharan, K.
\newblock Second-order information in non-convex stochastic optimization: Power and limitations.
\newblock In \emph{Proceedings of the Annual Conference on Learning Theory (COLT)}, volume 125 of \emph{Proceedings of Machine Learning Research}, pp.\  242--299. {PMLR}, 2020.

\bibitem[Arjevani et~al.(2023)Arjevani, Carmon, Duchi, Foster, Srebro, and Woodworth]{yossi2023lower}
Arjevani, Y., Carmon, Y., Duchi, J.~C., Foster, D.~J., Srebro, N., and Woodworth, B.~E.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock \emph{Math. Program.}, 199\penalty0 (1):\penalty0 165--214, 2023.

\bibitem[Azar et~al.(2013)Azar, Munos, and Kappen]{azar2013minimax}
Azar, M.~G., Munos, R., and Kappen, H.~J.
\newblock Minimax {PAC} bounds on the sample complexity of reinforcement learning with a generative model.
\newblock \emph{Machine Learning}, 91\penalty0 (3):\penalty0 325--349, 2013.

\bibitem[Azizzadenesheli et~al.(2018)Azizzadenesheli, Yue, and Anandkumar]{azizzadenesheli2018policy}
Azizzadenesheli, K., Yue, Y., and Anandkumar, A.
\newblock Policy gradient in partially observable environments: Approximation and convergence.
\newblock \emph{arXiv preprint arXiv:1810.07900}, 2018.

\bibitem[Baxter \& Bartlett(2001)Baxter and Bartlett]{baxter2001infinite}
Baxter, J. and Bartlett, P.~L.
\newblock Infinite-horizon policy-gradient estimation.
\newblock \emph{Journal of Artificial Intelligence Research (JAIR)}, 15:\penalty0 319--350, 2001.

\bibitem[Bhandari \& Russo(2024)Bhandari and Russo]{bhandari2019global}
Bhandari, J. and Russo, D.
\newblock Global optimality guarantees for policy gradient methods.
\newblock \emph{Operations Research}, 2024.

\bibitem[Bolland et~al.(2023)Bolland, Louppe, and Ernst]{bolland2023policy}
Bolland, A., Louppe, G., and Ernst, D.
\newblock Policy gradient algorithms implicitly optimize by continuation.
\newblock \emph{Transactions on Machine Learning Research}, 2023.

\bibitem[Deisenroth \& Rasmussen(2011)Deisenroth and Rasmussen]{deisenroth2011pilco}
Deisenroth, M.~P. and Rasmussen, C.~E.
\newblock {PILCO:} {A} model-based and data-efficient approach to policy search.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\  465--472. Omnipress, 2011.

\bibitem[Deisenroth et~al.(2013)Deisenroth, Neumann, and Peters]{deisenroth2013survey}
Deisenroth, M.~P., Neumann, G., and Peters, J.
\newblock A survey on policy search for robotics.
\newblock \emph{Foundations and Trends in Robotics}, 2\penalty0 (1-2):\penalty0 1--142, 2013.

\bibitem[Ding et~al.(2022)Ding, Zhang, and Lavaei]{ding2022global}
Ding, Y., Zhang, J., and Lavaei, J.
\newblock On the global optimum convergence of momentum-based policy gradient.
\newblock In \emph{{International Conference on Artificial Intelligence and Statistics (AISTATS)}}, volume 151 of \emph{Proceedings of Machine Learning Research}, pp.\  1910--1934. {PMLR}, 2022.

\bibitem[Duan et~al.(2016)Duan, Chen, Houthooft, Schulman, and Abbeel]{duan2016benchmarking}
Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\  1329--1338. PMLR, 2016.

\bibitem[Fatkhullin et~al.(2023)Fatkhullin, Barakat, Kireeva, and He]{fatkhullin2023stochastic}
Fatkhullin, I., Barakat, A., Kireeva, A., and He, N.
\newblock Stochastic policy gradient methods: Improved sample complexity for fisher-non-degenerate policies.
\newblock In \emph{International Conference on Machine Learning (ICML)}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  9827--9869. {PMLR}, 2023.

\bibitem[Fazel et~al.(2018)Fazel, Ge, Kakade, and Mesbahi]{fazel2018global}
Fazel, M., Ge, R., Kakade, S.~M., and Mesbahi, M.
\newblock Global convergence of policy gradient methods for the linear quadratic regulator.
\newblock In \emph{International Conference on Machine Learning (ICML)}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\  1466--1475. {PMLR}, 2018.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and Meger]{fujimoto2018addressing}
Fujimoto, S., van Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning (ICML)}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\  1582--1591. {PMLR}, 2018.

\bibitem[Ghavamzadeh \& Engel(2006)Ghavamzadeh and Engel]{ghavamzadeh2006bayesian}
Ghavamzadeh, M. and Engel, Y.
\newblock Bayesian policy gradient algorithms.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 19, 2006.

\bibitem[Ghavamzadeh et~al.(2020)Ghavamzadeh, Lazaric, and Pirotta]{glp2020aaaitutorial}
Ghavamzadeh, M., Lazaric, A., and Pirotta, M.
\newblock Exploration in reinforcement learning.
\newblock Tutorial at AAAI'20, 2020.

\bibitem[Gravell et~al.(2020)Gravell, Esfahani, and Summers]{gravell2020learning}
Gravell, B., Esfahani, P.~M., and Summers, T.
\newblock Learning optimal controllers for linear systems with multiplicative noise via policy gradient.
\newblock \emph{IEEE Transactions on Automatic Control}, 66\penalty0 (11):\penalty0 5283--5298, 2020.

\bibitem[Kakade(2001)]{kakade2001natural}
Kakade, S.~M.
\newblock A natural policy gradient.
\newblock In \emph{{Advances in Neural Information Processing Systems (NeurIPS)}}, pp.\  1531--1538. {MIT} Press, 2001.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
Karimi, H., Nutini, J., and Schmidt, M.
\newblock Linear convergence of gradient and proximal-gradient methods under the polyak-{\l}ojasiewicz condition.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases: European Conference (ECML PKDD)}, pp.\  795--811. Springer, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kucera(1992)]{kucera1992optimal}
Kucera, V.
\newblock Optimal control: Linear quadratic methods: Brian d. o. anderson and john b. moore.
\newblock \emph{Autom.}, 28\penalty0 (5):\penalty0 1068--1069, 1992.

\bibitem[Kumar et~al.(2020)Kumar, Kalogerias, Pappas, and Ribeiro]{kumar2020zeroth}
Kumar, H., Kalogerias, D.~S., Pappas, G.~J., and Ribeiro, A.
\newblock Zeroth-order deterministic policy gradient.
\newblock \emph{arXiv preprint arXiv:2006.07314}, 2020.

\bibitem[Li et~al.(2021)Li, Wei, Chi, Gu, and Chen]{lisoftmax2021}
Li, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y.
\newblock Softmax policy gradient methods can take exponential time to converge.
\newblock In \emph{Proceedings of the Annual Conference on Learning Theory (COLT)}, volume 134 of \emph{Proceedings of Machine Learning Research}, pp.\  3107--3110. {PMLR}, 2021.

\bibitem[Likmeta et~al.(2020)Likmeta, Metelli, Tirinzoni, Giol, Restelli, and Romano]{likmeta2020combining}
Likmeta, A., Metelli, A.~M., Tirinzoni, A., Giol, R., Restelli, M., and Romano, D.
\newblock Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving.
\newblock \emph{Robotics and Autonomous Systems}, 131:\penalty0 103568, 2020.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa, Silver, and Wierstra]{lillicrap2016continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2016.

\bibitem[Liu et~al.(2020)Liu, Zhang, Basar, and Yin]{liu2020improved}
Liu, Y., Zhang, K., Basar, T., and Yin, W.
\newblock An improved analysis of (variance-reduced) policy gradient and natural policy gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Lojasiewicz(1963)]{lojasiewicz1963propriete}
Lojasiewicz, S.
\newblock Une propri{\'e}t{\'e} topologique des sous-ensembles analytiques r{\'e}els.
\newblock \emph{Les {\'e}quations aux d{\'e}riv{\'e}es partielles}, 117:\penalty0 87--89, 1963.

\bibitem[Masiha et~al.(2022)Masiha, Salehkaleybar, He, Kiyavash, and Thiran]{masiha2022stochastic}
Masiha, S., Salehkaleybar, S., He, N., Kiyavash, N., and Thiran, P.
\newblock Stochastic second-order methods improve best-known sample complexity of sgd for gradient-dominated functions.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 35:\penalty0 10862--10875, 2022.

\bibitem[Mei et~al.(2020)Mei, Xiao, Szepesv{\'{a}}ri, and Schuurmans]{mei2020global}
Mei, J., Xiao, C., Szepesv{\'{a}}ri, C., and Schuurmans, D.
\newblock On the global convergence rates of softmax policy gradient methods.
\newblock In \emph{International Conference on Machine Learning (ICML)}, volume 119 of \emph{Proceedings of Machine Learning Research}, pp.\  6820--6829. {PMLR}, 2020.

\bibitem[Metelli et~al.(2018)Metelli, Papini, Faccio, and Restelli]{metelli2018policy}
Metelli, A.~M., Papini, M., Faccio, F., and Restelli, M.
\newblock Policy optimization via importance sampling.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, pp.\  5447--5459, 2018.

\bibitem[Metelli et~al.(2020)Metelli, Papini, Montali, and Restelli]{MetelliPMR20}
Metelli, A.~M., Papini, M., Montali, N., and Restelli, M.
\newblock Importance sampling techniques for policy optimization.
\newblock \emph{J. Mach. Learn. Res.}, 21:\penalty0 141:1--141:75, 2020.

\bibitem[Metelli et~al.(2021)Metelli, Papini, D'Oro, and Restelli]{MetelliPDR21}
Metelli, A.~M., Papini, M., D'Oro, P., and Restelli, M.
\newblock Policy optimization as online learning with mediator feedback.
\newblock In \emph{{AAAI} Conference on Artificial Intelligence ({AAAI})}, pp.\  8958--8966. {AAAI} Press, 2021.

\bibitem[Papini et~al.(2018)Papini, Binaghi, Canonaco, Pirotta, and Restelli]{papini2018stochastic}
Papini, M., Binaghi, D., Canonaco, G., Pirotta, M., and Restelli, M.
\newblock Stochastic variance-reduced policy gradient.
\newblock In \emph{International Conference on Machine Learning (ICML)}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\  4023--4032. {PMLR}, 2018.

\bibitem[Papini et~al.(2020)Papini, Battistello, and Restelli]{papini2020balancing}
Papini, M., Battistello, A., and Restelli, M.
\newblock Balancing learning speed and stability in policy gradient via adaptive exploration.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics (AISTATS)}, volume 108 of \emph{Proceedings of Machine Learning Research}, pp.\  1188--1199. {PMLR}, 2020.

\bibitem[Papini et~al.(2022)Papini, Pirotta, and Restelli]{papini2022smoothing}
Papini, M., Pirotta, M., and Restelli, M.
\newblock Smoothing policies and safe policy gradients.
\newblock \emph{Machine Learning}, 111\penalty0 (11):\penalty0 4081--4137, 2022.

\bibitem[Peters \& Schaal(2006)Peters and Schaal]{peters2006policy}
Peters, J. and Schaal, S.
\newblock Policy gradient methods for robotics.
\newblock In \emph{IEEE/RSJ International Conference on Intelligent Robots and Systems}, pp.\  2219--2225. IEEE, 2006.

\bibitem[Peters \& Schaal(2008)Peters and Schaal]{peters2008reinforcement}
Peters, J. and Schaal, S.
\newblock Reinforcement learning of motor skills with policy gradients.
\newblock \emph{Neural Networks}, 21\penalty0 (4):\penalty0 682--697, 2008.

\bibitem[Peters et~al.(2005)Peters, Vijayakumar, and Schaal]{peters2005natural}
Peters, J., Vijayakumar, S., and Schaal, S.
\newblock Natural actor-critic.
\newblock In \emph{{European Conference on Machine Learning (ECML)}}, volume 3720 of \emph{Lecture Notes in Computer Science}, pp.\  280--291. Springer, 2005.

\bibitem[Pirotta et~al.(2015)Pirotta, Restelli, and Bascetta]{pirotta2015policy}
Pirotta, M., Restelli, M., and Bascetta, L.
\newblock Policy gradient in lipschitz markov decision processes.
\newblock \emph{Machine Learning}, 100:\penalty0 255--283, 2015.

\bibitem[Polyak et~al.(1963)]{polyak1963gradient}
Polyak, B.~T. et~al.
\newblock Gradient methods for minimizing functionals.
\newblock \emph{Zhurnal vychislitel’noi matematiki i matematicheskoi fiziki}, 3\penalty0 (4):\penalty0 643--653, 1963.

\bibitem[Puterman(1990)]{puterman1990markov}
Puterman, M.~L.
\newblock Markov decision processes.
\newblock \emph{Handbooks in operations research and management science}, 2:\penalty0 331--434, 1990.

\bibitem[Raffin et~al.(2021)Raffin, Hill, Gleave, Kanervisto, Ernestus, and Dormann]{stable-baselines3}
Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., and Dormann, N.
\newblock Stable-baselines3: Reliable reinforcement learning implementations.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 22\penalty0 (268):\penalty0 1--8, 2021.

\bibitem[Saleh et~al.(2022)Saleh, Ghaffari, Bretl, and West]{saleh2022truly}
Saleh, E., Ghaffari, S., Bretl, T., and West, M.
\newblock Truly deterministic policy optimization.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[Scherrer \& Geist(2014)Scherrer and Geist]{scherrer2014local}
Scherrer, B. and Geist, M.
\newblock Local policy search in a convex space and conservative policy iteration as boosted policy search.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases: European Conference (ECML PKDD)}, volume 8726 of \emph{Lecture Notes in Computer Science}, pp.\  35--50. Springer, 2014.

\bibitem[Schwefel(1993)]{schwefel1993evolution}
Schwefel, H.-P.~P.
\newblock \emph{Evolution and optimum seeking: the sixth generation}.
\newblock John Wiley \& Sons, Inc., 1993.

\bibitem[Sehnke et~al.(2010)Sehnke, Osendorfer, Rückstieß, Graves, Peters, and Schmidhuber]{SEHNKE2010551}
Sehnke, F., Osendorfer, C., Rückstieß, T., Graves, A., Peters, J., and Schmidhuber, J.
\newblock Parameter-exploring policy gradients.
\newblock \emph{Neural Networks}, 23\penalty0 (4):\penalty0 551--559, 2010.
\newblock The International Conference on Artificial Neural Networks (ICANN).

\bibitem[Shani et~al.(2020)Shani, Efroni, and Mannor]{shani2020adaptive}
Shani, L., Efroni, Y., and Mannor, S.
\newblock Adaptive trust region policy optimization: Global convergence and faster rates for regularized mdps.
\newblock In \emph{{AAAI}}, pp.\  5668--5675. {AAAI} Press, 2020.

\bibitem[Shen et~al.(2019)Shen, Ribeiro, Hassani, Qian, and Mi]{shen2019hessian}
Shen, Z., Ribeiro, A., Hassani, H., Qian, H., and Mi, C.
\newblock Hessian aided policy gradient.
\newblock In \emph{International Conference on Machine Learning (ICML)}, volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\  5729--5738. {PMLR}, 2019.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and Riedmiller]{silver2014deterministic}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\  387--395. PMLR, 2014.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and Mansour]{sutton1999policy}
Sutton, R.~S., McAllester, D., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function approximation.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 12, 1999.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{IEEE/RSJ international conference on intelligent robots and systems}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist reinforcement learning.
\newblock \emph{Machine learning}, 8:\penalty0 229--256, 1992.

\bibitem[Xiong et~al.(2022)Xiong, Xu, Zhao, Liang, and Zhang]{xiong2022deterministic}
Xiong, H., Xu, T., Zhao, L., Liang, Y., and Zhang, W.
\newblock Deterministic policy gradient: Convergence analysis.
\newblock In \emph{Uncertainty in Artificial Intelligence (UAI)}, pp.\  2159--2169. PMLR, 2022.

\bibitem[Xu et~al.(2019)Xu, Gao, and Gu]{xu2019improved}
Xu, P., Gao, F., and Gu, Q.
\newblock An improved convergence analysis of stochastic variance-reduced policy gradient.
\newblock In \emph{Uncertainty in Artificial Intelligence (UAI)}, volume 115 of \emph{Proceedings of Machine Learning Research}, pp.\  541--551. {AUAI} Press, 2019.

\bibitem[Xu et~al.(2020)Xu, Gao, and Gu]{xu2020sample}
Xu, P., Gao, F., and Gu, Q.
\newblock Sample efficient policy gradient methods with recursive variance reduction.
\newblock In \emph{International Conference on Learning Representations (ICLR)}. OpenReview.net, 2020.

\bibitem[Yuan et~al.(2022)Yuan, Gower, and Lazaric]{yuan2022general}
Yuan, R., Gower, R.~M., and Lazaric, A.
\newblock A general sample complexity analysis of vanilla policy gradient.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics (AISTATS)}, pp.\  3332--3380. PMLR, 2022.

\bibitem[Zhao et~al.(2011)Zhao, Hachiya, Niu, and Sugiyama]{zhao2011analysis}
Zhao, T., Hachiya, H., Niu, G., and Sugiyama, M.
\newblock Analysis and improvement of policy gradient estimation.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, pp.\  262--270, 2011.

\end{thebibliography}
