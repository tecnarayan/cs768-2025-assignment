@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{pfaff2020learning,
  title={Learning Mesh-Based Simulation with Graph Networks},
  author={Pfaff, Tobias and Fortunato, Meire and Sanchez-Gonzalez, Alvaro and Battaglia, Peter},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@book{hughes2012finite,
  title={The finite element method: linear static and dynamic finite element analysis},
  author={Hughes, Thomas JR},
  year={2012},
  publisher={Courier Corporation}
}

@book{leveque2002finite,
  title={Finite volume methods for hyperbolic problems},
  author={LeVeque, Randall J and others},
  volume={31},
  year={2002},
  publisher={Cambridge university press}
}

@book{strikwerda2004finite,
  title={Finite difference schemes and partial differential equations},
  author={Strikwerda, John C},
  year={2004},
  publisher={SIAM}
}

@book{trefethen1997numerical,
  title={Numerical linear algebra},
  author={Trefethen, Lloyd N and Bau III, David},
  volume={50},
  year={1997},
  publisher={Siam}
}

@book{golub2013matrix,
  title={Matrix computations},
  author={Golub, Gene H and Van Loan, Charles F},
  year={2013},
  publisher={JHU press}
}

@book{madenci2015finite,
  title={The finite element method and applications in engineering using ANSYS{\textregistered}},
  author={Madenci, Erdogan and Guven, Ibrahim},
  year={2015},
  publisher={Springer}
}

@book{helwany2007applied,
  title={Applied soil mechanics with ABAQUS applications},
  author={Helwany, Sam},
  year={2007},
  publisher={John Wiley \& Sons}
}

@book{pryor2009multiphysics,
  title={Multiphysics modeling using COMSOL{\textregistered}: a first principles approach},
  author={Pryor, Roger W},
  year={2009},
  publisher={Jones \& Bartlett Publishers}
}

@book{heath2018scientific,
  title={Scientific computing: an introductory survey, revised second edition},
  author={Heath, Michael T},
  year={2018},
  publisher={SIAM}
}

@inproceedings{sanchez2020learning,
  title={Learning to simulate complex physics with graph networks},
  author={Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter},
  booktitle={International Conference on Machine Learning},
  pages={8459--8468},
  year={2020},
  organization={PMLR}
}

@article{jones1995, 
author = {Jones, Mark T. and Plassmann, Paul E.},
title = {An Improved Incomplete {Cholesky} Factorization},
year = {1995},
issue_date = {March 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/200979.200981},
doi = {10.1145/200979.200981},
abstract = {Incomplete factorization has been shown to be a good preconditioner for the conjugate gradient method on a wide variety of problems. It is well known that allowing some fill-in during the incomplete factorization can significantly reduce the number of iterations needed for convergence. Allowing fill-in, however, increases the time for the factorization and for the triangular system solutions. Additionally, it is difficult to predict a priori how much fill-in to allow and how to allow it. The unpredictability of the required storage/work and the unknown benefits of the additional fill-in make such strategies impractical to use in many situations. In this article we motivate, and then present, two “black-box” strategies that significantly increase the effectiveness of incomplete {Cholesky} factorization as a preconditioner. These strategies require no parameters from the user and do not increase the cost of the triangular system solutions. Efficient implementations for these algorithms are described. These algorithms are shown to be successful for a variety of problems from the Harwell-Boeing sparse matrix collection.},
journal = {ACM Trans. Math. Softw.},
month = {mar},
pages = {5–17},
numpages = {13},
keywords = {sparse matrices, incomplete {Cholesky}, preconditioners, incomplete factorization}
}


@book{axelsson1996iterative,
  title={Iterative solution methods},
  author={Axelsson, Owe},
  year={1996},
  publisher={Cambridge university press}
}

@inproceedings{kim2019deep,
  title={Deep fluids: A generative network for parameterized fluid simulations},
  author={Kim, Byungsoo and Azevedo, Vinicius C and Thuerey, Nils and Kim, Theodore and Gross, Markus and Solenthaler, Barbara},
  booktitle={Computer graphics forum},
  volume={38},
  number={2},
  pages={59--70},
  year={2019},
  organization={Wiley Online Library}
}

@article{raissi2019physics,
  title={Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
  journal={Journal of Computational physics},
  volume={378},
  pages={686--707},
  year={2019},
  publisher={Elsevier}
}

@article{karniadakis2021physics,
  title={Physics-informed machine learning},
  author={Karniadakis, George Em and Kevrekidis, Ioannis G and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  journal={Nature Reviews Physics},
  volume={3},
  number={6},
  pages={422--440},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{um2020solver,
  title={Solver-in-the-loop: Learning from differentiable physics to interact with iterative {PDE}-solvers},
  author={Um, Kiwon and Brand, Robert and Fei, Yun Raymond and Holl, Philipp and Thuerey, Nils},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6111--6122},
  year={2020}
}

@article{brandstetter2022message,
  title={Message passing neural PDE solvers},
  author={Brandstetter, Johannes and Worrall, Daniel and Welling, Max},
  journal={arXiv preprint arXiv:2202.03376},
  year={2022}
}

@article{brandstetter2022lie,
  title={Lie Point Symmetry Data Augmentation for Neural PDE Solvers},
  author={Brandstetter, Johannes and Welling, Max and Worrall, Daniel E},
  journal={arXiv preprint arXiv:2202.07643},
  year={2022}
}

@article{li2020fourier,
  title={Fourier neural operator for parametric partial differential equations},
  author={Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2010.08895},
  year={2020}
}

@article{li2020multipole,
  title={Multipole graph neural operator for parametric partial differential equations},
  author={Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Stuart, Andrew and Bhattacharya, Kaushik and Anandkumar, Anima},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6755--6766},
  year={2020}
}

@inproceedings{belbute2020combining,
  title={Combining differentiable PDE solvers and graph neural networks for fluid flow prediction},
  author={Belbute-Peres, Filipe De Avila and Economon, Thomas and Kolter, Zico},
  booktitle={International Conference on Machine Learning},
  pages={2402--2411},
  year={2020},
  organization={PMLR}
}

@article{marquez2017imposing,
  title={Imposing hard constraints on deep networks: Promises and limitations},
  author={M{\'a}rquez-Neila, Pablo and Salzmann, Mathieu and Fua, Pascal},
  journal={arXiv preprint arXiv:1706.02025},
  year={2017}
}

@inproceedings{kim2021dpm,
  title={DPM: a novel training method for physics-informed neural networks in extrapolation},
  author={Kim, Jungeun and Lee, Kookjin and Lee, Dongeun and Jhin, Sheo Yon and Park, Noseong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={9},
  pages={8146--8154},
  year={2021}
}







@article{azulay_arxiv,
author = {Azulay, Yael and Treister, Eran},
title = {Multigrid-Augmented Deep Learning Preconditioners for the {Helmholtz} Equation},
journal = {SIAM Journal on Scientific Computing},
volume = {0},
number = {0},
pages = {S127-S151},
year = {2022},
doi = {10.1137/21M1433514},
URL = {
        https://doi.org/10.1137/21M1433514
},
eprint = { https://doi.org/10.1137/21M1433514}
,
    abstract = { In this paper, we present a data-driven approach to iteratively solve the discrete heterogeneous Helmholtz equation at high wavenumbers. In our approach, we combine classical iterative solvers with convolutional neural networks (CNNs) to form a preconditioner which is applied within a Krylov solver. For the preconditioner, we use a CNN of type U-Net that operates in conjunction with multigrid ingredients. Two types of preconditioners are proposed: (1) U-Net as a coarse grid solver and (2) U-Net as a deflation operator with shifted Laplacian V-cycles. Following our training scheme and data-augmentation, our CNN preconditioner can generalize over residuals and a relatively general set of wave slowness models. On top of that, we also offer an encoder-solver framework where an “encoder" network generalizes over the medium and sends context vectors to another “solver" network, which generalizes over the right-hand sides. We show that this option is more robust and efficient than the standalone variant. Last, we also offer a mini-retraining procedure, to improve the solver after the model is known. This option is beneficial when solving multiple right-hand sides, like in inverse problems. We demonstrate the efficiency and generalization abilities of our approach on a variety of two-dimensional problems. }
}


@article{khare2012sparse,
  title={Sparse matrix decompositions and graph characterizations},
  author={Khare, Kshitij and Rajaratnam, Bala},
  journal={Linear Algebra and its Applications},
  volume={437},
  number={3},
  pages={932--947},
  year={2012},
  publisher={Elsevier}
}

@article{Liu_book,
  title={The role of elimination trees in sparse factorization},
  author={Liu, Joseph WH},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={11},
  number={1},
  pages={134--172},
  year={1990},
  publisher={SIAM}
}

@article{davis1999modifying,
  title={Modifying a sparse {Cholesky} factorization},
  author={Davis, Timothy A and Hager, William W},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={20},
  number={3},
  pages={606--627},
  year={1999},
  publisher={SIAM}
}

@article{schafer2021sparse,
  title={Sparse {Cholesky} Factorization by Kullback--Leibler Minimization},
  author={Schäfer, Florian and Katzfuss, Matthias and Owhadi, Houman},
  journal={SIAM Journal on Scientific Computing},
  volume={43},
  number={3},
  pages={A2019--A2046},
  year={2021},
  publisher={SIAM}
}

@article{chen2021multiscale,
  title={Multiscale {Cholesky} preconditioning for ill-conditioned problems},
  author={Chen, Jiong and Sch{\"a}fer, Florian and Huang, Jin and Desbrun, Mathieu},
  journal={ACM Transactions on Graphics (TOG)},
  volume={40},
  number={4},
  pages={1--13},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{grote1997parallel,
  title={Parallel preconditioning with sparse approximate inverses},
  author={Grote, Marcus J and Huckle, Thomas},
  journal={SIAM Journal on Scientific Computing},
  volume={18},
  number={3},
  pages={838--853},
  year={1997},
  publisher={SIAM}
}

@book{briggs2000multigrid,
  title={A multigrid tutorial},
  author={Briggs, William L and Henson, Van Emden and McCormick, Steve F},
  year={2000},
  publisher={SIAM}
}

@incollection{smith1997domain,
  title={Domain decomposition methods for partial differential equations},
  author={Smith, Barry F},
  booktitle={Parallel Numerical Algorithms},
  pages={225--243},
  year={1997},
  publisher={Springer}
}

@book{holmes2012turbulence,
  title={Turbulence, coherent structures, dynamical systems and symmetry},
  author={Holmes, Philip and Lumley, John L and Berkooz, Gahl and Rowley, Clarence W},
  year={2012},
  publisher={Cambridge university press}
}

@article{li2018learning,
  title={Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids},
  author={Li, Yunzhu and Wu, Jiajun and Tedrake, Russ and Tenenbaum, Joshua B and Torralba, Antonio},
  journal={arXiv preprint arXiv:1810.01566},
  year={2018}
}

@book{johnson2012numerical,
  title={Numerical solution of partial differential equations by the finite element method},
  author={Johnson, Claes},
  year={2012},
  publisher={Courier Corporation}
}

@book{solomon2015numerical,
  title={Numerical algorithms: methods for computer vision, machine learning, and graphics},
  author={Solomon, Justin},
  year={2015},
  publisher={CRC press}
}

@misc{sappl_arxiv,
  doi = {10.48550/ARXIV.1906.06925},
  
  url = {https://arxiv.org/abs/1906.06925},
  
  author = {Sappl, Johannes and Seiler, Laurent and Harders, Matthias and Rauch, Wolfgang},
  
  keywords = {Machine Learning (cs.LG), Numerical Analysis (math.NA), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Deep Learning of Preconditioners for Conjugate Gradient Solvers in Urban Water Related Problems},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{akmann_arxiv,
  doi = {10.48550/ARXIV.2010.02866},
  
  url = {https://arxiv.org/abs/2010.02866},
  
  author = {Ackmann, Jan and Düben, Peter D. and Palmer, Tim N. and Smolarkiewicz, Piotr K.},
  
  keywords = {Atmospheric and Oceanic Physics (physics.ao-ph), Machine Learning (cs.LG), Numerical Analysis (math.NA), Computational Physics (physics.comp-ph), Fluid Dynamics (physics.flu-dyn), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Machine-Learned Preconditioners for Linear Solvers in Geophysical Fluid Flows},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{cali_arxiv,
  doi = {10.48550/ARXIV.2208.02728},
  
  url = {https://arxiv.org/abs/2208.02728},
  
  author = {Calì, Salvatore and Hackett, Daniel C. and Lin, Yin and Shanahan, Phiala E. and Xiao, Brian},
  
  keywords = {High Energy Physics - Lattice (hep-lat), Machine Learning (cs.LG), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neural-network preconditioners for solving the {Dirac} equation in lattice gauge theory},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{koolstra_arxiv,
  title={Learning a preconditioner to accelerate compressed sensing reconstructions in MRI},
  author={Koolstra, Kirsten and Remis, Rob},
  journal={Magnetic Resonance in Medicine},
  volume={87},
  number={4},
  pages={2063--2073},
  year={2022},
  publisher={Wiley Online Library}
}

@book{TrotMult2001,
  added-at = {2013-10-18T11:46:14.000+0200},
  address = {San Diego [u.a.]},
  author = {Trottenberg, Ulrich and Oosterlee, Cornelis W. and Schüller, Anton},
  biburl = {https://www.bibsonomy.org/bibtex/2971fa53df9f7dbf69f11ca60f5ea8846/mathek},
  description = {Mathematisches Institut},
  interhash = {f24f6ab6a62e68bb6136c8848bcfbc16},
  intrahash = {971fa53df9f7dbf69f11ca60f5ea8846},
  isbn = {0-12-701070-X},
  keywords = {mathematik multigrid numerische},
  note = {With contributions by A. Brandt, P. Oswald and K. Stüben},
  publisher = {Academic Press},
  series = {Texts in Applied Mathematics. Bd.},
  timestamp = {2013-10-18T12:32:08.000+0200},
  title = {Multigrid},
  volume = 33,
  year = 2001
}

@article{hessian_probing,
author = {Demanet, Laurent and Létourneau, Pierre-David and Boumal, Nicolas and Calandra, H. and Chiu, Jiawei and Snelson, Stanley},
year = {2012},
month = {03},
pages = {155-168},
title = {Matrix probing: A randomized preconditioner for the wave-equation Hessian},
volume = {32},
journal = {Applied and Computational Harmonic Analysis},
doi = {10.1016/j.acha.2011.03.006}
}

@inproceedings{luz2020amg,
  title={Learning algebraic multigrid using graph neural networks},
  author={Luz, Ilay and Galun, Meirav and Maron, Haggai and Basri, Ronen and Yavneh, Irad},
  booktitle={International Conference on Machine Learning},
  pages={6489--6499},
  year={2020},
  organization={PMLR}
}

@article{wang2019backpropagation,
  title={Backpropagation-friendly eigendecomposition},
  author={Wang, Wei and Dang, Zheng and Hu, Yinlin and Fua, Pascal and Salzmann, Mathieu},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{Gould2001OnTS,
  title={On the Solution of Equality Constrained Quadratic Programming Problems Arising in Optimization},
  author={Nicholas I. M. Gould and Mary E. Hribar and Jorge Nocedal},
  journal={SIAM J. Sci. Comput.},
  year={2001},
  volume={23},
  pages={1376-1395}
}


@article{chen_2021_icsiggraph,
author = {Chen, Jiong and Sch\"{a}fer, Florian and Huang, Jin and Desbrun, Mathieu},
title = {Multiscale {Cholesky} Preconditioning for Ill-Conditioned Problems},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3450626.3459851},
doi = {10.1145/3450626.3459851},
abstract = {Many computer graphics applications boil down to solving sparse systems of linear equations. While the current arsenal of numerical solvers available in various specialized libraries and for different computer architectures often allow efficient and scalable solutions to image processing, modeling and simulation applications, an increasing number of graphics problems face large-scale and ill-conditioned sparse linear systems --- a numerical challenge which typically chokes both direct factorizations (due to high memory requirements) and iterative solvers (because of slow convergence). We propose a novel approach to the efficient preconditioning of such problems which often emerge from the discretization over unstructured meshes of partial differential equations with heterogeneous and anisotropic coefficients. Our numerical approach consists in simply performing a fine-to-coarse ordering and a multiscale sparsity pattern of the degrees of freedom, using which we apply an incomplete {Cholesky} factorization. By further leveraging supernodes for cache coherence, graph coloring to improve parallelism and partial diagonal shifting to remedy negative pivots, we obtain a preconditioner which, combined with a conjugate gradient solver, far exceeds the performance of existing carefully-engineered libraries for graphics problems involving bad mesh elements and/or high contrast of coefficients. We also back the core concepts behind our simple solver with theoretical foundations linking the recent method of operator-adapted wavelets used in numerical homogenization to the traditional {Cholesky} factorization of a matrix, providing us with a clear bridge between incomplete {Cholesky} factorization and multiscale analysis that we leverage numerically.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {81},
numpages = {13},
keywords = {numerical solvers, incomplete {Cholesky} decomposition, wavelets, preconditioned conjugate gradient}
}


@inproceedings{gotz_arxiv_cnnprecond,
  title={Machine learning-aided numerical linear algebra: Convolutional neural networks for the efficient preconditioner generation},
  author={G{\"o}tz, Markus and Anzt, Hartwig},
  booktitle={2018 IEEE/ACM 9th Workshop on Latest Advances in Scalable Algorithms for Large-Scale Systems (scalA)},
  pages={49--56},
  year={2018},
  organization={IEEE}
}
  
  
  


@article{davis_sparse_cholesky,
author = {Davis, Timothy A. and Hager, William W.},
title = {Modifying a Sparse {Cholesky} Factorization},
journal = {SIAM Journal on Matrix Analysis and Applications},
volume = {20},
number = {3},
pages = {606-627},
year = {1999},
doi = {10.1137/S0895479897321076},
    abstract = { Given a sparse symmetric positive definite matrix \${\bf AA}^{\sf T}\$ and an associated sparse {Cholesky} factorization \${\bf LDL}^{\sf T}\$ or \${\bf LL}^{\sf T}\$, we develop sparse techniques for obtaining the new factorization associated with either adding a column to \${\bf A}\$ or deleting a column from \${\bf A}\$. Our techniques are based on an analysis and manipulation of the underlying graph structure and on ideas of Gill et al.\ [ Math. Comp., 28 (1974), pp. 505--535] for modifying a dense {Cholesky} factorization. We show that our methods extend to the general case where an arbitrary sparse symmetric positive definite matrix is modified. Our methods are optimal in the sense that they take time proportional to the number of nonzero entries in \${\bf L}\$ and \${\bf D}\$ that change. }
}



@article{LIU198973,
title = {Reordering sparse matrices for parallel elimination},
journal = {Parallel Computing},
volume = {11},
number = {1},
pages = {73-91},
year = {1989},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(89)90064-1},
url = {https://www.sciencedirect.com/science/article/pii/0167819189900641},
author = {Joseph W.H Liu},
keywords = {Sparse matrix, parallel elimination, reordering, elimination tree, height, rotation},
abstract = {We consider the problem of finding equivalent reorderings of a sparse matrix so that the reordered matrix is suitable for parallel Gaussian elimination. The elimination tree structure is used as our parallel model. We show that the reordering scheme by Jess and Kees generates an elimination tree with minimum height among all such trees from the class of equivalent reorderings. A new height-reducing algorithm based on elimination tree rotation is also introduced. Experimental results are provided to compare these two approaches. The new reordering algorithm using rotation is shown to produce trees with minimum or near-minimum height. Yet, it requires significantly less reordering time.}
}




@article{brandhorst2011fast,
  title={Fast sparse {Cholesky} decomposition and inversion using nested dissection matrix reordering},
  author={Brandhorst, Kai and Head-Gordon, Martin},
  journal={Journal of chemical theory and computation},
  volume={7},
  number={2},
  pages={351--368},
  year={2011},
  publisher={ACS Publications}
}


@article{chen2021bals,
  title={{BALS}: Blocked alternating least squares for parallel sparse matrix factorization on {GPUs}},
  author={Chen, Jing and Fang, Jianbin and Liu, Weifeng and Yang, Canqun},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={32},
  number={9},
  pages={2291--2302},
  year={2021},
  publisher={IEEE}
}


@article{liew2007computational,
  title={A computational approach for predicting the hydroelasticity of flexible structures based on the pressure Poisson equation},
  author={Liew, Kim Meow and Wang, WQ and Zhang, LX and He, XQ},
  journal={International Journal for Numerical Methods in Engineering},
  volume={72},
  number={13},
  pages={1560--1583},
  year={2007},
  publisher={Wiley Online Library}
}


@article{SAAD200199_multilevel,
title = {Enhanced multi-level block ILU preconditioning strategies for general sparse linear systems},
journal = {Journal of Computational and Applied Mathematics},
volume = {130},
number = {1},
pages = {99-118},
year = {2001},
issn = {0377-0427},
author = {Yousef Saad and Jun Zhang},
keywords = {Incomplete LU factorization, Multi-level ILU preconditioner, Krylov subspace methods, Multi-elimination ILU factorization, Algebraic multigrid method},
abstract = {This paper introduces several strategies to deal with pivot blocks in multi-level block incomplete LU factorization (BILUM) preconditioning techniques. These techniques are aimed at increasing the robustness and controlling the amount of fill-ins of BILUM for solving large sparse linear systems when large-size blocks are used to form block-independent set. Techniques proposed in this paper include double-dropping strategies, approximate singular-value decomposition, variable size blocks and use of an arrowhead block submatrix. We point out the advantages and disadvantages of these strategies and discuss their efficient implementations. Numerical experiments are conducted to show the usefulness of the new techniques in dealing with hard-to-solve problems arising from computational fluid dynamics. In addition, we discuss the relation between multi-level ILU preconditioning methods and algebraic multi-level methods.}
}



@article{buranay2019approximate,
  title={Approximate {Schur}-block {ILU} preconditioners for regularized solution of discrete ill-posed problems},
  author={Buranay, Suzan C and Iyikal, Ovgu C},
  journal={Mathematical Problems in Engineering},
  volume={2019},
  year={2019},
  publisher={Hindawi}
}

@article{Demidov2019,
    author="Demidov, D.",
    title="{AMGCL}: An Efficient, Flexible, and Extensible Algebraic Multigrid Implementation",
    journal="Lobachevskii Journal of Mathematics",
    year="2019",
    month="May",
    day="01",
    volume="40",
    number="5",
    pages="535--546",
    issn="1818-9962",
    doi="10.1134/S1995080219050056",
    url="https://doi.org/10.1134/S1995080219050056"
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
}

@inproceedings{pytorch-geometric,
  title={Fast Graph Representation Learning with {PyTorch Geometric}},
  author={Fey, Matthias and Lenssen, Jan E.},
  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},
  year={2019},
}


@book{nocedal1999numerical,
  title={Numerical optimization},
  author={Nocedal, Jorge and Wright, Stephen J},
  year={1999},
  publisher={Springer}
}