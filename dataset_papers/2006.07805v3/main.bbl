\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{\k{e}}bski, Ballas, Krueger, Bengio,
  Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
Devansh Arpit, Stanis{\l}aw Jastrz{\k{e}}bski, Nicolas Ballas, David Krueger,
  Emmanuel Bengio, Maxinder~S Kanwal, Tegan Maharaj, Asja Fischer, Aaron
  Courville, Yoshua Bengio, et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{ICML}, pages 233--242. JMLR. org, 2017.

\bibitem[Bao et~al.(2018)Bao, Niu, and Sugiyama]{bao2018classification}
Han Bao, Gang Niu, and Masashi Sugiyama.
\newblock Classification from pairwise similarity and unlabeled data.
\newblock In \emph{ICML}, pages 452--461, 2018.

\bibitem[Boucheron et~al.(2013)Boucheron, Lugosi, and
  Massart]{boucheron2013concentration}
St{\'e}phane Boucheron, G{\'a}bor Lugosi, and Pascal Massart.
\newblock \emph{Concentration inequalities: A nonasymptotic theory of
  independence}.
\newblock Oxford university press, 2013.

\bibitem[Cheng et~al.(2020)Cheng, Liu, Ramamohanarao, and
  Tao]{cheng2017learning}
Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, and Dacheng Tao.
\newblock Learning with bounded instance-and label-dependent label noise.
\newblock In \emph{ICML}, 2020.

\bibitem[Csisz{\'a}r et~al.(2004)Csisz{\'a}r, Shields,
  et~al.]{csiszar2004information}
Imre Csisz{\'a}r, Paul~C Shields, et~al.
\newblock Information theory and statistics: A tutorial.
\newblock \emph{Foundations and Trends{\textregistered} in Communications and
  Information Theory}, 1\penalty0 (4):\penalty0 417--528, 2004.

\bibitem[Daniely and Granot(2019)]{daniely2019generalization}
Amit Daniely and Elad Granot.
\newblock Generalization bounds for neural networks via approximate description
  length.
\newblock In \emph{NeurIPS}, pages 12988--12996, 2019.

\bibitem[Elkan and Noto(2008)]{elkan2008learning}
Charles Elkan and Keith Noto.
\newblock Learning classifiers from only positive and unlabeled data.
\newblock In \emph{SIGKDD}, pages 213--220, 2008.

\bibitem[Goldberger and Ben-Reuven(2017)]{goldberger2016training}
Jacob Goldberger and Ehud Ben-Reuven.
\newblock Training deep neural-networks using a noise adaptation layer.
\newblock In \emph{ICLR}, 2017.

\bibitem[Guo et~al.(2018)Guo, Huang, Zhang, Zhuang, Dong, Scott, and
  Huang]{guo2018curriculumnet}
Sheng Guo, Weilin Huang, Haozhi Zhang, Chenfan Zhuang, Dengke Dong, Matthew~R
  Scott, and Dinglong Huang.
\newblock Curriculumnet: Weakly supervised learning from large-scale web
  images.
\newblock In \emph{ECCV}, pages 135--150, 2018.

\bibitem[Han et~al.(2018{\natexlab{a}})Han, Yao, Niu, Zhou, Tsang, Zhang, and
  Sugiyama]{han2018masking}
Bo~Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, Ivor Tsang, Ya~Zhang, and
  Masashi Sugiyama.
\newblock Masking: A new perspective of noisy supervision.
\newblock In \emph{NeurIPS}, pages 5836--5846, 2018{\natexlab{a}}.

\bibitem[Han et~al.(2018{\natexlab{b}})Han, Yao, Yu, Niu, Xu, Hu, Tsang, and
  Sugiyama]{han2018co}
Bo~Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and
  Masashi Sugiyama.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock In \emph{NeurIPS}, pages 8527--8537, 2018{\natexlab{b}}.

\bibitem[Han et~al.(2020)Han, Niu, Yu, Yao, Xu, Tsang, and
  Sugiyama]{han2020sigua}
Bo~Han, Gang Niu, Xingrui Yu, Quanming Yao, Miao Xu, Ivor~W Tsang, and Masashi
  Sugiyama.
\newblock Sigua: Forgetting may make learning with noisy labels more robust.
\newblock ICML, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, pages 770--778, 2016.

\bibitem[Jiang et~al.(2018)Jiang, Zhou, Leung, Li, and
  Fei-Fei]{jiang2018mentornet}
Lu~Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li~Fei-Fei.
\newblock {MentorNet}: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock In \emph{ICML}, pages 2309--2318, 2018.

\bibitem[Kremer et~al.(2018)Kremer, Sha, and Igel]{kremer2018robust}
Jan Kremer, Fei Sha, and Christian Igel.
\newblock Robust active label correction.
\newblock In \emph{AISTATS}, pages 308--316, 2018.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{lecun2010mnist}
Yann LeCun, Corinna Cortes, and CJ~Burges.
\newblock Mnist handwritten digit database.
\newblock \emph{ATT Labs [Online]. Available: http://yann. lecun.
  com/exdb/mnist}, 2, 2010.

\bibitem[Li et~al.(2020)Li, Soltanolkotabi, and Oymak]{li2019gradient}
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak.
\newblock Gradient descent with early stopping is provably robust to label
  noise for overparameterized neural networks.
\newblock In \emph{AISTATS}, 2020.

\bibitem[Li et~al.(2017)Li, Yang, Song, Cao, Luo, and Li]{li2017learning}
Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia
  Li.
\newblock Learning from noisy labels with distillation.
\newblock In \emph{ICCV}, pages 1910--1918, 2017.

\bibitem[Liu and Tao(2016)]{liu2016classification}
Tongliang Liu and Dacheng Tao.
\newblock Classification with noisy labels by importance reweighting.
\newblock \emph{IEEE Transactions on pattern analysis and machine
  intelligence}, 38\penalty0 (3):\penalty0 447--461, 2016.

\bibitem[Liu and Guo(2019)]{liu2019peer}
Yang Liu and Hongyi Guo.
\newblock Peer loss functions: Learning from noisy labels without knowing noise
  rates.
\newblock \emph{arXiv preprint arXiv:1910.03231}, 2019.

\bibitem[Lu et~al.(2018)Lu, Niu, Menon, and Sugiyama]{lu2018minimal}
Nan Lu, Gang Niu, Aditya~Krishna Menon, and Masashi Sugiyama.
\newblock On the minimal supervision for training any binary classifier from
  only unlabeled data.
\newblock In \emph{ICLR}, 2018.

\bibitem[Ma et~al.(2018)Ma, Wang, Houle, Zhou, Erfani, Xia, Wijewickrema, and
  Bailey]{ma2018dimensionality}
Xingjun Ma, Yisen Wang, Michael~E Houle, Shuo Zhou, Sarah~M Erfani, Shu-Tao
  Xia, Sudanthi Wijewickrema, and James Bailey.
\newblock Dimensionality-driven learning with noisy labels.
\newblock In \emph{ICML}, pages 3361--3370, 2018.

\bibitem[Malach and Shalev-Shwartz(2017)]{malach2017decoupling}
Eran Malach and Shai Shalev-Shwartz.
\newblock Decoupling" when to update" from" how to update".
\newblock In \emph{NeurIPS}, pages 960--970, 2017.

\bibitem[Natarajan et~al.(2013)Natarajan, Dhillon, Ravikumar, and
  Tewari]{natarajan2013learning}
Nagarajan Natarajan, Inderjit~S Dhillon, Pradeep~K Ravikumar, and Ambuj Tewari.
\newblock Learning with noisy labels.
\newblock In \emph{NeurIPS}, pages 1196--1204, 2013.

\bibitem[Northcutt et~al.(2017)Northcutt, Wu, and Chuang]{northcuttlearning}
Curtis~G Northcutt, Tailin Wu, and Isaac~L Chuang.
\newblock Learning with confident examples: Rank pruning for robust
  classification with noisy labels.
\newblock In \emph{UAI}, 2017.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Krishna~Menon, Nock, and
  Qu]{patrini2017making}
Giorgio Patrini, Alessandro Rozza, Aditya Krishna~Menon, Richard Nock, and
  Lizhen Qu.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In \emph{CVPR}, pages 1944--1952, 2017.

\bibitem[Ramaswamy et~al.(2016)Ramaswamy, Scott, and
  Tewari]{ramaswamy2016mixture}
Harish Ramaswamy, Clayton Scott, and Ambuj Tewari.
\newblock Mixture proportion estimation via kernel embeddings of distributions.
\newblock In \emph{ICML}, pages 2052--2060, 2016.

\bibitem[Reed et~al.(2014)Reed, Lee, Anguelov, Szegedy, Erhan, and
  Rabinovich]{reed2014training}
Scott~E Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan,
  and Andrew Rabinovich.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock \emph{CoRR}, 2014.

\bibitem[Ren et~al.(2018)Ren, Zeng, Yang, and Urtasun]{ren2018learning}
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun.
\newblock Learning to reweight examples for robust deep learning.
\newblock In \emph{ICML}, pages 4331--4340, 2018.

\bibitem[Scott(2015)]{scott2015rate}
Clayton Scott.
\newblock A rate of convergence for mixture proportion estimation, with
  application to learning from noisy labels.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 838--846,
  2015.

\bibitem[Scott et~al.(2013)Scott, Blanchard, and
  Handy]{scott2013classification}
Clayton Scott, Gilles Blanchard, and Gregory Handy.
\newblock Classification with asymmetric label noise: Consistency and maximal
  denoising.
\newblock In \emph{Conference On Learning Theory}, pages 489--511, 2013.

\bibitem[Tanaka et~al.(2018)Tanaka, Ikami, Yamasaki, and
  Aizawa]{tanaka2018joint}
Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa.
\newblock Joint optimization framework for learning with noisy labels.
\newblock In \emph{CVPR}, pages 5552--5560, 2018.

\bibitem[Thekumparampil et~al.(2018)Thekumparampil, Khetan, Lin, and
  Oh]{thekumparampil2018robustness}
Kiran~K Thekumparampil, Ashish Khetan, Zinan Lin, and Sewoong Oh.
\newblock Robustness of conditional gans to noisy labels.
\newblock In \emph{NeurIPS}, pages 10271--10282, 2018.

\bibitem[Vahdat(2017)]{vahdat2017toward}
Arash Vahdat.
\newblock Toward robustness against label noise in training deep discriminative
  neural networks.
\newblock In \emph{NeurIPS}, pages 5596--5605, 2017.

\bibitem[Veit et~al.(2017)Veit, Alldrin, Chechik, Krasin, Gupta, and
  Belongie]{veit2017learning}
Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge
  Belongie.
\newblock Learning from noisy large-scale datasets with minimal supervision.
\newblock In \emph{CVPR}, pages 839--847, 2017.

\bibitem[Wu et~al.(2020)Wu, Xia, Liu, Han, Gong, Wang, Liu, and
  Niu]{wu2020class2simi}
Songhua Wu, Xiaobo Xia, Tongliang Liu, Bo~Han, Mingming Gong, Nannan Wang,
  Haifeng Liu, and Gang Niu.
\newblock Class2simi: A new perspective on learning with label noise.
\newblock \emph{arXiv preprint arXiv:2006.07831}, 2020.

\bibitem[Xia et~al.(2019)Xia, Liu, Wang, Han, Gong, Niu, and
  Sugiyama]{xia2019anchor}
Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo~Han, Chen Gong, Gang Niu, and
  Masashi Sugiyama.
\newblock Are anchor points really indispensable in label-noise learning?
\newblock In \emph{NeurIPS}, pages 6835--6846, 2019.

\bibitem[Xia et~al.(2020)Xia, Liu, Han, Wang, Gong, Liu, Niu, Tao, and
  Sugiyama]{xia2020parts}
Xiaobo Xia, Tongliang Liu, Bo~Han, Nannan Wang, Mingming Gong, Haifeng Liu,
  Gang Niu, Dacheng Tao, and Masashi Sugiyama.
\newblock Parts-dependent label noise: Towards instance-dependent label noise.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Xiao et~al.(2015)Xiao, Xia, Yang, Huang, and Wang]{xiao2015learning}
Tong Xiao, Tian Xia, Yi~Yang, Chang Huang, and Xiaogang Wang.
\newblock Learning from massive noisy labeled data for image classification.
\newblock In \emph{CVPR}, pages 2691--2699, 2015.

\bibitem[Xu et~al.(2019)Xu, Cao, Kong, and Wang]{xu2019l_dmi}
Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang.
\newblock L\_dmi: A novel information-theoretic loss function for training deep
  nets robust to label noise.
\newblock In \emph{NeurIPS}, pages 6222--6233, 2019.

\bibitem[Yao et~al.(2020)Yao, Yang, Han, Niu, and Kwok]{yao2020searching}
Quanming Yao, Hansi Yang, Bo~Han, Gang Niu, and J~Kwok.
\newblock Searching to exploit memorization effect in learning with noisy
  labels.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, ICML}, volume~20, 2020.

\bibitem[Yu et~al.(2019)Yu, Han, Yao, Niu, Tsang, and Sugiyama]{yu2019does}
Xingrui Yu, Bo~Han, Jiangchao Yao, Gang Niu, Ivor~W Tsang, and Masashi
  Sugiyama.
\newblock How does disagreement help generalization against label corruption?
\newblock \emph{ICML}, 2019.

\bibitem[Yu et~al.(2017)Yu, Liu, Gong, Zhang, Batmanghelich, and
  Tao]{yu2017transfer}
Xiyu Yu, Tongliang Liu, Mingming Gong, Kun Zhang, Kayhan Batmanghelich, and
  Dacheng Tao.
\newblock Transfer learning with label noise.
\newblock \emph{arXiv preprint arXiv:1707.09724}, 2017.

\bibitem[Yu et~al.(2018)Yu, Liu, Gong, and Tao]{yu2018learning}
Xiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao.
\newblock Learning with biased complementary labels.
\newblock In \emph{ECCV}, pages 68--83, 2018.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{ICLR}, 2017.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{ICLR}, 2018.

\bibitem[Zhang and Sabuncu(2018)]{zhang2018generalized}
Zhilu Zhang and Mert Sabuncu.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock In \emph{NeurIPS}, pages 8778--8788, 2018.

\end{thebibliography}
