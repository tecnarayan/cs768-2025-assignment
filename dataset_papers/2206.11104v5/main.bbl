\begin{thebibliography}{81}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Fra()]{Framingh21}
Framingham heart study dataset | kaggle.
\newblock
  \url{https://www.kaggle.com/datasets/aasheesh200/framingham-heart-study-dataset}.
\newblock (Accessed on 08/15/2022).

\bibitem[sha()]{shapbench}
Shap benchmark.
\newblock URL \url{https://shap.readthedocs.io/en/latest/index.html}.

\bibitem[Agarwal and Nguyen(2020)]{agarwal2020explaining}
Chirag Agarwal and Anh Nguyen.
\newblock Explaining image classifiers by removing input features using
  generative models.
\newblock In \emph{ACCV}, 2020.

\bibitem[Agarwal et~al.(2022)Agarwal, Johnson, Pawelczyk, Krishna, Saxena,
  Zitnik, and Lakkaraju]{agarwal2022rethinking}
Chirag Agarwal, Nari Johnson, Martin Pawelczyk, Satyapriya Krishna, Eshika
  Saxena, Marinka Zitnik, and Himabindu Lakkaraju.
\newblock Rethinking stability for attribution-based explanations.
\newblock In \emph{ICLR 2022 Workshop on PAIR$^{2}$Struct}, 2022.

\bibitem[Agarwal et~al.(2021)Agarwal, Jabbari, Agarwal, Upadhyay, Wu, and
  Lakkaraju]{agarwal2021towards}
Sushant Agarwal, Shahin Jabbari, Chirag Agarwal, Sohini Upadhyay, Steven Wu,
  and Himabindu Lakkaraju.
\newblock Towards the unification and robustness of perturbation and gradient
  based explanations.
\newblock In \emph{ICML}, 2021.

\bibitem[Aivodji et~al.(2019)Aivodji, Arai, Fortineau, Gambs, Hara, and
  Tapp]{aivodji2019fairwashing}
Ulrich Aivodji, Hiromi Arai, Olivier Fortineau, S{\'e}bastien Gambs, Satoshi
  Hara, and Alain Tapp.
\newblock Fairwashing: the risk of rationalization.
\newblock In \emph{ICML}, 2019.

\bibitem[Alvarez-Melis and Jaakkola(2018)]{alvarez2018robustness}
David Alvarez-Melis and Tommi~S Jaakkola.
\newblock On the robustness of interpretability methods.
\newblock \emph{arXiv}, 2018.

\bibitem[Arrieta et~al.(2020)Arrieta, D{\'\i}az-Rodr{\'\i}guez, Del~Ser,
  Bennetot, Tabik, Barbado, Garc{\'\i}a, Gil-L{\'o}pez, Molina, Benjamins,
  et~al.]{arrieta2020explainable}
Alejandro~Barredo Arrieta, Natalia D{\'\i}az-Rodr{\'\i}guez, Javier Del~Ser,
  Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garc{\'\i}a, Sergio
  Gil-L{\'o}pez, Daniel Molina, Richard Benjamins, et~al.
\newblock Explainable artificial intelligence (xai): Concepts, taxonomies,
  opportunities and challenges toward responsible ai.
\newblock \emph{Information Fusion}, 2020.

\bibitem[Balagopalan et~al.(2022)Balagopalan, Zhang, Hamidieh, Hartvigsen,
  Rudzicz, and Ghassemi]{balagopalan2022road}
Aparna Balagopalan, Haoran Zhang, Kimia Hamidieh, Thomas Hartvigsen, Frank
  Rudzicz, and Marzyeh Ghassemi.
\newblock The road to explainability is paved with bias: Measuring the fairness
  of explanations.
\newblock \emph{arXiv}, 2022.

\bibitem[Bansal et~al.(2020)Bansal, Agarwal, and Nguyen]{bansal2020sam}
Naman Bansal, Chirag Agarwal, and Anh Nguyen.
\newblock Sam: The sensitivity of attribution methods to hyperparameters.
\newblock In \emph{CVPR}, 2020.

\bibitem[Barocas et~al.(2020)Barocas, Selbst, and Raghavan]{Barocas_2020}
Solon Barocas, Andrew Selbst, and Manish Raghavan.
\newblock The hidden assumptions behind counterfactual explanations and
  principal reasons.
\newblock In \emph{FAccT}, 2020.

\bibitem[Bastani et~al.(2017)Bastani, Kim, and
  Bastani]{bastani2017interpretability}
Osbert Bastani, Carolyn Kim, and Hamsa Bastani.
\newblock Interpretability via model extraction.
\newblock \emph{arXiv}, 2017.

\bibitem[Bien and Tibshirani(2009)]{bien2009classification}
Jacob Bien and Robert Tibshirani.
\newblock Classification by set cover: The prototype vector machine.
\newblock \emph{arXiv}, 2009.

\bibitem[Borisov et~al.(2021)Borisov, Leemann, Se{\ss}ler, Haug, Pawelczyk, and
  Kasneci]{borisov2021deep}
Vadim Borisov, Tobias Leemann, Kathrin Se{\ss}ler, Johannes Haug, Martin
  Pawelczyk, and Gjergji Kasneci.
\newblock Deep neural networks and tabular data: A survey.
\newblock \emph{arXiv}, 2021.

\bibitem[Caruana et~al.(2015)Caruana, Lou, Gehrke, Koch, Sturm, and
  Elhadad]{caruana15:intelligible}
Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie
  Elhadad.
\newblock Intelligible models for healthcare: Predicting pneumonia risk and
  hospital 30-day readmission.
\newblock In \emph{KDD}, 2015.

\bibitem[Chen et~al.(2022)Chen, Johnson, Topin, Plumb, and
  Talwalkar]{chen2022usecase}
Valerie Chen, Nari Johnson, Nicholay Topin, Gregory Plumb, and Ameet Talwalkar.
\newblock Use-case-grounded simulations for explanation evaluation.
\newblock \emph{arXiv}, 2022.

\bibitem[Covert et~al.(2021)Covert, Lundberg, and Lee]{covert2021explaining}
Ian Covert, Scott Lundberg, and Su-In Lee.
\newblock Explaining by removing: A unified framework for model explanation.
\newblock \emph{JMLR}, 2021.

\bibitem[Dai et~al.(2022)Dai, Upadhyay, Aivodji, Bach, and
  Lakkaraju]{dai2022fairness}
Jessica Dai, Sohini Upadhyay, Ulrich Aivodji, Stephen~H Bach, and Himabindu
  Lakkaraju.
\newblock Fairness via explanation quality: Evaluating disparities in the
  quality of post hoc explanations.
\newblock In \emph{AAAI Conference on AI, Ethics, and Society (AIES)}, 2022.

\bibitem[Dasgupta et~al.(2022)Dasgupta, Frost, and
  Moshkovitz]{dasgupta2022framework}
Sanjoy Dasgupta, Nave Frost, and Michal Moshkovitz.
\newblock Framework for evaluating faithfulness of local explanations.
\newblock \emph{arXiv}, 2022.

\bibitem[Dominguez-Olmedo et~al.(2022)Dominguez-Olmedo, Karimi, and
  Sch{\"o}lkopf]{dominguez2022adversarial}
Ricardo Dominguez-Olmedo, Amir~H Karimi, and Bernhard Sch{\"o}lkopf.
\newblock On the adversarial robustness of causal algorithmic recourse.
\newblock In \emph{ICML}. PMLR, 2022.

\bibitem[Doshi-Velez and Kim(2017)]{doshi2017towards}
Finale Doshi-Velez and Been Kim.
\newblock Towards a rigorous science of interpretable machine learning.
\newblock \emph{arXiv}, 2017.

\bibitem[Dua and Graff(2017)]{Dua:2019}
Dheeru Dua and Casey Graff.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Elshawi et~al.(2019)Elshawi, Al-Mallah, and
  Sakr]{elshawi2019interpretability}
Radwa Elshawi, Mouaz~H Al-Mallah, and Sherif Sakr.
\newblock On the interpretability of machine learning-based model for
  predicting hypertension.
\newblock \emph{BMC medical informatics and decision making}, 2019.

\bibitem[Faber et~al.(2021)Faber, K.~Moghaddam, and
  Wattenhofer]{faber2021comparing}
Lukas Faber, Amin K.~Moghaddam, and Roger Wattenhofer.
\newblock When comparing to ground truth is wrong: On evaluating gnn
  explanation methods.
\newblock In \emph{KDD}, 2021.

\bibitem[FICO(2022)]{HELOC}
FICO.
\newblock Explainable machine learning challenge.
\newblock
  \url{https://community.fico.com/s/explainable-machine-learning-challenge?tabset-158d9=3},
  2022.
\newblock (Accessed on 05/23/2022).

\bibitem[Fokkema et~al.(2022)Fokkema, de~Heide, and van
  Erven]{fokkema2022attribution}
Hidde Fokkema, Rianne de~Heide, and Tim van Erven.
\newblock Attribution-based explanations that provide recourse cannot be
  robust.
\newblock \emph{arXiv}, 2022.

\bibitem[Freshcorn(2022)]{GiveMeCredit}
Bryce Freshcorn.
\newblock Give me some credit :: 2011 competition data | kaggle.
\newblock
  \url{https://www.kaggle.com/datasets/brycecf/give-me-some-credit-dataset},
  2022.
\newblock (Accessed on 05/23/2022).

\bibitem[Ghassemi et~al.(2021)Ghassemi, Oakden-Rayner, and
  Beam]{ghassemi2021false}
Marzyeh Ghassemi, Luke Oakden-Rayner, and Andrew~L Beam.
\newblock The false hope of current approaches to explainable artificial
  intelligence in health care.
\newblock \emph{The Lancet Digital Health}, 2021.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Abid, and
  Zou]{ghorbani2019interpretation}
Amirata Ghorbani, Abubakar Abid, and James Zou.
\newblock Interpretation of neural networks is fragile.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2019.

\bibitem[Guidotti et~al.(2018)Guidotti, Monreale, Ruggieri, Turini, Giannotti,
  and Pedreschi]{guidotti2018survey}
Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca
  Giannotti, and Dino Pedreschi.
\newblock A survey of methods for explaining black box models.
\newblock \emph{ACM computing surveys (CSUR)}, 2018.

\bibitem[Han et~al.(2022)Han, Srinivas, and Lakkaraju]{han2022explanation}
Tessa Han, Suraj Srinivas, and Himabindu Lakkaraju.
\newblock Which explanation should i choose? a function approximation
  perspective to characterizing post hoc explanations.
\newblock \emph{arXiv}, 2022.

\bibitem[Hedstr{\"o}m et~al.(2022)Hedstr{\"o}m, Weber, Bareeva, Motzkus, Samek,
  Lapuschkin, and H{\"o}hne]{hedstrom2022quantus}
Anna Hedstr{\"o}m, Leander Weber, Dilyara Bareeva, Franz Motzkus, Wojciech
  Samek, Sebastian Lapuschkin, and Marina M-C H{\"o}hne.
\newblock Quantus: an explainable ai toolkit for responsible evaluation of
  neural network explanations.
\newblock \emph{arXiv}, 2022.

\bibitem[Hooker et~al.(2018)Hooker, Erhan, Kindermans, and
  Kim]{hooker2018evaluating}
Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim.
\newblock Evaluating feature importance estimates.
\newblock \emph{arXiv}, 2018.

\bibitem[Ibrahim et~al.(2019)Ibrahim, Louie, Modarres, and
  Paisley]{ibrahim2019global}
Mark Ibrahim, Melissa Louie, Ceena Modarres, and John Paisley.
\newblock Global explanations of neural networks: Mapping the landscape of
  predictions.
\newblock \emph{CoRR, abs/1902.02384}, 2019.

\bibitem[Jesus et~al.(2021)Jesus, Bel{\'e}m, Balayan, Bento, Saleiro, Bizarro,
  and Gama]{jesus2021can}
S{\'e}rgio Jesus, Catarina Bel{\'e}m, Vladimir Balayan, Jo{\~a}o Bento, Pedro
  Saleiro, Pedro Bizarro, and Jo{\~a}o Gama.
\newblock How can i choose an explainer? an application-grounded evaluation of
  post-hoc explanations.
\newblock In \emph{FAccT}, 2021.

\bibitem[Jordan and Freiburger(2015)]{jordan2015effect}
Kareem~L Jordan and Tina~L Freiburger.
\newblock The effect of race/ethnicity on sentencing: Examining sentence type,
  jail length, and prison length.
\newblock In \emph{Journal of Ethnicity in Criminal Justice}. Taylor \&
  Francis, 2015.

\bibitem[Karimi et~al.(2019)Karimi, Barthe, Balle, and Valera]{MACE}
Amir-Hossein Karimi, Gilles Barthe, Borja Balle, and Isabel Valera.
\newblock Model-agnostic counterfactual explanations for consequential
  decisions.
\newblock \emph{arXiv}, 2019.

\bibitem[Karimi et~al.(2020{\natexlab{a}})Karimi, Sch{\"o}lkopf, and
  Valera]{karimi2020algorithmic}
Amir-Hossein Karimi, Bernhard Sch{\"o}lkopf, and Isabel Valera.
\newblock Algorithmic recourse: from counterfactual explanations to
  interventions.
\newblock \emph{CoRR, abs/2002.06278}, 2020{\natexlab{a}}.

\bibitem[Karimi et~al.(2020{\natexlab{b}})Karimi, von K{\"u}gelgen,
  Sch{\"o}lkopf, and Valera]{karimi2020causal}
Amir-Hossein Karimi, Julius von K{\"u}gelgen, Bernhard Sch{\"o}lkopf, and
  Isabel Valera.
\newblock Algorithmic recourse under imperfect causal knowledge: a
  probabilistic approach.
\newblock \emph{CoRR}, 2020{\natexlab{b}}.

\bibitem[Kaur et~al.(2020)Kaur, Nori, Jenkins, Caruana, Wallach, and
  Wortman~Vaughan]{kaur2020interpreting}
Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, and
  Jennifer Wortman~Vaughan.
\newblock Interpreting interpretability: Understanding data scientists' use of
  interpretability tools for machine learning.
\newblock In \emph{CHI Conference on Human Factors in Computing Systems}, 2020.

\bibitem[Kim et~al.(2021)Kim, Plumb, and Talwalkar]{kim2021sanity}
Joon~Sik Kim, Gregory Plumb, and Ameet Talwalkar.
\newblock Sanity simulations for saliency methods.
\newblock \emph{arXiv}, 2021.

\bibitem[Kokhlikyan et~al.(2020)Kokhlikyan, Miglani, Martin, Wang, Alsallakh,
  Reynolds, Melnikov, Kliushkina, Araya, Yan, and
  Reblitz-Richardson]{kokhlikyan2020captum}
Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh,
  Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi
  Yan, and Orion Reblitz-Richardson.
\newblock Captum: A unified and generic model interpretability library for
  pytorch, 2020.

\bibitem[Krishna et~al.(2022)Krishna, Han, Gu, Pombra, Jabbari, Wu, and
  Lakkaraju]{krishna2022disagreement}
Satyapriya Krishna, Tessa Han, Alex Gu, Javin Pombra, Shahin Jabbari, Steven
  Wu, and Himabindu Lakkaraju.
\newblock The disagreement problem in explainable machine learning: A
  practitioner's perspective.
\newblock \emph{arXiv}, 2022.

\bibitem[Lage et~al.(2019)Lage, Chen, He, Narayanan, Kim, Gershman, and
  Doshi-Velez]{lage2019evaluation}
Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam Gershman,
  and Finale Doshi-Velez.
\newblock An evaluation of the human-interpretability of explanation.
\newblock \emph{arXiv}, 2019.

\bibitem[Lakkaraju and Bastani(2020)]{lakkaraju2020fool}
Himabindu Lakkaraju and Osbert Bastani.
\newblock ``how do i fool you?'' manipulating user trust via misleading black
  box explanations.
\newblock In \emph{AAAI Conference on AIES}, 2020.

\bibitem[Lakkaraju et~al.(2016)Lakkaraju, Bach, and
  Leskovec]{lakkaraju2016interpretable}
Himabindu Lakkaraju, Stephen~H Bach, and Jure Leskovec.
\newblock Interpretable decision sets: A joint framework for description and
  prediction.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD international conference
  on knowledge discovery and data mining}, pages 1675--1684, 2016.

\bibitem[Lakkaraju et~al.(2019)Lakkaraju, Kamar, Caruana, and
  Leskovec]{lakkaraju2019faithful}
Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec.
\newblock Faithful and customizable explanations of black box models.
\newblock In \emph{Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics,
  and Society}, pages 131--138, 2019.

\bibitem[Letham et~al.(2015)Letham, Rudin, McCormick, and
  Madigan]{letham2015interpretable}
Benjamin Letham, Cynthia Rudin, Tyler~H McCormick, and David Madigan.
\newblock Interpretable classifiers using rules and bayesian analysis: Building
  a better stroke prediction model.
\newblock \emph{The Annals of Applied Statistics}, 9\penalty0 (3):\penalty0
  1350--1371, 2015.

\bibitem[Linardatos et~al.(2021)Linardatos, Papastefanopoulos, and
  Kotsiantis]{linardatos2021explainable}
Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis.
\newblock Explainable ai: A review of machine learning interpretability
  methods.
\newblock \emph{Entropy}, 23\penalty0 (1):\penalty0 18, 2021.

\bibitem[Lipton(2016)]{lipton2016mythos}
Zachary~C Lipton.
\newblock The mythos of model interpretability.
\newblock \emph{CoRR, abs/1606.03490}, 2016.

\bibitem[Liu et~al.(2021)Liu, Khandagale, White, and
  Neiswanger]{liu2021synthetic}
Yang Liu, Sujay Khandagale, Colin White, and Willie Neiswanger.
\newblock Synthetic benchmarks for scientific research in explainable machine
  learning.
\newblock In \emph{NeurIPS Datasets and Benchmarks Track}, 2021.

\bibitem[Looveren and Klaise(2019)]{looveren2019interpretable}
Arnaud Looveren and Janis Klaise.
\newblock Interpretable counterfactual explanations guided by prototypes.
\newblock \emph{CoRR, abs/ 1907.02584}, 2019.

\bibitem[Lou et~al.(2012)Lou, Caruana, and Gehrke]{lou2012intelligible}
Yin Lou, Rich Caruana, and Johannes Gehrke.
\newblock Intelligible models for classification and regression.
\newblock In \emph{KDD}, 2012.

\bibitem[Lundberg and Lee(2017{\natexlab{a}})]{lundberg17:a-unified}
Scott~M Lundberg and Su-In Lee.
\newblock A unified approach to interpreting model predictions.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Neural Information Processing
  Systems (NIPS)}, pages 4765--4774. Curran Associates, Inc.,
  2017{\natexlab{a}}.

\bibitem[Lundberg and Lee(2017{\natexlab{b}})]{lundberg2017unified}
Scott~M Lundberg and Su-In Lee.
\newblock A unified approach to interpreting model predictions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4765--4774, 2017{\natexlab{b}}.

\bibitem[Murdoch et~al.(2019)Murdoch, Singh, Kumbier, Abbasi-Asl, and
  Yu]{murdoch2019definitions}
W~James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu.
\newblock Definitions, methods, and applications in interpretable machine
  learning.
\newblock \emph{Proceedings of the National Academy of Sciences}, 2019.

\bibitem[Pawelczyk et~al.(2020)Pawelczyk, Broelemann, and
  Kasneci]{pawelczyk2020learning}
Martin Pawelczyk, Klaus Broelemann, and Gjergji Kasneci.
\newblock Learning model-agnostic counterfactual explanations for tabular data.
\newblock In \emph{WWW}, 2020.

\bibitem[Pawelczyk et~al.(2021)Pawelczyk, Bielawski, Van~den Heuvel, Richter,
  and Kasneci]{pawelczyk2021carla}
Martin Pawelczyk, Sascha Bielawski, Johan Van~den Heuvel, Tobias Richter, and
  Gjergji Kasneci.
\newblock Carla: A python library to benchmark algorithmic recourse and
  counterfactual explanation algorithms.
\newblock In \emph{NeurIPS Benchmark and Datasets Track}, 2021.

\bibitem[Petsiuk et~al.(2018)Petsiuk, Das, and Saenko]{petsiuk2018rise}
Vitali Petsiuk, Abir Das, and Kate Saenko.
\newblock Rise: Randomized input sampling for explanation of black-box models.
\newblock \emph{arXiv}, 2018.

\bibitem[Poursabzi-Sangdeh et~al.(2018)Poursabzi-Sangdeh, Goldstein, Hofman,
  Vaughan, and Wallach]{poursabzi2018manipulating}
Forough Poursabzi-Sangdeh, Daniel~G Goldstein, Jake~M Hofman, Jennifer~Wortman
  Vaughan, and Hanna Wallach.
\newblock Manipulating and measuring model interpretability.
\newblock \emph{CoRR}, 2018.

\bibitem[Poyiadzi et~al.(2020)Poyiadzi, Sokol, Santos-Rodriguez, De~Bie, and
  Flach]{FACE}
Rafael Poyiadzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De~Bie, and Peter
  Flach.
\newblock {FACE}: Feasible and actionable counterfactual explanations.
\newblock In \emph{AAAI Conference on AIES}, 2020.

\bibitem[Ribeiro et~al.(2016)Ribeiro, Singh, and Guestrin]{ribeiro16:kdd}
Marco~Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
\newblock "why should i trust you?": Explaining the predictions of any
  classifier.
\newblock In \emph{KDD}, 2016.

\bibitem[Ribeiro et~al.(2018)Ribeiro, Singh, and Guestrin]{ribeiro2018anchors}
Marco~Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
\newblock Anchors: High-precision model-agnostic explanations.
\newblock In \emph{AAAI}, 2018.

\bibitem[Rudin(2019)]{rudin2019stop}
Cynthia Rudin.
\newblock Stop explaining black box machine learning models for high stakes
  decisions and use interpretable models instead.
\newblock \emph{Nature Machine Intelligence}, 2019.

\bibitem[Selvaraju et~al.(2017)Selvaraju, Cogswell, Das, Vedantam, Parikh, and
  Batra]{selvaraju2017grad}
Ramprasaath~R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam,
  Devi Parikh, and Dhruv Batra.
\newblock Grad-cam: Visual explanations from deep networks via gradient-based
  localization.
\newblock In \emph{ICCV}, 2017.

\bibitem[Shrikumar et~al.(2017)Shrikumar, Greenside, and
  Kundaje]{shrikumar2017learning}
Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje.
\newblock Learning important features through propagating activation
  differences.
\newblock In \emph{ICML}, 2017.

\bibitem[Simonyan et~al.(2014)Simonyan, Vedaldi, and
  Zisserman]{simonyan2013saliency}
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
\newblock Deep inside convolutional networks: Visualising image classification
  models and saliency maps.
\newblock In \emph{ICLR}, 2014.

\bibitem[Slack et~al.(2020)Slack, Hilgard, Jia, Singh, and
  Lakkaraju]{slack2020fooling}
Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju.
\newblock Fooling lime and shap: Adversarial attacks on post hoc explanation
  methods.
\newblock In \emph{AAAI Conference on AIES}, 2020.

\bibitem[Slack et~al.(2021)Slack, Hilgard, Singh, and
  Lakkaraju]{slack2021reliable}
Dylan Slack, Anna Hilgard, Sameer Singh, and Himabindu Lakkaraju.
\newblock Reliable post hoc explanations: Modeling uncertainty in
  explainability.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Smilkov et~al.(2017)Smilkov, Thorat, Kim, Vi{\'e}gas, and
  Wattenberg]{smilkov2017smoothgrad}
Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi{\'e}gas, and Martin
  Wattenberg.
\newblock Smoothgrad: removing noise by adding noise.
\newblock \emph{arXiv}, 2017.

\bibitem[Smith et~al.(1988)Smith, Everhart, Dickson, Knowler, and
  Johannes]{smith1988using}
Jack~W Smith, James~E Everhart, WC~Dickson, William~C Knowler, and Robert~Scott
  Johannes.
\newblock Using the adap learning algorithm to forecast the onset of diabetes
  mellitus.
\newblock In \emph{Proceedings of the annual symposium on computer application
  in medical care}, page 261. American Medical Informatics Association, 1988.

\bibitem[Sundararajan et~al.(2017)Sundararajan, Taly, and
  Yan]{sundararajan2017axiomatic}
Mukund Sundararajan, Ankur Taly, and Qiqi Yan.
\newblock Axiomatic attribution for deep networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Upadhyay et~al.(2021)Upadhyay, Joshi, and
  Lakkaraju]{upadhyay2021towards}
Sohini Upadhyay, Shalmali Joshi, and Himabindu Lakkaraju.
\newblock Towards robust and reliable algorithmic recourse.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Ustun et~al.(2019)Ustun, Spangher, and Liu]{ustun2019actionable}
Berk Ustun, Alexander Spangher, and Yang Liu.
\newblock Actionable recourse in linear classification.
\newblock In \emph{FAccT}, 2019.

\bibitem[Verma et~al.(2020)Verma, Dickerson, and
  Hines]{verma2020counterfactual}
Sahil Verma, John Dickerson, and Keegan Hines.
\newblock Counterfactual explanations for machine learning: A review.
\newblock \emph{arXiv}, 2020.

\bibitem[Wachter et~al.(2017)Wachter, Mittelstadt, and
  Russell]{wachter2017counterfactual}
Sandra Wachter, Brent Mittelstadt, and Chris Russell.
\newblock Counterfactual explanations without opening the black box: Automated
  decisions and the {GDPR}.
\newblock \emph{Harvard Journal of Law \& Technology}, 31:\penalty0 841, 2017.

\bibitem[Wang and Rudin(2015)]{wang2015falling}
Fulton Wang and Cynthia Rudin.
\newblock Falling rule lists.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 1013--1022.
  PMLR, 2015.

\bibitem[Whitmore et~al.(2016)Whitmore, George, and
  Hudson]{whitmore2016mapping}
Leanne~S Whitmore, Anthe George, and Corey~M Hudson.
\newblock Mapping chemical performance on molecular structures using locally
  interpretable explanations.
\newblock \emph{CoRR, abs/1611.07443}, 2016.

\bibitem[Yeh and Lien(2009)]{yeh2009comparisons}
I-Cheng Yeh and Che-hui Lien.
\newblock The comparisons of data mining techniques for the predictive accuracy
  of probability of default of credit card clients.
\newblock In \emph{Expert Systems with Applications}, 2009.

\bibitem[Zeng et~al.(2017)Zeng, Ustun, and Rudin]{zeng2017interpretable}
Jiaming Zeng, Berk Ustun, and Cynthia Rudin.
\newblock Interpretable classification models for recidivism prediction.
\newblock \emph{Journal of the Royal Statistical Society: Series A (Statistics
  in Society)}, 2017.

\bibitem[Zhou et~al.(2021)Zhou, Gandomi, Chen, and
  Holzinger]{zhou2021evaluating}
Jianlong Zhou, Amir~H Gandomi, Fang Chen, and Andreas Holzinger.
\newblock Evaluating the quality of machine learning explanations: A survey on
  methods and metrics.
\newblock \emph{Electronics}, 10\penalty0 (5):\penalty0 593, 2021.

\end{thebibliography}
