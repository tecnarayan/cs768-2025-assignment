% Encoding: UTF-8

@inproceedings{signsgd,
  added-at = {2019-04-03T00:00:00.000+0200},
  author = {Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  biburl = {https://www.bibsonomy.org/bibtex/2409dcbd9da24821e21edb290debe9944/dblp},
  booktitle = {ICML},
  crossref = {conf/icml/2018},
  editor = {Dy, Jennifer G. and Krause, Andreas},
  ee = {http://proceedings.mlr.press/v80/bernstein18a.html},
  interhash = {a579bb8583ea2fdba022637317600f88},
  intrahash = {409dcbd9da24821e21edb290debe9944},
  keywords = {dblp},
  pages = {559-568},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  timestamp = {2019-04-04T11:43:21.000+0200},
  title = {SIGNSGD: Compressed Optimisation for Non-Convex Problems.},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2018.html#BernsteinWAA18},
  volume = 80,
  year = 2018
}
@MISC{Torrey_transferlearning,
    author = {Lisa Torrey and Jude Shavlik},
    title = {Transfer Learning},
    year = {}
}

@article{Dettmers20158BitAF,
  title={8-Bit Approximations for Parallelism in Deep Learning},
  author={Tim Dettmers},
  journal={ICLR},
  year={2015}
}


@article{shazeer2020glu,
      title={{GLU} Variants Improve Transformer}, 
      author={Noam Shazeer},
      year={2020},
      journal={arXiv preprint arXiv:2002.05202},
}

@article{Narang2021DoTM,
  title={Do Transformer Modifications Transfer Across Implementations and Applications?},
  author={Sharan Narang and Hyung Won Chung and Yi Tay and William Fedus and Thibault F{\'e}vry and Michael Matena and Karishma Malkan and Noah Fiedel and Noam M. Shazeer and Zhenzhong Lan and Yanqi Zhou and Wen-hong Li and Nan Ding and Jake Marcus and Adam Roberts and Colin Raffel},
  journal={arXiv preprint arXiv:2102.11972},
  year={2021},
}

@article{su2021roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@misc{rope-eleutherai,
  title = {Rotary Embeddings: A Relative Revolution},
  author = {Biderman, Stella and Black, Sid and Foster, Charles and Gao, Leo and Hallahan, Eric and He, Horace and Wang, Ben and Wang, Phil},
  howpublished = {\url{blog.eleuther.ai/rotary-embeddings}},
  note = {{Accessed: 2021-05-20}},
  year = {2021}
}

@article{net2net,
author = {Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
year = {2015},
month = {11},
pages = {},
title = {{Net2Net}: Accelerating Learning via Knowledge Transfer},
journal = {arXiv preprint arXiv:1511.05641}
} 

@article{ba2016layer,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      journal={arXiv preprint arXiv:1607.06450},
}

@inproceedings{he2016identity,
  title={Identity mappings in deep residual networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European conference on computer vision},
  pages={630--645},
  year={2016},
  organization={Springer}
}

@article{2020t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@TechReport{TURN,
  author       = {T. Reddy and A. Johnston and P. Matthews and J. Rosenberg},
  title        = {Traversal Using Relays around {NAT} ({TURN}): Relay Extensions to Session Traversal Utilities for {NAT} ({STUN})},
  year         = {2020},
  month        = {02},
  number       = {8656},
  type         = {RFC},
  howpublished = {Internet Requests for Comments},
  issn         = {2070-1721},
  publisher    = {RFC Editor},
  school       = {RFC Editor},
}

@TechReport{STUN,
  author       = {J. Rosenberg and J. Weinberger and C. Huitema and R. Mahy},
  title        = {{STUN} - Simple Traversal of User Datagram Protocol ({UDP}) Through Network Address Translators ({NATs})},
  year         = {2003},
  month        = {03},
  number       = {3489},
  type         = {RFC},
  howpublished = {Internet Requests for Comments},
  issn         = {2070-1721},
  school       = {RFC Editor},
  url          = {http://www.rfc-editor.org/rfc/rfc3489.txt},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Misc from intro section start %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@article{protein2vec,
author = {Gao, Jianliang and Tian, Ling and Lv, Tengfei and Wang, Jianxin and Song, Bo and Hu, Xiaohua},
year = {2019},
month = {08},
pages = {1-1},
title = {Protein2Vec: Aligning Multiple PPI Networks with Representation Learning},
volume = {PP},
journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
doi = {10.1109/TCBB.2019.2937771}
}

@misc{bertologybiology,
    title={BERTology Meets Biology: Interpreting Attention in Protein Language Models},
    author={Jesse Vig and Ali Madani and Lav R. Varshney and Caiming Xiong and Richard Socher and Nazneen Fatema Rajani},
    year={2020},
    eprint={2006.15222},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2006.15222}
}

 @misc{torrent, 
 title={{The BitTorrent Protocol Specification}}, 
     author={Bram Cohen},
    year={2008},

 howpublished={\url{http://www.bittorrent.org/beps/bep_0003.html}}}
@inproceedings{FedLearningAtScale,
title	= {Towards Federated Learning at Scale: System Design},
author	= {K. A. Bonawitz and Hubert Eichner and Wolfgang Grieskamp and Dzmitry Huba and Alex Ingerman and Vladimir Ivanov and Chloé M Kiddon and Jakub Konečný and Stefano Mazzocchi and Brendan McMahan and Timon Van Overveldt and David Petrou and Daniel Ramage and Jason Roselander},
year	= {2019},
URL	= {https://arxiv.org/abs/1902.01046},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)}
}

@ARTICLE{FedLearningDiffPrivacy,
  author={Wei, Kang and Li, Jun and Ding, Ming and Ma, Chuan and Yang, Howard H. and Farokhi, Farhad and Jin, Shi and Quek, Tony Q. S. and Poor, H. Vincent},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Federated Learning With Differential Privacy: Algorithms and Performance Analysis}, 
  year={2020},
  volume={15},
  number={},
  pages={3454-3469},
  doi={10.1109/TIFS.2020.2988575}}

@inproceedings{FedLearningDecentralized,
  title={Decentralized federated learning preserves model and data privacy},
  author={Wittkopp, Thorsten and Acker, Alexander},
  booktitle={International Conference on Service-Oriented Computing},
  pages={176--187},
  year={2020},
  organization={Springer}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Federated Learning section end %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Volunteer computing section start %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@misc{folding_covid,
      title  = {Folding@home update on {SARS-CoV-2} (10 MAR 2020)},
      howpublished={\url{foldingathome.org/2020/03/10/covid19-update}},
     note = {Accessed: 2021-05-20}
}

@misc{folding_timeline,
    title  = "Folding@home project timeline",
    howpublished="\url{https://foldingathome.org/project-timeline}",
    note = {Accessed: 2021-05-20}

}

% https://folding.typepad.com/news/2007/09/crossing-the-pe.html
@article{folding_petaflop,
    author = {Gross, Michael},
    year = {2012},
    month = {01},
    pages = {R35-8},
    title = {Folding research recruits unconventional help},
    volume = {22},
    journal = {Current biology : CB},
    doi = {10.1016/j.cub.2012.01.008}
}

@misc{folding_exaflop_1,
      title  = "Folding@home active CPU and GPU by OS",
      howpublished   = "\url{https://archive.md/20200412111010/https://stats.foldingathome.org/os}",
      note = {Accessed: 2021-05-20}

}

@misc{folding_exaflop_2,
      title  = "Folding@home gets 1.5+ Exaflops to Fight COVID-19",
      howpublished   = "\url{https://blogs.nvidia.com/blog/2020/04/01/foldingathome-exaflop-coronavirus/}",
      note = {Accessed: 2021-05-20}

}

@article{folding_ps3,
    author = {Narumi, Tetsu and Kameoka, Shun and Taiji, Makoto and Yasuoka, Kenji},
    year = {2008},
    month = {01},
    pages = {3108-3125},
    title = {Accelerating Molecular Dynamics Simulations on PlayStation 3 Platform Using Virtual-GRAPE Programming Model},
    volume = {30},
    journal = {SIAM J. Scientific Computing},
    doi = {10.1137/070692054}
}

@article{larson_crowd,
      title={{Folding}@Home and {Genome}@Home: Using distributed computing to tackle previously intractable problems in computational biology}, 
      author={Stefan M. Larson and Christopher D. Snow and Michael Shirts and Vijay S. Pande},
      year={2009},
      journal={arXiv preprint arXiv:0901.0866},
}


@inproceedings{anderson2004boinc,
  title={{BOINC}: A system for public-resource computing and storage},
  author={Anderson, David P},
  booktitle={Fifth IEEE/ACM international workshop on grid computing},
  pages={4--10},
  year={2004},
  organization={IEEE}
}

@article{lhc_at_home,
author = {Barranco, Javier and Cai, Yunhi and Cameron, David and Crouch, Matthew and De Maria, Riccardo and Field, Laurence and Giovannozzi, M. and Hermes, Pascal and Høimyr, Nils and Kaltchev, Dobrin and Karastathis, Nikos and Luzzi, Cinzia and Maclean, Ewen and Mcintosh, Eric and Mereghetti, Alessio and Molson, James and Nosochkov, Yuri and Pieloni, Tatiana and Reid, Ivan and Zacharov, Igor},
year = {2017},
month = {12},
pages = {},
title = {{LHC@Home: a BOINC-based volunteer computing infrastructure for physics studies at CERN}},
volume = {7},
journal = {Open Engineering},
doi = {10.1515/eng-2017-0042}
}

@article{seti_at_home,
    author = {Anderson, David and Cobb, Jeff and Korpela, Eric and Lebofsky, Matt and Werthimer, Dan},
    year = {2002},
    month = {11},
    pages = {56-61},
    title = {{SETI@home}: An Experiment in Public-Resource Computing},
    volume = {45},
    journal = {Commun. ACM},
    doi = {10.1145/581571.581573}
}

@article{clemens2021mlds,
  title={{MLDS}: A Dataset for Weight-Space Analysis of Neural Networks}, 
  author={John Clemens},
  year={2021},
  journal={arXiv preprint arXiv:2104.10555},
}

@Misc{vastai,
  note  = {\url{https://vast.ai}},
  title = {vast.ai},
}

@article{volunteer_dl_async,
author = {Kijsipongse, Ekasit and Piyatumrong, Apivadee and U-ruekolan, Suriya},
year = {2018},
month = {04},
pages = {},
title = {A hybrid GPU cluster and volunteer computing platform for scalable deep learning},
journal = {The Journal of Supercomputing},
doi = {10.1007/s11227-018-2375-9}
}

@article{atre2021distributed,
  title={Distributed Deep Learning Using Volunteer Computing-Like Paradigm}, 
  author={Medha Atre and Birendra Jha and Ashwini Rao},
  year={2021},
  journal={arXiv preprint arXiv:2103.08894},
}

@article{qmc_at_home,
	doi = {10.1088/1361-648x/aab9c3},
	url = {https://doi.org/10.1088/1361-648x/aab9c3},
	year = {2018},
	month = {04},
	publisher = {{IOP} Publishing},
	volume = {30},
	number = {19},
	pages = {195901},
	author = {Jeongnim Kim and Andrew D Baczewski and Todd D Beaudet and Anouar Benali and M Chandler Bennett and Mark A Berrill and Nick S Blunt and Edgar Josu{\'{e}} Landinez Borda and Michele Casula and David M Ceperley and Simone Chiesa and Bryan K Clark and Raymond C Clay and Kris T Delaney and Mark Dewing and Kenneth P Esler and Hongxia Hao and Olle Heinonen and Paul R C Kent and Jaron T Krogel and Ilkka Kylänpää and Ying Wai Li and M Graham Lopez and Ye Luo and Fionn D Malone and Richard M Martin and Amrita Mathuriya and Jeremy McMinis and Cody A Melton and Lubos Mitas and Miguel A Morales and Eric Neuscamman and William D Parker and Sergio D Pineda Flores and Nichols A Romero and Brenda M Rubenstein and Jacqueline A R Shea and Hyeondeok Shin and Luke Shulenburger and Andreas F Tillack and Joshua P Townsend and Norm M Tubman and Brett Van Der Goetz and Jordan E Vincent and D ChangMo Yang and Yubo Yang and Shuai Zhang and Luning Zhao},
	title = {{QMCPACK}: an open sourceab initioquantum Monte Carlo package for the electronic structure of atoms, molecules and solids},
	journal = {Journal of Physics: Condensed Matter},
}

@inproceedings{hivemind_dmoe,
 author = {Ryabinin, Max and Gusev, Anton},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {3659--3672},
 title = {Towards Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts},
 url = {https://proceedings.neurips.cc/paper/2020/file/25ddc0f8c9d3e22e03d3076f98d83cb2-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{moshpit,
  title={Moshpit {SGD}: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices}, 
  author={Max Ryabinin and Eduard Gorbunov and Vsevolod Plokhotnyuk and Gennady Pekhimenko},
  year={2021},
  journal={arXiv preprint arXiv:2103.03239},
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Volunteer computing section end %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{Trask2015ModelingOI,
  title={Modeling Order in Neural Word Embeddings at Scale},
  author={Andrew Trask and David Gilmore and Matthew Russell},
  booktitle={ICML},
  year={2015}
}

@article{imagenet_cvpr09,
  author={Jia Deng and Wei Dong and Richard Socher and Li-Jia Li and Kai Li and Li Fei-Fei},year = {2009},
month = {06},
pages = {248-255},
title = {{ImageNet}: a Large-Scale Hierarchical Image Database},
journal = {IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2009.5206848}
}

@incollection{alexnet,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@article{resnet,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={770-778}
}



@inproceedings{huang2019gpipe,
  title={{GPipe}: Efficient training of giant neural networks using pipeline parallelism},
  author={Yanping Huang and Youlong Cheng and Ankur Bapna and Orhan Firat and Mia Xu Chen and Dehao Chen and HyoukJoong Lee and Jiquan Ngiam and Quoc V. Le and Yonghui Wu and Zhifeng Chen},
  booktitle={Advances in Neural Information Processing Systems},
  pages={103--112},
  year={2019}
}
@article{megatron2,
      title={Efficient Large-Scale Language Model Training on {GPU} Clusters Using {Megatron-LM}}, 
      author={Deepak Narayanan and Mohammad Shoeybi and Jared Casper and Patrick LeGresley and Mostofa Patwary and Vijay Anand Korthikanti and Dmitri Vainbrand and Prethvi Kashinkunti and Julie Bernauer and Bryan Catanzaro and Amar Phanishayee and Matei Zaharia},
  journal={arXiv preprint arXiv:2104.04473},
  year={2021}
}


@article{jft300data,
  title={Tencent ML-Images: A Large-Scale Multi-Label Image Database for Visual Representation Learning},
  author={Baoyuan Wu and Weidong Chen and Yanbo Fan and Yong Zhang and Jinlong Hou and Jie Liu and Tong Zhang},
  journal={IEEE Access},
  year={2019},
  volume={7},
  pages={172683-172693}
}

@article{kolesnikovlarge,
  author    = {Alexander Kolesnikov and
               Lucas Beyer and
               Xiaohua Zhai and
               Joan Puigcerver and
               Jessica Yung and
               Sylvain Gelly and
               Neil Houlsby},
  title     = {Large Scale Learning of General Visual Representations for Transfer},
  journal   = {CoRR},
  volume    = {abs/1912.11370},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.11370},
  archivePrefix = {arXiv},
  eprint    = {1912.11370},
  timestamp = {Fri, 03 Jan 2020 16:10:45 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1912-11370},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bert,
  title={{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = 06,
    pages = "4171--4186",
  year={2019}
}

@article{roberta,
  title={{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019},
}

@article{shoeybi2019megatron,
  title={Megatron-{LM}: Training multi-billion parameter language models using {GPU} model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{mpich_rabenseifner,
author = {Thakur, Rajeev and Rabenseifner, Rolf and Gropp, William},
title = {Optimization of Collective Communication Operations in {MPICH}},
year = {2005},
volume = {19},
number = {1},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342005051521},
doi = {10.1177/1094342005051521},
journal = {Int. J. High Perform. Comput. Appl.},
month = 02,
pages = {49–66},
numpages = {18},
keywords = {reduction, MPI, message passing, Collective communication}
}


@inproceedings{adam2015atlas,
  title={ATLAS@ Home: harnessing volunteer computing for HEP},
  author={Adam-Bourdarios, C and Cameron, D and Filip{\v{c}}i{\v{c}}, A and Lancon, E and Wu, Wenjing and others},
  booktitle={Journal of Physics: Conference Series},
  volume={664},
  number={2},
  pages={022009},
  year={2015},
  organization={IOP Publishing}
}

@article{li2017case,
  title={A Case Study of IPv6 Network Performance: Packet Delay, Loss, and Reordering},
  author={Li, Fuliang and Wang, Xingwei and Pan, Tian and Yang, Jiahai},
  journal={Mathematical Problems in Engineering},
  volume={2017},
  year={2017},
  publisher={Hindawi}
}

@article{Sun2019OptimizingNP,
  title={Optimizing Network Performance for Distributed DNN Training on GPU Clusters: ImageNet/AlexNet Training in 1.5 Minutes},
  author={Peng Sun and Wansen Feng and Ruobing Han and Shengen Yan and Yonggang Wen},
  journal={ArXiv},
  year={2019},
  volume={abs/1902.06855}
}

@inproceedings{recht2011hogwild,
  title={Hogwild: A lock-free approach to parallelizing stochastic gradient descent},
  author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
  booktitle={Advances in neural information processing systems},
  pages={693--701},
  year={2011}
}

@article{zhang2015staleness,
  title={Staleness-aware async-sgd for distributed deep learning},
  author={Zhang, Wei and Gupta, Suyog and Lian, Xiangru and Liu, Ji},
  journal={arXiv preprint arXiv:1511.05950},
  year={2015}
}

@misc{lc0,
  author = {Pascutto, Gian-Carlo and Linscott, Gary},
  title = {Leela Chess Zero},
  howpublished = {\url{lczero.org}},
  note = {Accessed: 2021-05-20},
  year = {2019},
} 

@INPROCEEDINGS{vc_evolve,
  author={T. {Desell}},
  booktitle={2017 IEEE 13th International Conference on e-Science (e-Science)}, 
  title={Developing a Volunteer Computing Project to Evolve Convolutional Neural Networks and Their Hyperparameters}, 
  year={2017},
  volume={},
  number={},
  pages={19-28},}


@article{moe_first,
author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
title = {Adaptive Mixtures of Local Experts},
year = {1991},
issue_date = {March 1991},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {3},
number = {1},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1991.3.1.79},
doi = {10.1162/neco.1991.3.1.79},
journal = {Neural Computation},
month = mar,
pages = {79–87},
numpages = {9}
}

@article{eigen2013learning,
  title={Learning factored representations in a deep mixture of experts},
  author={Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1312.4314},
  year={2013}
}

@article{jordan1994hierarchical,
  title={Hierarchical mixtures of experts and the EM algorithm},
  author={Jordan, Michael I and Jacobs, Robert A},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={181--214},
  year={1994},
  publisher={MIT Press}
}

@inproceedings{yao2009hierarchical,
  title={Hierarchical mixture of classification experts uncovers interactions between brain regions},
  author={Yao, Bangpeng and Walther, Dirk and Beck, Diane and Fei-Fei, Li},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2178--2186},
  year={2009}
}

@inproceedings{rasmussen2002infinite,
  title={Infinite mixtures of Gaussian process experts},
  author={Rasmussen, Carl E and Ghahramani, Zoubin},
  booktitle={Advances in neural information processing systems},
  pages={881--888},
  year={2002}
}


@inproceedings{moe_lifelong,
author = {Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
year = {2017},
month = {07},
pages = {7120-7129},
title = {Expert Gate: Lifelong Learning with a Network of Experts},
doi = {10.1109/CVPR.2017.753}
}

@inproceedings{moe_svm,
  title={A parallel mixture of SVMs for very large scale problems},
  author={Collobert, Ronan and Bengio, Samy and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems},
  pages={633--640},
  year={2002}
}

@article{moe_dirichlet,
  title={Nonlinear models using Dirichlet process mixtures},
  author={Shahbaba, Babak and Neal, Radford},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={Aug},
  pages={1829--1850},
  year={2009}
}

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@incollection{pkm,
title = {Large Memory Layers with Product Keys},
author = {Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc\' Aurelio and Denoyer, Ludovic and Jegou, Herve},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\' Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8546--8557},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf}
}

@inproceedings{can,
  title={A scalable content-addressable network},
  author={Ratnasamy, Sylvia and Francis, Paul and Handley, Mark and Karp, Richard and Shenker, Scott},
  booktitle={Proceedings of the 2001 conference on Applications, technologies, architectures, and protocols for computer communications},
  pages={161--172},
  year={2001}
}


@article{chord,
  title={Looking up data in P2P systems},
  author={Balakrishnan, Hari and Kaashoek, M Frans and Karger, David and Morris, Robert and Stoica, Ion},
  journal={Communications of the ACM},
  volume={46},
  number={2},
  pages={43--48},
  year={2003},
  publisher={ACM New York, NY, USA}
}

@inproceedings{pastry,
  title={Pastry: Scalable, decentralized object location, and routing for large-scale peer-to-peer systems},
  author={Rowstron, Antony and Druschel, Peter},
  booktitle={IFIP/ACM International Conference on Distributed Systems Platforms and Open Distributed Processing},
  pages={329--350},
  year={2001},
  organization={Springer}
}

@article{tapestry,
author = {Zhao, Ben and Huang, Ling and Stribling, Jeremy and Rhea, Sean and Joseph, Anthony and Kubiatowicz, John},
year = {2003},
month = {07},
pages = {},
title = {Tapestry: A Resilient Global-Scale Overlay for Service Deployment},
volume = {22},
journal = {IEEE Journal on Selected Areas in Communications},
doi = {10.1109/JSAC.2003.818784}
}

@TechReport{tewari1998beyond,
  author = {Tewari, Renu and Dahlin, Michael and Vin, Harrick and Kay, John},
  title  = {Beyond hierarchies: Design considerations for distributed caching on the internet},
  school = {Citeseer},
}


@misc{i2p,
  title = {Invisible Internet Project (I2P) Project Overview}, 
  author = {jrandom (Pseudonym)}, 
  year = {2003}, 
  month = {August}, 
  howpublished = {\url{geti2p.net/_static/pdf/i2p_philosophy.pdf}},
  note = {Accessed: 2021-05-20}
}



@inproceedings{kademlia,
  title={Kademlia: A peer-to-peer information system based on the {XOR} metric},
  author={Maymounkov, Petar and Mazieres, David},
  booktitle={International Workshop on Peer-to-Peer Systems},
  pages={53--65},
  year={2002},
  organization={Springer}
}

@inproceedings{kaashoek2003koorde,
  title={Koorde: A simple degree-optimal distributed hash table},
  author={Kaashoek, M Frans and Karger, David R},
  booktitle={International Workshop on Peer-to-Peer Systems},
  pages={98-107},
  year={2003},
  organization={Springer}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@article{stale_gradients_can_win,
author = {Dutta, Sanghamitra and Joshi, Gauri and Ghosh, Soumyadip and Dube, Parijat and Nagpurkar, Priya},
year = {2018},
month = {03},
pages = {},
title = {Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD}
}

@article{gradient_checkpointing_dl,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{gradient_checkpointing_autograd,
  title={Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation},
  author={Griewank, Andreas and Walther, Andrea},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={26},
  number={1},
  pages={19--45},
  year={2000},
  publisher={ACM New York, NY, USA}
}

@article{kaplan2020scaling,
    title={Scaling Laws for Neural Language Models},
    author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
    year={2020},
    journal={arXiv preprint arXiv:2001.08361},
}

@inproceedings{zellers2019defending,
  title={Defending against neural fake news},
  author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9051--9062},
  year={2019}
}

@incollection{NIPS2019_8736,
title = {Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks},
author = {Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi (Viji) and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\' Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {4901--4910},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8736-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks.pdf}
}

@inproceedings{sybil_attacks_dht,
  title={Real-world sybil attacks in BitTorrent mainline DHT},
  author={Wang, Liang and Kangasharju, Jussi},
  booktitle={2012 IEEE Global Communications Conference (GLOBECOM)},
  pages={826--832},
  year={2012},
  organization={IEEE}
}

@article{sybil_nodes,
  title={Sybil nodes as a mitigation strategy against sybil attack},
  author={Trifa, Zied and Khemakhem, Maher},
  journal={Procedia Computer Science},
  volume={32},
  pages={1135--1140},
  year={2014},
  publisher={Elsevier}
}

@inproceedings{dos_resistance,
  title={A denial-of-service resistant DHT},
  author={Awerbuch, Baruch and Scheideler, Christian},
  booktitle={International Symposium on Distributed Computing},
  pages={33--47},
  year={2007},
  organization={Springer}
}

@article{urdaneta2011survey,
  title={A survey of DHT security techniques},
  author={Urdaneta, Guido and Pierre, Guillaume and Steen, Maarten Van},
  journal={ACM Computing Surveys (CSUR)},
  volume={43},
  number={2},
  pages={1--49},
  year={2011},
  publisher={ACM New York, NY, USA}
}

@article{bagdasaryan2018backdoor,
  title={How to backdoor federated learning},
  author={Bagdasaryan, Eugene and Veit, Andreas and Hua, Yiqing and Estrin, Deborah and Shmatikov, Vitaly},
  journal={arXiv preprint arXiv:1807.00459},
  year={2018}
}

@article{bhagoji2018analyzing,
  title={Analyzing federated learning through an adversarial lens},
  author={Bhagoji, Arjun Nitin and Chakraborty, Supriyo and Mittal, Prateek and Calo, Seraphin},
  journal={arXiv preprint arXiv:1811.12470},
  year={2018}
}

@article{olah2018building,
  title={The building blocks of interpretability},
  author={Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  journal={Distill},
  volume={3},
  number={3},
  pages={e10},
  year={2018}
}

@ARTICLE{seq2seqvis,
author={H. {Strobelt} and S. {Gehrmann} and M. {Behrisch} and A. {Perer} and H. {Pfister} and A. M. {Rush}},
journal={IEEE Transactions on Visualization and Computer Graphics},
title={Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models},
year={2019},
volume={25},
number={1},
pages={353-363},
keywords={data visualisation;learning (artificial intelligence);neural nets;program debugging;sequences;seq2seq-Vis;source sequence;target sequence;visual debugging tool;neural sequence-to-sequence models;blackbox pipeline;vector space;deep learning methods;visual analysis tool;Analytical models;Visualization;Tools;Predictive models;Machine learning;Data models;Atmosphere;Explainable AI;Visual Debugging;Visual Analytics;Machine Learning;Deep Learning;NLP},
doi={10.1109/TVCG.2018.2865044},
ISSN={2160-9306},
month={Jan},}

@article{carter2019activation,
  title={Activation atlas},
  author={Carter, Shan and Armstrong, Zan and Schubert, Ludwig and Johnson, Ian and Olah, Chris},
  journal={Distill},
  volume={4},
  number={3},
  pages={e15},
  year={2019}
}

@inproceedings{pipedream,
author = {Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R. and Ganger, Gregory R. and Gibbons, Phillip B. and Zaharia, Matei},
title = {{PipeDream}: Generalized Pipeline Parallelism for {DNN} Training},
year = {2019},
isbn = {9781450368735},
url = {https://doi.org/10.1145/3341301.3359646},
doi = {10.1145/3341301.3359646},
booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
pages = {1–15},
numpages = {15},
}

@article{pipemare,
  title={PipeMare: Asynchronous Pipeline Parallel DNN Training},
  author={Bowen Yang and Jian Zhang and Jonathan Li and Christopher R{\'e} and Christopher R. Aberger and Christopher De Sa},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.05124}
}

@article{valiant1990bridging,
  title={A bridging model for parallel computation},
  author={Valiant, Leslie G},
  journal={Communications of the ACM},
  volume={33},
  number={8},
  pages={103--111},
  year={1990},
  publisher={ACM New York, NY, USA}
}
@article{goyal2017accurate,
    title={Accurate, Large Minibatch {SGD}: Training {ImageNet} in 1 Hour},
    author={Priya Goyal and Piotr Dollár and Ross Girshick and Pieter Noordhuis and Lukasz Wesolowski and Aapo Kyrola and Andrew Tulloch and Yangqing Jia and Kaiming He},
    year={2017},
    journal={arXiv preprint arXiv:1706.02677},
}

@article{sukhov2016generating,
  title={Generating a function for network delay},
  author={Sukhov, Andrei M and Astrakhantseva, MA and Pervitsky, AK and Boldyrev, SS and Bukatov, AA},
  journal={Journal of High Speed Networks},
  volume={22},
  number={4},
  pages={321--333},
  year={2016},
  publisher={IOS Press}
}

@inproceedings{jaderberg2017decoupled,
  title={Decoupled neural interfaces using synthetic gradients},
  author={Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1627--1635},
  year={2017},
  organization={JMLR. org}
}

@misc{ma2019hsic,
    title={The HSIC Bottleneck: Deep Learning without Back-Propagation},
    author={Wan-Duo Kurt Ma and J. P. Lewis and W. Bastiaan Kleijn},
    year={2019},
    eprint={1908.01580},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{zero,
  title={{ZeRO}: Memory Optimization Towards Training A Trillion Parameter Models},
  author={Samyam Rajbhandari and Jeff Rasley and Olatunji Ruwase and Yuxiong He},
  booktitle={SC},
  year={2020}
}

@misc{tnlg,
  author = {Corby Rosset},
  title = {Turing-NLG: A 17-billion-parameter language model by Microsoft},
  howpublished = {https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/}},
  note = {Accessed: 2020-2-10}
}

@misc{gpt3cost,
  author = {Elliot Turner},
  note = {Estimate of GPT-3 training cost based on public cloud GPU/TPU cost models, from Elliot Turner's personal page (accessed on May 29, 2020)}
}

@misc{lambdabenchmarks,
  author = {Stephen Balaban , Chuan Li},
  title = {Deep Learning GPU benchmarks, Lambda Labs website, 2018/10/08}
}

@inproceedings{paszke2019pytorch,
  title={{PyTorch}: An imperative style, high-performance deep learning library},
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8024--8035},
  year={2019}
}


 @InProceedings{li2020acceleration, 
title = {Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization}, 
author = {Li, Zhize and Kovalev, Dmitry and Qian, Xun and Richtarik, Peter}, 
booktitle = {Proceedings of the 37th International Conference on Machine Learning}, 
pages = {5895--5904}, 
year = {2020}, 
volume = {119}, 
series = {Proceedings of Machine Learning Research}, month = {07},
} 
 @inproceedings{
koloskova2020decentralized,
title={Decentralized Deep Learning with Arbitrary Communication Compression},
author={Anastasia Koloskova and Tao Lin and Sebastian U Stich and Martin Jaggi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkgGCkrKvH}
}

@article{wagma,
   title={Breaking (Global) Barriers in Parallel Stochastic Optimization with Wait-Avoiding Group Averaging},
   ISSN={2161-9883},
   url={http://dx.doi.org/10.1109/TPDS.2020.3040606},
   DOI={10.1109/tpds.2020.3040606},
   journal={IEEE Transactions on Parallel and Distributed Systems},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Li, Shigang and Ben-Nun, Tal and Nadiradze, Giorgi and Digirolamo, Salvatore and Dryden, Nikoli and Alistarh, Dan and Hoefler, Torsten},
   year={2020},
   pages={1–1}
}



@inproceedings{ps,
  title={Scaling Distributed Machine Learning with the Parameter Server},
  author={Mu Li and D. Andersen and J. Park and Alex Smola and Amr Ahmed and V. Josifovski and J. Long and E. Shekita and Bor-Yiing Su},
  booktitle={Proceedings of the 2014 International Conference on Big Data Science and Computing},
  year={2014}
}

@article{zerooffload,
      title={{ZeRO}-Offload: Democratizing Billion-Scale Model Training}, 
      author={Jie Ren and Samyam Rajbhandari and Reza Yazdani Aminabadi and Olatunji Ruwase and Shuangyan Yang and Minjia Zhang and Dong Li and Yuxiong He},
      year={2021},
      journal ={arXiv preprint arXiv:2101.06840},
}

@inproceedings {byteps,
author = {Yimin Jiang and Yibo Zhu and Chang Lan and Bairen Yi and Yong Cui and Chuanxiong Guo},
title = {A Unified Architecture for Accelerating Distributed {DNN} Training in Heterogeneous {GPU/CPU} Clusters},
booktitle = {14th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {463--479},
url = {https://www.usenix.org/conference/osdi20/presentation/jiang},
month = 11,
}


 @inproceedings{
slowmo,
title={{SlowMo}: Improving Communication-Efficient Distributed {SGD} with Slow Momentum},
author={Jianyu Wang and Vinayak Tantia and Nicolas Ballas and Michael Rabbat},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkxJ8REYPH}
}

@article{mikami2019massively,
      title={Massively Distributed {SGD}: {ImageNet/ResNet-50} Training in a Flash}, 
      author={Hiroaki Mikami and Hisahiro Suganuma and Pongsakorn U-chupala and Yoshiki Tanaka and Yuichi Kageyama},
      year={2019},
      journal={arXiv preprint arXiv:1811.05233},
}

 @misc{pytorch_elastic, title={{PyTorch Elastic}}, howpublished={\url{https://pytorch.org/elastic}},
  note = {Accessed: 2021-05-20}
 } 
 
 @misc{elastic_horovod, title={{Elastic Horovod}}, howpublished={\url{ https://horovod.rtfd.io/en/stable/elastic_include.html}},
   note = {Accessed: 2021-05-20}
}
 


@InProceedings{Biggadike05natblaster:establishing,
  author = 		 {Andrew Biggadike and Daniel Ferullo and Geoffrey Wilson and Adrian Perrig},
  title = 		 {{NATBLASTER}: Establishing {TCP} Connections Between Hosts Behind {NATs}},
  booktitle = 	 {Proceedings of ACM SIGCOMM ASIA Workshop},
  year =		 2005,
}


@inproceedings{Oscar,
    title = "A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages",
    author = "Ortiz Su{\'a}rez, Pedro Javier  and
      Romary, Laurent  and
      Sagot, Beno{\^\i}t",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = 07,
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.acl-main.156",
    pages = "1703--1714",
}


@inproceedings{kudo2018subword,
  author    = {Taku Kudo},
  title     = {Subword Regularization: Improving Neural Network Translation Models
               with Multiple Subword Candidates},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational
               Linguistics},
  pages     = {66-75},
  year      = {2018},
  doi       = {10.18653/v1/P18-1007},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{feyzmahdavian2016asynchronous,
  title={An asynchronous mini-batch algorithm for regularized stochastic optimization},
  author={Feyzmahdavian, Hamid Reza and Aytekin, Arda and Johansson, Mikael},
  journal={IEEE Transactions on Automatic Control},
  volume={61},
  number={12},
  pages={3740--3754},
  year={2016},
  publisher={IEEE}
}

@inproceedings{arjevani2020tight,
  title={A tight convergence analysis for stochastic gradient descent with delayed updates},
  author={Arjevani, Yossi and Shamir, Ohad and Srebro, Nathan},
  booktitle={Algorithmic Learning Theory},
  pages={111--132},
  year={2020},
  organization={PMLR}
}

@inproceedings{MLSYS2019_d09bf415,
 author = {Jayarajan, Anand and Wei, Jinliang and Gibson, Garth and Fedorova, Alexandra and Pekhimenko, Gennady},
 booktitle = {Proceedings of Machine Learning and Systems},
 editor = {A. Talwalkar and V. Smith and M. Zaharia},
 pages = {132--145},
 title = {Priority-based Parameter Propagation for Distributed DNN Training},
 url = {https://proceedings.mlsys.org/paper/2019/file/d09bf41544a3365a46c9077ebb5e35c3-Paper.pdf},
 volume = {1},
 year = {2019}
}


@inproceedings{agarwal2011distributed,
  title={Distributed delayed stochastic optimization},
  author={Agarwal, Alekh and Duchi, John C},
  booktitle={Proceedings of the 24th International Conference on Neural Information Processing Systems},
  pages={873--881},
  year={2011}
}

@inproceedings{mishchenko2018delay,
  title={A delay-tolerant proximal-gradient algorithm for distributed learning},
  author={Mishchenko, Konstantin and Iutzeler, Franck and Malick, J{\'e}r{\^o}me and Amini, Massih-Reza},
  booktitle={International Conference on Machine Learning},
  pages={3587--3595},
  year={2018},
  organization={PMLR}
}

@article{peng2016arock,
  title={Arock: an algorithmic framework for asynchronous parallel coordinate updates},
  author={Peng, Zhimin and Xu, Yangyang and Yan, Ming and Yin, Wotao},
  journal={SIAM Journal on Scientific Computing},
  volume={38},
  number={5},
  pages={A2851--A2879},
  year={2016},
  publisher={SIAM}
}

@inproceedings{leblond2017asaga,
  title={ASAGA: asynchronous parallel SAGA},
  author={Leblond, R{\'e}mi and Pedregosa, Fabian and Lacoste-Julien, Simon},
  booktitle={Artificial Intelligence and Statistics},
  pages={46--54},
  year={2017},
  organization={PMLR}
}

@inproceedings{zhao2016fast,
  title={Fast asynchronous parallel stochastic gradient descent: A lock-free approach with convergence guarantee},
  author={Zhao, Shen-Yi and Li, Wu-Jun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  number={1},
  year={2016}
}

@article{assran2020advances,
  title={Advances in Asynchronous Parallel and Distributed Optimization},
  author={Assran, Mahmoud and Aytekin, Arda and Feyzmahdavian, Hamid Reza and Johansson, Mikael and Rabbat, Michael G},
  journal={Proceedings of the IEEE},
  volume={108},
  number={11},
  pages={2013--2031},
  year={2020},
  publisher={IEEE}
}

@inproceedings{basu2019qsparse,
  title={Qsparse-local-{SGD}: Distributed {SGD} with Quantization, Sparsification and Local Computations},
  author={Basu, Debraj and Data, Deepesh and Karakus, Can and Diggavi, Suhas},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14668--14679},
  year={2019}
}

@article{yuan2020federated_comp,
  title={Federated Composite Optimization},
  author={Yuan, Honglin and Zaheer, Manzil and Reddi, Sashank},
  journal={arXiv preprint arXiv:2011.08474},
  year={2020}
}

@article{yuan2020federated,
  title={Federated Accelerated Stochastic Gradient Descent},
  author={Yuan, Honglin and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{woodworth2020minibatch,
  title={Minibatch vs local {SGD} for heterogeneous distributed learning},
  author={Woodworth, Blake and Patel, Kumar Kshitij and Srebro, Nathan},
  journal={arXiv preprint arXiv:2006.04735},
  year={2020}
}

@Article{LinSPJ2018local,
  author  = {Tao Lin and Sebastian Urban Stich and Kumar Kshitij Patel and Martin Jaggi},
  title   = {Don't Use Large Mini-Batches, Use Local {SGD}},
  journal = {ICLR},
  year    = {2020},
  pages   = {arXiv:1808.07217},
  url     = {https://arxiv.org/abs/1808.07217},
}

@inproceedings{Stich18local,
  author  = {Sebastian Urban Stich},
  title   = {Local {SGD} Converges Fast and Communicates Little},
  booktitle = {International Conference on Learning Representations},
  year    = {2019},
}

@article{konevcny2016federated,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}

@article{kovalev2020linearly,
  title={A Linearly Convergent Algorithm for Decentralized Optimization: Sending Less Bits for Free!},
  author={Kovalev, Dmitry and Koloskova, Anastasia and Jaggi, Martin and Richtarik, Peter and Stich, Sebastian U},
  journal={arXiv preprint arXiv:2011.01697},
  year={2020}
}

@article{reisizadeh2019exact,
  title={An exact quantized decentralized gradient descent algorithm},
  author={Reisizadeh, Amirhossein and Mokhtari, Aryan and Hassani, Hamed and Pedarsani, Ramtin},
  journal={IEEE Transactions on Signal Processing},
  volume={67},
  number={19},
  pages={4934--4947},
  year={2019},
  publisher={IEEE}
}

@article{qian2020error,
  title={Error Compensated Distributed {SGD} Can Be Accelerated},
  author={Qian, Xun and Richt{\'a}rik, Peter and Zhang, Tong},
  journal={arXiv preprint arXiv:2010.00091},
  year={2020}
}

@inproceedings{stich2018sparsified,
  title={Sparsified SGD with memory},
  author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={4452--4463},
  year={2018}
}

@article{das2020improved,
  title={Improved Convergence Rates for Non-Convex Federated Learning with Compression},
  author={Das, Rudrajit and Hashemi, Abolfazl and Sanghavi, Sujay and Dhillon, Inderjit S},
  journal={arXiv preprint arXiv:2012.04061},
  year={2020}
}

@article{haddadpour2020federated,
  title={Federated learning with compression: Unified analysis and sharp guarantees},
  author={Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mokhtari, Aryan and Mahdavi, Mehrdad},
  journal={arXiv preprint arXiv:2007.01154},
  year={2020}
}

@article{li2020unified,
  title={A unified analysis of stochastic gradient methods for nonconvex federated optimization},
  author={Li, Zhize and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2006.07013},
  year={2020}
}

@article{philippenko2020artemis,
  title={Artemis: tight convergence guarantees for bidirectional compression in federated learning},
  author={Philippenko, Constantin and Dieuleveut, Aymeric},
  journal={arXiv preprint arXiv:2006.14591},
  year={2020}
}

@article{gorbunov2020linearly,
  title={Linearly Converging Error Compensated SGD},
  author={Gorbunov, Eduard and Kovalev, Dmitry and Makarenko, Dmitry and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{horvath2019stochastic,
  title={Stochastic distributed learning with gradient quantization and variance reduction},
  author={Horv{\'a}th, Samuel and Kovalev, Dmitry and Mishchenko, Konstantin and Stich, Sebastian and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1904.05115},
  year={2019}
}

@article{mishchenko2019distributed,
  title={Distributed learning with compressed gradient differences},
  author={Mishchenko, Konstantin and Gorbunov, Eduard and Tak{\'a}{\v{c}}, Martin and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1901.09269},
  year={2019}
}

@inproceedings{wen2017terngrad,
  title={TernGrad: ternary gradients to reduce communication in distributed deep learning},
  author={Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={1508--1518},
  year={2017}
}

@article{beznosikov2020biased,
  title={On biased compression for distributed learning},
  author={Beznosikov, Aleksandr and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter and Safaryan, Mher},
  journal={arXiv preprint arXiv:2002.12410},
  year={2020}
}

@article{horvath2019natural,
  title={Natural compression for distributed deep learning},
  author={Horvath, Samuel and Ho, Chen-Yu and Horvath, Ludovit and Sahu, Atal Narayan and Canini, Marco and Richtarik, Peter},
  journal={arXiv preprint arXiv:1905.10988},
  year={2019}
}

@inproceedings{alistarh2017qsgd,
  title={QSGD: communication-efficient SGD via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry Z and Tomioka, Ryota and Vojnovic, Milan},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={1707--1718},
  year={2017}
}

@inproceedings{suresh2017distributed,
  title={Distributed mean estimation with limited communication},
  author={Suresh, Ananda Theertha and Felix, X Yu and Kumar, Sanjiv and McMahan, H Brendan},
  booktitle={International Conference on Machine Learning},
  pages={3329--3337},
  year={2017},
  organization={PMLR}
}

@inproceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Fifteenth Annual Conference of the International Speech Communication Association},
  year={2014}
}

@article{li2019communication,
  title={Communication efficient decentralized training with multiple local updates},
  author={Li, Xiang and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1910.09126},
  volume={5},
  year={2019}
}

@inproceedings{karimireddy2020scaffold,
  title={SCAFFOLD: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International Conference on Machine Learning},
  pages={5132--5143},
  year={2020},
  organization={PMLR}
}

@inproceedings{koloskova2020unified,
  title={A unified theory of decentralized SGD with changing topology and local updates},
  author={Koloskova, Anastasia and Loizou, Nicolas and Boreiri, Sadra and Jaggi, Martin and Stich, Sebastian},
  booktitle={International Conference on Machine Learning},
  pages={5381--5393},
  year={2020},
  organization={PMLR}
}

@inproceedings{khaled2020tighter,
  title={Tighter theory for local {SGD} on identical and heterogeneous data},
  author={Khaled, Ahmed and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4519--4529},
  year={2020},
  organization={PMLR}
}

@inproceedings{woodworth2020local,
  title={Is local {SGD} better than minibatch {SGD}?},
  author={Woodworth, Blake and Patel, Kumar Kshitij and Stich, Sebastian and Dai, Zhen and Bullins, Brian and Mcmahan, Brendan and Shamir, Ohad and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={10334--10343},
  year={2020},
  organization={PMLR}
}

@article{gorbunov2020local,
  title={Local {SGD}: Unified theory and new efficient methods},
  author={Gorbunov, Eduard and Hanzely, Filip and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2011.02828},
  year={2020}
}

@inproceedings{gower2019sgd,
  title={{SGD}: General analysis and improved rates},
  author={Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  pages={5200--5209},
  year={2019},
  organization={PMLR}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}

@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}



@inproceedings{scaman2017optimal,
  title={Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks},
  author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Lee, Yin Tat and Massouli{\'e}, Laurent},
  booktitle={International Conference on Machine Learning},
  pages={3027--3036},
  year={2017}
}

@inproceedings{scaman2018optimal,
  title={Optimal algorithms for non-smooth distributed optimization in networks},
  author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Massouli{\'e}, Laurent and Lee, Yin Tat},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2740--2749},
  year={2018}
}


@article{xiao2004fast,
  title={Fast linear iterations for distributed averaging},
  author={Xiao, Lin and Boyd, Stephen},
  journal={Systems \& Control Letters},
  volume={53},
  number={1},
  pages={65--78},
  year={2004},
  publisher={Elsevier}
}

@article{boyd2006randomized,
  title={Randomized gossip algorithms},
  author={Boyd, Stephen and Ghosh, Arpita and Prabhakar, Balaji and Shah, Devavrat},
  journal={IEEE transactions on information theory},
  volume={52},
  number={6},
  pages={2508--2530},
  year={2006},
  publisher={IEEE}
}

@article{merris1994laplacian,
  title={Laplacian matrices of graphs: a survey},
  author={Merris, Russell},
  journal={Linear algebra and its applications},
  volume={197},
  pages={143--176},
  year={1994},
  publisher={Elsevier}
}

@article{uribe2020dual,
  title={A dual approach for optimal algorithms in distributed optimization over networks},
  author={Uribe, C{\'e}sar A and Lee, Soomin and Gasnikov, Alexander and Nedi{\'c}, Angelia},
  journal={Optimization Methods and Software},
  pages={1--40},
  year={2020},
  publisher={Taylor \& Francis}
}

@TechReport{tsitsiklis1984problems,
  author = {Tsitsiklis, John Nikolas},
  title  = {Problems in decentralized decision making and computation.},
  year   = {1984},
  school = {Massachusetts Inst of Tech Cambridge Lab for Information and Decision Systems},
}

@article{scaman2019optimal,
  title={Optimal convergence rates for convex distributed optimization in networks},
  author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Lee, Yin and Massouli{\'e}, Laurent},
  journal={Journal of Machine Learning Research},
  volume={20},
  pages={1--31},
  year={2019}
}

@article{xu2020distributed,
  title={Distributed Algorithms for Composite Optimization: Unified and Tight Convergence Analysis},
  author={Xu, Jinming and Tian, Ye and Sun, Ying and Scutari, Gesualdo},
  journal={arXiv preprint arXiv:2002.11534},
  year={2020}
}

@article{kovalev2020optimal,
  title={Optimal and practical algorithms for smooth and strongly convex decentralized optimization},
  author={Kovalev, Dmitry and Salim, Adil and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{arjevani2015communication,
  title={Communication complexity of distributed convex learning and optimization},
  author={Arjevani, Yossi and Shamir, Ohad},
  journal={Advances in neural information processing systems},
  volume={28},
  pages={1756--1764},
  year={2015}
}

@article{fallah2019robust,
  title={Robust Distributed Accelerated Stochastic Gradient Methods for Multi-Agent Networks},
  author={Fallah, Alireza and Gurbuzbalaban, Mert and Ozdaglar, Asu and Simsekli, Umut and Zhu, Lingjiong},
  journal={arXiv preprint arXiv:1910.08701},
  year={2019}
}

@article{nedic2014distributed,
  title={Distributed optimization over time-varying directed graphs},
  author={Nedi{\'c}, Angelia and Olshevsky, Alex},
  journal={IEEE Transactions on Automatic Control},
  volume={60},
  number={3},
  pages={601--615},
  year={2014},
  publisher={IEEE}
}

@article{nedic2016stochastic,
  title={Stochastic gradient-push for strongly convex functions on time-varying directed graphs},
  author={Nedi{\'c}, Angelia and Olshevsky, Alex},
  journal={IEEE Transactions on Automatic Control},
  volume={61},
  number={12},
  pages={3936--3947},
  year={2016},
  publisher={IEEE}
}

@article{nedic2018network,
  title={Network topology and communication-computation tradeoffs in decentralized optimization},
  author={Nedi{\'c}, Angelia and Olshevsky, Alex and Rabbat, Michael G},
  journal={Proceedings of the IEEE},
  volume={106},
  number={5},
  pages={953--976},
  year={2018},
  publisher={IEEE}
}

@article{rogozin2019projected,
  title={Projected gradient method for decentralized optimization over time-varying networks},
  author={Rogozin, Alexander and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:1911.08527},
  year={2019}
}

@inproceedings{ram2009asynchronous,
  title={Asynchronous gossip algorithms for stochastic optimization},
  author={Ram, S Sundhar and Nedi{\'c}, A and Veeravalli, Venugopal V},
  booktitle={Proceedings of the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference},
  pages={3581--3586},
  year={2009},
  organization={IEEE}
}

@article{yan2012distributed,
  title={Distributed autonomous online learning: Regrets and intrinsic privacy-preserving properties},
  author={Yan, Feng and Sundaram, Shreyas and Vishwanathan, SVN and Qi, Yuan},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={25},
  number={11},
  pages={2483--2493},
  year={2012},
  publisher={IEEE}
}

@article{yuan2016convergence,
  title={On the convergence of decentralized gradient descent},
  author={Yuan, Kun and Ling, Qing and Yin, Wotao},
  journal={SIAM Journal on Optimization},
  volume={26},
  number={3},
  pages={1835--1854},
  year={2016},
  publisher={SIAM}
}

@inproceedings{squad,
  title={SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
  booktitle={EMNLP},
  year={2016}
}

@misc{aldous2002reversible,
  title={Reversible markov chains and random walks on graphs, 2002. Unfinished monograph, recompiled 2014},
  author={Aldous, David and Fill, James Allen},
  year={2002}
}

@misc{minimax,
  title={SA305 – Linear Programming, Lesson 32. Maximin and Minimax Objectives},
  author={David Phillips},
  year={2015}
}

@incollection{andersen,
	doi = {10.1007/978-1-4757-3216-0_8},
	url = {https://doi.org/10.1007%2F978-1-4757-3216-0_8},
	year = 2000,
	publisher = {Springer {US}},
	pages = {197--232},
	author = {Erling D. Andersen and Knud D. Andersen},
	title = {The {MOSEK} Interior Point Optimizer for Linear Programming: An Implementation of the Homogeneous Algorithm},
	booktitle = {Applied Optimization}
}

@MISC{Torrey_transferlearning,
    author = {Lisa Torrey and Jude Shavlik},
    title = {Transfer Learning},
    year = {}
}


@inproceedings{jft-300m,
title	= {Revisiting Unreasonable Effectiveness of Data in Deep Learning Era},
author	= {Chen Sun and Abhinav Shrivastava and Saurabh Singh and Abhinav Gupta},
year	= {2017},
URL	= {https://arxiv.org/abs/1707.02968},
booktitle	= {ICCV}
}

@inproceedings{Kolesnikov2020BigT,
  title={Big Transfer (BiT): General Visual Representation Learning},
  author={Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Joan Puigcerver and Jessica Yung and S. Gelly and N. Houlsby},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{xlnet,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Z. Yang and Zihang Dai and Yiming Yang and J. Carbonell and R. Salakhutdinov and Quoc V. Le},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{albert,
  title={{ALBERT}: A Lite {BERT} for Self-supervised Learning of Language Representations},
  author={Zhen-Zhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  booktitle={International Conference on Learning Representations},
year={2020},
}


@inproceedings{gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1877--1901},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}



@article{ringallreduce,
author = {Patarasuk, Pitch and Yuan, Xin},
year = {2009},
month = {02},
pages = {117-124},
title = {Bandwidth optimal all-reduce algorithms for clusters of workstations},
volume = {69},
journal = {Journal of Parallel and Distributed Computing},
doi = {10.1016/j.jpdc.2008.09.002}
}



@article{torus_allreduce,
author = {Sack, Paul and Gropp, William},
title = {Collective Algorithms for Multiported Torus Networks},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2329-4949},
url = {https://doi.org/10.1145/2686882},
doi = {10.1145/2686882},
abstract = {Modern supercomputers with torus networks allow each node to simultaneously pass messages on all of its links. However, most collective algorithms are designed to only use one link at a time. In this work, we present novel multiported algorithms for the scatter, gather, all-gather, and reduce-scatter operations. Our algorithms can be combined to create multiported reduce, all-reduce, and broadcast algorithms. Several of these algorithms involve a new technique where we relax the MPI message-ordering constraints to achieve high performance and restore the correctordering using an additional stage of redundant communication.According to our models, on an n-dimensional torus, our algorithms should allow for nearly a 2n-fold improvement in communication performance compared to known, single-ported torus algorithms. In practice, we have achieved nearly 6x better performance on a 32k-node 3-dimensional torus.},
journal = {ACM Trans. Parallel Comput.},
month = feb,
articleno = {12},
numpages = {33},
keywords = {collective algorithms, Message-passing}
}

@misc{speedtest,
  title = {Speedtest Global Index for Fixed Broadband},
  note = {\url{https://www.speedtest.net/global-index} (accessed on 11.08.2020, bandwidth for top countries and general trend)},
  
}

@inproceedings{survey_distributed2,
author = {Alqahtani, Salem and Demirbas, Murat},
year = {2019},
month = {07},
pages = {},
title = {Performance Analysis and Comparison of Distributed Machine Learning Systems}
}

@inproceedings{sharded_ps_first,
 author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc\textquotesingle aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc and Ng, Andrew},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1223--1231},
 title = {Large Scale Distributed Deep Networks},
 url = {https://proceedings.neurips.cc/paper/2012/file/6aca97005c68f1206823815f66102863-Paper.pdf},
 volume = {25},
 year = {2012}
}


@InProceedings{pmlr-v97-koloskova19a, title = {Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication}, author = {Koloskova, Anastasia and Stich, Sebastian and Jaggi, Martin}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {3478--3487}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, }

@inproceedings{localsgd_first,
 author = {Zinkevich, Martin and Weimer, Markus and Li, Lihong and Smola, Alex},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {2595--2603},
 title = {Parallelized Stochastic Gradient Descent},
 url = {https://proceedings.neurips.cc/paper/2010/file/abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf},
 volume = {23},
 year = {2010}
}

@article{survey_distributed,
author = {Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S.},
title = {A Survey on Distributed Machine Learning},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3377454},
doi = {10.1145/3377454},
abstract = {The demand for artificial intelligence has grown significantly over the past decade, and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges: first and foremost, the efficient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the field by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {30},
numpages = {33},
keywords = {Distributed machine learning, distributed systems}
}

@inproceedings{proteus,
author = {Harlap, Aaron and Tumanov, Alexey and Chung, Andrew and Ganger, Gregory R. and Gibbons, Phillip B.},
title = {Proteus: Agile ML Elasticity through Tiered Reliability in Dynamic Resource Markets},
year = {2017},
isbn = {9781450349383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3064176.3064182},
doi = {10.1145/3064176.3064182},
abstract = {Many shared computing clusters allow users to utilize excess idle resources at lower cost or priority, with the proviso that some or all may be taken away at any time. But, exploiting such dynamic resource availability and the often fluctuating markets for them requires agile elasticity and effective acquisition strategies. Proteus aggressively exploits such transient revocable resources to do machine learning (ML) cheaper and/or faster. Its parameter server framework, AgileML, efficiently adapts to bulk additions and revocations of transient machines, through a novel 3-stage active-backup approach, with minimal use of more costly non-transient resources. Its BidBrain component adaptively allocates resources from multiple EC2 spot markets to minimize average cost per work as transient resource availability and cost change over time. Our evaluations show that Proteus reduces cost by 85% relative to non-transient pricing, and by 43% relative to previous approaches, while simultaneously reducing runtimes by up to 37%.},
booktitle = {Proceedings of the Twelfth European Conference on Computer Systems},
pages = {589–604},
numpages = {16},
location = {Belgrade, Serbia},
series = {EuroSys '17}
}

@article{lin2020multinode,
      title={Multi-node {BERT}-pretraining: Cost-efficient Approach}, 
      author={Jiahuang Lin and Xin Li and Gennady Pekhimenko},
      year={2020},
      journal={arXiv preprint arXiv:2008.00177},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{Lepikhin2020GShardSG,
  title={GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  author={Dmitry Lepikhin and H. Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Y. Huang and M. Krikun and Noam Shazeer and Z. Chen},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.16668}
}


@inproceedings{mcmahan2017communication,
  title={Communication-Efficient Learning of Deep Networks from Decentralized Data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial Intelligence and Statistics},
  pages={1273--1282},
  year={2017}
}

@InProceedings{sgp, 
title = {Stochastic Gradient Push for Distributed Deep Learning}, 
author = {Assran, Mahmoud and Loizou, Nicolas and Ballas, Nicolas and Rabbat, Mike}, 
booktitle = {Proceedings of the 36th International Conference on Machine Learning}, 
pages = {344--353}, 
year = {2019}, 
volume = {97}, 
series = {Proceedings of Machine Learning Research},
month = {06}, 
}

@InProceedings{zeno,
  author    = {Xie, Cong and Koyejo, Sanmi and Gupta, Indranil},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  title     = {Zeno: Distributed Stochastic Gradient Descent with Suspicion-based Fault-tolerance},
  year      = {2019},
  editor    = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  month     = {09--15 Jun},
  pages     = {6893--6901},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  abstract  = {We present Zeno, a technique to make distributed machine learning, particularly Stochastic Gradient Descent (SGD), tolerant to an arbitrary number of faulty workers. Zeno generalizes previous results that assumed a majority of non-faulty nodes; we need assume only one non-faulty worker. Our key idea is to suspect workers that are potentially defective. Since this is likely to lead to false positives, we use a ranking-based preference mechanism. We prove the convergence of SGD for non-convex problems under these scenarios. Experimental results show that Zeno outperforms existing approaches.},
  pdf       = {http://proceedings.mlr.press/v97/xie19b/xie19b.pdf},
  url       = {http://proceedings.mlr.press/v97/xie19b.html},
}


@article{puigcerver2020scalable,
  title={Scalable transfer learning with expert models},
  author={Puigcerver, Joan and Riquelme, Carlos and Mustafa, Basil and Renggli, Cedric and Pinto, Andr{\'e} Susano and Gelly, Sylvain and Keysers, Daniel and Houlsby, Neil},
  journal={arXiv preprint arXiv:2009.13239},
  year={2020}
}

@article{lp_relaxation_largescale,
author = {Ralph E. Gomory},
title = {{Outline of an algorithm for integer solutions to linear programs}},
volume = {64},
journal = {Bulletin of the American Mathematical Society},
number = {5},
publisher = {American Mathematical Society},
pages = {275 -- 278},
year = {1958},
doi = {bams/1183522679},
URL = {https://doi.org/}
}


@inproceedings{secure_aggregation,
title	= {Practical Secure Aggregation  for Privacy-Preserving Machine Learning},
author	= {Aaron Segal and Antonio Marcedone and Benjamin Kreuter and Daniel Ramage and H. Brendan McMahan and Karn Seth and K. A. Bonawitz and Sarvar Patel and Vladimir Ivanov},
year	= {2017},
URL	= {https://eprint.iacr.org/2017/281.pdf},
booktitle	= {CCS}
}

@misc{fed_google1,
title	= {Federated Learning for Mobile Keyboard Prediction},
author	= {Andrew Hard and Chloé M Kiddon and Daniel Ramage and Francoise Beaufays and Hubert Eichner and Kanishka Rao and Rajiv Mathews and Sean Augenstein},
year	= {2018},
URL	= {https://arxiv.org/abs/1811.03604}
}

@misc{fed_google2,
title	= {Applied Federated Learning: Improving Google Keyboard Query Suggestions},
author	= {Timothy Yang and Galen Andrew and Hubert Eichner and Haicheng Sun and Wei Li and Nicholas Kong and Daniel Ramage and Françoise Beaufays},
year	= {2018},
URL	= {https://arxiv.org/abs/1812.02903}
}





﻿@Article{fed_intel,
author={Sheller, Micah J.
and Edwards, Brandon
and Reina, G. Anthony
and Martin, Jason
and Pati, Sarthak
and Kotrotsou, Aikaterini
and Milchenko, Mikhail
and Xu, Weilin
and Marcus, Daniel
and Colen, Rivka R.
and Bakas, Spyridon},
title={Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient data},
journal={Scientific Reports},
year={2020},
month={Jul},
day={28},
volume={10},
number={1},
pages={12598},
abstract={Several studies underscore the potential of deep learning in identifying complex patterns, leading to diagnostic and prognostic biomarkers. Identifying sufficiently large and diverse datasets, required for training, is a significant challenge in medicine and can rarely be found in individual institutions. Multi-institutional collaborations based on centrally-shared patient data face privacy and ownership challenges. Federated learning is a novel paradigm for data-private multi-institutional collaborations, where model-learning leverages all available data without sharing data between institutions, by distributing the model-training to the data-owners and aggregating their results. We show that federated learning among 10 institutions results in models reaching 99{\%} of the model quality achieved with centralized data, and evaluate generalizability on data from institutions outside the federation. We further investigate the effects of data distribution across collaborating institutions on model quality and learning patterns, indicating that increased access to data through data private multi-institutional collaborations can benefit model quality more than the errors introduced by the collaborative method. Finally, we compare with other collaborative-learning approaches demonstrating the superiority of federated learning, and discuss practical implementation considerations. Clinical adoption of federated learning is expected to lead to models trained on datasets of unprecedented size, hence have a catalytic impact towards precision/personalized medicine.},
issn={2045-2322},
doi={10.1038/s41598-020-69250-1},
url={https://doi.org/10.1038/s41598-020-69250-1}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019},
  journal = {}
}

@inbook{fed_nvidia,
title = "Privacy-Preserving Federated Brain Tumour Segmentation",
abstract = "Due to medical data privacy regulations, it is often infeasible to collect and share patient data in a centralised data lake. This poses challenges for training machine learning algorithms, such as deep convolutional networks, which often require large numbers of diverse training examples. Federated learning sidesteps this difficulty by bringing code to the patient data owners and only sharing intermediate model training updates among them. Although a high-accuracy model could be achieved by appropriately aggregating these model updates, the model shared could indirectly leak the local training examples. In this paper, we investigate the feasibility of applying differential-privacy techniques to protect the patient data in a federated learning setup. We implement and evaluate practical federated learning systems for brain tumour segmentation on the BraTS dataset. The experimental results show that there is a trade-off between model performance and privacy protection costs.",
author = "Wenqi Li and Fausto Milletar{\`i} and Daguang Xu and Nicola Rieke and Jonny Hancox and Wentao Zhu and Maximilian Baust and Yan Cheng and S{\'e}bastien Ourselin and Cardoso, {M. Jorge} and Andrew Feng",
year = "2019",
month = jan,
day = "1",
doi = "10.1007/978-3-030-32692-0_16",
language = "English",
isbn = "9783030326913",
series = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
publisher = "SPRINGER",
pages = "133--141",
editor = "Heung-Il Suk and Mingxia Liu and Chunfeng Lian and Pingkun Yan",
booktitle = "Machine Learning in Medical Imaging - 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI 2019, Proceedings",
note = "10th International Workshop on Machine Learning in Medical Imaging, MLMI 2019 held in conjunction with the 22nd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2019 ; Conference date: 13-10-2019 Through 13-10-2019",
}


@misc{tnlg,
  author = {Corby Rosset},
  title = {Turing-NLG: A 17-billion-parameter language model by Microsoft},
  howpublished = {https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/}},
  note = {Accessed: 2020-2-10}
}

@inproceedings{dai2019transformer,
  title={Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2978--2988},
  year={2019}
}

@article{wikitext103,
  title={Pointer Sentinel Mixture Models},
  author={Stephen Merity and Caiming Xiong and James Bradbury and R. Socher},
  journal={arXiv preprint arXiv:1609.07843},
  year={2017},
}

@article{datasets,
      title={Datasets: A Community Library for Natural Language Processing},
      author={Quentin Lhoest and Albert Villanova del Moral and Yacine Jernite and Abhishek Thakur and Patrick von Platen and Suraj Patil and Julien Chaumond and Mariama Drame and Julien Plu and Lewis Tunstall and Joe Davison and Mario Šaško and Gunjan Chhablani and Bhavitvya Malik and Simon Brandeis and Teven Le Scao and Victor Sanh and Canwen Xu and Nicolas Patry and Angelina McMillan-Major and Philipp Schmid and Sylvain Gugger and Clément Delangue and Théo Matussière and Lysandre Debut and Stas Bekman and Pierric Cistac and Thibault Goehringer and Victor Mustar and François Lagunas and Alexander M. Rush and Thomas Wolf},
      year={2021},
      journal={arXiv preprint arxiv:2109.02846},
}



@article{wikitext2,
title= {Wikitext-2},
keywords= {fastai},
journal= {},
author= {Stephen Merity et al., 2016},
year= {},
url= {https://arxiv.org/abs/1609.07843},
license= {},
abstract= {A subset of Wikitext-103; useful for testing language model training on smaller datasets.},
superseded= {},
terms= {}
}


@misc{gpt3costlambda,
  author = {Chuan Li},
  title = {Demystifying GPT-3 Language Model: A Technical Overview},
  note = {"\url{https://lambdalabs.com/blog/demystifying-gpt-3}"},
  year = {2020},
}

@misc{nvidia_perf,
   author = {NVIDIA},
   title = {NVIDIA Data Center Deep Learning Product Performance},
   note = {"\url{https://developer.nvidia.com/deep-learning-performance-training-inference}", accessed at 2021.02.03}
}

@misc{dettmerswikitext2,
  author = {Tim Dettmers},
  note = {https://github.com/TimDettmers/transformer-xl/tree/wikitext2}
}

@article{mlperf-arxiv,
  author    = {Peter Mattson and
               Christine Cheng and
               Cody Coleman and
               Greg Diamos and
               Paulius Micikevicius and
               David A. Patterson and
               Hanlin Tang and
               Gu{-}Yeon Wei and
               Peter Bailis and
               Victor Bittorf and
               David Brooks and
               Dehao Chen and
               Debojyoti Dutta and
               Udit Gupta and
               Kim M. Hazelwood and
               Andrew Hock and
               Xinyuan Huang and
               Bill Jia and
               Daniel Kang and
               David Kanter and
               Naveen Kumar and
               Jeffery Liao and
               Guokai Ma and
               Deepak Narayanan and
               Tayo Oguntebi and
               Gennady Pekhimenko and
               Lillian Pentecost and
               Vijay Janapa Reddi and
               Taylor Robie and
               Tom St. John and
               Carole{-}Jean Wu and
               Lingjie Xu and
               Cliff Young and
               Matei Zaharia},
  title     = {MLPerf Training Benchmark},
  journal   = {CoRR},
  volume    = {abs/1910.01500},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.01500},
  archivePrefix = {arXiv},
  eprint    = {1910.01500},
  timestamp = {Mon, 04 Nov 2019 08:16:51 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-01500.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{variability_azure,
  author={V. {Persico} and P. {Marchetta} and A. {Botta} and A. {Pescape}},
  booktitle={2015 IEEE Global Communications Conference (GLOBECOM)}, 
  title={On Network Throughput Variability in Microsoft Azure Cloud}, 
  year={2015},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/GLOCOM.2015.7416997}}

@article{variability_aws,
title = "Measuring network throughput in the cloud: The case of Amazon EC2",
journal = "Computer Networks",
volume = "93",
pages = "408 - 422",
year = "2015",
note = "Cloud Networking and Communications II",
issn = "1389-1286",
doi = "https://doi.org/10.1016/j.comnet.2015.09.037",
url = "http://www.sciencedirect.com/science/article/pii/S138912861500362X",
author = "Valerio Persico and Pietro Marchetta and Alessio Botta and Antonio Pescapè",
keywords = "Cloud networking monitoring and measurement, Cloud networking performance, Cloud network throughput",
abstract = "Cloud providers employ sophisticated virtualization techniques and strategies for sharing resources among a large number of largely uncoordinated and mutually untrusted customers. The shared networking environment, in particular, dictates the need for mechanisms to partition network resources among virtual machines. At the same time, the performance of applications deployed over these virtual machines may be heavily impacted by the performance of the underlying network, and therefore by such mechanisms. Nevertheless, due to security and commercial reasons, providers rarely provide detailed information on network organization, performance, and mechanisms employed to regulate it. In addition, the scientific literature only provides a blurred image of the network performance inside the cloud. The few available pioneer works marginally focus on this aspect, use different methodologies, operate in few limited scenarios, or report conflicting results. In this paper, we present a detailed analysis of the performance of the internal network of Amazon EC2, performed by adopting a non-cooperative experimental evaluation approach (i.e. not relying on provider support). Our aim is to provide a quantitative assessment of the networking performance as a function of the several variables available, such as geographic region, resource price or size. We propose a detailed methodology to perform this kind of analysis, which we believe is essential in a such complex and dynamic environment. During this analysis we have discovered and analyzed the limitations enforced by Amazon over customer traffic in terms of maximum throughput allowed. Thanks to our work it is possible to understand how the complex mechanisms enforced by the provider in order to manage its infrastructure impact the performance perceived by the cloud customers and potentially tamper with monitoring and controlling approaches previously proposed in literature. Leveraging our knowledge of the bandwidth-limiting mechanisms, we then present a clear picture of the maximum throughput achievable in Amazon EC2 network, shedding light on when and how such maximum throughput can be achieved and at which cost."
}


@misc{gpt3cost,
  author = {Elliot Turner},
  note = {Estimate of GPT-3 training cost based on public cloud GPU/TPU cost models, from Elliot Turner's personal page (accessed on May 29, 2020)}
}

@misc{lambdabenchmarks,
  author = {Stephen Balaban , Chuan Li},
  title = {Deep Learning GPU benchmarks, Lambda Labs website, 2018/10/08}
}
  
  
@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{Hendrycks2019NaturalAE,
  title={Natural Adversarial Examples},
  author={Dan Hendrycks and Kevin Keliang Zhao and Steven Basart and Jacob Steinhardt and Dawn Xiaodong Song},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.07174}
}

@inproceedings{Ott2019fairseqAF,
  title={fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author={Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle={NAACL-HLT},
  year={2019}
}

@article{hern2018facebook,
  title={Facebook translates ``good morning'' into ``attack them'', leading to arrest},
  author={Alex Hern},
  journal={The Guardian},
  year={2018}
}

@article{Papernot2018DeepKN,
  title={Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning},
  author={Nicolas Papernot and Patrick D. McDaniel},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.04765}
}

@article{Garnelo2018ConditionalNP,
  title={Conditional Neural Processes},
  author={Marta Garnelo and Dan Rosenbaum and Chris J. Maddison and Tiago Ramalho and David Saxton and Murray Shanahan and Yee Whye Teh and Danilo Jimenez Rezende and S. M. Ali Eslami},
  journal={ArXiv},
  year={2018},
  volume={abs/1807.01613}
}

@inproceedings{Wallace2019UniversalAT,
  title={Universal Adversarial Triggers for Attacking and Analyzing NLP},
  author={Eric Wallace and Feng Shi and Nikhil Kandpal and Matt Gardner and Sameer Singh},
  year={2019}
}

@article{Szegedy2013IntriguingPO,
  title={Intriguing properties of neural networks},
  author={Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian J. Goodfellow and Rob Fergus},
  journal={CoRR},
  year={2013},
  volume={abs/1312.6199}
}

@article{Yuan2017AdversarialEA,
  title={Adversarial Examples: Attacks and Defenses for Deep Learning},
  author={Xiaoyong Yuan and Pan He and Qile Zhu and Xiaolin Li},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2017},
  volume={30},
  pages={2805-2824}
}
@inproceedings{Ebrahimi2017HotFlipWA,
  title={HotFlip: White-Box Adversarial Examples for Text Classification},
  author={Javid Ebrahimi and Anyi Rao and Daniel Lowd and Dejing Dou},
  booktitle={ACL},
  year={2017}
}

@article{schmidthuber,
  title={First superhuman visual pattern recognition},
  author={D. C. Ciresan, U. Meier, J. Schmidhuber},
  year={2011},
  journal={IJCNN}
}

@article{microsoft_mt_parity,
  title={Achieving Human Parity on Automatic Chinese to English News Translation},
  author={Hany Hassan and Anthony Aue and Chang Chen and Vishal Chowdhary and Jonathan R. Clark and Christian Federmann and Xuedong Huang and Marcin Junczys-Dowmunt and William Lewis and Mu Li and Shujie Liu and T. M. Liu and Renqian Luo and Arul Menezes and Tao Qin and Frank Seide and Xu Tan and Fei Tian and Lijun Wu and Shuangzhi Wu and Yingce Xia and Dongdong Zhang and Zhirui Zhang and Ming Zhou},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.05567}
}

@article{Silver2016MasteringTG,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={David Silver and Aja Huang and Chris J. Maddison and Arthur Guez and Laurent Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Vedavyas Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy P. Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
  journal={Nature},
  year={2016},
  volume={529},
  pages={484-489}
}

@inproceedings{Singh2013OpticalCR,
  title={Optical Character Recognition Techniques: A survey},
  author={Sukhpreet Singh},
  year={2013}
}

@inproceedings{maml,
  title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author={Chelsea Finn and Pieter Abbeel and Sergey Levine},
  booktitle={ICML},
  year={2017}
}

@article{reptile,
  title={On First-Order Meta-Learning Algorithms},
  author={Alex Nichol and Joshua Achiam and John Schulman},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.02999}
}

@Article{elasticweightcolsolidation,
  author  = {James Kirkpatrick and Razvan Pascanu and Neil C. Rabinowitz and Joel Veness and Guillaume Desjardins and Andrei A. Rusu and Kieran Milan and John Quan and Tiago Ramalho and Agnieszka Grabska-Barwinska and Demis Hassabis and Claudia Clopath and Dharshan Kumaran and Raia Hadsell},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  title   = {Overcoming catastrophic forgetting in neural networks},
  year    = {2016},
  pages   = {3521-3526},
  volume  = {114 13},
}

@article{rehearsal,
  title={Catastrophic Forgetting, Rehearsal and Pseudorehearsal},
  author={Anthony V. Robins},
  journal={Connect. Sci.},
  year={1995},
  volume={7},
  pages={123-146}
}

@article{Houthooft2018EvolvedPG,
  title={Evolved Policy Gradients},
  author={Rein Houthooft and Yuhua Chen and Phillip Isola and Bradly C. Stadie and Filip Wolski and Jonathan Ho and Pieter Abbeel},
  journal={ArXiv},
  year={2018},
  volume={abs/1802.04821}
}

@article{Lin2019ConditionalCF,
  title={Conditional Computation for Continual Learning},
  author={Min Lin and Jie Fu and Yoshua Bengio},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.06635}
}

@article{clmetalearning1,
  title={Task Agnostic Continual Learning via Meta Learning},
  author={Xu He and Jakub Sygnowski and Alexandre Galashov and Andrei A. Rusu and Yee Whye Teh and Razvan Pascanu},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.05201}
}

@article{clmetalearning2,
  title={Meta-Learning Representations for Continual Learning},
  author={Khurram Javed and Martha White},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.12588}
}

@article{densenet,
  title={Densely Connected Convolutional Networks},
  author={Gao Huang and Zhuang Liu and Laurens van der Maaten and Kilian Q. Weinberger},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={2261-2269}
}

@article{adagrad,
  title={Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  author={John C. Duchi and Elad Hazan and Yoram Singer},
  journal={J. Mach. Learn. Res.},
  year={2010},
  volume={12},
  pages={2121-2159}
}


@article{adadelta,
  title={ADADELTA: An Adaptive Learning Rate Method},
  author={Matthew D. Zeiler},
  journal={ArXiv},
  year={2012},
  volume={abs/1212.5701}
}

@article{superconvergence,
  title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},
  author={Leslie N. Smith and Nicholay Topin},
  journal={ArXiv},
  year={2017},
  volume={abs/1708.07120}
}

@inproceedings{adversarial_training,
title	= {Explaining and Harnessing Adversarial Examples},
author	= {Ian Goodfellow and Jonathon Shlens and Christian Szegedy},
year	= {2015},
URL	= {http://arxiv.org/abs/1412.6572},
booktitle	= {International Conference on Learning Representations}
}



@misc{rmsprop,
  title={{Lecture 6.5---RmsProp: Divide the gradient by a running average of its recent magnitude}},
  author={Tieleman, T. and Hinton, G.},
  howpublished={COURSERA: Neural Networks for Machine Learning},
  year={2012}
}

@Article{cifar,
  author   = {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
  title    = {CIFAR-10 (Canadian Institute for Advanced Research)},
  abstract = {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 

The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.},
  keywords = {Dataset},
  url      = {http://www.cs.toronto.edu/~kriz/cifar.html},
}


@inproceedings{GAN,
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
title = {Generative Adversarial Nets},
year = {2014},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2672–2680},
numpages = {9},
}


@article{trainingtips,
author = {Popel, Martin and Bojar, Ondřej},
year = {2018},
month = {03},
pages = {},
title = {Training Tips for the Transformer Model},
volume = {110},
journal = {The Prague Bulletin of Mathematical Linguistics},
doi = {10.2478/pralin-2018-0002}
}


@inproceedings{Li2017VisualizingTL,
  title={Visualizing the Loss Landscape of Neural Nets},
  author={Hao Li and Zheng Xu and Gavin Taylor and Christoph Studer and Tom Goldstein},
  booktitle={NeurIPS},
  year={2017}
}

@inproceedings{tsne,
  title={Visualizing Data using t-SNE},
  author={Laurens van der Maaten and Geoffrey E. Hinton},
  year={2008}
}

@article{elu,
  title={Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)},
  author={Djork-Arn{\'e} Clevert and Thomas Unterthiner and Sepp Hochreiter},
  journal={CoRR},
  year={2015},
  volume={abs/1511.07289}
}

@article{Hinton2015DistillingTK,
  title={Distilling the Knowledge in a Neural Network},
  author={Geoffrey E. Hinton and Oriol Vinyals and Jeffrey Dean},
  journal={ArXiv},
  year={2015},
  volume={abs/1503.02531}
}

@inproceedings{iwslt14,
  title={Report on the 11 th IWSLT Evaluation Campaign , IWSLT 2014},
  author={Mauro Cettolo and Jan Niehues and Sebastian St{\"u}ker and Luisa Bentivogli and Marcello Federico},
  year={2015}
}

@inproceedings{koehn-etal-2007-moses,
    title = "{M}oses: Open Source Toolkit for Statistical Machine Translation",
    author = "Koehn, Philipp  and
      Hoang, Hieu  and
      Birch, Alexandra  and
      Callison-Burch, Chris  and
      Federico, Marcello  and
      Bertoldi, Nicola  and
      Cowan, Brooke  and
      Shen, Wade  and
      Moran, Christine  and
      Zens, Richard  and
      Dyer, Chris  and
      Bojar, Ond{\v{r}}ej  and
      Constantin, Alexandra  and
      Herbst, Evan",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P07-2045",
    pages = "177--180",
}

@inproceedings{transformer,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
pages = {5998--6008},
year = {2017},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@article{ratcliff1990connectionist,
  title={Connectionist models of recognition memory: constraints imposed by learning and forgetting functions.},
  author={Ratcliff, Roger},
  journal={Psychological review},
  volume={97},
  number={2},
  pages={285},
  year={1990},
  publisher={American Psychological Association}
}

@inproceedings{Furlanello2018BornAgainNN,
  title={Born-Again Neural Networks},
  author={Tommaso Furlanello and Zachary Chase Lipton and Michael Tschannen and Laurent Itti and Anima Anandkumar},
  booktitle={ICML},
  year={2018}
}

@article{inception_v3,
  title={Rethinking the Inception Architecture for Computer Vision},
  author={Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jon Shlens and Zbigniew Wojna},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={2818-2826}
}


@MISC{Torrey_transferlearning,
    author = {Lisa Torrey and Jude Shavlik},
    title = {Transfer Learning},
    year = {}
}

@inproceedings{real2017large,
  title={Large-scale evolution of image classifiers},
  author={Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc V and Kurakin, Alexey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2902--2911},
  year={2017},
  organization={JMLR. org}
}

@misc{hendrycks2016gaussian,
    title={Gaussian Error Linear Units (GELUs)},
    author={Dan Hendrycks and Kevin Gimpel},
    year={2016},
    eprint={1606.08415},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{mnist,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@inproceedings{mlperf,
  title={{MLPerf Training Benchmark}},
  author={Peter Mattson and Christine Cheng and Cody Coleman and Greg Diamos
    and Paulius Micikevicius and David Patterson and Hanlin Tang and Gu-Yeon
      Wei and Peter Bailis and Victor Bittorf and David Brooks and Dehao Chen
      and Debojyoti Dutta and Udit Gupta and Kim Hazelwood and Andrew Hock and
      Xinyuan Huang and Bill Jia and Daniel Kang and David Kanter and Naveen
      Kumar and Jeffery Liao and Guokai Ma and Deepak Narayanan and Tayo
      Oguntebi and Gennady Pekhimenko and Lillian Pentecost and Vijay Janapa
      Reddi and Taylor Robie and Tom St. John and Carole-Jean Wu and Lingjie Xu
      and Cliff Young and Matei Zaharia},
  year={2020},
  booktitle = {{Proceedings of the 3rd Conference on Machine Learning and
    Systems (MLSys'20)}},
}

@article{sgd_with_momentum,
 author = {Qian, Ning},
 title = {On the Momentum Term in Gradient Descent Learning Algorithms},
 journal = {Neural Netw.},
 issue_date = {Jan. 1999},
 volume = {12},
 number = {1},
 month = jan,
 year = {1999},
 pages = {145--151},
 numpages = {7},
 publisher = {Elsevier Science Ltd.},
 address = {Oxford, UK, UK},
 keywords = {critical damping, damped harmonic oscillator, gradient descent learning algorithm, learning rate, momentum, speed of convergence}
} 

@inproceedings{adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {International Conference on Learning Representations},
  year      = {2015},
}

@inproceedings{bookcorpus,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

@inproceedings{
reddi2021adaptive,
title={Adaptive Federated Optimization},
author={Sashank J. Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Kone{\v{c}}n{\'y} and Sanjiv Kumar and Hugh Brendan McMahan},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=LkFG3lB13U5}
}

@inproceedings{chen2020toward,
author = {Chen, Xiangyi and Li, Xiaoyun and Li, Ping},
title = {Toward Communication Efficient Adaptive Gradient Method},
year = {2020},
isbn = {9781450381031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412815.3416891},
doi = {10.1145/3412815.3416891},
booktitle = {Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference},
pages = {119–128},
numpages = {10},
keywords = {adaptive method, federated learning, convergence analysis},
location = {Virtual Event, USA},
series = {FODS '20}
}
 
@article{scipy,
  title={SciPy 1.0: fundamental algorithms for scientific computing in Python},
  author={Pauli Virtanen and Ralf Gommers and Travis E. Oliphant and Matt Haberland and Tyler Reddy and David Cournapeau and Evgeni Burovski and Pearu Peterson and Warren Weckesser and Jonathan Bright and St{\'e}fan J. van der Walt and Matthew Brett and Joshua Wilson and K. Jarrod Millman and Nikolay Mayorov and Andrew R. J. Nelson and Eric Jones and Robert Kern and Eric Larson and C J Carey and Ilhan Polat and Yu Feng and Eric W. Moore and J. Vanderplas and Denis Laxalde and Josef Perktold and Robert Cimrman and Ian Daniel Henriksen and E. A. Quintero and Charles R. Harris and A. M. Archibald and Ant{\^o}nio H. Ribeiro and Fabian Pedregosa and Paul van Mulbregt and Aditya Alessandro Pietro Alex Andreas Andreas Anthony Ant Vijaykumar Bardelli Rothberg Hilboll Kloeckner Sco and Aditya Vijaykumar and Alessandro Pietro Bardelli and Alex Rothberg and Andreas Hilboll and Andre Kloeckner and Anthony M. Scopatz and Antony Lee and Ariel S. Rokem and C. Nathan Woods and Chad Fulton and Charles Masson and Christian H{\"a}ggstr{\"o}m and Clark Fitzgerald and David A. Nicholson and David R. Hagen and Dmitrii V. Pasechnik and Emanuele Olivetti and Eric A. Martin and E. Wieser and Fabrice Silva and Felix Lenders and Florian Wilhelm and Gert Young and Gavin A. Price and Gert-Ludwig Ingold and Gregory E. Allen and Gregory R. Lee and Herv{\'e} Audren and Irvin Probst and Jorg P. Dietrich and Jacob Silterra and James T. Webber and Janko Slavic and Joel Nothman and Johannes Buchner and Johannes Kulick and Johannes L. Sch{\"o}nberger and Jos{\'e} Vin{\'i}cius de Miranda Cardoso and Joscha Reimer and Joseph E. Harrington and Juan Luis Cano Rodr{\'i}guez and Juan Nunez-Iglesias and Justin Kuczynski and Kevin Lee Tritz and Martin Dr Thoma and Matt Newville and Matthias K{\"u}mmerer and Maximilian Bolingbroke and Michael Tartre and Mikhail Pak and Nathaniel J. Smith and Nikolai Nowaczyk and Nikolay Shebanov and Oleksandr Pavlyk and Per Andreas Brodtkorb and Perry Lee and Robert T. McGibbon and Roman Feldbauer and Sam Lewis and Sam Tygier and Scott Sievert and Sebastiano Vigna and Stefan Peterson and Surhud More and Tadeusz Pudlik and Taku Oshima and Thomas J. Pingel and Thomas P. Robitaille and Thomas Spura and Thouis Raymond Jones and Tim Cera and Tim Leslie and Tiziano Zito and Tom Krauss and U. Upadhyay and Yaroslav O. Halchenko and Y. V{\'a}zquez-Baeza},
  journal={Nature Methods},
  year={2020},
  volume={17},
  pages={261 - 272}
}
@article{kaplan1974application,
  title={Application of programs with maximin objective functions to problems of optimal resource allocation},
  author={Kaplan, Seymour},
  journal={Operations Research},
  volume={22},
  number={4},
  pages={802--807},
  year={1974},
  publisher={INFORMS}
}

@inproceedings{Joshi2020TheSA,
    title = "The State and Fate of Linguistic Diversity and Inclusion in the {NLP} World",
    author = "Joshi, Pratik  and
      Santy, Sebastin  and
      Budhiraja, Amar  and
      Bali, Kalika  and
      Choudhury, Monojit",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = 07,
    year = "2020",
    pages = "6282-6293",
}

@article{Schwartz2020GreenA,
  title={Green {AI}},
  author={Roy Schwartz and Jesse Dodge and Noah Smith and Oren Etzioni},
  journal={Communications of the ACM},
  year={2020},
  volume={63},
  pages={54-63}
}



@inproceedings{Strubell2019EnergyAP,
  author    = {Emma Strubell and
               Ananya Ganesh and
               Andrew McCallum},
  title     = {Energy and Policy Considerations for Deep Learning in {NLP}},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics},
  pages     = {3645--3650},
  year      = {2019},
}

@article{Henderson2020TowardsTS,
  author  = {Peter Henderson and Jieru Hu and Joshua Romoff and Emma Brunskill and Dan Jurafsky and Joelle Pineau},
  title   = {Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {248},
  pages   = {1-43},
  url     = {http://jmlr.org/papers/v21/20-312.html}
}

@misc{Anthony2020CarbontrackerTA,
  title={Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models},
  author={Lasse F. Wolff Anthony and Benjamin Kanding and Raghavendra Selvan},
  howpublished={ICML Workshop on Challenges in Deploying and monitoring Machine Learning Systems},
  month={07},
  year={2020}
}


@article{Kline2016HolisticallyET,
  title={Holistically evaluating the environmental impacts in modern computing systems},
  author={Donald Kline and Nikolas Parshook and Xiaoyu Ge and E. Brunvand and R. Melhem and Panos K. Chrysanthis and A. Jones},
  journal={2016 Seventh International Green and Sustainable Computing Conference (IGSC)},
  year={2016},
  pages={1-8}
}

@article{Bashroush2018ACR,
  title={A Comprehensive Reasoning Framework for Hardware Refresh in Data Centers},
  author={Rabih Bashroush},
  journal={IEEE Transactions on Sustainable Computing},
  year={2018},
  volume={3},
  pages={209-220}
}

@article{Qiu2020CanFL,
    title={Can Federated Learning Save The Planet?},
    author={Xinchi Qiu and Titouan Parcollet and Daniel J. Beutel and Taner Topal and Akhil Mathur and Nicholas D. Lane},
  journal={arXiv preprint arXiv:2010.06537},
  year={2020},
}


@inproceedings{xlmr,
    title = "Unsupervised Cross-lingual Representation Learning at Scale",
    author = "Conneau, Alexis  and
      Khandelwal, Kartikay  and
      Goyal, Naman  and
      Chaudhary, Vishrav  and
      Wenzek, Guillaume  and
      Guzm{\'a}n, Francisco  and
      Grave, Edouard  and
      Ott, Myle  and
      Zettlemoyer, Luke  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = 07,
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.acl-main.747",
    doi = "10.18653/v1/2020.acl-main.747",
    pages = "8440--8451",
}

@article{switch,
    title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
    author={William Fedus and Barret Zoph and Noam Shazeer},
    year={2021},
    journal={arXiv preprint arXiv:2101.03961},
}

@article{lars,
      title={Large Batch Training of Convolutional Networks}, 
      author={Yang You and Igor Gitman and Boris Ginsburg},
      year={2017},
      journal={arXiv preprint arXiv:1708.03888},
      archivePrefix={arXiv},
}


@inproceedings{
lamb,
title={Large Batch Optimization for Deep Learning: Training {BERT} in 76 minutes},
author={Yang You and Jing Li and Sashank Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Syx4wnEtvH}
}


% Evaluation:
@inproceedings{rahimi-etal-2019-massively,
    title = "Massively Multilingual Transfer for {NER}",
    author = "Rahimi, Afshin  and
      Li, Yuan  and
      Cohn, Trevor",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = {07},
    year = {2019},
    url = "https://www.aclweb.org/anthology/P19-1015",
    pages = "151--164",
}

@inproceedings{pan-etal-2017-cross,
    title = "Cross-lingual Name Tagging and Linking for 282 Languages",
    author = "Pan, Xiaoman  and
      Zhang, Boliang  and
      May, Jonathan  and
      Nothman, Joel  and
      Knight, Kevin  and
      Ji, Heng",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
    month = 07,
    year = "2017",
    url = "https://www.aclweb.org/anthology/P17-1178",
    doi = "10.18653/v1/P17-1178",
    pages = "1946--1958",
}

@InProceedings{pmlr-v119-hu20b,
  author    = {Hu, Junjie and Ruder, Sebastian and Siddhant, Aditya and Neubig, Graham and Firat, Orhan and Johnson, Melvin},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  title     = {{XTREME}: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation},
  year      = {2020},
  editor    = {Hal Daumé III and Aarti Singh},
  month     = {13--18 Jul},
  pages     = {4411--4421},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  pdf       = {http://proceedings.mlr.press/v119/hu20b/hu20b.pdf},
  url       = {http://proceedings.mlr.press/v119/hu20b.html},
}


# IndicGLUE
@inproceedings{kakwani-etal-2020-indicnlpsuite,
    title = "{I}ndic{NLPS}uite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for {I}ndian Languages",
  author={Divyanshu Kakwani and Anoop Kunchukuttan and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = 11,
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.445",
    doi = "10.18653/v1/2020.findings-emnlp.445",
    pages = "4948--4961",
}


@misc{Sagor_2020,
  title   = {BanglaBERT: Bengali Mask Language Model for Bengali Language Understading},
  author  = {Sagor Sarker},
  year    = {2020},
  url    = {https://github.com/sagorbrur/bangla-bert}
}

% HF ecosystem
@misc{hftokenizers2019,
  author = {Anthony MOI and
                  Pierric Cistac and
                  Nicolas Patry and
                  Evan P. Walsh and
                  Funtowicz Morgan and
                  Sebastian Pütz and
                  Thomas Wolf and
                  Sylvain Gugger and
                  Clément Delangue and
                  Julien Chaumond and
                  Lysandre Debut and
                  Patrick von Platen},
  title = {Hugging Face Tokenizers library},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4784271},
  howpublished = {\url{https://github.com/huggingface/tokenizers}}
}

@inproceedings{butterfly_arsgd,
author = {Li, Zhenyu and Davis, James and Jarvis, Stephen},
title = {An Efficient Task-Based All-Reduce for Machine Learning Applications},
year = {2017},
isbn = {9781450351379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3146347.3146350},
doi = {10.1145/3146347.3146350},
booktitle = {Proceedings of the Machine Learning on HPC Environments},
articleno = {2},
numpages = {8},
keywords = {Data-flow Frameworks, Apache Spark, Synchronous Model Training, Butterfly All-Reduce},
location = {Denver, CO, USA},
series = {MLHPC'17}
}

  
@inproceedings{wolf-etal-2020-transformers,
    title = {Transformers: State-of-the-Art Natural Language Processing},
    author = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
    booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
    month = {10},
    year = {2020},
    pages = {38-45}
}

@misc{wandb,
title = {Experiment Tracking with {Weights and Biases}},
year = {2020},
note = {Software available from wandb.com},
url={https://www.wandb.com/},
author = {Biewald, Lukas},
}

@INPROCEEDINGS {dlhub,
  author={Chard, Ryan and Li, Zhuozhao and Chard, Kyle and Ward, Logan and Babuji, Yadu and Woodard, Anna and Tuecke, Steven and Blaiszik, Ben and Franklin, Michael J. and Foster, Ian},
 booktitle = {2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
title = {{DLHub}: Model and Data Serving for Science},
year = {2019},
volume = {},
issn = {},
pages = {283-292},
doi = {10.1109/IPDPS.2019.00038},
url = {https://doi.ieeecomputersociety.org/10.1109/IPDPS.2019.00038},
month = {05}
}

@inproceedings{dp_sgd,
 author = {Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 title = {Can Decentralized Algorithms Outperform Centralized Algorithms? {A} Case Study for Decentralized Parallel Stochastic Gradient Descent},
 url = {https://proceedings.neurips.cc/paper/2017/file/f75526659f31040afeb61cb7133e4e6d-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{ott2018scaling,
    title = "Scaling Neural Machine Translation",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Grangier, David  and
      Auli, Michael",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = 10,
    year = "2018",
    url = "https://www.aclweb.org/anthology/W18-6301",
    doi = "10.18653/v1/W18-6301",
    pages = "1-9",
}

@inproceedings{swav,
 author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {9912--9924},
 title = {Unsupervised Learning of Visual Features by Contrasting Cluster Assignments},
 volume = {33},
 year = {2020}
}

@misc{vissl,
  author =       {Priya Goyal and Quentin Duval and Jeremy Reizenstein and Matthew Leavitt and Min Xu and
                  Benjamin Lefaudeux and Mannat Singh and Vinicius Reis and Mathilde Caron and Piotr Bojanowski and
                  Armand Joulin and Ishan Misra},
  title =        {VISSL},
  howpublished = {\url{https://github.com/facebookresearch/vissl}},
  year =         {2021}
}

@inproceedings{
mixed_precision,
title={Mixed Precision Training},
author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=r1gs9JgRZ},
}

@article{foldingathome,
  title={Folding@home: Lessons from eight years of volunteer distributed computing},
  author={A. L. Beberg and D. Ensign and G. Jayachandran and S. Khaliq and V. Pande},
  journal={2009 IEEE International Symposium on Parallel \& Distributed Processing},
  year={2009},
  pages={1-8}
}
@article{einstein_at_home,
   title={{Einstein@Home} All-sky Search for Continuous Gravitational Waves in {LIGO O2} Public Data},
   volume={909},
   ISSN={1538-4357},
   url={http://dx.doi.org/10.3847/1538-4357/abc7c9},
   DOI={10.3847/1538-4357/abc7c9},
   number={1},
   journal={The Astrophysical Journal},
   publisher={American Astronomical Society},
    author={B. Steltner and M. A. Papa and H. -B. Eggenstein and B. Allen and V. Dergachev and R. Prix and B. Machenschalk and S. Walsh and S. J. Zhu and S. Kwang},
   year={2021},
   month={03},
   pages={79}
}

@article{pytorch_distributed,
author = {Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and Chintala, Soumith},
title = {PyTorch Distributed: Experiences on Accelerating Data Parallel Training},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415530},
doi = {10.14778/3415478.3415530},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3005–3018},
numpages = {14}
}

@article{zhu2021transfer,
  title={Transfer Learning in Deep Reinforcement Learning: A Survey},
  author={Zhuangdi Zhu and Kaixiang Lin and Jiayu Zhou},
  journal={arXiv preprint arXiv:2009.07888},
  year={2020},
}

@INPROCEEDINGS{7298965,

  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},

  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 

  title={Fully convolutional networks for semantic segmentation}, 

  year={2015},

  volume={},

  number={},

  pages={3431-3440},

  doi={10.1109/CVPR.2015.7298965}}

@inproceedings{10.1109/CVPR.2014.81,
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
year = {2014},
isbn = {9781479951185},
url = {https://doi.org/10.1109/CVPR.2014.81},
doi = {10.1109/CVPR.2014.81},
booktitle = {Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition},
pages = {580-587},
}
@inproceedings{pmlr-v32-donahue14,
  title={{DeCAF}: A Deep Convolutional Activation Feature for Generic Visual Recognition},
  author={J. Donahue and Y. Jia and Oriol Vinyals and Judy Hoffman and Ning Zhang and Eric Tzeng and Trevor Darrell},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning},
pages = {647-655},
  year={2014}
}


}
@inproceedings{johnson2016perceptual,
  title={Perceptual losses for real-time style transfer and super-resolution},
    author={Justin Johnson and Alexandre Alahi and Li Fei-Fei},
  booktitle={European Conference on Computer Vision},
year = {2016},
month = {10},
pages = {694-711},
volume = {9906},
isbn = {978-3-319-46474-9},
doi = {10.1007/978-3-319-46475-6_43}
}


@article{honda2019smiles,
  title={{SMILES} Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery},
  author={Shion Honda and Shoi Shi and H. Ueda},
  journal={arXiv preprint arXiv:1911.04738},
  year={2019},
}

@article {Lu2020.09.04.283929,
	author = {Lu, Amy X. and Zhang, Haoran and Ghassemi, Marzyeh and Moses, Alan},
	title = {Self-Supervised Contrastive Learning of Protein Representations By Mutual Information Maximization},
	elocation-id = {2020.09.04.283929},
	year = {2020},
	doi = {10.1101/2020.09.04.283929},
	publisher = {Cold Spring Harbor Laboratory},
	URL = {https://www.biorxiv.org/content/early/2020/09/06/2020.09.04.283929},
	eprint = {https://www.biorxiv.org/content/early/2020/09/06/2020.09.04.283929.full.pdf},
	journal = {bioRxiv}
}

@inproceedings{baevski2020wav2vec,
 author = {Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {12449--12460},
 title = {wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
 url = {https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{tfhub,
  title = {{TensorFlow} {H}ub},
  howpublished = {\url{https://www.tensorflow.org/hub}},
  note = {Accessed: 2021-05-20}
}

@misc{torchhub,
  title = {{PyTorch} {H}ub},
  howpublished = {\url{https://pytorch.org/hub/}},
  note = {Accessed: 2021-05-20}
}

@misc{hfhub,
  title = {{Hugging Face} {H}ub},
  howpublished = {\url{https://huggingface.co/models}},
  note = {Accessed: 2021-05-20}
}

@inproceedings{Tapparello2016VolunteerCO,
  title={Volunteer Computing on Mobile Devices: State of the Art and Future Research Directions},
  author={C. Tapparello and Colin Funai and Shurouq Hijazi and Abner Aquino and Bora Karaoglu and H. Ba and J. Shi and W. Heinzelman},
  year={2016},
  booktitle={Enabling Real-Time Mobile Cloud Computing through Emerging Technologies},
  pages = {153-181}
}

@INPROCEEDINGS{shi2018performance,
  author={Shi, Shaohuai and Wang, Qiang and Chu, Xiaowen},
  booktitle={IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress}, 
  title={Performance Modeling and Evaluation of Distributed Deep Learning Frameworks on {GPUs}}, 
  year={2018},
  pages={949-957},
  doi={10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.000-4}}

@article{sergeev2018horovod,
  Author = {Alexander Sergeev and Mike Del Balso},
  Journal = {arXiv preprint arXiv:1802.05799},
  Title = {Horovod: fast and easy distributed deep learning in {TensorFlow}},
  Year = {2018}
}

@misc{google_stadia,
  title = {{Google Stadia} data usage},
  howpublished = {\url{https://support.google.com/stadia/answer/9607891}},
  note = {Accessed: 2021-05-20}
}

@misc{netflix,
  title = {{N}etflix data usage},
  howpublished = {\url{https://help.netflix.com/en/node/87}},
  note = {Accessed: 2021-05-20}
}

@inproceedings{liu-etal-2020-understanding,
    title = "Understanding the Difficulty of Training Transformers",
    author = "Liu, Liyuan  and
      Liu, Xiaodong  and
      Gao, Jianfeng  and
      Chen, Weizhu  and
      Han, Jiawei",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = 11,
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.463",
    doi = "10.18653/v1/2020.emnlp-main.463",
    pages = "5747--5763",
}

@InProceedings{projectadam,
  author    = {Trishul Chilimbi and Yutaka Suzue and Johnson Apacible and Karthik Kalyanaraman},
  booktitle = {11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14)},
  title     = {Project Adam: Building an Efficient and Scalable Deep Learning Training System},
  year      = {2014},
  address   = {Broomfield, CO},
  month     = oct,
  pages     = {571--582},
  publisher = {{USENIX} Association},
  isbn      = {978-1-931971-16-4},
  url       = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/chilimbi},
}


@article{aji2019making,
   title={Making Asynchronous Stochastic Gradient Descent Work for Transformers},
   url={http://dx.doi.org/10.18653/v1/d19-5608},
   DOI={10.18653/v1/d19-5608},
   journal={Proceedings of the 3rd Workshop on Neural Generation and Translation},
    author={Alham Fikri Aji and Kenneth Heafield},
   year={2019}
}



@article{jain2020indictransformers,
      title={Indic-Transformers: An Analysis of Transformer Language Models for {Indian} Languages}, 
      author={Kushal Jain and Adwait Deshpande and Kumar Shridhar and Felix Laumann and Ayushman Dash},
      year={2020},
      journal={arXiv preprint arXiv:2011.02323},
}

@misc{libp2p,
  title = {libp2p},
  howpublished = {\url{https://libp2p.io/}},
  note = {Accessed: 2021-05-20}
}

@article{tensorflow2015-whitepaper,
      title={{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Distributed Systems}, 
      author={Martín Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Greg S. Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Yangqing Jia and Rafal Jozefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dan Mane and Rajat Monga and Sherry Moore and Derek Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda Viegas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
      year={2016},
      journal={arXiv preprint arXiv:1603.04467},
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@misc{hivemind,
  author = {Learning@home team},
  title = {{H}ivemind: a {L}ibrary for {D}ecentralized {D}eep {L}earning},
  year = 2020,
  howpublished = {\url{https://github.com/learning-at-home/hivemind}},
}



@article{normformer,
    title={NormFormer: Improved Transformer Pretraining with Extra Normalization},
    author={Sam Shleifer and Jason Weston and Myle Ott},
    year={2021},
    eprint={2110.09456},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{stich2019unified,
      title={Unified Optimal Analysis of the (Stochastic) Gradient Method}, 
      author={Sebastian U. Stich},
      year={2019},
      eprint={1907.04232},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{khaled2020unified,
      title={Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization}, 
      author={Ahmed Khaled and Othmane Sebbouh and Nicolas Loizou and Robert M. Gower and Peter Richtárik},
      year={2020},
      eprint={2006.11573},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@InProceedings{hole_punching,
  author    = {Bryan Ford and Pyda Srisuresh and Dan Kegel},
  booktitle = {Proceedings of the Annual Conference on USENIX Annual Technical Conference},
  title     = {Peer-to-Peer Communication Across Network Address Translators},
  year      = {2005},
  address   = {USA},
  pages     = {13},
  publisher = {USENIX Association},
  series    = {ATEC '05},
  location  = {Anaheim, CA},
  numpages  = {1},
}

@InProceedings{FedLearningOriginal,
  author    = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle = {Artificial Intelligence and Statistics},
  title     = {Communication-Efficient Learning of Deep Networks from Decentralized Data},
  year      = {2017},
  pages     = {1273--1282},
}

@InProceedings{PracticalSecureAggregation,
  author    = {Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  title     = {Practical secure aggregation for privacy-preserving machine learning},
  year      = {2017},
  pages     = {1175--1191},
}

@Article{FedLearningAdvancesProblems,
  author  = {Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Keith and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal = {arXiv preprint arXiv:1912.04977},
  title   = {Advances and open problems in federated learning},
  year    = {2019},
}

@Article{natural_compression,
  author        = {Samuel Horvath and Chen{-}Yu Ho and Ludovit Horvath and Atal Narayan Sahu and Marco Canini and Peter Richt{\'{a}}rik},
  journal       = {CoRR},
  title         = {Natural Compression for Distributed Deep Learning},
  year          = {2019},
  volume        = {abs/1905.10988},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1905-10988},
  eprint        = {1905.10988},
  primaryclass  = {cs.LG},
  timestamp     = {Mon, 03 Jun 2019 13:42:33 +0200},
  url           = {http://arxiv.org/abs/1905.10988},
}

@InProceedings{real2017large,
  author       = {Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc V and Kurakin, Alexey},
  booktitle    = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  title        = {Large-scale evolution of image classifiers},
  year         = {2017},
  organization = {JMLR. org},
  pages        = {2902--2911},
}

@Misc{hendrycks2016gaussian,
  author        = {Dan Hendrycks and Kevin Gimpel},
  title         = {Gaussian Error Linear Units (GELUs)},
  year          = {2016},
  archiveprefix = {arXiv},
  eprint        = {1606.08415},
  primaryclass  = {cs.LG},
}

@Article{mnist,
  author    = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal   = {Proceedings of the IEEE},
  title     = {Gradient-based learning applied to document recognition},
  year      = {1998},
  number    = {11},
  pages     = {2278--2324},
  volume    = {86},
  publisher = {Ieee},
}

@InProceedings{deepgradientcompression,
  author    = {Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J},
  booktitle = {The International Conference on Learning Representations},
  title     = {{Deep Gradient Compression: Reducing the communication bandwidth for distributed training}},
  year      = {2018},
  url       = {https://openreview.net/forum?id=SkhQHMW0W},
}

@InProceedings{karimireddy2019error,
  author       = {Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
  title        = {Error Feedback Fixes {S}ign{SGD} and other Gradient Compression Schemes},
  year         = {2019},
  editor       = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  month        = {09--15 Jun},
  organization = {PMLR},
  pages        = {3252--3261},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  volume       = {97},
  abstract     = {Sign-based algorithms (e.g. signSGD) have been proposed as a biased gradient compression technique to alleviate the communication bottleneck in training large neural networks across multiple workers. We show simple convex counter-examples where signSGD does not converge to the optimum. Further, even when it does converge, signSGD may generalize poorly when compared with SGD. These issues arise because of the biased nature of the sign compression operator. We then show that using error-feedback, i.e. incorporating the error made by the compression operator into the next step, overcomes these issues. We prove that our algorithm (EF-SGD) with arbitrary compression operator achieves the same rate of convergence as SGD without any additional assumptions. Thus EF-SGD achieves gradient compression for free. Our experiments thoroughly substantiate the theory.},
  pdf          = {http://proceedings.mlr.press/v97/karimireddy19a/karimireddy19a.pdf},
  url          = {http://proceedings.mlr.press/v97/karimireddy19a.html},
}

@InProceedings{bonawitz2017practical,
  author    = {Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  title     = {Practical secure aggregation for privacy-preserving machine learning},
  year      = {2017},
  pages     = {1175--1191},
}

@Article{natural_compression,
  author        = {Samuel Horvath and Chen{-}Yu Ho and Ludovit Horvath and Atal Narayan Sahu and Marco Canini and Peter Richt{\'{a}}rik},
  journal       = {CoRR},
  title         = {Natural Compression for Distributed Deep Learning},
  year          = {2019},
  volume        = {abs/1905.10988},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1905-10988},
  eprint        = {1905.10988},
  timestamp     = {Mon, 03 Jun 2019 13:42:33 +0200},
  url           = {http://arxiv.org/abs/1905.10988},
}

@Article{learning2learn,
  author  = {Marcin Andrychowicz and Misha Denil and Sergio Gomez Colmenarejo and Matthew W. Hoffman and David Pfau and Tom Schaul and Nando de Freitas},
  journal = {ArXiv},
  title   = {Learning to learn by gradient descent by gradient descent},
  year    = {2016},
  volume  = {abs/1606.04474},
}

@Comment{jabref-meta: databaseType:bibtex;}
