\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal et~al.(2018)Agrawal, Verschueren, Diamond, and
  Boyd]{cvxpy_rewriting}
Agrawal, A., Verschueren, R., Diamond, S., and Boyd, S.
\newblock A rewriting system for convex optimization problems.
\newblock \emph{Journal of Control and Decision}, 5\penalty0 (1):\penalty0
  42--60, 2018.

\bibitem[Anandkumar \& Ge(2016)Anandkumar and Ge]{anandkumar2016saddle}
Anandkumar, A. and Ge, R.
\newblock Efficient approaches for escaping higher order saddle points in
  non-convex optimization.
\newblock In \emph{Conference on learning theory}, pp.\  81--102, 2016.

\bibitem[Arora et~al.(2018)Arora, Cohen, and
  Hazan]{arora2018overparameterization}
Arora, S., Cohen, N., and Hazan, E.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock In \emph{35th International Conference on Machine Learning, ICML
  2018}, pp.\  372--389. International Machine Learning Society (IMLS), 2018.

\bibitem[Bach(2017)]{bach2017breaking}
Bach, F.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 629--681, 2017.

\bibitem[Bartlett \& Ben-David(1999)Bartlett and
  Ben-David]{bartlett1999hardness}
Bartlett, P. and Ben-David, S.
\newblock Hardness results for neural network approximation problems.
\newblock In \emph{European Conference on Computational Learning Theory}, pp.\
  50--62. Springer, 1999.

\bibitem[Blum \& Rivest(1989)Blum and Rivest]{blum1989threenode}
Blum, A. and Rivest, R.~L.
\newblock Training a 3-node neural network is np-complete.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  494--501, 1989.

\bibitem[Boyd \& Vandenberghe(2004)Boyd and Vandenberghe]{boyd_convex}
Boyd, S. and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Brutzkus \& Globerson(2017)Brutzkus and
  Globerson]{brutzkus_localminima}
Brutzkus, A. and Globerson, A.
\newblock Globally optimal gradient descent for a convnet with gaussian inputs.
\newblock \emph{arXiv preprint arXiv:1702.07966}, 2017.

\bibitem[Brutzkus et~al.(2017)Brutzkus, Globerson, Malach, and
  Shalev{-}Shwartz]{brutzkus_overparameterized_linear}
Brutzkus, A., Globerson, A., Malach, E., and Shalev{-}Shwartz, S.
\newblock {SGD} learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock \emph{CoRR}, abs/1710.10174, 2017.
\newblock URL \url{http://arxiv.org/abs/1710.10174}.

\bibitem[Chollet(2017)]{chollet2017xception}
Chollet, F.
\newblock Xception: Deep learning with depthwise separable convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  1251--1258, 2017.

\bibitem[Cover(1965)]{cover1965geometrical}
Cover, T.~M.
\newblock Geometrical and statistical properties of systems of linear
  inequalities with applications in pattern recognition.
\newblock \emph{IEEE transactions on electronic computers}, \penalty0
  (3):\penalty0 326--334, 1965.

\bibitem[Czerniak \& Zarzycki(2003)Czerniak and Zarzycki]{acute_dataset}
Czerniak, J. and Zarzycki, H.
\newblock Application of rough sets in the presumptive diagnosis of urinary
  system diseases.
\newblock In \emph{Artificial intelligence and security in computing systems},
  pp.\  41--51. Springer, 2003.

\bibitem[DasGupta et~al.(1995)DasGupta, Siegelmann, and
  Sontag]{dasgupta1995complexity}
DasGupta, B., Siegelmann, H.~T., and Sontag, E.
\newblock On the complexity of training neural networks with continuous
  activation functions.
\newblock \emph{IEEE Transactions on Neural Networks}, 6\penalty0 (6):\penalty0
  1490--1504, 1995.

\bibitem[Diamond \& Boyd(2016)Diamond and Boyd]{cvxpy}
Diamond, S. and Boyd, S.
\newblock {CVXPY}: A {P}ython-embedded modeling language for convex
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (83):\penalty0 1--5, 2016.

\bibitem[Du \& Lee(2018)Du and Lee]{du2018overparameterized}
Du, S.~S. and Lee, J.~D.
\newblock On the power of over-parametrization in neural networks with
  quadratic activation.
\newblock \emph{arXiv preprint arXiv:1803.01206}, 2018.

\bibitem[Dua \& Graff(2017)Dua and Graff]{uci_repository}
Dua, D. and Graff, C.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Ergen \& Pilanci(2019)Ergen and Pilanci]{ergen2019cutting}
Ergen, T. and Pilanci, M.
\newblock Convex duality and cutting plane methods for over-parameterized
  neural networks.
\newblock In \emph{OPT-ML workshop}, 2019.

\bibitem[{Ergen} \& {Pilanci}(2019){Ergen} and {Pilanci}]{ergen2019shallow}
{Ergen}, T. and {Pilanci}, M.
\newblock Convex optimization for shallow neural networks.
\newblock In \emph{2019 57th Annual Allerton Conference on Communication,
  Control, and Computing (Allerton)}, pp.\  79--83, 2019.

\bibitem[Ergen \& Pilanci(2020{\natexlab{a}})Ergen and
  Pilanci]{ergen2020aistats}
Ergen, T. and Pilanci, M.
\newblock Convex geometry of two-layer relu networks: Implicit autoencoding and
  interpretable models.
\newblock In Chiappa, S. and Calandra, R. (eds.), \emph{Proceedings of the
  Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of \emph{Proceedings of Machine Learning Research},
  pp.\  4024--4033, Online, 26--28 Aug 2020{\natexlab{a}}. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v108/ergen20a.html}.

\bibitem[Ergen \& Pilanci(2020{\natexlab{b}})Ergen and
  Pilanci]{ergen2020journal}
Ergen, T. and Pilanci, M.
\newblock Convex geometry and duality of over-parameterized neural networks.
\newblock \emph{arXiv preprint arXiv:2002.11219}, 2020{\natexlab{b}}.

\bibitem[Ergen \& Pilanci(2020{\natexlab{c}})Ergen and
  Pilanci]{ergen2020workshop}
Ergen, T. and Pilanci, M.
\newblock Convex programs for global optimization of convolutional neural
  networks in polynomial-time.
\newblock In \emph{OPT-ML workshop}, 2020{\natexlab{c}}.

\bibitem[Ergen \& Pilanci(2020{\natexlab{d}})Ergen and
  Pilanci]{ergen2021revealing}
Ergen, T. and Pilanci, M.
\newblock Convex duality of deep neural networks.
\newblock \emph{CoRR}, abs/2002.09773, 2020{\natexlab{d}}.
\newblock URL \url{https://arxiv.org/abs/2002.09773}.

\bibitem[Ergen \& Pilanci(2021)Ergen and Pilanci]{ergen2020cnn}
Ergen, T. and Pilanci, M.
\newblock Implicit convex regularizers of cnn architectures: Convex
  optimization of two- and three-layer networks in polynomial time.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=0N8jUH4JMv6}.

\bibitem[Ergen et~al.(2021)Ergen, Sahiner, Ozturkler, Pauly, Mardani, and
  Pilanci]{ergen2021bn}
Ergen, T., Sahiner, A., Ozturkler, B., Pauly, J.~M., Mardani, M., and Pilanci,
  M.
\newblock Demystifying batch normalization in relu networks: Equivalent convex
  optimization models and implicit regularization.
\newblock \emph{CoRR}, abs/2103.01499, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.01499}.

\bibitem[Ge et~al.(2017)Ge, Lee, and Ma]{ge2017onehidden}
Ge, R., Lee, J.~D., and Ma, T.
\newblock Learning one-hidden-layer neural networks with landscape design,
  2017.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
Goodfellow, I., Bengio, Y., Courville, A., and Bengio, Y.
\newblock \emph{Deep learning}, volume~1.
\newblock MIT press Cambridge, 2016.

\bibitem[Grant \& Boyd(2014)Grant and Boyd]{cvx}
Grant, M. and Boyd, S.
\newblock {CVX}: Matlab software for disciplined convex programming, version
  2.1.
\newblock \url{http://cvxr.com/cvx}, March 2014.

\bibitem[Gupta et~al.(2021)Gupta, Bartan, Ergen, and
  Pilanci]{vikul2021generative}
Gupta, V., Bartan, B., Ergen, T., and Pilanci, M.
\newblock Convex neural autoregressive models: Towards tractable, expressive,
  and theoretically-backed models for sequential forecasting and generation.
\newblock In \emph{ICASSP 2021 - 2021 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}, pp.\  3890--3894, 2021.
\newblock \doi{10.1109/ICASSP39728.2021.9413662}.

\bibitem[Haeffele \& Vidal(2017)Haeffele and Vidal]{haeffele2017global}
Haeffele, B.~D. and Vidal, R.
\newblock Global optimality in neural network training.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  7331--7339, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Iandola et~al.(2016)Iandola, Han, Moskewicz, Ashraf, Dally, and
  Keutzer]{iandola2016squeezenet}
Iandola, F.~N., Han, S., Moskewicz, M.~W., Ashraf, K., Dally, W.~J., and
  Keutzer, K.
\newblock Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5
  mb model size.
\newblock \emph{arXiv preprint arXiv:1602.07360}, 2016.

\bibitem[Krizhevsky et~al.(2014)Krizhevsky, Nair, and Hinton]{cifar10}
Krizhevsky, A., Nair, V., and Hinton, G.
\newblock The {CIFAR}-10 dataset.
\newblock \url{http://www. cs. toronto. edu/kriz/cifar. html}, 2014.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and Srebro]{neyshabur_reg}
Neyshabur, B., Tomioka, R., and Srebro, N.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1412.6614}, 2014.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Li, Bhojanapalli, LeCun, and
  Srebro]{neyshabur2018overparameterization}
Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N.
\newblock Towards understanding the role of over-parametrization in
  generalization of neural networks.
\newblock \emph{arXiv preprint arXiv:1805.12076}, 2018.

\bibitem[Ojha(2000)]{ojha2000enumeration}
Ojha, P.~C.
\newblock Enumeration of linear threshold functions from the lattice of
  hyperplane intersections.
\newblock \emph{IEEE Transactions on Neural Networks}, 11\penalty0
  (4):\penalty0 839--850, 2000.

\bibitem[Pilanci \& Ergen(2020)Pilanci and Ergen]{pilanci2020convex}
Pilanci, M. and Ergen, T.
\newblock Neural networks are convex regularizers: Exact polynomial-time convex
  optimization formulations for two-layer networks.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  7695--7705. PMLR,
  13--18 Jul 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/pilanci20a.html}.

\bibitem[Rosset et~al.(2007)Rosset, Swirszcz, Srebro, and Zhu]{rosset2007}
Rosset, S., Swirszcz, G., Srebro, N., and Zhu, J.
\newblock L1 regularization in infinite dimensional feature spaces.
\newblock In \emph{International Conference on Computational Learning Theory},
  pp.\  544--558. Springer, 2007.

\bibitem[Safran \& Shamir(2018)Safran and Shamir]{shamir2018spurious}
Safran, I. and Shamir, O.
\newblock Spurious local minima are common in two-layer relu neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4433--4441. PMLR, 2018.

\bibitem[Sahiner et~al.(2021{\natexlab{a}})Sahiner, Ergen, Ozturkler, Bartan,
  Pauly, Mardani, and Pilanci]{sahiner2021gan}
Sahiner, A., Ergen, T., Ozturkler, B., Bartan, B., Pauly, J., Mardani, M., and
  Pilanci, M.
\newblock Hidden convexity of wasserstein gans: Interpretable generative models
  with closed-form solutions.
\newblock \emph{arXiv preprint arXiv:2107.05680}, 2021{\natexlab{a}}.

\bibitem[Sahiner et~al.(2021{\natexlab{b}})Sahiner, Ergen, Pauly, and
  Pilanci]{sahiner2021vectoroutput}
Sahiner, A., Ergen, T., Pauly, J.~M., and Pilanci, M.
\newblock Vector-output relu neural network problems are copositive programs:
  Convex analysis of two layer networks and polynomial-time algorithms.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=fGF8qAqpXXG}.

\bibitem[Savarese et~al.(2019)Savarese, Evron, Soudry, and
  Srebro]{infinite_width}
Savarese, P., Evron, I., Soudry, D., and Srebro, N.
\newblock How do infinite width bounded norm networks look in function space?
\newblock \emph{CoRR}, abs/1902.05040, 2019.
\newblock URL \url{http://arxiv.org/abs/1902.05040}.

\bibitem[Shalev-Shwartz et~al.(2017)Shalev-Shwartz, Shamir, and
  Shammah]{shalev2017failures}
Shalev-Shwartz, S., Shamir, O., and Shammah, S.
\newblock Failures of gradient-based deep learning.
\newblock \emph{arXiv preprint arXiv:1703.07950}, 2017.

\bibitem[Sion(1958)]{sion_minimax}
Sion, M.
\newblock On general minimax theorems.
\newblock \emph{Pacific J. Math.}, 8\penalty0 (1):\penalty0 171--176, 1958.
\newblock URL \url{https://projecteuclid.org:443/euclid.pjm/1103040253}.

\bibitem[Stanley et~al.(2004)]{stanley2004introduction}
Stanley, R.~P. et~al.
\newblock An introduction to hyperplane arrangements.
\newblock \emph{Geometric combinatorics}, 13:\penalty0 389--496, 2004.

\bibitem[Szegedy et~al.(2017)Szegedy, Ioffe, Vanhoucke, and
  Alemi]{szegedy2017inception}
Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi, A.~A.
\newblock Inception-v4, inception-resnet and the impact of residual connections
  on learning.
\newblock In \emph{Thirty-first AAAI conference on artificial intelligence},
  2017.

\bibitem[T{\"u}t{\"u}nc{\"u} et~al.(2001)T{\"u}t{\"u}nc{\"u}, Toh, and
  Todd]{tutuncu2001sdpt3}
T{\"u}t{\"u}nc{\"u}, R., Toh, K., and Todd, M.
\newblock Sdpt3—a matlab software package for semidefinite-quadratic-linear
  programming, version 3.0.
\newblock \emph{Web page http://www. math. nus. edu. sg/mattohkc/sdpt3. html},
  2001.

\bibitem[Veit et~al.(2016)Veit, Wilber, and Belongie]{veit2016ensembleresidual}
Veit, A., Wilber, M.~J., and Belongie, S.
\newblock Residual networks behave like ensembles of relatively shallow
  networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  550--558, 2016.

\bibitem[Winder(1966)]{winder1966partitions}
Winder, R.
\newblock Partitions of n-space by hyperplanes.
\newblock \emph{SIAM Journal on Applied Mathematics}, 14\penalty0 (4):\penalty0
  811--818, 1966.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{fashionmnist}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and
  He]{xie2017aggregatedresidual}
Xie, S., Girshick, R., Doll{\'a}r, P., Tu, Z., and He, K.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  1492--1500, 2017.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wideresidual}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zhang et~al.(2019)Zhang, Shao, and
  Salakhutdinov]{zhang2019multibranch}
Zhang, H., Shao, J., and Salakhutdinov, R.
\newblock Deep neural networks with multi-branch architectures are
  intrinsically less non-convex.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  1099--1109, 2019.

\end{thebibliography}
