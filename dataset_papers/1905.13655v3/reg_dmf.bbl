\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Advani and Saxe(2017)]{advani2017high}
Madhu~S Advani and Andrew~M Saxe.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock \emph{arXiv preprint arXiv:1710.03667}, 2017.

\bibitem[Agrawal et~al.(2018)Agrawal, Verschueren, Diamond, and
  Boyd]{agrawal2018rewriting}
Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd.
\newblock A rewriting system for convex optimization problems.
\newblock \emph{Journal of Control and Decision}, 5\penalty0 (1):\penalty0
  42--60, 2018.

\bibitem[Arora et~al.(2018)Arora, Cohen, and Hazan]{arora2018optimization}
Sanjeev Arora, Nadav Cohen, and Elad Hazan.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock In \emph{International Conference on Machine Learning}, pages
  244--253, 2018.

\bibitem[Arora et~al.(2019)Arora, Cohen, Golowich, and
  Hu]{arora2019convergence}
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu.
\newblock A convergence analysis of gradient descent for deep linear neural
  networks.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Bartlett et~al.(2018)Bartlett, Helmbold, and
  Long]{bartlett2018gradient}
Peter Bartlett, Dave Helmbold, and Phil Long.
\newblock Gradient descent with identity initialization efficiently learns
  positive definite linear transformations.
\newblock In \emph{International Conference on Machine Learning}, pages
  520--529, 2018.

\bibitem[Bhojanapalli et~al.(2016)Bhojanapalli, Neyshabur, and
  Srebro]{bhojanapalli2016global}
Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
\newblock Global optimality of local search for low rank matrix recovery.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3873--3881, 2016.

\bibitem[Bunse-Gerstner et~al.(1991)Bunse-Gerstner, Byers, Mehrmann, and
  Nichols]{bunse1991numerical}
Angelika Bunse-Gerstner, Ralph Byers, Volker Mehrmann, and Nancy~K Nichols.
\newblock Numerical computation of an analytic singular value decomposition of
  a matrix valued function.
\newblock \emph{Numerische Mathematik}, 60\penalty0 (1):\penalty0 1--39, 1991.

\bibitem[Burer and Monteiro(2003)]{burer2003nonlinear}
Samuel Burer and Renato~DC Monteiro.
\newblock A nonlinear programming algorithm for solving semidefinite programs
  via low-rank factorization.
\newblock \emph{Mathematical Programming}, 95\penalty0 (2):\penalty0 329--357,
  2003.

\bibitem[Cand{\`e}s and Recht(2009)]{candes2009exact}
Emmanuel~J Cand{\`e}s and Benjamin Recht.
\newblock Exact matrix completion via convex optimization.
\newblock \emph{Foundations of Computational mathematics}, 9\penalty0
  (6):\penalty0 717, 2009.

\bibitem[Chi et~al.(2018)Chi, Lu, and Chen]{chi2018nonconvex}
Yuejie Chi, Yue~M Lu, and Yuxin Chen.
\newblock Nonconvex optimization meets low-rank matrix factorization: An
  overview.
\newblock \emph{arXiv preprint arXiv:1809.09573}, 2018.

\bibitem[Davenport and Romberg(2016)]{davenport2016overview}
Mark~A Davenport and Justin Romberg.
\newblock An overview of low-rank matrix recovery from incomplete observations.
\newblock \emph{IEEE Journal of Selected Topics in Signal Processing},
  10\penalty0 (4):\penalty0 608--622, 2016.

\bibitem[De~Moor and Boyd(1989)]{de1989analytic}
B~De~Moor and S~Boyd.
\newblock Analytic properties of singular values and vectors.
\newblock \emph{Katholic Univ. Leuven, Belgium Tech. Rep}, 28:\penalty0 1989,
  1989.

\bibitem[Diamond and Boyd(2016)]{diamond2016cvxpy}
Steven Diamond and Stephen Boyd.
\newblock {CVXPY}: A {P}ython-embedded modeling language for convex
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (83):\penalty0 1--5, 2016.

\bibitem[Du and Hu(2019)]{du2019width}
Simon~S Du and Wei Hu.
\newblock Width provably matters in optimization for deep linear neural
  networks.
\newblock \emph{arXiv preprint arXiv:1901.08572}, 2019.

\bibitem[Du et~al.(2018)Du, Hu, and Lee]{du2018algorithmic}
Simon~S Du, Wei Hu, and Jason~D Lee.
\newblock Algorithmic regularization in learning deep homogeneous models:
  Layers are automatically balanced.
\newblock \emph{arXiv preprint arXiv:1806.00900}, 2018.

\bibitem[Fan and Cheng(2018)]{fan2018matrix}
Jicong Fan and Jieyu Cheng.
\newblock Matrix completion by deep matrix factorization.
\newblock \emph{Neural Networks}, 98:\penalty0 34--41, 2018.

\bibitem[Ge et~al.(2016)Ge, Lee, and Ma]{ge2016matrix}
Rong Ge, Jason~D Lee, and Tengyu Ma.
\newblock Matrix completion has no spurious local minimum.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2973--2981, 2016.

\bibitem[Ge et~al.(2017)Ge, Jin, and Zheng]{ge2017no}
Rong Ge, Chi Jin, and Yi~Zheng.
\newblock No spurious local minima in nonconvex low rank problems: A unified
  geometric analysis.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1233--1242. JMLR. org, 2017.

\bibitem[Gidel et~al.(2019)Gidel, Bach, and Lacoste-Julien]{gidel2019implicit}
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien.
\newblock Implicit regularization of discrete gradient dynamics in deep linear
  neural networks.
\newblock \emph{arXiv preprint arXiv:1904.13262}, 2019.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Suriya Gunasekar, Blake~E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,
  and Nati Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6151--6159, 2017.

\bibitem[Gunasekar et~al.(2018{\natexlab{a}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018characterizing}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, volume~80, pages 1832--1841, 2018{\natexlab{a}}.

\bibitem[Gunasekar et~al.(2018{\natexlab{b}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018implicit}
Suriya Gunasekar, Jason~D Lee, Daniel Soudry, and Nati Srebro.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9461--9471, 2018{\natexlab{b}}.

\bibitem[Hardt and Ma(2016)]{hardt2016identity}
Moritz Hardt and Tengyu Ma.
\newblock Identity matters in deep learning.
\newblock \emph{International Conference on Learning Representations}, 2016.

\bibitem[Harper and Konstan(2016)]{harper2016movielens}
F~Maxwell Harper and Joseph~A Konstan.
\newblock The movielens datasets: History and context.
\newblock \emph{Acm transactions on interactive intelligent systems (tiis)},
  5\penalty0 (4):\penalty0 19, 2016.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997flat}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Flat minima.
\newblock \emph{Neural Computation}, 9\penalty0 (1):\penalty0 1--42, 1997.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{hoffer2017train}
Elad Hoffer, Itay Hubara, and Daniel Soudry.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1731--1741, 2017.

\bibitem[Ilyashenko and Yakovenko(2008)]{ilyashenko2008lectures}
Yulij Ilyashenko and Sergei Yakovenko.
\newblock \emph{Lectures on analytic differential equations}, volume~86.
\newblock American Mathematical Soc., 2008.

\bibitem[Ji and Telgarsky(2019)]{ji2019gradient}
Ziwei Ji and Matus Telgarsky.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2017large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Krantz and Parks(2002)]{krantz2002primer}
Steven~G Krantz and Harold~R Parks.
\newblock \emph{A primer of real analytic functions}.
\newblock Springer Science \& Business Media, 2002.

\bibitem[Lampinen and Ganguli(2019)]{lampinen2019analytic}
Andrew~K Lampinen and Surya Ganguli.
\newblock An analytic theory of generalization dynamics and transfer learning
  in deep linear networks.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Li et~al.(2018)Li, Ma, and Zhang]{li2018algorithmic}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In \emph{Proceedings of the 31st Conference On Learning Theory},
  pages 2--47, 2018.

\bibitem[Li and Tang(2015)]{li2015deep}
Zechao Li and Jinhui Tang.
\newblock Deep matrix factorization for social image tag refinement and
  assignment.
\newblock In \emph{2015 IEEE 17th International Workshop on Multimedia Signal
  Processing (MMSP)}, pages 1--6. IEEE, 2015.

\bibitem[Lin et~al.(2016)Lin, Camoriano, and Rosasco]{lin2016generalization}
Junhong Lin, Raffaello Camoriano, and Lorenzo Rosasco.
\newblock Generalization properties and implicit regularization for multiple
  passes sgm.
\newblock In \emph{International Conference on Machine Learning}, pages
  2340--2348, 2016.

\bibitem[Ma et~al.(2018)Ma, Wang, Chi, and Chen]{ma2018implicit}
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen.
\newblock Implicit regularization in nonconvex statistical estimation: Gradient
  descent converges linearly for phase retrieval and matrix completion.
\newblock In \emph{International Conference on Machine Learning}, pages
  3351--3360, 2018.

\bibitem[Nacson et~al.(2019)Nacson, Lee, Gunasekar, Savarese, Srebro, and
  Soudry]{nacson2019convergence}
Mor~Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique~Pamplona
  Savarese, Nathan Srebro, and Daniel Soudry.
\newblock Convergence of gradient descent on separable data.
\newblock In \emph{Proceedings of Machine Learning Research}, volume~89, pages
  3420--3428, 2019.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and
  Srebro]{neyshabur2014search}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1412.6614}, 2014.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{neyshabur2017exploring}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5947--5956, 2017.

\bibitem[Park et~al.(2017)Park, Kyrillidis, Carmanis, and
  Sanghavi]{park2017nonsquare}
Dohyung Park, Anastasios Kyrillidis, Constantine Carmanis, and Sujay Sanghavi.
\newblock Non-square matrix sensing without spurious local minima via the
  burer-monteiro approach.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics}, pages 65--74, 2017.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock In \emph{NIPS-W}, 2017.

\bibitem[Rahaman et~al.(2018)Rahaman, Arpit, Baratin, Draxler, Lin, Hamprecht,
  Bengio, and Courville]{rahaman2018spectral}
Nasim Rahaman, Devansh Arpit, Aristide Baratin, Felix Draxler, Min Lin, Fred~A
  Hamprecht, Yoshua Bengio, and Aaron Courville.
\newblock On the spectral bias of deep neural networks.
\newblock \emph{arXiv preprint arXiv:1806.08734}, 2018.

\bibitem[Recht et~al.(2010)Recht, Fazel, and Parrilo]{recht2010guaranteed}
Benjamin Recht, Maryam Fazel, and Pablo~A Parrilo.
\newblock Guaranteed minimum-rank solutions of linear matrix equations via
  nuclear norm minimization.
\newblock \emph{SIAM review}, 52\penalty0 (3):\penalty0 471--501, 2010.

\bibitem[Roy and Vetterli(2007)]{roy2007effective}
Olivier Roy and Martin Vetterli.
\newblock The effective rank: A measure of effective dimensionality.
\newblock In \emph{2007 15th European Signal Processing Conference}, pages
  606--610. IEEE, 2007.

\bibitem[Saxe et~al.(2014)Saxe, McClelland, and Ganguli]{saxe2014exact}
Andrew~M Saxe, James~L McClelland, and Surya Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{International Conference on Learning Representations}, 2014.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Townsend(2016)]{townsend2016differentiating}
James Townsend.
\newblock Differentiating the singular value decomposition.
\newblock Technical report, 2016.

\bibitem[Trigeorgis et~al.(2017)Trigeorgis, Bousmalis, Zafeiriou, and
  Schuller]{trigeorgis2017deep}
George Trigeorgis, Konstantinos Bousmalis, Stefanos Zafeiriou, and Bj{\"o}rn~W
  Schuller.
\newblock A deep matrix factorization method for learning attribute
  representations.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 39\penalty0 (3):\penalty0 417--429, 2017.

\bibitem[Tu et~al.(2016)Tu, Boczar, Simchowitz, Soltanolkotabi, and
  Recht]{tu2016low}
Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Ben Recht.
\newblock Low-rank solutions of linear matrix equations via procrustes flow.
\newblock In \emph{International Conference on Machine Learning}, pages
  964--973, 2016.

\bibitem[Wang et~al.(2017)Wang, Sun, Zhan, Thompson, Ji, and
  Zhou]{wang2017multi}
Qi~Wang, Mengying Sun, Liang Zhan, Paul Thompson, Shuiwang Ji, and Jiayu Zhou.
\newblock Multi-modality disease modeling via collective deep matrix
  factorization.
\newblock In \emph{Proceedings of the 23rd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pages 1155--1164. ACM, 2017.

\bibitem[Xue et~al.(2017)Xue, Dai, Zhang, Huang, and Chen]{xue2017deep}
Hong-Jian Xue, Xinyu Dai, Jianbing Zhang, Shujian Huang, and Jiajun Chen.
\newblock Deep matrix factorization models for recommender systems.
\newblock In \emph{IJCAI}, pages 3203--3209, 2017.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you2017scaling}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Scaling sgd batch size to 32k for imagenet training.
\newblock \emph{arXiv preprint arXiv:1708.03888}, 2017.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Zhao et~al.(2017)Zhao, Ding, and Fu]{zhao2017multi}
Handong Zhao, Zhengming Ding, and Yun Fu.
\newblock Multi-view clustering via deep matrix factorization.
\newblock In \emph{Thirty-First AAAI Conference on Artificial Intelligence},
  2017.

\end{thebibliography}
