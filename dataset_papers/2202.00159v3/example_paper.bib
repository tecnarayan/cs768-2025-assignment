% -----------------------------------------
% Sugandha Sharma
% MESH citations

@ARTICLE{Figueredo:2009dg,
   author = {Figueredo, A.~J. and Wolf, P. S.~A.},
   title = {Assortative pairing and life history strategy -- a cross-cultural study},
   journal = {Human Nature},
   volume = {20},
   pages = {317-330},
   year = {2009},
   doi={https://doi.org/10.1007/s12110-009-9068-2}
}

@MISC{Hao:gidmaps:2014,
  author = {Hao, Z. and AghaKouchak, A. and Nakhjiri, N. and Farahmand, A},
  year = {2014},
  title = {Global integrated drought monitoring and prediction system ({GIDMaPS}) data sets},
  howpublished = {\emph{figshare} \url{http://dx.doi.org/10.6084/m9.figshare.853801}}
}

-----------------------------------------------------
@article{majani1988k,
  title={On the K-winners-take-all network},
  author={Majani, E and Erlanson, Ruth and Abu-Mostafa, Yaser},
  journal={Advances in neural information processing systems},
  volume={1},
  year={1988}
}

@article{hopfield1984neurons,
  title={Neurons with graded response have collective computational properties like those of two-state neurons},
  author={Hopfield, John J},
  journal={Proceedings of the national academy of sciences},
  volume={81},
  number={10},
  pages={3088--3092},
  year={1984},
  publisher={National Acad Sciences}
}

@article{hopfield1982neural,
  title={Neural networks and physical systems with emergent collective computational abilities},
  author={Hopfield, John J},
  journal={Proceedings of the national academy of sciences},
  volume={79},
  number={8},
  pages={2554--2558},
  year={1982},
  publisher={National Acad Sciences}
}


% pseudo inverse learning: degenerate spln when trained on N patterns
@article{personnaz1986collective,
  title={Collective computational properties of neural networks: New learning mechanisms},
  author={Personnaz, L and Guyon, I and Dreyfus, G},
  journal={Physical Review A},
  volume={34},
  number={5},
  pages={4217},
  year={1986},
  publisher={APS}
}

@article{dominguez2007information,
  title={Information and topology in attractor neural networks},
  author={Dominguez, David and Koroutchev, Kostadin and Serrano, Eduardo and Rodr{\'\i}guez, Francisco B},
  journal={Neural computation},
  volume={19},
  number={4},
  pages={956--973},
  year={2007},
  publisher={MIT Press}
}

% online psuedo inverse
@article{tapson2013learning,
  title={Learning the pseudoinverse solution to network weights},
  author={Tapson, Jonathan and van Schaik, Andr{\'e}},
  journal={Neural Networks},
  volume={45},
  pages={94--100},
  year={2013},
  publisher={Elsevier}
}

@article{personnaz1985information,
  title={Information storage and retrieval in spin-glass like neural networks},
  author={Personnaz, L and Guyon, I and Dreyfus, G},
  journal={Journal de Physique Lettres},
  volume={46},
  number={8},
  pages={359--365},
  year={1985},
  publisher={Les Editions de Physique}
}

@article{kanter1987associative,
  title={Associative recall of memory without errors},
  author={Kanter, I and Sompolinsky, Haim},
  journal={Physical Review A},
  volume={35},
  number={1},
  pages={380},
  year={1987},
  publisher={APS}
}

@book{eliasmith2003neural,
  title={Neural engineering: Computation, representation, and dynamics in neurobiological systems},
  author={Eliasmith, Chris and Anderson, Charles H},
  year={2003},
  publisher={MIT press}
}

@article{krotov2016dense,
  title={Dense associative memory for pattern recognition},
  author={Krotov, Dmitry and Hopfield, John J},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={1172--1180},
  year={2016}
}

@article{ramsauer2020hopfield,
  title={Hopfield networks is all you need},
  author={Ramsauer, Hubert and Sch{\"a}fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlovi{\'c}, Milena and Sandve, Geir Kjetil and others},
  journal={arXiv preprint arXiv:2008.02217},
  year={2020}
}

@article{krotov2020large,
  title={Large associative memory problem in neurobiology and machine learning},
  author={Krotov, Dmitry and Hopfield, John},
  journal={arXiv preprint arXiv:2008.06996},
  year={2020}
}

@article{abu1989information,
  title={Information theory, complexity and neural networks},
  author={Abu-Mostafa, Yaser S},
  journal={IEEE Communications Magazine},
  volume={27},
  number={11},
  pages={25--28},
  year={1989},
  publisher={IEEE}
}

@article{gardner1988space,
  title={The space of interactions in neural network models},
  author={Gardner, Elizabeth},
  journal={Journal of physics A: Mathematical and general},
  volume={21},
  number={1},
  pages={257},
  year={1988},
  publisher={IOP Publishing}
}

@article{dragoi2011preplay,
  title={Preplay of future place cell sequences by hippocampal cellular assemblies},
  author={Dragoi, George and Tonegawa, Susumu},
  journal={Nature},
  volume={469},
  number={7330},
  pages={397--401},
  year={2011},
  publisher={Nature Publishing Group}
}

@article{hopfield2007hopfield,
  title={Hopfield network},
  author={Hopfield, John J},
  journal={Scholarpedia},
  volume={2},
  number={5},
  pages={1977},
  year={2007}
}


@article{tsodyks1988enhanced,
  title={The enhanced storage capacity in neural networks with low activity level},
  author={Tsodyks, Mikhail V and Feigel'man, Mikhail V},
  journal={EPL (Europhysics Letters)},
  volume={6},
  number={2},
  pages={101},
  year={1988},
  publisher={IOP Publishing}
}

@article{radhakrishnan2020overparameterized,
  title={Overparameterized neural networks implement associative memory},
  author={Radhakrishnan, Adityanarayanan and Belkin, Mikhail and Uhler, Caroline},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={44},
  pages={27162--27170},
  year={2020},
  publisher={National Acad Sciences}
}

@article{chaudhuri2019bipartite,
  title={Bipartite expander Hopfield networks as self-decoding high-capacity error correcting codes},
  author={Chaudhuri, Rishidev and Fiete, Ila},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{sharma2017spiking,
  title={A Spiking Neural Bayesian Model of Life Span Inference.},
  author={Sharma, Sugandha and Voelker, Aaron and Eliasmith, Chris},
  booktitle={CogSci},
  year={2017}
}

@inproceedings{sharma2016neural,
  title={A Neural Model of Context Dependent Decision Making in the Prefrontal Cortex.},
  author={Sharma, Sugandha and Komer, Brent and Stewart, Terrence C and Eliasmith, Chris},
  booktitle={CogSci},
  year={2016}
}

% bounded synapses
@article{fusi2007limits,
  title={Limits on the memory storage capacity of bounded synapses},
  author={Fusi, Stefano and Abbott, LF},
  journal={Nature neuroscience},
  volume={10},
  number={4},
  pages={485--493},
  year={2007},
  publisher={Nature Publishing Group}
}

% bounded synapses
@article{parisi1986memory,
  title={A memory which forgets},
  author={Parisi, Giorgio},
  journal={Journal of Physics A: Mathematical and General},
  volume={19},
  number={10},
  pages={L617},
  year={1986},
  publisher={IOP Publishing}
}

@article{bolle2000mutual,
  title={Mutual information of sparsely coded associative memory with self-control and ternary neurons},
  author={Boll{\'e}, D{\'e}sir{\'e} and Dominguez, DRC and Amari, Shun-ichi},
  journal={Neural Networks},
  volume={13},
  number={4-5},
  pages={455--462},
  year={2000},
  publisher={Elsevier}
}


@article{rutishauser2011collective,
  title={Collective stability of networks of winner-take-all circuits},
  author={Rutishauser, Ueli and Douglas, Rodney J and Slotine, Jean-Jacques},
  journal={Neural computation},
  volume={23},
  number={3},
  pages={735--773},
  year={2011},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{wang2003k,
  title={K-winners-take-all computation with neural oscillators},
  author={Wang, Wei and Slotine, Jean-Jacques E},
  journal={arXiv preprint q-bio/0401001},
  year={2003}
}

@article{yang1997dynamic,
  title={A dynamic K-winners-take-all neural network},
  author={Yang, Jar-Ferr and Chen, Chi-Ming},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  volume={27},
  number={3},
  pages={523--526},
  year={1997},
  publisher={IEEE}
}

@article{crisanti1986saturation,
  title={Saturation level of the Hopfield model for neural network},
  author={Crisanti, Andrea and Amit, Daniel J and Gutfreund, Hanoch},
  journal={EPL (Europhysics Letters)},
  volume={2},
  number={4},
  pages={337},
  year={1986},
  publisher={IOP Publishing}
}

@article{nadal1986networks,
  title={Networks of formal neurons and memory palimpsests},
  author={Nadal, JP and Toulouse, G and Changeux, JP and Dehaene, S},
  journal={EPL (Europhysics Letters)},
  volume={1},
  number={10},
  pages={535},
  year={1986},
  publisher={IOP Publishing}
}

@inproceedings{storkey1997increasing,
  title={Increasing the capacity of a Hopfield network without sacrificing functionality},
  author={Storkey, Amos},
  booktitle={International Conference on Artificial Neural Networks},
  pages={451--456},
  year={1997},
  organization={Springer}
}

@article{refregier1989improved,
  title={An improved version of the pseudo-inverse solution for classification and neural networks},
  author={Refregier, Ph and Vignolle, J-M},
  journal={EPL (Europhysics Letters)},
  volume={10},
  number={4},
  pages={387},
  year={1989},
  publisher={IOP Publishing}
}


@article{van2012soft,
  title={Soft-bound synaptic plasticity increases storage capacity},
  author={Van Rossum, Mark CW and Shippi, Maria and Barrett, Adam B},
  journal={PLoS computational biology},
  volume={8},
  number={12},
  pages={e1002836},
  year={2012},
  publisher={Public Library of Science San Francisco, USA}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{sukhbaatar2015end,
  title={End-to-end memory networks},
  author={Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
  journal={arXiv preprint arXiv:1503.08895},
  year={2015}
}

@article{graves2014neural,
  title={Neural turing machines},
  author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal={arXiv preprint arXiv:1410.5401},
  year={2014}
}

@article{graves2016hybrid,
  title={Hybrid computing using a neural network with dynamic external memory},
  author={Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'n}ska, Agnieszka and Colmenarejo, Sergio G{\'o}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and others},
  journal={Nature},
  volume={538},
  number={7626},
  pages={471--476},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{le2019neural,
  title={Neural stored-program memory},
  author={Le, Hung and Tran, Truyen and Venkatesh, Svetha},
  journal={arXiv preprint arXiv:1906.08862},
  year={2019}
}

@article{banino2020memo,
  title={Memo: A deep network for flexible combination of episodic memories},
  author={Banino, Andrea and Badia, Adri{\`a} Puigdom{\`e}nech and K{\"o}ster, Raphael and Chadwick, Martin J and Zambaldi, Vinicius and Hassabis, Demis and Barry, Caswell and Botvinick, Matthew and Kumaran, Dharshan and Blundell, Charles},
  journal={arXiv preprint arXiv:2001.10913},
  year={2020}
}

@article{eichenbaum2014can,
  title={Can we reconcile the declarative memory and spatial navigation views on hippocampal function?},
  author={Eichenbaum, Howard and Cohen, Neal J},
  journal={Neuron},
  volume={83},
  number={4},
  pages={764--770},
  year={2014},
  publisher={Elsevier}
}

@article{manns2006evolution,
  title={Evolution of declarative memory},
  author={Manns, Joseph R and Eichenbaum, Howard},
  journal={Hippocampus},
  volume={16},
  number={9},
  pages={795--808},
  year={2006},
  publisher={Wiley Online Library}
}

@article{davidson2009hippocampal,
  title={Hippocampal replay of extended experience},
  author={Davidson, Thomas J and Kloosterman, Fabian and Wilson, Matthew A},
  journal={Neuron},
  volume={63},
  number={4},
  pages={497--507},
  year={2009},
  publisher={Elsevier}
}

@article{foster2006reverse,
  title={Reverse replay of behavioural sequences in hippocampal place cells during the awake state},
  author={Foster, David J and Wilson, Matthew A},
  journal={Nature},
  volume={440},
  number={7084},
  pages={680--683},
  year={2006},
  publisher={Nature Publishing Group}
}

@article{pfeiffer2013hippocampal,
  title={Hippocampal place-cell sequences depict future paths to remembered goals},
  author={Pfeiffer, Brad E and Foster, David J},
  journal={Nature},
  volume={497},
  number={7447},
  pages={74--79},
  year={2013},
  publisher={Nature Publishing Group}
}


@article{Mulders2021.11.20.469406,
	Author = {Mulders, Dounia and Yim, Man Yi and Lee, Jae Sung and Lee, Albert K. and Taillefumier, Thibaud and Fiete, Ila R.},
	Journal = {bioRxiv},
	Title = {A structured scaffold underlies activity in the hippocampus},
	Year = {2021}}


@book{hertz2018introduction,
  title={Introduction to the theory of neural computation},
  author={Hertz, John and Krogh, Anders and Palmer, Richard G},
  year={2018},
  publisher={CRC Press}
}

@article{xiao2017fashion,
  title={Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}

@article{demircigil2017model,
  title={On a model of associative memory with huge storage capacity},
  author={Demircigil, Mete and Heusel, Judith and L{\"o}we, Matthias and Upgang, Sven and Vermet, Franck},
  journal={Journal of Statistical Physics},
  volume={168},
  number={2},
  pages={288--299},
  year={2017},
  publisher={Springer}
}


@article{komlos1988convergence,
  title={Convergence results in an associative memory model},
  author={Koml{\'o}s, J{\'a}nos and Paturi, Ramamohan},
  journal={Neural Networks},
  volume={1},
  number={3},
  pages={239--250},
  year={1988},
  publisher={Elsevier}
}

@inproceedings{frolov2000convergence,
  title={Convergence time in hopfield network},
  author={Frolov, Alexander A and H{\'u}sek, Dusan},
  booktitle={Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium},
  volume={5},
  pages={622--626},
  year={2000},
  organization={IEEE}
}

@article{kohring1990convergence,
  title={Convergence time and finite size effects in neural networks},
  author={Kohring, GA},
  journal={Journal of Physics A: Mathematical and General},
  volume={23},
  number={11},
  pages={2237},
  year={1990},
  publisher={IOP Publishing}
}

@inproceedings{
tyulmankov2021biological,
title={Biological learning in key-value memory networks},
author={Danil Tyulmankov and Ching Fang and Annapurna Vadaparty and Guangyu Robert Yang},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=sMRdrUIrZbT}
}


@article{10.7554/eLife.62702,
	abstract = {What factors constrain the arrangement of the multiple fields of a place cell? By modeling place cells as perceptrons that act on multiscale periodic grid-cell inputs, we analytically enumerate a place cell's \textit{repertoire} -- how many field arrangements it can realize without external cues while its grid inputs are unique -- and derive its \textit{capacity} -- the spatial range over which it can achieve any field arrangement. We show that the repertoire is very large and relatively noise-robust. However, the repertoire is a vanishing fraction of all arrangements, while capacity scales only as the sum of the grid periods so field arrangements are constrained over larger distances. Thus, grid-driven place field arrangements define a large response scaffold that is strongly constrained by its structured inputs. Finally, we show that altering grid-place weights to generate an arbitrary new place field strongly affects existing arrangements, which could explain the volatility of the place code.},
	article_type = {journal},
	author = {Yim, Man Yi and Sadun, Lorenzo A and Fiete, Ila R and Taillefumier, Thibaud},
	citation = {eLife 2021;10:e62702},
	doi = {10.7554/eLife.62702},
	editor = {Berman, Gordon J and Frank, Michael J and Brunel, Nicolas},
	issn = {2050-084X},
	journal = {eLife},
	keywords = {place cells, grid cells, perceptron, linear separability, capacity, volatility},
	month = {may},
	pages = {e62702},
	pub_date = {2021-05-24},
	publisher = {eLife Sciences Publications, Ltd},
	title = {Place-cell capacity and volatility with grid-like inputs},
	url = {https://doi.org/10.7554/eLife.62702},
	volume = 10,
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.7554/eLife.62702}}

@article{lukovsevivcius2009reservoir,
  title={Reservoir computing approaches to recurrent neural network training},
  author={Luko{\v{s}}evi{\v{c}}ius, Mantas and Jaeger, Herbert},
  journal={Computer Science Review},
  volume={3},
  number={3},
  pages={127--149},
  year={2009},
  publisher={Elsevier}
}

@article{jaeger2001echo,
  title={The “echo state” approach to analysing and training recurrent neural networks-with an erratum note},
  author={Jaeger, Herbert},
  journal={Bonn, Germany: German National Research Center for Information Technology GMD Technical Report},
  volume={148},
  number={34},
  pages={13},
  year={2001},
  publisher={Bonn}
}



@article{10.1371/journal.pcbi.0030141,
	abstract = {A fundamental problem in neuroscience is understanding how working memory---the ability to store information at intermediate timescales, like tens of seconds---is implemented in realistic neuronal networks. The most likely candidate mechanism is the attractor network, and a great deal of effort has gone toward investigating it theoretically. Yet, despite almost a quarter century of intense work, attractor networks are not fully understood. In particular, there are still two unanswered questions. First, how is it that attractor networks exhibit irregular firing, as is observed experimentally during working memory tasks? And second, how many memories can be stored under biologically realistic conditions? Here we answer both questions by studying an attractor neural network in which inhibition and excitation balance each other. Using mean-field analysis, we derive a three-variable description of attractor networks. From this description it follows that irregular firing can exist only if the number of neurons involved in a memory is large. The same mean-field analysis also shows that the number of memories that can be stored in a network scales with the number of excitatory connections, a result that has been suggested for simple models but never shown for realistic ones. Both of these predictions are verified using simulations with large networks of spiking neurons.},
	author = {Roudi, Yasser AND Latham, Peter E},
	doi = {10.1371/journal.pcbi.0030141},
	journal = {PLOS Computational Biology},
	month = {09},
	number = {9},
	pages = {1-22},
	publisher = {Public Library of Science},
	title = {A Balanced Memory Network},
	url = {https://doi.org/10.1371/journal.pcbi.0030141},
	volume = {3},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pcbi.0030141}}
