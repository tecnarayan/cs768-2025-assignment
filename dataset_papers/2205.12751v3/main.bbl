\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abernethy et~al.(2014)Abernethy, Lee, Sinha, and
  Tewari]{abernethy2014online}
Jacob Abernethy, Chansoo Lee, Abhinav Sinha, and Ambuj Tewari.
\newblock Online linear optimization via smoothing.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2014.

\bibitem[Abernethy et~al.(2016)Abernethy, Lee, and
  Tewari]{abernethy2016perturbation}
Jacob Abernethy, Chansoo Lee, and Ambuj Tewari.
\newblock Perturbation techniques in online learning and optimization.
\newblock \emph{Perturbations, Optimization, and Statistics}, page 223, 2016.

\bibitem[Aybat et~al.(2019)Aybat, Fallah, Gurbuzbalaban, and
  Ozdaglar]{aybat2019universally}
Necdet~S. Aybat, Alireza Fallah, Mert Gurbuzbalaban, and Asuman Ozdaglar.
\newblock A universally optimal multistage accelerated stochastic gradient
  method.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Bach(2015)]{bach2015duality}
Francis Bach.
\newblock Duality between subgradient and conditional gradient methods.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (1):\penalty0
  115--129, 2015.

\bibitem[Bach(2021)]{bach2020effectiveness}
Francis Bach.
\newblock On the effectiveness of {R}ichardson extrapolation in data science.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 3\penalty0
  (4):\penalty0 1251--1277, 2021.

\bibitem[Ball et~al.(1994)Ball, Carlen, and Lieb]{ball1994sharp}
Keith Ball, Eric~A Carlen, and Elliott~H Lieb.
\newblock Sharp uniform convexity and smoothness inequalities for trace norms.
\newblock \emph{Inventiones mathematicae}, 115\penalty0 (1):\penalty0 463--482,
  1994.

\bibitem[Berthet et~al.(2020)Berthet, Blondel, Teboul, Cuturi, Vert, and
  Bach]{berthet2020learning}
Quentin Berthet, Mathieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe
  Vert, and Francis Bach.
\newblock Learning with differentiable pertubed optimizers.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Blondel et~al.(2021)Blondel, Berthet, Cuturi, Frostig, Hoyer,
  Llinares-L{\'o}pez, Pedregosa, and Vert]{jaxopt_implicit_diff}
Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer,
  Felipe Llinares-L{\'o}pez, Fabian Pedregosa, and Jean-Philippe Vert.
\newblock Efficient and modular implicit differentiation.
\newblock \emph{arXiv preprint arXiv:2105.15183}, 2021.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and
  Nocedal]{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{Siam Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Cohen et~al.(2021)Cohen, Sidford, and Tian]{cohen2021relative}
Michael~B Cohen, Aaron Sidford, and Kevin Tian.
\newblock Relative lipschitzness in extragradient methods and a direct recipe
  for acceleration.
\newblock In \emph{12th Innovations in Theoretical Computer Science Conference
  (ITCS 2021)}, volume 185, 2021.

\bibitem[Demyanov and Rubinov(1970)]{demianov1970approximate}
Vladimir~Fedorovich Demyanov and Aleksandr~Moiseevich Rubinov.
\newblock \emph{Approximate methods in optimization problems}.
\newblock Elsevier Publishing Company, 1970.

\bibitem[Devolder(2013)]{devolder2013exactness}
Olivier Devolder.
\newblock \emph{Exactness, inexactness and stochasticity in first-order methods
  for large-scale convex optimization}.
\newblock PhD thesis, PhD thesis, ICTEAM and CORE, Universit{\'e} Catholique de
  Louvain, 2013.

\bibitem[Diakonikolas and Guzm{\'a}n(2021)]{diakonikolas2021complementary}
Jelena Diakonikolas and Crist{\'o}bal Guzm{\'a}n.
\newblock Complementary composite minimization, small gradients in general
  norms, and applications to regression problems.
\newblock \emph{arXiv preprint 2101.11041}, 2021.

\bibitem[Duchi et~al.(2012)Duchi, Bartlett, and
  Wainwright]{duchi2012randomized}
John~C. Duchi, Peter~L. Bartlett, and Martin~J. Wainwright.
\newblock Randomized smoothing for stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  674--701, 2012.

\bibitem[d’Aspremont et~al.(2021)d’Aspremont, Scieur, and
  Taylor]{d2021acceleration}
Alexandre d’Aspremont, Damien Scieur, and Adrien Taylor.
\newblock Acceleration methods.
\newblock \emph{Foundations and Trends{\textregistered} in Optimization},
  5\penalty0 (1-2):\penalty0 1--245, 2021.

\bibitem[Frank and Wolfe(1956)]{frank1956algorithm}
Marguerite Frank and Philip Wolfe.
\newblock An algorithm for quadratic programming.
\newblock \emph{Naval research logistics quarterly}, 3\penalty0 (1-2):\penalty0
  95--110, 1956.

\bibitem[Freund et~al.(2017)Freund, Grigas, and Mazumder]{freund2017extended}
Robert~M. Freund, Paul Grigas, and Rahul Mazumder.
\newblock An extended {F}rank--{W}olfe method with “in-face” directions,
  and its application to low-rank matrix completion.
\newblock \emph{SIAM Journal on optimization}, 27\penalty0 (1):\penalty0
  319--346, 2017.

\bibitem[Garber and Hazan(2015)]{garber2015faster}
Dan Garber and Elad Hazan.
\newblock Faster rates for the frank-wolfe method over strongly-convex sets.
\newblock In \emph{International Conference on Machine Learning}, pages
  541--549. PMLR, 2015.

\bibitem[Gasnikov and Nesterov(2018)]{gasnikov2018universal}
Alexander~V. Gasnikov and Yurii Nesterov.
\newblock Universal method for stochastic composite optimization problems.
\newblock \emph{Computational Mathematics and Mathematical Physics},
  58\penalty0 (1):\penalty0 48--64, 2018.

\bibitem[Ghadimi(2019)]{ghadimi2019conditional}
Saeed Ghadimi.
\newblock Conditional gradient type methods for composite nonlinear and
  stochastic optimization.
\newblock \emph{Mathematical Programming}, 173\penalty0 (1):\penalty0 431--464,
  2019.

\bibitem[Gu{\'e}lat and Marcotte(1986)]{guelat1986some}
Jacques Gu{\'e}lat and Patrice Marcotte.
\newblock Some comments on {W}olfe's ‘away step’.
\newblock \emph{Mathematical Programming}, 35\penalty0 (1):\penalty0 110--119,
  1986.

\bibitem[Gumbel(1954)]{gumbel1954statistical}
Emil~J. Gumbel.
\newblock \emph{Statistical theory of extreme values and some practical
  applications: a series of lectures}, volume~33.
\newblock US Government Printing Office, 1954.

\bibitem[Harchaoui et~al.(2015)Harchaoui, Juditsky, and
  Nemirovski]{harchaoui2015conditional}
Zaid Harchaoui, Anatoli Juditsky, and Arkadi Nemirovski.
\newblock Conditional gradient algorithms for norm-regularized smooth convex
  optimization.
\newblock \emph{Mathematical Programming}, 152\penalty0 (1):\penalty0 75--112,
  2015.

\bibitem[Hazan and Luo(2016)]{hazan2016variance}
Elad Hazan and Haipeng Luo.
\newblock Variance-reduced and projection-free stochastic optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1263--1271. PMLR, 2016.

\bibitem[Hendrikx et~al.(2020)Hendrikx, Bach, and
  Massouli{\'e}]{hendrikx2020dual}
Hadrien Hendrikx, Francis Bach, and Laurent Massouli{\'e}.
\newblock Dual-free stochastic decentralized optimization with variance
  reduction.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem[Jaggi(2011)]{jaggi2011sparse}
Martin Jaggi.
\newblock \emph{Sparse Convex Optimization Methods for Machine Learning}.
\newblock PhD thesis, ETH Zurich, 2011.

\bibitem[Jaggi(2013)]{jaggi2013revisiting}
Martin Jaggi.
\newblock Revisiting frank-wolfe: Projection-free sparse convex optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2013.

\bibitem[Jin et~al.(2022)Jin, Sidford, and Tian]{jin2022sharper}
Yujia Jin, Aaron Sidford, and Kevin Tian.
\newblock Sharper rates for separable minimax and finite sum optimization via
  primal-dual extragradient methods.
\newblock \emph{arXiv preprint arXiv:2202.04640}, 2022.

\bibitem[Kakade et~al.(2009)Kakade, Shalev-Shwartz, Tewari,
  et~al.]{kakade2009duality}
Sham Kakade, Shai Shalev-Shwartz, Ambuj Tewari, et~al.
\newblock On the duality of strong convexity and strong smoothness: Learning
  applications and matrix regularization.
\newblock \emph{Unpublished Manuscript, http://ttic. uchicago.
  edu/shai/papers/KakadeShalevTewari09. pdf}, 2\penalty0 (1), 2009.

\bibitem[Kakade et~al.(2008)Kakade, Sridharan, and
  Tewari]{kakade2008complexity}
Sham~M Kakade, Karthik Sridharan, and Ambuj Tewari.
\newblock On the complexity of linear prediction: Risk bounds, margin bounds,
  and regularization.
\newblock \emph{Advances in neural information processing systems}, 21, 2008.

\bibitem[Lacoste-Julien and Jaggi(2015)]{lacoste2015global}
Simon Lacoste-Julien and Martin Jaggi.
\newblock On the global linear convergence of frank-wolfe optimization
  variants.
\newblock \emph{Advances in Neural Information Processing Systems (NIPS)},
  2015.

\bibitem[Lan(2012)]{lan2012optimal}
Guanghui Lan.
\newblock An optimal method for stochastic composite optimization.
\newblock \emph{Mathematical Programming}, 133\penalty0 (1):\penalty0 365--397,
  2012.

\bibitem[Lan(2013)]{lan2013complexity}
Guanghui Lan.
\newblock The complexity of large-scale convex programming under a linear
  optimization oracle.
\newblock \emph{arXiv preprint 1309.5550}, 2013.

\bibitem[Lan and Zhou(2016)]{lan2016conditional}
Guanghui Lan and Yi~Zhou.
\newblock Conditional gradient sliding for convex optimization.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (2):\penalty0
  1379--1409, 2016.

\bibitem[Levitin and Polyak(1966)]{levitin1966constrained}
Evgeny~S. Levitin and Boris~T. Polyak.
\newblock Constrained minimization methods.
\newblock \emph{USSR Computational mathematics and mathematical physics},
  6\penalty0 (5):\penalty0 1--50, 1966.

\bibitem[Li et~al.(2020)Li, Coutino, Giannakis, and Leus]{li2020does}
Bingcong Li, Mario Coutino, Georgios~B. Giannakis, and Geert Leus.
\newblock How does momentum help {F}rank {W}olfe?
\newblock \emph{arXiv preprint 2006.11116}, 2020.

\bibitem[N{\'e}giar et~al.(2020)N{\'e}giar, Dresdner, Tsai, El~Ghaoui,
  Locatello, Freund, and Pedregosa]{negiar2020stochastic}
Geoffrey N{\'e}giar, Gideon Dresdner, Alicia Tsai, Laurent El~Ghaoui, Francesco
  Locatello, Robert Freund, and Fabian Pedregosa.
\newblock Stochastic frank-wolfe for constrained finite-sum minimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  7253--7262. PMLR, 2020.

\bibitem[Nesterov(1983)]{nesterov1983method}
Yurii Nesterov.
\newblock A method for unconstrained convex minimization problem with the rate
  of convergence ${O}(1/k^2)$.
\newblock In \emph{Doklady an USSR}, volume 269, pages 543--547, 1983.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Yurii Nesterov.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Nesterov(2013)]{nesterov2013gradient}
Yurii Nesterov.
\newblock Gradient methods for minimizing composite functions.
\newblock \emph{Mathematical Programming}, 140\penalty0 (1):\penalty0 125--161,
  2013.

\bibitem[Nesterov and Spokoiny(2017)]{nesterov2017random}
Yurii Nesterov and Vladimir Spokoiny.
\newblock Random gradient-free minimization of convex functions.
\newblock \emph{Foundations of Computational Mathematics}, 17\penalty0
  (2):\penalty0 527--566, 2017.

\bibitem[Rockafellar(2015)]{rockafellar2015convex}
Ralph~T. Rockafellar.
\newblock \emph{Convex Analysis}.
\newblock Princeton University Press, 2015.

\bibitem[Rockafellar and Wets(2009)]{rockafellar2009variational}
Ralph~T. Rockafellar and Roger J.-B. Wets.
\newblock \emph{Variational Analysis}.
\newblock Springer Science \& Business Media, 2009.

\bibitem[Vaswani et~al.(2019)Vaswani, Mishkin, Laradji, Schmidt, Gidel, and
  Lacoste-Julien]{vaswani2019painless}
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and
  Simon Lacoste-Julien.
\newblock Painless stochastic gradient: Interpolation, line-search, and
  convergence rates.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Vaswani et~al.(2022)Vaswani, Dubois-Taine, and
  Babanezhad]{vaswani2022towards}
Sharan Vaswani, Benjamin Dubois-Taine, and Reza Babanezhad.
\newblock Towards noise-adaptive, problem-adaptive (accelerated) stochastic
  gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  22015--22059. PMLR, 2022.

\bibitem[Wang and Xiao(2017)]{wang2017exploiting}
Jialei Wang and Lin Xiao.
\newblock Exploiting strong convexity from data with primal-dual first-order
  algorithms.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\end{thebibliography}
