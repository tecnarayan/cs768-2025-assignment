\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2022)Abbasi-Yadkori, Gy\"{o}rgy, and
  Lazic]{abbasi-yadkori2022}
Yasin Abbasi-Yadkori, Andr\'{a}s Gy\"{o}rgy, and Nevena Lazic.
\newblock A new look at dynamic regret for non-stationary stochastic bandits.
\newblock \emph{arXiv preprint arXiv:2201.06532}, 2022.

\bibitem[Agarwal et~al.(2014)Agarwal, Hsu, Kale, Langford, Li, and
  Schapire]{agarwal14}
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert
  Schapire.
\newblock Taming the monster: A fast and simple algorithm for contextual
  bandits.
\newblock volume~32 of \emph{Proceedings of Machine Learning Research}, pages
  1638--1646. PMLR, 22--24 Jun 2014.

\bibitem[Allesiardo et~al.(2017)Allesiardo, F\'{e}raud, and
  Maillard]{allesiardo2017}
Robin Allesiardo, Rapha\"{e}l F\'{e}raud, and Odalric-Ambrym Maillard.
\newblock The non-stationary stochastic multi-armed bandit problem.
\newblock \emph{International Journal of Data Science and Analytics},
  3\penalty0 (4):\penalty0 267--283, 2017.

\bibitem[Arya and Yang(2020)]{arya20}
Sakshi Arya and Yuhong Yang.
\newblock Randomized allocation with nonparametric estimation for contextual
  multi-armed bandits with delayed rewards.
\newblock \emph{Statistics \& Probability Letters}, 164:\penalty0 108818, 2020.
\newblock ISSN 0167-7152.

\bibitem[Audibert and Tsybakov(2007)]{audibert-tsybakov}
Jean-Yves Audibert and Alexander~B Tsybakov.
\newblock Fast learning rates for plug-in classifiers.
\newblock \emph{The Annals of Statistics}, 35\penalty0 (2):\penalty0 608--633,
  2007.

\bibitem[Auer et~al.(2019)Auer, Gajane, and Ortner]{auer2019}
Peter Auer, Pratik Gajane, and Ronald Ortner.
\newblock Adaptively tracking the best bandit arm with an unknown number of
  distribution changes.
\newblock \emph{Conference on Learning Theory}, pages 138--158, 2019.

\bibitem[Besbes et~al.(2019)Besbes, Gur, and Zeevi]{besbes2014}
Omar Besbes, Yonatan Gur, and Assaf Zeevi.
\newblock Optimal exploration-exploitation in a multi-armed-bandit problem with
  non-stationary rewards.
\newblock \emph{Stochastic Systems}, 9\penalty0 (4):\penalty0 319--337, 2019.

\bibitem[Besson et~al.(2022)Besson, Kaufmann, Maillard, and Seznec]{besson2019}
Lilian Besson, Emilie Kaufmann, Odalric-Ambrym Maillard, and Julien Seznec.
\newblock Efficient change-point detection for tackling piecewise-stationary
  bandits.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (77):\penalty0 1--40, 2022.

\bibitem[Beygelzimer et~al.(2011)Beygelzimer, Langford, Li, Reyzin, and
  Schapire]{beygelzimer2011}
Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert~E.
  Schapire.
\newblock Contextual bandit algorithms with supervised learning guarantees.
\newblock \emph{AISTATS}, 2011.

\bibitem[Blanchard et~al.(2023)Blanchard, Hanneke, and Jaillet]{blanchard23}
Moise Blanchard, Steve Hanneke, and Patrick Jaillet.
\newblock Non-stationary contextual bandits and universal learning.
\newblock \emph{arXiv preprint arXiv:2302.07186}, 2023.

\bibitem[Cai et~al.(2022)Cai, Cai, and Li]{cai22}
Changxiao Cai, T.~Tony Cai, and Hongzhe Li.
\newblock Transfer learning for contextual multi-armed bandits.
\newblock \emph{arxiv preprint arXiv:2211.12612}, 2022.

\bibitem[Cao et~al.(2019)Cao, Wen, Kveton, and Xie]{cao2019}
Yang Cao, Zheng Wen, Branislav Kveton, and Yao Xie.
\newblock Nearly optimal adaptive procedure with change detection for
  piecewise-stationary bandit.
\newblock \emph{Proceedings of the 22nd International Conference on Artificial
  Intelligence and Statistics (AISTATS)}, 2019.

\bibitem[Chen and Luo(2022)]{chen2022goal}
Liyu Chen and Haipeng Luo.
\newblock Near-optimal goal-oriented reinforcement learning in non-stationary
  environments.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Chen et~al.(2019)Chen, Lee, Luo, and Wei]{chen2019}
Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-Yu Wei.
\newblock A new algorithm for non-stationary contextual bandits: efficient,
  optimal, and parameter-free.
\newblock In \emph{32nd Annual Conference on Learning Theory}, 2019.

\bibitem[Cheung et~al.(2020)Cheung, Simchi-Levi, and
  Zhu]{cheung2020reinforcement}
Wang~Chi Cheung, David Simchi-Levi, and Ruihao Zhu.
\newblock Reinforcement learning for non-stationary {M}arkov decision
  processes: The blessing of (more) optimism.
\newblock In \emph{International Conference on Machine Learning}, pages
  1843--1854. PMLR, 2020.

\bibitem[Chi~Cheung et~al.(2019)Chi~Cheung, Simchi-Levi, and
  Zhu]{cheung2019drift}
Wang Chi~Cheung, David Simchi-Levi, and Ruihao Zhu.
\newblock Hedging the drift: learning to optimize under non-stationarity.
\newblock In \emph{Proceedings of the 22nd International Conference on
  Artificial Intelligence and Statistics}, 2019.

\bibitem[Ding and Lavaei(2023)]{ding2022provably}
Yuhao Ding and Javad Lavaei.
\newblock Provably efficient primal-dual reinforcement learning for {CMDP}s
  with non-stationary objectives and constraints.
\newblock \emph{AAAI Conference on Artificial Intelligence (AAAI)}, 2023.

\bibitem[Domingues et~al.(2021)Domingues, M{\'e}nard, Pirotta, Kaufmann, and
  Valko]{domingues2021kernel}
Omar~Darwiche Domingues, Pierre M{\'e}nard, Matteo Pirotta, Emilie Kaufmann,
  and Michal Valko.
\newblock A kernel-based approach to non-stationary reinforcement learning in
  metric spaces.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3538--3546. PMLR, 2021.

\bibitem[Dudik et~al.(2011)Dudik, Hsu, Kale, Karampatziakis, Langford, Reyzin,
  and Zhang]{dudik11}
Miroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford,
  Lev Reyzin, and Tong Zhang.
\newblock Efficient optimal learning for contextual bandits.
\newblock In \emph{Proceedings of the Twenty-Seventh Conference on Uncertainty
  in Artificial Intelligence}, page 169–178. AUAI Press, 2011.

\bibitem[Fei et~al.(2020)Fei, Yang, Wang, and Xie]{fei2020dynamic}
Yingjie Fei, Zhuoran Yang, Zhaoran Wang, and Qiaomin Xie.
\newblock Dynamic regret of policy optimization in non-stationary environments.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6743--6754, 2020.

\bibitem[Foster and Rakhlin(2020)]{foster20b}
Dylan Foster and Alexander Rakhlin.
\newblock Beyond ucb: Optimal and efficient contextual bandits with regression
  oracles.
\newblock \emph{Proceedings of the 37th International Conference on Machine
  Learning}, 119:\penalty0 3199--3210, 2020.

\bibitem[Foster et~al.(2018)Foster, Agarwal, Dudik, Luo, and
  Schapire]{foster18}
Dylan Foster, Alekh Agarwal, Miroslav Dudik, Haipeng Luo, and Robert Schapire.
\newblock Practical contextual bandits with regression oracles.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, volume~80 of \emph{Proceedings of Machine Learning Research},
  pages 1539--1548. PMLR, 10--15 Jul 2018.

\bibitem[Gajane et~al.(2018)Gajane, Ortner, and Auer]{gajane2018sliding}
Pratik Gajane, Ronald Ortner, and Peter Auer.
\newblock A sliding-window algorithm for {M}arkov decision processes with
  arbitrarily changing rewards and transitions.
\newblock \emph{arXiv preprint arXiv:1805.10066}, 2018.

\bibitem[Garivier and Moulines(2011)]{garivier2011}
Aur\'{e}lien Garivier and Eric Moulines.
\newblock On upper-confidence bound policies for switching bandit problems.
\newblock In \emph{Proceedings of the 22nd International Conference on
  Algorithmic Learning Theory}, pages 174--188. ALT 2011, Springer, 2011.

\bibitem[Guan and Jiang(2018)]{guan-jiang}
Melody~Y Guan and Heinrich Jiang.
\newblock Nonparametric stochastic contextual bandits.
\newblock \emph{AAAI}, 2018.

\bibitem[Gur et~al.(2022)Gur, Momeni, and Wager]{gur-momeni-wager}
Yonatan Gur, Ahmadreza Momeni, and Stefan Wager.
\newblock Smoothness-adaptive contextual bandits.
\newblock \emph{Operations Research}, 70\penalty0 (6):\penalty0 3198--3216,
  2022.

\bibitem[Hu et~al.(2020)Hu, Kallus, and Mao]{hu20}
Yichun Hu, Nathan Kallus, and Xiaojie Mao.
\newblock Smooth contextual bandits: Bridging the parametric and
  non-differentiable regret regimes.
\newblock \emph{Conference on Learning Theory}, 2020.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11:\penalty0 1563--1600,
  2010.

\bibitem[Karnin and Anava(2016)]{karnin2016multi}
Zohar~S Karnin and Oren Anava.
\newblock Multi-armed bandits: Competing with optimal sequences.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  199--207, 2016.

\bibitem[Krishnamurthy et~al.(2019)Krishnamurthy, Langford, Slivkins, and
  Zhang]{krishnamurthy19a}
Akshay Krishnamurthy, John Langford, Aleksandrs Slivkins, and Chicheng Zhang.
\newblock Contextual bandits with continuous actions: Smoothing, zooming, and
  adapting.
\newblock In \emph{Proceedings of the Thirty-Second Conference on Learning
  Theory}, volume~99 of \emph{Proceedings of Machine Learning Research}, pages
  2025--2027. PMLR, 25--28 Jun 2019.

\bibitem[Langford and Zhang(2008)]{langford2008epoch}
John Langford and Tong Zhang.
\newblock The epoch-greedy algorithm for multi-armed bandits with side
  information.
\newblock In \emph{Advances in neural information processing systems}, pages
  817--824, 2008.

\bibitem[Liu et~al.(2018)Liu, Lee, and Shroff]{liu2018}
Fang Liu, Joohyun Lee, and Ness Shroff.
\newblock A change-detection based framework for piecewise-stationary
  multi-armed bandit problem.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Lu et~al.(2009)Lu, P{\'a}l, and P{\'a}l]{lu2009showing}
Tyler Lu, D{\'a}vid P{\'a}l, and Martin P{\'a}l.
\newblock Showing relevant ads via context multi-armed bandits.
\newblock In \emph{Proceedings of AISTATS}, 2009.

\bibitem[Luo et~al.(2018)Luo, Wei, Agarwal, and Langford]{luo2018efficient}
Haipeng Luo, Chen-Yu Wei, Alekh Agarwal, and John Langford.
\newblock Efficient contextual bandits in non-stationary worlds.
\newblock In \emph{Conference On Learning Theory}, pages 1739--1776. PMLR,
  2018.

\bibitem[Lykouris et~al.(2021)Lykouris, Simchowitz, Slivkins, and
  Sun]{lykouris2021corruption}
Thodoris Lykouris, Max Simchowitz, Alex Slivkins, and Wen Sun.
\newblock Corruption-robust exploration in episodic reinforcement learning.
\newblock In \emph{Conference on Learning Theory}, pages 3242--3245. PMLR,
  2021.

\bibitem[Mao et~al.(2021)Mao, Zhang, Zhu, Simchi-Levi, and Basar]{mao2020model}
Weichao Mao, Kaiqing Zhang, Ruihao Zhu, David Simchi-Levi, and Tamer Basar.
\newblock Near-optimal model-free reinforcement learning in non-stationary
  episodic mdps.
\newblock In \emph{International Conference on Machine Learning}, pages
  7447--7458. PMLR, 2021.

\bibitem[Mukherjee and Maillard(2019)]{mukherjee2019}
Subhojyoti Mukherjee and Odalric-Ambrym Maillard.
\newblock Distribution-dependent and time-uniform bounds for piecewise i.i.d
  bandits.
\newblock \emph{Reinforcement Learning for Real Life (RL4RealLife) Workshop in
  the 36th International Conference on Mearning Learning}, 2019.

\bibitem[Ortner et~al.(2020)Ortner, Gajane, and Auer]{pmlr-v115-ortner20a}
Ronald Ortner, Pratik Gajane, and Peter Auer.
\newblock Variational regret bounds for reinforcement learning.
\newblock In \emph{Proceedings of The 35th Uncertainty in Artificial
  Intelligence Conference}, volume 115 of \emph{Proceedings of Machine Learning
  Research}, pages 81--90. PMLR, 22--25 Jul 2020.

\bibitem[Perchet and Rigollet(2013)]{perchet-rigollet}
Vianney Perchet and Philippe Rigollet.
\newblock The multi-armed bandit problem with covariates.
\newblock \emph{The Annals of Statistics}, 41\penalty0 (2):\penalty0 693–721,
  2013.

\bibitem[Polyanskiy and Wu(2022)]{info-book}
Yury Polyanskiy and Yihong Wu.
\newblock \emph{Information Theory: From Coding to Learning}.
\newblock Cambridge University Press, 2022.

\bibitem[Qian and Yang(2016{\natexlab{a}})]{qian16a}
Wei Qian and Yuhong Yang.
\newblock Kernel estimation and model combination in a bandit problem with
  covariates.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (149):\penalty0 1--37, 2016{\natexlab{a}}.

\bibitem[Qian and Yang(2016{\natexlab{b}})]{qian16b}
Wei Qian and Yuhong Yang.
\newblock {Randomized allocation with arm elimination in a bandit problem with
  covariates}.
\newblock \emph{Electronic Journal of Statistics}, 10\penalty0 (1):\penalty0
  242 -- 270, 2016{\natexlab{b}}.

\bibitem[Reeve et~al.(2018)Reeve, Mellor, and Brown]{reeve}
Henry Reeve, Joe Mellor, and Gavin Brown.
\newblock The k-nearest neighbour ucb algorithm for multi-armed bandits with
  covariates.
\newblock In \emph{Proceedings of Algorithmic Learning Theory}, volume~83 of
  \emph{Proceedings of Machine Learning Research}, pages 725--752. PMLR, 07--09
  Apr 2018.

\bibitem[Rigollet and Zeevi(2010)]{rigollet-zeevi}
Phillipe Rigollet and Assaf Zeevi.
\newblock Nonparametric bandits with covariates.
\newblock \emph{COLT}, 2010.

\bibitem[Sarkar(1991)]{sarkar1991one}
Jyotirmoy Sarkar.
\newblock One-armed bandit problems with covariates.
\newblock \emph{The Annals of Statistics}, pages 1978--2002, 1991.

\bibitem[Simchi-Levi and Xu(2021)]{simchi21}
David Simchi-Levi and Yunzong Xu.
\newblock Bypassing the monster: a faster and simpler optimal algorithm for
  contextual bandits under realizability.
\newblock \emph{Mathematics of Operations Research}, 47\penalty0 (3):\penalty0
  1904--1931, 2021.

\bibitem[Slivkins(2014)]{slivkins2014contextual}
Aleksandrs Slivkins.
\newblock Contextual bandits with similarity information.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 2533--2568, 2014.

\bibitem[Suk and Kpotufe(2022)]{suk22}
Joe Suk and Samory Kpotufe.
\newblock Tracking most significant arm switches in bandits.
\newblock In \emph{Proceedings of Thirty Fifth Conference on Learning Theory},
  volume 178 of \emph{Proceedings of Machine Learning Research}, pages
  2160--2182. PMLR, 02--05 Jul 2022.

\bibitem[Suk and Kpotufe(2021)]{suk21}
Joseph Suk and Samory Kpotufe.
\newblock Self-tuning bandits over unknown covariate-shifts.
\newblock \emph{International Conference on Algorithmic Learning Theory}, 2021.

\bibitem[Touati and Vincent(2020)]{touati2020efficient}
Ahmed Touati and Pascal Vincent.
\newblock Efficient learning in non-stationary linear {M}arkov decision
  processes.
\newblock \emph{arXiv preprint arXiv:2010.12870}, 2020.

\bibitem[Wei and Luo(2021)]{wei2021}
Chen-Yu Wei and Haipeng Luo.
\newblock Non-stationary reinforcement learning without prior knowledge: An
  optimal black-box approach.
\newblock \emph{Proceedings of the 32nd International Conference on Learning
  Theory}, 2021.

\bibitem[Wei et~al.(2022)Wei, Dann, and Zimmert]{wei2022model}
Chen-Yu Wei, Christoph Dann, and Julian Zimmert.
\newblock A model selection approach for corruption robust reinforcement
  learning.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 1043--1096. PMLR, 2022.

\bibitem[Wei and Srivatsva(2018)]{wei-srivastava}
Lai Wei and Vaihbav Srivatsva.
\newblock On abruptly-changing and slowly-varying multiarmed bandit problems.
\newblock \emph{Annual American Control Conference (ACC)}, 2018.

\bibitem[Woodroofe(1979)]{woodroofe1979one}
Michael Woodroofe.
\newblock A one-armed bandit problem with a concomitant variable.
\newblock \emph{Journal of the American Statistical Association}, 74\penalty0
  (368):\penalty0 799--806, 1979.

\bibitem[Wu et~al.(2018)Wu, Iyer, and Wang]{wu2018learning}
Qingyun Wu, Naveen Iyer, and Hongning Wang.
\newblock Learning contextual bandits in a non-stationary environment.
\newblock In \emph{The 41st International ACM SIGIR Conference on Research \&
  Development in Information Retrieval}, pages 495--504, 2018.

\bibitem[Yang et~al.(2002)Yang, Zhu, et~al.]{yang2002randomized}
Yuhong Yang, Dan Zhu, et~al.
\newblock Randomized allocation with nonparametric estimation for a multi-armed
  bandit problem with covariates.
\newblock \emph{The Annals of Statistics}, 30\penalty0 (1):\penalty0 100--121,
  2002.

\bibitem[Zhou et~al.(2022)Zhou, Chen, Varshney, and
  Jagmohan]{zhou2022nonstationary}
Huozhi Zhou, Jinglin Chen, Lav~R. Varshney, and Ashish Jagmohan.
\newblock Nonstationary reinforcement learning with linear function
  approximation.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock ISSN 2835-8856.

\end{thebibliography}
