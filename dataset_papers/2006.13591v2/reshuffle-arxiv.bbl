\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Armijo(1966)]{armijo1966}
Larry Armijo.
\newblock Minimization of functions having lipschitz continuous first partial
  derivatives.
\newblock \emph{Pacific J. Math.}, 16\penalty0 (1):\penalty0 1--3, 1966.

\bibitem[Birgin et~al.(2017)Birgin, Gardenghi, Mart{\'\i}nez, Santos, and
  Toint]{birgin2017worst}
Ernesto~G Birgin, JL~Gardenghi, Jos{\'e}~Mario Mart{\'\i}nez, Sandra~Augusta
  Santos, and Ph~L Toint.
\newblock Worst-case evaluation complexity for unconstrained nonlinear
  optimization using high-order regularized models.
\newblock \emph{Mathematical Programming}, 163\penalty0 (1-2):\penalty0
  359--368, 2017.

\bibitem[Blanchet et~al.(2016)Blanchet, Cartis, Menickelly, and
  Scheinberg]{blanchet2016convergence}
Jose Blanchet, Coralia Cartis, Matt Menickelly, and Katya Scheinberg.
\newblock Convergence rate analysis of a stochastic trust region method for
  nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1609.07428}, 5, 2016.

\bibitem[Cartis et~al.(2011)Cartis, Gould, and Toint]{Cartis2011}
Coralia Cartis, Nicholas I.~M. Gould, and Philippe~L. Toint.
\newblock Adaptive cubic regularisation methods for unconstrained optimization.
  part i: motivation, convergence and numerical results.
\newblock \emph{Mathematical Programming}, 127\penalty0 (2):\penalty0 245--295,
  Apr 2011.
\newblock ISSN 1436-4646.

\bibitem[Dekel et~al.(2012)Dekel, Gilad-Bachrach, Shamir, and
  Xiao]{minibatchsgd}
Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao.
\newblock {Optimal Distributed Online Prediction Using Mini-Batches}.
\newblock \emph{JMLR}, 13:\penalty0 165--202, 2012.

\bibitem[Dennis and Mor{\'e}(1977)]{dennis1977quasi}
John~E Dennis, Jr and Jorge~J Mor{\'e}.
\newblock Quasi-newton methods, motivation and theory.
\newblock \emph{SIAM review}, 19\penalty0 (1):\penalty0 46--89, 1977.

\bibitem[Dua and Graff(2017)]{ucirepo}
Dheeru Dua and Casey Graff.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[D{\"u}nner et~al.(2018)D{\"u}nner, Lucchi, Gargiani, Bian, Hofmann,
  and Jaggi]{duenner2018adn}
Celestine D{\"u}nner, Aurelien Lucchi, Matilde Gargiani, An~Bian, Thomas
  Hofmann, and Martin Jaggi.
\newblock A distributed second-order algorithm you can trust.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, volume~80, pages 1358--1366, 2018.

\bibitem[Erdogdu and Montanari(2015)]{erdogdu2015convergence}
Murat~A. Erdogdu and Andrea Montanari.
\newblock Convergence rates of sub-sampled newton methods.
\newblock In \emph{Proceedings of the 28th International Conference on Neural
  Information Processing Systems}, page 3052â€“3060, 2015.

\bibitem[Hsieh et~al.(2016)Hsieh, Si, and Dhillon]{Hsieh:2016wg}
Cho-Jui Hsieh, Si~Si, and Inderjit~S Dhillon.
\newblock {Communication-Efficient Parallel Block Minimization for Kernel
  Machines}.
\newblock \emph{arXiv}, August 2016.

\bibitem[Ioannou et~al.(2019)Ioannou, Mendler-D{\"u}nner, and Parnell]{syscd}
Nikolaos Ioannou, Celestine Mendler-D{\"u}nner, and Thomas Parnell.
\newblock {SySCD: A System-Aware Parallel Coorindate Descent Algorithm}.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Jaggi et~al.(2014)Jaggi, Smith, Tak{\'a}{\v c}, Terhorst, Krishnan,
  Hofmann, and Jordan]{Jaggi:2014vi}
Martin Jaggi, Virginia Smith, Martin Tak{\'a}{\v c}, Jonathan Terhorst, Sanjay
  Krishnan, Thomas Hofmann, and Michael~I Jordan.
\newblock {Communication-efficient distributed dual coordinate ascent}.
\newblock In \emph{Neural Information Processing Systems}, 2014.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the {P}olyak-{\l}ojasiewicz condition.
\newblock In \emph{European Conference on Machine Learning}, 2016.

\bibitem[Kohler and Lucchi(2017)]{kohler2017sub}
Jonas~M. Kohler and Aurelien Lucchi.
\newblock Sub-sampled cubic regularization for non-convex optimization.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1895--1904. JMLR. org, 2017.

\bibitem[Lee and Chang(2017)]{lee2017distributed}
Ching-pei Lee and Kai-Wei Chang.
\newblock Distributed block-diagonal approximation methods for regularized
  empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1709.03043}, 2017.

\bibitem[Liu et~al.(2015)Liu, Wright, R{\'e}, Bittorf, and Sridhar]{Liu:2015wj}
Ji~Liu, Stephen~J Wright, Christopher R{\'e}, Victor Bittorf, and Srikrishna
  Sridhar.
\newblock {An Asynchronous Parallel Stochastic Coordinate Descent Algorithm}.
\newblock \emph{Journal of Machine Learning Research}, 16:\penalty0 285--322,
  2015.

\bibitem[Mahajan et~al.(2017)Mahajan, Keerthi, and
  Sundararajan]{mahajan2017distributed}
Dhruv Mahajan, S~Sathiya Keerthi, and S~Sundararajan.
\newblock {A distributed block coordinate descent method for training L1
  regularized linear classifiers}.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (91):\penalty0 1--35, 2017.

\bibitem[Nesterov and Polyak(2006)]{nesterov2006cubic}
Yurii Nesterov and Boris~T Polyak.
\newblock Cubic regularization of newton method and its global performance.
\newblock \emph{Mathematical Programming}, 108\penalty0 (1):\penalty0 177--205,
  2006.

\bibitem[Niu et~al.(2011)Niu, Recht, R{\'e}, and Wright]{Niu:2011wo}
Feng Niu, Benjamin Recht, Christopher R{\'e}, and Stephen~J Wright.
\newblock Hogwild!: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In \emph{Neural Information Processing Systems}, 2011.

\bibitem[Nocedal and Wright(1999)]{convex_optimization_nocedal}
J.~Nocedal and S.~Wright.
\newblock \emph{Numerical optimization}.
\newblock Springer Series in Operations Research, 1999.

\bibitem[Pilanci and Wainwright(2016)]{pilanci2016iterative}
Mert Pilanci and Martin~J Wainwright.
\newblock Iterative hessian sketch: Fast and accurate solution approximation
  for constrained least-squares.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1842--1879, 2016.

\bibitem[Polyak(1963)]{polyak}
B.~T. Polyak.
\newblock Gradient methods for minimizing functionals (in russian).
\newblock \emph{Zh. Vychisl. Mat. Mat. Fiz}, 1963.

\bibitem[Reddi et~al.(2016)Reddi, Kone{\v{c}}n{\`y}, Richt{\'a}rik,
  P{\'o}cz{\'o}s, and Smola]{reddi2016aide}
Sashank~J Reddi, Jakub Kone{\v{c}}n{\`y}, Peter Richt{\'a}rik, Barnab{\'a}s
  P{\'o}cz{\'o}s, and Alex Smola.
\newblock Aide: Fast and communication efficient distributed optimization.
\newblock \emph{arXiv preprint arXiv:1608.06879}, 2016.

\bibitem[Richt{\'a}rik and Tak{\'a}{\v c}(2016)]{Richtarik:2012vf}
Peter Richt{\'a}rik and Martin Tak{\'a}{\v c}.
\newblock Parallel coordinate descent methods for big data optimization.
\newblock \emph{Mathematical Programming}, 156\penalty0 (1):\penalty0 433--484,
  2016.

\bibitem[Shamir et~al.(2014)Shamir, Srebro, and Zhang]{shamir2014communication}
Ohad Shamir, Nati Srebro, and Tong Zhang.
\newblock Communication-efficient distributed optimization using an approximate
  newton-type method.
\newblock In \emph{International conference on machine learning}, pages
  1000--1008, 2014.

\bibitem[Smith et~al.(2018)Smith, Forte, Ma, Tak{\'a}{\v c}, Jordan, and
  Jaggi]{Smith:2016wp}
Virginia Smith, Simone Forte, Chenxin Ma, Martin Tak{\'a}{\v c}, Michael~I
  Jordan, and Martin Jaggi.
\newblock {CoCoA: A General Framework for Communication-Efficient Distributed
  Optimization}.
\newblock \emph{Journal of Machine Learning Research (and arXiv:1611.02189)},
  2018.

\bibitem[Wang et~al.(2018)Wang, Roosta-Khorasani, Xu, and
  Mahoney]{wang2017giant}
Shusen Wang, Farbod Roosta-Khorasani, Peng Xu, and Michael~W Mahoney.
\newblock Giant: Globally improved approximate newton method for distributed
  optimization.
\newblock \emph{Advances in Neural Information Processing Systems (and
  arXiv:1709.03528)}, 2018.

\bibitem[Zhang and Lin(2015)]{zhang2015disco}
Yuchen Zhang and Xiao Lin.
\newblock Disco: Distributed optimization for self-concordant empirical loss.
\newblock In \emph{International conference on machine learning}, pages
  362--370, 2015.

\end{thebibliography}
