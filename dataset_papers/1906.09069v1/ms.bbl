\begin{thebibliography}{10}

\bibitem{bottou2012stochastic}
L{\'e}on Bottou.
\newblock Stochastic gradient descent tricks.
\newblock In {\em Neural networks: Tricks of the trade}, pages 421--436.
  Springer, 2012.

\bibitem{bottou2010large}
L{\'e}on Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In {\em Proceedings of COMPSTAT'2010}, pages 177--186. Springer,
  2010.

\bibitem{chaudhari2018stochastic}
P.~Chaudhari and S.~Soatto.
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{chaudhari2016entropy}
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi,
  Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock {\em arXiv preprint arXiv:1611.01838}, 2016.

\bibitem{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock {\em nature}, 521(7553):436, 2015.

\bibitem{simsekli_tail_ICML2019}
U.~\c{S}im\c{s}ekli, L.~Sagun, and G\"{u}rb\"{u}zbalaban.
\newblock {A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural
  Networks}.
\newblock In {\em ICML}, 2019.

\bibitem{jastrzkebski2017three}
S.~Jastrzebski, Z.~Kenton, D.~Arpit, N.~Ballas, A.~Fischer, Y.~Bengio, and
  A.~Storkey.
\newblock Three factors influencing minima in {SGD}.
\newblock {\em arXiv preprint arXiv:1711.04623}, 2017.

\bibitem{hochreiter1997flat}
Sepp Hochreiter and {J{\"u}rgen} Schmidhuber.
\newblock Flat minima.
\newblock {\em Neural Computation}, 9(1):1--42, 1997.

\bibitem{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock {On Large-Batch Training for Deep Learning: Generalization Gap and
  Sharp Minima}.
\newblock {\em arXiv preprint arXiv:1609.04836}, 2016.

\bibitem{mandt2016variational}
S.~Mandt, M.~Hoffman, and D.~Blei.
\newblock A variational analysis of stochastic gradient algorithms.
\newblock In {\em International Conference on Machine Learning}, pages
  354--363, 2016.

\bibitem{pmlr-v70-li17f}
Q.~Li, C.~Tai, and W.~E.
\newblock {Stochastic Modified Equations and Adaptive Stochastic Gradient
  Algorithms}.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning}, pages 2101--2110, 06--11 Aug 2017.

\bibitem{hu2017diffusion}
W.~Hu, C.~J. Li, L.~Li, and J.-G. Liu.
\newblock On the diffusion approximation of nonconvex stochastic gradient
  descent.
\newblock {\em arXiv preprint arXiv:1705.07562}, 2017.

\bibitem{zhu2018anisotropic}
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma.
\newblock {The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior
  of Escaping from Minima and Regularization Effects}.
\newblock {\em arXiv preprint arXiv:1803.00195}, 2018.

\bibitem{nguyen2019non}
Thanh~Huy Nguyen, Umut {\c{S}}im{\c{s}}ekli, and Ga{\"e}l Richard.
\newblock {Non-Asymptotic Analysis of Fractional Langevin Monte Carlo for
  Non-Convex Optimization}.
\newblock In {\em International Conference on Machine Learning}, 2019.

\bibitem{oksendal2005applied}
Bernt~Karsten {\O}ksendal and Agnes Sulem.
\newblock {\em Applied stochastic control of jump diffusions}, volume 498.
\newblock Springer, 2005.

\bibitem{imkeller2006first}
Peter Imkeller and Ilya Pavlyukevich.
\newblock First exit times of sdes driven by stable {L}\'{e}vy processes.
\newblock {\em Stochastic Processes and their Applications}, 116(4):611--642,
  2006.

\bibitem{imkeller2010hierarchy}
P.~Imkeller, I.~Pavlyukevich, and T.~Wetzel.
\newblock The hierarchy of exit times of {L}{\'e}vy-driven {L}angevin
  equations.
\newblock {\em The European Physical Journal Special Topics}, 191(1):211--222,
  2010.

\bibitem{imkeller2010first}
Peter Imkeller, Ilya Pavlyukevich, and Michael Stauch.
\newblock First exit times of non-linear dynamical systems in rd perturbed by
  multifractal {L}\'{e}vy noise.
\newblock {\em Journal of Statistical Physics}, 141(1):94--119, 2010.

\bibitem{yaida2018fluctuationdissipation}
S.~Yaida.
\newblock Fluctuation-dissipation relations for stochastic gradient descent.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{tzen2018local}
B.~Tzen, T.~Liang, and M.~Raginsky.
\newblock {Local Optimality and Generalization Guarantees for the Langevin
  Algorithm via Empirical Metastability}.
\newblock In {\em Proceedings of the 2018 Conference on Learning Theory}, 2018.

\bibitem{duan}
J.~Duan.
\newblock {\em An Introduction to Stochastic Dynamics}.
\newblock Cambridge University Press, New York, 2015.

\bibitem{chambers1976method}
J.~M. Chambers, C.~L. Mallows, and B.~W. Stuck.
\newblock A method for simulating stable random variables.
\newblock {\em Journal of the american statistical association},
  71(354):340--344, 1976.

\bibitem{tankov2003financial}
Peter Tankov.
\newblock {\em Financial modelling with jump processes}.
\newblock Chapman and Hall/CRC, 2003.

\bibitem{bovier2004metastability}
Anton Bovier, Michael Eckhoff, V{\'e}ronique Gayrard, and Markus Klein.
\newblock {Metastability in reversible diffusion processes I: Sharp asymptotics
  for capacities and exit times}.
\newblock {\em Journal of the European Mathematical Society}, 6(4):399--424,
  2004.

\bibitem{pavlyukevich2011first}
Ilya Pavlyukevich.
\newblock First exit times of solutions of stochastic differential equations
  driven by multiplicative {L}{\'e}vy noise with heavy tails.
\newblock {\em Stochastics and Dynamics}, 11(02n03):495--519, 2011.

\bibitem{berglund2011kramers}
Nils Berglund.
\newblock Kramers' law: Validity, derivations and generalisations.
\newblock {\em arXiv preprint arXiv:1106.5799}, 2011.

\bibitem{burghoff2015spectral}
Toralf Burghoff and Ilya Pavlyukevich.
\newblock {Spectral Analysis for a Discrete Metastable System Driven by
  L{\'e}vy Flights}.
\newblock {\em Journal of Statistical Physics}, 161(1):171--196, 2015.

\bibitem{priola2012pathwise}
Enrico Priola et~al.
\newblock Pathwise uniqueness for singular sdes driven by stable processes.
\newblock {\em Osaka Journal of Mathematics}, 49(2):421--447, 2012.

\bibitem{kulik2019weak}
Alexei~M Kulik.
\newblock On weak uniqueness and distributional properties of a solution to an
  sde with $\alpha$-stable noise.
\newblock {\em Stochastic Processes and their Applications}, 129(2):473--506,
  2019.

\bibitem{liang2018gradient}
Mingjie Liang and Jian Wang.
\newblock {Gradient Estimates and Ergodicity for SDEs Driven by Multiplicative
  L\'evy Noises via Coupling}.
\newblock {\em arXiv preprint arXiv:1801.05936}, 2018.

\bibitem{raginsky17a}
M.~Raginsky, A.~Rakhlin, and M.~Telgarsky.
\newblock Non-convex learning via stochastic gradient {L}angevin dynamics: a
  nonasymptotic analysis.
\newblock In {\em Proceedings of the 2017 Conference on Learning Theory},
  volume~65, pages 1674--1703, 2017.

\bibitem{xu2018global}
Pan Xu, Jinghui Chen, Difan Zou, and Quanquan Gu.
\newblock Global convergence of {L}angevin dynamics based algorithms for
  nonconvex optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3125--3136, 2018.

\bibitem{erdogdu2018global}
M.~A. Erdogdu, L.~Mackey, and O.~Shamir.
\newblock {Global Non-convex Optimization with Discretized Diffusions}.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9693--9702, 2018.

\bibitem{gao18}
Xuefeng {Gao}, Mert {Gurbuzbalaban}, and Lingjiong {Zhu}.
\newblock {Breaking Reversibility Accelerates Langevin Dynamics for Global
  Non-Convex Optimization}.
\newblock {\em arXiv e-prints}, page arXiv:1812.07725, Dec 2018.

\bibitem{gao-sghmc}
Xuefeng {Gao}, Mert {G{\"u}rb{\"u}zbalaban}, and Lingjiong {Zhu}.
\newblock {Global Convergence of Stochastic Gradient Hamiltonian Monte Carlo
  for Non-Convex Stochastic Optimization: Non-Asymptotic Performance Bounds and
  Momentum-Based Acceleration}.
\newblock {\em arXiv e-prints}, page arXiv:1809.04618, Sep 2018.

\bibitem{yang17relu}
Yuanzhi Li and Yang Yuan.
\newblock {Convergence Analysis of Two-layer Neural Networks with ReLU
  Activation}.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems 30}, pages 597--607. Curran Associates, Inc., 2017.

\bibitem{sagun2016eigenvalues}
Levent Sagun, Leon Bottou, and Yann LeCun.
\newblock Eigenvalues of the hessian in deep learning: Singularity and beyond.
\newblock {\em arXiv preprint arXiv:1611.07476}, 2016.

\bibitem{papyan2018full}
Vardan Papyan.
\newblock The full spectrum of deep net hessians at scale: Dynamics with sample
  size.
\newblock {\em arXiv preprint arXiv:1811.07062}, 2018.

\bibitem{mikulevivcius2018rate}
R~Mikulevi{\v{c}}ius and Fanhui Xu.
\newblock On the rate of convergence of strong {E}uler approximation for {SDE}s
  driven by l{\'e}vy processes.
\newblock {\em Stochastics}, 90(4):569--604, 2018.

\bibitem{dalalyan2017theoretical}
A.~S. Dalalyan.
\newblock Theoretical guarantees for approximate sampling from smooth and
  log-concave densities.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 79(3):651--676, 2017.

\bibitem{debussche2013existence}
Arnaud Debussche and Nicolas Fournier.
\newblock Existence of densities for stable-like driven sde's with {H}\"{o}lder
  continuous coefficients.
\newblock {\em Journal of Functional Analysis}, 264(8):1757--1778, 2013.

\bibitem{lindvall2002lectures}
Torgny Lindvall.
\newblock {\em Lectures on the coupling method}.
\newblock Courier Corporation, 2002.

\bibitem{bayraktar2015weak}
Erhan Bayraktar, Sergey Nadtochiy, et~al.
\newblock Weak reflection principle for {L}{\'e}vy processes.
\newblock {\em The Annals of Applied Probability}, 25(6):3251--3294, 2015.

\bibitem{xie2017ergodicity}
Longjie Xie and Xicheng Zhang.
\newblock Ergodicity of stochastic differential equations with jumps and
  singular coefficients.
\newblock {\em arXiv preprint arXiv:1705.07402}, 2017.

\bibitem{winkelbauer2012moments}
Andreas Winkelbauer.
\newblock Moments and absolute moments of the normal distribution.
\newblock {\em arXiv preprint arXiv:1209.4340}, 2012.

\end{thebibliography}
