\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal \& Jia(2017)Agrawal and Jia]{agrawal2017}
Agrawal, Shipra and Jia, Randy.
\newblock Posterior sampling for reinforcement learning: worst-case regret
  bounds.
\newblock \emph{arXiv preprint arXiv:1705.07041}, 2017.

\bibitem[Azar et~al.(2013)Azar, Munos, and Kappen]{azar2013minimax}
Azar, Mohammad~Gheshlaghi, Munos, R{\'e}mi, and Kappen, Hilbert~J.
\newblock Minimax pac bounds on the sample complexity of reinforcement learning
  with a generative model.
\newblock \emph{Machine learning}, 91\penalty0 (3):\penalty0 325--349, 2013.

\bibitem[Bartlett \& Tewari(2009)Bartlett and Tewari]{Bartlett2009}
Bartlett, Peter~L. and Tewari, Ambuj.
\newblock {REGAL}: A regularization based algorithm for reinforcement learning
  in weakly communicating {MDPs}.
\newblock In \emph{Proceedings of the 25th Conference on Uncertainty in
  Artificial Intelligence (UAI2009)}, pp.\  35--42, June 2009.

\bibitem[Bernstein(1927)]{bernstein1927theory}
Bernstein, S.
\newblock Theory of probability, 1927.

\bibitem[Bertsekas(2007)]{Berst07a}
Bertsekas, D.~P.
\newblock \emph{Dynamic Programming and Optimal Control}, volume~I.
\newblock Athena Scientific, Belmount, Massachusetts, third edition, 2007.

\bibitem[Bertsekas \& Tsitsiklis(1996)Bertsekas and Tsitsiklis]{Tsitsiklis96}
Bertsekas, D.~P. and Tsitsiklis, J.~N.
\newblock \emph{Neuro-Dynamic Programming}.
\newblock Athena Scientific, Belmont, Massachusetts, 1996.

\bibitem[Brafman \& Tennenholtz(2002)Brafman and Tennenholtz]{Brafman2002}
Brafman, Ronen~I. and Tennenholtz, Moshe.
\newblock R-max - a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 3:\penalty0 213--231,
  2002.

\bibitem[Bubeck \& Cesa{-}Bianchi(2012)Bubeck and
  Cesa{-}Bianchi]{Bubeck2012regretBandit}
Bubeck, S{\'{e}}bastien and Cesa{-}Bianchi, Nicol{\`{o}}.
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit
  problems.
\newblock \emph{CoRR}, abs/1204.5721, 2012.
\newblock URL \url{http://arxiv.org/abs/1204.5721}.

\bibitem[Bubeck et~al.(2011)Bubeck, Munos, Stoltz, and
  Szepesv{\'a}ri]{bubeck2011xarmed}
Bubeck, S{\'e}bastien, Munos, R{\'e}mi, Stoltz, Gilles, and Szepesv{\'a}ri,
  Csaba.
\newblock X-armed bandits.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0
  1587â€“1627, 2011.

\bibitem[Burnetas \& Katehakis(1997)Burnetas and
  Katehakis]{burnetas1997optimal}
Burnetas, Apostolos~N and Katehakis, Michael~N.
\newblock Optimal adaptive policies for markov decision processes.
\newblock \emph{Mathematics of Operations Research}, 22\penalty0 (1):\penalty0
  222--255, 1997.

\bibitem[Cesa-Bianchi \& Lugosi(2006)Cesa-Bianchi and Lugosi]{CBLu06:book}
Cesa-Bianchi, N. and Lugosi, G.
\newblock \emph{Prediction, Learning, and Games}.
\newblock Cambridge University Press, New York, NY, USA, 2006.

\bibitem[Dann \& Brunskill(2015)Dann and Brunskill]{dann2015sample}
Dann, Christoph and Brunskill, Emma.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017}
Dann, Christoph, Lattimore, Tor, and Brunskill, Emma.
\newblock Ubev-a more practical algorithm for episodic rl with near-optimal pac
  and regret guarantees.
\newblock \emph{arXiv preprint arXiv:1703.07710}, 2017.

\bibitem[Freedman(1975)]{freedman1975tail}
Freedman, David~A.
\newblock On tail probabilities for martingales.
\newblock \emph{the Annals of Probability}, pp.\  100--118, 1975.

\bibitem[Guez et~al.(2013)Guez, Silver, and Dayan]{guez2013scalable}
Guez, Arthur, Silver, David, and Dayan, Peter.
\newblock Scalable and efficient bayes-adaptive reinforcement learning based on
  monte-carlo tree search.
\newblock \emph{Journal of Artificial Intelligence Research}, pp.\  841--883,
  2013.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{UCRLAuer}
Jaksch, T., Ortner, R., and Auer, P.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11:\penalty0 1563--1600,
  2010.

\bibitem[Kearns \& Singh(2002)Kearns and Singh]{Kearns2002}
Kearns, Michael~J. and Singh, Satinder~P.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine Learning}, 49\penalty0 (2-3):\penalty0 209--232, 2002.

\bibitem[Lattimore \& Hutter(2012)Lattimore and Hutter]{corr/LattimoreHutter}
Lattimore, Tor and Hutter, Marcus.
\newblock {PAC} bounds for discounted {MDP}s.
\newblock \emph{CoRR}, abs/1202.3890, 2012.

\bibitem[Maurer \& Pontil(2009)Maurer and Pontil]{maurer2009empirical}
Maurer, Andreas and Pontil, Massimiliano.
\newblock Empirical bernstein bounds and sample variance penalization.
\newblock \emph{stat}, 1050:\penalty0 21, 2009.

\bibitem[Munos \& Moore(1999)Munos and Moore]{Munos99cdc}
Munos, R. and Moore, A.
\newblock Influence and variance of a {M}arkov chain : Application to adaptive
  discretizations in optimal control.
\newblock In \emph{Proceedings of the 38th IEEE Conference on Decision and
  Control}, 1999.

\bibitem[Munos(2014)]{munos2014bandits}
Munos, R{\'e}mi.
\newblock From bandits to {M}onte-{C}arlo {T}ree {S}earch: The optimistic
  principle applied to optimization and planning.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  7\penalty0 (1):\penalty0 1--129, 2014.

\bibitem[Osband \& {Van Roy}(2016{\natexlab{a}})Osband and {Van
  Roy}]{osband2016lower}
Osband, Ian and {Van Roy}, Benjamin.
\newblock On lower bounds for regret in reinforcement learning.
\newblock \emph{stat}, 1050:\penalty0 9, 2016{\natexlab{a}}.

\bibitem[Osband \& {Van Roy}(2016{\natexlab{b}})Osband and {Van
  Roy}]{osband2016posterior}
Osband, Ian and {Van Roy}, Benjamin.
\newblock Why is posterior sampling better than optimism for reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1607.00215}, 2016{\natexlab{b}}.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{osband2013more}
Osband, Ian, Russo, Dan, and Van~Roy, Benjamin.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3003--3011, 2013.

\bibitem[Strehl \& Littman(2005)Strehl and Littman]{strehl2005theoretical}
Strehl, Alexander~L and Littman, Michael~L.
\newblock A theoretical analysis of model-based interval estimation.
\newblock In \emph{Proceedings of the 22nd international conference on Machine
  learning}, pp.\  856--863. ACM, 2005.

\bibitem[Strehl \& Littman(2008)Strehl and Littman]{strehl2008analysis}
Strehl, Alexander~L and Littman, Michael~L.
\newblock An analysis of model-based interval estimation for markov decision
  processes.
\newblock \emph{Journal of Computer and System Sciences}, 74\penalty0
  (8):\penalty0 1309--1331, 2008.

\bibitem[Strehl et~al.(2006)Strehl, Li, Wiewiora, Langford, and
  Littman]{Strehl2006}
Strehl, Alexander~L., Li, Lihong, Wiewiora, Eric, Langford, John, and Littman,
  Michael~L.
\newblock {PAC} model-free reinforcement learning.
\newblock In \emph{ICML}, pp.\  881--888, 2006.

\bibitem[Strens(2000)]{Strens00}
Strens, Malcolm J.~A.
\newblock A {Bayesian} framework for reinforcement learning.
\newblock In \emph{ICML}, pp.\  943--950, 2000.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{Sutton1998}
Sutton, Richard and Barto, Andrew.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock MIT Press, March 1998.

\bibitem[Thompson(1933)]{Thompson1933}
Thompson, W.R.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock \emph{Biometrika}, 25\penalty0 (3/4):\penalty0 285--294, 1933.

\bibitem[Weissman et~al.(2003)Weissman, Ordentlich, Seroussi, Verdu, and
  Weinberger]{weissman2003inequalities}
Weissman, Tsachy, Ordentlich, Erik, Seroussi, Gadiel, Verdu, Sergio, and
  Weinberger, Marcelo~J.
\newblock Inequalities for the l1 deviation of the empirical distribution.
\newblock \emph{Hewlett-Packard Labs, Tech. Rep}, 2003.

\end{thebibliography}
