\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2019)Agarwal, Bullins, Hazan, Kakade, and
  Singh]{agarwal2019online}
N.~Agarwal, B.~Bullins, E.~Hazan, S.~Kakade, and K.~Singh.
\newblock Online control with adversarial disturbances.
\newblock In \emph{International Conference on Machine Learning}, pages
  111--119. PMLR, 2019.

\bibitem[Balsubramani(2014)]{balsubramani2014sharp}
A.~Balsubramani.
\newblock Sharp finite-time iterated-logarithm martingale concentration.
\newblock \emph{arXiv preprint arXiv:1405.2639}, 2014.

\bibitem[Bubeck et~al.(2013)Bubeck, Cesa-Bianchi, and
  Lugosi]{bubeck2013bandits}
S.~Bubeck, N.~Cesa-Bianchi, and G.~Lugosi.
\newblock Bandits with heavy tail.
\newblock \emph{IEEE Transactions on Information Theory}, 59\penalty0
  (11):\penalty0 7711--7717, 2013.

\bibitem[Carmon and Hinder(2022)]{carmon2022making}
Y.~Carmon and O.~Hinder.
\newblock Making sgd parameter-free.
\newblock \emph{Conference on Learning Theory}, 2022.

\bibitem[Cesa-Bianchi and Lugosi(2006)]{cesabianchi06prediction}
N.~Cesa-Bianchi and G.~Lugosi.
\newblock \emph{Prediction, learning, and games}.
\newblock Cambridge University Press, 2006.

\bibitem[Chen et~al.(2021)Chen, Luo, and Wei]{chen2021impossible}
L.~Chen, H.~Luo, and C.-Y. Wei.
\newblock Impossible tuning made possible: A new expert algorithm and its
  applications.
\newblock In \emph{Conference on Learning Theory}, pages 1216--1259. PMLR,
  2021.

\bibitem[Cutkosky(2018)]{cutkosky2018algorithms}
A.~Cutkosky.
\newblock \emph{Algorithms and Lower Bounds for Parameter-free Online
  Learning}.
\newblock PhD thesis, Stanford University, 2018.

\bibitem[Cutkosky(2019)]{cutkosky2019combining}
A.~Cutkosky.
\newblock Combining online learning guarantees.
\newblock In \emph{Proceedings of the Thirty-Second Conference on Learning
  Theory}, pages 895--913, 2019.

\bibitem[Cutkosky and Boahen(2017)]{cutkosky2017online}
A.~Cutkosky and K.~Boahen.
\newblock Online learning without prior information.
\newblock In \emph{Conference on Learning Theory}, pages 643--677, 2017.

\bibitem[Cutkosky and Mehta(2021)]{cutkosky2021high}
A.~Cutkosky and H.~Mehta.
\newblock High-probability bounds for non-convex stochastic optimization with
  heavy tails.
\newblock In \emph{Proceedings of the 35st International Conference on Neural
  Information Processing Systems}, pages 4883--4895, 2021.

\bibitem[Cutkosky and Orabona(2018)]{cutkosky2018black}
A.~Cutkosky and F.~Orabona.
\newblock Black-box reductions for parameter-free online learning in banach
  spaces.
\newblock In \emph{Conference On Learning Theory}, pages 1493--1529, 2018.

\bibitem[Duchi et~al.(2010{\natexlab{a}})Duchi, Hazan, and
  Singer]{duchi10adagrad}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock In \emph{Conference on Learning Theory (COLT)}, pages 257--269,
  2010{\natexlab{a}}.

\bibitem[Duchi et~al.(2010{\natexlab{b}})Duchi, Shalev-Shwartz, Singer, and
  Tewari]{duchi2010composite}
J.~C. Duchi, S.~Shalev-Shwartz, Y.~Singer, and A.~Tewari.
\newblock Composite objective mirror descent.
\newblock In \emph{COLT}, volume~10, pages 14--26. Citeseer,
  2010{\natexlab{b}}.

\bibitem[Foster et~al.(2017)Foster, Kale, Mohri, and
  Sridharan]{foster2017parameter}
D.~J. Foster, S.~Kale, M.~Mohri, and K.~Sridharan.
\newblock Parameter-free online learning via model selection.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6020--6030, 2017.

\bibitem[Gorbunov et~al.(2020)Gorbunov, Danilova, and
  Gasnikov]{gorbunov2020stochastic}
E.~Gorbunov, M.~Danilova, and A.~Gasnikov.
\newblock Stochastic optimization with heavy-tailed noise via accelerated
  gradient clipping.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15042--15053, 2020.

\bibitem[Harvey et~al.(2019)Harvey, Liaw, and Randhawa]{harvey2019simple}
N.~J. Harvey, C.~Liaw, and S.~Randhawa.
\newblock Simple and optimal high-probability bounds for strongly-convex
  stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:1909.00843}, 2019.

\bibitem[Hazan(2019)]{hazan2019introduction}
E.~Hazan.
\newblock Introduction to online convex optimization.
\newblock \emph{arXiv preprint arXiv:1909.05207}, 2019.

\bibitem[Howard et~al.(2021)Howard, Ramdas, McAuliffe, and
  Sekhon]{howard2021time}
S.~R. Howard, A.~Ramdas, J.~McAuliffe, and J.~Sekhon.
\newblock Time-uniform, nonparametric, nonasymptotic confidence sequences.
\newblock \emph{The Annals of Statistics}, 49\penalty0 (2):\penalty0
  1055--1080, 2021.

\bibitem[Jun and Orabona(2019)]{jun2019parameter}
K.-S. Jun and F.~Orabona.
\newblock Parameter-free online convex optimization with sub-exponential noise.
\newblock In \emph{Conference on Learning Theory}, pages 1802--1823. PMLR,
  2019.

\bibitem[Kavis et~al.(2022)Kavis, Levy, and Cevher]{kavis2022high}
A.~Kavis, K.~Y. Levy, and V.~Cevher.
\newblock High probability bounds for a class of nonconvex algorithms with
  adagrad stepsize.
\newblock \emph{arXiv preprint arXiv:2204.02833}, 2022.

\bibitem[Kempka et~al.(2019)Kempka, Kotlowski, and Warmuth]{kempka2019adaptive}
M.~Kempka, W.~Kotlowski, and M.~K. Warmuth.
\newblock Adaptive scale-invariant online algorithms for learning linear
  models.
\newblock In \emph{International Conference on Machine Learning}, pages
  3321--3330, 2019.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Li and Orabona(2020)]{li2020high}
X.~Li and F.~Orabona.
\newblock A high probability analysis of adaptive sgd with momentum.
\newblock In \emph{Workshop on Beyond First Order Methods in ML Systems at
  ICML'20}, 2020.

\bibitem[Madden et~al.(2020)Madden, Dall'Anese, and Becker]{madden2020high}
L.~Madden, E.~Dall'Anese, and S.~Becker.
\newblock High probability convergence bounds for stochastic gradient descent
  assuming the polyak-lojasiewicz inequality.
\newblock \emph{arXiv preprint arXiv:2006.05610}, 2020.

\bibitem[Mhammedi and Koolen(2020)]{mhammedi2020lipschitz}
Z.~Mhammedi and W.~M. Koolen.
\newblock Lipschitz and comparator-norm adaptivity in online learning.
\newblock \emph{Conference on Learning Theory}, pages 2858--2887, 2020.

\bibitem[Orabona(2019)]{orabona2019modern}
F.~Orabona.
\newblock A modern introduction to online learning.
\newblock \emph{arXiv preprint arXiv:1912.13213}, 2019.

\bibitem[Orabona and Jun(2021)]{orabona2021tight}
F.~Orabona and K.-S. Jun.
\newblock Tight concentrations and confidence sequences from the regret of
  universal portfolio.
\newblock \emph{arXiv preprint arXiv:2110.14099}, 2021.

\bibitem[Orabona and P{\'a}l(2016)]{orabona2016coin}
F.~Orabona and D.~P{\'a}l.
\newblock Coin betting and parameter-free online learning.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems 29}, pages
  577--585. Curran Associates, Inc., 2016.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi2018on}
S.~J. Reddi, S.~Kale, and S.~Kumar.
\newblock On the convergence of adam and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Shalev-Shwartz(2011)]{shalev2011online}
S.~Shalev-Shwartz.
\newblock Online learning and online convex optimization.
\newblock \emph{Foundations and Trends in Machine Learning}, 4\penalty0
  (2):\penalty0 107--194, 2011.

\bibitem[Tropp(2011)]{tropp2011freedman}
J.~Tropp.
\newblock Freedman's inequality for matrix martingales.
\newblock \emph{Electronic Communications in Probability}, 16:\penalty0
  262--270, 2011.

\bibitem[van~der Hoeven(2019)]{van2019user}
D.~van~der Hoeven.
\newblock User-specified local differential privacy in unconstrained adaptive
  online learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  14103--14112, 2019.

\bibitem[Ville(1939)]{ville1939etude}
J.~Ville.
\newblock Etude critique de la notion de collectif, gauthier-villars, paris,
  1939.
\newblock \emph{Monographies des Probabilit{\'e}s. Calcul des Probabilit{\'e}s
  et ses Applications}, 1939.

\bibitem[Vovk(2007)]{vovk2007hoeffding}
V.~Vovk.
\newblock Hoeffding's inequality in game-theoretic probability.
\newblock \emph{arXiv preprint arXiv:0708.2502}, 2007.

\bibitem[Vural et~al.(2022)Vural, Yu, Balasubramanian, Volgushev, and
  Erdogdu]{vural2022mirror}
N.~M. Vural, L.~Yu, K.~Balasubramanian, S.~Volgushev, and M.~A. Erdogdu.
\newblock Mirror descent strikes again: Optimal stochastic convex optimization
  under infinite noise variance.
\newblock \emph{arXiv preprint arXiv:2202.11632}, 2022.

\bibitem[Waudby-Smith and Ramdas(2020)]{waudby2020estimating}
I.~Waudby-Smith and A.~Ramdas.
\newblock Estimating means of bounded random variables by betting.
\newblock \emph{arXiv preprint arXiv:2010.09686}, 2020.

\bibitem[Zhang et~al.(2020)Zhang, Karimireddy, Veit, Kim, Reddi, Kumar, and
  Sra]{zhang2020adaptive}
J.~Zhang, S.~P. Karimireddy, A.~Veit, S.~Kim, S.~Reddi, S.~Kumar, and S.~Sra.
\newblock Why are adaptive methods good for attention models?
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15383--15393, 2020.

\bibitem[Zhou et~al.(2020)Zhou, Feng, Ma, Xiong, Hoi, et~al.]{zhou2020towards}
P.~Zhou, J.~Feng, C.~Ma, C.~Xiong, S.~C.~H. Hoi, et~al.
\newblock Towards theoretically understanding why sgd generalizes better than
  adam in deep learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 21285--21296, 2020.

\bibitem[Zinkevich(2003)]{zinkevich2003online}
M.~Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{Proceedings of the 20th International Conference on Machine
  Learning (ICML-03)}, pages 928--936, 2003.

\end{thebibliography}
