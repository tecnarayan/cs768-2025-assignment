\begin{thebibliography}{}

\bibitem[Banerjee, 2006]{banerjee_bayesian_2006}
Banerjee, A. (2006).
\newblock On bayesian bounds.
\newblock In {\em Proceedings of the 23rd International Conference on
  {{Machine}} Learning}, pages 81--88. {ACM}.

\bibitem[Banerjee, 2007]{banerjee_analysis_2007}
Banerjee, A. (2007).
\newblock An {{Analysis}} of {{Logistic Models}}: {{Exponential Family
  Connections}} and {{Online Performance}}.
\newblock In {\em Proceedings of the 2007 {{SIAM International Conference}} on
  {{Data Mining}}}, Proceedings, pages 204--215. {Society for Industrial and
  Applied Mathematics}.

\bibitem[Bartlett et~al., 2006]{bartlett_convexity_2006}
Bartlett, P.~L., Jordan, M.~I., and McAuliffe, J.~D. (2006).
\newblock Convexity, {{Classification}}, and {{Risk Bounds}}.
\newblock {\em Journal of the American Statistical Association},
  101(473):138--156.

\bibitem[Behrmann et~al., 2019]{behrmann_invertible_2019}
Behrmann, J., Grathwohl, W., Chen, R. T.~Q., Duvenaud, D., and Jacobsen, J.-H.
  (2019).
\newblock Invertible {{Residual Networks}}.
\newblock In {\em International {{Conference}} on {{Machine Learning}}},
  page~10.

\bibitem[Beygelzimer et~al., 2015]{beygelzimer_online_2015}
Beygelzimer, A., Hazan, E., Kale, S., and Luo, H. (2015).
\newblock Online {{Gradient Boosting}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}},
  page~9.

\bibitem[Blei et~al., 2017]{blei_variational_2017}
Blei, D.~M., Kucukelbir, A., and McAuliffe, J.~D. (2017).
\newblock Variational {{Inference}}: {{A Review}} for {{Statisticians}}.
\newblock {\em Journal of the American Statistical Association},
  112(518):859--877.

\bibitem[Bowman et~al., 2016]{bowman_generating_2016}
Bowman, S.~R., Vilnis, L., Vinyals, O., Dai, A., Jozefowicz, R., and Bengio, S.
  (2016).
\newblock Generating {{Sentences}} from a {{Continuous Space}}.
\newblock In {\em Proceedings of {{The}} 20th {{SIGNLL Conference}} on
  {{Computational Natural Language Learning}}}, pages 10--21, {Berlin,
  Germany}. {Association for Computational Linguistics}.

\bibitem[B{\"u}hlmann and Hothorn, 2007]{buhlmann_boosting_2007}
B{\"u}hlmann, P. and Hothorn, T. (2007).
\newblock Boosting {{Algorithms}}: {{Regularization}}, {{Prediction}} and
  {{Model Fitting}}.
\newblock {\em Statistical Science}, 22(4):477--505.

\bibitem[Campbell and Li, 2019]{campbell_universal_2019}
Campbell, T. and Li, X. (2019).
\newblock Universal {{Boosting Variational Inference}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}}.

\bibitem[Casale et~al., 2018]{casale_gaussian_2018}
Casale, F.~P., Dalca, A.~V., Saglietti, L., Listgarten, J., and Fusi, N.
  (2018).
\newblock Gaussian {{Process Prior Variational Autoencoders}}.
\newblock {\em Advances in Neural Information Processing Systems}, page~11.

\bibitem[Chen et~al., 2017a]{chen_continuoustime_2017}
Chen, C., Li, C., Chen, L., Wang, W., Pu, Y., and Carin, L. (2017a).
\newblock Continuous-{{Time Flows}} for {{Efficient Inference}} and {{Density
  Estimation}}.
\newblock {\em arXiv:1709.01179 [stat]}.

\bibitem[Chen et~al., 2020]{chen_vflow_2020}
Chen, J., Lu, C., Chenli, B., Zhu, J., and Tian, T. (2020).
\newblock {{VFlow}}: {{More Expressive Generative Flows}} with {{Variational
  Data Augmentation}}.
\newblock {\em arXiv:2002.09741 [cs, stat]}.

\bibitem[Chen et~al., 2018]{chen_neural_2018}
Chen, R. T.~Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. (2018).
\newblock Neural {{Ordinary Differential Equations}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}}.

\bibitem[Chen et~al., 2019]{chen_residual_2019}
Chen, T.~Q., Behrmann, J., Duvenaud, D.~K., and Jacobsen, J.-H. (2019).
\newblock Residual {{Flows}} for {{Invertible Generative Modeling}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}},
  page~11.

\bibitem[Chen et~al., 2017b]{chen_variational_2017}
Chen, X., Kingma, D.~P., Salimans, T., Duan, Y., Dhariwal, P., Schulman, J.,
  Sutskever, I., and Abbeel, P. (2017b).
\newblock Variational {{Lossy Autoencoder}}.
\newblock {\em ICLR}.

\bibitem[Cornish et~al., 2020]{cornish_relaxing_2020}
Cornish, R., Caterini, A.~L., Deligiannidis, G., and Doucet, A. (2020).
\newblock Relaxing {{Bijectivity Constraints}} with {{Continuously Indexed
  Normalising Flows}}.
\newblock {\em arXiv:1909.13833 [cs, stat]}.

\bibitem[Cranko and Nock, 2019]{cranko_boosting_2019}
Cranko, Z. and Nock, R. (2019).
\newblock Boosting {{Density Estimation Remastered}}.
\newblock {\em Proceedings of the 36th International Conference on Machine
  Learning}, 97:1416--1425.

\bibitem[Cremer et~al., 2018]{cremer_inference_2018}
Cremer, C., Li, X., and Duvenaud, D. (2018).
\newblock Inference {{Suboptimality}} in {{Variational Autoencoders}}.
\newblock In {\em International {{Conference}} on {{Machine Learning}}},
  {Stockholm, Sweden}.

\bibitem[De~Cao et~al., 2019]{decao_block_2019}
De~Cao, N., Titov, I., and Aziz, W. (2019).
\newblock Block {{Neural Autoregressive Flow}}.
\newblock {\em 35th Conference on Uncertainty in Artificial Intelligence
  (UAI19)}.

\bibitem[Dinh et~al., 2015]{dinh_nice_2015}
Dinh, L., Krueger, D., and Bengio, Y. (2015).
\newblock {{NICE}}: {{Non}}-linear {{Independent Components Estimation}}.
\newblock {\em ICLR}.

\bibitem[Dinh et~al., 2017]{dinh_density_2017}
Dinh, L., {Sohl-Dickstein}, J., and Bengio, S. (2017).
\newblock Density estimation using {{Real NVP}}.
\newblock {\em ICLR}.

\bibitem[Dinh et~al., 2019]{dinh_rad_2019}
Dinh, L., {Sohl-Dickstein}, J., Pascanu, R., and Larochelle, H. (2019).
\newblock A {{RAD}} approach to deep mixture models.
\newblock {\em arXiv:1903.07714 [cs, stat]}.

\bibitem[Donsker and Varadhan, 1976]{donsker_principal_1976}
Donsker, M.~D. and Varadhan, S. R.~S. (1976).
\newblock On the principal eigenvalue of second-order elliptic differential
  operators.
\newblock {\em Communications on Pure and Applied Mathematics}, 29(6):595--621.

\bibitem[Dua and Taniskidou, 2017]{dua_uci_2017}
Dua, D. and Taniskidou, E.~K. (2017).
\newblock {{UCI Machine Learning Repository}}.
\newblock https://archive.ics.uci.edu/ml/index.php.

\bibitem[Durkan et~al., 2019]{durkan_neural_2019}
Durkan, C., Bekasov, A., Murray, I., and Papamakarios, G. (2019).
\newblock Neural {{Spline Flows}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}}.

\bibitem[Frank and Wolfe, 1956]{frank_algorithm_1956}
Frank, M. and Wolfe, P. (1956).
\newblock An {{Algorithm}} for {{Quadratic Programming}}.
\newblock {\em Naval Research Logistics Quarterly}, 3:95--110.

\bibitem[Freund and Schapire, 1996]{freund_experiments_1996}
Freund, Y. and Schapire, R.~E. (1996).
\newblock Experiments with a {{New Boosting Algorithm}}.
\newblock In {\em International {{Conference}} on {{Machine Learning}}},
  page~9.

\bibitem[Freund and Schapire, 1997]{freund_decisiontheoretic_1997}
Freund, Y. and Schapire, R.~E. (1997).
\newblock A {{Decision}}-{{Theoretic Generalization}} of {{On}}-{{Line
  Learning}} and an {{Application}} to {{Boosting}}.
\newblock {\em Journal of Computer and System Sciences}, 55(1):119--139.

\bibitem[Friedman et~al., 2000]{friedman_additive_2000}
Friedman, J., Hastie, T., and Tibshirani, R. (2000).
\newblock Additive logistic regression: A statistical view of boosting.
\newblock {\em The annals of statistics}, 28(2):337--407.

\bibitem[Friedman, 2001]{friedman_greedy_2001}
Friedman, J.~H. (2001).
\newblock Greedy function approximation: A gradient boosting machine.
\newblock {\em Annals of statistics}, pages 1189--1232.

\bibitem[Friedman, 2002]{friedman_stochastic_2002}
Friedman, J.~H. (2002).
\newblock Stochastic gradient boosting.
\newblock {\em Computational Statistics \& Data Analysis}, 38(4):367--378.

\bibitem[Fu et~al., 2019]{fu_cyclical_2019}
Fu, H., Li, C., Liu, X., Gao, J., Celikyilmaz, A., and Carin, L. (2019).
\newblock Cyclical {{Annealing Schedule}}: {{A Simple Approach}} to
  {{Mitigating KL Vanishing}}.
\newblock In {\em {{NAACL}}}.

\bibitem[Germain et~al., 2015]{germain_made_2015}
Germain, M., Gregor, K., Murray, I., and Larochelle, H. (2015).
\newblock {{MADE}}: {{Masked Autoencoder}} for {{Distribution Estimation}}.
\newblock In {\em International {{Conference}} on {{Machine Learning}}},
  volume~37, {Lille, France}.

\bibitem[Grathwohl et~al., 2019]{grathwohl_ffjord_2019}
Grathwohl, W., Chen, R. T.~Q., Bettencourt, J., Sutskever, I., and Duvenaud, D.
  (2019).
\newblock {{FFJORD}}: {{Free}}-form {{Continuous Dynamics}} for {{Scalable
  Reversible Generative Models}}.
\newblock In {\em International {{Conference}} on {{Learning
  Representations}}}.

\bibitem[Grover and Ermon, 2018]{grover_boosted_2018}
Grover, A. and Ermon, S. (2018).
\newblock Boosted {{Generative Models}}.
\newblock In {\em {{AAAI}}}.

\bibitem[Guo et~al., 2016]{guo_boosting_2016}
Guo, F., Wang, X., Fan, K., Broderick, T., and Dunson, D.~B. (2016).
\newblock Boosting {{Variational Inference}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}},
  {Barcelona, Spain}.

\bibitem[Hastie et~al., 2001]{hastie_elements_2001}
Hastie, T., Tibshirani, R., and Friedman, J. (2001).
\newblock {\em The {{Elements}} of {{Statistical Learning}}}, volume~1 of {\em
  Springer {{Series}} in {{Statistics}}}.
\newblock {Springer New York Inc.}, second edition.

\bibitem[Higgins et~al., 2017]{higgins_vvae_2017}
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M.,
  Mohamed, S., and Lerchner, A. (2017).
\newblock {$\beta$}-{{VAE}}: {{Learning Basic Visual Concepts}} with a
  {{Constrained Variational Framework}}.
\newblock {\em ICLR}, page~22.

\bibitem[Ho et~al., 2019]{ho_flow_2019}
Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. (2019).
\newblock Flow++: {{Improving Flow}}-{{Based Generative Models}} with
  {{Variational Dequantization}} and {{Architecture Design}}.
\newblock In {\em International {{Conference}} on {{Machine Learning}}}.

\bibitem[Hu et~al., 2018]{hu_unifying_2018}
Hu, Z., Yang, Z., Salakhutdinov, R., and Xing, E.~P. (2018).
\newblock On {{Unifying Deep Generative Models}}.
\newblock In {\em {{ICLR}}}, pages 1--19.

\bibitem[Huang et~al., 2018a]{huang_neural_2018}
Huang, C.-W., Krueger, D., Lacoste, A., and Courville, A. (2018a).
\newblock Neural {{Autoregressive Flows}}.
\newblock In {\em International {{Conference}} on {{Machine Learning}}},
  page~10, {Stockholm, Sweden}.

\bibitem[Huang et~al., 2018b]{huang_improving_2018}
Huang, C.-W., Tan, S., Lacoste, A., and Courville, A. (2018b).
\newblock Improving {{Explorability}} in {{Variational Inference}} with
  {{Annealed Variational Objectives}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}},
  page~11, {Montr\'eal, Canada}.

\bibitem[Jaggi, 2013]{jaggi_revisiting_2013}
Jaggi, M. (2013).
\newblock Revisiting {{Frank}}-{{Wolfe}}: {{Projection}}-{{Free Sparse Convex
  Optimization}}.
\newblock In {\em Proceedings of the 30th {{International Conference}} on
  {{Machine Learning}}}, volume 28(1) of {\em Proceedings of {{Machine Learning
  Research}}}, page~9. {PMLR}.

\bibitem[Jaini et~al., 2019]{jaini_sumofsquares_2019}
Jaini, P., Selby, K.~A., and Yu, Y. (2019).
\newblock Sum-of-{{Squares Polynomial Flow}}.
\newblock In {\em International {{Conference}} on {{Machine Learning}}},
  volume~97, {Long Beach, California}. {PMLR}.

\bibitem[Jordan et~al., 1999]{jordan_introduction_1999}
Jordan, M.~I., Ghahramani, Z., Jaakkola, T.~S., and Saul, L.~K. (1999).
\newblock An {{Introduction}} to {{Variational Methods}} for {{Graphical
  Models}}.
\newblock In Jordan, M.~I., editor, {\em Learning in {{Graphical Models}}},
  pages 105--161. {Springer Netherlands}, {Dordrecht}.

\bibitem[Kingma and Ba, 2015]{kingma_adam_2015}
Kingma, D.~P. and Ba, J. (2015).
\newblock Adam: {{A}} method for stochastic optimization.
\newblock {\em ICLR}.

\bibitem[Kingma and Dhariwal, 2018]{kingma_glow_2018}
Kingma, D.~P. and Dhariwal, P. (2018).
\newblock Glow: {{Generative Flow}} with {{Invertible}} 1x1 {{Convolutions}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}},
  {Montr\'eal, Canada}.

\bibitem[Kingma et~al., 2016]{kingma_improving_2016}
Kingma, D.~P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and
  Welling, M. (2016).
\newblock Improving {{Variational Inference}} with {{Inverse Autoregressive
  Flow}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}}.

\bibitem[Kingma and Welling, 2014]{kingma_autoencoding_2014}
Kingma, D.~P. and Welling, M. (2014).
\newblock Auto-{{Encoding Variational Bayes}}.
\newblock {\em Proceedings of the 2nd International Conference on Learning
  Representations (ICLR)}, pages 1--14.

\bibitem[Lake et~al., 2015]{lake_humanlevel_2015}
Lake, B.~M., Salakhutdinov, R., and Tenenbaum, J.~B. (2015).
\newblock Human-level concept learning through probabilistic program induction.
\newblock {\em Science}, 350(6266):1332--1338.

\bibitem[Larochelle and Murray, 2011]{larochelle_neural_2011}
Larochelle, H. and Murray, I. (2011).
\newblock The {{Neural Autoregressive Distribution Estimator}}.
\newblock {\em International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 15:9.

\bibitem[Lebanon and Lafferty, 2002]{lebanon_boosting_2002}
Lebanon, G. and Lafferty, J.~D. (2002).
\newblock Boosting and {{Maximum Likelihood}} for {{Exponential Models}}.
\newblock {\em NIPS}, page~8.

\bibitem[Locatello et~al., 2018]{locatello_boosting_2018}
Locatello, F., Khanna, R., Ghosh, J., and R{\"a}tsch, G. (2018).
\newblock Boosting {{Variational Inference}}: An {{Optimization Perspective}}.
\newblock {\em International Conference on Artificial Intelligence and
  Statistics}.

\bibitem[Ma et~al., 2019]{ma_macow_2019}
Ma, X., Kong, X., Zhang, S., and Hovy, E. (2019).
\newblock {{MaCow}}: {{Masked Convolutional Generative Flow}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}},
  page~10.

\bibitem[Marlin et~al., 2010]{marlin_inductive_2010}
Marlin, B.~M., Swersky, K., Chen, B., and {de Freitas}, N. (2010).
\newblock Inductive {{Principles}} for {{Restricted Boltzmann Machine
  Learning}}.
\newblock {\em 13thInternational Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 9:8.

\bibitem[Martin et~al., 2001]{martin_database_2001}
Martin, D., Fowlkes, C., Tal, D., and Malik, J. (2001).
\newblock A database of human segmented natural images and its application to
  evaluating segmentation algorithms and measuring ecological statistics.
\newblock In {\em Proceedings {{Eighth IEEE International Conference}} on
  {{Computer Vision}}. {{ICCV}} 2001}, volume~2, pages 416--423, {Vancouver,
  BC, Canada}. {IEEE Comput. Soc}.

\bibitem[Mason et~al., 1999]{mason_boosting_1999}
Mason, L., Baxter, J., Bartlett, P.~L., and Frean, M.~R. (1999).
\newblock Boosting {{Algorithms}} as {{Gradient Descent}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}}, pages
  512--518.

\bibitem[Miller et~al., 2017]{miller_variational_2017}
Miller, A.~C., Foti, N., and Adams, R.~P. (2017).
\newblock Variational {{Boosting}}: {{Iteratively Refining Posterior
  Approximations}}.
\newblock In {\em Proceedings of the 34th {{International Conference}} on
  {{Machine Learning}}}, volume~70, pages 2420--2429. {PMLR}.

\bibitem[Nguyen et~al., 2005]{nguyen_divergences_2005}
Nguyen, X., Wainwright, M.~J., and Jordan, M.~I. (2005).
\newblock On divergences, surrogate loss functions, and decentralized
  detection.
\newblock {\em CoRR}, abs/math/0510521:35.

\bibitem[Nguyen et~al., 2009]{nguyen_surrogate_2009}
Nguyen, X., Wainwright, M.~J., and Jordan, M.~I. (2009).
\newblock On surrogate loss functions and \$f\$-divergences.
\newblock {\em The Annals of Statistics}, 37(2):876--904.

\bibitem[Papamakarios et~al., 2019]{papamakarios_normalizing_2019}
Papamakarios, G., Nalisnick, E., Rezende, D.~J., Mohamed, S., and
  Lakshminarayanan, B. (2019).
\newblock Normalizing {{Flows}} for {{Probabilistic Modeling}} and
  {{Inference}}.
\newblock {\em arXiv:1912.02762 [cs, stat]}.

\bibitem[Papamakarios et~al., 2017]{papamakarios_masked_2017}
Papamakarios, G., Pavlakou, T., and Murray, I. (2017).
\newblock Masked {{Autoregressive Flow}} for {{Density Estimation}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}}.

\bibitem[Paszke et~al., 2017]{paszke_automatic_2017}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A. (2017).
\newblock Automatic differentiation in {{PyTorch}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}},
  page~4.

\bibitem[Prenger et~al., 2018]{prenger_waveglow_2018}
Prenger, R., Valle, R., and Catanzaro, B. (2018).
\newblock {{WaveGlow}}: {{A Flow}}-based {{Generative Network}} for {{Speech
  Synthesis}}.
\newblock {\em arXiv:1811.00002 [cs, eess, stat]}.

\bibitem[Rainforth et~al., 2018]{rainforth_tighter_2018}
Rainforth, T., Kosiorek, A.~R., Le, T.~A., Maddison, C.~J., Igl, M., Wood, F.,
  and Teh, Y.~W. (2018).
\newblock Tighter {{Variational Bounds Are Not Necessarily Better}}.
\newblock In {\em International {{Conference}} on {{Machine Learning}}},
  {Stockholm, Sweden}.

\bibitem[Rezende and Mohamed, 2015]{rezende_variational_2015}
Rezende, D.~J. and Mohamed, S. (2015).
\newblock Variational {{Inference}} with {{Normalizing Flows}}.
\newblock In {\em International {{Conference}} on {{Machine Learning}}},
  volume~37, pages 1530--1538, {Lille, France}. {PMLR}.

\bibitem[Rezende et~al., 2014]{rezende_stochastic_2014}
Rezende, D.~J., Mohamed, S., and Wierstra, D. (2014).
\newblock Stochastic {{Backpropagation}} and {{Approximate Inference}} in
  {{Deep Generative Models}}.
\newblock In {\em Proceedings of the 31st {{International Conference}} on
  {{Machine Learning}}}, volume~32 of {\em 2}, pages 1278--1286, {Beijing,
  China}. {PMLR}.

\bibitem[Rippel and Adams, 2013]{rippel_highdimensional_2013}
Rippel, O. and Adams, R.~P. (2013).
\newblock High-{{Dimensional Probability Estimation}} with {{Deep Density
  Models}}.
\newblock {\em arXiv:1302.5125 [cs, stat]}.

\bibitem[Rosset and Segal, 2002]{rosset_boosting_2002}
Rosset, S. and Segal, E. (2002).
\newblock Boosting {{Density Estimation}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}},
  page~8.

\bibitem[Salman et~al., 2018]{salman_deep_2018}
Salman, H., Yadollahpour, P., Fletcher, T., and Batmanghelich, K. (2018).
\newblock Deep {{Diffeomorphic Normalizing Flows}}.
\newblock {\em arXiv:1810.03256 [cs, stat]}.

\bibitem[Schapire et~al., 1998]{schapire_boosting_1998}
Schapire, R.~E., Freund, Y., Bartlett, P., and Lee, W.~S. (1998).
\newblock Boosting the margin: A new explanation for the effectiveness of
  voting methods.
\newblock {\em The Annals of Statistics}, 26(5):1651--1686.

\bibitem[Smith, 2017]{smith_cyclical_2017}
Smith, L.~N. (2017).
\newblock Cyclical {{Learning Rates}} for {{Training Neural Networks}}.
\newblock In {\em 2017 {{IEEE Winter Conference}} on {{Applications}} of
  {{Computer Vision}} ({{WACV}})}, pages 464--472, {Santa Rosa, CA, USA}.
  {IEEE}.

\bibitem[S{\o}nderby et~al., 2016]{sonderby_ladder_2016}
S{\o}nderby, C.~K., Raiko, T., Maal{\o}e, L., S{\o}nderby, S.~K., and Winther,
  O. (2016).
\newblock Ladder {{Variational Autoencoders}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}}.

\bibitem[Tabak and Turner, 2013]{tabak_family_2013}
Tabak, E.~G. and Turner, C.~V. (2013).
\newblock A {{Family}} of {{Nonparametric Density Estimation Algorithms}}.
\newblock {\em Communications on Pure and Applied Mathematics}, 66(2):145--164.

\bibitem[Tabak and {Vanden-Eijnden}, 2010]{tabak_density_2010}
Tabak, E.~G. and {Vanden-Eijnden}, E. (2010).
\newblock Density estimation by dual ascent of the log-likelihood.
\newblock {\em Communications in Mathematical Sciences}, 8(1):217--233.

\bibitem[Tolstikhin et~al., 2017]{tolstikhin_adagan_2017}
Tolstikhin, I., Gelly, S., Bousquet, O., {Simon-Gabriel}, C.-J., and
  Sch{\"o}lkopf, B. (2017).
\newblock {{AdaGAN}}: {{Boosting Generative Models}}.
\newblock {\em arXiv:1701.02386 [cs, stat]}.

\bibitem[Tomczak and Welling, 2018]{tomczak_vae_2018}
Tomczak, J. and Welling, M. (2018).
\newblock {{VAE}} with a {{VampPrior}}.
\newblock In {\em International {{Conference}} on {{Artificial Intelligence}}
  and {{Statistics}} ({{AISTATS}})}, volume~84, {Lanzarote, Spain}.

\bibitem[Uria et~al., 2013]{uria_rnade_2013}
Uria, B., Murray, I., and Larochelle, H. (2013).
\newblock {{RNADE}}: {{The}} real-valued neural autoregressive
  density-estimator.
\newblock In {\em Advances in {{Neural Information Processing Systems}}}.

\bibitem[{van den Berg} et~al., 2018]{vandenberg_sylvester_2018}
{van den Berg}, R., {Leonard Hasenclever}, {Jakub M. Tomczak}, and {Max
  Welling} (2018).
\newblock Sylvester {{Normalizing Flows}} for {{Variational Inference}}.
\newblock In {\em Uncertainty in {{Artificial Intelligence}} ({{UAI}})}.

\bibitem[Wainwright and Jordan, 2007]{wainwright_graphical_2007}
Wainwright, M.~J. and Jordan, M.~I. (2007).
\newblock Graphical {{Models}}, {{Exponential Families}}, and {{Variational
  Inference}}.
\newblock {\em Foundations and Trends\textregistered{} in Machine Learning},
  1(1\textendash 2):1--305.

\bibitem[Ziegler and Rush, 2019]{ziegler_latent_2019}
Ziegler, Z.~M. and Rush, A.~M. (2019).
\newblock Latent {{Normalizing Flows}} for {{Discrete Sequences}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}}.

\end{thebibliography}
