@article{alberto2024can,
    author = {Testolin, Alberto},
    title = {Can Neural Networks Do Arithmetic? A Survey on the Elementary Numerical Skills of State-of-the-Art Deep Learning Models},
    journal = {Applied Sciences},
    volume = {14},
    year = {2024},
    number = {2},
    article-number = {744},
    url = {https://www.mdpi.com/2076-3417/14/2/744},
    issn = {2076-3417},
    doi = {10.3390/app14020744}
}

@article{anil2022exploring,
  title={Exploring length generalization in large language models},
  author={Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38546--38556},
  year={2022}
}

@article{anil2022path,
  title={Path independent equilibrium models can better exploit test-time computation},
  author={Anil, Cem and Pokle, Ashwini and Liang, Kaiqu and Treutlein, Johannes and Wu, Yuhuai and Bai, Shaojie and Kolter, J Zico and Grosse, Roger B},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7796--7809},
  year={2022}
}

@inproceedings{sukhbaatar-etal-2019-adaptive,
    title = "Adaptive Attention Span in Transformers",
    author = "Sukhbaatar, Sainbayar  and
      Grave, Edouard  and
      Bojanowski, Piotr  and
      Joulin, Armand",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1032",
    doi = "10.18653/v1/P19-1032",
    pages = "331--335",
}


@inproceedings{Lan2020ALBERT,
title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1eA7AEtvS}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{bansal2022endtoend,
  title={End-to-end Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking}, 
  author={Bansal, Arpit and Schwarzschild, Avi and Borgnia, Eitan and Emam, Zeyad and Huang, Furong and Goldblum, Micah and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  year={2022}
}

@inproceedings{dehghani2018universal,
  title={Universal Transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Lukasz},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{chi2023dissecting,
  title={Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis},
  author={Chi, Ta-Chung and Fan, Ting-Han and Rudnicky, Alexander and Ramadge, Peter},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={13522--13537},
  year={2023}
}

@inproceedings{chi2022kerple,
  title={KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation},
  author={Chi, Ta-Chung and Fan, Ting-Han and Ramadge, Peter and Rudnicky, Alexander},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{cognolato2022transformers,
  title={Transformers discover an elementary calculation system exploiting local attention and grid-like problem representation},
  author={Cognolato, Samuel and Testolin, Alberto},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2022},
  organization={IEEE}
}

@article{de2024simulation,
  title={Simulation of Graph Algorithms with Looped Transformers},
  author={de Luca, Artur Back and Fountoulakis, Kimon},
  journal={arXiv preprint arXiv:2402.01107},
  year={2024}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}


@article{duan2023interpolation,
  title={From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers},
  author={Duan, Shaoxiong and Shi, Yining},
  journal={arXiv preprint arXiv:2310.11984},
  year={2023}
}

@article{dziri2023faith,
  title={Faith and Fate: Limits of Transformers on Compositionality},
  author={Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jian, Liwei and Lin, Bill Yuchen and West, Peter and Bhagavatula, Chandra and Bras, Ronan Le and Hwang, Jena D and others},
  journal={arXiv preprint arXiv:2305.18654},
  year={2023}
}

@inproceedings{geiping2023cramming,
  title={Cramming: Training a Language Model on a single GPU in one day.},
  author={Geiping, Jonas and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={11117--11143},
  year={2023},
  organization={PMLR}
}

@inproceedings{giannou2023looped,
  title={Looped transformers as programmable computers},
  author={Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D and Papailiopoulos, Dimitris},
  booktitle={International Conference on Machine Learning},
  pages={11398--11442},
  year={2023},
  organization={PMLR}
}

@article{golkar2023xval,
  title={xval: A continuous number encoding for large language models},
  author={Golkar, Siavash and Pettee, Mariel and Eickenberg, Michael and Bietti, Alberto and Cranmer, Miles and Krawezik, Geraud and Lanusse, Francois and McCabe, Michael and Ohana, Ruben and Parker, Liam and others},
  journal={arXiv preprint arXiv:2310.02989},
  year={2023}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@inproceedings{ibarz2022generalist,
  title={A generalist neural algorithmic learner},
  author={Ibarz, Borja and Kurin, Vitaly and Papamakarios, George and Nikiforou, Kyriacos and Bennani, Mehdi and Csord{\'a}s, R{\'o}bert and Dudzik, Andrew Joseph and Bo{\v{s}}njak, Matko and Vitvitskyi, Alex and Rubanova, Yulia and others},
  booktitle={Learning on graphs conference},
  pages={2--1},
  year={2022},
  organization={PMLR}
}

@article{jelassi2023length,
  title={Length generalization in arithmetic transformers},
  author={Jelassi, Samy and d'Ascoli, St{\'e}phane and Domingo-Enrich, Carles and Wu, Yuhuai and Li, Yuanzhi and Charton, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:2306.15400},
  year={2023}
}

@article{kazemnejad2023impact,
  title={The Impact of Positional Encoding on Length Generalization in Transformers},
  author={Kazemnejad, Amirhossein and Padhi, Inkit and Ramamurthy, Karthikeyan Natesan and Das, Payel and Reddy, Siva},
  journal={arXiv preprint arXiv:2305.19466},
  year={2023}
}

@article{lee2023teaching,
  title={Teaching arithmetic to small transformers},
  author={Lee, Nayoung and Sreenivasan, Kartik and Lee, Jason D and Lee, Kangwook and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2307.03381},
  year={2023}
}

@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3843--3857},
  year={2022}
}

@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{li2023functional,
  title={Functional Interpolation for Relative Positions Improves Long Context Transformers},
  author={Li, Shanda and You, Chong and Guruganesh, Guru and Ainslie, Joshua and Ontanon, Santiago and Zaheer, Manzil and Sanghai, Sumit and Yang, Yiming and Kumar, Sanjiv and Bhojanapalli, Srinadh},
  journal={arXiv preprint arXiv:2310.04418},
  year={2023}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{ma2022mega,
  title={Mega: moving average equipped gated attention},
  author={Ma, Xuezhe and Zhou, Chunting and Kong, Xiang and He, Junxian and Gui, Liangke and Neubig, Graham and May, Jonathan and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2209.10655},
  year={2022}
}

@article{mankowitz2023faster,
  title={Faster sorting algorithms discovered using deep reinforcement learning},
  author={Mankowitz, Daniel J and Michi, Andrea and Zhernov, Anton and Gelmi, Marco and Selvi, Marco and Paduraru, Cosmin and Leurent, Edouard and Iqbal, Shariq and Lespiau, Jean-Baptiste and Ahern, Alex and others},
  journal={Nature},
  volume={618},
  number={7964},
  pages={257--263},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{mcleish2024benchmarking,
  title={Benchmarking ChatGPT on Algorithmic Reasoning},
  author={McLeish, Sean and Schwarzschild, Avi and Goldstein, Tom},
  journal={arXiv preprint arXiv:2404.03441},
  year={2024}
}

@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}

@article{openai2023gpt4tr,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.08774},
  url={https://api.semanticscholar.org/CorpusID:257532815}
}

@inproceedings{
  press2022train,
  title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  author={Ofir Press and Noah Smith and Mike Lewis},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=R8sQPpGCv0}
}

@article{qian2022limitations,
  title={Limitations of language models in arithmetic and symbolic induction},
  author={Qian, Jing and Wang, Hong and Li, Zekun and Li, Shiyang and Yan, Xifeng},
  journal={arXiv preprint arXiv:2208.05051},
  year={2022}
}

@article{ruoss2023randomized,
  title={Randomized Positional Encodings Boost Length Generalization of Transformers},
  author={Ruoss, Anian and Del{\'e}tang, Gr{\'e}goire and Genewein, Tim and Grau-Moya, Jordi and Csord{\'a}s, R{\'o}bert and Bennani, Mehdi and Legg, Shane and Veness, Joel},
  journal={arXiv preprint arXiv:2305.16843},
  year={2023}
}

@article{rodionov2024discrete,
  title={Discrete Neural Algorithmic Reasoning},
  author={Rodionov, Gleb and Prokhorenkova, Liudmila},
  journal={arXiv preprint arXiv:2402.11628},
  year={2024}
}

@article{saxton2019analysing,
  title={Analysing mathematical reasoning abilities of neural models},
  author={Saxton, David and Grefenstette, Edward and Hill, Felix and Kohli, Pushmeet},
  journal={arXiv preprint arXiv:1904.01557},
  year={2019}
}

@article{schwarzschild2021can,
  title={Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks},
  author={Schwarzschild, Avi and Borgnia, Eitan and Gupta, Arjun and Huang, Furong and Vishkin, Uzi and Goldblum, Micah and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{shaw2018self,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018}
}

@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@article{shen2023positional,
  title={Positional Description Matters for Transformers Arithmetic},
  author={Shen, Ruoqi and Bubeck, S{\'e}bastien and Eldan, Ronen and Lee, Yin Tat and Li, Yuanzhi and Zhang, Yi},
  journal={arXiv preprint arXiv:2311.14737},
  year={2023}
}

@article{singh2024tokenization,
  title={Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs},
  author={Singh, Aaditya K and Strouse, DJ},
  journal={arXiv preprint arXiv:2402.14903},
  year={2024}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{velivckovic2022clrs,
  title={The clrs algorithmic reasoning benchmark},
  author={Veli{\v{c}}kovi{\'c}, Petar and Badia, Adri{\`a} Puigdom{\`e}nech and Budden, David and Pascanu, Razvan and Banino, Andrea and Dashevskiy, Misha and Hadsell, Raia and Blundell, Charles},
  booktitle={International Conference on Machine Learning},
  pages={22084--22102},
  year={2022},
  organization={PMLR}
}

@article{wang_deepnet_2022,
  title = {{{DeepNet}}: {{Scaling Transformers}} to 1,000 {{Layers}}},
  shorttitle = {{{DeepNet}}},
  author = {Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dongdong and Wei, Furu},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.00555 [cs]},
  eprint = {2203.00555},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2203.00555},
  urldate = {2022-03-07},
  abstract = {In this paper, we propose a simple yet effective method to stabilize extremely deep Transformers. Specifically, we introduce a new normalization function (DeepNorm) to modify the residual connection in Transformer, accompanying with theoretically derived initialization. In-depth theoretical analysis shows that model updates can be bounded in a stable way. The proposed method combines the best of two worlds, i.e., good performance of Post-LN and stable training of Pre-LN, making DeepNorm a preferred alternative. We successfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and feed-forward network sublayers) without difficulty, which is one order of magnitude deeper than previous deep Transformers. Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{wu2022autoformalization,
  title={Autoformalization with large language models},
  author={Wu, Yuhuai and Jiang, Albert Qiaochu and Li, Wenda and Rabe, Markus and Staats, Charles and Jamnik, Mateja and Szegedy, Christian},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={32353--32368},
  year={2022}
}

@article{yang2023gpt,
  title={Gpt can solve mathematical problems without a calculator},
  author={Yang, Zhen and Ding, Ming and Lv, Qingsong and Jiang, Zhihuan and He, Zehai and Guo, Yuyi and Bai, Jinfeng and Tang, Jie},
  journal={arXiv preprint arXiv:2309.03241},
  year={2023}
}

@article{yang2023looped,
  title={Looped Transformers are Better at Learning Learning Algorithms},
  author={Yang, Liu and Lee, Kangwook and Nowak, Robert and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2311.12424},
  year={2023}
}

@inproceedings{zhai2022scaling,
  title={Scaling vision transformers},
  author={Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12104--12113},
  year={2022}
}

@article{zhao2018n,
  title={An $ O (N) $ Sorting Algorithm: Machine Learning Sort},
  author={Zhao, Hanqing and Luo, Yuehan},
  journal={arXiv preprint arXiv:1805.04272},
  year={2018}
}

@article{zhou2023algorithms,
  title={What algorithms can transformers learn? a study in length generalization},
  author={Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2310.16028},
  year={2023}
}

@article{zhou2024transformers,
  title={Transformers Can Achieve Length Generalization But Not Robustly},
  author={Zhou, Yongchao and Alon, Uri and Chen, Xinyun and Wang, Xuezhi and Agarwal, Rishabh and Zhou, Denny},
  journal={arXiv preprint arXiv:2402.09371},
  year={2024}
}

@article{peng2024yarn,
  title={Yarn: Efficient context window extension of large language models},
  author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
  journal={International Conference on Learning Representations},
  year={2024}
}

@misc{loeber_16_2024,
  type = {Substack Newsletter},
  title = {\#16: {{Notes}} on {{Arithmetic}} in {{GPT-4}}},
  shorttitle = {\#16},
  author = {Loeber, John},
  year = {2024},
  month = feb,
  journal = {Loeber on Substack},
  url = {https://loeber.substack.com/p/16-notes-on-arithmetic-in-gpt-4},
  urldate = {2024-05-17},
  abstract = {A few weeks ago, I had a list of dollar amounts that I needed to sum up. I thought: ``GPT is good at converting formats,'' and copy-pasted them into ChatGPT. The result looked plausible. But I had a moment of doubt: why should GPT be good at addition? So I double-checked the sum myself. It turned out that GPT was wrong; the right number was \$660.44.}
}

