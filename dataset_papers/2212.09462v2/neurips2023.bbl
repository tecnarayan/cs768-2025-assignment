\begin{thebibliography}{76}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[chi(2008)]{chisquare}
\emph{Chi-square Distribution}, pages 70--72.
\newblock Springer New York, New York, NY, 2008.
\newblock ISBN 978-0-387-32833-1.
\newblock \doi{10.1007/978-0-387-32833-1_54}.
\newblock URL \url{https://doi.org/10.1007/978-0-387-32833-1_54}.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
  Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
  et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 23716--23736, 2022.

\bibitem[Austin et~al.(2021)Austin, Johnson, Ho, Tarlow, and van~den
  Berg]{structured_diffusion}
Jacob Austin, Daniel~D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van~den
  Berg.
\newblock Structured denoising diffusion models in discrete state-spaces.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan,
  editors, \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=h7-XixPCAL}.

\bibitem[Bao et~al.(2023)Bao, Nie, Xue, Cao, Li, Su, and Zhu]{bao2022all}
Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu.
\newblock All are worth words: A vit backbone for diffusion models.
\newblock In \emph{CVPR}, 2023.

\bibitem[Bojar et~al.(2014)Bojar, Buck, Federmann, Haddow, Koehn, Leveling,
  Monz, Pecina, Post, Saint-Amand, Soricut, Specia, and
  Tamchyna]{bojar-EtAl:2014:W14-33}
Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn,
  Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand,
  Radu Soricut, Lucia Specia, and Ale~{s} Tamchyna.
\newblock Findings of the 2014 workshop on statistical machine translation.
\newblock In \emph{Proceedings of the Ninth Workshop on Statistical Machine
  Translation}, pages 12--58, Baltimore, Maryland, USA, June 2014. Association
  for Computational Linguistics.
\newblock URL \url{http://www.aclweb.org/anthology/W/W14/W14-3302}.

\bibitem[Chen et~al.(2021)Chen, Zhang, Zen, Weiss, Norouzi, and
  Chan]{chen2021wavegrad}
Nanxin Chen, Yu~Zhang, Heiga Zen, Ron~J Weiss, Mohammad Norouzi, and William
  Chan.
\newblock Wavegrad: Estimating gradients for waveform generation.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=NsMLjcFaO8O}.

\bibitem[Chen(2023)]{chen2023importance}
Ting Chen.
\newblock On the importance of noise scheduling for diffusion models.
\newblock \emph{arXiv preprint arXiv:2301.10972}, 2023.

\bibitem[Chen et~al.(2022)Chen, Zhang, and Hinton]{chen2022analog}
Ting Chen, Ruixiang Zhang, and Geoffrey Hinton.
\newblock Analog bits: Generating discrete data using diffusion models with
  self-conditioning.
\newblock \emph{arXiv preprint arXiv:2208.04202}, 2022.

\bibitem[Chen et~al.(2017)Chen, Zhang, Zhang, and Zhao]{quora-question-pairs}
Zihang Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi Zhao.
\newblock Quora question pairs.
\newblock 2017.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma, et~al.]{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}, 2022.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek,
  Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, et~al.]{dehghani2023scaling}
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan
  Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim
  Alabdulmohsin, et~al.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock \emph{arXiv preprint arXiv:2302.05442}, 2023.

\bibitem[Dhariwal and Nichol(2021)]{diffusion_beats_gans2021}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 8780--8794. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf}.

\bibitem[Dieleman et~al.(2022)Dieleman, Sartran, Roshannai, Savinov, Ganin,
  Richemond, Doucet, Strudel, Dyer, Durkan, et~al.]{dieleman2022continuous}
Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav
  Ganin, Pierre~H Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor
  Durkan, et~al.
\newblock Continuous diffusion for categorical data.
\newblock \emph{arXiv preprint arXiv:2211.15089}, 2022.

\bibitem[Ficler and Goldberg(2017)]{ficler2017controlling}
Jessica Ficler and Yoav Goldberg.
\newblock Controlling linguistic style aspects in neural language generation.
\newblock \emph{EMNLP 2017}, page~94, 2017.

\bibitem[Goel and Byrne(2000)]{goel-2000-mbr}
Vaibhava Goel and William~J Byrne.
\newblock Minimum bayes-risk automatic speech recognition.
\newblock \emph{Computer Speech \& Language}, 14\penalty0 (2):\penalty0
  115--135, 2000.
\newblock ISSN 0885-2308.
\newblock \doi{https://doi.org/10.1006/csla.2000.0138}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0885230800901384}.

\bibitem[Gong et~al.(2023)Gong, Li, Feng, Wu, and Kong]{gong2023diffuseq}
Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and Lingpeng Kong.
\newblock Diffuseq: Sequence to sequence text generation with diffusion models.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=jQj-_rLVXsj}.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{gans}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In Z.~Ghahramani, M.~Welling, C.~Cortes, N.~Lawrence, and K.Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~27. Curran Associates, Inc., 2014.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf}.

\bibitem[He et~al.(2021)He, Liu, Gao, and Chen]{he2021deberta}
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.
\newblock Deberta: Decoding-enhanced bert with disentangled attention.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=XPZIaotutsD}.

\bibitem[He et~al.(2023)He, Sun, Tang, Wang, Huang, and
  Qiu]{he-etal-2023-diffusionbert}
Zhengfu He, Tianxiang Sun, Qiong Tang, Kuanning Wang, Xuanjing Huang, and
  Xipeng Qiu.
\newblock {D}iffusion{BERT}: Improving generative masked language models with
  diffusion models.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 4521--4534,
  Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.248}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.248}.

\bibitem[Hendrycks and Gimpel(2016)]{hendrycks2016gaussian}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Ho and Salimans(2022)]{ho2022classifier}
Jonathan Ho and Tim Salimans.
\newblock Classifier-free diffusion guidance.
\newblock \emph{arXiv preprint arXiv:2207.12598}, 2022.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ddpm}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{CoRR}, abs/2006.11239, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.11239}.

\bibitem[Ho et~al.(2022)Ho, Chan, Saharia, Whang, Gao, Gritsenko, Kingma,
  Poole, Norouzi, Fleet, et~al.]{ho2022imagenvideo}
Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey
  Gritsenko, Diederik~P Kingma, Ben Poole, Mohammad Norouzi, David~J Fleet,
  et~al.
\newblock Imagen video: High definition video generation with diffusion models.
\newblock \emph{arXiv preprint arXiv:2210.02303}, 2022.

\bibitem[Holtzman et~al.(2020)Holtzman, Buys, Du, Forbes, and
  Choi]{Holtzman2020The}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rygGQyrFvH}.

\bibitem[Hoogeboom et~al.(2021)Hoogeboom, Nielsen, Jaini, Forré, and
  Welling]{multinomial_diffusion}
Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max
  Welling.
\newblock Argmax flows and multinomial diffusion: Towards non-autoregressive
  language models.
\newblock \emph{CoRR}, abs/2102.05379, 2021.
\newblock URL \url{https://arxiv.org/abs/2102.05379}.

\bibitem[Hoogeboom et~al.(2022)Hoogeboom, Gritsenko, Bastings, Poole, van~den
  Berg, and Salimans]{autoregressive_diffusion}
Emiel Hoogeboom, Alexey~A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne
  van~den Berg, and Tim Salimans.
\newblock Autoregressive diffusion models.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=Lm8T39vLDTE}.

\bibitem[Hoogeboom et~al.(2023)Hoogeboom, Heek, and
  Salimans]{hoogeboom2023simple}
Emiel Hoogeboom, Jonathan Heek, and Tim Salimans.
\newblock simple diffusion: End-to-end diffusion for high resolution images.
\newblock \emph{arXiv preprint arXiv:2301.11093}, 2023.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem[Keskar et~al.(2019)Keskar, McCann, Varshney, Xiong, and
  Socher]{keskar2019ctrl}
Nitish~Shirish Keskar, Bryan McCann, Lav~R Varshney, Caiming Xiong, and Richard
  Socher.
\newblock Ctrl: A conditional transformer language model for controllable
  generation.
\newblock \emph{arXiv preprint arXiv:1909.05858}, 2019.

\bibitem[Kingma et~al.(2021{\natexlab{a}})Kingma, Salimans, Poole, and
  Ho]{kingma2021variational}
Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho.
\newblock Variational diffusion models.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 21696--21707, 2021{\natexlab{a}}.

\bibitem[Kingma et~al.(2021{\natexlab{b}})Kingma, Salimans, Poole, and
  Ho]{kingma2021on}
Diederik~P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho.
\newblock On density estimation with diffusion models.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan,
  editors, \emph{Advances in Neural Information Processing Systems},
  2021{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=2LdBqxc1Yv}.

\bibitem[Kong et~al.(2021)Kong, Ping, Huang, Zhao, and
  Catanzaro]{kong2021diffwave}
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro.
\newblock Diffwave: A versatile diffusion model for audio synthesis.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=a-xFK8Ymz5J}.

\bibitem[Korbak et~al.(2023)Korbak, Shi, Chen, Bhalerao, Buckley, Phang,
  Bowman, and Perez]{korbak2023pretraining}
Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher~L
  Buckley, Jason Phang, Samuel~R Bowman, and Ethan Perez.
\newblock Pretraining language models with human preferences.
\newblock \emph{arXiv preprint arXiv:2302.08582}, 2023.

\bibitem[Kumar and Byrne(2004)]{kumar-byrne-2004-minimum}
Shankar Kumar and William Byrne.
\newblock Minimum {B}ayes-risk decoding for statistical machine translation.
\newblock In \emph{Proceedings of the Human Language Technology Conference of
  the North {A}merican Chapter of the Association for Computational
  Linguistics: {HLT}-{NAACL} 2004}, pages 169--176, Boston, Massachusetts, USA,
  May 2 - May 7 2004. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/N04-1022}.

\bibitem[Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer]{bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension, 2019.
\newblock URL \url{https://arxiv.org/abs/1910.13461}.

\bibitem[Li et~al.(2022)Li, Thickstun, Gulrajani, Liang, and
  Hashimoto]{diffusion_lm}
Xiang~Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori~B.
  Hashimoto.
\newblock Diffusion-lm improves controllable text generation, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.14217}.

\bibitem[Lin(2004)]{lin2004rouge}
Chin-Yew Lin.
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In \emph{Text summarization branches out}, pages 74--81, 2004.

\bibitem[Lin et~al.(2023)Lin, Gong, Shen, Wu, Fan, Lin, Chen, and
  Duan]{lin2023genie}
Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Weizhu
  Chen, and Nan Duan.
\newblock Text generation with diffusion language models: A pre-training
  approach with continuous paragraph denoise.
\newblock 2023.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[Lu et~al.(2022)Lu, Welleck, Hessel, Jiang, Qin, West, Ammanabrolu, and
  Choi]{lu2022quark}
Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West,
  Prithviraj Ammanabrolu, and Yejin Choi.
\newblock {QUARK}: Controllable text generation with reinforced unlearning.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=5HaIds3ux5O}.

\bibitem[Meng et~al.(2022)Meng, He, Song, Song, Wu, Zhu, and
  Ermon]{meng2022sdedit}
Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and
  Stefano Ermon.
\newblock {SDE}dit: Guided image synthesis and editing with stochastic
  differential equations.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=aBsCjcPu_tE}.

\bibitem[Mostafazadeh et~al.(2016)Mostafazadeh, Chambers, He, Parikh, Batra,
  Vanderwende, Kohli, and Allen]{mostafazadeh2016corpus}
Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra,
  Lucy Vanderwende, Pushmeet Kohli, and James Allen.
\newblock A corpus and cloze evaluation for deeper understanding of commonsense
  stories.
\newblock In \emph{Proceedings of the 2016 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 839--849, 2016.

\bibitem[Narayan et~al.(2018{\natexlab{a}})Narayan, Cohen, and
  Lapata]{narayan-etal-2018-dont}
Shashi Narayan, Shay~B. Cohen, and Mirella Lapata.
\newblock Don{'}t give me the details, just the summary! topic-aware
  convolutional neural networks for extreme summarization.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 1797--1807, Brussels, Belgium,
  October-November 2018{\natexlab{a}}. Association for Computational
  Linguistics.
\newblock \doi{10.18653/v1/D18-1206}.
\newblock URL \url{https://aclanthology.org/D18-1206}.

\bibitem[Narayan et~al.(2018{\natexlab{b}})Narayan, Cohen, and
  Lapata]{narayan2018don}
Shashi Narayan, Shay~B Cohen, and Mirella Lapata.
\newblock Don't give me the details, just the summary! topic-aware
  convolutional neural networks for extreme summarization.
\newblock \emph{arXiv preprint arXiv:1808.08745}, 2018{\natexlab{b}}.

\bibitem[Nichol and Dhariwal(2021)]{improved-ddpm-nichol21a}
Alexander~Quinn Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 8162--8171. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/nichol21a.html}.

\bibitem[Peebles and Xie(2022)]{peebles2022scalable}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock \emph{arXiv preprint arXiv:2212.09748}, 2022.

\bibitem[Pillutla et~al.(2021)Pillutla, Swayamdipta, Zellers, Thickstun,
  Welleck, Choi, and Harchaoui]{pillutla2021mauve}
Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean
  Welleck, Yejin Choi, and Zaid Harchaoui.
\newblock Mauve: Measuring the gap between neural text and human text using
  divergence frontiers.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 4816--4828, 2021.

\bibitem[Post(2018)]{post-2018-call}
Matt Post.
\newblock A call for clarity in reporting {BLEU} scores.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation:
  Research Papers}, pages 186--191, Brussels, Belgium, October 2018.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W18-6319}.
\newblock URL \url{https://aclanthology.org/W18-6319}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{gpt_2}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 5485--5551, 2020.

\bibitem[Rombach et~al.(2021)Rombach, Blattmann, Lorenz, Esser, and
  Ommer]{latent_diffusion}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.10752}.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and
  Ommer]{Rombach_2022_CVPR}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\"orn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 10684--10695, June 2022.

\bibitem[Saharia et~al.(2022{\natexlab{a}})Saharia, Chan, Chang, Lee, Ho,
  Salimans, Fleet, and Norouzi]{saharia2022palette}
Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim
  Salimans, David Fleet, and Mohammad Norouzi.
\newblock Palette: Image-to-image diffusion models.
\newblock In \emph{ACM SIGGRAPH 2022 Conference Proceedings}, pages 1--10,
  2022{\natexlab{a}}.

\bibitem[Saharia et~al.(2022{\natexlab{b}})Saharia, Chan, Saxena, Li, Whang,
  Denton, Ghasemipour, Gontijo-Lopes, Ayan, Salimans, Ho, Fleet, and
  Norouzi]{saharia2022imagen}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily
  Denton, Seyed Kamyar~Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu~Karagol
  Ayan, Tim Salimans, Jonathan Ho, David~J. Fleet, and Mohammad Norouzi.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=08Yk-n5l2Al}.

\bibitem[Saharia et~al.(2022{\natexlab{c}})Saharia, Chan, Saxena, Li, Whang,
  Denton, Ghasemipour, Gontijo~Lopes, Karagol~Ayan, Salimans, Ho, Fleet, and
  Norouzi]{imagen}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily~L
  Denton, Kamyar Ghasemipour, Raphael Gontijo~Lopes, Burcu Karagol~Ayan, Tim
  Salimans, Jonathan Ho, David~J Fleet, and Mohammad Norouzi.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~35,
  pages 36479--36494. Curran Associates, Inc., 2022{\natexlab{c}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf}.

\bibitem[Saharia et~al.(2022{\natexlab{d}})Saharia, Ho, Chan, Salimans, Fleet,
  and Norouzi]{saharia2022image}
Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David~J Fleet, and
  Mohammad Norouzi.
\newblock Image super-resolution via iterative refinement.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2022{\natexlab{d}}.

\bibitem[Salimans and Ho(2022)]{salimans2022progressive}
Tim Salimans and Jonathan Ho.
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=TIdIXIpzhoI}.

\bibitem[Schneider et~al.(2023)Schneider, Jin, and
  Sch{\"o}lkopf]{schneider2023mo}
Flavio Schneider, Zhijing Jin, and Bernhard Sch{\"o}lkopf.
\newblock Mo$\backslash$\^{} usai: Text-to-music generation with long-context
  latent diffusion.
\newblock \emph{arXiv preprint arXiv:2301.11757}, 2023.

\bibitem[Shazeer(2020)]{shazeer2020glu}
Noam Shazeer.
\newblock Glu variants improve transformer.
\newblock \emph{arXiv preprint arXiv:2002.05202}, 2020.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{socher2013recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D Manning,
  Andrew~Y Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pages 1631--1642, 2013.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and
  Ganguli]{diffusion_orig}
Jascha Sohl-Dickstein, Eric~A. Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics, 2015.
\newblock URL \url{https://arxiv.org/abs/1503.03585}.

\bibitem[Song et~al.(2020)Song, Meng, and Ermon]{ddim}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock \emph{CoRR}, abs/2010.02502, 2020.
\newblock URL \url{https://arxiv.org/abs/2010.02502}.

\bibitem[Song and Ermon(2019)]{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Song et~al.(2022)Song, Shen, Xing, and Ermon]{song2022solving}
Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon.
\newblock Solving inverse problems in medical imaging with score-based
  generative models.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=vaRCHVj0uGI}.

\bibitem[Song et~al.(2023)Song, Dhariwal, Chen, and
  Sutskever]{song2023consistency}
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.
\newblock Consistency models, 2023.

\bibitem[Strudel et~al.(2022{\natexlab{a}})Strudel, Tallec, Altch{\'e}, Du,
  Ganin, Mensch, Grathwohl, Savinov, Dieleman, Sifre, et~al.]{strudel2022self}
Robin Strudel, Corentin Tallec, Florent Altch{\'e}, Yilun Du, Yaroslav Ganin,
  Arthur Mensch, Will Grathwohl, Nikolay Savinov, Sander Dieleman, Laurent
  Sifre, et~al.
\newblock Self-conditioned embedding diffusion for text generation.
\newblock \emph{arXiv preprint arXiv:2211.04236}, 2022{\natexlab{a}}.

\bibitem[Strudel et~al.(2022{\natexlab{b}})Strudel, Tallec, Altché, Du, Ganin,
  Mensch, Grathwohl, Savinov, Dieleman, Sifre, and
  Leblond]{self_cond_text_diffusion}
Robin Strudel, Corentin Tallec, Florent Altché, Yilun Du, Yaroslav Ganin,
  Arthur Mensch, Will Grathwohl, Nikolay Savinov, Sander Dieleman, Laurent
  Sifre, and Rémi Leblond.
\newblock Self-conditioned embedding diffusion for text generation,
  2022{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2211.04236}.

\bibitem[Su et~al.(2022)Su, Lan, Wang, Yogatama, Kong, and
  Collier]{su2022contrastive}
Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel Collier.
\newblock A contrastive framework for neural text generation.
\newblock \emph{arXiv preprint arXiv:2202.06417}, 2022.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, \L{}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, NIPS'17, page 6000–6010, Red Hook, NY,
  USA, 2017. Curran Associates Inc.
\newblock ISBN 9781510860964.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan,
  Wang, and Liu]{pmlr-v119-xiong20b}
Ruibin Xiong, Yunchang Yang, Di~He, Kai Zheng, Shuxin Zheng, Chen Xing,
  Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In Hal~Daumé III and Aarti Singh, editors, \emph{Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 10524--10533. PMLR,
  13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/xiong20b.html}.

\bibitem[Xu et~al.(2023)Xu, Powers, Dror, Ermon, and Leskovec]{xu2023geometric}
Minkai Xu, Alexander Powers, Ron Dror, Stefano Ermon, and Jure Leskovec.
\newblock Geometric latent diffusion models for 3d molecule generation.
\newblock \emph{arXiv preprint arXiv:2305.01140}, 2023.

\bibitem[Xue et~al.(2021)Xue, Constant, Roberts, Kale, Al-Rfou, Siddhant,
  Barua, and Raffel]{xue-etal-2021-mt5}
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
  Siddhant, Aditya Barua, and Colin Raffel.
\newblock m{T}5: A massively multilingual pre-trained text-to-text transformer.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 483--498, Online, June 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.41}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.41}.

\bibitem[Ye et~al.(2023)Ye, Zheng, Bao, Qian, and Wang]{ye2023dinoiser}
Jiasheng Ye, Zaixiang Zheng, Yu~Bao, Lihua Qian, and Mingxuan Wang.
\newblock Dinoiser: Diffused conditional sequence learning by manipulating
  noises.
\newblock \emph{arXiv preprint arXiv:2302.10025}, 2023.

\bibitem[Zhang and Sennrich(2019)]{zhang2019root}
Biao Zhang and Rico Sennrich.
\newblock Root mean square layer normalization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and
  Artzi]{zhang2019bertscore}
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi.
\newblock Bertscore: Evaluating text generation with bert.
\newblock \emph{arXiv preprint arXiv:1904.09675}, 2019.

\bibitem[Zheng et~al.(2023)Zheng, Yuan, Yu, and Kong]{zheng2023rdm}
Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong.
\newblock A reparameterized discrete diffusion model for text generation.
\newblock \emph{arXiv preprint arXiv:2302.05737}, 2023.

\end{thebibliography}
