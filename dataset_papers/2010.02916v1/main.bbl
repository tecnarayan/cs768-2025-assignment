\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d'~Alch\'{e}-Buc,
  E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural Information
  Processing Systems 32}, pages 8139--8148. Curran Associates, Inc.,
  2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Li, and
  Lyu]{arora2018theoretical}
Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu.
\newblock Theoretical analysis of auto rate-tuning by batch normalization.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bengio(2012)]{bengio2012practical}
Yoshua Bengio.
\newblock Practical recommendations for gradient-based training of deep
  architectures.
\newblock In \emph{Neural networks: Tricks of the trade}, pages 437--478.
  Springer, 2012.

\bibitem[Bjorck et~al.(2018)Bjorck, Gomes, Selman, and
  Weinberger]{bjorck2018understanding}
Nils Bjorck, Carla~P Gomes, Bart Selman, and Kilian~Q Weinberger.
\newblock Understanding batch normalization.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems 31}, pages 7694--7705. Curran Associates, Inc., 2018.

\bibitem[Bovier(2004)]{bovier2004metastability}
Eckhoff-Michael Gayrard VÃ©ronique Klein~Markus Bovier, Anton.
\newblock Metastability in reversible diffusion processes i: Sharp asymptotics
  for capacities and exit times.
\newblock \emph{Journal of the European Mathematical Society}, 006\penalty0
  (4):\penalty0 399--424, 2004.

\bibitem[Cai et~al.(2019)Cai, Li, and Shen]{cai2019aquantitative}
Yongqiang Cai, Qianxiao Li, and Zuowei Shen.
\newblock A quantitative analysis of the effect of batch normalization on
  gradient descent.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages 882--890,
  Long Beach, California, USA, 09--15 Jun 2019. PMLR.

\bibitem[Chaudhari and Soatto(2018)]{chaudhari2018stochastic}
Pratik Chaudhari and Stefano Soatto.
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Cheng et~al.(2019)Cheng, Yin, Bartlett, and
  Jordan]{cheng2019stochastic}
Xiang Cheng, Dong Yin, Peter~L Bartlett, and Michael~I Jordan.
\newblock Stochastic gradient and langevin processes.
\newblock \emph{arXiv preprint arXiv:1907.03215}, 2019.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1019--1028. JMLR.org, 2017.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and
  Hamprecht]{draxler2018essentially}
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred~A Hamprecht.
\newblock Essentially no barriers in neural network energy landscape.
\newblock \emph{arXiv preprint arXiv:1803.00885}, 2018.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{garipov2018loss}
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry~P Vetrov, and
  Andrew~G Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8789--8798, 2018.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Hanson and Pratt(1989)]{hanson1989comparing}
Stephen~Jose Hanson and Lorien~Y. Pratt.
\newblock Comparing biases for minimal network construction with
  back-propagation.
\newblock In D.~S. Touretzky, editor, \emph{Advances in Neural Information
  Processing Systems 1}, pages 177--185. Morgan-Kaufmann, 1989.

\bibitem[He et~al.(2019)He, Huang, and Yuan]{he2019asym}
Haowei He, Gao Huang, and Yang Yuan.
\newblock Asymmetric valleys: Beyond sharp and flat local minima.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d'~Alch\'{e}-Buc,
  E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural Information
  Processing Systems 32}, pages 2553--2564. Curran Associates, Inc., 2019.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997flat}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Flat minima.
\newblock \emph{Neural Computation}, 9\penalty0 (1):\penalty0 1--42, 1997.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{hoffer2017train}
Elad Hoffer, Itay Hubara, and Daniel Soudry.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems 30}, pages 1731--1741. Curran Associates,
  Inc., 2017.

\bibitem[Hoffer et~al.(2018{\natexlab{a}})Hoffer, Banner, Golan, and
  Soudry]{hoffer2018norm}
Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry.
\newblock Norm matters: efficient and accurate normalization schemes in deep
  networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems 31}, pages 2160--2170. Curran Associates, Inc., 2018{\natexlab{a}}.

\bibitem[Hoffer et~al.(2018{\natexlab{b}})Hoffer, Hubara, and
  Soudry]{hoffer2018fix}
Elad Hoffer, Itay Hubara, and Daniel Soudry.
\newblock Fix your classifier: the marginal value of training the last weight
  layer.
\newblock In \emph{International Conference on Learning Representations},
  2018{\natexlab{b}}.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{Proceedings of the 32nd International Conference on
  International Conference on Machine Learning-Volume 37}, pages 448--456.
  JMLR. org, 2015.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018averaging}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and {Andrew
  Gordon} Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In Ricardo Silva, Amir Globerson, and Amir Globerson, editors,
  \emph{34th Conference on Uncertainty in Artificial Intelligence 2018, UAI
  2018}, 34th Conference on Uncertainty in Artificial Intelligence 2018, UAI
  2018, pages 876--885. Association For Uncertainty in Artificial Intelligence
  (AUAI), January 2018.
\newblock 34th Conference on Uncertainty in Artificial Intelligence 2018, UAI
  2018 ; Conference date: 06-08-2018 Through 10-08-2018.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018ntk}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems 31}, pages 8571--8580. Curran Associates, Inc., 2018.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Kohler et~al.(2019)Kohler, Daneshmand, Lucchi, Hofmann, Zhou, and
  Neymeyr]{kohler2019exp}
Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Thomas Hofmann, Ming Zhou, and
  Klaus Neymeyr.
\newblock Exponential convergence rates for batch normalization: The power of
  length-direction decoupling in non-convex optimization.
\newblock In Kamalika Chaudhuri and Masashi Sugiyama, editors,
  \emph{Proceedings of Machine Learning Research}, volume~89 of
  \emph{Proceedings of Machine Learning Research}, pages 806--815. PMLR, 16--18
  Apr 2019.

\bibitem[LeCun et~al.(2012)LeCun, Bottou, Orr, and
  M{\"u}ller]{LeCun2012efficient}
Yann~A. LeCun, L{\'e}on Bottou, Genevieve~B. Orr, and Klaus-Robert M{\"u}ller.
\newblock \emph{Efficient BackProp}, pages 9--48.
\newblock Springer Berlin Heidelberg, Berlin, Heidelberg, 2012.
\newblock ISBN 978-3-642-35289-8.
\newblock \doi{10.1007/978-3-642-35289-8_3}.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems 31}, pages 6389--6399. Curran Associates, Inc., 2018.

\bibitem[Li and Tai(2019)]{li2019stochastic}
Qianxiao Li and Cheng Tai.
\newblock Stochastic modified equations and dynamics of stochastic gradient
  algorithms i: Mathematical foundations.
\newblock \emph{J. Mach. Learn. Res.}, 20:\penalty0 40--1, 2019.

\bibitem[Li et~al.(2017)Li, Tai, et~al.]{li2017stochastic}
Qianxiao Li, Cheng Tai, et~al.
\newblock Stochastic modified equations and adaptive stochastic gradient
  algorithms.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 2101--2110. JMLR. org, 2017.

\bibitem[Li et~al.(2019)Li, Wei, and Ma]{li19learningrate}
Yuanzhi Li, Colin Wei, and Tengyu Ma.
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d'~Alch\'{e}-Buc,
  E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural Information
  Processing Systems 32}, pages 11674--11685. Curran Associates, Inc., 2019.

\bibitem[Li and Arora(2020)]{li2020exp}
Zhiyuan Li and Sanjeev Arora.
\newblock An exponential learning rate schedule for deep learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Salimans and Kingma(2016)]{salimans2016weight}
Tim Salimans and Diederik~P Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  901--909, 2016.

\bibitem[Santurkar et~al.(2018)Santurkar, Tsipras, Ilyas, and
  Madry]{santurkar2018does}
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry.
\newblock How does batch normalization help optimization?
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems 31}, pages 2483--2493. Curran Associates, Inc., 2018.

\bibitem[Shallue et~al.(2019)Shallue, Lee, Antognini, Sohl-Dickstein, Frostig,
  and Dahl]{shallue2019measuring}
Christopher~J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein,
  Roy Frostig, and George~E. Dahl.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (112):\penalty0 1--49, 2019.
\newblock URL \url{http://jmlr.org/papers/v20/18-789.html}.

\bibitem[Shi et~al.(2020)Shi, Su, and Jordan]{shi2020learning}
Bin Shi, Weijie~J Su, and Michael~I Jordan.
\newblock On learning rates and schr\"{o}dinger operators.
\newblock \emph{arXiv preprint arXiv:2004.06977}, 2020.

\bibitem[Smith and Le(2018)]{smith2018a}
Samuel~L. Smith and Quoc~V. Le.
\newblock A bayesian perspective on generalization and stochastic gradient
  descent.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Smith et~al.(2018)Smith, Kindermans, and Le]{smith2018dont}
Samuel~L. Smith, Pieter-Jan Kindermans, and Quoc~V. Le.
\newblock Don't decay the learning rate, increase the batch size.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Smith et~al.(2020)Smith, Elsen, and De]{smith2020generalization}
Samuel~L. Smith, Erich Elsen, and Soham De.
\newblock On the generalization benefit of noise in stochastic gradient
  descent, 2020.

\bibitem[van Laarhoven(2017)]{van2017l2}
Twan van Laarhoven.
\newblock L2 regularization versus batch and weight normalization.
\newblock \emph{arXiv preprint arXiv:1706.05350}, 2017.

\bibitem[Wu and He(2018)]{wu2018group}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock \emph{arXiv preprint arXiv:1803.08494}, 2018.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko16wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In Edwin R.~Hancock Richard C.~Wilson and William A.~P. Smith,
  editors, \emph{Proceedings of the British Machine Vision Conference (BMVC)},
  pages 87.1--87.12. BMVA Press, September 2016.
\newblock ISBN 1-901725-59-6.
\newblock \doi{10.5244/C.30.87}.

\bibitem[Zhang et~al.(2019)Zhang, Wang, Xu, and Grosse]{zhang2018three}
Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse.
\newblock Three mechanisms of weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\end{thebibliography}
