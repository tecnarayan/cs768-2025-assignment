\begin{thebibliography}{10}

\bibitem{bertinetto2018meta}
L.~Bertinetto, J.~F. Henriques, P.~Torr, and A.~Vedaldi.
\newblock Meta-learning with differentiable closed-form solvers.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{chen2021single}
T.~Chen, Y.~Sun, and W.~Yin.
\newblock A single-timescale stochastic bilevel optimization method.
\newblock {\em arXiv preprint arXiv:2102.04671}, 2021.

\bibitem{cutkosky2019momentum}
A.~Cutkosky and F.~Orabona.
\newblock Momentum-based variance reduction in non-convex {SGD}.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem{fang2018spider}
C.~Fang, C.~J. Li, Z.~Lin, and T.~Zhang.
\newblock Spider: Near-optimal non-convex optimization via stochastic path
  integrated differential estimator.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2018.

\bibitem{feurer2019hyperparameter}
M.~Feurer and F.~Hutter.
\newblock Hyperparameter optimization.
\newblock In {\em Automated Machine Learning}, pages 3--33. Springer, Cham,
  2019.

\bibitem{franceschi2017forward}
L.~Franceschi, M.~Donini, P.~Frasconi, and M.~Pontil.
\newblock Forward and reverse gradient-based hyperparameter optimization.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  1165--1173, 2017.

\bibitem{franceschi2018bilevel}
L.~Franceschi, P.~Frasconi, S.~Salzo, R.~Grazzi, and M.~Pontil.
\newblock Bilevel programming for hyperparameter optimization and
  meta-learning.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  1568--1577, 2018.

\bibitem{ghadimi2018approximation}
S.~Ghadimi and M.~Wang.
\newblock Approximation methods for bilevel programming.
\newblock {\em arXiv preprint arXiv:1802.02246}, 2018.

\bibitem{gould2016differentiating}
S.~Gould, B.~Fernando, A.~Cherian, P.~Anderson, R.~S. Cruz, and E.~Guo.
\newblock On differentiating parameterized argmin and argmax problems with
  application to bi-level optimization.
\newblock {\em arXiv preprint arXiv:1607.05447}, 2016.

\bibitem{grazzi2020iteration}
R.~Grazzi, L.~Franceschi, M.~Pontil, and S.~Salzo.
\newblock On the iteration complexity of hypergradient computation.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  3748--3758, 2020.

\bibitem{guo2021stochastic}
Z.~Guo, Y.~Xu, W.~Yin, R.~Jin, and T.~Yang.
\newblock On stochastic moving-average estimators for non-convex optimization.
\newblock {\em arXiv preprint arXiv:2104.14840}, 2021.

\bibitem{guo2021randomized}
Z.~Guo and T.~Yang.
\newblock Randomized stochastic variance-reduced methods for stochastic bilevel
  optimization.
\newblock {\em arXiv preprint arXiv:2105.02266}, 2021.

\bibitem{hansen1992new}
P.~Hansen, B.~Jaumard, and G.~Savard.
\newblock New branch-and-bound rules for linear bilevel programming.
\newblock {\em SIAM Journal on Scientific and Statistical Computing},
  13(5):1194--1217, 1992.

\bibitem{hong2020two}
M.~Hong, H.-T. Wai, Z.~Wang, and Z.~Yang.
\newblock A two-timescale framework for bilevel optimization: Complexity
  analysis and application to actor-critic.
\newblock {\em arXiv preprint arXiv:2007.05170}, 2020.

\bibitem{huang2020accelerated}
F.~Huang, S.~Gao, J.~Pei, and H.~Huang.
\newblock Accelerated zeroth-order momentum methods from mini to minimax
  optimization.
\newblock {\em arXiv preprint arXiv:2008.08170}, 2020.

\bibitem{ji2020convergence}
K.~Ji, J.~D. Lee, Y.~Liang, and H.~V. Poor.
\newblock Convergence of meta-learning with task-specific adaptation over
  partial parameters.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem{ji2021lower}
K.~Ji and Y.~Liang.
\newblock Lower bounds and accelerated algorithms for bilevel optimization.
\newblock {\em arXiv preprint arXiv:2102.03926}, 2021.

\bibitem{ji2019improved}
K.~Ji, Z.~Wang, Y.~Zhou, and Y.~Liang.
\newblock Improved zeroth-order variance reduced algorithms and analysis for
  nonconvex optimization.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  3100--3109, 2019.

\bibitem{ji2020multi}
K.~Ji, J.~Yang, and Y.~Liang.
\newblock Multi-step model-agnostic meta-learning: Convergence and improved
  algorithms.
\newblock {\em arXiv preprint arXiv:2002.07836}, 2020.

\bibitem{ji2021bilevel}
K.~Ji, J.~Yang, and Y.~Liang.
\newblock Bilevel optimization: Nonasymptotic analysis and faster algorithms.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2021.

\bibitem{johnson2013accelerating}
R.~Johnson and T.~Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  26:315--323, 2013.

\bibitem{khanduri2021momentum}
P.~Khanduri, S.~Zeng, M.~Hong, H.-T. Wai, Z.~Wang, and Z.~Yang.
\newblock A momentum-assisted single-timescale stochastic approximation
  algorithm for bilevel optimization.
\newblock {\em arXiv preprint arXiv:2102.07367v1}, 2021.

\bibitem{khanduri2021near}
P.~Khanduri, S.~Zeng, M.~Hong, H.-T. Wai, Z.~Wang, and Z.~Yang.
\newblock A near-optimal algorithm for stochastic bilevel optimization via
  double-momentum.
\newblock {\em arXiv preprint arXiv:2102.07367}, 2021.

\bibitem{konda2000actor}
V.~R. Konda and J.~N. Tsitsiklis.
\newblock Actor-critic algorithms.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 1008--1014, 2000.

\bibitem{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{li2018simple}
Z.~Li and J.~Li.
\newblock A simple proximal stochastic gradient method for nonsmooth nonconvex
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2018.

\bibitem{luo2020stochastic}
L.~Luo, H.~Ye, Z.~Huang, and T.~Zhang.
\newblock Stochastic recursive gradient descent ascent for stochastic
  nonconvex-strongly-concave minimax problems.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem{maclaurin2015gradient}
D.~Maclaurin, D.~Duvenaud, and R.~Adams.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  2113--2122, 2015.

\bibitem{moore2010bilevel}
G.~M. Moore.
\newblock {\em Bilevel programming algorithms for machine learning model
  selection}.
\newblock Rensselaer Polytechnic Institute, 2010.

\bibitem{nguyen2017sarah}
L.~M. Nguyen, J.~Liu, K.~Scheinberg, and M.~Tak{\'a}{\v{c}}.
\newblock {SARAH}: A novel method for machine learning problems using
  stochastic recursive gradient.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  2613--2621, 2017.

\bibitem{pedregosa2016hyperparameter}
F.~Pedregosa.
\newblock Hyperparameter optimization with approximate gradient.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  737--746, 2016.

\bibitem{rafique2021weakly}
H.~Rafique, M.~Liu, Q.~Lin, and T.~Yang.
\newblock Weakly-convex--concave min--max optimization: provable algorithms and
  applications in machine learning.
\newblock {\em Optimization Methods and Software}, pages 1--35, 2021.

\bibitem{rajeswaran2019meta}
A.~Rajeswaran, C.~Finn, S.~M. Kakade, and S.~Levine.
\newblock Meta-learning with implicit gradients.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 113--124, 2019.

\bibitem{shaban2019truncated}
A.~Shaban, C.-A. Cheng, N.~Hatch, and B.~Boots.
\newblock Truncated back-propagation for bilevel optimization.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, pages 1723--1732, 2019.

\bibitem{shi2005extended}
C.~Shi, J.~Lu, and G.~Zhang.
\newblock An extended kuhn-{Tucker} approach for linear bilevel programming.
\newblock {\em Applied Mathematics and Computation}, 162(1):51--63, 2005.

\bibitem{tran2019hybrid}
Q.~Tran-Dinh, N.~H. Pham, D.~T. Phan, and L.~M. Nguyen.
\newblock Hybrid stochastic gradient descent algorithms for stochastic
  nonconvex optimization.
\newblock {\em arXiv preprint arXiv:1905.05920}, 2019.

\bibitem{wang2018spiderboost}
Z.~Wang, K.~Ji, Y.~Zhou, Y.~Liang, and V.~Tarokh.
\newblock Spiderboost: A class of faster variance-reduced algorithms for
  nonconvex optimization.
\newblock {\em arXiv preprint arXiv:1810.10690}, 2018.

\bibitem{wang2019spiderboost}
Z.~Wang, K.~Ji, Y.~Zhou, Y.~Liang, and V.~Tarokh.
\newblock Spiderboost and momentum: Faster variance reduction algorithms.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem{xu2020gradient}
T.~Xu, Z.~Wang, Y.~Liang, and H.~V. Poor.
\newblock Gradient free minimax optimization: Variance reduction and faster
  convergence.
\newblock {\em arXiv preprint arXiv:2006.09361}, 2020.

\bibitem{yang2020global}
J.~Yang, N.~Kiyavash, and N.~He.
\newblock Global convergence and variance reduction for a class of
  nonconvex-nonconcave minimax problems.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  33, 2020.

\bibitem{zhou2018stochastic}
D.~Zhou, P.~Xu, and Q.~Gu.
\newblock Stochastic nested variance reduced gradient descent for nonconvex
  optimization.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2018.

\end{thebibliography}
