@article{hendrycks2018using,
  title={Using trusted data to train deep networks on labels corrupted by severe noise},
  author={Hendrycks, Dan and Mazeika, Mantas and Wilson, Duncan and Gimpel, Kevin},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{lovasz1986matching,
  title={Matching Theory, vol. 29},
  author={Lov{\'a}sz, L{\'a}szl{\'o} and Plummer, Michael D},
  journal={Annals of Discrete Mathematics, North-Holland, Amsterdam},
  year={1986}
}

@book{de2012algebraic,
  title={Algebraic and geometric ideas in the theory of discrete optimization},
  author={De Loera, Jes{\'u}s A and Hemmecke, Raymond and K{\"o}ppe, Matthias},
  year={2012},
  publisher={SIAM}
}

@book{schrijver1998theory,
  title={Theory of linear and integer programming},
  author={Schrijver, Alexander},
  year={1998},
  publisher={John Wiley \& Sons}
}

@article{griffin2007caltech,
  title={Caltech-256 object category dataset},
  author={Griffin, Gregory and Holub, Alex and Perona, Pietro},
  year={2007},
  publisher={California Institute of Technology}
}

@inproceedings{bossard14,
  title = {Food-101 -- Mining Discriminative Components with Random Forests},
  author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},
  booktitle = {European Conference on Computer Vision},
  year = {2014}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  journal={arXiv preprint arXiv:1503.02531},
  volume={2},
  number={7},
  year={2015}
}

@inproceedings{furlanello2018born,
  title={Born again neural networks},
  author={Furlanello, Tommaso and Lipton, Zachary and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
  booktitle={International Conference on Machine Learning},
  pages={1607--1616},
  year={2018},
  organization={PMLR}
}

@article{sun2019patient,
  title={Patient knowledge distillation for bert model compression},
  author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  journal={arXiv preprint arXiv:1908.09355},
  year={2019}
}

@article{chen2020big,
  title={Big self-supervised models are strong semi-supervised learners},
  author={Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={22243--22255},
  year={2020}
}

@inproceedings{sarfraz2021knowledge,
  title={Knowledge distillation beyond model compression},
  author={Sarfraz, Fahad and Arani, Elahe and Zonooz, Bahram},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)},
  pages={6136--6143},
  year={2021},
  organization={IEEE}
}

@article{li2021align,
  title={Align before fuse: Vision and language representation learning with momentum distillation},
  author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={9694--9705},
  year={2021}
}

@inproceedings{xie2020self,
  title={Self-training with noisy student improves imagenet classification},
  author={Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10687--10698},
  year={2020}
}

@inproceedings{ahn2019variational,
  title={Variational information distillation for knowledge transfer},
  author={Ahn, Sungsoo and Hu, Shell Xu and Damianou, Andreas and Lawrence, Neil D and Dai, Zhenwen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9163--9171},
  year={2019}
}

@article{mobahi2020self,
  title={Self-distillation amplifies regularization in hilbert space},
  author={Mobahi, Hossein and Farajtabar, Mehrdad and Bartlett, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3351--3361},
  year={2020}
}

@article{dong2019distillation,
  title={Distillation $\approx$ Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network},
  author={Dong, Bin and Hou, Jikai and Lu, Yiping and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1910.01255},
  year={2019}
}

@inproceedings{phuong2019towards,
  title={Towards understanding knowledge distillation},
  author={Phuong, Mary and Lampert, Christoph},
  booktitle={International Conference on Machine Learning},
  pages={5142--5151},
  year={2019},
  organization={PMLR}
}

@article{kaplun2022knowledge,
  title={Knowledge Distillation: Bad Models Can Be Good Role Models},
  author={Kaplun, Gal and Malach, Eran and Nakkiran, Preetum and Shalev-Shwartz, Shai},
  journal={arXiv preprint arXiv:2203.14649},
  year={2022}
}

@inproceedings{menon2021statistical,
  title={A statistical perspective on distillation},
  author={Menon, Aditya K and Rawat, Ankit Singh and Reddi, Sashank and Kim, Seungyeon and Kumar, Sanjiv},
  booktitle={International Conference on Machine Learning},
  pages={7632--7642},
  year={2021},
  organization={PMLR}
}

@article{gou2021knowledge,
  title={Knowledge distillation: A survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  volume={129},
  number={6},
  pages={1789--1819},
  year={2021},
  publisher={Springer}
}

@inproceedings{li2017learning,
  title={Learning from noisy labels with distillation},
  author={Li, Yuncheng and Yang, Jianchao and Song, Yale and Cao, Liangliang and Luo, Jiebo and Li, Li-Jia},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1910--1918},
  year={2017}
}

@article{lopez2015unifying,
  title={Unifying distillation and privileged information},
  author={Lopez-Paz, David and Bottou, L{\'e}on and Sch{\"o}lkopf, Bernhard and Vapnik, Vladimir},
  journal={arXiv preprint arXiv:1511.03643},
  year={2015}
}

@article{ji2020knowledge,
  title={Knowledge distillation in wide neural networks: Risk bound, data efficiency and imperfect teacher},
  author={Ji, Guangda and Zhu, Zhanxing},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20823--20833},
  year={2020}
}

@article{jiang2021equiangular,
  title={Equiangular lines with a fixed angle},
  author={Jiang, Zilin and Tidor, Jonathan and Yao, Yuan and Zhang, Shengtong and Zhao, Yufei},
  journal={Annals of Mathematics},
  volume={194},
  number={3},
  pages={729--743},
  year={2021},
  publisher={Department of Mathematics of Princeton University}
}

@article{kakade2008complexity,
  title={On the complexity of linear prediction: Risk bounds, margin bounds, and regularization},
  author={Kakade, Sham M and Sridharan, Karthik and Tewari, Ambuj},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}

@inproceedings{KrauseStarkDengFei-Fei_3DRR2013,
  title = {3D Object Representations for Fine-Grained Categorization},
  booktitle = {4th International IEEE Workshop on  3D Representation and Recognition (3dRR-13)},
  year = {2013},
  address = {Sydney, Australia},
  author = {Jonathan Krause and Michael Stark and Jia Deng and Li Fei-Fei}
}

@inproceedings{chen2019understanding,
  title={Understanding and utilizing deep neural networks trained with noisy labels},
  author={Chen, Pengfei and Liao, Ben Ben and Chen, Guangyong and Zhang, Shengyu},
  booktitle={International Conference on Machine Learning},
  pages={1062--1070},
  year={2019},
  organization={PMLR}
}

@inproceedings{beyer2022knowledge,
  title={Knowledge distillation: A good teacher is patient and consistent},
  author={Beyer, Lucas and Zhai, Xiaohua and Royer, Am{\'e}lie and Markeeva, Larisa and Anil, Rohan and Kolesnikov, Alexander},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10925--10934},
  year={2022}
}

@inproceedings{cheng2020explaining,
  title={Explaining knowledge distillation by quantifying the knowledge},
  author={Cheng, Xu and Rao, Zhefan and Chen, Yilan and Zhang, Quanshi},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12925--12935},
  year={2020}
}

@article{stanton2021does,
  title={Does knowledge distillation really work?},
  author={Stanton, Samuel and Izmailov, Pavel and Kirichenko, Polina and Alemi, Alexander A and Wilson, Andrew G},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={6906--6919},
  year={2021}
}

@article{pham2022revisiting,
  title={Revisiting self-distillation},
  author={Pham, Minh and Cho, Minsu and Joshi, Ameya and Hegde, Chinmay},
  journal={arXiv preprint arXiv:2206.08491},
  year={2022}
}

@inproceedings{nilsback2008automated,
  title={Automated flower classification over a large number of classes},
  author={Nilsback, Maria-Elena and Zisserman, Andrew},
  booktitle={2008 Sixth Indian Conference on Computer Vision, Graphics \& Image Processing},
  pages={722--729},
  year={2008},
  organization={IEEE}
}

@article{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}

@article{kumar2022fine,
  title={Fine-tuning can distort pretrained features and underperform out-of-distribution},
  author={Kumar, Ananya and Raghunathan, Aditi and Jones, Robbie and Ma, Tengyu and Liang, Percy},
  journal={arXiv preprint arXiv:2202.10054},
  year={2022}
}

@article{baykal2022robust,
  title={Robust Active Distillation},
  author={Baykal, Cenk and Trinh, Khoa and Iliopoulos, Fotis and Menghani, Gaurav and Vee, Erik},
  journal={arXiv preprint arXiv:2210.01213},
  year={2022}
}

@inproceedings{pham2021meta,
  title={Meta pseudo labels},
  author={Pham, Hieu and Dai, Zihang and Xie, Qizhe and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11557--11568},
  year={2021}
}

