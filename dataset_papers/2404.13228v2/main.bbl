\begin{thebibliography}{77}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anderson(1965)]{Anderson1965_iterativea}
Anderson, D.~G.
\newblock Iterative procedures for nonlinear integral equations.
\newblock \emph{Journal of the ACM}, 12\penalty0 (4):\penalty0 547--560, 1965.

\bibitem[Aubin \& Cellina(1984)Aubin and Cellina]{Jean-PierreArrigo1984_differential}
Aubin, J.-P. and Cellina, A.
\newblock \emph{Differential {{Inclusions}}}.
\newblock {Springer}, 1984.

\bibitem[Baillon \& Bruck(1992)Baillon and Bruck]{BaillonBruck1992_optimal}
Baillon, J.-B. and Bruck, R.~E.
\newblock Optimal rates of asymptotic regularity for averaged nonexpansive mappings.
\newblock \emph{Fixed Point Theory and Applications}, 128:\penalty0 27--66, 1992.

\bibitem[Banach(1922)]{Banach1922_operations}
Banach, S.
\newblock {Sur les op{\'e}rations dans les ensembles abstraits et leur application aux {\'e}quations int{\'e}grales}.
\newblock \emph{Fundamenta Mathematicae}, 3\penalty0 (1):\penalty0 133--181, 1922.

\bibitem[Bauschke \& Combettes(2017)Bauschke and Combettes]{bauschke2011convex}
Bauschke, H.~H. and Combettes, P.~L.
\newblock \emph{Convex Analysis and Monotone Operator Theory in Hilbert Spaces}.
\newblock Springer, second edition, 2017.

\bibitem[Borwein et~al.(1992)Borwein, Reich, and Shafrir]{BorweinReichShafrir1992_krasnoselskimann}
Borwein, J., Reich, S., and Shafrir, I.
\newblock Krasnoselski-{{Mann}} iterations in normed spaces.
\newblock \emph{Canadian Mathematical Bulletin}, 35\penalty0 (1):\penalty0 21--28, 1992.

\bibitem[Bo{\c t} et~al.(2023)Bo{\c t}, Csetnek, and Nguyen]{BotCsetnekNguyen2023_fast}
Bo{\c t}, R.~I., Csetnek, E.~R., and Nguyen, D.-K.
\newblock Fast optimistic gradient descent ascent ({{OGDA}}) method in continuous and discrete time.
\newblock \emph{Foundations of Computational Mathematics}, 2023.

\bibitem[Bravo \& Cominetti(2018)Bravo and Cominetti]{BravoCominetti2018_sharp}
Bravo, M. and Cominetti, R.
\newblock Sharp convergence rates for averaged nonexpansive maps.
\newblock \emph{Israel Journal of Mathematics}, 227:\penalty0 163--188, 2018.

\bibitem[Cai \& Zheng(2023)Cai and Zheng]{CaiZheng2023_accelerated}
Cai, Y. and Zheng, W.
\newblock Accelerated single-call methods for constrained min-max optimization.
\newblock \emph{International Conference on Learning Representations}, 2023.

\bibitem[Cai et~al.(2022)Cai, Oikonomou, and Zheng]{CaiOikonomouZheng2022_finitetime}
Cai, Y., Oikonomou, A., and Zheng, W.
\newblock Finite-time last-iterate convergence for learning in multi-player games.
\newblock \emph{Neural Information Processing Systems}, 2022.

\bibitem[Cominetti et~al.(2014)Cominetti, Soto, and Vaisman]{CominettiSotoVaisman2014_rate}
Cominetti, R., Soto, J.~A., and Vaisman, J.
\newblock On the rate of convergence of {{Krasnosel}}'ski{\u \i}-{{Mann}} iterations and their connection with sums of {{Bernoullis}}.
\newblock \emph{Israel Journal of Mathematics}, 199\penalty0 (2):\penalty0 757--772, 2014.

\bibitem[Csetnek et~al.(2019)Csetnek, Malitsky, and Tam]{CsetnekMalitskyTam2019_shadow}
Csetnek, E.~R., Malitsky, Y., and Tam, M.~K.
\newblock Shadow {{Douglas}}{\textendash}{{Rachford Splitting}} for {{Monotone Inclusions}}.
\newblock \emph{Applied Mathematics \& Optimization}, 80\penalty0 (3):\penalty0 665--678, 2019.

\bibitem[Das~Gupta et~al.(2023)Das~Gupta, Van~Parys, and Ryu]{DasGuptaVanParysRyu2023_branchandbound}
Das~Gupta, S., Van~Parys, B. P.~G., and Ryu, E.~K.
\newblock Branch-and-bound performance estimation programming: A unified methodology for constructing optimal optimization methods.
\newblock \emph{Mathematical Programming}, 2023.

\bibitem[Daskalakis et~al.(2018)Daskalakis, Ilyas, Syrgkanis, and Zeng]{DaskalakisIlyasSyrgkanisZeng2018_training}
Daskalakis, C., Ilyas, A., Syrgkanis, V., and Zeng, H.
\newblock Training {{GANs}} with optimism.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Davis \& Yin(2016)Davis and Yin]{DavisYin2016_convergence}
Davis, D. and Yin, W.
\newblock Convergence rate analysis of several splitting schemes.
\newblock In Glowinski, R., Osher, S.~J., and Yin, W. (eds.), \emph{Splitting {{Methods}} in {{Communication}}, {{Imaging}}, {{Science}} and {{Engineering}}}, Chapter 4, pp.\  115--163. {Springer}, 2016.

\bibitem[Diakonikolas(2020)]{Diakonikolas2020_halpern}
Diakonikolas, J.
\newblock Halpern iteration for near-optimal and parameter-free monotone inclusion and strong solutions to variational inequalities.
\newblock \emph{Conference on Learning Theory}, 2020.

\bibitem[Dong et~al.(2018)Dong, Yuan, Cho, and Rassias]{DongYuanChoRassias2018_modified}
Dong, {\relax QL}., Yuan, {\relax HB}., Cho, {\relax YJ}., and Rassias, T.~M.
\newblock Modified inertial {{Mann}} algorithm and inertial {{CQ-algorithm}} for nonexpansive mappings.
\newblock \emph{Optimization Letters}, 12\penalty0 (1):\penalty0 87--102, 2018.

\bibitem[Drori \& Taylor(2020)Drori and Taylor]{DroriTaylor2020_efficient}
Drori, Y. and Taylor, A.~B.
\newblock Efficient first-order methods for convex minimization: A constructive approach.
\newblock \emph{Mathematical Programming}, 184\penalty0 (1--2):\penalty0 183--220, 2020.

\bibitem[Drori \& Teboulle(2014)Drori and Teboulle]{DroriTeboulle2014_performance}
Drori, Y. and Teboulle, M.
\newblock Performance of first-order methods for smooth convex minimization: {{A}} novel approach.
\newblock \emph{Mathematical Programming}, 145\penalty0 (1):\penalty0 451--482, 2014.

\bibitem[Drori \& Teboulle(2016)Drori and Teboulle]{DroriTeboulle2016_optimal}
Drori, Y. and Teboulle, M.
\newblock An optimal variant of {{Kelley}}'s cutting-plane method.
\newblock \emph{Mathematical Programming}, 160\penalty0 (1--2):\penalty0 321--351, 2016.

\bibitem[Eckstein \& Bertsekas(1992)Eckstein and Bertsekas]{EcksteinBertsekas1992_Douglas}
Eckstein, J. and Bertsekas, D.~P.
\newblock On the {{Douglas}}{\textemdash}{{Rachford}} splitting method and the proximal point algorithm for maximal monotone operators.
\newblock \emph{Mathematical Programming}, 55\penalty0 (1):\penalty0 293--318, 1992.

\bibitem[Gorbunov et~al.(2022)Gorbunov, Loizou, and Gidel]{GorbunovLoizouGidel2022_extragradient}
Gorbunov, E., Loizou, N., and Gidel, G.
\newblock Extragradient method: {$O(1/K)$} last-iterate convergence for monotone variational inequalities and connections with cocoercivity.
\newblock \emph{International Conference on Artificial Intelligence and Statistics}, 2022.

\bibitem[Halpern(1967)]{Halpern1967_fixed}
Halpern, B.
\newblock Fixed points of nonexpanding maps.
\newblock \emph{Bulletin of the American Mathematical Society}, 73\penalty0 (6):\penalty0 957--961, 1967.

\bibitem[Ishikawa(1976)]{Ishikawa1976_fixed}
Ishikawa, S.
\newblock Fixed points and iteration of a nonexpansive mapping in a {{Banach}} space.
\newblock \emph{Proceedings of the American Mathematical Society}, 59\penalty0 (1):\penalty0 65--71, 1976.

\bibitem[Jang et~al.(2023)Jang, Gupta, and Ryu]{JangGuptaRyu2023_computerassisted}
Jang, U., Gupta, S.~D., and Ryu, E.~K.
\newblock Computer-assisted design of accelerated composite optimization methods: {{OptISTA}}.
\newblock \emph{arXiv:2305.15704}, 2023.

\bibitem[Kim(2021)]{Kim2021_accelerated}
Kim, D.
\newblock Accelerated proximal point method for maximally monotone operators.
\newblock \emph{Mathematical Programming}, 190\penalty0 (1{\textendash}2):\penalty0 57--87, 2021.

\bibitem[Kim \& Fessler(2016)Kim and Fessler]{KimFessler2016_optimized}
Kim, D. and Fessler, J.~A.
\newblock Optimized first-order methods for smooth convex minimization.
\newblock \emph{Mathematical Programming}, 159\penalty0 (1-2):\penalty0 81--107, 2016.

\bibitem[Kim \& Fessler(2021)Kim and Fessler]{KimFessler2021_optimizing}
Kim, D. and Fessler, J.~A.
\newblock Optimizing the efficiency of first-order methods for decreasing the gradient of smooth convex functions.
\newblock \emph{Journal of Optimization Theory and Applications}, 188\penalty0 (1):\penalty0 192--219, 2021.

\bibitem[Kim \& Yang(2023{\natexlab{a}})Kim and Yang]{KimYang2023_convergence}
Kim, J. and Yang, I.
\newblock Convergence analysis of {{ODE}} models for accelerated first-order methods via positive semidefinite kernels.
\newblock \emph{NeurIPS}, 2023{\natexlab{a}}.

\bibitem[Kim \& Yang(2023{\natexlab{b}})Kim and Yang]{KimYang2023_unifying}
Kim, J. and Yang, I.
\newblock Unifying {{Nesterov}}'s accelerated gradient methods for convex and strongly convex objective functions.
\newblock \emph{International Conference on Machine Learning}, 2023{\natexlab{b}}.

\bibitem[Kim et~al.(2023{\natexlab{a}})Kim, Ozdaglar, Park, and Ryu]{KimOzdaglarParkRyu2023_timereversed}
Kim, J., Ozdaglar, A.~E., Park, C., and Ryu, E.~K.
\newblock Time-reversed dissipation induces duality between minimizing gradient norm and function value.
\newblock \emph{Neural Information Processing Systems}, 2023{\natexlab{a}}.

\bibitem[Kim et~al.(2023{\natexlab{b}})Kim, Park, Ozdaglar, Diakonikolas, and Ryu]{KimParkOzdaglarDiakonikolasRyu2023_mirror}
Kim, J., Park, C., Ozdaglar, A., Diakonikolas, J., and Ryu, E.~K.
\newblock Mirror duality in convex optimization.
\newblock \emph{arXiv:2311.17296}, 2023{\natexlab{b}}.

\bibitem[Kohlenbach(2011)]{Kohlenbach2011_quantitative}
Kohlenbach, U.
\newblock On quantitative versions of theorems due to {{F}}. {{E}}. {{Browder}} and {{R}}. {{Wittmann}}.
\newblock \emph{Advances in Mathematics}, 226\penalty0 (3):\penalty0 2764--2795, 2011.

\bibitem[Korpelevich(1976)]{Korpelevich1976_extragradient}
Korpelevich, G.~M.
\newblock The extragradient method for finding saddle points and other problems.
\newblock \emph{Ekonomika i Matematicheskie Metody}, 12\penalty0 (4):\penalty0 747--756, 1976.

\bibitem[Krasnosel'skii(1955)]{Krasnoselskii1955_two}
Krasnosel'skii, M.~A.
\newblock Two remarks on the method of successive approximations.
\newblock \emph{Uspekhi Matematicheskikh Nauk}, 10\penalty0 (1):\penalty0 123--127, 1955.

\bibitem[Krichene et~al.(2015)Krichene, Bayen, and Bartlett]{KricheneBayenBartlett2015_accelerated}
Krichene, W., Bayen, A., and Bartlett, P.~L.
\newblock Accelerated mirror descent in continuous and discrete time.
\newblock \emph{Neural Information Processing Systems}, 2015.

\bibitem[Lee \& Kim(2021)Lee and Kim]{LeeKim2021_fast}
Lee, S. and Kim, D.
\newblock Fast extra gradient methods for smooth structured nonconvex-nonconcave minimax problems.
\newblock \emph{Neural Information Processing Systems}, 2021.

\bibitem[Leustean(2007)]{Leustean2007_rates}
Leustean, L.
\newblock Rates of asymptotic regularity for {{Halpern}} iterations of nonexpansive mappings.
\newblock \emph{Journal of Universal Computer Science}, 13\penalty0 (11):\penalty0 1680--1691, 2007.

\bibitem[Liang et~al.(2016)Liang, Fadili, and Peyr{\'e}]{LiangFadiliPeyre2016_convergence}
Liang, J., Fadili, J., and Peyr{\'e}, G.
\newblock Convergence rates with inexact non-expansive operators.
\newblock \emph{Mathematical Programming}, 159\penalty0 (1):\penalty0 403--434, 2016.

\bibitem[Lieder(2018)]{Lieder2018}
Lieder, F.
\newblock \emph{Projection Based Methods for Conic Linear Programming—Optimal First Order Complexities and Norm Constrained Quasi Newton Methods}.
\newblock Doctoral dissertation, Heinrich-Heine-Universität Düsseldorf, 2018.

\bibitem[Lieder(2021)]{Lieder2021_convergence}
Lieder, F.
\newblock On the convergence rate of the {{Halpern-iteration}}.
\newblock \emph{Optimization Letters}, 15\penalty0 (2):\penalty0 405--418, 2021.

\bibitem[Lu(2022)]{Lu2022_os}
Lu, H.
\newblock An {$O(s^r)$}-resolution {{ODE}} framework for understanding discrete-time algorithms and applications to the linear convergence of minimax problems.
\newblock \emph{Mathematical Programming}, 194\penalty0 (1):\penalty0 1061--1112, 2022.

\bibitem[Maing{\'e}(2008)]{Mainge2008_convergence}
Maing{\'e}, P.-E.
\newblock Convergence theorems for inertial {{KM-type}} algorithms.
\newblock \emph{Journal of Computational and Applied Mathematics}, 219\penalty0 (1):\penalty0 223--236, 2008.

\bibitem[Mann(1953)]{Mann1953_mean}
Mann, W.~R.
\newblock Mean value methods in iteration.
\newblock \emph{Proceedings of the American Mathematical Society}, 4\penalty0 (3):\penalty0 506--510, 1953.

\bibitem[Matsushita(2017)]{Matsushita2017_convergence}
Matsushita, S.-Y.
\newblock On the convergence rate of the {{Krasnosel}}'ski{\u \i}{\textendash}{{Mann}} iteration.
\newblock \emph{Bulletin of the Australian Mathematical Society}, 96\penalty0 (1):\penalty0 162--170, 2017.

\bibitem[Minty(1962)]{Minty1962_monotone}
Minty, G.~J.
\newblock Monotone (nonlinear) operators in {{Hilbert}} space.
\newblock \emph{Duke Mathematical Journal}, 29\penalty0 (3):\penalty0 341--346, 1962.

\bibitem[Mokhtari et~al.(2020)Mokhtari, Ozdaglar, and Pattathil]{MokhtariOzdaglarPattathil2020_unified}
Mokhtari, A., Ozdaglar, A., and Pattathil, S.
\newblock A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: {{Proximal}} point approach.
\newblock \emph{International Conference on Artificial Intelligence and Statistics}, 2020.

\bibitem[Nemirovski(2004)]{Nemirovski2004_proxmethod}
Nemirovski, A.
\newblock Prox-method with rate of convergence {$O(1/t)$} for variational inequalities with {{Lipschitz}} continuous monotone operators and smooth convex-concave saddle point problems.
\newblock \emph{SIAM Journal on Optimization}, 15\penalty0 (1):\penalty0 229--251, 2004.

\bibitem[Nesterov(1983)]{Nesterov1983_method}
Nesterov, Y.
\newblock A method of solving a convex programming problem with convergence rate {$O(1/k^2)$}.
\newblock \emph{Doklady Akademii Nauk SSSR}, 269\penalty0 (3):\penalty0 543--547, 1983.

\bibitem[Nesterov(2004)]{Nesterov2004_introductory}
Nesterov, Y.
\newblock \emph{Introductory {{Lectures}} on {{Convex Optimization}}: {{A Basic Course}}}.
\newblock {Springer}, 2004.

\bibitem[Nesterov(2007)]{Nesterov2007_dual}
Nesterov, Y.
\newblock Dual extrapolation and its applications to solving variational inequalities and related problems.
\newblock \emph{Mathematical Programming}, 109\penalty0 (2):\penalty0 319--344, 2007.

\bibitem[Ouyang \& Xu(2021)Ouyang and Xu]{OuyangXu2021_lower}
Ouyang, Y. and Xu, Y.
\newblock Lower complexity bounds of first-order methods for convex-concave bilinear saddle-point problems.
\newblock \emph{Mathematical Programming}, 185\penalty0 (1):\penalty0 1--35, 2021.

\bibitem[Park \& Ryu(2021)Park and Ryu]{ParkRyu2021_optimal}
Park, C. and Ryu, E.~K.
\newblock Optimal first-order algorithms as a function of inequalities.
\newblock \emph{arXiv:2110.11035}, 2021.

\bibitem[Park \& Ryu(2022)Park and Ryu]{ParkRyu2022_exact}
Park, J. and Ryu, E.~K.
\newblock Exact optimal accelerated complexity for fixed-point iterations.
\newblock \emph{International Conference on Machine Learning}, 2022.

\bibitem[Park \& Ryu(2023)Park and Ryu]{ParkRyu2023_accelerated}
Park, J. and Ryu, E.~K.
\newblock Accelerated infeasibility detection of constrained optimization and fixed-point iterations.
\newblock \emph{International Conference on Machine Learning}, 2023.

\bibitem[Popov(1980)]{Popov1980_modification}
Popov, L.~D.
\newblock {A modification of the Arrow--Hurwicz method for search of saddle points}.
\newblock \emph{Mathematical Notes of the Academy of Sciences of the USSR}, 28\penalty0 (5):\penalty0 845--848, 1980.

\bibitem[Rakhlin \& Sridharan(2013)Rakhlin and Sridharan]{RakhlinSridharan2013_online}
Rakhlin, A. and Sridharan, K.
\newblock Online learning with predictable sequences.
\newblock \emph{Conference on Learning Theory}, 2013.

\bibitem[Reich et~al.(2021)Reich, Thong, Cholamjiak, and Van~Long]{ReichThongCholamjiakVanLong2021_inertial}
Reich, S., Thong, D.~V., Cholamjiak, P., and Van~Long, L.
\newblock Inertial projection-type methods for solving pseudomonotone variational inequality problems in {{Hilbert}} space.
\newblock \emph{Numerical Algorithms}, 88\penalty0 (2):\penalty0 813--835, 2021.

\bibitem[Ryu \& Yin(2022)Ryu and Yin]{RyuYin2022_largescale}
Ryu, E.~K. and Yin, W.
\newblock \emph{Large-{{Scale Convex Optimization}} via {{Monotone Operators}}}.
\newblock {Cambridge University Press}, 2022.

\bibitem[Ryu et~al.(2019)Ryu, Yuan, and Yin]{RyuYuanYin2019_ode}
Ryu, E.~K., Yuan, K., and Yin, W.
\newblock {{ODE}} analysis of stochastic gradient methods with optimism and anchoring for minimax problems and {{GANs}}.
\newblock \emph{arXiv:1905.10899}, 2019.

\bibitem[Ryu et~al.(2020)Ryu, Taylor, Bergeling, and Giselsson]{RyuTaylorBergelingGiselsson2020_operator}
Ryu, E.~K., Taylor, A.~B., Bergeling, C., and Giselsson, P.
\newblock Operator splitting performance estimation: {{Tight}} contraction factors and optimal parameter selection.
\newblock \emph{SIAM Journal on Optimization}, 30\penalty0 (3):\penalty0 2251--2271, 2020.

\bibitem[Sabach \& Shtern(2017)Sabach and Shtern]{SabachShtern2017_first}
Sabach, S. and Shtern, S.
\newblock A first order method for solving convex bilevel optimization problems.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (2):\penalty0 640--660, 2017.

\bibitem[Shehu(2018)]{Shehu2018_convergence}
Shehu, Y.
\newblock Convergence rate analysis of inertial {{Krasnoselskii}}{\textendash}{{Mann}} type iteration with applications.
\newblock \emph{Numerical Functional Analysis and Optimization}, 39\penalty0 (10):\penalty0 1077--1091, 2018.

\bibitem[Solodov \& Svaiter(1999)Solodov and Svaiter]{SolodovSvaiter1999_hybrid}
Solodov, M.~V. and Svaiter, B.~F.
\newblock A hybrid approximate extragradient {\textendash} proximal point algorithm using the enlargement of a maximal monotone operator.
\newblock \emph{Set-Valued Analysis}, 7\penalty0 (4):\penalty0 323--345, 1999.

\bibitem[Su et~al.(2014)Su, Boyd, and Cand{\`e}s]{SuBoydCandes2014_differential}
Su, W., Boyd, S., and Cand{\`e}s, E.~J.
\newblock A differential equation for modeling {{Nesterov}}'s accelerated gradient method: {{Theory}} and insights.
\newblock \emph{Neural Information Processing Systems}, 2014.

\bibitem[Suh et~al.(2022)Suh, Roh, and Ryu]{SuhRohRyu2022_continuoustime}
Suh, J.~J., Roh, G., and Ryu, E.~K.
\newblock Continuous-time analysis of {{AGM}} via conservation laws in dilated coordinate systems.
\newblock \emph{International Conference on Machine Learning}, 2022.

\bibitem[Suh et~al.(2023)Suh, Park, and Ryu]{SuhParkRyu2023_continuoustime}
Suh, J.~J., Park, J., and Ryu, E.~K.
\newblock Continuous-time analysis of anchor acceleration.
\newblock \emph{Neural Information Processing Systems}, 2023.

\bibitem[Taylor \& Bach(2019)Taylor and Bach]{TaylorBach2019_stochastic}
Taylor, A. and Bach, F.
\newblock Stochastic first-order methods: Non-asymptotic and computer-aided analyses via potential functions.
\newblock \emph{Conference on Learning Theory}, 2019.

\bibitem[Taylor \& Drori(2022)Taylor and Drori]{TaylorDrori2022_optimal}
Taylor, A. and Drori, Y.
\newblock An optimal gradient method for smooth strongly convex minimization.
\newblock \emph{Mathematical Programming}, 2022.

\bibitem[Taylor et~al.(2017)Taylor, Hendrickx, and Glineur]{TaylorHendrickxGlineur2017_smooth}
Taylor, A.~B., Hendrickx, J.~M., and Glineur, F.
\newblock Smooth strongly convex interpolation and exact worst-case performance of first-order methods.
\newblock \emph{Mathematical Programming}, 161\penalty0 (1):\penalty0 307--345, 2017.

\bibitem[{Tran-Dinh} \& Luo(2021){Tran-Dinh} and Luo]{Tran-DinhLuo2021_halperntype}
{Tran-Dinh}, Q. and Luo, Y.
\newblock Halpern-type accelerated and splitting algorithms for monotone inclusions.
\newblock \emph{arXiv:2110.08150}, 2021.

\bibitem[Walker \& Ni(2011)Walker and Ni]{WalkerNi2011_andersona}
Walker, H.~F. and Ni, P.
\newblock Anderson acceleration for fixed-point iterations.
\newblock \emph{SIAM Journal on Numerical Analysis}, 49\penalty0 (4):\penalty0 1715--1735, 2011.

\bibitem[Wittmann(1992)]{Wittmann1992_approximation}
Wittmann, R.
\newblock Approximation of fixed points of nonexpansive mappings.
\newblock \emph{Archiv der Mathematik}, 58\penalty0 (5):\penalty0 486--491, 1992.

\bibitem[Xu(2002)]{Xu2002_iterative}
Xu, H.-K.
\newblock Iterative algorithms for nonlinear operators.
\newblock \emph{Journal of the London Mathematical Society}, 66\penalty0 (1):\penalty0 240--256, 2002.

\bibitem[Yoon \& Ryu(2021)Yoon and Ryu]{YoonRyu2021_accelerated}
Yoon, T. and Ryu, E.~K.
\newblock Accelerated algorithms for smooth convex-concave minimax problems with $\mathcal{O}(1/k^2)$ rate on squared gradient norm.
\newblock \emph{International Conference on Machine Learning}, 2021.

\bibitem[Yoon \& Ryu(2022)Yoon and Ryu]{YoonRyu2022_accelerated}
Yoon, T. and Ryu, E.~K.
\newblock Accelerated minimax algorithms flock together.
\newblock \emph{arXiv:2205.11093}, 2022.

\bibitem[Zhang et~al.(2020)Zhang, O'Donoghue, and Boyd]{ZhangODonoghueBoyd2020_globally}
Zhang, J., O'Donoghue, B., and Boyd, S.
\newblock Globally convergent type-{{I Anderson}} acceleration for nonsmooth fixed-point iterations.
\newblock \emph{SIAM Journal on Optimization}, 30\penalty0 (4):\penalty0 3170--3197, 2020.

\end{thebibliography}
