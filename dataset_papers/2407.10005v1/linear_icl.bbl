\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2024)Agarwal, Singh, Zhang, Bohnet, Chan, Anand, Abbas,
  Nova, Co-Reyes, Chu, et~al.]{agarwal2024many}
Rishabh Agarwal, Avi Singh, Lei~M Zhang, Bernd Bohnet, Stephanie Chan, Ankesh
  Anand, Zaheer Abbas, Azade Nova, John~D Co-Reyes, Eric Chu, et~al.
\newblock Many-shot in-context learning.
\newblock \emph{arXiv preprint arXiv:2404.11018}, 2024.

\bibitem[Ahn et~al.(2023)Ahn, Cheng, Daneshmand, and Sra]{ahn2024transformers}
Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra.
\newblock Transformers learn to implement preconditioned gradient descent for
  in-context learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Aky{\"u}rek et~al.(2023)Aky{\"u}rek, Schuurmans, Andreas, Ma, and
  Zhou]{akyrek2023what}
Ekin Aky{\"u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
\newblock What learning algorithm is in-context learning? investigations with
  linear models.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=0g0X4H8yN4I}.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  322--332. PMLR, 2019.

\bibitem[Bai et~al.(2024)Bai, Chen, Wang, Xiong, and Mei]{bai2024transformers}
Yu~Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei.
\newblock Transformers as statisticians: Provable in-context learning with
  in-context algorithm selection.
\newblock \emph{Advances in neural information processing systems}, 36, 2024.

\bibitem[Basu et~al.(2023)Basu, Rawat, and Zaheer]{basu2023statistical}
Soumya Basu, Ankit~Singh Rawat, and Manzil Zaheer.
\newblock A statistical perspective on retrieval-based models.
\newblock In \emph{International Conference on Machine Learning}, pages
  1852--1886. PMLR, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Canatar et~al.(2021)Canatar, Bordelon, and
  Pehlevan]{canatar2021spectral}
Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan.
\newblock Spectral bias and task-model alignment explain generalization in
  kernel regression and infinitely wide neural networks.
\newblock \emph{Nature communications}, 12\penalty0 (1):\penalty0 2914, 2021.

\bibitem[Cao et~al.(2019)Cao, Fang, Wu, Zhou, and Gu]{cao2019towards}
Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu.
\newblock Towards understanding the spectral bias of deep learning.
\newblock \emph{arXiv preprint arXiv:1912.01198}, 2019.

\bibitem[Collins et~al.(2024)Collins, Parulekar, Mokhtari, Sanghavi, and
  Shakkottai]{collins2024context}
Liam Collins, Advait Parulekar, Aryan Mokhtari, Sujay Sanghavi, and Sanjay
  Shakkottai.
\newblock In-context learning with transformers: Softmax attention adapts to
  function lipschitzness.
\newblock \emph{arXiv preprint arXiv:2402.11639}, 2024.

\bibitem[Dai et~al.(2023)Dai, Sun, Dong, Hao, Ma, Sui, and Wei]{dai2023gpt}
Damai Dai, Yutao Sun, Li~Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei.
\newblock Why can {GPT} learn in-context? language models secretly perform
  gradient descent as meta-optimizers.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors,
  \emph{Findings of the Association for Computational Linguistics: ACL 2023},
  pages 4005--4019, Toronto, Canada, July 2023. Association for Computational
  Linguistics.
\newblock \doi{10.18653/v1/2023.findings-acl.247}.
\newblock URL \url{https://aclanthology.org/2023.findings-acl.247}.

\bibitem[Dauphin et~al.(2017)Dauphin, Fan, Auli, and
  Grangier]{dauphin2017language}
Yann~N Dauphin, Angela Fan, Michael Auli, and David Grangier.
\newblock Language modeling with gated convolutional networks.
\newblock In \emph{International conference on machine learning}, pages
  933--941. PMLR, 2017.

\bibitem[Du et~al.(2023)Du, Balim, Oymak, and Ozay]{du2023can}
Zhe Du, Haldun Balim, Samet Oymak, and Necmiye Ozay.
\newblock Can transformers learn optimal filtering for unknown systems?
\newblock \emph{IEEE Control Systems Letters}, 7:\penalty0 3525--3530, 2023.

\bibitem[Duraisamy(2024)]{duraisamy2024finite}
Karthik Duraisamy.
\newblock Finite sample analysis and bounds of generalization error of gradient
  descent in in-context linear regression.
\newblock \emph{arXiv preprint arXiv:2405.02462}, 2024.

\bibitem[Fu et~al.(2023)Fu, Dao, Saab, Thomas, Rudra, and Re]{fu2023hungry}
Daniel~Y Fu, Tri Dao, Khaled~Kamal Saab, Armin~W Thomas, Atri Rudra, and
  Christopher Re.
\newblock Hungry hungry hippos: Towards language modeling with state space
  models.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=COZDy0WYGg}.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{garg2022can}
Shivam Garg, Dimitris Tsipras, Percy~S Liang, and Gregory Valiant.
\newblock What can transformers learn in-context? a case study of simple
  function classes.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 30583--30598, 2022.

\bibitem[Gatmiry et~al.()Gatmiry, Saunshi, Reddi, Jegelka, and
  Kumar]{gatmirycan}
Khashayar Gatmiry, Nikunj Saunshi, Sashank~J Reddi, Stefanie Jegelka, and
  Sanjiv Kumar.
\newblock Can looped transformers learn to implement multi-step gradient
  descent for in-context learning?
\newblock In \emph{Forty-first International Conference on Machine Learning}.

\bibitem[GeminiTeam et~al.(2023)GeminiTeam, Anil, Borgeaud, Wu, Alayrac, Yu,
  Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini}
GeminiTeam, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac,
  Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Grazzi et~al.(2024)Grazzi, Siems, Schrodi, Brox, and
  Hutter]{grazzi2024mamba}
Riccardo Grazzi, Julien Siems, Simon Schrodi, Thomas Brox, and Frank Hutter.
\newblock Is mamba capable of in-context learning?
\newblock \emph{arXiv preprint arXiv:2402.03170}, 2024.

\bibitem[Gu and Dao(2023)]{gu2023mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock \emph{arXiv preprint arXiv:2312.00752}, 2023.

\bibitem[Gu et~al.(2021{\natexlab{a}})Gu, Goel, and Re]{gu2021efficiently}
Albert Gu, Karan Goel, and Christopher Re.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.

\bibitem[Gu et~al.(2021{\natexlab{b}})Gu, Johnson, Goel, Saab, Dao, Rudra, and
  R{\'e}]{gu2021combining}
Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and
  Christopher R{\'e}.
\newblock Combining recurrent, convolutional, and continuous-time models with
  linear state space layers.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 572--585, 2021{\natexlab{b}}.

\bibitem[Han et~al.(2023)Han, Wang, Zhao, and Ji]{han2023context}
Chi Han, Ziqi Wang, Han Zhao, and Heng Ji.
\newblock In-context learning of large language models explained as kernel
  regression.
\newblock \emph{arXiv preprint arXiv:2305.12766}, 2023.

\bibitem[Hollmann et~al.(2022)Hollmann, M{\"u}ller, Eggensperger, and
  Hutter]{hollmann2022tabpfn}
Noah Hollmann, Samuel M{\"u}ller, Katharina Eggensperger, and Frank Hutter.
\newblock Tabpfn: A transformer that solves small tabular classification
  problems in a second.
\newblock \emph{arXiv preprint arXiv:2207.01848}, 2022.

\bibitem[Hu et~al.(2022)Hu, yelong shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen]{hu2022lora}
Edward~J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen.
\newblock Lo{RA}: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=nZeVKeeFYf9}.

\bibitem[Izacard et~al.(2023)Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick,
  Dwivedi-Yu, Joulin, Riedel, and Grave]{izacard2023atlas}
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni,
  Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard
  Grave.
\newblock Atlas: Few-shot learning with retrieval augmented language models.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0
  (251):\penalty0 1--43, 2023.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{katharopoulos2020transformers}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In \emph{International conference on machine learning}, pages
  5156--5165. PMLR, 2020.

\bibitem[Lee et~al.(2023)Lee, Jiang, and Berg-Kirkpatrick]{lee2023exploring}
Ivan Lee, Nan Jiang, and Taylor Berg-Kirkpatrick.
\newblock Exploring the relationship between model architecture and in-context
  learning ability.
\newblock \emph{arXiv preprint arXiv:2310.08049}, 2023.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal,
  K{\"u}ttler, Lewis, Yih, Rockt{\"a}schel, et~al.]{lewis2020retrieval}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
  Karpukhin, Naman Goyal, Heinrich K{\"u}ttler, Mike Lewis, Wen-tau Yih, Tim
  Rockt{\"a}schel, et~al.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 9459--9474, 2020.

\bibitem[Li et~al.(2020)Li, Soltanolkotabi, and Oymak]{li2020gradient}
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak.
\newblock Gradient descent with early stopping is provably robust to label
  noise for overparameterized neural networks.
\newblock In \emph{International conference on artificial intelligence and
  statistics}, pages 4313--4324. PMLR, 2020.

\bibitem[Li et~al.(2023)Li, Ildiz, Papailiopoulos, and
  Oymak]{li2023transformers}
Yingcong Li, Muhammed~Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak.
\newblock Transformers as algorithms: Generalization and stability in
  in-context learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  19565--19594. PMLR, 2023.

\bibitem[Li et~al.(2024)Li, Sreenivasan, Giannou, Papailiopoulos, and
  Oymak]{li2024dissecting}
Yingcong Li, Kartik Sreenivasan, Angeliki Giannou, Dimitris Papailiopoulos, and
  Samet Oymak.
\newblock Dissecting chain-of-thought: Compositionality through in-context
  filtering and learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Lin and Lee(2024)]{lin2024dual}
Ziqian Lin and Kangwook Lee.
\newblock Dual operating modes of in-context learning.
\newblock \emph{arXiv preprint arXiv:2402.18819}, 2024.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Ash, Goel, Krishnamurthy, and
  Zhang]{liu2023transformers}
Bingbin Liu, Jordan~T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang.
\newblock Transformers learn shortcuts to automata.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=De4FYqjFueZ}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Yuan, Fu, Jiang, Hayashi, and
  Neubig]{liu2023pre}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
  Graham Neubig.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting
  methods in natural language processing.
\newblock \emph{ACM Computing Surveys}, 55\penalty0 (9):\penalty0 1--35,
  2023{\natexlab{b}}.

\bibitem[Lu et~al.(2024)Lu, Letey, Zavatone-Veth, Maiti, and Pehlevan]{yuelu}
Yue Lu, Mary~I. Letey, Jacob~A. Zavatone-Veth, Anindita Maiti, and Cengiz
  Pehlevan.
\newblock Asymptotic theory of in-context learning by linear attention.
\newblock \emph{arXiv preprint arXiv:2310.08391}, 2024.

\bibitem[Mahankali et~al.(2024)Mahankali, Hashimoto, and Ma]{mahankali2024one}
Arvind~V. Mahankali, Tatsunori Hashimoto, and Tengyu Ma.
\newblock One step of gradient descent is provably the optimal in-context
  learner with one layer of linear self-attention.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=8p3fu56lKc}.

\bibitem[Mahdavi et~al.(2024)Mahdavi, Liao, and
  Thrampoulidis]{mahdavi2024revisiting}
Sadegh Mahdavi, Renjie Liao, and Christos Thrampoulidis.
\newblock Revisiting the equivalence of in-context learning and gradient
  descent: The impact of data distribution.
\newblock In \emph{ICASSP 2024-2024 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 7410--7414. IEEE, 2024.

\bibitem[M{\"u}ller et~al.(2021)M{\"u}ller, Hollmann, Arango, Grabocka, and
  Hutter]{muller2021transformers}
Samuel M{\"u}ller, Noah Hollmann, Sebastian~Pineda Arango, Josif Grabocka, and
  Frank Hutter.
\newblock Transformers can do bayesian inference.
\newblock \emph{arXiv preprint arXiv:2112.10510}, 2021.

\bibitem[M{\"u}ller et~al.(2023)M{\"u}ller, Feurer, Hollmann, and
  Hutter]{muller2023pfns4bo}
Samuel M{\"u}ller, Matthias Feurer, Noah Hollmann, and Frank Hutter.
\newblock Pfns4bo: In-context learning for bayesian optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  25444--25470. PMLR, 2023.

\bibitem[Nakano et~al.(2021)Nakano, Hilton, Balaji, Wu, Ouyang, Kim, Hesse,
  Jain, Kosaraju, Saunders, et~al.]{nakano2021webgpt}
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
  Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders,
  et~al.
\newblock Webgpt: Browser-assisted question-answering with human feedback.
\newblock \emph{arXiv preprint arXiv:2112.09332}, 2021.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan,
  Mann, Askell, Bai, Chen, et~al.]{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
  Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et~al.
\newblock In-context learning and induction heads.
\newblock \emph{arXiv preprint arXiv:2209.11895}, 2022.

\bibitem[OpenAI(2023)]{gpt4_techreport}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprintarXiv:2303.08774}, 2023.

\bibitem[Park et~al.(2024)Park, Park, Xiong, Lee, Cho, Oymak, Lee, and
  Papailiopoulos]{park2024can}
Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet
  Oymak, Kangwook Lee, and Dimitris Papailiopoulos.
\newblock Can mamba learn how to learn? a comparative study on in-context
  learning tasks.
\newblock \emph{International Conference on Machine Learning}, 2024.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  et~al.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem[Schlag et~al.(2021)Schlag, Irie, and Schmidhuber]{schlag2021linear}
Imanol Schlag, Kazuki Irie, and J{\"u}rgen Schmidhuber.
\newblock Linear transformers are secretly fast weight programmers.
\newblock In \emph{International Conference on Machine Learning}, pages
  9355--9366. PMLR, 2021.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023_llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Von~Oswald et~al.(2023)Von~Oswald, Niklasson, Randazzo, Sacramento,
  Mordvintsev, Zhmoginov, and Vladymyrov]{von2023transformers}
Johannes Von~Oswald, Eyvind Niklasson, Ettore Randazzo, Jo{\~a}o Sacramento,
  Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  35151--35174. PMLR, 2023.

\bibitem[von Oswald et~al.(2023)von Oswald, Niklasson, Schlegel, Kobayashi,
  Zucchet, Scherrer, Miller, Sandler, Vladymyrov, Pascanu,
  et~al.]{von2023uncovering}
Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi,
  Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov,
  Razvan Pascanu, et~al.
\newblock Uncovering mesa-optimization algorithms in transformers.
\newblock \emph{arXiv preprint arXiv:2309.05858}, 2023.

\bibitem[Wang et~al.(2022)Wang, Xu, Fang, Liu, Sun, Xu, Zhu, and
  Zeng]{wang2022training}
Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu,
  Chenguang Zhu, and Michael Zeng.
\newblock Training data is more valuable than you think: A simple and effective
  method by retrieving from training data.
\newblock \emph{arXiv preprint arXiv:2203.08773}, 2022.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou,
  et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V
  Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock \emph{Advances in neural information processing systems},
  35:\penalty0 24824--24837, 2022.

\bibitem[Wu et~al.(2023)Wu, Zou, Chen, Braverman, Gu, and Bartlett]{wu2023many}
Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and
  Peter~L Bartlett.
\newblock How many pretraining tasks are needed for in-context learning of
  linear regression?
\newblock \emph{arXiv preprint arXiv:2310.08391}, 2023.

\bibitem[Xie et~al.(2022)Xie, Raghunathan, Liang, and Ma]{xie2022an}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=RdJVFCHjUMI}.

\bibitem[Zhang et~al.(2023)Zhang, Frei, and Bartlett]{zhang2023trained}
Ruiqi Zhang, Spencer Frei, and Peter~L Bartlett.
\newblock Trained transformers learn linear models in-context.
\newblock \emph{arXiv preprint arXiv:2306.09927}, 2023.

\bibitem[Zhang et~al.(2024)Zhang, Frei, and Bartlett]{zhang2024trained}
Ruiqi Zhang, Spencer Frei, and Peter~L Bartlett.
\newblock Trained transformers learn linear models in-context.
\newblock \emph{Journal of Machine Learning Research}, 25\penalty0
  (49):\penalty0 1--55, 2024.

\bibitem[Zucchet et~al.(2023)Zucchet, Kobayashi, Akram, Von~Oswald, Larcher,
  Steger, and Sacramento]{zucchet2023gated}
Nicolas Zucchet, Seijin Kobayashi, Yassir Akram, Johannes Von~Oswald, Maxime
  Larcher, Angelika Steger, and Joao Sacramento.
\newblock Gated recurrent neural networks discover attention.
\newblock \emph{arXiv preprint arXiv:2309.01775}, 2023.

\end{thebibliography}
