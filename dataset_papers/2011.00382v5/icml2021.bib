@string{aaai = "Association for the Advancement of Artificial Intelligence (AAAI)"}
@string{nips = "Neural Information Processing Systems (NeurIPS)"}
@string{iclr = "International Conference on Learning Representations (ICLR)"}
@string{icml = "International Conference on Machine Learning (ICML)"}
@string{aamas = "International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)"}

@book{thrun98learning_to_learn,
editor = {Thrun, Sebastian and Pratt, Lorien},
title = {Learning to Learn},
year = {1998},
isbn = {0792380479},
publisher = {Kluwer Academic Publishers},
address = {USA}
}

@mastersthesis{schmidhuber87meta_learning,
  added-at = {2008-06-19T17:46:40.000+0200},
  author = {Schmidhuber, Jurgen},
  biburl = {https://www.bibsonomy.org/bibtex/2a96f7c3d42103ab94b13badef5d869f0/brazovayeye},
  interhash = {b1d12416bd2edc34c30961f0ae978d8f},
  intrahash = {a96f7c3d42103ab94b13badef5d869f0},
  keywords = {EURISKO, PSALM, SALM, algorithm, algorithms, associative brigade, bucket evolution, fractals genetic genetical introsepection, learning, meta, nets, neuronal programming self-reference,},
  month = {14 May},
  school = {Technische Universitat Munchen, Germany},
  size = {62 pages},
  timestamp = {2008-06-19T17:51:06.000+0200},
  title = {Evolutionary Principles in Self-Referential Learning.
                 On Learning now to Learn: The Meta-Meta-Meta...-Hook},
  type = {Diploma Thesis},
  url = {http://www.idsia.ch/~juergen/diploma.html},
  year = 1987
}

@inproceedings{schulmanetal-16-gae,
title = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
author = {John Schulman and Philipp Moritz and Sergey Levine and Michael Jordan and Pieter Abbeel},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
year  = 2016
}

@INPROCEEDINGS{chan19assistive,
  author={L. {Chan} and D. {Hadfield-Menell} and S. {Srinivasa} and A. {Dragan}},
  booktitle={2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
  title={The Assistive Multi-Armed Bandit}, 
  year={2019},
  volume={},
  number={},
  pages={354-363},
  doi={10.1109/HRI.2019.8673234}}

@article{wang16acer,
  author    = {Ziyu Wang and
               Victor Bapst and
               Nicolas Heess and
               Volodymyr Mnih and
               R{\'{e}}mi Munos and
               Koray Kavukcuoglu and
               Nando de Freitas},
  title     = {Sample Efficient Actor-Critic with Experience Replay},
  journal   = {CoRR},
  volume    = {abs/1611.01224},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.01224},
  archivePrefix = {arXiv},
  eprint    = {1611.01224},
  timestamp = {Mon, 13 Aug 2018 16:48:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/WangBHMMKF16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{yu20pcgrad,
  title={Gradient surgery for multi-task learning},
  author={Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},
  journal={arXiv preprint arXiv:2001.06782},
  year={2020}
}

@InProceedings{fallah20_metatheory, title = {On the Convergence Theory of Gradient-Based Model-Agnostic Meta-Learning Algorithms}, author = {Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman}, pages = {1082--1092}, year = {2020}, editor = {Silvia Chiappa and Roberto Calandra}, volume = {108}, series = {Proceedings of Machine Learning Research}, address = {Online}, month = {26--28 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v108/fallah20a/fallah20a.pdf}, url = {http://proceedings.mlr.press/v108/fallah20a.html}, abstract = {We study the convergence of a class of gradient-based Model-Agnostic Meta-Learning (MAML) methods and characterize their overall complexity as well as their best achievable accuracy in terms of gradient norm for nonconvex loss functions. We start with the MAML method and its first-order approximation (FO-MAML) and highlight the challenges that emerge in their analysis. By overcoming these challenges not only we provide the first theoretical guarantees for MAML and FO-MAML in nonconvex settings, but also we answer some of the unanswered questions for the implementation of these algorithms including how to choose their learning rate and the batch size for both tasks and datasets corresponding to tasks. In particular, we show that MAML can find an ?-first-order stationary point ( ?-FOSP) for any positive ? after at most O(1/?^2) iterations at the expense of requiring second-order information. We also show that FO-MAML which ignores the second-order information required in the update of MAML cannot achieve any small desired level of accuracy, i.e., FO-MAML cannot find an ?-FOSP for any ?>0. We further propose a new variant of the MAML algorithm called Hessian-free MAML which preserves all theoretical guarantees of MAML, without requiring access to second-order information.} }

@article{dewitt2020deep_multimujoco,
  title={Deep Multi-Agent Reinforcement Learning for Decentralized Continuous Cooperative Control}, 
  author={Christian Schroeder de Witt and Bei Peng and Pierre-Alexandre Kamienny and Philip Torr and Wendelin Böhmer and Shimon Whiteson},
  journal={arXiv preprint arXiv:2003.06709},
  year={2020},
}

@misc{prajapat2020competitive,
      title={Competitive Policy Optimization}, 
      author={Manish Prajapat and Kamyar Azizzadenesheli and Alexander Liniger and Yisong Yue and Anima Anandkumar},
      year={2020},
      eprint={2006.10611},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{duan-16-linearbaseline, 
title = {Benchmarking Deep Reinforcement Learning for Continuous Control}, 
author = {Yan Duan and Xi Chen and Rein Houthooft and John Schulman and Pieter Abbeel}, 
pages = {1329--1338}, year = {2016}, editor = {Maria Florina Balcan and Kilian Q. Weinberger}, volume = {48}, series = {Proceedings of Machine Learning Research}, address = {New York, New York, USA}, month = {20--22 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v48/duan16.pdf}, url = {http://proceedings.mlr.press/v48/duan16.html}, 
abstract = {Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers.} 
}

@inproceedings{foerster17lola,
author = {Foerster, Jakob and Chen, Richard Y. and Al-Shedivat, Maruan and Whiteson, Shimon and Abbeel, Pieter and Mordatch, Igor},
title = {Learning with Opponent-Learning Awareness},
year = {2018},
booktitle = aamas,
pages = {122–130},
numpages = {9},
}

@InProceedings{silver14dpg,
  title={Deterministic Policy Gradient Algorithms},
  author={David Silver and Guy Lever and Nicolas Heess and Thomas Degris and Daan Wierstra and Martin Riedmiller},
  booktitle={International Conference on Machine Learning},
  pages={387--395},
  year={2014}
}

@InProceedings{fujimoto18td3,
title={Addressing Function Approximation Error in Actor-Critic Methods},
author={Fujimoto, Scott and van Hoof, Herke and Meger, David},
booktitle={International Conference on Machine Learning (ICML)},
year={2018}
}

@inproceedings{littman94markov,
 author = {Littman, Michael L.},
 title = {Markov Games As a Framework for Multi-agent Reinforcement Learning},
 booktitle = {Proceedings of the Eleventh International Conference on International Conference on Machine Learning},
 series = {ICML'94},
 year = {1994},
 isbn = {1-55860-335-2},
 location = {New Brunswick, NJ, USA},
 pages = {157--163},
 numpages = {7},
 url = {http://dl.acm.org/citation.cfm?id=3091574.3091594},
 acmid = {3091594},
 publisher = {Morgan Kaufmann Publishers Inc.},
} 

@article{liu17memory,
  author    = {Ruishan Liu and
              James Zou},
  title     = {The Effects of Memory Replay in Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1710.06574},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.06574},
  archivePrefix = {arXiv},
  eprint    = {1710.06574},
  timestamp = {Mon, 13 Aug 2018 16:46:47 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-06574},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hindsight2018,
  author    = {Andrew Levy and
              Robert Platt Jr. and
              Kate Saenko},
  title     = {Hierarchical Reinforcement Learning with Hindsight},
  journal   = {CoRR},
  volume    = {abs/1805.08180},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.08180},
  archivePrefix = {arXiv},
  eprint    = {1805.08180},
  timestamp = {Mon, 13 Aug 2018 16:46:55 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1805-08180},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{OliehoekAmato16book,
    author ={Frans A. Oliehoek and Christopher Amato},
    title =         {A Concise Introduction to Decentralized POMDPs},
    year =          2016,
    month =         may,
    url =           {http://www.fransoliehoek.net/docs/OliehoekAmato16book.pdf},
    publisher =     {Springer},
    OPTurl =           {http://www.springer.com/us/book/9783319289274},
    wwwnote =          {Authors' pre-print. Final version availabe at <a href="http://www.springer.com/us/book/9783319289274">Springer</a>.},
    keywords =   {nonrefereed, book},
    doi =           {10.1007/978-3-319-28929-8}
}

@unknown{vrancx2011,
author = {Vrancx, Peter and De Hauwere, Yann-Michaël and Nowe, Ann},
year = {2011},
month = {01},
pages = {263-272},
title = {Transfer Learning for Multi-agent Coordination.},
volume = {2},
journal = {ICAART 2011 - Proceedings of the 3rd International Conference on Agents and Artificial Intelligence}
}

@inproceedings{Garant2015AcceleratingMR,
  title={Accelerating Multi-agent Reinforcement Learning with Dynamic Co-learning},
  author={Dan Garant and Bruno Castro da Silva and Victor R. Lesser},
  year={2015}
}

@inproceedings{Taylor2013TransferLI,
  title={Transfer Learning in Multi-Agent Systems Through Parallel Transfer},
  author={Adam Taylor and E. Galvan-Lopez},
  year={2013}
}

@article{xu18meta,
  author    = {Tianbing Xu and
              Qiang Liu and
              Liang Zhao and
              Jian Peng},
  title     = {Learning to Explore with Meta-Policy Gradient},
  journal   = {CoRR},
  volume    = {abs/1803.05044},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.05044},
  archivePrefix = {arXiv},
  eprint    = {1803.05044},
  timestamp = {Mon, 13 Aug 2018 16:48:00 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-05044},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{GuHLL16,
  author    = {Shixiang Gu and
              Ethan Holly and
              Timothy P. Lillicrap and
              Sergey Levine},
  title     = {Deep Reinforcement Learning for Robotic Manipulation},
  journal   = {CoRR},
  volume    = {abs/1610.00633},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.00633},
  archivePrefix = {arXiv},
  eprint    = {1610.00633},
  timestamp = {Mon, 13 Aug 2018 16:49:14 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GuHLL16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{LillicrapHPHETS15,
  author    = {Timothy P. Lillicrap and
              Jonathan J. Hunt and
              Alexander Pritzel and
              Nicolas Heess and
              Tom Erez and
              Yuval Tassa and
              David Silver and
              Daan Wierstra},
  title     = {Continuous control with deep reinforcement learning},
  journal   = {CoRR},
  volume    = {abs/1509.02971},
  year      = {2015},
  url       = {http://arxiv.org/abs/1509.02971},
  archivePrefix = {arXiv},
  eprint    = {1509.02971},
  timestamp = {Mon, 13 Aug 2018 16:46:11 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LillicrapHPHETS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{SilvaGC17,
  author    = {Felipe Leno da Silva and
              Ruben Glatt and
              Anna Helena Reali Costa},
  title     = {Simultaneously Learning and Advising in Multiagent Reinforcement Learning},
  booktitle = {International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
  pages     = {1100--1108},
  year      = {2017},
}


@article{taylorandnich,
author = {Matthew E. Taylor and Nicholas Carboni and Anestis Fachantidis and Ioannis Vlahavas and Lisa Torrey},
title = {Reinforcement learning agents providing advice in complex video games},
journal = {Connection Science},
volume = {26},
number = {1},
pages = {45-63},
year  = {2014},
publisher = {Taylor & Francis},
doi = {10.1080/09540091.2014.885279},

URL = { 
        https://doi.org/10.1080/09540091.2014.885279
    
},
eprint = { 
        https://doi.org/10.1080/09540091.2014.885279
    
}

}

@article{FachantidisTV17,
  author    = {Anestis Fachantidis and
              Matthew E. Taylor and
              Ioannis P. Vlahavas},
  title     = {Learning to Teach Reinforcement Learning Agents},
  journal   = {CoRR},
  volume    = {abs/1707.09079},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.09079},
  archivePrefix = {arXiv},
  eprint    = {1707.09079},
  timestamp = {Mon, 13 Aug 2018 16:47:30 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/FachantidisTV17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{omidshafiei18teach, 
title={Learning to Teach in Cooperative Multiagent Reinforcement Learning}, 
volume={33}, 
url={https://aaai.org/ojs/index.php/AAAI/article/view/4570}, 
DOI={10.1609/aaai.v33i01.33016128}, 
number={01}, 
journal={Association for the Advancement of Artificial Intelligence (AAAI)},
author={Omidshafiei, Shayegan and Kim, Dong-Ki and Liu, Miao and Tesauro, Gerald and Riemer, Matthew and Amato, Christopher and Campbell, Murray and How, Jonathan P.}, 
year={2019}, 
month={Jul.}, 
pages={6128-6136}
}

@article{lillicrap15ddpg,
  author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
  title={Continuous control with deep reinforcement learning},
  journal={CoRR},
  volume={abs/1509.02971},
  year={2015},
  url={http://arxiv.org/abs/1509.02971},
  archivePrefix={arXiv},
  eprint={1509.02971},
}

@article{nachum18hrl,
  author={Ofir Nachum and Shixiang Gu and Honglak Lee and Sergey Levine},
  title={Data-Efficient Hierarchical Reinforcement Learning},
  journal={CoRR},
  volume={abs/1805.08296},
  year={2018},
  url={http://arxiv.org/abs/1805.08296},
  archivePrefix={arXiv},
  eprint={1805.08296},
}

@article{mnih15dqn,
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  journal={Nature},
  pages={529--533},
  title={Human-level control through deep reinforcement learning},
  year={2015}
}

@InProceedings{daswani15imitation,
title={Reinforcement learning with value advice},
author={Mayank Daswani and Peter Sunehag and Marcus Hutter},
booktitle={Asian Conference on Machine Learning (ACML)},
pages={299--314},
year={2015},
volume={39},
month={26--28 Nov},
pdf={http://proceedings.mlr.press/v39/daswani14.pdf},
url={http://proceedings.mlr.press/v39/daswani14.html},
}

@article{graves17curriculum,
  author    = {Alex Graves and
              Marc G. Bellemare and
              Jacob Menick and
              R{\'{e}}mi Munos and
              Koray Kavukcuoglu},
  title     = {Automated Curriculum Learning for Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1704.03003},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.03003},
  archivePrefix = {arXiv},
  eprint    = {1704.03003},
  timestamp = {Mon, 13 Aug 2018 16:46:00 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GravesBMMK17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sukhbaatar16learningcommm,
  title={Learning multiagent communication with backpropagation},
  author={Sukhbaatar, Sainbayar and Fergus, Rob and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2244--2252},
  year={2016}
}

@inproceedings{foerster16learningcomm,
  title={Learning to communicate with deep multi-agent reinforcement learning},
  author={Foerster, Jakob and Assael, Ioannis Alexandros and de Freitas, Nando and Whiteson, Shimon},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2137--2145},
  year={2016}
}

@article{SukhbaatarSF16,
  author    = {Sainbayar Sukhbaatar and
              Arthur Szlam and
              Rob Fergus},
  title     = {Learning Multiagent Communication with Backpropagation},
  journal   = {CoRR},
  volume    = {abs/1605.07736},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.07736},
  archivePrefix = {arXiv},
  eprint    = {1605.07736},
  timestamp = {Mon, 13 Aug 2018 16:47:28 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SukhbaatarSF16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ross14imitation,
  author    = {St{\'{e}}phane Ross and
              J. Andrew Bagnell},
  title     = {Reinforcement and Imitation Learning via Interactive No-Regret Learning},
  journal   = {CoRR},
  volume    = {abs/1406.5979},
  year      = {2014},
  url       = {http://arxiv.org/abs/1406.5979},
  archivePrefix = {arXiv},
  eprint    = {1406.5979},
  timestamp = {Mon, 13 Aug 2018 16:47:16 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/RossB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bacon16hrl,
author={Pierre{-}Luc Bacon and Jean Harb and Doina Precup},
title={The Option-Critic Architecture},
booktitle={Association for the Advancement of Artificial Intelligence (AAAI)},
pages={1726--1734},
year={2017},
url={http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14858},
timestamp={Mon, 06 Mar 2017 08:17:31 +0100},
}

@inproceedings{lowe17maddpg,
  title={Multi-agent actor-critic for mixed cooperative-competitive environments},
  author={Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, OpenAI Pieter and Mordatch, Igor},
  booktitle=nips,
  pages={6382--6393},
  year={2017}
}

@book{rogers2010diffusion,
  title={Diffusion of innovations},
  author={Rogers, Everett M},
  year={2010},
  publisher={Simon and Schuster}
}

@inproceedings{Kulkarni16hrl,
 author={Kulkarni, Tejas D. and Narasimhan, Karthik R. and Saeedi, Ardavan and Tenenbaum, Joshua B.},
 title={Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation},
 booktitle={Neural Information Processing Systems (NIPS)},
 year={2016},
 isbn={978-1-5108-3881-9},
 location={Barcelona, Spain},
 pages={3682--3690},
 numpages={9},
 url={http://dl.acm.org/citation.cfm?id=3157382.3157509},
 acmid={3157509},
} 

@inproceedings{foerster2017counterfactual,
  title = "Counterfactual Multi-Agent Policy Gradients",
  author = "Jakob Foerster and Gregory Farquhar and Triantafyllos Afouras and Nantas Nardelli and Shimon Whiteson",
  year = "2018",
  booktitle = aaai,
  month = "February",
  url = "http://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/foersteraaai18.pdf",
}

@article{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}

@inproceedings{omidshafiei2017deep,
  title={Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability},
  author={Omidshafiei, Shayegan and Pazis, Jason and Amato, Christopher and How, Jonathan P and Vian, John},
  booktitle=icml,
  pages={2681--2690},
  year={2017}
}

@InProceedings{vezhnevets17hrl,
title = 	 {{F}e{U}dal Networks for Hierarchical Reinforcement Learning},
author = 	 {Alexander Sasha Vezhnevets and Simon Osindero and Tom Schaul and Nicolas Heess and Max Jaderberg and David Silver and Koray Kavukcuoglu},
booktitle = 	 {International Conference on Machine Learning (ICML)},
year = 	 {2017},
pdf = 	 {http://proceedings.mlr.press/v70/vezhnevets17a/vezhnevets17a.pdf},
url = 	 {http://proceedings.mlr.press/v70/vezhnevets17a.html},
}

@Article{Lin1992,
author="Lin, Long-Ji",
title="Self-improving reactive agents based on reinforcement learning, planning and teaching",
journal="Machine Learning",
year="1992",
month="May",
day="01",
volume="8",
number="3",
pages="293--321",
abstract="To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.",
issn="1573-0565",
doi="10.1007/BF00992699",
url="https://doi.org/10.1007/BF00992699"
}

@Article{Williams1992,
author="Williams, Ronald J.",
title="Simple statistical gradient-following algorithms for connectionist reinforcement learning",
journal="Machine Learning",
year="1992",
month="May",
day="01",
volume="8",
number="3",
pages="229--256",
abstract="This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.",
issn="1573-0565",
doi="10.1007/BF00992696",
url="https://doi.org/10.1007/BF00992696"
}

@article{ROCKOFF2011687,
title = "Subjective and objective evaluations of teacher effectiveness: Evidence from New York City",
journal = "Labour Economics",
volume = "18",
number = "5",
pages = "687 - 696",
year = "2011",
issn = "0927-5371",
doi = "https://doi.org/10.1016/j.labeco.2011.02.004",
url = "http://www.sciencedirect.com/science/article/pii/S0927537111000315",
author = "Jonah E. Rockoff and Cecilia Speroni",
keywords = "Teachers, Employee evaluation"
}

@book{Sutton:1998,
 author = {Sutton, Richard S. and Barto, Andrew G.},
 title = {Introduction to Reinforcement Learning},
 year = {1998},
 isbn = {0262193981},
 edition = {1st},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@inproceedings{continousDRL16,
  title={Continuous control with deep reinforcement learning},
  author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
  booktitle={International Conference on Learning Representation (ICLR)},
  pages={},
  year={2016}
}

@inproceedings{Duan2016,
 author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
 title = {Benchmarking Deep Reinforcement Learning for Continuous Control},
 booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
 series = {ICML'16},
 year = {2016},
 location = {New York, NY, USA},
 pages = {1329--1338},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=3045390.3045531},
 acmid = {3045531},
 publisher = {JMLR.org},
} 

@article{mnih2015humanlevel,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}

@InProceedings{mniha16A3C,
  title = 	 {Asynchronous Methods for Deep Reinforcement Learning},
  author = 	 {Volodymyr Mnih and Adria Puigdomenech Badia and Mehdi Mirza and Alex Graves and Timothy Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  year={2016},
  pdf={http://proceedings.mlr.press/v48/mniha16.pdf},
  url={http://proceedings.mlr.press/v48/mniha16.html},
}

@inproceedings{amir2016interactive,
title={Interactive teaching strategies for agent training},
author={Amir, Ofra and Kamar, Ece and Kolobov, Andrey and Grosz, Barbara J},
year={2016},
booktitle={International Joint Conferences on Artificial Intelligence (IJCAI)}
}

@inproceedings{abstractoptions,
  title={Learning abstract options},
  author={Riemer, Matthew and Liu, Miao and Tesauro, Gerald},
  booktitle={Neural Information Processing Systems (NIPS)},
  year={2018}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@inproceedings{torrey2013teaching,
  title={Teaching on a budget: Agents advising agents in reinforcement learning},
  author={Torrey, Lisa and Taylor, Matthew},
  booktitle = {International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
  pages={1053--1060},
  year={2013},
}

@article{clouse1996integrating,
  title={On integrating apprentice learning and reinforcement learning},
  author={Clouse, Jeffery Allen},
  year={1996}
}

@article{sutton1999,
title = "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
journal = "Artificial Intelligence",
year = "1999",
issn = "0004-3702",
doi = "https://doi.org/10.1016/S0004-3702(99)00052-1",
url = "http://www.sciencedirect.com/science/article/pii/S0004370299000521",
author = "Richard S. Sutton and Doina Precup and Satinder Singh",
}

@inproceedings{Parr1998,
 author = {Parr, Ronald and Russell, Stuart},
 title = {Reinforcement Learning with Hierarchies of Machines},
 booktitle = {Neural Information Processing Systems (NIPS)},
 year = {1998},
 isbn = {0-262-10076-2},
 location = {Denver, Colorado, USA},
 pages = {1043--1049},
 numpages = {7},
 url = {http://dl.acm.org/citation.cfm?id=302528.302894},
 acmid = {302894},
} 

@article{MAXQ2000,
 author = {Dietterich, Thomas G.},
 title = {Hierarchical Reinforcement Learning with the {MAXQ} Value Function Decomposition},
 journal = {J. Artif. Int. Res.},
 issue_date = {August 2000},
 volume = {13},
 number = {1},
 month = nov,
 year = {2000},
 issn = {1076-9757},
 pages = {227--303},
 numpages = {77},
 url = {http://dl.acm.org/citation.cfm?id=1622262.1622268},
 acmid = {1622268},
 publisher = {AI Access Foundation},
 address = {USA},
} 

@article{Taylor:2009:TLR:1577069.1755839,
 author = {Taylor, Matthew E. and Stone, Peter},
 title = {Transfer Learning for Reinforcement Learning Domains: A Survey},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2009},
 volume = {10},
 month = dec,
 year = {2009},
 issn = {1532-4435},
 pages = {1633--1685},
 numpages = {53},
 url = {http://dl.acm.org/citation.cfm?id=1577069.1755839},
 acmid = {1755839},
 publisher = {JMLR.org},
} 

@article{le2018hierarchical,
  author    = {Hoang Minh Le and
               Nan Jiang and
               Alekh Agarwal and
               Miroslav Dud{\'{\i}}k and
               Yisong Yue and
               Hal Daum{\'{e}} III},
  title     = {Hierarchical Imitation and Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1803.00590},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.00590},
  archivePrefix = {arXiv},
  eprint    = {1803.00590},
  timestamp = {Thu, 06 Jun 2019 18:03:56 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-00590},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{le2017coordinated,
  title={Coordinated Multi-Agent Imitation Learning},
  author={Le, Hoang M and Yue, Yisong and Carr, Peter and Lucey, Patrick},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={1995--2003},
  year={2017}
}

@inproceedings{wang2018efficient,
  title={Efficient Convention Emergence through Decoupled Reinforcement Social Learning with Teacher-Student Mechanism},
  author={Wang, Yixi and Lu, Wenhuan and Hao, Jianye and Wei, Jianguo and Leung, Ho-Fung},
  booktitle={Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
  pages={795--803},
  year={2018},
  organization={International Foundation for Autonomous Agents and Multiagent Systems}
}

@InProceedings{ross11imitation,
  title = 	 {A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning},
  author = 	 {Stephane Ross and Geoffrey Gordon and Drew Bagnell},
  booktitle = 	 {International Conference on Artificial Intelligence and Statistics (AIStat)},
  pages = 	 {627--635},
  year = 	 {2011},
  volume = 	 {15},
  pdf = 	 {http://proceedings.mlr.press/v15/ross11a/ross11a.pdf},
  url = 	 {http://proceedings.mlr.press/v15/ross11a.html},
}

@InProceedings{jiang18c,
  title = 	 {{M}entor{N}et: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels},
  author = 	 {Jiang, Lu and Zhou, Zhengyuan and Leung, Thomas and Li, Li-Jia and Fei-Fei, Li},
  booktitle = {International Conference on Machine Learning (ICML)},
  pages = 	 {2304--2313},
  year = 	 {2018},
  volume = 	 {80},
  month = 	 {10--15 Jul},
  pdf = 	 {http://proceedings.mlr.press/v80/jiang18c/jiang18c.pdf},
}

@inproceedings{fan2018learning,
title={Learning to Teach},
author={Yang Fan and Fei Tian and Tao Qin and Xiang-Yang Li and Tie-Yan Liu},
booktitle={International Conference on Learning Representations (ICLR)},
year={2018},
url={https://openreview.net/forum?id=HJewuJWCZ},
}

@InProceedings{tsvetkov16,
  author = 	"Tsvetkov, Yulia
		and Faruqui, Manaal
		and Ling, Wang
		and MacWhinney, Brian
		and Dyer, Chris",
  title = 	"Learning the Curriculum with Bayesian Optimization for Task-Specific Word      Representation Learning    ",
  booktitle = 	"Association for Computational Linguistics (ACL)",
  year = 	"2016",
  location = 	"Berlin, Germany",
  doi = 	"10.18653/v1/P16-1013",
  url = 	"http://aclweb.org/anthology/P16-1013"
}

@inproceedings{bengio2009curriculum,
title={Curriculum learning},
author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
booktitle={International Conference on Machine Learning (ICML)},
pages={41--48},
year={2009},
}

@inproceedings{feudalrl,
title={Feudal reinforcement learning},
author={Dayan, Peter and Hinton, Geoffrey E},
booktitle={Neural Information Processing Systems (NIPS)},
year={1993}
}

@inproceedings{wen2018probabilistic,
title={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},
author={Ying Wen and Yaodong Yang and Rui Luo and Jun Wang and Wei Pan},
booktitle=iclr,
year={2019},
url={https://openreview.net/forum?id=rkl6As0cF7},
}

@article{li19minimax, 
title={Robust Multi-Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient}, volume={33}, 
url={https://aaai.org/ojs/index.php/AAAI/article/view/4327}, DOI={10.1609/aaai.v33i01.33014213}, 
number={01}, 
journal={Association for the Advancement of Artificial Intelligence (AAAI)},
author={Li, Shihui and Wu, Yi and Cui, Xinyue and Dong, Honghua and Fang, Fei and Russell, Stuart}, 
year={2019}, 
month={Jul.}, 
pages={4213-4220} }

@inproceedings{thomaz06hri,
author = {Thomaz, Andrea L. and Breazeal, Cynthia},
title = {Reinforcement Learning with Human Teachers: Evidence of Feedback and Guidance with Implications for Learning Performance},
booktitle = {Association for the Advancement of Artificial Intelligence (AAAI)},
year = {2006},
isbn = {978-1-57735-281-5},
pages = {1000--1005},
numpages = {6},
url = {http://dl.acm.org/citation.cfm?id=1597538.1597696},
acmid = {1597696},
} 

@Inbook{Busoniu2010,
author="Bu{\c{s}}oniu, Lucian and Babu{\v{s}}ka, Robert and De Schutter, Bart",
title="Multi-agent Reinforcement Learning: An Overview",
bookTitle="Innovations in Multi-Agent Systems and Applications - 1",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="183--221",
isbn="978-3-642-14435-6",
doi="10.1007/978-3-642-14435-6_7",
url="https://doi.org/10.1007/978-3-642-14435-6_7"
}

@article{tuyls12survey,
author = {Tuyls, Karl and Weiss, Gerhard},
year = {2012},
month = {12},
pages = {41-52},
title = {Multiagent Learning: Basics, Challenges, and Prospects},
volume = {33},
journal = {{AI} Magazine},
doi = {10.1609/aimag.v33i3.2426}
}

@article{hernandezLealK17survey,
  author    = {Pablo Hernandez{-}Leal and
               Michael Kaisers and
               Tim Baarslag and
               Enrique Munoz de Cote},
  title     = {A Survey of Learning in Multiagent Environments: Dealing with Non-Stationarity},
  journal   = {CoRR},
  volume    = {abs/1707.09183},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.09183},
  archivePrefix = {arXiv},
  eprint    = {1707.09183},
  timestamp = {Mon, 13 Aug 2018 16:47:28 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Hernandez-LealK17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Matignon2012IndependentRL,
  title={Independent reinforcement learners in cooperative Markov games: a survey regarding coordination problems},
  author={La{\"e}titia Matignon and Guillaume J. Laurent and Nadine Le Fort-Piat},
  journal={Knowledge Engineering Review},
  year={2012},
  volume={27},
  pages={1-31}
}

@inproceedings{tesauro2004extending,
  title={Extending Q-learning to general adaptive multi-agent systems},
  author={Tesauro, Gerald},
  booktitle={Advances in neural information processing systems},
  pages={871--878},
  year={2004}
}

@inproceedings{alshedivat2018continuous,
title={Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments},
author={Maruan Al-Shedivat and Trapit Bansal and Yura Burda and Ilya Sutskever and Igor Mordatch and Pieter Abbeel},
booktitle=iclr,
year={2018},
url={https://openreview.net/forum?id=Sk2u1g-0-},
}

@InProceedings{yang18mean-field-marl,
  title = 	 {Mean Field Multi-Agent Reinforcement Learning},
  author = 	 {Yang, Yaodong and Luo, Rui and Li, Minne and Zhou, Ming and Zhang, Weinan and Wang, Jun},
  booktitle =icml, 
  pages = 	 {5571--5580},
  year = 	 {2018},
  volume = 	 {80},
  month = 	 {10--15 Jul},
  pdf = 	 {http://proceedings.mlr.press/v80/yang18d/yang18d.pdf},
  url = 	 {http://proceedings.mlr.press/v80/yang18d.html},
  abstract = 	 {Existing multi-agent reinforcement learning methods are limited typically to a small number of agents. When the agent number increases largely, the learning becomes intractable due to the curse of the dimensionality and the exponential growth of agent interactions. In this paper, we present Mean Field Reinforcement Learning where the interactions within the population of agents are approximated by those between a single agent and the average effect from the overall population or neighboring agents; the interplay between the two entities is mutually reinforced: the learning of the individual agent’s optimal policy depends on the dynamics of the population, while the dynamics of the population change according to the collective patterns of the individual policies. We develop practical mean field Q-learning and mean field Actor-Critic algorithms and analyze the convergence of the solution to Nash equilibrium. Experiments on Gaussian squeeze, Ising model, and battle games justify the learning effectiveness of our mean field approaches. In addition, we report the first result to solve the Ising model via model-free reinforcement learning methods.}
}

@InProceedings{shariq19maac,
  title =    {Actor-Attention-Critic for Multi-Agent Reinforcement Learning},
  author =   {Iqbal, Shariq and Sha, Fei},
  booktitle =    {Proceedings of the 36th International Conference on Machine Learning},
  pages =    {2961--2970},
  year =     {2019},
  editor =   {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume =   {97},
  series =   {Proceedings of Machine Learning Research},
  address =      {Long Beach, California, USA},
  month =    {09--15 Jun},
  publisher =    {PMLR},
  pdf =      {http://proceedings.mlr.press/v97/iqbal19a/iqbal19a.pdf},
  url =      {http://proceedings.mlr.press/v97/iqbal19a.html},
}

@InProceedings{he16opponent-modeling,
  title = 	 {Opponent Modeling in Deep Reinforcement Learning},
  author = 	 {He He and Jordan Boyd-Graber and Kevin Kwok and Hal Daumé III},
  booktitle =icml, 	
  pages = 	 {1804--1813},
  year = 	 {2016},
  volume = 	 {48},
  month = 	 {20--22 Jun},
  pdf = 	 {http://proceedings.mlr.press/v48/he16.pdf},
  url = 	 {http://proceedings.mlr.press/v48/he16.html},
}

@InProceedings{raileanu18opponent-modeling,
  title = 	 {Modeling Others using Oneself in Multi-Agent Reinforcement Learning},
  author = 	 {Raileanu, Roberta and Denton, Emily and Szlam, Arthur and Fergus, Rob},
  booktitle = icml, 
  pages = 	 {4257--4266},
  year = 	 {2018},
  volume = 	 {80},
  month = 	 {10--15 Jul},
  pdf = 	 {http://proceedings.mlr.press/v80/raileanu18a/raileanu18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/raileanu18a.html},
}

@InProceedings{grover18policy-representation,
  title = 	 {Learning Policy Representations in Multiagent Systems},
  author = 	 {Grover, Aditya and Al-Shedivat, Maruan and Gupta, Jayesh and Burda, Yuri and Edwards, Harrison},
  booktitle = icml,
  pages = 	 {1802--1811},
  year = 	 {2018},
  volume = 	 {80},
  month = 	 {10--15 Jul},
  pdf = 	 {http://proceedings.mlr.press/v80/grover18a/grover18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/grover18a.html},
}

@article{wei18multiagent-softQ,
  author    = {Ermo Wei and
               Drew Wicke and
               David Freelan and
               Sean Luke},
  title     = {Multiagent Soft {Q}-Learning},
  journal   = {CoRR},
  volume    = {abs/1804.09817},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.09817},
  archivePrefix = {arXiv},
  eprint    = {1804.09817},
  timestamp = {Mon, 13 Aug 2018 16:49:01 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1804-09817},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{shapley53stochastic,
	author = {Shapley, L. S.},
	title = {Stochastic Games},
	volume = {39},
	number = {10},
	pages = {1095--1100},
	year = {1953},
	doi = {10.1073/pnas.39.10.1095},
	publisher = {National Academy of Sciences},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/39/10/1095},
	eprint = {https://www.pnas.org/content/39/10/1095.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{papoudakis19nonstationarity,
  author    = {Georgios Papoudakis and
               Filippos Christianos and
               Arrasy Rahman and
               Stefano V. Albrecht},
  title     = {Dealing with Non-Stationarity in Multi-Agent Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1906.04737},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.04737},
  archivePrefix = {arXiv},
  eprint    = {1906.04737},
  timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1906-04737},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhang10lookahead,
  title={Multi-Agent Learning with Policy Prediction},
  author={Chongjie Zhang and Victor R. Lesser},
  booktitle=aaai,
  year={2010}
}

@inproceedings{foerster2018dice,
  title={{D}i{CE}: The Infinitely Differentiable {M}onte {C}arlo Estimator},
  author={Foerster, Jakob and Farquhar, Gregory and Al-Shedivat, Maruan and Rockt{\"a}schel, Tim and Xing, Eric and Whiteson, Shimon},
  booktitle =icml,
  pages={1524--1533},
  year={2018},
  volume={80},
  month={10--15 Jul},
  pdf={http://proceedings.mlr.press/v80/foerster18a/foerster18a.pdf},
  url={http://proceedings.mlr.press/v80/foerster18a.html},
}

@inproceedings{letcher2018stable,
title={Stable Opponent Shaping in Differentiable Games},
author={Alistair Letcher and Jakob Foerster and David Balduzzi and Tim Rocktäschel and Shimon Whiteson},
booktitle=iclr,
year={2019},
url={https://openreview.net/forum?id=SyGjjsC5tQ},
}

@book{book_gametheory,
 author = {Myerson, Roger B},
 title = {Game theory: Analysis of conflict},
 year = {1991},
 publisher = {Harvard University Press},
} 

@inproceedings{maml,
  title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={ICML},
  year={2017}
}

@inproceedings{bengio1992optimization,
  title={On the optimization of a synaptic learning rule},
  author={Bengio, Samy and Bengio, Yoshua and Cloutier, Jocelyn and Gecsei, Jan},
  booktitle={Preprints Conf. Optimality in Artificial and Biological Neural Networks},
  volume={2},
  year={1992},
  organization={Univ. of Texas}
}

@article{MER,
  title={Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference},
  author={Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
  journal=iclr,
  year={2019}
}

@incollection{Javed2019Meta,
title={Meta-Learning Representations for Continual Learning},
author={Javed, Khurram and White, Martha},
booktitle=nips,
editor={H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages={1818--1828},
year={2019},
publisher={Curran Associates, Inc.},
keywords={Meta-Continual Learning}
}

@inproceedings{hochreiter2001learning,
  title={Learning to learn using gradient descent},
  author={Hochreiter, Sepp and Younger, A Steven and Conwell, Peter R},
  booktitle={International Conference on Artificial Neural Networks},
  pages={87--94},
  year={2001},
  organization={Springer}
}

@article{duan2016rl,
  title={{RL}2: Fast reinforcement learning via slow reinforcement learning},
  author={Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L and Sutskever, Ilya and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1611.02779},
  year={2016}
}

@article{wang2016learning,
  title={Learning to reinforcement learn},
  author={Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
  journal={arXiv preprint arXiv:1611.05763},
  year={2016}
}

@article{snail,
  title={A simple neural attentive meta-learner},
  author={Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1707.03141},
  year={2017}
}

@article{Reptile,
  title={Reptile: a Scalable Metalearning Algorithm},
  author={Nichol, Alex and Schulman, John},
  journal={arXiv preprint arXiv:1803.02999},
  year={2018}
}

@article{vilalta2002perspective,
  title={A perspective view and survey of meta-learning},
  author={Vilalta, Ricardo and Drissi, Youssef},
  journal={Artificial intelligence review},
  volume={18},
  number={2},
  pages={77--95},
  year={2002},
  publisher={Springer}
}

@article{hospedales2020meta,
  title={Meta-Learning in Neural Networks: A Survey},
  author={Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  journal={arXiv preprint arXiv:2004.05439},
  year={2020}
}

@article{spigler2019meta,
  title={Meta-learnt priors slow down catastrophic forgetting in neural networks},
  author={Spigler, Giacomo},
  journal={arXiv preprint arXiv:1909.04170},
  year={2019},
    keywords={Meta-Continual Learning},
    url={https://arxiv.org/pdf/1909.04170.pdf}
}

@article{caccia2020online,
  title={Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning},
  author={Caccia, Massimo and Rodriguez, Pau and Ostapenko, Oleksiy and Normandin, Fabrice and Lin, Min and Caccia, Lucas and Laradji, Issam and Rish, Irina and Lacoste, Alexande and Vazquez, David and others},
  journal=nips,
  year={2020}
}

@inproceedings{Gupta2020LaMAMLLM,
  title={La-MAML: Look-ahead Meta Learning for Continual Learning},
  author={Gunshi Gupta and Karmesh Yadav and Liam Paull},
  url={https://arxiv.org/abs/2007.13904},
  year={2020},
  keywords={"Meta-Continual Learning"}
}

@article{beaulieu2020learning,
  title={Learning to Continually Learn},
  author={Beaulieu, Shawn and Frati, Lapo and Miconi, Thomas and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff and Cheney, Nick},
  journal={arXiv preprint arXiv:2002.09571},
  year={2020},
    keywords={Meta-Continual Learning},
    url={https://arxiv.org/abs/2002.09571}
}

@inproceedings{bansal2018emergent,
title={Emergent Complexity via Multi-Agent Competition},
author={Trapit Bansal and Jakub Pachocki and Szymon Sidor and Ilya Sutskever and Igor Mordatch},
booktitle=iclr,
year={2018},
url={https://openreview.net/forum?id=Sy0GnUxCb},
}