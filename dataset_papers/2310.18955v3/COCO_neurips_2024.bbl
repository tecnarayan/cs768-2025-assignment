\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Liakopoulos et~al.(2019)Liakopoulos, Destounis, Paschos, Spyropoulos,
  and Mertikopoulos]{georgios-cautious}
Nikolaos Liakopoulos, Apostolos Destounis, Georgios Paschos, Thrasyvoulos
  Spyropoulos, and Panayotis Mertikopoulos.
\newblock Cautious regret minimization: Online optimization with long-term
  budget constraints.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  3944--3952. PMLR, 09--15 Jun 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/liakopoulos19a.html}.

\bibitem[Sinha(2024)]{sinha2023banditq}
Abhishek Sinha.
\newblock Bandit{Q} - {F}air {B}andits with {G}uaranteed {R}ewards.
\newblock In \emph{Uncertainty in Artificial Intelligence}. PMLR, 2024.

\bibitem[Zinkevich(2003)]{zinkevich2003online}
Martin Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{Proceedings of the 20th international conference on machine
  learning (icml-03)}, pages 928--936, 2003.

\bibitem[Hazan et~al.(2007)Hazan, Rakhlin, and Bartlett]{hazan2007adaptive}
Elad Hazan, Alexander Rakhlin, and Peter Bartlett.
\newblock Adaptive online gradient descent.
\newblock \emph{Advances in neural information processing systems}, 20, 2007.

\bibitem[Orabona and P{\'a}l(2018)]{orabona2018scale}
Francesco Orabona and D{\'a}vid P{\'a}l.
\newblock Scale-free online learning.
\newblock \emph{Theoretical Computer Science}, 716:\penalty0 50--69, 2018.

\bibitem[Orabona(2019)]{orabona2019modern}
Francesco Orabona.
\newblock A modern introduction to online learning.
\newblock \emph{arXiv preprint arXiv:1912.13213}, 2019.

\bibitem[Yuan and Lamperski(2018)]{yuan2018online}
Jianjun Yuan and Andrew Lamperski.
\newblock Online convex optimization for cumulative constraints.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Jenatton et~al.(2016)Jenatton, Huang, and
  Archambeau]{jenatton2016adaptive}
Rodolphe Jenatton, Jim Huang, and C{\'e}dric Archambeau.
\newblock Adaptive algorithms for online convex optimization with long-term
  constraints.
\newblock In \emph{International Conference on Machine Learning}, pages
  402--411. PMLR, 2016.

\bibitem[Mahdavi et~al.(2012)Mahdavi, Jin, and Yang]{mahdavi2012trading}
Mehrdad Mahdavi, Rong Jin, and Tianbao Yang.
\newblock Trading regret for efficiency: online convex optimization with long
  term constraints.
\newblock \emph{The Journal of Machine Learning Research}, 13\penalty0
  (1):\penalty0 2503--2528, 2012.

\bibitem[Yi et~al.(2021)Yi, Li, Yang, Xie, Chai, and Johansson]{yi2021regret}
Xinlei Yi, Xiuxian Li, Tao Yang, Lihua Xie, Tianyou Chai, and Karl Johansson.
\newblock Regret and cumulative constraint violation analysis for online convex
  optimization with long term constraints.
\newblock In \emph{International Conference on Machine Learning}, pages
  11998--12008. PMLR, 2021.

\bibitem[Neely and Yu(2017)]{neely2017online}
Michael~J Neely and Hao Yu.
\newblock Online convex optimization with time-varying constraints.
\newblock \emph{arXiv preprint arXiv:1702.04783}, 2017.

\bibitem[Yu et~al.(2017)Yu, Neely, and Wei]{yu2017online}
Hao Yu, Michael Neely, and Xiaohan Wei.
\newblock Online convex optimization with stochastic constraints.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Sun et~al.(2017)Sun, Dey, and Kapoor]{pmlr-v70-sun17a}
Wen Sun, Debadeepta Dey, and Ashish Kapoor.
\newblock Safety-aware algorithms for adversarial contextual bandit.
\newblock In Doina Precup and Yee~Whye Teh, editors, \emph{Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of
  \emph{Proceedings of Machine Learning Research}, pages 3280--3288. PMLR,
  06--11 Aug 2017.
\newblock URL \url{https://proceedings.mlr.press/v70/sun17a.html}.

\bibitem[Yi et~al.(2023)Yi, Li, Yang, Xie, Hong, Chai, and
  Johansson]{yi2023distributed}
Xinlei Yi, Xiuxian Li, Tao Yang, Lihua Xie, Yiguang Hong, Tianyou Chai, and
  Karl~H Johansson.
\newblock Distributed online convex optimization with adversarial constraints:
  Reduced cumulative constraint violation bounds under slater's condition.
\newblock \emph{arXiv preprint arXiv:2306.00149}, 2023.

\bibitem[Neely(2010)]{neely2010stochastic}
Michael~J Neely.
\newblock Stochastic network optimization with application to communication and
  queueing systems.
\newblock \emph{Synthesis Lectures on Communication Networks}, 3\penalty0
  (1):\penalty0 1--211, 2010.

\bibitem[Guo et~al.(2022)Guo, Liu, Wei, and Ying]{guo2022online}
Hengquan Guo, Xin Liu, Honghao Wei, and Lei Ying.
\newblock Online convex optimization with hard constraints: Towards the best of
  two worlds and beyond.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 36426--36439, 2022.

\bibitem[Chen and Giannakis(2018)]{chen2018bandit}
Tianyi Chen and Georgios~B Giannakis.
\newblock Bandit convex optimization for scalable and dynamic iot management.
\newblock \emph{IEEE Internet of Things Journal}, 6\penalty0 (1):\penalty0
  1276--1286, 2018.

\bibitem[Cao and Liu(2018)]{cao2018online}
Xuanyu Cao and KJ~Ray Liu.
\newblock Online convex optimization with time-varying constraints and bandit
  feedback.
\newblock \emph{IEEE Transactions on automatic control}, 64\penalty0
  (7):\penalty0 2665--2680, 2018.

\bibitem[Vaze(2022)]{vazecocowiopt2022}
Rahul Vaze.
\newblock On dynamic regret and constraint violations in constrained online
  convex optimization.
\newblock In \emph{2022 20th International Symposium on Modeling and
  Optimization in Mobile, Ad hoc, and Wireless Networks (WiOpt)}, pages 9--16,
  2022.
\newblock \doi{10.23919/WiOpt56218.2022.9930613}.

\bibitem[Liu et~al.(2022)Liu, Wu, Huang, and Fang]{liu2022simultaneously}
Qingsong Liu, Wenfei Wu, Longbo Huang, and Zhixuan Fang.
\newblock Simultaneously achieving sublinear regret and constraint violations
  for online convex optimization with time-varying constraints.
\newblock \emph{ACM SIGMETRICS Performance Evaluation Review}, 49\penalty0
  (3):\penalty0 4--5, 2022.

\bibitem[Yi et~al.(2022)Yi, Li, Yang, Xie, Chai, and Karl]{yi2022regret}
Xinlei Yi, Xiuxian Li, Tao Yang, Lihua Xie, Tianyou Chai, and H~Karl.
\newblock Regret and cumulative constraint violation analysis for distributed
  online constrained convex optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 2022.

\bibitem[Yu and Neely(2016)]{yu2016low}
Hao Yu and Michael~J Neely.
\newblock A low complexity algorithm with $ o (\sqrt{T}) $ regret and $ o (1) $
  constraint violations for online convex optimization with long term
  constraints.
\newblock \emph{arXiv preprint arXiv:1604.02218}, 2016.

\bibitem[Hazan(2022)]{hazan2022introduction}
Elad Hazan.
\newblock \emph{Introduction to online convex optimization}.
\newblock MIT Press, 2022.

\bibitem[Hazan(2019)]{HazanBook}
Elad Hazan.
\newblock Introduction to online convex optimization.
\newblock \emph{CoRR}, abs/1909.05207, 2019.
\newblock URL \url{http://arxiv.org/abs/1909.05207}.

\bibitem[Asmussen(2003)]{asmussen2003applied}
S{\o}ren Asmussen.
\newblock \emph{Applied probability and queues}, volume~2.
\newblock Springer, 2003.

\bibitem[Hazan et~al.(2016)]{hazan2016introduction}
Elad Hazan et~al.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends{\textregistered} in Optimization},
  2\penalty0 (3-4):\penalty0 157--325, 2016.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of machine learning research}, 12\penalty0 (7), 2011.

\bibitem[Abernethy et~al.(2014)Abernethy, Lee, Sinha, and
  Tewari]{abernethy2014online}
Jacob Abernethy, Chansoo Lee, Abhinav Sinha, and Ambuj Tewari.
\newblock Online linear optimization via smoothing.
\newblock In \emph{Conference on Learning Theory}, pages 807--823, 2014.

\bibitem[Joulani et~al.(2016)Joulani, Gyorgy, and
  Szepesv{\'a}ri]{joulani2016delay}
Pooria Joulani, Andras Gyorgy, and Csaba Szepesv{\'a}ri.
\newblock Delay-tolerant online convex optimization: Unified analysis and
  adaptive-gradient algorithms.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~30, 2016.

\bibitem[Ruder(2017)]{ruder2017overview}
Sebastian Ruder.
\newblock An overview of multi-task learning in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1706.05098}, 2017.

\bibitem[Dekel et~al.(2006)Dekel, Long, and Singer]{dekel2006online}
Ofer Dekel, Philip~M Long, and Yoram Singer.
\newblock Online multitask learning.
\newblock In \emph{International Conference on Computational Learning Theory},
  pages 453--467. Springer, 2006.

\bibitem[Murugesan et~al.(2016)Murugesan, Liu, Carbonell, and
  Yang]{murugesan2016adaptive}
Keerthiram Murugesan, Hanxiao Liu, Jaime Carbonell, and Yiming Yang.
\newblock Adaptive smoothed online multi-task learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Bansal et~al.(2018)Bansal, B{\"o}hm, Eli{\'a}{\v{s}}, Koumoutsos, and
  Umboh]{bansa2018nested}
Nikhil Bansal, Martin B{\"o}hm, Marek Eli{\'a}{\v{s}}, Grigorios Koumoutsos,
  and Seeun~William Umboh.
\newblock Nested convex bodies are chaseable.
\newblock In \emph{Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 1253--1260. SIAM, 2018.

\bibitem[Argue et~al.(2019)Argue, Bubeck, Cohen, Gupta, and
  Lee]{argue2019nearly}
CJ~Argue, S{\'e}bastien Bubeck, Michael~B Cohen, Anupam Gupta, and Yin~Tat Lee.
\newblock A nearly-linear bound for chasing nested convex bodies.
\newblock In \emph{Proceedings of the Thirtieth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 117--122. SIAM, 2019.

\bibitem[Bubeck et~al.(2020)Bubeck, Klartag, Lee, Li, and
  Sellke]{bubeck2020chasing}
S{\'e}bastien Bubeck, Bo'az Klartag, Yin~Tat Lee, Yuanzhi Li, and Mark Sellke.
\newblock Chasing nested convex bodies nearly optimally.
\newblock In \emph{Proceedings of the Fourteenth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 1496--1508. SIAM, 2020.

\end{thebibliography}
