@article{Yang19,
  author    = {Zhuoran Yang and
               Yuchen Xie and
               Zhaoran Wang},
  title     = {A Theoretical Analysis of Deep Q-Learning},
  journal   = {CoRR},
  volume    = {abs/1901.00137},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.00137},
  archivePrefix = {arXiv},
  eprint    = {1901.00137},
  timestamp = {Thu, 31 Jan 2019 13:52:49 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-00137},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{geist19a,
  title = 	 {A Theory of Regularized {M}arkov Decision Processes},
  author = 	 {Geist, Matthieu and Scherrer, Bruno and Pietquin, Olivier},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2160--2169},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/geist19a/geist19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/geist19a.html},
  abstract = 	 {Many recent successful (deep) reinforcement learning algorithms make use of regularization, generally based on entropy or Kullback-Leibler divergence. We propose a general theory of regularized Markov Decision Processes that generalizes these approaches in two directions: we consider a larger class of regularizers, and we consider the general modified policy iteration approach, encompassing both policy iteration and value iteration. The core building blocks of this theory are a notion of regularized Bellman operator and the Legendre-Fenchel transform, a classical tool of convex optimization. This approach allows for error propagation analyses of general algorithmic schemes of which (possibly variants of) classical algorithms such as Trust Region Policy Optimization, Soft Q-learning, Stochastic Actor Critic or Dynamic Policy Programming are special cases. This also draws connections to proximal convex optimization, especially to Mirror Descent.}
}

@misc{mahajan2019maven,
	title={MAVEN: Multi-Agent Variational Exploration},
	author={Anuj Mahajan and Tabish Rashid and Mikayel Samvelyan and Shimon Whiteson},
	year={2019},
	eprint={1910.07483},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1910.07483}
}

@inproceedings{mahajan2017symmetry,
	title={Symmetry Detection and Exploitation for Function Approximation in Deep RL},
	author={Mahajan, Anuj and Tulabandhula, Theja},
	booktitle={Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
	pages={1619--1621},
	year={2017},
	organization={International Foundation for Autonomous Agents and Multiagent Systems}
}

@article{mahajan2017asymmetry,
	title={Symmetry learning for function approximation in reinforcement learning},
	author={Mahajan, Anuj and Tulabandhula, Theja},
	journal={arXiv preprint arXiv:1706.02999},
	year={2017}
}

@article{Sallans04,
 author = {Sallans, Brian and Hinton, Geoffrey E.},
 title = {Reinforcement Learning with Factored States and Actions},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2004},
 volume = {5},
 month = {dec},
 year = {2004},
 issn = {1532-4435},
 pages = {1063--1088},
 numpages = {26},
 url = {http://dl.acm.org/citation.cfm?id=1005332.1016794},
 acmid = {1016794},
 publisher = {JMLR.org}
} 

@article{Song19,
author = {Song, Zhao and E. Parr, Ronald and Carin, Lawrence},
year = {2019},
pages = {},
title = {Revisiting the Softmax Bellman Operator: Theoretical Properties and Practical Benefits},
journal= {Proceedings of the 36th International Conference on Machine
Learning} 
}

@inbook{Kelly08,
author = {Kelly, John},
publisher = {John Wiley \& Sons, Ltd},
isbn = {9783527618897},
title = {Generalized Functions},
booktitle = {Graduate Mathematical Physics},
chapter = {4},
pages = {111-124},
doi = {10.1002/9783527618897.ch4},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9783527618897.ch4},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9783527618897.ch4},
year = {2008},
keywords = {instantaneously, idealizations, impulse, derivative, homogeneous},
abstract = {Summary This chapter contains sections titled: Motivation Properties of the Dirac Delta Function Other Useful Generalized Functions Green Functions Multidimensional Delta Functions}
}

@article{Bertsekas95,
 author = {Bertsekas, Dimitri P.},
 title = {A Counterexample to Temporal Differences Learning},
 journal = {Neural Comput.},
 issue_date = {March 1995},
 volume = {7},
 number = {2},
 month = mar,
 year = {1995},
 issn = {0899-7667},
 pages = {270--279},
 numpages = {10},
 url = {http://dx.doi.org/10.1162/neco.1995.7.2.270},
 doi = {10.1162/neco.1995.7.2.270},
 acmid = {206098},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@MISC{Williams93,
    author = {Ronald J. Williams and Leemon C. Baird and III},
    title = {Analysis of Some Incremental Variants of Policy Iteration: First Steps Toward Understanding Actor-Critic Learning Systems},
    year = {1993}
}

@InProceedings{Foerster18,
  title = 	 {{D}i{CE}: The Infinitely Differentiable {M}onte {C}arlo Estimator},
  author = 	 {Foerster, Jakob and Farquhar, Gregory and Al-Shedivat, Maruan and Rockt{\"a}schel, Tim and Xing, Eric and Whiteson, Shimon},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1529--1538},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/foerster18a/foerster18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/foerster18a.html},
}

@article{Pearlmutter94,
    author = {Barak A. Pearlmutter},
    title = {Fast Exact Multiplication by the Hessian},
    journal = {Neural Computation},
    year = {1994},
    volume = {6},
    pages = {147--160}
}

@book{Bass13,
  title={Real Analysis for Graduate Students},
  author={Bass, R.F.},
  isbn={9781481869140},
  url={https://books.google.co.uk/books?id=s6mVlgEACAAJ},
  year={2013},
  publisher={Createspace Ind Pub}
}


@article{Gunawardana05,
 author = {Gunawardana, Asela and Byrne, William},
 title = {Convergence Theorems for Generalized Alternating Minimization Procedures},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2005},
 volume = {6},
 month = dec,
 year = {2005},
 issn = {1532-4435},
 pages = {2049--2073},
 numpages = {25},
 url = {http://dl.acm.org/citation.cfm?id=1046920.1194913},
 acmid = {1194913},
 publisher = {JMLR.org},
} 

@inbook{Turner11, 
place={Cambridge}, 
title={Two problems with variational expectation maximisation for time series models}, DOI={10.1017/CBO9780511984679.006}, booktitle={Bayesian Time Series Models}, publisher={Cambridge University Press}, author={Turner, Richard Eric and Sahani, Maneesh}, editor={Barber, David and Cemgil, A. Taylan and Chiappa, SilviaEditors}, year={2011}, pages={104–124}}

@inproceedings{Sutton09a,
 author = {Sutton, Richard S. and Maei, Hamid Reza and Precup, Doina and Bhatnagar, Shalabh and Silver, David and Szepesv\'{a}ri, Csaba and Wiewiora, Eric},
 title = {Fast Gradient-descent Methods for Temporal-difference Learning with Linear Function Approximation},
 booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
 series = {ICML '09},
 year = {2009},
 isbn = {978-1-60558-516-1},
 location = {Montreal, Quebec, Canada},
 pages = {993--1000},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1553374.1553501},
 doi = {10.1145/1553374.1553501},
 acmid = {1553501},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@incollection{Maei09,
title = {Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation},
author = {Bhatnagar, Shalabh and Precup, Doina and Silver, David and Sutton, Richard S and Hamid R. Maei and Csaba Szepesv\'{a}ri},
booktitle = {Advances in Neural Information Processing Systems 22},
editor = {Y. Bengio and D. Schuurmans and J. D. Lafferty and C. K. I. Williams and A. Culotta},
pages = {1204--1212},
year = {2009},
publisher = {Curran Associates, Inc.},
}


@article{Bertsekas76,
title = "Multiplier methods: A survey",
journal = "Automatica",
volume = "12",
number = "2",
pages = "133 - 145",
year = "1976",
issn = "0005-1098",
doi = "https://doi.org/10.1016/0005-1098(76)90077-7",
url = "http://www.sciencedirect.com/science/article/pii/0005109876900777",
author = "Dimitri P. Bertsekas",
abstract = "The purpose of this paper is to provide a survey of convergence and rate of convergence aspects of a class of recently proposed methods for constrained minimization—the, so-called, multiplier methods. The results discussed highlight the operational aspects of multiplier methods and demonstrate their significant advantages over ordinary penalty methods."
}

@article{Powell78,
 author = {Powell, M. J.},
 title = {Algorithms for Nonlinear Constraints That Use Lagrangian Functions},
 journal = {Math. Program.},
 issue_date = {December  1978},
 volume = {14},
 number = {1},
 month = dec,
 year = {1978},
 issn = {0025-5610},
 pages = {224--248},
 numpages = {25},
 url = {https://doi.org/10.1007/BF01588967},
 doi = {10.1007/BF01588967},
 acmid = {3114388},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA},
 keywords = {Augmented Lagrangian, Lagrangian Function, Nonlinear Constraints, Nonlinear Programming, Optimization Algorithm},
} 

@book{Liberzon11,
 author = {Liberzon, Daniel},
 title = {Calculus of Variations and Optimal Control Theory: A Concise Introduction},
 year = {2011},
 isbn = {0691151873, 9780691151878},
 publisher = {Princeton University Press},
 address = {Princeton, NJ, USA},
} 

@book{Bertsekas96,
  title={Constrained Optimization and Lagrange Multiplier Methods},
  author={Bertsekas, D.P.},
  isbn={9781886529045},
  lccn={96079307},
  series={Athena scientific series in optimization and neural computation},
  url={http://web.mit.edu/dimitrib/www/Constrained-Opt.pdf},
  year={1996},
  publisher={Athena Scientific}
}


@book{Bertsekas16,
  title={Nonlinear Programming},
  author={Bertsekas, D.P.},
  isbn={9781886529052},
  series={Athena scientific optimization and computation series},
  url={https://books.google.co.uk/books?id=TwOujgEACAAJ},
  year={2016},
  publisher={Athena Scientific}
}

@book{Luenberger15,
 author = {Luenberger, David G. and Ye, Yinyu},
 title = {Linear and Nonlinear Programming},
 year = {2015},
 isbn = {3319188410, 9783319188416},
 publisher = {Springer Publishing Company, Incorporated},
} 


@incollection{Munos2016,
title = {Safe and Efficient Off-Policy Reinforcement Learning},
author = {Munos, Remi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {1054--1062},
year = {2016},
publisher = {Curran Associates, Inc.},
}


@article{Schulman2017ppo,
  title={Proximal Policy Optimization Algorithms},
  author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  journal={CoRR},
  year={2017},
  volume={abs/1707.06347}
}

@article{Wang2016,
  title={Sample Efficient Actor-Critic with Experience Replay},
  author={Ziyu Wang and Victor Bapst and Nicolas Heess and Volodymyr Mnih and Remi Munos and Koray Kavukcuoglu and Nando de Freitas},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2016},
  volume={abs/1611.01224}
}

@TECHREPORT{Meuleau00,
    author = {Nicolas Meuleau and Leonid Peshkin and Leslie P. Kaelbling and Kee-eung Kim},
    title = {Off-Policy Policy Search},
    institution = {},
    year = {2000}
}

@article{Goyal18,
  author    = {Anirudh Goyal and
               Philemon Brakel and
               William Fedus and
               Timothy P. Lillicrap and
               Sergey Levine and
               Hugo Larochelle and
               Yoshua Bengio},
  title     = {Recall Traces: Backtracking Models for Efficient Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1804.00379},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.00379},
  archivePrefix = {arXiv},
  eprint    = {1804.00379},
  timestamp = {Mon, 13 Aug 2018 16:46:10 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1804-00379},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{a3c,
  title = 	 {Asynchronous Methods for Deep Reinforcement Learning},
  author = 	 {Volodymyr Mnih and Adria Puigdomenech Badia and Mehdi Mirza and Alex Graves and Timothy Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1928--1937},
  year = 	 {2016},
  editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/mniha16.pdf},
  url = 	 {http://proceedings.mlr.press/v48/mniha16.html},
  abstract = 	 {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.}
}

@phdthesis{Beal03,
author = {Beal, Matthew James},
year = {2003},
title = {Variational algorithms for approximate Bayesian inference},
booktitle = {PhD thesis}
}

@article{Li17,
abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We start with background of deep learning and reinforcement learning, as well as introduction of testbeds. Next we discuss Deep Q-Network (DQN) and its extensions, asynchronous methods, policy optimization, reward, and planning. After that, we talk about attention and memory, unsupervised learning, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, spoken dialogue systems (a.k.a. chatbot), machine translation, text sequence prediction, neural architecture design, personalized web services, healthcare, finance, and music generation. We mention topics/papers not reviewed yet. After listing a collection of RL resources, we close with discussions.},
archivePrefix = {arXiv},
arxivId = {1701.07274},
author = {Li, Yuxi},
doi = {10.1007/978-3-319-56991-8_32},
eprint = {1701.07274},
isbn = {9781509011216},
issn = {1701.07274},
journal = {arXiv},
keywords = {()},
pmid = {15040217},
title = {{Deep Reinforcement Learning: An Overview}},
year = {2017}
}


@inproceedings{rllab,
  title={Benchmarking deep reinforcement learning for continuous control},
  author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={1329--1338},
  year={2016}
}
@incollection{Mnih13,
  title = {Playing Atari With Deep Reinforcement Learning},
  author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
  booktitle = {NIPS Deep Learning Workshop},
  year = {2013}
}

@article{Silver16,
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  doi = {10.1038/nature16961},
  groups = {public},
  issn = {0028-0836},
  journal = {Nature},
  number = 7587,
  pages = {484--489},
  title = {Mastering the Game of {Go} with Deep Neural Networks and Tree Search},
  volume = 529,
  year = 2016
}

@misc{OpenAI18,
author = {OpenAI},
title = {OpenAI Five},
year = {2018},
howpublished = {\url{https://blog.openai.com/openai-five/},
}
}


@InProceedings{fujimoto2018addressing,
title = 	 {Addressing Function Approximation Error in Actor-Critic Methods},
author = 	 {Fujimoto, Scott and van Hoof, Herke and Meger, David},
booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
pages = 	 {1587--1596},
year = 	 {2018},
editor = 	 {Dy, Jennifer and Krause, Andreas},
volume = 	 {80},
series = 	 {Proceedings of Machine Learning Research},
address = 	 {Stockholmsmässan, Stockholm Sweden},
month = 	 {10--15 Jul},
publisher = 	 {PMLR},
pdf = 	 {http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf},
url = 	 {http://proceedings.mlr.press/v80/fujimoto18a.html},
}

@book{Sutton17,
 author = {Sutton, Richard S. and Barto, Andrew G.},
 title = {Introduction to Reinforcement Learning},
 year = {2017},
 isbn = {0262193981},
 edition = {2nd},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 


@incollection{Sutton09,
title = {A Convergent O(n) Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation},
author = {Sutton, Richard S and Hamid R. Maei and Csaba Szepesv\'{a}ri},
booktitle = {Advances in Neural Information Processing Systems 21},
editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
pages = {1609--1616},
year = {2009},
publisher = {Curran Associates, Inc.},
}


@InProceedings{Mnih16,
  title = 	 {Asynchronous Methods for Deep Reinforcement Learning},
  author = 	 {Volodymyr Mnih and Adria Puigdomenech Badia and Mehdi Mirza and Alex Graves and Timothy Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1928--1937},
  year = 	 {2016},
  editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/mniha16.pdf},
  url = 	 {http://proceedings.mlr.press/v48/mniha16.html},
  abstract = 	 {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.}
}

@article{Williams91,
author = {Williams, Ronald J. and Peng, Jing},
title = {Function Optimization using Connectionist Reinforcement Learning Algorithms},
journal = {Connection Science},
volume = {3},
number = {3},
pages = {241-268},
year  = {1991},
publisher = {Taylor & Francis},
doi = {10.1080/09540099108946587},

URL = { 
        https://doi.org/10.1080/09540099108946587
    
},
eprint = { 
        https://doi.org/10.1080/09540099108946587
    
}

}

@inProceedings{Heess15,
title = {Learning Continuous Control Policies by Stochastic Value Gradients},
author = {Heess, Nicolas and Wayne, Gregory and Silver, David and Lillicrap, Tim and Erez, Tom and Tassa, Yuval},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2944--2952},
year = {2015},
publisher = {Curran Associates, Inc.}
}


@inProceedings{Schulman15a,
title = {Gradient Estimation Using Stochastic Computation Graphs},
author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {3528--3536},
year = {2015},
publisher = {Curran Associates, Inc.},
}

@misc{Schulman17,
Author = {John Schulman and Xi Chen and Pieter Abbeel},
Title = {Equivalence Between Policy Gradients and Soft Q-Learning},
Year = {2017},
Eprint = {arXiv:1704.06440},
}

@InProceedings{Chen18,
  title = 	 {Variational Inference and Model Selection with Generalized Evidence Bounds},
  author = 	 {Chen, Liqun and Tao, Chenyang and Zhang, Ruiyi and Henao, Ricardo and Duke, Lawrence Carin},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {893--902},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/chen18k/chen18k.pdf},
  url = 	 {http://proceedings.mlr.press/v80/chen18k.html},
  abstract = 	 {Recent advances on the scalability and flexibility of variational inference have made it successful at unravelling hidden patterns in complex data. In this work we propose a new variational bound formulation, yielding an estimator that extends beyond the conventional variational bound. It naturally subsumes the importance-weighted and Renyi bounds as special cases, and it is provably sharper than these counterparts. We also present an improved estimator for variational learning, and advocate a novel high signal-to-variance ratio update rule for the variational parameters. We discuss model-selection issues associated with existing evidence-lower-bound-based variational inference procedures, and show how to leverage the flexibility of our new formulation to address them. Empirical evidence is provided to validate our claims.}
}

@InProceedings{Rainforth18b,
  title = 	 {Tighter Variational Bounds are Not Necessarily Better},
  author = 	 {Rainforth, Tom and Kosiorek, Adam and Le, Tuan Anh and Maddison, Chris and Igl, Maximilian and Wood, Frank and Teh, Yee Whye},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4277--4285},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/rainforth18b/rainforth18b.pdf},
  url = 	 {http://proceedings.mlr.press/v80/rainforth18b.html},
  abstract = 	 {We provide theoretical and empirical evidence that using tighter evidence lower bounds (ELBOs) can be detrimental to the process of learning an inference network by reducing the signal-to-noise ratio of the gradient estimator. Our results call into question common implicit assumptions that tighter ELBOs are better variational objectives for simultaneous model learning and inference amortization schemes. Based on our insights, we introduce three new algorithms: the partially importance weighted auto-encoder (PIWAE), the multiply importance weighted auto-encoder (MIWAE), and the combination importance weighted autoencoder (CIWAE), each of which includes the standard importance weighted auto-encoder (IWAE) as a special case. We show that each can deliver improvements over IWAE, even when performance is measured by the IWAE target itself. Furthermore, our results suggest that PIWAE may be able to deliver simultaneous improvements in the training of both the inference and generative networks.}
}

@incollection{Levine13,
title = {Variational Policy Search via Trajectory Optimization},
author = {Levine, Sergey and Koltun, Vladlen},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {207--215},
year = {2013},
publisher = {Curran Associates, Inc.},
}

@inproceedings{Fox16,
 author = {Fox, Roy and Pakman, Ari and Tishby, Naftali},
 title = {Taming the Noise in Reinforcement Learning via Soft Updates},
 booktitle = {Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence},
 series = {UAI'16},
 year = {2016},
 isbn = {978-0-9966431-1-5},
 location = {Jersey City, New Jersey, USA},
 pages = {202--211},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=3020948.3020970},
 acmid = {3020970},
 publisher = {AUAI Press},
 address = {Arlington, Virginia, United States},
} 

@InProceedings{Thomas14,
  title =    {Bias in Natural Actor-Critic Algorithms},
  author =   {Philip Thomas},
  booktitle =    {Proceedings of the 31st International Conference on Machine Learning},
  pages =    {441--448},
  year =   {2014},
  editor =   {Eric P. Xing and Tony Jebara},
  volume =   {32},
  number =       {1},
  series =   {Proceedings of Machine Learning Research},
  address =    {Bejing, China},
  month =    {22--24 Jun},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v32/thomas14.pdf},
  url =    {http://proceedings.mlr.press/v32/thomas14.html},
  abstract =   {We show that several popular discounted reward natural actor-critics, including the popular NAC-LSTD and eNAC algorithms, do not generate unbiased estimates of the natural policy gradient as claimed. We derive the first unbiased discounted reward natural actor-critics using batch and iterative approaches to gradient estimation. We argue that the bias makes the existing algorithms more appropriate for the average reward setting. We also show that, when Sarsa(lambda) is guaranteed to converge to an optimal policy, the objective function used by natural actor-critics is concave, so policy gradient methods are guaranteed to converge to globally optimal policies as well.}
}

@inproceedings{Rawlik12,
  title={On Stochastic Optimal Control and Reinforcement Learning by Approximate Inference},
  author={Konrad Rawlik and Marc Toussaint and Sethu Vijayakumar},
  booktitle={Robotics: Science and Systems},
  year={2012}
}

@article{Rawlik10,
  author    = {Konrad Rawlik and
               Marc Toussaint and
               Sethu Vijayakumar},
  title     = {Approximate Inference and Stochastic Optimal Control},
  journal   = {CoRR},
  volume    = {abs/1009.3958},
  year      = {2010},
  url       = {http://arxiv.org/abs/1009.3958},
  archivePrefix = {arXiv},
  eprint    = {1009.3958},
  timestamp = {Mon, 13 Aug 2018 16:47:14 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1009-3958},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Abdolmaleki18,
title={Maximum a Posteriori Policy Optimisation},
author={Abbas Abdolmaleki and Jost Tobias Springenberg and Yuval Tassa and Remi Munos and Nicolas Heess and Martin Riedmiller},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=S1ANxQW0b},
}

@InProceedings{Heess12,
  title =    {Actor-Critic Reinforcement Learning with Energy-Based Policies},
  author =   {Nicolas Heess and David Silver and Yee Whye Teh},
  booktitle =    {Proceedings of the Tenth European Workshop on Reinforcement Learning},
  pages =    {45--58},
  year =   {2013},
  editor =   {Marc Peter Deisenroth and Csaba Szepesvári and Jan Peters},
  volume =   {24},
  series =   {Proceedings of Machine Learning Research},
  address =    {Edinburgh, Scotland},
  month =    {30 Jun--01 Jul},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v24/heess12a/heess12a.pdf},
  url =    {http://proceedings.mlr.press/v24/heess12a.html},
  abstract =   {We consider reinforcement learning in Markov decision processes with high dimensional state and action spaces. We parametrize policies using energy-based models (particularly restricted Boltzmann machines), and train them using policy gradient learning. Our approach builds upon Sallans and Hinton (2004), who parameterized value functions using energy-based models, trained using a non-linear variant of temporal-difference (TD) learning. Unfortunately, non-linear TD is known to diverge in theory and practice. We introduce the first sound and efficient algorithm for training energy-based policies, based on an actor-critic architecture. Our algorithm is computationally efficient, converges close to a local optimum, and outperforms Sallans and Hinton (2004) in several high dimensional domains.}
}

@inproceedings{Hausman18,
        title={Learning an Embedding Space for Transferable Robot Skills},
        author={Karol Hausman and Jost Tobias Springenberg and Ziyu Wang and Nicolas Heess and Martin Riedmiller},
        booktitle={International Conference on Learning Representations},
        year={2018},
        url={https://openreview.net/forum?id=rk07ZXZRb},
        }

@INPROCEEDINGS{Attias03,
    author = {Hagai Attias},
    title = {Planning by Probabilistic Inference},
    booktitle = {Proc. of the 9th Int. Workshop on Artificial Intelligence and Statistics},
    year = {2003}
}

@InProceedings{Haarnoja17,
  title = 	 {Reinforcement Learning with Deep Energy-Based Policies},
  author = 	 {Tuomas Haarnoja and Haoran Tang and Pieter Abbeel and Sergey Levine},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1352--1361},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/haarnoja17a/haarnoja17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/haarnoja17a.html},
  abstract = 	 {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.}
}

@InProceedings{Haarnoja18,
  title = 	 {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author = 	 {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1861--1870},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf},
  url = 	 {http://proceedings.mlr.press/v80/haarnoja18b.html},
  abstract = 	 {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.}
}

@InProceedings{Hachiya09,
author="Hachiya, Hirotaka
and Peters, Jan
and Sugiyama, Masashi",
editor="Buntine, Wray
and Grobelnik, Marko
and Mladeni{\'{c}}, Dunja
and Shawe-Taylor, John",
title="Efficient Sample Reuse in EM-Based Policy Search",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="469--484",
abstract="Direct policy search is a promising reinforcement learning framework in particular for controlling in continuous, high-dimensional systems such as anthropomorphic robots. Policy search often requires a large number of samples for obtaining a stable policy update estimator due to its high flexibility. However, this is prohibitive when the sampling cost is expensive. In this paper, we extend an EM-based policy search method so that previously collected samples can be efficiently reused. The usefulness of the proposed method, called Reward-weighted Regression with sample Reuse (R3), is demonstrated through a robot learning experiment.",
isbn="978-3-642-04180-8"
}

@inproceedings{Neumann11,
 author = {Neumann, Gerhard},
 title = {Variational Inference for Policy Search in Changing Situations},
 booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
 series = {ICML'11},
 year = {2011},
 isbn = {978-1-4503-0619-5},
 location = {Bellevue, Washington, USA},
 pages = {817--824},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=3104482.3104585},
 acmid = {3104585},
 publisher = {Omnipress},
 address = {USA},
}

@inproceedings{Peters07,
 author = {Peters, Jan and Schaal, Stefan},
 title = {Reinforcement Learning by Reward-weighted Regression for Operational Space Control},
 booktitle = {Proceedings of the 24th International Conference on Machine Learning},
 series = {ICML '07},
 year = {2007},
 isbn = {978-1-59593-793-3},
 location = {Corvalis, Oregon, USA},
 pages = {745--750},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1273496.1273590},
 doi = {10.1145/1273496.1273590},
 acmid = {1273590},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Ziebart08,
 author = {Ziebart, Brian D. and Maas, Andrew and Bagnell, J. Andrew and Dey, Anind K.},
 title = {Maximum Entropy Inverse Reinforcement Learning},
 booktitle = {Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 3},
 series = {AAAI'08},
 year = {2008},
 isbn = {978-1-57735-368-3},
 location = {Chicago, Illinois},
 pages = {1433--1438},
 numpages = {6},
 url = {http://dl.acm.org/citation.cfm?id=1620270.1620297},
 acmid = {1620297},
 publisher = {AAAI Press},
}

@inproceedings{Ziebart10a,
 author = {Ziebart, Brian D. and Bagnell, J. Andrew and Dey, Anind K.},
 title = {Modeling Interaction via the Principle of Maximum Causal Entropy},
 booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
 series = {ICML'10},
 year = {2010},
 isbn = {978-1-60558-907-7},
 location = {Haifa, Israel},
 pages = {1255--1262},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=3104322.3104481},
 acmid = {3104481},
 publisher = {Omnipress},
 address = {USA},
}

@phdthesis{Ziebart10b,
author = {Ziebart, Brian D},
title = {Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy},
url = {http://www.cs.cmu.edu/{~}bziebart/publications/thesis-bziebart.pdf},
year = {2010}
}

@inproceedings{Toussaint09a,
abstract = {The general stochastic optimal control (SOC) problem in robotics scenarios is often too complex to be solved exactly and in near real time. A classical approximate solution is to first compute an optimal (deterministic) trajectory and then solve a local linear-quadratic-gaussian (LQG) perturbation model to handle the system stochasticity. We present a new algorithm for this approach which improves upon previous algorithms like iLQG. We consider a probabilistic model for which the maximum likelihood (ML) trajectory coincides with the optimal trajectory and which, in the LQG case, re-produces the classical SOC solution. The algorithm then utilizes approximate inference methods (similar to expectation propagation) that efficiently generalize to non-LQG systems. We demonstrate the algorithm on a simulated 39-DoF humanoid robot.},
author = {Toussaint, Marc},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
doi = {10.1145/1553374.1553508},
file = {:Users/matows/Library/Application Support/Mendeley Desktop/Downloaded/Toussaint - Unknown - Robot Trajectory Optimization using Approximate Inference.pdf:pdf},
isbn = {9781605585161},
pages = {1--8},
title = {{Robot trajectory optimization using approximate inference}},
url = {https://homes.cs.washington.edu/{~}todorov/courses/amath579/reading/Toussaint.pdf http://portal.acm.org/citation.cfm?doid=1553374.1553508},
year = {2009}
}

@article{Toussaint09b,
author = {Toussaint, Marc},
year = {2009},
month = {01},
pages = {},
title = {Probabilistic inference as a model of planned behavior},
volume = {3},
journal = {Kunstliche Intelligenz}
}

@article{virl_review,
abstract = {The framework of reinforcement learning or optimal control provides a mathe-matical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learn-ing and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: for-malizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy rein-forcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynam-ics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
archivePrefix = {arXiv},
arxivId = {arXiv:1805.00909v3},
author = {Levine, Sergey},
eprint = {arXiv:1805.00909v3},
file = {:Users/matows/Library/Application Support/Mendeley Desktop/Downloaded/Levine - 2018 - Reinforcement Learning and Control as Probabilistic Inference Tutorial and Review.pdf:pdf},
title = {{Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review}},
url = {https://arxiv.org/pdf/1805.00909.pdf},
year = {2018}
}

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop
@phdthesis{Levine14,
author = {Levine, Sergey},
file = {:Users/matows/Library/Application Support/Mendeley Desktop/Downloaded/Levine - 2014 - MOTOR SKILL LEARNING WITH LOCAL TRAJECTORY METHODS.pdf:pdf},
title = {Motor Skill Learning with Trajectory Methods},
url = {https://people.eecs.berkeley.edu/{~}svlevine/papers/thesis.pdf},
year = {2014}
}

@article{Wu83,
abstract = {Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission. JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Two convergence aspects of the EM algorithm are studied: (i) does the EM algorithm find a local maximum or a stationary value of the (incomplete-data) likelihood function? (ii) does the sequence of parameter estimates generated by EM converge? Several convergence results are obtained under conditions that are applicable to many practical situations. Two useful special cases are: (a) if the unobserved complete-data specification can be described by a curved exponential family with compact parameter space, all the limit points of any EM sequence are stationary points of the likelihood function; (b) if the likelihood function is unimodal and a certain differentiability condition is satisfied, then any EM sequence converges to the unique maxi-mum likelihood estimate. A list of key properties of the algorithm is included. 1. Introduction. Dempster, Laird and Rubin (1977) (henceforth abbreviated DLR) introduced the EM algorithm for computing maximum likelihood estimates from incom-plete data. The essential ideas underlying the EM algorithm have been presented in special cases by many authors; see DLR for a detailed account. Among them we mention Baum et al. (1970), Hartley and Hocking (1971), Orchard and Woodbury (1972), Sundberg (1974). The DLR paper has made three significant contributions: (i) it recognizes the expectation step (E-step) and the maximization step (M-step) in their general forms, (ii) it gives some theoretical properties of the algorithm, and (iii) it recognizes and gives a wide range of applications in statistics. However, the proof of convergence of EM sequences in DLR contains an error. The implication from (3.13) to (3.14) in their Theorem 2 fails due to an incorrect use of the triangle inequality. Additional comments on this proof are given in Section 2.2. Therefore the convergence of EM sequence as proved in their Theorems 2 and 3 is cast in doubt. Other results on the monotonicity of likelihood sequence and the convergence rate of EM sequence (Theorems 1 and 4 of DLR) remain valid. Despite its slow numerical convergence, the EM algorithm has become a very popular computational method in statistics. Contrary to the general experience in numerical optimization, the implementation of the E-step and M-step is easy for many statistical problems, thanks to the nice form of the complete-data likelihood function. Solutions of the M-step often exist in closed form. In many cases the M-step can be performed with a standard statistical package, thus saving programming time. Another reason for statisti-cians to prefer EM is that it does not require large storage space. These two features are especially attractive to those with free access to small computers. In this paper, instead of patching up the original proof of DLR, we study more broadly two convergence aspects of the EM algorithm. Our approach is to view EM as a special optimization algorithm and to utilize existing results in the optimization literature. Formally we have two sample spaces . and cN and a many-to-one mapping from . to OY. Instead of observing the "complete data" x in ?, we observe the "incomplete data" y},
author = {Wu, C F Jeff},
file = {::},
journal = {Source: The Annals of Statistics The Annals of Statistics},
number = {1},
pages = {95--103},
title = {{On the Convergence Properties of the EM Algorithm'}},
volume = {11},
year = {1983}
}

@Book{Bert05,
  Title                    = {Dynamic Programming and Optimal Control},
  Author                   = {Dimitri P. Bertsekas},
  Publisher                = {Athena Scientific},
  Year                     = {2005},
  Address                  = {Belmont, MA, USA},
  Edition                  = {3rd},
  Volume                   = {I}
}
@inproceedings{r_max,
abstract = {R-max is a very simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time. In R-max, the agent always maintains a complete, but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model. The model is initialized in an optimistic fashion: all actions in all states return the maximal possible reward (hence the name). During execution, it is updated based on the agent's observations. R-max improves upon several previous algo-rithms: (1) It is simpler and more general than Kearns and Singh's E 3 algorithm, covering zero-sum stochastic games. (2) It has a built-in mechanism for resolving the exploration vs. exploitation dilemma. (3) It formally justifies the " optimism under uncertainty " bias used in many RL algorithms. (4) It is simpler, more general, and more efficient than Brafman and Tennenholtz's LSG algorithm for learning in single controller stochastic games. (5) It generalizes the algorithm by Monderer and Tennenholtz for learning in repeated games. (6) It is the only algorithm for learning in repeated games, to date, which is provably efficient, considerably improving and simplifying previous algorithms by Banos and by Megiddo.},
author = {Brafman, Ronen I. and Tennenholtz, Moshe},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.1162/153244303765208377},
isbn = {1532-4435},
issn = {10450823},
pages = {953--958},
title = {{R-MAX - A general polynomial time algorithm for near-optimal reinforcement learning}},
year = {2001}
}
@inproceedings{Strehl06,
abstract = {For a Markov Decision Process with finite state (size S) and action$\backslash$nspaces (size A per state), we propose a new algorithm---Delayed Q-Learning.$\backslash$nWe prove it is PAC, achieving near optimal performance except for$\backslash$n�(SA) timesteps using O(SA) space, improving on the �(S2A) bounds$\backslash$nof best previous algorithms. This result proves efficient reinforcement$\backslash$nlearning is possible without learning a model of the MDP from experience.$\backslash$nLearning takes place from a single continuous thread of experience---no$\backslash$nresets nor parallel sampling is used. Beyond its smaller storage$\backslash$nand experience requirements, Delayed Q-learning's per-experience$\backslash$ncomputation cost is much less than that of previous PAC algorithms.},
author = {Strehl, Alexander L. and Li, Lihong and Wiewiora, Eric and Langford, John and Littman, Michael L.},
booktitle = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
doi = {10.1145/1143844.1143955},
isbn = {1595933832},
pages = {881--888},
title = {{PAC model-free reinforcement learning}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143955},
year = {2006}
}

@article{Kearns99,
abstract = {In this paper, we address two issues of long-standing interest in the reinforcement learning literature. First, what kinds of performance guarantees can be made for Q-learning after only a nite number of actions? Second, what quantitative comparisons can be made between Q-learning and model-based (indirect) approaches, which use experience to estimate next-state distributions for on-line value iteration? We rst show that both Q-learning and the indirect approach enjoy rather rapid convergence to the optimal policy as a function of the number of state transitions observed. In particular, on the order of onlyN log1= = logN + log log1= transitions are su cient for both algorithms to come within of the optimal policy, in an idealized model that assumes the observed transitions are well-mixed" throughout an N -state MDP. Thus, the two approaches have roughly the same sample complexity. Perhaps surprisingly, this sample complexity is far less than what is required for the model-based approach to actually construct a good approximation to the next-state distribution. The result also shows that the amount of memory required by the model-based approach is closer to N than to N 2 . For either approach, to remove the assumption that the observed transitions are well-mixed, we consider a model in which the transitions are determined by a xed, arbitrary exploration policy. Bounds on the number of transitions required in order to achieve a desired level of performance are then related to the stationary distribution and mixing time of this policy.},
author = {Kearns, Michael and Singh, Satinder P.},
doi = {no DOI, URL correct},
isbn = {0-262-11245-0},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 11},
pages = {996--1002},
title = {{Finite-Sample Convergence Rates for Q-Learning and Indirect Algorithms}},
volume = {12},
year = {1999}
}
@book{Bellman1957,
abstract = {Dynamic Programming is a recursive method for solving sequential decision problems (hereafter abbre- viated as SDP). Also known as backward induction, it is used to find optimal decision rules in games against nature and subgame perfect equilibria of dynamic multi-agent games, and competitive equilib- ria in dynamic economic models. Dynamic programming has enabled economists to formulate and solve a huge variety of problems involving sequential decision making under uncertainty, and as a result it is now widely regarded as the single most important tool in economics. Section 2 provides a brief history of dynamic programming. Section 3 discusses some of the main theoretical results underlying dynamic programming, and its relation to game theory and optimal control theory. Section 4 provides a brief survey on numerical dynamic programming. Section 5 surveys the experimental and econometric literature that uses dynamic programming to construct empirical models economic behavior.},
author = {Bellman, R},
booktitle = {Princeton University Press Princeton New Jersey},
doi = {10.1108/eb059970},
isbn = {978-0-691-07951-6},
issn = {00029092},
keywords = {dynamic{\_}programming},
mendeley-tags = {dynamic{\_}programming},
pages = {342},
title = {{Dynamic Programming}},
url = {http://ajae.oxfordjournals.org/cgi/doi/10.2307/1241949},
volume = {70},
year = {1957}
}


@article{Liu2017,
abstract = {Policy gradient methods have been successfully applied to many complex reinforcement learning problems. However, policy gradient methods suffer from high variance, slow convergence, and inefficient exploration. In this work, we introduce a maximum entropy policy optimization framework which explicitly encourages parameter exploration, and show that this framework can be reduced to a Bayesian inference problem. We then propose a novel Stein variational policy gradient method (SVPG) which combines existing policy gradient methods and a repulsive functional to generate a set of diverse but well-behaved policies. SVPG is robust to initialization and can easily be implemented in a parallel manner. On continuous control problems, we find that implementing SVPG on top of REINFORCE and advantage actor-critic algorithms improves both average return and data efficiency.},
archivePrefix = {arXiv},
arxivId = {1704.02399},
author = {Liu, Yang and Ramachandran, Prajit and Liu, Qiang and Peng, Jian},
eprint = {1704.02399},
file = {:Users/matows/Desktop/1704.02399.pdf:pdf},
title = {{Stein Variational Policy Gradient}},
url = {http://arxiv.org/abs/1704.02399},
year = {2017}
}
@article{Liu2016,
abstract = {Although optimization can be done very efficiently using gradient-based optimiza-tion these days, Bayesian inference or probabilistic sampling has been considered to be much more difficult. Stein variational gradient descent (SVGD) is a new particle-based inference method derived using a functional gradient descent for minimizing KL divergence without explicit parametric assumptions. SVGD can be viewed a natural counterpart of gradient descent for optimization, and in fact exactly reduces to the typical gradient ascent for MAP using only a single particle. This short paper gives a brief introduction to SVGD, and discusses its theoretical foundation and applications.},
archivePrefix = {arXiv},
arxivId = {1704.07520},
author = {Liu, Qiang},
eprint = {1704.07520},
journal = {nips},
pages = {1--6},
title = {{Stein Variational Gradient Descent: Theory and Applications}},
url = {https://pdfs.semanticscholar.org/35e4/e5b8f5697e5771a06e923bda8e40e663b6c5.pdf},
year = {2016}
}
@article{bdl,
abstract = {Deep learning has attracted tremendous attention from researchers in various fields of information engineering such as AI, computer vision, and language processing [Kalchbrenner and Blunsom, 2013; Krizhevsky et al., 2012; Mnih et al., 2013], but also from more traditional sciences such as physics, biology, and manufacturing [Anjos et al., 2015; Baldi et al., 2014; Bergmann et al., 2014]. Neural networks, image processing tools such as convolutional neural networks, sequence processing models such as recurrent neural networks, and regularisation tools such as dropout, are used extensively. However, fields such as physics, biology, and manufacturing are ones in which representing model uncertainty is of crucial importance [Ghahramani, 2015; Krzywinski and Altman, 2013]. With the recent shift in many of these fields towards the use of Bayesian uncertainty [Herzog and Ostwald, 2013; Nuzzo, 2014; Trafimow and Marks, 2015], new needs arise from deep learning. In this work we develop tools to obtain practical uncertainty estimates in deep learning, casting recent deep learning tools as Bayesian models without changing either the models or the optimisation. In the first part of this thesis we develop the theory for such tools, providing applications and illustrative examples. We tie approximate inference in Bayesian models to dropout and other stochastic regularisation techniques, and assess the approximations empirically. We give example applications arising from this connection between modern deep learning and Bayesian modelling such as active learning of image data and data efficient deep reinforcement learning. We further demonstrate the method's practicality through a survey of recent applications making use of the suggested tools in language applications, medical diagnostics, bioinformatics, image processing, and autonomous driving. In the second part of the thesis we explore its theoretical implications, and the insights stemming from the link between Bayesian modelling and deep learning. We discuss what determines model uncertainty properties, analyse the approximate inference analytically in the linear case, and theoretically examine various priors such as spike and slab priors.},
author = {Gal, Yarin},
journal = {PhD Thesis},
number = {October},
title = {{Uncertainty in Deep Learning}},
year = {2016}
}
@article{Zhao2012,
abstract = {Policy gradient is a useful model-free reinforcement learning approach, but it tends to suffer from instability of gradient estimates. In this paper, we analyze and improve the stability of policy gradient methods. We first prove that the variance of gradient estimates in the PGPE (policy gradients with parameter-based exploration) method is smaller than that of the classical REINFORCE method under a mild assumption. We then derive the optimal baseline for PGPE, which contributes to further reducing the variance. We also theoretically show that PGPE with the optimal baseline is more preferable than REINFORCE with the optimal baseline in terms of the variance of gradient estimates. Finally, we demonstrate the usefulness of the improved PGPE method through experiments. {\textcopyright} 2011 Elsevier Ltd.},
author = {Zhao, Tingting and Hachiya, Hirotaka and Niu, Gang and Sugiyama, Masashi},
doi = {10.1016/j.neunet.2011.09.005},
isbn = {1879-2782 (Electronic)$\backslash$n0893-6080 (Linking)},
issn = {08936080},
journal = {Neural Networks},
keywords = {Policy gradients,Policy gradients with parameter-based exploration,Reinforcement learning,Variance reduction},
pages = {118--129},
pmid = {22019189},
title = {{Analysis and improvement of policy gradient estimation}},
volume = {26},
year = {2012}
}
@article{Hoffman2012,
abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
archivePrefix = {arXiv},
arxivId = {1206.7051},
author = {Hoffman, Matt and Blei, David M. and Wang, Chong and Paisley, John},
doi = {citeulike-article-id:10852147},
eprint = {1206.7051},
file = {:Users/matows/Desktop/HoffmanBleiWangPaisley2013.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
pages = {1303--1347},
pmid = {19926898},
title = {{Stochastic Variational Inference}},
url = {http://arxiv.org/abs/1206.7051},
volume = {14},
year = {2013}
}

@article{Toussaint06b,
abstract = {Inference in Markov Decision Processes has recently received interest as a means to infer goals of an observed action, policy recognition, and also as a tool to compute policies. A particularly interesting aspect of the approach is that any existing inference technique in DBNs now becomes available for answering behavioral questionsincluding those on continuous, factorial, or hierarchical state representations. Here we present an Expectation Maximization algorithm for computing optimal policies. Unlike previous approaches we can show that this actually optimizes the discounted expected future return for arbitrary reward functions and without assuming an ad hoc finite total time. The algorithm is generic in that any inference technique can be utilized in the E-step. We demonstrate this for exact inference on a discrete maze and Gaussian belief state propagation in continuous stochastic optimal control problems.},
author = {Toussaint, Marc and Storkey, Amos},
doi = {10.1145/1143844.1143963},
isbn = {1595933832},
journal = {Proceedings of the 23rd international conference on Machine learning  - ICML '06},
pages = {945--952},
title = {{Probabilistic inference for solving discrete and continuous state Markov Decision Processes}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143963},
year = {2006}
}
@article{Zhang2017,
abstract = {Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a high-dimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully used in various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.},
archivePrefix = {arXiv},
arxivId = {1711.05597},
author = {Zhang, Cheng and Butepage, Judith and Kjellstrom, Hedvig and Mandt, Stephan},
eprint = {1711.05597},
file = {:Users/matows/Desktop/1711.05597.pdf:pdf},
pages = {1--22},
title = {{Advances in Variational Inference}},
url = {http://arxiv.org/abs/1711.05597},
year = {2017}
}


@article{vi_tutorial,
abstract = {This tutorial describes the mean-field variational Bayesian approximation to inference in graphical models, using modern machine learning terminology rather than statistical physics concepts. It begins by seeking to find an approximate mean- field distribution close to the target joint in the KL-divergence sense. It then derives local node updates and reviews the recent Variational Message Passing framework.},
author = {Fox, C W and Roberts, S J},
doi = {10.1007/s10462-011-9236-8},
isbn = {1367-4811 (Electronic) 1367-4803 (Linking)},
issn = {0269-2821},
journal = {Artificial Intelligence Review},
keywords = {mean-field,tutorial,variational bayes},
pages = {1--11},
pmid = {25123903},
title = {{A tutorial on variational Bayesian inference}},
url = {papers2://publication/uuid/1B6D2DDA-67F6-4EEE-9720-2907FFB14789},
year = {2010}
}
@article{Sutton00,
abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the ¯rst time that a version of policy iteration with arbitrary di{\textregistered}erentiable function approximation is convergent to a locally optimal policy.},
author = {Sutton, Richard S. and Mcallester, David and Singh, Satinder and Mansour, Yishay},
doi = {10.1.1.37.9714},
isbn = {0-262-19450-3},
issn = {0047-2875},
journal = {Advances in Neural Information Processing Systems 12},
pages = {1057--1063},
title = {{Policy Gradient Methods for Reinforcement Learning with Function Approximation}},
year = {1999}
}
@inproceedings{Hoffman2009,
abstract = {We derive a new expectation maximization algorithm for policy optimization in linear Gaussian Markov decision processes, where the reward function is parameterized in terms of a flexible mixture of Gaussians. This approach exploits both analytical tractability and numerical optimization. Consequently, on the one hand, it is more flexible and general than closed-form solutions, such as the widely used linear quadratic Gaussian (LQG) controllers. On the other hand, it is more accurate and faster than optimization methods that rely on approximation and simulation. Partial analytical solutions (though costly) eliminate the need for simulation and, hence, avoid approximation error. The experiments will show that for the same cost of computation, policy optimization methods that rely on analytical tractability have higher value than the ones that rely on simulation. {\textcopyright} 2009 by the authors.},
author = {Hoffman, Matt and {De Freitas}, Nando and Doucet, Arnaud and Peters, Jan},
booktitle = {Journal of Machine Learning Research},
issn = {15324435},
pages = {232--239},
title = {{An expectation maximization algorithm for continuous Markov decision processes with arbitrary rewards}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84862277035{\&}partnerID=tZOtx3y1},
volume = {5},
year = {2009}
}
@article{Furmston2010,
abstract = {We consider reinforcement learning as solv-ing a Markov decision process with unknown transition distribution. Based on interac-tion with the environment, an estimate of the transition matrix is obtained from which the optimal decision policy is formed. The classical maximum likelihood point estimate of the transition model does not reflect the uncertainty in the estimate of the transition model and the resulting policies may con-sequently lack a sufficient degree of explo-ration. We consider a Bayesian alternative that maintains a distribution over the tran-sition so that the resulting policy takes into account the limited experience of the envi-ronment. The resulting algorithm is formally intractable and we discuss two approximate solution methods, Variational Bayes and Ex-pectation Propagation.},
author = {Furmston, Thomas and Barber, D},
issn = {15324435},
journal = {In AISTATS},
pages = {241--248},
title = {{Variational Methods For Reinforcement Learning}},
year = {2010}
}
@inproceedings{Todorov2008,
abstract = {Optimal control and estimation are dual in the LQG setting, as Kalman discovered, however this duality has proven difficult to extend beyond LQG. Here we obtain a more natural form of LQG duality by replacing the Kalman-Bucy filter with the information filter. We then generalize this result to non-linear stochastic systems, discrete stochastic systems, and deterministic systems. All forms of duality are established by relating exponentiated costs to probabilities. Unlike the LQG setting where control and estimation are in one-to-one correspondence, in the general case control turns out to be a larger problem class than estimation and only a sub-class of control problems have estimation duals. These are problems where the Bellman equation is intrinsically linear. Apart from their theoretical significance, our results make it possible to apply estimation algorithms to control problems and vice versa.},
author = {Todorov, Emanuel},
booktitle = {Proceedings of the IEEE Conference on Decision and Control},
doi = {10.1109/CDC.2008.4739438},
isbn = {9781424431243},
issn = {01912216},
pages = {4286--4292},
title = {{General duality between optimal control and estimation}},
year = {2008}
}

@incollection{Todorov06,
title = {Linearly-solvable Markov decision problems},
author = {Emanuel Todorov},
booktitle = {Advances in Neural Information Processing Systems 19},
editor = {B. Sch\"{o}lkopf and J. C. Platt and T. Hoffman},
pages = {1369--1376},
year = {2007},
publisher = {MIT Press},
}


@article{Wang2013,
abstract = {Mean-field variational methods are widely used for approximate posterior inference in many probabilistic models. In a typical application, mean-field methods approximately compute the posterior with a coordinate-ascent optimization algorithm. When the model is conditionally conjugate, the coordinate updates are easily derived and in closed form. However, many models of interest---like the correlated topic model and Bayesian logistic regression---are nonconjuate. In these models, mean-field methods cannot be directly applied and practitioners have had to develop variational algorithms on a case-by-case basis. In this paper, we develop two generic methods for nonconjugate models, Laplace variational inference and delta method variational inference. Our methods have several advantages: they allow for easily derived variational algorithms with a wide class of nonconjugate models; they extend and unify some of the existing algorithms that have been derived for specific models; and they work well on real-world datasets. We studied our methods on the correlated topic model, Bayesian logistic regression, and hierarchical Bayesian logistic regression.},
archivePrefix = {arXiv},
arxivId = {1209.4360},
author = {Wang, Chong and Blei, David M.},
eprint = {1209.4360},
file = {:Users/matows/Desktop/1209.4360.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {laplace approximations,nonconjugate models,the multivari-,variational inference},
number = {April},
pages = {1005−1031},
title = {{Variational Inference in Nonconjugate Models}},
url = {http://arxiv.org/abs/1209.4360},
volume = {14},
year = {2013}
}
@article{auto_bayes,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
eprint = {1312.6114},
isbn = {1312.6114v10},
issn = {1312.6114v10},
journal = {Proceedings of the 2nd International Conference on Learning Representations (ICLR)},
title = {{Auto-Encoding Variational Bayes PPT}},
url = {http://arxiv.org/abs/1312.6114},
year = {2014}
}
@misc{vi_review,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
archivePrefix = {arXiv},
arxivId = {1601.00670},
author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
booktitle = {Journal of the American Statistical Association},
doi = {10.1080/01621459.2017.1285773},
eprint = {1601.00670},
isbn = {1601.00670},
issn = {1537274X},
keywords = {Algorithms,Computationally intensive methods,Statistical computing},
number = {518},
pages = {859--877},
title = {{Variational Inference: A Review for Statisticians}},
volume = {112},
year = {2017}
}
@article{Tzikas2008,
abstract = {The influence of this Thomas Bayes' work was immense. It was from here that "Bayesian" ideas first spread through the mathematical world, as Bayes's own article was ignored until 1780 and played no important role in scientific debate until the 20th century. It was also this article of Laplace's that introduced the mathematical techniques for the asymptotic analysis of posterior distributions that are still employed today. And it was here that the earliest example of optimum estimation can be found, the derivation and characterization of an estimator that minimized a particular measure of posterior expected loss. After more than two centuries, we mathematicians, statisticians cannot only recognize our roots in this masterpiece of our science, we can still learn from it.},
author = {Tzikas, D.G. and a.C. Likas and Galatsanos, N.P.},
doi = {10.1109/MSP.2008.929620},
isbn = {1053-5888},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
number = {November},
pages = {131--146},
title = {{The variational approximation for Bayesian inference}},
volume = {25},
year = {2008}
}
@article{Ghavamzadeh2006,
abstract = {Policy gradient methods are reinforcement learning algorithms that adapt a pa- rameterized policy by following a performance gradient estimate. Conventional policy gradientmethods useMonte-Carlo techniques to estimate this gradient, and improve the policy by adjusting the parameters in the direction of that gradient es- timate. SinceMonte Carlo methods tend to have high variance, a large number of samples is required to attain accurate estimates, resulting in slow convergence. In this paper, we propose a Bayesian framework that models the policy gradient as a Gaussian process. This reduces the number of samples needed to obtain accurate gradient estimates. Moreover, estimates of the natural gradient and a measure of the uncertainty in the gradient estimates are provided at little extra cost.},
author = {Ghavamzadeh, Mohammad and Engel, Yaakov},
isbn = {9780262195683},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems (NIPS) 19},
pages = {457--464},
title = {{Bayesian Policy Gradient Algorithms}},
url = {http://books.nips.cc/papers/files/nips19/NIPS2006{\_}0865.pdf},
year = {2006}
}
@phdthesis{kakadephd,
abstract = {This thesis is a detailed investigation into the following question: how much data must an agent collect in order to perform "reinforcement learning" successfully? This question is analogous to the classical issue of the sample complexity in supervised learning, but is harder because of the increased realism of the reinforcement learning setting. This thesis summarizes recent sample complexity results in the reinforcement learning literature and builds on these results to provide novel algorithms with strong performance guarantees.

We focus on a variety of reasonable performance criteria and sampling models by which agents may access the environment. For instance, in a policy search setting, we consider the problem of how much simulated experience is required to reliably choose a "good" policy among a restricted class of policies \Pi (as in Kearns, Mansour, and Ng [2000]). In a more online setting, we consider the case in which an agent is placed in an environment and must follow one unbroken chain of experience with no access to "offline" simulation (as in Kearns and Singh [1998]). 

We build on the sample based algorithms suggested by Kearns, Mansour, and Ng [2000]. Their sample complexity bounds have no dependence on the size of the state space, an exponential dependence on the planning horizon time, and linear dependence on the complexity of \Pi . We suggest novel algorithms with more restricted guarantees whose sample complexities are again independent of the size of the state space and depend linearly on the complexity of the policy class \Pi , but have only a polynomial dependence on the horizon time. We pay particular attention to the tradeoffs made by such algorithms. 
},
author = {Kakade, Sham},
year = {2013},
title = {On the Sample Complexity of Reinforcement Learning},
school = {Gatsby Computational Neuroscience Unit}
}

@article{Watkins89,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989). We show thatQ-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
author = {Watkins, Christopher J. C. H. and Dayan, Peter},
doi = {10.1007/BF00992698},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Machine Learning},
number = {3-4},
pages = {279--292},
title = {{Q-learning}},
url = {http://link.springer.com/10.1007/BF00992698},
volume = {8},
year = {1992}
}

@article{Abbeel04,
abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our. algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
archivePrefix = {arXiv},
arxivId = {1206.5264},
author = {Abbeel, Pieter and Ng, Andrew Y.},
booktitle = {Twenty-first international conference on Machine learning - ICML '04},
doi = {10.1145/1015330.1015430},
eprint = {1206.5264},
isbn = {1581138285},
issn = {0028-0836},
pages = {1},
pmid = {25719670},
title = {{Apprenticeship learning via inverse reinforcement learning}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015430},
year = {2004}
}
@article{Boyan95,
abstract = {A straightforward approach to the curse of dimensionality in reinforcement learning and dynamic programming is to replace the lookup table with a generalizing function approximator such as a neural net. Although this has been successful in the domain of backgammon, there is no guarantee of convergence. In this paper, we show that the combination of dynamic programming and function approximation is not robust, and in even very benign cases, may produce an entirely wrong policy. We then introduce Grow-Support, a new algorithm which is safe from divergence yet can still reap the benefits of successful generalization.},
author = {Boyan, Justin a and Moore, Andrew W.},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 7},
pages = {369--376},
pmid = {5558611976696861513},
title = {{Generalization in Reinforcement Learning: Safely Approximating the Value Function}},
year = {1995}
}
@article{Tsitsiklis1997,
abstract = {We discuss the temporal-difference learning algorithm, as applied to approximating the cost-to-go function of an infinite-horizon discounted Markov chain. The algorithm we analyze updates parameters of a linear function approximator online during a single endless trajectory of an irreducible aperiodic Markov chain with a finite or infinite state space. We present a proof of convergence (with probability one), a characterization of the limit of convergence, and a bound on the resulting approximation error. Furthermore, our analysis is based on a new line of reasoning that provides new intuition about the dynamics of temporal-difference learning. In addition to proving new and stronger positive results than those previously available, we identify the significance of online updating and potential hazards associated with the use of nonlinear function approximators. First, we prove that divergence may occur when updates are not based on trajectories of the Markov chain. This fact reconciles positive and negative results that have been discussed in the literature, regarding the soundness of temporal-difference learning. Second, we present an example illustrating the possibility of divergence when temporal difference learning is used in the presence of a nonlinear function approximator},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tsitsiklis, John N. and {Van Roy}, Benjamin},
doi = {10.1109/9.580874},
eprint = {arXiv:1011.1669v3},
isbn = {9781605582054},
issn = {00189286},
journal = {IEEE Transactions on Automatic Control},
keywords = {Dynamic programming,Function approximation,Markov chains,Neuro-dynamic programming,Renforcement learning,Temporal-difference learning},
number = {5},
pages = {674--690},
pmid = {25246403},
title = {{An analysis of temporal-difference learning with function approximation}},
volume = {42},
year = {1997}
}
@article{Schaal97,
abstract = {By now it is widely accepted that learning a task from scratch, i.e., without any prior knowledge, is a daunting undertaking. Humans, however, rarely at- tempt to learn from scratch. They extract initial biases as well as strategies how to approach a learning problem from instructions and/or demonstrations of other humans. For learning control, this paper investigates how learning from demonstration can be applied in the context of reinforcement learning. We consider priming the Q-function, the value function, the policy, and the model of the task dynamics as possible areas where demonstrations can speed up learning. In general nonlinear learning problems, only model-based rein- forcement learning shows significant speed-up after a demonstration, while in the special case of linear quadratic regulator (LQR) problems, all methods profit from the demonstration. In an implementation of pole balancing on a complex anthropomorphic robot arm, we demonstrate that, when facing the complexities of real signal processing, model-based reinforcement learning offers the most robustness for LQR problems. Using the suggested methods, the robot learns pole balancing in just a single trial after a 30 second long demonstration of the human instructor.},
author = {Schaal, Stefan},
doi = {10.1016/j.robot.2004.03.001},
isbn = {1558604863},
issn = {1049-5258},
journal = {Advances in Neural Information Processing Systems},
number = {9},
pages = {1040--1046},
pmid = {11540378},
title = {{Learning from Demonstration}},
url = {http://wwwiaim.ira.uka.de/users/rogalla/WebOrdnerMaterial/ml-robotlearning.pdf},
year = {1997}
}
@article{Baird95,
abstract = {A number of reinforcement learning algorithms have been developed that are guaranteed to converge to the optimal solution when used with lookup tables. It is shown, however, that these algorithms can easily become unstable when implemented directly with a general function-approximation system, such as a sigmoidal multilayer perceptron, a radial-basisfunction system, a memory-based learning system, or even a linear function-approximation system. A new class of algorithms, residual gradient algorithms, is proposed, which perform gradient descent on the mean squared Bellman residual, guaranteeing convergence. It is shown, however, that they may learn very slowly in some cases. A larger class of algorithms, residual algorithms, is proposed that has the guaranteed convergence of the residual gradient algorithms, yet can retain the fast learning speed of direct algorithms. In fact, both direct and residual gradient algorithms are shown to be special cases of residual algorithms, and it is shown that residual algorithms can combine the advantages of each approach. The direct, residual gradient, and residual forms of value iteration, Qlearning, and advantage learning are all presented. Theoretical analysis is given explaining the properties these algorithms have, and simulation results are given that demonstrate these properties.},
author = {Baird, Leemon},
doi = {10.1.1.48.3256},
issn = {00043702},
journal = {Machine Learning-International Workshop Then Conference-},
number = {July},
pages = {30--37},
title = {{Residual algorithms: Reinforcement learning with function approximation}},
year = {1995}
}
@inproceedings{Nachum2017,
abstract = {We establish a new connection between value and policy based reinforcement learning (RL) based on a relationship between softmax temporal value consistency and policy optimality under entropy regularization. Specifically, we show that softmax consistent action values satisfy a strong consistency property with optimal entropy regularized policy probabilities along any action sequence, regardless of provenance. From this observation, we develop a new RL algorithm, Path Consistency Learning (PCL), that minimizes inconsistency measured along multi-step action sequences extracted from both on- and off-policy traces. We subsequently deepen the relationship by showing how a single model can be used to represent both a policy and its softmax action values. Beyond eliminating the need for a separate critic, the unification demonstrates how policy gradients can be stabilized via self-bootstrapping from both on- and off-policy data. An experimental evaluation demonstrates that both algorithms can significantly outperform strong actor-critic and Q-learning baselines across several benchmark tasks.},
archivePrefix = {arXiv},
arxivId = {1702.08892},
author = {Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
booktitle = {NIPS},
eprint = {1702.08892},
number = {Nips},
pages = {1--11},
title = {{Bridging the Gap Between Value and Policy Based Reinforcement Learning}},
url = {http://arxiv.org/abs/1702.08892},
year = {2017}
}

@phdthesis{Watkins89,
author = {Watkins, Christopher},
year = {1989},
month = {01},
title = {Learning From Delayed Rewards}
}
@inproceedings{Harmvan2009,
abstract = {Expected SARSA does not use the Q of the next state/action combination but the weighted average over all actions. Is has better convegence than SARSA and has some advantages over Q-learning (which is off-policy) too.},
author = {Harm van, Seijen and Hado van, Hasselt and Whiteson, Shimon and Wiering, Marco},
booktitle = {2009 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning, ADPRL 2009 - Proceedings},
doi = {10.1109/ADPRL.2009.4927542},
isbn = {9781424427611},
pages = {177--184},
title = {{A theoretical and empirical analysis of expected sarsa}},
year = {2009}
}

@article{survey_deep_rl,
abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep {\$}Q{\$}-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
archivePrefix = {arXiv},
arxivId = {1708.05866},
author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
doi = {10.1109/MSP.2017.2743240},
eprint = {1708.05866},
isbn = {9781424469178},
issn = {10535888},
journal = {IEEE Signal Processing Magazine Special Issue on Deep Learning for Image Understanding},
pages = {1--14},
pmid = {25719670},
title = {{A Brief Survey of Deep Reinforcement Learning}},
year = {2017}
}

@article{learning_to_comm,
abstract = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
archivePrefix = {arXiv},
arxivId = {1605.06676},
author = {Foerster, Jakob N. and Assael, Yannis M. and de Freitas, Nando and Whiteson, Shimon},
doi = {10.7551/ecal},
eprint = {1605.06676},
file = {:Users/matows/Desktop/1605.06676.pdf:pdf},
isbn = {2004012439},
issn = {10495258},
pages = {1--13},
title = {{Learning to Communicate with Deep Multi-Agent Reinforcement Learning}},
url = {http://arxiv.org/abs/1605.06676},
year = {2016}
}

@article{drqn,
abstract = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting $\backslash$textit{\{}Deep Recurrent Q-Network{\}} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
archivePrefix = {arXiv},
arxivId = {1507.06527},
author = {Hausknecht, Matthew and Stone, Peter},
eprint = {1507.06527},
file = {:Users/matows/Desktop/1507.06527.pdf:pdf},
isbn = {9781577357520},
issn = {9781577357520},
journal = {AAAI},
pages = {29--37},
title = {{Deep Recurrent Q-Learning for Partially Observable MDPs}},
url = {http://arxiv.org/abs/1507.06527},
year = {2015}
}

@article{sarsa1,
abstract = {On large problems, reinforcement learning systems must use parame- terized function approximators such as neural networks in order to gen- eralize between similar situations and actions. In these cases there are no strong theoretical results on the accuracy of convergence, and com- putational results have been mixed. In particular, Boyan and Moore reported at last year'smeeting a series of negative results in attempting to apply dynamic programming together with function approximation to simple control problems with continuous state spaces. In this paper, we present positive results for all the control tasks they attempted, and for one that is significantly larger. The most important differences are that we used sparse-coarse-coded function approximators (CMACs) whereas they used mostly global function approximators, and that we learned online whereas they learned offline. Boyan and Moore and others have suggested that the problems they encountered could be solved by using actual outcomes (“rollouts”), as in classical Monte Carlo methods, and as in the TD($\lambda$) algorithm when $\lambda$ = 1. However, in our experiments this always resulted in substantially poorer perfor- mance. We conclude that reinforcement learning can work robustly in conjunction with function approximators, and that there is little justification at present for avoiding the case of general $\lambda$.},
author = {Sutton, R S},
doi = {10.1.1.51.4764},
isbn = {9784907764272},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1038--1044},
title = {{Generalization in Reinforcement Learning : Successful Examples Using Sparse Coarse Coding}},
volume = {8},
year = {1996}
}
@article{sarsa2,
author = {A. Rummery, G and Niranjan, Mahesan},
year = {1994},
month = {11},
title = {On-Line Q-Learning Using Connectionist Systems},
booktitle = {Technical Report CUED/F-INFENG/TR 166}
}
@article{Singh2000,
abstract = {An important application of reinforcement learning$\backslash$n(RL) is to finite-state control problems and one of the most$\backslash$ndifficult problems in learning for control is balancing the$\backslash$nexploration/exploitation tradeoff. Existing theoretical results for$\backslash$nRL give very little guidance on reasonable ways to perform$\backslash$nexploration. In this paper, we examine the convergence of$\backslash$nsingle-step on-policy RL algorithms for control. On-policy$\backslash$nalgorithms cannot separate exploration from learning and therefore$\backslash$nmust confront the exploration problem directly. We prove convergence$\backslash$nresults for several related on-policy algorithms with both decaying$\backslash$nexploration and persistent exploration. We also provide examples of$\backslash$nexploration strategies that can be followed during learning that$\backslash$nresult in convergence to both optimal values and optimal policies.},
author = {Singh, Satinder and Jaakkola, Tommi and Littman, Michael L. and Szepesv{\'{a}}ri, Csaba},
doi = {10.1023/A:1007678930559},
issn = {08856125},
journal = {Machine Learning},
number = {3},
pages = {287--308},
title = {{Convergence results for single-step on-policy reinforcement-learning algorithms}},
volume = {38},
year = {2000}
}
@inproceedings{Harmvan2009,
abstract = {Expected SARSA does not use the Q of the next state/action combination but the weighted average over all actions. Is has better convegence than SARSA and has some advantages over Q-learning (which is off-policy) too.},
author = {Harm van, Seijen and Hado van, Hasselt and Whiteson, Shimon and Wiering, Marco},
booktitle = {2009 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning, ADPRL 2009 - Proceedings},
doi = {10.1109/ADPRL.2009.4927542},
isbn = {9781424427611},
pages = {177--184},
title = {{A theoretical and empirical analysis of expected sarsa}},
year = {2009}
}
@article{Amari1998,
abstract = {When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed.},
author = {Amari, Shun-ichi},
doi = {10.1162/089976698300017746},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
number = {2},
pages = {251--276},
title = {{Natural Gradient Works Efficiently in Learning}},
url = {http://www.mitpressjournals.org/doi/10.1162/089976698300017746},
volume = {10},
year = {1998}
}
@article{Kakade2001,
abstract = {We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as deo/ned by Sutton et al. 9. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kakade, Sham Machandranath},
doi = {10.1.1.19.8165},
eprint = {arXiv:1011.1669v3},
isbn = {9780874216561},
issn = {0717-6163},
journal = {Advances in neural information processing systems},
pages = {1531--1538},
pmid = {15003161},
title = {{A Natural Policy Gradient}},
url = {http://www-2.cs.cmu.edu/Groups/NIPS/NIPS2001/papers/psgz/CN11.ps.gz},
year = {2001}
}
@article{reinforce,
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
author = {Williams, Ronald J.},
doi = {10.1023/A:1022672621406},
isbn = {0885-6125},
issn = {15730565},
journal = {Machine Learning},
keywords = {Reinforcement learning,connectionist networks,gradient descent,mathematical analysis},
number = {3},
pages = {229--256},
pmid = {903},
title = {{Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning}},
volume = {8},
year = {1992}
}
@book{cox1974theoretical,
  title={Theoretical Statistics},
  author={Cox, D.R. and Hinkley, D.V.},
  isbn={9780412124204},
  lccn={74180431},
  url={https://books.google.co.uk/books?id=cgihSgAACAAJ},
  year={1974},
  publisher={Chapman and Hall}
}
@article{scorevi,
abstract = {Consider a computer system having a CPU that feeds jobs to two input/output (I/O) devices having different speeds. Let {\&}thgr; be the fraction of jobs routed to the first I/O device, so that 1 - {\&}thgr; is the fraction routed to the second. Suppose that {\&}agr; = {\&}agr;({\&}thgr;) is the steady-sate amount of time that a job spends in the system. Given that {\&}thgr; is a decision variable, a designer might wish to minimize {\&}agr;({\&}thgr;) over {\&}thgr;. Since {\&}agr;({\textperiodcentered}) is typically difficult to evaluate analytically, Monte Carlo optimization is an attractive methodology. By analogy with deterministic mathematical programming, efficient Monte Carlo gradient estimation is an important ingredient of simulation-based optimization algorithms. As a consequence, gradient estimation has recently attracted considerable attention in the simulation community. It is our goal, in this article, to describe one efficient method for estimating gradients in the Monte Carlo setting, namely the likelihood ratio method (also known as the efficient score method). This technique has been previously described (in less general settings than those developed in this article) in [6, 16, 18, 21]. An alternative gradient estimation procedure is infinitesimal perturbation analysis; see [11, 12] for an introduction. While it is typically more difficult to apply to a given application than the likelihood ratio technique of interest here, it often turns out to be statistically more accurate. In this article, we first describe two important problems which motivate our study of efficient gradient estimation algorithms. Next, we will present the likelihood ratio gradient estimator in a general setting in which the essential idea is most transparent. The section that follows then specializes the estimator to discrete-time stochastic processes. We derive likelihood-ratio-gradient estimators for both time-homogeneous and non-time homogeneous discrete-time Markov chains. Later, we discuss likelihood ratio gradient estimation in continuous time. As examples of our analysis, we present the gradient estimators for time-homogeneous continuous-time Markov chains; non-time homogeneous continuous-time Markov chains; semi-Markov processes; and generalized semi-Markov processes. (The analysis throughout these sections assumes the performance measure that defines {\&}agr;({\&}thgr;) corresponds to a terminating simulation.) Finally, we conclude the article with a brief discussion of the basic issues that arise in extending the likelihood ratio gradient estimator to steady-state performance measures.},
author = {Glynn, Peter W.},
doi = {10.1145/84537.84552},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM (Association of Computing Machinery)},
number = {10},
pages = {75--84},
title = {{Likelihood ratio gradient estimation for stochastic systems}},
volume = {33},
year = {1990}
}
@article{Schulman16,
abstract = {Policy gradient methods are an appealing approach in reinforcement learning be-cause they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the diffi-culty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substan-tially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD($\lambda$). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomo-tion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy repre-sentations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experi-ence required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02438v5},
author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael I and Abbeel, Pieter},
eprint = {arXiv:1506.02438v5},
journal = {International Conference on Learning Representations},
title = {{High Dimensional Continuous Control Using Generalized Advantage Estimation}},
url = {https://arxiv.org/pdf/1506.02438.pdf},
year = {2016}
}
@article{Precup2000,
abstract = {Eligibility traces have been shown to speed re- $\backslash$n$\backslash$ninforcement learning, to make it more robust $\backslash$n$\backslash$nto hidden states, and to provide a link between $\backslash$n$\backslash$nMonte Carlo and temporal-difference methods. $\backslash$n$\backslash$nHere we generalize eligibility traces to off-policy $\backslash$n$\backslash$nlearning, in which one learns about a policy dif- $\backslash$n$\backslash$nferent from the policy that generates the data. $\backslash$n$\backslash$nOff-policy methods can greatly multiply learn- $\backslash$n$\backslash$ning, as many policies can be learned about from $\backslash$n$\backslash$nthe same data stream, and have been identified $\backslash$n$\backslash$nas particularly useful for learning about subgoals $\backslash$n$\backslash$nand temporally extended macro-actions. In this $\backslash$n$\backslash$npaper we consider the off-policy version of the $\backslash$n$\backslash$npolicy evaluation problem, for which only one $\backslash$n$\backslash$neligibility trace algorithm is known, a Monte $\backslash$n$\backslash$nCarlo method. We analyze and compare this and $\backslash$n$\backslash$nfour new eligibility trace algorithms, emphasiz- $\backslash$n$\backslash$ning their relationships to the classical statistical $\backslash$n$\backslash$ntechnique known as importance sampling. Our $\backslash$n$\backslash$nmain results are 1) to establish the consistency $\backslash$n$\backslash$nand bias properties of the new methods and 2) to $\backslash$n$\backslash$nempirically rank the new methods, showing im- $\backslash$n$\backslash$nprovement over one-step and Monte Carlo meth- $\backslash$n$\backslash$nods. Our results are restricted to model-free, $\backslash$n$\backslash$ntable-lookup methods and to offline updating (at $\backslash$n$\backslash$nthe end of each episode) although several of the $\backslash$n$\backslash$nalgorithms could be applied more generally.},
author = {Precup, Doina and Sutton, Richard S and Singh, Satinder P},
isbn = {1-55860-707-2},
journal = {ICML '00: Proceedings of the Seventeenth International Conference on Machine Learning},
keywords = {reinforcement},
pages = {759--766},
title = {{Eligibility Traces for Off-Policy Policy Evaluation}},
year = {2000}
}
@article{Mahmood2015,
abstract = {Importance sampling is an essential component of model-free off-policy learning algorithms. Weighted importance sampling (WIS) is gener-ally considered superior to ordinary importance sampling but, when combined with function ap-proximation, it has hitherto required computa-tional complexity that is O(n 2) or more in the number of features. In this paper we introduce new off-policy learning algorithms that obtain the benefits of WIS with O(n) computational complexity. Our algorithms maintain for each component of the parameter vector a measure of the extent to which that component has been used in previous examples. This measure is used to determine component-wise step sizes, merging the ideas of stochastic gradient descent and sam-ple averages. We present our main WIS-based algorithm first in an intuitive acausal form (the forward view) and then derive a causal algorithm using eligibility traces that is equivalent but more efficient (the backward view). In three small experiments, our algorithms performed signifi-cantly better than prior O(n) algorithms for off-policy policy evaluation. We also show that our adaptive step-size technique alone can improve the performance of on-policy algorithms such as TD() and true online TD().},
author = {Mahmood, Ashique Rupam and Sutton, Richard S},
isbn = {978-0-9966431-0-8},
journal = {Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, {\{}UAI{\}} 2015, July 12-16, 2015, Amsterdam, The Netherlands},
pages = {552--561},
title = {{Off-policy learning based on weighted importance sampling with linear computational complexity}},
url = {http://auai.org/uai2015/proceedings/uai-2015-proceedings.pdf},
year = {2015}
}
@article{Geist14,
 author = {Geist, Matthieu and Scherrer, Bruno},
 title = {Off-policy Learning with Eligibility Traces: A Survey},
 journal = {J. Mach. Learn. Res.},
 issue_date = {January 2014},
 volume = {15},
 number = {1},
 month = jan,
 year = {2014},
 issn = {1532-4435},
 pages = {289--333},
 numpages = {45},
 url = {http://dl.acm.org/citation.cfm?id=2627435.2627445},
 acmid = {2627445},
 publisher = {JMLR.org},
 keywords = {eligibility traces, off-policy learning, reinforcement learning, value function estimation}
} 

@article{Degris12,
    author = {Thomas Degris, Martha White and Richard S. Sutton},
    title = {Linear off-policy actor-critic},
    booktitle = {In International Conference on Machine Learning},
    year = {2012}
}
@article{bdl,
abstract = {Deep learning has attracted tremendous attention from researchers in various fields of information engineering such as AI, computer vision, and language processing [Kalchbrenner and Blunsom, 2013; Krizhevsky et al., 2012; Mnih et al., 2013], but also from more traditional sciences such as physics, biology, and manufacturing [Anjos et al., 2015; Baldi et al., 2014; Bergmann et al., 2014]. Neural networks, image processing tools such as convolutional neural networks, sequence processing models such as recurrent neural networks, and regularisation tools such as dropout, are used extensively. However, fields such as physics, biology, and manufacturing are ones in which representing model uncertainty is of crucial importance [Ghahramani, 2015; Krzywinski and Altman, 2013]. With the recent shift in many of these fields towards the use of Bayesian uncertainty [Herzog and Ostwald, 2013; Nuzzo, 2014; Trafimow and Marks, 2015], new needs arise from deep learning. In this work we develop tools to obtain practical uncertainty estimates in deep learning, casting recent deep learning tools as Bayesian models without changing either the models or the optimisation. In the first part of this thesis we develop the theory for such tools, providing applications and illustrative examples. We tie approximate inference in Bayesian models to dropout and other stochastic regularisation techniques, and assess the approximations empirically. We give example applications arising from this connection between modern deep learning and Bayesian modelling such as active learning of image data and data efficient deep reinforcement learning. We further demonstrate the method's practicality through a survey of recent applications making use of the suggested tools in language applications, medical diagnostics, bioinformatics, image processing, and autonomous driving. In the second part of the thesis we explore its theoretical implications, and the insights stemming from the link between Bayesian modelling and deep learning. We discuss what determines model uncertainty properties, analyse the approximate inference analytically in the linear case, and theoretically examine various priors such as spike and slab priors.},
author = {Gal, Yarin},
journal = {PhD Thesis},
number = {October},
title = {{Uncertainty in Deep Learning}},
year = {2016}
}
@article{svg,
abstract = {We present a unified framework for learning continuous control policies using backpropagation. It supports stochastic control by treating stochasticity in the Bellman equation as a deterministic function of exogenous noise. The product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions. We use learned models but only require observations from the environment in- stead of observations from model-predicted trajectories, minimizing the impact of compounded model errors. We apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation. One of these variants, SVG(1), shows the effectiveness of learning models, value functions, and policies simultaneously in continuous domains.},
archivePrefix = {arXiv},
arxivId = {1510.09142},
author = {Heess, Nicolas and Wayne, Greg and Silver, David and Lillicrap, Timothy and Tassa, Yuval and Erez, Tom},
eprint = {1510.09142},
file = {:Users/matows/Library/Application Support/Mendeley Desktop/Downloaded/Heess et al. - 2015 - Learning Continuous Control Policies by Stochastic Value Gradients.pdf:pdf},
issn = {10495258},
pages = {1--13},
title = {{Learning Continuous Control Policies by Stochastic Value Gradients}},
url = {http://arxiv.org/abs/1510.09142},
year = {2015}
}
@article{rezende14,
  title = 	 {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
  author = 	 {Danilo Jimenez Rezende and Shakir Mohamed and Daan Wierstra},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {1278--1286},
  year = 	 {2014},
  editor = 	 {Eric P. Xing and Tony Jebara},
  volume = 	 {32},
  number =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/rezende14.pdf},
  url = 	 {http://proceedings.mlr.press/v32/rezende14.html},
  abstract = 	 {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning.   Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound.  We develop stochastic backpropagation – rules for gradient backpropagation through stochastic variables – and   derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models.  We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to  generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.}
}
@article{openai,
  author    = {Greg Brockman and
               Vicki Cheung and
               Ludwig Pettersson and
               Jonas Schneider and
               John Schulman and
               Jie Tang and
               Wojciech Zaremba},
  title     = {OpenAI Gym},
  journal   = {CoRR},
  volume    = {abs/1606.01540},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.01540},
  archivePrefix = {arXiv},
  eprint    = {1606.01540},
  timestamp = {Wed, 07 Jun 2017 14:41:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BrockmanCPSSTZ16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@techreport{mountaincar,
    author = {Andrew William Moore},
    title = {Efficient Memory-based Learning for Robot Control},
    institution = {University of Cambridge},
    year = {1990}
}
@article{Engel2005,
abstract = {The Octopus arm is a highly versatile and complex limb. How the Octo- pus controls such a hyper-redundant arm (not to mention eight of them!) is as yet unknown. Robotic arms based on the same mechanical prin- ciples may render present day robotic arms obsolete. In this paper, we tackle this control problem using an online reinforcement learning al- gorithm, based on a Bayesian approach to policy evaluation known as Gaussian process temporal difference (GPTD) learning. Our substitute for the real arm is a computer simulation of a 2-dimensional model of an Octopus arm. Even with the simplifications inherent to this model, the state space we face is a high-dimensional one. We apply a GPTD- based algorithm to this domain, and demonstrate its operation on several learning tasks of varying degrees of difficulty},
author = {Engel, Y and Szabo, P and Volkinshtein, D},
booktitle = {Proc. NIPS},
isbn = {9780262232531},
issn = {10495258},
keywords = {Gaussian,NIPS05},
title = {{Learning to Control an Octopus Arm with Gaussian Process Temporal Difference Methods}},
url = {http://papers.nips.cc/paper/2785-learning-to-control-an-octopus-arm-with-gaussian-process-temporal-difference-methods.pdf},
volume = {c},
year = {2005}
}
@article{Riedmiller05, 
author={M. Riedmiller}, 
booktitle={2005 IEEE International Conference on Systems, Man and Cybernetics}, 
title={Neural reinforcement learning to swing-up and balance a real pole}, 
year={2005}, 
volume={4}, 
number={}, 
pages={3191-3196 Vol. 4}, 
keywords={learning (artificial intelligence);neural nets;nonlinear control systems;control policy;inverted pendulum;learning process;neural Q-value function;neural network;neural reinforcement learning;reinforcement learning controller;Acceleration;Algorithm design and analysis;Learning;Multi-layer neural network;Multilayer perceptrons;Neural networks;Regression tree analysis;State-space methods;Stochastic systems;Stress}, 
doi={10.1109/ICSMC.2005.1571637}, 
ISSN={1062-922X}, 
month={Oct},}
@article{variational_gaussian,
abstract = {The variational approximation of posterior distributions by multivariate gaussians has been much less popular in the machine learning community compared to the corresponding approximation by factorizing distributions. This is for a good reason: the gaussian approximation is in general plagued by an number of variational parameters to be optimized, N being the number of random variables. In this letter, we discuss the relationship between the Laplace and the variational approximation, and we show that for models with gaussian priors and factorizing likelihoods, the number of variational parameters is actually . The approach is applied to gaussian process regression with nongaussian likelihoods.},
author = {Opper, Manfred and Archambeau, C{\'{e}}dric},
doi = {10.1162/neco.2008.08-07-592},
issn = {0899-7667},
journal = {Neural Computation},
number = {3},
pages = {786--792},
pmid = {18785854},
title = {{The Variational Gaussian Approximation Revisited}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.2008.08-07-592},
volume = {21},
year = {2009}
}
@article{Konda2003,
abstract = {In this paper, we propose and analyze a class of actor-critic algorithms. These ar two-time-scale algorithms in which the critic uses temporal difference (TD) learning with a linearly parameterized approximation architecture, and the actor is updated in an approximate gradient direction based on information provided by the critic. We show that the features for the critic should ideally span a subspace prescribed by the choice of parameterization of the actor. We study actorcritic algorithms for Markov decision processes with general state and action spaces. We state and prove two results regarding their convergence.},
archivePrefix = {arXiv},
arxivId = {1607.07086},
author = {Konda, Vijay R and Tsitsiklis, John N},
doi = {10.1137/S0363012901385691},
eprint = {1607.07086},
isbn = {0363-0129},
issn = {0363-0129},
journal = {Control Optim},
keywords = {actor-critic algorithms,markov decision processes,reinforcement learning,stochas-},
number = {4},
pages = {1143--1166},
pmid = {21222527},
title = {{Actor-Critic Algorithms}},
volume = {42},
year = {2003}
}

@article{AC_survey, 
author={I. Grondman and L. Busoniu and G. A. D. Lopes and R. Babuska}, 
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)}, 
title={A Survey of Actor-Critic Reinforcement Learning: Standard and Natural Policy Gradients}, 
year={2012}, 
volume={42}, 
number={6}, 
pages={1291-1307}, 
doi={10.1109/TSMCC.2012.2218595}, 
ISSN={1094-6977}, 
month={Nov},}
@article{Geist10,
author="Geist, Matthieu
and Pietquin, Olivier",
editor="Torra, Vicen{\c{c}}
and Narukawa, Yasuo
and Daumas, Marc",
title="Revisiting Natural Actor-Critics with Value Function Approximation",
booktitle="Modeling Decisions for Artificial Intelligence",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="207--218",
abstract="Actor-critics architectures have become popular during the last decade in the field of reinforcement learning because of the introduction of the policy gradient with function approximation theorem. It allows combining rationally actor-critic architectures with value function approximation and therefore addressing large-scale problems. Recent researches led to the replacement of policy gradient by a natural policy gradient, improving the efficiency of the corresponding algorithms. However, a common drawback of these approaches is that they require the manipulation of the so-called advantage function which does not satisfy any Bellman equation. Consequently, derivation of actor-critic algorithms is not straightforward. In this paper, we re-derive theorems in a way that allows reasoning directly with the state-action value function (or Q-function) and thus relying on the Bellman equation again. Consequently, new forms of critics can easily be integrated in the actor-critic framework.",
isbn="978-3-642-16292-3"
}
@article{Seijen2014,
abstract = {TD(lambda) is a core algorithm of modern reinforcement learning. Its appeal comes from its equivalence to a clear and conceptually simple forward view, and the fact that it can be implemented online in an inexpensive manner. However, the equivalence between TD(lambda) and the forward view is exact only for the off-line version of the algorithm (in which updates are made only at the end of each episode). In the online version of TD(lambda) (in which updates are made at each step, which generally performs better and is always used in applications) the match to the forward view is only approximate. In a sense this is unavoidable for the conventional forward view, as it itself presumes that the estimates are unchanging during an episode. In this paper we introduce a new forward view that takes into account the possibility of changing estimates and a new variant of TD(lambda) that exactly achieves it. Our algorithm uses a new form of eligibility trace similar to but different from conventional accumulating and replacing traces. The overall computational complexity is the same as TD(lambda), even when using function approximation. In our empirical comparisons, our algorithm outperformed TD(lambda) in all of its variations. It seems, by adhering more truly to the original goal of TD(lambda)—matching an intuitively clear forward view even in the online case—that we have found a new algorithm that simply improves on classical TD(lambda).},
author = {Seijen, Harm and Sutton, Rich},
doi = {10.13140/2.1.1456.2568},
isbn = {9781634393973},
issn = {1938-7228},
journal = {Proceedings of the 31st International Conference on Machine Learning},
pages = {692--700},
title = {{True Online TD(lambda)}},
url = {http://proceedings.mlr.press/v32/seijen14.html},
volume = {32},
year = {2014}
}
@article{Leng2009,
abstract = {Temporal difference learning and eligibility traces are two mechanisms for solving reinforcement learning problems. The temporal difference technique bootstraps the state value or state-action value at every step as with dynamic programming, and learns by sampling episodes from experience as in the Monte Carlo approach. Eligibility traces is a mechanism that offers a means for recording the degree of which state is eligible for undergoing learning process. This paper aims to investigate the underlying mechanism of eligibility traces strategies using on-policy and off-policy learning algorithms. In doing so, the performance metrics can be obtained by defining the learning problem in a simulation environment, in conjunction with different learning algorithms. However, measuring learning performance and analysing sensibility are very expensive because such performance metrics can only be obtained by running an experiment with different parameter values. This paper proposes a comparative study for analysing the mechanism of eligibility traces. The objective of this paper is to compare and investigate the influences on performance caused by those different approaches.{\textcopyright} 2009 - IOS Press and the authors. {\textcopyright} 2009 - IOS Press and the authors. All rights reserved.},
author = {Leng, Jinsong and Fyfe, Colin and Jain, Lakhmi C.},
doi = {10.3233/IFS-2009-0416},
issn = {10641246},
journal = {Journal of Intelligent and Fuzzy Systems},
keywords = {Agent,Decision-making,Eligibility traces,Temporal difference learning},
number = {1-2},
pages = {73--82},
title = {{Experimental analysis on Sarsa($\lambda$) and Q($\lambda$) with different eligibility traces strategies}},
volume = {20},
year = {2009}
}

@article{Schulman15b,
  title = 	 {Trust Region Policy Optimization},
  author = 	 {John Schulman and Sergey Levine and Pieter Abbeel and Michael Jordan and Philipp Moritz},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1889--1897},
  year = 	 {2015},
  editor = 	 {Francis Bach and David Blei},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schulman15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/schulman15.html},
  abstract = 	 {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
}
@article{Peters2010,
abstract = {Policy search is a successful approach to reinforcement learning. However, policy improvements often result in the loss of information. Hence, it has been marred by premature convergence and implausible solutions. As first suggested in the context of covariant policy gradients (Bagnell and Schneider 2003), many of these problems may be addressed by constraining the infor- mation loss. In this paper, we continue this path of rea- soning and suggest the Relative Entropy Policy Search (REPS) method. The resulting method differs signif- icantly from previous policy gradient approaches and yields an exact update step. It works well on typical reinforcement learning benchmark problems. Introduction},
author = {Peters, Jan and M{\"{u}}lling, K and Alt{\"{u}}n, Y},
journal = {Artificial Intelligence},
pages = {1607--1612},
title = {{Relative Entropy Policy Search}},
url = {http://www-clmc.usc.edu/publications/P/Peters{\_}POTTNCOAIPGAT{\_}2010.pdf},
year = {2010}
}
@article{Lillicrap15,
  author    = {Timothy P. Lillicrap and
               Jonathan J. Hunt and
               Alexander Pritzel and
               Nicolas Heess and
               Tom Erez and
               Yuval Tassa and
               David Silver and
               Daan Wierstra},
  title     = {Continuous control with deep reinforcement learning},
  journal   = {CoRR},
  volume    = {abs/1509.02971},
  year      = {2015},
  url       = {http://arxiv.org/abs/1509.02971},
  archivePrefix = {arXiv},
  eprint    = {1509.02971},
  timestamp = {Wed, 07 Jun 2017 14:40:26 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LillicrapHPHETS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{control_variates,
author="Glynn, Peter W.
and Szechtman, Roberto",
editor="Fang, Kai-Tai
and Niederreiter, Harald
and Hickernell, Fred J.",
title="Some New Perspectives on the Method of Control Variates",
booktitle="Monte Carlo and Quasi-Monte Carlo Methods 2000",
year="2002",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="27--49",
abstract="The method of control variates is one of the most widely used variance reduction techniques associated with Monte Carlo simulation. This paper studies the method of control variates from several different viewpoints, and establishes new connections between the method of control variates and: conditional Monte Carlo, antithetics, rotation sampling, stratification, and nonparametric maximum likelihood. We also develop limit theory for the method of control variates under weak assumptions on the estimator of the optimal control coefficient.",
isbn="978-3-642-56046-0"
}
@article{Gu17,
  author    = {Shixiang Gu and
               Timothy P. Lillicrap and
               Zoubin Ghahramani and
               Richard E. Turner and
               Bernhard Sch{\"{o}}lkopf and
               Sergey Levine},
  title     = {Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient
               Estimation for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1706.00387},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.00387},
  archivePrefix = {arXiv},
  eprint    = {1706.00387},
  timestamp = {Mon, 03 Jul 2017 13:29:02 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GuLGTSL17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Nachum17,
title = {Bridging the Gap Between Value and Policy Based Reinforcement Learning},
author  = {Ofir Nachum and Mohammad Norouzi and Kelvin Xu and Dale Schuurmans},
year  = {2017},
URL = {https://arxiv.org/pdf/1702.08892.pdf}
}
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@inproceedings{Gal2015Modern,
Author = {Yarin Gal and Zoubin Ghahramani},
Title = {On Modern Deep Learning and Variational Inference},
Year = {2015},
booktitle = {Advances in Approximate Bayesian Inference workshop, NIPS},
}
@inproceedings{Gal2016Dropout,
Author = {Yarin Gal and Zoubin Ghahramani},
Title = {Dropout as a {B}ayesian Approximation: Representing Model Uncertainty in Deep Learning},
booktitle = {Proceedings of the 33rd International Conference on Machine Learning (ICML-16)},
year={2016}
}
@inproceedings{KendallGalCipolla2017Multi,
   author = {Alex Kendall and Yarin Gal and Roberto Cipolla},
    title = "{Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics}",
booktitle = {arXiv:1705.07115},
     year = 2017,
    month = may,
}
@article{Zhang17,
  author    = {Cheng Zhang and
               Judith B{\"{u}}tepage and
               Hedvig Kjellstr{\"{o}}m and
               Stephan Mandt},
  title     = {Advances in Variational Inference},
  journal   = {CoRR},
  volume    = {abs/1711.05597},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.05597},
  archivePrefix = {arXiv},
  eprint    = {1711.05597},
  timestamp = {Sat, 02 Dec 2017 12:40:56 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-05597},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{doersch2016tutorial,
  title = {Tutorial on Variational Autoencoders},
  author = {Doersch, Carl},
  journal = {arXiv preprint arXiv:1606.05908},
  year = {2016},
}

@ARTICLE{Dempster77maximumlikelihood,
    author = {A. P. Dempster and N. M. Laird and D. B. Rubin},
    title = {Maximum likelihood from incomplete data via the EM algorithm},
    journal = {Journal of the Royal Statistical Society, Series B},
    year = {1977},
    volume = {39},
    number = {1},
    pages = {1--38}
}
@article{Dayan97,
author = { Peter Dayan  and  Geoffrey E. Hinton },
title = {Using Expectation-Maximization for Reinforcement Learning},
journal = {Neural Computation},
volume = {9},
number = {2},
pages = {271-278},
year = {1997},
doi = {10.1162/neco.1997.9.2.271},

URL = { 
        https://doi.org/10.1162/neco.1997.9.2.271
    
},
eprint = { 
        https://doi.org/10.1162/neco.1997.9.2.271
    
},
    abstract = { We discuss Hinton's (1989) relative payoff procedure (RPP), a static reinforcement learning algorithm whose foundation is not stochastic gradient ascent. We show circumstances under which applying the RPP is guaranteed to increase the mean return, even though it can make large changes in the values of the parameters. The proof is based on a mapping between the RPP and a form of the expectation-maximization procedure of Dempster, Laird, and Rubin (1977). }
}
@book{Murphy2012,
 author = {Murphy, Kevin P.},
 title = {Machine Learning: A Probabilistic Perspective},
 year = {2012},
 isbn = {0262018020, 9780262018029},
 publisher = {The MIT Press}
} 
@book{Bishop2006,
 author = {Bishop, Christopher M.},
 title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
 year = {2006},
 isbn = {0387310738},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA}
} 
@book{Jordan1999,
 editor = {Jordan, Michael I.},
 title = {Learning in Graphical Models},
 year = {1999},
 isbn = {0-262-60032-3},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA}
} 
@Article{Jordan1999a,
author="Jordan, Michael I.
and Ghahramani, Zoubin
and Jaakkola, Tommi S.
and Saul, Lawrence K.",
title="An Introduction to Variational Methods for Graphical Models",
journal="Machine Learning",
year="1999",
month="Nov",
day="01",
volume="37",
number="2",
pages="183--233",
abstract="This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.",
issn="1573-0565",
doi="10.1023/A:1007665907178",
url="https://doi.org/10.1023/A:1007665907178"
}
@article{Liu2012,
 author = {Liu, Qiang and Ihler, Alexander},
 title = {Belief Propagation for Structured Decision Making},
 booktitle = {Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence},
 series = {UAI'12},
 year = {2012},
 isbn = {978-0-9749039-8-9},
 location = {Catalina Island, CA},
 pages = {523--532},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=3020652.3020708},
 acmid = {3020708},
 publisher = {AUAI Press},
 address = {Arlington, Virginia, United States}
} 
@article{Kiselev2011,
 author = {Kiselev, Igor and Poupart, Pascal},
 title = {Policy Optimization by Marginal-map Probabilistic Inference in Generative Models},
 booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems},
 series = {AAMAS '14},
 year = {2014},
 isbn = {978-1-4503-2738-1},
 location = {Paris, France},
 pages = {1611--1612},
 numpages = {2},
 url = {http://dl.acm.org/citation.cfm?id=2615731.2616087},
 acmid = {2616087},
 publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
 address = {Richland, SC},
 keywords = {dynamic bayesian network, bounds, quality, performance, planning, pomdp, probabilistic inference, variational approximation}
} 
@article{Bernardo03,
    author = {J. M. Bernardo and M. J. Bayarri and J. O. Berger and A. P. Dawid and D. Heckerman and A. F. M. Smith and M. West (eds and Matthew J. Beal and Zoubin Ghahramani},
    title = {The Variational Bayesian EM Algorithm for Incomplete Data: with Application to Scoring Graphical Model Structures},
    year = {2003}
}
@article{ranganath14,
  title = 	 {{Black Box Variational Inference}},
  author = 	 {Rajesh Ranganath and Sean Gerrish and David Blei},
  booktitle = 	 {Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {814--822},
  year = 	 {2014},
  editor = 	 {Samuel Kaski and Jukka Corander},
  volume = 	 {33},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Reykjavik, Iceland},
  month = 	 {22--25 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v33/ranganath14.pdf},
  url = 	 {http://proceedings.mlr.press/v33/ranganath14.html},
  abstract = 	 {Variational inference has become a widely used method to approximate posteriors in complex latent variables models.  However, deriving a variational inference algorithm generally requires significant model-specific analysis. These efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand.  In this paper, we present a “black box” variational inference algorithm, one that can be quickly applied to many models with little additional derivation.  Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution.  We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations.  We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.}
}
@article{Kober09,
title = {Policy Search for Motor Primitives in Robotics},
author = {Jens Kober and Jan R. Peters},
booktitle = {Advances in Neural Information Processing Systems 21},
editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
pages = {849--856},
year = {2009},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3545-policy-search-for-motor-primitives-in-robotics.pdf}
}

@inproceedings{Koller00,
 author = {Koller, Daphne and Parr, Ronald},
 title = {Policy Iteration for Factored MDPs},
 booktitle = {Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence},
 series = {UAI'00},
 year = {2000},
 isbn = {1-55860-709-9},
 location = {Stanford, California},
 pages = {326--334},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2073946.2073985},
 acmid = {2073985},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 

@article{Bengio2013,
 author = {Bengio, Yoshua and Yao, Li and Alain, Guillaume and Vincent, Pascal},
 title = {Generalized Denoising Auto-encoders As Generative Models},
 booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1},
 series = {NIPS'13},
 year = {2013},
 location = {Lake Tahoe, Nevada},
 pages = {899--907},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2999611.2999712},
 acmid = {2999712},
 publisher = {Curran Associates Inc.},
 address = {USA},
}
@article{Johnson16,
title = {Composing graphical models with neural networks for structured representations and fast inference},
author = {Johnson, Matthew and Duvenaud, David K and Wiltschko, Alex and Adams, Ryan P and Datta, Sandeep R},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {2946--2954},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6379-composing-graphical-models-with-neural-networks-for-structured-representations-and-fast-inference.pdf}
}






@InProceedings{FPG,
  title = 	 {{F}ourier Policy Gradients},
  author = 	 {Fellows, Matthew and Ciosek, Kamil and Whiteson, Shimon},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1486--1495},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/fellows18a/fellows18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/fellows18a.html},
  abstract = 	 {We propose a new way of deriving policy gradient updates for reinforcement learning. Our technique, based on Fourier analysis, recasts integrals that arise with expected policy gradients as convolutions and turns them into multiplications. The obtained analytical solutions allow us to capture the low variance benefits of EPG in a broad range of settings. For the critic, we treat trigonometric and radial basis functions, two function families with the universal approximation property. The choice of policy can be almost arbitrary, including mixtures or hybrid continuous-discrete probability distributions. Moreover, we derive a general family of sample-based estimators for stochastic policy gradients, which unifies existing results on sample-based approximation. We believe that this technique has the potential to shape the next generation of policy gradient approaches, powered by analytical results.}
}

@article{Wainwright2008,
 author = {Wainwright, Martin J. and Jordan, Michael I.},
 title = {Graphical Models, Exponential Families, and Variational Inference},
 journal = {Found. Trends Mach. Learn.},
 issue_date = {January 2008},
 volume = {1},
 number = {1-2},
 month = jan,
 year = {2008},
 issn = {1935-8237},
 pages = {1--305},
 numpages = {305},
 url = {http://dx.doi.org/10.1561/2200000001},
 doi = {10.1561/2200000001},
 acmid = {1498841},
 publisher = {Now Publishers Inc.},
 address = {Hanover, MA, USA}
} 
@book{intro_to_exp,
author = {Barndorff-Nielsen, Ole},
year = {1978},
month = {04},
title = {Information and exponential families : in statistical theory / O. Barndorff-Nielsen},
booktitle = {SERBIULA (sistema Librum 2.0)}
}
@article{Wainwright03tree,
    author = {Martin J. Wainwright and Tommi S. Jaakkola and Alan S. Willsky},
    title = {Tree-reweighted belief propagation algorithms and approximate ML estimation by pseudo-moment matching},
    booktitle = {In AISTATS},
    year = {2003}
}
@article{Wainright06bounds, 
author={M. J. Wainwright and M. I. Jordan}, 
journal={IEEE Transactions on Signal Processing}, 
title={Log-determinant relaxation for approximate inference in discrete Markov random fields}, 
year={2006}, 
volume={54}, 
number={6}, 
pages={2099-2109}, 
keywords={Gaussian processes;Markov processes;graph theory;inference mechanisms;optimisation;signal denoising;Gaussian bound;approximate inference;arbitrary graphs;denoising problems;discrete Markov random fields;discrete entropy;graphical model;log-determinant maximization problem;log-determinant relaxation;marginalization problem;signal processing;sum-product algorithm;Graphical models;Markov random fields;Noise reduction;Probability;Random variables;Signal processing;Signal processing algorithms;Statistics;Sum product algorithm;Tree graphs;Belief propagation;Gaussian mixture;Markov random field;denoising;sum-product algorithm}, 
doi={10.1109/TSP.2006.874409}, 
ISSN={1053-587X}, 
month={June},}
@article{Konidaris11a,
	Author = {G.D. Konidaris and S. Osentoski and P.S. Thomas},
	Booktitle = {Proceedings of the Twenty-Fifth Conference on Artificial Intelligence},
	Pages = {380-385},
	Title = {Value Function Approximation in Reinforcement Learning using the {F}ourier Basis},
	Year = 2011,
	Month = {August},
    keywords={Reinforcement Learning},
	url={http://lis.csail.mit.edu/pubs/konidaris-aaai11a.pdf}
}
@article{salimans15,
  title = 	 {Markov Chain Monte Carlo and Variational Inference: Bridging the Gap},
  author = 	 {Tim Salimans and Diederik Kingma and Max Welling},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1218--1226},
  year = 	 {2015},
  editor = 	 {Francis Bach and David Blei},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/salimans15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/salimans15.html},
  abstract = 	 {Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.}
}
@incollection{Kulkarni15,
title = {Deep Convolutional Inverse Graphics Network},
author = {Kulkarni, Tejas D and Whitney, William F. and Kohli, Pushmeet and Tenenbaum, Josh},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2539--2547},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5851-deep-convolutional-inverse-graphics-network.pdf}
}
@article{Doersch2016TutorialOV,
  title={Tutorial on Variational Autoencoders},
  author={Carl Doersch},
  journal={CoRR},
  year={2016},
  volume={abs/1606.05908}
}
@article{van_Hoof16, 
author={H. van Hoof and N. Chen and M. Karl and P. van der Smagt and J. Peters}, 
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
title={Stable reinforcement learning with autoencoders for tactile and visual data}, 
year={2016}, 
volume={}, 
number={}, 
pages={3928-3934}, 
keywords={learning (artificial intelligence);robots;autoencoders;controller design;nonlinear policy learning;reinforcement learning algorithm;robot learning;robot tactile stabilization task;tactile data;tactile feedback;visual data;visual feedback;Aerospace electronics;Decoding;Learning (artificial intelligence);Neural networks;Robot sensing systems;Visualization}, 
doi={10.1109/IROS.2016.7759578}, 
ISSN={}, 
month={Oct},}
@article{watter15,
title = {Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images},
author = {Watter, Manuel and Springenberg, Jost and Boedecker, Joschka and Riedmiller, Martin},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2746--2754},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5964-embed-to-control-a-locally-linear-latent-dynamics-model-for-control-from-raw-images.pdf}
}
@article{Qiang16,
title = {Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm},
author = {Liu, Qiang and Wang, Dilin},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {2378--2386},
year = {2016},
publisher = {Curran Associates, Inc.},
}

@article{starcraft,
  author    = {Oriol Vinyals and
               Timo Ewalds and
               Sergey Bartunov and
               Petko Georgiev and
               Alexander Sasha Vezhnevets and
               Michelle Yeo and
               Alireza Makhzani and
               Heinrich K{\"{u}}ttler and
               John Agapiou and
               Julian Schrittwieser and
               John Quan and
               Stephen Gaffney and
               Stig Petersen and
               Karen Simonyan and
               Tom Schaul and
               Hado van Hasselt and
               David Silver and
               Timothy P. Lillicrap and
               Kevin Calderone and
               Paul Keet and
               Anthony Brunasso and
               David Lawrence and
               Anders Ekermo and
               Jacob Repp and
               Rodney Tsing},
  title     = {StarCraft {II:} {A} New Challenge for Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1708.04782},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.04782},
  archivePrefix = {arXiv},
  eprint    = {1708.04782},
  timestamp = {Tue, 05 Sep 2017 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-04782},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@misc{baselines,
	Author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
	Date-Added = {2018-02-09 21:30:35 +0000},
	Date-Modified = {2018-02-09 21:30:35 +0000},
	Howpublished = {\url{https://github.com/openai/baselines}},
	Journal = {GitHub repository},
	Publisher = {GitHub},
	Title = {OpenAI Baselines},
	Year = {2017}}

@inbook{Karr1993,
	Abstract = {The characteristic function of a random variable is a complex-valued function calculated from its distribution function, but is more tractable in many ways, primarily 			because of its superior smoothness properties. The characteristic function uniquely determines the distribution function, so that recognizing the characteristic function of a random 		variable identifies its distribution function. The density function, if it exists, can be recovered algorithmically from the characteristic function. Characteristic functions convert 			convolution to the simpler operation of pointwise multiplication. Moments of a random variable are derivatives at zero of its characteristic function, while existence of even-order 		derivatives of the characteristic function implies existence of the corresponding moments. Finally, random variables converge in distribution if and only if their characteristic functions 	converge pointwise.},
	Address = {New York, NY},
	Author = {Karr, Alan F.},
	Booktitle = {Probability},
	Doi = {10.1007/978-1-4612-0891-4_7},
	Isbn = {978-1-4612-0891-4},
	Pages = {163--182},
	Publisher = {Springer New York},
	Title = {Characteristic Functions},
	Year = {1993},
	Bdsk-Url-1 = {https://dx.doi.org/10.1007/978-1-4612-0891-4_7}}

@article{alesker2008characterization,
	Author = {Alesker, Semyon and Artstein-Avidan, Shiri and Milman, Vitali},
	Date-Added = {2018-02-07 03:22:55 +0000},
	Date-Modified = {2018-02-07 03:22:55 +0000},
	Journal = {Comptes Rendus Mathematique},
	Number = {11-12},
	Pages = {625--628},
	Publisher = {Elsevier},
	Title = {A characterization of the Fourier transform and related topics},
	Volume = {346},
	Year = {2008}}

@book{nisan2007algorithmic,
	Author = {Nisan, Noam and Roughgarden, Tim and Tardos, Eva and Vazirani, Vijay V},
	Date-Added = {2018-02-06 21:19:12 +0000},
	Date-Modified = {2018-02-07 11:08:22 +0000},
	Publisher = {Cambridge University Press Cambridge},
	Title = {Algorithmic game theory},
	Volume = {1},
	Year = {2007}}

@inproceedings{carr2001reconstruction,
	Author = {Carr, Jonathan C and Beatson, Richard K and Cherrie, Jon B and Mitchell, Tim J and Fright, W Richard and McCallum, Bruce C and Evans, Tim R},
	Booktitle = {Proceedings of the 28th annual conference on Computer graphics and interactive techniques},
	Date-Added = {2018-02-06 20:22:12 +0000},
	Date-Modified = {2018-02-06 20:22:12 +0000},
	Organization = {ACM},
	Pages = {67--76},
	Title = {Reconstruction and representation of 3D objects with radial basis functions},
	Year = {2001}}

@book{buhmann2003radial,
	Author = {Buhmann, Martin D},
	Date-Added = {2018-02-06 20:19:20 +0000},
	Date-Modified = {2018-02-06 20:19:20 +0000},
	Publisher = {Cambridge university press},
	Title = {Radial basis functions: theory and implementations},
	Volume = {12},
	Year = {2003}}

@article{park1991universal,
	Author = {Park, Jooyoung and Sandberg, Irwin W},
	Date-Added = {2018-02-06 19:44:01 +0000},
	Date-Modified = {2018-02-06 19:44:01 +0000},
	Journal = {Neural computation},
	Number = {2},
	Pages = {246--257},
	Publisher = {MIT Press},
	Title = {Universal approximation using radial-basis-function networks},
	Volume = {3},
	Year = {1991}}

@article{epg,
	Author = {Ciosek, Kamil and Whiteson, Shimon},
	Date-Added = {2018-02-01 17:24:22 +0000},
	Date-Modified = {2018-02-01 17:24:36 +0000},
	Featured = {true},
	Journal = {The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)},
	Pdf = {./dwnl/ciosek-whiteson-epg.pdf},
	Title = {{E}xpected {P}olicy {G}radients},
	Year = {2018}}

@article{peters2008reinforcement,
	Author = {Peters, Jan and Schaal, Stefan},
	Date-Added = {2018-02-01 17:22:11 +0000},
	Date-Modified = {2018-02-01 17:22:11 +0000},
	Journal = {Neural networks},
	Number = {4},
	Pages = {682--697},
	Publisher = {Elsevier},
	Title = {Reinforcement learning of motor skills with policy gradients},
	Volume = {21},
	Year = {2008}}

@inproceedings{wu2017scalable,
	Author = {Wu, Yuhuai and Mansimov, Elman and Grosse, Roger B and Liao, Shun and Ba, Jimmy},
	Booktitle = {Advances in neural information processing systems},
	Date-Added = {2018-02-01 17:21:02 +0000},
	Date-Modified = {2018-02-01 17:21:02 +0000},
	Pages = {5285--5294},
	Title = {Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation},
	Year = {2017}}

@inproceedings{schulman2015trust,
	Author = {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
	Booktitle = {International Conference on Machine Learning},
	Date-Added = {2018-02-01 17:18:26 +0000},
	Date-Modified = {2018-02-01 17:18:26 +0000},
	Pages = {1889--1897},
	Title = {Trust region policy optimization},
	Year = {2015}}

@article{lillicrap2015continuous,
	Author = {Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	Date-Added = {2018-02-01 17:17:28 +0000},
	Date-Modified = {2018-02-01 17:17:28 +0000},
	Journal = {arXiv preprint arXiv:1509.02971},
	Title = {Continuous control with deep reinforcement learning},
	Year = {2015}}

@inproceedings{sutton1996generalization,
	Author = {Sutton, Richard S},
	Booktitle = {Advances in neural information processing systems},
	Date-Added = {2018-02-01 17:14:16 +0000},
	Date-Modified = {2018-02-01 17:14:16 +0000},
	Pages = {1038--1044},
	Title = {Generalization in reinforcement learning: Successful examples using sparse coarse coding},
	Year = {1996}}

@inproceedings{van2009theoretical,
	Author = {Van Seijen, Harm and Van Hasselt, Hado and Whiteson, Shimon and Wiering, Marco},
	Booktitle = {Adaptive Dynamic Programming and Reinforcement Learning, 2009. ADPRL'09. IEEE Symposium on},
	Date-Added = {2018-02-01 17:12:31 +0000},
	Date-Modified = {2018-02-01 17:12:31 +0000},
	Organization = {IEEE},
	Pages = {177--184},
	Title = {A theoretical and empirical analysis of Expected Sarsa},
	Year = {2009}}

@article{epg-journal,
	Author = {Ciosek, Kamil and Whiteson, Shimon},
	Date-Added = {2018-01-30 10:30:43 +0000},
	Date-Modified = {2018-02-07 18:17:17 +0000},
	Journal = {journal submission, arXiv preprint arXiv:1801.03326},
	Title = {Expected Policy Gradients for Reinforcement Learning},
	Year = {2018}}


@misc{actor_critic,
	Abstract = {Policy-gradient-based actor-critic algorithms are amongst the most popular algorithms in the reinforcement learning framework. Their advantage of being able to search for optimal policies using low-variance gradient estimates has made them useful in several real-life applications, such as robotics, power control, and finance. Although general surveys on reinforcement learning techniques already exist, no survey is specifically dedicated to actor-critic algorithms in particular. This paper, therefore, describes the state of the art of actor-critic algorithms, with a focus on methods that can work in an online setting and use function approximation in order to deal with continuous state and action spaces. After starting with a discussion on the concepts of reinforcement learning and the origins of actor-critic algorithms, this paper describes the workings of the natural gradient, which has made its way into many actor-critic algorithms over the past few years. A review of several standard and natural actor-critic algorithms is given, and the paper concludes with an overview of application areas and a discussion on open issues.},
	Author = {Grondman, Ivo and Busoniu, Lucian and Lopes, Gabriel A.D. and Babu{\v{s}}ka, Robert},
	Booktitle = {IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews},
	Doi = {10.1109/TSMCC.2012.2218595},
	Isbn = {1094-6977$\backslash$r1558-2442},
	Issn = {10946977},
	Keywords = {Actor-critic,natural gradient,policy gradient,reinforcement learning (RL)},
	Number = {6},
	Pages = {1291--1307},
	Title = {{A survey of actor-critic reinforcement learning: Standard and natural policy gradients}},
	Volume = {42},
	Year = {2012},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/TSMCC.2012.2218595}}

@book{fourieranalysis,
	Abstract = {This first volume, a three-part introduction to the subject, is intended for students with a beginning knowledge of mathematical analysis who are motivated to discover the ideas that shape Fourier analysis. It begins with the simple conviction that Fourier arrived at in the early nineteenth century when studying problems in the physical sciences--that an arbitrary function can be written as an infinite sum of the most basic trigonometric functions. The first part implements this idea in terms of notions of convergence and summability of Fourier series, while highlighting applications such as th. Chapter 1. The Genesis of Fourier Analysis -- Chapter 2. Basic Properties of Fourier Series -- Chapter 3. Convergence of Fourier Series -- Chapter 4. Some Applications of Fourier Series -- Chapter 5. The Fourier Transform on R -- Chapter 6. The Fourier Transform on R[sup(d)] -- Chapter 7. Finite Fourier Analysis -- Chapter 8. Dirichlet's Theorem.},
	Author = {Stein, Elias M. and Shakarchi, Rami},
	Booktitle = {October},
	Isbn = {9780691113845},
	Pages = {326},
	Title = {{Fourier Analysis: An Introduction}},
	Year = {2003}}

@article{mills_ratio,
	Abstract = {Consider the Mills ratio f(x) = (1 - $\Phi$(x))/$\phi$(x), x{\^a}¥ 0, where $\phi$ is the density function of the standard Gaussian law and $\Phi$ its cumulative distribution. We introduce a general procedure to approximate f on the whole [0, {\^a}) which allows to prove interesting properties where f is involved. As applications we present a new proof that 1/. f is strictly convex, and we give new sharp bounds of f involving rational functions, functions with square roots or exponential terms. Also Chernoff type bounds for the Gaussian Q-function are studied. {\textcopyright} 2014 Elsevier Inc.},
	Author = {Gasull, Armengol and Utzet, Frederic},
	Doi = {10.1016/j.jmaa.2014.05.034},
	Issn = {10960813},
	Journal = {Journal of Mathematical Analysis and Applications},
	Keywords = {Error function,Gaussian Q-function,Gaussian law,Mills ratio},
	Number = {2},
	Pages = {1832--1853},
	Title = {{Approximating Mills ratio}},
	Volume = {420},
	Year = {2014},
	Bdsk-Url-1 = {https://dx.doi.org/10.1016/j.jmaa.2014.05.034}}

@article{qlearning,
	Abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989). We show thatQ-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where manyQ values can be changed each iteration, rather than just one.},
	Author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	Doi = {10.1007/BF00992698},
	Isbn = {0885-6125},
	Issn = {0885-6125},
	Journal = {Machine Learning},
	Number = {3-4},
	Pages = {279--292},
	Title = {{Q-learning}},
	Url = {http://link.springer.com/10.1007/BF00992698},
	Volume = {8},
	Year = {1992},
	Bdsk-Url-1 = {http://link.springer.com/10.1007/BF00992698},
	Bdsk-Url-2 = {https://dx.doi.org/10.1007/BF00992698}}

@article{doubleq,
	Abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	Archiveprefix = {arXiv},
	Arxivid = {1509.06461},
	Author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	Doi = {10.1016/j.artint.2015.09.002},
	Eprint = {1509.06461},
	File = {:Users/matows/Desktop/1509.06461.pdf:pdf},
	Isbn = {0004-3702},
	Issn = {00043702},
	Pmid = {26150344},
	Title = {{Deep Reinforcement Learning with Double Q-learning}},
	Url = {http://arxiv.org/abs/1509.06461},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1509.06461},
	Bdsk-Url-2 = {https://dx.doi.org/10.1016/j.artint.2015.09.002}}

@article{alphago,
	Abstract = {Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games.},
	Author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {Van Den Driessche}, George and Graepel, Thore and Hassabis, Demis},
	Doi = {10.1038/nature24270},
	Isbn = {3013372370},
	Issn = {14764687},
	Journal = {Nature},
	Number = {7676},
	Pages = {354--359},
	Pmid = {29052630},
	Title = {{Mastering the game of Go without human knowledge}},
	Volume = {550},
	Year = {2017},
	Bdsk-Url-1 = {https://dx.doi.org/10.1038/nature24270}}

@article{qprop,
	Abstract = {Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.},
	Archiveprefix = {arXiv},
	Arxivid = {1611.02247},
	Author = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E. and Levine, Sergey},
	Eprint = {1611.02247},
	File = {:Users/matows/Desktop/qprop.pdf:pdf},
	Pages = {1--13},
	Title = {{Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic}},
	Url = {http://arxiv.org/abs/1611.02247},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1611.02247}}


@article{reinforce,
	Abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
	Author = {Williams, Ronald J.},
	Doi = {10.1023/A:1022672621406},
	Isbn = {0885-6125},
	Issn = {15730565},
	Journal = {Machine Learning},
	Keywords = {Reinforcement learning,connectionist networks,gradient descent,mathematical analysis},
	Number = {3},
	Pages = {229--256},
	Pmid = {903},
	Title = {{Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning}},
	Volume = {8},
	Year = {1992},
	Bdsk-Url-1 = {https://dx.doi.org/10.1023/A:1022672621406}}

@article{double_vb,
	Abstract = {We propose a simple and effective variational inference algorithm based on stochastic optimi-sation that can be widely applied for Bayesian non-conjugate inference in continuous parameter spaces. This algorithm is based on stochastic ap-proximation and allows for efficient use of gra-dient information from the model joint density. We demonstrate these properties using illustra-tive examples as well as in challenging and di-verse Bayesian inference problems such as vari-able selection in logistic regression and fully Bayesian inference over kernel hyperparameters in Gaussian process regression.},
	Author = {Titsias, Michalis and L{\'{a}}zaro-Gredilla, Miguel},
	Isbn = {9781634393973},
	Journal = {Proceedings of The 31st International Conference on Machine Learning},
	Pages = {1971--1979},
	Title = {{Doubly Stochastic Variational Bayes for non-Conjugate Inference}},
	Url = {http://jmlr.org/proceedings/papers/v32/titsias14},
	Volume = {32},
	Year = {2014},
	Bdsk-Url-1 = {http://jmlr.org/proceedings/papers/v32/titsias14}}
	
@article{learning_multi_comm,
	Abstract = {Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNet, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.},
	Archiveprefix = {arXiv},
	Arxivid = {1605.07736},
	Author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Fergus, Rob},
	Eprint = {1605.07736},
	File = {:Users/matows/Desktop/6398-learning-multiagent-communication-with-backpropagation.pdf:pdf},
	Issn = {10495258},
	Number = {Nips},
	Title = {{Learning Multiagent Communication with Backpropagation}},
	Url = {http://arxiv.org/abs/1605.07736},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1605.07736}}

@article{svb,
	Abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
	Archiveprefix = {arXiv},
	Arxivid = {1401.4082},
	Author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	Doi = {10.1051/0004-6361/201527329},
	Eprint = {1401.4082},
	File = {:Users/matows/Downloads/paper6.pdf:pdf},
	Isbn = {9781634393973},
	Issn = {10495258},
	Pmid = {23459267},
	Title = {{Stochastic Backpropagation and Approximate Inference in Deep Generative Models}},
	Url = {http://arxiv.org/abs/1401.4082},
	Year = {2014},
	Bdsk-Url-1 = {http://arxiv.org/abs/1401.4082},
	Bdsk-Url-2 = {https://dx.doi.org/10.1051/0004-6361/201527329}}

@article{rl_algorithms,
	Abstract = {Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective.What distin- guishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the al- gorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book,we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming.We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.},
	Author = {Szepesv{\'{a}}ri, Csaba},
	Doi = {10.2200/S00268ED1V01Y201005AIM009},
	Isbn = {9781608454921},
	Issn = {1939-4608},
	Journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	Number = {1},
	Pages = {1--103},
	Title = {{Algorithms for Reinforcement Learning}},
	Url = {http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009},
	Volume = {4},
	Year = {2010},
	Bdsk-Url-1 = {http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009},
	Bdsk-Url-2 = {https://dx.doi.org/10.2200/S00268ED1V01Y201005AIM009}}

@article{multiv_normal_cdf1,
	Abstract = {Simulation from the truncated multivariate normal distribution in high dimensions is a recurrent problem in statistical computing, and is typically only feasible using approximate MCMC sampling. In this article we propose a minimax tilting method for exact iid simulation from the truncated multivariate normal distribution. The new methodology provides both a method for simulation and an efficient estimator to hitherto intractable Gaussian integrals. We prove that the estimator possesses a rare vanishing relative error asymptotic property. Numerical experiments suggest that the proposed scheme is accurate in a wide range of setups for which competing estimation schemes fail. We give an application to exact iid simulation from the Bayesian posterior of the probit regression model.},
	Archiveprefix = {arXiv},
	Arxivid = {1603.04166},
	Author = {Botev, Z. I.},
	Doi = {10.1111/rssb.12162},
	Eprint = {1603.04166},
	Issn = {14679868},
	Journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
	Keywords = {Exact simulation,Exponential tilting,Linear inequalities,Multivariate normal distribution,Polytope probabilities,Probit posterior simulation},
	Number = {1},
	Pages = {125--148},
	Title = {{The normal law under linear restrictions: simulation and estimation via minimax tilting}},
	Volume = {79},
	Year = {2017},
	Bdsk-Url-1 = {https://dx.doi.org/10.1111/rssb.12162}}

@article{ipg,
	Abstract = {Off-policy model-free deep reinforcement learning methods using previously collected data can improve sample efficiency over on-policy policy gradient techniques. On the other hand, on-policy algorithms are often more stable and easier to use. This paper examines, both theoretically and empirically, approaches to merging on- and off-policy updates for deep reinforcement learning. Theoretical results show that off-policy updates with a value function estimator can be interpolated with on-policy policy gradient updates whilst still satisfying performance bounds. Our analysis uses control variate methods to produce a family of policy gradient algorithms, with several recently proposed algorithms being special cases of this family. We then provide an empirical comparison of these techniques with the remaining algorithmic details fixed, and show how different mixing of off-policy gradient estimates with on-policy samples contribute to improvements in empirical performance. The final algorithm provides a generalization and unification of existing deep policy gradient techniques, has theoretical guarantees on the bias introduced by off-policy updates, and improves on the state-of-the-art model-free deep RL methods on a number of OpenAI Gym continuous control benchmarks.},
	Archiveprefix = {arXiv},
	Arxivid = {1706.00387},
	Author = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E. and Sch{\"{o}}lkopf, Bernhard and Levine, Sergey},
	Eprint = {1706.00387},
	File = {:Users/matows/Desktop/ipg.pdf:pdf},
	Title = {{Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning}},
	Url = {http://arxiv.org/abs/1706.00387},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1706.00387}}

@article{seq2,
	Abstract = {We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.},
	Archiveprefix = {arXiv},
	Arxivid = {arXiv:1412.7755v2},
	Author = {Ba, Jimmy Lei and Mnih, Volodymyr and Kavukcuoglu, Koray},
	Eprint = {arXiv:1412.7755v2},
	Isbn = {9781424465163},
	Journal = {Iclr},
	Pages = {1--10},
	Title = {{Multiple Object Recognition With Visual Attention}},
	Year = {2015}}

@article{scorevi,
	Abstract = {Consider a computer system having a CPU that feeds jobs to two input/output (I/O) devices having different speeds. Let {\&}thgr; be the fraction of jobs routed to the first I/O device, so that 1 - {\&}thgr; is the fraction routed to the second. Suppose that {\&}agr; = {\&}agr;({\&}thgr;) is the steady-sate amount of time that a job spends in the system. Given that {\&}thgr; is a decision variable, a designer might wish to minimize {\&}agr;({\&}thgr;) over {\&}thgr;. Since {\&}agr;({\textperiodcentered}) is typically difficult to evaluate analytically, Monte Carlo optimization is an attractive methodology. By analogy with deterministic mathematical programming, efficient Monte Carlo gradient estimation is an important ingredient of simulation-based optimization algorithms. As a consequence, gradient estimation has recently attracted considerable attention in the simulation community. It is our goal, in this article, to describe one efficient method for estimating gradients in the Monte Carlo setting, namely the likelihood ratio method (also known as the efficient score method). This technique has been previously described (in less general settings than those developed in this article) in [6, 16, 18, 21]. An alternative gradient estimation procedure is infinitesimal perturbation analysis; see [11, 12] for an introduction. While it is typically more difficult to apply to a given application than the likelihood ratio technique of interest here, it often turns out to be statistically more accurate. In this article, we first describe two important problems which motivate our study of efficient gradient estimation algorithms. Next, we will present the likelihood ratio gradient estimator in a general setting in which the essential idea is most transparent. The section that follows then specializes the estimator to discrete-time stochastic processes. We derive likelihood-ratio-gradient estimators for both time-homogeneous and non-time homogeneous discrete-time Markov chains. Later, we discuss likelihood ratio gradient estimation in continuous time. As examples of our analysis, we present the gradient estimators for time-homogeneous continuous-time Markov chains; non-time homogeneous continuous-time Markov chains; semi-Markov processes; and generalized semi-Markov processes. (The analysis throughout these sections assumes the performance measure that defines {\&}agr;({\&}thgr;) corresponds to a terminating simulation.) Finally, we conclude the article with a brief discussion of the basic issues that arise in extending the likelihood ratio gradient estimator to steady-state performance measures.},
	Author = {Glynn, Peter W.},
	Doi = {10.1145/84537.84552},
	Isbn = {0001-0782},
	Issn = {00010782},
	Journal = {Communications of the ACM (Association of Computing Machinery)},
	Number = {10},
	Pages = {75--84},
	Title = {{Likelihood ratio gradient estimation for stochastic systems}},
	Url = {http://portal.acm.org/citation.cfm?id=84552{\%}5Cnhttp://portal.acm.org/ft{\_}gateway.cfm?id=84552{\&}type=pdf{\&}coll=GUIDE{\&}dl=GUIDE{\&}CFID=92520986{\&}CFTOKEN=53025364},
	Volume = {33},
	Year = {1990},
	Bdsk-Url-1 = {https://dx.doi.org/10.1145/84537.84552}}

@article{sutton,
	Abstract = {This introductory textbook on reinforcement learning is targeted toward engineers and scientists in artificial intelligence, operations research, neural networks, and control systems, and we hope it will also be of interest to psychologists and neuroscientists. If you would like to order a copy of the book, or if you are qualified instructor and would like to see an examination copy, please see the MIT Press home page for this book. Or you might be interested in the reviews at amazon.com. There is also a Japanese translation available. The table of contents of the book is given below, with associated HTML. The HTML version has a number of presentation problems, and its text is slightly different from the real book, but it may be useful for some purposes. q Preface Part I: The Problem q 1 Introduction r 1.1 Reinforcement Learning r 1.2 Examples r 1.3 Elements of Reinforcement Learning r 1.4 An Extended Example: Tic-Tac-Toe r 1.5 Summary r 1.6 History of Reinforcement Learning r 1.7 Bibliographical Remarks q 2 Evaluative Feedback r 2.1 An n-armed Bandit Problem r 2.2 Action-Value Methods r 2.3 Softmax Action Selection r 2.4 Evaluation versus Instruction r 2.5 Incremental Implementation r 2.6 Tracking a Nonstationary Problem r 2.7 Optimistic Initial Values r 2.8 Reinforcement Comparison r 2.9 Pursuit Methods r 2.10 Associative Search r 2.11 Conclusion r 2.12 Bibliographical and Historical Remarks q 3 The Reinforcement Learning Problem r 3.1 The Agent-Environment Interface r 3.2 Goals and Rewards r 3.3 Returns r 3.4 A Unified Notation for Episodic and Continual Tasks r 3.5 The Markov Property r 3.6 Markov Decision Processes r 3.7 Value Functions r 3.8 Optimal Value Functions r 3.9 Optimality and Approximation r 3.10 Summary r 3.11 Bibliographical and Historical Remarks Part II: Elementary Methods},
	Archiveprefix = {arXiv},
	Arxivid = {1603.02199},
	Author = {Sutton, Richard S. and Barto, Andrew G.},
	Doi = {10.1109/TNN.1998.712192},
	Eprint = {1603.02199},
	Isbn = {0262193981},
	Issn = {10459227},
	Journal = {MIT Press, Cambridge, MA, A Bradford Book},
	Keywords = {reinforcement learning theory},
	Pmid = {18255791},
	Title = {{Sutton {\&} Barto Book: Reinforcement Learning: An Introduction}},
	Year = {1998},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/TNN.1998.712192}}

@article{variational_gaussian,
	Abstract = {The variational approximation of posterior distributions by multivariate gaussians has been much less popular in the machine learning community compared to the corresponding approximation by factorizing distributions. This is for a good reason: the gaussian approximation is in general plagued by an number of variational parameters to be optimized, N being the number of random variables. In this letter, we discuss the relationship between the Laplace and the variational approximation, and we show that for models with gaussian priors and factorizing likelihoods, the number of variational parameters is actually . The approach is applied to gaussian process regression with nongaussian likelihoods.},
	Author = {Opper, Manfred and Archambeau, C{\'{e}}dric},
	Doi = {10.1162/neco.2008.08-07-592},
	Issn = {0899-7667},
	Journal = {Neural Computation},
	Number = {3},
	Pages = {786--792},
	Pmid = {18785854},
	Title = {{The Variational Gaussian Approximation Revisited}},
	Url = {http://www.mitpressjournals.org/doi/10.1162/neco.2008.08-07-592},
	Volume = {21},
	Year = {2009},
	Bdsk-Url-1 = {http://www.mitpressjournals.org/doi/10.1162/neco.2008.08-07-592},
	Bdsk-Url-2 = {https://dx.doi.org/10.1162/neco.2008.08-07-592}}



@article{highdimcontrol,
	Abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
	Archiveprefix = {arXiv},
	Arxivid = {1506.02438},
	Author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
	Eprint = {1506.02438},
	File = {:Users/matows/Desktop/1506.02438.pdf:pdf},
	Pages = {1--14},
	Title = {{High-Dimensional Continuous Control Using Generalized Advantage Estimation}},
	Url = {http://arxiv.org/abs/1506.02438},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1506.02438}}

@article{learning_to_comm,
	Abstract = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
	Archiveprefix = {arXiv},
	Arxivid = {1605.06676},
	Author = {Foerster, Jakob N. and Assael, Yannis M. and de Freitas, Nando and Whiteson, Shimon},
	Doi = {10.7551/ecal},
	Eprint = {1605.06676},
	File = {:Users/matows/Desktop/1605.06676.pdf:pdf},
	Isbn = {2004012439},
	Issn = {10495258},
	Pages = {1--13},
	Title = {{Learning to Communicate with Deep Multi-Agent Reinforcement Learning}},
	Url = {http://arxiv.org/abs/1605.06676},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1605.06676},
	Bdsk-Url-2 = {https://dx.doi.org/10.7551/ecal}}

@article{dpg,
	Abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic pol- icy gradient has a particularly appealing form: it is the expected gradient of the action-value func- tion. This simple form means that the deter- ministic policy gradient can be estimated much more efficiently than the usual stochastic pol- icy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counter- parts in high-dimensional action spaces.},
	Author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
	Isbn = {9781634393973},
	Issn = {1938-7228},
	Journal = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
	Pages = {387--395},
	Title = {{Deterministic Policy Gradient Algorithms}},
	Year = {2014}}

@article{seq1,
	Abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
	Archiveprefix = {arXiv},
	Arxivid = {1502.03044},
	Author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
	Doi = {10.1109/72.279181},
	Eprint = {1502.03044},
	File = {:Users/matows/Desktop/1502.03044.pdf:pdf},
	Isbn = {1045-9227 VO - 5},
	Issn = {19410093},
	Pmid = {18267787},
	Title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
	Url = {http://arxiv.org/abs/1502.03044},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1502.03044},
	Bdsk-Url-2 = {https://dx.doi.org/10.1109/72.279181}}

@article{bdl,
	Abstract = {Deep learning has attracted tremendous attention from researchers in various fields of information engineering such as AI, computer vision, and language processing [Kalchbrenner and Blunsom, 2013; Krizhevsky et al., 2012; Mnih et al., 2013], but also from more traditional sciences such as physics, biology, and manufacturing [Anjos et al., 2015; Baldi et al., 2014; Bergmann et al., 2014]. Neural networks, image processing tools such as convolutional neural networks, sequence processing models such as recurrent neural networks, and regularisation tools such as dropout, are used extensively. However, fields such as physics, biology, and manufacturing are ones in which representing model uncertainty is of crucial importance [Ghahramani, 2015; Krzywinski and Altman, 2013]. With the recent shift in many of these fields towards the use of Bayesian uncertainty [Herzog and Ostwald, 2013; Nuzzo, 2014; Trafimow and Marks, 2015], new needs arise from deep learning. In this work we develop tools to obtain practical uncertainty estimates in deep learning, casting recent deep learning tools as Bayesian models without changing either the models or the optimisation. In the first part of this thesis we develop the theory for such tools, providing applications and illustrative examples. We tie approximate inference in Bayesian models to dropout and other stochastic regularisation techniques, and assess the approximations empirically. We give example applications arising from this connection between modern deep learning and Bayesian modelling such as active learning of image data and data efficient deep reinforcement learning. We further demonstrate the method's practicality through a survey of recent applications making use of the suggested tools in language applications, medical diagnostics, bioinformatics, image processing, and autonomous driving. In the second part of the thesis we explore its theoretical implications, and the insights stemming from the link between Bayesian modelling and deep learning. We discuss what determines model uncertainty properties, analyse the approximate inference analytically in the linear case, and theoretically examine various priors such as spike and slab priors.},
	Author = {Gal, Yarin},
	Journal = {PhD Thesis},
	Number = {October},
	Title = {{Uncertainty in Deep Learning}},
	Year = {2016}}

@article{dqn,
	Abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6{\^a}8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9{\^a}11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
	Archiveprefix = {arXiv},
	Arxivid = {1312.5602},
	Author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	Doi = {10.1038/nature14236},
	Eprint = {1312.5602},
	Isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
	Issn = {14764687},
	Journal = {Nature},
	Number = {7540},
	Pages = {529--533},
	Pmid = {25719670},
	Title = {{Human-level control through deep reinforcement learning}},
	Volume = {518},
	Year = {2015},
	Bdsk-Url-1 = {https://dx.doi.org/10.1038/nature14236}}

@article{coma,
	Abstract = {Cooperative multi-agent systems can be naturally used to model many real world problems, such as network packet routing and the coordination of autonomous vehicles. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.},
	Archiveprefix = {arXiv},
	Arxivid = {1705.08926},
	Author = {Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
	Eprint = {1705.08926},
	File = {:Users/matows/Library/Application Support/Mendeley Desktop/Downloaded/Foerster et al. - 2017 - Counterfactual Multi-Agent Policy Gradients.pdf:pdf},
	Pages = {1--12},
	Title = {{Counterfactual Multi-Agent Policy Gradients}},
	Url = {http://arxiv.org/abs/1705.08926},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1705.08926}}

@article{multiv_normal_cdf2,
	Abstract = {This article compares methods for the numerical computation of multivariate t probabilities for hyper-rectangular integration regions. Methods based on acceptance-rejection, spherical-radial transformations, and separation-of-variables transformations are considered. Tests using randomly chosen problems show that the most efficient numerical methods use a transformation developed by Genz for multivariate normal probabilities. These methods allow moderately accurate multivariate t probabilities to be quickly computed for problems with as many as 20 variables. Methods for the noncentral multivariate t distribution are also described.},
	Author = {Genz, Alan and Bretz, Frank},
	Doi = {10.1198/106186002394},
	Isbn = {1061-8600},
	Issn = {1061-8600},
	Journal = {Journal of Computational and Graphical Statistics},
	Keywords = {Multivariate t distribution,Noncentral distribution,Numerical integration,Statistical computation},
	Number = {4},
	Pages = {950--971},
	Title = {{Comparison of Methods for the Computation of Multivariate t Probabilities}},
	Url = {http://amstat.tandfonline.com/doi/abs/10.1198/106186002394{\#}.UxiBrnVdUic},
	Volume = {11},
	Year = {2002},
	Bdsk-Url-1 = {http://amstat.tandfonline.com/doi/abs/10.1198/106186002394%7B%5C#%7D.UxiBrnVdUic},
	Bdsk-Url-2 = {https://dx.doi.org/10.1198/106186002394}}

@article{fourier_value,
	Abstract = {We describe the Fourier basis, a linear value function approx- imation scheme based on the Fourier series. We empirically demonstrate that it performs well compared to radial basis functions and the polynomial basis, the two most popular fixed bases for linear value function approximation, and is competitive with learned proto-value functions.},
	Author = {Konidaris, George and Osentoski, Sarah and Thomas, Philip},
	Isbn = {9781577355083},
	Journal = {Proceedings of the Twenty-Fifth Conference on Artificial Intelligence},
	Pages = {380--385},
	Title = {{Value Function Approximation in Reinforcement Learning using the Fourier Basis}},
	Year = {2011}}
