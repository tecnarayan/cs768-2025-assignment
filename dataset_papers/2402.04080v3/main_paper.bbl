\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[An et~al.(2021)An, Moon, Kim, and Song]{an2021edac}
Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun~Oh Song.
\newblock Uncertainty-based offline reinforcement learning with diversified q-ensemble.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 7436--7447, 2021.

\bibitem[Anderson(1982)]{anderson1982reverse}
Brian~DO Anderson.
\newblock Reverse-time diffusion equation models.
\newblock \emph{Stochastic Processes and their Applications}, 12\penalty0 (3):\penalty0 313--326, 1982.

\bibitem[Bai et~al.(2022)Bai, Wang, Yang, Deng, Garg, Liu, and Wang]{bai2022pessimistic}
Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhihong Deng, Animesh Garg, Peng Liu, and Zhaoran Wang.
\newblock Pessimistic bootstrapping for uncertainty-driven offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2202.11566}, 2022.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel, Srinivas, and Mordatch]{chen2021DT}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 15084--15097, 2021.

\bibitem[Chen et~al.(2023)Chen, Kiami, Gupta, and Kumar]{chen2023genaug}
Zoey Chen, Sho Kiami, Abhishek Gupta, and Vikash Kumar.
\newblock Genaug: Retargeting behaviors to unseen situations via generative augmentation.
\newblock \emph{arXiv preprint arXiv:2302.06671}, 2023.

\bibitem[Dhariwal and Nichol(2021)]{dhariwal2021diffusion}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 8780--8794, 2021.

\bibitem[Eysenbach et~al.(2018)Eysenbach, Gupta, Ibarz, and Levine]{eysenbach2018diversity}
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine.
\newblock Diversity is all you need: Learning skills without a reward function.
\newblock \emph{arXiv preprint arXiv:1802.06070}, 2018.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Fujimoto and Gu(2021)]{fujimoto2021minimalist}
Scott Fujimoto and Shixiang~Shane Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 20132--20145, 2021.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International conference on machine learning}, pages 2052--2062. PMLR, 2019.

\bibitem[Ghasemipour et~al.(2022)Ghasemipour, Gu, and Nachum]{ghasemipour2022-msg}
Kamyar Ghasemipour, Shixiang~Shane Gu, and Ofir Nachum.
\newblock Why so pessimistic? estimating uncertainties for offline rl through ensembles, and why their independence matters.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 18267--18281, 2022.

\bibitem[Gillespie(1996)]{gillespie1996exact}
Daniel~T Gillespie.
\newblock Exact numerical simulation of the ornstein-uhlenbeck process and its integral.
\newblock \emph{Physical review E}, 54\penalty0 (2):\penalty0 2084, 1996.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and Levine]{haarnoja2017reinforcement}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International conference on machine learning}, pages 1352--1361. PMLR, 2017.

\bibitem[Haarnoja et~al.(2018{\natexlab{a}})Haarnoja, Zhou, Abbeel, and Levine]{haarnoja2018sac}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pages 1861--1870. PMLR, 2018{\natexlab{a}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{b}})Haarnoja, Zhou, Abbeel, and Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pages 1861--1870. PMLR, 2018{\natexlab{b}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{c}})Haarnoja, Zhou, Hartikainen, Tucker, Ha, Tan, Kumar, Zhu, Gupta, Abbeel, et~al.]{haarnoja2018soft2}
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et~al.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{arXiv preprint arXiv:1812.05905}, 2018{\natexlab{c}}.

\bibitem[Hansen-Estruch et~al.(2023)Hansen-Estruch, Kostrikov, Janner, Kuba, and Levine]{hansen2023idql}
Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub~Grudzien Kuba, and Sergey Levine.
\newblock Idql: Implicit q-learning as an actor-critic method with diffusion policies.
\newblock \emph{arXiv preprint arXiv:2304.10573}, 2023.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 6840--6851, 2020.

\bibitem[Janner et~al.(2022)Janner, Du, Tenenbaum, and Levine]{janner2022diffuser}
Michael Janner, Yilun Du, Joshua~B Tenenbaum, and Sergey Levine.
\newblock Planning with diffusion for flexible behavior synthesis.
\newblock \emph{arXiv preprint arXiv:2205.09991}, 2022.

\bibitem[Jin et~al.(2021)Jin, Yang, and Wang]{jin2021pessimism}
Ying Jin, Zhuoran Yang, and Zhaoran Wang.
\newblock Is pessimism provably efficient for offline rl?
\newblock In \emph{International Conference on Machine Learning}, pages 5084--5096. PMLR, 2021.

\bibitem[Kang et~al.(2023{\natexlab{a}})Kang, Ma, Du, Pang, and Yan]{kang2023edp}
Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng Yan.
\newblock Efficient diffusion policies for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2305.20081}, 2023{\natexlab{a}}.

\bibitem[Kang et~al.(2023{\natexlab{b}})Kang, Ma, Du, Pang, and Yan]{kang2023efficient}
Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng Yan.
\newblock Efficient diffusion policies for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2305.20081}, 2023{\natexlab{b}}.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and Joachims]{kidambi2020morel}
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
\newblock Morel: Model-based offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 21810--21823, 2020.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma and Welling(2013)]{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Nair, and Levine]{kostrikov2021IQL}
Ilya Kostrikov, Ashvin Nair, and Sergey Levine.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock \emph{arXiv preprint arXiv:2110.06169}, 2021.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and Levine]{kumar2019stabilizing}
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and Levine]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 1179--1191, 2020.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{lange2012batchrl}
Sascha Lange, Thomas Gabel, and Martin Riedmiller.
\newblock Batch reinforcement learning.
\newblock In \emph{Reinforcement learning: State-of-the-art}, pages 45--73. Springer, 2012.

\bibitem[Lattimore and Szepesv{\'a}ri(2020)]{lattimore2020bandit}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Luo et~al.(2023{\natexlab{a}})Luo, Gustafsson, Zhao, Sj{\"o}lund, and Sch{\"o}n]{luo2023image}
Ziwei Luo, Fredrik~K Gustafsson, Zheng Zhao, Jens Sj{\"o}lund, and Thomas~B Sch{\"o}n.
\newblock Image restoration with mean-reverting stochastic differential equations.
\newblock \emph{International Conference on Machine Learning}, 2023{\natexlab{a}}.

\bibitem[Luo et~al.(2023{\natexlab{b}})Luo, Gustafsson, Zhao, Sj{\"o}lund, and Sch{\"o}n]{luo2023refusion}
Ziwei Luo, Fredrik~K Gustafsson, Zheng Zhao, Jens Sj{\"o}lund, and Thomas~B Sch{\"o}n.
\newblock Refusion: Enabling large-size realistic image restoration with latent-space diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 1680--1691, 2023{\natexlab{b}}.

\bibitem[Misra(2019)]{misra2019mish}
Diganta Misra.
\newblock Mish: A self regularized non-monotonic activation function.
\newblock \emph{arXiv preprint arXiv:1908.08681}, 2019.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, and Kavukcuoglu]{mnih2016asynchronous}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages 1928--1937. PMLR, 2016.

\bibitem[Nichol and Dhariwal(2021)]{nichol2021improved}
Alexander~Quinn Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock In \emph{International Conference on Machine Learning}, pages 8162--8171. PMLR, 2021.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and Van~Roy]{osband2016bootstrap}
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van~Roy.
\newblock Deep exploration via bootstrapped {DQN}.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Pan and Yang(2009)]{pan2009survey-transfer}
Sinno~Jialin Pan and Qiang Yang.
\newblock A survey on transfer learning.
\newblock \emph{IEEE Transactions on knowledge and data engineering}, 22\penalty0 (10):\penalty0 1345--1359, 2009.

\bibitem[Richter et~al.(2023)Richter, Welker, Lemercier, Lay, and Gerkmann]{richter2023speech}
Julius Richter, Simon Welker, Jean-Marie Lemercier, Bunlong Lay, and Timo Gerkmann.
\newblock Speech enhancement and dereverberation with diffusion-based generative models.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 2023.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695, 2022.

\bibitem[Saharia et~al.(2022)Saharia, Chan, Saxena, Li, Whang, Denton, Ghasemipour, Gontijo~Lopes, Karagol~Ayan, Salimans, et~al.]{saharia2022photorealistic}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily~L Denton, Kamyar Ghasemipour, Raphael Gontijo~Lopes, Burcu Karagol~Ayan, Tim Salimans, et~al.
\newblock Photorealistic text-to-image diffusion models with deep language understanding.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 36479--36494, 2022.

\bibitem[Song et~al.(2020{\natexlab{a}})Song, Meng, and Ermon]{song2020denoising}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock \emph{arXiv preprint arXiv:2010.02502}, 2020{\natexlab{a}}.

\bibitem[Song et~al.(2020{\natexlab{b}})Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and Poole]{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock \emph{arXiv preprint arXiv:2011.13456}, 2020{\natexlab{b}}.

\bibitem[Wang et~al.(2022)Wang, Hunt, and Zhou]{wang2022diffusionpolicy}
Zhendong Wang, Jonathan~J Hunt, and Mingyuan Zhou.
\newblock Diffusion policies as an expressive policy class for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2208.06193}, 2022.

\bibitem[Welker et~al.(2022)Welker, Richter, and Gerkmann]{welker22speech}
Simon Welker, Julius Richter, and Timo Gerkmann.
\newblock Speech enhancement with score-based generative models in the complex {STFT} domain.
\newblock In \emph{Proc. Interspeech 2022}, pages 2928--2932, 2022.
\newblock \doi{10.21437/Interspeech.2022-10653}.

\bibitem[Xiao et~al.(2019)Xiao, Wu, Ma, Schuurmans, and M{\"u}ller]{xiao2019compounding}
Chenjun Xiao, Yifan Wu, Chen Ma, Dale Schuurmans, and Martin M{\"u}ller.
\newblock Learning to combat compounding-error in model-based reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.11206}, 2019.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and Ma]{yu2020mopo}
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James~Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma.
\newblock Mopo: Model-based offline policy optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 14129--14142, 2020.

\bibitem[Yu et~al.(2023)Yu, Xiao, Stone, Tompson, Brohan, Wang, Singh, Tan, Peralta, Ichter, et~al.]{yu2023scaling}
Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony Brohan, Su~Wang, Jaspiar Singh, Clayton Tan, Jodilyn Peralta, Brian Ichter, et~al.
\newblock Scaling robot learning with semantically imagined experience.
\newblock \emph{arXiv preprint arXiv:2302.11550}, 2023.

\bibitem[Zhu et~al.(2023)Zhu, Zhao, He, Zhong, Zhang, Yu, and Zhang]{zhu2023diffusion}
Zhengbang Zhu, Hanye Zhao, Haoran He, Yichao Zhong, Shenyu Zhang, Yong Yu, and Weinan Zhang.
\newblock Diffusion models for reinforcement learning: A survey.
\newblock \emph{arXiv preprint arXiv:2311.01223}, 2023.

\bibitem[Ziebart(2010)]{ziebart2010modeling}
Brian~D Ziebart.
\newblock \emph{Modeling purposeful adaptive behavior with the principle of maximum causal entropy}.
\newblock Carnegie Mellon University, 2010.

\end{thebibliography}
