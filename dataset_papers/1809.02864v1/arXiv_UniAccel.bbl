\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[{Allen-Zhu}(2017)]{Allenzhu2017-katyusha}
Z.~{Allen-Zhu}.
\newblock {Katyusha: The First Direct Acceleration of Stochastic Gradient
  Methods}.
\newblock In \emph{STOC}, 2017.
\newblock Full version available at \url{http://arxiv.org/abs/1603.05953}.

\bibitem[{Allen-Zhu} and Orecchia(2017)]{AllenOrecchia2017}
Z.~{Allen-Zhu} and L.~Orecchia.
\newblock {Linear Coupling: An Ultimate Unification of Gradient and Mirror
  Descent}.
\newblock In \emph{Proceedings of the 8th Innovations in Theoretical Computer
  Science}, ITCS~'17, 2017.
\newblock Full version available at \url{http://arxiv.org/abs/1407.1537}.

\bibitem[Arjevani et~al.(2016)Arjevani, Shalev-Shwartz, and
  Shamir]{arjevani2016lower}
Y.~Arjevani, S.~Shalev-Shwartz, and O.~Shamir.
\newblock On lower and upper bounds in smooth and strongly convex optimization.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 4303--4353, 2016.

\bibitem[Attouch and Chbani(2015)]{attouch2015fast}
H.~Attouch and Z.~Chbani.
\newblock Fast inertial dynamics and fista algorithms in convex optimization.
  perturbation aspects.
\newblock \emph{arXiv preprint arXiv:1507.01367}, 2015.

\bibitem[Aujol and Dossal(2017)]{aujol2017optimal}
J.~Aujol and C.~Dossal.
\newblock Optimal rate of convergence of an ode associated to the fast gradient
  descent schemes for b> 0.
\newblock 2017.

\bibitem[Beck and Teboulle(2009)]{beck2009fast}
A.~Beck and M.~Teboulle.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock \emph{SIAM journal on imaging sciences}, 2\penalty0 (1):\penalty0
  183--202, 2009.

\bibitem[Bubeck et~al.(2015)Bubeck, Lee, and Singh]{bubeck2015geometric}
S.~Bubeck, Y.~T. Lee, and M.~Singh.
\newblock A geometric alternative to nesterov's accelerated gradient descent.
\newblock \emph{arXiv preprint arXiv:1506.08187}, 2015.

\bibitem[Cesa-Bianchi et~al.(2004)Cesa-Bianchi, Conconi, and
  Gentile]{cesa2004generalization}
N.~Cesa-Bianchi, A.~Conconi, and C.~Gentile.
\newblock On the generalization ability of on-line learning algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 50\penalty0
  (9):\penalty0 2050--2057, 2004.

\bibitem[Chambolle and Pock(2011)]{chambolle2011first}
A.~Chambolle and T.~Pock.
\newblock A first-order primal-dual algorithm for convex problems with
  applications to imaging.
\newblock \emph{Journal of mathematical imaging and vision}, 40\penalty0
  (1):\penalty0 120--145, 2011.

\bibitem[Cohen et~al.(2018)Cohen, Diakonikolas, and
  Orecchia]{cohen2018acceleration}
M.~B. Cohen, J.~Diakonikolas, and L.~Orecchia.
\newblock On acceleration with noise-corrupted gradients.
\newblock \emph{arXiv preprint arXiv:1805.12591}, 2018.

\bibitem[Cutkosky and Orabona(2018)]{cutkosky2018black}
A.~Cutkosky and F.~Orabona.
\newblock Black-box reductions for parameter-free online learning in banach
  spaces.
\newblock \emph{arXiv preprint arXiv:1802.06293}, 2018.

\bibitem[Diakonikolas and Orecchia(2017)]{diakonikolas2017accelerated}
J.~Diakonikolas and L.~Orecchia.
\newblock Accelerated extra-gradient descent: A novel accelerated first-order
  method.
\newblock \emph{arXiv preprint arXiv:1706.04680}, 2017.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Flammarion and Bach(2015)]{flammarion2015averaging}
N.~Flammarion and F.~Bach.
\newblock From averaging to acceleration, there is only a step-size.
\newblock In \emph{Conference on Learning Theory}, pages 658--695, 2015.

\bibitem[Foucart and Rauhut(2013)]{foucart2013mathematical}
S.~Foucart and H.~Rauhut.
\newblock \emph{A mathematical introduction to compressive sensing}, volume~1.
\newblock Birkh{\"a}user Basel, 2013.

\bibitem[Frostig et~al.(2015)Frostig, Ge, Kakade, and
  Sidford]{frostig2015regularizing}
R.~Frostig, R.~Ge, S.~Kakade, and A.~Sidford.
\newblock Un-regularizing: approximate proximal point and faster stochastic
  algorithms for empirical risk minimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  2540--2548, 2015.

\bibitem[Hu et~al.(2009)Hu, Pan, and Kwok]{hu2009accelerated}
C.~Hu, W.~Pan, and J.~T. Kwok.
\newblock Accelerated gradient methods for stochastic optimization and online
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  781--789, 2009.

\bibitem[Lan(2012)]{lan2012optimal}
G.~Lan.
\newblock An optimal method for stochastic composite optimization.
\newblock \emph{Mathematical Programming}, 133\penalty0 (1-2):\penalty0
  365--397, 2012.

\bibitem[Lan(2015)]{lan2015bundle}
G.~Lan.
\newblock Bundle-level type methods uniformly optimal for smooth and nonsmooth
  convex optimization.
\newblock \emph{Mathematical Programming}, 149\penalty0 (1-2):\penalty0 1--45,
  2015.

\bibitem[Lessard et~al.(2016)Lessard, Recht, and Packard]{lessard2016analysis}
L.~Lessard, B.~Recht, and A.~Packard.
\newblock Analysis and design of optimization algorithms via integral quadratic
  constraints.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (1):\penalty0
  57--95, 2016.

\bibitem[Levy(2017)]{levy2017online}
K.~Levy.
\newblock Online to offline conversions, universality and adaptive minibatch
  sizes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1612--1621, 2017.

\bibitem[Lin et~al.(2015)Lin, Mairal, and Harchaoui]{lin2015universal}
H.~Lin, J.~Mairal, and Z.~Harchaoui.
\newblock A universal catalyst for first-order optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3384--3392, 2015.

\bibitem[McMahan and Streeter(2010)]{mcmahan2010adaptive}
H.~B. McMahan and M.~Streeter.
\newblock Adaptive bound optimization for online convex optimization.
\newblock \emph{COLT 2010}, page 244, 2010.

\bibitem[Nemirovskii et~al.(1983)Nemirovskii, Yudin, and
  Dawson]{nemirovskii1983problem}
A.~Nemirovskii, D.~B. Yudin, and E.~Dawson.
\newblock Problem complexity and method efficiency in optimization.
\newblock 1983.

\bibitem[Nesterov(1983)]{nesterov1983method}
Y.~Nesterov.
\newblock A method of solving a convex programming problem with convergence
  rate o (1/k2).
\newblock In \emph{Soviet Mathematics Doklady}, volume~27, pages 372--376,
  1983.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Y.~Nesterov.
\newblock Introductory lectures on convex optimization. 2004, 2003.

\bibitem[Nesterov(2015)]{nesterov2015universal}
Y.~Nesterov.
\newblock Universal gradient methods for convex optimization problems.
\newblock \emph{Mathematical Programming}, 152\penalty0 (1-2):\penalty0
  381--404, 2015.

\bibitem[Neumaier(2016)]{neumaier2016osga}
A.~Neumaier.
\newblock Osga: a fast subgradient algorithm with optimal complexity.
\newblock \emph{Mathematical Programming}, 158\penalty0 (1-2):\penalty0 1--21,
  2016.

\bibitem[Orabona and P{\'a}l(2015)]{orabona2015scale}
F.~Orabona and D.~P{\'a}l.
\newblock Scale-free algorithms for online linear optimization.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 287--301. Springer, 2015.

\bibitem[Scieur et~al.(2016)Scieur, d'Aspremont, and
  Bach]{scieur2016regularized}
D.~Scieur, A.~d'Aspremont, and F.~Bach.
\newblock Regularized nonlinear acceleration.
\newblock In \emph{Advances In Neural Information Processing Systems}, pages
  712--720, 2016.

\bibitem[Shalev-Shwartz and Zhang(2014)]{shalev2014accelerated}
S.~Shalev-Shwartz and T.~Zhang.
\newblock Accelerated proximal stochastic dual coordinate ascent for
  regularized loss minimization.
\newblock In \emph{International Conference on Machine Learning}, pages 64--72,
  2014.

\bibitem[Su et~al.(2014)Su, Boyd, and Candes]{su14}
W.~Su, S.~Boyd, and E.~Candes.
\newblock A differential equation for modeling nesterov's accelerated gradient
  method: Theory and insights.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2510--2518, 2014.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013importance}
I.~Sutskever, J.~Martens, G.~Dahl, and G.~Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International conference on machine learning}, pages
  1139--1147, 2013.

\bibitem[Wibisono et~al.(2016)Wibisono, Wilson, and
  Jordan]{wibisono2016variational}
A.~Wibisono, A.~C. Wilson, and M.~I. Jordan.
\newblock A variational perspective on accelerated methods in optimization.
\newblock \emph{Proceedings of the National Academy of Sciences}, 113\penalty0
  (47):\penalty0 E7351--E7358, 2016.

\bibitem[Xiao(2010)]{xiao2010dual}
L.~Xiao.
\newblock Dual averaging methods for regularized stochastic learning and online
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Oct):\penalty0 2543--2596, 2010.

\bibitem[Yurtsever et~al.(2015)Yurtsever, Dinh, and
  Cevher]{yurtsever2015universal}
A.~Yurtsever, Q.~T. Dinh, and V.~Cevher.
\newblock A universal primal-dual convex optimization framework.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3150--3158, 2015.

\end{thebibliography}
