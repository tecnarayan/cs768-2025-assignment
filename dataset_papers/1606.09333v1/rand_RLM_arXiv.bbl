\begin{thebibliography}{10}

\bibitem{agarwal2014lower}
Alekh Agarwal and Leon Bottou.
\newblock A lower bound for the optimization of finite sums.
\newblock {\em arXiv preprint arXiv:1410.0723}, 2014.

\bibitem{akhiezer1956theory}
Naum~Il'ich AKHIEZER and Charles~J Hyman.
\newblock {\em Theory of approximation. Translated by Charles J. Hyman}.
\newblock New York, 1956.

\bibitem{arjevani2015lower}
Yossi Arjevani, Shai Shalev-Shwartz, and Ohad Shamir.
\newblock On lower and upper bounds for smooth and strongly convex optimization
  problems.
\newblock {\em arXiv preprint arXiv:1503.06833}, 2015.

\bibitem{arjevani2015communication}
Yossi Arjevani and Ohad Shamir.
\newblock Communication complexity of distributed convex learning and
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1747--1755, 2015.

\bibitem{arjevani2016iteration}
Yossi Arjevani and Ohad Shamir.
\newblock On the iteration complexity of oblivious first-order optimization
  algorithms.
\newblock {\em arXiv preprint arXiv:1605.03529}, 2016.

\bibitem{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1646--1654, 2014.

\bibitem{frostig2015regularizing}
Roy Frostig, Rong Ge, Sham~M Kakade, and Aaron Sidford.
\newblock Un-regularizing: approximate proximal point and faster stochastic
  algorithms for empirical risk minimization.
\newblock {\em arXiv preprint arXiv:1506.07512}, 2015.

\bibitem{gurbuzbalaban2015random}
Mert G{\"u}rb{\"u}zbalaban, Asu Ozdaglar, and Pablo Parrilo.
\newblock Why random reshuffling beats stochastic gradient descent.
\newblock {\em arXiv preprint arXiv:1510.08560}, 2015.

\bibitem{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  315--323, 2013.

\bibitem{lan2015optimal}
Guanghui Lan.
\newblock An optimal randomized incremental gradient method.
\newblock {\em arXiv preprint arXiv:1507.02000}, 2015.

\bibitem{lin2015universal}
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.
\newblock A universal catalyst for first-order optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3366--3374, 2015.

\bibitem{nemirovskyproblem}
AS~Nemirovsky and DB~Yudin.
\newblock Problem complexity and method efficiency in optimization. 1983.
\newblock {\em Willey-Interscience, New York}, 1983.

\bibitem{nesterov2004introductory}
Yurii Nesterov.
\newblock {\em Introductory lectures on convex optimization}, volume~87.
\newblock Springer Science \& Business Media, 2004.

\bibitem{polyak1987introduction}
Boris~T Polyak.
\newblock {\em Introduction to optimization}.
\newblock Optimization Software New York, 1987.

\bibitem{recht2012beneath}
Benjamin Recht and Christopher R{\'e}.
\newblock Beneath the valley of the noncommutative arithmetic-geometric mean
  inequality: conjectures, case-studies, and consequences.
\newblock {\em arXiv preprint arXiv:1202.4184}, 2012.

\bibitem{roux2012stochastic}
Nicolas~Le Roux, Mark Schmidt, and Francis Bach.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock {\em arXiv preprint arXiv:1202.6258}, 2012.

\bibitem{shalev2015sdca}
Shai Shalev-Shwartz.
\newblock Sdca without duality.
\newblock {\em arXiv preprint arXiv:1502.06177}, 2015.

\bibitem{shalev2013stochastic}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss.
\newblock {\em The Journal of Machine Learning Research}, 14(1):567--599, 2013.

\bibitem{shalev2016accelerated}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Accelerated proximal stochastic dual coordinate ascent for
  regularized loss minimization.
\newblock {\em Mathematical Programming}, 155(1-2):105--145, 2016.

\bibitem{shamir2016without}
Ohad Shamir.
\newblock Without-replacement sampling for stochastic gradient methods:
  Convergence results and application to distributed optimization.
\newblock {\em arXiv preprint arXiv:1603.00570}, 2016.

\bibitem{tchebyshev1899polynomials}
P~Tchebyshev.
\newblock On polynomials which yield the best values for the simplest rational
  functions.
\newblock {\em Collected Papers}, 1, 1899.

\end{thebibliography}
