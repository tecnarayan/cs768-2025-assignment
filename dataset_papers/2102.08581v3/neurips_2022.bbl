\begin{thebibliography}{10}

\bibitem{achille2018critical}
Alessandro Achille, Matteo Rovere, and Stefano Soatto.
\newblock Critical learning periods in deep networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{arora2019implicit}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7411--7422, 2019.

\bibitem{auer2002using}
Peter Auer.
\newblock Using confidence bounds for exploitation-exploration trade-offs.
\newblock {\em Journal of Machine Learning Research}, 3(Nov):397--422, 2002.

\bibitem{bellman2015adaptive}
Richard~E Bellman.
\newblock {\em Adaptive control processes}.
\newblock Princeton university press, 2015.

\bibitem{cobbe2020leveraging}
Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock In {\em International conference on machine learning}, pages
  2048--2056. PMLR, 2020.

\bibitem{cobbe2019quantifying}
Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman.
\newblock Quantifying generalization in reinforcement learning, 2019.

\bibitem{erhan2010does}
Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent.
\newblock Why does unsupervised pre-training help deep learning?
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 201--208. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem{espeholt2018impala}
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom
  Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock {\em arXiv preprint arXiv:1802.01561}, 2018.

\bibitem{farebrother2020generalization}
Jesse Farebrother, Marlos~C. Machado, and Michael Bowling.
\newblock Generalization and regularization in dqn, 2020.

\bibitem{gidel2019implicit}
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien.
\newblock Implicit regularization of discrete gradient dynamics in linear
  neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3196--3206, 2019.

\bibitem{golatkar2019time}
Aditya~Sharad Golatkar, Alessandro Achille, and Stefano Soatto.
\newblock Time matters in regularizing deep networks: Weight decay and data
  augmentation affect early learning dynamics, matter little near convergence.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10678--10688, 2019.

\bibitem{gudovskiy2021autodo}
Denis Gudovskiy, Luca Rigazio, Shun Ishizaka, Kazuki Kozuka, and Sotaro
  Tsukizawa.
\newblock Autodo: Robust autoaugment for biased data with label noise via
  scalable probabilistic implicit differentiation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 16601--16610, 2021.

\bibitem{gunasekar2017implicit}
Suriya Gunasekar, Blake~E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,
  and Nati Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6151--6159, 2017.

\bibitem{hansen2020self}
Nicklas Hansen, Yu~Sun, Pieter Abbeel, Alexei~A Efros, Lerrel Pinto, and
  Xiaolong Wang.
\newblock Self-supervised policy adaptation during deployment.
\newblock {\em arXiv preprint arXiv:2007.04309}, 2020.

\bibitem{hansen2021generalization}
Nicklas Hansen and Xiaolong Wang.
\newblock Generalization in reinforcement learning by soft data augmentation.
\newblock In {\em 2021 IEEE International Conference on Robotics and Automation
  (ICRA)}, pages 13611--13617. IEEE, 2021.

\bibitem{henderson2019deep}
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup,
  and David Meger.
\newblock Deep reinforcement learning that matters, 2019.

\bibitem{higgins2018darla}
Irina Higgins, Arka Pal, Andrei~A. Rusu, Loic Matthey, Christopher~P Burgess,
  Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander
  Lerchner.
\newblock Darla: Improving zero-shot transfer in reinforcement learning, 2018.

\bibitem{igl2020impact}
Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon
  Whiteson.
\newblock The impact of non-stationarity on generalisation in deep
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:2006.05826}, 2020.

\bibitem{kalashnikov2018qt}
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog,
  Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent
  Vanhoucke, et~al.
\newblock Qt-opt: Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock {\em arXiv preprint arXiv:1806.10293}, 2018.

\bibitem{kostrikov2020image}
Ilya Kostrikov, Denis Yarats, and Rob Fergus.
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock {\em arXiv preprint arXiv:2004.13649}, 2020.

\bibitem{laskin2020reinforcement}
Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and
  Aravind Srinivas.
\newblock Reinforcement learning with augmented data.
\newblock {\em arXiv preprint arXiv:2004.14990}, 2020.

\bibitem{lee2019network}
Kimin Lee, Kibok Lee, Jinwoo Shin, and Honglak Lee.
\newblock Network randomization: A simple technique for generalization in deep
  reinforcement learning.
\newblock {\em arXiv}, pages arXiv--1910, 2019.

\bibitem{liu2020regularization}
Zhuang Liu, Xuanlin Li, Bingyi Kang, and Trevor Darrell.
\newblock Regularization matters in policy optimization -- an empirical study
  on continuous control, 2020.

\bibitem{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em nature}, 518(7540):529--533, 2015.

\bibitem{pinto2017asymmetric}
Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter
  Abbeel.
\newblock Asymmetric actor critic for image-based robot learning.
\newblock {\em arXiv preprint arXiv:1710.06542}, 2017.

\bibitem{raileanu2020automatic}
Roberta Raileanu, Max Goldstein, Denis Yarats, Ilya Kostrikov, and Rob Fergus.
\newblock Automatic data augmentation for generalization in deep reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:2006.12862}, 2020.

\bibitem{raparthy2020generating}
Sharath~Chandra Raparthy, Bhairav Mehta, Florian Golemo, and Liam Paull.
\newblock Generating automatic curricula via self-supervised active domain
  randomization, 2020.

\bibitem{schulman2015high}
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter
  Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock {\em arXiv preprint arXiv:1506.02438}, 2015.

\bibitem{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}, 2017.

\bibitem{silver2018general}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
  Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
  Graepel, et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and go through self-play.
\newblock {\em Science}, 362(6419):1140--1144, 2018.

\bibitem{silver2017mastering}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  et~al.
\newblock Mastering the game of go without human knowledge.
\newblock {\em nature}, 550(7676):354--359, 2017.

\bibitem{srinivas2020curl}
Aravind Srinivas, Michael Laskin, and Pieter Abbeel.
\newblock Curl: Contrastive unsupervised representations for reinforcement
  learning, 2020.

\bibitem{stooke2020decoupling}
Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin.
\newblock Decoupling representation learning from reinforcement learning.
\newblock {\em arXiv preprint arXiv:2009.08319}, 2020.

\bibitem{tobin2017domain}
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and
  Pieter Abbeel.
\newblock Domain randomization for transferring deep neural networks from
  simulation to the real world.
\newblock In {\em 2017 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pages 23--30. IEEE, 2017.

\bibitem{wu2021curricula}
Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur.
\newblock When do curricula work?, 2021.

\bibitem{yarats2020improving}
Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob
  Fergus.
\newblock Improving sample efficiency in model-free reinforcement learning from
  images, 2020.

\bibitem{you2019does}
Kaichao You, Mingsheng Long, Jianmin Wang, and Michael~I. Jordan.
\newblock How does learning rate decay help modern neural networks?, 2019.

\bibitem{yu2020gradient}
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and
  Chelsea Finn.
\newblock Gradient surgery for multi-task learning.
\newblock {\em arXiv preprint arXiv:2001.06782}, 2020.

\bibitem{zhao2021dataset}
Bo~Zhao and Hakan Bilen.
\newblock Dataset condensation with differentiable siamese augmentation.
\newblock In {\em International Conference on Machine Learning}, pages
  12674--12685. PMLR, 2021.

\end{thebibliography}
