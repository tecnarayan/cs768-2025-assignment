\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bengio et~al.(2013)Bengio, L{\'e}onard, and
  Courville]{bengio2013estimating}
Bengio, Y., L{\'e}onard, N., and Courville, A.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock \emph{arXiv preprint arXiv:1308.3432}, 2013.

\bibitem[Blalock et~al.(2020)Blalock, Ortiz, Frankle, and
  Guttag]{blalock2020state}
Blalock, D., Ortiz, J. J.~G., Frankle, J., and Guttag, J.
\newblock What is the state of neural network pruning?
\newblock \emph{arXiv preprint arXiv:2003.03033}, 2020.

\bibitem[Chmiel et~al.(2022)Chmiel, Hubara, Banner, and
  Soudry]{chmiel2022optimal}
Chmiel, B., Hubara, I., Banner, R., and Soudry, D.
\newblock Optimal fine-grained n: M sparsity for activations and neural
  gradients.
\newblock \emph{arXiv preprint arXiv:2203.10991}, 2022.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  248--255, 2009.

\bibitem[Dettmers \& Zettlemoyer(2019)Dettmers and
  Zettlemoyer]{dettmers2019sparse}
Dettmers, T. and Zettlemoyer, L.
\newblock Sparse networks from scratch: Faster training without losing
  performance.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Evci et~al.(2020)Evci, Gale, Menick, Castro, and
  Elsen]{evci2020rigging}
Evci, U., Gale, T., Menick, J., Castro, P.~S., and Elsen, E.
\newblock Rigging the lottery: Making all tickets winners.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  2943--2952, 2020.

\bibitem[Fang et~al.(2022)Fang, Zhou, and Wang]{fang2022algorithm}
Fang, C., Zhou, A., and Wang, Z.
\newblock An algorithm--hardware co-optimized framework for accelerating n: M
  sparse transformers.
\newblock \emph{IEEE Transactions on Very Large Scale Integration (VLSI)
  Systems}, 30\penalty0 (11):\penalty0 1573--1586, 2022.

\bibitem[Frankle \& Carbin(2019)Frankle and Carbin]{frankle2018lottery}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Gale et~al.(2019)Gale, Elsen, and Hooker]{gale2019state}
Gale, T., Elsen, E., and Hooker, S.
\newblock The state of sparsity in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1902.09574}, 2019.

\bibitem[Girshick et~al.(2014)Girshick, Donahue, Darrell, and
  Malik]{girshick2014rich}
Girshick, R., Donahue, J., Darrell, T., and Malik, J.
\newblock Rich feature hierarchies for accurate object detection and semantic
  segmentation.
\newblock In \emph{IEEE International Conference on Computer Vision (ICCV)},
  pp.\  580--587, 2014.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015learning}
Han, S., Pool, J., Tran, J., and Dally, W.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  1135--1143, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  770--778, 2016.

\bibitem[He et~al.(2017{\natexlab{a}})He, Gkioxari, Dollar, and
  Girshick]{he2017mask}
He, K., Gkioxari, G., Dollar, P., and Girshick, R.
\newblock Mask r-cnn.
\newblock In \emph{IEEE International Conference on Computer Vision (ICCV)},
  2017{\natexlab{a}}.

\bibitem[He et~al.(2017{\natexlab{b}})He, Zhang, and Sun]{he2017channel}
He, Y., Zhang, X., and Sun, J.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, pp.\  1389--1397, 2017{\natexlab{b}}.

\bibitem[Hoefler et~al.(2021)Hoefler, Alistarh, Ben-Nun, Dryden, and
  Peste]{hoefler2021sparsity}
Hoefler, T., Alistarh, D., Ben-Nun, T., Dryden, N., and Peste, A.
\newblock Sparsity in deep learning: Pruning and growth for efficient inference
  and training in neural networks.
\newblock \emph{Journal of Machine Learning Research}, 22:\penalty0 1â€“124,
  2021.

\bibitem[Hubara et~al.(2016)Hubara, Courbariaux, Soudry, El-Yaniv, and
  Bengio]{hubara2016binarized}
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y.
\newblock Binarized neural networks.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  29, 2016.

\bibitem[Hubara et~al.(2021)Hubara, Chmiel, Island, Banner, Naor, and
  Soudry]{hubara2021accelerated}
Hubara, I., Chmiel, B., Island, M., Banner, R., Naor, J., and Soudry, D.
\newblock Accelerated sparse neural training: A provable and efficient method
  to find n: m transposable masks.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  34, 2021.

\bibitem[Ji et~al.(2018)Ji, Liang, Deng, Zhang, Zhang, and Xie]{ji2018tetris}
Ji, Y., Liang, L., Deng, L., Zhang, Y., Zhang, Y., and Xie, Y.
\newblock Tetris: Tile-matching the tremendous irregular sparsity.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  31, 2018.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[LeCun et~al.(1989)LeCun, Denker, and Solla]{lecun1989optimal}
LeCun, Y., Denker, J., and Solla, S.
\newblock Optimal brain damage.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  598--605, 1989.

\bibitem[Li et~al.(2017)Li, Kadav, Durdanovic, Samet, and Graf]{li2016pruning}
Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H.~P.
\newblock Pruning filters for efficient convnets.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Lin et~al.(2020)Lin, Ji, Wang, Zhang, Zhang, Tian, and
  Shao]{lin2020hrank}
Lin, M., Ji, R., Wang, Y., Zhang, Y., Zhang, B., Tian, Y., and Shao, L.
\newblock Hrank: Filter pruning using high-rank feature map.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  1529--1538, 2020.

\bibitem[Liu et~al.(2019{\natexlab{a}})Liu, Mu, Zhang, Guo, Yang, Cheng, and
  Sun]{liu2019metapruning}
Liu, Z., Mu, H., Zhang, X., Guo, Z., Yang, X., Cheng, T. K.-T., and Sun, J.
\newblock Metapruning: Meta learning for automatic neural network channel
  pruning.
\newblock In \emph{IEEE International Conference on Computer Vision (ICCV)},
  pp.\  3296--3305, 2019{\natexlab{a}}.

\bibitem[Liu et~al.(2019{\natexlab{b}})Liu, Sun, Zhou, Huang, and
  Darrell]{liu2018rethinking}
Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T.
\newblock Rethinking the value of network pruning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019{\natexlab{b}}.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2016sgdr}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Meng et~al.(2020)Meng, Cheng, Li, Luo, Guo, Lu, and
  Sun]{meng2020pruning}
Meng, F., Cheng, H., Li, K., Luo, H., Guo, X., Lu, G., and Sun, X.
\newblock Pruning filter in filter.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  33:\penalty0 17629--17640, 2020.

\bibitem[Mostafa \& Wang(2019)Mostafa and Wang]{mostafa2019parameter}
Mostafa, H. and Wang, X.
\newblock Parameter efficient training of deep convolutional neural networks by
  dynamic sparse reparameterization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  4646--4655, 2019.

\bibitem[Nvidia(2020)]{nvidia2020a100}
Nvidia.
\newblock Nvidia a100 tensor core gpu architecture.
\newblock \url{https://www.nvidia.com/content/dam/en-
  zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf}, 2020.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{pytorch2015}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  8026--8037, 2019.

\bibitem[Pool \& Yu(2021)Pool and Yu]{pool2021channel}
Pool, J. and Yu, C.
\newblock Channel permutations for n: M sparsity.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  34, 2021.

\bibitem[Renda et~al.(2021)Renda, Frankle, and Carbin]{renda2020comparing}
Renda, A., Frankle, J., and Carbin, M.
\newblock Comparing rewinding and fine-tuning in neural network pruning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Ronny~Krashinsky(2020)]{ronny2020nvidia}
Ronny~Krashinsky, O. G. e.~a.
\newblock Nvidia ampere sparse tensor core.
\newblock \url{https://
  developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/}, 2020.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{sandler2018mobilenetv2}
Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  4510--4520, 2018.

\bibitem[Sanh et~al.(2020)Sanh, Wolf, and Rush]{sanh2020movement}
Sanh, V., Wolf, T., and Rush, A.~M.
\newblock Movement pruning: Adaptive sparsity by fine-tuning.
\newblock \emph{arXiv preprint arXiv:2005.07683}, 2020.

\bibitem[Sun et~al.(2021)Sun, Zhou, Stuijk, Wijnhoven, Nelson, Corporaal,
  et~al.]{sun2021dominosearch}
Sun, W., Zhou, A., Stuijk, S., Wijnhoven, R., Nelson, A.~O., Corporaal, H.,
  et~al.
\newblock Dominosearch: Find layer-wise fine-grained n: M sparse schemes from
  dense neural networks.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  34, 2021.

\bibitem[Tan \& Le(2019)Tan and Le]{tan2019efficientnet}
Tan, M. and Le, Q.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  6105--6114. PMLR, 2019.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J{\'e}gou]{touvron2021training}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J{\'e}gou,
  H.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  10347--10357, 2021.

\bibitem[Wightman(2019)]{rw2019timm}
Wightman, R.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem[Zhang et~al.(2022)Zhang, Lin, Lin, Luo, Li, Chao, Wu, and
  Ji]{zhang2022learning}
Zhang, Y., Lin, M., Lin, Z., Luo, Y., Li, K., Chao, F., Wu, Y., and Ji, R.
\newblock Learning best combination for efficient n: M sparsity.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2022.

\bibitem[Zhou et~al.(2021)Zhou, Ma, Zhu, Liu, Zhang, Yuan, Sun, and
  Li]{zhou2021learning}
Zhou, A., Ma, Y., Zhu, J., Liu, J., Zhang, Z., Yuan, K., Sun, W., and Li, H.
\newblock Learning n: M fine-grained structured sparse neural networks from
  scratch.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\end{thebibliography}
