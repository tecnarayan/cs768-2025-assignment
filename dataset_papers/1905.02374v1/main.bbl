\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2012)Agarwal, Wainwright, Bartlett, and
  Ravikumar]{agarwal2009information}
Agarwal, A., Wainwright, M.~J., Bartlett, P.~L., and Ravikumar, P.~K.
\newblock Information-theoretic lower bounds on the oracle complexity of convex
  optimization.
\newblock \emph{IEEE Transactions on Information Theory}, 58\penalty0
  (5):\penalty0 3235--3249, 2012.

\bibitem[Allen-Zhu(2017)]{accsvrg}
Allen-Zhu, Z.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock In \emph{Proceedings of Symposium on Theory of Computing (STOC)},
  2017.

\bibitem[Arjevani \& Shamir(2016)Arjevani and Shamir]{arjevani2016dimension}
Arjevani, Y. and Shamir, O.
\newblock Dimension-free iteration complexity of finite sum optimization
  problems.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2016.

\bibitem[Aybat et~al.(2019)Aybat, Fallah, Gurbuzbalaban, and
  Ozdaglar]{aybat2019universally}
Aybat, N.~S., Fallah, A., Gurbuzbalaban, M., and Ozdaglar, A.
\newblock A universally optimal multistage accelerated stochastic gradient
  method.
\newblock \emph{preprint arXiv:1901.08022}, 2019.

\bibitem[Beck \& Teboulle(2009)Beck and Teboulle]{fista}
Beck, A. and Teboulle, M.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock \emph{SIAM Journal on Imaging Sciences}, 2\penalty0 (1):\penalty0
  183--202, 2009.

\bibitem[Bietti \& Mairal(2017)Bietti and Mairal]{smiso}
Bietti, A. and Mairal, J.
\newblock Stochastic optimization with variance reduction for infinite datasets
  with finite-sum structure.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and
  Nocedal]{bottou2018optimization}
Bottou, L., Curtis, F.~E., and Nocedal, J.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{SIAM Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Cohen et~al.(2018)Cohen, Diakonikolas, and
  Orecchia]{cohen2018acceleration}
Cohen, M.~B., Diakonikolas, J., and Orecchia, L.
\newblock On acceleration with noise-corrupted gradients.
\newblock In \emph{Proceedings of the International Conferences on Machine
  Learning (ICML)}, 2018.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and Lacoste-Julien]{saga}
Defazio, A., Bach, F., and Lacoste-Julien, S.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2014.

\bibitem[Devolder(2011)]{devolder_2011}
Devolder, O.
\newblock Stochastic first order methods in smooth convex optimization.
\newblock CORE Discussion Papers 2011070, Universit√© catholique de Louvain,
  Center for Operations Research and Econometrics (CORE), 2011.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{spider2018}
Fang, C., Li, C.~J., Lin, Z., and Zhang, T.
\newblock Spider: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Ghadimi \& Lan(2012)Ghadimi and Lan]{ghadimi2012optimal}
Ghadimi, S. and Lan, G.
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization {I}: A generic algorithmic framework.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (4):\penalty0
  1469--1492, 2012.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013optimal}
Ghadimi, S. and Lan, G.
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization {II}: Shrinking procedures and optimal
  algorithms.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2061--2089, 2013.

\bibitem[Hiriart-Urruty \& Lemar\'echal(1996)Hiriart-Urruty and
  Lemar\'echal]{hiriart_urruty_lemarechal_1993ii}
Hiriart-Urruty, J.-B. and Lemar\'echal, C.
\newblock \emph{Convex analysis and minimization algorithms. {II}.}
\newblock Springer, 1996.

\bibitem[Hofmann et~al.(2015{\natexlab{a}})Hofmann, Lucchi, Lacoste-Julien, and
  McWilliams]{hofmann_variance_2015}
Hofmann, T., Lucchi, A., Lacoste-Julien, S., and McWilliams, B.
\newblock Variance reduced stochastic gradient descent with neighbors.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2015{\natexlab{a}}.

\bibitem[Hofmann et~al.(2015{\natexlab{b}})Hofmann, Lucchi, and
  McWilliams]{HofmannLM15}
Hofmann, T., Lucchi, A., and McWilliams, B.
\newblock Neighborhood watch: Stochastic gradient descent with neighbors.
\newblock \emph{CoRR}, abs/1506.03662, 2015{\natexlab{b}}.

\bibitem[Hu et~al.(2009)Hu, Pan, and Kwok]{kwok2009}
Hu, C., Pan, W., and Kwok, J.~T.
\newblock Accelerated gradient methods for stochastic optimization and online
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)}.
  2009.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{johnson2013accelerating}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2013.

\bibitem[Kovalev et~al.(2019)Kovalev, Horvath, and Richtarik]{kovalev2019don}
Kovalev, D., Horvath, S., and Richtarik, P.
\newblock Don't jump through hoops and remove those loops: {SVRG} and
  {Katyusha} are better without the outer loop.
\newblock \emph{preprint arXiv:1901.08689}, 2019.

\bibitem[Kulunchakov \& Mairal(2019)Kulunchakov and
  Mairal]{kulunchakov2019estimate}
Kulunchakov, A. and Mairal, J.
\newblock Estimate sequences for stochastic composite optimization: Variance
  reduction, acceleration, and robustness to noise.
\newblock \emph{preprint arXiv:1901.08788}, 2019.

\bibitem[Lan(2012)]{Lan2012}
Lan, G.
\newblock An optimal method for stochastic composite optimization.
\newblock \emph{Mathematical Programming}, 133\penalty0 (1):\penalty0 365--397,
  2012.

\bibitem[Lan \& Zhou(2018{\natexlab{a}})Lan and Zhou]{conjugategradient}
Lan, G. and Zhou, Y.
\newblock An optimal randomized incremental gradient method.
\newblock \emph{Mathematical Programming}, 171\penalty0 (1--2):\penalty0
  167--215, 2018{\natexlab{a}}.

\bibitem[Lan \& Zhou(2018{\natexlab{b}})Lan and Zhou]{lan_zhou_distributed2018}
Lan, G. and Zhou, Y.
\newblock Random gradient extrapolation for distributed and stochastic
  optimization.
\newblock \emph{{SIAM} Journal on Optimization}, 28\penalty0 (4):\penalty0
  2753--2782, 2018{\natexlab{b}}.

\bibitem[Lei et~al.(2017)Lei, Ju, Chen, and Jordan]{scsg2017}
Lei, L., Ju, C., Chen, J., and Jordan, M.~I.
\newblock Non-convex finite-sum optimization via {SCSG} methods.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)}.
  2017.

\bibitem[Lin et~al.(2014)Lin, Chen, and Pe{\~{n}}a]{lin_pena2014}
Lin, Q., Chen, X., and Pe{\~{n}}a, J.
\newblock A sparsity preserving stochastic gradient methods for sparse
  regression.
\newblock \emph{Computational Optimization and Applications}, 58\penalty0
  (2):\penalty0 455--482, 2014.

\bibitem[Lu \& Xiao(2015)Lu and Xiao]{Lu2015}
Lu, Z. and Xiao, L.
\newblock On the complexity analysis of randomized block-coordinate descent
  methods.
\newblock \emph{Mathematical Programming}, 152\penalty0 (1):\penalty0 615--642,
  2015.

\bibitem[Mairal(2015)]{miso}
Mairal, J.
\newblock Incremental majorization-minimization optimization with application
  to large-scale machine learning.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0
  829--855, 2015.

\bibitem[Mairal(2016)]{mairal2016end}
Mairal, J.
\newblock End-to-end kernel learning with supervised convolutional kernel
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2016.

\bibitem[Meinshausen \& B{\"u}hlmann(2010)Meinshausen and
  B{\"u}hlmann]{meinshausen2010stability}
Meinshausen, N. and B{\"u}hlmann, P.
\newblock Stability selection.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 72\penalty0 (4):\penalty0 417--473, 2010.

\bibitem[Moreau(1962)]{moreau1962fonctions}
Moreau, J.-J.
\newblock Fonctions convexes duales et points proximaux dans un espace
  hilbertien.
\newblock \emph{CR Acad. Sci. Paris S{\'e}r. A Math}, 255:\penalty0 2897--2899,
  1962.

\bibitem[Moreau(1965)]{moreau1965}
Moreau, J.-J.
\newblock Proximit{\'e} et dualit{\'e} dans un espace hilbertien.
\newblock \emph{Bull. Soc. Math. France}, 93\penalty0 (2):\penalty0 273--299,
  1965.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski}
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{{SIAM} Journal on Optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.

\bibitem[Nesterov(2004)]{nesterov}
Nesterov, Y.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Springer, 2004.

\bibitem[Nesterov(2013)]{nesterov2013gradient}
Nesterov, Y.
\newblock Gradient methods for minimizing composite functions.
\newblock \emph{Mathematical Programming}, 140\penalty0 (1):\penalty0 125--161,
  2013.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{sarah}
Nguyen, L.~M., Liu, J., Scheinberg, K., and Tak{\'a}{\v{c}}, M.
\newblock Sarah: A novel method for machine learning problems using stochastic
  recursive gradient.
\newblock In \emph{Proceedings of the International Conferences on Machine
  Learning (ICML)}, 2017.

\bibitem[Nguyen et~al.(2018)Nguyen, Nguyen, van Dijk, Richt{\'a}rik,
  Scheinberg, and Tak{\'a}{\v{c}}]{nguyen2018sgd}
Nguyen, L.~M., Nguyen, P.~H., van Dijk, M., Richt{\'a}rik, P., Scheinberg, K.,
  and Tak{\'a}{\v{c}}, M.
\newblock {SGD and Hogwild! convergence without the bounded gradients
  assumption}.
\newblock In \emph{Proceedings of the International Conferences on Machine
  Learning (ICML)}, 2018.

\bibitem[Paquette et~al.(2018)Paquette, Lin, Drusvyatskiy, Mairal, and
  Harchaoui]{paquette2017catalyst}
Paquette, C., Lin, H., Drusvyatskiy, D., Mairal, J., and Harchaoui, Z.
\newblock Catalyst acceleration for gradient-based non-convex optimization.
\newblock \emph{preprint arXiv:1703.10993}, 2018.

\bibitem[Schmidt et~al.(2015)Schmidt, Babanezhad, Ahmed, Defazio, Clifton, and
  Sarkar]{saganonu}
Schmidt, M., Babanezhad, R., Ahmed, M., Defazio, A., Clifton, A., and Sarkar,
  A.
\newblock Non-uniform stochastic average gradient method for training
  conditional random fields.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics (AISTATS)}, 2015.

\bibitem[Schmidt et~al.(2017)Schmidt, Le~Roux, and Bach]{schmidt2017minimizing}
Schmidt, M., Le~Roux, N., and Bach, F.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, 162\penalty0 (1-2):\penalty0
  83--112, 2017.

\bibitem[Shalev-Shwartz \& Zhang(2016)Shalev-Shwartz and Zhang]{accsdca}
Shalev-Shwartz, S. and Zhang, T.
\newblock Accelerated proximal stochastic dual coordinate ascent for
  regularized loss minimization.
\newblock \emph{Mathematical Programming}, 155\penalty0 (1):\penalty0 105--145,
  2016.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava_dropout:_2014}
Srivastava, N., Hinton, G.~E., Krizhevsky, A., Sutskever, I., and
  Salakhutdinov, R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Wainwright et~al.(2012)Wainwright, Jordan, and
  Duchi]{wainwright2012privacy}
Wainwright, M.~J., Jordan, M.~I., and Duchi, J.~C.
\newblock Privacy aware learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2012.

\bibitem[Xiao \& Zhang(2014)Xiao and Zhang]{proxsvrg}
Xiao, L. and Zhang, T.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock \emph{{SIAM} Journal on Optimization}, 24\penalty0 (4):\penalty0
  2057--2075, 2014.

\bibitem[Zheng \& Kwok(2018)Zheng and Kwok]{zheng2018lightweight}
Zheng, S. and Kwok, J.~T.
\newblock Lightweight stochastic optimization for minimizing finite sums with
  infinite data.
\newblock In \emph{Proceedings of the International Conferences on Machine
  Learning (ICML)}, 2018.

\bibitem[Zheng et~al.(2016)Zheng, Song, Leung, and
  Goodfellow]{zheng2016improving}
Zheng, S., Song, Y., Leung, T., and Goodfellow, I.
\newblock Improving the robustness of deep neural networks via stability
  training.
\newblock In \emph{Proceedings of the Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2016.

\bibitem[Zhou(2019)]{zhou2018direct}
Zhou, K.
\newblock Direct acceleration of {SAGA} using sampled negative momentum.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics (AISTATS)}, 2019.

\bibitem[Zhou et~al.(2018)Zhou, Shang, and Cheng]{MiG2018}
Zhou, K., Shang, F., and Cheng, J.
\newblock A simple stochastic variance reduced algorithm with fast convergence
  rates.
\newblock In \emph{Proceedings of the International Conferences on Machine
  Learning (ICML)}, 2018.

\end{thebibliography}
