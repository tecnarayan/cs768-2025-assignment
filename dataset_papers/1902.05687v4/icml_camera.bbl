\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adler \& Lunz(2018)Adler and Lunz]{adler2018banach}
Adler, J. and Lunz, S.
\newblock {Banach} {Wasserstein} {GAN}.
\newblock \emph{arXiv preprint arXiv:1806.06621}, 2018.

\bibitem[Arjovsky \& Bottou(2017)Arjovsky and Bottou]{principled_methods}
Arjovsky, M. and Bottou, L.
\newblock Towards principled methods for training generative adversarial
  networks.
\newblock In \emph{ICLR}, 2017.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and Bottou]{wgan}
Arjovsky, M., Chintala, S., and Bottou, L.
\newblock {Wasserstein} {GAN}.
\newblock \emph{arXiv preprint arXiv:1701.07875}, 2017.

\bibitem[Arora et~al.(2017)Arora, Ge, Liang, Ma, and
  Zhang]{arora2017generalization}
Arora, S., Ge, R., Liang, Y., Ma, T., and Zhang, Y.
\newblock Generalization and equilibrium in generative adversarial nets
  ({GANs}).
\newblock \emph{arXiv preprint arXiv:1703.00573}, 2017.

\bibitem[Bellemare et~al.(2017)Bellemare, Danihelka, Dabney, Mohamed,
  Lakshminarayanan, Hoyer, and Munos]{bellemare2017cramer}
Bellemare, M.~G., Danihelka, I., Dabney, W., Mohamed, S., Lakshminarayanan, B.,
  Hoyer, S., and Munos, R.
\newblock The {Cramer} distance as a solution to biased {Wasserstein}
  gradients.
\newblock \emph{arXiv preprint arXiv:1705.10743}, 2017.

\bibitem[Farnia \& Tse(2018)Farnia and Tse]{convex_duality}
Farnia, F. and Tse, D.
\newblock A convex duality framework for {GANs}.
\newblock In \emph{Advances in Neural Information Processing Systems 31}. 2018.

\bibitem[Fedus et~al.(2017)Fedus, Rosca, Lakshminarayanan, Dai, Mohamed, and
  Goodfellow]{fedus2017many}
Fedus, W., Rosca, M., Lakshminarayanan, B., Dai, A.~M., Mohamed, S., and
  Goodfellow, I.
\newblock Many paths to equilibrium: {GANs} do not need to decrease divergence
  at every step.
\newblock \emph{arXiv preprint arXiv:1710.08446}, 2017.

\bibitem[Goodfellow(2016)]{gan_tutorial}
Goodfellow, I.
\newblock Nips 2016 tutorial: Generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1701.00160}, 2016.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{gan}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2672--2680, 2014.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{wgangp}
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A.
\newblock Improved training of {Wasserstein} {GANs}.
\newblock \emph{arXiv preprint arXiv:1704.00028}, 2017.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel2017gans}
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S.
\newblock {GANs} trained by a two time-scale update rule converge to a local
  nash equilibrium.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6626--6637, 2017.

\bibitem[Karras et~al.(2017)Karras, Aila, Laine, and
  Lehtinen]{progressive_growing_gan}
Karras, T., Aila, T., Laine, S., and Lehtinen, J.
\newblock Progressive growing of {GANs} for improved quality, stability, and
  variation.
\newblock \emph{arXiv preprint arXiv:1710.10196}, 2017.

\bibitem[Kodali et~al.(2017)Kodali, Abernethy, Hays, and
  Kira]{kodali2017convergence}
Kodali, N., Abernethy, J., Hays, J., and Kira, Z.
\newblock On convergence and stability of {GANs}.
\newblock \emph{arXiv preprint arXiv:1705.07215}, 2017.

\bibitem[Lim \& Ye(2017)Lim and Ye]{geometric_gan}
Lim, J.~H. and Ye, J.~C.
\newblock Geometric {GAN}.
\newblock \emph{arXiv preprint arXiv:1705.02894}, 2017.

\bibitem[Liu et~al.(2017)Liu, Bousquet, and Chaudhuri]{liu2017approximation}
Liu, S., Bousquet, O., and Chaudhuri, K.
\newblock Approximation and convergence properties of generative adversarial
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5545--5553, 2017.

\bibitem[Lucic et~al.(2017)Lucic, Kurach, Michalski, Gelly, and
  Bousquet]{lucic2017gans}
Lucic, M., Kurach, K., Michalski, M., Gelly, S., and Bousquet, O.
\newblock Are {GANs} created equal? a large-scale study.
\newblock \emph{arXiv preprint arXiv:1711.10337}, 2017.

\bibitem[Mao et~al.(2016)Mao, Li, Xie, Lau, Wang, and Smolley]{lsgan}
Mao, X., Li, Q., Xie, H., Lau, R.~Y., Wang, Z., and Smolley, S.~P.
\newblock Least squares generative adversarial networks.
\newblock \emph{arXiv preprint ArXiv:1611.04076}, 2016.

\bibitem[Mescheder et~al.(2017)Mescheder, Nowozin, and
  Geiger]{mescheder2017numerics}
Mescheder, L., Nowozin, S., and Geiger, A.
\newblock The numerics of {GANs}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1825--1835, 2017.

\bibitem[Mescheder et~al.(2018)Mescheder, Geiger, and
  Nowozin]{mescheder2018training}
Mescheder, L., Geiger, A., and Nowozin, S.
\newblock Which training methods for {GANs} do actually converge?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3478--3487, 2018.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and Yoshida]{sngan}
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y.
\newblock Spectral normalization for generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1802.05957}, 2018.

\bibitem[Mroueh \& Sercu(2017)Mroueh and Sercu]{fishergan}
Mroueh, Y. and Sercu, T.
\newblock Fisher {GAN}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2510--2520, 2017.

\bibitem[Mroueh et~al.(2017)Mroueh, Li, Sercu, Raj, and Cheng]{sobolevgan}
Mroueh, Y., Li, C., Sercu, T., Raj, A., and Cheng, Y.
\newblock Sobolev {GAN}.
\newblock \emph{arXiv preprint arXiv:1711.04894}, 2017.

\bibitem[Nowozin et~al.(2016)Nowozin, Cseke, and Tomioka]{fgan}
Nowozin, S., Cseke, B., and Tomioka, R.
\newblock f-{GAN}: Training generative neural samplers using variational
  divergence minimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  271--279, 2016.

\bibitem[Odena et~al.(2018)Odena, Buckman, Olsson, Brown, Olah, Raffel, and
  Goodfellow]{odena2018generator}
Odena, A., Buckman, J., Olsson, C., Brown, T.~B., Olah, C., Raffel, C., and
  Goodfellow, I.
\newblock Is generator conditioning causally related to {GAN} performance?
\newblock \emph{arXiv preprint arXiv:1802.08768}, 2018.

\bibitem[Petzka et~al.(2017)Petzka, Fischer, and Lukovnicov]{wganlp}
Petzka, H., Fischer, A., and Lukovnicov, D.
\newblock On the regularization of {Wasserstein} {GANs}.
\newblock \emph{arXiv preprint arXiv:1709.08894}, 2017.

\bibitem[Qi(2017)]{qi2017loss}
Qi, G.-J.
\newblock Loss-sensitive generative adversarial networks on lipschitz
  densities.
\newblock \emph{arXiv preprint arXiv:1701.06264}, 2017.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{improved_gan}
Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen,
  X.
\newblock Improved techniques for training {GANs}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2226--2234, 2016.

\bibitem[Unterthiner et~al.(2017)Unterthiner, Nessler, Klambauer, Heusel,
  Ramsauer, and Hochreiter]{coulombgan}
Unterthiner, T., Nessler, B., Klambauer, G., Heusel, M., Ramsauer, H., and
  Hochreiter, S.
\newblock Coulomb {GANs}: Provably optimal nash equilibria via potential
  fields.
\newblock \emph{arXiv preprint arXiv:1708.08819}, 2017.

\bibitem[Villani(2008)]{oldandnew}
Villani, C.
\newblock \emph{Optimal Transport: Old and New}, volume 338.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Yadav et~al.(2017)Yadav, Shah, Xu, Jacobs, and
  Goldstein]{yadav2017stabilizing}
Yadav, A., Shah, S., Xu, Z., Jacobs, D., and Goldstein, T.
\newblock Stabilizing adversarial nets with prediction methods.
\newblock \emph{arXiv preprint arXiv:1705.07364}, 2017.

\bibitem[Zhang et~al.(2018)Zhang, Goodfellow, Metaxas, and
  Odena]{zhang2018self}
Zhang, H., Goodfellow, I., Metaxas, D., and Odena, A.
\newblock Self-attention generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1805.08318}, 2018.

\bibitem[Zhao et~al.(2016)Zhao, Mathieu, and LeCun]{energy_based_gan}
Zhao, J., Mathieu, M., and LeCun, Y.
\newblock Energy-based generative adversarial network.
\newblock \emph{arXiv preprint arXiv:1609.03126}, 2016.

\end{thebibliography}
