\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alon et~al.(2013)Alon, Cesa-Bianchi, Gentile, and
  Mansour]{AlonCesa-BianchiGentileMansour2013}
N.~Alon, N.~Cesa-Bianchi, C.~Gentile, and Y.~Mansour.
\newblock From bandits to experts: A tale of domination and independence.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1610--1618, 2013.

\bibitem[Arora et~al.(2019)Arora, Marinov, and Mohri]{AroraMarinovMohri2019}
R.~Arora, T.~V. Marinov, and M.~Mohri.
\newblock Bandits with feedback graphs and switching costs.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14
  December 2019, Vancouver, BC, Canada}, pages 10397--10407, 2019.

\bibitem[Azar et~al.(2012)Azar, Munos, and Kappen]{azar2012sample}
M.~G. Azar, R.~Munos, and H.~J. Kappen.
\newblock On the sample complexity of reinforcement learning with a generative
  model.
\newblock In \emph{Proceedings of the 29th International Coference on
  International Conference on Machine Learning}, pages 1707--1714. Omnipress,
  2012.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
M.~G. Azar, I.~Osband, and R.~Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  263--272, 2017.

\bibitem[Boutilier et~al.(1999)Boutilier, Dean, and
  Hanks]{boutilier1999decision}
C.~Boutilier, T.~Dean, and S.~Hanks.
\newblock Decision-theoretic planning: Structural assumptions and computational
  leverage.
\newblock \emph{Journal of Artificial Intelligence Research}, 11:\penalty0
  1--94, 1999.

\bibitem[Buccapatnam et~al.(2014)Buccapatnam, Eryilmaz, and
  Shroff]{BuccapatnamEryilmazShroff2014}
S.~Buccapatnam, A.~Eryilmaz, and N.~B. Shroff.
\newblock Stochastic bandits with side observations on networks.
\newblock In \emph{The 2014 ACM International Conference on Measurement and
  Modeling of Computer Systems}, SIGMETRICS '14, pages 289--300. ACM, 2014.

\bibitem[Caron et~al.(2012)Caron, Kveton, Lelarge, and
  Bhagat]{CaronKvetonLelargeBhagat2012}
S.~Caron, B.~Kveton, M.~Lelarge, and S.~Bhagat.
\newblock Leveraging side observations in stochastic bandits.
\newblock In \emph{UAI}, 2012.

\bibitem[Cohen et~al.(2016)Cohen, Hazan, and Koren]{CohenHazanKoren2016}
A.~Cohen, T.~Hazan, and T.~Koren.
\newblock Online learning with feedback graphs without the graphs.
\newblock In \emph{International Conference on Machine Learning}, pages
  811--819, 2016.

\bibitem[Cortes et~al.(2018)Cortes, DeSalvo, Gentile, Mohri, and
  Yang]{CortesDeSalvoGentileMohriYang2018}
C.~Cortes, G.~DeSalvo, C.~Gentile, M.~Mohri, and S.~Yang.
\newblock Online learning with abstention.
\newblock In \emph{35th ICML}, 2018.

\bibitem[Cortes et~al.(2019)Cortes, DeSalvo, Gentile, Mohri, and
  Yang]{CortesDeSalvoGentileMohriYang2019}
C.~Cortes, G.~DeSalvo, C.~Gentile, M.~Mohri, and S.~Yang.
\newblock Online learning with sleeping experts and feedback graphs.
\newblock In \emph{Proceedings of {ICML}}, pages 1370--1378, 2019.

\bibitem[Dann(2019)]{dann2019strategic}
C.~Dann.
\newblock \emph{Strategic Exploration in Reinforcement Learning - New
  Algorithms and Learning Guarantees}.
\newblock PhD thesis, Carnegie Mellon University, 2019.

\bibitem[Dann and Brunskill(2015)]{dann2015sample}
C.~Dann and E.~Brunskill.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2818--2826, 2015.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
C.~Dann, T.~Lattimore, and E.~Brunskill.
\newblock Unifying {PAC} and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5713--5723, 2017.

\bibitem[Dann et~al.(2018)Dann, Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{dann2018oracle}
C.~Dann, N.~Jiang, A.~Krishnamurthy, A.~Agarwal, J.~Langford, and R.~E.
  Schapire.
\newblock On oracle-efficient {PAC} reinforcement learning with rich
  observations.
\newblock \emph{arXiv preprint arXiv:1803.00606}, 2018.

\bibitem[Dann et~al.(2019)Dann, Li, Wei, and Brunskill]{dann2019policy}
C.~Dann, L.~Li, W.~Wei, and E.~Brunskill.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock \emph{International Conference on Machine Learning}, 2019.

\bibitem[Dong et~al.(2019)Dong, Van~Roy, and Zhou]{dong2019provably}
S.~Dong, B.~Van~Roy, and Z.~Zhou.
\newblock Provably efficient reinforcement learning with aggregated states.
\newblock \emph{arXiv preprint arXiv:1912.06366}, 2019.

\bibitem[Du et~al.(2019)Du, Krishnamurthy, Jiang, Agarwal, Dudik, and
  Langford]{du2019provably}
S.~Du, A.~Krishnamurthy, N.~Jiang, A.~Agarwal, M.~Dudik, and J.~Langford.
\newblock Provably efficient rl with rich observations via latent state
  decoding.
\newblock In \emph{International Conference on Machine Learning}, pages
  1665--1674, 2019.

\bibitem[Goel et~al.(2017)Goel, Dann, and Brunskill]{goel2017sample}
K.~Goel, C.~Dann, and E.~Brunskill.
\newblock Sample efficient policy search for optimal stopping domains.
\newblock In \emph{Proceedings of the 26th International Joint Conference on
  Artificial Intelligence}, pages 1711--1717. AAAI Press, 2017.

\bibitem[Henderson et~al.(2017)Henderson, Islam, Bachman, Pineau, Precup, and
  Meger]{henderson2017deep}
P.~Henderson, R.~Islam, P.~Bachman, J.~Pineau, D.~Precup, and D.~Meger.
\newblock Deep reinforcement learning that matters.
\newblock \emph{arXiv preprint arXiv:1709.06560}, 2017.

\bibitem[Howard et~al.(2018)Howard, Ramdas, Mc~Auliffe, and
  Sekhon]{howard2018uniform}
S.~R. Howard, A.~Ramdas, J.~Mc~Auliffe, and J.~Sekhon.
\newblock Uniform, nonparametric, non-asymptotic confidence sequences.
\newblock \emph{arXiv preprint arXiv:1810.08240}, 2018.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
N.~Jiang, A.~Krishnamurthy, A.~Agarwal, J.~Langford, and R.~E. Schapire.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \emph{International Conference on Machine Learning}, pages
  1704--1713, 2017.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
C.~Jin, Z.~Allen-Zhu, S.~Bubeck, and M.~I. Jordan.
\newblock Is {Q}-learning provably efficient?
\newblock \emph{arXiv preprint arXiv:1807.03765}, 2018.

\bibitem[Jin et~al.(2019)Jin, Yang, Wang, and Jordan]{jin2019provably}
C.~Jin, Z.~Yang, Z.~Wang, and M.~I. Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock \emph{arXiv preprint arXiv:1907.05388}, 2019.

\bibitem[Jin et~al.(2020)Jin, Krishnamurthy, Simchowitz, and Yu]{jin2020reward}
C.~Jin, A.~Krishnamurthy, M.~Simchowitz, and T.~Yu.
\newblock Reward-free exploration for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2002.02794}, 2020.

\bibitem[Koc{\'a}k et~al.(2016)Koc{\'a}k, Neu, and Valko]{kocak2016online}
T.~Koc{\'a}k, G.~Neu, and M.~Valko.
\newblock Online learning with noisy side observations.
\newblock In \emph{AISTATS}, pages 1186--1194, 2016.

\bibitem[Kostrikov et~al.(2020)Kostrikov, Yarats, and
  Fergus]{kostrikov2020image}
I.~Kostrikov, D.~Yarats, and R.~Fergus.
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock \emph{arXiv preprint arXiv:2004.13649}, 2020.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem[Laskin et~al.(2020)Laskin, Lee, Stooke, Pinto, Abbeel, and
  Srinivas]{laskin2020reinforcement}
M.~Laskin, K.~Lee, A.~Stooke, L.~Pinto, P.~Abbeel, and A.~Srinivas.
\newblock Reinforcement learning with augmented data.
\newblock \emph{arXiv preprint arXiv:2004.14990}, 2020.

\bibitem[Lattimore and Czepesvari(2018)]{lattimore2018bandit}
T.~Lattimore and C.~Czepesvari.
\newblock \emph{Bandit Algorithms}.
\newblock Cambridge University Press, 2018.

\bibitem[Lattimore and Hutter(2012)]{lattimore2012pac}
T.~Lattimore and M.~Hutter.
\newblock {PAC} bounds for discounted {MDP}s.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 320--334. Springer, 2012.

\bibitem[Lin et~al.(2019)Lin, Huang, Zimmer, Rojas, and Weng]{lin2019towards}
Y.~Lin, J.~Huang, M.~Zimmer, J.~Rojas, and P.~Weng.
\newblock Towards more sample efficiency in reinforcement learning with data
  augmentation.
\newblock \emph{arXiv preprint arXiv:1910.09959}, 2019.

\bibitem[Lykouris et~al.(2019)Lykouris, Tardos, and Wali]{lykouris2019graph}
T.~Lykouris, E.~Tardos, and D.~Wali.
\newblock Graph regret bounds for {T}hompson sampling and {UCB}.
\newblock \emph{arXiv preprint arXiv:1905.09898}, 2019.

\bibitem[Mannor and Shamir(2011)]{mannor2011bandits}
S.~Mannor and O.~Shamir.
\newblock From bandits to experts: On the value of side-observations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  684--692, 2011.

\bibitem[Mannor and Tsitsiklis(2004)]{mannor2004sample}
S.~Mannor and J.~N. Tsitsiklis.
\newblock The sample complexity of exploration in the multi-armed bandit
  problem.
\newblock \emph{Journal of Machine Learning Research}, 5\penalty0
  (Jun):\penalty0 623--648, 2004.

\bibitem[Maurer and Pontil(2009)]{maurer2009empirical}
A.~Maurer and M.~Pontil.
\newblock Empirical {B}ernstein bounds and sample variance penalization.
\newblock \emph{arXiv preprint arXiv:0907.3740}, 2009.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529, 2015.

\bibitem[Osband and Van~Roy(2016)]{osband2016lower}
I.~Osband and B.~Van~Roy.
\newblock On lower bounds for regret in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1608.02732}, 2016.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{ross2011reduction}
S.~Ross, G.~J. Gordon, and J.~A. Bagnell.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2011.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
D.~Silver, J.~Schrittwieser, K.~Simonyan, I.~Antonoglou, A.~Huang, A.~Guez,
  T.~Hubert, L.~Baker, M.~Lai, A.~Bolton, et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354, 2017.

\bibitem[Simchowitz and Jamieson(2019)]{simchowitz2019non}
M.~Simchowitz and K.~Jamieson.
\newblock Non-asymptotic gap-dependent regret bounds for tabular {MDP}s.
\newblock \emph{arXiv preprint arXiv:1905.03814}, 2019.

\bibitem[Zanette and Brunskill(2019)]{zanette2019tighter}
A.~Zanette and E.~Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock \emph{https://arxiv.org/abs/1901.00210}, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
H.~Zhang, M.~Cisse, Y.~N. Dauphin, and D.~Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\end{thebibliography}
