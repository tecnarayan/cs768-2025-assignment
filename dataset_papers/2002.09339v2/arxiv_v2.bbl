\begin{thebibliography}{10}

\bibitem{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em ICLR 2017, arXiv preprint arXiv:1611.03530}, 2016.

\bibitem{neyshabur2017exploring}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5947--5956, 2017.

\bibitem{vapnik1998statistical}
Vladimir~N. Vapnik.
\newblock {\em The nature of statistical learning theory}.
\newblock Wiley, New York, 1st edition, September 1998.

\bibitem{bartlett2002rademacher}
Peter~L Bartlett and Shahar Mendelson.
\newblock Rademacher and gaussian complexities: risk bounds and structural
  results.
\newblock {\em Journal of Machine Learning Research}, 3(Nov):463--482, 2002.

\bibitem{shalev2014understanding}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock {\em Understanding machine learning: from theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem{seung1992statistical}
Hyunjune~Sebastian Seung, Haim Sompolinsky, and Naftali Tishby.
\newblock Statistical mechanics of learning from examples.
\newblock {\em Physical review A}, 45(8):6056, 1992.

\bibitem{watkin1993statistical}
Timothy~LH Watkin, Albrecht Rau, and Michael Biehl.
\newblock The statistical mechanics of learning a rule.
\newblock {\em Reviews of Modern Physics}, 65(2):499, 1993.

\bibitem{advani2013statistical}
Madhu Advani, Subhaneil Lahiri, and Surya Ganguli.
\newblock Statistical mechanics of complex neural systems and high dimensional
  data.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2013(03):P03014, 2013.

\bibitem{advani2017high}
Madhu~S Advani and Andrew~M Saxe.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock {\em arXiv preprint arXiv:1710.03667}, 2017.

\bibitem{aubin2018committee}
Benjamin Aubin, Antoine Maillard, Florent Krzakala, Nicolas Macris, Lenka
  Zdeborov{\'a}, et~al.
\newblock The committee machine: computational to statistical gaps in learning
  a two-layers neural network.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3223--3234, 2018.

\bibitem{candes2020}
Emmanuel~J. Cand{\`e}s and Pragya Sur.
\newblock The phase transition for the existence of the maximum likelihood
  estimate in high-dimensional logistic regression.
\newblock {\em Ann. Statist.}, 48(1):27--42, 02 2020.

\bibitem{hastie2019surprises}
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan~J Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock {\em arXiv preprint arXiv:1903.08560}, 2019.

\bibitem{mei2019generalisation}
Song Mei and Andrea Montanari.
\newblock The generalization error of random features regression: precise
  asymptotics and double descent curve.
\newblock {\em arXiv preprint arXiv:1908.05355}, 2019.

\bibitem{goldt2019modelling}
Sebastian Goldt, Marc M{\'e}zard, Florent Krzakala, and Lenka Zdeborov{\'a}.
\newblock Modelling the influence of data structure on learning in neural
  networks.
\newblock {\em arXiv preprint arXiv:1909.11500}, 2019.

\bibitem{MezardHopfield}
Marc M{\'e}zard.
\newblock Mean-field message-passing equations in the hopfield model and its
  generalizations.
\newblock {\em Physical Review E}, 95(2):022117, 2017.

\bibitem{chizat2018note}
L\'{e}na\"{\i}c Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  2933--2943. 2019.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem{geiger2019disentangling}
Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart.
\newblock Disentangling feature and lazy learning in deep neural networks: an
  empirical study.
\newblock {\em arXiv preprint arXiv:1906.08034}, 2019.

\bibitem{louart2018random}
Cosme Louart, Zhenyu Liao, Romain Couillet, et~al.
\newblock A random matrix approach to neural networks.
\newblock {\em The Annals of Applied Probability}, 28(2):1190--1248, 2018.

\bibitem{NIPS2007_3182}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In {\em Advances in Neural Information Processing Systems 20}, pages
  1177--1184. 2008.

\bibitem{mezard1987spin}
Marc M{\'e}zard, Giorgio Parisi, and Miguel Virasoro.
\newblock {\em Spin glass theory and beyond: an introduction to the Replica
  Method and its applications}, volume~9.
\newblock World Scientific Publishing Company, 1987.

\bibitem{engel2001statistical}
Andreas Engel and Christian Van~den Broeck.
\newblock {\em Statistical mechanics of learning}.
\newblock Cambridge University Press, 2001.

\bibitem{zdeborova2016statistical}
Lenka Zdeborov{\'a} and Florent Krzakala.
\newblock Statistical physics of inference: thresholds and algorithms.
\newblock {\em Advances in Physics}, 65(5):453--552, 2016.

\bibitem{talagrand2006parisi}
Michel Talagrand.
\newblock The {P}arisi formula.
\newblock {\em Annals of mathematics}, 163:221--263, 2006.

\bibitem{barbier2019optimal}
Jean Barbier, Florent Krzakala, Nicolas Macris, L{\'e}o Miolane, and Lenka
  Zdeborov{\'a}.
\newblock Optimal errors and phase transitions in high-dimensional generalized
  linear models.
\newblock {\em Proceedings of the National Academy of Sciences},
  116(12):5451--5460, 2019.

\bibitem{GAN}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in neural information processing systems}, pages
  2672--2680, 2014.

\bibitem{VAE}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{el2020random}
Mohamed El~Amine Seddik, Cosme Louart, Mohamed Tamaazousti, and Romain
  Couillet.
\newblock Random matrix theory proves that deep learning representations of
  gan-data behave as gaussian mixtures.
\newblock {\em arXiv preprint arXiv:2001.08370}, 2020.

\bibitem{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock {\em ICLR 2019, arXiv preprint arXiv:1810.02054}, 2018.

\bibitem{allen2018convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In {\em International Conference on Machine Learning}, pages
  242--252, 2019.

\bibitem{woodworth2019kernel}
Blake Woodworth, Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and deep regimes in overparametrized models.
\newblock {\em arXiv preprint arXiv:1906.05827}, 2019.

\bibitem{montanari2019generalisation}
Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan.
\newblock The generalization error of max-margin linear classifiers:
  high-dimensional asymptotics in the overparametrized regime.
\newblock {\em arXiv preprint arXiv:1911.01544}, 2019.

\bibitem{belkin2018reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock {\em Proceedings of the National Academy of Sciences},
  116(32):15849--15854, 2019.

\bibitem{spigler2018jamming}
S~Spigler, M~Geiger, S~dâ€™Ascoli, L~Sagun, G~Biroli, and M~Wyart.
\newblock A jamming transition from under-to over-parametrization affects
  generalization in deep learning.
\newblock {\em Journal of Physics A: Mathematical and Theoretical},
  52(47):474001, 2019.

\bibitem{choromanski2017unreasonable}
Krzysztof~M Choromanski, Mark Rowland, and Adrian Weller.
\newblock The unreasonable effectiveness of structured random orthogonal
  embeddings.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  219--228, 2017.

\bibitem{gardner1989three}
Elizabeth Gardner and Bernard Derrida.
\newblock Three unfinished works on the optimal storage capacity of networks.
\newblock {\em Journal of Physics A: Mathematical and General}, 22(12):1983,
  1989.

\bibitem{kabashima2009typical}
Yoshiyuki Kabashima, Tadashi Wadayama, and Toshiyuki Tanaka.
\newblock A typical reconstruction limit for compressed sensing based on
  lp-norm minimization.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2009(09):L09003, 2009.

\bibitem{krzakala2012statistical}
Florent Krzakala, Marc M{\'e}zard, Fran{\c{c}}ois Sausset, YF~Sun, and Lenka
  Zdeborov{\'a}.
\newblock Statistical-physics-based reconstruction in compressed sensing.
\newblock {\em Physical Review X}, 2(2):021005, 2012.

\bibitem{cheng2013}
Xiuyuan Cheng and Amit Singer.
\newblock The spectrum of random inner-product kernel matrices.
\newblock {\em Random Matrices: Theory and Applications}, 02(04):1350010, 2013.

\bibitem{NIPS2017_6857}
Jeffrey Pennington and Pratik Worah.
\newblock Nonlinear random matrix theory for deep learning.
\newblock In {\em Advances in Neural Information Processing Systems 30}, pages
  2637--2646. 2017.

\bibitem{seddik2019kernel}
Mohamed El~Amine Seddik, Mohamed Tamaazousti, and Romain Couillet.
\newblock Kernel random matrices of large concentrated data: the example of
  gan-generated images.
\newblock In {\em ICASSP 2019-2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 7480--7484. IEEE, 2019.

\bibitem{neal2018modern}
Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna,
  Simon Lacoste-Julien, and Ioannis Mitliagkas.
\newblock A modern take on the bias-variance tradeoff in neural networks.
\newblock {\em arXiv preprint arXiv:1810.08591}, 2018.

\bibitem{geiger2019scaling}
Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun,
  St{\'e}phane d'Ascoli, Giulio Biroli, Cl{\'e}ment Hongler, and Matthieu
  Wyart.
\newblock Scaling description of generalization with number of parameters in
  deep learning.
\newblock {\em arXiv preprint arXiv:1901.01608}, 2019.

\bibitem{nakkiran2019deep}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya
  Sutskever.
\newblock Deep double descent: where bigger models and more data hurt.
\newblock {\em ICLR 2020, arXiv preprint arXiv:1912.02292}, 2019.

\bibitem{geman1992neural}
Stuart Geman, Elie Bienenstock, and Ren{\'e} Doursat.
\newblock Neural networks and the bias/variance dilemma.
\newblock {\em Neural computation}, 4(1):1--58, 1992.

\bibitem{breiman1995reflections}
Leo Breiman.
\newblock Reflections after refereeing papers for nips.
\newblock {\em The Mathematics of Generalization}, pages 11--15, 1995.

\bibitem{opper1996statistical}
Manfred Opper and Wolfgang Kinzel.
\newblock Statistical mechanics of generalization.
\newblock In {\em Models of neural networks III}, pages 151--209. Springer,
  1996.

\bibitem{cover1965geometrical}
Thomas~M Cover.
\newblock Geometrical and statistical properties of systems of linear
  inequalities with applications in pattern recognition.
\newblock {\em IEEE transactions on electronic computers}, EC-14(3):326--334,
  1965.

\bibitem{scholkopf2002learning}
Bernhard Sch{\"o}lkopf, Alexander~J Smola, Francis Bach, et~al.
\newblock {\em Learning with kernels: support vector machines, regularization,
  optimization, and beyond}.
\newblock MIT press, 2002.

\bibitem{rudi2017falkon}
Alessandro Rudi, Luigi Carratino, and Lorenzo Rosasco.
\newblock Falkon: An optimal large scale kernel method.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3888--3898, 2017.

\bibitem{caponnetto2007optimal}
Andrea Caponnetto and Ernesto De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock {\em Foundations of Computational Mathematics}, 7(3):331--368, 2007.

\bibitem{zhang2015divide}
Yuchen Zhang, John Duchi, and Martin Wainwright.
\newblock Divide and conquer kernel ridge regression: a distributed algorithm
  with minimax optimal rates.
\newblock {\em The Journal of Machine Learning Research}, 16(1):3299--3340,
  2015.

\bibitem{saade2016random}
Alaa Saade, Francesco Caltagirone, Igor Carron, Laurent Daudet, Ang{\'e}lique
  Dr{\'e}meau, Sylvain Gigan, and Florent Krzakala.
\newblock Random projections through multiple optical scattering: approximating
  kernels at the speed of light.
\newblock In {\em 2016 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 6215--6219. IEEE, 2016.

\bibitem{ohana2019kernel}
Ruben Ohana, Jonas Wacker, Jonathan Dong, S{\'e}bastien Marmin, Florent
  Krzakala, Maurizio Filippone, and Laurent Daudet.
\newblock Kernel computations from large-scale random features obtained by
  optical processing units.
\newblock {\em arXiv preprint arXiv:1910.09880}, 2019.

\bibitem{le2013fastfood}
Quoc Le, Tam{\'a}s Sarl{\'o}s, and Alex Smola.
\newblock Fastfood-approximating kernel expansions in loglinear time.
\newblock In {\em Proceedings of the international conference on machine
  learning}, volume~85, 2013.

\bibitem{andoni2015practical}
Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig
  Schmidt.
\newblock Practical and optimal lsh for angular distance.
\newblock In {\em Advances in neural information processing systems}, pages
  1225--1233, 2015.

\bibitem{bojarski2016structured}
Mariusz Bojarski, Anna Choromanska, Krzysztof Choromanski, Francois Fagan,
  Cedric Gouy-Pailler, Anne Morvan, Nourhan Sakr, Tamas Sarlos, and Jamal Atif.
\newblock Structured adaptive and random spinners for fast machine learning
  computations.
\newblock {\em arXiv preprint arXiv:1610.06209}, 2016.

\bibitem{hachem2007}
Walid Hachem, Philippe Loubaton, and Jamal Najim.
\newblock Deterministic equivalents for certain functionals of large random
  matrices.
\newblock {\em Ann. Appl. Probab.}, 17(3):875--930, 06 2007.

\bibitem{Fan2019}
Zhou Fan and Andrea Montanari.
\newblock The spectral norm of random inner-product kernel matrices.
\newblock {\em Probability Theory and Related Fields}, 173(1):27--85, Feb 2019.

\bibitem{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock {\em Journal of Machine Learning Research}, 12:2825--2830, 2011.

\bibitem{sklearn_api}
Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas
  Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre
  Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian
  Holt, and Ga{\"{e}}l Varoquaux.
\newblock {API} design for machine learning software: experiences from the
  scikit-learn project.
\newblock In {\em ECML PKDD Workshop: Languages for Data Mining and Machine
  Learning}, pages 108--122, 2013.

\end{thebibliography}
