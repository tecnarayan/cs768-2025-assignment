\begin{thebibliography}{82}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahmad et~al.(2021)Ahmad, Chakraborty, Ray, and Chang]{ahmad2021unified}
Ahmad, W.~U., Chakraborty, S., Ray, B., and Chang, K.-W.
\newblock Unified pre-training for program understanding and generation.
\newblock \emph{arXiv preprint arXiv:2103.06333}, 2021.

\bibitem[Allamanis et~al.(2016)Allamanis, Peng, and Sutton]{allamanis2016convolutional}
Allamanis, M., Peng, H., and Sutton, C.
\newblock A convolutional attention network for extreme summarization of source code.
\newblock In \emph{International conference on machine learning}, pp.\  2091--2100. PMLR, 2016.

\bibitem[Allamanis et~al.(2017)Allamanis, Brockschmidt, and Khademi]{allamanis2017learning}
Allamanis, M., Brockschmidt, M., and Khademi, M.
\newblock Learning to represent programs with graphs.
\newblock \emph{arXiv preprint arXiv:1711.00740}, 2017.

\bibitem[Alon et~al.(2018)Alon, Brody, Levy, and Yahav]{alon2018code2seq}
Alon, U., Brody, S., Levy, O., and Yahav, E.
\newblock code2seq: Generating sequences from structured representations of code.
\newblock \emph{arXiv preprint arXiv:1808.01400}, 2018.

\bibitem[Alon et~al.(2019)Alon, Zilberstein, Levy, and Yahav]{alon2019code2vec}
Alon, U., Zilberstein, M., Levy, O., and Yahav, E.
\newblock code2vec: Learning distributed representations of code.
\newblock \emph{Proceedings of the ACM on Programming Languages}, 3\penalty0 (POPL):\penalty0 1--29, 2019.

\bibitem[Arp et~al.(2022)Arp, Quiring, Pendlebury, Warnecke, Pierazzi, Wressnegger, Cavallaro, and Rieck]{arp2022and}
Arp, D., Quiring, E., Pendlebury, F., Warnecke, A., Pierazzi, F., Wressnegger, C., Cavallaro, L., and Rieck, K.
\newblock Dos and don'ts of machine learning in computer security.
\newblock In \emph{31st USENIX Security Symposium (USENIX Security 22)}, pp.\  3971--3988, 2022.

\bibitem[Basu et~al.(2023)Basu, Sattigeri, Ramamurthy, Chenthamarakshan, Varshney, Varshney, and Das]{basu2023equi}
Basu, S., Sattigeri, P., Ramamurthy, K.~N., Chenthamarakshan, V., Varshney, K.~R., Varshney, L.~R., and Das, P.
\newblock Equi-tuning: Group equivariant fine-tuning of pretrained models.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pp.\  6788--6796, 2023.

\bibitem[Bessey et~al.(2010)Bessey, Block, Chelf, Chou, Fulton, Hallem, Henri-Gros, Kamsky, McPeak, and Engler]{coverity:cacm}
Bessey, A., Block, K., Chelf, B., Chou, A., Fulton, B., Hallem, S., Henri-Gros, C., Kamsky, A., McPeak, S., and Engler, D.
\newblock A few billion lines of code later: using static analysis to find bugs in the real world.
\newblock \emph{Commun. ACM}, 53:\penalty0 66--75, February 2010.

\bibitem[Bieber et~al.(2020)Bieber, Sutton, Larochelle, and Tarlow]{bieber2020learning}
Bieber, D., Sutton, C., Larochelle, H., and Tarlow, D.
\newblock Learning to execute programs with instruction pointer attention graph neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 8626--8637, 2020.

\bibitem[Bieber et~al.(2022)Bieber, Goel, Zheng, Larochelle, and Tarlow]{bieber2022static}
Bieber, D., Goel, R., Zheng, D., Larochelle, H., and Tarlow, D.
\newblock Static prediction of runtime errors by learning to execute programs with external resource descriptions.
\newblock \emph{arXiv preprint arXiv:2203.03771}, 2022.

\bibitem[Biggs et~al.(1993)Biggs, Biggs, and Norman]{biggs1993algebraic}
Biggs, N., Biggs, N.~L., and Norman, B.
\newblock \emph{Algebraic graph theory}.
\newblock Number~67. Cambridge university press, 1993.

\bibitem[Bogatskiy et~al.(2020)Bogatskiy, Anderson, Offermann, Roussi, Miller, and Kondor]{bogatskiy2020lorentz}
Bogatskiy, A., Anderson, B., Offermann, J., Roussi, M., Miller, D., and Kondor, R.
\newblock Lorentz group equivariant neural network for particle physics.
\newblock In \emph{International Conference on Machine Learning}, pp.\  992--1002. PMLR, 2020.

\bibitem[Bronstein et~al.(2017)Bronstein, Bruna, LeCun, Szlam, and Vandergheynst]{bronstein2017geometric}
Bronstein, M.~M., Bruna, J., LeCun, Y., Szlam, A., and Vandergheynst, P.
\newblock Geometric deep learning: going beyond euclidean data.
\newblock \emph{IEEE Signal Processing Magazine}, 34\penalty0 (4):\penalty0 18--42, 2017.

\bibitem[Bundt et~al.(2022)Bundt, Davinroy, Agadakos, Oprea, and Robertson]{bundt2022black}
Bundt, J., Davinroy, M., Agadakos, I., Oprea, A., and Robertson, W.
\newblock Black-box attacks against neural binary function detection.
\newblock \emph{arXiv preprint arXiv:2208.11667}, 2022.

\bibitem[Chaudhuri et~al.(2021)Chaudhuri, Ellis, Polozov, Singh, Solar-Lezama, Yue, et~al.]{chaudhuri2021neurosymbolic}
Chaudhuri, S., Ellis, K., Polozov, O., Singh, R., Solar-Lezama, A., Yue, Y., et~al.
\newblock Neurosymbolic programming.
\newblock \emph{Foundations and Trends{\textregistered} in Programming Languages}, 7\penalty0 (3):\penalty0 158--243, 2021.

\bibitem[Chua et~al.(2017)Chua, Shen, Saxena, and Liang]{chua2017neural}
Chua, Z.~L., Shen, S., Saxena, P., and Liang, Z.
\newblock {Neural nets can learn function type signatures from binaries}.
\newblock In \emph{26th USENIX Security Symposium}, 2017.

\bibitem[Cohen \& Welling(2016)Cohen and Welling]{cohen2016group}
Cohen, T. and Welling, M.
\newblock Group equivariant convolutional networks.
\newblock In \emph{International conference on machine learning}, pp.\  2990--2999. PMLR, 2016.

\bibitem[Dehmamy et~al.(2021)Dehmamy, Walters, Liu, Wang, and Yu]{dehmamy2021automatic}
Dehmamy, N., Walters, R., Liu, Y., Wang, D., and Yu, R.
\newblock Automatic symmetry discovery with lie algebra convolutional network.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 2503--2515, 2021.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Ding et~al.(2023)Ding, Steenhoek, Pei, Kaiser, Le, and Ray]{ding2023traced}
Ding, Y., Steenhoek, B., Pei, K., Kaiser, G., Le, W., and Ray, B.
\newblock Traced: Execution-aware pre-training for source code.
\newblock \emph{arXiv preprint arXiv:2306.07487}, 2023.

\bibitem[Esteves et~al.(2018)Esteves, Allen-Blanchette, Makadia, and Daniilidis]{esteves2018learning}
Esteves, C., Allen-Blanchette, C., Makadia, A., and Daniilidis, K.
\newblock Learning so (3) equivariant representations with spherical cnns.
\newblock In \emph{Proceedings of the European Conference on Computer Vision (ECCV)}, pp.\  52--68, 2018.

\bibitem[Feng et~al.(2020)Feng, Guo, Tang, Duan, Feng, Gong, Shou, Qin, Liu, Jiang, et~al.]{feng2020codebert}
Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., et~al.
\newblock Codebert: A pre-trained model for programming and natural languages.
\newblock \emph{arXiv preprint arXiv:2002.08155}, 2020.

\bibitem[Fernandes et~al.(2018)Fernandes, Allamanis, and Brockschmidt]{fernandes2018structured}
Fernandes, P., Allamanis, M., and Brockschmidt, M.
\newblock Structured neural summarization.
\newblock \emph{arXiv preprint arXiv:1811.01824}, 2018.

\bibitem[Fort et~al.(2019)Fort, Hu, and Lakshminarayanan]{fort2019deep}
Fort, S., Hu, H., and Lakshminarayanan, B.
\newblock Deep ensembles: A loss landscape perspective.
\newblock \emph{arXiv preprint arXiv:1912.02757}, 2019.

\bibitem[Gao et~al.(2023{\natexlab{a}})Gao, Wang, and Wang]{gao2023discrete}
Gao, F., Wang, Y., and Wang, K.
\newblock Discrete adversarial attack to models of code.
\newblock \emph{Proceedings of the ACM on Programming Languages}, 7\penalty0 (PLDI):\penalty0 172--195, 2023{\natexlab{a}}.

\bibitem[Gao et~al.(2023{\natexlab{b}})Gao, Gao, Wang, Sun, Lo, and Yu]{gao2023two}
Gao, S., Gao, C., Wang, C., Sun, J., Lo, D., and Yu, Y.
\newblock Two sides of the same coin: Exploiting the impact of identifiers in neural code comprehension.
\newblock In \emph{2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)}, pp.\  1933--1945. IEEE, 2023{\natexlab{b}}.

\bibitem[Garcez et~al.(2022)Garcez, Bader, Bowman, Lamb, de~Penning, Illuminoo, Poon, and Zaverucha]{garcez2022neural}
Garcez, A.~d., Bader, S., Bowman, H., Lamb, L.~C., de~Penning, L., Illuminoo, B., Poon, H., and Zaverucha, C.~G.
\newblock Neural-symbolic learning and reasoning: A survey and interpretation.
\newblock \emph{Neuro-Symbolic Artificial Intelligence: The State of the Art}, 342\penalty0 (1):\penalty0 327, 2022.

\bibitem[Gordon et~al.(2019)Gordon, Lopez-Paz, Baroni, and Bouchacourt]{gordon2019permutation}
Gordon, J., Lopez-Paz, D., Baroni, M., and Bouchacourt, D.
\newblock Permutation equivariant models for compositional generalization in language.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Guo et~al.(2020)Guo, Ren, Lu, Feng, Tang, Liu, Zhou, Duan, Svyatkovskiy, Fu, et~al.]{guo2020graphcodebert}
Guo, D., Ren, S., Lu, S., Feng, Z., Tang, D., Liu, S., Zhou, L., Duan, N., Svyatkovskiy, A., Fu, S., et~al.
\newblock Graphcodebert: Pre-training code representations with data flow.
\newblock \emph{arXiv preprint arXiv:2009.08366}, 2020.

\bibitem[Guo et~al.(2022)Guo, Lu, Duan, Wang, Zhou, and Yin]{guo2022unixcoder}
Guo, D., Lu, S., Duan, N., Wang, Y., Zhou, M., and Yin, J.
\newblock Unixcoder: Unified cross-modal pre-training for code representation.
\newblock \emph{arXiv preprint arXiv:2203.03850}, 2022.

\bibitem[Guo et~al.(2019)Guo, Mu, Xing, Du, and Song]{guo2019vsa}
Guo, W., Mu, D., Xing, X., Du, M., and Song, D.
\newblock {DEEPVSA}: Facilitating value-set analysis with deep learning for postmortem program analysis.
\newblock In \emph{28th {USENIX} Security Symposium ({USENIX} Security 19)}, 2019.

\bibitem[Hellendoorn et~al.(2019)Hellendoorn, Sutton, Singh, Maniatis, and Bieber]{hellendoorn2019global}
Hellendoorn, V.~J., Sutton, C., Singh, R., Maniatis, P., and Bieber, D.
\newblock Global relational models of source code.
\newblock In \emph{International conference on learning representations}, 2019.

\bibitem[Henke et~al.(2022)Henke, Ramakrishnan, Wang, Albarghouth, Jha, and Reps]{henke2022semantic}
Henke, J., Ramakrishnan, G., Wang, Z., Albarghouth, A., Jha, S., and Reps, T.
\newblock Semantic robustness of models of source code.
\newblock In \emph{2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)}, pp.\  526--537. IEEE, 2022.

\bibitem[Higgins et~al.(2018)Higgins, Amos, Pfau, Racaniere, Matthey, Rezende, and Lerchner]{higgins2018towards}
Higgins, I., Amos, D., Pfau, D., Racaniere, S., Matthey, L., Rezende, D., and Lerchner, A.
\newblock Towards a definition of disentangled representations.
\newblock \emph{arXiv preprint arXiv:1812.02230}, 2018.

\bibitem[Hille \& Phillips(1996)Hille and Phillips]{hille1996functional}
Hille, E. and Phillips, R.~S.
\newblock \emph{Functional analysis and semi-groups}, volume~31.
\newblock American Mathematical Soc., 1996.

\bibitem[Hu et~al.(2016)Hu, Ma, Liu, Hovy, and Xing]{hu2016harnessing}
Hu, Z., Ma, X., Liu, Z., Hovy, E., and Xing, E.
\newblock Harnessing deep neural networks with logic rules.
\newblock \emph{arXiv preprint arXiv:1603.06318}, 2016.

\bibitem[HuggingFace \& ServiceNow(2022)HuggingFace and ServiceNow]{bigcode-project}
HuggingFace and ServiceNow.
\newblock {BigCode is an open scientific collaboration working on the responsible development and use of large language models for code}.
\newblock \url{https://www.bigcode-project.org/}, 2022.

\bibitem[Huh(2024)]{huh2024discovering}
Huh, D.
\newblock Discovering symmetry group structures via implicit orthogonality bias.
\newblock \emph{arXiv preprint arXiv:2402.17002}, 2024.

\bibitem[Hutchinson et~al.(2021)Hutchinson, Le~Lan, Zaidi, Dupont, Teh, and Kim]{hutchinson2021lietransformer}
Hutchinson, M.~J., Le~Lan, C., Zaidi, S., Dupont, E., Teh, Y.~W., and Kim, H.
\newblock Lietransformer: Equivariant self-attention for lie groups.
\newblock In \emph{International Conference on Machine Learning}, pp.\  4533--4543. PMLR, 2021.

\bibitem[Ji et~al.(2019)Ji, Xie, and Gao]{ji2019mathematical}
Ji, S., Xie, Y., and Gao, H.
\newblock A mathematical view of attention models in deep learning.
\newblock \emph{Texas A\&M University: College Station, TX, USA}, 2019.

\bibitem[Jin et~al.(2022)Jin, Pei, Won, and Lin]{jin2022symlm}
Jin, X., Pei, K., Won, J.~Y., and Lin, Z.
\newblock Symlm: Predicting function names in stripped binaries via context-sensitive execution-aware code embeddings.
\newblock In \emph{Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security}, pp.\  1631--1645, 2022.

\bibitem[Just et~al.(2014)Just, Jalali, and Ernst]{just2014defects4j}
Just, R., Jalali, D., and Ernst, M.~D.
\newblock Defects4j: A database of existing faults to enable controlled testing studies for java programs.
\newblock In \emph{Proceedings of the 2014 international symposium on software testing and analysis}, pp.\  437--440, 2014.

\bibitem[Kim et~al.(2021)Kim, Zhao, Tian, and Chandra]{kim2021code}
Kim, S., Zhao, J., Tian, Y., and Chandra, S.
\newblock Code prediction by feeding trees to transformers.
\newblock In \emph{2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)}, pp.\  150--162. IEEE, 2021.

\bibitem[Kim et~al.(2017)Kim, Denton, Hoang, and Rush]{kim2017structured}
Kim, Y., Denton, C., Hoang, L., and Rush, A.~M.
\newblock Structured attention networks.
\newblock \emph{arXiv preprint arXiv:1702.00887}, 2017.

\bibitem[Knuth(1970)]{knuth1970permutations}
Knuth, D.
\newblock Permutations, matrices, and generalized young tableaux.
\newblock \emph{Pacific journal of mathematics}, 34\penalty0 (3):\penalty0 709--727, 1970.

\bibitem[Lachaux et~al.(2021)Lachaux, Roziere, Szafraniec, and Lample]{lachaux2021dobf}
Lachaux, M.-A., Roziere, B., Szafraniec, M., and Lample, G.
\newblock Dobf: A deobfuscation pre-training objective for programming languages.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 14967--14979, 2021.

\bibitem[Lee et~al.(2019)Lee, Lee, Kim, Kosiorek, Choi, and Teh]{lee2019set}
Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh, Y.~W.
\newblock Set transformer: A framework for attention-based permutation-invariant neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  3744--3753. PMLR, 2019.

\bibitem[Li et~al.(2021)Li, Yu, and Yin]{li2021palmtree}
Li, X., Yu, Q., and Yin, H.
\newblock Palmtree: Learning an assembly language model for instruction embedding.
\newblock In \emph{2021 ACM SIGSAC Conference on Computer and Communications Security}, 2021.

\bibitem[Li et~al.(2023)Li, Huang, and Naik]{li2023scallop}
Li, Z., Huang, J., and Naik, M.
\newblock Scallop: A language for neurosymbolic programming.
\newblock \emph{Proceedings of the ACM on Programming Languages}, 7\penalty0 (PLDI):\penalty0 1463--1487, 2023.

\bibitem[Liu et~al.(2023)Liu, Metzman, and Chang]{oss_llm_fuzz}
Liu, D., Metzman, J., and Chang, O.
\newblock {AI-Powered Fuzzing: Breaking the Bug Hunting Barrier}.
\newblock \url{https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html}, 2023.

\bibitem[Luo et~al.(2023)Luo, Xu, Zhao, Sun, Geng, Hu, Tao, Ma, Lin, and Jiang]{luo2023wizardcoder}
Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., and Jiang, D.
\newblock Wizardcoder: Empowering code large language models with evol-instruct.
\newblock \emph{arXiv preprint arXiv:2306.08568}, 2023.

\bibitem[Maniatis \& Tarlow(2023)Maniatis and Tarlow]{didact-google}
Maniatis, P. and Tarlow, D.
\newblock {Large sequence models for software development activities}.
\newblock \url{https://ai.googleblog.com/2023/05/large-sequence-models-for-software.html?m=1}, 2023.

\bibitem[Marcelli et~al.(2022)Marcelli, Graziano, Ugarte-Pedrero, Fratantonio, Mansouri, and Balzarotti]{marcelli2022machine}
Marcelli, A., Graziano, M., Ugarte-Pedrero, X., Fratantonio, Y., Mansouri, M., and Balzarotti, D.
\newblock How machine learning is solving the binary function similarity problem.
\newblock In \emph{31st USENIX Security Symposium (USENIX Security 22)}, pp.\  2099--2116, 2022.

\bibitem[Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber, Dohan, Lewkowycz, Bosma, Luan, et~al.]{nye2021show}
Nye, M., Andreassen, A.~J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et~al.
\newblock Show your work: Scratchpads for intermediate computation with language models.
\newblock \emph{arXiv preprint arXiv:2112.00114}, 2021.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and Auli]{ott2019fairseq}
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M.
\newblock {Fairseq: A fast, extensible toolkit for sequence modeling}.
\newblock In \emph{2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations}, 2019.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Pei et~al.(2021)Pei, Guan, Broughton, Chen, Yao, Williams-King, Ummadisetty, Yang, Ray, and Jana]{pei2021stateformer}
Pei, K., Guan, J., Broughton, M., Chen, Z., Yao, S., Williams-King, D., Ummadisetty, V., Yang, J., Ray, B., and Jana, S.
\newblock Stateformer: fine-grained type recovery from binaries using generative state modeling.
\newblock In \emph{Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering}, pp.\  690--702, 2021.

\bibitem[Pei et~al.(2022)Pei, Xuan, Yang, Jana, and Ray]{pei2020trex}
Pei, K., Xuan, Z., Yang, J., Jana, S., and Ray, B.
\newblock Trex: Learning execution semantics from micro-traces for binary similarity.
\newblock \emph{IEEE Transactions on Software Engineering}, 2022.

\bibitem[Peng et~al.(2021)Peng, Li, Wang, Zhao, and Jin]{peng2021integrating}
Peng, H., Li, G., Wang, W., Zhao, Y., and Jin, Z.
\newblock Integrating tree path in transformer for code representation.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 9343--9354, 2021.

\bibitem[Perraudin et~al.(2019)Perraudin, Defferrard, Kacprzak, and Sgier]{perraudin2019deepsphere}
Perraudin, N., Defferrard, M., Kacprzak, T., and Sgier, R.
\newblock Deepsphere: Efficient spherical convolutional neural network with healpix sampling for cosmological applications.
\newblock \emph{Astronomy and Computing}, 27:\penalty0 130--146, 2019.

\bibitem[Rabin et~al.(2021)Rabin, Bui, Wang, Yu, Jiang, and Alipour]{rabin2021generalizability}
Rabin, M. R.~I., Bui, N.~D., Wang, K., Yu, Y., Jiang, L., and Alipour, M.~A.
\newblock On the generalizability of neural program models with respect to semantic-preserving program transformations.
\newblock \emph{Information and Software Technology}, 135:\penalty0 106552, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{2020t5}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0 (140):\penalty0 1--67, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Ramakrishnan et~al.(2020)Ramakrishnan, Henkel, Wang, Albarghouthi, Jha, and Reps]{ramakrishnan2020semantic}
Ramakrishnan, G., Henkel, J., Wang, Z., Albarghouthi, A., Jha, S., and Reps, T.
\newblock Semantic robustness of models of source code.
\newblock \emph{arXiv preprint arXiv:2002.03043}, 2020.

\bibitem[Reiser et~al.(2022)Reiser, Neubert, Eberhard, Torresi, Zhou, Shao, Metni, van Hoesel, Schopmans, Sommer, et~al.]{reiser2022graph}
Reiser, P., Neubert, M., Eberhard, A., Torresi, L., Zhou, C., Shao, C., Metni, H., van Hoesel, C., Schopmans, H., Sommer, T., et~al.
\newblock Graph neural networks for materials science and chemistry.
\newblock \emph{Communications Materials}, 3\penalty0 (1):\penalty0 93, 2022.

\bibitem[Romero \& Cordonnier(2020)Romero and Cordonnier]{romero2020group}
Romero, D.~W. and Cordonnier, J.-B.
\newblock Group equivariant stand-alone self-attention for vision.
\newblock \emph{arXiv preprint arXiv:2010.00977}, 2020.

\bibitem[Roziere et~al.(2023)Roziere, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Remez, Rapin, et~al.]{roziere2023code}
Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X.~E., Adi, Y., Liu, J., Remez, T., Rapin, J., et~al.
\newblock Code llama: Open foundation models for code.
\newblock \emph{arXiv preprint arXiv:2308.12950}, 2023.

\bibitem[Souza \& Pradel(2023)Souza and Pradel]{souza2023lexecutor}
Souza, B. and Pradel, M.
\newblock Lexecutor: Learning-guided execution.
\newblock In \emph{Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering}, pp.\  1522--1534, 2023.

\bibitem[Sun et~al.(2020)Sun, Zhu, Xiong, Sun, Mou, and Zhang]{sun2020treegen}
Sun, Z., Zhu, Q., Xiong, Y., Sun, Y., Mou, L., and Zhang, L.
\newblock Treegen: A tree-based transformer architecture for code generation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~34, pp.\  8984--8991, 2020.

\bibitem[Ullah et~al.(2023)Ullah, Han, Pujar, Pearce, Coskun, and Stringhini]{ullah2023can}
Ullah, S., Han, M., Pujar, S., Pearce, H., Coskun, A., and Stringhini, G.
\newblock Can large language models identify and reason about security vulnerabilities? not yet.
\newblock \emph{arXiv preprint arXiv:2312.12575}, 2023.

\bibitem[Wang \& Su(2020)Wang and Su]{wang2020blended}
Wang, K. and Su, Z.
\newblock Blended, precise semantic program embeddings.
\newblock In \emph{Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation}, pp.\  121--134, 2020.

\bibitem[Wang et~al.(2020)Wang, Walters, and Yu]{wang2020incorporating}
Wang, R., Walters, R., and Yu, R.
\newblock Incorporating symmetry into deep dynamics models for improved generalization.
\newblock \emph{arXiv preprint arXiv:2002.03061}, 2020.

\bibitem[Wang et~al.(2022)Wang, Li, Qian, Yang, Wang, Shang, Kumar, Tan, Ray, Bhatia, et~al.]{wang2022recode}
Wang, S., Li, Z., Qian, H., Yang, C., Wang, Z., Shang, M., Kumar, V., Tan, S., Ray, B., Bhatia, P., et~al.
\newblock Recode: Robustness evaluation of code generation models.
\newblock \emph{arXiv preprint arXiv:2212.10264}, 2022.

\bibitem[Wang et~al.(2021)Wang, Wang, Joty, and Hoi]{wang2021codet5}
Wang, Y., Wang, W., Joty, S., and Hoi, S.~C.
\newblock Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation.
\newblock \emph{arXiv preprint arXiv:2109.00859}, 2021.

\bibitem[Wen et~al.(2024)Wen, Yin, Shi, Michalewski, Chaudhuri, and Polozov]{wen2024grounding}
Wen, Y., Yin, P., Shi, K., Michalewski, H., Chaudhuri, S., and Polozov, A.
\newblock Grounding data science code generation with input-output specifications.
\newblock \emph{arXiv preprint arXiv:2402.08073}, 2024.

\bibitem[West et~al.(2001)]{west2001introduction}
West, D.~B. et~al.
\newblock \emph{Introduction to graph theory}, volume~2.
\newblock Prentice hall Upper Saddle River, 2001.

\bibitem[Xu et~al.(2017)Xu, Liu, Feng, Yin, Song, and Song]{xu2017neural}
Xu, X., Liu, C., Feng, Q., Yin, H., Song, L., and Song, D.
\newblock Neural network-based graph embedding for cross-platform binary code similarity detection.
\newblock In \emph{Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security}, pp.\  363--376, 2017.

\bibitem[Ye et~al.(2022)Ye, Martinez, and Monperrus]{ye2022neural}
Ye, H., Martinez, M., and Monperrus, M.
\newblock Neural program repair with execution-based backpropagation.
\newblock In \emph{Proceedings of the 44th international conference on software engineering}, pp.\  1506--1518, 2022.

\bibitem[Yefet et~al.(2020)Yefet, Alon, and Yahav]{yefet2020adversarial}
Yefet, N., Alon, U., and Yahav, E.
\newblock Adversarial examples for models of code.
\newblock \emph{Proceedings of the ACM on Programming Languages}, 4\penalty0 (OOPSLA):\penalty0 1--30, 2020.

\bibitem[Yi et~al.(2018)Yi, Wu, Gan, Torralba, Kohli, and Tenenbaum]{yi2018neural}
Yi, K., Wu, J., Gan, C., Torralba, A., Kohli, P., and Tenenbaum, J.
\newblock Neural-symbolic vqa: Disentangling reasoning from vision and language understanding.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Ying et~al.(2021)Ying, Cai, Luo, Zheng, Ke, He, Shen, and Liu]{ying2021transformers}
Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., and Liu, T.-Y.
\newblock Do transformers really perform badly for graph representation?
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 28877--28888, 2021.

\bibitem[Zhang(2017)]{hikari}
Zhang, N.
\newblock {Hikari -- an improvement over Obfuscator-LLVM}.
\newblock \url{https://github.com/HikariObfuscator/Hikari}, 2017.

\bibitem[Zhang et~al.(2023)Zhang, Tao, Shen, An, Xu, Liu, Ye, Wu, and Zhang]{zhang2023pelican}
Zhang, Z., Tao, G., Shen, G., An, S., Xu, Q., Liu, Y., Ye, Y., Wu, Y., and Zhang, X.
\newblock Pelican: Exploiting backdoors of naturally trained deep learning models in binary code analysis.
\newblock In \emph{32nd USENIX Security Symposium}, 2023.

\end{thebibliography}
