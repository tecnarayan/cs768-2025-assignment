\begin{thebibliography}{10}

\bibitem{smola2000sparse}
Alex~J Smola and Bernhard Sch{\"o}kopf.
\newblock Sparse greedy matrix approximation for machine learning.
\newblock In {\em Proceedings of the Seventeenth International Conference on
  Machine Learning}, pages 911--918, 2000.

\bibitem{fine2001efficient}
Shai Fine and Katya Scheinberg.
\newblock Efficient {SVM} training using low-rank kernel representations.
\newblock {\em Journal of Machine Learning Research}, 2(Dec):243--264, 2001.

\bibitem{rahimi2007random}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In {\em Advances in Neural Information Processing Systems}, 2007.

\bibitem{joachims2006training}
Thorsten Joachims.
\newblock Training linear {SVM}'s in linear time.
\newblock In {\em Proceedings of the 12th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pages 217--226, 2006.

\bibitem{minh2006mercer}
Ha~Quang Minh, Partha Niyogi, and Yuan Yao.
\newblock Mercer's theorem, feature maps, and smoothing.
\newblock In {\em International Conference on Computational Learning Theory},
  pages 154--168. Springer, 2006.

\bibitem{cotter2011explicit}
Andrew Cotter, Joseph Keshet, and Nathan Srebro.
\newblock Explicit approximations of the gaussian kernel.
\newblock {\em arXiv preprint arXiv:1109.4603}, 2011.

\bibitem{mallat1993matching}
St{\'e}phane~G Mallat and Zhifeng Zhang.
\newblock Matching pursuits with time-frequency dictionaries.
\newblock {\em IEEE Transactions on Signal Processing}, 41(12):3397--3415,
  1993.

\bibitem{tropp2004greed}
Joel~A Tropp.
\newblock Greed is good: Algorithmic results for sparse approximation.
\newblock {\em IEEE Transactions on Information Theory}, 50(10):2231--2242,
  2004.

\bibitem{belkin2018approximation}
Mikhail Belkin.
\newblock Approximation beats concentration? an approximation view on inference
  with smooth radial kernels.
\newblock {\em arXiv preprint arXiv:1801.03437}, 2018.

\bibitem{gonen2011multiple}
Mehmet G{\"o}nen and Ethem Alpayd{\i}n.
\newblock Multiple kernel learning algorithms.
\newblock {\em Journal of Machine Learning Research}, 12(Jul):2211--2268, 2011.

\bibitem{oppenheim1998senales}
Alan~V Oppenheim, Alan~S Willsky, and S~Hamid Nawab.
\newblock {\em Signals \& Systems}.
\newblock Pearson Educaci{\'o}n, 1998.

\bibitem{pati1993orthogonal}
Yagyensh~Chandra Pati, Ramin Rezaiifar, and Perinkulam~Sambamurthy
  Krishnaprasad.
\newblock Orthogonal matching pursuit: Recursive function approximation with
  applications to wavelet decomposition.
\newblock In {\em 1993 Conference Record of The Twenty-Seventh Asilomar
  Conference on Signals, Systems and Computers}, pages 40--44. IEEE, 1993.

\bibitem{davis1994adaptive}
Geoffrey~M Davis, Stephane~G Mallat, and Zhifeng Zhang.
\newblock Adaptive time-frequency decompositions.
\newblock {\em Optical Engineering}, 33(7):2183--2192, 1994.

\bibitem{wang2012generalized}
Jian Wang, Seokbeop Kwon, and Byonghyo Shim.
\newblock Generalized orthogonal matching pursuit.
\newblock {\em IEEE Transactions on Signal Processing}, 60(12):6202--6216,
  2012.

\bibitem{cortes2010generalization}
Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh.
\newblock Generalization bounds for learning kernels.
\newblock In {\em International Conference on Machine Learning}, pages
  247--254, 2010.

\bibitem{gribonval2006exponential}
R{\'e}mi Gribonval and Pierre Vandergheynst.
\newblock On the exponential convergence of matching pursuits in
  quasi-incoherent dictionaries.
\newblock {\em IEEE Transactions on Information Theory}, 52(1):255--261, 2006.

\bibitem{shalev2010trading}
Shai Shalev-Shwartz, Nathan Srebro, and Tong Zhang.
\newblock Trading accuracy for sparsity in optimization problems with sparsity
  constraints.
\newblock {\em SIAM Journal on Optimization}, 20(6):2807--2832, 2010.

\bibitem{williams2001using}
Christopher Williams and Matthias Seeger.
\newblock Using the {N}ystr{\"o}m method to speed up kernel machines.
\newblock In {\em Advances in Neural Information Processing Systems}, 2001.

\bibitem{drineas2005nystrom}
Petros Drineas and Michael~W Mahoney.
\newblock On the {N}ystr{\"o}m method for approximating a gram matrix for
  improved kernel-based learning.
\newblock {\em Journal of Machine Learning Research}, 6(Dec):2153--2175, 2005.

\bibitem{yang2004efficient}
Changjiang Yang, Ramani Duraiswami, and Larry Davis.
\newblock Efficient kernel machines using the improved fast gauss transform.
\newblock In {\em Proceedings of the 17th International Conference on Neural
  Information Processing Systems}, pages 1561--1568, 2004.

\bibitem{xu2006explicit}
Jian-Wu Xu, Puskal~P Pokharel, Kyu-Hwa Jeong, and Jose~C Principe.
\newblock An explicit construction of a reproducing gaussian kernel {H}ilbert
  space.
\newblock In {\em IEEE International Conference on Acoustics, Speech and Signal
  Processing}, volume~5, 2006.

\bibitem{vedaldi2012efficient}
Andrea Vedaldi and Andrew Zisserman.
\newblock Efficient additive kernels via explicit feature maps.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  34(3):480--492, 2012.

\bibitem{rahimi2009weighted}
Ali Rahimi and Benjamin Recht.
\newblock Weighted sums of random kitchen sinks: Replacing minimization with
  randomization in learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1313--1320, 2009.

\bibitem{yang2014quasi}
Jiyan Yang, Vikas Sindhwani, Haim Avron, and Michael Mahoney.
\newblock Quasi-monte carlo feature maps for shift-invariant kernels.
\newblock In {\em International Conference on Machine Learning}, pages
  485--493, 2014.

\bibitem{kar2012random}
Purushottam Kar and Harish Karnick.
\newblock Random feature maps for dot product kernels.
\newblock In {\em International conference on Artificial Intelligence and
  Statistics}, pages 583--591, 2012.

\bibitem{le2013fastfood}
Quoc Le, Tam{\'a}s Sarl{\'o}s, and Alex Smola.
\newblock Fastfood-approximating kernel expansions in loglinear time.
\newblock In {\em International Conference on Machine Learning}, volume~85,
  2013.

\bibitem{felix2016orthogonal}
X~Yu Felix, Ananda~Theertha Suresh, Krzysztof~M Choromanski, Daniel~N
  Holtmann-Rice, and Sanjiv Kumar.
\newblock Orthogonal random features.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1975--1983, 2016.

\bibitem{rudi2016generalization}
Alessandro Rudi and Lorenzo Rosasco.
\newblock Generalization properties of learning with random features.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3218--3228, 2017.

\bibitem{yen2014sparse}
Ian En-Hsu Yen, Ting-Wei Lin, Shou-De Lin, Pradeep~K Ravikumar, and Inderjit~S
  Dhillon.
\newblock Sparse random feature algorithm as coordinate descent in hilbert
  space.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2456--2464, 2014.

\bibitem{yu2015compact}
Felix~X Yu, Sanjiv Kumar, Henry Rowley, and Shih-Fu Chang.
\newblock Compact nonlinear maps and circulant extensions.
\newblock {\em arXiv preprint arXiv:1503.03893}, 2015.

\bibitem{yang2015carte}
Zichao Yang, Andrew Wilson, Alex Smola, and Le~Song.
\newblock A la carte--learning fast kernels.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1098--1106,
  2015.

\bibitem{oliva2016bayesian}
Junier~B Oliva, Avinava Dubey, Andrew~G Wilson, Barnab{\'a}s P{\'o}czos, Jeff
  Schneider, and Eric~P Xing.
\newblock Bayesian nonparametric kernel-learning.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1078--1086,
  2016.

\bibitem{chang2017data}
Wei-Cheng Chang, Chun-Liang Li, Yiming Yang, and Barnabas Poczos.
\newblock Data-driven random fourier features using stein effect.
\newblock {\em Proceedings of the Twenty-Sixth International Joint Conference
  on Artificial Intelligence (IJCAI-17)}, 2017.

\bibitem{sinha2016learning}
Aman Sinha and John~C Duchi.
\newblock Learning kernels with random features.
\newblock In {\em Advances In Neural Information Processing Systems}, pages
  1298--1306, 2016.

\bibitem{AAAI1816684}
Shahin Shahrampour, Ahmad Beirami, and Vahid Tarokh.
\newblock On data-dependent random features for improved generalization in
  supervised learning.
\newblock In {\em AAAI Conference on Artificial Intelligence}, 2018.

\bibitem{cdc2018}
Shahin Shahrampour, Ahmad Beirami, and Vahid Tarokh.
\newblock Supervised learning using data-dependent random features with
  application to seizure detection.
\newblock In {\em IEEE Conference on Decision and Control}, 2018.

\bibitem{bullins2017not}
Brian Bullins, Cyril Zhang, and Yi~Zhang.
\newblock Not-so-random features.
\newblock {\em International Conference on Learning Representations}, 2018.

\bibitem{friedman1981projection}
Jerome~H Friedman and Werner Stuetzle.
\newblock Projection pursuit regression.
\newblock {\em Journal of the American Statistical Association},
  76(376):817--823, 1981.

\bibitem{vincent2002kernel}
Pascal Vincent and Yoshua Bengio.
\newblock Kernel matching pursuit.
\newblock {\em Machine Learning}, 48(1-3):165--187, 2002.

\bibitem{nair2002some}
Prasanth~B Nair, Arindam Choudhury, and Andy~J Keane.
\newblock Some greedy learning algorithms for sparse regression and
  classification with mercer kernels.
\newblock {\em Journal of Machine Learning Research}, 3(Dec):781--801, 2002.

\bibitem{sindhwani2011non}
Vikas Sindhwani and Aur{\'e}lie~C Lozano.
\newblock Non-parametric group orthogonal matching pursuit for sparse learning
  with multiple kernels.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2519--2527, 2011.

\bibitem{lozano2011group}
Aurelie Lozano, Grzegorz Swirszcz, and Naoki Abe.
\newblock Group orthogonal matching pursuit for logistic regression.
\newblock In {\em Artificial Intelligence and Statistics}, pages 452--460,
  2011.

\bibitem{locatello2017unified}
Francesco Locatello, Rajiv Khanna, Michael Tschannen, and Martin Jaggi.
\newblock A unified optimization view on generalized matching pursuit and
  frank-wolfe.
\newblock In {\em Artificial Intelligence and Statistics}, pages 860--868,
  2017.

\bibitem{oglic2016greedy}
Dino Oglic and Thomas G{\"a}rtner.
\newblock Greedy feature construction.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3945--3953, 2016.

\bibitem{kandola2002optimizing}
Jaz Kandola, John Shawe-Taylor, and Nello Cristianini.
\newblock Optimizing kernel alignment over combinations of kernel.
\newblock 2002.

\bibitem{cortes2009learning}
Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh.
\newblock Learning non-linear combinations of kernels.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  396--404, 2009.

\bibitem{cortes2012algorithms}
Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh.
\newblock Algorithms for learning kernels based on centered alignment.
\newblock {\em Journal of Machine Learning Research}, 13(Mar):795--828, 2012.

\bibitem{kloft2011lp}
Marius Kloft, Ulf Brefeld, S{\"o}ren Sonnenburg, and Alexander Zien.
\newblock Lp-norm multiple kernel learning.
\newblock {\em Journal of Machine Learning Research}, 12(Mar):953--997, 2011.

\bibitem{lanckriet2004learning}
Gert~RG Lanckriet, Nello Cristianini, Peter Bartlett, Laurent~El Ghaoui, and
  Michael~I Jordan.
\newblock Learning the kernel matrix with semidefinite programming.
\newblock {\em Journal of Machine Learning Research}, 5(Jan):27--72, 2004.

\bibitem{bartlett2002rademacher}
Peter~L Bartlett and Shahar Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock {\em Journal of Machine Learning Research}, 3(Nov):463--482, 2002.

\bibitem{cortes2014deep}
Corinna Cortes, Mehryar Mohri, and Umar Syed.
\newblock Deep boosting.
\newblock In {\em International Conference on Machine Learning}, pages
  1179--1187, 2014.

\bibitem{cortes2017adanet}
Corinna Cortes, Xavier Gonzalvo, Vitaly Kuznetsov, Mehryar Mohri, and Scott
  Yang.
\newblock Adanet: Adaptive structural learning of artificial neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  874--883, 2017.

\bibitem{huang2018learning}
Furong Huang, Jordan Ash, John Langford, and Robert Schapire.
\newblock Learning deep resnet blocks sequentially using boosting theory.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{beirami2017optimal}
Ahmad Beirami, Meisam Razaviyayn, Shahin Shahrampour, and Vahid Tarokh.
\newblock On optimal generalizability in parametric learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3455--3465, 2017.

\bibitem{wang2018approximate}
Shuaiwen Wang, Wenda Zhou, Haihao Lu, Arian Maleki, and Vahab Mirrokni.
\newblock Approximate leave-one-out for fast parameter tuning in high
  dimensions.
\newblock {\em arXiv preprint arXiv:1807.02694}, 2018.

\bibitem{giordano2018return}
Ryan Giordano, Will Stephenson, Runjing Liu, Michael~I Jordan, and Tamara
  Broderick.
\newblock Return of the infinitesimal jackknife.
\newblock {\em arXiv preprint arXiv:1806.00550}, 2018.

\bibitem{mohri2012foundations}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock {\em Foundations of machine learning}.
\newblock MIT press, 2012.

\end{thebibliography}
