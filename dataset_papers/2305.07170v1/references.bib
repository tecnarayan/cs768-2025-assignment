@inproceedings{bengio2021flow,
    title={Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation},
    author={Emmanuel Bengio and Moksh Jain and Maksym Korablyov and Doina Precup and Yoshua Bengio},
    booktitle={Advances in Neural Information Processing Systems},
    editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
    year={2021},
    url={https://openreview.net/forum?id=Arn2E4IRjEB}
}

@article{gfnfoundations,
  author    = {Yoshua Bengio and
               Tristan Deleu and
               Edward J. Hu and
               Salem Lahlou and
               Mo Tiwari and
               Emmanuel Bengio},
  title     = {GFlowNet Foundations},
  journal   = {CoRR},
  volume    = {abs/2111.09266},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.09266},
  eprinttype = {arXiv},
  eprint    = {2111.09266},
}

@InCollection{stanford-compositionality,
	author       =	{Szabó, Zoltán Gendler},
	title        =	{{Compositionality}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	year         =	{2020},
	edition      =	{Fall 2020},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@inproceedings{andreas,
    title={Measuring Compositionality in Representation Learning},
    author={Jacob Andreas},
    editor={None},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=HJz05o0qK7},
}

@inproceedings{hupkes,
  title     = {Compositionality Decomposed: How do Neural Networks Generalise?},
  author    = {Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Christian Bessiere},
  pages     = {5065--5069},
  year      = {2020},
  month     = {7},
  note      = {Journal track},
  doi       = {10.24963/ijcai.2020/708},
  url       = {https://doi.org/10.24963/ijcai.2020/708},
}

@article{gfnbio,
  author    = {
        Moksh Jain and
        Emmanuel Bengio and
        Alex-Hernandez Garcia and
        Jarrid Rector-Brooks and
        Bonaventure F. P. Dossou and
        Chanakya Ekbote and
        Jie Fu and
        Tianyu Zhang and
        Micheal Kilgour and
        Dinghuai Zhang and
        Lena Simine and
        Payel Das and
        Yoshua Bengio
               },
  title     = {Biological Sequence Design with GFlowNets},
  volume    = {abs/2203.04115},
  year      = {2022},
  journal   = {arXiv},
  url       = {https://arxiv.org/abs/2203.04115},
  eprinttype = {arXiv},
  eprint    = {2203.04115},
}

@article{deleu,
  author    = {
        Tristan Deleu and
        António Góis and
        Chris Emezue and
        Mansi Rankawat and
        Simon Lacoste-Julien and
        Stefan Bauer and
        Yoshua Bengio
               },
  title     = {Bayesian Structure Learning with Generative Flow Networks},
  volume    = {abs/2202.13903},
  journal   = {arXiv},
  year      = {2022},
  url       = {https://arxiv.org/abs/2202.13903},
  eprinttype = {arXiv},
  eprint    = {2202.13903},
}

@inproceedings{tb,
  author    = {
        Nikolay Malkin and
        Moksh Jain and
        Emmanuel Bengio and
        Chen Sun and
        Yoshua Bengio
               },
  title     = {Trajectory Balance: Improved Credit Assignment in GFlowNets},
  volume    = {abs/2201.13259},
  year      = {2022},
  url       = {https://arxiv.org/abs/2201.13259},
  journal   = {arXiv},
  eprinttype = {arXiv},
  eprint    = {2201.13259},
  booktitle = {Advances in Neural Information Processing Systems},
}

@article{discrete-zhang,
  author    = {
        Dinghuai Zhang and
        Nikolay Malkin and
        Zhen Liu and
        Alexandra Volokhova and
        Aaron Courville and
        Yoshua Bengio
               },
  title     = {Generative Flow Networks for Discrete Probabilistic Modeling},
  volume    = {abs/2202.01361},
  journal   = {arXiv},
  year      = {2022},
  url       = {https://arxiv.org/abs/2202.01361},
  eprinttype = {arXiv},
  eprint    = {2202.01361},
}

@book{suttonbarto,
  author = {Sutton, Richard S. and Barto, Andrew G.},
  edition = {Second},
  keywords = {},
  publisher = {The MIT Press},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018}
}

@article{ucb,
  author    = {Niranjan Srinivas and
              Andreas Krause and
              Sham M. Kakade and
              Matthias W. Seeger},
  title     = {Gaussian Process Bandits without Regret: An Experimental Design Approach},
  journal   = {CoRR},
  volume    = {abs/0912.3995},
  year      = {2009},
  url       = {http://arxiv.org/abs/0912.3995},
  eprinttype = {arXiv},
  eprint    = {0912.3995},
}

@inproceedings{generalizationingenerativemodels,
 author = {Zhao, Shengjia and Ren, Hongyu and Yuan, Arianna and Song, Jiaming and Goodman, Noah and Ermon, Stefano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Bias and Generalization in Deep Generative Models: An Empirical Study},
 url = {https://proceedings.neurips.cc/paper/2018/file/5317b6799188715d5e00a638a4278901-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{sacha,
    author = {Sacha, Mikołaj and Błaż, Mikołaj and Byrski, Piotr and Dąbrowski-Tumański, Paweł and Chromiński, Mikołaj and Loska, Rafał and Włodarczyk-Pruszyński, Paweł and Jastrzębski, Stanisław},
    title = {Molecule Edit Graph Attention Network: Modeling Chemical Reactions as Sequences of Graph Edits},
    journal = {Journal of Chemical Information and Modeling},
    volume = {61},
    number = {7},
    pages = {3273-3284},
    year = {2021},
    doi = {10.1021/acs.jcim.1c00537},
        note ={PMID: 34251814},
    URL = { 
            https://doi.org/10.1021/acs.jcim.1c00537
    },
    eprint = { 
            https://doi.org/10.1021/acs.jcim.1c00537
    }
}

@inproceedings{maziarz,
    title={Learning to Extend Molecular Scaffolds with Structural Motifs},
    author={Krzysztof Maziarz and Henry Richard Jackson-Flux and Pashmina Cameron and Finton Sirockin and Nadine Schneider and Nikolaus Stiefl and Marwin Segler and Marc Brockschmidt},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=ZTsoE8G3GG}
}


@InProceedings{jtvae,
  title = 	 {Junction Tree Variational Autoencoder for Molecular Graph Generation},
  author =       {Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2323--2332},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/jin18a/jin18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/jin18a.html},
  abstract = 	 {We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.}
}


@InProceedings{welleck,
  title = 	 {Non-Monotonic Sequential Text Generation},
  author =       {Welleck, Sean and Brantley, Kiant{\'e} and Iii, Hal Daum{\'e} and Cho, Kyunghyun},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6716--6726},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/welleck19a/welleck19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/welleck19a.html},
  abstract = 	 {Standard sequential generation methods assume a pre-specified generation order, such as text generation methods which generate words from left to right. In this work, we propose a framework for training models of text generation that operate in non-monotonic orders; the model directly learns good orders, without any additional annotation. Our framework operates by generating a word at an arbitrary position, and then recursively generating words to its left and then words to its right, yielding a binary tree. Learning is framed as imitation learning, including a coaching method which moves from imitating an oracle to reinforcing the policy’s own preferences. Experimental results demonstrate that using the proposed method, it is possible to learn policies which generate text without pre-specifying a generation order, while achieving competitive performance with conventional left-to-right generation.}
}

@article{minsky,
  added-at = {2017-03-16T11:50:55.000+0100},
  author = {Minsky, Marvin},
  biburl = {https://www.bibsonomy.org/bibtex/2f1f2d84d704a03a03ef4e0282c44a6f3/krevelen},
  interhash = {e3ea30125d28343ae56ccc83d6e32cf9},
  intrahash = {f1f2d84d704a03a03ef4e0282c44a6f3},
  journal = {Proc. IRE},
  keywords = {imported thesis},
  month = jan,
  owner = {Rick},
  timestamp = {2017-03-16T11:54:14.000+0100},
  title = {Steps toward artificial intelligence},
  url = {http://web.media.mit.edu/~minsky/papers/steps.html},
  year = 1961
}

@misc{gfnpartial,
  doi = {10.48550/ARXIV.2209.12782},
  url = {https://arxiv.org/abs/2209.12782},
  author = {Madan, Kanika and Rector-Brooks, Jarrid and Korablyov, Maksym and Bengio, Emmanuel and Jain, Moksh and Nica, Andrei and Bosc, Tom and Bengio, Yoshua and Malkin, Nikolay},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning GFlowNets from partial episodes for improved convergence and stability},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{
nica2022evaluating,
title={Evaluating Generalization in {GF}lowNets for Molecule Design},
author={Andrei Cristian Nica and Moksh Jain and Emmanuel Bengio and Cheng-Hao Liu and Maksym Korablyov and Michael M. Bronstein and Yoshua Bengio},
booktitle={ICLR2022 Machine Learning for Drug Discovery},
year={2022},
url={https://openreview.net/forum?id=JFSaHKNZ35b}
}

@misc{prioritizedexpreplay,
  doi = {10.48550/ARXIV.1511.05952},
  
  url = {https://arxiv.org/abs/1511.05952},
  
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Prioritized Experience Replay},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{barrera,
  author = {Barrera LA and Vedenko A and Kurland JV and Rogers JM and Gisselbrecht SS and Rossin EJ and Woodard J and Mariani L and Kock KH and Inukai S and Siggers T and Shokri L and Gordân R and Sahni N and Cotsapas C and Hao T and Yi S and Kellis M and Daly MJ and Vidal M and Hill DE and Bulyk ML.},
  journal = {Science},
  month = Mar,
  title = {Survey of variation in human transcription factors reveals prevalent DNA binding changes},
  url = {http://web.media.mit.edu/~minsky/papers/steps.html},
  year = 2016,
}

@misc{designbench,
  doi = {10.48550/ARXIV.2202.08450},
  
  url = {https://arxiv.org/abs/2202.08450},
  
  author = {Trabucco, Brandon and Geng, Xinyang and Kumar, Aviral and Levine, Sergey},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Design-Bench: Benchmarks for Data-Driven Offline Model-Based Optimization},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{
    gflownetpathreg,
    title={Improving Generative Flow Networks with Path Regularization},
    author={Anonymous},
    booktitle={Submitted to The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=7qyLeRm1e3},
    note={under review}
}


@InProceedings{revisiting,
  title = 	 {Revisiting Fundamentals of Experience Replay},
  author =       {Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {3061--3071},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/fedus20a/fedus20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/fedus20a.html},
  abstract = 	 {Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: the replay capacity and the ratio of learning updates to experience collected (replay ratio). Our additive and ablative studies upend conventional wisdom around experience replay {—} greater capacity is found to substantially increase the performance of certain algorithms, while leaving others unaffected. Counterintuitively we show that theoretically ungrounded, uncorrected n-step returns are uniquely beneficial while other techniques confer limited benefit for sifting through larger memory. Separately, by directly controlling the replay ratio we contextualize previous observations in the literature and empirically measure its importance across a variety of deep RL algorithms. Finally, we conclude by testing a set of hypotheses on the nature of these performance benefits.}
}
