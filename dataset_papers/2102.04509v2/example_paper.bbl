\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Besag(1975)]{besag1975statistical}
Besag, J.
\newblock Statistical analysis of non-lattice data.
\newblock \emph{Journal of the Royal Statistical Society: Series D (The
  Statistician)}, 24\penalty0 (3):\penalty0 179--195, 1975.

\bibitem[Besag(1994)]{besag1994comments}
Besag, J.
\newblock Comments on “representations of knowledge in complex systems” by
  u. grenander and mi miller.
\newblock \emph{J. Roy. Statist. Soc. Ser. B}, 56:\penalty0 591--592, 1994.

\bibitem[Burda et~al.(2015)Burda, Grosse, and Salakhutdinov]{burda2015accurate}
Burda, Y., Grosse, R., and Salakhutdinov, R.
\newblock Accurate and conservative estimates of mrf log-likelihood using
  reverse annealing.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  102--110,
  2015.

\bibitem[Deng et~al.(2020)Deng, Bakhtin, Ott, Szlam, and
  Ranzato]{deng2020residual}
Deng, Y., Bakhtin, A., Ott, M., Szlam, A., and Ranzato, M.
\newblock Residual energy-based models for text generation.
\newblock \emph{arXiv preprint arXiv:2004.11714}, 2020.

\bibitem[Du \& Mordatch(2019)Du and Mordatch]{du2019implicit}
Du, Y. and Mordatch, I.
\newblock Implicit generation and generalization in energy-based models.
\newblock \emph{arXiv preprint arXiv:1903.08689}, 2019.

\bibitem[Gers et~al.(1999)Gers, Schmidhuber, and Cummins]{gers1999learning}
Gers, F.~A., Schmidhuber, J., and Cummins, F.
\newblock Learning to forget: Continual prediction with lstm.
\newblock 1999.

\bibitem[Grathwohl et~al.(2019)Grathwohl, Wang, Jacobsen, Duvenaud, Norouzi,
  and Swersky]{grathwohl2019your}
Grathwohl, W., Wang, K.-C., Jacobsen, J.-H., Duvenaud, D., Norouzi, M., and
  Swersky, K.
\newblock Your classifier is secretly an energy based model and you should
  treat it like one.
\newblock \emph{arXiv preprint arXiv:1912.03263}, 2019.

\bibitem[Grathwohl et~al.(2020{\natexlab{a}})Grathwohl, Kelly, Hashemi,
  Norouzi, Swersky, and Duvenaud]{grathwohl2020no}
Grathwohl, W., Kelly, J., Hashemi, M., Norouzi, M., Swersky, K., and Duvenaud,
  D.
\newblock No mcmc for me: Amortized sampling for fast and stable training of
  energy-based models.
\newblock \emph{arXiv preprint arXiv:2010.04230}, 2020{\natexlab{a}}.

\bibitem[Grathwohl et~al.(2020{\natexlab{b}})Grathwohl, Wang, Jacobsen,
  Duvenaud, and Zemel]{grathwohl2020learning}
Grathwohl, W., Wang, K.-C., Jacobsen, J.-H., Duvenaud, D., and Zemel, R.
\newblock Learning the stein discrepancy for training and evaluating
  energy-based models without sampling.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3732--3747. PMLR, 2020{\natexlab{b}}.

\bibitem[Gretton et~al.(2012)Gretton, Borgwardt, Rasch, Sch{\"o}lkopf, and
  Smola]{gretton2012kernel}
Gretton, A., Borgwardt, K.~M., Rasch, M.~J., Sch{\"o}lkopf, B., and Smola, A.
\newblock A kernel two-sample test.
\newblock \emph{The Journal of Machine Learning Research}, 13\penalty0
  (1):\penalty0 723--773, 2012.

\bibitem[Gutmann \& Hyv{\"a}rinen(2010)Gutmann and
  Hyv{\"a}rinen]{gutmann2010noise}
Gutmann, M. and Hyv{\"a}rinen, A.
\newblock Noise-contrastive estimation: A new estimation principle for
  unnormalized statistical models.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  297--304. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem[Han \& Liu(2018)Han and Liu]{han2018stein}
Han, J. and Liu, Q.
\newblock Stein variational gradient descent without gradient.
\newblock \emph{arXiv preprint arXiv:1806.02775}, 2018.

\bibitem[Han et~al.(2020)Han, Ding, Liu, Torresani, Peng, and
  Liu]{han2020stein}
Han, J., Ding, F., Liu, X., Torresani, L., Peng, J., and Liu, Q.
\newblock Stein variational inference for discrete distributions.
\newblock \emph{arXiv preprint arXiv:2003.00605}, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2021)He, McCann, Xiong, and Hosseini-Asl]{he2021joint}
He, T., McCann, B., Xiong, C., and Hosseini-Asl, E.
\newblock Joint energy-based model training for better calibrated natural
  language understanding models.
\newblock \emph{arXiv preprint arXiv:2101.06829}, 2021.

\bibitem[Hill et~al.(2020)Hill, Mitchell, and Zhu]{hill2020stochastic}
Hill, M., Mitchell, J., and Zhu, S.-C.
\newblock Stochastic security: Adversarial defense using long-run dynamics of
  energy-based models.
\newblock \emph{arXiv preprint arXiv:2005.13525}, 2020.

\bibitem[Hinton(2002)]{hinton2002training}
Hinton, G.~E.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural computation}, 14\penalty0 (8):\penalty0 1771--1800,
  2002.

\bibitem[Hinton(2009)]{hinton2009deep}
Hinton, G.~E.
\newblock Deep belief networks.
\newblock \emph{Scholarpedia}, 4\penalty0 (5):\penalty0 5947, 2009.

\bibitem[Ingraham \& Marks(2017)Ingraham and Marks]{ingraham2017variational}
Ingraham, J. and Marks, D.
\newblock Variational inference for sparse and undirected models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1607--1616. PMLR, 2017.

\bibitem[Ising(1924)]{ising1924beitrag}
Ising, E.
\newblock \emph{Beitrag zur Theorie des Ferround Paramagnetismus}.
\newblock PhD thesis, PhD thesis (Mathematisch-Naturwissenschaftliche
  Fakult{\"a}t der Hamburgischen~…, 1924.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Lapedes et~al.(1999)Lapedes, Giraud, Liu, and
  Stormo]{lapedes1999correlated}
Lapedes, A.~S., Giraud, B.~G., Liu, L., and Stormo, G.~D.
\newblock Correlated mutations in models of protein sequences: phylogenetic and
  structural effects.
\newblock \emph{Lecture Notes-Monograph Series}, pp.\  236--256, 1999.

\bibitem[{\L}atuszy{\'n}ski et~al.(2013){\L}atuszy{\'n}ski, Roberts, Rosenthal,
  et~al.]{latuszynski2013adaptive}
{\L}atuszy{\'n}ski, K., Roberts, G.~O., Rosenthal, J.~S., et~al.
\newblock Adaptive gibbs samplers and related mcmc methods.
\newblock \emph{The Annals of Applied Probability}, 23\penalty0 (1):\penalty0
  66--98, 2013.

\bibitem[Levin \& Peres(2017)Levin and Peres]{levin2017markov}
Levin, D.~A. and Peres, Y.
\newblock \emph{Markov chains and mixing times}, volume 107.
\newblock American Mathematical Soc., 2017.

\bibitem[Liu(1996)]{liu1996peskun}
Liu, J.~S.
\newblock Peskun's theorem and a modified discrete-state gibbs sampler.
\newblock \emph{Biometrika}, 83\penalty0 (3), 1996.

\bibitem[Liu \& Wang(2016)Liu and Wang]{liu2016stein}
Liu, Q. and Wang, D.
\newblock Stein variational gradient descent: A general purpose bayesian
  inference algorithm.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 2378--2386, 2016.

\bibitem[Lyu(2012)]{lyu2012interpretation}
Lyu, S.
\newblock Interpretation and generalization of score matching.
\newblock \emph{arXiv preprint arXiv:1205.2629}, 2012.

\bibitem[Maddison et~al.(2016)Maddison, Mnih, and Teh]{maddison2016concrete}
Maddison, C.~J., Mnih, A., and Teh, Y.~W.
\newblock The concrete distribution: A continuous relaxation of discrete random
  variables.
\newblock \emph{arXiv preprint arXiv:1611.00712}, 2016.

\bibitem[Marks et~al.(2011)Marks, Colwell, Sheridan, Hopf, Pagnani, Zecchina,
  and Sander]{marks2011protein}
Marks, D.~S., Colwell, L.~J., Sheridan, R., Hopf, T.~A., Pagnani, A., Zecchina,
  R., and Sander, C.
\newblock Protein 3d structure computed from evolutionary sequence variation.
\newblock \emph{PloS one}, 6\penalty0 (12):\penalty0 e28766, 2011.

\bibitem[Neal(2001)]{neal2001annealed}
Neal, R.~M.
\newblock Annealed importance sampling.
\newblock \emph{Statistics and computing}, 11\penalty0 (2):\penalty0 125--139,
  2001.

\bibitem[Neal et~al.(2011)]{neal2011mcmc}
Neal, R.~M. et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock \emph{Handbook of markov chain monte carlo}, 2\penalty0
  (11):\penalty0 2, 2011.

\bibitem[Nijkamp et~al.(2020)Nijkamp, Hill, Han, Zhu, and
  Wu]{nijkamp2020anatomy}
Nijkamp, E., Hill, M., Han, T., Zhu, S.-C., and Wu, Y.~N.
\newblock On the anatomy of mcmc-based maximum likelihood learning of
  energy-based models.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  5272--5280, 2020.

\bibitem[Nishimura et~al.(2017)Nishimura, Dunson, and
  Lu]{nishimura2017discontinuous}
Nishimura, A., Dunson, D., and Lu, J.
\newblock Discontinuous hamiltonian monte carlo for sampling discrete
  parameters.
\newblock \emph{arXiv preprint arXiv:1705.08510}, 2, 2017.

\bibitem[Ramachandran et~al.(2017)Ramachandran, Zoph, and
  Le]{ramachandran2017searching}
Ramachandran, P., Zoph, B., and Le, Q.~V.
\newblock Searching for activation functions.
\newblock \emph{arXiv preprint arXiv:1710.05941}, 2017.

\bibitem[Richardson et~al.(2010)Richardson, Bottolo, and
  Rosenthal]{richardson2010bayesian}
Richardson, S., Bottolo, L., and Rosenthal, J.~S.
\newblock Bayesian models for sparse regression analysis of high dimensional
  data.
\newblock \emph{Bayesian Statistics}, 9:\penalty0 539--569, 2010.

\bibitem[Song \& Ermon(2019)Song and Ermon]{song2019generative}
Song, Y. and Ermon, S.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  11918--11930, 2019.

\bibitem[Song \& Ou(2018)Song and Ou]{song2018learning}
Song, Y. and Ou, Z.
\newblock Learning neural random fields with inclusive auxiliary generators.
\newblock \emph{arXiv preprint arXiv:1806.00271}, 2018.

\bibitem[Taylor et~al.(2003)Taylor, Marcus, and Santorini]{taylor2003penn}
Taylor, A., Marcus, M., and Santorini, B.
\newblock The penn treebank: an overview.
\newblock \emph{Treebanks}, pp.\  5--22, 2003.

\bibitem[Tieleman(2008)]{tieleman2008training}
Tieleman, T.
\newblock Training restricted boltzmann machines using approximations to the
  likelihood gradient.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pp.\  1064--1071, 2008.

\bibitem[Tieleman \& Hinton(2009)Tieleman and Hinton]{tieleman2009using}
Tieleman, T. and Hinton, G.
\newblock Using fast weights to improve persistent contrastive divergence.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, pp.\  1033--1040, 2009.

\bibitem[Titsias \& Yau(2017)Titsias and Yau]{titsias2017hamming}
Titsias, M.~K. and Yau, C.
\newblock The hamming ball sampler.
\newblock \emph{Journal of the American Statistical Association}, 112\penalty0
  (520):\penalty0 1598--1611, 2017.

\bibitem[Tomczak \& Welling(2018)Tomczak and Welling]{tomczak2018vae}
Tomczak, J. and Welling, M.
\newblock Vae with a vampprior.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1214--1223. PMLR, 2018.

\bibitem[Umrigar(1993)]{umrigar1993accmet}
Umrigar, C.~J.
\newblock Accelerated metropolis method.
\newblock \emph{Phys. Rev. Lett.}, 71:\penalty0 408--411, Jul 1993.
\newblock \doi{10.1103/PhysRevLett.71.408}.
\newblock URL \url{https://link.aps.org/doi/10.1103/PhysRevLett.71.408}.

\bibitem[Zanella(2020)]{zanella2020informed}
Zanella, G.
\newblock Informed proposals for local mcmc in discrete spaces.
\newblock \emph{Journal of the American Statistical Association}, 115\penalty0
  (530):\penalty0 852--865, 2020.

\end{thebibliography}
