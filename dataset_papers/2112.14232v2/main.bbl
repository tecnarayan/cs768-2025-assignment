\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akhtar et~al.(2021)Akhtar, Mian, Kardan, and
  Shah]{arxiv21:Akhtar2021AdvancesIA}
Akhtar, N., Mian, A.~S., Kardan, N., and Shah, M.
\newblock Advances in adversarial attacks and defenses in computer vision: A
  survey.
\newblock \emph{ArXiv}, 2021.
\newblock \url{https://arxiv.org/abs/2108.00401}.

\bibitem[Athalye \& Carlini(2018)Athalye and Carlini]{arxiv18:Athalye2018OnTR}
Athalye, A. and Carlini, N.
\newblock On the robustness of the cvpr 2018 white-box adversarial example
  defenses.
\newblock \emph{ArXiv}, abs/1804.03286, 2018.

\bibitem[Athalye et~al.(2018)Athalye, Carlini, and Wagner]{icml18:Obfuscated}
Athalye, A., Carlini, N., and Wagner, D.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock In \emph{International conference on machine learning}, 2018.
\newblock \url{https://arxiv.org/abs/1802.00420}.

\bibitem[Bai~Li \& Carin(2020)Bai~Li and Carin]{arxiv20:CompareDefenses}
Bai~Li, Shiqi~Wang, S.~J. and Carin, L.
\newblock Towards understanding fast adversarial training.
\newblock \emph{ArXiv}, abs/2006.03089, 2020.

\bibitem[Bonnet et~al.(2020)Bonnet, Furon, and Bas]{ihmmsec20:linfrounding}
Bonnet, B., Furon, T., and Bas, P.
\newblock What if adversarial samples were digital images?
\newblock In \emph{IH{\&}MMSEC}, 2020.

\bibitem[Carlini \& Wagner(2017{\natexlab{a}})Carlini and
  Wagner]{Oakland17:CarliniWagner}
Carlini, N. and Wagner, D.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{IEEE Symposium on Security and Privacy}, volume~1, pp.\
  39--57, 2017{\natexlab{a}}.
\newblock \url{https://arxiv.org/abs/1608.04644}.

\bibitem[Carlini \& Wagner(2017{\natexlab{b}})Carlini and
  Wagner]{aisec17:detection}
Carlini, N. and Wagner, D.
\newblock Adversarial examples are not easily detected: {B}ypassing ten
  detection methods.
\newblock In \emph{Proceedings of the 10th ACM workshop on artificial
  intelligence and security (AISec)}, 2017{\natexlab{b}}.

\bibitem[Carmon et~al.(2019)Carmon, Raghunathan, Schmidt, Liang, and
  Duchi]{NeurIPS19:CRSLD19}
Carmon, Y., Raghunathan, A., Schmidt, L., Liang, P., and Duchi, J.~C.
\newblock Unlabeled data improves adversarial robustness.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2019.
\newblock \url{https://arxiv.org/abs/1905.13736}.

\bibitem[Cohen et~al.(2019)Cohen, Rosenfeld, and Kolter]{icml19:CSmooth}
Cohen, J., Rosenfeld, E., and Kolter, Z.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Croce \& Hein(2020{\natexlab{a}})Croce and
  Hein]{icml20:Croce2020Minimally}
Croce, F. and Hein, M.
\newblock Minimally distorted adversarial examples with a fast adaptive
  boundary attack.
\newblock In \emph{International Conference on Machine Learning},
  2020{\natexlab{a}}.
\newblock \url{https://arxiv.org/abs/1907.02044}.

\bibitem[Croce \& Hein(2020{\natexlab{b}})Croce and Hein]{icml20:autopgd}
Croce, F. and Hein, M.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock In \emph{International Conference on Machine Learning},
  2020{\natexlab{b}}.
\newblock \url{https://arxiv.org/abs/2003.01690}.

\bibitem[Croce et~al.(2021)Croce, Andriushchenko, Sehwag, Debenedetti,
  Flammarion, Chiang, Mittal, and Hein]{arxiv21:robustbench}
Croce, F., Andriushchenko, M., Sehwag, V., Debenedetti, E., Flammarion, N.,
  Chiang, M., Mittal, P., and Hein, M.
\newblock Robustbench: a standardized adversarial robustness benchmark.
\newblock arXiv preprint 2010.09670, June 2021.
\newblock URL \url{https://arxiv.org/abs/2010.09670}.

\bibitem[Cureton(1967)]{ASA67:Normal}
Cureton, E.~E.
\newblock The normal approximation to the signed-rank sampling distribution
  when zero differences are present.
\newblock In \emph{Journal of the American Statistical Association}, volume~62,
  pp.\  1068--1069, 1967.
\newblock
  \url{https://www.tandfonline.com/doi/abs/10.1080/01621459.1967.10500917}.

\bibitem[Ding et~al.(2020)Ding, Sharma, Lui, and Huang]{iclr20:DSLH20}
Ding, G.~W., Sharma, Y., Lui, K. Y.~C., and Huang, R.
\newblock Mma training: Direct input space margin maximization through
  adversarial training.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock \url{https://openreview.net/forum?id=HkeryxBtPB}.

\bibitem[Dong et~al.(2018)Dong, Liao, Pang, Su, Zhu, Hu, and
  Li]{CVPR18:Momentum}
Dong, Y., Liao, F., Pang, T., Su, H., Zhu, J., Hu, X., and Li, J.
\newblock Boosting adversarial attacks with momentum.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  2018.
\newblock \url{https://arxiv.org/abs/1710.06081}.

\bibitem[Feinman et~al.(2017)Feinman, Curtin, Shintre, and
  Gardner]{arxiv17:Detect}
Feinman, R., Curtin, R.~R., Shintre, S., and Gardner, A.~B.
\newblock Detecting adversarial samples from artifacts.
\newblock \emph{arXiv preprint arXiv:1703.00410}, 2017.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{iclr15:adversarial}
Goodfellow, I.~J., Shlens, J., and Szegedy, C.
\newblock Explaining and harnessing adversarial examples.
\newblock In \emph{International Conference on Learning Representations}, 2015.
\newblock \url{https://arxiv.org/abs/1412.6572}.

\bibitem[Gowal et~al.(2019)Gowal, Uesato, Qin, Huang, Mann, and
  Kohli]{arxiv19:nonsign}
Gowal, S., Uesato, J., Qin, C., Huang, P.-S., Mann, T., and Kohli, P.
\newblock An alternative surrogate loss for {PGD}-based adversarial testing.
\newblock \emph{arXiv:1910.09338}, 2019.

\bibitem[Gowal et~al.(2020)Gowal, Qin, Uesato, Mann, and
  Kohli]{arxiv20:uncover}
Gowal, S., Qin, C., Uesato, J., Mann, T., and Kohli, P.
\newblock Uncovering the limits of adversarial training against norm-bounded
  adversarial examples.
\newblock \emph{arXiv preprint arXiv:2010.03593}, 2020.

\bibitem[Guo et~al.(2018)Guo, Rana, Cisse, and Van Der~Maaten]{iclr18:JPEG}
Guo, C., Rana, M., Cisse, M., and Van Der~Maaten, L.
\newblock Countering adversarial images using input transformations.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[He et~al.(2017)He, Wei, Chen, Carlini, and Song]{woot17:ensembles}
He, W., Wei, J., Chen, X., Carlini, N., and Song, D.
\newblock Adversarial example defense: Ensembles of weak defenses are not
  strong.
\newblock In \emph{11th $\{$USENIX$\}$ workshop on offensive technologies
  ($\{$WOOT$\}$ 17)}, 2017.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Lee, and Mazeika]{icml19:HLM19}
Hendrycks, D., Lee, K., and Mazeika, M.
\newblock Using pre-training can improve model robustness and uncertainty.
\newblock In \emph{International Conference on Machine Learning}, 2019.
\newblock \url{https://arxiv.org/abs/1901.09960}.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{iclr15:Adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.
\newblock \url{https://arxiv.org/abs/1412.6980}.

\bibitem[Kumar et~al.(2020)Kumar, Levine, Goldstein, and Feizi]{icml20:CSmooth}
Kumar, A., Levine, A., Goldstein, T., and Feizi, S.
\newblock Curse of dimensionality on randomized smoothing for certifiable
  robustness.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Kurakin et~al.(2017{\natexlab{a}})Kurakin, Goodfellow, and
  Bengio]{iclr17:BasicIterative}
Kurakin, A., Goodfellow, I., and Bengio, S.
\newblock Adversarial examples in the physical world.
\newblock In \emph{International Conference on Learning Representations
  Workshop}, 2017{\natexlab{a}}.
\newblock \url{https://arxiv.org/abs/1607.02533}.

\bibitem[Kurakin et~al.(2017{\natexlab{b}})Kurakin, Goodfellow, and
  Bengio]{iclr17:Kurakin2017AdversarialML}
Kurakin, A., Goodfellow, I.~J., and Bengio, S.
\newblock Adversarial machine learning at scale.
\newblock In \emph{International Conference on Learning Representations},
  2017{\natexlab{b}}.
\newblock \url{https://arxiv.org/abs/1611.01236}.

\bibitem[Lecuyer et~al.(2019)Lecuyer, Atlidakis, Geambasu, Hsu, and
  Jana]{oakland19:DPDefense}
Lecuyer, M., Atlidakis, V., Geambasu, R., Hsu, D., and Jana, S.
\newblock Certified robustness to adversarial examples with differential
  privacy.
\newblock In \emph{2019 IEEE Symposium on Security and Privacy (SP)}, 2019.

\bibitem[Liao et~al.(2018)Liao, Liang, Dong, Pang, Zhu, and
  Hu]{cvpr18:Liao2018DefenseAA}
Liao, F., Liang, M., Dong, Y., Pang, T., Zhu, J., and Hu, X.
\newblock Defense against adversarial attacks using high-level representation
  guided denoiser.
\newblock \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pp.\  1778--1787, 2018.

\bibitem[Ma et~al.(2018)Ma, Li, Wang, Erfani, Wijewickrema, Schoenebeck, Song,
  Houle, and Bailey]{iclr18:LID}
Ma, X., Li, B., Wang, Y., Erfani, S.~M., Wijewickrema, S., Schoenebeck, G.,
  Song, D., Houle, M.~E., and Bailey, J.
\newblock Characterizing adversarial subspaces using local intrinsic
  dimensionality.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{iclr18:PGD}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock \url{https://arxiv.org/pdf/1706.06083}.

\bibitem[Metzen et~al.(2017)Metzen, Genewein, Fischer, and
  Bischoff]{iclr17:metzen2017detecting}
Metzen, J.~H., Genewein, T., Fischer, V., and Bischoff, B.
\newblock On detecting adversarial perturbations.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock \url{https://arxiv.org/abs/1702.04267}.

\bibitem[Moosavi-Dezfooli et~al.(2016)Moosavi-Dezfooli, Fawzi, and
  Frossard]{CVPR16:Deepfool}
Moosavi-Dezfooli, S.-M., Fawzi, A., and Frossard, P.
\newblock Deepfool: a simple and accurate method to fool deep neural networks.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  2016.
\newblock \url{https://arxiv.org/abs/1511.04599}.

\bibitem[Mosbach et~al.(2018)Mosbach, Andriushchenko, Trost, Hein, and
  Klakow]{neuripssecml18:Mosbach2018LogitPM}
Mosbach, M., Andriushchenko, M., Trost, T.~A., Hein, M., and Klakow, D.
\newblock Logit pairing methods can fool gradient-based attacks.
\newblock In \emph{NeurIPS 2018 Workshop on Security in Machine Learning},
  2018.
\newblock \url{https://arxiv.org/abs/1810.12042}.

\bibitem[Papernot et~al.(2016)Papernot, McDaniel, Jha, Fredrikson, Celik, and
  Swami]{eurosp16:PapernotLimitations}
Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z.~B., and Swami,
  A.
\newblock The limitations of deep learning in adversarial settings.
\newblock In \emph{2016 IEEE European Symposium on Security and Privacy (EuroS
  P)}, pp.\  372--387, 2016.
\newblock \doi{10.1109/EuroSP.2016.36}.

\bibitem[Pratt(1959)]{stat59:Pratt}
Pratt, J.~W.
\newblock Remarks on zeros and ties in the wilcoxon signed rank procedures.
\newblock \emph{Journal of the American Statistical Association}, 54, 1959.

\bibitem[Raghunathan et~al.(2018)Raghunathan, Steinhardt, and
  Liang]{iclr18:Raghunathan2018CertifiedDA}
Raghunathan, A., Steinhardt, J., and Liang, P.
\newblock Certified defenses against adversarial examples.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock \url{https://arxiv.org/abs/1801.09344}.

\bibitem[Rebuffi et~al.(2021)Rebuffi, Gowal, Calian, Stimberg, Wiles, and
  Mann]{arxiv21:fixing}
Rebuffi, S.-A., Gowal, S., Calian, D.~A., Stimberg, F., Wiles, O., and Mann, T.
\newblock Fixing data augmentation to improve adversarial robustness.
\newblock \emph{arXiv preprint arXiv:2103.01946}, 2021.

\bibitem[Salman et~al.(2020)Salman, Ilyas, Engstrom, Kapoor, and
  Madry]{NeurIPS20:SIEKM20}
Salman, H., Ilyas, A., Engstrom, L., Kapoor, A., and Madry, A.
\newblock Do adversarially robust imagenet models transfer better?
\newblock In \emph{Conference on Neural Information Processing Systems}, 2020.
\newblock \url{https://arxiv.org/abs/2007.08489}.

\bibitem[Samangouei et~al.(2018)Samangouei, Kabkab, and Chellappa]{iclr18:DGAN}
Samangouei, P., Kabkab, M., and Chellappa, R.
\newblock Defense-{GAN}: {P}rotecting classifiers against adversarial attacks
  using generative models.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Sehwag et~al.(2020)Sehwag, Wang, Mittal, and Jana]{NeurIPS20:SWMJ20}
Sehwag, V., Wang, S., Mittal, P., and Jana, S.
\newblock Hydra: Pruning adversarially robust neural networks.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2020.
\newblock \url{https://arxiv.org/abs/2002.10509}.

\bibitem[Shafahi et~al.(2019)Shafahi, Najibi, Ghiasi, Xu, Dickerson, Studer,
  Davis, Taylor, and Goldstein]{NeurIPS19:FreeTraining}
Shafahi, A., Najibi, M., Ghiasi, A., Xu, Z., Dickerson, J., Studer, C., Davis,
  L.~S., Taylor, G., and Goldstein, T.
\newblock Adversarial training for free!
\newblock In \emph{Conference on Neural Information Processing Systems}, 2019.
\newblock \url{https://arxiv.org/abs/1904.12843}.

\bibitem[Song et~al.(2018)Song, Kim, Nowozin, Ermon, and
  Kushman]{iclr18:PixelDefend}
Song, Y., Kim, T., Nowozin, S., Ermon, S., and Kushman, N.
\newblock Pixeldefend: Leveraging generative models to understand and defend
  against adversarial examples.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock \url{https://arxiv.org/abs/1710.10766}.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{iclr14:Intriguing}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
  and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2014.
\newblock \url{https://arxiv.org/abs/1312.6199}.

\bibitem[Tashiro et~al.(2020)Tashiro, Song, and
  Ermon]{NeurIPS20:Initializations}
Tashiro, Y., Song, Y., and Ermon, S.
\newblock Diversity can be transferred: {O}utput diversification for white-and
  black-box attacks.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2020.
\newblock \url{}.

\bibitem[Uesato et~al.(2018)Uesato, O'Donoghue, van~den Oord, and
  Kohli]{icml18:Uesato2018AdversarialRA}
Uesato, J., O'Donoghue, B., van~den Oord, A., and Kohli, P.
\newblock Adversarial risk and the dangers of evaluating against weak attacks.
\newblock \emph{International Conference on Machine Learning}, 2018.
\newblock \url{https://arxiv.org/abs/1802.05666}.

\bibitem[Wang et~al.(2020)Wang, Zou, Yi, Bailey, Ma, and Gu]{iclr20:WZYBMG20}
Wang, Y., Zou, D., Yi, J., Bailey, J., Ma, X., and Gu, Q.
\newblock Improving adversarial robustness requires revisiting misclassified
  examples.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock \url{https://openreview.net/forum?id=rklOg6EFwS}.

\bibitem[Wilcoxon(1945)]{stat45:Wilcoxon}
Wilcoxon, F.
\newblock Individual comparisons by ranking methods.
\newblock \emph{Biometrics Bulletin}, 1, 1945.

\bibitem[Wong \& Kolter(2018)Wong and Kolter]{icml19:Provable}
Wong, E. and Kolter, Z.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Wong et~al.(2020)Wong, Rice, and Kolter]{iclr20:WRK20}
Wong, E., Rice, L., and Kolter, J.~Z.
\newblock Fast is better than free: Revisiting adversarial training.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock \url{https://arxiv.org/abs/2001.03994}.

\bibitem[Wu et~al.(2020)Wu, tao Xia, and Wang]{NeurIPS20:WXW20}
Wu, D., tao Xia, S., and Wang, Y.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2020.
\newblock \url{https://arxiv.org/abs/2004.05884}.

\bibitem[Xiao et~al.(2018)Xiao, Li, Zhu, He, Liu, and
  Song]{arxiv18:Xiao2018GeneratingAE}
Xiao, C., Li, B., Zhu, J.-Y., He, W., Liu, M., and Song, D.~X.
\newblock Generating adversarial examples with adversarial networks, 2018.

\bibitem[Xu et~al.(2018)Xu, Evans, and Qi]{ndss18:Squeezing}
Xu, W., Evans, D., and Qi, Y.
\newblock Feature squeezing: {D}etecting adversarial examples in deep neural
  networks.
\newblock In \emph{The Network and Distributed System Security Symposium
  (NDSS)}, 2018.
\newblock \url{https://arxiv.org/abs/1704.01155}.

\bibitem[Zhang et~al.(2020)Zhang, Chen, Xiao, Li, Boning, and
  Hsieh]{iclr20:Zhang2020TowardsSA}
Zhang, H., Chen, H., Xiao, C., Li, B., Boning, D.~S., and Hsieh, C.-J.
\newblock Towards stable and efficient training of verifiably robust neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock \url{https://arxiv.org/abs/1906.06316}.

\end{thebibliography}
