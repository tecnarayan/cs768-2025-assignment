\begin{thebibliography}{82}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Agarwal et~al.(2019)Agarwal, Jiang, Kakade and
  Sun}]{agarwal2019reinforcement}
\textsc{Agarwal, A.}, \textsc{Jiang, N.}, \textsc{Kakade, S.~M.} and
  \textsc{Sun, W.} (2019).
\newblock Reinforcement learning: Theory and algorithms.
\newblock \textit{CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep}  10--4.

\bibitem[{Agarwal et~al.(2023)Agarwal, Jin and Zhang}]{agarwal2023vo}
\textsc{Agarwal, A.}, \textsc{Jin, Y.} and \textsc{Zhang, T.} (2023).
\newblock Vo $ q $ l: Towards optimal regret in model-free rl with nonlinear
  function approximation.
\newblock In \textit{The Thirty Sixth Annual Conference on Learning Theory}.
  PMLR.

\bibitem[{Auer et~al.(2008)Auer, Jaksch and Ortner}]{auer2008near}
\textsc{Auer, P.}, \textsc{Jaksch, T.} and \textsc{Ortner, R.} (2008).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \textit{Advances in neural information processing systems}
  \textbf{21}.

\bibitem[{Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang and
  Yang}]{ayoub2020model}
\textsc{Ayoub, A.}, \textsc{Jia, Z.}, \textsc{Szepesvari, C.}, \textsc{Wang,
  M.} and \textsc{Yang, L.} (2020).
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Azar et~al.(2017)Azar, Osband and Munos}]{azar2017minimax}
\textsc{Azar, M.~G.}, \textsc{Osband, I.} and \textsc{Munos, R.} (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Badrinath and Kalathil(2021)}]{badrinath2021robust}
\textsc{Badrinath, K.~P.} and \textsc{Kalathil, D.} (2021).
\newblock Robust reinforcement learning using least squares policy iteration
  with provable performance guarantees.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Blanchet et~al.(2023)Blanchet, Lu, Zhang and
  Zhong}]{blanchet2023double}
\textsc{Blanchet, J.}, \textsc{Lu, M.}, \textsc{Zhang, T.} and \textsc{Zhong,
  H.} (2023).
\newblock Double pessimism is provably efficient for distributionally robust
  offline reinforcement learning: Generic algorithm and robust partial
  coverage.
\newblock \textit{arXiv preprint arXiv:2305.09659} .

\bibitem[{Clavier et~al.(2023)Clavier, Pennec and Geist}]{clavier2023towards}
\textsc{Clavier, P.}, \textsc{Pennec, E.~L.} and \textsc{Geist, M.} (2023).
\newblock Towards minimax optimality of model-based robust reinforcement
  learning.
\newblock \textit{arXiv preprint arXiv:2302.05372} .

\bibitem[{Dann et~al.(2017)Dann, Lattimore and Brunskill}]{dann2017unifying}
\textsc{Dann, C.}, \textsc{Lattimore, T.} and \textsc{Brunskill, E.} (2017).
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{30}.

\bibitem[{Ding et~al.(2024)Ding, Shi, Chi and Zhao}]{ding2024seeing}
\textsc{Ding, W.}, \textsc{Shi, L.}, \textsc{Chi, Y.} and \textsc{Zhao, D.}
  (2024).
\newblock Seeing is not believing: Robust reinforcement learning against
  spurious correlation.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{36}.

\bibitem[{Dong et~al.(2022)Dong, Li, Wang and Zhang}]{dong2022online}
\textsc{Dong, J.}, \textsc{Li, J.}, \textsc{Wang, B.} and \textsc{Zhang, J.}
  (2022).
\newblock Online policy optimization for robust mdp.
\newblock \textit{arXiv preprint arXiv:2209.13841} .

\bibitem[{Du et~al.(2021)Du, Kakade, Lee, Lovett, Mahajan, Sun and
  Wang}]{du2021bilinear}
\textsc{Du, S.}, \textsc{Kakade, S.}, \textsc{Lee, J.}, \textsc{Lovett, S.},
  \textsc{Mahajan, G.}, \textsc{Sun, W.} and \textsc{Wang, R.} (2021).
\newblock Bilinear classes: A structural framework for provable generalization
  in rl.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{El~Ghaoui and Nilim(2005)}]{el2005robust}
\textsc{El~Ghaoui, L.} and \textsc{Nilim, A.} (2005).
\newblock Robust solutions to markov decision problems with uncertain
  transition matrices.
\newblock \textit{Operations Research} \textbf{53} 780--798.

\bibitem[{Foster et~al.(2021)Foster, Kakade, Qian and
  Rakhlin}]{foster2021statistical}
\textsc{Foster, D.~J.}, \textsc{Kakade, S.~M.}, \textsc{Qian, J.} and
  \textsc{Rakhlin, A.} (2021).
\newblock The statistical complexity of interactive decision making.
\newblock \textit{arXiv preprint arXiv:2112.13487} .

\bibitem[{He et~al.(2023)He, Zhao, Zhou and Gu}]{he2023nearly}
\textsc{He, J.}, \textsc{Zhao, H.}, \textsc{Zhou, D.} and \textsc{Gu, Q.}
  (2023).
\newblock Nearly minimax optimal reinforcement learning for linear markov
  decision processes.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Hu et~al.(2022)Hu, Zhong, Jin and Wang}]{hu2022provable}
\textsc{Hu, J.}, \textsc{Zhong, H.}, \textsc{Jin, C.} and \textsc{Wang, L.}
  (2022).
\newblock Provable sim-to-real transfer in continuous domain with partial
  observations.
\newblock \textit{arXiv preprint arXiv:2210.15598} .

\bibitem[{Huang et~al.(2023{\natexlab{a}})Huang, Zhong, Wang and
  Yang}]{huang2023horizon}
\textsc{Huang, J.}, \textsc{Zhong, H.}, \textsc{Wang, L.} and \textsc{Yang,
  L.~F.} (2023{\natexlab{a}}).
\newblock Horizon-free and instance-dependent regret bounds for reinforcement
  learning with general function approximation.
\newblock \textit{arXiv preprint arXiv:2312.04464} .

\bibitem[{Huang et~al.(2023{\natexlab{b}})Huang, Zhong, Wang and
  Yang}]{huang2023tackling}
\textsc{Huang, J.}, \textsc{Zhong, H.}, \textsc{Wang, L.} and \textsc{Yang,
  L.~F.} (2023{\natexlab{b}}).
\newblock Tackling heavy-tailed rewards in reinforcement learning with function
  approximation: Minimax optimal and instance-dependent regret bounds.
\newblock \textit{arXiv preprint arXiv:2306.06836} .

\bibitem[{Iyengar(2005)}]{iyengar2005robust}
\textsc{Iyengar, G.~N.} (2005).
\newblock Robust dynamic programming.
\newblock \textit{Mathematics of Operations Research} \textbf{30} 257--280.

\bibitem[{Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford and
  Schapire}]{jiang2017contextual}
\textsc{Jiang, N.}, \textsc{Krishnamurthy, A.}, \textsc{Agarwal, A.},
  \textsc{Langford, J.} and \textsc{Schapire, R.~E.} (2017).
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Jin et~al.(2018)Jin, Allen-Zhu, Bubeck and Jordan}]{jin2018q}
\textsc{Jin, C.}, \textsc{Allen-Zhu, Z.}, \textsc{Bubeck, S.} and
  \textsc{Jordan, M.~I.} (2018).
\newblock Is q-learning provably efficient?
\newblock \textit{Advances in neural information processing systems}
  \textbf{31}.

\bibitem[{Jin et~al.(2021)Jin, Liu and Miryoosefi}]{jin2021bellman}
\textsc{Jin, C.}, \textsc{Liu, Q.} and \textsc{Miryoosefi, S.} (2021).
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock \textit{Advances in neural information processing systems}
  \textbf{34} 13406--13418.

\bibitem[{Jin et~al.(2020)Jin, Yang, Wang and Jordan}]{jin2020provably}
\textsc{Jin, C.}, \textsc{Yang, Z.}, \textsc{Wang, Z.} and \textsc{Jordan,
  M.~I.} (2020).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Kiran et~al.(2021)Kiran, Sobh, Talpaert, Mannion, Al~Sallab, Yogamani
  and P{\'e}rez}]{kiran2021deep}
\textsc{Kiran, B.~R.}, \textsc{Sobh, I.}, \textsc{Talpaert, V.},
  \textsc{Mannion, P.}, \textsc{Al~Sallab, A.~A.}, \textsc{Yogamani, S.} and
  \textsc{P{\'e}rez, P.} (2021).
\newblock Deep reinforcement learning for autonomous driving: A survey.
\newblock \textit{IEEE Transactions on Intelligent Transportation Systems}
  \textbf{23} 4909--4926.

\bibitem[{Kober et~al.(2013)Kober, Bagnell and Peters}]{kober2013reinforcement}
\textsc{Kober, J.}, \textsc{Bagnell, J.~A.} and \textsc{Peters, J.} (2013).
\newblock Reinforcement learning in robotics: A survey.
\newblock \textit{The International Journal of Robotics Research} \textbf{32}
  1238--1274.

\bibitem[{Kuang et~al.(2022)Kuang, Lu, Wang, Zhou, Li and
  Li}]{kuang2022learning}
\textsc{Kuang, Y.}, \textsc{Lu, M.}, \textsc{Wang, J.}, \textsc{Zhou, Q.},
  \textsc{Li, B.} and \textsc{Li, H.} (2022).
\newblock Learning robust policy against disturbance in transition dynamics via
  state-conservative policy optimization.
\newblock In \textit{Proceedings of the AAAI Conference on Artificial
  Intelligence}, vol.~36.

\bibitem[{Li et~al.(2023)Li, Cai, Chen, Wei and Chi}]{li2023q}
\textsc{Li, G.}, \textsc{Cai, C.}, \textsc{Chen, Y.}, \textsc{Wei, Y.} and
  \textsc{Chi, Y.} (2023).
\newblock Is q-learning minimax optimal? a tight sample complexity analysis.
\newblock \textit{Operations Research} .

\bibitem[{Li and Lan(2023)}]{li2023first}
\textsc{Li, Y.} and \textsc{Lan, G.} (2023).
\newblock First-order policy optimization for robust policy evaluation.
\newblock \textit{arXiv preprint arXiv:2307.15890} .

\bibitem[{Liu et~al.(2022)Liu, Lu, Wang, Jordan and Yang}]{liu2022welfare}
\textsc{Liu, Z.}, \textsc{Lu, M.}, \textsc{Wang, Z.}, \textsc{Jordan, M.} and
  \textsc{Yang, Z.} (2022).
\newblock Welfare maximization in competitive equilibrium: Reinforcement
  learning for markov exchange economy.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Liu et~al.(2023)Liu, Lu, Xiong, Zhong, Hu, Zhang, Zheng, Yang and
  Wang}]{liu2023one}
\textsc{Liu, Z.}, \textsc{Lu, M.}, \textsc{Xiong, W.}, \textsc{Zhong, H.},
  \textsc{Hu, H.}, \textsc{Zhang, S.}, \textsc{Zheng, S.}, \textsc{Yang, Z.}
  and \textsc{Wang, Z.} (2023).
\newblock One objective to rule them all: A maximization objective fusing
  estimation and planning for exploration.
\newblock \textit{arXiv preprint arXiv:2305.18258} .

\bibitem[{Liu and Xu(2024{\natexlab{a}})}]{liu2024distributionally}
\textsc{Liu, Z.} and \textsc{Xu, P.} (2024{\natexlab{a}}).
\newblock Distributionally robust off-dynamics reinforcement learning: Provable
  efficiency with linear function approximation.
\newblock \textit{arXiv preprint arXiv:2402.15399} .

\bibitem[{Liu and Xu(2024{\natexlab{b}})}]{liu2024minimax}
\textsc{Liu, Z.} and \textsc{Xu, P.} (2024{\natexlab{b}}).
\newblock Minimax optimal and computationally efficient algorithms for
  distributionally robust offline reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2403.09621} .

\bibitem[{Lykouris et~al.(2021)Lykouris, Simchowitz, Slivkins and
  Sun}]{lykouris2021corruption}
\textsc{Lykouris, T.}, \textsc{Simchowitz, M.}, \textsc{Slivkins, A.} and
  \textsc{Sun, W.} (2021).
\newblock Corruption-robust exploration in episodic reinforcement learning.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Ma et~al.(2022)Ma, Liang, Xia, Zhang, Blanchet, Liu, Zhao and
  Zhou}]{ma2022distributionally}
\textsc{Ma, X.}, \textsc{Liang, Z.}, \textsc{Xia, L.}, \textsc{Zhang, J.},
  \textsc{Blanchet, J.}, \textsc{Liu, M.}, \textsc{Zhao, Q.} and \textsc{Zhou,
  Z.} (2022).
\newblock Distributionally robust offline reinforcement learning with linear
  function approximation.
\newblock \textit{arXiv preprint arXiv:2209.06620} .

\bibitem[{Maurer and Pontil(2009)}]{maurer2009empirical}
\textsc{Maurer, A.} and \textsc{Pontil, M.} (2009).
\newblock Empirical bernstein bounds and sample variance penalization.
\newblock \textit{arXiv preprint arXiv:0907.3740} .

\bibitem[{M{\'e}nard et~al.(2021)M{\'e}nard, Domingues, Shang and
  Valko}]{menard2021ucb}
\textsc{M{\'e}nard, P.}, \textsc{Domingues, O.~D.}, \textsc{Shang, X.} and
  \textsc{Valko, M.} (2021).
\newblock Ucb momentum q-learning: Correcting the bias without forgetting.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Moos et~al.(2022)Moos, Hansel, Abdulsamad, Stark, Clever and
  Peters}]{moos2022robust}
\textsc{Moos, J.}, \textsc{Hansel, K.}, \textsc{Abdulsamad, H.}, \textsc{Stark,
  S.}, \textsc{Clever, D.} and \textsc{Peters, J.} (2022).
\newblock Robust reinforcement learning: A review of foundations and recent
  advances.
\newblock \textit{Machine Learning and Knowledge Extraction} \textbf{4}
  276--315.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray et~al.}]{ouyang2022training}
\textsc{Ouyang, L.}, \textsc{Wu, J.}, \textsc{Jiang, X.}, \textsc{Almeida, D.},
  \textsc{Wainwright, C.}, \textsc{Mishkin, P.}, \textsc{Zhang, C.},
  \textsc{Agarwal, S.}, \textsc{Slama, K.}, \textsc{Ray, A.} \textsc{et~al.}
  (2022).
\newblock Training language models to follow instructions with human feedback.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{35} 27730--27744.

\bibitem[{Panaganti and Kalathil(2022)}]{panaganti2022sample}
\textsc{Panaganti, K.} and \textsc{Kalathil, D.} (2022).
\newblock Sample complexity of robust reinforcement learning with a generative
  model.
\newblock In \textit{International Conference on Artificial Intelligence and
  Statistics}. PMLR.

\bibitem[{Panaganti et~al.(2022)Panaganti, Xu, Kalathil and
  Ghavamzadeh}]{panaganti2022robust}
\textsc{Panaganti, K.}, \textsc{Xu, Z.}, \textsc{Kalathil, D.} and
  \textsc{Ghavamzadeh, M.} (2022).
\newblock Robust reinforcement learning using offline data.
\newblock \textit{arXiv preprint arXiv:2208.05129} .

\bibitem[{Peng et~al.(2018)Peng, Andrychowicz, Zaremba and
  Abbeel}]{peng2018sim}
\textsc{Peng, X.~B.}, \textsc{Andrychowicz, M.}, \textsc{Zaremba, W.} and
  \textsc{Abbeel, P.} (2018).
\newblock Sim-to-real transfer of robotic control with dynamics randomization.
\newblock In \textit{2018 IEEE international conference on robotics and
  automation (ICRA)}. IEEE.

\bibitem[{Pinto et~al.(2017)Pinto, Davidson, Sukthankar and
  Gupta}]{pinto2017robust}
\textsc{Pinto, L.}, \textsc{Davidson, J.}, \textsc{Sukthankar, R.} and
  \textsc{Gupta, A.} (2017).
\newblock Robust adversarial reinforcement learning.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Sadeghi and Levine(2016)}]{sadeghi2016cad2rl}
\textsc{Sadeghi, F.} and \textsc{Levine, S.} (2016).
\newblock Cad2rl: Real single-image flight without a single real image.
\newblock \textit{arXiv preprint arXiv:1611.04201} .

\bibitem[{Shi and Chi(2022)}]{shi2022distributionally}
\textsc{Shi, L.} and \textsc{Chi, Y.} (2022).
\newblock Distributionally robust model-based offline reinforcement learning
  with near-optimal sample complexity.
\newblock \textit{arXiv preprint arXiv:2208.05767} .

\bibitem[{Shi et~al.(2023)Shi, Li, Wei, Chen, Geist and Chi}]{shi2023curious}
\textsc{Shi, L.}, \textsc{Li, G.}, \textsc{Wei, Y.}, \textsc{Chen, Y.},
  \textsc{Geist, M.} and \textsc{Chi, Y.} (2023).
\newblock The curious price of distributional robustness in reinforcement
  learning with a generative model.
\newblock \textit{arXiv preprint arXiv:2305.16589} .

\bibitem[{Si et~al.(2023)Si, Zhang, Zhou and Blanchet}]{si2023distributionally}
\textsc{Si, N.}, \textsc{Zhang, F.}, \textsc{Zhou, Z.} and \textsc{Blanchet,
  J.} (2023).
\newblock Distributionally robust batch contextual bandits.
\newblock \textit{Management Science} .

\bibitem[{Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou,
  Huang, Guez, Hubert, Baker, Lai, Bolton et~al.}]{silver2017mastering}
\textsc{Silver, D.}, \textsc{Schrittwieser, J.}, \textsc{Simonyan, K.},
  \textsc{Antonoglou, I.}, \textsc{Huang, A.}, \textsc{Guez, A.},
  \textsc{Hubert, T.}, \textsc{Baker, L.}, \textsc{Lai, M.}, \textsc{Bolton,
  A.} \textsc{et~al.} (2017).
\newblock Mastering the game of go without human knowledge.
\newblock \textit{nature} \textbf{550} 354--359.

\bibitem[{Sun et~al.(2019)Sun, Jiang, Krishnamurthy, Agarwal and
  Langford}]{sun2019model}
\textsc{Sun, W.}, \textsc{Jiang, N.}, \textsc{Krishnamurthy, A.},
  \textsc{Agarwal, A.} and \textsc{Langford, J.} (2019).
\newblock Model-based rl in contextual decision processes: Pac bounds and
  exponential improvements over model-free approaches.
\newblock In \textit{Conference on learning theory}. PMLR.

\bibitem[{Sutton and Barto(2018)}]{sutton2018reinforcement}
\textsc{Sutton, R.~S.} and \textsc{Barto, A.~G.} (2018).
\newblock \textit{Reinforcement learning: An introduction}.
\newblock MIT press.

\bibitem[{Wang et~al.(2024)Wang, Shi and Chi}]{wang2024sample}
\textsc{Wang, H.}, \textsc{Shi, L.} and \textsc{Chi, Y.} (2024).
\newblock Sample complexity of offline distributionally robust linear markov
  decision processes.
\newblock \textit{arXiv preprint arXiv:2403.12946} .

\bibitem[{Wang et~al.(2018)Wang, Zhang, He and Zha}]{wang2018supervised}
\textsc{Wang, L.}, \textsc{Zhang, W.}, \textsc{He, X.} and \textsc{Zha, H.}
  (2018).
\newblock Supervised reinforcement learning with recurrent neural network for
  dynamic treatment recommendation.
\newblock In \textit{Proceedings of the 24th ACM SIGKDD international
  conference on knowledge discovery \& data mining}.

\bibitem[{Wang et~al.(2022)Wang, Ho and Petrik}]{wang2022convergence}
\textsc{Wang, Q.}, \textsc{Ho, C.~P.} and \textsc{Petrik, M.} (2022).
\newblock On the convergence of policy gradient in robust mdps.
\newblock \textit{arXiv preprint arXiv:2212.10439} .

\bibitem[{Wang et~al.(2023{\natexlab{a}})Wang, Ho and
  Petrik}]{pmlr-v202-wang23i}
\textsc{Wang, Q.}, \textsc{Ho, C.~P.} and \textsc{Petrik, M.}
  (2023{\natexlab{a}}).
\newblock Policy gradient in robust {MDP}s with global convergence guarantee.
\newblock In \textit{Proceedings of the 40th International Conference on
  Machine Learning} (A.~Krause, E.~Brunskill, K.~Cho, B.~Engelhardt, S.~Sabato
  and J.~Scarlett, eds.), vol. 202 of \textit{Proceedings of Machine Learning
  Research}. PMLR.

\bibitem[{Wang et~al.(2023{\natexlab{b}})Wang, Si, Blanchet and
  Zhou}]{wang2023finite}
\textsc{Wang, S.}, \textsc{Si, N.}, \textsc{Blanchet, J.} and \textsc{Zhou, Z.}
  (2023{\natexlab{b}}).
\newblock A finite sample complexity bound for distributionally robust
  q-learning.
\newblock In \textit{International Conference on Artificial Intelligence and
  Statistics}. PMLR.

\bibitem[{Wang et~al.(2023{\natexlab{c}})Wang, Si, Blanchet and
  Zhou}]{wang2023foundation}
\textsc{Wang, S.}, \textsc{Si, N.}, \textsc{Blanchet, J.} and \textsc{Zhou, Z.}
  (2023{\natexlab{c}}).
\newblock On the foundation of distributionally robust reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2311.09018} .

\bibitem[{Wang et~al.(2023{\natexlab{d}})Wang, Si, Blanchet and
  Zhou}]{wang2023sample}
\textsc{Wang, S.}, \textsc{Si, N.}, \textsc{Blanchet, J.} and \textsc{Zhou, Z.}
  (2023{\natexlab{d}}).
\newblock Sample complexity of variance-reduced distributionally robust
  q-learning.
\newblock \textit{arXiv preprint arXiv:2305.18420} .

\bibitem[{Wang and Zou(2021)}]{wang2021online}
\textsc{Wang, Y.} and \textsc{Zou, S.} (2021).
\newblock Online robust reinforcement learning with model uncertainty.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{34} 7193--7206.

\bibitem[{Wang and Zou(2022)}]{wang2022policy}
\textsc{Wang, Y.} and \textsc{Zou, S.} (2022).
\newblock Policy gradient method for robust reinforcement learning.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Wei et~al.(2022)Wei, Dann and Zimmert}]{wei2022model}
\textsc{Wei, C.-Y.}, \textsc{Dann, C.} and \textsc{Zimmert, J.} (2022).
\newblock A model selection approach for corruption robust reinforcement
  learning.
\newblock In \textit{International Conference on Algorithmic Learning Theory}.
  PMLR.

\bibitem[{Wiesemann et~al.(2013)Wiesemann, Kuhn and
  Rustem}]{wiesemann2013robust}
\textsc{Wiesemann, W.}, \textsc{Kuhn, D.} and \textsc{Rustem, B.} (2013).
\newblock Robust markov decision processes.
\newblock \textit{Mathematics of Operations Research} \textbf{38} 153--183.

\bibitem[{Wu et~al.(2022)Wu, Yang, Zhong, Wang, Du and Jiao}]{wu2022nearly}
\textsc{Wu, T.}, \textsc{Yang, Y.}, \textsc{Zhong, H.}, \textsc{Wang, L.},
  \textsc{Du, S.} and \textsc{Jiao, J.} (2022).
\newblock Nearly optimal policy optimization with stable at any time guarantee.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Xu and Mannor(2010)}]{xu2010distributionally}
\textsc{Xu, H.} and \textsc{Mannor, S.} (2010).
\newblock Distributionally robust markov decision processes.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{23}.

\bibitem[{Xu and Zeevi(2023)}]{xu2023bayesian}
\textsc{Xu, Y.} and \textsc{Zeevi, A.} (2023).
\newblock Bayesian design principles for frequentist sequential learning.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Xu et~al.(2023)Xu, Panaganti and Kalathil}]{xu2023improved}
\textsc{Xu, Z.}, \textsc{Panaganti, K.} and \textsc{Kalathil, D.} (2023).
\newblock Improved sample complexity bounds for distributionally robust
  reinforcement learning.
\newblock In \textit{International Conference on Artificial Intelligence and
  Statistics}. PMLR.

\bibitem[{Yang et~al.(2023{\natexlab{a}})Yang, Zhong, Xu, Zhang, Zhang, Han and
  Zhang}]{yang2023towards}
\textsc{Yang, R.}, \textsc{Zhong, H.}, \textsc{Xu, J.}, \textsc{Zhang, A.},
  \textsc{Zhang, C.}, \textsc{Han, L.} and \textsc{Zhang, T.}
  (2023{\natexlab{a}}).
\newblock Towards robust offline reinforcement learning under diverse data
  corruption.
\newblock \textit{arXiv preprint arXiv:2310.12955} .

\bibitem[{Yang et~al.(2023{\natexlab{b}})Yang, Wang, Kozuno, Jordan and
  Zhang}]{yang2023avoiding}
\textsc{Yang, W.}, \textsc{Wang, H.}, \textsc{Kozuno, T.}, \textsc{Jordan,
  S.~M.} and \textsc{Zhang, Z.} (2023{\natexlab{b}}).
\newblock Avoiding model estimation in robust markov decision processes with a
  generative model.
\newblock \textit{arXiv preprint arXiv:2302.01248} .

\bibitem[{Yang et~al.(2022)Yang, Zhang and Zhang}]{yang2022toward}
\textsc{Yang, W.}, \textsc{Zhang, L.} and \textsc{Zhang, Z.} (2022).
\newblock Toward theoretical understandings of robust markov decision
  processes: Sample complexity and asymptotics.
\newblock \textit{The Annals of Statistics} \textbf{50} 3223--3248.

\bibitem[{Ye et~al.(2024)Ye, He, Gu and Zhang}]{ye2024towards}
\textsc{Ye, C.}, \textsc{He, J.}, \textsc{Gu, Q.} and \textsc{Zhang, T.}
  (2024).
\newblock Towards robust model-based reinforcement learning against adversarial
  corruption.
\newblock \textit{arXiv preprint arXiv:2402.08991} .

\bibitem[{Ye et~al.(2023{\natexlab{a}})Ye, Xiong, Gu and
  Zhang}]{ye2023corruptiononline}
\textsc{Ye, C.}, \textsc{Xiong, W.}, \textsc{Gu, Q.} and \textsc{Zhang, T.}
  (2023{\natexlab{a}}).
\newblock Corruption-robust algorithms with uncertainty weighting for nonlinear
  contextual bandits and markov decision processes.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Ye et~al.(2023{\natexlab{b}})Ye, Yang, Gu and
  Zhang}]{ye2023corruption}
\textsc{Ye, C.}, \textsc{Yang, R.}, \textsc{Gu, Q.} and \textsc{Zhang, T.}
  (2023{\natexlab{b}}).
\newblock Corruption-robust offline reinforcement learning with general
  function approximation.
\newblock \textit{arXiv preprint arXiv:2310.14550} .

\bibitem[{Yu et~al.(2023)Yu, Dai, Xu, Gao and Ho}]{yu2023fast}
\textsc{Yu, Z.}, \textsc{Dai, L.}, \textsc{Xu, S.}, \textsc{Gao, S.} and
  \textsc{Ho, C.~P.} (2023).
\newblock Fast bellman updates for wasserstein distributionally robust mdps.
\newblock In \textit{Thirty-seventh Conference on Neural Information Processing
  Systems}.

\bibitem[{Zanette and Brunskill(2019)}]{zanette2019tighter}
\textsc{Zanette, A.} and \textsc{Brunskill, E.} (2019).
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Zhang et~al.(2022)Zhang, Chen, Zhu and Sun}]{zhang2022corruption}
\textsc{Zhang, X.}, \textsc{Chen, Y.}, \textsc{Zhu, X.} and \textsc{Sun, W.}
  (2022).
\newblock Corruption-robust offline reinforcement learning.
\newblock In \textit{International Conference on Artificial Intelligence and
  Statistics}. PMLR.

\bibitem[{Zhang et~al.(2023)Zhang, Chen, Lee and Du}]{zhang2023settling}
\textsc{Zhang, Z.}, \textsc{Chen, Y.}, \textsc{Lee, J.~D.} and \textsc{Du,
  S.~S.} (2023).
\newblock Settling the sample complexity of online reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2307.13586} .

\bibitem[{Zhang et~al.(2021)Zhang, Ji and Du}]{zhang2021reinforcement}
\textsc{Zhang, Z.}, \textsc{Ji, X.} and \textsc{Du, S.} (2021).
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Zhang et~al.(2020)Zhang, Zhou and Ji}]{zhang2020almost}
\textsc{Zhang, Z.}, \textsc{Zhou, Y.} and \textsc{Ji, X.} (2020).
\newblock Almost optimal model-free reinforcement learningvia
  reference-advantage decomposition.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{33} 15198--15207.

\bibitem[{Zhao et~al.(2020)Zhao, Queralta and Westerlund}]{zhao2020sim}
\textsc{Zhao, W.}, \textsc{Queralta, J.~P.} and \textsc{Westerlund, T.} (2020).
\newblock Sim-to-real transfer in deep reinforcement learning for robotics: a
  survey.
\newblock In \textit{2020 IEEE Symposium Series on Computational Intelligence
  (SSCI)}. IEEE.

\bibitem[{Zhong et~al.(2022)Zhong, Xiong, Zheng, Wang, Wang, Yang and
  Zhang}]{zhong2022gec}
\textsc{Zhong, H.}, \textsc{Xiong, W.}, \textsc{Zheng, S.}, \textsc{Wang, L.},
  \textsc{Wang, Z.}, \textsc{Yang, Z.} and \textsc{Zhang, T.} (2022).
\newblock Gec: A unified framework for interactive decision making in mdp,
  pomdp, and beyond.
\newblock \textit{arXiv preprint arXiv:2211.01962} .

\bibitem[{Zhong and Zhang(2023)}]{zhong2023theoretical}
\textsc{Zhong, H.} and \textsc{Zhang, T.} (2023).
\newblock A theoretical analysis of optimistic proximal policy optimization in
  linear markov decision processes.
\newblock \textit{arXiv preprint arXiv:2305.08841} .

\bibitem[{Zhou et~al.(2021{\natexlab{a}})Zhou, Gu and
  Szepesvari}]{zhou2021nearly}
\textsc{Zhou, D.}, \textsc{Gu, Q.} and \textsc{Szepesvari, C.}
  (2021{\natexlab{a}}).
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Zhou et~al.(2023)Zhou, Liu, Cheng, Kalathil, Kumar and
  Tian}]{zhou2023natural}
\textsc{Zhou, R.}, \textsc{Liu, T.}, \textsc{Cheng, M.}, \textsc{Kalathil, D.},
  \textsc{Kumar, P.} and \textsc{Tian, C.} (2023).
\newblock Natural actor-critic for robust reinforcement learning with function
  approximation.
\newblock In \textit{Thirty-seventh Conference on Neural Information Processing
  Systems}.

\bibitem[{Zhou et~al.(2021{\natexlab{b}})Zhou, Zhou, Bai, Qiu, Blanchet and
  Glynn}]{zhou2021finite}
\textsc{Zhou, Z.}, \textsc{Zhou, Z.}, \textsc{Bai, Q.}, \textsc{Qiu, L.},
  \textsc{Blanchet, J.} and \textsc{Glynn, P.} (2021{\natexlab{b}}).
\newblock Finite-sample regret bound for distributionally robust offline
  tabular reinforcement learning.
\newblock In \textit{International Conference on Artificial Intelligence and
  Statistics}. PMLR.

\end{thebibliography}
