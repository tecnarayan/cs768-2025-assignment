\newcommand{\etalchar}[1]{$^{#1}$}
\providecommand{\bysame}{\leavevmode\hbox to3em{\hrulefill}\thinspace}
\providecommand{\MR}{\relax\ifhmode\unskip\space\fi MR }
% \MRhref is called by the amsart/book/proc definition of \MR.
\providecommand{\MRhref}[2]{%
  \href{http://www.ams.org/mathscinet-getitem?mr=#1}{#2}
}
\providecommand{\href}[2]{#2}
\begin{thebibliography}{GMMM19b}

\bibitem[ABC{\etalchar{+}}16]{abadi2016tensorflow}
Mart{\'\i}n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
  Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, and Michael Isard,
  \emph{Tensorflow: A system for large-scale machine learning}, 12th
  $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation
  ($\{$OSDI$\}$ 16), 2016, pp.~265--283.

\bibitem[ADH{\etalchar{+}}19]{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and
  Ruosong Wang, \emph{On exact computation with an infinitely wide neural net},
  Advances in Neural Information Processing Systems, 2019, pp.~8139--8148.

\bibitem[ADL{\etalchar{+}}20]{Arora2020Harnessing}
Sanjeev Arora, Simon~S. Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and
  Dingli Yu, \emph{Harnessing the power of infinitely wide deep nets on
  small-data tasks}, International Conference on Learning Representations,
  2020.

\bibitem[AZL19]{allen2019can}
Zeyuan Allen-Zhu and Yuanzhi Li, \emph{What can resnet learn efficiently, going
  beyond kernels?}, Advances in Neural Information Processing Systems, 2019,
  pp.~9017--9028.

\bibitem[AZL20]{allen2020backward}
\bysame, \emph{Backward feature correction: How deep learning performs deep
  learning}, arXiv preprint arXiv:2001.04413 (2020).

\bibitem[AZLS19]{allen2018convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song, \emph{A convergence theory for
  deep learning via over-parameterization}, Proceedings of the 36th
  International Conference on Machine Learning (Long Beach, California, USA)
  (Kamalika Chaudhuri and Ruslan Salakhutdinov, eds.), Proceedings of Machine
  Learning Research, vol.~97, PMLR, 09--15 Jun 2019, pp.~242--252.

\bibitem[Bac17]{bach2017breaking}
Francis Bach, \emph{Breaking the curse of dimensionality with convex neural
  networks}, The Journal of Machine Learning Research \textbf{18} (2017),
  no.~1, 629--681.

\bibitem[BFH{\etalchar{+}}18]{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, and Skye Wanderman-Milne, \emph{{JAX}: composable
  transformations of {P}ython+{N}um{P}y programs}, 2018.

\bibitem[CB18]{chizat2018global}
Lenaic Chizat and Francis Bach, \emph{On the global convergence of gradient
  descent for over-parameterized models using optimal transport}, Advances in
  neural information processing systems, 2018, pp.~3036--3046.

\bibitem[CB20]{chizat2020implicit}
\bysame, \emph{Implicit bias of gradient descent for wide two-layer neural
  networks trained with the logistic loss}, arXiv preprint arXiv:2002.04486
  (2020).

\bibitem[COB19]{chizat2019lazy}
Lenaic Chizat, Edouard Oyallon, and Francis Bach, \emph{On lazy training in
  differentiable programming}, Advances in Neural Information Processing
  Systems, 2019, pp.~2933--2943.

\bibitem[DJ95]{donoho1995adapting}
David~L Donoho and Iain~M Johnstone, \emph{Adapting to unknown smoothness via
  wavelet shrinkage}, Journal of the American Statistical Association
  \textbf{90} (1995), no.~432, 1200--1224.

\bibitem[DLL{\etalchar{+}}19]{du2019gradient}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai, \emph{Gradient
  descent finds global minima of deep neural networks}, Proceedings of the 36th
  International Conference on Machine Learning (Long Beach, California, USA)
  (Kamalika Chaudhuri and Ruslan Salakhutdinov, eds.), Proceedings of Machine
  Learning Research, vol.~97, PMLR, 09--15 Jun 2019, pp.~1675--1685.

\bibitem[DMHR{\etalchar{+}}18]{de2018gaussian}
AGG De~Matthews, J~Hron, M~Rowland, RE~Turner, and Z~Ghahramani, \emph{Gaussian
  process behaviour in wide deep neural networks}, 6th International Conference
  on Learning Representations, ICLR 2018-Conference Track Proceedings, 2018.

\bibitem[DZPS19]{du2018gradient}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh, \emph{Gradient
  descent provably optimizes over-parameterized neural networks}, International
  Conference on Learning Representations, 2019.

\bibitem[GARA19]{garriga2018deep}
Adri√† Garriga-Alonso, Carl~Edward Rasmussen, and Laurence Aitchison,
  \emph{Deep convolutional networks as shallow gaussian processes},
  International Conference on Learning Representations, 2019.

\bibitem[GKX19]{ghorbani2019investigation}
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao, \emph{An investigation into
  neural net optimization via hessian eigenvalue density}, International
  Conference on Machine Learning, 2019, pp.~2232--2241.

\bibitem[GMMM19a]{ghorbani2019limitations}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari,
  \emph{Limitations of lazy training of two-layers neural network}, Advances in
  Neural Information Processing Systems, 2019, pp.~9108--9118.

\bibitem[GMMM19b]{ghorbani2019linearized}
\bysame, \emph{Linearized two-layers neural networks in high dimension},
  arXiv:1904.12191 (2019).

\bibitem[GSJW19]{geiger2019disentangling}
Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart,
  \emph{Disentangling feature and lazy learning in deep neural networks: an
  empirical study}, arXiv preprint arXiv:1906.08034 (2019).

\bibitem[IS15]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy, \emph{Batch normalization: Accelerating
  deep network training by reducing internal covariate shift}, International
  Conference on Machine Learning, 2015, pp.~448--456.

\bibitem[JGH18]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler, \emph{Neural tangent
  kernel: Convergence and generalization in neural networks}, Advances in
  neural information processing systems, 2018, pp.~8571--8580.

\bibitem[KB14]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba, \emph{Adam: A method for stochastic
  optimization}, arXiv preprint arXiv:1412.6980 (2014).

\bibitem[LSdP{\etalchar{+}}18]{lee2018deep}
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam
  Schoenholz, and Yasaman Bahri, \emph{Deep neural networks as gaussian
  processes}, International Conference on Learning Representations, 2018.

\bibitem[LWY{\etalchar{+}}19]{li2019enhanced}
Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon~S Du, Wei Hu, Ruslan Salakhutdinov,
  and Sanjeev Arora, \emph{Enhanced convolutional neural tangent kernels},
  arXiv preprint arXiv:1911.00809 (2019).

\bibitem[LXS{\etalchar{+}}19]{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha
  Sohl-Dickstein, and Jeffrey Pennington, \emph{Wide neural networks of any
  depth evolve as linear models under gradient descent}, Advances in neural
  information processing systems, 2019, pp.~8570--8581.

\bibitem[MBM18]{mei2018landscape}
Song Mei, Yu~Bai, and Andrea Montanari, \emph{The landscape of empirical risk
  for nonconvex losses}, The Annals of Statistics \textbf{46} (2018), no.~6A,
  2747--2774.

\bibitem[MMN18]{mei2018mean}
Song Mei, Andrea Montanari, and Phan-Minh Nguyen, \emph{A mean field view of
  the landscape of two-layer neural networks}, Proceedings of the National
  Academy of Sciences \textbf{115} (2018), no.~33, E7665--E7671.

\bibitem[Ngu19]{nguyen2019mean}
Phan-Minh Nguyen, \emph{Mean field limit of the learning dynamics of multilayer
  neural networks}, arXiv:1902.02880 (2019).

\bibitem[NP20]{nguyen2020rigorous}
Phan-Minh Nguyen and Huy~Tuan Pham, \emph{A rigorous framework for the mean
  field limit of multilayer neural networks}, arXiv preprint arXiv:2001.11443
  (2020).

\bibitem[NXB{\etalchar{+}}19]{novak2019bayesian}
Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel~A.
  Abolafia, Jeffrey Pennington, and Jascha Sohl-dickstein, \emph{Bayesian deep
  convolutional networks with many channels are gaussian processes},
  International Conference on Learning Representations, 2019.

\bibitem[NXH{\etalchar{+}}20]{neuraltangents2020}
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander~A. Alemi, Jascha
  Sohl-Dickstein, and Samuel~S. Schoenholz, \emph{Neural tangents: Fast and
  easy infinite neural networks in python}, International Conference on
  Learning Representations, 2020.

\bibitem[OS20]{oymak2020towards}
Samet Oymak and Mahdi Soltanolkotabi, \emph{Towards moderate
  overparameterization: global convergence guarantees for training shallow
  neural networks}, IEEE Journal on Selected Areas in Information Theory
  (2020).

\bibitem[Pag18]{DavidPage2018}
David Page, \emph{Myrtle.ai},
  https://myrtle.ai/how-to-train-your-resnet-4-architecture/, 2018.

\bibitem[RR08]{rahimi2008random}
Ali Rahimi and Benjamin Recht, \emph{Random features for large-scale kernel
  machines}, Advances in neural information processing systems, 2008,
  pp.~1177--1184.

\bibitem[RVE18]{rotskoff2018neural}
Grant~M Rotskoff and Eric Vanden-Eijnden, \emph{Neural networks as interacting
  particle systems: Asymptotic convexity of the loss landscape and universal
  scaling of the approximation error}, arXiv:1805.00915 (2018).

\bibitem[SFG{\etalchar{+}}20]{shankar2020neural}
Vaishaal Shankar, Alex Fang, Wenshuo Guo, Sara Fridovich-Keil, Ludwig Schmidt,
  Jonathan Ragan-Kelley, and Benjamin Recht, \emph{Neural kernels without
  tangents}, arXiv:2003.02237 (2020).

\bibitem[SS18]{sirignano2018mean}
Justin Sirignano and Konstantinos Spiliopoulos, \emph{Mean field analysis of
  neural networks}, arXiv:1805.01053 (2018).

\bibitem[Tsy08]{tsybakov2008introduction}
Alexandre~B Tsybakov, \emph{Introduction to nonparametric estimation}, Springer
  Science \& Business Media, 2008.

\bibitem[YLS{\etalchar{+}}19]{yin2019fourier}
Dong Yin, Raphael~Gontijo Lopes, Jon Shlens, Ekin~Dogus Cubuk, and Justin
  Gilmer, \emph{A fourier perspective on model robustness in computer vision},
  Advances in Neural Information Processing Systems, 2019, pp.~13255--13265.

\bibitem[YS19]{yehudai2019power}
Gilad Yehudai and Ohad Shamir, \emph{On the power and limitations of random
  features for understanding neural networks}, Advances in Neural Information
  Processing Systems, 2019, pp.~6594--6604.

\bibitem[ZCZG18]{zou2018stochastic}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu, \emph{Stochastic gradient
  descent optimizes over-parameterized deep relu networks}, arXiv:1811.08888
  (2018).

\end{thebibliography}
