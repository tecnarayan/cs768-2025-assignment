\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alquier(2023)]{alquier2021user}
Alquier, P.
\newblock User-friendly introduction to pac-bayes bounds.
\newblock \emph{arXiv preprint arXiv:2110.11216}, 2023.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{arora2019implicit}
Arora, S., Cohen, N., Hu, W., and Luo, Y.
\newblock Implicit regularization in deep matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Berchenko(2024)]{Berchenko_2024}
Berchenko, Y.
\newblock Simplicity bias in overparameterized machine learning.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 38\penalty0 (10):\penalty0 11052--11060, Mar. 2024.
\newblock \doi{10.1609/aaai.v38i10.28981}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/28981}.

\bibitem[Blumer et~al.(1987)Blumer, Ehrenfeucht, Haussler, and Warmuth]{blumer1987occam}
Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M.~K.
\newblock Occam's razor.
\newblock \emph{Information processing letters}, 24\penalty0 (6):\penalty0 377--380, 1987.

\bibitem[Catoni(2007)]{cantoni2007pac}
Catoni, O.
\newblock {PAC}-{B}ayesian supervised classification: The thermodynamics of statistical learning.
\newblock \emph{IMS Lecture Notes Monograph Series}, 56:\penalty0 1–163, 2007.
\newblock \doi{10.1214/074921707000000391}.
\newblock URL \url{http://dx.doi.org/10.1214/074921707000000391}.
\newblock arXiv preprint arXiv:0712.0248.

\bibitem[Chiang et~al.(2023)Chiang, Ni, Miller, Bansal, Geiping, Goldblum, and Goldstein]{chiang2023loss}
Chiang, P., Ni, R., Miller, D.~Y., Bansal, A., Geiping, J., Goldblum, M., and Goldstein, T.
\newblock Loss landscapes are all you need: Neural network generalization can be explained without the implicit bias of gradient descent.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Chizat \& Bach(2020)Chizat and Bach]{pmlr-v125-chizat20a}
Chizat, L. and Bach, F.
\newblock Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss.
\newblock In Abernethy, J. and Agarwal, S. (eds.), \emph{Proceedings of Thirty Third Conference on Learning Theory}, volume 125 of \emph{Proceedings of Machine Learning Research}, pp.\  1305--1338. PMLR, 09--12 Jul 2020.

\bibitem[Frankle \& Carbin(2019)Frankle and Carbin]{frankle2019lottery}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Golowich et~al.(2018)Golowich, Rakhlin, and Shamir]{golowich2018size}
Golowich, N., Rakhlin, A., and Shamir, O.
\newblock Size-independent sample complexity of neural networks.
\newblock In \emph{Conference On Learning Theory}, pp.\  297--299. PMLR, 2018.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur, and Srebro]{Gunasekar2017}
Gunasekar, S., Woodworth, B.~E., Bhojanapalli, S., Neyshabur, B., and Srebro, N.
\newblock Implicit regularization in matrix factorization.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Hubara et~al.(2018)Hubara, Courbariaux, Soudry, El-Yaniv, and Bengio]{hubara2018quantized}
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y.
\newblock Quantized neural networks: Training neural networks with low precision weights and activations.
\newblock \emph{journal of machine learning research}, 18\penalty0 (187):\penalty0 1--30, 2018.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing internal covariate shift.
\newblock In \emph{International conference on machine learning}, pp.\  448--456. pmlr, 2015.

\bibitem[Kidger \& Lyons(2020)Kidger and Lyons]{kidger2020universal}
Kidger, P. and Lyons, T.
\newblock Universal approximation with deep narrow networks.
\newblock In \emph{Conference on learning theory}, pp.\  2306--2327. PMLR, 2020.

\bibitem[Langford \& Seeger(2002)Langford and Seeger]{langford2001bounds}
Langford, J. and Seeger, M.
\newblock \emph{Bounds for averaging classifiers}.
\newblock CMU Technical Report CMU-CS-01-102, 2002, 2002.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Cheng, Huang, Xing, and Shen]{Liu2022}
Liu, Z., Cheng, K.-T., Huang, D., Xing, E.~P., and Shen, Z.
\newblock Nonuniform-to-uniform quantization: Towards accurate quantization via generalized straight-through estimation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  4942--4952, June 2022{\natexlab{a}}.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Cheng, Huang, Xing, and Shen]{liu2022nonuniform}
Liu, Z., Cheng, K.-T., Huang, D., Xing, E.~P., and Shen, Z.
\newblock Nonuniform-to-uniform quantization: Towards accurate quantization via generalized straight-through estimation.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  4942--4952, 2022{\natexlab{b}}.

\bibitem[Lotfi et~al.(2022)Lotfi, Finzi, Kapoor, Potapczynski, Goldblum, and Wilson]{lotfi2022pacbayes}
Lotfi, S., Finzi, M., Kapoor, S., Potapczynski, A., Goldblum, M., and Wilson, A.~G.
\newblock Pac-bayes compression bounds so tight that they can explain generalization.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 31459--31473, 2022.

\bibitem[Lyu \& Li(2020)Lyu and Li]{lyu2019gradient}
Lyu, K. and Li, J.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Maas et~al.(2013)Maas, Hannun, Ng, et~al.]{Maas2013RectifierNI}
Maas, A.~L., Hannun, A.~Y., Ng, A.~Y., et~al.
\newblock Rectifier nonlinearities improve neural network acoustic models.
\newblock In \emph{Proc. icml}, volume~30, pp.\ ~3. Atlanta, GA, 2013.

\bibitem[Malach et~al.(2020)Malach, Yehudai, Shalev-Schwartz, and Shamir]{malach2020proving}
Malach, E., Yehudai, G., Shalev-Schwartz, S., and Shamir, O.
\newblock Proving the lottery ticket hypothesis: Pruning is all you need.
\newblock In \emph{International Conference on Machine Learning}, pp.\  6682--6691. PMLR, 2020.

\bibitem[McAllester(2003)]{SimplifiedPAC}
McAllester, D.
\newblock Simplified pac-bayesian margin bounds.
\newblock In Sch{\"o}lkopf, B. and Warmuth, M.~K. (eds.), \emph{Learning Theory and Kernel Machines}, pp.\  203--215, Berlin, Heidelberg, 2003. Springer Berlin Heidelberg.
\newblock ISBN 978-3-540-45167-9.

\bibitem[McAllester(1999)]{McAllester1999}
McAllester, D.~A.
\newblock Some pac-bayesian theorems.
\newblock \emph{Machine Learning}, 37\penalty0 (3):\penalty0 355--363, Dec 1999.
\newblock ISSN 1573-0565.
\newblock \doi{10.1023/A:1007618624809}.

\bibitem[Mingard et~al.(2021)Mingard, Valle-P{\'e}rez, Skalse, and Louis]{mingard2021sgd}
Mingard, C., Valle-P{\'e}rez, G., Skalse, J., and Louis, A.~A.
\newblock Is sgd a bayesian sampler? well, almost.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0 (1):\penalty0 3579--3642, 2021.

\bibitem[Moroshko et~al.(2020)Moroshko, Woodworth, Gunasekar, Lee, Srebro, and Soudry]{moroshko2020implicit}
Moroshko, E., Woodworth, B.~E., Gunasekar, S., Lee, J.~D., Srebro, N., and Soudry, D.
\newblock Implicit bias in deep linear classification: Initialization scale vs training accuracy.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 22182--22193, 2020.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and Srebro]{neyshabur2015norm}
Neyshabur, B., Tomioka, R., and Srebro, N.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{Conference on learning theory}, pp.\  1376--1401. PMLR, 2015.

\bibitem[Ramanujan et~al.(2020)Ramanujan, Wortsman, Kembhavi, Farhadi, and Rastegari]{ramanujan2020whats}
Ramanujan, V., Wortsman, M., Kembhavi, A., Farhadi, A., and Rastegari, M.
\newblock What's hidden in a randomly weighted neural network?
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  11893--11902, 2020.

\bibitem[Salimans \& Kingma(2016)Salimans and Kingma]{salimans2016weight}
Salimans, T. and Kingma, D.~P.
\newblock Weight normalization: A simple reparameterization to accelerate training of deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Shalev-Shwartz \& Ben-David(2014)Shalev-Shwartz and Ben-David]{shalev2014understanding}
Shalev-Shwartz, S. and Ben-David, S.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Soudry \& Hoffer(2017)Soudry and Hoffer]{soudry2017exponentially}
Soudry, D. and Hoffer, E.
\newblock Exponentially vanishing sub-optimal local minima in multilayer neural networks.
\newblock In \emph{ICLR workshop paper}, 2017.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, and Srebro]{soudry2017implicit}
Soudry, D., Hoffer, E., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Teney et~al.(2024)Teney, Nicolicioiu, Hartmann, and Abbasnejad]{teney2024neural}
Teney, D., Nicolicioiu, A., Hartmann, V., and Abbasnejad, E.
\newblock Neural redshift: Random networks are not random functions.
\newblock \emph{arXiv preprint arXiv:2403.02241}, 2024.

\bibitem[Theisen et~al.(2021)Theisen, Klusowski, and Mahoney]{theisen2021good}
Theisen, R., Klusowski, J., and Mahoney, M.
\newblock Good classifiers are abundant in the interpolating regime.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  3376--3384. PMLR, 2021.

\bibitem[TV(2017)]{2263641}
TV, M.
\newblock If $x,y$ are independent $\chi ^2$ with $m$ and $n$ degrees of freedom, then $\frac{X}{X+Y} \sim\beta(m/2, n/2)$.
\newblock Mathematics Stack Exchange, 2017.
\newblock URL \url{https://math.stackexchange.com/q/2263641}.
\newblock URL:https://math.stackexchange.com/q/2263641 (version: 2017-05-03).

\bibitem[Udell \& Townsend(2019)Udell and Townsend]{udell2019big}
Udell, M. and Townsend, A.
\newblock Why are big data matrices approximately low rank?
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 1\penalty0 (1):\penalty0 144--160, 2019.

\bibitem[Valle-Perez et~al.(2019)Valle-Perez, Camargo, and Louis]{vallepérez2019deep}
Valle-Perez, G., Camargo, C.~Q., and Louis, A.~A.
\newblock Deep learning generalizes because the parameter-function map is biased towards simple functions.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Vardi(2023)]{vardi2023implicit}
Vardi, G.
\newblock On the implicit bias in deep-learning algorithms.
\newblock \emph{Communications of the ACM}, 66\penalty0 (6):\penalty0 86--93, 2023.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese, Golan, Soudry, and Srebro]{pmlr-v125-woodworth20a}
Woodworth, B., Gunasekar, S., Lee, J.~D., Moroshko, E., Savarese, P., Golan, I., Soudry, D., and Srebro, N.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In Abernethy, J. and Agarwal, S. (eds.), \emph{Proceedings of Thirty Third Conference on Learning Theory}, volume 125 of \emph{Proceedings of Machine Learning Research}, pp.\  3635--3673. PMLR, 09--12 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v125/woodworth20a.html}.

\bibitem[Zeno et~al.(2024)Zeno, Ongie, Blumenfeld, Weinberger, and Soudry]{zeno2024minimum}
Zeno, C., Ongie, G., Blumenfeld, Y., Weinberger, N., and Soudry, D.
\newblock How do minimum-norm shallow denoisers look in function space?
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and Vinyals]{Zhang16}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Zhang et~al.(2019)Zhang, Dauphin, and Ma]{zhang2019fixup}
Zhang, H., Dauphin, Y.~N., and Ma, T.
\newblock Residual learning without normalization via better initialization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=H1gsz30cKX}.

\end{thebibliography}
