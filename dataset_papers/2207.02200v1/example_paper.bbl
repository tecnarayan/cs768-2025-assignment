\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Schuurmans, and Norouzi]{Agarwal2020AnOP}
Agarwal, R., Schuurmans, D., and Norouzi, M.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In \emph{ICML}, 2020.

\bibitem[An et~al.(2021)An, Moon, Kim, and Song]{an2021uncertainty}
An, G., Moon, S., Kim, J.-H., and Song, H.~O.
\newblock Uncertainty-based offline reinforcement learning with diversified
  q-ensemble.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and Munos]{Bellemare2017ADP}
Bellemare, M.~G., Dabney, W., and Munos, R.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{ICML}, 2017.

\bibitem[Chen et~al.(2021)Chen, Wang, Zhou, and Ross]{Chen2021RandomizedED}
Chen, X., Wang, C., Zhou, Z., and Ross, K.~W.
\newblock Randomized ensembled double q-learning: Learning fast without a
  model.
\newblock \emph{ArXiv}, abs/2101.05982, 2021.

\bibitem[Cobbe et~al.(2020)Cobbe, Hesse, Hilton, and
  Schulman]{Cobbe2020LeveragingPG}
Cobbe, K., Hesse, C., Hilton, J., and Schulman, J.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock \emph{ArXiv}, abs/1912.01588, 2020.

\bibitem[Dorfman \& Tamar(2020)Dorfman and Tamar]{Dorfman2020OfflineML}
Dorfman, R. and Tamar, A.
\newblock Offline meta learning of exploration.
\newblock \emph{arXiv: Learning}, 2020.

\bibitem[Duff(2002)]{Duff2002OptimalLC}
Duff, M.~O.
\newblock \emph{Optimal Learning: Computational procedures for Bayes-adaptive
  Markov decision processes}.
\newblock University of Massachusetts Amherst, 2002.

\bibitem[Eysenbach \& Levine(2019)Eysenbach and Levine]{eysenbachMaxEnt}
Eysenbach, B. and Levine, S.
\newblock If maxent rl is the answer, what is the question?
\newblock \emph{arXiv preprint arXiv:1910.01913}, 2019.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{Fu2020D4RLDF}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{ArXiv}, abs/2004.07219, 2020.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and
  Meger]{Fujimoto2018AddressingFA}
Fujimoto, S., Hoof, H.~V., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock \emph{ArXiv}, abs/1802.09477, 2018.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Fujimoto, S., Meger, D., and Precup, D.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2052--2062. PMLR, 2019.

\bibitem[Ghasemipour et~al.(2021)Ghasemipour, Schuurmans, and
  Gu]{ghasemipour2021emaq}
Ghasemipour, S. K.~S., Schuurmans, D., and Gu, S.~S.
\newblock Emaq: Expected-max q-learning operator for simple yet effective
  offline and online rl.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3682--3691. PMLR, 2021.

\bibitem[Ghasemipour et~al.(2022)Ghasemipour, Gu, and Nachum]{anonymous2022why}
Ghasemipour, S. K.~S., Gu, S.~S., and Nachum, O.
\newblock Why so pessimistic? estimating uncertainties for offline rl through
  ensembles, and why their independence matters, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.13703}.

\bibitem[Ghavamzadeh et~al.(2015)Ghavamzadeh, Mannor, Pineau, and
  Tamar]{Ghavamzadeh2015BayesianRL}
Ghavamzadeh, M., Mannor, S., Pineau, J., and Tamar, A.
\newblock Bayesian reinforcement learning: A survey.
\newblock \emph{Found. Trends Mach. Learn.}, 8:\penalty0 359--483, 2015.

\bibitem[Ghosh et~al.(2021)Ghosh, Rahme, Kumar, Zhang, Adams, and
  Levine]{Ghosh2021WhyGI}
Ghosh, D., Rahme, J., Kumar, A., Zhang, A., Adams, R.~P., and Levine, S.
\newblock Why generalization in rl is difficult: Epistemic pomdps and implicit
  partial observability.
\newblock \emph{ArXiv}, abs/2107.06277, 2021.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pp.\
  1861--1870. PMLR, 2018.

\bibitem[Hasselt et~al.(2016)Hasselt, Guez, and Silver]{Hasselt2016DeepRL}
Hasselt, H.~V., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double q-learning.
\newblock \emph{ArXiv}, abs/1509.06461, 2016.

\bibitem[Jeon et~al.(2018)Jeon, Seo, and Kim]{Jeon2018ABA}
Jeon, W., Seo, S., and Kim, K.-E.
\newblock A bayesian approach to generative adversarial imitation learning.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Jin et~al.(2021)Jin, Yang, and Wang]{jin2021pessimism}
Jin, Y., Yang, Z., and Wang, Z.
\newblock Is pessimism provably efficient for offline rl?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5084--5096. PMLR, 2021.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kostrikov et~al.(2021{\natexlab{a}})Kostrikov, Fergus, Tompson, and
  Nachum]{kostrikov2021fisher}
Kostrikov, I., Fergus, R., Tompson, J., and Nachum, O.
\newblock Offline reinforcement learning with fisher divergence critic
  regularization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5774--5783. PMLR, 2021{\natexlab{a}}.

\bibitem[Kostrikov et~al.(2021{\natexlab{b}})Kostrikov, Nair, and
  Levine]{kostrikov2021offline}
Kostrikov, I., Nair, A., and Levine, S.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock \emph{arXiv preprint arXiv:2110.06169}, 2021{\natexlab{b}}.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Tucker, and Levine]{kumar2019stabilizing}
Kumar, A., Fu, J., Tucker, G., and Levine, S.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock \emph{arXiv preprint arXiv:1906.00949}, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.04779}, 2020.

\bibitem[Kumar et~al.(2021)Kumar, Singh, Tian, Finn, and
  Levine]{kumar2021workflow}
Kumar, A., Singh, A., Tian, S., Finn, C., and Levine, S.
\newblock A workflow for offline model-free robotic reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2109.10813}, 2021.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{Lange2012BatchRL}
Lange, S., Gabel, T., and Riedmiller, M.~A.
\newblock Batch reinforcement learning.
\newblock In \emph{Reinforcement Learning}, 2012.

\bibitem[Lazaric \& Ghavamzadeh(2010)Lazaric and
  Ghavamzadeh]{Lazaric2010BayesianMR}
Lazaric, A. and Ghavamzadeh, M.
\newblock Bayesian multi-task reinforcement learning.
\newblock In \emph{ICML}, 2010.

\bibitem[Lee et~al.(2021)Lee, Laskin, Srinivas, and Abbeel]{Lee2021SUNRISEAS}
Lee, K., Laskin, M., Srinivas, A., and Abbeel, P.
\newblock Sunrise: A simple unified framework for ensemble learning in deep
  reinforcement learning.
\newblock In \emph{ICML}, 2021.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Luo et~al.(2021)Luo, Balakrishna, Thananjeyan, Nair, Ibarz, Tan, Finn,
  Stoica, and Goldberg]{luo2021mesa}
Luo, M., Balakrishna, A., Thananjeyan, B., Nair, S., Ibarz, J., Tan, J., Finn,
  C., Stoica, I., and Goldberg, K.
\newblock Mesa: Offline meta-rl for safe adaptation and fault tolerance.
\newblock \emph{arXiv preprint arXiv:2112.03575}, 2021.

\bibitem[Mandlekar et~al.(2021)Mandlekar, Xu, Wong, Nasiriany, Wang, Kulkarni,
  Fei-Fei, Savarese, Zhu, and Mart{\'\i}n-Mart{\'\i}n]{mandlekar2021matters}
Mandlekar, A., Xu, D., Wong, J., Nasiriany, S., Wang, C., Kulkarni, R.,
  Fei-Fei, L., Savarese, S., Zhu, Y., and Mart{\'\i}n-Mart{\'\i}n, R.
\newblock What matters in learning from offline human demonstrations for robot
  manipulation.
\newblock \emph{arXiv preprint arXiv:2108.03298}, 2021.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Monahan(2007)]{Monahan2007ASO}
Monahan, G.~E.
\newblock A survey of partially observable markov decision processes: Theory,
  models, and algorithms.
\newblock 2007.

\bibitem[Osband et~al.(2013)Osband, Russo, and Roy]{Osband2013MoreER}
Osband, I., Russo, D., and Roy, B.~V.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock In \emph{NIPS}, 2013.

\bibitem[Puterman(1994)]{puterman1994markov}
Puterman, M.~L.
\newblock \emph{Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, Inc., 1994.

\bibitem[Ramachandran \& Amir(2007)Ramachandran and
  Amir]{Ramachandran2007BayesianIR}
Ramachandran, D. and Amir, E.
\newblock Bayesian inverse reinforcement learning.
\newblock In \emph{IJCAI}, 2007.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and
  Silver]{Schaul2016PrioritizedER}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock \emph{CoRR}, abs/1511.05952, 2016.

\bibitem[Siegrist(2020)]{2020Discrete}
Siegrist, K.
\newblock Discrete-{Time} {Birth}-{Death} {Chains}, aug 10 2020.
\newblock [Online; accessed 2022-01-28].

\bibitem[Singh et~al.(1994)Singh, Jaakkola, and Jordan]{singhPOMDP}
Singh, S.~P., Jaakkola, T.~S., and Jordan, M.~I.
\newblock Learning without state-estimation in partially observable markovian
  decision processes.
\newblock In \emph{Proceedings of the Eleventh International Conference on
  International Conference on Machine Learning}, pp.\  284–292, San
  Francisco, CA, USA, 1994. Morgan Kaufmann Publishers Inc.
\newblock ISBN 1558603352.

\bibitem[Zintgraf et~al.(2020)Zintgraf, Shiarlis, Igl, Schulze, Gal, Hofmann,
  and Whiteson]{Zintgraf2020VariBADAV}
Zintgraf, L.~M., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hofmann, K., and
  Whiteson, S.
\newblock Varibad: A very good method for bayes-adaptive deep rl via
  meta-learning.
\newblock \emph{ArXiv}, abs/1910.08348, 2020.

\end{thebibliography}
