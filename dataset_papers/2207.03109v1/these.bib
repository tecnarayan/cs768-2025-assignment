
@article{10.2307/1911307,
  title = {The Folk Theorem in Repeated Games with Discounting or with Incomplete Information},
  author = {Fudenberg, Drew and Maskin, Eric},
  year = {1986},
  journal = {Econometrica : journal of the Econometric Society},
  volume = {54},
  number = {3},
  pages = {533--554},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {00129682, 14680262},
  abstract = {When either there are only two players or a "full dimensionality" condition holds, any individually rational payoff vector of a one-shot game of complete information can arise in a perfect equilibrium of the infinitely-repeated game if players are sufficiently patient. In contrast to earlier work, mixed strategies are allowed in determining the individually rational payoffs (even when only realized actions are observable). Any individually rational payoffs of a one-shot game can be approximated by sequential equilibrium payoffs of a long but finite game of incomplete information, where players' payoffs are almost certainly as in the one-shot game.}
}

@article{10.2307/3132156,
  title = {Uncoupled Dynamics Do Not Lead to Nash Equilibrium},
  author = {Hart, Sergiu and {Mas-Colell}, Andreu},
  year = {2003},
  journal = {The American Economic Review},
  volume = {93},
  number = {5},
  pages = {1830--1836},
  publisher = {{American Economic Association}},
  issn = {00028282}
}

@inproceedings{10.5555/2969442.2969581,
  title = {No-Regret Learning in Bayesian Games},
  booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
  author = {Hartline, Jason and Syrgkanis, Vasilis and Tardos, {\'E}va},
  year = {2015},
  series = {{{NIPS}}'15},
  pages = {3061--3069},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  abstract = {Recent price-of-anarchy analyses of games of complete information suggest that coarse correlated equilibria, which characterize outcomes resulting from no-regret learning dynamics, have near-optimal welfare. This work provides two main technical results that lift this conclusion to games of incomplete information, a.k.a., Bayesian games. First, near-optimal welfare in Bayesian games follows directly from the smoothness-based proof of near-optimal welfare in the same game when the private information is public. Second, no-regret learning dynamics converge to Bayesian coarse correlated equilibrium in these incomplete information games. These results are enabled by interpretation of a Bayesian game as a stochastic game of complete information.},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/U9ET6BKS/Hartline et al. - No-Regret Learning in Bayesian Games.pdf}
}

@inproceedings{10.5555/645530.655661,
  title = {Friend-or-Foe q-Learning in General-Sum Games},
  booktitle = {Proceedings of the Eighteenth International Conference on Machine Learning},
  author = {Littman, Michael L.},
  year = {2001},
  series = {{{ICML}} '01},
  pages = {322--328},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  isbn = {1-55860-778-1},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/45QT762S/Littman - 2001 - Friend-or-foe q-learning in general-sum games.pdf}
}

@article{abrahamneymanBoundedComplexityJustifies1985,
  title = {Bounded Complexity Justifies Cooperation in the Finitely Repeated Prisoners' Dilemma},
  author = {Neyman, Abraham},
  year = {1985},
  month = jan,
  journal = {Economics Letters},
  volume = {19},
  number = {3},
  pages = {227--229},
  issn = {0165-1765},
  doi = {10.1016/0165-1765(85)90026-6},
  abstract = {Cooperation in the finitely repeated prisoner's dilemma is justified, without departure from strict utility maximization or complete information, but under the assumption that there are bounds (possibly very large) to the complexity of the strategies that the players may use.},
  langid = {english},
  keywords = {pas lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/66MWGNUA/Neyman - 1985 - Bounded complexity justifies cooperation in the fi.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/XNZVKT8Q/0165176585900266.html}
}

@article{abrahamneymanStrategicEntropyComplexity1999,
  title = {Strategic {{Entropy}} and {{Complexity}} in {{Repeated Games}}},
  author = {Neyman, Abraham and Okada, Daijiro},
  year = {1999},
  month = oct,
  journal = {Games and Economic Behavior},
  volume = {29},
  number = {1-2},
  pages = {191--223},
  issn = {08998256},
  doi = {10.1006/game.1998.0674},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/QD4KJN5M/Neyman et Okada - 1999 - Strategic Entropy and Complexity in Repeated Games.pdf}
}

@article{abramPoliticalCorruptionPublic2018,
  title = {Political {{Corruption}} and {{Public Activism}}: {{An Evolutionary Game-Theoretic Analysis}}},
  shorttitle = {Political {{Corruption}} and {{Public Activism}}},
  author = {Abram, W. C. and Noray, K.},
  year = {2018},
  month = mar,
  journal = {Dynamic Games and Applications},
  volume = {8},
  number = {1},
  pages = {1--21},
  issn = {2153-0793},
  doi = {10.1007/s13235-017-0214-x},
  abstract = {We study a two-population evolutionary game that models the role of public activism as a deterrent to political corruption. In particular, suppose that politicians can choose whether or not to engage in corruption, lowering the public good in exchange for personal gain, and citizens can choose whether or not to engage in public activism for corruption reform, influencing the rate of detection and severity of punishment of corrupt politicians. We study the Nash equilibria of this game and also conduct static and dynamic evolutionary analyses.},
  langid = {english},
  keywords = {Evolutionary stability,Logit dynamics,Multi-population game,Political corruption,Public activism,Replicator dynamics}
}

@inproceedings{abramskyConcurrentGamesFull1999,
  title = {Concurrent Games and Full Completeness},
  booktitle = {Proceedings. 14th {{Symposium}} on {{Logic}} in {{Computer Science}} ({{Cat}}. {{No}}. {{PR00158}})},
  author = {Abramsky, S. and Mellies, P.-A.},
  year = {1999},
  pages = {431--442},
  publisher = {{IEEE Comput. Soc}},
  address = {{Trento, Italy}},
  doi = {10.1109/LICS.1999.782638},
  abstract = {A new concurrent form of game semantics is introduced. This overcomes the problems which had arisen with previous, sequential forms of game semantics in modelling Linear Logic. It also admits an elegant and robust formalization. A Full Completeness Theorem for MultiplicativeAdditive Linear Logic is proved for this semantics.},
  isbn = {978-0-7695-0158-1},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ZXEBT3UR/Abramsky et Mellies - 1999 - Concurrent games and full completeness.pdf}
}

@article{abreuStructureNashEquilibrium1988a,
  title = {The {{Structure}} of {{Nash Equilibrium}} in {{Repeated Games}} with {{Finite Automata}}},
  author = {Abreu, Dilip and Rubinstein, Ariel},
  year = {1988},
  month = nov,
  journal = {Econometrica},
  volume = {56},
  number = {6},
  pages = {1259},
  issn = {00129682},
  doi = {10.2307/1913097},
  langid = {english},
  keywords = {lu,preuve pas lue},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/FLN6T6WD/Abreu et Rubinstein - 1988 - The Structure of Nash Equilibrium in Repeated Game.pdf}
}

@article{acemogluTransitionCleanTechnology,
  title = {Transition to {{Clean Technology}}},
  author = {Acemoglu, Daron and Akcigit, Ufuk and Hanley, Douglas and Kerr, William},
  journal = {journal of political economy},
  pages = {53},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/89IXBV5M/Acemoglu et al. - Transition to Clean Technology.pdf}
}

@article{acemogluTransitionCleanTechnology2016,
  title = {Transition to {{Clean Technology}}},
  author = {Acemoglu, Daron and Akcigit, Ufuk and Hanley, Douglas and Kerr, William},
  year = {2016},
  month = jan,
  journal = {Journal of Political Economy},
  publisher = {{University of Chicago PressChicago, IL}},
  issn = {0022-3808},
  doi = {10.1086/684511},
  abstract = {We develop an endogenous growth model in which clean and dirty technologies compete in production. Research can be directed to either technology. If dirty technologies are more advanced, the transition to clean technology can be difficult. Carbon taxes and research subsidies may encourage production and innovation in clean technologies, though the transition will typically be slow. We estimate the model using microdata from the US energy sector. We then characterize the optimal policy path that heavily relies on both subsidies and taxes. Finally, we evaluate various alternative policies. Relying only on carbon taxes or delaying intervention has significant welfare costs.},
  copyright = {\textcopyright{} 2016 by The University of Chicago. All rights reserved.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/PSR3HPWC/Acemoglu et al. - 2016 - Transition to Clean Technology.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/LI2GUHPQ/684511.html}
}

@article{acemogluWhyDidWest2000,
  title = {Why {{Did}} the {{West Extend}} the {{Franchise}}? {{Democracy}}, {{Inequality}}, and {{Growth}} in {{Historical Perspective}}},
  shorttitle = {Why {{Did}} the {{West Extend}} the {{Franchise}}?},
  author = {Acemoglu, D. and Robinson, J. A.},
  year = {2000},
  month = nov,
  journal = {The Quarterly Journal of Economics},
  volume = {115},
  number = {4},
  pages = {1167--1199},
  issn = {0033-5533, 1531-4650},
  doi = {10.1162/003355300555042},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/BB7TAMKG/Acemoglu et Robinson - 2000 - Why Did the West Extend the Franchise Democracy, .pdf}
}

@article{adamikAtomicSplittableFlow2020,
  title = {Atomic {{Splittable Flow Over Time Games}}},
  author = {Adamik, Antonia and Sering, Leon},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.02148 [cs, math]},
  eprint = {2010.02148},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {In an atomic splittable flow over time game, finitely many players route flow dynamically through a network, in which edges are equipped with transit times, specifying the traversing time, and with capacities, restricting flow rates. Infinitesimally small flow particles controlled by the same player arrive at a constant rate at the player's origin and the player's goal is to maximize the flow volume that arrives at the player's destination within a given time horizon. Hereby, the flow dynamics described by the deterministic queuing model, i.e., flow of different players merges perfectly, but excessive flow has to wait in a queue in front of the bottle-neck. In order to determine Nash equilibria in such games, the main challenge is to consider suitable definitions for the players' strategies, which depend on the level of information the players receive throughout the game. For the most restricted version, in which the players receive no information on the network state at all, we can show that there is no Nash equilibrium in general, not even for networks with only two edges. However, if the current edge congestions are provided over time, the players can adopt their route choices dynamically. We show that a profile of those strategies always lead to a unique feasible flow over time. Hence, those atomic splittable flow over time games are well-defined. For parallel-edge networks Nash equilibria exists and the total flow arriving in time equals the value of a maximum flow over time leading to a price of anarchy is 1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {05C21; 91A12,Computer Science - Computer Science and Game Theory,Mathematics - Optimization and Control},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/SK8UK886/Adamik et Sering - 2020 - Atomic Splittable Flow Over Time Games.pdf}
}

@misc{afinetheoremAsymptoticCalibrationFoster2011,
  title = {``{{Asymptotic Calibration}},'' {{D}}. {{Foster}} \& {{R}}. {{Vohra}} (1998)},
  author = {{afinetheorem}},
  year = {2011},
  month = jan,
  journal = {A Fine Theorem},
  abstract = {In the last post, I wrote about Dawid's result that no forecasting technique, no matter how clever, will be able to calibrate itself against nature in a coherent way. Is there a way to save c\ldots},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/9QE56ADZ/asymptotic-calibration-d-foster-r-vohra-1998.html}
}

@techreport{aguirregabiriaRecentDevelopmentsEmpirical2010,
  title = {Recent Developments in Empirical {{IO}}: Dynamic Demand and Dynamic Games},
  shorttitle = {Recent Developments in Empirical {{IO}}},
  author = {Aguirregabiria, Victor and Nevo, Aviv},
  year = {2010},
  month = dec,
  journal = {MPRA Paper},
  number = {27814},
  institution = {{University Library of Munich, Germany}},
  abstract = {Empirically studying dynamic competition in oligopoly markets requires dealing with large states spaces and tackling difficult computational problems, while handling heterogeneity and multiple equilibria. In this paper, we discuss some of the ways recent work in Industrial Organization has dealt with these challenges. We illustrate problems and proposed solutions using as examples recent work on dynamic demand for differentiated products and on dynamic games of oligopoly competition. Our discussion of dynamic demand focuses on models for storable and durable goods and surveys how researchers have used the "inclusive value" to deal with dimensionality problems and reduce the computational burden. We clarify the assumptions needed for this approach to work, the implications for the treatment of heterogeneity and the different ways it has been used. In our discussion of the econometrics of dynamics games of oligopoly competition, we deal with challenges related to estimation and counterfactual experiments in models with multiple equilibria. We also examine methods for the estimation of models with persistent unobserved heterogeneity in product characteristics, firms' costs, or local market profitability. Finally, we discuss different approaches to deal with large state spaces in dynamic games.},
  langid = {english},
  keywords = {Counterfactual experiments,Dynamic demand,Dynamic games,Estimation,Inclusive values,Industrial Organization,Multiple equilibria,Oligopoly competition,Unobserved heterogeneity.},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/X8MVU9WF/Aguirregabiria et Nevo - 2010 - Recent developments in empirical IO dynamic deman.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/EVB2LYRQ/27814.html}
}

@incollection{aguirregabiriaRecentDevelopmentsEmpirical2013,
  title = {Recent {{Developments}} in {{Empirical IO}}: {{Dynamic Demand}} and {{Dynamic Games}}},
  shorttitle = {Recent {{Developments}} in {{Empirical IO}}},
  booktitle = {Advances in {{Economics}} and {{Econometrics}}: {{Tenth World Congress}}: {{Volume}} 3: {{Econometrics}}},
  author = {Aguirregabiria, Victor and Nevo, Aviv},
  editor = {Acemoglu, Daron and Dekel, Eddie and Arellano, Manuel},
  year = {2013},
  series = {Econometric {{Society Monographs}}},
  volume = {3},
  pages = {53--122},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9781139060035.003},
  abstract = {IntroductionImportant aspects of competition in oligopoly markets are dynamic. Demand can be dynamic if products are storable or durable, or if utility from consumption is linked intertemporally. On the supply side, dynamics can be present as well. For example, investment and production decisions have dynamic implications if there is ``learning-by-doing'' or if there are sunk costs. Identifying the factors governing the dynamics is key to understanding competition and the evolution of market structure and for the evaluation of public policy. Advances in econometric methods and modeling techniques and the increased availability of data have led to a large body of empirical papers that study the dynamics of demand and competition in oligopoly markets.A key lesson learned early by most researchers is the complexity and challenges of modeling and estimating dynamic structural models. The complexity and ``curse of dimensionality'' are present even in relatively simple models but are especially problematic in oligopoly markets in which firms produce differentiated products or have heterogeneous costs. These sources of heterogeneity typically imply that the dimension of these models, and the computational cost of solving and estimating them, increases exponentially with the number of products and the number of firms. As a result, much of the recent work in structural econometrics in IO focuses on finding ways to make dynamic problems more tractable in terms of computation and careful modeling to reduce the state space while properly accounting for rich heterogeneity, dynamics, and strategic interactions.},
  isbn = {978-1-107-62731-4},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ML7GRN49/Aguirregabiria et Nevo - 2013 - Recent Developments in Empirical IO Dynamic Deman.pdf}
}

@article{altmanDynamicGamesNovel2016,
  title = {Dynamic {{Games}} in {{Novel Networks}}: {{Guest Editors}}' {{Forewords}}},
  shorttitle = {Dynamic {{Games}} in {{Novel Networks}}},
  author = {Altman, E. and De Pellegrini, F.},
  year = {2016},
  month = dec,
  journal = {Dynamic Games and Applications},
  volume = {6},
  number = {4},
  pages = {427--428},
  issn = {2153-0785, 2153-0793},
  doi = {10.1007/s13235-016-0193-3},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ITN5WIYA/Altman et De Pellegrini - 2016 - Dynamic Games in Novel Networks Guest Editors’ Fo.pdf}
}

@article{altmanStochasticGameApproach2013,
  title = {A {{Stochastic Game Approach}} for {{Competition}} over {{Popularity}} in {{Social Networks}}},
  author = {Altman, Eitan},
  year = {2013},
  month = jun,
  journal = {Dynamic Games and Applications},
  volume = {3},
  number = {2},
  pages = {313--323},
  issn = {2153-0785, 2153-0793},
  doi = {10.1007/s13235-012-0057-4},
  abstract = {The global Internet has enabled a massive access of internauts to content. At the same time it allowed individuals to use the Internet in order to distribute content. When individuals pass through a conotent provider to distribute contents, they can benefit from many tools that the content provider has in order to accelerate the dessiminaton of the content. These include cashing as well as recommendation systems. The content provider gives preferencial treatment to individuals who pay for advertisement. In this paper we study competition between several contents, each characterized by some given potential popularity. We answer the question of when is it worthwhile to invest in adveretisement as a function of the potential popularity of a content as well as its competing contents, who are faced with a similar question. We formulate the problem as a stochastic game with a finite state and action space and obtain the structure of the equilibria policy under a linear structure of the dissemination utility as well as on the advertisement costs. We then consider open loop control (no state information) and solve the game using a transformation into a differential game with a compact state space.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/LIDGBZZ9/Altman - 2013 - A Stochastic Game Approach for Competition over Po.pdf}
}

@book{amadaePrisonersReasonGame2015,
  title = {Prisoners of {{Reason}}: {{Game Theory}} and {{Neoliberal Political Economy}}},
  shorttitle = {Prisoners of {{Reason}}},
  author = {Amadae, S. M.},
  year = {2015},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9781107565258},
  isbn = {978-1-107-56525-8},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/3LDJ74PN/Amadae - 2015 - Prisoners of Reason Game Theory and Neoliberal Po.pdf}
}

@inproceedings{amirStochasticGamesEconomics2003,
  title = {Stochastic {{Games}} in {{Economics}} and {{Related Fields}}: {{An Overview}}},
  shorttitle = {Stochastic {{Games}} in {{Economics}} and {{Related Fields}}},
  booktitle = {Stochastic {{Games}} and {{Applications}}},
  author = {Amir, Rabah},
  editor = {Neyman, Abraham and Sorin, Sylvain},
  year = {2003},
  series = {{{NATO Science Series}}},
  pages = {455--470},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-010-0189-2_30},
  abstract = {This survey provides an extensive account of research in economics based on the stochastic games paradigm. Its area-by-area coverage is in the form of an overview, and includes applications in resource economics, industrial organization, macroeconomics, market games, and experimental and empirical economics. As to methodologically defined frameworks, the coverage is somewhat more detailed (to the extent that the material is not covered elsewhere in this volume), and includes the open-loop concept, the linear-quadratic model, myopic equilibrium, games of perfect information, and stochastic games with a continuum of players. It is hoped that the survey might be useful as a general guide both to economists and to game theorists.},
  isbn = {978-94-010-0189-2},
  langid = {english},
  keywords = {Dynamic Game,Exhaustible Resource,Market Game,Perfect Information,Stochastic Game}
}

@article{anagnostidesNearOptimalNoRegretLearning2021,
  title = {Near-{{Optimal No-Regret Learning}} for {{Correlated Equilibria}} in {{Multi-Player General-Sum Games}}},
  author = {Anagnostides, Ioannis and Daskalakis, Constantinos and Farina, Gabriele and Fishelson, Maxwell and Golowich, Noah and Sandholm, Tuomas},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.06008 [cs]},
  eprint = {2111.06008},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recently, Daskalakis, Fishelson, and Golowich (DFG) (NeurIPS `21) showed that if all agents in a multi-player general-sum normal-form game employ Optimistic Multiplicative Weights Update (OMWU), the external regret of every player is O(polylog(T )) after T repetitions of the game. We extend their result from external regret to internal regret and swap regret, thereby establishing uncoupled learning dynamics that converge to an approximate correlated equilibrium at the rate of O T -1 . This substantially improves over the prior best rate of convergence for correlated equilibria of O(T -3/4) due to Chen and Peng (NeurIPS `20), and it is optimal\textemdash within the no-regret framework\textemdash up to polylogarithmic factors in T .},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/HCUF5Z5S/Anagnostides et al. - 2021 - Near-Optimal No-Regret Learning for Correlated Equ.pdf}
}

@article{angelImpactLocalPolicies2008,
  title = {The Impact of Local Policies on the Quality of Packet Routing in Paths, Trees, and Rings},
  author = {Angel, Eric and Bampis, Evripidis and Pascual, Fanny},
  year = {2008},
  month = oct,
  journal = {Journal of Scheduling},
  volume = {11},
  number = {5},
  pages = {311--322},
  issn = {1094-6136, 1099-1425},
  doi = {10.1007/s10951-008-0069-5},
  abstract = {We consider the packet routing problem in storeand-forward networks whose topologies are either paths, trees, or rings. We are interested by the quality of the solution produced, with respect to a global optimal solution, if each link uses a (fixed) local policy to schedule the packets which go through it. The quality of the derived solutions is measured using the worst case analysis for two global optimality criteria, namely the maximum arrival date of a packet at its destination (or makespan) and the average arrival date of the packets at their destinations.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/7HMBX5SG/Angel et al. - 2008 - The impact of local policies on the quality of pac.pdf}
}

@misc{ApplicationsTheorieJeux,
  title = {Applications de {{Th\'eorie}} Des Jeux Coop\'eratifs et Non Coop\'eratifs Application Aux Sciences Sociales},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/BY5LKYUI/Capture du 2020-03-26 23-25-59.png;/home/lucas/nextcloud.lamsade/Zotero/storage/GBZLZY59/Capture du 2020-03-26 23-25-33.png}
}

@article{arenaTheorieJeuxPeutelle2016,
  title = {{La th\'eorie des jeux peut-elle aider \`a comprendre l'\'evolution des politiques de la concurrence et des politiques industrielles depuis la fin des ann\'ees 1970 ?}},
  author = {Arena, Richard and Dutraive, V{\'e}ronique},
  year = {2016},
  journal = {Revue \'economique},
  volume = {67},
  number = {HS1},
  pages = {9},
  issn = {0035-2764, 1950-6694},
  doi = {10.3917/reco.hs01.0009},
  langid = {french},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/KWJHU6SV/Arena et Dutraive - 2016 - La théorie des jeux peut-elle aider à comprendre l.pdf}
}

@article{arenaTheorieJeuxPeutelle2016a,
  title = {{La th\'eorie des jeux peut-elle aider \`a comprendre l'\'evolution des politiques de la concurrence et des politiques industrielles depuis la fin des ann\'ees 1970 ?}},
  author = {Arena, Richard and Dutraive, V{\'e}ronique},
  year = {2016},
  month = mar,
  journal = {Revue economique},
  volume = {Vol. 67},
  number = {HS1},
  pages = {9--24},
  publisher = {{Presses de Sciences Po}},
  issn = {0035-2764},
  abstract = {Cet article s'inscrit dans un projet plus g\'en\'eral d'\'etude des relations entre les politiques de la concurrence et les politiques industrielles effectives, et les diff\'erents fondements th\'eoriques associ\'es \`a ces politiques. Plus pr\'ecis\'ement ici, l'objet principal du texte est l'\'etude d'une double \'evolution dans l'utilisation de la th\'eorie des jeux en \'economie industrielle depuis la fin des ann\'ees 1970 jusqu'\`a nos jours, celle qui tient d'abord \`a la dynamique intellectuelle propre \`a cette th\'eorie, puis \`a la mani\`ere dont elle s'est adapt\'ee pour aider \`a mieux comprendre les changements des politiques de la concurrence ou des politiques industrielles men\'ees depuis les ann\'ees 1970.},
  langid = {french},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/54ULJ53M/Arena et Dutraive - 2016 - La théorie des jeux peut-elle aider à comprendre l.pdf}
}

@article{arieliAverageTestingPareto2012,
  title = {Average Testing and {{Pareto}} Efficiency},
  author = {Arieli, Itai and Babichenko, Yakov},
  year = {2012},
  month = nov,
  journal = {Journal of Economic Theory},
  volume = {147},
  number = {6},
  pages = {2376--2398},
  issn = {00220531},
  doi = {10.1016/j.jet.2012.05.001},
  abstract = {We propose a simple adaptive procedure for playing strategic games: average testing. In this procedure each player sticks to her current strategy if it yields a payoff that exceeds her average payoff by at least some fixed {$\epsilon$} {$>$} 0; otherwise she chooses a strategy at random. We consider generic two-person games where both players play according to the average testing procedure on blocks of k-periods. We demonstrate that for all k large enough, the pair of time-average payoffs converges (almost surely) to the 3{$\epsilon$}-Pareto efficient boundary.},
  langid = {english},
  keywords = {lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/RA2ZYIA2/Arieli et Babichenko - 2012 - Average testing and Pareto efficiency.pdf}
}

@article{arieliSequentialNaiveLearning2021,
  title = {Sequential {{Naive Learning}}},
  author = {Arieli, Itai and Babichenko, Yakov and {Mueller-Frank}, Manuel},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.02897 [cs]},
  eprint = {2101.02897},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We analyze boundedly rational updating from aggregate statistics in a model with binary actions and binary states. Agents each take an irreversible action in sequence after observing the unordered set of previous actions. Each agent first forms her prior based on the aggregate statistic, then incorporates her signal with the prior based on Bayes rule, and finally applies a decision rule that assigns a (mixed) action to each belief. If priors are formed according to a discretized DeGroot rule, then actions converge to the state (in probability), i.e., \textbackslash emph\{asymptotic learning\}, in any informative information structure if and only if the decision rule satisfies probability matching. This result generalizes to unspecified information settings where information structures differ across agents and agents know only the information structure generating their own signal. Also, the main result extends to the case of \$n\$ states and \$n\$ actions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/2SZSSKTG/Arieli et al. - 2021 - Sequential Naive Learning.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/3G877HIJ/2101.html}
}

@article{arnottDoesProvidingInformation1991,
  title = {Does Providing Information to Drivers Reduce Traffic Congestion?},
  author = {Arnott, Richard and {de Palma}, Andre and Lindsey, Robin},
  year = {1991},
  month = sep,
  journal = {Transportation Research Part A: General},
  volume = {25},
  number = {5},
  pages = {309--318},
  issn = {01912607},
  doi = {10.1016/0191-2607(91)90146-H},
  abstract = {The purpose of this article is to question the presumption that route guidance and information systems necessarily reduce traffic congestion, and to point out the need to consider the general equilibrium effects of information. A simple model of the morning rush hour is adopted in which commuters choose a departure time and one of two routes to work, the capacities of which are stochastic. While expected travel costs are reduced by perfectly informing all drivers about route capacities, this is not necessarily the case if imperfect information is provided. A heuristic explanation is that, absent tolls, congestion is an uninternalized externality. Information can cause drivers to change their departure times in such a way as to exacerbate congestion.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/3YNZ4SMH/Arnott et al. - 1991 - Does providing information to drivers reduce traff.pdf}
}

@article{aroraPolicyRegretRepeated2020,
  title = {Policy {{Regret}} in {{Repeated Games}}},
  author = {Arora, Raman and Dinitz, Michael and Marinov, Teodor V. and Mohri, Mehryar},
  year = {2020},
  month = mar,
  journal = {arXiv:1811.04127 [cs, stat]},
  eprint = {1811.04127},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The notion of policy regret in online learning is a well defined performance measure for the common scenario of adaptive adversaries, which more traditional quantities such as external regret do not take into account. We revisit the notion of policy regret and first show that there are online learning settings in which policy regret and external regret are incompatible: any sequence of play that achieves a favorable regret with respect to one definition must do poorly with respect to the other. We then focus on the game-theoretic setting where the adversary is a self-interested agent. In that setting, we show that external regret and policy regret are not in conflict and, in fact, that a wide class of algorithms can ensure a favorable regret with respect to both definitions, so long as the adversary is also using such an algorithm. We also show that the sequence of play of no-policy regret algorithms converges to a policy equilibrium, a new notion of equilibrium that we introduce. Relating this back to external regret, we show that coarse correlated equilibria, which no-external regret players converge to, are a strict subset of policy equilibria. Thus, in game-theoretic settings, every sequence of play with no external regret also admits no policy regret, but the converse does not hold.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/69G3WXHZ/Arora et al. - 2020 - Policy Regret in Repeated Games.pdf}
}

@article{arslanDecentralizedQLearningStochastic2016,
  title = {Decentralized {{Q-Learning}} for {{Stochastic Teams}} and {{Games}}},
  author = {Arslan, G{\"u}rdal and Y{\"u}ksel, Serdar},
  year = {2016},
  month = may,
  journal = {arXiv:1506.07924 [cs, math]},
  eprint = {1506.07924},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {There are only a few learning algorithms applicable to stochastic dynamic teams and games which generalize Markov decision processes to decentralized stochastic control problems involving possibly self-interested decision makers. Learning in games is generally difficult because of the non-stationary environment in which each decision maker aims to learn its optimal decisions with minimal information in the presence of the other decision makers who are also learning. In stochastic dynamic games, learning is more challenging because, while learning, the decision makers alter the state of the system and hence the future cost. In this paper, we present decentralized Q-learning algorithms for stochastic games, and study their convergence for the weakly acyclic case which includes team problems as an important special case. The algorithm is decentralized in that each decision maker has access to only its local information, the state information, and the local cost realizations; furthermore, it is completely oblivious to the presence of other decision makers. We show that these algorithms converge to equilibrium policies almost surely in large classes of stochastic games.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/PS3II5SG/Arslan et Yüksel - 2016 - Decentralized Q-Learning for Stochastic Teams and .pdf}
}

@book{aubinDifferentialInclusionsSetValued1984,
  title = {Differential {{Inclusions}}: {{Set-Valued Maps}} and {{Viability Theory}}},
  shorttitle = {Differential {{Inclusions}}},
  author = {Aubin, Jean-Pierre and Cellina, Arrigo},
  year = {1984},
  series = {Grundlehren Der Mathematischen {{Wissenschaften}}},
  volume = {264},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-69512-4},
  isbn = {978-3-642-69514-8 978-3-642-69512-4},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/CT6B455S/Aubin et Cellina - 1984 - Differential Inclusions Set-Valued Maps and Viabi.pdf}
}

@article{audibertRegretBoundsMinimax,
  title = {Regret {{Bounds}} and {{Minimax Policies}} under {{Partial Monitoring}}},
  author = {Audibert, Jean-Yves},
  pages = {52},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/3XVYRUGE/Audibert - Regret Bounds and Minimax Policies under Partial M.pdf}
}

@article{aumannCooperationBoundedRecall1989,
  title = {Cooperation and Bounded Recall},
  author = {Aumann, Robert J and Sorin, Sylvain},
  year = {1989},
  month = mar,
  journal = {Games and Economic Behavior},
  volume = {1},
  number = {1},
  pages = {5--39},
  issn = {08998256},
  doi = {10.1016/0899-8256(89)90003-1},
  langid = {english},
  keywords = {lu,preuve pas lue},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/MQ2RPS2Y/Aumann et Sorin - 1989 - Cooperation and bounded recall.pdf}
}

@article{aumannEpistemicConditionsNash1995,
  title = {Epistemic {{Conditions}} for {{Nash Equilibrium}}},
  author = {Aumann, Robert and Brandenburger, Adam},
  year = {1995},
  month = sep,
  journal = {Econometrica},
  volume = {63},
  number = {5},
  pages = {1161},
  issn = {00129682},
  doi = {10.2307/2171725},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/H6H5V6SA/Aumann et Brandenburger - 1995 - Epistemic Conditions for Nash Equilibrium.pdf}
}

@incollection{aumannLongTermCompetitionGameTheoretic1994,
  title = {Long-{{Term Competition}}\textemdash{{A Game-Theoretic Analysis}}},
  booktitle = {Essays in {{Game Theory}}: {{In Honor}} of {{Michael Maschler}}},
  author = {Aumann, Robert J. and Shapley, Lloyd S.},
  editor = {Megiddo, Nimrod},
  year = {1994},
  pages = {1--15},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-2648-2_1},
  abstract = {There have been continuing expressions of interest from a variety of quarters in the development of techniques for modelling national behavior in a long-term context of continuing international rivalry \textemdash{} for short, ``long term competition''. The most characteristic feature of these models is that they extend over time in a fairly regular or repetitive manner. The underlying structure of possible actions and consequences remains the same, though parameters may vary and balances shift, and the decisions and policies of the national decision makers are by no means constrained to be constant or smoothly-varying, or even ``rational'' in any precisely identifiable sense. The use of game theory or an extension thereof is obviously indicated, and considerable theoretical progress has been made in this area. But the ability of the theory to handle real applications is still far from satisfactory. The trouble lies less with the descriptive modelling, \textemdash{} i.e., formulating the ``rules of the game'' in a dynamic setting, than with the choice of a solu- tion concept that will do dynamic justice to the interplay of motivations of the actors. (Game theoreticians, like mathematical economists, have always been more comfortable with static than dynamic models.) Since any predictions, recommendations, etc. that a mathematical analysis can produce will likely be very sensitive to the rationale of the solution that is used, and since the big difficulties are conceptual rather than technical, it seems both possible and worthwhile to discuss salient features of the theory without recourse to heavy mathematical apparatus or overly formal arguments, and thereby perhaps make the issues involved accessible to at least some of the potential customers for the practical analyses that we wish we could carry out in a more satisfactory and convincing manner.},
  isbn = {978-1-4612-2648-2},
  langid = {english},
  keywords = {Average Payoff,Equilibrium Point,Mixed Strategy,Pure Strategy,Repeated Game},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/4JNMBYH7/Aumann et Shapley - 1994 - Long-Term Competition—A Game-Theoretic Analysis.pdf}
}

@article{babesQlearningTwoPlayerTwoAction2009,
  title = {Q-Learning in {{Two-Player Two-Action Games}}},
  author = {Babes, Monica and Wunder, Michael and Littman, Michael},
  year = {2009},
  pages = {4},
  abstract = {Q-learning is a simple, powerful algorithm for behavior learning. It was derived in the context of single agent decision making in Markov decision process environments, but its applicability is much broader\textemdash in experiments in multiagent environments, Q-learning has also performed well. Our preliminary analysis using dynamical systems finds that Qlearning's indirect control of behavior via estimates of value contributes to its beneficial performance in general-sum 2player games like the Prisoner's Dilemma.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/W9HAUIXK/Babes et al. - Q-learning in Two-Player Two-Action Games.pdf}
}

@article{babichenkoUncoupledAutomataPure2010,
  title = {Uncoupled Automata and Pure {{Nash}} Equilibria},
  author = {Babichenko, Yakov},
  year = {2010},
  month = jul,
  journal = {International Journal of Game Theory},
  volume = {39},
  number = {3},
  pages = {483--502},
  issn = {0020-7276, 1432-1270},
  doi = {10.1007/s00182-010-0227-9},
  abstract = {We study the problem of reaching a pure Nash equilibrium in multi-person games that are repeatedly played, under the assumption of uncoupledness: EVERY player knows only his own payoff function. We consider strategies that can be implemented by finite-state automata, and characterize the minimal number of states needed in order to guarantee that a pure Nash equilibrium is reached in every game where such an equilibrium exists.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ANC3DMLF/Babichenko - 2010 - Uncoupled automata and pure Nash equilibria.pdf}
}

@book{badiouConceptModeleIntroduction1969,
  title = {{Le concept de mod\`ele: introduction \`a une \'epist\'emologiemat\'erialiste des math\'ematiques}},
  shorttitle = {{Le concept de mod\`ele}},
  author = {Badiou, Alain and Debru, Claude and Dupont, Jean-Yves and Francioli, Fran{\c c}oise and Simon, G{\'e}rard},
  year = {1969},
  series = {{Th\'eorie}},
  number = {4},
  publisher = {{F. Maspero}},
  address = {{Paris}},
  langid = {french}
}

@article{bagagioloMeanFieldGamesDynamic2014,
  title = {Mean-{{Field Games}} and {{Dynamic Demand Management}} in {{Power Grids}}},
  author = {Bagagiolo, Fabio and Bauso, Dario},
  year = {2014},
  month = jun,
  journal = {Dynamic Games and Applications},
  volume = {4},
  number = {2},
  pages = {155--176},
  issn = {2153-0785, 2153-0793},
  doi = {10.1007/s13235-013-0097-4},
  abstract = {This paper applies mean field game theory to dynamic demand management. For a large population of electrical heating or cooling appliances (called agents), we provide a mean field game that guarantees desynchronization of the agents thus improving the power network resilience. Second, for the game at hand, we exhibit a mean field equilibrium, where each agent adopts a bang-bang switching control with threshold placed at a nominal temperature. At the equilibrium, through an opportune design of the terminal penalty, the switching control regulates the mean temperature (computed over the population) and the mains frequency around the nominal value. To overcome Zeno phenomena we also adjust the bang-bang control by introducing a thermostat. Third, we show that the equilibrium is stable in the sense that all agents' states, initially at different values, converge to the equilibrium value or remain confined within a given interval for an opportune initial distribution.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/NA477KLP/Bagagiolo et Bauso - 2014 - Mean-Field Games and Dynamic Demand Management in .pdf}
}

@incollection{bajariGameTheoryEconometrics2013,
  title = {Game {{Theory}} and {{Econometrics}}: {{A Survey}} of {{Some Recent Research}}},
  shorttitle = {Game {{Theory}} and {{Econometrics}}},
  booktitle = {Advances in {{Economics}} and {{Econometrics}}},
  author = {Bajari, Patrick and Hong, Han and Nekipelov, Denis},
  editor = {Acemoglu, Daron and Arellano, Manuel and Dekel, Eddie},
  year = {2013},
  pages = {3--52},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9781139060035.002},
  isbn = {978-1-139-06003-5},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/6GLNZY85/Bajari et al. - 2013 - Game Theory and Econometrics A Survey of Some Rec.pdf}
}

@article{balduzziOpenendedLearningSymmetric2019,
  title = {Open-Ended {{Learning}} in {{Symmetric Zero-sum Games}}},
  author = {Balduzzi, David and Garnelo, Marta and Bachrach, Yoram and Czarnecki, Wojciech M. and Perolat, Julien and Jaderberg, Max and Graepel, Thore},
  year = {2019},
  month = may,
  journal = {arXiv:1901.08106 [cs, stat]},
  eprint = {1901.08106},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Zero-sum games such as chess and poker are, abstractly, functions that evaluate pairs of agents, for example labeling them `winner' and `loser'. If the game is approximately transitive, then selfplay generates sequences of agents of increasing strength. However, nontransitive games, such as rock-paper-scissors, can exhibit strategic cycles, and there is no longer a clear objective \textendash{} we want agents to increase in strength, but against whom is unclear. In this paper, we introduce a geometric framework for formulating agent objectives in zero-sum games, in order to construct adaptive sequences of objectives that yield openended learning. The framework allows us to reason about population performance in nontransitive games, and enables the development of a new algorithm (rectified Nash response, PSROrN) that uses game-theoretic niching to construct diverse populations of effective agents, producing a stronger set of agents than existing algorithms. We apply PSROrN to two highly nontransitive resource allocation games and find that PSROrN consistently outperforms the existing alternatives.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/RJU7NBU5/Balduzzi et al. - 2019 - Open-ended Learning in Symmetric Zero-sum Games.pdf}
}

@article{balkenborgRefinedBestReply2013,
  title = {Refined Best Reply Correspondence and Dynamics: {{Best}} Reply Correspondence and Dynamics},
  shorttitle = {Refined Best Reply Correspondence and Dynamics},
  author = {Balkenborg, Dieter and Hofbauer, Josef and Kuzmics, Christoph},
  year = {2013},
  month = jan,
  journal = {Theoretical Economics},
  volume = {8},
  number = {1},
  pages = {165--192},
  issn = {19336837},
  doi = {10.3982/TE652},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/W48VJ9AC/Balkenborg et al. - 2013 - Refined best reply correspondence and dynamics Be.pdf}
}

@article{baudinBestResponseDynamicsFictitious,
  title = {Best-{{Response Dynamics}} and {{Fictitious Play}} in {{Identical Interest Stochastic Games}}},
  author = {Baudin, Lucas},
  pages = {32},
  abstract = {This paper combines ideas from Q-learning [44] and fictitious play [8, 36, 30] to define three reinforcement learning procedures which converge to the set of stationary mixed Nash equilibria in identical interest discounted stochastic games. First, we analyse three continuous-time systems that generalize the best-response dynamics defined by Leslie et al. [26] for zero-sum discounted stochastic games. Under some assumptions depending on the system, the dynamics are shown to converge to the set of stationary equilibria in identical interest discounted stochastic games. Then, we introduce three analog discrete-time procedures in the spirit of Sayin et al. [37] and demonstrate their convergence to the set of stationary equilibria using our results in continuous time together with stochastic approximation techniques [5]. Some numerical experiments complement our theoretical findings.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/C2NT58KG/Baudin et Paris-Dauphine-PSL - Best-Response Dynamics and Fictitious Play in Iden.pdf}
}

@misc{baudinBestResponseDynamicsFictitious2021,
  title = {Best-{{Response Dynamics}} and {{Fictitious Play}} in {{Identical Interest Stochastic Games}}},
  author = {Baudin, Lucas},
  year = {2021},
  month = may,
  abstract = {Soumission NeurIPS},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/S5AZ7TJN/Best-Response Dynamics and Fictitious Play in Iden.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/USVMXIRZ/BRD_stochastic_games-appendix.zip}
}

@article{baudinFictitiousPlayBestResponse2022,
  title = {Fictitious {{Play}} and {{Best-Response Dynamics}}  in {{Identical-Interest}} and {{Zero-Sum Stochastic Games}}},
  author = {Baudin, Lucas and Laraki, Rida},
  year = {2022},
  month = may,
  journal = {https://arxiv.org/pdf/2111.04317.pdf, accepted in ICML 2022},
  abstract = {This paper proposes an extension of a popular decentralized discrete-time learning procedure when repeating a static game called fictitious play (FP) (Brown, 1951; Robinson, 1951) to a dynamic model called discounted stochastic game (Shapley, 1953). Our family of discretetime FP procedures is proven to converge to the set of stationary Nash equilibria in identical interest discounted stochastic games. This extends similar convergence results for static games (Monderer \& Shapley, 1996a). We then analyze the continuous-time counterpart of our FP procedures, which include as a particular case the best-response dynamic introduced and studied by Leslie et al. (2020) in the context of zero-sum stochastic games. We prove the converge of this dynamics to stationary Nash equilibria in identical-interest and zero-sum discounted stochastic games. Thanks to stochastic approximations, we can infer from the continuous-time convergence some discrete time results such as the convergence to stationary equilibria in zero sum and team stochastic games (Holler, 2020).},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/DB4DMW3N/Baudin et Laraki - Fictitious Play and Best-Response Dynamics  in Ide.pdf}
}

@article{belmegaOnlineConvexOptimization2018a,
  title = {Online Convex Optimization and No-Regret Learning: {{Algorithms}}, Guarantees and Applications},
  shorttitle = {Online Convex Optimization and No-Regret Learning},
  author = {Belmega, E. Veronica and Mertikopoulos, Panayotis and Negrel, Romain and Sanguinetti, Luca},
  year = {2018},
  month = apr,
  journal = {arXiv:1804.04529 [cs, math, stat]},
  eprint = {1804.04529},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Spurred by the enthusiasm surrounding the "Big Data" paradigm, the mathematical and algorithmic tools of online optimization have found widespread use in problems where the trade-off between data exploration and exploitation plays a predominant role. This trade-off is of particular importance to several branches and applications of signal processing, such as data mining, statistical inference, multimedia indexing and wireless communications (to name but a few). With this in mind, the aim of this tutorial paper is to provide a gentle introduction to online optimization and learning algorithms that are asymptotically optimal in hindsight - i.e., they approach the performance of a virtual algorithm with unlimited computational power and full knowledge of the future, a property known as no-regret. Particular attention is devoted to identifying the algorithms' theoretical performance guarantees and to establish links with classic optimization paradigms (both static and stochastic). To allow a better understanding of this toolbox, we provide several examples throughout the tutorial ranging from metric learning to wireless resource allocation problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Optimization and Control,Primary 68Q32; 90C90; secondary 68T05; 91A26; 94A12,Statistics - Machine Learning},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/HALXBHH9/Belmega et al. - 2018 - Online convex optimization and no-regret learning.pdf}
}

@article{ben-porathRepeatedGamesFinite1993a,
  title = {Repeated {{Games}} with {{Finite Automata}}},
  author = {{Ben-Porath}, Elchanan},
  year = {1993},
  month = feb,
  journal = {Journal of Economic Theory},
  volume = {59},
  number = {1},
  pages = {17--32},
  issn = {00220531},
  doi = {10.1006/jeth.1993.1002},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/MD6EJKKU/Ben-Porath - 1993 - Repeated Games with Finite Automata.pdf}
}

@article{benaimConsistencyVanishinglySmooth2013,
  title = {Consistency of {{Vanishingly Smooth Fictitious Play}}},
  author = {Bena{\"i}m, Michel and Faure, Mathieu},
  year = {2013},
  month = aug,
  journal = {Mathematics of Operations Research},
  volume = {38},
  number = {3},
  pages = {437--450},
  issn = {0364-765X, 1526-5471},
  doi = {10.1287/moor.1120.0568},
  langid = {english},
  keywords = {lu,preuve pas lue},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/G3MVD2DA/Benaïm et Faure - 2013 - Consistency of Vanishingly Smooth Fictitious Play.pdf}
}

@article{benaimDynamicalSystemApproach1996,
  title = {A {{Dynamical System Approach}} to {{Stochastic Approximations}}},
  author = {Benaim, Michel},
  year = {1996},
  month = mar,
  journal = {SIAM Journal on Control and Optimization},
  volume = {34},
  number = {2},
  pages = {437--472},
  issn = {0363-0129, 1095-7138},
  doi = {10.1137/S0363012993253534},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/5S4AHW94/Benaim - 1996 - A Dynamical System Approach to Stochastic Approxim.pdf}
}

@incollection{benaimDynamicsStochasticApproximation1999,
  title = {Dynamics of Stochastic Approximation Algorithms},
  booktitle = {S\'eminaire de {{Probabilit\'es XXXIII}}},
  author = {Bena{\"i}m, Michel},
  editor = {Az{\'e}ma, Jacques and {\'E}mery, Michel and Ledoux, Michel and Yor, Marc},
  year = {1999},
  volume = {1709},
  pages = {1--68},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0096509},
  abstract = {These notes were written for a D.E.A course given at Ecole Normale Sup\'erieure de Cachan during the 1996-97 and 1997-98 academic years and at University Toulouse III during the 1997-98 academic year. Their aim is to introduce the reader to the dynamical system aspects of the theory of stochastic approximations.},
  isbn = {978-3-540-66342-3 978-3-540-48407-3},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/V72H4PYB/Benaïm - 1999 - Dynamics of stochastic approximation algorithms.pdf}
}

@article{benaimMixedEquilibriaDynamical1999,
  title = {Mixed {{Equilibria}} and {{Dynamical Systems Arising}} from {{Fictitious Play}} in {{Perturbed Games}}},
  author = {Bena{\"{\i}}m, Michel and Hirsch, Morris W},
  year = {1999},
  month = oct,
  journal = {Games and Economic Behavior},
  volume = {29},
  number = {1-2},
  pages = {36--72},
  issn = {08998256},
  doi = {10.1006/game.1999.0717},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/JBV8T37G/Benaı̈m et Hirsch - 1999 - Mixed Equilibria and Dynamical Systems Arising fro.pdf}
}

@article{benaimMixedEquilibriaDynamical1999a,
  title = {Mixed {{Equilibria}} and {{Dynamical Systems Arising}} from {{Fictitious Play}} in {{Perturbed Games}}},
  author = {Bena{\"{\i}}m, Michel and Hirsch, Morris W},
  year = {1999},
  month = oct,
  journal = {Games and Economic Behavior},
  volume = {29},
  number = {1-2},
  pages = {36--72},
  issn = {08998256},
  doi = {10.1006/game.1999.0717},
  abstract = {Fudenberg and Kreps consider adaptive learning processes, in the spirit of fictitious play, for infinitely repeated games of incomplete information having randomly perturbed payoffs. They proved the convergence of the adaptive process for 2 X 2 games with a unique completely mixed Nash equilibrium. Kaniovski and Young proved the convergence of the process for generic 2 X 2 games subjected to small perturbations. We extend their result to games with several equilibria--possibly infinitely many, and not necessarily completely mixed. For a broad class of such games we prove convergence of the adaptive process; stable and unstable equilibria are characterized.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/QFUTTMML/Benaı̈m et Hirsch - 1999 - Mixed Equilibria and Dynamical Systems Arising fro.pdf}
}

@article{benaimStochasticApproximationsDifferential2005,
  title = {Stochastic {{Approximations}} and {{Differential Inclusions}}},
  author = {Bena{\"i}m, Michel and Hofbauer, Josef and Sorin, Sylvain},
  year = {2005},
  month = jan,
  journal = {SIAM Journal on Control and Optimization},
  volume = {44},
  number = {1},
  pages = {328--348},
  issn = {0363-0129, 1095-7138},
  doi = {10.1137/S0363012904439301},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/C3NRLYF4/Benaïm et al. - 2005 - Stochastic Approximations and Differential Inclusi.pdf}
}

@article{bendorReinforcementLearningRepeated2001,
  title = {Reinforcement {{Learning}} in {{Repeated Interaction Games}}},
  author = {Bendor, Jonathan and Mookherjee, Dilip and Ray, Debraj},
  year = {2001},
  month = jan,
  journal = {Advances in Theoretical Economics},
  volume = {1},
  number = {1},
  issn = {1534-5963},
  doi = {10.2202/1534-5963.1008},
  abstract = {We study long run implications of reinforcement learning when two players repeatedly interact with one another over multiple rounds to play a finite action game. Within each round, the players play the game many successive times with a fixed set of aspirations used to evaluate payoff experiences as successes or failures. The probability weight on successful actions is increased, while failures result in players trying alternative actions in subsequent rounds. The learning rule is supplemented by small amounts of inertia and random perturbations to the states of players. Aspirations are adjusted across successive rounds on the basis of the discrepancy between the average payoff and aspirations in the most recently concluded round. We define and characterize pure steady states of this model, and establish convergence to these under appropriate conditions. Pure steady states are shown to be individually rational, and are either Pareto-efficient or a protected Nash equilibrium of the stage game. Conversely, any Pareto-efficient and strictly individually rational action pair, or any strict protected Nash equilibrium, constitutes a pure steady state, to which the process converges from non-negligible sets of initial aspirations. Applications to games of coordination, cooperation, oligopoly, and electoral competition are discussed.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/Z4T69U4Y/Bendor et al. - 2001 - Reinforcement Learning in Repeated Interaction Gam.pdf}
}

@book{benvenisteAdaptiveAlgorithmsStochastic1990,
  title = {Adaptive {{Algorithms}} and {{Stochastic Approximations}}},
  author = {Benveniste, Albert and M{\'e}tivier, Michel and Priouret, Pierre},
  year = {1990},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-75894-2},
  isbn = {978-3-642-75896-6 978-3-642-75894-2},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/F392IP9B/Benveniste et al. - 1990 - Adaptive Algorithms and Stochastic Approximations.pdf}
}

@article{bergerBrownOriginalFictitious2007,
  title = {Brown's Original Fictitious Play},
  author = {Berger, Ulrich},
  year = {2007},
  month = jul,
  journal = {Journal of Economic Theory},
  volume = {135},
  number = {1},
  pages = {572--578},
  issn = {00220531},
  doi = {10.1016/j.jet.2005.12.010},
  abstract = {What modern game theorists describe as ``fictitious play'' is not the learning process George W. Brown defined in his 1951 paper. Brown's original version differs in a subtle detail, namely the order of belief updating. In this note we revive Brown's original fictitious play process and demonstrate that this seemingly innocent detail allows for an extremely simple and intuitive proof of convergence in an interesting and large class of games: nondegenerate ordinal potential games.},
  langid = {english},
  keywords = {lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/6FJA7XMK/Berger - 2007 - Brown's original fictitious play.pdf}
}

@article{bergerFictitiousPlayGames2005,
  title = {Fictitious Play in 2\texttimes n Games},
  author = {Berger, Ulrich},
  year = {2005},
  month = feb,
  journal = {Journal of Economic Theory},
  volume = {120},
  number = {2},
  pages = {139--154},
  issn = {00220531},
  doi = {10.1016/j.jet.2004.02.003},
  abstract = {It is known that every discrete-time fictitious play process approaches equilibrium in nondegenerate 2 \^A 2 games, and that every continuous-time fictitious play process approaches equilibrium in nondegenerate 2 \^A 2 and 2 \^A 3 games. It has also been conjectured that convergence to the set of equilibria holds generally for nondegenerate 2 \^A n games. We give a simple geometric proof of this for the continuous-time process, and also extend the result to discrete-time fictitious play.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/PFU9IQ8A/Berger - 2005 - Fictitious play in 2×n games.pdf}
}

@article{bergerNonalgebraicConvergenceProofs2012,
  title = {Non-Algebraic {{Convergence Proofs}} for {{Continuous-Time Fictitious Play}}},
  author = {Berger, Ulrich},
  year = {2012},
  month = mar,
  journal = {Dynamic Games and Applications},
  volume = {2},
  number = {1},
  pages = {4--17},
  issn = {2153-0785, 2153-0793},
  doi = {10.1007/s13235-011-0033-4},
  abstract = {In this technical note we use insights from the theory of projective geometry to provide novel and non-algebraic proofs of convergence of continuous-time fictitious play for a class of games. As a corollary we obtain a kind of equilibrium selection result, whereby continuous-time fictitious play converges to a particular equilibrium contained in a continuum of equivalent equilibria for symmetric 4 \texttimes{} 4 zero-sum games.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/EHD6ZFDQ/Berger - 2012 - Non-algebraic Convergence Proofs for Continuous-Ti.pdf}
}

@article{bergerTwoMoreClasses2007,
  title = {Two More Classes of Games with the Continuous-Time Fictitious Play Property},
  author = {Berger, Ulrich},
  year = {2007},
  month = aug,
  journal = {Games and Economic Behavior},
  volume = {60},
  number = {2},
  pages = {247--261},
  issn = {0899-8256},
  doi = {10.1016/j.geb.2006.10.008},
  abstract = {Fictitious Play is the oldest and most studied learning process for games. Since the already classical result for zero-sum games, convergence of beliefs to the set of Nash equilibria has been established for several classes of games, including weighted potential games, supermodular games with diminishing returns, and 3\texttimes 3 supermodular games. Extending these results, we establish convergence of Continuous-time Fictitious Play for ordinal potential games and quasi-supermodular games with diminishing returns. As a by-product we obtain convergence for 3\texttimes m and 4\texttimes 4 quasi-supermodular games.},
  langid = {english},
  keywords = {Fictitious play,Learning process,Ordinal potential games,Quasi-supermodular games},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/53IXJNJD/Berger - 2007 - Two more classes of games with the continuous-time.pdf}
}

@article{bertrandDynamicNetworkCongestion2020,
  title = {Dynamic Network Congestion Games},
  author = {Bertrand, Nathalie and Markey, Nicolas and Sadhukhan, Suman and Sankur, Ocan},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.13632 [cs]},
  eprint = {2009.13632},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Congestion games are a classical type of games studied in game theory, in which n players choose a resource, and their individual cost increases with the number of other players choosing the same resource. In network congestion games (NCGs), the resources correspond to simple paths in a graph, e.g. representing routing options from a source to a target. In this paper, we introduce a variant of NCGs, referred to as dynamic NCGs: in this setting, players take transitions synchronously, they select their next transitions dynamically, and they are charged a cost that depends on the number of players simultaneously using the same transition.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Science and Game Theory},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/QSQUN77I/Bertrand et al. - 2020 - Dynamic network congestion games.pdf}
}

@article{bervoetsLearningMinimalInformation2018,
  title = {Learning with Minimal Information in Continuous Games},
  author = {Bervoets, Sebastian and Bravo, Mario and Faure, Mathieu},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.11506 [cs]},
  eprint = {1806.11506},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a stochastic learning process called the dampened gradient approximation process. While learning models have almost exclusively focused on finite games, in this paper we design a learning process for games with continuous action sets. It is payoff-based and thus requires from players no sophistication and no knowledge of the game. We show that despite such limited information, players will converge to Nash in large classes of games. In particular, convergence to a Nash equilibrium which is stable is guaranteed in all games with strategic complements as well as in concave games; convergence to Nash often occurs in all locally ordinal potential games; convergence to a stable Nash occurs with positive probability in all games with isolated equilibria.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Science and Game Theory},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/PFMRYN3Q/Bervoets et al. - 2018 - Learning with minimal information in continuous ga.pdf}
}

@article{bervoetsLearningMinimalInformation2020,
  title = {Learning with Minimal Information in Continuous Games},
  author = {Bervoets, Sebastian and Bravo, Mario and Faure, Mathieu},
  year = {2020},
  journal = {Theoretical Economics},
  volume = {15},
  number = {4},
  pages = {1471--1508},
  issn = {1555-7561},
  doi = {10.3982/TE3435},
  abstract = {While payoff-based learning models are almost exclusively devised for finite action games, where players can test every action, it is harder to design such learning processes for continuous games. We construct a stochastic learning rule, designed for games with continuous action sets, which requires no sophistication from the players and is simple to implement: players update their actions according to variations in own payoff between current and previous action. We then analyze its behavior in several classes of continuous games and show that convergence to a stable Nash equilibrium is guaranteed in all games with strategic complements as well as in concave games, while convergence to Nash equilibrium occurs in all locally ordinal potential games as soon as Nash equilibria are isolated.},
  copyright = {Copyright \textcopyright{} 2020 The Authors.},
  langid = {english},
  keywords = {C6,C72,continuous games,D83,Payoff-based learning,stochastic approximation},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.3982/TE3435},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ZAKFZH9F/Bervoets et al. - 2020 - Learning with minimal information in continuous ga.pdf}
}

@article{blackwellAnalogMinimaxTheorem1956,
  title = {An Analog of the Minimax Theorem for Vector Payoffs},
  author = {Blackwell, David},
  year = {1956},
  month = mar,
  journal = {Pacific Journal of Mathematics},
  volume = {6},
  number = {1},
  pages = {1--8},
  issn = {0030-8730, 0030-8730},
  doi = {10.2140/pjm.1956.6.1},
  langid = {english},
  keywords = {lu,procedure},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/HVWG4IBU/Blackwell - 1956 - An analog of the minimax theorem for vector payoff.pdf}
}

@inproceedings{blockiAdaptiveRegretMinimization2013,
  title = {Adaptive {{Regret Minimization}} in {{Bounded-Memory Games}}},
  booktitle = {Decision and {{Game Theory}} for {{Security}}},
  author = {Blocki, Jeremiah and Christin, Nicolas and Datta, Anupam and Sinha, Arunesh},
  editor = {Das, Sajal K. and {Nita-Rotaru}, Cristina and Kantarcioglu, Murat},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {65--84},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-02786-9_5},
  abstract = {Organizations that collect and use large volumes of personal information often use security audits to protect data subjects from inappropriate uses of this information by authorized insiders. In face of unknown incentives of employees, a reasonable audit strategy for the organization is one that minimizes its regret. While regret minimization has been extensively studied in repeated games, the standard notion of regret for repeated games cannot capture the complexity of the interaction between the organization (defender) and an adversary, which arises from dependence of rewards and actions on history. To account for this generality, we introduce a richer class of games called bounded-memory games, which can provide a more accurate model of the audit process. We introduce the notion of k-adaptive regret, which compares the reward obtained by playing actions prescribed by the algorithm against a hypothetical k-adaptive adversary with the reward obtained by the best expert in hindsight against the same adversary. Roughly, a hypothetical k-adaptive adversary adapts her strategy to the defender's actions exactly as the real adversary would within each window of k rounds. A k-adaptive adversary is a natural model for temporary adversaries (e.g., company employees) who stay for a certain number of audit cycles and are then replaced by a different person. Our definition is parameterized by a set of experts, which can include both fixed and adaptive defender strategies. We investigate the inherent complexity of and design algorithms for adaptive regret minimization in bounded-memory games of perfect and imperfect information. We prove a hardness result showing that, with imperfect information, any k-adaptive regret minimizing algorithm (with fixed strategies as experts) must be inefficient unless NP = RP even when playing against an oblivious adversary. In contrast, for bounded-memory games of perfect and imperfect information we present approximate 0-adaptive regret minimization algorithms against an oblivious adversary running in time {$\mathsl{n}\mathsl{O}$}(1)nO(1)n\^\{O\textbackslash left(1\textbackslash right)\}.},
  isbn = {978-3-319-02786-9},
  langid = {english},
  keywords = {Imperfect Information,Perfect Information,Repeated Game,Stackelberg Equilibrium,Stochastic Game},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/J6KHYE6P/Blocki et al. - 2013 - Adaptive Regret Minimization in Bounded-Memory Gam.pdf}
}

@incollection{blumExternalInternalRegret2005,
  title = {From {{External}} to {{Internal Regret}}},
  booktitle = {Learning {{Theory}}},
  author = {Blum, Avrim and Mansour, Yishay},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Auer, Peter and Meir, Ron},
  year = {2005},
  volume = {3559},
  pages = {621--636},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11503415_42},
  abstract = {External regret compares the performance of an online algorithm, selecting among N actions, to the performance of the best of those actions in hindsight. Internal regret compares the loss of an online algorithm to the loss of a modified online algorithm, which consistently replaces one action by another.},
  isbn = {978-3-540-26556-6 978-3-540-31892-7},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/CSNJN9NJ/Blum et Mansour - 2005 - From External to Internal Regret.pdf}
}

@incollection{BM07a,
  title = {Learning, Regret Minimization, and Equilibria},
  booktitle = {Algorithmic Game Theory},
  author = {Blum, Avrim and Mansour, Yishay},
  editor = {Nisan, Noam and Roughgarden, Tim and Tardos, {\'E}va and Vazirani, V. V.},
  year = {2007},
  publisher = {{Cambridge University Press}},
  chapter = {4},
  date-added = {2016-07-27 14:42:13 +0000},
  date-modified = {2017-02-13 09:12:50 +0000}
}

@book{bonneuilSciencesTechniquesSociete2013,
  title = {Sciences, Techniques et Soci\'et\'e},
  author = {Bonneuil, Christophe and Joly, Pierre-Beno{\^i}t},
  year = {2013},
  series = {Rep\`eres ; {{Sociologie}}},
  number = {620},
  publisher = {{La D\'ecouverte}},
  address = {{Paris}},
  isbn = {978-2-7071-5097-4},
  lccn = {Q175.5 .B65 2013},
  keywords = {Political aspects,Science,Social aspects,Technological innovations,Technology},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/PS9KZMXM/Bonneuil et Joly - 2013 - Sciences, techniques et société.pdf}
}

@article{borkarMETHODCONVERGENCESTOCHASTIC,
  title = {{{THE O}}.{{D}}.{{E}}. {{METHOD FOR CONVERGENCE OF STOCHASTIC APPROXIMATION AND REINFORCEMENT LEARNING}}},
  author = {Borkar, V S and Meyn, S P},
  pages = {23},
  abstract = {It is shown here that stability of the stochastic approximation algorithm is implied by the asymptotic stability of the origin for an associated ODE. This in turn implies convergence of the algorithm. Several specific classes of algorithms are considered as applications. It is found that the results provide (i) a simpler derivation of known results for reinforcement learning algorithms; (ii) a proof for the first time that a class of asynchronous stochastic approximation algorithms are convergent without using any a priori assumption of stability; (iii) a proof for the first time that asynchronous adaptive critic and Q-learning algorithms are convergent for the average cost optimal control problem.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/EKD3XQ85/Borkar et Meyn - THE O.D.E. METHOD FOR CONVERGENCE OF STOCHASTIC AP.pdf}
}

@article{borkarStochasticApproximationTwo1997,
  title = {Stochastic Approximation with Two Time Scales},
  author = {Borkar, Vivek S.},
  year = {1997},
  month = feb,
  journal = {Systems \& Control Letters},
  volume = {29},
  number = {5},
  pages = {291--294},
  issn = {01676911},
  doi = {10.1016/S0167-6911(97)90015-3},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/67M9FJVA/Borkar - Stochastic approximation with two time scales.pdf}
}

@article{bourgeoisEnseignementNouvelleMatiere2014,
  title = {{‪Enseignement : une nouvelle mati\`ere, l'\'economie‪}},
  shorttitle = {{‪Enseignement}},
  author = {Bourgeois, Isabelle},
  year = {2014},
  month = aug,
  journal = {Regards sur l'economie allemande},
  volume = {n\textdegree{} 113},
  number = {2},
  pages = {46--46},
  publisher = {{CIRAC}},
  issn = {1156-8992},
  langid = {french}
}

@incollection{bournezLearningEquilibriaGames2013,
  title = {Learning {{Equilibria}} in {{Games}} by {{Stochastic Distributed Algorithms}}},
  booktitle = {Computer and {{Information Sciences III}}},
  author = {Bournez, Olivier and Cohen, Johanne},
  editor = {Gelenbe, Erol and Lent, Ricardo},
  year = {2013},
  pages = {31--38},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-4471-4594-3_4},
  abstract = {We consider a family of stochastic distributed dynamics to learn equilibria in games, that we prove to correspond to an Ordinary Differential Equation (ODE). We focus then on a class of stochastic dynamics where this ODE turns out to be related to multipopulation replicator dynamics. Using facts known about convergence of this ODE, we discuss the convergence of the initial stochastic dynamics. For general games, there might be non-convergence, but when the convergence of the ODE holds, considered stochastic algorithms converge towards Nash equilibria. For games admitting a multiaffine Lyapunov function, we prove that this Lyapunov function is a super-martingale over the stochastic dynamics and that the stochastic dynamics converge. This leads a way to provide bounds on their time of convergence by martingale arguments. This applies in particular for many classes of games considered in literature, including several load balancing games and congestion games.},
  isbn = {978-1-4471-4593-6 978-1-4471-4594-3},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/D6QFS32G/Bournez et Cohen - 2013 - Learning Equilibria in Games by Stochastic Distrib.pdf}
}

@inproceedings{boutilierPlanningLearningCoordination1996,
  title = {Planning, Learning and Coordination in Multiagent Decision Processes},
  booktitle = {In {{Proceedings}} of the {{Sixth Conference}} on {{Theoretical Aspects}} of {{Rationality}} and {{Knowledge}} ({{TARK96}}},
  author = {Boutilier, Craig},
  year = {1996},
  pages = {195--210},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/NBH2BL8W/Boutilier - Planning, Learning and Coordination in Multiagent .pdf}
}

@inproceedings{bowlingRationalConvergentLearning2001,
  title = {Rational and Convergent Learning in Stochastic Games},
  booktitle = {Proceedings of the 17th International Joint Conference on Artificial Intelligence - Volume 2},
  author = {Bowling, Michael and Veloso, Manuela},
  year = {2001},
  series = {{{IJCAI}}'01},
  pages = {1021--1026},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  abstract = {This paper investigates the problem of policy learning in multiagent environments using the stochastic game framework, which we briefly overview. We introduce two properties as desirable for a learning agent when in the presence of other learning agents, namely rationality and convergence. We examine existing reinforcement learning algorithms according to these two properties and notice that they fail to simultaneously meet both criteria. We then contribute a new learning algorithm,WoLF policy hillclimbing, that is based on a simple principle: "learn quickly while losing, slowly while winning." The algorithm is proven to be rational and we present empirical results for a number of stochastic games showing the algorithm converges.},
  isbn = {1-55860-812-5},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/5G76E8GM/Bowling et Veloso - Rational and Convergent Learning in Stochastic Gam.pdf}
}

@incollection{brandtRateConvergenceFictitious2010,
  title = {On the {{Rate}} of {{Convergence}} of {{Fictitious Play}}},
  booktitle = {Algorithmic {{Game Theory}}},
  author = {Brandt, Felix and Fischer, Felix and Harrenstein, Paul},
  editor = {Kontogiannis, Spyros and Koutsoupias, Elias and Spirakis, Paul G.},
  year = {2010},
  volume = {6386},
  pages = {102--113},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-16170-4_10},
  isbn = {978-3-642-16169-8 978-3-642-16170-4},
  keywords = {lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/SFB7Y2CJ/Brandt et al. - On the Rate of Convergence of Fictitious Play.pdf}
}

@article{bravoBanditLearningConcave,
  title = {Bandit Learning in Concave {{N-person}} Games},
  author = {Bravo, Mario and Leslie, David S and Mertikopoulos, Panayotis},
  pages = {25},
  abstract = {This paper examines the long-run behavior of learning with bandit feedback in non-cooperative concave games. The bandit framework accounts for extremely low-information environments where the agents may not even know they are playing a game; as such, the agents' most sensible choice in this setting would be to employ a no-regret learning algorithm. In general, this does not mean that the players' behavior stabilizes in the long run: no-regret learning may lead to cycles, even with perfect gradient information. However, if a standard monotonicity condition is satisfied, our analysis shows that noregret learning based on mirror descent with bandit feedback converges to Nash equilibrium with probability 1. We also derive an upper bound for the convergence rate of the process that nearly matches the best attainable rate for single-agent bandit stochastic optimization.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/KZSNYAS3/Bravo et al. - Bandit learning in concave N-person games.pdf}
}

@article{bresnahanEmpiricalModelsDiscrete1991,
  title = {Empirical Models of Discrete Games},
  author = {Bresnahan, Timothy F. and Reiss, Peter C.},
  year = {1991},
  month = apr,
  journal = {Journal of Econometrics},
  volume = {48},
  number = {1-2},
  pages = {57--81},
  issn = {03044076},
  doi = {10.1016/0304-4076(91)90032-9},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/BPIEVUVH/Bresnahan et Reiss - 1991 - Empirical models of discrete games.pdf}
}

@article{brown1951iterative,
  title = {Iterative Solution of Games by Fictitious Play},
  author = {Brown, George W},
  year = {1951},
  journal = {Activity analysis of production and allocation},
  volume = {13},
  number = {1},
  pages = {374--376},
  publisher = {{New York}},
  keywords = {procedure}
}

@article{brownCompetitionPricingAlgorithms,
  title = {Competition in {{Pricing Algorithms}}},
  author = {Brown, Zach Y and MacKay, Alexander},
  pages = {65},
  abstract = {We document new facts about pricing technology using high-frequency data, and we examine the implications for competition. Some online retailers employ technology that allows for more frequent price changes and automated responses to price changes by rivals. Motivated by these facts, we consider a model in which firms can differ in pricing frequency and choose pricing algorithms that are a function of rivals' prices. In competitive (Markov perfect) equilibrium, the introduction of simple pricing algorithms can generate price dispersion, increase price levels, and exacerbate the price effects of mergers.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/IRLCXET9/Brown et MacKay - Competition in Pricing Algorithms.pdf}
}

@article{brownDeepCounterfactualRegret,
  title = {Deep {{Counterfactual Regret Minimization}}},
  author = {Brown, Noam and Lerer, Adam and Gross, Sam and Sandholm, Tuomas},
  pages = {10},
  abstract = {Counterfactual Regret Minimization (CFR) is the leading framework for solving large imperfectinformation games. It converges to an equilibrium by iteratively traversing the game tree. In order to deal with extremely large games, abstraction is typically applied before running CFR. The abstracted game is solved with tabular CFR, and its solution is mapped back to the full game. This process can be problematic because aspects of abstraction are often manual and domain specific, abstraction algorithms may miss important strategic nuances of the game, and there is a chickenand-egg problem because determining a good abstraction requires knowledge of the equilibrium of the game. This paper introduces Deep Counterfactual Regret Minimization, a form of CFR that obviates the need for abstraction by instead using deep neural networks to approximate the behavior of CFR in the full game. We show that Deep CFR is principled and achieves strong performance in large poker games. This is the first non-tabular variant of CFR to be successful in large games.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/WZGDE5L9/Brown et al. - Deep Counterfactual Regret Minimization.pdf}
}

@article{bubeckBoundedRegretStochastic,
  title = {Bounded Regret in Stochastic Multi-Armed Bandits},
  author = {Bubeck, Sebastien and Perchet, Vianney and Rigollet, Philippe},
  pages = {13},
  abstract = {We study the stochastic multi-armed bandit problem when one knows the value \textmu ({$\star$}) of an optimal arm, as a well as a positive lower bound on the smallest positive gap {$\increment$}. We propose a new randomized policy that attains a regret uniformly bounded over time in this setting. We also prove several lower bounds, which show in particular that bounded regret is not possible if one only knows {$\increment$}, and bounded regret of order 1/{$\increment$} is not possible if one only knows \textmu ({$\star$}).},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/SFPNVGHG/Bubeck et al. - Bounded regret in stochastic multi-armed bandits.pdf}
}

@article{bubeckRegretAnalysisStochastic2012a,
  title = {Regret {{Analysis}} of {{Stochastic}} and {{Nonstochastic Multi-armed Bandit Problems}}},
  author = {Bubeck, S{\'e}bastien},
  year = {2012},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {5},
  number = {1},
  pages = {1--122},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000024},
  abstract = {Multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration\textendash exploitation trade-off. This is the balance between staying with the option that gave highest payoffs in the past and exploring new options that might give higher payoffs in the future. Although the study of bandit problems dates back to the 1930s, exploration\textendash exploitation trade-offs arise in several modern applications, such as ad placement, website optimization, and packet routing. Mathematically, a multi-armed bandit is defined by the payoff process associated with each option. In this monograph, we focus on two extreme cases in which the analysis of regret is particularly simple and elegant: i.i.d. payoffs and adversarial payoffs. Besides the basic setting of finitely many actions, we also analyze some of the most important variants and extensions, such as the contextual bandit model.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/H247VQZ4/Bubeck - 2012 - Regret Analysis of Stochastic and Nonstochastic Mu.pdf}
}

@article{busoniuComprehensiveSurveyMultiagent2008,
  title = {A {{Comprehensive Survey}} of {{Multiagent Reinforcement Learning}}},
  author = {Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
  year = {2008},
  month = mar,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume = {38},
  number = {2},
  pages = {156--172},
  issn = {1094-6977, 1558-2442},
  doi = {10.1109/TSMCC.2007.913919},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/YS6QQ25V/Busoniu et al. - 2008 - A Comprehensive Survey of Multiagent Reinforcement.pdf}
}

@article{cahnGeneralProceduresLeading2004,
  title = {General Procedures Leading to Correlated Equilibria},
  author = {Cahn, Amotz},
  year = {2004},
  month = dec,
  journal = {International Journal of Games Theory},
  volume = {33},
  number = {1},
  pages = {21--40},
  issn = {0020-7276, 1432-1270},
  doi = {10.1007/s001820400182},
  abstract = {Hart and Mas-Colell [2000] show that if all players play ``regretmatching'' strategies, i.e., they play with probabilities proportional to the regrets, then the empirical distribution of play converges to the set of correlated equilibria, and the regrets of every player converge to zero. Here we show that if only one player, say player i; plays with these probabilities, while the other players are ``not too sophisticated,'' then the result that player i's regrets converge to zero continues to hold. The condition of ``not too sophisticated'' essentially says that the effect of one change of action of player i on the future actions of the other players decreases to zero as the horizon goes to infinity. Furthermore, we generalize all these results to a whole class of ``regret-based'' strategies introduced in Hart and Mas-Colell [2001]. In particular, these simplify the ``conditional smooth fictitious play'' of Fudenberg and Levine [1999].},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/P5CLXCRC/Cahn - 2004 - General procedures leading to correlated equilibri.pdf}
}

@article{cahucQueRestetilTheorie1994,
  title = {{Que reste-t-il de la th\'eorie du salaire d'efficience ?}},
  author = {Cahuc, Pierre and Zylberberg, Andr{\'e}},
  year = {1994},
  journal = {Revue \'economique},
  volume = {45},
  number = {3},
  pages = {385--398},
  doi = {10.3406/reco.1994.409532},
  abstract = {Que reste-t-il de la th\'eorie du salaire d'efficience ? Cet article montre que les co\^uts de rotation de la main-d'\oe uvre ou de contr\^ole de l'effort des travailleurs ne peuvent justifier, \`a eux seuls, l'existence de ch\^omage involontaire. En revanche, de tels co\^uts peuvent emp\^echer un travailleur et un employeur de passer des accords constituant des am\'eliorations par\'etiennes.},
  copyright = {free},
  langid = {french},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/B7IC5G8K/reco_0035-2764_1994_num_45_3_409532.html}
}

@inproceedings{caiMinmaxTheoremsMultiplayer2011,
  title = {On {{Minmax Theorems}} for {{Multiplayer Games}}},
  booktitle = {Proceedings of the {{Twenty-Second Annual ACM-SIAM Symposium}} on {{Discrete Algorithms}}},
  author = {Cai, Yang and Daskalakis, Constantinos},
  year = {2011},
  month = jan,
  pages = {217--234},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611973082.20},
  abstract = {We prove a generalization of von Neumann's minmax theorem to the class of separable multiplayer zerosum games, introduced in [Bregman and Fokin 1998]. These games are polymatrix\textemdash that is, graphical games in which every edge is a two-player game between its endpoints\textemdash in which every outcome has zero total sum of players' payoffs. Our generalization of the minmax theorem implies convexity of equilibria, polynomialtime tractability, and convergence of no-regret learning algorithms to Nash equilibria. Given that Nash equilibria in 3-player zero-sum games are already PPADcomplete, this class of games, i.e. with pairwise separable utility functions, defines essentially the broadest class of multi-player constant-sum games to which we can hope to push tractability results. Our result is obtained by establishing a certain game-class collapse, showing that separable constant-sum games are payoff equivalent to pairwise constant-sum polymatrix games\textemdash polymatrix games in which all edges are constant-sum games, and invoking a recent result of [Daskalakis, Papadimitriou 2009] for these games.},
  isbn = {978-0-89871-993-2 978-1-61197-308-2},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/9WXIAV5S/Cai et Daskalakis - 2011 - On Minmax Theorems for Multiplayer Games.pdf}
}

@article{calvanoAlgorithmicCollusionImperfect2021,
  title = {Algorithmic Collusion with Imperfect Monitoring},
  author = {Calvano, Emilio and Calzolari, Giacomo and Denicol{\'o}, Vincenzo and Pastorello, Sergio},
  year = {2021},
  month = feb,
  journal = {International Journal of Industrial Organization},
  pages = {102712},
  issn = {01677187},
  doi = {10.1016/j.ijindorg.2021.102712},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/VSXQC6DS/Calvano et al. - 2021 - Algorithmic collusion with imperfect monitoring.pdf}
}

@article{calvanoArtificialIntelligenceAlgorithmic2020,
  title = {Artificial {{Intelligence}}, {{Algorithmic Pricing}}, and {{Collusion}}},
  author = {Calvano, Emilio and Calzolari, Giacomo and Denicol{\`o}, Vincenzo and Pastorello, Sergio},
  year = {2020},
  month = oct,
  journal = {American Economic Review},
  volume = {110},
  number = {10},
  pages = {3267--3297},
  issn = {0002-8282},
  doi = {10.1257/aer.20190623},
  abstract = {Increasingly, algorithms are supplanting human decision-makers in pricing goods and services. To analyze the possible consequences, we study experimentally the behavior of algorithms powered by Artificial Intelligence (Q-learning) in a workhorse oligopoly model of repeated price competition. We find that the algorithms consistently learn to charge supracompetitive prices, without communicating with one another. The high prices are sustained by collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand, changes in the number of players, and various forms of uncertainty. (JEL D21, D43, D83, L12, L13)},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/PEH8FWSJ/Calvano et al. - 2020 - Artificial Intelligence, Algorithmic Pricing, and .pdf}
}

@article{calveteEfficientEvolutionaryAlgorithm2013,
  title = {An Efficient Evolutionary Algorithm for the Ring Star Problem},
  author = {Calvete, Herminia I},
  year = {2013},
  journal = {European Journal of Operational Research},
  pages = {12},
  abstract = {This paper addresses the ring star problem (RSP). The goal is to locate a cycle through a subset of nodes of a network aiming to minimize the sum of the cost of installing facilities on the nodes on the cycle, the cost of connecting them and the cost of assigning the nodes not on the cycle to their closest node on the cycle. A fast and efficient evolutionary algorithm is developed which is based on a new formulation of the RSP as a bilevel programming problem with one leader and two independent followers. The leader decides which nodes to include in the ring, one follower decides about the connections of the cycle and the other follower decides about the assignment of the nodes not on the cycle. The bilevel approach leads to a new form of chromosome encoding in which genes are associated to values of the upper level variables. The quality of each chromosome is evaluated by its fitness, by means of the objective function of the RSP. Hence, in order to compute the value of the lower level variables, two optimization problems are solved for each chromosome. The computational results show the efficiency of the algorithm in terms of the quality of the solutions yielded and the computing time. A study to select the best configuration of the algorithm is presented. The algorithm is tested on a set of benchmark problems providing very accurate solutions within short computing times. Moreover, for one of the problems a new best solution is found. \'O 2013 Elsevier B.V. All rights reserved.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/IF8356ET/Calvete - 2013 - An efficient evolutionary algorithm for the ring s.pdf}
}

@article{camaraMechanismsNoRegretAgent2020,
  title = {Mechanisms for a {{No-Regret Agent}}: {{Beyond}} the {{Common Prior}}},
  shorttitle = {Mechanisms for a {{No-Regret Agent}}},
  author = {Camara, Modibo and Hartline, Jason and Johnsen, Aleck},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.05518 [cs, econ]},
  eprint = {2009.05518},
  eprinttype = {arxiv},
  primaryclass = {cs, econ},
  abstract = {A rich class of mechanism design problems can be understood as incomplete-information games between a principal who commits to a policy and an agent who responds, with payoffs determined by an unknown state of the world. Traditionally, these models require strong and often-impractical assumptions about beliefs (a common prior over the state). In this paper, we dispense with the common prior. Instead, we consider a repeated interaction where both the principal and the agent may learn over time from the state history. We reformulate mechanism design as a reinforcement learning problem and develop mechanisms that attain natural benchmarks without any assumptions on the state-generating process. Our results make use of novel behavioral assumptions for the agent \textendash{} centered around counterfactual internal regret \textendash{} that capture the spirit of rationality without relying on beliefs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Science and Game Theory,Economics - Theoretical Economics},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/6HKJ6JUM/Camara et al. - 2020 - Mechanisms for a No-Regret Agent Beyond the Commo.pdf}
}

@article{canningAverageBehaviorLearning1992,
  title = {Average Behavior in Learning Models},
  author = {Canning, David},
  year = {1992},
  month = aug,
  journal = {Journal of Economic Theory},
  volume = {57},
  number = {2},
  pages = {442--472},
  issn = {0022-0531},
  doi = {10.1016/0022-0531(92)90045-J},
  abstract = {We examine a general class of adaptive behavior models in which the distant past has only a weak effect on current actions, and assume that agents sometimes make mistakes, to show that average behavior (averaged over time) converges, with probability one, to a unique limit. Mistakes generate global convergence and are an equilibrium selection device; for small mistake probabilities the equilibrium selected is close to an equilibrium of the model without mistakes. The overlapping generations model, and learning in games with bounded memory, fit into this framework and are examined as examples of the result.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/A726NRKG/Canning - 1992 - Average behavior in learning models.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/8PRSZZ3E/002205319290045J.html}
}

@incollection{celliLearningCorrelateMultiPlayer2019,
  title = {Learning to {{Correlate}} in {{Multi-Player General-Sum Sequential Games}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Celli, Andrea and Marchesi, Alberto and Bianchi, Tommaso and Gatti, Nicola},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {13076--13086},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/NXGG5S3I/Celli et al. - Learning to Correlate in Multi-Player General-Sum .pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/ZH8UHBZS/Celli et al. - 2019 - Learning to Correlate in Multi-Player General-Sum .pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/PS9BGNIY/9465-learning-to-correlate-in-multi-player-general-sum-sequential-games.html}
}

@article{celliNoregretLearningDynamics2020,
  title = {No-Regret Learning Dynamics for Extensive-Form Correlated and Coarse Correlated Equilibria},
  author = {Celli, Andrea and Marchesi, Alberto and Farina, Gabriele and Gatti, Nicola},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.00603 [cs]},
  eprint = {2004.00603},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recently, there has been growing interest around less-restrictive solution concepts than Nash equilibrium in extensive-form games, with significant effort towards the computation of extensive-form correlated equilibrium (EFCE) and extensive-form coarse correlated equilibrium (EFCCE). In this paper, we show how to leverage the popular counterfactual regret minimization (CFR) paradigm to induce simple no-regret dynamics that converge to the set of EFCEs and EFCCEs in an n-player general-sum extensive-form games. For EFCE, we define a notion of internal regret suitable for extensive-form games and exhibit an efficient no-internal-regret algorithm. These results complement those for normal-form games introduced in the seminal paper by Hart and Mas-Colell. For EFCCE, we show that no modification of CFR is needed, and that in fact the empirical frequency of play generated when all the players use the original CFR algorithm converges to the set of EFCCEs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Computer Science - Multiagent Systems,lu,preuve pas lue},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/GY52WWIP/Celli et al. - 2020 - No-regret learning dynamics for extensive-form cor.pdf}
}

@inproceedings{cesa-bianchiPotentialBasedAlgorithmsOnline2001,
  title = {Potential-{{Based Algorithms}} in {{Online Prediction}} and {{Game Theory}}},
  booktitle = {Computational {{Learning Theory}}},
  author = {{Cesa-Bianchi}, Nicol{\`o} and Lugosi, G{\'a}bor},
  editor = {Helmbold, David and Williamson, Bob},
  year = {2001},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {48--64},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-44581-1_4},
  abstract = {In this paper we show that several known algorithms for sequential prediction problems (including the quasi-additive family of Grove et al. and Littlestone and Warmuth's Weighted Majority), for playing iterated games (including Freund and Schapire's Hedge and MW, as well as the {$\Lambda$}-strategies of Hart and Mas-Colell), and for boosting (including AdaBoost) are special cases of a general decision strategy based on the notion of potential. By analyzing this strategy we derive known performance bounds, as well as new bounds, as simple corollaries of a single general theorem. Besides offering a new and unified view on a large family of algorithms, we establish a connection between potential-based analysis in learning and their counterparts independently developed in game theory. By exploiting this connection, we show that certain learning problems are instances of more general game-theoretic problems. In particular, we describe a notion of generalized regret and show its applications in learning theory.},
  isbn = {978-3-540-44581-4},
  langid = {english},
  keywords = {Blackwell’s strategy,boosting,internal regret,on-line learning,Perceptron algorithm,universal prediction,weighted average predictors}
}

@book{cesa-bianchiPredictionLearningGames2006,
  title = {Prediction, Learning, and Games},
  author = {{Cesa-Bianchi}, Nicol{\`o} and Lugosi, G{\'a}bor},
  year = {2006},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge; New York}},
  abstract = {The central theme here is a model of prediction using expert advice, a general framework within which many related problems can be cast and discussed, including repeated game playing, adaptive data compression, sequential investment in the stock market, and sequential pattern analysis.},
  isbn = {978-0-511-19178-7 978-0-511-54692-1 978-0-511-18995-1 978-0-511-19059-9 978-0-511-19091-9 978-0-511-19131-2 978-0-521-84108-5},
  langid = {english},
  annotation = {OCLC: 70056026},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/QZT93IKP/Cesa-Bianchi et Lugosi - 2006 - Prediction, learning, and games.pdf}
}

@book{cesa-bianchiPredictionLearningGames2006a,
  title = {Prediction, Learning, and Games},
  author = {{Cesa-Bianchi}, Nicol{\`o} and Lugosi, G{\'a}bor},
  year = {2006},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge; New York}},
  abstract = {The central theme here is a model of prediction using expert advice, a general framework within which many related problems can be cast and discussed, including repeated game playing, adaptive data compression, sequential investment in the stock market, and sequential pattern analysis.},
  isbn = {978-0-511-19178-7 978-0-511-54692-1 978-0-511-18995-1 978-0-511-19059-9 978-0-511-19091-9 978-0-511-19131-2 978-0-521-84108-5},
  langid = {english},
  annotation = {OCLC: 1104397279},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/6KUI4GRB/Cesa-Bianchi et Lugosi - 2006 - Prediction, learning, and games.pdf}
}

@article{chambolleContinuousOptimizationIntroduction,
  title = {Continuous Optimization, an Introduction},
  author = {Chambolle, Antonin},
  pages = {65},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/IDPUYGA2/Chambolle - Continuous optimization, an introduction.pdf}
}

@article{chatelainHowMacroeconomistsLost2020,
  title = {How {{Macroeconomists Lost Control}} of {{Stabilization Policy}}: {{Towards Dark Ages}}},
  shorttitle = {How {{Macroeconomists Lost Control}} of {{Stabilization Policy}}},
  author = {Chatelain, Jean Bernard and Ralf, Kirsten},
  year = {2020},
  month = sep,
  journal = {The European Journal of the History of Economic Thought},
  eprint = {2010.00212},
  eprinttype = {arxiv},
  pages = {1--45},
  issn = {0967-2567, 1469-5936},
  doi = {10.1080/09672567.2020.1817119},
  abstract = {This paper is a study of the history of the transplant of mathematical tools using negative feedback for macroeconomic stabilization policy from 1948 to 1975 and the subsequent break of the use of control for stabilization policy which occurred from 1975 to 1993. New-classical macroeconomists selected a subset of the tools of control that favored their support of rules against discretionary stabilization policy. The Lucas critique and Kydland and Prescott's time-inconsistency were over-statements that led to the "dark ages" of the prevalence of the stabilization-policy-ineffectiveness idea. These over-statements were later revised following the success of the Taylor rule.},
  archiveprefix = {arXiv},
  keywords = {01A60; 91.02; 91.03; 93-02; 93-03,Mathematics - History and Overview,Mathematics - Optimization and Control,Quantitative Finance - General Finance,Statistics - Applications},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/BZV6LEVU/Chatelain et Ralf - 2020 - How Macroeconomists Lost Control of Stabilization .pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/FDNVEVEU/2010.html}
}

@article{chatelPourHistoireSociologie2015,
  title = {{Pour une histoire et une sociologie de l'enseignement de l'\'economie}},
  author = {Chatel, {\'E}lisabeth},
  year = {2015},
  month = nov,
  journal = {Education et societes},
  volume = {n\textdegree{} 35},
  number = {1},
  pages = {5--21},
  publisher = {{De Boeck Sup\'erieur}},
  issn = {1373-847X},
  abstract = {Ce dossier interroge l'\'evolution des contenus destin\'es \`a l'enseignement de l'\'economie, scolaire et universitaire, selon l'approche fructueuse des conflits, d\'ebats et controverses qui l'ont accompagn\'ee. Le point de vue choisi est \`a la fois historique et international. Sont privil\'egi\'es les pays o\`u l'\'economie est le plus anciennement implant\'ee dans l'enseignement. Les contributions offrent des analyses sociohistoriques et des t\'emoignages d'acteurs de ces d\'ebats. Elles montrent l'existence d'oppositions sur les finalit\'es de cet enseignement \`a l'encontre d'une tendance, \'egalement pr\'esente, \`a en faire une formation pour des experts, conform\'ement au mod\`ele \'etatsunien qui s'est install\'e notamment apr\`es guerre et diffus\'e ensuite.},
  langid = {french},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/I8C3Z3BH/Chatel - 2015 - Pour une histoire et une sociologie de l’enseignem.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/628J6JR9/revue-education-et-societes-2015-1-page-5.html}
}

@inproceedings{clausDynamicsReinforcementLearning1998,
  title = {The {{Dynamics}} of {{Reinforcement Learning}} in {{Cooperative Multiagent Systems}}},
  booktitle = {Proceedings of the Fifteenth {{National}}/{{Tenth}} Conference on Artificial {{Intelligence}}/{{Innovative}} Applications of Artificial Intelligence},
  author = {Claus, Caroline and Boutilier, Craig},
  year = {1998},
  series = {{{AAAI}} '98/{{IAAI}} '98},
  pages = {746--752},
  publisher = {{American Association for Artificial Intelligence}},
  address = {{USA}},
  abstract = {Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multi agent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study (a simple form of) Q-leaming in cooperative multi agent systems under these two perspectives, focusing on the influence of that game structure and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria. We then propose alternative optimistic exploration strategies that increase the likelihood of convergence to an optimal equilibrium.},
  isbn = {0-262-51098-7},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/KHFTLJZ9/Claus et Boutilier - The Dynamics of Reinforcement Learning in Cooperat.pdf}
}

@inproceedings{cohenLearningBanditFeedback,
 author = {Heliou, Am\'{e}lie and Cohen, Johanne and Mertikopoulos, Panayotis},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning with Bandit Feedback in Potential Games},
 url = {https://proceedings.neurips.cc/paper/2017/file/39ae2ed11b14a4ccb41d35e9d1ba5d11-Paper.pdf},
 volume = {30},
 year = {2017}
}


@article{cominettiApproximationConvergenceLarge2021,
  title = {Approximation and {{Convergence}} of {{Large Atomic Congestion Games}}},
  author = {Cominetti, Roberto and Scarsini, Marco and Schr{\"o}der, Marc and {Stier-Moses}, Nicol{\'a}s},
  year = {2021},
  month = may,
  journal = {arXiv:2001.02797 [cs, math]},
  eprint = {2001.02797},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {We consider the question of whether, and in what sense, Wardrop equilibria provide a good approximation for Nash equilibria in atomic unsplittable congestion games with a large number of small players. We examine two different definitions of small players. In the first setting, we consider games where each player's weight is small. We prove that when the number of players goes to infinity and their weights to zero, the random flows in all (mixed) Nash equilibria for the finite games converge in distribution to the set of Wardrop equilibria of the corresponding nonatomic limit game. In the second setting, we consider an increasing number of players with a unit weight that participate in the game with a decreasingly small probability. In this case, the Nash equilibrium flows converge in total variation towards Poisson random variables whose expected values are Wardrop equilibria of a different nonatomic game with suitably-defined costs. The latter can be viewed as symmetric equilibria in a Poisson game in the sense of Myerson, establishing a plausible connection between the Wardrop model for routing games and the stochastic fluctuations observed in real traffic. In both settings we provide explicit approximation bounds, and we study the convergence of the price of anarchy. Beyond the case of congestion games, we prove a general result on the convergence of large games with random players towards Poisson games.},
  archiveprefix = {arXiv},
  keywords = {91A13; 91A06; 91A10,Computer Science - Computer Science and Game Theory,Mathematics - Optimization and Control,Mathematics - Probability},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/BG2ISMLQ/Cominetti et al. - 2021 - Approximation and Convergence of Large Atomic Cong.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/XGPLLAJ3/2001.html}
}

@article{cominettiLongTermBehaviorDynamic2021,
  title = {Long-{{Term Behavior}} of {{Dynamic Equilibria}} in {{Fluid Queuing Networks}}},
  author = {Cominetti, Roberto and Correa, Jos{\'e} and Olver, Neil},
  year = {2021},
  month = mar,
  journal = {Operations Research},
  pages = {opre.2020.2081},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.2020.2081},
  abstract = {A fluid queuing network constitutes one of the simplest models in which to study flow dynamics over a network. In this model we have a single source-sink pair, and each link has a per-time-unit capacity and a transit time. A dynamic equilibrium (or equilibrium flow over time) is a flow pattern over time such that no flow particle has incentives to unilaterally change its path. Although the model has been around for almost 50 years, only recently results regarding existence and characterization of equilibria have been obtained. In particular, the long-term behavior remains poorly understood. Our main result in this paper is to show that, under a natural (and obviously necessary) condition on the queuing capacity, a dynamic equilibrium reaches a steady state (after which queue lengths remain constant) in finite time. Previously, it was not even known that queue lengths would remain bounded. The proof is based on the analysis of a rather nonobvious potential function that turns out to be monotone along the evolution of the equilibrium. Furthermore, we show that the steady state is characterized as an optimal solution of a certain linear program. When this program has a unique solution, which occurs generically, the long-term behavior is completely predictable. On the contrary, if the linear program has multiple solutions, the steady state is more difficult to identify as it depends on the whole temporal evolution of the equilibrium.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/4UKV4JL9/Cominetti et al. - 2021 - Long-Term Behavior of Dynamic Equilibria in Fluid .pdf}
}

@article{congerSOCIALCONTROLSOCIAL1976,
  title = {{{SOCIAL CONTROL AND SOCIAL LEARNING MODELS OF DELINQUENT BEHAVIOR A Synthesis}}},
  author = {Conger, Rand D.},
  year = {1976},
  month = may,
  journal = {Criminology},
  volume = {14},
  number = {1},
  pages = {17--40},
  issn = {0011-1384, 1745-9125},
  doi = {10.1111/j.1745-9125.1976.tb00002.x},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/93QZ5G2J/Conger - 1976 - SOCIAL CONTROL AND SOCIAL LEARNING MODELS OF DELIN.pdf}
}

@article{conitzerAWESOMEGeneralMultiagent2007,
  title = {{{AWESOME}}: {{A}} General Multiagent Learning Algorithm That Converges in Self-Play and Learns a Best Response against Stationary Opponents},
  shorttitle = {{{AWESOME}}},
  author = {Conitzer, Vincent and Sandholm, Tuomas},
  year = {2007},
  month = may,
  journal = {Machine Learning},
  volume = {67},
  number = {1-2},
  pages = {23--43},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-006-0143-1},
  abstract = {Two minimal requirements for a satisfactory multiagent learning algorithm are that it 1. learns to play optimally against stationary opponents and 2. converges to a Nash equilibrium in self-play. The previous algorithm that has come closest, WoLF-IGA, has been proven to have these two properties in 2-player 2-action (repeated) games\textemdash assuming that the opponent's mixed strategy is observable. Another algorithm, ReDVaLeR (which was introduced after the algorithm described in this paper), achieves the two properties in games with arbitrary numbers of actions and players, but still requires that the opponents' mixed strategies are observable. In this paper we present AWESOME, the first algorithm that is guaranteed to have the two properties in games with arbitrary numbers of actions and players. It is still the only algorithm that does so while only relying on observing the other players' actual actions (not their mixed strategies). It also learns to play optimally against opponents that eventually become stationary. The basic idea behind AWESOME (Adapt When Everybody is Stationary, Otherwise Move to Equilibrium) is to try to adapt to the others' strategies when they appear stationary, but otherwise to retreat to a precomputed equilibrium strategy. We provide experimental results that suggest that AWESOME converges fast in practice. The techniques used to prove the properties of AWESOME are fundamentally different from those used for previous algorithms, and may help in analyzing future multiagent learning algorithms as well.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/GZ3VFTNH/Conitzer et Sandholm - 2007 - AWESOME A general multiagent learning algorithm t.pdf}
}

@article{crawfordLearningMixedstrategyEquilibria1989a,
  title = {Learning and Mixed-Strategy Equilibria in Evolutionary Games},
  author = {Crawford, Vincent P.},
  year = {1989},
  month = oct,
  journal = {Journal of Theoretical Biology},
  volume = {140},
  number = {4},
  pages = {537--550},
  issn = {00225193},
  doi = {10.1016/S0022-5193(89)80113-4},
  langid = {english},
  keywords = {lu,preuve pas lue},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/9F6RFHIL/Crawford - 1989 - Learning and mixed-strategy equilibria in evolutio.pdf}
}

@misc{dandreaConstantStepSizeMultiplicative2021,
  title = {Constant {{Step-Size Multiplicative Weights Algorithm}} in {{Potential Games}}},
  author = {D'Andrea, Maurizio},
  year = {2021},
  month = oct,
  address = {{Quimper}},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/6WY4CVBA/D’Andrea - Constant Step-Size Multiplicative Weights Algorith.pdf}
}

@inproceedings{daskalakisComplexityConstrainedMinmax2021,
  title = {The Complexity of Constrained Min-Max Optimization},
  booktitle = {Proceedings of the 53rd {{Annual ACM SIGACT Symposium}} on {{Theory}} of {{Computing}}},
  author = {Daskalakis, Constantinos and Skoulakis, Stratis and Zampetakis, Manolis},
  year = {2021},
  month = jun,
  pages = {1466--1478},
  publisher = {{ACM}},
  address = {{Virtual Italy}},
  doi = {10.1145/3406325.3451125},
  abstract = {Despite its important applications in Machine Learning, min-max optimization of objective functions that are nonconvex-nonconcave remains elusive. Not only are there no known first-order methods converging to even approximate local min-max equilibria (a.k.a. approximate saddle points), but the computational complexity of identifying them is also poorly understood. In this paper, we provide a characterization of the computational complexity as well as of the limitations of first-order methods in this problem. Specifically, we show that in linearly constrained min-max optimization problems with nonconvex-nonconcave objectives an approximate local minmax equilibrium of large enough approximation is guaranteed to exist, but computing such a point is PPAD-complete. The same is true of computing an approximate fixed point of the (Projected) Gradient Descent/Ascent update dynamics, which is computationally equivalent to computing approximate local min-max equilibria. An important byproduct of our proof is to establish an unconditional hardness result in the Nemirovsky-Yudin [36] oracle optimization model, where we are given oracle access to the values of some function f : P \textrightarrow{} [-1, 1] and its gradient {$\nabla$}f , where P {$\subseteq$} [0, 1]d is a known convex polytope. We show that any algorithm that uses such first-order oracle access to f and finds an {$\epsilon$}-approximate local min-max equilibrium needs to make a number of oracle queries that is exponential in at least one of 1/{$\epsilon$}, L, G, or d, where L and G are respectively the smoothness and Lipschitzness of f . This comes in sharp contrast to minimization problems, where finding approximate local minima in the same setting can be done with Projected Gradient Descent using O(L/{$\epsilon$}) many queries. Our result is the first to show an exponential separation between these two fundamental optimization problems in the oracle model.},
  isbn = {978-1-4503-8053-9},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/34KBJH3E/Daskalakis et al. - 2021 - The complexity of constrained min-max optimization.pdf}
}

@article{daskalakisNearoptimalNoregretAlgorithms2015,
  title = {Near-Optimal No-Regret Algorithms for Zero-Sum Games},
  author = {Daskalakis, Constantinos and Deckelbaum, Alan and Kim, Anthony},
  year = {2015},
  month = jul,
  journal = {Games and Economic Behavior},
  volume = {92},
  pages = {327--348},
  issn = {08998256},
  doi = {10.1016/j.geb.2014.01.003},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/QZNPAV87/Daskalakis et al. - 2015 - Near-optimal no-regret algorithms for zero-sum gam.pdf}
}

@article{dautumeTheorieJeuxMarche1992,
  title = {{Th\'eorie des jeux et march\'e}},
  author = {{d'Autume}, Antoine},
  year = {1992},
  journal = {Cahiers d'\'economie politique},
  volume = {20},
  number = {1},
  pages = {155--165},
  issn = {0154-8344},
  doi = {10.3406/cep.1992.1131},
  abstract = {Game Theory offers the natural tool for a theoretical analysis of markets. Its usefulness becomes evident in the analysis of imperfect competition. To illustrate this theme we first examine the analysis of price competition stemming from the BertrandEdgeworth model which has led to the Theory of Contestable Markets. We then review some recent applications of Dynamic Games in the field of Industrial Organization.},
  langid = {french},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/DYKTMLAA/d'Autume - 1992 - Théorie des jeux et marché.pdf}
}

@article{dawidNumericalAnalysisMarkovPerfect2017,
  title = {Numerical {{Analysis}} of {{Markov-Perfect Equilibria}} with {{Multiple Stable Steady States}}: {{A Duopoly Application}} with {{Innovative Firms}}},
  shorttitle = {Numerical {{Analysis}} of {{Markov-Perfect Equilibria}} with {{Multiple Stable Steady States}}},
  author = {Dawid, Herbert and Keoula, Michel Y. and Kort, Peter M.},
  year = {2017},
  month = dec,
  journal = {Dynamic Games and Applications},
  volume = {7},
  number = {4},
  pages = {555--577},
  issn = {2153-0785, 2153-0793},
  doi = {10.1007/s13235-016-0213-3},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/BIKMA6D6/Dawid et al. - 2017 - Numerical Analysis of Markov-Perfect Equilibria wi.pdf}
}

@article{degenneBridgingGapRegret2019,
  title = {Bridging the Gap between Regret Minimization and Best Arm Identification, with Application to {{A}}/{{B}} Tests},
  author = {Degenne, R{\'e}my and Nedelec, Thomas and Calauz{\`e}nes, Cl{\'e}ment and Perchet, Vianney},
  year = {2019},
  month = feb,
  journal = {arXiv:1810.04088 [cs, stat]},
  eprint = {1810.04088},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {State of the art online learning procedures focus either on selecting the best alternative (``best arm identification'') or on minimizing the cost (the ``regret''). We merge these two objectives by providing the theoretical analysis of cost minimizing algorithms that are also {$\delta$}-PAC (with a proven guaranteed bound on the decision time), hence fulfilling at the same time regret minimization and best arm identification. This analysis sheds light on the common observation that ill-callibrated UCB-algorithms minimize regret while still identifying quickly the best arm. We also extend these results to the non-iid case faced by many practitioners. This provides a technique to make cost versus decision time compromise when doing adaptive tests with applications ranging from website A/B testing to clinical trials.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/FGP35SB6/Degenne et al. - 2019 - Bridging the gap between regret minimization and b.pdf}
}

@article{DEMICHELIS2000192,
  title = {On the Indices of Zeros of Nash Fields},
  author = {DeMichelis, Stefano and Germano, Fabrizio},
  year = {2000},
  journal = {Journal of Economic Theory},
  volume = {94},
  number = {2},
  pages = {192--217},
  issn = {0022-0531},
  doi = {10.1006/jeth.2000.2669},
  abstract = {We show a fundamental property of dynamics whose zeros are essentially the Nash equilibria of underlying games; namely, the indices of zeros coincide with the degrees of the projection from the graph of the Nash correspondence onto the underlying space of games. This is important since it implies that for a wide class of dynamics local stability properties of zeros are determined by the geometry of the Nash correspondence, providing further links between learning or evolutionary game theory, the theory of equilibrium refinements, and the geometry of Nash equilibrium. The result extends beyond general n-player games e.g. to Walrasian equilibrium theory. Journal of Economic Literature Classification Numbers: C62, C72, D50.},
  keywords = {degree,dynamics,geometry of equilibrium correspondences,index,stability}
}

@article{dermedSolvingStochasticGames2009,
  title = {Solving {{Stochastic Games}}},
  author = {Dermed, Liam M and Isbell, Charles L},
  year = {2009},
  journal = {NIPS},
  pages = {9},
  abstract = {Solving multi-agent reinforcement learning problems has proven difficult because of the lack of tractable algorithms. We provide the first approximation algorithm which solves stochastic games with cheap-talk to within absolute error of the optimal game-theoretic solution, in time polynomial in 1/ . Our algorithm extends Murray's and Gordon's (2007) modified Bellman equation which determines the set of all possible achievable utilities; this provides us a truly general framework for multi-agent learning. Further, we empirically validate our algorithm and find the computational cost to be orders of magnitude less than what the theory predicts.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/TY42I6JL/Dermed et Isbell - Solving Stochastic Games.pdf}
}

@article{desilvaEfficiencyDecentralizedEpidemic2021,
  title = {On the Efficiency of Decentralized Epidemic Management and Application to {{Covid-19}}},
  author = {De Silva, Olivier Lindamulage and Lasaulce, Samson and Mor{\u a}rescu, Irinel-Constantin},
  year = {2021},
  journal = {IEEE Control Systems Letters},
  eprint = {2106.06220},
  eprinttype = {arxiv},
  pages = {1--1},
  issn = {2475-1456},
  doi = {10.1109/LCSYS.2021.3087101},
  abstract = {In this paper, we introduce a game that allows one to assess the potential loss of efficiency induced by a decentralized control or local management of a global epidemic. Each player typically represents a region or a country which is assumed to choose its control action to implement a tradeoff between socioeconomic aspects and the health aspect. We conduct the Nash equilibrium analysis of this game. Since the analysis is not trivial in general, sufficient conditions for existence and uniqueness are provided. Then we quantify through numerical results the loss induced by decentralization, measured in terms of price of anarchy (PoA) and price of connectedness (PoC). These results allow one to clearly identify scenarios where decentralization is acceptable or not regarding to the retained global efficiency measures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {91A06; 91A10; 91A80,Computer Science - Computer Science and Game Theory,Electrical Engineering and Systems Science - Systems and Control},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/KWBWBTAD/De Silva et al. - 2021 - On the efficiency of decentralized epidemic manage.pdf}
}

@article{dongSqrtnRegretLearning2020,
  title = {\$\textbackslash sqrt\{n\}\$-{{Regret}} for {{Learning}} in {{Markov Decision Processes}} with {{Function Approximation}} and {{Low Bellman Rank}}},
  author = {Dong, Kefan and Peng, Jian and Wang, Yining and Zhou, Yuan},
  year = {2020},
  month = jun,
  journal = {arXiv:1909.02506 [cs, stat]},
  eprint = {1909.02506},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this paper, we consider the problem of online learning of Markov decision processes (MDPs) with very large state spaces. Under the assumptions of realizable function approximation and low Bellman ranks, we develop an online learning algorithm that learns the optimal value function while at the same time achieving very low cumulative regret during the learning process. Our learning algorithm, Adaptive Value-function Elimination (AVE), is inspired by the policy elimination algorithm proposed in [1], known as OLIVE. One of our key technical contributions in AVE is to formulate the elimination steps in OLIVE as contextual bandit problems. This technique enables us to apply the active elimination and expert weighting methods from [2], instead of the random action exploration scheme used in the original OLIVE algorithm, for more efficient exploration and better control of the regret incurred in each policy elimination step. To the best of our knowledge, this is the first {$\surd$}n-regret result for reinforcement learning in stochastic MDPs with general value function approximation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/B7QVDEWJ/Dong et al. - 2020 - $sqrt n $-Regret for Learning in Markov Decision .pdf}
}

@article{driesFarmersVerticalCoordination2009a,
  title = {Farmers, {{Vertical Coordination}}, and the {{Restructuring}} of {{Dairy Supply Chains}} in {{Central}} and {{Eastern Europe}}},
  author = {Dries, Liesbeth and Germenji, Etleva and Noev, Nivelin and Swinnen, Johan F.M.},
  year = {2009},
  month = nov,
  journal = {World Development},
  volume = {37},
  number = {11},
  pages = {1742--1758},
  issn = {0305750X},
  doi = {10.1016/j.worlddev.2008.08.029},
  abstract = {The combination of transition and globalization since the early 1990s has caused dramatic changes in the dairy chains in Central and Eastern Europe. This paper uses survey evidence from several Central and East European countries to document the growth of vertical coordination in the dairy chain, its relationship with policy reforms, its effects and the implications for small farms. Evidence suggests that in several countries small dairy farms have benefited from vertical coordination processes by providing them access to inputs and higher value markets.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/DJRDHSRA/Dries et al. - 2009 - Farmers, Vertical Coordination, and the Restructur.pdf}
}

@article{duchiDualAveragingDistributed2012,
  title = {Dual {{Averaging}} for {{Distributed Optimization}}: {{Convergence Analysis}} and {{Network Scaling}}},
  shorttitle = {Dual {{Averaging}} for {{Distributed Optimization}}},
  author = {Duchi, J. C. and Agarwal, A. and Wainwright, M. J.},
  year = {2012},
  month = mar,
  journal = {IEEE Transactions on Automatic Control},
  volume = {57},
  number = {3},
  pages = {592--606},
  issn = {0018-9286, 1558-2523},
  doi = {10.1109/TAC.2011.2161027},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/3IW8AZQ7/Duchi et al. - 2012 - Dual Averaging for Distributed Optimization Conve.pdf}
}

@book{dugatkinGameTheoryAnimal1998,
  title = {Game Theory \& Animal Behavior},
  editor = {Dugatkin, Lee Alan and Reeve, Hudson Kern},
  year = {1998},
  publisher = {{Oxford University Press}},
  address = {{New York Oxford}},
  isbn = {978-0-19-513790-3 978-0-19-509692-7},
  langid = {english},
  annotation = {OCLC: ocm42953947}
}

@inproceedings{dunkelComplexityPureStrategyNash2006,
  title = {On the {{Complexity}} of {{Pure-Strategy Nash Equilibria}} in {{Congestion}} and {{Local-Effect Games}}},
  booktitle = {Internet and {{Network Economics}}},
  author = {Dunkel, Juliane and Schulz, Andreas S.},
  year = {2006},
  month = dec,
  pages = {62--73},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/11944874_7},
  abstract = {Congestion games are a fundamental class of noncooperative games possessing pure-strategy Nash equilibria. In the network version, each player wants to route one unit of flow on a path from her...},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/YZCMLXQS/Dunkel et Schulz - 2006 - On the Complexity of Pure-Strategy Nash Equilibria.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/ZK9H24IJ/11944874_7.html}
}

@article{durrNashEquilibriaVoronoi2007,
  title = {Nash Equilibria in {{Voronoi}} Games on Graphs},
  author = {Durr, Christoph and Thang, Nguyen Kim},
  year = {2007},
  month = apr,
  journal = {arXiv:cs/0702054},
  eprint = {cs/0702054},
  eprinttype = {arxiv},
  abstract = {In this paper we study a game where every player is to choose a vertex (facility) in a given undirected graph. All vertices (customers) are then assigned to closest facilities and a player's payoff is the number of customers assigned to it. We show that deciding the existence of a Nash equilibrium for a given graph is N P-hard which to our knowledge is the first result of this kind for a zero-sum game. We also introduce a new measure, the social cost discrepancy, defined as the ratio of the costs between the worst and the b{$\surd$}est Nash equilibria. We show that the social cost discrepancy in our game is {$\Omega$}( n/k) and O( kn), where n is the number of vertices and k the number of players.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Data Structures and Algorithms},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/23QZGEU8/Durr et Thang - 2007 - Nash equilibria in Voronoi games on graphs.pdf}
}

@book{EconomiePolitique2013,
  title = {{L'\'Economie politique 2013/2 (n\textdegree{} 58)}},
  issn = {1293-6146},
  abstract = {Mais qui sont les \'economistes ?},
  isbn = {978-2-35240-082-0},
  langid = {french}
}

@book{EducationSocietes2015,
  title = {{\'Education et soci\'et\'es 2015/1 (n\textdegree{} 35)}},
  issn = {1373-847X},
  abstract = {L'enseignement de l'\'economie : conflits, d\'ebats et controverses},
  isbn = {978-2-8073-0085-9},
  langid = {french}
}

@article{eigruberClimateEngineeringInterconnected2018,
  title = {Climate {{Engineering}} in an {{Interconnected World}}: {{The Role}} of {{Tariffs}}},
  shorttitle = {Climate {{Engineering}} in an {{Interconnected World}}},
  author = {Eigruber, Markus and Wirl, Franz},
  year = {2018},
  month = sep,
  journal = {Dynamic Games and Applications},
  volume = {8},
  number = {3},
  pages = {573--587},
  issn = {2153-0793},
  doi = {10.1007/s13235-018-0261-y},
  abstract = {This paper investigates strategic trade policies as a response to negative externalities linked to climate engineering. Parties negatively affected, or which only perceive damages, may react to geoengineering by deploying trade sanctions, i.e. the imposition of tariffs. By introducing a dynamic trade model, we show that geoengineering-averse countries have an incentive to implement or increase existing tariffs when the other country uses geoengineering. Our contribution is to highlight that potential consequences on trade should be taken into account before climate engineering techniques are applied. This is particularly crucial in our globalized world since a successful climate policy demands large scale if not global cooperation.},
  langid = {english},
  keywords = {Asymmetric differential game,Climate engineering,Externalities,Global warming,Tariffs},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/WL467ZRI/Eigruber et Wirl - 2018 - Climate Engineering in an Interconnected World Th.pdf}
}

@misc{EJPNewsWestern2007,
  title = {{{EJP}} | {{News}} | {{Western Europe}} | {{Anti-Israel}} Protests against {{Nobel}} Prize Award},
  year = {2007},
  month = sep,
  howpublished = {http://web.archive.org/web/20070929091902/http://www.ejpress.org/article/4556},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/UNCSUREI/4556.html}
}

@article{EnseignementEconomieDans2013,
  title = {{L'enseignement de l'\'economie dans le sup\'erieur : bilan critique et perspectives}},
  shorttitle = {{L'enseignement de l'\'economie dans le sup\'erieur}},
  year = {2013},
  month = may,
  journal = {L'Economie politique},
  volume = {n\textdegree{} 58},
  number = {2},
  pages = {6--23},
  publisher = {{Alternatives \'economiques}},
  issn = {1293-6146},
  langid = {french},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/7JSGL97R/2013 - L'enseignement de l'économie dans le supérieur  b.pdf}
}

@book{ericksonWorldGameTheorists2015,
  title = {The World the Game Theorists Made},
  author = {Erickson, Paul},
  year = {2015},
  publisher = {{The University of Chicago Press}},
  address = {{Chicago : London}},
  isbn = {978-0-226-09703-9 978-0-226-09717-6},
  lccn = {HB144 .E75 2015},
  keywords = {Game theory,Methodology,Science,Theory of games and economic behavior,Von Neumann; John}
}

@book{ericksonWorldGameTheorists2015a,
  title = {The {{World}} the {{Game Theorists Made}}},
  author = {Erickson, Paul},
  year = {2015},
  publisher = {{University of Chicago Press}},
  doi = {10.7208/chicago/9780226097206.001.0001},
  isbn = {978-0-226-09717-6 978-0-226-09703-9 978-0-226-09720-6},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/WP9WKSTM/Erickson - 2015 - The World the Game Theorists Made.pdf}
}

@book{evansPartialDifferentialEquations2010,
  title = {Partial Differential Equations},
  author = {Evans, Lawrence C.},
  year = {2010},
  series = {Graduate Studies in Mathematics},
  edition = {2nd ed},
  number = {v. 19},
  publisher = {{American Mathematical Society}},
  address = {{Providence, R.I}},
  abstract = {"This is the second edition of the now definitive text on partial differential equations (PDE). It offers a comprehensive survey of modern techniques in the theoretical study of PDE with particular emphasis on nonlinear equations. Its wide scope and clear exposition make it a great text for a graduate course in PDE. For this edition, the author has made numerous changes, including: a new chapter on nonlinear wave equations, more than 80 new exercises, several new sections, and a significantly expanded bibliography."--Publisher's description},
  isbn = {978-0-8218-4974-3},
  lccn = {QA377 .E95 2010},
  keywords = {Differential equations; Partial},
  annotation = {OCLC: ocn465190110},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/PNJP357K/[Graduate Studies in Mathematics] Lawrence C. Evans - Partial Differential Equations_ Second Edition (2010, AMS) - libgen.lc.djvu}
}

@article{ewerhartFictitiousPlayNetworks2020,
  title = {Fictitious Play in Networks},
  author = {Ewerhart, Christian and Valkanova, Kremena},
  year = {2020},
  month = sep,
  journal = {Games and Economic Behavior},
  volume = {123},
  pages = {182--206},
  issn = {08998256},
  doi = {10.1016/j.geb.2020.06.006},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/JVBUNAS2/Ewerhart et Valkanova - 2020 - Fictitious play in networks.pdf}
}

@article{ewerhartFictitiousPlayNetworks2020a,
  title = {Fictitious Play in Networks},
  author = {Ewerhart, Christian and Valkanova, Kremena},
  year = {2020},
  month = sep,
  journal = {Games and Economic Behavior},
  volume = {123},
  pages = {182--206},
  issn = {08998256},
  doi = {10.1016/j.geb.2020.06.006},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/KZ5ZQWM2/Ewerhart et Valkanova - 2020 - Fictitious play in networks.pdf}
}

@inproceedings{fabrikantComplexityPureNash2004,
  title = {The Complexity of Pure {{Nash}} Equilibria},
  booktitle = {Proceedings of the Thirty-Sixth Annual {{ACM}} Symposium on {{Theory}} of Computing  - {{STOC}} '04},
  author = {Fabrikant, Alex and Papadimitriou, Christos and Talwar, Kunal},
  year = {2004},
  pages = {604},
  publisher = {{ACM Press}},
  address = {{Chicago, IL, USA}},
  doi = {10.1145/1007352.1007445},
  abstract = {We investigate from the computational viewpoint multi-player games that are guaranteed to have pure Nash equilibria. We focus on congestion games, and show that a pure Nash equilibrium can be computed in polynomial time in the symmetric network case, while the problem is PLS-complete in general. We discuss implications to non-atomic congestion games, and we explore the scope of the potential function method for proving existence of pure Nash equilibria.},
  isbn = {978-1-58113-852-8},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/SW4MCPGL/Fabrikant et al. - 2004 - The complexity of pure Nash equilibria.pdf}
}

@article{facchineiGeneralizedNashEquilibrium2010,
  title = {Generalized {{Nash Equilibrium Problems}}},
  author = {Facchinei, Francisco and Kanzow, Christian},
  year = {2010},
  month = mar,
  journal = {Annals of Operations Research},
  volume = {175},
  number = {1},
  pages = {177--211},
  issn = {0254-5330, 1572-9338},
  doi = {10.1007/s10479-009-0653-x},
  abstract = {The Generalized Nash Equilibrium Problem is an important model that has its roots in the economic sciences but is being fruitfully used in many different fields. In this survey paper we aim at discussing its main properties and solution algorithms, pointing out what could be useful topics for future research in the field.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/U8XVQFFC/Facchinei et Kanzow - 2010 - Generalized Nash Equilibrium Problems.pdf}
}

@article{fangWhenSecurityGames2015,
  title = {When {{Security Games Go Green}}: {{Designing Defender Strategies}} to {{Prevent Poaching}} and {{Illegal Fishing}}},
  author = {Fang, Fei and Stone, Peter and Tambe, Milind},
  year = {2015},
  pages = {7},
  abstract = {Building on the successful applications of Stackelberg Security Games (SSGs) to protect infrastructure, researchers have begun focusing on applying game theory to green security domains such as protection of endangered animals and fish stocks. Previous efforts in these domains optimize defender strategies based on the standard Stackelberg assumption that the adversaries become fully aware of the defender's strategy before taking action. Unfortunately, this assumption is inappropriate since adversaries in green security domains often lack the resources to fully track the defender strategy. This paper (i) introduces Green Security Games (GSGs), a novel game model for green security domains with a generalized Stackelberg assumption; (ii) provides algorithms to plan effective sequential defender strategies \textemdash{} such planning was absent in previous work; (iii) proposes a novel approach to learn adversary models that further improves defender performance; and (iv) provides detailed experimental analysis of proposed approaches.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/TDLL96SX/Fang et al. - When Security Games Go Green Designing Defender S.pdf}
}

@article{farahaniOrderNowPickup2022,
  title = {Order {{Now}}, {{Pickup}} in 30 {{Minutes}}: {{Managing Queues}} with {{Static Delivery Guarantees}}},
  shorttitle = {Order {{Now}}, {{Pickup}} in 30 {{Minutes}}},
  author = {Farahani, Mehdi H. and Dawande, Milind and Janakiraman, Ganesh},
  year = {2022},
  month = mar,
  journal = {Operations Research},
  pages = {opre.2021.2203},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.2021.2203},
  abstract = {The shift in the restaurant industry toward digital ordering argues for major changes in how orders are managed. The main difficulty in managing queues in online food-ordering services arises from the fact that, as opposed to dine-in customers, online customers are promised a pick-up time; customers are dissatisfied if the order is not completed by that time and are also dissatisfied if the order is completed much ahead of time because the food loses freshness. In ``Order Now, Pickup in 30 Minutes: Managing Queues with Static Delivery Guarantees,'' Farahani, Dawande, and Janakiraman propose and analyze strategies for managing queues in online food-ordering services with the goal of keeping customer satisfaction as high as possible.           ,              We study the problem of managing queues in online food-ordering services where customers, who place orders online and pick up at the store, are offered a common quote time, that is, the promised pick-up time minus the time the order is placed. The objective is to minimize the long-run average expected earliness and tardiness cost incurred by the customers. We introduce the family of static threshold policies for managing such queues. A static threshold policy is one that starts serving the first customer in the queue as soon as the server is free and the time remaining until the promised pick-up time of that customer falls below a fixed threshold. In important technical contributions for establishing the attractiveness of the optimal static threshold policy, we develop two sets of lower bounds on the optimal cost. The first set of lower bounds uses the idea of a clairvoyant optimal policy by considering a decision maker who has either full or partial knowledge of the outcomes of future uncertainties. To obtain our second set of lower bounds, we develop bounds on the optimal earliness and tardiness costs by establishing lower and upper bounds on the steady-state waiting time under an optimal policy. The optimal static threshold policy is asymptotically optimal in several cases, including the heavy traffic and the light traffic regimes. We also develop a dynamic threshold policy in which the threshold depends on the queue length. Finally, through a comprehensive numerical study, we demonstrate the excellent performance of both the static and the dynamic threshold policies.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/SIA8DJJQ/Farahani et al. - 2020 - Order Now, Pickup in 30 Minutes Managing Queues w.pdf}
}

@article{fareIntermediateInputModel1995,
  title = {An {{Intermediate Input Model}} of {{Dairy Production Using Complex Survey Data}}},
  author = {F{\"a}re, Rolf and Whittaker, Gerald},
  year = {1995},
  journal = {Journal of Agricultural Economics},
  volume = {46},
  number = {2},
  pages = {201--213},
  issn = {1477-9552},
  doi = {10.1111/j.1477-9552.1995.tb00766.x},
  abstract = {Agricultural production is often characterised by multiple inputs and multiple outputs to multiple production processes. Where an output from one process is used as an input to another, this output is called an intermediate product. This is a common situation when a farm produces both crops and livestock. The analysis of production efficiency is important for the evaluation of agricultural policy, but until recently, no methods have explicitly included intermediate products. This study applies a non-parametric technique of efficiency measurement which includes intermediate products. The data set is a sample of dairy farms drawn using a complex survey design. The use of non-parametric efficiency measurement and the subsequent application of bootstrapping and kernel density estimation to the results allow inferences to be drawn concerning the whole population from which the sample was drawn. We find that the decomposition of production into subproduction processes reduces the dimensions of problem specification, with the effect that a larger number of variables may be usefully included in the model.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1477-9552.1995.tb00766.x}
}

@article{farinaOnlineConvexOptimization2018,
  title = {Online {{Convex Optimization}} for {{Sequential Decision Processes}} and {{Extensive-Form Games}}},
  author = {Farina, Gabriele and Kroer, Christian and Sandholm, Tuomas},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.03075 [cs]},
  eprint = {1809.03075},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Regret minimization is a powerful tool for solving large-scale extensive-form games. State-of-the-art methods rely on minimizing regret locally at each decision point. In this work we derive a new framework for regret minimization on sequential decision problems and extensive-form games with general compact convex sets at each decision point and general convex losses, as opposed to prior work which has been for simplex decision points and linear losses. We call our framework laminar regret decomposition. It generalizes the CFR algorithm to this more general setting. Furthermore, our framework enables a new proof of CFR even in the known setting, which is derived from a perspective of decomposing polytope regret, thereby leading to an arguably simpler interpretation of the algorithm. Our generalization to convex compact sets and convex losses allows us to develop new algorithms for several problems: regularized sequential decision making, regularized Nash equilibria in extensive-form games, and computing approximate extensive-form perfect equilibria. Our generalization also leads to the first regret-minimization algorithm for computing reduced-normal-form quantal response equilibria based on minimizing local regrets. Experiments show that our framework leads to algorithms that scale at a rate comparable to the fastest variants of counterfactual regret minimization for computing Nash equilibrium, and therefore our approach leads to the first algorithm for computing quantal response equilibria in extremely large games. Finally we show that our framework enables a new kind of scalable opponent exploitation approach.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/3WCVYCXP/Farina et al. - 2018 - Online Convex Optimization for Sequential Decision.pdf}
}

@inproceedings{farinaOptimisticRegretMinimization2019,
  title = {Optimistic Regret Minimization for Extensive-Form Games via Dilated Distance-Generating Functions},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Farina, Gabriele and Kroer, Christian and Sandholm, Tuomas},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/X6N8TL5T/Farina et al. - 2019 - Optimistic Regret Minimization for Extensive-Form .pdf}
}

@article{feldmanCapacitatedNetworkDesign2015,
  title = {Capacitated {{Network Design Games}}},
  author = {Feldman, Michal and Ron, Tom},
  year = {2015},
  month = oct,
  journal = {Theory of Computing Systems},
  volume = {57},
  number = {3},
  pages = {576--597},
  issn = {1432-4350, 1433-0490},
  doi = {10.1007/s00224-014-9540-1},
  abstract = {We study a capacitated symmetric network design game, where each of n agents wishes to construct a path from a network's source to its sink, and the cost of each edge is shared equally among the agents using it. The uncapacitated version of this problem has been introduced by Anshelevich et al. (2003) and has been extensively studied. We find that the consideration of edge capacities entails a significant effect on the quality of the obtained Nash equilibria (NE), under both the utilitarian and the egalitarian objective functions, as well as on the convergence rate to an equilibrium. The following results are established. First, we provide bounds for the price of anarchy (PoA) and the price of stability (PoS) measures with respect to the utilitarian (i.e., sum of costs) and egalitarian (i.e., maximum cost) objective functions. Our main result here is that unlike the uncapacitated version, the network topology is a crucial factor in the quality of NE. Specifically, a network topology has a bounded PoA if and only if it is series-parallel (SP), i.e., a network that is built inductively by series compositions and parallel compositions of SP networks. Second, we show that the convergence rate of best-response dynamics (BRD) may take (n1.5) steps. This is in contrast to the uncapacitated version, where convergence is guaranteed within at most n iterations.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/MTUM868M/Feldman et Ron - 2015 - Capacitated Network Design Games.pdf}
}

@article{finkEquilibriumStochasticPerson1964,
  title = {Equilibrium in a Stochastic \$n\$-Person Game},
  author = {Fink, A. M.},
  year = {1964},
  month = jan,
  journal = {Hiroshima Mathematical Journal},
  volume = {28},
  number = {1},
  issn = {0018-2079},
  doi = {10.32917/hmj/1206139508},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/AHZUNKEX/Fink - 1964 - Equilibrium in a stochastic $n$-person game.pdf}
}

@inproceedings{foersterCounterfactualMultiagentPolicy2018,
  title = {Counterfactual Multi-Agent Policy Gradients},
  booktitle = {{{AAAI}}},
  author = {Foerster, Jakob N. and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
  year = {2018},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/FVD3K5P2/Foerster et al. - Counterfactual Multi-Agent Policy Gradients.pdf}
}

@article{fosterAsymptoticCalibration1998,
  title = {Asymptotic Calibration},
  author = {Foster, D. and Vohra, Rakesh V.},
  year = {1998},
  month = jun,
  journal = {Biometrika},
  volume = {85},
  number = {2},
  pages = {379--390},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/85.2.379},
  abstract = {Can we forecast the probability of an arbitrary sequence of events happening so that the stated probability of an event happening is close to its empirical probability? We can view this prediction problem as a game played against Nature, where at the beginning of the game Nature picks a data sequence and the forecaster picks a forecasting algorithm. If the forecaster is not allowed to randomise, then Nature wins; there will always be data for which the forecaster does poorly. This paper shows that, if the forecaster can randomise, the forecaster wins in the sense that the forecasted probabilities and the empirical probabilities can be made arbitrarily close to each other.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/7HXXJUC8/Foster - 1998 - Asymptotic calibration.pdf}
}

@article{fosterCalibratedLearningCorrelated1997,
  ids = {fosterCalibratedLearningCorrelated1997a},
  title = {Calibrated {{Learning}} and {{Correlated Equilibrium}}},
  author = {Foster, Dean P. and Vohra, Rakesh V.},
  year = {1997},
  month = oct,
  journal = {Games and Economic Behavior},
  volume = {21},
  number = {1-2},
  pages = {40--55},
  issn = {08998256},
  doi = {10.1006/game.1997.0595},
  langid = {english},
  keywords = {lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/5HIX9L6K/Foster et Vohra - 1997 - Calibrated Learning and Correlated Equilibrium.pdf}
}

@article{fosterImpossibilityPredictingBehavior2001,
  title = {On the Impossibility of Predicting the Behavior of Rational Agents},
  author = {Foster, D. P. and Young, H. P.},
  year = {2001},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {98},
  number = {22},
  pages = {12848--12853},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.211534898},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/6YC9RDF5/Foster et Young - 2001 - On the impossibility of predicting the behavior of.pdf}
}

@article{fosterLearningHypothesisTesting2003,
  title = {Learning, Hypothesis Testing, and {{Nash}} Equilibrium},
  author = {Foster, Dean P. and Young, H.Peyton},
  year = {2003},
  month = oct,
  journal = {Games and Economic Behavior},
  volume = {45},
  number = {1},
  pages = {73--96},
  issn = {08998256},
  doi = {10.1016/S0899-8256(03)00025-3},
  abstract = {Consider a finite stage game G that is repeated infinitely often. At each time, the players have hypotheses about their opponents' repeated game strategies. They frequently test their hypotheses against the opponents' recent actions. When a hypothesis fails a test, a new one is adopted. Play is almost rational in the sense that, at each point in time, the players' strategies are -best replies to their beliefs. We show that, at least 1 - of the time t these hypothesis testing strategies constitute an -equilibrium of the repeated game from t on; in fact the strategies are close to being subgame perfect for long stretches of time. This approach solves the problem of learning to play equilibrium with no prior knowledge (even probabilistic knowledge) of the opponents' strategies or their payoffs.  2003 Elsevier Inc. All rights reserved.},
  langid = {english},
  keywords = {lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/3VWIPAHU/Foster et Young - 2003 - Learning, hypothesis testing, and Nash equilibrium.pdf}
}

@article{fosterProofCalibrationBlackwell1999,
  title = {A {{Proof}} of {{Calibration}} via {{Blackwell}}'s {{Approachability Theorem}}},
  author = {Foster, Dean P},
  year = {1999},
  month = oct,
  journal = {Games and Economic Behavior},
  volume = {29},
  number = {1-2},
  pages = {73--78},
  issn = {08998256},
  doi = {10.1006/game.1999.0719},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/WUWXGLYP/Foster - 1999 - A Proof of Calibration via Blackwell's Approachabi.pdf}
}

@article{fosterRandomizationRuleSelecting1993,
  title = {A {{Randomization Rule}} for {{Selecting Forecasts}}},
  author = {Foster, Dean P. and Vohra, Rakesh V.},
  year = {1993},
  month = aug,
  journal = {Operations Research},
  volume = {41},
  number = {4},
  pages = {704--709},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.41.4.704},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/AXDV7F8Z/Foster et Vohra - 1993 - A Randomization Rule for Selecting Forecasts.pdf}
}

@article{fosterRegretOnLineDecision1999,
  title = {Regret in the {{On-Line Decision Problem}}},
  author = {Foster, Dean P. and Vohra, Rakesh},
  year = {1999},
  month = oct,
  journal = {Games and Economic Behavior},
  volume = {29},
  number = {1-2},
  pages = {7--35},
  issn = {08998256},
  doi = {10.1006/game.1999.0740},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/KE7SK4XS/Foster et Vohra - 1999 - Regret in the On-Line Decision Problem.pdf}
}

@article{fosterRegretOnLineDecision1999a,
  title = {Regret in the {{On-Line Decision Problem}}},
  author = {Foster, Dean P. and Vohra, Rakesh},
  year = {1999},
  month = oct,
  journal = {Games and Economic Behavior},
  volume = {29},
  number = {1-2},
  pages = {7--35},
  issn = {08998256},
  doi = {10.1006/game.1999.0740},
  abstract = {At each point in time a decision maker must make a decision. The payoff in a period from the decision made depends on the decision as well as on the state of the world that obtains at that time. The difficulty is that the decision must be made in advance of any knowledge, even probabilistic, about which state of the world will obtain. A range of problems from a variety of disciplines can be framed in this way. In this paper we survey the main results obtained, as well as some of their applications.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/R7NJLEP2/Foster et Vohra - 1999 - Regret in the On-Line Decision Problem.pdf}
}

@article{fosterRegretTestingLearning2006,
  title = {Regret Testing: Learning to Play {{Nash}} Equilibrium without Knowing You Have an Opponent},
  author = {Foster, Dean P and Young, H Peyton},
  year = {2006},
  journal = {Theoretical Economics},
  pages = {27},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/5IKR5KC7/Foster et Young - 2006 - Regret testing learning to play Nash equilibrium .pdf}
}

@article{fosterSmoothCalibrationLeaky2018,
  title = {Smooth Calibration, Leaky Forecasts, Finite Recall, and {{Nash}} Dynamics},
  author = {Foster, Dean P. and Hart, Sergiu},
  year = {2018},
  month = may,
  journal = {Games and Economic Behavior},
  volume = {109},
  pages = {271--293},
  issn = {08998256},
  doi = {10.1016/j.geb.2017.12.022},
  abstract = {We propose to smooth out the calibration score, which measures how good a forecaster is, by combining nearby forecasts. While regular calibration can be guaranteed only by randomized forecasting procedures, we show that smooth calibration can be guaranteed by deterministic procedures. As a consequence, it does not matter if the forecasts are leaked, i.e., made known in advance: smooth calibration can nevertheless be guaranteed (while regular calibration cannot). Moreover, our procedure has finite recall, is stationary, and all forecasts lie on a finite grid. To construct the procedure, we deal also with the related setups of online linear regression and weak calibration. Finally, we show that smooth calibration yields uncoupled finite-memory dynamics in n-person games\textemdash ``smooth calibrated learning''\textemdash in which the players play approximate Nash equilibria in almost all periods (by contrast, calibrated learning, which uses regular calibration, yields only that the time averages of play are approximate correlated equilibria).},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/Y5BAKL2S/Foster et Hart - 2018 - Smooth calibration, leaky forecasts, finite recall.pdf}
}

@article{fosterStochasticEvolutionaryGame1990,
  title = {Stochastic Evolutionary Game Dynamics},
  author = {Foster, Dean and Young, Peyton},
  year = {1990},
  month = oct,
  journal = {Theoretical Population Biology},
  volume = {38},
  number = {2},
  pages = {219--232},
  issn = {0040-5809},
  doi = {10.1016/0040-5809(90)90011-J},
  abstract = {The concept of an evolutionary stable strategy (ESS) is a useful tool for studying the dynamics of natural selection. One of its limitations, however, is that it does not capture the notion of long-run stability when the system is subjected to stochastic effects. We define the concept of stability in a stochastic dynamical system, and show that it differs from both the traditional ESS and the concept of an attractor in a dynamical system. The stochastically stable set may be computed analytically using recent advances in potential theory.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/45UPEDJG/6519056.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/8R4EFRXP/Foster et Young - 1990 - Stochastic evolutionary game dynamics∗.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/59GMTEYG/004058099090011J.html}
}

@article{fournierHotellingGamesNetworks2014,
  title = {Hotelling {{Games}} on {{Networks}}: {{Efficiency}} of {{Equilibria}}},
  shorttitle = {Hotelling {{Games}} on {{Networks}}},
  author = {Fournier, Gaatan and Scarsini, Marco},
  year = {2014},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.2423345},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/9TEDWM9Z/Fournier et Scarsini - 2014 - Hotelling Games on Networks Efficiency of Equilib.pdf}
}

@article{franciGametheoreticApproachGenerative2020,
  title = {A Game-Theoretic Approach for {{Generative Adversarial Networks}}},
  author = {Franci, Barbara and Grammatico, Sergio},
  year = {2020},
  month = sep,
  journal = {arXiv:2003.13637 [cs, math, stat]},
  eprint = {2003.13637},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Generative adversarial networks (GANs) are a class of generative models, known for producing accurate samples. The key feature of GANs is that there are two antagonistic neural networks: the generator and the discriminator. The main bottleneck for their implementation is that the neural networks are very hard to train. One way to improve their performance is to design reliable algorithms for the adversarial process. Since the training can be cast as a stochastic Nash equilibrium problem, we rewrite it as a variational inequality and introduce an algorithm to compute an approximate solution. Specifically, we propose a stochastic relaxed forward\textendash backward algorithm for GANs. We prove that when the pseudogradient mapping of the game is monotone, we have convergence to an exact solution or in a neighbourhood of it.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/AJTSALL3/Franci et Grammatico - 2020 - A game-theoretic approach for Generative Adversari.pdf}
}

@book{freidlinRandomPerturbationsDynamical1998,
  title = {Random Perturbations of Dynamical Systems},
  author = {Fre{\u \i}dlin, M. I. and Wentzell, Alexander D. and Wentzell, Alexander D.},
  year = {1998},
  series = {Grundlehren Der Mathematischen {{Wissenschaften}}},
  edition = {2nd ed},
  number = {260},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-98362-2},
  langid = {english},
  lccn = {QA274 .F73 1998},
  keywords = {Perturbation (Mathematics),Stochastic processes},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/7UQF82DM/Freĭdlin et al. - 1998 - Random perturbations of dynamical systems.pdf}
}

@article{freundAdaptiveGamePlaying1999,
  title = {Adaptive {{Game Playing Using Multiplicative Weights}}},
  author = {Freund, Yoav and Schapire, Robert E.},
  year = {1999},
  month = oct,
  journal = {Games and Economic Behavior},
  volume = {29},
  number = {1-2},
  pages = {79--103},
  issn = {08998256},
  doi = {10.1006/game.1999.0738},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/JYQGQWXV/Freund et Schapire - 1999 - Adaptive Game Playing Using Multiplicative Weights.pdf}
}

@article{fudenbergConditionalUniversalConsistency1999,
  title = {Conditional {{Universal Consistency}}},
  author = {Fudenberg, Drew and Levine, David K.},
  year = {1999},
  month = oct,
  journal = {Games and Economic Behavior},
  volume = {29},
  number = {1-2},
  pages = {104--130},
  issn = {08998256},
  doi = {10.1006/game.1998.0705},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/6TE7Q5MB/Fudenberg et Levine - 1999 - Conditional Universal Consistency.pdf}
}

@article{fudenbergConsistencyCautiousFictitious1995,
  title = {Consistency and Cautious Fictitious Play},
  author = {Fudenberg, Drew and Levine, David K.},
  year = {1995},
  month = jul,
  journal = {Journal of Economic Dynamics and Control},
  volume = {19},
  number = {5-7},
  pages = {1065--1089},
  issn = {01651889},
  doi = {10.1016/0165-1889(94)00819-4},
  abstract = {We study a variation of fictitious play, in which the probability of each action is an exponential function of that action's utility against the historical frequency of opponents' play. Regardless of the opponents' strategies, the utility received by an agent using this rule is nearly the best that could be achieved against the historical frequency. Such rules are approximately optimal in i.i.d. environments, and guarantee nearly the minmax regardless of opponents' behavior. Fictitious play shares these properties provided it switches `infrequently' between actions. We also study the long-run outcomes when all players use consistent and cautious rules.},
  langid = {english},
  keywords = {lu,preuve pas lue},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/2JTJHHBJ/Fudenberg et Levine - 1995 - Consistency and cautious fictitious play.pdf}
}

@article{fudenbergEasierWayCalibrate1999,
  title = {An {{Easier Way}} to {{Calibrate}}},
  author = {Fudenberg, Drew and Levine, David K.},
  year = {1999},
  month = oct,
  journal = {Games and Economic Behavior},
  volume = {29},
  number = {1-2},
  pages = {131--137},
  issn = {08998256},
  doi = {10.1006/game.1999.0726},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/CACMPX57/Fudenberg et Levine - 1999 - An Easier Way to Calibrate.pdf}
}

@misc{fudenbergLearningExtensiveformGames1994,
  title = {Learning in Extensive-Form Games: Experimentation and {{Nash}} Equilibrium},
  author = {Fudenberg, Drew and Kreps, David},
  year = {1994},
  month = jun,
  howpublished = {http://economics.mit.edu/files/11903},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/2DS56J2J/11903.pdf}
}

@article{fudenbergLearningMixedEquilibria1993,
  title = {Learning {{Mixed Equilibria}}},
  author = {Fudenberg, Drew and Kreps, David M.},
  year = {1993},
  month = jul,
  journal = {Games and Economic Behavior},
  volume = {5},
  number = {3},
  pages = {320--367},
  issn = {0899-8256},
  doi = {10.1006/game.1993.1021},
  abstract = {We study learning processes for finite strategic-form games, in which players use the history of past play to forecast play in the current period. In a generalization of fictitious play, we assume only that players asymptotically choose best responses to the historical frequencies of opponents{${'}$} past play. This implies that if the stage-game strategies converge, the limit is a Nash equilibrium. In the basic model, plays seems unlikely to converge to a mixed-strategy equilibrium, but such convergence is natural when the stage game is perturbed in the manner of Harsanyi{${'}$}s purification theorem. Journal of Economic Literature Classification Number: C72.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/YQCA89P3/Fudenberg et Kreps - 1993 - Learning Mixed Equilibria.pdf}
}

@book{fudenbergTheoryLearningGames1998,
  title = {The Theory of Learning in Games},
  author = {Fudenberg, Drew and Levine, David K.},
  year = {1998},
  series = {{{MIT Press}} Series on Economic Learning and Social Evolution},
  number = {2},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-06194-0},
  lccn = {QA269 .F83 1998},
  keywords = {Game theory,lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/3RE7ZXXT/The Theory of Learning in Games.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/C5ZKHLHV/Fudenberg et Levine - 1998 - The theory of learning in games.pdf}
}

@book{fullbrookGuideWhatWrong2004,
  title = {A {{Guide}} to {{What}}'s {{Wrong}} with {{Economics}}},
  editor = {Fullbrook, Edward},
  year = {2004},
  publisher = {{Anthem Press}},
  abstract = {A prescient examination of the serious faults and pitfalls of neoclassical economics.},
  isbn = {978-1-84331-148-5}
}

@misc{funderbergGameTheory1991,
  title = {Game {{Theory}}},
  author = {Funderberg, Drew and Tirole, Jean},
  year = {1991},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/EWQJWDJ4/Drew Fudenberg, Jean Tirole - Game Theory (1991, The MIT Press).pdf}
}

@article{gabszewiczMicroeconomicTheoriesImperfect2000,
  title = {Microeconomic Theories of Imperfect Competition},
  author = {Gabszewicz, Jean and Thisse, Jacques-Fran{\c c}ois},
  year = {2000},
  journal = {Cahiers d'\'economie politique},
  volume = {37},
  number = {1},
  pages = {47--99},
  issn = {0154-8344},
  doi = {10.3406/cep.2000.1288},
  abstract = {This paper aims at providing an overview of what has been accomplished in the economics of imperfect competition. Our starting point is that imperfect competition arises when at least one of the four traditional assumptions of perfect competition is violated. In our review, we neglect the monopoly case and offer a brief survey of general equilibrium model with imperfect competition.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/D5EDBIBF/Gabszewicz et Thisse - 2000 - Microeconomic theories of imperfect competition.pdf}
}

@article{gaitondeStabilityLearningStrategic2020,
  title = {Stability and {{Learning}} in {{Strategic Queuing Systems}}},
  author = {Gaitonde, Jason and Tardos, Eva},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.07009 [cs, math]},
  eprint = {2003.07009},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {Bounding the price of anarchy, which quantifies the damage to social welfare due to selfish behavior of the participants, has been an important area of research. In this paper, we study this phenomenon in the context of a game modeling queuing systems: routers compete for servers, where packets that do not get service will be resent at future rounds, resulting in a system where the number of packets at each round depends on the success of the routers in the previous rounds. We model this as an (infinitely) repeated game, where the system holds a state (number of packets held by each queue) that arises from the results of the previous round. We assume that routers satisfy the no-regret condition, e.g. they use learning strategies to identify the server where their packets get the best service. Classical work on repeated games makes the strong assumption that the subsequent rounds of the repeated games are independent (beyond the influence on learning from past history). The carryover effect caused by packets remaining in this system makes learning in our context result in a highly dependent random process. We analyze this random process and find that if the capacity of the servers is high enough to allow a centralized and knowledgeable scheduler to get all packets served even with double the packet arrival rate, and queues use no-regret learning algorithms, then the expected number of packets in the queues will remain bounded throughout time, assuming older packets have priority. This paper is the first to study the effect of selfish learning in a queuing system, where the learners compete for resources, but rounds are not all independent: the number of packets to be routed at each round depends on the success of the routers in the previous rounds.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Mathematics - Probability},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/CX2ZGVQH/Gaitonde et Tardos - 2020 - Stability and Learning in Strategic Queuing System.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/DDV8N4RQ/2003.html}
}

@article{gaitondeVirtuesPatienceStrategic2020,
  title = {Virtues of {{Patience}} in {{Strategic Queuing Systems}}},
  author = {Gaitonde, Jason and Tardos, Eva},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.10205 [cs, math]},
  eprint = {2011.10205},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {We consider the problem of selfish agents in discrete-time queuing systems, where competitive queues try to get their packets served. In this model, a queue gets to send a packet each step to one of the servers, which will attempt to serve the oldest arriving packet, and unprocessed packets are returned to each queue. We model this as a repeated game where queues compete for the capacity of the servers, but where the state of the game evolves as the length of each queue varies, resulting in a highly dependent random process. Earlier work by the authors [EC'20] shows that with no-regret learners, the system needs twice the capacity as would be required in the coordinated setting to ensure queue lengths remain stable despite the selfish behavior of the queues. In this paper, we demonstrate that this way of evaluating outcomes is myopic: if more patient queues choose strategies that selfishly maximize their long-run success rate, stability can be ensured with just \$\textbackslash frac\{e\}\{e-1\}\textbackslash approx 1.58\$ times extra capacity, better than what is possible assuming the no-regret property. As these systems induce highly dependent processes, our analysis draws heavily on techniques from probability theory. Though these systems are random under any fixed policies by the queues, we show that, surprisingly, these systems have deterministic and explicit asymptotic behavior. We show that the asymptotic growth rates of queues can be written as a ratio of a submodular and modular function, which provides significant game-theoretic properties. Our equilibrium analysis then relies on a novel deformation argument towards a more analyzable solution that differs significantly from previous price of anarchy results. While the intermediate points will not be equilibria, this analytic structure will ensure that this deformation is monotonic along this continuous path.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Science and Game Theory,Mathematics - Probability},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/DNJ4U898/Gaitonde et Tardos - 2020 - Virtues of Patience in Strategic Queuing Systems.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/H9X4J8SU/2011.html}
}

@article{ganzfriedFictitiousPlayOutperforms2020,
  title = {Fictitious {{Play Outperforms Counterfactual Regret Minimization}}},
  author = {Ganzfried, Sam},
  year = {2020},
  month = apr,
  journal = {arXiv:2001.11165 [cs, econ]},
  eprint = {2001.11165},
  eprinttype = {arxiv},
  primaryclass = {cs, econ},
  abstract = {We compare the performance of two popular iterative algorithms, fictitious play and counterfactual regret minimization, in approximating Nash equilibrium in multiplayer games. Despite recent success of counterfactual regret minimization in multiplayer poker and conjectures of its superiority, we show that fictitious play leads to improved Nash equilibrium approximation with statistical significance over a variety of game sizes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {lu,peut-être faux,preuve pas lue},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/QP8NTIUJ/Ganzfried - 2020 - Fictitious Play Outperforms Counterfactual Regret .pdf}
}

@article{ganzfriedParallelAlgorithmApproximating2020,
  title = {Parallel {{Algorithm}} for {{Approximating Nash Equilibrium}} in {{Multiplayer Stochastic Games}} with {{Application}} to {{Naval Strategic Planning}}},
  author = {Ganzfried, Sam and Laughlin, Conner and Morefield, Charles},
  year = {2020},
  month = mar,
  journal = {arXiv:1910.00193 [cs, econ]},
  eprint = {1910.00193},
  eprinttype = {arxiv},
  primaryclass = {cs, econ},
  abstract = {Many real-world domains contain multiple agents behaving strategically with probabilistic transitions and uncertain (potentially infinite) duration. Such settings can be modeled as stochastic games. While algorithms have been developed for solving (i.e., computing a game-theoretic solution concept such as Nash equilibrium) two-player zero-sum stochastic games, research on algorithms for non-zero-sum and multiplayer stochastic games is limited. We present a new algorithm for these settings, which constitutes the first parallel algorithm for multiplayer stochastic games. We present experimental results on a 4-player stochastic game motivated by a naval strategic planning scenario, showing that our algorithm is able to quickly compute strategies constituting Nash equilibrium up to a very small degree of approximation error.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Multiagent Systems,Economics - Theoretical Economics},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/FIKTZXNL/Ganzfried et al. - 2020 - Parallel Algorithm for Approximating Nash Equilibr.pdf}
}

@article{garapinRapprocherHomoOeconomicus2009,
  title = {{Rapprocher l'Homo Oeconomicus de l'Homo Sapiens. Vers une th\'eorie des jeux r\'ealiste et pr\'edictive des comportements humains}},
  author = {Garapin, Alexis},
  year = {2009},
  month = oct,
  journal = {Revue d'economie politique},
  volume = {Vol. 119},
  number = {1},
  pages = {1--40},
  publisher = {{Dalloz}},
  issn = {0373-2630},
  abstract = {La th\&\#233;orie des jeux comportementale propose un cadre d\&\#8217;\&\#233;laboration pour une mod\&\#233;lisation g\&\#233;n\&\#233;rale, pr\&\#233;dictive et r\&\#233;aliste des comportements humains en situation d\&\#8217;interactions strat\&\#233;giques. Appuy\&\#233;e sur les r\&\#233;sultats exp\&\#233;rimentaux accumul\&\#233;s depuis vingt cinq ans et, plus r\&\#233;cemment, sur les apports de la neuro\&\#233;conomie, cette approche renouvelle la th\&\#233;orie des jeux sans renoncer \&\#224; ses bases conceptuelles et formelles. L\&\#8217;article pr\&\#233;sente une synth\&\#232;se des apports r\&\#233;cents de cette approche \&\#224; partir de quelques uns de ses th\&\#232;mes centraux\&\#160;: la profondeur de raisonnement strat\&\#233;gique, la coordination, les pr\&\#233;f\&\#233;rences en situation de n\&\#233;gociation et l\&\#8217;apprentissage collectif. On montre, au-del\&\#224; de la vari\&\#233;t\&\#233; des questions trait\&\#233;es, la coh\&\#233;rence et l\&\#8217;unit\&\#233; de l\&\#8217;approche ainsi que son potentiel.},
  langid = {french},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/8GVDDR8J/Garapin - 2009 - Rapprocher l'Homo Oeconomicus de l'Homo Sapiens. V.pdf}
}

@article{gaulyFutureConsequencesChallenges2013,
  title = {Future Consequences and Challenges for Dairy Cow Production Systems Arising from Climate Change in {{Central Europe}} \textendash{} a Review},
  author = {Gauly, M. and Bollwein, H. and Breves, G. and Br{\"u}gemann, K. and D{\"a}nicke, S. and Da{\c s}, G. and Demeler, J. and Hansen, H. and Isselstein, J. and K{\"o}nig, S. and Loh{\"o}lter, M. and Martinsohn, M. and Meyer, U. and Potthoff, M. and Sanker, C. and Schr{\"o}der, B. and Wrage, N. and Meibaum, B. and {von Samson-Himmelstjerna}, G. and Stinshoff, H. and Wrenzycki, C.},
  year = {2013},
  month = jan,
  journal = {Animal},
  volume = {7},
  number = {5},
  pages = {843--859},
  issn = {1751-7311},
  doi = {10.1017/S1751731112002352},
  abstract = {It is well documented that global warming is unequivocal. Dairy production systems are considered as important sources of greenhouse gas emissions; however, little is known about the sensitivity and vulnerability of these production systems themselves to climate warming. This review brings different aspects of dairy cow production in Central Europe into focus, with a holistic approach to emphasize potential future consequences and challenges arising from climate change. With the current understanding of the effects of climate change, it is expected that yield of forage per hectare will be influenced positively, whereas quality will mainly depend on water availability and soil characteristics. Thus, the botanical composition of future grassland should include species that are able to withstand the changing conditions (e.g. lucerne and bird's foot trefoil). Changes in nutrient concentration of forage plants, elevated heat loads and altered feeding patterns of animals may influence rumen physiology. Several promising nutritional strategies are available to lower potential negative impacts of climate change on dairy cow nutrition and performance. Adjustment of feeding and drinking regimes, diet composition and additive supplementation can contribute to the maintenance of adequate dairy cow nutrition and performance. Provision of adequate shade and cooling will reduce the direct effects of heat stress. As estimated genetic parameters are promising, heat stress tolerance as a functional trait may be included into breeding programmes. Indirect effects of global warming on the health and welfare of animals seem to be more complicated and thus are less predictable. As the epidemiology of certain gastrointestinal nematodes and liver fluke is favourably influenced by increased temperature and humidity, relations between climate change and disease dynamics should be followed closely. Under current conditions, climate change associated economic impacts are estimated to be neutral if some form of adaptation is integrated. Therefore, it is essential to establish and adopt mitigation strategies covering available tools from management, nutrition, health and plant and animal breeding to cope with the future consequences of climate change on dairy farming.},
  langid = {english},
  keywords = {cow comfort,functional traits,global warming,heat stress,heat tolerance,lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/GAV793LV/Gauly et al. - 2013 - Future consequences and challenges for dairy cow p.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/7DBSLVYH/S1751731112002352.html}
}

@article{geddesGameTheoreticModel1991,
  title = {A {{Game Theoretic Model}} of {{Reform}} in {{Latin American Democracies}}},
  author = {Geddes, Barbara},
  year = {1991},
  month = jun,
  journal = {American Political Science Review},
  volume = {85},
  number = {2},
  pages = {371--392},
  issn = {0003-0554, 1537-5943},
  doi = {10.2307/1963165},
  abstract = {In this article I develop a simple game-theoretic model of administrative reform in Latin American democracies. The model, which is based on the incentives facing the politicians who must initiate reforms if any are to occur, yields two predictions: (1) reforms are more likely to pass the legislative hurdle when patronage is evenly distributed among the strongest parties, and (2) initial reforms are more likely to be followed by further extensions of reform where the electoral weight of the top parties remains relatively even and stable. Attention to the incentives facing legislators and party leaders also results in the expectation that certain political institutions, such as open list proportional representation and electoral rules that minimize party control over candidate lists, reduce the probability of reform. I test these predictions and expectations on a set of Latin American democracies and find them consistent with historical events.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/A3CFQX5I/Geddes - 1991 - A Game Theoretic Model of Reform in Latin American.pdf}
}

@article{georgalosNashCoarseCorrelation2020,
  title = {Nash versus Coarse Correlation},
  author = {Georgalos, Konstantinos and Ray, Indrajit and SenGupta, Sonali},
  year = {2020},
  month = dec,
  journal = {Experimental Economics},
  volume = {23},
  number = {4},
  pages = {1178--1204},
  issn = {1386-4157, 1573-6938},
  doi = {10.1007/s10683-020-09647-x},
  abstract = {We run a laboratory experiment to test the concept of coarse correlated equilibrium (Moulin and Vial in Int J Game Theory 7:201\textendash 221, 1978), with a two-person game with unique pure Nash equilibrium which is also the solution of iterative elimination of strictly dominated strategies. The subjects are asked to commit to a device that randomly picks one of three symmetric outcomes (including the Nash point) with higher ex-ante expected payoff than the Nash equilibrium payoff. We find that the subjects do not accept this lottery (which is a coarse correlated equilibrium); instead, they choose to play the game and coordinate on the Nash equilibrium. However, given an individual choice between a lottery with equal probabilities of the same outcomes and the sure payoff as in the Nash point, the lottery is chosen by the subjects. This result is robust against a few variations. We explain our result as selecting risk-dominance over payoff dominance in equilibrium.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/5H6W5KYM/Georgalos et al. - 2020 - Nash versus coarse correlation.pdf}
}

@article{gerringPoliticalInstitutionsCorruption2004,
  title = {Political {{Institutions}} and {{Corruption}}: {{The Role}} of {{Unitarism}} and {{Parliamentarism}}},
  shorttitle = {Political {{Institutions}} and {{Corruption}}},
  author = {Gerring, John and Thacker, Strom C.},
  year = {2004},
  month = apr,
  journal = {British Journal of Political Science},
  volume = {34},
  number = {2},
  pages = {295--330},
  issn = {0007-1234, 1469-2112},
  doi = {10.1017/S0007123404000067},
  abstract = {A raft of new research on the causes and effects of political corruption has emerged in recent years, in tandem with a separate, growing focus on the effects of political institutions on important outcomes such as economic growth, social equality and political stability. Yet we know little about the possible role of different political institutional arrangements on political corruption. This article examines the impact of territorial sovereignty (unitary or federal) and the composition of the executive (parliamentary or presidential) on levels of perceived political corruption cross-nationally. We find that unitary and parliamentary forms of government help reduce levels of corruption. To explain this result, we explore a series of seven potential causal mechanisms that emerge out of the competing centralist and decentralist theoretical paradigms: (1) openness, transparency and information costs, (2) intergovernmental competition, (3) localism, (4) party competition, (5) decision rules, (6) collective action problems, and (7) public administration. Our empirical findings and our analysis of causal mechanisms suggest that centralized constitutions help foster lower levels of political corruption.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/EJ59V8CS/Gerring et Thacker - 2004 - Political Institutions and Corruption The Role of.pdf}
}

@article{gerstungEvolutionaryGamesAffine2011,
  title = {Evolutionary {{Games}} with {{Affine Fitness Functions}}: {{Applications}} to {{Cancer}}},
  shorttitle = {Evolutionary {{Games}} with {{Affine Fitness Functions}}},
  author = {Gerstung, Moritz and Nakhoul, Hani and Beerenwinkel, Niko},
  year = {2011},
  month = aug,
  journal = {Dynamic Games and Applications},
  volume = {1},
  number = {3},
  pages = {370},
  issn = {2153-0793},
  doi = {10.1007/s13235-011-0029-0},
  abstract = {We analyze the dynamics of evolutionary games in which fitness is defined as an affine function of the expected payoff and a constant contribution. The resulting inhomogeneous replicator equation has an homogeneous equivalent with modified payoffs. The affine terms also influence the stochastic dynamics of a two-strategy Moran model of a finite population. We then apply the affine fitness function in a model for tumor\textendash normal cell interactions to determine which are the most successful tumor strategies. In order to analyze the dynamics of concurrent strategies within a tumor population, we extend the model to a three-strategy game involving distinct tumor cell types as well as normal cells. In this model, interaction with normal cells, in combination with an increased constant fitness, is the most effective way of establishing a population of tumor cells in normal tissue.},
  langid = {english},
  keywords = {Cancer,Evolutionary game theory,Moran process,Prisoner’s Dilemma,Replicator equation,Stroma}
}

@article{gethinApportsLimitesTheorie2018,
  title = {{Apports et limites de la th\'eorie des jeux}},
  author = {Gethin, Amory},
  year = {2018},
  journal = {Regards crois\'es sur l'\'economie},
  volume = {22},
  number = {1},
  pages = {68},
  issn = {1956-7413, 2119-3975},
  doi = {10.3917/rce.022.0068},
  langid = {french},
  keywords = {lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/HXXVWYWF/Gethin - 2018 - Apports et limites de la théorie des jeux.pdf}
}

@article{gibsonRegretMinimizationNonZeroSum2013,
  title = {Regret {{Minimization}} in {{Non-Zero-Sum Games}} with {{Applications}} to {{Building Champion Multiplayer Computer Poker Agents}}},
  author = {Gibson, Richard},
  year = {2013},
  month = apr,
  journal = {arXiv:1305.0034 [cs]},
  eprint = {1305.0034},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In two-player zero-sum games, if both players minimize their average external regret, then the average of the strategy profiles converges to a Nash equilibrium. For n-player general-sum games, however, theoretical guarantees for regret minimization are less understood. Nonetheless, Counterfactual Regret Minimization (CFR), a popular regret minimization algorithm for extensive-form games, has generated winning three-player Texas Hold'em agents in the Annual Computer Poker Competition (ACPC). In this paper, we provide the first set of theoretical properties for regret minimization algorithms in non-zero-sum games by proving that solutions eliminate iterative strict domination. We formally define \textbackslash emph\{dominated actions\} in extensive-form games, show that CFR avoids iteratively strictly dominated actions and strategies, and demonstrate that removing iteratively dominated actions is enough to win a mock tournament in a small poker game. In addition, for two-player non-zero-sum games, we bound the worst case performance and show that in practice, regret minimization can yield strategies very close to equilibrium. Our theoretical advancements lead us to a new modification of CFR for games with more than two players that is more efficient and may be used to generate stronger strategies than previously possible. Furthermore, we present a new three-player Texas Hold'em poker agent that was built using CFR and a novel game decomposition method. Our new agent wins the three-player events of the 2012 ACPC and defeats the winning three-player programs from previous competitions while requiring less resources to generate than the 2011 winner. Finally, we show that our CFR modification computes a strategy of equal quality to our new agent in a quarter of the time of standard CFR using half the memory.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68T37; 68T42; 91A06; 91A18,Computer Science - Computer Science and Game Theory,Computer Science - Multiagent Systems,I.2.1},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ATHKG44J/Gibson - 2013 - Regret Minimization in Non-Zero-Sum Games with App.pdf}
}

@book{gingrasSociologieSciences2017,
  title = {{Sociologie des sciences}},
  author = {Gingras, Yves},
  year = {2017},
  abstract = {"Comment le savoir scientifique se constitue-t-il ? Y a-t-il des facteurs sociaux et culturels qui favorisent le d\'eveloppement des sciences ? Quelles institutions accompagnent ou freinent ce d\'eveloppement ? Comment travaillent les savants ? Comment valident-ils leurs connaissances ?Pourquoi y a-t-il des controverses ? En se penchant sur les rapports entre science et soci\'et\'e, les sociologues des sciences ont interrog\'e la mani\`ere dont la connaissance scientifique se construit. Loin de la figure, tant\^ot fascinante, tant\^ot inqui\'etante, du savant travaillant seul dans son laboratoire, ils nous donnent \`a voir la recherche en sciences dures d'aujourd'hui comme \'etant essentiellement une entreprise collective, souvent transnationale." [Source : 4e de couv.].},
  isbn = {978-2-13-078943-7},
  langid = {french},
  annotation = {OCLC: 972526766},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ML3494QQ/Gingras - 2017 - Sociologie des sciences.pdf}
}

@book{gintisBoundsReasonGame2009,
  title = {The Bounds of Reason: Game Theory and the Unification of the Behavioral Sciences},
  shorttitle = {The Bounds of Reason},
  author = {Gintis, Herbert},
  year = {2009},
  publisher = {{Princeton University Press}},
  address = {{Princeton, N.J}},
  isbn = {978-0-691-14052-0},
  langid = {english},
  lccn = {HB144 .G55 2009},
  keywords = {Game theory,Human behavior,Methodology,Practical reason,Psychology,Social sciences},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/RESYLDGW/Gintis - 2009 - The bounds of reason game theory and the unificat.pdf}
}

@book{gintisGameTheoryEvolving2009,
  title = {Game Theory Evolving: A Problem-Centered Introduction to Modeling Strategic Interaction},
  shorttitle = {Game Theory Evolving},
  author = {Gintis, Herbert},
  year = {2009},
  edition = {2nd ed},
  publisher = {{Princeton University Press}},
  address = {{Princeton}},
  abstract = {This revised edition contains new material \& shows students how to apply game theory to model human behaviour in ways that reflect the special nature of sociality \& individuality. It continues its in-depth look at cooperation in teams, agent-based simulations, experimental economics, \& the evolution \& diffusion of preferences},
  isbn = {978-0-691-14050-6 978-0-691-14051-3},
  langid = {english},
  lccn = {HB144 .G56 2009},
  keywords = {Economics; Mathematical,Game theory},
  annotation = {OCLC: ocn244177303},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/R5M8VW8G/Gintis - 2009 - Game theory evolving a problem-centered introduct.pdf}
}

@book{giraudTheorieJeux2000,
  title = {La Th\'eorie Des Jeux},
  author = {Giraud, Ga{\"e}l},
  year = {2000},
  series = {Champs Universit\'e},
  number = {3001},
  publisher = {{Flammarion}},
  address = {{Paris}},
  isbn = {978-2-08-083001-2},
  lccn = {QA269 .G52 2000},
  keywords = {Game theory,Games of strategy (Mathematics)},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/NINUM66P/Gael Giraud - La theorie des jeux (2000).pdf}
}

@article{godinPourquoiAtilAchats2020,
  title = {{Pourquoi y a-t-il des achats massifs de papier toilette ?}},
  author = {Godin, Romaric},
  year = {2020},
  month = mar,
  journal = {M\'ediapart},
  pages = {6},
  langid = {french},
  keywords = {lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/8KHIB8HT/Godin - Pourquoi y a-t-il des achats massifs de papier toi.pdf}
}

@inproceedings{goldbergApproximationPerformanceFictitious2011,
  title = {On the {{Approximation Performance}} of {{Fictitious Play}} in {{Finite Games}}},
  booktitle = {Algorithms \textendash{} {{ESA}} 2011},
  author = {Goldberg, Paul W. and Savani, Rahul and S{\o}rensen, Troels Bjerre and Ventre, Carmine},
  editor = {Demetrescu, Camil and Halld{\'o}rsson, Magn{\'u}s M.},
  year = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {93--105},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-23719-5_9},
  abstract = {We study the performance of Fictitious Play, when used as a heuristic for finding an approximate Nash equilibrium of a two-player game. We exhibit a class of two-player games having payoffs in the range [0,1] that show that Fictitious Play fails to find a solution having an additive approximation guarantee significantly better than 1/2. Our construction shows that for n\texttimes n games, in the worst case both players may perpetually have mixed strategies whose payoffs fall short of the best response by an additive quantity 1/2 - O(1/n 1 - {$\delta$} ) for arbitrarily small {$\delta$}. We also show an essentially matching upper bound of 1/2 - O(1/n).},
  isbn = {978-3-642-23719-5},
  langid = {english},
  keywords = {Adjacent Block,Approximation Performance,Mixed Strategy,Nash Equilibrium,Pure Strategy},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/42SVS7QC/Goldberg et al. - 2011 - On the Approximation Performance of Fictitious Pla.pdf}
}

@article{gomesDynamicAnalysisMultiagent2009,
  title = {Dynamic {{Analysis}} of {{Multiagent Q-learning}} with -Greedy {{Exploration}}},
  author = {Gomes, Eduardo Rodrigues and Kowalczyk, Ryszard},
  year = {2009},
  pages = {8},
  abstract = {The development of mechanisms to understand and model the expected behaviour of multiagent learners is becoming increasingly important as the area rapidly find application in a variety of domains. In this paper we present a framework to model the behaviour of Q-learning agents using the \k{o}-greedy exploration mechanism. For this, we analyse a continuous-time version of the Q-learning update rule and study how the presence of other agents and the \k{o}-greedy mechanism affect it. We then model the problem as a system of difference equations which is used to theoretically analyse the expected behaviour of the agents. The applicability of the framework is tested through experiments in typical games selected from the literature.},
  langid = {english},
  keywords = {classé},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/MJX89FC2/Gomes et Kowalczyk - ǫ Dynamic Analysis of Multiagent Q-learning with -.pdf}
}

@article{gourvesCongestionGamesCapacitated2015,
  title = {Congestion {{Games}} with {{Capacitated Resources}}},
  author = {Gourv{\`e}s, Laurent and Monnot, J{\'e}r{\^o}me and Moretti, Stefano and Thang, Nguyen Kim},
  year = {2015},
  month = oct,
  journal = {Theory of Computing Systems},
  volume = {57},
  number = {3},
  pages = {598--616},
  issn = {1432-4350, 1433-0490},
  doi = {10.1007/s00224-014-9541-0},
  abstract = {The players of a congestion game interact by allocating bundles of resources from a common pool. This type of games leads to well studied models for analyzing strategic situations, including networks operated by uncoordinated selfish users. Congestion games constitute a subclass of potential games, meaning that a pure Nash equilibrium emerges from a myopic process where the players iteratively react by switching to a strategy that diminishes their individual cost. With the aim of covering more applications, for instance in communication networks, we extend congestion games to the setting where every resource is endowed with a capacity which possibly limits its number of users. From the negative side, we show that a pure Nash equilibrium is not guaranteed to exist in any case and we prove that deciding whether a game possesses a pure Nash equilibrium is NP-complete. Our positive results state that congestion games with capacities are potential games in the well studied singleton case. Polynomial algorithms that compute these equilibria are also provided.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/QLREJ9WT/Gourvès et al. - 2015 - Congestion Games with Capacitated Resources.pdf}
}

@article{gualaBuildingEconomicMachines2001,
  title = {Building Economic Machines: {{The FCC}} Auctions},
  shorttitle = {Building Economic Machines},
  author = {Guala, Francesco},
  year = {2001},
  month = sep,
  journal = {Studies in History and Philosophy of Science Part A},
  volume = {32},
  number = {3},
  pages = {453--477},
  issn = {0039-3681},
  doi = {10.1016/S0039-3681(01)00008-5},
  abstract = {The auctions of the Federal Communication Commission, designed in 1994 to sell spectrum licences, are one of the few widely acclaimed and copied cases of economic engineering to date. This paper includes a detailed narrative of the process of designing, testing and implementing the FCC auctions, focusing in particular on the role played by game theoretical modelling and laboratory experimentation. Some general remarks about the scope, interpretation and use of rational choice models open and conclude the paper.},
  langid = {english},
  keywords = {experiments,methodology of economics,rational choice theory,social mechanisms},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/F6YJ9U96/S0039368101000085.html}
}

@article{guerrienOuEstTheorie2017,
  title = {{O\`u en est la th\'eorie des jeux ?. \`A propos de l'article de Larry Samuelson : \guillemotleft{} Game Theory in Economics and Beyond \guillemotright}},
  shorttitle = {{O\`u en est la th\'eorie des jeux ?}},
  author = {Guerrien, Bernard},
  year = {2017},
  month = dec,
  journal = {Revue de la r\'egulation. Capitalisme, institutions, pouvoirs},
  number = {22},
  publisher = {{Association Recherche \& R\'egulation}},
  issn = {1957-7796},
  abstract = {Dans un article publi\'e fin 2016, Larry Samuelson, un sp\'ecialiste de la th\'eorie des jeux, se livre \`a une critique s\'ev\`ere de la th\'eorie des jeux, du moins dans sa version \guillemotleft{} non coop\'erative \guillemotright. Ni l'\'economie industrielle, version th\'eorie des jeux, ni l'approche dite \guillemotleft{} \'evolutionniste \guillemotright{} ne trouvent gr\^ace \`a ses yeux. Les divers \guillemotleft{} raffinements \guillemotright{} de l'\'equilibre de Nash, destin\'es \`a surmonter le probl\`eme lancinant de la multiplicit\'e de ces \'equilibres, n'a rien fait selon lui pour arranger les choses. Bien au contraire. En r\'ealit\'e, seule l'approche coop\'erative \textendash{} largement n\'eglig\'ee depuis des d\'ecennies \textendash{} lui semble pouvoir encore ouvrir des perspectives int\'eressantes, notamment sur le plan pratique.},
  copyright = {Revue de la r\'egulation est mise \`a disposition selon les termes de la Licence Creative Commons Attribution - Pas d'Utilisation Commerciale - Pas de Modification 4.0 International.},
  langid = {french},
  keywords = {lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/8SVIHQ82/Guerrien - 2017 - Où en est la théorie des jeux . À propos de l’art.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/U5UB74RA/12423.html}
}

@article{gurtnerStrategicAllocationFlight2018,
  title = {Strategic {{Allocation}} of {{Flight Plans}} in {{Air Traffic Management}}: {{An Evolutionary Point}} of {{View}}},
  shorttitle = {Strategic {{Allocation}} of {{Flight Plans}} in {{Air Traffic Management}}},
  author = {Gurtner, G{\'e}rald and Lillo, Fabrizio},
  year = {2018},
  month = dec,
  journal = {Dynamic Games and Applications},
  volume = {8},
  number = {4},
  pages = {799--821},
  issn = {2153-0785, 2153-0793},
  doi = {10.1007/s13235-017-0236-4},
  abstract = {We present a simplified model of the strategic allocation of trajectories in a generic airspace for commercial flights. In this model, two types of companies, characterized by different cost functions and different strategies, compete for the allocation of trajectories in the airspace. With an analytical model and numerical simulations, we show that the relative advantage of the two populations\textemdash companies\textemdash depends on external factors like traffic demand as well as on the composition of the population. We show that there exists a stable equilibrium state which depends on the traffic demand. We also show that the equilibrium solution is not the optimal at the global level, but rather that it tends to favour one of the two business models\textemdash the archetype for low-cost companies. Finally, linking the cost of allocated flights with the fitness of a company, we study the evolutionary dynamics of the system, investigating the fluctuations of population composition around the equilibrium and the speed of convergence towards it. We prove that in the presence of noise due to finite populations, the equilibrium point is shifted and is reached more slowly.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/DLX6MI4G/Gurtner et Lillo - 2018 - Strategic Allocation of Flight Plans in Air Traffi.pdf}
}

@article{Williams1992,
author = {Williams, Ronald J.},
title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
year = {1992},
issue_date = {May 1992},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {3–4},
issn = {0885-6125},
url = {https://doi.org/10.1007/BF00992696},
doi = {10.1007/BF00992696},
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
journal = {Mach. Learn.},
month = {may},
pages = {229–256},
numpages = {28},
keywords = {mathematical analysis, gradient descent, Reinforcement learning, connectionist networks}
}

  
@article{GSSV2013,
title = {On the approximation performance of fictitious play in finite games},
 author = {Goldberg, Paul W. and Rahul, Savani and Troels Bjerre, Sørensen, and Carmine, Ventre},
  year = {2013},
 journal = {Int. J. Game Theory}, 
 number = {42},
  pages = {1059-1083}
 
 }




@article{hadikhanlooFiniteMeanField2019,
  title = {Finite {{Mean Field Games}}: {{Fictitious}} Play and Convergence to a First Order Continuous Mean Field Game},
  shorttitle = {Finite {{Mean Field Games}}},
  author = {Hadikhanloo, Saeed and Silva, Francisco J.},
  year = {2019},
  month = dec,
  journal = {Journal de Math\'ematiques Pures et Appliqu\'ees},
  volume = {132},
  pages = {369--397},
  issn = {00217824},
  doi = {10.1016/j.matpur.2019.02.006},
  abstract = {In this article we consider finite Mean Field Games (MFGs), i.e. with finite time and finite states. We adopt the framework introduced in [16] and study two seemly unexplored subjects. In the first one, we analyze the convergence of the fictitious play learning procedure, inspired by the results in continuous MFGs (see [12] and [20]). In the second one, we consider the relation of some finite MFGs and continuous first order MFGs. Namely, given a continuous first order MFG problem and a sequence of refined space/time grids, we construct a sequence finite MFGs whose solutions admit limit points and every such limit point solves the continuous first order MFG problem.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/S4AIY2KS/Hadikhanloo et Silva - 2019 - Finite Mean Field Games Fictitious play and conve.pdf}
}

@article{hadikhanlooLearningNonatomicGames2021,
  title = {Learning in Nonatomic Games, Part {{I}}: Finite Action Spaces and Population Games},
  author = {Hadikhanloo, Saeed and Laraki, Rida and Mertikopoulos, Panayotis and Sorin, Sylvain},
   doi = {https://arxiv.org/pdf/2107.01595.pdf},
   journal = {Journal of Dynamics and Games, to appear},
  year = {2021},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/BSQBD9GD/Main.pdf}
}

 
 
@article{hakamiLearningStationaryCorrelated2016,
  title = {Learning {{Stationary Correlated Equilibria}} in {{Constrained General-Sum Stochastic Games}}},
  author = {Hakami, Vesal and Dehghan, Mehdi},
  year = {2016},
  month = jul,
  journal = {IEEE Transactions on Cybernetics},
  volume = {46},
  number = {7},
  pages = {1640--1654},
  issn = {2168-2267, 2168-2275},
  doi = {10.1109/TCYB.2015.2453165},
  abstract = {We study constrained general-sum stochastic games with unknown Markovian dynamics. A distributed constrained no-regret Q-learning scheme (CNRQ) is presented to guarantee convergence to the set of stationary correlated equilibria of the game. Prior art addresses the unconstrained case only, is structured with nested control loops, and has no convergence result. CNRQ is cast as a single-loop three-timescale asynchronous stochastic approximation algorithm with set-valued update increments. A rigorous convergence analysis with differential inclusion arguments is given which draws on recent extensions of the theory of stochastic approximation to the case of asynchronous recursive inclusions with set-valued mean fields. Numerical results are given for the exemplary application of CNRQ to decentralized resource control in heterogeneous wireless networks.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/Q36ABSID/Hakami et Dehghan - 2016 - Learning Stationary Correlated Equilibria in Const.pdf}
}

@article{hammersteinPayoffsStrategiesTerritorial1988,
  title = {Payoffs and Strategies in Territorial Contests: {{ESS}} Analyses of Two Ecotypes of the {{spiderAgelenopsis}} Aperta},
  shorttitle = {Payoffs and Strategies in Territorial Contests},
  author = {Hammerstein, Peter and Riechert, Susan E.},
  year = {1988},
  month = apr,
  journal = {Evolutionary Ecology},
  volume = {2},
  number = {2},
  pages = {115--138},
  issn = {0269-7653, 1573-8477},
  doi = {10.1007/BF02067272},
  abstract = {Game-theoretic analyses were completed on the territorial contest behavior of two populations of a desert spider that exhibit markedly different levels of within-species competition. Numerical payoff matrices were constructed from field data collected on the behavior and demography of each population. Payoffs were expressed in terms of expected future egg production. Three behavior patterns that a spider might exhibit following assessment of its weight relative to that of its opponent and the value of the site were considered: withdraw, display, or escalate. The model predicts for the more harsh grassland habitat an evolutionarily stable strategy (ESS) that makes ownership decisive in settling contests between opponents with small weight differences, whereas it otherwise assigns victory to the heavier opponent. Whereas the empirical data collected for this grassland population closely approximates the predicted ESS, that for a population occupying a more favorable riparian habitat deviates significantly. The ESS prediction for this latter population is that an intruding spider will withdraw from a contest if it is similar in weight to the web-owner. Withdrawal is common in this population, but so are display and threat and these actions were not predicted. We hypothesize that gene flow from surrounding habitats is preventing the riparian population from completely adapting to its local environment.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/YCCLBDD6/Hammerstein et Riechert - 1988 - Payoffs and strategies in territorial contests ES.pdf}
}

@book{HandbookGameTheory,
  title = {Handbook of {{Game Theory}}},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/2H7L5LSV/Chapter-30-Voting-procedur_1994_Handbook-of-Game-Theory-with-Economic-Applic.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/2PYQJBWW/Young - 1994 - Chapter 34 Cost allocation.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/3KMJWSEM/Chapter-37-Coalition-struct_1994_Handbook-of-Game-Theory-with-Economic-Appli.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/4PAPRXPQ/Gabszewicz et Shitovitz - 1992 - Chapter 15 The core in imperfectly competitive eco.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/7EIJWZWL/Introduction-to-the-seri_1994_Handbook-of-Game-Theory-with-Economic-Applicat.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/845JMIKI/Fishburn - 1994 - Chapter 39 Utility and subjective probability.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/85Y4XV7J/Peleg et al. - Axiomatizations of the Core.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/9UTLF3YW/Chapter-29-Game-theory-models-of-p_1994_Handbook-of-Game-Theory-with-Economi.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/9VJ63GJP/Index_1994_Handbook-of-Game-Theory-with-Economic-Applications.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/9ZSNNDPC/Chapter-31-Social-choic_1994_Handbook-of-Game-Theory-with-Economic-Applicati.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/APAV56I8/Chapter-28-Game-theory-and-evoluti_1994_Handbook-of-Game-Theory-with-Economi.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/CDQG5IIF/Chapter-25-Signalling_1994_Handbook-of-Game-Theory-with-Economic-Application.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/E3T59D2C/Chapter-20-Zero-sum-two-person_1994_Handbook-of-Game-Theory-with-Economic-Ap.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/GJAB2HVS/Clemhout et Wan - 1994 - Chapter 23 Differential games — Economic applicati.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/JEIKVMLT/Preface_1994_Handbook-of-Game-Theory-with-Economic-Applications.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/JETZ5KMI/Chapter-40-Common-knowled_1994_Handbook-of-Game-Theory-with-Economic-Applica.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/JRT7CEB4/Chapter-21-Game-theory-and-sta_1994_Handbook-of-Game-Theory-with-Economic-Ap.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/KW3Q46PM/Chapter-27-Search_1994_Handbook-of-Game-Theory-with-Economic-Applications.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/LAKCLEFB/Chapter-24-Communication--correlated-equil_1994_Handbook-of-Game-Theory-with.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/PK7SJ4SW/Chapter-33-Game-theory-and-public_1994_Handbook-of-Game-Theory-with-Economic.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/PYHG5998/Chapter-35-Cooperative-models-of-_1994_Handbook-of-Game-Theory-with-Economic.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/TG62NX84/Chapter-38-Game-theoretic-aspects-_1994_Handbook-of-Game-Theory-with-Economi.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/UE437DP5/Chapter-32-Power-and-stability-i_1994_Handbook-of-Game-Theory-with-Economic-.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/XBPN5VAR/Chapter-36-Games-in-coalitiona_1994_Handbook-of-Game-Theory-with-Economic-Ap.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/YABABWWC/Chapter-22-Differential-ga_1994_Handbook-of-Game-Theory-with-Economic-Applic.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/ZHEZ72WH/Dutta et Radner - 1994 - Chapter 26 Moral hazard.pdf}
}

@inproceedings{hanDeepFictitiousPlay2020,
  title = {Deep {{Fictitious Play}} for {{Finding Markovian Nash Equilibrium}} in {{Multi-Agent Games}}},
  booktitle = {Mathematical and {{Scientific Machine Learning}}},
  author = {Han, Jiequn and Hu, Ruimeng},
  year = {2020},
  month = aug,
  pages = {221--245},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We propose a deep neural network-based algorithm to identify the Markovian Nash equilibrium of general large \$N\$-player stochastic differential games. Following the idea of fictitious play, we reca...},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/BNVLUEB6/Han et Hu - 2020 - Deep Fictitious Play for Finding Markovian Nash Eq.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/2IST6LA7/han20a.html}
}

@article{hannanApproximationBayesRisk1957,
  title = {Approximation to {{Bayes Risk}} in {{Repeated Play}}},
  author = {Hannan, James},
  year = {1957},
  journal = {Contributions to the Theory of Games},
  volume = {3},
  pages = {97--139},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/QSSHYC6I/Hannan.pdf}
}

@article{harrisonExpectedUtilityTheory1994,
  title = {Expected Utility Theory and the Experiments},
  author = {Harrison, Glenn W.},
  year = {1994},
  month = jun,
  journal = {Empirical Economics},
  volume = {19},
  number = {2},
  pages = {223--253},
  issn = {0377-7332, 1435-8921},
  doi = {10.1007/BF01175873},
  abstract = {The experimental evidence against expected utility theory is, on balance, either uninformative or unconvincing. When one modifies the experiments to mitigate these criticisms the evidence tends to support traditional theory.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ZGMRA9E4/Harrison - 1994 - Expected utility theory and the experiments.pdf}
}

@article{harrisRateConvergenceContinuousTime1998a,
  title = {On the {{Rate}} of {{Convergence}} of {{Continuous-Time Fictitious Play}}},
  author = {Harris, Christopher},
  year = {1998},
  month = feb,
  journal = {Games and Economic Behavior},
  volume = {22},
  number = {2},
  pages = {238--259},
  issn = {08998256},
  doi = {10.1006/game.1997.0582},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/9REMCVTL/Harris - 1998 - On the Rate of Convergence of Continuous-Time Fict.pdf}
}

@book{harsanyiGeneralTheoryEquilibrium1988,
  title = {A General Theory of Equilibrium Selection in Games},
  author = {Harsanyi, John C. and Selten, Reinhard},
  year = {1988},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-08173-3},
  lccn = {HB145 .H38 1988},
  keywords = {Cooperation,Equilibrium (Economics),Game theory,pas lu}
}

@article{hartAdaptiveHeuristics,
  title = {Adaptive {{Heuristics}}},
  author = {Hart', Sergiu},
  pages = {31},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/VAN5KG7K/Hart' - Adaptive Heuristics.pdf}
}

@book{hartCooperationGametheoreticApproaches1997,
  title = {Cooperation: Game-Theoretic Approaches},
  shorttitle = {Cooperation},
  author = {Hart, Sergiu and {Mas-Colell}, Andreu and {North Atlantic Treaty Organization} and {Scientific Affairs Division} and {NATO Advanced Study Institute on Cooperation: Game-Theoretic Approaches}},
  year = {1997},
  publisher = {{Springer}},
  address = {{Berlin; New York}},
  abstract = {Issues relating to the emergence, persistence, and stability of cooperation among social agents of every type are widely recognized to be of paramount importance. They are also analytically difficult and intellectually challenging. This book, arising from a NATO Advanced Study Institute held at SUNY in 1994, is an up-to-date presentation of the contribution of game theory to the subject. The contributors are leading specialists who focus on the problem from the many different angles of game theory, including axiomatic bargaining theory, the Nash program of non-cooperative foundations, game with complete information, repeated and sequential games, bounded rationality methods, evolutionary theory, experimental approaches, and others. Together they offer significant progress in understanding cooperation.},
  isbn = {978-3-642-60454-6},
  langid = {english},
  annotation = {OCLC: 696256410},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/Q6QLVIYX/Hart et al. - 1997 - Cooperation game-theoretic approaches.pdf}
}

@article{hartGeneralClassAdaptive2001,
  title = {A {{General Class}} of {{Adaptive Strategies}}},
  author = {Hart, Sergiu and {Mas-Colell}, Andreu},
  year = {2001},
  month = may,
  journal = {Journal of Economic Theory},
  volume = {98},
  number = {1},
  pages = {26--54},
  issn = {00220531},
  doi = {10.1006/jeth.2000.2746},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/8HVV7KPN/Hart et Mas-Colell - 2001 - A General Class of Adaptive Strategies.pdf}
}

@article{hartSimpleAdaptiveProcedure2000a,
  title = {A {{Simple Adaptive Procedure Leading}} to {{Correlated Equilibrium}}},
  author = {Hart, Sergiu and {Mas-Colell}, Andreu},
  year = {2000},
  journal = {Econometrica},
  volume = {68},
  number = {5},
  pages = {1127--1150},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {0012-9682},
  abstract = {We propose a new and simple adaptive procedure for playing a game: "regret-matching." In this procedure, players may depart from their current play with probabilities that are proportional to measures of regret for not having used other strategies in the past. It is shown that our adaptive procedure guarantees that, with probability one, the empirical distributions of play converge to the set of correlated equilibria of the game.},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ZCRBLG5L/Hart et Mas-Colell - 2000 - A Simple Adaptive Procedure Leading to Correlated .pdf}
}

@book{hartSimpleAdaptiveStrategies2013,
  title = {Simple Adaptive Strategies: From Regret-Matching to Uncoupled Dynamics},
  shorttitle = {Simple Adaptive Strategies},
  author = {Hart, Sergiu and {Mas-Colell}, Andreu},
  year = {2013},
  series = {World {{Scientific}} Series in Economic Theory},
  number = {v. 4},
  publisher = {{World Scientific}},
  address = {{New Jersey}},
  isbn = {978-981-4390-69-9},
  langid = {english},
  lccn = {QA270 .H37 2013},
  keywords = {Games of strategy (Mathematics),Heuristic algorithms},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/WKJDVBUH/Hart et Mas-Colell - 2013 - Simple adaptive strategies from regret-matching t.pdf}
}

@inproceedings{hasseltDeepReinforcementLearning2016,
  title = {Deep Reinforcement Learning with Double Q-Learning},
  booktitle = {Proceedings of the Thirtieth {{AAAI}} Conference on Artificial Intelligence},
  author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
  year = {2016},
  series = {{{AAAI}}'16},
  pages = {2094--2100},
  publisher = {{AAAI Press}},
  address = {{Phoenix, Arizona}},
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.}
}

@article{heinrichFictitiousSelfPlayExtensiveForm2015,
  title = {Fictitious {{Self-Play}} in {{Extensive-Form Games}}},
  author = {Heinrich, Johannes and Lanctot, Marc},
  year = {2015},
  pages = {9},
  abstract = {Fictitious play is a popular game-theoretic model of learning in games. However, it has received little attention in practical applications to large problems. This paper introduces two variants of fictitious play that are implemented in behavioural strategies of an extensive-form game. The first variant is a full-width process that is realization equivalent to its normal-form counterpart and therefore inherits its convergence guarantees. However, its computational requirements are linear in time and space rather than exponential. The second variant, Fictitious Self-Play, is a machine learning framework that implements fictitious play in a sample-based fashion. Experiments in imperfect-information poker games compare our approaches and demonstrate their convergence to approximate Nash equilibria.},
  langid = {english},
  keywords = {lu,preuve pas lue},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/VWI53RYS/Heinrich et Lanctot - Fictitious Self-Play in Extensive-Form Games.pdf}
}

@article{heMinimaxOptimalReinforcement2020,
  title = {Minimax {{Optimal Reinforcement Learning}} for {{Discounted MDPs}}},
  author = {He, Jiafan and Zhou, Dongruo and Gu, Quanquan},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.00587 [cs, math, stat]},
  eprint = {2010.00587},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {We study the reinforcement learning problem for discounted Markov Decision Processes (MDPs) in the tabular setting. We propose a model-based algorithm named UCBVI-\$\textbackslash gamma\$, which is based on the optimism in the face of uncertainty principle and the Bernstein-type bonus. It achieves \$\textbackslash tilde\{O\}\textbackslash big(\{\textbackslash sqrt\{SAT\}\}/\{(1-\textbackslash gamma)\^\{1.5\}\}\textbackslash big)\$ regret, where \$S\$ is the number of states, \$A\$ is the number of actions, \$\textbackslash gamma\$ is the discount factor and \$T\$ is the number of steps. In addition, we construct a class of hard MDPs and show that for any algorithm, the expected regret is at least \$\textbackslash tilde\{\textbackslash Omega\}\textbackslash big(\{\textbackslash sqrt\{SAT\}\}/\{(1-\textbackslash gamma)\^\{1.5\}\}\textbackslash big)\$. Our upper bound matches the minimax lower bound up to logarithmic factors, which suggests that UCBVI-\$\textbackslash gamma\$ is near optimal for discounted MDPs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/DXFYDBKF/He et al. - 2020 - Minimax Optimal Reinforcement Learning for Discoun.pdf}
}

@article{hendonFictitiousPlayExtensive1996,
  title = {Fictitious {{Play}} in {{Extensive Form Games}}},
  author = {Hendon, Ebbe and Jacobsen, Hans J{\o}rgen and Sloth, Birgitte},
  year = {1996},
  month = aug,
  journal = {Games and Economic Behavior},
  volume = {15},
  number = {2},
  pages = {177--202},
  issn = {08998256},
  doi = {10.1006/game.1996.0065},
  langid = {english},
  keywords = {preuve pas lue},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/8LJR2G87/Hendon et al. - 1996 - Fictitious Play in Extensive Form Games.pdf}
}

@article{henshawEconomicsEggTrading2014,
  title = {The {{Economics}} of {{Egg Trading}}: {{Mating Rate}}, {{Sperm Competition}} and {{Positive Frequency-Dependence}}},
  shorttitle = {The {{Economics}} of {{Egg Trading}}},
  author = {Henshaw, Jonathan M. and Jennions, Michael D. and Kokko, Hanna},
  year = {2014},
  month = dec,
  journal = {Dynamic Games and Applications},
  volume = {4},
  number = {4},
  pages = {379--390},
  issn = {2153-0793},
  doi = {10.1007/s13235-014-0107-1},
  abstract = {Egg trading\textemdash the alternating exchange of egg parcels during mating by simultaneous hermaphrodites\textemdash is one of the best-documented examples of reciprocity between non-relatives. By offering eggs only to partners who reciprocate, traders increase their reproductive success in the male role, but at a potential cost of delaying or reducing fertilisation of their own eggs. Although several authors have considered the evolutionary stability of egg trading once it has evolved, little attention has been paid to how egg trading can invade a population in the first place. We begin to tackle this problem by formally showing that egg trading is under positive frequency-dependent selection: once the proportion of traders in a population exceeds a certain threshold, egg trading will go to fixation. We show that if mate encounters occur frequently, then the cost of withholding eggs from unreciprocating partners is reduced, making it easier for egg trading to evolve. In contrast, the presence of opportunistic `streaking', where unpaired individuals join mating pairs but contribute only sperm, makes it more difficult for egg trading to invade. This is because streakers weaken the link between the number of eggs an individual can offer and its male-role reproductive success.},
  langid = {english},
  keywords = {Altruism,Assortative mating,Cooperation,Direct reciprocity,Mate choice}
}

@article{heringsStationaryEquilibriaStochastic2004,
  title = {Stationary Equilibria in Stochastic Games: Structure, Selection, and Computation},
  shorttitle = {Stationary Equilibria in Stochastic Games},
  author = {Herings, P. Jean-Jacques and Peeters, Ronald J. A. P.},
  year = {2004},
  journal = {Journal of Economic Theory},
  volume = {118},
  number = {1},
  pages = {32--60},
  publisher = {{Elsevier}},
  abstract = {No abstract is available for this item.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/JJ26F856/Herings et Peeters - 2004 - Stationary equilibria in stochastic games structu.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/MN2Z6TYG/v118y2004i1p32-60.html}
}

@article{hernandez-lealSurveyLearningMultiagent2019,
  title = {A {{Survey}} of {{Learning}} in {{Multiagent Environments}}: {{Dealing}} with {{Non-Stationarity}}},
  shorttitle = {A {{Survey}} of {{Learning}} in {{Multiagent Environments}}},
  author = {{Hernandez-Leal}, Pablo and Kaisers, Michael and Baarslag, Tim and {de Cote}, Enrique Munoz},
  year = {2019},
  month = mar,
  journal = {arXiv:1707.09183 [cs]},
  eprint = {1707.09183},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The key challenge in multiagent learning is learning a best response to the behaviour of other agents, which may be non-stationary: if the other agents adapt their strategy as well, the learning target moves. Disparate streams of research have approached non-stationarity from several angles, which make a variety of implicit assumptions that make it hard to keep an overview of the state of the art and to validate the innovation and significance of new works. This survey presents a coherent overview of work that addresses opponent-induced non-stationarity with tools from game theory, reinforcement learning and multi-armed bandits. Further, we reflect on the principle approaches how algorithms model and cope with this non-stationarity, arriving at a new framework and five categories (in increasing order of sophistication): ignore, forget, respond to target models, learn models, and theory of mind. A wide range of state-of-the-art algorithms is classified into a taxonomy, using these categories and key characteristics of the environment (e.g., observability) and adaptation behaviour of the opponents (e.g., smooth, abrupt). To clarify even further we present illustrative variations of one domain, contrasting the strengths and limitations of each category. Finally, we discuss in which environments the different approaches yield most merit, and point to promising avenues of future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/43B8EVZG/Hernandez-Leal et al. - 2019 - A Survey of Learning in Multiagent Environments D.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/C6Y3TNR8/1707.html}
}

@article{hilbeEvolutionCooperationStochastic2018,
  title = {Evolution of Cooperation in Stochastic Games},
  author = {Hilbe, Christian and {\v S}imsa, {\v S}t{\v e}p{\'a}n and Chatterjee, Krishnendu and Nowak, Martin A.},
  year = {2018},
  month = jul,
  journal = {Nature},
  volume = {559},
  number = {7713},
  pages = {246--249},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-018-0277-x},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/VAUUHE8J/Hilbe et al. - 2018 - Evolution of cooperation in stochastic games.pdf}
}

@article{hirschmanEconomistsMakePolicies2014,
  title = {Do Economists Make Policies? {{On}} the Political Effects of Economics},
  shorttitle = {Do Economists Make Policies?},
  author = {Hirschman, D. and Berman, E. P.},
  year = {2014},
  month = oct,
  journal = {Socio-Economic Review},
  volume = {12},
  number = {4},
  pages = {779--811},
  issn = {1475-1461, 1475-147X},
  doi = {10.1093/ser/mwu017},
  abstract = {Economics is often described as the most politically influential social science and yet economic advice is often largely irrelevant to prominent policy debates. We draw on literatures in political science, sociology and science and technology studies to explain this apparent contradiction. Existing research suggests that the influence of economics is mediated by local circumstances and meso-level social structures, and that much of it flows through indirect channels. We elaborate three sites of analysis useful for unpacking these influences: the broad professional authority of economics, the institutional position of economists in government, and the role of economics in the cognitive infrastructure of policymaking, including the diffusion of economic styles of reasoning and the establishment of economic policy devices for seeing and deciding.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/FK6NJDES/Hirschman et Berman - 2014 - Do economists make policies On the political effe.pdf}
}

@book{hofbauer_sigmund_1998,
  title = {Evolutionary Games and Population Dynamics},
  author = {Hofbauer, Josef and Sigmund, Karl},
  year = {1998},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9781139173179}
}

@article{hofbauerBestResponseDynamics2006,
  title = {Best Response Dynamics for Continuous Zero--Sum Games},
  author = {Hofbauer, Josef and Sorin, Sylvain and {,Department of Mathematics, University College London, London WC1E 6BT} and {,Laboratoire d'Econom\'etrie, Ecole Polytechnique, 1 rue Descartes, 75005 Paris}},
  year = {2006},
  journal = {Discrete \& Continuous Dynamical Systems - B},
  volume = {6},
  number = {1},
  pages = {215--224},
  issn = {1553-524X},
  doi = {10.3934/dcdsb.2006.6.215},
  abstract = {We study best response dynamics in continuous time for continuous concave-convex zero-sum games and prove convergence of its trajectories to the set of saddle points, thus providing a dynamical proof of the minmax theorem. Consequences for the corresponding discrete time process with small or diminishing step-sizes are established, including convergence of the fictitious play procedure.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/3TU4YEGX/Hofbauer et al. - 2006 - Best response dynamics for continuous zero--sum ga.pdf}
}

@article{hofbauerGlobalConvergenceStochastic2002,
  title = {On the {{Global Convergence}} of {{Stochastic Fictitious Play}}},
  author = {Hofbauer, Josef and Sandholm, William H.},
  year = {2002},
  journal = {Econometrica},
  volume = {70},
  number = {6},
  pages = {2265--2294},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {0012-9682},
  abstract = {We establish global convergence results for stochastic fictitious play for four classes of games: games with an interior ESS, zero sum games, potential games, and supermodular games. We do so by appealing to techniques from stochastic approximation theory, which relate the limit behavior of a stochastic process to the limit behavior of a differential equation defined by the expected motion of the process. The key result in our analysis of supermodular games is that the relevant differential equation defines a strongly monotone dynamical system. Our analyses of the other cases combine Lyapunov function arguments with a discrete choice theory result: that the choice probabilities generated by any additive random utility model can be derived from a deterministic model based on payoff perturbations that depend nonlinearly on the vector of choice probabilities.},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/N2QD8GUP/Hofbauer et Sandholm - 2002 - On the Global Convergence of Stochastic Fictitious.pdf}
}

@article{hofbauerStableGamesTheir2009,
  title = {Stable Games and Their Dynamics},
  author = {Hofbauer, Josef and Sandholm, William H.},
  year = {2009},
  month = jul,
  journal = {Journal of Economic Theory},
  volume = {144},
  number = {4},
  pages = {1665-1693.e4},
  issn = {00220531},
  doi = {10.1016/j.jet.2009.01.007},
  abstract = {We study a class of population games called stable games. These games are characterized by selfdefeating externalities: when agents revise their strategies, the improvements in the payoffs of strategies to which revising agents are switching are always exceeded by the improvements in the payoffs of strategies which revising agents are abandoning. We prove that the set of Nash equilibria of a stable game is globally asymptotically stable under a wide range of evolutionary dynamics. Convergence results for stable games are not as general as those for potential games: in addition to monotonicity of the dynamics, integrability of the agents' revision protocols plays a key role.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/27QGZP9D/Hofbauer et Sandholm - 2009 - Stable games and their dynamics.pdf}
}

@phdthesis{hollerLearningDynamicsReinforcement2020,
  title = {Learning {{Dynamics}} and {{Reinforcement}} in {{Stochastic Games}}},
  author = {Holler, John Edward},
  year = {2020},
  abstract = {The theory of Reinforcement Learning provides learning algorithms that are guaranteed to converge to optimal behavior in single-agent learning environments. While these algorithms often do not scale well to large problems without modification, a vast amount of recent research has combined them with function approximators with remarkable success in a diverse range of large-scale and complex problems. Motivated by this success in single-agent learning environments, the first half of this work aims to study convergent learning algorithms in multi-agent environments. The theory of multi-agent learning is itself a rich subject, however classically it has confined itself to learning in iterated games where there are no state dynamics. In contrast, this work examines learning in stochastic games, where agents play one another in a temporally extended game that has nontrivial state dynamics. We do so by first defining two classes of stochastic games: Stochastic Potential Games (SPGs) and Global Stochastic Potential Games (GSPGs). We show that both games admit pure Nash equilibria, as well as further refinements of their equilibrium sets. We discuss possible applications of these games in the context of congestion and traffic routing scenarios. Finally, we define learning algorithms that 1. converge to pure Nash equilibria and 2. converge to further refinements of Nash equilibria. In the final chapter we combine a simple type of multi-agent learning - individual Q-learning - with neural networks in order to solve a large scale vehicle routing and assignment problem. Individual Q-learning is a heuristic learning algorithm that, even in small multi-agent problems, does not provide convergence guarantees. Nonetheless, we observe good performance of this algorithm in this setting.},
  langid = {english},
  school = {University of Michigan},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/I2Z45PG6/Holler - Learning Dynamics and Reinforcement in Stochastic .pdf}
}

@article{hopkinsNoteBestResponse1999,
  title = {A {{Note}} on {{Best Response Dynamics}}},
  author = {Hopkins, Ed},
  year = {1999},
  month = oct,
  journal = {Games and Economic Behavior},
  volume = {29},
  number = {1-2},
  pages = {138--150},
  issn = {08998256},
  doi = {10.1006/game.1997.0636},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/NYCSM58R/Hopkins - 1999 - A Note on Best Response Dynamics.pdf}
}

@article{hubertsCapacityChoiceStrategic2015,
  title = {Capacity {{Choice}} in ({{Strategic}}) {{Real Options Models}}: {{A Survey}}},
  shorttitle = {Capacity {{Choice}} in ({{Strategic}}) {{Real Options Models}}},
  author = {Huberts, Nick F. D. and Huisman, Kuno J. M. and Kort, Peter M. and Lavrutich, Maria N.},
  year = {2015},
  month = dec,
  journal = {Dynamic Games and Applications},
  volume = {5},
  number = {4},
  pages = {424--439},
  issn = {2153-0785, 2153-0793},
  doi = {10.1007/s13235-015-0162-2},
  abstract = {The theory of real options determines the optimal time to invest in a project of given size. As a main result, it is found that in a more uncertain environment, it is optimal for a firm to delay its investment. In other words, uncertainty generates a ``value of waiting.'' Recently, contributions appeared that in addition determine the optimal size of the investment. This paper surveys this literature. As a general result, it is obtained that more uncertainty results in larger investments taking place at a later point in time. So, where from the traditional real options literature one can conclude that uncertainty is bad for growth, this is not so clear anymore when also the size of the investment needs to be determined. The survey consists of two parts. First, we present single firm models, and second, we give an overview of the oligopoly models that have appeared up until now.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/FPIZKLNW/Huberts et al. - 2015 - Capacity Choice in (Strategic) Real Options Models.pdf}
}

@article{huDeepFictitiousPlay2020,
  title = {Deep {{Fictitious Play}} for {{Stochastic Differential Games}}},
  author = {Hu, Ruimeng},
  year = {2020},
  month = sep,
  journal = {arXiv:1903.09376 [cs, math, stat]},
  eprint = {1903.09376},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {In this paper, we apply the idea of fictitious play to design deep neural networks (DNNs), and develop deep learning theory and algorithms for computing the Nash equilibrium of asymmetric \$N\$-player non-zero-sum stochastic differential games, for which we refer as \textbackslash emph\{deep fictitious play\}, a multi-stage learning process. Specifically at each stage, we propose the strategy of letting individual player optimize her own payoff subject to the other players' previous actions, equivalent to solve \$N\$ decoupled stochastic control optimization problems, which are approximated by DNNs. Therefore, the fictitious play strategy leads to a structure consisting of \$N\$ DNNs, which only communicate at the end of each stage. The resulted deep learning algorithm based on fictitious play is scalable, parallel and model-free, \{\textbackslash it i.e.\}, using GPU parallelization, it can be applied to any \$N\$-player stochastic differential game with different symmetries and heterogeneities (\{\textbackslash it e.g.\}, existence of major players). We illustrate the performance of the deep learning algorithm by comparing to the closed-form solution of the linear quadratic game. Moreover, we prove the convergence of fictitious play under appropriate assumptions, and verify that the convergent limit forms an open-loop Nash equilibrium. We also discuss the extensions to other strategies designed upon fictitious play and closed-loop Nash equilibrium in the end.},
  archiveprefix = {arXiv},
  keywords = {91A15; 91B50; 91A26; 68T20; 60G99,Computer Science - Computer Science and Game Theory,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/Z3MI4IL9/Hu - 2020 - Deep Fictitious Play for Stochastic Differential G.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/JPIEWA68/1903.html}
}

@article{huDeepFictitiousPlay2020a,
  title = {Deep {{Fictitious Play}} for {{Stochastic Differential Games}}},
  author = {Hu, Ruimeng},
  year = {2020},
  month = sep,
  journal = {arXiv:1903.09376 [cs, math, stat]},
  eprint = {1903.09376},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {In this paper, we apply the idea of fictitious play to design deep neural networks (DNNs), and develop deep learning theory and algorithms for computing the Nash equilibrium of asymmetric \$N\$-player non-zero-sum stochastic differential games, for which we refer as \textbackslash emph\{deep fictitious play\}, a multi-stage learning process. Specifically at each stage, we propose the strategy of letting individual player optimize her own payoff subject to the other players' previous actions, equivalent to solve \$N\$ decoupled stochastic control optimization problems, which are approximated by DNNs. Therefore, the fictitious play strategy leads to a structure consisting of \$N\$ DNNs, which only communicate at the end of each stage. The resulted deep learning algorithm based on fictitious play is scalable, parallel and model-free, \{\textbackslash it i.e.\}, using GPU parallelization, it can be applied to any \$N\$-player stochastic differential game with different symmetries and heterogeneities (\{\textbackslash it e.g.\}, existence of major players). We illustrate the performance of the deep learning algorithm by comparing to the closed-form solution of the linear quadratic game. Moreover, we prove the convergence of fictitious play under appropriate assumptions, and verify that the convergent limit forms an open-loop Nash equilibrium. We also discuss the extensions to other strategies designed upon fictitious play and closed-loop Nash equilibrium in the end.},
  archiveprefix = {arXiv},
  keywords = {91A15; 91B50; 91A26; 68T20; 60G99,Computer Science - Computer Science and Game Theory,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/DWMJ7V6K/Hu - 2020 - Deep Fictitious Play for Stochastic Differential G.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/D8LAZBNY/1903.html}
}

@inproceedings{huMultiagentReinforcementLearning1998,
  title = {Multiagent Reinforcement Learning: {{Theoretical}} Framework and an Algorithm},
  booktitle = {Proceedings of the Fifteenth International Conference on Machine Learning},
  author = {Hu, Junling and Wellman, Michael P.},
  year = {1998},
  series = {{{ICML}} '98},
  pages = {242--250},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  isbn = {1-55860-556-8},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/VIRV8A9T/Hu et Wellman - Multiagent Reinforcement LeAarlgnoinrigthmTheoret.pdf}
}

@article{huNashQlearningGeneralsum2003,
  title = {Nash Q-Learning for General-Sum Stochastic Games},
  author = {Hu, Junling and Wellman, Michael P.},
  year = {2003},
  month = dec,
  journal = {The Journal of Machine Learning Research},
  volume = {4},
  number = {null},
  pages = {1039--1069},
  issn = {1532-4435},
  abstract = {We extend Q-learning to a noncooperative multiagent context, using the framework of general-sum stochastic games. A learning agent maintains Q-functions over joint actions, and performs updates based on assuming Nash equilibrium behavior over the current Q-values. This learning protocol provably converges given certain restrictions on the stage games (defined by Q-values) that arise during learning. Experiments with a pair of two-player grid games suggest that such restrictions on the game structure are not necessarily required. Stage games encountered during learning in both grid environments violate the conditions. However, learning consistently converges in the first grid game, which has a unique equilibrium Q-function, but sometimes fails to converge in the second, which has three different equilibrium Q-functions. In a comparison of offline learning performance in both games, we find agents are more likely to reach a joint optimal path with Nash Q-learning than with a single-agent Q-learning method. When at least one agent adopts Nash Q-learning, the performance of both agents is better than using single-agent Q-learning. We have also implemented an online version of Nash Q-learning that balances exploration with exploitation, yielding improved performance.},
  keywords = {lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/294M4BCP/Hu et Wellman - 2003 - Nash q-learning for general-sum stochastic games.pdf}
}

@article{imhofEvolutionaryCyclesCooperation2005,
  title = {Evolutionary Cycles of Cooperation and Defection},
  shorttitle = {From {{The Cover}}},
  author = {Imhof, L. A. and Fudenberg, D. and Nowak, M. A.},
  year = {2005},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {102},
  number = {31},
  pages = {10797--10800},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0502589102},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/MFPPGLU9/Imhof et al. - 2005 - Evolutionary cycles of cooperation and defection.pdf}
}

@article{ismailiRoutingGamesTime2017,
  title = {Routing {{Games}} over {{Time}} with {{FIFO}} Policy},
  author = {Ismaili, Anisse},
  year = {2017},
  month = aug,
  journal = {arXiv:1709.09484 [cs]},
  eprint = {1709.09484},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We study atomic routing games where every agent travels both along its decided edges and through time. The agents arriving on an edge are first lined up in a first-in-first-out queue and may wait: an edge is associated with a capacity, which defines how many agents-pertime-step can pop from the queue's head and enter the edge, to transit for a fixed delay. We show that the best-response optimization problem is not approximable, and that deciding the existence of a Nash equilibrium is complete for the second level of the polynomial hierarchy. Then, we drop the rationality assumption, introduce a behavioral concept based on GPS navigation, and study its worst-case efficiency ratio to coordination.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computational Complexity,Computer Science - Computer Science and Game Theory},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/6HPLK3CJ/Ismaili - 2017 - Routing Games over Time with FIFO policy.pdf}
}

@article{jafarnia-jahromiLearningZerosumStochastic2021,
  title = {Learning {{Zero-sum Stochastic Games}} with {{Posterior Sampling}}},
  author = {{Jafarnia-Jahromi}, Mehdi and Jain, Rahul and Nayyar, Ashutosh},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.03396 [cs]},
  eprint = {2109.03396},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper, we propose Posterior Sampling Reinforcement Learning for Zero-sum Stochastic Games (PSRL-ZSG), the first online learning {$\surd$}algorithm that achieves Bayesian regret bound of O(HS AT ) in the infinite-horizon zero-sum stochastic games with average-reward criterion. Here H is an upper bound on the span of the bias function, S is the number of states, A is the number of joint actions and T is the horizon. We consider the online setting where the opponent can not be controlled and can take any arbitrary timeadaptive history-dependent {$\surd$}strategy. This improves the best existing regret bound of O( 3 DS2AT 2) by (Wei, Hong, and Lu 2017) under the same assumption and matches the theoretical lower bound in A and T .},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/L22DQUGD/Jafarnia-Jahromi et al. - 2021 - Learning Zero-sum Stochastic Games with Posterior .pdf}
}

@article{jalleConventionCeQue2012,
  title = {{La convention : ce que Lewis doit (ou non) \`a Hume}},
  author = {Jall{\'e}, El{\'e}onore Le},
  year = {2012},
  pages = {33},
  langid = {french},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/5DS6J8S3/Jallé - 2012 - La convention  ce que Lewis doit (ou non) à Hume.pdf}
}

@misc{jindaniEquilibriumSelectionRepeated2020,
  title = {Equilibrium Selection in Repeated Games},
  author = {Jindani, Sam},
  year = {2020},
  month = jan,
  keywords = {lu,preuve pas lue},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/B7JUL9LE/Jindani - 2020 - Equilibrium selection in repeated games.pdf}
}

@article{jinQlearningProvablyEfficient2018,
  title = {Is {{Q-learning Provably Efficient}}?},
  author = {Jin, Chi and {Allen-Zhu}, Zeyuan and Bubeck, Sebastien and Jordan, Michael I.},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.03765 [cs, math, stat]},
  eprint = {1807.03765},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Model-free reinforcement learning (RL) algorithms, such as Q-learning, directly parameterize and update value functions or policies without explicitly modeling the environment. They are typically simpler, more flexible to use, and thus more prevalent in modern deep RL than model-based approaches. However, empirical work has suggested that model-free algorithms may require more samples to learn [7, 22]. The theoretical question of ``whether model-free algorithms can be made sample efficient'' is one of the most fundamental questions in RL, and remains unsolved even in the basic scenario with finitely many states and actions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/W3W7JRAG/Jin et al. - 2018 - Is Q-learning Provably Efficient.pdf}
}

@misc{JohnCacioppoWilliam,
  title = {John {{T}}. {{Cacioppo}}, {{William Patrick}} - {{Loneliness}}\_ {{Human Nature}} and the {{Need}} for {{Social Connection}} (2009).Pdf},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/FRRCWT2C/John T. Cacioppo, William Patrick - Loneliness_ Human Nature and the Need for Social Connection (2009).pdf}
}

@misc{JohnNeumannOskar,
  title = {[{{John}}\_{{Von}}\_{{Neumann}},\_{{Oskar}}\_{{Morgenstern}}]\_{{Theory}}\_of\_{{Ga}}(z-Lib.Org).Pdf},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/NRIHCDXI/[John_Von_Neumann,_Oskar_Morgenstern]_Theory_of_Ga(z-lib.org).pdf}
}

@article{jordanThreeProblemsLearning1993,
  title = {Three {{Problems}} in {{Learning Mixed-Strategy Nash Equilibria}}},
  author = {Jordan, J.S.},
  year = {1993},
  month = jul,
  journal = {Games and Economic Behavior},
  volume = {5},
  number = {3},
  pages = {368--386},
  issn = {08998256},
  doi = {10.1006/game.1993.1022},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/V2FNGJ2F/1-s2.0-S0899825683710225-main.pdf}
}

@article{juditskyUnifyingMirrorDescent2019,
  title = {Unifying Mirror Descent and Dual Averaging},
  author = {Juditsky, Anatoli and Kwon, Joon and Moulines, {\'E}ric},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.13742 [cs, math]},
  eprint = {1910.13742},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {We introduce and analyse a new family of algorithms which generalizes and unifies both the mirror descent and the dual averaging algorithms. The unified analysis of the algorithms involves the introduction of a generalized Bregman divergence which utilizes subgradients instead of gradients. Our approach is general enough to encompass classical settings in convex optimization, online learning, and variational inequalities such as saddle-point problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/3SDRXB6V/Juditsky et al. - 2019 - Unifying mirror descent and dual averaging.pdf}
}

@article{kakadeDeterministicCalibrationNash2008,
  title = {Deterministic Calibration and {{Nash}} Equilibrium},
  author = {Kakade, Sham M. and Foster, Dean P.},
  year = {2008},
  month = feb,
  journal = {Journal of Computer and System Sciences},
  volume = {74},
  number = {1},
  pages = {115--130},
  issn = {00220000},
  doi = {10.1016/j.jcss.2007.04.017},
  abstract = {We provide a natural learning process in which the joint frequency of (time-averaged) empirical play converges into the set of convex combinations of Nash equilibria. Furthermore, the actual distribution of players' actions is close to some (approximate) Nash equilibria on most rounds (on all but a vanishing fraction of the rounds). In this process, all players rationally choose their actions using a public prediction made by a deterministic, weakly calibrated algorithm. For this to be possible, we show that such a deterministic (weakly) calibrated learning algorithm exists.},
  langid = {english},
  keywords = {lu,preuve pas lue},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/P8Y4PH2A/Kakade et Foster - 2008 - Deterministic calibration and Nash equilibrium.pdf}
}

@article{kalaiCalibratedForecastingMerging1999,
  title = {Calibrated {{Forecasting}} and {{Merging}}},
  author = {Kalai, Ehud and Lehrer, Ehud and Smorodinsky, Rann},
  year = {1999},
  month = oct,
  journal = {Games and Economic Behavior},
  volume = {29},
  number = {1-2},
  pages = {151--169},
  issn = {08998256},
  doi = {10.1006/game.1998.0608},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/JPNN2RNQ/Kalai et al. - 1999 - Calibrated Forecasting and Merging.pdf}
}

@article{kalaiRationalLearningLeads1993,
  title = {Rational {{Learning Leads}} to {{Nash Equilibrium}}},
  author = {Kalai, Ehud and Lehrer, Ehud},
  year = {1993},
  month = sep,
  journal = {Econometrica},
  volume = {61},
  number = {5},
  pages = {1019},
  issn = {00129682},
  doi = {10.2307/2951492},
  abstract = {Each of n players, in an infinitely repeated game, starts with subjective beliefs about his opponents' strategies. If the individual beliefs are compatible with the true strategies chosen, then Bayesian updating will lead in the long run to accurate prediction of the future play of the game. It follows that individual players, who know their own payoff matrices and choose strategies to maximize their expected utility, must eventually play according to a Nash equilibrium of the repeated game. An immediate corollary is that, when playing a Harsanyi-Nash equilibrium of a repeated game of incomplete information about opponents' payoff matrices, players will eventually play a Nash equilibrium of the real game, as if they had complete information.},
  langid = {english},
  keywords = {lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/CZEPVE5V/Kalai et Lehrer - 1993 - Rational Learning Leads to Nash Equilibrium.pdf}
}

@article{kalaiSubjectiveGamesEquilibria1995a,
  title = {Subjective Games and Equilibria},
  author = {Kalai, Ehud and Lehrer, Ehud},
  year = {1995},
  month = jan,
  journal = {Games and Economic Behavior},
  volume = {8},
  number = {1},
  pages = {123--163},
  issn = {0899-8256},
  doi = {10.1016/S0899-8256(05)80019-3},
  abstract = {Applying the concepts of Nash, Bayesian, and correlated equilibria to the analysis of strategic interaction requires that players possess objective knowledge of the game and opponents' strategies. Such knowledge is often not available. The proposed notions of subjective games and of subjective Nash and correlated equilibria replace essential but unavailable objective knowledge by subjective assessments. When playing a subjective game repeatedly, subjective optimizers converge to a subjective equilibrium. We apply this approach to some well known examples including single- and multi-person, multi-arm bandit games and repeated Cournot oligopoly games. Journal of Economic Literature Classification Numbers: C73 and C83.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/WHA5T8SI/Kalai et Lehrer - 1995 - Subjective games and equilibria.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/BPT2483Q/S0899825605800193.html}
}

@article{kamraDeepFictitiousPlay,
  title = {Deep {{Fictitious Play}} for {{Games}} with {{Continuous Action Spaces}}},
  author = {Kamra, Nitin and Gupta, Umang and Wang, Kai and Fang, Fei and Liu, Yan and Tambe, Milind},
  pages = {3},
  abstract = {Fictitious play has been a classic algorithm to solve two-player adversarial games with discrete action spaces. In this work we develop an approximate extension of fictitious play to two-player games with high-dimensional continuous action spaces. We use generative neural networks to approximate players' best responses while also learning a differentiable approximate model to the players' rewards given their actions. Both these networks are trained jointly with gradient-based optimization to emulate fictitious play. We explore our approach in zero-sum games, non zero-sum games and security game domains.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ZX762U4A/Kamra et al. - Deep Fictitious Play for Games with Continuous Act.pdf}
}

@inproceedings{kashCombiningNoregretQlearning2020,
  title = {Combining No-Regret and q-Learning},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and {{MultiAgent}} Systems},
  author = {Kash, Ian A. and Sullins, Michael and Hofmann, Katja},
  year = {2020},
  series = {{{AAMAS}} '20},
  pages = {593--601},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  address = {{Richland, SC}},
  abstract = {Counterfactual Regret Minimization (CFR) has found success in settings like poker which have both terminal states and perfect recall. We seek to understand how to relax these requirements. As a first step, we introduce a simple algorithm, local no-regret learning (LONR), which uses a Q-learning-like update rule to allow learning without terminal states or perfect recall. We prove its convergence for the basic case of MDPs (where Q-learning already suffices), as well as limited extensions of them. With a straightforward modification, we extend the basic premise of LONR to work in multi-agent settings and present empirical results showing that it achieves last iterate convergence in a number of settings. Most notably, we show this for NoSDE games, a class of Markov games specifically designed to be impossible for Q-value-based methods to learn and where no prior algorithm is known to achieve convergence to a stationary equilibrium even on average. Furthermore, by leveraging last iterate converging no-regret algorithms (one of which we introduce), we show empirical last iterate convergence in all domains tested with LONR.},
  isbn = {978-1-4503-7518-4},
  keywords = {cfr,markov games,multi-agent,no-regret learning,reinforcement learning},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/T6VGXF3A/Kash et al. - 2020 - Combining No-regret and Q-learning.pdf}
}

@article{khanalAdoptionTechnologyManagement2010,
  title = {Adoption of Technology, Management Practices, and Production Systems in {{US}} Milk Production},
  author = {Khanal, A.R. and Gillespie, J. and MacDonald, J.},
  year = {2010},
  month = dec,
  journal = {Journal of Dairy Science},
  volume = {93},
  number = {12},
  pages = {6012--6022},
  issn = {00220302},
  doi = {10.3168/jds.2010-3425},
  abstract = {The introduction of new technology, management practices, and alternative production systems has resulted in rapid structural change in the US dairy industry. This paper examines adoption rates and adopter characteristics for the following dairy technologies, practices, and systems: holding pen with an udder washer, milking units with automatic take-offs, genetic selection technologies, recombinant bovine somatotropin, membership in the Dairy Herd Improvement Association, computerized feed delivery systems, computerized milking systems, use of a nutritionist to design feed rations, grazing, milking cows 3 times daily, and milking parlors. Four of these were used on a greater percentage of farms in 2005 than in 2000, but increased farm sizes and the interaction of farm size with adoption suggest a greater percentage of milk being produced under each, with the exception of grazing. Except for grazing, technologies were generally complementary.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/FKBZWHS2/Khanal et al. - 2010 - Adoption of technology, management practices, and .pdf}
}

@article{kianercyDynamicsBoltzmannLearning2012,
  title = {Dynamics of {{Boltzmann Q}} Learning in Two-Player Two-Action Games},
  author = {Kianercy, Ardeshir and Galstyan, Aram},
  year = {2012},
  month = apr,
  journal = {Physical Review E},
  volume = {85},
  number = {4},
  pages = {041145},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.85.041145},
  langid = {english},
  keywords = {classé},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/2UPNLVBY/Kianercy et Galstyan - 2012 - Dynamics of Boltzmann Q learning in two-player two.pdf}
}

@article{knezIncreasingCooperationPrisoner2000,
  title = {Increasing {{Cooperation}} in {{Prisoner}}'s {{Dilemmas}} by {{Establishing}} a {{Precedent}} of {{Efficiency}} in {{Coordination Games}}},
  author = {Knez, Marc and Camerer, Colin},
  year = {2000},
  month = jul,
  journal = {Organizational Behavior and Human Decision Processes},
  volume = {82},
  number = {2},
  pages = {194--216},
  issn = {07495978},
  doi = {10.1006/obhd.2000.2882},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/WT9RF54Q/Knez et Camerer - 2000 - Increasing Cooperation in Prisoner's Dilemmas by E.pdf}
}

@article{koellingerRelationshipTechnologyInnovation2008,
  title = {The Relationship between Technology, Innovation, and Firm Performance\textemdash{{Empirical}} Evidence from e-Business in {{Europe}}},
  author = {Koellinger, Philipp},
  year = {2008},
  month = sep,
  journal = {Research Policy},
  volume = {37},
  number = {8},
  pages = {1317--1328},
  issn = {00487333},
  doi = {10.1016/j.respol.2008.04.024},
  abstract = {This article analyzes the relationship between the usage of Internet-based technologies, different types of innovation, and performance at the firm level. Data for the empirical investigation originates from a sample of 7302 European enterprises. The empirical results show that Internet-based technologies were an important enabler of innovation in the year 2003. It was found that all studied types of innovation, including Internet-enabled and non-Internet-enabled product or process innovations, are positively associated with turnover and employment growth. Firms that rely on Internet-enabled innovations are at least as likely to grow as firms that rely on non-Internet-enabled innovations. Finally, it was found that innovative activity is not necessarily associated with higher profitability. Possible reasons for this and implications are discussed.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/Y9Y9ZAWU/Koellinger - 2008 - The relationship between technology, innovation, a.pdf}
}

@article{kondaActorCriticTypeLearning1999,
  title = {Actor-{{Critic--Type Learning Algorithms}} for {{Markov Decision Processes}}},
  author = {Konda, Vijaymohan R. and Borkar, Vivek S.},
  year = {1999},
  month = jan,
  journal = {SIAM Journal on Control and Optimization},
  volume = {38},
  number = {1},
  pages = {94--123},
  issn = {0363-0129, 1095-7138},
  doi = {10.1137/S036301299731669X},
  abstract = {Algorithms for learning the optimal policy of a Markov decision process (MDP) based on simulated transitions are formulated and analyzed. These are variants of the well-known ``actor-critic'' (or ``adaptive critic'') algorithm in the artificial intelligence literature. Distributed asynchronous implementations are considered. The analysis involves two time scale stochastic approximations.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/G66DCMWY/Konda et Borkar - 1999 - Actor-Critic--Type Learning Algorithms for Markov .pdf}
}

@article{kondaConvergenceRateLinear2004,
  title = {Convergence Rate of Linear Two-Time-Scale Stochastic Approximation},
  author = {Konda, Vijay R. and Tsitsiklis, John N.},
  year = {2004},
  month = may,
  journal = {The Annals of Applied Probability},
  volume = {14},
  number = {2},
  eprint = {math/0405287},
  eprinttype = {arxiv},
  issn = {1050-5164},
  doi = {10.1214/105051604000000116},
  abstract = {We study the rate of convergence of linear two-time-scale stochastic approximation methods. We consider two-time-scale linear iterations driven by i.i.d. noise, prove some results on their asymptotic covariance and establish asymptotic normality. The well-known result [Polyak, B. T. (1990). Automat. Remote Contr. 51 937-946; Ruppert, D. (1988). Technical Report 781, Cornell Univ.] on the optimality of Polyak-Ruppert averaging techniques specialized to linear stochastic approximation is established as a consequence of the general results in this paper.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {62L20 (Primary),Mathematics - Probability},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/LYM424M8/Konda et Tsitsiklis - 2004 - Convergence rate of linear two-time-scale stochast.pdf}
}

@article{kozmaReinforcedRandomWalk2012,
  title = {Reinforced Random Walk},
  author = {Kozma, Gady},
  year = {2012},
  month = aug,
  journal = {arXiv:1208.0364 [math]},
  eprint = {1208.0364},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {A survey of reinforced random walk, with emphasis on the linear case.},
  archiveprefix = {arXiv},
  keywords = {60K35; 60K37; 60G09; 60J10,Mathematics - Probability},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/RLZBWRAI/Kozma - 2012 - Reinforced random walk.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/TYFSL6JB/1208.html}
}

@article{krishnaConvergenceFictitiousPlay1998,
  title = {On the {{Convergence}} of {{Fictitious Play}}},
  author = {Krishna, Vijay and Sj{\"o}str{\"o}m, Tomas},
  year = {1998},
  journal = {Mathematics of Operations Research},
  volume = {23},
  number = {2},
  pages = {479--511},
  publisher = {{INFORMS}},
  issn = {0364-765X},
  abstract = {We study the Brown-Robinson fictitious play process for non-zero sum games. We show that, in general, fictitious play cannot converge cyclically to a mixed strategy equilibrium in which both players use more than two pure strategies.}
}

@inproceedings{kurinCanQlearningGraph2020,
  title = {Can Q-Learning with Graph Networks Learn a Generalizable Branching Heuristic for a {{SAT}} Solver?},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Kurin, Vitaly and Godil, Saad and Whiteson, Shimon and Catanzaro, Bryan},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {9608--9621},
  publisher = {{Curran Associates, Inc.}}
}

@article{kwonContinuoustimeApproachOnline2017,
  title = {A Continuous-Time Approach to Online Optimization},
  author = {Kwon, Joon and Mertikopoulos, Panayotis},
  year = {2017},
  journal = {Journal of Dynamics \& Games},
  volume = {4},
  number = {2},
  pages = {125--148},
  issn = {2164-6074},
  doi = {10.3934/jdg.2017008},
  abstract = {We consider a family of mirror descent strategies for online optimization in continuous-time and we show that they lead to no regret. From a more traditional, discrete-time viewpoint, this continuous-time approach allows us to derive the no-regret properties of a large class of discrete-time algorithms including as special cases the exponential weights algorithm, online mirror descent, smooth fictitious play and vanishingly smooth fictitious play. In so doing, we obtain a unified view of many classical regret bounds, and we show that they can be decomposed into a term stemming from continuous-time considerations and a term which measures the disparity between discrete and continuous time. This generalizes the continuous-time based analysis of the exponential weights algorithm from [29]. As a result, we obtain a general class of infinite horizon learning strategies that guarantee an O(n-1/2) regret bound without having to resort to a doubling trick.},
  langid = {english},
  keywords = {lu,preuve pas lue},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/S2GLX26T/Kwon et al. - 2017 - A continuous-time approach to online optimization.pdf}
}

@article{larakiExplicitFormulasRepeated2010,
  title = {Explicit Formulas for Repeated Games with Absorbing States},
  author = {Laraki, Rida},
  year = {2010},
  month = mar,
  journal = {International Journal of Game Theory},
  volume = {39},
  number = {1-2},
  pages = {53--69},
  issn = {0020-7276, 1432-1270},
  doi = {10.1007/s00182-009-0193-2},
  abstract = {Explicit formulas are given for the asymptotic value lim{$\lambda\rightarrow$}0 v({$\lambda$}) and the asymptotic minmax lim w({$\lambda$}) of finite {$\lambda$}-discounted absorbing games together with new simple proofs for the existence of the limits as {$\lambda$} goes to zero. Similar characterizations for stationary Nash equilibrium payoffs are obtained. The results may be extended to absorbing games with compact metric action sets and jointly-continuous payoff functions.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/5RXIKDQK/Laraki - 2010 - Explicit formulas for repeated games with absorbin.pdf}
}

@article{larakiExplicitFormulasRepeated2010a,
  title = {Explicit Formulas for Repeated Games with Absorbing States},
  author = {Laraki, Rida},
  year = {2010},
  month = mar,
  journal = {International Journal of Game Theory},
  volume = {39},
  number = {1-2},
  pages = {53--69},
  issn = {0020-7276, 1432-1270},
  doi = {10.1007/s00182-009-0193-2},
  abstract = {Explicit formulas for the asymptotic value lim{$\lambda\rightarrow$}0 v({$\lambda$}) and the asymptotic minmax lim{$\lambda\rightarrow$}0 w({$\lambda$}) of finite {$\lambda$}-discounted absorbing games are provided. New simple proofs for the existence of the limits as {$\lambda$} goes zero are given. Similar characterizations for stationary Nash equilibrium payoffs are obtained. The results may be extended to absorbing games with compact action sets and jointly continuous payoff functions.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/G9ILLN55/Laraki - 2010 - Explicit formulas for repeated games with absorbin.pdf}
}

@book{larakiMathematicalFoundationsGame2019,
  title = {Mathematical {{Foundations}} of {{Game Theory}}},
  author = {Laraki, Rida and Renault, J{\'e}r{\^o}me and Sorin, Sylvain},
  year = {2019},
  series = {Universitext},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-26646-2},
  isbn = {978-3-030-26645-5 978-3-030-26646-2},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/B36D3LIW/Laraki et al. - 2019 - Mathematical Foundations of Game Theory.pdf}
}

@misc{larakiSujetThese2019,
  title = {Sujet de Th\`ese},
  author = {Laraki, Rida and Vigeral, Guillaume and Gourv{\`e}s, Laurent and Baudin, Lucas},
  year = {2019},
  month = may,
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/2I9ISJ8W/sujet.pdf}
}

@book{latourPetitesLeconsSociologie2007,
  title = {{Petites le\c{c}ons de sociologie des sciences}},
  author = {Latour, Bruno},
  year = {2007},
  series = {{La D\'ecouverte poche Sciences humanines et sociales}},
  number = {236},
  publisher = {{La D\'ecouverte}},
  address = {{Paris}},
  isbn = {978-2-7071-5012-7},
  langid = {french},
  annotation = {OCLC: 600397173},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/KFEYCN8C/Latour - 2007 - Petites leçons de sociologie des sciences.pdf}
}

@inproceedings{Lauer00analgorithm,
  title = {An Algorithm for Distributed Reinforcement Learning in Cooperative Multi-Agent Systems},
  booktitle = {In Proceedings of the Seventeenth International Conference on Machine Learning},
  author = {Lauer, Martin and Riedmiller, Martin},
  year = {2000},
  pages = {535--542},
  publisher = {{Morgan Kaufmann}},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/QZMGH9ED/Lauer et Riedmiller - 2000 - An algorithm for distributed reinforcement learnin.pdf}
}

@book{lebaronFormationEconomistesOrdre2013,
  title = {{7. La formation des \'economistes et l'ordre symbolique marchand}},
  author = {Lebaron, Fr{\'e}d{\'e}ric},
  year = {2013},
  journal = {Trait\'e de sociologie \'economique},
  pages = {239--280},
  publisher = {{Presses Universitaires de France}},
  issn = {0291-0489},
  isbn = {978-2-13-060831-8},
  langid = {french}
}

@book{lecourtPhilosophieSciences2018,
  title = {{La philosophie des sciences}},
  author = {Lecourt, Dominique},
  year = {2018},
  abstract = {"L'ambivalence des sentiments qui entourent les progr\`es actuels des sciences et la puissance croissante de leurs applications appellent une r\'eflexion philosophique approfondie. Entre une confiance souvent aveugle et une inqui\'etude parfois excessive, comment trouver la voie de la raison ? Le XIXe si\`ecle, dans l'\'elan de la r\'evolution industrielle, a forg\'e le projet d'une "philosophie des sciences" pour faire face aux d\'efis intellectuels et sociaux des sciences physico-chimiques. Une discipline est n\'ee, qui associe les comp\'etences des scientifiques et des philosophes.[...]" (source : 4\`eme de couverture).},
  isbn = {978-2-13-080397-3},
  langid = {french},
  annotation = {OCLC: 1034017720},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/2GI5DKHH/Dominique Lecourt - La philosophie des sciences-Presses Universitaires de France - PUF (2010).epub}
}

@article{lenfantEquilibreGeneralDepuis,
  title = {{L'\'equilibre g\'en\'eral depuis Sonnenschein, Mantel et Debreu: courants et perspectives}},
  author = {Lenfant, Jean-S{\'e}bastien},
  pages = {33},
  langid = {french},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/EU7UTQ9F/Lenfant - L'équilibre général depuis Sonnenschein, Mantel et.pdf}
}

@book{leonardNeumannMorgensternCreation2010,
  title = {Von {{Neumann}}, {{Morgenstern}}, and the Creation of Game Theory: From Chess to Social Science, 1900--1960},
  shorttitle = {Von {{Neumann}}, {{Morgenstern}}, and the Creation of Game Theory},
  author = {Leonard, Robert},
  year = {2010},
  series = {Historical Perspectives on Modern Economics},
  publisher = {{Cambridge University Press}},
  address = {{New York}},
  isbn = {978-0-521-56266-9},
  lccn = {HB144 .L46 2010},
  keywords = {Game theory,History,Morgenstern; Oskar,Von Neumann; John},
  annotation = {OCLC: ocn436358398},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/87XWLVH3/bibliodata.html}
}

@article{leslieBestresponseDynamicsZerosum2018,
  title = {Best-Response {{Dynamics}} in {{Zero-sum Stochastic Games}}},
  author = {Leslie, David S and Perkins, Steven and Xu, Zibo},
  year = {2018},
  month = apr,
  pages = {34},
  abstract = {Given a two-player zero-sum discounted-payoff stochastic game, we define the closed-loop best-response dynamic, and show the global convergence of the dynamic to the set of optimal strategy profiles, which can thus be viewed as a dynamical proof of the existence of value in these games. We then present a continuous-time learning process to implement this dynamic in a stochastic game. We also show that the payoffs in a modified closed-loop best-response dynamic converge to the asymptotic value in a zero-sum stochastic game.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/XKTTXLTR/Leslie et al. - Best-response Dynamics in Zero-sum Stochastic Game.pdf}
}

@article{leslieBestresponseDynamicsZerosum2020,
  title = {Best-Response Dynamics in Zero-Sum Stochastic Games},
  author = {Leslie, David S. and Perkins, Steven and Xu, Zibo},
  year = {2020},
  month = sep,
  journal = {Journal of Economic Theory},
  volume = {189},
  pages = {105095},
  issn = {00220531},
  doi = {10.1016/j.jet.2020.105095},
  abstract = {We define and analyse three learning dynamics for two-player zero-sum discounted-payoff stochastic games. A continuous-time best-response dynamic in mixed strategies is proved to converge to the set of Nash equilibrium stationary strategies. Extending this, we introduce a fictitious-play-like process in a continuous-time embedding of a stochastic zero-sum game, which is again shown to converge to the set of Nash equilibrium strategies. Finally, we present a modified {$\delta$}-converging best-response dynamic, in which the discount rate converges to 1, and the learned value converges to the asymptotic value of the zero-sum stochastic game. The critical feature of all the dynamic processes is a separation of adaption rates: beliefs about the value of states adapt more slowly than the strategies adapt, and in the case of the {$\delta$}-converging dynamic the discount rate adapts more slowly than everything else.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ARWGQZI2/Leslie et al. - 2020 - Best-response dynamics in zero-sum stochastic game.pdf}
}

@article{leslieGeneralisedWeakenedFictitious2006,
  title = {Generalised Weakened Fictitious Play},
  author = {Leslie, David S. and Collins, E. J.},
  year = {2006},
  month = aug,
  journal = {Games and Economic Behavior},
  volume = {56},
  number = {2},
  pages = {285--298},
  issn = {0899-8256},
  doi = {10.1016/j.geb.2005.08.005},
  abstract = {A general class of adaptive processes in games is developed, which significantly generalises weakened fictitious play [Van der Genugten, B., 2000. A weakened form of fictitious play in two-person zero-sum games. Int. Game Theory Rev. 2, 307\textendash 328] and includes several interesting fictitious-play-like processes as special cases. The general model is rigorously analysed using the best response differential inclusion, and shown to converge in games with the fictitious play property. Furthermore, a new actor\textendash critic process is introduced, in which the only information given to a player is the reward received as a result of selecting an action\textemdash a player need not even know they are playing a game. It is shown that this results in a generalised weakened fictitious play process, and can therefore be considered as a first step towards explaining how players might learn to play Nash equilibrium strategies without having any knowledge of the game, or even that they are playing a game.},
  langid = {english},
  keywords = {Actor–critic process,Best response differential inclusion,Fictitious play,Stochastic approximation}
}

@article{leslieIndividualQLearningNormal2005,
  title = {Individual {{Q-Learning}} in {{Normal Form Games}}},
  author = {Leslie, David S and Collins, E J},
  year = {2005},
  journal = {SIAM Journal on Control and Optimization},
  volume = {44},
  number = {2},
  pages = {20},
  doi = {10.1137/S0363012903437976},
  abstract = {The single-agent multi-armed bandit problem can be solved by an agent that learns the values of each action using reinforcement learning. However, the multi-agent version of the problem, the iterated normal form game, presents a more complex challenge, since the rewards available to each agent depend on the strategies of the others. We consider the behavior of valuebased learning agents in this situation, and show that such agents cannot generally play at a Nash equilibrium, although if smooth best responses are used, a Nash distribution can be reached. We introduce a particular value-based learning algorithm, which we call individual Q-learning, and use stochastic approximation to study the asymptotic behavior, showing that strategies will converge to Nash distribution almost surely in 2-player zero-sum games and 2-player partnership games. Playerdependent learning rates are then considered, and it is shown that this extension converges in some games for which many algorithms, including the basic algorithm initially considered, fail to converge.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ZET99CSP/Leslie et Collins - INDIVIDUAL Q-LEARNING IN NORMAL FORM GAMES.pdf}
}

@phdthesis{leslieReinforcementLearningGames2004,
  type = {Doctoral {{Thesis}}},
  title = {Reinforcement Learning in Games},
  author = {Leslie, David S},
  year = {2004},
  school = {University of Bristol},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/GX4XKYZV/399389.pdf}
}

@article{levyUniformlySupportedApproximate1994,
  title = {Uniformly {{Supported Approximate Equilibria}} in {{Families}} of {{Games}}},
  author = {Levy, Yehuda John},
  year = {1994},
  pages = {61},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ARD27HF5/Levy - 1994 - Uniformly Supported Approximate Equilibria in Fami.pdf}
}

@inproceedings{leyton-brownLocaleffectGames2003,
  title = {Local-Effect Games},
  booktitle = {Proceedings of the 18th International Joint Conference on {{Artificial}} Intelligence},
  author = {{Leyton-Brown}, Kevin and Tennenholtz, Moshe},
  year = {2003},
  month = aug,
  series = {{{IJCAI}}'03},
  pages = {772--777},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  abstract = {We present a new class of games, local-effect games (LEGs), which exploit structure in a different way from other compact game representations studied in AI. We show both theoretically and empirically that these games often (but not always) have pure-strategy Nash equilibria. Finding a potential function is a good technique for finding such equilibria. We give a complete characterization of which LEGs have potential functions and provide the functions in each case; we also show a general case where pure-strategy equilibria exist in the absence of potential functions. In experiments, we show that myopic best-response dynamics converge quickly to pure strategy equilibria in games not covered by our positive theoretical results.},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/EZJGTAQW/Leyton-Brown et Tennenholtz - 2003 - Local-effect games.pdf}
}

@misc{LinearQuadraticLyapunov,
  title = {Linear Quadratic {{Lyapunov}} Theory},
  publisher = {{stanford}},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/IQ5JM5KI/lq-lyap.pdf}
}

@inproceedings{littlestoneWeightedMajorityAlgorithm1989,
  title = {The Weighted Majority Algorithm},
  booktitle = {30th {{Annual Symposium}} on {{Foundations}} of {{Computer Science}}},
  author = {Littlestone, N. and Warmuth, M.K.},
  year = {1989},
  pages = {256--261},
  publisher = {{IEEE}},
  address = {{Research Triangle Park, NC, USA}},
  doi = {10.1109/SFCS.1989.63487},
  isbn = {978-0-8186-1982-3},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/NWWV5MXH/Littlestone et Warmuth - 1989 - The weighted majority algorithm.pdf}
}

@inproceedings{littmanGeneralizedReinforcementlearningModel1996,
  title = {A Generalized Reinforcement-Learning Model: Convergence and Applications},
  shorttitle = {A Generalized Reinforcement-Learning Model},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Littman, Michael L. and Szepesv{\'a}ri, Csaba},
  year = {1996},
  month = jul,
  series = {{{ICML}}'96},
  pages = {310--318},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  isbn = {978-1-55860-419-3},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/G6WSWHNR/Littman et Szepesvári - 1996 - A generalized reinforcement-learning model conver.pdf}
}

@incollection{littmanMarkovGamesFramework1994,
  title = {Markov Games as a Framework for Multi-Agent Reinforcement Learning},
  booktitle = {Machine {{Learning Proceedings}} 1994},
  author = {Littman, Michael L.},
  year = {1994},
  pages = {157--163},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-1-55860-335-6.50027-1},
  abstract = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsistic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.},
  isbn = {978-1-55860-335-6},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/IUWKBLYB/Littman - 1994 - Markov games as a framework for multi-agent reinfo.pdf}
}

@article{littmanValuefunctionReinforcementLearning2001,
  title = {Value-Function Reinforcement Learning in {{Markov}} Games},
  author = {Littman, Michael L.},
  year = {2001},
  month = apr,
  journal = {Cognitive Systems Research},
  volume = {2},
  number = {1},
  pages = {55--66},
  issn = {13890417},
  doi = {10.1016/S1389-0417(01)00015-8},
  abstract = {Markov games are a model of multiagent environments that are convenient for studying multiagent reinforcement learning. This paper describes a set of reinforcement-learning algorithms based on estimating value functions and presents convergence theorems for these algorithms. The main contribution of this paper is that it presents the convergence theorems in a way that makes it easy to reason about the behavior of simultaneous learners in a shared environment. \textcopyright{} 2001 Elsevier Science B.V. All rights reserved.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/HLV2WNVK/Littman - 2001 - Value-function reinforcement learning in Markov ga.pdf}
}

@article{longDynamicGamesEconomics2011,
  title = {Dynamic {{Games}} in the {{Economics}} of {{Natural Resources}}: {{A Survey}}},
  shorttitle = {Dynamic {{Games}} in the {{Economics}} of {{Natural Resources}}},
  author = {Long, Ngo Van},
  year = {2011},
  month = mar,
  journal = {Dynamic Games and Applications},
  volume = {1},
  number = {1},
  pages = {115--148},
  issn = {2153-0785, 2153-0793},
  doi = {10.1007/s13235-010-0003-2},
  abstract = {This article provides a comprehensive survey of models of dynamic games in the exploitation of renewable and exhaustible resources. It includes dynamic games at the industry level (oligopoly, cartel versus fringe, tragedy of the commons) and at the international level (tariffs on exhaustible resources, fish wars, entry deterrence). Among more recent topics are international strategic issues involving the link between resource uses and transboundary pollution, the design of taxation to ensure efficient outcomes under symmetric and asymmetric information, the rivalry among factions in countries where property rights on natural resources are not well established. Various extensions are considered, such as (i) modeling the effects of the concern for relative performance (relative income, relative consumption, and social status) on the over-exploitation of resources, (ii) applying the tragedy of the commons paradigm to the declining effectiveness of antibiotics and pesticides. Outcomes under Nash equilibria and Stackelberg equilibria are compared. The paper ends with some suggestions for future research.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/BAMESLGZ/Long - 2011 - Dynamic Games in the Economics of Natural Resource.pdf}
}

@incollection{ludkovskiGameTheoreticModels2015,
  title = {Game {{Theoretic Models}} for {{Energy Production}}},
  booktitle = {Commodities, {{Energy}} and {{Environmental Finance}}},
  author = {Ludkovski, Michael and Sircar, Ronnie},
  editor = {A{\"i}d, Ren{\'e} and Ludkovski, Michael and Sircar, Ronnie},
  year = {2015},
  volume = {74},
  pages = {317--333},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4939-2733-3_12},
  isbn = {978-1-4939-2732-6 978-1-4939-2733-3},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/SITJINCT/Ludkovski et Sircar - 2015 - Game Theoretic Models for Energy Production.pdf}
}

@article{macyLearningDynamicsSocial2002,
  title = {Learning Dynamics in Social Dilemmas},
  author = {Macy, M. W. and Flache, A.},
  year = {2002},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {99},
  number = {Supplement 3},
  pages = {7229--7236},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.092080099},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/LT7GTHZH/Macy et Flache - 2002 - Learning dynamics in social dilemmas.pdf}
}

@article{madiesIIControversesAutour2020,
  title = {{II. Controverses autour de la concurrence fiscale : quels sont les enseignements de la th\'eorie \'economique ?}},
  shorttitle = {{II. Controverses autour de la concurrence fiscale}},
  author = {Madi{\`e}s, Thierry},
  year = {2020},
  month = feb,
  journal = {Reperes},
  pages = {32--50},
  publisher = {{La D\'ecouverte}},
  issn = {0993-7625},
  isbn = {9782348040474},
  langid = {french}
}

@article{mairalOnlineLearningMatrix2010,
  title = {Online {{Learning}} for {{Matrix Factorization}} and {{Sparse Coding}}},
  author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
  year = {2010},
  pages = {42},
  abstract = {Sparse coding\textemdash that is, modelling data vectors as sparse linear combinations of basis elements\textemdash is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to specific data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/MPVKJAN7/Mairal et al. - Online Learning for Matrix Factorization and Spars.pdf}
}

@article{mannorEmpiricalBayesEnvelope2003,
  title = {The {{Empirical Bayes Envelope}} and {{Regret Minimization}} in {{Competitive Markov Decision Processes}}},
  author = {Mannor, Shie and Shimkin, Nahum},
  year = {2003},
  month = may,
  journal = {Mathematics of Operations Research},
  volume = {28},
  number = {2},
  pages = {327--345},
  issn = {0364-765X, 1526-5471},
  doi = {10.1287/moor.28.2.327.14483},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ZLT3VVUP/Mannor et Shimkin - 2003 - The Empirical Bayes Envelope and Regret Minimizati.pdf}
}

@article{mannorMultiagentLearningEngineers2007,
  title = {Multi-Agent Learning for Engineers},
  author = {Mannor, Shie and Shamma, Jeff S.},
  year = {2007},
  month = may,
  journal = {Artificial Intelligence},
  volume = {171},
  number = {7},
  pages = {417--422},
  issn = {00043702},
  doi = {10.1016/j.artint.2007.01.003},
  abstract = {As suggested by the title of Shoham, Powers, and Grenager's position paper [Y. Shoham, R. Powers, T. Grenager, If multi-agent learning is the answer, what is the question? Artificial Intelligence 171 (7) (2007) 365\textendash 377, this issue], the ultimate lens through which the multi-agent learning framework should be assessed is ``what is the question?''. In this paper, we address this question by presenting challenges motivated by engineering applications and discussing the potential appeal of multi-agent learning to meet these challenges. Moreover, we highlight various differences in the underlying assumptions and issues of concern that generally distinguish engineering applications from models that are typically considered in the economic game theory literature.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/7SSZYGHR/Mannor et Shamma - 2007 - Multi-agent learning for engineers.pdf}
}

@article{mannorRegretMinimizationRepeated2008,
  title = {Regret Minimization in Repeated Matrix Games with Variable Stage Duration},
  author = {Mannor, Shie and Shimkin, Nahum},
  year = {2008},
  month = may,
  journal = {Games and Economic Behavior},
  volume = {63},
  number = {1},
  pages = {227--258},
  issn = {08998256},
  doi = {10.1016/j.geb.2007.07.006},
  abstract = {Regret minimization in repeated matrix games has been extensively studied ever since Hannan's seminal paper [Hannan, J., 1957. Approximation to Bayes risk in repeated play. In: Dresher, M., Tucker, A.W., Wolfe, P. (Eds.), Contributions to the Theory of Games, vol. III. Ann. of Math. Stud., vol. 39, Princeton Univ. Press, Princeton, NJ, pp. 97\textendash 193]. Several classes of no-regret strategies now exist; such strategies secure a long-term average payoff as high as could be obtained by the fixed action that is best, in hindsight, against the observed action sequence of the opponent. We consider an extension of this framework to repeated games with variable stage duration, where the duration of each stage may depend on actions of both players, and the performance measure of interest is the average payoff per unit time. We start by showing that no-regret strategies, in the above sense, do not exist in general. Consequently, we consider two classes of adaptive strategies, one based on Blackwell's approachability theorem and the other on calibrated play, and examine their performance guarantees. We further provide sufficient conditions for existence of no-regret strategies in this model.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ID4V5Y66/Mannor et Shimkin - 2008 - Regret minimization in repeated matrix games with .pdf}
}

@article{maoDecentralizedCooperativeMultiAgent2021,
  title = {Decentralized {{Cooperative Multi-Agent Reinforcement Learning}} with {{Exploration}}},
  author = {Mao, Weichao and Ba{\c s}ar, Tamer and Yang, Lin F. and Zhang, Kaiqing},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.05707 [cs]},
  eprint = {2110.05707},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Many real-world applications of multi-agent reinforcement learning (RL), such as multi-robot navigation and decentralized control of cyber-physical systems, involve the cooperation of agents as a team with aligned objectives. We study multi-agent RL in the most basic cooperative setting \textemdash Markov teams \textemdash{} a class of Markov games where the cooperating agents share a common reward. We propose an algorithm in which each agent independently runs stage-based V-learning (a Qlearning style algorithm) to efficiently explore the unknown environment, while using a stochastic gradient descent (SGD) subroutine for policy updates. We show that the agents can learn an {$\epsilon$}-approximate Nash equilibrium policy in at most {$\propto$} O(1/{$\epsilon$}4) episodes. Our results advocate the use of a novel stage-based V-learning approach to create a stage-wise stationary environment. We also show that under certain smoothness assumptions of the team, our algorithm can achieve a nearly team-optimal Nash equilibrium. Simulation results corroborate our theoretical findings. One key feature of our algorithm is being decentralized, in the sense that each agent has access to only the state and its local actions, and is even oblivious to the presence of the other agents. Neither communication among teammates nor coordination by a central controller is required during learning. Hence, our algorithm can readily generalize to an arbitrary number of agents, without suffering from the exponential dependence on the number of agents.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/RWIAUDAU/Mao et al. - 2021 - Decentralized Cooperative Multi-Agent Reinforcemen.pdf}
}

@article{mardenAchievingParetoOptimality2014,
  title = {Achieving {{Pareto Optimality Through Distributed Learning}}},
  author = {Marden, Jason R and Young, H Peyton and Pao, Lucy Y},
  year = {2014},
  pages = {24},
  abstract = {We propose a simple payoff-based learning rule that is completely decentralized, and that leads to an efficient configuration of actions in any n-person finite strategic-form game with generic payoffs. The algorithm follows the theme of exploration versus exploitation and is hence stochastic in nature. We prove that if all agents adhere to this algorithm, then the agents will select the action profile that maximizes the sum of the agents' payoffs a high percentage of time. The algorithm requires no communication. Agents respond solely to changes in their own realized payoffs, which are affected by the actions of other agents in the system in ways that they do not necessarily understand. The method can be applied to the optimization of complex systems with many distributed components, such as the routing of information in networks and the design and control of wind farms. The proof of the proposed learning algorithm relies on the theory of large deviations for perturbed Markov chains.},
  langid = {english},
  keywords = {lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/52VGLD3S/Marden et al. - 2014 - Achieving Pareto Optimality Through Distributed Le.pdf}
}

@incollection{mardenGameTheoryDistributed2015,
  title = {Game {{Theory}} and {{Distributed Control}}},
  booktitle = {Handbook of {{Game Theory}} with {{Economic Applications}}},
  author = {Marden, Jason R. and Shamma, Jeff S.},
  year = {2015},
  volume = {4},
  pages = {861--899},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-444-53766-9.00016-1},
  isbn = {978-0-444-53766-9},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/SETPRNT3/Marden et Shamma - Game Theory and Distributed Control.pdf}
}

@article{mardenJointStrategyFictitious2009,
  title = {Joint {{Strategy Fictitious Play With Inertia}} for {{Potential Games}}},
  author = {Marden, Jason R. and Arslan, G{\"U}rdal and Shamma, Jeff S.},
  year = {2009},
  month = feb,
  journal = {IEEE Transactions on Automatic Control},
  volume = {54},
  number = {2},
  pages = {208--220},
  issn = {0018-9286},
  doi = {10.1109/TAC.2008.2010885},
  abstract = {We consider multi-player repeated games involving a large number of players with large strategy spaces and enmeshed utility structures. In these ``large-scale'' games, players are inherently faced with limitations in both their observational and computational capabilities. Accordingly, players in large-scale games need to make their decisions using algorithms that accommodate limitations in information gathering and processing. This disqualifies some of the well known decision making models such as ``Fictitious Play'' (FP), in which each player must monitor the individual actions of every other player and must optimize over a high dimensional probability space. We will show that Joint Strategy Fictitious Play (JSFP), a close variant of FP, alleviates both the informational and computational burden of FP. Furthermore, we introduce JSFP with inertia, i.e., a probabilistic reluctance to change strategies, and establish the convergence to a pure Nash equilibrium in all generalized ordinal potential games in both cases of averaged or exponentially discounted historical data. We illustrate JSFP with inertia on the specific class of congestion games, a subset of generalized ordinal potential games. In particular, we illustrate the main results on a distributed traffic routing problem and derive tolling procedures that can lead to optimized total traffic congestion.},
  langid = {english},
  keywords = {lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/KBWYFTMJ/Marden et al. - 2009 - Joint Strategy Fictitious Play With Inertia for Po.pdf}
}

@article{mardenStateBasedPotential2012,
  title = {State Based Potential Games},
  author = {Marden, Jason R},
  year = {2012},
  pages = {14},
  abstract = {There is a growing interest in the application of game theoretic methods to the design and control of multiagent systems. However, the existing game theoretic framework possesses inherent limitations with regards to these new prescriptive challenges. In this paper we propose a new framework, termed state based potential games, which introduces an underlying state space into the framework of potential games. This state space provides a system designer with an additional degree of freedom to help coordinate group behavior and overcome these limitations. Within the context of state based potential games, we characterize the limiting behavior of two learning algorithms termed finite memory better reply processes and log-linear learning. Lastly, we demonstrate the applicability of state based potential games on two cooperative control problems pertaining to distributed resource allocation and the design of local and distributed control laws.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/F37EE3B3/Marden - 2012 - State based potential games.pdf}
}

@article{MarkovChain2019,
  title = {Markov Chain},
  year = {2019},
  month = nov,
  journal = {Wikipedia},
  abstract = {A Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.In probability theory and related fields, a Markov process, named after the Russian mathematician Andrey Markov, is a stochastic process that satisfies the Markov property (sometimes characterized as "memorylessness"). Roughly speaking, a process satisfies the Markov property if one can make predictions for the future of the process based solely on its present state just as well as one could knowing the process's full history, hence independently from such history. In other words, conditional on the present state of the system, its future and past states are independent. A Markov chain is a type of Markov process that has either a discrete state space or a discrete index set (often representing time), but the precise definition of a Markov chain varies. For example, it is common to define a Markov chain as a Markov process in either discrete or continuous time with a countable state space (thus regardless of the nature of time), but it is also common to define a Markov chain as having discrete time in either countable or continuous state space (thus regardless of the state space).Markov studied Markov processes in the early 20th century, publishing his first paper on the topic in 1906. Random walks based on integers and the gambler's ruin problem are examples of Markov processes. Some variations of these processes were studied hundreds of years earlier in the context of independent variables. Two important examples of Markov processes are the Wiener process, also known as the Brownian motion process, and the Poisson process, which are considered the most important and central stochastic processes in the theory of stochastic processes. These two processes are Markov processes in continuous time, while random walks on the integers and the gambler's ruin problem are examples of Markov processes in discrete time.Markov chains have many applications as statistical models of real-world processes, such as studying cruise control systems in motor vehicles, queues or lines of customers arriving at an airport, exchange rates of currencies, storage systems such as dams, and population growths of certain animal species. The algorithm known as PageRank, which was originally proposed for the internet search engine Google, is based on a Markov process.Markov processes are the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found extensive application in Bayesian statistics.The adjective Markovian is used to describe something that is related to a Markov process.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 927351268},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/Z9C5W7T2/index.html}
}

@article{MarkovDecisionProcess2019,
  title = {Markov Decision Process},
  year = {2019},
  month = dec,
  journal = {Wikipedia},
  abstract = {A Markov decision process (MDP) is a discrete time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying optimization problems solved via dynamic programming and reinforcement learning. MDPs were known at least as early as the 1950s; a core body of research on Markov decision processes resulted from Ronald Howard's 1960 book, Dynamic Programming and Markov Processes. They are used in many disciplines, including robotics, automatic control, economics and manufacturing. The name of MDPs comes from the Russian mathematician Andrey Markov as they are an extension of the Markov chains. At each time step, the process is in some state                         s                 \{\textbackslash displaystyle s\}   , and the decision maker may choose any action                         a                 \{\textbackslash displaystyle a\}    that is available in state                         s                 \{\textbackslash displaystyle s\}   . The process responds at the next time step by randomly moving into a new state                                    s           {${'}$}                          \{\textbackslash displaystyle s'\}   , and giving the decision maker a corresponding reward                                    R                        a                             (         s         ,                    s           {${'}$}                  )                 \{\textbackslash displaystyle R\_\{a\}(s,s')\}   . The probability that the process moves into its new state                                    s           {${'}$}                          \{\textbackslash displaystyle s'\}    is influenced by the chosen action. Specifically, it is given by the state transition function                                    P                        a                             (         s         ,                    s           {${'}$}                  )                 \{\textbackslash displaystyle P\_\{a\}(s,s')\}   . Thus, the next state                                    s           {${'}$}                          \{\textbackslash displaystyle s'\}    depends on the current state                         s                 \{\textbackslash displaystyle s\}    and the decision maker's action                         a                 \{\textbackslash displaystyle a\}   . But given                         s                 \{\textbackslash displaystyle s\}    and                         a                 \{\textbackslash displaystyle a\}   , it is conditionally independent of all previous states and actions; in other words, the state transitions of an MDP satisfies the Markov property. Markov decision processes are an extension of Markov chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state (e.g. "wait") and all rewards are the same (e.g. "zero"), a Markov decision process reduces to a Markov chain.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 929369202},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/4BH6JPLV/index.html}
}

@misc{Martingales,
  title = {Martingales},
  howpublished = {https://www.math.univ-paris13.fr/\textasciitilde tournier/fichiers/agreg/enbref\_martingales.pdf},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/JSI7YAC7/enbref_martingales.pdf}
}

@article{martinsohnImpactClimateChange2012,
  title = {The {{Impact}} of {{Climate Change}} on the {{Economics}} of {{Dairy Farming}} \textendash{} a {{Review}} and {{Evaluation}}},
  author = {Martinsohn, Maria and Hansen, Heiko},
  year = {2012},
  number = {2},
  pages = {16},
  abstract = {The impact of climate change has become a major concern within the agricultural profession. While many studies on Climate Change Impact Assessment (CCIA) deal with crop farming, little has been done with regard to livestock farming. This paper aims to shed light on the present state of research in the field of dairy farming, one of the major sectors in agriculture, in a three-fold manner. First, potential climate change impacts in dairy farming are discussed qualitatively. Second, challenges and methodological approaches in economic CCIA are presented, with a closer look at the issue of climate data and farm-level adaptation. Third, an overview and assessment of available studies on economic CCIA in dairy farming along a set of evaluation criteria is provided. The paper concludes with further research opportunities in this field.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/MBBKZ3A8/Martinsohn et Hansen - 2012 - The Impact of Climate Change on the Economics of D.pdf}
}

@misc{mathtauVianneyPerchetApproachability2021,
  title = {Vianney {{Perchet}} on "{{Approachability}} and {{Regret}}" in the {{GAMENET}} 2021  {{Workshop}}},
  author = {{Math TAU}},
  year = {2021},
  month = jan,
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/IXXIWVCC/Math TAU - 2021 - Vianney Perchet on Approachability and Regret in.pdf}
}

@article{matignonIndependentReinforcementLearners2012,
  title = {Independent Reinforcement Learners in Cooperative {{Markov}} Games: A Survey Regarding Coordination Problems},
  shorttitle = {Independent Reinforcement Learners in Cooperative {{Markov}} Games},
  author = {Matignon, Laetitia and Laurent, Guillaume J. and {Le Fort-Piat}, Nadine},
  year = {2012},
  month = feb,
  journal = {The Knowledge Engineering Review},
  volume = {27},
  number = {1},
  pages = {1--31},
  issn = {0269-8889, 1469-8005},
  doi = {10.1017/S0269888912000057},
  abstract = {In the framework of fully cooperative multi-agent systems, independent (non-communicative) agents that learn by reinforcement must overcome several difficulties to manage to coordinate. This paper identifies several challenges responsible for the non-coordination of independent agents: Pareto-selection, nonstationarity, stochasticity, alter-exploration and shadowed equilibria. A selection of multi-agent domains is classified according to those challenges: matrix games, Boutilier's coordination game, predators pursuit domains and a special multi-state game. Moreover the performance of a range of algorithms for independent reinforcement learners is evaluated empirically. Those algorithms are Q-learning variants: decentralized Q-learning, distributed Q-learning, hysteretic Q-learning, recursive FMQ and WoLF PHC. An overview of the learning algorithms' strengths and weaknesses against each challenge concludes the paper and can serve as a basis for choosing the appropriate algorithm for a new domain. Furthermore, the distilled challenges may assist in the design of new learning algorithms that overcome these problems and achieve higher performance in multi-agent applications.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/KB6AI52L/Matignon et al. - 2012 - Independent reinforcement learners in cooperative .pdf}
}

@book{matousekUnderstandingUsingLinear2007,
  title = {Understanding and Using Linear Programming},
  author = {Matou{\v s}ek, Ji{\v r}{\'i} and G{\"a}rtner, Bernd},
  year = {2007},
  series = {Universitext},
  publisher = {{Springer}},
  address = {{Berlin ; New York}},
  isbn = {978-3-540-30697-9 978-3-540-30717-4},
  lccn = {T57.74 .M374 2007},
  keywords = {Linear programming}
}

@article{matsuiBestResponseDynamics1992,
  title = {Best Response Dynamics and Socially Stable Strategies},
  author = {Matsui, Akihiko},
  year = {1992},
  month = aug,
  journal = {Journal of Economic Theory},
  volume = {57},
  number = {2},
  pages = {343--362},
  issn = {0022-0531},
  doi = {10.1016/0022-0531(92)90040-O},
  abstract = {An evolutionarily stable strategy (Maynard Smith and Price, Nature (London) 246 (1973), 15\textendash 18) is a strategy which is robust against a tiny invasion of mutants. Best response dynamics is a dynamic process in which the frequency of a strategy increases only if it is a best response to the present strategy distribution. Gilboa and Matsui (Econometrica 59 (1991), 859\textendash 867) proposed a stability concept directly derived from this dynamic process. Modifying the above two stability concepts, this paper shows the equivalence between the static concept and the dynamic one. Their set-valued versions always exist. Examples are given to see their usefulness in analyzing forward induction and preplay communication.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/WGFYVGLQ/002205319290040O.html}
}

@article{mccarthyMetaanalysisImpactStocking2011,
  title = {Meta-Analysis of the Impact of Stocking Rate on the Productivity of Pasture-Based Milk Production Systems},
  author = {McCarthy, B. and Delaby, L. and Pierce, K. M. and Journot, F. and Horan, B.},
  year = {2011},
  month = jan,
  journal = {Animal},
  volume = {5},
  number = {5},
  pages = {784--794},
  issn = {1751-7311},
  doi = {10.1017/S1751731110002314},
  abstract = {The objective of this study is to quantify the milk production response per cow and per hectare (ha) for an incremental stocking rate (SR) change, based on a meta-analysis of published research papers. Suitable experiments for inclusion in the database required a comparison of at least two SRs under the same experimental conditions in addition to details on experimental length and milk production results per cow and per ha. Each additional increased SR treatment was also described in terms of the relative milk production change per cow and per ha compared to the lower base SR (b\_SR). A database containing 109 experiments of various lengths with 131 comparisons of SR was sub-divided into Type I experiments (common experimental lengths) and Type II experiments (variable experimental lengths). Actual and proportional changes in milk production according to SR change were analysed using linear mixed model procedures with study included as a random effect in the model. Low residual standard errors indicated a good precision of the predictive equations with the exception of proportional change in milk production per cow. For all milk yield variables analysed, the results illustrate that while production per cow is reduced, a strong positive relationship exists between SR and milk production per ha. An SR increase of one cow/ha resulted in a decrease in daily milk yield per cow of 7.4\% and 8.7\% for Type I and Type II data, respectively, whereas milk yield per ha increased by 20.1\% and 19.6\%, respectively. Within the Type II data set, a one cow/ha increase in SR also resulted in a 15.1\% reduction in lactation length (equivalent to 42 days). The low predictability of proportional change in milk production per cow according to the classical SR definition of cows per ha over a defined period suggests that SR may be more appropriately defined in terms of the change in available feed offered per animal within each treatment.},
  langid = {english},
  keywords = {dairy cow,meta-analysis,pasture-based systems,stocking rate},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/87X4SGVS/McCarthy et al. - 2011 - Meta-analysis of the impact of stocking rate on th.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/V44YDXNQ/S1751731110002314.html}
}

@book{mccartyPoliticalGameTheory2007,
  title = {Political {{Game Theory}}: {{An Introduction}}},
  shorttitle = {Political {{Game Theory}}},
  author = {McCarty, Nolan and Meirowitz, Adam},
  year = {2007},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511813122},
  isbn = {978-0-511-81312-2},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/UNCFW7JA/McCarty et Meirowitz - 2007 - Political Game Theory An Introduction.pdf}
}

@article{mcgrealCallsGrowWithdrawal2005,
  title = {Calls Grow for Withdrawal of {{Nobel}} Prize},
  author = {McGreal, Chris},
  year = {2005},
  month = dec,
  journal = {The Guardian},
  issn = {0261-3077},
  abstract = {{$\cdot$} Israeli group objects to award for 'warmongers' {$\cdot$} Game theory used for political bias, say critics},
  chapter = {World news},
  langid = {british},
  keywords = {Israel,Middle East and North Africa,Science,World news},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/7FS3QAPJ/israel.html}
}

@incollection{melliesAsynchronousGamesInnocence2007,
  title = {Asynchronous {{Games}}: {{Innocence Without Alternation}}},
  shorttitle = {Asynchronous {{Games}}},
  booktitle = {{{CONCUR}} 2007 \textendash{} {{Concurrency Theory}}},
  author = {Melli{\`e}s, Paul-Andr{\'e} and Mimram, Samuel},
  editor = {Caires, Lu{\'i}s and Vasconcelos, Vasco T.},
  year = {2007},
  volume = {4703},
  pages = {395--411},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  issn = {0302-9743, 1611-3349},
  doi = {10.1007/978-3-540-74407-8_27},
  abstract = {The notion of innocent strategy was introduced by Hyland and Ong in order to capture the interactive behaviour of {$\lambda$}-terms and PCF programs. An innocent strategy is defined as an alternating strategy with partial memory, in which the strategy plays according to its view. Extending the definition to nonalternating strategies is problematic, because the traditional definition of views is based on the hypothesis that Opponent and Proponent alternate during the interaction. Here, we take advantage of the diagrammatic reformulation of alternating innocence in asynchronous games, in order to provide a tentative definition of innocence in non-alternating games. The task is interesting, and far from easy. It requires the combination of true concurrency and game semantics in a clean and organic way, clarifying the relationship between asynchronous games and concurrent games in the sense of Abramsky and Melli\`es. It also requires an interactive reformulation of the usual acyclicity criterion of linear logic, as well as a directed variant, as a scheduling criterion.},
  isbn = {978-3-540-74406-1 978-3-540-74407-8},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/RLCKJW4M/Melliès et Mimram - 2007 - Asynchronous Games Innocence Without Alternation.pdf}
}

@article{melliesEtudeMicrologiqueNegation,
  title = {{Une \'etude micrologique de la n\'egation}},
  author = {Melli{\`e}s, Paul-Andr{\'e}},
  pages = {291},
  langid = {french},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/PG7IUHYW/Melliès - Une étude micrologique de la négation.pdf}
}

@book{menshikovNonhomogeneousRandomWalks2017,
  title = {Non-Homogeneous {{Random Walks}}: {{Lyapunov Function Methods}} for {{Near-Critical Stochastic Systems}}},
  shorttitle = {Non-Homogeneous {{Random Walks}}},
  author = {Menshikov, Mikhail and Popov, Serguei and Wade, Andrew},
  year = {2017},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/9781139208468},
  abstract = {Stochastic systems provide powerful abstract models for a variety of important real-life applications: for example, power supply, traffic flow, data transmission. They (and the real systems they model) are often subject to phase transitions, behaving in one way when a parameter is below a certain critical value, then switching behaviour as soon as that critical value is reached. In a real system, we do not necessarily have control over all the parameter values, so it is important to know how to find critical points and to understand system behaviour near these points. This book is a modern presentation of the 'semimartingale' or 'Lyapunov function' method applied to near-critical stochastic systems, exemplified by non-homogeneous random walks. Applications treat near-critical stochastic systems and range across modern probability theory from stochastic billiards models to interacting particle systems. Spatially non-homogeneous random walks are explored in depth, as they provide prototypical near-critical systems.},
  isbn = {978-1-139-20846-8},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/6RIPNWGX/Menshikov et al. - 2017 - Non-homogeneous Random Walks.pdf}
}

@article{mertikopoulosLearningGamesReinforcement2016,
  title = {Learning in {{Games}} via {{Reinforcement}} and {{Regularization}}},
  author = {Mertikopoulos, Panayotis and Sandholm, William H.},
  year = {2016},
  month = nov,
  journal = {Mathematics of Operations Research},
  volume = {41},
  number = {4},
  pages = {1297--1324},
  issn = {0364-765X, 1526-5471},
  doi = {10.1287/moor.2016.0778},
  abstract = {We investigate a class of reinforcement learning dynamics in which players adjust their strategies based on their actions' cumulative payoffs over time \textendash{} specifically, by playing mixed strategies that maximize their expected cumulative payoff minus a strongly convex, regularizing penalty term. In contrast to the class of penalty functions used to define smooth best responses in models of stochastic fictitious play, the regularizers used in this paper need not be infinitely steep at the boundary of the simplex; in fact, dropping this requirement gives rise to an important dichotomy between steep and nonsteep cases. In this general setting, our main results extend several properties of the replicator dynamics such as the elimination of dominated strategies, the asymptotic stability of strict Nash equilibria and the convergence of timeaveraged trajectories to interior Nash equilibria in zero-sum games.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/KEQUUYHM/Mertikopoulos et Sandholm - 2016 - Learning in Games via Reinforcement and Regulariza.pdf}
}

@article{mertikopoulosLearningGamesReinforcement2016a,
  title = {Learning in {{Games}} via {{Reinforcement}} and {{Regularization}}},
  author = {Mertikopoulos, Panayotis and Sandholm, William H.},
  year = {2016},
  month = nov,
  journal = {Mathematics of Operations Research},
  volume = {41},
  number = {4},
  pages = {1297--1324},
  publisher = {{INFORMS}},
  issn = {0364-765X},
  doi = {10.1287/moor.2016.0778},
  abstract = {We investigate a class of reinforcement learning dynamics where players adjust their strategies based on their actions' cumulative payoffs over time\textemdash specifically, by playing mixed strategies that maximize their expected cumulative payoff minus a regularization term. A widely studied example is exponential reinforcement learning, a process induced by an entropic regularization term which leads mixed strategies to evolve according to the replicator dynamics. However, in contrast to the class of regularization functions used to define smooth best responses in models of stochastic fictitious play, the functions used in this paper need not be infinitely steep at the boundary of the simplex; in fact, dropping this requirement gives rise to an important dichotomy between steep and nonsteep cases. In this general framework, we extend several properties of exponential learning, including the elimination of dominated strategies, the asymptotic stability of strict Nash equilibria, and the convergence of time-averaged trajectories in zero-sum games with an interior Nash equilibrium.},
  keywords = {Bregman divergence,dominated strategies,equilibrium stability,Fenchel coupling,penalty functions,projection dynamics,regularization,reinforcement learning,replicator dynamics,time averages},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/RZH3CPH5/Mertikopoulos et Sandholm - 2016 - Learning in Games via Reinforcement and Regulariza.pdf}
}

@phdthesis{mertikopoulosOnlineOptimizationLearning,
  title = {Online {{Optimization}} and {{Learning}} in {{Games}}: {{Theory}} and {{Applications}}},
  author = {Mertikopoulos, Panayotis},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/DSJ86EKS/Mertikopoulos - ONLINE OPTIMIZATION AND LEARNING IN GAMES THEORY .pdf}
}

@article{milgromAdaptiveSophisticatedLearning1991,
  title = {Adaptive and Sophisticated Learning in Normal Form Games},
  author = {Milgrom, Paul and Roberts, John},
  year = {1991},
  month = feb,
  journal = {Games and Economic Behavior},
  volume = {3},
  number = {1},
  pages = {82--100},
  issn = {0899-8256},
  doi = {10.1016/0899-8256(91)90006-Z},
  abstract = {In a class of games including some Cournot and Bertrand games, a sequence of plays converges to the unique Nash equilibrium if and only if the sequence is ``consistent with adaptive learning'' according to the new definition we propose. In the Arrow-Debreu model with gross substitutes, a sequence of prices converges to the competitive equilibrium if and only if the sequence is consistent with adaptive learning by price-setting market makers for the individual goods. Similar results are obtained for ``sophisticated'' learning. All the familiar learning algorithms generate play that is consistent with adaptive learning. Journal of Economic Literature Classification Numbers: 026, 021.},
  langid = {english},
  keywords = {lu,procedure},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/2ZQ78BHU/Milgrom et Roberts - 1991 - Adaptive and sophisticated learning in normal form.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/5F2BUKSA/089982569190006Z.html}
}

@article{milgromRationalizabilityLearningEquilibrium1990,
  title = {Rationalizability, {{Learning}}, and {{Equilibrium}} in {{Games}} with {{Strategic Complementarities}}},
  author = {Milgrom, Paul and Roberts, John},
  year = {1990},
  month = nov,
  journal = {Econometrica},
  volume = {58},
  number = {6},
  pages = {1255},
  issn = {00129682},
  doi = {10.2307/2938316},
  langid = {english},
  keywords = {lu,procedure},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/SUHW8ATI/Milgrom et Roberts - 1990 - Rationalizability, Learning, and Equilibrium in Ga.pdf}
}

@article{millerRoleAbsoluteContinuity,
  title = {The {{Role}} of {{Absolute Continuity}} in ``{{Merging}} of {{Opinions}}'' and ``{{Rational Learning}}''},
  author = {Miller, Ronald I},
  pages = {21},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/G548I9CH/Miller - The Role of Absolute Continuity in ‘‘Merging of Op.pdf}
}

@article{milmanApproachableSetsVector2006,
  title = {Approachable Sets of Vector Payoffs in Stochastic Games},
  author = {Milman, Emanuel},
  year = {2006},
  month = jul,
  journal = {Games and Economic Behavior},
  volume = {56},
  number = {1},
  pages = {135--147},
  issn = {08998256},
  doi = {10.1016/j.geb.2005.06.005},
  abstract = {The notion of an approachable set in a recurrent zero-sum vector-payoff game was introduced by Blackwell, who proved a sufficient condition for approachability in such games. This notion has been recently generalized to the case of arbitrary stochastic games with vector-payoffs, becoming dependent on the initial state. In this paper, we generalize Blackwell's condition to this wider context, giving a sufficient condition for approachability from all initial states. In addition, we show that this condition is also necessary for convex sets, thereby providing a complete characterization of all the approachable convex sets in a game.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/XTC6MFX8/Milman - 2006 - Approachable sets of vector payoffs in stochastic .pdf}
}

@article{mirandaEmpiricalModelAsset1995,
  title = {An Empirical Model of Asset Replacement in Dairy Production},
  author = {Miranda, Mario J. and Schnitkey, Gary D.},
  year = {1995},
  journal = {Journal of Applied Econometrics},
  volume = {10},
  number = {S1},
  pages = {S41-S55},
  issn = {1099-1255},
  doi = {10.1002/jae.3950100504},
  abstract = {Throughout the US dairy farm industry, observed rates of dairy cow replacement consistently exceed the rates prescribed as optimal by dairy economists. We attempt to uncover the causes of this discrepancy by estimating a series of dynamic discrete choice models of dairy cow replacement using historical farm-level data. We also advance the methods used to estimate dynamic discrete choice models by demonstrating how orthogonal polynomial projection methods can be effectively combined with standard maximum likelihood techniques to estimate structural models of dynamic decision making.},
  copyright = {Copyright \textcopyright{} 1995 John Wiley \& Sons, Ltd.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jae.3950100504}
}

@article{ModelPredictiveControl2020,
  title = {Model Predictive Control},
  year = {2020},
  month = aug,
  journal = {Wikipedia},
  abstract = {Model predictive control (MPC) is an advanced method of process control that is used to control a process while satisfying a set of constraints. It has been in use in the process industries in chemical plants and oil refineries since the 1980s. In recent years it has also been used in power system balancing models and in power electronics. Model predictive controllers rely on dynamic models of the process, most often linear empirical models obtained by system identification. The main advantage of MPC is the fact that it allows the current timeslot to be optimized, while keeping future timeslots in account. This is achieved by optimizing a finite time-horizon, but only implementing the current timeslot and then optimizing again, repeatedly, thus differing from Linear-Quadratic Regulator (LQR). Also MPC has the ability to anticipate future events and can take control actions accordingly. PID controllers do not have this predictive ability. MPC is nearly universally implemented as a digital control, although there is research into achieving faster response times with specially designed analog circuitry.Generalized predictive control (GPC) and dynamic matrix control (DMC) are classical examples of MPC.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 972052963},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/NPLS2CJ6/index.html}
}

@article{mondererFictitiousPlayProperty1996,
  title = {Fictitious {{Play Property}} for {{Games}} with {{Identical Interests}}},
  author = {Monderer, Dov and Shapley, Lloyd S.},
  year = {1996},
  month = jan,
  journal = {Journal of Economic Theory},
  volume = {68},
  number = {1},
  pages = {258--265},
  issn = {00220531},
  doi = {10.1006/jeth.1996.0014},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/3M8TS29M/Monderer et Shapley - 1996 - Fictitious Play Property for Games with Identical .pdf}
}

@article{mondererPotentialGames1996,
  title = {Potential {{Games}}},
  author = {Monderer, Dov and Shapley, Lloyd S.},
  year = {1996},
  month = may,
  journal = {Games and Economic Behavior},
  volume = {14},
  number = {1},
  pages = {124--143},
  issn = {0899-8256},
  doi = {10.1006/game.1996.0044},
  abstract = {We define and discuss several notions of potential functions for games in strategic form. We characterize games that have a potential function, and we present a variety of applications.Journal of Economic LiteratureClassification Numbers:C72, C73.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/BVA3J55P/Monderer-Shapley\; Potential Games.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/9DAJPE6Q/S0899825696900445.html}
}

@book{morganWorldModelHow2012,
  title = {The {{World}} in the {{Model}}: {{How Economists Work}} and {{Think}}},
  shorttitle = {The {{World}} in the {{Model}}},
  author = {Morgan, Mary S.},
  year = {2012},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9781139026185},
  isbn = {978-1-139-02618-5},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/BPSPC8CA/Morgan - 2012 - The World in the Model How Economists Work and Th.pdf}
}

@inproceedings{moriyamaLearningRateAdjustingQLearning2008,
  title = {Learning-{{Rate Adjusting Q-Learning}} for {{Prisoner}}'s {{Dilemma Games}}},
  booktitle = {2008 {{IEEE}}/{{WIC}}/{{ACM International Conference}} on {{Web Intelligence}} and {{Intelligent Agent Technology}}},
  author = {Moriyama, Koichi},
  year = {2008},
  month = dec,
  pages = {322--325},
  publisher = {{IEEE}},
  address = {{Sydney, Australia}},
  doi = {10.1109/WIIAT.2008.170},
  abstract = {Many multiagent Q-learning algorithms have been proposed to date, and most of them aim to converge to a Nash equilibrium, which is not desirable in games like the Prisoner's Dilemma (PD). In the previous paper, the author proposed the utility-based Q-learning for PD, which used utilities as rewards in order to maintain mutual cooperation once it had occurred. However, since the agent's action depends on the relation of Q-values the agent has, the mutual cooperation can be maintained by adjusting the learning rate of Q-learning. Thus, in this paper, we deal with the learning rate directly and introduce a new Q-learning method called the learning-rate adjusting Q-learning, or LRA-Q.},
  isbn = {978-0-7695-3496-1},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/Y5AQYT9W/Moriyama - 2008 - Learning-Rate Adjusting Q-Learning for Prisoner's .pdf}
}

@incollection{moriyamaLearningRateAdjustingQLearning2009,
  title = {Learning-{{Rate Adjusting Q-Learning}} for {{Two-Person Two-Action Symmetric Games}}},
  booktitle = {Agent and {{Multi-Agent Systems}}: {{Technologies}} and {{Applications}}},
  author = {Moriyama, Koichi},
  editor = {H{\aa}kansson, Anne and Nguyen, Ngoc Thanh and Hartung, Ronald L. and Howlett, Robert J. and Jain, Lakhmi C.},
  year = {2009},
  volume = {5559},
  pages = {223--232},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-01665-3_23},
  isbn = {978-3-642-01664-6 978-3-642-01665-3},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/3VIVXFG3/Moriyama - 2009 - Learning-Rate Adjusting Q-Learning for Two-Person .pdf}
}

@inproceedings{moriyamaUtilityBasedQlearning2007,
  title = {Utility {{Based Q-learning}} to {{Maintain Cooperation}} in {{Prisoner}}'s {{Dilemma Games}}},
  booktitle = {2007 {{IEEE}}/{{WIC}}/{{ACM International Conference}} on {{Intelligent Agent Technology}} ({{IAT}}'07)},
  author = {Moriyama, Koichi},
  year = {2007},
  month = nov,
  pages = {146--152},
  publisher = {{IEEE}},
  address = {{Fremont, CA, USA}},
  doi = {10.1109/IAT.2007.60},
  abstract = {This work deals with Q-learning in a multiagent environment. There are many multiagent Q-learning methods, and most of them aim to converge to a Nash equilibrium, which is not desirable in games like the Prisoner's Dilemma (PD). However, normal Q-learning agents that use a stochastic method in choosing actions to avoid local optima may bring mutual cooperation in PD. Although such mutual cooperation usually occurs singly, it can be maintained if the Qfunction of cooperation becomes larger than that of defection after the cooperation. This work derives a theorem on how many times the cooperation is needed to make the Qfunction of cooperation larger than that of defection. In addition, from the perspective of the author's previous works that discriminate utilities from rewards and use utilities for learning in PD, this work also derives a corollary on how much utility is necessary to make the Q-function larger by one-shot mutual cooperation.},
  isbn = {978-0-7695-3027-7},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/RNBR6E9N/Moriyama - 2007 - Utility Based Q-learning to Maintain Cooperation i.pdf}
}

@misc{morrowGameTheoryPolitical,
  title = {Game {{Theory}} for {{Political Scientists}}},
  author = {Morrow},
  journal = {Princeton University Press},
  abstract = {Game theory is the mathematical analysis of strategic interaction. In the fifty years since the appearance of von Neumann and Morgenstern's classic Theory of Games and Economic Behavior (Princeton, 1944), game theory has been widely applied to problems in economics. Until recently, however, its usefulness in political science has been underappreciated, in part because of the technical difficulty of the methods developed by economists. James Morrow's book is the first to provide a standard text adapting contemporary game theory to political analysis. It uses a minimum of mathematics to teach the essentials of game theory and contains problems and their solutions suitable for advanced undergraduate and graduate students in all branches of political science. Morrow begins with classical utility and game theory and ends with current research on repeated games and games of incomplete information. The book focuses on noncooperative game theory and its application to international relations, political economy, and American and comparative politics. Special attention is given to models of four topics: bargaining, legislative voting rules, voting in mass elections, and deterrence. An appendix reviews relevant mathematical techniques. Brief bibliographic essays at the end of each chapter suggest further readings, graded according to difficulty. This rigorous but accessible introduction to game theory will be of use not only to political scientists but also to psychologists, sociologists, and others in the social sciences.},
  howpublished = {https://press.princeton.edu/titles/5590.html},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/46ARSRWB/5590.html}
}

@article{MouvementEtudiantsPour2020,
  title = {{Mouvement des \'etudiants pour la r\'eforme de l'enseignement de l'\'economie}},
  year = {2020},
  month = dec,
  journal = {Wikip\'edia},
  abstract = {Le \guillemotleft{} Mouvement des \'etudiants pour la r\'eforme de l'enseignement de l'\'economie \guillemotright, alias \guillemotleft{} Autisme-economie \guillemotright{} est une association compos\'ee d'enseignants et d'\'etudiants en \'economie n\'e en 2000 et dont le but est de demander un autre enseignement de l'\'economie, \'eloign\'e des \guillemotleft{} mod\`eles d\'ecrivant des mondes imaginaires \guillemotright, sans aucun rapport avec les \'economies dans lesquelles nous vivons. Il propose au contraire de mettre au c\oe ur de l'enseignement l'\'etude concr\`ete de l'\'economie mais aussi une approche critique et pluraliste des th\'eories enseign\'ees. Il est proche de l'\'economiste de l'universit\'e Paris 1 Bernard Guerrien. Sur son site, il propose un m\'elange d'articles critiques de la th\'eorie dominante actuelle (la th\'eorie n\'eoclassique) et de textes, traductions qui traitent de probl\`emes \'economiques concrets (monnaie, finance, r\^ole et histoire de l'\'Etat...), afin de montrer qu'il existe des alternatives.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {french},
  annotation = {Page Version ID: 178001156},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/WEXPPAB8/index.html}
}

@article{nedicDistributedSubgradientMethods2009,
  title = {Distributed {{Subgradient Methods}} for {{Multi-Agent Optimization}}},
  author = {Nedic, Angelia and Ozdaglar, Asuman},
  year = {2009},
  month = jan,
  journal = {IEEE Transactions on Automatic Control},
  volume = {54},
  number = {1},
  pages = {48--61},
  issn = {0018-9286},
  doi = {10.1109/TAC.2008.2009515},
  abstract = {We study a distributed computation model for optimizing a sum of convex objective functions corresponding to multiple agents. For solving this (not necessarily smooth) optimization problem, we consider a subgradient method that is distributed among the agents. In this model, each agent minimizes his/her own objective while exchanging information directly or indirectly with other agents in the network. We allow such communication to be asynchronous, local, and with time varying connectivity. We provide convergence results and convergence rate estimates for the subgradient method. Our convergence rate results explicitly characterize the tradeoff between a desired accuracy of the generated approximate optimal solutions and the number of iterations needed to achieve the accuracy.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/XSFRVFZT/Nedic et Ozdaglar - 2009 - Distributed Subgradient Methods for Multi-Agent Op.pdf}
}

@inproceedings{NEURIPS2020_0d2b2061,
  title = {Conservative Q-Learning for Offline Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {1179--1191},
  publisher = {{Curran Associates, Inc.}}
}

@article{newtonEvolutionaryGameTheory2018,
  title = {Evolutionary {{Game Theory}}: {{A Renaissance}}},
  shorttitle = {Evolutionary {{Game Theory}}},
  author = {Newton, Jonathan},
  year = {2018},
  month = may,
  journal = {Games},
  volume = {9},
  number = {2},
  pages = {31},
  issn = {2073-4336},
  doi = {10.3390/g9020031},
  abstract = {Economic agents are not always rational or farsighted and can make decisions according to simple behavioral rules that vary according to situation and can be studied using the tools of evolutionary game theory. Furthermore, such behavioral rules are themselves subject to evolutionary forces. Paying particular attention to the work of young researchers, this essay surveys the progress made over the last decade towards understanding these phenomena, and discusses open research topics of importance to economics and the broader social sciences.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/TRHN63RZ/Newton - 2018 - Evolutionary Game Theory A Renaissance.pdf}
}

@article{neymanContinuoustimeStochasticGames2017,
  title = {Continuous-Time Stochastic Games},
  author = {Neyman, Abraham},
  year = {2017},
  month = jul,
  journal = {Games and Economic Behavior},
  volume = {104},
  pages = {92--130},
  issn = {08998256},
  doi = {10.1016/j.geb.2017.02.004},
  abstract = {We study continuous-time stochastic games, with a focus on the existence of their equilibria that are insensitive to a small imprecision in the specification of players' evaluations of streams of payoffs.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/RPGJ6PWA/Neyman - 2017 - Continuous-time stochastic games.pdf}
}

@book{neymanStochasticGamesApplications2003,
  title = {Stochastic {{Games}} and {{Applications}}},
  editor = {Neyman, Abraham and Sorin, Sylvain},
  year = {2003},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-010-0189-2},
  isbn = {978-1-4020-1493-2 978-94-010-0189-2},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/TCEAZYUK/Neyman et Sorin - 2003 - Stochastic Games and Applications.pdf}
}

@article{nicholasInnovationsLowInput2014,
  title = {Innovations in Low Input and Organic Dairy Supply Chains\textemdash{{What}} Is Acceptable in {{Europe}}?},
  author = {Nicholas, P.K. and Mandolesi, S. and Naspetti, S. and Zanoli, R.},
  year = {2014},
  month = feb,
  journal = {Journal of Dairy Science},
  volume = {97},
  number = {2},
  pages = {1157--1167},
  issn = {00220302},
  doi = {10.3168/jds.2013-7314},
  abstract = {The growth in organic and low-input farming practices is driven by both market demand for high quality, safe food, and European Union policy support, and these types of farming practices are considered in European Union policies for sustainable production, food quality, healthy life, and rural development. However, many constraints to the development of low-input and organic dairy farming supply chains have been identified, including economic, political, and technical constraints. In order for these types of supply chains to develop and provide further benefits to society, innovations are required to improve their sustainability. However, an innovation will only be taken up and result in desirable change if the whole supply chain accepts the innovation. In this paper, Q methodology is used to identify the acceptability of dairy supply chain innovations to low-input and organic supply chain members and consumers in Belgium, Finland, Italy, and the United Kingdom. A strong consensus existed across all respondents on innovations that were deemed as unacceptable. The use of genetically modified and transgenic organisms in the farming system and innovations perceived as conflicting with the naturalness of the production system and products were strongly rejected. Innovations that were strongly liked across all participants in the study were those related to improving animal welfare and improving forage quality to be able to reduce the need for purchased concentrate feeds. Only minor differences existed between countries as to where the priorities lay in terms of innovation acceptability.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/4NKAUPIM/Nicholas et al. - 2014 - Innovations in low input and organic dairy supply .pdf}
}

@article{nik-khahTaleTwoAuctions2008,
  title = {A Tale of Two Auctions},
  author = {{Nik-Khah}, Edward},
  year = {2008},
  month = apr,
  journal = {Journal of Institutional Economics},
  volume = {4},
  number = {1},
  pages = {73--97},
  publisher = {{Cambridge University Press}},
  issn = {1744-1382, 1744-1374},
  doi = {10.1017/S1744137407000859},
  abstract = {Advocates for a `different and innovative approach' to conceptualizing markets have argued that it is possible to reengineer markets to deliver any number of salutary public policy goals. These `consulting engineers for the market economy' have supported their ambitions by referring to the participation of game theorists in the design and implementation of spectrum auctions. However, the variegated and inconsistent lessons drawn from their participation indicate that the role game theorists actually played in the auctions is not well understood. The confusion appears to stem from significant omissions in the available (mostly first-hand) accounts, which are boastful in taking credit for the performance of the auctions but strangely demure in recounting the precise measures undertaken to bring it about. In this paper, I provide an unexpurgated account of the circumstances surrounding the participation of game theorists in the most celebrated of spectrum auctions, those held under the auspices of the US Federal Communications Commission (FCC). Using the FCC's archival records, I recover the suppressed role of the commercial funding of economic research in determining both the extent and the nature of the economists' participation. This analysis emphasizes the crucial importance of the method of funding in determining how economic research is brought to bear on public policy.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/2KMA26NH/A8CD471D2EC2F69BB23B3707CB0B6354.html}
}

@inproceedings{NIPS2010_091d584f,
  title = {Double Q-Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hasselt, Hado},
  editor = {Lafferty, J. and Williams, C. and {Shawe-Taylor}, J. and Zemel, R. and Culotta, A.},
  year = {2010},
  volume = {23},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/5Q5YQJAC/Hasselt - 2010 - Double q-learning.pdf}
}

@inproceedings{NIPS2017_36e729ec,
  title = {Online Reinforcement Learning in Stochastic Games},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wei, Chen-Yu and Hong, Yi-Te and Lu, Chi-Jen},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}}
}

@book{nisanAlgorithmicGameTheory2008,
  title = {Algorithmic Game Theory},
  editor = {Nisan, Noam},
  year = {2008},
  edition = {Repr., [Nachdr.]},
  publisher = {{Cambridge Univ. Press}},
  address = {{Cambridge}},
  isbn = {978-0-521-87282-9},
  langid = {english},
  annotation = {OCLC: 315869343},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/CPYC4KDE/Nisan - 2008 - Algorithmic game theory.pdf}
}

@misc{NouveauModeScrutin,
  title = {\guillemotleft{} {{Un}} Nouveau Mode de Scrutin : Le Jugement Majoritaire \guillemotright, Par {{R}}. {{Laraki}}, Professeur \`a l'{{X}}},
  shorttitle = {\guillemotleft{} {{Un}} Nouveau Mode de Scrutin},
  abstract = {Rida Laraki, professeur au D\'epartement d'\'Economie de l'\'Ecole polytechnique, pr\'esente "Un nouveau mode de scrutin : le jugement majoritaire" \`a l'occasion \`a l'occasion de la 5e \'edition des Jeudis de la Recherche de l'X le 5 f\'evrier 2015. Tous les premiers jeudis du mois, les chercheurs pr\'esentent les travaux men\'es dans les 22 laboratoires de l'\'Ecole polytechnique.}
}

@article{nowakEvolutionaryGamesSpatial1992,
  title = {Evolutionary Games and Spatial Chaos},
  author = {Nowak, Martin A. and May, Robert M.},
  year = {1992},
  month = oct,
  journal = {Nature},
  volume = {359},
  number = {6398},
  pages = {826--829},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/359826a0},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/UAKGEWQM/Nowak et May - 1992 - Evolutionary games and spatial chaos.pdf}
}

@incollection{olszewskiChapter18Calibration2015,
  title = {Chapter 18 - {{Calibration}} and {{Expert Testing}}},
  booktitle = {Handbook of {{Game Theory}} with {{Economic Applications}}},
  author = {Olszewski, Wojciech},
  editor = {Young, H. Peyton and Zamir, Shmuel},
  year = {2015},
  month = jan,
  volume = {4},
  pages = {949--984},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-444-53766-9.00018-5},
  abstract = {I survey and discuss the recent literature on testing experts or probabilistic forecasts, which I would describe as a literature on ``strategic hypothesis testing'' The starting point of this literature is some surprising results of the following type: suppose that a criterion forjudging probabilistic forecasts (which I will call a test) has the property that if data are generated by a probabilistic model, then forecasts generated by that model pass the test. It, then, turns out an agent who knows only the test by which she is going to be judged, but knows nothing about the data-generating process, is able to pass the test by generating forecasts strategically. The literature identifies a large number of tests that are vulnerable to strategic manipulation of uninformed forecasters, but also delivers some tests that cannot be passed without knowledge of the data-generating process. It also provides some results on philosophy of science and financial markets that are related to, and inspired by the results on testing experts.},
  langid = {english},
  keywords = {C18,C70,Calibration and other tests,Probabilistic models,Strategic forecasters},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/FDKBBCZX/Olszewski - 2015 - Chapter 18 - Calibration and Expert Testing.pdf}
}

@misc{OpposeNobelPrize,
  title = {Oppose {{Nobel Prize}} to Warmongers},
  howpublished = {http://www.refusingtokill.net/Israel/NobelPrizeOpposition.htm},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/2LDIZK7Y/NobelPrizeOpposition.html}
}

@phdthesis{panCounterexampleKarlinStrong2015,
  type = {Thesis},
  title = {A Counter-Example to {{Karlin}}'s Strong Conjecture for Fictitious Play},
  author = {Pan, Qinxuan},
  year = {2015},
  abstract = {Fictitious play is a natural dynamic for equilibrium play in zero-sum games, proposed by Brown , and shown to converge by Robinson . Samuel Karlin conjectured in 1959 that fictitious play converges at rate O(t- 1/ 2) with respect to the number of steps t. We disprove this conjecture by showing that, when the payoff matrix of the row player is the n x n identity matrix, fictitious play may converge (for some tie-breaking) at rate as slow as [Omega](t- 1/n).},
  copyright = {M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  annotation = {Accepted: 2016-01-04T20:53:37Z},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/4TZCJQV2/Pan - 2015 - A counter-example to Karlin's strong conjecture fo.pdf}
}

@article{pangalloBestReplyStructure2019,
  title = {Best Reply Structure and Equilibrium Convergence in Generic Games},
  author = {Pangallo, Marco and Heinrich, Torsten and Farmer, J. Doyne},
  year = {2019},
  month = feb,
  journal = {Science Advances},
  volume = {5},
  number = {2},
  pages = {eaat1328},
  publisher = {{American Association for the Advancement of Science}},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aat1328},
  abstract = {Game theory is widely used to model interacting biological and social systems. In some situations, players may converge to an equilibrium, e.g., a Nash equilibrium, but in other situations their strategic dynamics oscillate endogenously. If the system is not designed to encourage convergence, which of these two behaviors can we expect a priori? To address this question, we follow an approach that is popular in theoretical ecology to study the stability of ecosystems: We generate payoff matrices at random, subject to constraints that may represent properties of real-world games. We show that best reply cycles, basic topological structures in games, predict nonconvergence of six well-known learning algorithms that are used in biology or have support from experiments with human players. Best reply cycles are dominant in complicated and competitive games, indicating that in this case equilibrium is typically an unrealistic assumption, and one must explicitly model the dynamics of learning. We offer a new approach to understanding convergence to equilibrium in game theory, inspired by ecology and statistical mechanics. We offer a new approach to understanding convergence to equilibrium in game theory, inspired by ecology and statistical mechanics.},
  chapter = {Research Article},
  copyright = {Copyright \textcopyright{} 2018 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution License 4.0 (CC BY).. This is an open-access article distributed under the terms of the Creative Commons Attribution license, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/IHPCAEGJ/Pangallo et al. - 2019 - Best reply structure and equilibrium convergence i.pdf}
}

@article{pangalloTaxonomyLearningDynamics2022,
  title = {Towards a Taxonomy of Learning Dynamics in 2 \texttimes{} 2 Games},
  author = {Pangallo, Marco and Sanders, James B.T. and Galla, Tobias and Farmer, J. Doyne},
  year = {2022},
  month = mar,
  journal = {Games and Economic Behavior},
  volume = {132},
  pages = {1--21},
  issn = {08998256},
  doi = {10.1016/j.geb.2021.11.015},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/TCNNKPHL/Pangallo et al. - 2022 - Towards a taxonomy of learning dynamics in 2 × 2 g.pdf}
}

@article{parientyEnseignementEconomieLycee2016,
  title = {{L'enseignement de l'\'economie, du lyc\'ee au premier cycle du sup\'erieur}},
  author = {Parienty, Arnaud},
  year = {2016},
  month = nov,
  journal = {L'Economie politique},
  volume = {N\textdegree{} 72},
  number = {4},
  pages = {33--42},
  publisher = {{Alternatives \'economiques}},
  issn = {1293-6146},
  abstract = {{$<$}titre{$>$}R\'esum\'e{$<$}/titre{$>$}Les \'etudiants issus de la s\'erie ES sont bien pr\'epar\'es aux formations qui n\'ecessitent une bonne culture \'economique g\'en\'erale, mais mal aux fili\`eres universitaires d'\'economie-gestion, qui exigent un bon niveau en math\'ematiques.},
  langid = {french},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/Q4BCV9F4/Parienty - 2016 - L'enseignement de l'économie, du lycée au premier .pdf}
}

@article{pemantleMomentConditionsSequence1999,
  title = {Moment Conditions for a Sequence with Negative Drift to Be Uniformly Bounded in {{Lr}}},
  author = {Pemantle, Robin and Rosenthal, Jeffrey S.},
  year = {1999},
  month = jul,
  journal = {Stochastic Processes and their Applications},
  volume = {82},
  number = {1},
  pages = {143--155},
  issn = {03044149},
  doi = {10.1016/S0304-4149(99)00012-5},
  abstract = {Suppose a sequence of random variables \{Xn\} has negative drift when above a certain threshold and has increments bounded in Lp. When p {$>$} 2 this implies that EXn is bounded above by a constant independent of n and the particular sequence \{Xn\}. When p {$\leq$} 2 there are counterexamples showing this does not hold. In general, increments bounded in Lp lead to a uniform Lr bound on Xn+ for any r {$<$} p - 1, but not for r {$\geq$} p - 1. These results are motivated by questions about stability of queueing networks.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/XC7X4DW5/Pemantle et Rosenthal - 1999 - Moment conditions for a sequence with negative dri.pdf}
}

@article{pemantleSurveyRandomProcesses2007,
  title = {A Survey of Random Processes with Reinforcement},
  author = {Pemantle, Robin},
  year = {2007},
  month = jan,
  journal = {Probability Surveys},
  volume = {4},
  number = {none},
  issn = {1549-5787},
  doi = {10.1214/07-PS094},
  abstract = {The models surveyed include generalized Po\textasciiacute lya urns, reinforced random walks, interacting urn models, and continuous reinforced processes. Emphasis is on methods and results, with sketches provided of some proofs. Applications are discussed in statistics, biology, economics and a number of other areas.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/I22AFDUV/Pemantle - 2007 - A survey of random processes with reinforcement.pdf}
}

@book{pepallIndustrialOrganizationContemporary2014,
  title = {Industrial Organization: Contemporary Theory and Empirical Applications},
  shorttitle = {Industrial Organization},
  author = {Pepall, Lynne and Richards, Daniel Jay and Norman, George},
  year = {2014},
  edition = {Fifth edition},
  publisher = {{Wiley}},
  address = {{Hoboken, NJ}},
  isbn = {978-1-118-25030-3},
  langid = {english},
  lccn = {HD31 .P377 2014},
  keywords = {Industrial organization,lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/HJ49E6SU/Pepall et al. - 2014 - Industrial organization contemporary theory and e.pdf}
}

@article{perchetApproachabilityRegret,
  title = {Approachability and {{Regret}}},
  author = {Perchet, Professor Vianney},
  pages = {3},
  abstract = {The relationship between the concept of approachability and that of regret is explored in the finite and linear cases along with an analysis of convergence.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/327LDJTQ/Perchet - Approachability and Regret.pdf}
}

@article{perchetApproachabilityRegretCalibration2014a,
  title = {Approachability, Regret and Calibration: {{Implications}} and Equivalences},
  shorttitle = {Approachability, Regret and Calibration},
  author = {Perchet, Vianney},
  year = {2014},
  journal = {Journal of Dynamics \& Games},
  volume = {1},
  number = {2},
  pages = {181--254},
  issn = {2164-6074},
  doi = {10.3934/jdg.2014.1.181},
  abstract = {Blackwell approachability, regret minimization and calibration are three criteria used to evaluate a strategy (or an algorithm) in sequential decision problems, described as repeated games between a player and Nature. Although they have at first sight not much in common, links between them have been discovered: for instance, both consistent and calibrated strategies can be constructed by following, in some auxiliary game, an approachability strategy.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/7KY25VI9/Perchet - 2013 - Approachability, Regret and Calibration\; implicati.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/D25AUYVS/Perchet et ,Université Paris-Diderot, Laboratoire de Probabilités et Modèles Aléatoires, UMR 7599, 8 place FM13, Paris - 2014 - Approachability, regret and calibration Implicati.pdf}
}

@misc{perchetApproachabilityRegretOnline2021,
  title = {Approachability, Regret and Online Learning. {{Theory}} and {{Open}} Questions},
  author = {Perchet, Vianney},
  year = {2021},
  month = jan,
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/55EW97F2/Perchet.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/ZW7VCNCQ/out-2.ogv}
}

@article{perchetBatchedBanditProblems,
  title = {Batched {{Bandit Problems}}},
  author = {Perchet, Vianney and Rigollet, Philippe and Chassang, Sylvain and Snowberg, Erik},
  pages = {25},
  abstract = {Motivated by practical applications, chiefly clinical trials, we study the regret achievable for stochastic bandits under the constraint that the employed policy must split trials into a small number of batches. We propose a simple policy that operates under this contraint and show that a very small number of batches gives close to minimax optimal regret bounds. As a byproduct, we derive optimal policies with low switching cost for stochastic bandits.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/7RT6VNHY/Perchet et al. - Batched Bandit Problems.pdf}
}

@phdthesis{perkinsAdvancedStochasticApproximation2013,
  title = {Advanced {{Stochastic Approximation Frameworks}} and Their {{Applications}}},
  author = {Perkins, Steven},
  year = {2013},
  month = sep,
  langid = {english},
  school = {University of Bristol},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/L4TNIWHD/Perkins - Advanced Stochastic Approximation Frameworks and t.pdf}
}

@article{perkinsAsynchronousStochasticApproximation2012,
  title = {Asynchronous {{Stochastic Approximation}} with {{Differential Inclusions}}},
  author = {Perkins, Steven and Leslie, David S.},
  year = {2012},
  month = dec,
  journal = {Stochastic Systems},
  volume = {2},
  number = {2},
  pages = {409--446},
  issn = {1946-5238, 1946-5238},
  doi = {10.1287/11-SSY056},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/QIJEQNNM/Perkins et Leslie - 2012 - Asynchronous Stochastic Approximation with Differe.pdf}
}

@article{perkinsGAMETHEORETICALCONTROLCONTINUOUS,
  title = {{{GAME-THEORETICAL CONTROL WITH CONTINUOUS ACTION SETS}}},
  author = {Perkins, Steven and Mertikopoulos, Panayotis and Leslie, David S},
  pages = {20},
  abstract = {Motivated by the recent applications of game-theoretical learning techniques to the design of distributed control systems, we study a class of control problems that can be formulated as potential games with continuous action sets, and we propose an actor-critic reinforcement learning algorithm that provably converges to equilibrium in this class of problems. The method employed is to analyse the learning process under study through a meanfield dynamical system that evolves in an infinite-dimensional function space (the space of probability distributions over the players' continuous controls). To do so, we extend the theory of finite-dimensional two-timescale stochastic approximation to an infinite-dimensional, Banach space setting, and we prove that the continuous dynamics of the process converge to equilibrium in the case of potential games. These results combine to give a provably-convergent learning algorithm in which players do not need to keep track of the controls selected by the other agents.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/RE4C34Q4/Perkins et al. - GAME-THEORETICAL CONTROL WITH CONTINUOUS ACTION SE.pdf}
}

@inproceedings{perolatActorcriticFictitiousPlay2018,
  title = {Actor-Critic Fictitious Play in Simultaneous Move Multistage Games},
  booktitle = {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  author = {Perolat, Julien and Piot, Bilal and Pietquin, Olivier},
  editor = {Storkey, Amos and {Perez-Cruz}, Fernando},
  year = {2018},
  month = apr,
  series = {Proceedings of Machine Learning Research},
  volume = {84},
  pages = {919--928},
  publisher = {{PMLR}},
  abstract = {Fictitious play is a game theoretic iterative procedure meant to learn an equilibrium in normal form games. However, this algorithm requires that each player has full knowledge of other players' strategies. Using an architecture inspired by actor-critic algorithms, we build a stochastic approximation of the fictitious play process. This procedure is on-line, decentralized (an agent has no information of others' strategies and rewards) and applies to multistage games (a generalization of normal form games). In addition, we prove convergence of our method towards a Nash equilibrium in both the cases of zero-sum two-player multistage games and cooperative multistage games. We also provide empirical evidence of the soundness of our approach on the game of Alesia with and without function approximation.},
  pdf = {http://proceedings.mlr.press/v84/perolat18a/perolat18a.pdf},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/P4729C2S/Pérolat et al. - Actor-Critic Fictitious Play in Simultaneous Move .pdf}
}

@inproceedings{perrinFictitiousPlayMean2020,
  title = {Fictitious Play for Mean Field Games: {{Continuous}} Time Analysis and Applications},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Perrin, Sarah and Perolat, Julien and Lauriere, Mathieu and Geist, Matthieu and Elie, Romuald and Pietquin, Olivier},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {13199--13213},
  publisher = {{Curran Associates, Inc.}}
}

@article{PeytonYoung2020,
  title = {Peyton {{Young}}},
  year = {2020},
  month = apr,
  journal = {Wikipedia},
  abstract = {Hobart Peyton Young (born March 9, 1945) is an American game theorist and economist known for his contributions to evolutionary game theory and its application to the study of institutional and technological change, as well as the theory of learning in games. He is currently centennial professor at the London School of Economics, James Meade Professor of Economics Emeritus at the University of Oxford, professorial fellow at Nuffield College Oxford, and research principal at the Office of Financial Research at the U.S. Department of the Treasury. Peyton Young was named a fellow of the Econometric Society in 1995, a fellow of the British Academy in 2007, and a fellow of the American Academy of Arts and Sciences in 2018.  He served as president of the Game Theory Society from 2006\textendash 08.[1] He has published widely on learning in games, the evolution of social norms and institutions, cooperative game theory, bargaining and negotiation, taxation and cost allocation, political representation, voting procedures, and distributive justice.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 948429593},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/W5P4568Q/index.html}
}

@article{piresVertexReinforcedRandom2022,
  title = {Vertex Reinforced Random Walks with Exponential Interaction on Complete Graphs},
  author = {Pires, Benito and Prado, Fernando P. A. and Rosales, Rafael A.},
  year = {2022},
  month = jun,
  journal = {Stochastic Processes and their Applications},
  volume = {148},
  eprint = {2012.14598},
  eprinttype = {arxiv},
  pages = {353--379},
  issn = {03044149},
  doi = {10.1016/j.spa.2022.03.007},
  abstract = {We describe a model for m vertex reinforced interacting random walks on complete graphs with d {$\geq$} 2 vertices. The transition probability of a random walk to a given vertex depends exponentially on the proportion of visits made by all walks to that vertex. The individual proportion of visits is modulated by a strength parameter that can be set equal to any real number. This model covers a large variety of interactions including different vertex repulsion and attraction strengths between any two random walks as well as self-reinforced interactions. We show that the process of empirical vertex occupation measures defined by the interacting random walks converges (a.s.) to the limit set of the flow induced by a smooth vector field. Further, if the set of equilibria of the field is formed by isolated points, then the vertex occupation measures converge (a.s.) to an equilibrium of the field. These facts are shown by means of the construction of a strict Lyapunov function. We show that if the absolute value of the interaction strength parameters are smaller than a certain upper bound, then, for any number of random walks (m {$\geq$} 2) on any graph (d {$\geq$} 2), the vertex occupation measure converges toward a unique equilibrium. We provide two additional examples of repelling random walks for the cases m = d = 2 and m = 3, d = 2. The latter is used to study some properties of three exponentially repelling random walks on Z.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {60K35 (Primary) 37C10; 60J10; 60G50 (Secondary),Mathematics - Probability},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ZSSS7JU6/Pires et al. - 2022 - Vertex reinforced random walks with exponential in.pdf}
}

@book{plonTheorieJeuxPolitique1976,
  title = {{La Th\'eorie des jeux : une politique imaginaire / Michel Plon}},
  shorttitle = {{La Th\'eorie des jeux}},
  author = {du texte Plon, Michel Auteur},
  year = {1976},
  abstract = {La Th\'eorie des jeux : une politique imaginaire / Michel Plon -- 1976 -- livre},
  copyright = {conditions sp\'ecifiques d'utilisation - Projet de num\'erisation des indisponibles},
  langid = {french},
  keywords = {lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/FQNJD2AS/f32.html}
}

@article{polaskyOilProducersAct1992,
  title = {Do Oil Producers Act as `{{Oil}}'Igopolists?},
  author = {Polasky, Stephen},
  year = {1992},
  month = nov,
  journal = {Journal of Environmental Economics and Management},
  volume = {23},
  number = {3},
  pages = {216--247},
  issn = {00950696},
  doi = {10.1016/0095-0696(92)90002-E},
  langid = {english},
  keywords = {lu,numérique},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ZQ4XFBS4/Polasky - 1992 - Do oil producers act as ‘Oil’igopolists.pdf}
}

@misc{PotentialGames,
  title = {Potential {{Games}}},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/E8F88R4Q/201704757-3244-0-41.pdf}
}

@article{powersNewCriteriaNew,
  title = {New Criteria and a New Algorithm for Learning in Multi-Agent Systems},
  author = {Powers, Rob and Shoham, Yoav},
  pages = {8},
  abstract = {We propose a new set of criteria for learning algorithms in multi-agent systems, one that is more stringent and (we argue) better justified than previous proposed criteria. Our criteria, which apply most straightforwardly in repeated games with average rewards, consist of three requirements: (a) against a specified class of opponents (this class is a parameter of the criterion) the algorithm yield a payoff that approaches the payoff of the best response, (b) against other opponents the algorithm's payoff at least approach (and possibly exceed) the security level payoff (or maximin value), and (c) subject to these requirements, the algorithm achieve a close to optimal payoff in self-play. We furthermore require that these average payoffs be achieved quickly. We then present a novel algorithm, and show that it meets these new criteria for a particular parameter class, the class of stationary opponents. Finally, we show that the algorithm is effective not only in theory, but also empirically. Using a recently introduced comprehensive game theoretic test suite, we show that the algorithm almost universally outperforms previous learning algorithms.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ZYXJ52ZE/Powers et Shoham - New criteria and a new algorithm for learning in m.pdf}
}

@article{pradelskiLearningEfficientNash2012,
  title = {Learning Efficient {{Nash}} Equilibria in Distributed Systems},
  author = {Pradelski, Bary S.R. and Young, H. Peyton},
  year = {2012},
  month = jul,
  journal = {Games and Economic Behavior},
  volume = {75},
  number = {2},
  pages = {882--897},
  issn = {08998256},
  doi = {10.1016/j.geb.2012.02.017},
  abstract = {An individual's learning rule is completely uncoupled if it does not depend directly on the actions or payoffs of anyone else. We propose a variant of log linear learning that is completely uncoupled and that selects an efficient (welfare-maximizing) pure Nash equilibrium in all generic n-person games that possess at least one pure Nash equilibrium. In games that do not have such an equilibrium, there is a simple formula that expresses the long-run probability of the various disequilibrium states in terms of two factors: (i) the sum of payoffs over all agents, and (ii) the maximum payoff gain that results from a unilateral deviation by some agent. This welfare/stability trade-off criterion provides a novel framework for analyzing the selection of disequilibrium as well as equilibrium states in n-person games.},
  langid = {english},
  keywords = {lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/PDLW39H8/Pradelski et Young - 2012 - Learning efficient Nash equilibria in distributed .pdf}
}

@article{prescottPrefaceSpecialIssue2016,
  title = {Preface: {{Special Issue}} on {{Dynamic Games}} in {{Macroeconomics}}},
  shorttitle = {Preface},
  author = {Prescott, Edward C. and Reffett, Kevin L.},
  year = {2016},
  month = jun,
  journal = {Dynamic Games and Applications},
  volume = {6},
  number = {2},
  pages = {157--160},
  issn = {2153-0785, 2153-0793},
  doi = {10.1007/s13235-015-0180-0},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/44MQXP53/Prescott et Reffett - 2016 - Preface Special Issue on Dynamic Games in Macroec.pdf}
}

@misc{pressLogicGames,
  title = {Logic in {{Games}}},
  author = {Press, The MIT},
  journal = {The MIT Press},
  abstract = {A comprehensive examination of the interfaces of logic, computer science, and game theory, drawing on twenty years of research on logic and games.                 This book draws on ideas from philosophical logic, computational logic, multi-agent systems, and game theory to offer a comprehensive account of logic and games viewed in two complementary ways. It examines the logic of games: the development of sophisticated modern dynamic logics that model information flow, communication, and interactive structures in games. It also examines logic as games: the idea that logical activities of reasoning and many related tasks can be viewed in the form of games.In doing so, the book takes up the ``intelligent interaction'' of agents engaging in competitive or cooperative activities and examines the patterns of strategic behavior that arise. It develops modern logical systems that can analyze information-driven changes in players' knowledge and beliefs, and introduces the ``Theory of Play'' that emerges from the combination of logic and game theory. This results in a new view of logic itself as an interactive rational activity based on reasoning, perception, and communication that has particular relevance for games.                     Logic in Games, based on a course taught by the author at Stanford University, the University of Amsterdam, and elsewhere, can be used in advanced seminars and as a resource for researchers.},
  howpublished = {https://mitpress.mit.edu/books/logic-games},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/2FS8FJQZ/logic-games.html}
}

@book{preteceilleJeuxModelsSimulations2017,
  title = {{Jeux mod\`els et simulations: Critique des jeux urbains}},
  shorttitle = {{Jeux mod\`els et simulations}},
  author = {Preteceille, Edmond},
  year = {2017},
  month = sep,
  publisher = {{Walter de Gruyter GmbH \& Co KG}},
  googlebooks = {LZRdDwAAQBAJ},
  isbn = {978-3-11-056904-9},
  langid = {french},
  keywords = {Social Science / General,Social Science / Sociology / General,Social Science / Sociology / Urban}
}

@misc{pudlakProofsGames,
  title = {Proofs as {{Games}}},
  author = {Pudl{\'a}k, Puvel},
  howpublished = {https://www.jstor.org/stable/pdf/2589349.pdf?refreqid=excelsior\%3Ad9d69ca4d7d282bbc87233ae6602a8fd},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/SPT4PLWE/2589349.pdf}
}

@article{rajanModellingNoncompetitiveBehavior1990,
  title = {Modelling Non-Competitive Behavior in Commodity Markets: {{A}} Game Theoretic Approach},
  shorttitle = {Modelling Non-Competitive Behavior in Commodity Markets},
  author = {Rajan, R.},
  year = {1990},
  month = dec,
  journal = {Empirical Economics},
  volume = {15},
  number = {4},
  pages = {347--366},
  issn = {0377-7332, 1435-8921},
  doi = {10.1007/BF02307287},
  abstract = {In this paper, we provide a coalitional alternative to the perfectly competitive and purely noncooperative assumptions commonly employed, in the modelling of commodity markets. These assumptions of perfect competition or pure non-cooperation are usually imposed exogenouslywithout providing an economic basis for assuming why firms that could stand to gain by cooperating would not in fact do so. Three behavioral rules embodied in three different cooperative games are discussedin this paper and a methodology for predicting the coalition structures that would result from each of these is offered. By applying these games to the US copper industry of the 1970's, we show that the theory of games can be profitably employed in conjunction with the traditional "institutional approach" of industrial organization to yield useful economic predictions.},
  langid = {english},
  keywords = {numérique},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/QFMAJBSF/Rajan - 1990 - Modelling non-competitive behavior in commodity ma.pdf}
}

@article{ramaswamyStochasticRecursiveInclusion2016,
  title = {Stochastic Recursive Inclusion in Two Timescales with an Application to the {{Lagrangian}} Dual Problem},
  author = {Ramaswamy, Arunselvan and Bhatnagar, Shalabh},
  year = {2016},
  month = nov,
  journal = {Stochastics},
  volume = {88},
  number = {8},
  eprint = {1502.01956},
  eprinttype = {arxiv},
  pages = {1173--1187},
  issn = {1744-2508, 1744-2516},
  doi = {10.1080/17442508.2016.1215450},
  abstract = {In this paper we present a framework to analyze the asymptotic behavior of two timescale stochastic approximation algorithms including those with set-valued mean fields. This paper builds on the works of Borkar and Perkins \& Leslie. The framework presented herein is more general as compared to the synchronous two timescale framework of Perkins \textbackslash\& Leslie, however the assumptions involved are easily verifiable. As an application, we use this framework to analyze the two timescale stochastic approximation algorithm corresponding to the Lagrangian dual problem in optimization theory.},
  archiveprefix = {arXiv},
  keywords = {62L20; 93E03; 93E35; 34A60,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Dynamical Systems,Statistics - Machine Learning},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/PCUS6Q5X/Ramaswamy et Bhatnagar - 2016 - Stochastic recursive inclusion in two timescales w.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/4JL6F22L/1502.html}
}

@article{raquelApplicationGameTheory2007,
  title = {Application of Game Theory for a Groundwater Conflict in {{Mexico}}},
  author = {Raquel, Salazar and Ferenc, Szidarovszky and Emery, Coppola and Abraham, Rojano},
  year = {2007},
  month = sep,
  journal = {Journal of Environmental Management},
  volume = {84},
  number = {4},
  pages = {560--571},
  issn = {0301-4797},
  doi = {10.1016/j.jenvman.2006.07.011},
  abstract = {Exploitation of scarce water resources, particularly in areas of high demand, inevitably produces conflict among disparate stakeholders, each of whom may have their own set of priorities. In order to arrive at a socially acceptable compromise, the decision-makers should seek an optimal trade-off between conflicting objectives that reflect the priorities of the various stakeholders. In this study, game theory was applied to a multiobjective conflict problem for the Alto Rio Lerma Irrigation District, located in the state of Guanajuato in Mexico, where economic benefits from agricultural production should be balanced with associated negative environmental impacts. The short period of rainfall in this area, combined with high groundwater withdrawals from irrigation wells, has produced severe aquifer overdraft. In addition, current agricultural practices of applying high loads of fertilizers and pesticides have contaminated regions of the aquifer. The net economic benefit to this agricultural region in the short-term lies with increasing crop yields, which requires large pumping extractions for irrigation as well as high chemical loading. In the longer term, this can produce economic loss due to higher pumping costs (i.e., higher lift requirements), or even loss of the aquifer as a viable source of water. Negative environmental impacts include continued diminishment of groundwater quality, and declining groundwater levels in the basin, which can damage surface water systems that support environmental habitats. The two primary stakeholders or players, the farmers in the irrigation district and the community at large, must find an optimal balance between positive economic benefits and negative environmental impacts. In this paper, game theory was applied to find the optimal solution between the two conflicting objectives among 12 alternative groundwater extraction scenarios. Different attributes were used to quantify the benefits and costs of the two objectives, and, following generation of the Pareto frontier or trade-off curve, four conflict resolution methods were then applied.},
  langid = {english},
  keywords = {Conflict resolution,Game theory,Optimization techniques,Systems modeling,Water management.},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/AW4GF7FZ/Raquel et al. - 2007 - Application of game theory for a groundwater confl.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/CQBZ4L8W/S0301479706002015.html}
}

@article{raquelApplicationGameTheory2007a,
  title = {Application of Game Theory for a Groundwater Conflict in {{Mexico}}},
  author = {Raquel, Salazar and Ferenc, Szidarovszky and Emery, Coppola and Abraham, Rojano},
  year = {2007},
  month = sep,
  journal = {Journal of Environmental Management},
  volume = {84},
  number = {4},
  pages = {560--571},
  issn = {0301-4797},
  doi = {10.1016/j.jenvman.2006.07.011},
  abstract = {Exploitation of scarce water resources, particularly in areas of high demand, inevitably produces conflict among disparate stakeholders, each of whom may have their own set of priorities. In order to arrive at a socially acceptable compromise, the decision-makers should seek an optimal trade-off between conflicting objectives that reflect the priorities of the various stakeholders. In this study, game theory was applied to a multiobjective conflict problem for the Alto Rio Lerma Irrigation District, located in the state of Guanajuato in Mexico, where economic benefits from agricultural production should be balanced with associated negative environmental impacts. The short period of rainfall in this area, combined with high groundwater withdrawals from irrigation wells, has produced severe aquifer overdraft. In addition, current agricultural practices of applying high loads of fertilizers and pesticides have contaminated regions of the aquifer. The net economic benefit to this agricultural region in the short-term lies with increasing crop yields, which requires large pumping extractions for irrigation as well as high chemical loading. In the longer term, this can produce economic loss due to higher pumping costs (i.e., higher lift requirements), or even loss of the aquifer as a viable source of water. Negative environmental impacts include continued diminishment of groundwater quality, and declining groundwater levels in the basin, which can damage surface water systems that support environmental habitats. The two primary stakeholders or players, the farmers in the irrigation district and the community at large, must find an optimal balance between positive economic benefits and negative environmental impacts. In this paper, game theory was applied to find the optimal solution between the two conflicting objectives among 12 alternative groundwater extraction scenarios. Different attributes were used to quantify the benefits and costs of the two objectives, and, following generation of the Pareto frontier or trade-off curve, four conflict resolution methods were then applied.},
  langid = {english},
  keywords = {Conflict resolution,Game theory,Optimization techniques,Systems modeling,Water management.},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/YZMGDYPP/Raquel et al. - 2007 - Application of game theory for a groundwater confl.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/MHIR3KE4/S0301479706002015.html}
}

@article{rastogiQLearningProvablyEfficient2020,
  title = {Is {{Q-Learning Provably Efficient}}? {{An Extended Analysis}}},
  shorttitle = {Is {{Q-Learning Provably Efficient}}?},
  author = {Rastogi, Kushagra and Lee, Jonathan and {Harel-Canada}, Fabrice and Joglekar, Aditya},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.10396 [cs, math, stat]},
  eprint = {2009.10396},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {This work extends the analysis of the theoretical results presented within the paper Is Q-Learning Provably Efficient? by Jin et al. We include a survey of related research to contextualize the need for strengthening the theoretical guarantees related to perhaps the most important threads of model-free reinforcement learning. We also expound upon the reasoning used in the proofs to highlight the critical steps leading to the main result showing that Q-learning with UCB exploration achieves a sample efficiency that matches the optimal regret that can be achieved by any model-based approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/FLQ2QFCV/Rastogi et al. - 2020 - Is Q-Learning Provably Efficient An Extended Anal.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/Y3CQCAL6/2009.html}
}

@article{reinganumMarketStructureDiffusion23,
  title = {Market {{Structure}} and the {{Diffusion}} of {{New Technology}}},
  author = {Reinganum, Jennifer F.},
  year = 1981,
  journal = {The Bell Journal of Economics},
  volume = {12},
  number = {2},
  pages = {618},
  issn = {0361915X},
  doi = {10.2307/3003576},
  langid = {english},
  keywords = {lu,preuve pas lue},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/8BQRJ92Y/Reinganum - 1981 - Market Structure and the Diffusion of New Technolo.pdf}
}

@techreport{RePEc:cla:levarc:419,
  type = {Levine's Working Paper Archive},
  title = {On the Convergence of Learning Processes in a 2x2 Non-Zero-Person Game},
  author = {Miyasawa, K.},
  year = {1961},
  number = {419},
  institution = {{David K. Levine}},
  abstract = {No abstract is available for this item.}
}

@techreport{riveraEfficiencyCorrelationBottleneck2018,
  type = {{{SSRN Scholarly Paper}}},
  title = {Efficiency of {{Correlation}} in a {{Bottleneck Game}}},
  author = {Rivera, Thomas J. and Scarsini, Marco and Tomala, Tristan},
  year = {2018},
  month = jul,
  number = {ID 3219767},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  doi = {10.2139/ssrn.3219767},
  abstract = {We consider a model of bottleneck congestion in discrete time with a penalty cost for being late. This model can be applied to several situations where agents need to use a capacitated facility in order to complete a task before a hard deadline. A possible example is a situation where commuters use a train service to go from home to office in the early morning. Trains run at regular intervals, take always the same time to cover their itinerary, and have a fixed capacity. Commuters must reach their office in time. This is a hard constraint whose violation involves a heavy penalty. Conditionally on meeting the deadline, commuters want to take the train as late as possible. With the intent of considering strategic choices of departure, we model this situation as a game and we show that it does not have pure Nash equilibria. Then we characterize the best and worst mixed Nash equilibria, and show that they are both inefficient with respect to the social optimum. We then show that there exists a correlated equilibrium that approximates the social optimum when the penalty for missing the deadline is sufficiently large.},
  langid = {english},
  keywords = {correlated equilibrium,efficiency of equilibria,Nash equilibrium,price of anarchy,price of correlated stability.,price of stability},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/RZ6YT2SL/Rivera et al. - 2018 - Efficiency of Correlation in a Bottleneck Game.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/9U7TMYDN/papers.html}
}

@article{riveraStrategicInventoryManagment,
  title = {Strategic {{Inventory Managment}} in {{Capacity Constrained Supply Chains}}},
  author = {Rivera, Thomas J and Scarsini, Marco and Tomala, Tristan},
  pages = {34},
  abstract = {We consider a supply chain with a single wholesaler facing random production disruptions and multiple retailers who decide how early to order their seasonal inventory. When the wholesaler is capacity constrained, there is a production bottleneck which can result in later orders not being fulfilled, imposing a penalty cost on those retailers. When holding inventory is costly, this makes order timing a strategic decision among retailers. We show that when the penalty cost is large then, in any Nash equilibrium, retailers stock their inventory inefficiently early as compared to the centralized optimum, imposing high inventory costs. We then show how pricing can help reduce this inefficiency but that above a certain penalty cost threshold, it is instead optimal to utilize a correlated equilibrium implementation scheme, generating a system of order time recommendations drawn from a joint distribution that are incentive compatible for the retailers to obey.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/P5XULCL6/Rivera et al. - Strategic Inventory Managment in Capacity Constrai.pdf}
}

@article{RobertAumann2020,
  title = {Robert {{Aumann}}},
  year = {2020},
  month = jun,
  journal = {Wikipedia},
  abstract = {Robert John Aumann (Hebrew name: ישראל אומן‎, Yisrael Aumann; born June 8, 1930) is an Israeli-American mathematician, and a member of the United States National Academy of Sciences. He is a professor at the Center for the Study of Rationality in the Hebrew University of Jerusalem in Israel. He also holds a visiting position at Stony Brook University, and is one of the founding members of the Stony Brook Center for Game Theory. Aumann received the Nobel Memorial Prize in Economic Sciences in 2005 for his work on conflict and cooperation through game-theory analysis. He shared the prize with Thomas Schelling.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 964164936},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/2GN7J7AS/index.html}
}

@article{robinsonIterativeMethodSolving1951,
  title = {An {{Iterative Method}} of {{Solving}} a {{Game}}},
  author = {Robinson, Julia},
  year = {1951},
  month = sep,
  journal = {The Annals of Mathematics},
  volume = {54},
  number = {2},
  pages = {296},
  issn = {0003486X},
  doi = {10.2307/1969530},
  langid = {english},
  keywords = {procedure},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/SMI48ZE4/Robinson - 1951 - An Iterative Method of Solving a Game.pdf}
}

@book{rockafellarConvexAnalysis1970,
  title = {Convex Analysis},
  author = {Rockafellar, R. Tyrrell},
  year = {1970},
  series = {Princeton Mathematical Series},
  number = {28},
  publisher = {{Princeton University Press}},
  address = {{Princeton, N.J}},
  isbn = {978-0-691-08069-7},
  lccn = {QA300 .R573},
  keywords = {Convex domains,Mathematical analysis},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ND9JLTGP/Rockafellar - 1970 - Convex analysis.djvu}
}

@article{roemerNewDirectionsMarxian1982,
  title = {New {{Directions}} in the {{Marxian Theory}} of {{Exploitation}} and {{Class}}},
  author = {Roemer, John E.},
  year = {1982},
  month = sep,
  journal = {Politics \& Society},
  volume = {11},
  number = {3},
  pages = {253--287},
  issn = {0032-3292, 1552-7514},
  doi = {10.1177/003232928201100302},
  langid = {english},
  keywords = {lu}
}

@article{rolnickTacklingClimateChange2019,
  title = {Tackling {{Climate Change}} with {{Machine Learning}}},
  author = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and {Milojevic-Dupont}, Nikola and Jaques, Natasha and {Waldman-Brown}, Anna and Luccioni, Alexandra and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
  year = {2019},
  month = nov,
  journal = {arXiv:1906.05433 [cs, stat]},
  eprint = {1906.05433},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/QVEHYXBC/Rolnick et al. - 2019 - Tackling Climate Change with Machine Learning.pdf}
}

@misc{roseDiffusionNewTechnologies1990,
  title = {The Diffusion of New Technologies: Evidence from the Electric Utility Industry},
  author = {Rose, Nancy and Joskow, Paul},
  year = {1990},
  howpublished = {https://economics.mit.edu/files/10813},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/7KQE8ELI/10813.pdf}
}

@article{rosenthalCorrelatedEquilibriaClasses1974,
  title = {Correlated Equilibria in Some Classes of Two-Person Games},
  author = {Rosenthal, R. W.},
  year = {1974},
  month = sep,
  journal = {International Journal of Game Theory},
  volume = {3},
  number = {3},
  pages = {119--128},
  issn = {0020-7276, 1432-1270},
  doi = {10.1007/BF01763252},
  abstract = {A correlated equilibrium in a two-person game is "good" if for every NASH equilibrium there is a player who prefers the correlated equilibrium to the NASn equilibrium. If a game is "bestresponse equivalent" to a two-person zero-sum game, then it has no good correlated equilibria. But games which are "almost strictly competitive" or "order equivalent" to a two-person zero-sum game may have good correlated equilibria.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/UCZ8RQ9H/Rosenthal - 1974 - Correlated equilibria in some classes of two-perso.pdf}
}

@article{rotembergSupergameTheoreticModelPrice1986,
  title = {A {{Supergame-Theoretic Model}} of {{Price Wars}} during {{Booms}}},
  author = {Rotemberg, Julio J and Saloner, Garth},
  year = {1986},
  journal = {American Economic Review},
  number = {76},
  pages = {19},
  langid = {english},
  keywords = {lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/FNAFRL37/Rotemberg et Saloner - A Supergame-Theoretic Model of Price Wars during B.pdf}
}

@article{rothLearningExtensiveformGames1995,
  title = {Learning in Extensive-Form Games: {{Experimental}} Data and Simple Dynamic Models in the Intermediate Term},
  shorttitle = {Learning in Extensive-Form Games},
  author = {Roth, Alvin E. and Erev, Ido},
  year = {1995},
  month = jan,
  journal = {Games and Economic Behavior},
  volume = {8},
  number = {1},
  pages = {164--212},
  issn = {08998256},
  doi = {10.1016/S0899-8256(05)80020-X},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/TT45WZUW/Roth et Erev - 1995 - Learning in extensive-form games Experimental dat.pdf}
}

@article{roughgardenCS261SecondCourse,
  title = {{{CS261}}: {{A Second Course}} in {{Algorithms Lecture}} \#11: {{Online Learning}} and the {{Multiplicative Weights Algorithm}}},
  author = {Roughgarden, Tim},
  pages = {9},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/XWUV9R6X/Roughgarden - CS261 A Second Course in Algorithms Lecture #11 .pdf}
}

@book{rubinsteinEconomicFables,
  title = {Economic {{Fables}}},
  author = {Rubinstein, Ariel},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/5Q4THKRR/Rubinstein - Economic Fables.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/7S2WJB5N/Ariel Rubinstein - Economic Fables-Open Book Publishers (2012).epub}
}

@article{rustichiniMinimizingRegretGeneral1999,
  title = {Minimizing {{Regret}}: {{The General Case}}},
  shorttitle = {Minimizing {{Regret}}},
  author = {Rustichini, Aldo},
  year = {1999},
  month = oct,
  journal = {Games and Economic Behavior},
  volume = {29},
  number = {1-2},
  pages = {224--243},
  issn = {08998256},
  doi = {10.1006/game.1998.0690},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/7A4S224E/Rustichini - 1999 - Minimizing Regret The General Case.pdf}
}

@article{rustichiniOptimalPropertiesStimulus1999,
  title = {Optimal {{Properties}} of {{Stimulus}}\textemdash{{Response Learning Models}}},
  author = {Rustichini, Aldo},
  year = {1999},
  month = oct,
  journal = {Games and Economic Behavior},
  volume = {29},
  number = {1-2},
  pages = {244--273},
  issn = {08998256},
  doi = {10.1006/game.1999.0712},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/LDPU8WK5/Rustichini - 1999 - Optimal Properties of Stimulus—Response Learning M.pdf}
}

@article{samuelsonGameTheoryEconomics2016a,
  title = {Game {{Theory}} in {{Economics}} and {{Beyond}}},
  author = {Samuelson, Larry},
  year = {2016},
  month = nov,
  journal = {Journal of Economic Perspectives},
  volume = {30},
  number = {4},
  pages = {107--130},
  issn = {0895-3309},
  doi = {10.1257/jep.30.4.107},
  abstract = {Within economics, game theory occupied a rather isolated niche in the 1960s and 1970s. It was pursued by people who were known specifically as game theorists and who did almost nothing but game theory, while other economists had little idea what game theory was. Game theory is now a standard tool in economics. Contributions to game theory are made by economists across the spectrum of fields and interests, and economists regularly combine work in game theory with work in other areas. Students learn the basic techniques of game theory in the first-year graduate theory core. Excitement over game theory in economics has given way to an easy familiarity. This essay first examines this transition, arguing that the initial excitement surrounding game theory has dissipated not because game theory has retreated from its initial bridgehead, but because it has extended its reach throughout economics. Next, it discusses some key challenges for game theory, including the continuing problem of dealing with multiple equilibria, the need to make game theory useful in applications, and the need to better integrate noncooperative and cooperative game theory. Finally it considers the current status and future prospects of game theory.},
  langid = {english},
  keywords = {lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/2V22M7RT/Samuelson - 2016 - Game Theory in Economics and Beyond.pdf}
}

@article{saranRegretMatchingFinite2012,
  title = {Regret {{Matching}} with {{Finite Memory}}},
  author = {Saran, Rene and Serrano, Roberto},
  year = {2012},
  month = mar,
  journal = {Dynamic Games and Applications},
  volume = {2},
  number = {1},
  pages = {160--175},
  issn = {2153-0785, 2153-0793},
  doi = {10.1007/s13235-011-0021-8},
  abstract = {We consider the regret matching process with finite memory. For general games in normal form, it is shown that any recurrent class of the dynamics must be such that the action profiles that appear in it constitute a closed set under the ``same or better reply'' correspondence (CUSOBR set) that does not contain a smaller product set that is closed under ``same or better replies,'' i.e., a smaller PCUSOBR set. Two characterizations of the recurrent classes are offered. First, for the class of weakly acyclic games under better replies, each recurrent class is monomorphic and corresponds to each pure Nash equilibrium. Second, for a modified process with random sampling, if the sample size is sufficiently small with respect to the memory bound, the recurrent classes consist of action profiles that are minimal PCUSOBR sets. Our results are used in a robust example that shows that the limiting empirical distribution of play can be arbitrarily far from correlated equilibria for any large but finite choice of the memory bound.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/JL2VQYFM/Saran et Serrano - 2012 - Regret Matching with Finite Memory.pdf}
}

@article{sarrafzadehInternalStabilityLinar1990,
  title = {Internal  {{Stability}} for {{Linar Time Invariant Systems}}},
  author = {Sarrafzadeh, M.},
  year = {1990},
  month = jun,
  journal = {ACM SIGDA Newsletter},
  volume = {20},
  number = {1},
  pages = {91},
  issn = {0163-5743},
  doi = {10.1145/378886.380416},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/K7N88ZKE/Sarrafzadeh - 1990 - Department of electrical engineering and computer .pdf}
}

@article{sayinDecentralizedQLearningZerosum2021,
  title = {Decentralized {{Q-Learning}} in {{Zero-sum Markov Games}}},
  author = {Sayin, Muhammed O. and Zhang, Kaiqing and Leslie, David S. and Basar, Tamer and Ozdaglar, Asuman},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.02748 [cs, math]},
  eprint = {2106.02748},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {We study multi-agent reinforcement learning (MARL) in infinite-horizon discounted zero-sum Markov games. We focus on the practical but challenging setting of decentralized MARL, where agents make decisions without coordination by a centralized controller, but only based on their own payoffs and local actions executed. The agents need not observe the opponent's actions or payoffs, possibly being even oblivious to the presence of the opponent, nor be aware of the zero-sum structure of the underlying game, a setting also referred to as radically uncoupled in the literature of learning in games. In this paper, we develop for the first time a radically uncoupled Q-learning dynamics that is both rational and convergent: the learning dynamics converges to the best response to the opponent's strategy when the opponent follows an asymptotically stationary strategy; the value function estimates converge to the payoffs at a Nash equilibrium when both agents adopt the dynamics. The key challenge in this decentralized setting is the non-stationarity of the learning environment from an agent's perspective, since both her own payoffs and the system evolution depend on the actions of other agents, and each agent adapts their policies simultaneously and independently. To address this issue, we develop a two-timescale learning dynamics where each agent updates her local Q-function and value function estimates concurrently, with the latter happening at a slower timescale.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Mathematics - Dynamical Systems},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/789QGT8R/Sayin et al. - 2021 - Decentralized Q-Learning in Zero-sum Markov Games.pdf}
}

@article{sayinFictitiousPlayZerosum2020,
  title = {Fictitious Play in Zero-Sum Stochastic Games},
  author = {Sayin, Muhammed O. and Parise, Francesca and Ozdaglar, Asu},
  year = {2021},
  month = dec,
  journal = {arXiv:2010.04223 [cs, math]},
  eprint = {2010.04223},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {We present fictitious play dynamics for the general class of stochastic games and analyze its convergence properties in zero-sum stochastic games. Our dynamics involves agents forming beliefs on opponent strategy and their own continuation payoff (Q-function), and playing a myopic best response using estimated continuation payoffs. Agents update their beliefs at states visited from observations of opponent actions. A key property of the learning dynamics is that update of the beliefs on Q-functions occurs at a slower timescale than update of the beliefs on strategies. We show both in the model-based and model-free cases (without knowledge of agent payoff functions and state transition probabilities), the beliefs on strategies converge to a stationary mixed Nash equilibrium of the zero-sum stochastic game.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Mathematics - Dynamical Systems},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/L472WLXS/8 oct - Sayin et al. - 2020 - Fictitious play in zero-sum stochastic games.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/VVHKYA7N/12 oct - Sayin et al. - 2020 - Fictitious play in zero-sum stochastic games.pdf}
}

@article{sbrigliaExperimentsMultiStageCompetition,
  title = {Experiments in {{Multi-Stage R}}\&{{D Competition}}},
  author = {Sbriglia, Patrizia and Hey, John D},
  pages = {26},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/DY2VTAI4/Sbriglia et Hey - Experiments in Multi-Stage R&D Competition.pdf}
}

@article{scarsiniDynamicAtomicCongestion2018,
  title = {Dynamic {{Atomic Congestion Games}} with {{Seasonal Flows}}},
  author = {Scarsini, Marco and Schr{\"o}der, Marc and Tomala, Tristan},
  year = {2018},
  month = apr,
  journal = {Operations Research},
  volume = {66},
  number = {2},
  pages = {327--339},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.2017.1683},
  abstract = {We propose a model of discrete time dynamic congestion games with atomic players and a single source-destination pair. The latencies of edges are composed of freeflow transit times and possible queuing time due to capacity constraints. We give a precise description of the dynamics induced by the individual strategies of players and of the corresponding costs, either when the traffic is controlled by a planner, or when players act selfishly. In parallel networks, optimal and equilibrium behavior eventually coincide, but the selfish behavior of the initial players has consequences that cannot be undone and are paid by all future generations. In more general topologies, our main contributions are threefold. First, we illustrate a new dynamic version of Braess paradox: the presence of initial queues in the network may decrease the long-run costs in equilibrium. This paradox can arise in networks for which no Braess paradox was previously known. Second, we show that equilibria are not unique and can induce very different long-run costs. In particular, we give a sequence of networks such that the price of stability is equal to 1, and the price of anarchy is equal to n - 1, where n is the number of vertices. Third, we propose an extension to model seasonalities by assuming that departure flows fluctuate periodically over time. We introduce a measure that captures the queues induced by periodicity of inflows. For optimal and equilibrium flows in parallel networks this measure is the increase in cost compared to uniform departures.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/IQ2M6PK3/Scarsini et al. - 2018 - Dynamic Atomic Congestion Games with Seasonal Flow.pdf}
}

@article{scarsiniDynamicAtomicCongestion2018a,
  title = {Dynamic {{Atomic Congestion Games}} with {{Seasonal Flows}}},
  author = {Scarsini, Marco and Schr{\"o}der, Marc and Tomala, Tristan},
  year = {2018},
  month = apr,
  journal = {Operations Research},
  volume = {66},
  number = {2},
  eprint = {1606.05691},
  eprinttype = {arxiv},
  pages = {327--339},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.2017.1683},
  abstract = {We propose a model of discrete time dynamic congestion games with atomic players and a single source-destination pair. The latencies of edges are composed by free-flow transit times and possible queuing time due to capacity constraints. We give a precise description of the dynamics induced by the individual strategies of players and of the corresponding costs, either when the traffic is controlled by a planner, or when players act selfishly. In parallel networks, optimal and equilibrium behavior eventually coincides, but the selfish behavior of the first players has consequences that cannot be undone and are paid by all future generations. In more general topologies, our main contributions are three-fold. First, we show that equilibria are usually not unique. In particular, we prove that there exists a sequence of networks such that the price of anarchy is equal to n - 1, where n is the number of vertices, and the price of stability is equal to 1. Second, we illustrate a new dynamic version of Braess's paradox: the presence of initial queues in a network may decrease the long-run costs in equilibrium. This paradox may arise even in networks for which no Braess's paradox was previously known. Third, we propose an extension to model seasonalities by assuming that departure flows fluctuate periodically over time. We introduce a measure that captures the queues induced by periodicity of inflows. This measure is the increase in costs compared to uniform departures for optimal and equilibrium flows in parallel networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {91A43,Computer Science - Computer Science and Game Theory,Mathematics - Optimization and Control},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/DKYA5WWH/Scarsini et al. - 2018 - Dynamic Atomic Congestion Games with Seasonal Flow.pdf}
}

@article{schiffelAutomatedTheoremProving,
  title = {Automated {{Theorem Proving}} for {{General Game Playing}}},
  author = {Schiffel, Stephan and Thielscher, Michael},
  pages = {6},
  abstract = {A general game player is a system that understands the rules of an unknown game and learns to play this game well without human intervention. To succeed in this endeavor, systems need to be able to extract and prove game-specific knowledge from the mere game rules. We present a practical approach to this challenge with the help of Answer Set Programming. The key idea is to reduce the automated theorem proving task to a simple proof of an induction step and its base case. We prove correctness of this method and report on experiments with an offthe-shelf Answer Set Programming system in combination with a successful general game player.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/H9MKURRB/Schiffel et Thielscher - Automated Theorem Proving for General Game Playing.pdf}
}

@article{schmidtDeuxPrixNobel2006,
  title = {{Deux prix Nobel pour la th\'eorie des jeux}},
  author = {Schmidt, Christian},
  year = {2006},
  journal = {Revue d'economie politique},
  volume = {Vol. 116},
  number = {2},
  pages = {133--145},
  publisher = {{Dalloz}},
  issn = {0373-2630},
  abstract = {L\&\#8217;article propose un panorama des principales contributions d\&\#8217;Aumann et de Schelling \&\#224; la th\&\#233;orie des jeux. Il distingue, pour Aumann, les travaux consacr\&\#233;s aux jeux coop\&\#233;ratifs et non coop\&\#233;ratifs. Pour Schelling, il met l\&\#8217;accent sur les jeux de pure coordination et les points focaux, d\&\#8217;une part, les effets collectifs de l\&\#8217;interd\&\#233;pendance des anticipations strat\&\#233;giques, d\&\#8217;autre part. Il montre, en conclusion, l\&\#8217;existence des liens profonds, quoique partiellement cach\&\#233;s, entre les recherches d\&\#8217;Aumann et celles de Schelling concernant les croyances des joueurs et l\&\#8217;approche cognitive d\&\#8217;une situation de jeu.},
  langid = {french},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/GPEHY5GC/Schmidt - 2006 - Deux prix Nobel pour la théorie des jeux.pdf}
}

@article{schoenmakersFictitiousPlayStochastic2007,
  title = {Fictitious Play in Stochastic Games},
  author = {Schoenmakers, G. and Flesch, J. and Thuijsman, F.},
  year = {2007},
  month = sep,
  journal = {Mathematical Methods of Operations Research},
  volume = {66},
  number = {2},
  pages = {315--325},
  issn = {1432-2994, 1432-5217},
  doi = {10.1007/s00186-007-0158-9},
  abstract = {In this paper we examine an extension of the fictitious play process for bimatrix games to stochastic games. We show that the fictitious play process does not necessarily converge, not even in the 2 \texttimes{} 2 \texttimes{} 2 case with a unique equilibrium in stationary strategies. Here 2 \texttimes{} 2 \texttimes{} 2 stands for 2 players, 2 states, 2 actions for each player in each state.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/FVUR7GG3/Schoenmakers et al. - 2007 - Fictitious play in stochastic games.pdf}
}

@article{schreiderGameTheoreticApproach2013,
  title = {Game Theoretic Approach for Fertilizer Application: Looking for the Propensity to Cooperate},
  shorttitle = {Game Theoretic Approach for Fertilizer Application},
  author = {Schreider, S. and Zeephongsekul, P. and Abbasi, B. and Fernandes, M.},
  year = {2013},
  month = jul,
  journal = {Annals of Operations Research},
  volume = {206},
  number = {1},
  pages = {385--400},
  issn = {0254-5330, 1572-9338},
  doi = {10.1007/s10479-013-1381-9},
  abstract = {This paper continues the research implemented in previous work of (Schreider et al. in Environ. Model. Assess. 15(4):223\textendash 238, 2010) where a game theoretic model for optimal fertilizer application in the Hopkins River catchment was formulated, implemented and solved for its optimal strategies. In that work, the authors considered farmers from this catchment as individual players whose objective is to maximize their objective functions which are constituted from two components: economic gain associated with the application of fertilizers which contain phosphorus to the soil and environmental harms associated with this application. The environmental losses are associated with the blue-green algae blooming of the coastal waterways due to phosphorus exported from upstream areas of the catchment. In the previous paper, all agents are considered as rational players and two types of equilibria were considered: fully non-cooperative Nash equilibrium and cooperative Pareto optimum solutions. Among the plethora of Pareto optima, the solution corresponding to the equally weighted individual objective functions were selected. In this paper, the cooperative game approach involving the formation of coalitions and modeling of characteristic value function will be applied and Shapley values for the players obtained. A significant contribution of this approach is the construction of a characteristic function which incorporates both the Nash and Pareto equilibria, showing that it is superadditive. It will be shown that this approach will allow each players to obtain payoffs which strictly dominate their payoffs obtained from their Nash equilibria.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/E2V9G2N9/Schreider et al. - 2013 - Game theoretic approach for fertilizer application.pdf}
}

@article{SciencesRecoursAux2021,
  title = {{Sciences : \guillemotleft{} Le recours aux m\'etriques et aux nombres pr\'esente des revers souvent ignor\'es ou sous-estim\'es \guillemotright}},
  shorttitle = {{Sciences}},
  year = {2021},
  month = sep,
  journal = {Le Monde.fr},
  abstract = {TRIBUNE. Les chercheurs Alexandre Asselineau, Gilles Grolleau et Naoufel Mzoughi d\'ecryptent, dans une tribune au \guillemotleft ~Monde~\guillemotright, les effets parfois nocifs de l'omnipr\'esence des chiffres dans la vie des salari\'es.},
  langid = {french},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/LCGCSTXH/sciences-le-recours-aux-metriques-et-aux-nombres-presente-des-revers-souvent-ignores-ou-sous-es.html}
}

@misc{SelfPlay,
  title = {Self-{{Play}}},
  howpublished = {https://hackernoon.com/self-play-1f69ceb06a4d},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/9Y7DG88P/self-play-1f69ceb06a4d.html}
}

@incollection{seltenAnticipatoryLearningTwoPerson1991,
  title = {Anticipatory {{Learning}} in {{Two-Person Games}}},
  booktitle = {Game {{Equilibrium Models I}}},
  author = {Selten, Reinhard},
  editor = {Selten, Reinhard},
  year = {1991},
  pages = {98--154},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-02674-8_5},
  isbn = {978-3-642-08108-8 978-3-662-02674-8},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/MWS4665W/Selten - 1991 - Anticipatory Learning in Two-Person Games.pdf}
}

@article{seltenChainStoreParadox1978,
  title = {The Chain Store Paradox},
  author = {Selten, Reinhard},
  year = {1978},
  month = apr,
  journal = {Theory and Decision},
  volume = {9},
  number = {2},
  pages = {127--159},
  issn = {1573-7187},
  doi = {10.1007/BF00131770},
  abstract = {The chain store game is a simple game in extensive form which produces an inconsistency between game theoretical reasoning and plausible human behavior. Well-informed players must be expected to disobey game theoretical recommendations.},
  langid = {english},
  keywords = {lu}
}

@book{serfozoBasicsAppliedStochastic2009,
  title = {Basics of Applied Stochastic Processes},
  author = {Serfozo, Richard},
  year = {2009},
  publisher = {{Springer}},
  address = {{Berlin}},
  isbn = {978-3-540-89331-8 978-3-540-89332-5},
  langid = {english},
  lccn = {QA274 .S48 2009},
  keywords = {Probabilities,Stochastic processes},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/D6TSDSEG/Serfozo - 2009 - Basics of applied stochastic processes.pdf}
}

@article{seringConvergencePacketRouting2021,
  title = {Convergence of a {{Packet Routing Model}} to {{Flows Over Time}}},
  author = {Sering, Leon and Koch, Laura Vargas and Ziemke, Theresa},
  year = {2021},
  month = jul,
  journal = {Proceedings of the 22nd ACM Conference on Economics and Computation},
  eprint = {2105.13202},
  eprinttype = {arxiv},
  pages = {797--816},
  doi = {10.1145/3465456.3467626},
  abstract = {CCS Concepts: \textbullet{} Theory of computation \textrightarrow{} Algorithmic game theory; Network games; Network flows; Exact and approximate computation of equilibria; Routing and network design problems; \textbullet{} Mathematics of computing \textrightarrow{} Network flows.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {05C21; 91A07,Computer Science - Computer Science and Game Theory,Computer Science - Data Structures and Algorithms,Mathematics - Optimization and Control},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/5GFU8FPA/Sering et al. - 2021 - Convergence of a Packet Routing Model to Flows Ove.pdf}
}

@article{shalev-shwartzOnlineLearningOnline2011a,
  title = {Online {{Learning}} and {{Online Convex Optimization}}},
  author = {{Shalev-Shwartz}, Shai},
  year = {2011},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {4},
  number = {2},
  pages = {107--194},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000018},
  abstract = {Online learning is a well established learning paradigm which has both theoretical and practical appeals. The goal of online learning is to make a sequence of accurate predictions given knowledge of the correct answer to previous prediction tasks and possibly additional available information. Online learning has been studied in several research fields including game theory, information theory, and machine learning. It also became of great interest to practitioners due the recent emergence of large scale applications such as online advertisement placement and online web ranking. In this survey we provide a modern overview of online learning. Our goal is to give the reader a sense of some of the interesting ideas and in particular to underscore the centrality of convexity in deriving efficient online learning algorithms. We do not mean to be comprehensive but rather to give a high-level, rigorous yet easy to follow, survey.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/PTQ77UEU/Shalev-Shwartz - 2011 - Online Learning and Online Convex Optimization.pdf}
}

@article{shapleyStochasticGames1953,
  title = {Stochastic {{Games}}},
  author = {Shapley, L. S.},
  year = {1953},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {39},
  number = {10},
  pages = {1095--1100},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.39.10.1095},
  abstract = {In a stochastic game the play proceeds by steps from position to position, according to transition probabilities controlled jointly by the two players. We shall assume a finite number, N , of positions, and finite numbers m  K , n  K  of choices at each position; nevertheless, the game may not be bounded in length. If, when at position k , the players choose their i th and j th alternatives, respectively, then with probability {$<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"{$><$}mml:mrow{$><$}mml:msubsup{$><$}mml:mi{$>$}s{$<$}/mml:mi{$><$}mml:mrow{$><$}mml:mi{$>$}i{$<$}/mml:mi{$><$}mml:mi{$>$}j{$<$}/mml:mi{$><$}/mml:mrow{$><$}mml:mi{$>$}k{$<$}/mml:mi{$><$}/mml:msubsup{$><$}mml:mo{$>><$}/mml:mo{$><$}mml:mn{$>$}0{$<$}/mml:mn{$><$}/mml:mrow{$><$}/mml:math{$>$} the game stops, while with probability {$<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"{$><$}mml:mrow{$><$}mml:msubsup{$><$}mml:mi{$>$}p{$<$}/mml:mi{$><$}mml:mrow{$><$}mml:mi{$>$}i{$<$}/mml:mi{$><$}mml:mi{$>$}j{$<$}/mml:mi{$><$}/mml:mrow{$><$}mml:mrow{$><$}mml:mi{$>$}k{$<$}/mml:mi{$><$}mml:mi{$>$}l{$<$}/mml:mi{$><$}/mml:mrow{$><$}/mml:msubsup{$><$}/mml:mrow{$><$}/mml:math{$>$} the game moves to position l . Define{$<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"{$><$}mml:mrow{$><$}mml:mi{$>$}s{$<$}/mml:mi{$><$}mml:mo{$>$}={$<$}/mml:mo{$><$}mml:munder{$><$}mml:mrow{$><$}mml:mi{$>$}min{$<$}/mml:mi{$><$}/mml:mrow{$><$}mml:mrow{$><$}mml:mi{$>$}k{$<$}/mml:mi{$><$}mml:mo{$>$},{$<$}/mml:mo{$><$}mml:mi{$>$}i{$<$}/mml:mi{$><$}mml:mo{$>$},{$<$}/mml:mo{$><$}mml:mi{$>$}j{$<$}/mml:mi{$><$}/mml:mrow{$><$}/mml:munder{$><$}mml:mo{$>$}\,{$<$}/mml:mo{$><$}mml:msubsup{$><$}mml:mi{$>$}s{$<$}/mml:mi{$><$}mml:mrow{$><$}mml:mi{$>$}i{$<$}/mml:mi{$><$}mml:mi{$>$}j{$<$}/mml:mi{$><$}/mml:mrow{$><$}mml:mi{$>$}k{$<$}/mml:mi{$><$}/mml:msubsup{$><$}mml:mo{$>$}.{$<$}/mml:mo{$><$}/mml:mrow{$><$}/mml:math{$>$}Since s is positive, the game ends with probability 1 after a finite number of steps, because, for any number t , the probability that it has not stopped after t steps is not more than (1 - s ) t  .  Payments accumulate throughout the course of play: the first player takes {$<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"{$><$}mml:mrow{$><$}mml:msubsup{$><$}mml:mi{$>$}a{$<$}/mml:mi{$><$}mml:mrow{$><$}mml:mi{$>$}i{$<$}/mml:mi{$><$}mml:mi{$>$}j{$<$}/mml:mi{$><$}/mml:mrow{$><$}mml:mi{$>$}k{$<$}/mml:mi{$><$}/mml:msubsup{$><$}/mml:mrow{$><$}/mml:math{$>$} from the second whenever the pair i , j is chosen at position k. If we define the bound M: {$<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"{$><$}mml:mrow{$><$}mml:mi{$>$}M{$<$}/mml:mi{$><$}mml:mo{$>$}={$<$}/mml:mo{$><$}mml:munder{$><$}mml:mrow{$><$}mml:mi{$>$}max{$<$}/mml:mi{$><$}/mml:mrow{$><$}mml:mrow{$><$}mml:mi{$>$}k{$<$}/mml:mi{$><$}mml:mo{$>$},{$<$}/mml:mo{$><$}mml:mi{$>$}i{$<$}/mml:mi{$><$}mml:mo{$>$},{$<$}/mml:mo{$><$}mml:mi{$>$}j{$<$}/mml:mi{$><$}/mml:mrow{$><$}/mml:munder{$><$}mml:mrow{$><$}mml:mo{$>$}|{$<$}/mml:mo{$><$}mml:mrow{$><$}mml:msubsup{$><$}mml:mi{$>$}a{$<$}/mml:mi{$><$}mml:mrow{$><$}mml:mi{$>$}i{$<$}/mml:mi{$><$}mml:mi{$>$}j{$<$}/mml:mi{$><$}/mml:mrow{$><$}mml:mi{$>$}k{$<$}/mml:mi{$><$}/mml:msubsup{$><$}/mml:mrow{$><$}mml:mo{$>$}|{$<$}/mml:mo{$><$}/mml:mrow{$><$}mml:mo{$>$},{$<$}/mml:mo{$><$}/mml:mrow{$><$}/mml:math{$>$}then we see that the expected total gain or loss is bounded by{$<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"{$><$}mml:mrow{$><$}mml:mi{$>$}M{$<$}/mml:mi{$><$}mml:mo{$>$}+{$<$}/mml:mo{$><$}mml:mrow{$><$}mml:mo{$>$}({$<$}/mml:mo{$><$}mml:mrow{$><$}mml:mn{$>$}1{$<$}/mml:mn{$><$}mml:mo{$>-<$}/mml:mo{$><$}mml:mi{$>$}s{$<$}/mml:mi{$><$}/mml:mrow{$><$}mml:mo{$>$}){$<$}/mml:mo{$><$}/mml:mrow{$><$}mml:mi{$>$}M{$<$}/mml:mi{$><$}mml:mo{$>$}+{$<$}/mml:mo{$><$}mml:msup{$><$}mml:mrow{$><$}mml:mrow{$><$}mml:mo{$>$}({$<$}/mml:mo{$><$}mml:mrow{$><$}mml:mn{$>$}1{$<$}/mml:mn{$><$}mml:mo{$>-<$}/mml:mo{$><$}mml:mi{$>$}s{$<$}/mml:mi{$><$}/mml:mrow{$><$}mml:mo{$>$}){$<$}/mml:mo{$><$}/mml:mrow{$><$}/mml:mrow{$><$}mml:mn{$>$}2{$<$}/mml:mn{$><$}/mml:msup{$><$}mml:mi{$>$}M{$<$}/mml:mi{$><$}mml:mo{$>$}+{$<$}/mml:mo{$><$}mml:mo{$>\ldots <$}/mml:mo{$><$}mml:mo{$>$}={$<$}/mml:mo{$><$}mml:mrow{$><$}mml:mi{$>$}M{$<$}/mml:mi{$><$}mml:mo{$>$}/{$<$}/mml:mo{$><$}mml:mi{$>$}s{$<$}/mml:mi{$><$}/mml:mrow{$><$}mml:mo{$>$}.{$<$}/mml:mo{$><$}/mml:mrow{$><$}/mml:math{$>$}(1) The process therefore depends on N 2 + N matrices{$<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"{$><$}mml:mrow{$><$}mml:msup{$><$}mml:mi{$>$}P{$<$}/mml:mi{$><$}mml:mrow{$><$}mml:mi{$>$}K{$<$}/mml:mi{$><$}mml:mi{$>$}l{$<$}/mml:mi{$><$}/mml:mrow{$><$}/mml:msup{$><$}mml:mo{$>$}={$<$}/mml:mo{$><$}mml:mrow{$><$}mml:mo{$>$}({$<$}/mml:mo{$><$}mml:mrow{$><$}mml:mrow{$><$}mml:mrow{$><$}mml:msubsup{$><$}mml:mi{$>$}p{$<$}/mml:mi{$><$}mml:mrow{$><$}mml:mi{$>$}i{$<$}/mml:mi{$><$}mml:mi{$>$}j{$<$}/mml:mi{$><$}/mml:mrow{$><$}mml:mrow{$><$}mml:mi{$>$}k{$<$}/mml:mi{$><$}mml:mi{$>$}l{$<$}/mml:mi{$><$}/mml:mrow{$><$}/mml:msubsup{$><$}/mml:mrow{$><$}mml:mo{$>$}|{$<$}/mml:mo{$><$}/mml:mrow{$><$}mml:mi{$>$}i{$<$}/mml:mi{$><$}mml:mo{$>$}={$<$}/mml:mo{$><$}mml:mn{$>$}1{$<$}/mml:mn{$><$}mml:mo{$>$},{$<$}/mml:mo{$><$}mml:mn{$>$}2{$<$}/mml:mn{$><$}mml:mo{$>$},{$<$}/mml:mo{$><$}mml:mo{$>\ldots <$}/mml:mo{$><$}mml:mo{$>$},{$<$}/mml:mo{$><$}mml:msub{$><$}mml:mi{$>$}m{$<$}/mml:mi{$><$}mml:mi{$>$}K{$<$}/mml:mi{$><$}/mml:msub{$><$}mml:mi mathvariant="italic"{$>$};{$<$}/mml:mi{$><$}mml:mo{$>$}\hspace{0.6em}{$<$}/mml:mo{$><$}mml:mi{$>$}j{$<$}/mml:mi{$><$}mml:mo{$>$}={$<$}/mml:mo{$><$}mml:mn{$>$}1{$<$}/mml:mn{$><$}mml:mo{$>$},{$<$}/mml:mo{$><$}mml:mn{$>$}2{$<$}/mml:mn{$><$}mml:mo{$>$},{$<$}/mml:mo{$><$}mml:mo{$>\ldots <$}/mml:mo{$><$}mml:mo{$>$},{$<$}/mml:mo{$><$}mml:msub{$><$}mml:mi{$>$}n{$<$}/mml:mi{$><$}mml:mi{$>$}K{$<$}/mml:mi{$><$}/mml:msub{$><$}/mml:mrow{$><$}mml:mo{$>$}){$<$}/mml:mo{$><$}/mml:mrow{$><$}/mml:mrow{$><$}/mml:math{$><$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"{$><$}mml:mrow{$><$}mml:msup{$><$}mml:mi{$>$}A{$<$}/mml:mi{$><$}mml:mi{$>$}K{$<$}/mml:mi{$><$}/mml:msup{$><$}mml:mo{$>$}={$<$}/mml:mo{$><$}mml:mrow{$><$}mml:mo{$>$}({$<$}/mml:mo{$><$}mml:mrow{$><$}mml:mrow{$><$}mml:mrow{$><$}mml:msubsup{$><$}mml:mi{$>$}a{$<$}/mml:mi{$><$}mml:mrow{$><$}mml:mi{$>$}i{$<$}/mml:mi{$><$}mml:mi{$>$}j{$<$}/mml:mi{$><$}/mml:mrow{$><$}mml:mi{$>$}k{$<$}/mml:mi{$><$}/mml:msubsup{$><$}/mml:mrow{$><$}mml:mo{$>$}| \ldots{} {$<$}/mml:mo{$><$}/mml:mrow{$><$}/mml:mrow{$><$}/mml:mrow{$><$}/mml:mrow{$><$}/mml:math{$>$}},
  chapter = {Mathematics},
  langid = {english},
  pmid = {16589380},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/GKR6UBW7/Shapley - 1953 - Stochastic Games.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/GWZ4UCND/1095.html}
}

@incollection{shapleyTopicsTwoPersonGames1964,
  title = {Some {{Topics}} in {{Two-Person Games}}},
  booktitle = {Advances in {{Game Theory}}. ({{AM-52}}), {{Volume}} 52},
  author = {Shapley, L. S.},
  editor = {Dresher, Melvin and Shapley, Lloyd S. and Tucker, Albert William},
  year = {1964},
  pages = {1--28},
  publisher = {{Princeton University Press}},
  doi = {doi:10.1515/9781400882014-002}
}

@book{shiriaevProbability1984,
  title = {Probability},
  author = {Shir\t{ia}ev, Al'bert Nikolaevich},
  year = {1984},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  abstract = {This textbook is based on a three-semester course of lectures given by the author in recent years in the Mechanics-Mathematics Faculty of Moscow State University and issued, in part, in mimeographed form under the title Probability, Statistics, Stochastic Processes, I, II by the Moscow State University Press. We follow tradition by devoting the first part of the course (roughly one semester) to the elementary theory of probability (Chapter I). This begins with the construction of probabilistic models with finitely many outcomes and introduces such fundamental probabilistic concepts as sample spaces, events, probability, independence, random variables, expectation, corre lation, conditional probabilities, and so on. Many probabilistic and statistical regularities are effectively illustrated even by the simplest random walk generated by Bernoulli trials. In this connection we study both classical results (law of large numbers, local and integral De Moivre and Laplace theorems) and more modern results (for example, the arc sine law). The first chapter concludes with a discussion of dependent random vari ables generated by martingales and by Markov chains.},
  isbn = {978-1-4899-0018-0 978-1-4899-0020-3},
  langid = {english},
  annotation = {OCLC: 681172491},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/CUBPHD7R/Shiri︠a︡ev - 1984 - Probability.pdf}
}

@article{shohamIfMultiagentLearning2007,
  title = {If Multi-Agent Learning Is the Answer, What Is the Question?},
  author = {Shoham, Yoav and Powers, Rob and Grenager, Trond},
  year = {2007},
  month = may,
  journal = {Artificial Intelligence},
  volume = {171},
  number = {7},
  pages = {365--377},
  issn = {00043702},
  doi = {10.1016/j.artint.2006.02.006},
  abstract = {The area of learning in multi-agent systems is today one of the most fertile grounds for interaction between game theory and artificial intelligence. We focus on the foundational questions in this interdisciplinary area, and identify several distinct agendas that ought to, we argue, be separated. The goal of this article is to start a discussion in the research community that will result in firmer foundations for the area.1 \textcopyright{} 2007 Published by Elsevier B.V.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/Z3G42TSH/Shoham et al. - 2007 - If multi-agent learning is the answer, what is the.pdf}
}

@book{shohamMultiagentSystemsAlgorithmic2009,
  title = {Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations},
  shorttitle = {Multiagent Systems},
  author = {Shoham, Yoav and {Leyton-Brown}, Kevin},
  year = {2009},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge ; New York}},
  isbn = {978-0-521-89943-7},
  lccn = {QA76.76.I58 S75 2009},
  keywords = {Distributed processing,Electronic data processing,Intelligent agents (Computer software)},
  annotation = {OCLC: ocn213408653},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/CWCQIQDJ/Shoham - Multiagent Systems Algorithmic, Game-Theoretic, a.pdf}
}

@book{shwartzHandbookMarkovDecision2001,
  title = {Handbook of {{Markov Decision Processes}}: {{Methods}} and {{Applications}} ({{International Series}} in {{Operations Research}} \& {{Management Science}})},
  shorttitle = {Handbook of {{Markov Decision Processes}}},
  author = {Shwartz, Adam},
  year = {2001},
  month = aug,
  edition = {1 edition},
  publisher = {{Springer}},
  keywords = {Business \& Economics / Operations Research,Business \& Economics-Operations Research,Business \& Management,General,Markov Processes,Mathematical Statistics,Mathematics,Mathematics / Statistics,Medical-General,Operations Research,Probability \& statistics,Probability \& Statistics - General,Science/Mathematics,Statistical decision,Stochastics},
  annotation = {Open Library ID: OL7809686M},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/XD3ZC3ZA/Shwartz - 2001 - Handbook of Markov Decision Processes Methods and.djvu}
}

@article{smerdonEverybodyDoingIt2019,
  title = {`{{Everybody}}'s Doing It': On the Persistence of Bad Social Norms},
  shorttitle = {`{{Everybody}}'s Doing It'},
  author = {Smerdon, David and Offerman, Theo and Gneezy, Uri},
  year = {2019},
  month = may,
  journal = {Experimental Economics},
  issn = {1386-4157, 1573-6938},
  doi = {10.1007/s10683-019-09616-z},
  abstract = {We investigate how information about the preferences of others affects the persistence of `bad' social norms. One view is that bad norms thrive even when people are informed of the preferences of others, since the bad norm is an equilibrium of a coordination game. The other view is based on pluralistic ignorance, in which uncertainty about others' preferences is crucial. In an experiment, we find clear support for the pluralistic ignorance perspective . In addition, the strength of social interactions is important for a bad norm to persist. These findings help in understanding the causes of such bad norms, and in designing interventions to change them.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/FMC37NNV/Smerdon et al. - 2019 - ‘Everybody’s doing it’ on the persistence of bad .pdf}
}

@article{smithGameTheoryExperimental1992,
  title = {Game {{Theory}} and {{Experimental Economics}}: {{Beginnings}} and {{Early Influences}}},
  shorttitle = {Game {{Theory}} and {{Experimental Economics}}},
  author = {Smith, V. L.},
  year = {1992},
  month = jan,
  journal = {History of Political Economy},
  volume = {24},
  number = {Supplement},
  pages = {241--282},
  issn = {0018-2702, 1527-1919},
  doi = {10.1215/00182702-24-Supplement-241},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/TMHJ3PFZ/Smith - 1992 - Game Theory and Experimental Economics Beginnings.pdf}
}

@article{solalQuoiSertTheorie2014,
  title = {{\`A quoi sert la th\'eorie des jeux ?}},
  author = {Solal, Philippe},
  year = {2014},
  month = dec,
  journal = {La Vie des id\'ees},
  publisher = {{La Vie des id\'ees}},
  abstract = {\`A propos de : Robert Leonard, Von Neumann, Morgenstern and the Creation of Game Theory. From Chess to Social Science, 1900-1960. New York, Cambridge UP},
  langid = {french},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/7MAT8455/A-quoi-sert-la-theorie-des-jeux.html}
}

@article{solanCorrelatedEquilibriumStochastic2002,
  title = {Correlated {{Equilibrium}} in {{Stochastic Games}}},
  author = {Solan, Eilon and Vieille, Nicolas},
  year = {2002},
  month = feb,
  journal = {Games and Economic Behavior},
  volume = {38},
  number = {2},
  pages = {362--399},
  issn = {08998256},
  doi = {10.1006/game.2001.0887},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/SDCC5J6T/Solan et Vieille - 2002 - Correlated Equilibrium in Stochastic Games.pdf}
}

@article{solferinoEconomicsAnalysisQlearning2018,
  title = {The Economics Analysis of a {{Q-learning}} Model of Cooperation with Punishment and Risk Taking Preferences},
  author = {Solferino, Nazaria and Solferino, Viviana and Taurino, Serena F.},
  year = {2018},
  month = oct,
  journal = {Journal of Economic Interaction and Coordination},
  volume = {13},
  number = {3},
  pages = {601--613},
  issn = {1860-711X, 1860-7128},
  doi = {10.1007/s11403-017-0195-2},
  abstract = {The aim of this paper is to better understand how cooperation mechanisms work in the context of a Q-learning model. We apply a learning reinforcement model to analyse the conditions needed to have a stable cooperative equilibrium when people take part in a common project and could take advantages of free-riding. Our results show that a stable equilibrium can be reached thank to mechanisms of punishment, but the final result strongly depends on the risk-taking individuals' preferences. In particular, we find that the penalties will be effective only with people having high exploration rates,namely with people able to adapt their strategies and learn to cooperate. Otherwise, it is possible to have an unstable equilibrium with cooperation until individuals have a very high intrinsic motivation to cooperate, whatever the others do.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/8RV878ES/Solferino et al. - 2018 - The economics analysis of a Q-learning model of co.pdf}
}

@article{sorinMergingReputationRepeated1999,
  title = {Merging, {{Reputation}}, and {{Repeated Games}} with {{Incomplete Information}}},
  author = {Sorin, Sylvain},
  year = {1999},
  month = oct,
  journal = {Games and Economic Behavior},
  volume = {29},
  number = {1-2},
  pages = {274--308},
  issn = {08998256},
  doi = {10.1006/game.1999.0722},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/H4IV6Q26/Sorin - 1999 - Merging, Reputation, and Repeated Games with Incom.pdf}
}

@misc{SortirMondesImaginaires,
  title = {Sortir Des Mondes Imaginaires ({{Autisme-Economie}}.Org)},
  howpublished = {http://www.autisme-economie.org/article12.html}
}

@article{sparrowFictitiousPlayGames2008,
  title = {Fictitious Play in 3\texttimes 3 Games: {{The}} Transition between Periodic and Chaotic Behaviour},
  shorttitle = {Fictitious Play in 3\texttimes 3 Games},
  author = {Sparrow, Colin and {van Strien}, Sebastian and Harris, Christopher},
  year = {2008},
  month = may,
  journal = {Games and Economic Behavior},
  volume = {63},
  number = {1},
  pages = {259--291},
  issn = {0899-8256},
  doi = {10.1016/j.geb.2007.08.005},
  abstract = {In the 1960s Shapley provided an example of a two-player fictitious game with periodic behaviour. In this game, player A aims to copy B's behaviour and player B aims to play one ahead of player A. In this paper we generalise Shapley's example by introducing an external parameter. We show that the periodic behaviour in Shapley's example at some critical parameter value disintegrates into unpredictable (chaotic) behaviour, with players dithering a huge number of times between different strategies. At a further critical parameter the dynamics becomes periodic again, but now both players aim to play one ahead of the other. In this paper we adopt a geometric (dynamical systems) approach. Here we prove rigorous results on continuity of the dynamics and on the periodic behaviour, while in the sequel to this paper we shall describe the chaotic behaviour.},
  langid = {english},
  keywords = {Bifurcation,Chaos,Fictitious pay,Learning process}
}

@inproceedings{NEURIPS2020_3b2acfe2,
 author = {Daskalakis, Constantinos and Foster, Dylan J and Golowich, Noah},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {5527--5540},
 publisher = {Curran Associates, Inc.},
 title = {Independent Policy Gradient Methods for Competitive Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/3b2acfe2e38102074656ed938abf4ac3-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{LPOP2022,
 author = {Leonardos, S. and I., Panageas and W., Overman and G., Piliouras},
 booktitle = {ICLR-22: 10th International Conference on Learning Representations, to appear},
 title = { Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games },
 doi = {https://arxiv.org/pdf/2106.01969.pdf},
 year = {2022}
}

@article{stoltzInternalRegretOnline2005,
  title = {Internal {{Regret}} in {{On-line Portfolio Selection}}},
  author = {Stoltz, Gilles and Lugosi, Gabor},
  year = {2005},
  pages = {40},
  abstract = {This paper extends the game-theoretic notion of internal regret to the case of on-line potfolio selection problems. New sequential investment strategies are designed to minimize the cumulative internal regret for all possible market behaviors. Some of the introduced strategies, apart from achieving a small internal regret, achieve an accumulated wealth almost as large as that of the best constantly rebalanced portfolio. It is argued that the low-internal-regret property is related to stability and experiments on real stock exchange data demonstrate that the new strategies achieve better returns compared to some known algorithms.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/TDXL3AQP/Stoltz et Lugosi - Internal Regret in On-line Portfolio Selection.pdf}
}

@article{stoltzLearningCorrelatedEquilibria2007,
  title = {Learning Correlated Equilibria in Games with Compact Sets of Strategies},
  author = {Stoltz, Gilles and Lugosi, G{\'a}bor},
  year = {2007},
  month = apr,
  journal = {Games and Economic Behavior},
  volume = {59},
  number = {1},
  pages = {187--208},
  issn = {08998256},
  doi = {10.1016/j.geb.2006.04.007},
  abstract = {Hart and Schmeidler's extension of correlated equilibrium to games with infinite sets of strategies is studied. General properties of the set of correlated equilibria are described. It is shown that, just like for finite games, if all players play according to an appropriate regret-minimizing strategy then the empirical frequencies of play converge to the set of correlated equilibria whenever the strategy sets are convex and compact.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/6VRMDES2/Stoltz et Lugosi - 2007 - Learning correlated equilibria in games with compa.pdf}
}

@book{stroockIntroductionMarkovProcesses2014,
  title = {An Introduction to {{Markov}} Processes},
  author = {Stroock, Daniel W.},
  year = {2014},
  series = {Graduate Texts in Mathematics},
  edition = {2. ed},
  number = {230},
  publisher = {{Springer}},
  address = {{Berlin}},
  isbn = {978-3-642-40522-8},
  langid = {english},
  annotation = {OCLC: 864568346},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/FVCDEER4/Stroock - 2014 - An introduction to Markov processes.pdf}
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  lccn = {Q325.6 .R45 2018},
  keywords = {Reinforcement learning},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/WYHZILAU/Sutton et Barto - 2018 - Reinforcement learning an introduction.pdf}
}

@article{swensonBestResponseDynamicsPotential2018,
  title = {On {{Best-Response Dynamics}} in {{Potential Games}}},
  author = {Swenson, Brian and Murray, Ryan and Kar, Soummya},
  year = {2018},
  month = jan,
  journal = {SIAM Journal on Control and Optimization},
  volume = {56},
  number = {4},
  pages = {2734--2767},
  issn = {0363-0129, 1095-7138},
  doi = {10.1137/17M1139461},
  abstract = {The paper studies the convergence properties of (continuous) best-response dynamics from game theory. Despite their fundamental role in game theory, best-response dynamics are poorly understood in many games of interest due to the discontinuous, set-valued nature of the best-response map. The paper focuses on elucidating several important properties of best-response dynamics in the class of multi-agent games known as potential games\textemdash a class of games with fundamental importance in multi-agent systems and distributed control. It is shown that in almost every potential game and for almost every initial condition, the best-response dynamics (i) have a unique solution, (ii) converge to pure-strategy Nash equilibria, and (iii) converge at an exponential rate.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ULZRZY8T/Swenson et al. - 2018 - On Best-Response Dynamics in Potential Games.pdf}
}

@inproceedings{swensonExponentialRateConvergence2017,
  title = {On the Exponential Rate of Convergence of Fictitious Play in Potential Games},
  booktitle = {2017 55th {{Annual Allerton Conference}} on {{Communication}}, {{Control}}, and {{Computing}} ({{Allerton}})},
  author = {Swenson, Brian and Kar, Soummya},
  year = {2017},
  month = oct,
  pages = {275--279},
  publisher = {{IEEE}},
  address = {{Monticello, IL}},
  doi = {10.1109/ALLERTON.2017.8262748},
  abstract = {The paper studies fictitious play (FP) learning dynamics in continuous time. It is shown that in almost every potential game, and for almost every initial condition, the rate of convergence of FP is exponential. In particular, the paper focuses on studying the behavior of FP in potential games in which all equilibria of the game are regular, as introduced by Harsanyi. Such games are referred to as regular potential games. Recently it has been shown that almost all potential games (in the sense of the Lebesgue measure) are regular. In this paper it is shown that in any regular potential game (and hence, in almost every potential game), FP converges to the set of Nash equilibria at an exponential rate from almost every initial condition.},
  isbn = {978-1-5386-3266-6},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/CAS68927/Swenson et Kar - 2017 - On the exponential rate of convergence of fictitio.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/N3GYVJGL/Swenson et Kar - 2017 - On the Exponential Rate of Convergence of Fictitio.pdf}
}

@article{swensonRobustnessPropertiesFictitiousPlayType2017,
  title = {Robustness {{Properties}} in {{Fictitious-Play-Type Algorithms}}},
  author = {Swenson, Brian and Kar, Soummya and Xavier, Joa͂o and Leslie, David S.},
  year = {2017},
  month = jan,
  journal = {SIAM Journal on Control and Optimization},
  volume = {55},
  number = {5},
  pages = {3295--3318},
  issn = {0363-0129, 1095-7138},
  doi = {10.1137/16M1093227},
  abstract = {Fictitious play (FP) is a canonical game-theoretic learning algorithm which has been deployed extensively in decentralized control scenarios. However standard treatments of FP, and of many other game-theoretic models, assume rather idealistic conditions which rarely hold in realistic control scenarios. This paper considers a broad class of best response learning algorithms that we refer to as FP-type algorithms. In such an algorithm, given some (possibly limited) information about the history of actions, each individual forecasts the future play and chooses a (myopic) best response strategy given their forecast. We provide a unified analysis of the behavior of FP-type algorithms under an important class of perturbations, thus demonstrating robustness to deviations from the idealistic operating conditions that have been previously assumed. This robustness result is then used to derive convergence results for two control-relevant relaxations of standard gametheoretic applications: distributed (network-based) implementation without full observability and asynchronous deployment (including in continuous time). In each case the results follow as a direct consequence of the main robustness result.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/PYF374EA/Swenson et al. - 2017 - Robustness Properties in Fictitious-Play-Type Algo.pdf}
}

@article{swensonSmoothFictitiousPlay2019,
  title = {Smooth {{Fictitious Play}} in \${{N}}\textbackslash times 2\$ {{Potential Games}}},
  author = {Swenson, Brian and Poor, H. Vincent},
  year = {2019},
  month = nov,
  journal = {arXiv:1912.00251 [cs]},
  eprint = {1912.00251},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The paper shows that smooth fictitious play converges to a neighborhood of a pure-strategy Nash equilibrium with probability 1 in almost all \$N\textbackslash times 2\$ (\$N\$-player, two-action) potential games. The neighborhood of convergence may be made arbitrarily small by taking the smoothing parameter to zero. Simple proof techniques are furnished by considering regular potential games.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Science and Game Theory},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/69H5A6WA/Swenson et Poor - 2019 - Smooth Fictitious Play in $Ntimes 2$ Potential Ga.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/2GQV7MPU/1912.html}
}

@article{swensonSmoothFictitiousPlay2019a,
  title = {Smooth {{Fictitious Play}} in \${{N}}\textbackslash times 2\$ {{Potential Games}}},
  author = {Swenson, Brian and Poor, H. Vincent},
  year = {2019},
  month = nov,
  journal = {arXiv:1912.00251 [cs]},
  eprint = {1912.00251},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The paper shows that smooth fictitious play converges to a neighborhood of a pure-strategy Nash equilibrium with probability 1 in almost all \$N\textbackslash times 2\$ (\$N\$-player, two-action) potential games. The neighborhood of convergence may be made arbitrarily small by taking the smoothing parameter to zero. Simple proof techniques are furnished by considering regular potential games.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Science and Game Theory},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/FB9EDRWS/Swenson et Poor - 2019 - Smooth Fictitious Play in $Ntimes 2$ Potential Ga.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/NR3W9GFW/1912.html}
}

@article{swensonWeakLearningStrong2015,
  title = {From {{Weak Learning}} to {{Strong Learning}} in {{Fictitious Play Type Algorithms}}},
  author = {Swenson, Brian and Kar, Soummya and Xavier, Joao},
  year = {2015},
  month = apr,
  journal = {arXiv:1504.04920 [cs, math]},
  eprint = {1504.04920},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {The paper studies the highly prototypical Fictitious Play (FP) algorithm, as well as a broad class of learning processes based on best-response dynamics, that we refer to as FP-type algorithms. A well-known shortcoming of FP is that, while players may learn an equilibrium strategy in some abstract sense, there are no guarantees that the period-by-period strategies generated by the algorithm actually converge to equilibrium themselves. This issue is fundamentally related to the discontinuous nature of the best response correspondence and is inherited by many FP-type algorithms. Not only does it cause problems in the interpretation of such algorithms as a mechanism for economic and social learning, but it also greatly diminishes the practical value of these algorithms for use in distributed control. We refer to forms of learning in which players learn equilibria in some abstract sense only (to be defined more precisely in the paper) as weak learning, and we refer to forms of learning where players' period-by-period strategies converge to equilibrium as strong learning. An approach is presented for modifying an FP-type algorithm that achieves weak learning in order to construct a variant that achieves strong learning. Theoretical convergence results are proved.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Science and Game Theory,Mathematics - Optimization and Control,Mathematics - Probability},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/KK3QP27C/Swenson et al. - 2015 - From Weak Learning to Strong Learning in Fictitiou.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/EIU9TTS2/1504.html}
}

@inproceedings{taiRobotExplorationStrategy2016,
  title = {A Robot Exploration Strategy Based on {{Q-learning}} Network},
  booktitle = {2016 {{IEEE International Conference}} on {{Real-time Computing}} and {{Robotics}} ({{RCAR}})},
  author = {Tai, Lei and Liu, Ming},
  year = {2016},
  month = jun,
  pages = {57--62},
  publisher = {{IEEE}},
  address = {{Angkor Wat}},
  doi = {10.1109/RCAR.2016.7784001},
  isbn = {978-1-4673-8959-4},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/5LJYSAEE/Lei et Ming - A Robot Exploration Strategy Based on Q-learning N.pdf}
}

@book{takayamaMathematicalEconomics1985,
  title = {Mathematical Economics},
  author = {Takayama, Akira},
  year = {1985},
  edition = {2nd ed},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge [Cambridgeshire] ; New York}},
  isbn = {978-0-521-25707-7 978-0-521-31498-5},
  lccn = {HB135 .T34 1985},
  keywords = {Economics; Mathematical},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/KHY6WL6Z/Takayama - 1985 - Mathematical economics.pdf}
}

@inproceedings{tangBanditLearningDelayed2021,
  title = {Bandit {{Learning}} with {{Delayed Impact}} of {{Actions}}},
  booktitle = {Conference on {{Neural Information Processing Systems}} ({{NeurIPS}} 2021)},
  author = {Tang, Wei and Ho, Chien-Ju and Liu, Yang},
  year = {2021},
  pages = {14},
  address = {{Sydney, Australia}},
  abstract = {We consider a stochastic multi-armed bandit (MAB) problem with delayed impact of actions. In our setting, actions taken in the past impact the arm rewards in the subsequent future. This delayed impact of actions is prevalent in the real world. For example, the capability to pay back a loan for people in a certain social group might depend on historically how frequently that group has been approved loan applications. If banks keep rejecting loan applications to people in a disadvantaged group, it could create a feedback loop and further damage the chance of getting loans for people in that group. In this paper, we formulate this delayed and longterm impact of actions within the context of multi-armed bandits. We generalize the bandit setting to encode the dependency of this ``bias" due to the action history during learning. The goal is to maximize the collected utilities over time while taking into account the dynamics created by the delayed impacts of historical actions. We propose an algorithm that achieves a regret of O\texttildelow (KT 2/3) and show a matching regret lower bound of ⌦(KT 2/3), where K is the number of arms and T is the learning horizon. Our results complement the bandit literature by adding techniques to deal with actions with long-term impacts and have implications in designing fair algorithms.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/NG57PIU7/Tang et al. - Bandit Learning with Delayed Impact of Actions.pdf}
}

@incollection{tanMultiagentReinforcementLearning1997,
  title = {Multi-Agent Reinforcement Learning: {{Independent}} vs. {{Cooperative}} Agents},
  booktitle = {Readings in Agents},
  author = {Tan, Ming},
  year = {1997},
  pages = {487--494},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  isbn = {1-55860-495-2},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/LCPF8IFM/Tan - 1997 - Multi-agent reinforcement learning Independent vs.pdf}
}

@article{tesauroExtendingQLearningGeneral2003,
  title = {Extending {{Q-Learning}} to {{General Adaptive Multi-Agent Systems}}},
  author = {Tesauro, Gerald},
  year = {2003},
  pages = {8},
  abstract = {Recent multi-agent extensions of Q-Learning require knowledge of other agents' payoffs and Q-functions, and assume game-theoretic play at all times by all other agents. This paper proposes a fundamentally different approach, dubbed ``Hyper-Q'' Learning, in which values of mixed strategies rather than base actions are learned, and in which other agents' strategies are estimated from observed actions via Bayesian inference. Hyper-Q may be effective against many different types of adaptive agents, even if they are persistently dynamic. Against certain broad categories of adaptation, it is argued that Hyper-Q may converge to exact optimal time-varying policies. In tests using Rock-Paper-Scissors, Hyper-Q learns to significantly exploit an Infinitesimal Gradient Ascent (IGA) player, as well as a Policy Hill Climber (PHC) player. Preliminary analysis of Hyper-Q against itself is also presented.},
  langid = {english},
  keywords = {classé},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/GWBUKM6J/Tesauro - Extending Q-Learning to General Adaptive Multi-Age.pdf}
}

@misc{TheorieJeux,
  title = {{La th\'eorie des jeux.}},
  journal = {Alternatives Economiques},
  abstract = {Le dilemme du prisonnier est bien connu~: deux pr\'evenus sont soup\c{c}onn\'es d'un m\^eme crime et enferm\'es chacun dans une pi\`ece, sans pouvoir communiqu},
  howpublished = {https://www.alternatives-economiques.fr/theorie-jeux/00015056},
  langid = {french}
}

@misc{TheorieJeuxMarche,
  title = {{Th\'eorie des jeux : le march\'e a besoin de la soci\'et\'e}},
  shorttitle = {{Th\'eorie des jeux}},
  journal = {Alternatives Economiques},
  abstract = {" L "'intervention des pouvoirs publics dans le jeu conjoncturel des forces \'economiques ne sert pas \`a grand-chose.},
  howpublished = {https://www.alternatives-economiques.fr/theorie-jeux-marche-a-besoin-de-societe/00017093},
  langid = {french}
}

@inproceedings{thuijsmanRepeatedGamesAbsorbing2003,
  title = {Repeated {{Games}} with {{Absorbing States}}},
  booktitle = {Stochastic {{Games}} and {{Applications}}},
  author = {Thuijsman, Frank},
  editor = {Neyman, Abraham and Sorin, Sylvain},
  year = {2003},
  series = {{{NATO Science Series}}},
  pages = {205--213},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-010-0189-2_13},
  abstract = {In this paper we shall present a proof for the existence of limiting average {$\epsilon$}-equilibria in non-zero-sum repeated games with absorbing states, i.e., stochastic games in which all states but one are absorbing. We assume that the action spaces shall be finite; hence there are only finitely many absorbing states. A limiting average {$\epsilon$}-equilibrium is a pair of strategies ({$\sigma$} {$\epsilon$} , {$\tau$} {$\epsilon$} , with {$\epsilon$} {$>$} 0, such that for all {$\sigma$} and {$\tau$} we have {$\gamma$}1({$\sigma$}, {$\tau$} {$\epsilon$} ) {$\leq$} {$\gamma$}1({$\sigma$} {$\epsilon$} , {$\tau$} {$\epsilon$} ) + {$\epsilon$} and {$\gamma$}2({$\sigma$} {$\epsilon$} ,{$\tau$}) {$\leq$} {$\gamma$}2({$\sigma$} {$\epsilon$} {$\tau$} {$\epsilon$} ) + {$\epsilon$}. The proof presented in this chapter is based on the publications by Vrieze and Thuijsman [7] and by Thuijsman [5]. Several examples will illustrate the proof.},
  isbn = {978-94-010-0189-2},
  langid = {english},
  keywords = {Average Reward,Repeated Game,Small Triangle,Stationary Strategy,Stochastic Game},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/DIVVZCKT/Thuijsman - 2003 - Repeated Games with Absorbing States.pdf}
}

@article{treutweinAdaptivePsychophysicalProcedures,
  title = {Adaptive {{Psychophysical Procedures}}},
  author = {Treutwein, Bernhard},
  pages = {20},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/7YY3PZHU/Treutwein - Adaptive Psychophysical Procedures.pdf}
}

@article{TRIBUNEAuCoeur,
  title = {{Deirdre McCloskey, \'economiste libertarienne d'un autre genre}},
  year = {2019},
  month = dec,
  journal = {Le Monde.fr},
  abstract = {L'\'economiste Am\'ericaine transgenre, sp\'ecialiste de l'histoire du capitalisme, critique ses confr\`eres \guillemotleft{} mainstream \guillemotright{} mais revendique son amour fou pour le lib\'eralisme. Ses 18~livres ont \'et\'e traduits dans de nombreuses langues, mais pas en fran\c{c}ais.},
  langid = {french},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/J696S8IA/deirdre-mccloskey-economiste-libertarienne-d-un-autre-genre_6024369_3232.html}
}

@article{tsitsiklisAsynchronousStochasticApproximation1994,
  title = {Asynchronous Stochastic Approximation and {{Q-learning}}},
  author = {Tsitsiklis, John N},
  year = {1994},
  journal = {Machine learning},
  volume = {16},
  number = {3},
  pages = {185--202},
  publisher = {{Springer}},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ZYAVW744/Tsitsiklis - 1994 - Asynchronous stochastic approximation and Q-learni.pdf}
}

@article{vanstrienFictitiousPlayGames2011,
  title = {Fictitious Play in 3\texttimes 3 Games: {{Chaos}} and Dithering Behaviour},
  shorttitle = {Fictitious Play in 3\texttimes 3 Games},
  author = {{van Strien}, Sebastian and Sparrow, Colin},
  year = {2011},
  month = sep,
  journal = {Games and Economic Behavior},
  volume = {73},
  number = {1},
  pages = {262--286},
  issn = {0899-8256},
  doi = {10.1016/j.geb.2010.12.004},
  abstract = {In the 60's Shapley provided an example of a two player fictitious play which generates periodic behaviour. In this game, player A prefers to copy B's behaviour and player B prefers to play one strategy ahead of player A. In this paper we continue to study a family of games which generalise Shapley's example by introducing an external parameter, and prove that there exists an abundance of periodic and chaotic behaviour with players dithering between different strategies. The reason for all this, is that there exists a periodic orbit (consisting of playing mixed strategies) which is of `jitter type': such an orbit is neither attracting, repelling or of saddle type as nearby orbits jitter closer and further away from it in a manner which is reminiscent of a random walk motion. We prove that this behaviour holds for an open set of games.},
  langid = {english},
  keywords = {Bifurcation,Chaos,Fictitious play,Learning process,Replicator dynamics},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/Y7BPB3R3/van Strien et Sparrow - 2011 - Fictitious play in 3×3 games Chaos and dithering .pdf}
}

@book{varoufakisGameTheoryCritical2001,
  title = {Game Theory: Critical Concepts in the Social Sciences},
  shorttitle = {Game Theory},
  editor = {Varoufakis, Yanis and Housego, Anthony},
  year = {2001},
  publisher = {{Routledge}},
  address = {{London ; New York}},
  isbn = {978-0-415-22240-2 978-0-415-22241-9 978-0-415-22242-6 978-0-415-22243-3 978-0-415-22244-0},
  lccn = {H61.25 .G36 2001},
  keywords = {Economics,Game theory,Mathematical models,Methodology,Social sciences},
  annotation = {OCLC: ocm45129076}
}

@book{vega-redondoEconomicsTheoryGames2003,
  title = {Economics and the Theory of Games},
  author = {{Vega-Redondo}, Fernando},
  year = {2003},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK ; New York}},
  isbn = {978-0-521-77251-8 978-0-521-77590-8},
  lccn = {HB144 .V428 2003},
  keywords = {Economics,Equilibrium (Economics),Game theory},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/TW5RCGWA/Vega-Redondo - Economics and the theory of games.pdf}
}

@book{veyneCommentEcritHistoire1996,
  title = {{Comment on \'ecrit l'histoire: texte int\'egral}},
  shorttitle = {{Comment on \'ecrit l'histoire}},
  author = {Veyne, Paul},
  year = {1996},
  series = {{Points Histoire}},
  edition = {Nouv. \'ed.},
  number = {226},
  publisher = {{Ed. du Seuil}},
  address = {{Paris}},
  isbn = {978-2-02-028778-4},
  langid = {french},
  annotation = {OCLC: 833183929}
}

@article{viossatNoregretDynamicsFictitious2013,
  title = {No-Regret Dynamics and Fictitious Play},
  author = {Viossat, Yannick and Zapechelnyuk, Andriy},
  year = {2013},
  month = mar,
  journal = {Journal of Economic Theory},
  volume = {148},
  number = {2},
  pages = {825--842},
  issn = {00220531},
  doi = {10.1016/j.jet.2012.07.003},
  abstract = {Potential based no-regret dynamics are shown to be related to fictitious play. Roughly, these are {$\epsilon$}-best reply dynamics where {$\epsilon$} is the maximal regret, which vanishes with time. This allows for alternative and sometimes much shorter proofs of known results on convergence of no-regret dynamics to the set of Nash equilibria.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/Y7INRLQ3/Viossat et Zapechelnyuk - 2013 - No-regret dynamics and fictitious play.pdf}
}

@article{vlassisConciseIntroductionMultiagent2007,
  title = {A {{Concise Introduction}} to {{Multiagent Systems}} and {{Distributed Artificial Intelligence}}},
  author = {Vlassis, Nikos},
  year = {2007},
  month = jan,
  journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  volume = {1},
  number = {1},
  pages = {1--71},
  issn = {1939-4608, 1939-4616},
  doi = {10.2200/S00091ED1V01Y200705AIM002},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ZQQLXB69/Vlassis - 2007 - A Concise Introduction to Multiagent Systems and D.pdf}
}

@inproceedings{vlatakis-gkaragkounisNoregretLearningMixed2020,
  title = {No-Regret Learning and Mixed Nash Equilibria: {{They}} Do Not Mix},
  booktitle = {{{NeurIPS}}},
  author = {{Vlatakis-Gkaragkounis}, Emmanouil-Vasileios and Flokas, Lampros and Lianeas, Thanasis and Mertikopoulos, Panayotis and Piliouras, Georgios},
  year = {2020},
  cdate = {1577836800000},
  crossref = {conf/nips/2020},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/84Y52XSH/Flokas et al. - No-Regret Learning and Mixed Nash Equilibria They.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/U5GXFXCR/Flokas et al. - 2020 annexes - No-regret learning and mixed Nash equilibria They.pdf}
}

@article{vohraIntroductionSpecialIssue,
  title = {Introduction to the {{Special Issue}}},
  author = {et al Vohra, R},
  pages = {6},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/MS66NL6Y/Vohra - Introduction to the Special Issue.pdf}
}

@article{vriezeFictitiousPlayApplied1982,
  title = {Fictitious Play Applied to Sequences of Games and Discounted Stochastic Games},
  author = {Vrieze, O. J. and Tijs, S. H.},
  year = {1982},
  month = jun,
  journal = {International Journal of Game Theory},
  volume = {11},
  number = {2},
  pages = {71--85},
  issn = {0020-7276, 1432-1270},
  doi = {10.1007/BF01769064},
  abstract = {In this paper, we show that the iterative method of Brown and Robinson, for solving a matrix game, is also applicable to a converging sequence of matrices, where the players choose at stage t a row and a column of the t-th matrix in the sequence. As an application of this result, we describe a new solution method for discounted stochastic games with finite state and action spaces.},
  langid = {english},
  keywords = {lu,procedure},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/V2YQS7K9/Vrieze et Tijs - 1982 - Fictitious play applied to sequences of games and .pdf}
}

@article{walliserTheorieJeuxInstitutions2003,
  title = {{Th\'eorie des jeux et institutions}},
  author = {Walliser, Bernard},
  year = {2003},
  journal = {Cahiers d \'Economie Politique},
  volume = {44},
  number = {1},
  pages = {165},
  issn = {0154-8344, 1969-6779},
  doi = {10.3917/cep.044.0165},
  abstract = {Game Theory and Institutions The genesis of institutions can be treated by game theory, which is expressed at a sufficiently general level, considers direct relations among agents not mediated by former institutions and respects a sophisticated methodological individualism. In that framework, if interested not in the voluntary design of an institution but in the spontaneous arise of an institution, the last will be considered as an emergent structure of the game and practically as an equilibrium state of the game. Hence, the genesis of an institution parallels the genesis of an equilibrium, either by an educative process where the agents are able to induce it from their sole reasoning, or by an evolutionist process where it results from a learning mechanism foIIowed by the agents.},
  langid = {french},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/8ZFJ2JMD/Walliser - 2003 - Théorie des jeux et institutions.pdf}
}

@article{waltmanLearningAgentsCournot2008,
  title = {Q-Learning Agents in a {{Cournot}} Oligopoly Model},
  author = {Waltman, Ludo and Kaymak, Uzay},
  year = {2008},
  month = oct,
  journal = {Journal of Economic Dynamics and Control},
  volume = {32},
  number = {10},
  pages = {3275--3293},
  issn = {01651889},
  doi = {10.1016/j.jedc.2008.01.003},
  abstract = {Q-learning is a reinforcement learning model from the field of artificial intelligence. We study the use of Q-learning for modeling the learning behavior of firms in repeated Cournot oligopoly games. Based on computer simulations, we show that Q-learning firms generally learn to collude with each other, although full collusion usually does not emerge. We also present some analytical results. These results provide insight into the underlying mechanism that causes collusive behavior to emerge. Q-learning is one of the few learning models available that can explain the emergence of collusive behavior in settings in which there is no punishment mechanism and no possibility for explicit communication between firms.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/PZQNG87S/Waltman et Kaymak - 2008 - Q-learning agents in a Cournot oligopoly model.pdf}
}

@article{wangLearningContinuousConsistent2020,
  title = {Learning Continuous and Consistent Strategy Promotes Cooperation in Prisoner's Dilemma Game with Mixed Strategy},
  author = {Wang, Jianwei and Wang, Rong and Yu, Fengyuan and Wang, Ziwei and Li, Qiaochu},
  year = {2020},
  month = apr,
  journal = {Applied Mathematics and Computation},
  volume = {370},
  pages = {124887},
  issn = {00963003},
  doi = {10.1016/j.amc.2019.124887},
  abstract = {Research on cooperative evolution behavior based on memory mechanism has been a hot topic of many scholars in recent years. However, most previous studies have considered neighbors' historical payoffs when individuals choose role models whom they will learn from, but paid less attention to the stability of neighbors' strategy which indicates how frequently they change their strategies in memory length. In this paper, we study the memory length M and the strategy persistence level u of individuals and the change rate of cooperative tendency {$\delta$} on the cooperative evolution under the mixed strategy of the prisoner's dilemma. Strategy persistence level is measured by the number of times which the strategy in neighbor's memory length is continuous and consistent with neighbor's current strategy, and can determine the probability that the individual learns the neighbor's strategy when updating the strategy; the change rate of cooperative tendency {$\delta$} is described by the standard deviation of the normal distribution. We investigate the effects of M and {$\delta$} on the evolutionary of cooperation. The results show that the persistence strategy mechanism which we proposed in the memory length can improve network reciprocity and facilitate the generation and maintenance of cooperation, the larger the memory length is, the smaller the standard deviation value is, the more conducive to cooperation.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/WH6E2DIY/Wang et al. - 2020 - Learning continuous and consistent strategy promot.pdf}
}

@article{wangModifiedQLearningAlgorithm2014,
  title = {A {{Modified Q-Learning Algorithm}} for {{Potential Games}}},
  author = {Wang, Yatao and Pavel, Lacra},
  year = {2014},
  journal = {IFAC Proceedings Volumes},
  volume = {47},
  number = {3},
  pages = {8710--8718},
  issn = {14746670},
  doi = {10.3182/20140824-6-ZA-1003.02646},
  abstract = {This paper presents a modified Q-learning algorithm and provides conditions for convergence to a pure Nash equilibrium in potential games. In general Q-learning schemes, convergence to a Nash equilibrium may require decreasing step-sizes and long learning time. In this paper, we consider a modified Q-learning algorithm based on constant step-sizes, inspired by JSFP. When compared to JSFP, the Q-learning with constant step-sizes requires less information aggregation, but only reaches an approximation of a Nash equilibrium. We show that by appropriately choosing frequency dependent step-sizes, sufficient exploration of all actions is ensured and the estimated equilibrium approaches a Nash equilibrium.},
  langid = {english},
  keywords = {classé},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/WENDXK3N/Wang et Pavel - 2014 - A Modified Q-Learning Algorithm for Potential Game.pdf}
}

@inproceedings{wangReinforcementLearningPlay2002,
  title = {Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games},
  booktitle = {Proceedings of the 15th International Conference on Neural Information Processing Systems},
  author = {Wang, Xiaofeng and Sandholm, Tuomas},
  year = {2002},
  series = {{{NIPS}}'02},
  pages = {1603--1610},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  abstract = {Multiagent learning is a key problem in AI. In the presence of multiple Nash equilibria, even agents with non-conflicting interests may not be able to learn an optimal coordination policy. The problem is exaccerbated if the agents do not know the game and independently receive noisy payoffs. So, multiagent reinforfcement learning involves two interrelated problems: identifying the game and learning to play. In this paper, we present optimal adaptive learning, the first algorithm that converges to an optimal Nash equilibrium with probability 1 in any team Markov game. We provide a convergence proof, and show that the algorithm's parameters are easy to set to meet the convergence conditions.},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/J5V6DTEV/Wang et Sandholm - Reinforcement Learning to Play an Optimal Nash Equ.pdf}
}

@phdthesis{watkinsLearningDelayedRewards1989,
  title = {Learning from Delayed Rewards},
  author = {Watkins, C. J. C. H.},
  year = {1989},
  school = {King's College, Oxford},
  keywords = {juergen}
}

@article{watkinsTechnicalNoteQLearning1992,
  title = {Technical {{Note}}: {{Q-Learning}}},
  author = {Watkins, Christopher J.C.H. and Dayan, Peter},
  year = {1992},
  journal = {Machine Learning},
  volume = {8},
  number = {3/4},
  pages = {279--292},
  issn = {08856125},
  doi = {10.1023/A:1022676722315},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/YLI5E885/Watkins et Dayan - 1992 - [No title found].pdf}
}

@misc{weibullDoesMoralPlay2020,
  title = {Does Moral Play Equilibrate?},
  author = {Weibull, Jorgen W},
  year = {4-5-20},
  address = {{https://gametheorynetwork.com/one-world-game-theory-seminar/}},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/HQMA3GNX/Weibull - Does moral play equilibrate.pdf}
}

@book{weibullEvolutionaryGameTheory2004,
  title = {Evolutionary Game Theory},
  author = {Weibull, J{\"o}rgen W.},
  year = {2004},
  edition = {1. MIT Press paperback ed., [Nachd.]},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass.}},
  isbn = {978-0-262-73121-8},
  langid = {english},
  annotation = {OCLC: 255505178},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/CXRHK7CS/Jörgen W. Weibull - Evolutionary game theory   (1995, MIT Press).epub}
}

@article{weiOnlineReinforcementLearning2017,
  title = {Online {{Reinforcement Learning}} in {{Stochastic Games}}},
  author = {Wei, Chen-Yu and Hong, Yi-Te and Lu, Chi-Jen},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.00579 [cs]},
  eprint = {1712.00579},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We study online reinforcement learning in average-reward stochastic games (SGs). An SG models a two-player zero-sum game in a Markov environment, where state transitions and one-step payoffs are determined simultaneously by a learner and an adversary. We propose the UCSG algorithm that achieves a sublinear regret compared to the game value when competing with an arbitrary opponent. This result improves previous ones under the same setting. The regret bound has a dependency on the diameter, which is an intrinsic value related to the mixing property of SGs. If we let the opponent play an optimistic best response to the learner, UCSG finds an {$\epsilon$}-maximin stationary policy with a sample complexity of O\texttildelow{} (poly(1/{$\epsilon$})), where {$\epsilon$} is the gap to the best policy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,preuve pas lue},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/SNF3HLTV/Wei et al. - 2017 - Online Reinforcement Learning in Stochastic Games.pdf}
}

@misc{WhyCanConstant,
  title = {Why Can Constant Alpha Be Used for {{Q-Learning}} in Practice?},
  journal = {Cross Validated},
  howpublished = {https://stats.stackexchange.com/questions/451668/why-can-constant-alpha-be-used-for-q-learning-in-practice},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/7ERVNFIH/why-can-constant-alpha-be-used-for-q-learning-in-practice.html}
}

@article{wolpertGeneralPrinciplesLearningBased1999,
  title = {General {{Principles}} of {{Learning-Based Multi-Agent Systems}}},
  author = {Wolpert, David H. and Wheeler, Kevin R. and Tumer, Kagan},
  year = {1999},
  month = may,
  journal = {arXiv:cs/9905005},
  eprint = {cs/9905005},
  eprinttype = {arxiv},
  abstract = {We consider the problem of how to design large decentralized multi-agent systems (MAS's) in an automated fashion, with little or no hand-tuning. Our approach has each agent run a reinforcement learning algorithm. This converts the problem into one of how to automatically set/update the reward functions for each of the agents so that the global goal is achieved. In particular we do not want the agents to ``work at cross-purposes'' as far as the global goal is concerned. We use the term artificial COllective INtelligence (COIN) to refer to systems that embody solutions to this problem. In this paper we present a summary of a mathematical framework for COINs. We then investigate the real-world applicability of the core concepts of that framework via two computer experiments: we show that our COINs perform near optimally in a difficult variant of Arthur's bar problem [1] (and in particular avoid the tragedy of the commons for that problem), and we also illustrate optimal performance for our COINs in the leader-follower problem.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Condensed Matter - Statistical Mechanics,I.2.11,I.2.6,Nonlinear Sciences - Adaptation and Self-Organizing Systems},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ZRCZ7B74/Wolpert et al. - 1999 - General Principles of Learning-Based Multi-Agent S.pdf}
}

@article{wuLifelongLearningBranching,
  title = {Lifelong {{Learning}} with {{Branching Experts}}},
  author = {Wu, Yi-Shan and Hong, Yi-Te and Lu, Chi-Jen},
  pages = {15},
  abstract = {The problem of branching experts is an extension of the experts problem where the set of experts may grow over time. We compare this problem in different learning settings along several axes: adversarial versus stochastic losses; a fixed versus a growing set of experts (branching experts); and single-task versus lifelong learning with expert advice. First, for the branching experts problem, we achieve tight regret bounds in both adversarial and stochastic setting with a single algorithm. While it was known that the adversarial branching experts problem is strictly harder than the non-branching one, the stochastic branching experts problem is in fact no harder. Next, we study the extension to the lifelong learning with expert advice in which one has to make online predictions with a sequence of tasks. For this problem, we provide a single algorithm which works for both adversarial and stochastic setting, and our bounds when specialized to the case without branching recover the regret bounds previously achieved separately via different algorithms. Furthermore, we prove a regret lower bound which shows that in the lifelong learning scenario, the case with branching experts now becomes strictly harder than the non-branching case in the stochastic setting.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/NBQ8QVBW/Wu et al. - Lifelong Learning with Branching Experts.pdf}
}

@inproceedings{wunderClassesMultiagentQlearning2010,
  title = {Classes of Multiagent Q-Learning Dynamics with Epsilon-Greedy Exploration},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Wunder, Michael and Littman, Michael and Babes, Monica},
  year = {2010},
  month = jun,
  series = {{{ICML}}'10},
  pages = {1167--1174},
  publisher = {{Omnipress}},
  address = {{Madison, WI, USA}},
  abstract = {Q-learning in single-agent environments is known to converge in the limit given sufficient exploration. The same algorithm has been applied, with some success, in multi-agent environments, where traditional analysis techniques break down. Using established dynamical systems methods, we derive and study an idealization of Q-learning in 2-player 2-action repeated general-sum games. In particular, we address the discontinuous case of {$\epsilon$}-greedy exploration and use it as a proxy for value-based algorithms to highlight a contrast with existing results in policy search. Analogously to previous results for gradient ascent algorithms, we provide a complete catalog of the convergence behavior of the {$\epsilon$}-greedy Q-learning algorithm by introducing new subclasses of these games. We identify two subclasses of Prisoner's Dilemma-like games where the application of Q-learning with {$\epsilon$}-greedy exploration results in higher-than-Nash average payoffs for some initial conditions.},
  isbn = {978-1-60558-907-7},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/8HYL2J3I/Wunder et al. - 2010 - Classes of multiagent q-learning dynamics with eps.pdf}
}

@article{xiaoDualAveragingMethod,
  title = {Dual {{Averaging Method}} for {{Regularized Stochastic Learning}} and {{Online Optimization}}},
  author = {Xiao, Lin},
  pages = {9},
  abstract = {We consider regularized stochastic learning and online optimization problems, where the objective function is the sum of two convex terms: one is the loss function of the learning task, and the other is a simple regularization term such as {$\mathscr{l}$}1-norm for promoting sparsity. We develop a new online algorithm, the regularized dual averaging (RDA) method, that can explicitly exploit the regularization structure in an online setting. In particular, at each iteration, the learning variables are adjusted by solving a simple optimization problem that involves the running average of all past subgradients of the loss functions and the whole regularization term, not just its subgradient. Computational experiments show that the RDA method can be very effective for sparse online learning with {$\mathscr{l}$}1-regularization.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/NDM7C4MQ/Xiao - Dual Averaging Method for Regularized Stochastic L.pdf}
}

@article{xuConvergenceBestresponseDynamic,
  title = {Convergence of the {{Best-response Dynamic}} in {{Potential Games}}},
  author = {Xu, Zibo},
  pages = {48},
  abstract = {We prove that the continuous-time best-response dynamic from a generic initial point converges to a pure-strategy Nash equilibrium in an ordinal potential game under a minor condition for the payoff matrix. We then study the best-response dynamic defined in a consideration-set game where players face random strategy constraints with a small probability when playing the underlying game. In the case that the underlying game is a two-player common-payoff game with cheap talk, we show that if one player is under a strategy constraint slightly biased towards the efficient outcome, then the best-response dynamic from a generic initial point must approach to the efficient outcome, regardless of the constraint for the other player.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/PVDUCAQK/Xu - Convergence of the Best-response Dynamic in Potent.pdf}
}

@article{xuConvergenceBestresponseDynamica,
  title = {Convergence of the {{Best-response Dynamic}} in {{Potential Games}}},
  author = {Xu, Zibo},
  pages = {48},
  abstract = {We prove that the continuous-time best-response dynamic from a generic initial point converges to a pure-strategy Nash equilibrium in an ordinal potential game under a minor condition for the payoff matrix. We then study the best-response dynamic defined in a consideration-set game where players face random strategy constraints with a small probability when playing the underlying game. In the case that the underlying game is a two-player common-payoff game with cheap talk, we show that if one player is under a strategy constraint slightly biased towards the efficient outcome, then the best-response dynamic from a generic initial point must approach to the efficient outcome, regardless of the constraint for the other player.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/TMDE4ZZV/Xu - Convergence of the Best-response Dynamic in Potent.pdf}
}

@article{yangOverviewMultiAgentReinforcement2021,
  title = {An {{Overview}} of {{Multi-Agent Reinforcement Learning}} from {{Game Theoretical Perspective}}},
  author = {Yang, Yaodong and Wang, Jun},
  year = {2021},
  month = mar,
  journal = {arXiv:2011.00583 [cs]},
  eprint = {2011.00583},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Following the remarkable success of the AlphaGO series, 2019 was a booming year that witnessed significant advances in multi-agent reinforcement learning (MARL) techniques. MARL corresponds to the learning problem in a multi-agent system in which multiple agents learn simultaneously. It is an interdisciplinary domain with a long history that includes game theory, machine learning, stochastic control, psychology, and optimisation. Although MARL has achieved considerable empirical success in solving real-world games, there is a lack of a self-contained overview in the literature that elaborates the game theoretical foundations of modern MARL methods and summarises the recent advances. In fact, the majority of existing surveys are outdated and do not fully cover the recent developments since 2010. In this work, we provide a monograph on MARL that covers both the fundamentals and the latest developments in the research frontier.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/YP8KQ8PJ/Yang et Wang - 2021 - An Overview of Multi-Agent Reinforcement Learning .pdf}
}

@article{yeNeuralNetworkBasedApproach2020,
  title = {A {{Neural-Network}} Based {{Approach}} for {{Nash Equilibrium Seeking}} in {{Mixed-order Multi-player Games}}},
  author = {Ye, Maojiao and Yin, Jizhao},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.12744 [math]},
  eprint = {2009.12744},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {Noticing that agents with different dynamics may work together, this paper considers Nash equilibrium computation for a class of games in which first-order integrator-type players and second-order integrator-type players interact in a distributed network. To deal with this situation, we firstly exploit a centralized method for full information games. In the considered scenario, the players can employ its own gradient information, though it may rely on all players' actions. Based on the proposed centralized algorithm, we further develop a distributed counterpart. Different from the centralized one, the players are assumed to have limited access into the other players' actions. In addition, noticing that unmodeled dynamics and disturbances are inevitable for practical engineering systems, the paper further considers games in which the players' dynamics are suffering from unmodeled dynamics and time-varying disturbances. In this situation, an adaptive neural network is utilized to approximate the unmodeled dynamics and disturbances, based on which a centralized Nash equilibrium seeking algorithm and a distributed Nash equilibrium seeking algorithm are established successively. Appropriate Lyapunov functions are constructed to investigate the effectiveness of the proposed methods analytically. It is shown that if the considered mixed-order game is free of unmodeled dynamics and disturbances, the proposed method would drive the players' actions to the Nash equilibrium exponentially. Moreover, if unmodeled dynamics and disturbances are considered, the players' actions would converge to arbitrarily small neighborhood of the Nash equilibrium. Lastly, the theoretical results are numerically verified by simulation examples.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/S4B8GQJX/Ye et Yin - 2020 - A Neural-Network based Approach for Nash Equilibri.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/58BP3SIV/2009.html}
}

@article{youngCompetitionCustomEconomic2001,
  title = {Competition and {{Custom}} in {{Economic Contracts}}: {{A Case Study}} of {{Illinois Agriculture}}},
  shorttitle = {Competition and {{Custom}} in {{Economic Contracts}}},
  author = {Young, H. Peyton and Burke, Mary A.},
  year = {2001},
  month = jun,
  journal = {American Economic Review},
  volume = {91},
  number = {3},
  pages = {559--573},
  issn = {0002-8282},
  doi = {10.1257/aer.91.3.559},
  abstract = {Survey data suggest that cropsharing contracts exhibit a much higher degree of uniformity than is warranted by economic fundamentals. We propose a dynamic model of contract choice to explain this phenomenon. Landowners and tenants recontract periodically, taking into account expected returns as well as conformity with local practice. The resulting stochastic dynamical system is studied using techniques from statistical mechanics. The most likely states consist of patches where contractual terms are nearly uniform, separated by boundaries where the terms shift abruptly. These and other predictions of the model are borne out by survey data on agricultural contracts in Illinois.},
  langid = {english},
  keywords = {Contracts and Reputation,Economic Anthropology; Agricultural Labor Markets; Transactional Relationships,Irrigation; Economic Sociology,Land Ownership and Tenure,Land Reform,Land Use,Networks},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/9K4B36TM/EL8.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/XQVHIRRJ/Young et Burke - 2001 - Competition and Custom in Economic Contracts A Ca.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/6JE3Z3PN/articles.html}
}

@article{youngCooperationShortLong1991,
  title = {Cooperation in the {{Short}} and in the {{Long Run}}},
  author = {Young, H. Peyton and Foster, Dean},
  year = {1991},
  journal = {Games and Economic Behavior},
  volume = {3},
  pages = {145--156},
  abstract = {The long-run behavior of economic and biological processes is often dramatically altered when stochastic influences are taken into account. In fact, the smaller the noise, the more drastic the change can be. This seemingly paradoxical point is illustrated with the evolution of cooperation in repeated Prisoner's Dilemma.},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/DQNJCVXV/Young et Foster - 1991 - Cooperation in the Short and in the Long Run.pdf}
}

@article{youngEvolutionConventions1993,
  title = {The {{Evolution}} of {{Conventions}}},
  author = {Young, H. Peyton},
  year = {1993},
  month = jan,
  journal = {Econometrica},
  volume = {61},
  number = {1},
  pages = {57},
  issn = {00129682},
  doi = {10.2307/2951778},
  langid = {english},
  keywords = {lu},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/LIAHTMM6/Young - 1993 - The Evolution of Conventions.pdf}
}

@book{youngIndividualStrategySocial2001,
  title = {Individual Strategy and Social Structure: An Evolutionary Theory of Institutions},
  shorttitle = {Individual Strategy and Social Structure},
  author = {Young, H. Peyton},
  year = {2001},
  series = {Princeton Paperbacks},
  edition = {2. print and 1. paperback print},
  publisher = {{Princeton Univ. Press}},
  address = {{Princeton, NJ}},
  isbn = {978-0-691-08687-3},
  langid = {english},
  annotation = {OCLC: 180138812},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/KZN7KCZP/Young - 2001 - Individual strategy and social structure an evolu.pdf}
}

@incollection{youngInteractiveLearningProblem2004,
  title = {1. {{The Interactive Learning Problem}}},
  booktitle = {Strategic Learning and Its Limits},
  author = {Young, H. Peyton},
  year = {2004},
  publisher = {{Oxford University Press}},
  address = {{Oxford [England] ; New York}},
  abstract = {"This book examines a conceptual framework for studying strategic learning. Topics include interactive learning problem; reinforcement and regret; equilibrium; conditional no-regret learning; prediction, postdiction, and calibration; fictitious play and its variants; Bayesian learning; and hypothesis testing"--Provided by publisher},
  isbn = {978-0-19-926918-1},
  lccn = {HD58.82 .Y68 2004},
  keywords = {Knowledge management,Organizational learning},
  annotation = {OCLC: ocm56894489},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/L22IE2NZ/Young - 2004 - 1. The Interactive Learning Problem.pdf}
}

@incollection{youngReinforcementRegret2004,
  title = {2. {{Reinforcement}} and {{Regret}}},
  booktitle = {Strategic Learning and Its Limits},
  author = {Young, H. Peyton},
  year = {2004},
  publisher = {{Oxford University Press}},
  address = {{Oxford [England] ; New York}},
  abstract = {"This book examines a conceptual framework for studying strategic learning. Topics include interactive learning problem; reinforcement and regret; equilibrium; conditional no-regret learning; prediction, postdiction, and calibration; fictitious play and its variants; Bayesian learning; and hypothesis testing"--Provided by publisher},
  isbn = {978-0-19-926918-1},
  lccn = {HD58.82 .Y68 2004},
  keywords = {Knowledge management,Organizational learning},
  annotation = {OCLC: ocm56894489},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/Y88YPUMC/Young - 2004 - 2. Reinforcement and Regret.pdf}
}

@book{youngStrategicLearningIts2004,
  title = {Strategic Learning and Its Limits},
  author = {Young, H. Peyton},
  year = {2004},
  publisher = {{Oxford University Press}},
  address = {{Oxford [England] ; New York}},
  abstract = {"This book examines a conceptual framework for studying strategic learning. Topics include interactive learning problem; reinforcement and regret; equilibrium; conditional no-regret learning; prediction, postdiction, and calibration; fictitious play and its variants; Bayesian learning; and hypothesis testing"--Provided by publisher},
  isbn = {978-0-19-926918-1},
  lccn = {HD58.82 .Y68 2004},
  keywords = {Knowledge management,Organizational learning},
  annotation = {OCLC: ocm56894489},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/2JK7QIB3/acprof-9780199269181-chapter-9.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/3AV8E4TE/acprof-9780199269181-chapter-4.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/CLIWGXN3/acprof-9780199269181-chapter-7.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/H64VZ34Y/acprof-9780199269181-chapter-8.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/KKH9ZGX3/acprof-9780199269181-chapter-6.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/NVQRFSGU/acprof-9780199269181-bibliography-1.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/SS3BABIY/chapter-3-equilibrium.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/WIAVGA8Z/acprof-9780199269181-chapter-5.pdf}
}

@inproceedings{zhouMirrorDescentLearning2017,
  title = {Mirror Descent Learning in Continuous Games},
  booktitle = {2017 {{IEEE}} 56th {{Annual Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Zhou, Zhengyuan and Mertikopoulos, Panayotis and Moustakas, Aris L. and Bambos, Nicholas and Glynn, Peter},
  year = {2017},
  month = dec,
  pages = {5776--5783},
  publisher = {{IEEE}},
  address = {{Melbourne, Australia}},
  doi = {10.1109/CDC.2017.8264532},
  abstract = {Online Mirror Descent (OMD) is an important and widely used class of adaptive learning algorithms that enjoys good regret performance guarantees. It is therefore natural to study the evolution of the joint action in a multi-agent decision process (typically modeled as a repeated game) where every agent employs an OMD algorithm. This well-motivated question has received much attention in the literature that lies at the intersection between learning and games. However, much of the existing literature has been focused on the time average of the joint iterates. In this paper, we tackle a harder problem that is of practical utility, particularly in the online decision making setting: the convergence of the last iterate when all the agents make decisions according to OMD. We introduce an equilibrium stability notion called variational stability (VS) and show that in variationally stable games, the last iterate of OMD converges to the set of Nash equilibria. We also extend the OMD learning dynamics to a more general setting where the exact gradient is not available and show that the last iterate (now random) of OMD converges to the set of Nash equilibria almost surely.},
  isbn = {978-1-5090-2873-3},
  langid = {english},
  keywords = {lu,preuve pas lue},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ARK65H7W/Zhou et al. - 2017 - Mirror descent learning in continuous games.pdf}
}

@article{ziemkeFlowsTimeContinuous2021,
  title = {Flows {{Over Time}} as {{Continuous Limits}} of {{Packet-Based Network Simulations}}},
  author = {Ziemke, Theresa and Sering, Leon and Koch, Laura Vargas and Zimmer, Max and Nagel, Kai and Skutella, Martin},
  year = {2021},
  journal = {Transportation Research Procedia},
  volume = {52},
  pages = {123--130},
  issn = {23521465},
  doi = {10.1016/j.trpro.2021.01.014},
  langid = {english},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/ADU8UKFJ/Ziemke et al. - 2021 - Flows Over Time as Continuous Limits of Packet-Bas.pdf}
}

@misc{ZiliakMcCloskeyCult,
  title = {Ziliak and {{McCloskey}}: {{The Cult}} of {{Statistical Significance}}},
  shorttitle = {Ziliak and {{McCloskey}}},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/KAXT9BEL/preface_ziliak.html}
}

@article{zinkevichRegretMinimizationGames2007,
  title = {Regret {{Minimization}} in {{Games}} with {{Incomplete Information}}},
  author = {Zinkevich, Martin and Johanson, Michael and Bowling, Michael and Piccione, Carmelo},
  year = {2007},
  journal = {NIPS},
  pages = {8},
  abstract = {Extensive games are a powerful model of multiagent decision-making scenarios with incomplete information. Finding a Nash equilibrium for very large instances of these games has received a great deal of recent attention. In this paper, we describe a new technique for solving large games based on regret minimization. In particular, we introduce the notion of counterfactual regret, which exploits the degree of incomplete information in an extensive game. We show how minimizing counterfactual regret minimizes overall regret, and therefore in self-play can be used to compute a Nash equilibrium. We demonstrate this technique in the domain of poker, showing we can solve abstractions of limit Texas Hold'em with as many as 1012 states, two orders of magnitude larger than previous methods.},
  langid = {english},
  keywords = {lu,preuve pas lue},
  file = {/home/lucas/nextcloud.lamsade/Zotero/storage/VVLVKESN/Zinkevich et al. - Regret Minimization in Games with Incomplete Infor.pdf;/home/lucas/nextcloud.lamsade/Zotero/storage/WLZEF7CC/Zinkevich et al. - Regret Minimization in Games with Incomplete Infor.pdf}
}


@article{Nes09,
	author = {Yurii Nesterov},
	journal = {Mathematical Programming},
	number = {1},
	pages = {221-259},
	title = {Primal-Dual Subgradient Methods for Convex Problems},
	volume = {120},
	year = {2009}}

@article{MZ19,
	author = {Mertikopoulos, Panayotis and Zhengyuan, Zhou},
	journal = {Mathematical Programming},
	month = {January},
	number = {1-2},
	pages = {465-507},
	title = {Learning in games with continuous action sets and unknown payoff functions},
	volume = {173},
	year = {2019}}

@inproceedings{GVM21b,
	author = {Giannou, Angeliki and Emmanouil-Vasileios, Vlatakis-Gkaragkounis and Panayotis, Mertikopoulos},
	booktitle = {NeurIPS '21: Proceedings of the 35th International Conference on Neural Information Processing Systems},
	title = {The convergence rate of regularized learning in games: {From} bandits and uncertainty to optimism and beyond},
	year = {2021}}

@inproceedings{GVM21,
	author = {Giannou, Angeliki and Emmanouil-Vasileios, Vlatakis-Gkaragkounis and Panayotis, Mertikopoulos},
	booktitle = {COLT '21: Proceedings of the 34th Annual Conference on Learning Theory},
	title = {Survival of the strictest: {Stable} and unstable equilibria under regularized learning with partial information},
	year = {2021}}
