\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Barber and Bishop(1998)]{barber_ensemble_1998}
David Barber and Christopher~M Bishop.
\newblock Ensemble {{Learning}} in {{Bayesian Neural Networks}}.
\newblock In \emph{Neural {{Networks}} and {{Machine Learning}}}, pages
  215--237. {Springer}, 1998.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell_weight_2015}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight {{Uncertainty}} in {{Neural Networks}}.
\newblock \emph{Proceedings of the 32nd International Conference on Machine
  Learning}, 37:\penalty0 1613--1622, 2015.

\bibitem[Bondesson(2015)]{bondesson_class_2015}
Lennart Bondesson.
\newblock A {{Class}} of {{Probability Distributions}} that is {{Closed}} with
  {{Respect}} to {{Addition}} as {{Well}} as {{Multiplication}} of
  {{Independent Random Variables}}.
\newblock \emph{Journal of Theoretical Probability}, 28\penalty0 (3):\penalty0
  1063--1081, September 2015.

\bibitem[Cobb et~al.(2019)Cobb, Baydin, Markham, and
  Roberts]{cobb_introducing_2019}
Adam~D. Cobb, At{\i}l{\i}m~G{\"u}ne{\c s} Baydin, Andrew Markham, and
  Stephen~J. Roberts.
\newblock Introducing an {{Explicit Symplectic Integration Scheme}} for
  {{Riemannian Manifold Hamiltonian Monte Carlo}}.
\newblock \emph{arXiv}, October 2019.

\bibitem[Farquhar et~al.(2020)Farquhar, Osborne, and Gal]{farquhar_radial_2020}
Sebastian Farquhar, Michael Osborne, and Yarin Gal.
\newblock Radial {{Bayesian Neural Networks}}: {{Robust Variational Inference
  In Big Models}}.
\newblock \emph{Proceedings of the 23rd International Conference on Artificial
  Intelligence and Statistics}, 2020.

\bibitem[Foong et~al.(2020)Foong, Burt, Li, and
  Turner]{foong_expressiveness_2020}
Andrew Y.~K. Foong, David~R. Burt, Yingzhen Li, and Richard~E. Turner.
\newblock On the {{Expressiveness}} of {{Approximate Inference}} in {{Bayesian
  Neural Networks}}.
\newblock \emph{arXiv}, February 2020.

\bibitem[Gal(2016)]{gal_uncertainty_2016}
Yarin Gal.
\newblock Uncertainty in {{Deep Learning}}.
\newblock \emph{PhD Thesis}, 2016.

\bibitem[Goldblum et~al.(2019)Goldblum, Geiping, Schwarzschild, Moeller, and
  Goldstein]{goldblum_truth_2019}
Micah Goldblum, Jonas Geiping, Avi Schwarzschild, Michael Moeller, and Tom
  Goldstein.
\newblock Truth or backpropaganda? {{An}} empirical investigation of deep
  learning theory.
\newblock In \emph{International {{Conference}} on {{Learning
  Representations}}}, September 2019.

\bibitem[Graves(2011)]{graves_practical_2011}
Alex Graves.
\newblock Practical {{Variational Inference}} for {{Neural Networks}}.
\newblock \emph{Neural Information Processing Systems}, 2011.

\bibitem[Gray(2011)]{gray_entropy_2011}
Robert~M. Gray.
\newblock \emph{Entropy and {{Information Theory}}}.
\newblock {Springer US}, second edition, 2011.
\newblock ISBN 978-1-4419-7969-8.
\newblock \doi{10.1007/978-1-4419-7970-4}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he_deep_2016}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep {{Residual Learning}} for {{Image Recognition}}.
\newblock \emph{CVPR}, 7\penalty0 (3):\penalty0 171--180, 2016.

\bibitem[Hinton and {van Camp}(1993)]{hinton_keeping_1993}
Geoffrey Hinton and Drew {van Camp}.
\newblock Keeping {{Neural Networks Simple}} by {{Minimizing}} the
  {{Description Length}} of the {{Weights}}.
\newblock \emph{Proceedings of the 6th Annual ACM Conference on Computational
  Learning Theory}, 1993.

\bibitem[Hoffman and Gelman(2014)]{hoffman_no-u-turn_2014}
Matthew~D Hoffman and Andrew Gelman.
\newblock The {{No}}-{{U}}-{{Turn Sampler}}: {{Adaptively Setting Path
  Lengths}} in {{Hamiltonian Monte Carlo}}.
\newblock \emph{Journal of Machine Learning Research}, 15:\penalty0 1593--1623,
  2014.

\bibitem[Jaakkola and Jordan(1998)]{jaakkola_improving_1998}
Tommi~S. Jaakkola and Michael~I. Jordan.
\newblock Improving the {{Mean Field Approximation Via}} the {{Use}} of
  {{Mixture Distributions}}.
\newblock In Michael~I. Jordan, editor, \emph{Learning in {{Graphical
  Models}}}, {{NATO ASI Series}}, pages 163--173. {Springer Netherlands},
  {Dordrecht}, 1998.
\newblock \doi{10.1007/978-94-011-5014-9_6}.

\bibitem[Khan et~al.(2018)Khan, Nielsen, Tangkaratt, Lin, Gal, and
  Srivastava]{khan_fast_2018}
Mohammad~Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu~Lin, Yarin Gal, and
  Akash Srivastava.
\newblock Fast and {{Scalable Bayesian Deep Learning}} by
  {{Weight}}-{{Perturbation}} in {{Adam}}.
\newblock \emph{International Conference on Machine Learning}, 2018.

\bibitem[Leshno et~al.(1993)Leshno, Lin, Pinkus, and
  Schocken]{leshno_multilayer_1993}
Moshe Leshno, Vladimir~Ya. Lin, Allan Pinkus, and Shimon Schocken.
\newblock Multilayer feedforward networks with a nonpolynomial activation
  function can approximate any function.
\newblock \emph{Neural Networks}, 6\penalty0 (6):\penalty0 861--867, January
  1993.
\newblock ISSN 08936080.
\newblock \doi{10.1016/S0893-6080(05)80131-5}.

\bibitem[Louizos and Welling(2016)]{louizos_structured_2016}
Christos Louizos and Max Welling.
\newblock Structured and {{Efficient Variational Deep Learning}} with {{Matrix
  Gaussian Posteriors}}.
\newblock \emph{International Conference on Machine Learning}, pages
  1708--1716, 2016.

\bibitem[Louizos and Welling(2017)]{louizos_multiplicative_2017}
Christos Louizos and Max Welling.
\newblock Multiplicative {{Normalizing Flows}} for {{Variational Bayesian
  Neural Networks}}.
\newblock In \emph{International {{Conference}} on {{Machine Learning}}}, pages
  2218--2227, July 2017.

\bibitem[MacKay(1992)]{mackay_practical_1992}
David J.~C. MacKay.
\newblock A {{Practical Bayesian Framework}} for {{Backpropagation Networks}}.
\newblock \emph{Neural Computation}, 4\penalty0 (3):\penalty0 448--472, 1992.

\bibitem[Maddox et~al.(2019)Maddox, Garipov, Izmailov, Vetrov, and
  Wilson]{maddox_simple_2019}
Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, and Andrew~Gordon
  Wilson.
\newblock A {{Simple Baseline}} for {{Bayesian Uncertainty}} in {{Deep
  Learning}}.
\newblock \emph{Neural Information Processing Systems}, December 2019.

\bibitem[Mishkin et~al.(2019)Mishkin, Kunstner, Nielsen, Schmidt, and
  Khan]{mishkin_slang_2019}
Aaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark Schmidt, and
  Mohammad~Emtiyaz Khan.
\newblock {{SLANG}}: {{Fast Structured Covariance Approximations}} for
  {{Bayesian Deep Learning}} with {{Natural Gradient}}.
\newblock \emph{arXiv:1811.04504 [cs, stat]}, January 2019.

\bibitem[Mnih and Gregor(2014)]{mnih_neural_2014}
Andriy Mnih and Karol Gregor.
\newblock Neural {{Variational Inference}} and {{Learning}} in {{Belief
  Networks}}.
\newblock \emph{International Conference on Machine Learning}, June 2014.

\bibitem[Mont{\'u}far et~al.(2014)Mont{\'u}far, Pascanu, Cho, and
  Bengio]{montufar_number_2014}
Guido Mont{\'u}far, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio.
\newblock On the {{Number}} of {{Linear Regions}} of {{Deep Neural Networks}}.
\newblock \emph{Neural Information Processing Systems}, June 2014.

\bibitem[Neal(1995)]{neal_bayesian_1995}
Radford~M. Neal.
\newblock \emph{Bayesian Learning for Neural Networks}.
\newblock {{PhD Thesis}}, University of Toronto, 1995.

\bibitem[Oh et~al.(2019)Oh, Adamczewski, and Park]{oh_radial_2019}
Changyong Oh, Kamil Adamczewski, and Mijung Park.
\newblock Radial and {{Directional Posteriors}} for {{Bayesian Neural
  Networks}}.
\newblock \emph{Bayesian Deep Learning Workshop at NeurIPS}, February 2019.

\bibitem[Osawa et~al.(2019)Osawa, Swaroop, Jain, Eschenhagen, Turner, Yokota,
  and Khan]{osawa_practical_2019}
Kazuki Osawa, Siddharth Swaroop, Anirudh Jain, Runa Eschenhagen, Richard~E.
  Turner, Rio Yokota, and Mohammad~Emtiyaz Khan.
\newblock Practical {{Deep Learning}} with {{Bayesian Principles}}.
\newblock \emph{arXiv:1906.02506 [cs, stat]}, June 2019.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi_convergence_2018}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the {{Convergence}} of {{Adam}} and {{Beyond}}.
\newblock \emph{International Conference on Learning Representations}, February
  2018.

\bibitem[Rezende and Mohamed(2015)]{rezende_variational_2015}
Danilo~Jimenez Rezende and Shakir Mohamed.
\newblock Variational {{Inference}} with {{Normalizing Flows}}.
\newblock \emph{International Conference on Machine Learning}, 2015.

\bibitem[Rudin(1976)]{differentiation_uniform_convergence}
Walter Rudin.
\newblock \emph{Principles of Mathematical Analysis}.
\newblock McGraw Hill, 3rd edition, 1976.

\bibitem[Saxe et~al.(2014)Saxe, McClelland, and Ganguli]{saxe_exact_2014}
A.~M. Saxe, J.~L. McClelland, and S~Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{International Conference on Learning Representations}, 2014.

\bibitem[Shamir et~al.(2019)Shamir, Safran, Ronen, and
  Dunkelman]{shamir_simple_2019}
Adi Shamir, Itay Safran, Eyal Ronen, and Orr Dunkelman.
\newblock A {{Simple Explanation}} for the {{Existence}} of {{Adversarial
  Examples}} with {{Small Hamming Distance}}.
\newblock \emph{arXiv}, January 2019.

\bibitem[Springer and Thompson(1970)]{springer_distribution_1970}
M.~D. Springer and W.~E. Thompson.
\newblock The {{Distribution}} of {{Products}} of {{Beta}}, {{Gamma}} and
  {{Gaussian Random Variables}}.
\newblock \emph{SIAM Journal on Applied Mathematics}, 18\penalty0 (4):\penalty0
  721--737, 1970.
\newblock ISSN 0036-1399.

\bibitem[Sun et~al.(2019)Sun, Zhang, Shi, and Grosse]{sun_functional_2019}
Shengyang Sun, Guodong Zhang, Jiaxin Shi, and Roger Grosse.
\newblock Functional variational {{Bayesian Neural Networks}}.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Swiatkowski et~al.(2020)Swiatkowski, Roth, Veeling, Tran, Dillon,
  Snoek, Mandt, Salimans, Jenatton, and Nowozin]{swiatkowski_k-tied_2020}
Jakub Swiatkowski, Kevin Roth, Bastiaan~S. Veeling, Linh Tran, Joshua~V.
  Dillon, Jasper Snoek, Stephan Mandt, Tim Salimans, Rodolphe Jenatton, and
  Sebastian Nowozin.
\newblock The k-tied {{Normal Distribution}}: {{A Compact Parameterization}} of
  {{Gaussian Mean Field Posteriors}} in {{Bayesian Neural Networks}}.
\newblock \emph{International Conference on Machine Learning}, 2020.

\bibitem[Wen et~al.(2018)Wen, Vicol, Ba, Tran, and Grosse]{wen_flipout_2018}
Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse.
\newblock Flipout: {{Efficient Pseudo}}-{{Independent Weight Perturbations}} on
  {{Mini}}-{{Batches}}.
\newblock \emph{International Conference on Learning Representations}, April
  2018.

\bibitem[Wu et~al.(2019)Wu, Nowozin, Meeds, Turner, {Hernandez-Lobato}, and
  Gaunt]{wu_deterministic_2019}
Anqi Wu, Sebastian Nowozin, Edward Meeds, Richard~E Turner, Jose~Miguel
  {Hernandez-Lobato}, and Alexander~L Gaunt.
\newblock Deterministic {{Variational Inference}} for {{Robust Bayesian Neural
  Networks}}.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao_fashion-mnist_2017}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-{{MNIST}}: A {{Novel Image Dataset}} for {{Benchmarking
  Machine Learning Algorithms}}.
\newblock \emph{arXiv}, August 2017.

\bibitem[Zhang et~al.(2018)Zhang, Sun, Duvenaud, and Grosse]{zhang_noisy_2018}
Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse.
\newblock Noisy {{Natural Gradient}} as {{Variational Inference}}.
\newblock \emph{International Conference on Machine Learning}, 2018.

\end{thebibliography}
