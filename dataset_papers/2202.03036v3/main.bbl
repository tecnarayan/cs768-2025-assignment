\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbe(2018)]{PatternCluster2018}
Abbe, E.
\newblock Community detection and stochastic block models: Recent developments.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 18\penalty0
  (177):\penalty0 1--86, 2018.

\bibitem[Alon \& Yahav(2021)Alon and Yahav]{alon2021on}
Alon, U. and Yahav, E.
\newblock On the bottleneck of graph neural networks and its practical
  implications.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Alsentzer et~al.(2020)Alsentzer, Finlayson, Li, and
  Zitnik]{alsentzer2020subgraph}
Alsentzer, E., Finlayson, S.~G., Li, M.~M., and Zitnik, M.
\newblock Subgraph neural networks.
\newblock In \emph{Proceedings of Neural Information Processing Systems,
  NeurIPS}, 2020.

\bibitem[Bodnar et~al.(2021)Bodnar, Frasca, Otter, Wang, Li{\`o}, Montufar, and
  Bronstein]{bodnar2021weisfeiler}
Bodnar, C., Frasca, F., Otter, N., Wang, Y.~G., Li{\`o}, P., Montufar, G.~F.,
  and Bronstein, M.
\newblock Weisfeiler and lehman go cellular: Cw networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Chen et~al.(2020)Chen, Lin, Li, Li, Zhou, and Sun]{chen2020measuring}
Chen, D., Lin, Y., Li, W., Li, P., Zhou, J., and Sun, X.
\newblock Measuring and relieving the over-smoothing problem for graph neural
  networks from the topological view.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2020.

\bibitem[Corso et~al.(2020)Corso, Cavalleri, Beaini, Li{\`o}, and
  Veli{\v{c}}kovi{\'c}]{corso2020principal}
Corso, G., Cavalleri, L., Beaini, D., Li{\`o}, P., and Veli{\v{c}}kovi{\'c}, P.
\newblock Principal neighbourhood aggregation for graph nets.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Dwivedi \& Bresson(2021)Dwivedi and
  Bresson]{dwivedi2021generalization}
Dwivedi, V.~P. and Bresson, X.
\newblock A generalization of transformer networks to graphs.
\newblock In \emph{AAAI Workshop on Deep Learning on Graphs: Methods and
  Applications}, 2021.

\bibitem[Dwivedi et~al.(2020)Dwivedi, Joshi, Laurent, Bengio, and
  Bresson]{dwivedi2020benchmarkgnns}
Dwivedi, V.~P., Joshi, C.~K., Laurent, T., Bengio, Y., and Bresson, X.
\newblock Benchmarking graph neural networks.
\newblock \emph{arXiv preprint arXiv:2003.00982}, 2020.

\bibitem[Dwivedi et~al.(2022)Dwivedi, Luu, Laurent, Bengio, and
  Bresson]{dwivedi2022graph}
Dwivedi, V.~P., Luu, A.~T., Laurent, T., Bengio, Y., and Bresson, X.
\newblock Graph neural networks with learnable structural and positional
  representations.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Fan et~al.(2019)Fan, Ma, Li, He, Zhao, Tang, and Yin]{fan2019graph}
Fan, W., Ma, Y., Li, Q., He, Y., Zhao, E., Tang, J., and Yin, D.
\newblock Graph neural networks for social recommendation.
\newblock In \emph{The World Wide Web Conference}, 2019.

\bibitem[Gao \& Pavel(2017)Gao and Pavel]{gao2017properties}
Gao, B. and Pavel, L.
\newblock On the properties of the softmax function with application in game
  theory and reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1704.00805}, 2017.

\bibitem[Gao \& Ji(2019)Gao and Ji]{gao2019graph}
Gao, H. and Ji, S.
\newblock Graph u-nets.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2083--2092, 2019.

\bibitem[Gaudelet et~al.(2021)Gaudelet, Day, Jamasb, Soman, Regep, Liu, Hayter,
  Vickers, Roberts, Tang, et~al.]{gaudelet2021utilizing}
Gaudelet, T., Day, B., Jamasb, A.~R., Soman, J., Regep, C., Liu, G., Hayter,
  J.~B., Vickers, R., Roberts, C., Tang, J., et~al.
\newblock Utilizing graph machine learning within drug discovery and
  development.
\newblock \emph{Briefings in Bioinformatics}, 22\penalty0 (6):\penalty0
  bbab159, 2021.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and
  Dahl]{gilmer2017neural}
Gilmer, J., Schoenholz, S.~S., Riley, P.~F., Vinyals, O., and Dahl, G.~E.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Hamilton et~al.(2017)Hamilton, Ying, and
  Leskovec]{hamilton2017inductive}
Hamilton, W.~L., Ying, R., and Leskovec, J.
\newblock Inductive representation learning on large graphs.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2017.

\bibitem[Horn et~al.(2021)Horn, {De Brouwer}, Moor, Moreau, Rieck, and
  Borgwardt]{Horn21a}
Horn, M., {De Brouwer}, E., Moor, M., Moreau, Y., Rieck, B., and Borgwardt, K.
\newblock Topological graph neural networks.
\newblock 2021.

\bibitem[Hornik(1991)]{hornik1991approximation}
Hornik, K.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock \emph{Neural networks}, 4\penalty0 (2):\penalty0 251--257, 1991.

\bibitem[Hu et~al.(2020{\natexlab{a}})Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta,
  and Leskovec]{hu2020open}
Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and
  Leskovec, J.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020{\natexlab{a}}.

\bibitem[Hu et~al.(2020{\natexlab{b}})Hu, Liu, Gomes, Zitnik, Liang, Pande, and
  Leskovec]{hu2020pretraining}
Hu, W., Liu, B., Gomes, J., Zitnik, M., Liang, P., Pande, V., and Leskovec, J.
\newblock Strategies for pre-training graph neural networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020{\natexlab{b}}.

\bibitem[Ingraham et~al.(2019)Ingraham, Garg, Barzilay, and
  Jaakkola]{ingraham2019generative}
Ingraham, J., Garg, V., Barzilay, R., and Jaakkola, T.
\newblock Generative models for graph-based protein design.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Irwin et~al.(2012)Irwin, Sterling, Mysinger, Bolstad, and
  Coleman]{ZINC}
Irwin, J.~J., Sterling, T., Mysinger, M.~M., Bolstad, E.~S., and Coleman, R.~G.
\newblock Zinc: A free tool to discover chemistry for biology.
\newblock \emph{Journal of Chemical Information and Modeling}, 52\penalty0
  (7):\penalty0 1757--1768, 2012.

\bibitem[Jain et~al.(2021)Jain, Wu, Wright, Mirhoseini, Gonzalez, and
  Stoica]{jain2021representing}
Jain, P., Wu, Z., Wright, M., Mirhoseini, A., Gonzalez, J.~E., and Stoica, I.
\newblock Representing long-range context for graph neural networks with global
  attention.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Kersting et~al.(2016)Kersting, Kriege, Morris, Mutzel, and
  Neumann]{KKMMN2016}
Kersting, K., Kriege, N.~M., Morris, C., Mutzel, P., and Neumann, M.
\newblock Benchmark data sets for graph kernels, 2016.
\newblock \url{http://graphkernels.cs.tu-dortmund.de}.

\bibitem[Kipf \& Welling(2017)Kipf and Welling]{Kipf2017}
Kipf, T.~N. and Welling, M.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Kreuzer et~al.(2021)Kreuzer, Beaini, Hamilton, L{\'e}tourneau, and
  Tossou]{kreuzer2021rethinking}
Kreuzer, D., Beaini, D., Hamilton, W.~L., L{\'e}tourneau, V., and Tossou, P.
\newblock Rethinking graph transformers with spectral attention.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Li et~al.(2019)Li, Müller, Thabet, and Ghanem]{li2019deepgcns}
Li, G., Müller, M., Thabet, A., and Ghanem, B.
\newblock Deepgcns: Can gcns go as deep as cnns?
\newblock In \emph{Proceedings of the International Conference on Computer
  Vision (ICCV)}, 2019.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Xiong, Thabet, and
  Ghanem]{li2020deepergcn}
Li, G., Xiong, C., Thabet, A., and Ghanem, B.
\newblock Deepergcn: All you need to train deeper gcns, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Wang, Wang, and
  Leskovec]{li2020distance}
Li, P., Wang, Y., Wang, H., and Leskovec, J.
\newblock Distance encoding: Design provably more powerful neural networks for
  graph representation learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2018)Li, Han, and Wu]{Li2018}
Li, Q., Han, Z., and Wu, X.
\newblock Deeper insights into graph convolutional networks for semi-supervised
  learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2018.

\bibitem[Loshchilov \& Hutter(2016)Loshchilov and Hutter]{loshchilov2016sgdr}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Loshchilov \& Hutter(2018)Loshchilov and
  Hutter]{loshchilov2018decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Mesquita et~al.(2020)Mesquita, Souza, and Kaski]{rethinkpooling2020}
Mesquita, D., Souza, A.~H., and Kaski, S.
\newblock Rethinking pooling in graph neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Mialon et~al.(2021)Mialon, Chen, Selosse, and
  Mairal]{mialon2021graphit}
Mialon, G., Chen, D., Selosse, M., and Mairal, J.
\newblock Graphit: Encoding graph structure in transformers, 2021.

\bibitem[Micchelli et~al.(2006)Micchelli, Xu, and
  Zhang]{micchelli2006universal}
Micchelli, C.~A., Xu, Y., and Zhang, H.
\newblock Universal kernels.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 7\penalty0 (12),
  2006.

\bibitem[Morris et~al.(2019)Morris, Ritzert, Fey, Hamilton, Lenssen, Rattan,
  and Grohe]{morris2019weisfeiler}
Morris, C., Ritzert, M., Fey, M., Hamilton, W.~L., Lenssen, J.~E., Rattan, G.,
  and Grohe, M.
\newblock Weisfeiler and leman go neural: Higher-order graph neural networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2019.

\bibitem[Nikolentzos \& Vazirgiannis(2020)Nikolentzos and
  Vazirgiannis]{nikolentzos2020random}
Nikolentzos, G. and Vazirgiannis, M.
\newblock Random walk graph neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Oono \& Suzuki(2020)Oono and Suzuki]{oono2020graph}
Oono, K. and Suzuki, T.
\newblock Graph neural networks exponentially lose expressive power for node
  classification.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Qin et~al.(2022)Qin, Sun, Deng, Li, Wei, Lv, Yan, Kong, and
  Zhong]{zhen2022cosformer}
Qin, Z., Sun, W., Deng, H., Li, D., Wei, Y., Lv, B., Yan, J., Kong, L., and
  Zhong, Y.
\newblock cosformer: Rethinking softmax in attention.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Rives et~al.(2021)Rives, Meier, Sercu, Goyal, Lin, Liu, Guo, Ott,
  Zitnick, Ma, et~al.]{rives2021biological}
Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M.,
  Zitnick, C.~L., Ma, J., et~al.
\newblock Biological structure and function emerge from scaling unsupervised
  learning to 250 million protein sequences.
\newblock \emph{Proceedings of the National Academy of Sciences}, 118\penalty0
  (15), 2021.

\bibitem[Rong et~al.(2020)Rong, Bian, Xu, Xie, Wei, Huang, and Huang]{Rong2020}
Rong, Y., Bian, Y., Xu, T., Xie, W., Wei, Y., Huang, W., and Huang, J.
\newblock Self-supervised graph transformer on large-scale molecular data.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Shaw et~al.(2018)Shaw, Uszkoreit, and Vaswani]{shaw2018self}
Shaw, P., Uszkoreit, J., and Vaswani, A.
\newblock Self-attention with relative position representations.
\newblock In \emph{Proceedings of the North American Chapter of the Association
  for Computational Linguistics (NAACL)}, 2018.

\bibitem[Shi et~al.(2021)Shi, Huang, Feng, Zhong, Wang, and Sun]{Shi2021}
Shi, Y., Huang, Z., Feng, S., Zhong, H., Wang, W., and Sun, Y.
\newblock Masked label prediction: Unified message passing model for
  semi-supervised classification.
\newblock In Zhou, Z.-H. (ed.), \emph{Proceedings of the Thirtieth
  International Joint Conference on Artificial Intelligence~(IJCAI-21)}, pp.\
  1548--1554. International Joint Conferences on Artificial Intelligence
  Organization, 8 2021.

\bibitem[Tay et~al.(2020)Tay, Dehghani, Bahri, and Metzler]{tay2020efficient}
Tay, Y., Dehghani, M., Bahri, D., and Metzler, D.
\newblock Efficient transformers: A survey.
\newblock \emph{arXiv preprint arXiv:2009.06732}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2017.

\bibitem[Veli{\v{c}}kovi{\'{c}} et~al.(2018)Veli{\v{c}}kovi{\'{c}}, Cucurull,
  Casanova, Romero, Li{\`{o}}, and Bengio]{velickovic2018graph}
Veli{\v{c}}kovi{\'{c}}, P., Cucurull, G., Casanova, A., Romero, A., Li{\`{o}},
  P., and Bengio, Y.
\newblock {Graph Attention Networks}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{linformer}
Wang, S., Li, B.~Z., Khabsa, M., Fang, H., and Ma, H.
\newblock Linformer: Self-attention with linear complexity, 2020.

\bibitem[Wijesinghe \& Wang(2022)Wijesinghe and Wang]{wijesinghe2022a}
Wijesinghe, A. and Wang, Q.
\newblock A new perspective on ''how graph neural networks go beyond
  weisfeiler-lehman?''.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Xu et~al.(2019)Xu, Hu, Leskovec, and Jegelka]{xu2018how}
Xu, K., Hu, W., Leskovec, J., and Jegelka, S.
\newblock How powerful are graph neural networks?
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Yang et~al.(2022)Yang, Wang, Shen, Qi, and Yin]{Yang2022}
Yang, M., Wang, R., Shen, Y., Qi, H., and Yin, B.
\newblock Breaking the expression bottleneck of graph neural networks.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, pp.\
  1--1, 2022.
\newblock \doi{10.1109/TKDE.2022.3168070}.

\bibitem[Ying et~al.(2021)Ying, Cai, Luo, Zheng, Ke, He, Shen, and
  Liu]{ying2021do}
Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., and Liu, T.-Y.
\newblock Do transformers really perform badly for graph representation?
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Ying et~al.(2018)Ying, You, Morris, Ren, Hamilton, and
  Leskovec]{ying2018hierarchical}
Ying, Z., You, J., Morris, C., Ren, X., Hamilton, W., and Leskovec, J.
\newblock Hierarchical graph representation learning with differentiable
  pooling.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[You et~al.(2019)You, Ying, and Leskovec]{you2019position}
You, J., Ying, R., and Leskovec, J.
\newblock Position-aware graph neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Zhang, Xia, and Sun]{zhang2020graph}
Zhang, J., Zhang, H., Xia, C., and Sun, L.
\newblock Graph-bert: Only attention is needed for learning graph
  representations.
\newblock \emph{arXiv preprint arXiv:2001.05140}, 2020.

\bibitem[Zhang \& Li(2021)Zhang and Li]{zhang2021nested}
Zhang, M. and Li, P.
\newblock Nested graph neural networks.
\newblock In \emph{Proceedings of the 35th Conference on Neural Information
  Processing Systems (NeurIPS)}, 2021.

\end{thebibliography}
