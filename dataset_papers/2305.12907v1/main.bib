@article{gershman2018deconstructing,
  title={Deconstructing the human algorithms for exploration},
  author={Gershman, Samuel J},
  journal={Cognition},
  volume={173},
  pages={34--42},
  year={2018},
  publisher={Elsevier}
}

@article{binz2022using,
  title={Using cognitive psychology to understand GPT-3},
  author={Binz, Marcel and Schulz, Eric},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={6},
  pages={e2218523120},
  year={2023},
  publisher={National Acad Sciences}
}

@article{hagendorff2023machine,
  title={Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods},
  author={Hagendorff, Thilo},
  journal={arXiv preprint arXiv:2303.13988},
  year={2023}
}


@inproceedings{hochreiter2001learning,
  title={Learning to learn using gradient descent},
  author={Hochreiter, Sepp and Younger, A Steven and Conwell, Peter R},
  booktitle={International Conference on Artificial Neural Networks},
  pages={87--94},
  year={2001},
  organization={Springer}
}


@article{wang2016learning,
  title={Learning to reinforcement learn},
  author={Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
  journal={arXiv preprint arXiv:1611.05763},
  year={2016}
}

@article{duan2016rl,
  title={Rl $^2$: Fast reinforcement learning via slow reinforcement learning},
  author={Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L and Sutskever, Ilya and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1611.02779},
  year={2016}
}



@article{binz2023meta,
  title={Meta-Learned Models of Cognition},
  author={Binz, Marcel and Dasgupta, Ishita and Jagadish, Akshay and Botvinick, Matthew and Wang, Jane X and Schulz, Eric},
  journal={arXiv preprint arXiv:2304.06729},
  year={2023}
}

@article{drori2022neural,
  title={A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level},
  author={Drori, Iddo and Zhang, Sarah and Shuttleworth, Reece and Tang, Leonard and Lu, Albert and Ke, Elizabeth and Liu, Kevin and Chen, Linda and Tran, Sunny and Cheng, Newman and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={119},
  number={32},
  pages={e2123433119},
  year={2022},
  publisher={National Acad Sciences}
}

@article{webb2022emergent,
  title={Emergent Analogical Reasoning in Large Language Models},
  author={Webb, Taylor and Holyoak, Keith J and Lu, Hongjing},
  journal={arXiv preprint arXiv:2212.09196},
  year={2022}
}


@article{eloundou2023gpts,
  title={Gpts are gpts: An early look at the labor market impact potential of large language models},
  author={Eloundou, Tyna and Manning, Sam and Mishkin, Pamela and Rock, Daniel},
  journal={arXiv preprint arXiv:2303.10130},
  year={2023}
}

@article{kasneci2023chatgpt,
  title={ChatGPT for good? On opportunities and challenges of large language models for education},
  author={Kasneci, Enkelejda and Se{\ss}ler, Kathrin and K{\"u}chemann, Stefan and Bannert, Maria and Dementieva, Daryna and Fischer, Frank and Gasser, Urs and Groh, Georg and G{\"u}nnemann, Stephan and H{\"u}llermeier, Eyke and others},
  journal={Learning and Individual Differences},
  volume={103},
  pages={102274},
  year={2023},
  publisher={Elsevier}
}




@misc{Lovre,
  author = {Lovre},
  title = {Who models the models that model models? An exploration of GPT-3's in-context model fitting ability},
  howpublished = {\url{https://www.alignmentforum.org/posts/c2RzFadrxkzyRAFXa/who-models-the-models-that-model-models-an-exploration-of}},
  notes = {Accessed: 2023-02-05}
}

@inproceedings{binz2022modeling,
  title={Modeling Human Exploration Through Resource-Rational Reinforcement Learning},
  author={Binz, Marcel and Schulz, Eric},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@inproceedings{rulebased,
  title={Functional learning: The learning of continuous functional mappings relating stimulus and response continua},
  author={Carroll, J. D.},
  booktitle={Educational Testing Service},
  year={1963}
}

@inproceedings{sinequanon,
  title={The sine qua non for abstraction in function
learning},
  author={DeLosh, E. L., Busemeyer, J. R., & McDaniel, M. A.},
  booktitle={Journal of Experimental Psychology: Learning,
Memory, and Cognition},
pages={968–986},
  year={1997}
}

@incollection{similaritybased_anns,
  title={Learning functional relations based on experience
with input-output pairs by humans and artificial neural networks},
  author={Busemeyer, J. R., Byun, E., DeLosh, E. L., & McDaniel, M. A},
  booktitle={Knowledge, Concepts and Categories},
  year={1997},
  pages={405-437},
  publisher ={Cambridge: MIT Press},
  editor={KE Lamberts}
}

@inproceedings{Lichtenberg2017SimpleRM,
  title={Simple Regression Models},
  author={Jan Malte Lichtenberg and {\"O}zg{\"u}r Simsek},
  booktitle={IDM@NIPS},
  year={2017}
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lampinen2022language,
      title={Can language models learn from explanations in context?}, 
      author={Andrew K. Lampinen and Ishita Dasgupta and Stephanie C. Y. Chan and Kory Matthewson and Michael Henry Tessler and Antonia Creswell and James L. McClelland and Jane X. Wang and Felix Hill},
      year={2022},
      eprint={2204.02329},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{reynolds2021prompt,
      title={Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm}, 
      author={Laria Reynolds and Kyle McDonell},
      year={2021},
      eprint={2102.07350},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{prystawski2023think,
      title={Why think step-by-step? Reasoning emerges from the locality of experience}, 
      author={Ben Prystawski and Noah D. Goodman},
      year={2023},
      eprint={2304.03843},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{min2022rethinking,
      title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?}, 
      author={Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
      year={2022},
      eprint={2202.12837},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hegselmann2023tabllm,
      title={TabLLM: Few-shot Classification of Tabular Data with Large Language Models}, 
      author={Stefan Hegselmann and Alejandro Buendia and Hunter Lang and Monica Agrawal and Xiaoyi Jiang and David Sontag},
      year={2023},
      eprint={2210.10723},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{akyurek2022learning,
  title={What learning algorithm is in-context learning? investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  journal={arXiv preprint arXiv:2211.15661},
  year={2022}
}


@misc{garg2023transformers,
      title={What Can Transformers Learn In-Context? A Case Study of Simple Function Classes}, 
      author={Shivam Garg and Dimitris Tsipras and Percy Liang and Gregory Valiant},
      year={2023},
      eprint={2208.01066},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{vonoswald2022transformers,
      title={Transformers learn in-context by gradient descent}, 
      author={Johannes von Oswald and Eyvind Niklasson and Ettore Randazzo and João Sacramento and Alexander Mordvintsev and Andrey Zhmoginov and Max Vladymyrov},
      year={2022},
      eprint={2212.07677},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{laskin2022incontext,
      title={In-context Reinforcement Learning with Algorithm Distillation}, 
      author={Michael Laskin and Luyu Wang and Junhyuk Oh and Emilio Parisotto and Stephen Spencer and Richie Steigerwald and DJ Strouse and Steven Hansen and Angelos Filos and Ethan Brooks and Maxime Gazeau and Himanshu Sahni and Satinder Singh and Volodymyr Mnih},
      year={2022},
      eprint={2210.14215},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lee2022multigame,
      title={Multi-Game Decision Transformers}, 
      author={Kuang-Huei Lee and Ofir Nachum and Mengjiao Yang and Lisa Lee and Daniel Freeman and Winnie Xu and Sergio Guadarrama and Ian Fischer and Eric Jang and Henryk Michalewski and Igor Mordatch},
      year={2022},
      eprint={2205.15241},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{reed2022generalist,
      title={A Generalist Agent}, 
      author={Scott Reed and Konrad Zolna and Emilio Parisotto and Sergio Gomez Colmenarejo and Alexander Novikov and Gabriel Barth-Maron and Mai Gimenez and Yury Sulsky and Jackie Kay and Jost Tobias Springenberg and Tom Eccles and Jake Bruce and Ali Razavi and Ashley Edwards and Nicolas Heess and Yutian Chen and Raia Hadsell and Oriol Vinyals and Mahyar Bordbar and Nando de Freitas},
      year={2022},
      eprint={2205.06175},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{coda2023inducing,
  title={Inducing anxiety in large language models increases exploration and bias},
  author={Coda-Forno, Julian and Witte, Kristin and Jagadish, Akshay K and Binz, Marcel and Akata, Zeynep and Schulz, Eric},
  journal={arXiv preprint arXiv:2304.11111},
  year={2023}
}

@article{lampinen2023language,
  title={Can language models handle recursively nested grammatical structures? A case study on comparing models and humans},
  author={Lampinen, Andrew Kyle},
  journal={arXiv preprint arXiv:2210.15303},
  year={2022}
}

@article{birhane2023science,
  title={Science in the age of large language models},
  author={Birhane, Abeba and Kasirzadeh, Atoosa and Leslie, David and Wachter, Sandra},
  journal={Nature Reviews Physics},
  pages={1--4},
  year={2023},
  publisher={Nature Publishing Group UK London}
}


@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{wilsonexplo-exploit,
author = {Wilson, Robert and Geana, Andra and White, John and Ludvig, Elliot and Cohen, Jonathan},
year = {2014},
month = {10},
title = {Humans Use Directed and Random Exploration to Solve the Explore-Exploit Dilemma.},
volume = {143},
journal = {Journal of experimental psychology. General},
doi = {10.1037/a0038199}
}

@article{random_forest,
author = {Breiman, L},
year = {2001},
month = {10},
pages = {5-32},
title = {Random Forests},
volume = {45},
journal = {Machine Learning},
doi = {10.1023/A:1010950718922}
}

@misc{xie2022bayesianexplanation,
      title={An Explanation of In-context Learning as Implicit Bayesian Inference}, 
      author={Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
      year={2022},
      eprint={2111.02080},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chan2022data&burstiness,
      title={Data Distributional Properties Drive Emergent In-Context Learning in Transformers}, 
      author={Stephanie C. Y. Chan and Adam Santoro and Andrew K. Lampinen and Jane X. Wang and Aaditya Singh and Pierre H. Richemond and Jay McClelland and Felix Hill},
      year={2022},
      eprint={2205.05055},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{webb2023emergent,
      title={Emergent Analogical Reasoning in Large Language Models}, 
      author={Taylor Webb and Keith J. Holyoak and Hongjing Lu},
      year={2023},
      eprint={2212.09196},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{dasgupta2022language,
  title={Language models show human-like content effects on reasoning},
  author={Dasgupta, Ishita and Lampinen, Andrew K and Chan, Stephanie CY and Creswell, Antonia and Kumaran, Dharshan and McClelland, James L and Hill, Felix},
  journal={arXiv preprint arXiv:2207.07051},
  year={2022}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{Harlow1949-HARTFO-23,
	number = {1},
	author = {Harry F. Harlow},
	pages = {51--65},
	title = {The Formation of Learning Sets},
	volume = {56},
	year = {1949},
	journal = {Psychological Review},
	doi = {10.1037/h0062474}
}


@misc{ortega2019metalearning,
      title={Meta-learning of Sequential Strategies}, 
      author={Pedro A. Ortega and Jane X. Wang and Mark Rowland and Tim Genewein and Zeb Kurth-Nelson and Razvan Pascanu and Nicolas Heess and Joel Veness and Alex Pritzel and Pablo Sprechmann and Siddhant M. Jayakumar and Tom McGrath and Kevin Miller and Mohammad Azar and Ian Osband and Neil Rabinowitz and András György and Silvia Chiappa and Simon Osindero and Yee Whye Teh and Hado van Hasselt and Nando de Freitas and Matthew Botvinick and Shane Legg},
      year={2019},
      eprint={1905.03030},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{svm,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  number={3},
  pages={273--297},
  year={1995},
  publisher={Springer}
}

@misc{openaiAPI,
  title = {OpenAI API},
  howpublished ={\url{https://platform.openai.com}},
  note = {Accessed: 2023-05-10}
}

@inproceedings{santoro2016meta,
  title={Meta-learning with memory-augmented neural networks},
  author={Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  booktitle={International conference on machine learning},
  pages={1842--1850},
  year={2016},
  organization={PMLR}
}