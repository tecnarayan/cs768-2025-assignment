\begin{thebibliography}{10}

\bibitem{polyak1992acceleration}
B.~T. Polyak and A.~B. Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock {\em SIAM Journal on Control and Optimization}, 30(4):838--855, 1992.

\bibitem{lan2012optimal}
Guanghui Lan.
\newblock An optimal method for stochastic composite optimization.
\newblock {\em Mathematical Programming}, 133(1-2):365--397, 2012.

\bibitem{roux2012stochastic}
Nicolas~L. Roux, Mark Schmidt, and Francis Bach.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2012.

\bibitem{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em Advances in Neural Information Processing Systems}, 2013.

\bibitem{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In {\em Advances in Neural Information Processing Systems}, 2014.

\bibitem{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em SIAM Review}, 60(2):223--311, 2018.

\bibitem{nemirovsky1983problem}
A.~S. Nemirovski and D.~B. Yudin.
\newblock {\em Problem complexity and method efficiency in optimization}.
\newblock John Wiley, 1983.

\bibitem{hardt2016train}
Moritz Hardt, Ben Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In {\em International Conference on Machine Learning}, 2016.

\bibitem{Lin2017multiplepass}
Junhong Lin and Lorenzo Rosasco.
\newblock Optimal rates for multi-pass stochastic gradient methods.
\newblock {\em Journal of Machine Learning Research}, 18(97):1--47, 2017.

\bibitem{fischer2017sobolev}
Simon Fischer and Ingo Steinwart.
\newblock Sobolev norm learning rates for regularized least-squares algorithm.
\newblock Technical Report 1702.07254, arXiv, 2017.

\bibitem{caponnetto2007optimal}
Andrea Caponnetto and Ernesto De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock {\em Foundations of Computational Mathematics}, 7(3):331--368, 2007.

\bibitem{yao2007early}
Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto.
\newblock On early stopping in gradient descent learning.
\newblock {\em Constructive Approximation}, 26(2):289--315, 2007.

\bibitem{dieuleveut2016nonparametric}
Aymeric Dieuleveut and Francis Bach.
\newblock Nonparametric stochastic approximation with large step-sizes.
\newblock {\em The Annals of Statistics}, 44(4):1363--1399, 2016.

\bibitem{lin2018optimal}
Junhong Lin, Alessandro Rudi, Lorenzo Rosasco, and Volkan Cevher.
\newblock Optimal rates for spectral algorithms with least-squares regression
  over hilbert spaces.
\newblock {\em Applied and Computational Harmonic Analysis}, 2018.

\bibitem{gerfo2008spectral}
L~Lo Gerfo, L~Rosasco, F~Odone, E~De Vito, and A~Verri.
\newblock Spectral algorithms for supervised learning.
\newblock {\em Neural Computation}, 20(7):1873--1897, 2008.

\bibitem{rosasco2015learning}
Lorenzo Rosasco and Silvia Villa.
\newblock Learning with incremental iterative regularization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1630--1638, 2015.

\bibitem{blanchard2016convergence}
Gilles Blanchard and Nicole Kr{\"a}mer.
\newblock Convergence rates of kernel conjugate gradient for random design
  regression.
\newblock {\em Analysis and Applications}, 14(06):763--794, 2016.

\bibitem{bach2013non}
Francis Bach and Eric Moulines.
\newblock Non-strongly-convex smooth stochastic approximation with convergence
  rate ${O}(1/n)$.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 773--781, 2013.

\bibitem{dieuleveut2017harder}
Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach.
\newblock Harder, better, faster, stronger convergence rates for least-squares
  regression.
\newblock {\em Journal of Machine Learning Research}, 18(1):3520--3570, 2017.

\bibitem{Ying2008}
Yiming Ying and Massimiliano Pontil.
\newblock Online gradient descent learning algorithms.
\newblock {\em Foundations of Computational Mathematics}, 8(5):561--596, Oct
  2008.

\bibitem{rudi2017generalization}
Alessandro Rudi and Lorenzo Rosasco.
\newblock Generalization properties of learning with random features.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3215--3225, 2017.

\bibitem{steinwart2009optimal}
Ingo Steinwart, Don~R. Hush, and Clint Scovel.
\newblock Optimal rates for regularized least squares regression.
\newblock In {\em Proc. COLT}, 2009.

\bibitem{Adams1975Sobolev}
R.~A. Adams.
\newblock {\em Sobolev spaces / Robert A. Adams}.
\newblock Academic Press New York, 1975.

\bibitem{Wahba1990spline}
G.~Wahba.
\newblock {\em Spline Models for Observational Data}.
\newblock Society for Industrial and Applied Mathematics, 1990.

\bibitem{wendland2004scattered}
Holger Wendland.
\newblock {\em Scattered data approximation}, volume~17.
\newblock Cambridge university press, 2004.

\bibitem{bach2017equivalence}
Francis Bach.
\newblock On the equivalence between kernel quadrature rules and random feature
  expansions.
\newblock {\em Journal of Machine Learning Research}, 18(21):1--38, 2017.

\bibitem{blanchard2017optimal}
Gilles Blanchard and Nicole M{\"u}cke.
\newblock Optimal rates for regularization of statistical inverse learning
  problems.
\newblock {\em Foundations of Computational Mathematics}, pages 1--43, 2017.

\bibitem{NIPS2016_6245}
Ohad Shamir.
\newblock Without-replacement sampling for stochastic gradient methods.
\newblock In {\em Advances in Neural Information Processing Systems 29}, pages
  46--54, 2016.

\bibitem{gurbuzbalaban2015random}
Mert G{\"u}rb{\"u}zbalaban, Asu Ozdaglar, and Pablo Parrilo.
\newblock Why random reshuffling beats stochastic gradient descent.
\newblock Technical Report 1510.08560, arXiv, 2015.

\bibitem{loucas}
Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach.
\newblock Exponential convergence of testing error for stochastic gradient
  methods.
\newblock In {\em Proceedings of the 31st Conference On Learning Theory},
  volume~75, pages 250--296, 2018.

\bibitem{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock Technical Report 1611.03530, arXiv, 2016.

\bibitem{ciliberto2016consistent}
Carlo Ciliberto, Lorenzo Rosasco, and Alessandro Rudi.
\newblock A consistent regularization approach for structured prediction.
\newblock In {\em Advances in neural information processing systems}, pages
  4412--4420, 2016.

\bibitem{osokin2017structured}
Anton Osokin, Francis Bach, and Simon Lacoste-Julien.
\newblock On structured prediction theory with calibrated convex surrogate
  losses.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  302--313, 2017.

\bibitem{ciliberto2018localized}
Carlo Ciliberto, Francis Bach, and Alessandro Rudi.
\newblock Localized structured prediction.
\newblock {\em arXiv preprint arXiv:1806.02402}, 2018.

\bibitem{ciliberto2017consistent}
Carlo Ciliberto, Alessandro Rudi, Lorenzo Rosasco, and Massimiliano Pontil.
\newblock Consistent multitask learning with nonlinear output relations.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1986--1996, 2017.

\bibitem{rahimi2008random}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In {\em Advances in neural information processing systems}, pages
  1177--1184, 2008.

\bibitem{carratino2018learning}
Luigi Carratino, Alessandro Rudi, and Lorenzo Rosasco.
\newblock Learning with sgd and random features.
\newblock {\em arXiv preprint arXiv:1807.06343}, 2018.

\bibitem{aguech2000perturbation}
R.~Aguech, E.~Moulines, and P.~Priouret.
\newblock On a perturbation approach for the analysis of stochastic tracking
  algorithms.
\newblock {\em SIAM J. Control and Optimization}, 39(3):872--899, 2000.

\bibitem{rudi2013sample}
Alessandro Rudi, Guillermo~D Canas, and Lorenzo Rosasco.
\newblock On the sample complexity of subspace learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2067--2075, 2013.

\bibitem{tropp2012user}
Joel~A Tropp.
\newblock User-friendly tail bounds for sums of random matrices.
\newblock {\em Foundations of computational mathematics}, 12(4):389--434, 2012.

\bibitem{rudi2017falkon}
Alessandro Rudi, Luigi Carratino, and Lorenzo Rosasco.
\newblock Falkon: An optimal large scale kernel method.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3888--3898, 2017.

\end{thebibliography}
