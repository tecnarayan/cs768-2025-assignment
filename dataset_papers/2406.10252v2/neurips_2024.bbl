\begin{thebibliography}{10}

\bibitem{pouyanfar2018survey}
Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian, Yudong Tao, Maria~Presa Reyes, Mei-Ling Shyu, Shu-Ching Chen, and Sundaraja~S Iyengar.
\newblock A survey on deep learning: Algorithms, techniques, and applications.
\newblock {\em ACM Computing Surveys (CSUR)}, 51(5):1--36, 2018.

\bibitem{chang2023survey}
Yupeng Chang, Xu~Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et~al.
\newblock A survey on evaluation of large language models.
\newblock {\em ACM Transactions on Intelligent Systems and Technology}, 2023.

\bibitem{zhao2023survey}
Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al.
\newblock A survey of large language models.
\newblock {\em arXiv preprint arXiv:2303.18223}, 2023.

\bibitem{khan2022transformers}
Salman Khan, Muzammal Naseer, Munawar Hayat, Syed~Waqas Zamir, Fahad~Shahbaz Khan, and Mubarak Shah.
\newblock Transformers in vision: A survey.
\newblock {\em ACM computing surveys (CSUR)}, 54(10s):1--41, 2022.

\bibitem{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock {\em nature}, 521(7553):436--444, 2015.

\bibitem{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock {\em Deep learning}.
\newblock MIT press, 2016.

\bibitem{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{kirillov2023segment}
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander~C Berg, Wan-Yen Lo, et~al.
\newblock Segment anything.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 4015--4026, 2023.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{chen2023extending}
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.
\newblock Extending context window of large language models via positional interpolation.
\newblock {\em arXiv preprint arXiv:2306.15595}, 2023.

\bibitem{chen2023longlora}
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.
\newblock Longlora: Efficient fine-tuning of long-context large language models.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{wang2024augmenting}
Weizhi Wang, Li~Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei.
\newblock Augmenting language models with long-term memory.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{liu2024lost}
Nelson~F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.
\newblock Lost in the middle: How language models use long contexts.
\newblock {\em Transactions of the Association for Computational Linguistics}, 12:157--173, 2024.

\bibitem{kaddour2023challenges}
Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy.
\newblock Challenges and applications of large language models.
\newblock {\em arXiv preprint arXiv:2307.10169}, 2023.

\bibitem{shi2023large}
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed~H Chi, Nathanael Sch{\"a}rli, and Denny Zhou.
\newblock Large language models can be easily distracted by irrelevant context.
\newblock In {\em International Conference on Machine Learning}, pages 31210--31227. PMLR, 2023.

\bibitem{li2023long}
Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang.
\newblock How long can context length of open-source llms truly promise?
\newblock In {\em NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following}, 2023.

\bibitem{li2024long}
Tianle Li, Ge~Zhang, Quy~Duc Do, Xiang Yue, and Wenhu Chen.
\newblock Long-context llms struggle with long in-context learning.
\newblock {\em arXiv preprint arXiv:2404.02060}, 2024.

\bibitem{wang2023surveyfact}
Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, et~al.
\newblock Survey on factuality in large language models: Knowledge, retrieval and domain-specificity.
\newblock {\em arXiv preprint arXiv:2310.07521}, 2023.

\bibitem{ji2023survey}
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye~Jin Bang, Andrea Madotto, and Pascale Fung.
\newblock Survey of hallucination in natural language generation.
\newblock {\em ACM Computing Surveys}, 55(12):1--38, 2023.

\bibitem{shao2024assisting}
Yijia Shao, Yucheng Jiang, Theodore~A. Kanell, Peter Xu, Omar Khattab, and Monica~S. Lam.
\newblock {Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models}.
\newblock In {\em Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, 2024.

\bibitem{pandalm2024}
Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang.
\newblock Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization.
\newblock 2024.

\bibitem{zheng2024judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{yu2024kieval}
Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, and Shikun Zhang.
\newblock Kieval: A knowledge-grounded interactive evaluation framework for large language models.
\newblock 2024.

\bibitem{gao2023retrieval}
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi~Dai, Jiawei Sun, and Haofen Wang.
\newblock Retrieval-augmented generation for large language models: A survey.
\newblock {\em arXiv preprint arXiv:2312.10997}, 2023.

\bibitem{lewis2020retrieval}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K{\"u}ttler, Mike Lewis, Wen-tau Yih, Tim Rockt{\"a}schel, et~al.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock {\em Advances in Neural Information Processing Systems}, 33:9459--9474, 2020.

\bibitem{jiang2023active}
Zhengbao Jiang, Frank~F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig.
\newblock Active retrieval augmented generation.
\newblock In {\em Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 7969--7992, 2023.

\bibitem{gao2023citation}
Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.
\newblock Enabling large language models to generate text with citations.
\newblock In {\em Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, 2023.

\bibitem{tan2021long-form}
Bowen Tan, Zichao Yang, Maruan Al{-}Shedivat, Eric~P. Xing, and Zhiting Hu.
\newblock Progressive generation of long text with pretrained language models.
\newblock In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani{-}T{\"{u}}r, Iz~Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, {\em Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2021, Online, June 6-11, 2021}, pages 4313--4324. Association for Computational Linguistics, 2021.

\bibitem{bai2023longbench}
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li.
\newblock Longbench: {A} bilingual, multitask benchmark for long context understanding.
\newblock {\em CoRR}, abs/2308.14508, 2023.

\bibitem{dong2023bamboo}
Zican Dong, Tianyi Tang, Junyi Li, Wayne~Xin Zhao, and Ji{-}Rong Wen.
\newblock {BAMBOO:} {A} comprehensive benchmark for evaluating long text modeling capacities of large language models.
\newblock {\em CoRR}, abs/2309.13345, 2023.

\bibitem{li2023LooGLE}
Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang.
\newblock Loogle: Can long-context language models understand long contexts?
\newblock {\em CoRR}, abs/2311.04939, 2023.

\bibitem{shaw2018rpe}
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
\newblock Self-attention with relative position representations.
\newblock In Marilyn~A. Walker, Heng Ji, and Amanda Stent, editors, {\em Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers)}, pages 464--468. Association for Computational Linguistics, 2018.

\bibitem{wang2019spe}
Xing Wang, Zhaopeng Tu, Longyue Wang, and Shuming Shi.
\newblock Self-attention with structural position representations.
\newblock In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, {\em Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, {EMNLP-IJCNLP} 2019, Hong Kong, China, November 3-7, 2019}, pages 1403--1409. Association for Computational Linguistics, 2019.

\bibitem{zhou2023recurrentgpt}
Wangchunshu Zhou, Yuchen~Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan.
\newblock Recurrentgpt: Interactive generation of (arbitrarily) long text.
\newblock {\em arXiv preprint arXiv:2305.13304}, 2023.

\bibitem{wang2024templora}
Y~Wang, D~Ma, and D~Cai.
\newblock With greater text comes greater necessity: Inference-time training helps long text generation.
\newblock {\em arXiv preprint arXiv:2401.11504}, 2024.

\bibitem{fan2018hierarchicalstory}
Angela Fan, Mike Lewis, and Yann Dauphin.
\newblock Hierarchical neural story generation.
\newblock {\em arXiv preprint arXiv:1805.04833}, 2018.

\bibitem{wu2021recursively}
Jeff Wu, Long Ouyang, Daniel~M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano.
\newblock Recursively summarizing books with human feedback.
\newblock {\em arXiv preprint arXiv:2109.10862}, 2021.

\bibitem{chang2023booookscore}
Yapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer.
\newblock Booookscore: A systematic exploration of book-length summarization in the era of llms.
\newblock {\em arXiv preprint arXiv:2310.00785}, 2023.

\bibitem{cho2018traingcohernt}
Woon~Sang Cho, Pengchuan Zhang, Yizhe Zhang, Xiujun Li, Michel Galley, Chris Brockett, Mengdi Wang, and Jianfeng Gao.
\newblock Towards coherent and cohesive long-form text generation.
\newblock {\em arXiv preprint arXiv:1811.00511}, 2018.

\bibitem{bosselut2018awardcoherent}
Antoine Bosselut, Asli Celikyilmaz, Xiaodong He, Jianfeng Gao, Po-Sen Huang, and Yejin Choi.
\newblock Discourse-aware neural rewards for coherent text generation.
\newblock {\em arXiv preprint arXiv:1805.03766}, 2018.

\bibitem{zhou2023contextfaithfulness}
Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen.
\newblock Context-faithful prompting for large language models.
\newblock {\em arXiv preprint arXiv:2303.11315}, 2023.

\bibitem{liu2024probingstructure}
Jinxin Liu, Shulin Cao, Jiaxin Shi, Tingjian Zhang, Lei Hou, and Juanzi Li.
\newblock Probing structured semantics understanding and generation of language models via question answering.
\newblock {\em arXiv preprint arXiv:2401.05777}, 2024.

\bibitem{zhang2024style}
Chiyu Zhang, Honglong Cai, Yuexin Wu, Le~Hou, Muhammad Abdul-Mageed, et~al.
\newblock Distilling text style transfer with self-explanation from llms.
\newblock {\em arXiv preprint arXiv:2403.01106}, 2024.

\bibitem{schramowski2022ethics}
Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin~A Rothkopf, and Kristian Kersting.
\newblock Large pre-trained language models contain human-like biases of what is right and wrong to do.
\newblock {\em Nature Machine Intelligence}, 4(3):258--268, 2022.

\bibitem{menick2022teachingcitations}
Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et~al.
\newblock Teaching language models to support answers with verified quotes.
\newblock {\em arXiv preprint arXiv:2203.11147}, 2022.

\bibitem{balepur2023expository}
Nishant Balepur, Jie Huang, and Kevin Chen-Chuan Chang.
\newblock Expository text generation: Imitate, retrieve, paraphrase.
\newblock {\em arXiv preprint arXiv:2305.03276}, 2023.

\bibitem{wang2019paperrobot}
Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji, Mohit Bansal, and Yi~Luan.
\newblock Paperrobot: Incremental draft generation of scientific ideas.
\newblock {\em arXiv preprint arXiv:1905.07870}, 2019.

\bibitem{nussbaum2024nomic}
Zach Nussbaum, John~X. Morris, Brandon Duderstadt, and Andriy Mulyar.
\newblock Nomic embed: Training a reproducible long context text embedder, 2024.

\end{thebibliography}
