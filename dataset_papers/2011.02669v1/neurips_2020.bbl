\begin{thebibliography}{10}

\bibitem{DevlinK11}
Sam Devlin and Daniel Kudenko.
\newblock Theoretical considerations of potential-based reward shaping for
  multi-agent systems.
\newblock In {\em Proceedings of the 10th International Conference on
  Autonomous Agents and Multiagent Systems {(AAMAS}'11)}, pages 225--232, 2011.

\bibitem{devlin2012dynamic}
Sam~Michael Devlin and Daniel Kudenko.
\newblock Dynamic potential-based reward shaping.
\newblock In {\em Proceedings of the 11th International Conference on
  Autonomous Agents and Multiagent Systems (AAMAS'12)}, pages 433--440, 2012.

\bibitem{dorigo1994robot}
Marco Dorigo and Marco Colombetti.
\newblock Robot shaping: Developing autonomous agents through learning.
\newblock {\em Artificial intelligence}, 71(2):321--370, 1994.

\bibitem{FuZLL19}
Zhao{-}Yang Fu, De{-}Chuan Zhan, Xin{-}Chun Li, and Yi{-}Xing Lu.
\newblock Automatic successive reinforcement learning with multiple auxiliary
  rewards.
\newblock In {\em Proceedings of the 28th International Joint Conference on
  Artificial Intelligence ({IJCAI}'19)}, pages 2336--2342, 2019.

\bibitem{grzes2008learning}
Marek Grzes and Daniel Kudenko.
\newblock Learning potential for reward shaping in reinforcement learning with
  tile coding.
\newblock In {\em Proceedings of the AAMAS 2008 Workshop on Adaptive and
  Learning Agents and Multi-Agent Systems (ALAMAS-ALAg 2008)}, pages 17--23,
  2008.

\bibitem{harutyunyan2015expressing}
Anna Harutyunyan, Sam Devlin, Peter Vrancx, and Ann Nowe.
\newblock Expressing arbitrary reward functions as potential-based advice.
\newblock In {\em Proceedings of the 29th AAAI Conference on Artificial
  Intelligence (AAAI'15)}, pages 2652--2658, 2015.

\bibitem{jaderberg2019human}
Max Jaderberg, Wojciech~M Czarnecki, Iain Dunning, Luke Marris, Guy Lever,
  Antonio~Garcia Castaneda, Charles Beattie, Neil~C Rabinowitz, Ari~S Morcos,
  Avraham Ruderman, et~al.
\newblock Human-level performance in 3d multiplayer games with population-based
  reinforcement learning.
\newblock {\em Science}, 364(6443):859--865, 2019.

\bibitem{lample2017playing}
Guillaume Lample and Devendra~Singh Chaplot.
\newblock Playing fps games with deep reinforcement learning.
\newblock In {\em Proceedings of the 31st AAAI Conference on Artificial
  Intelligence (AAAI'17)}, pages 2140--2146, 2017.

\bibitem{LaudD03}
Adam Laud and Gerald DeJong.
\newblock The influence of reward on the speed of reinforcement learning: An
  analysis of shaping.
\newblock In {\em Proceedings of the 20th International Conference on Machine
  Learning {(ICML}'03)}, pages 440--447, 2003.

\bibitem{MaromR18}
Ofir Marom and Benjamin Rosman.
\newblock Belief reward shaping in reinforcement learning.
\newblock In {\em Proceedings of the 32nd {AAAI} Conference on Artificial
  Intelligence (AAAI'18)}, pages 3762--3769, 2018.

\bibitem{Marthi07}
Bhaskara Marthi.
\newblock Automatic shaping and decomposition of reward functions.
\newblock In {\em Proceedings of the 24th International Conference on Machine
  Learning {(ICML}'07)}, pages 601--608, 2007.

\bibitem{ng1999policy}
Andrew~Y Ng, Daishi Harada, and Stuart Russell.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In {\em Proceedings of the 16th International Conference on Machine
  Learning {(ICML}'99)}, pages 278--287, 1999.

\bibitem{OpenAI_dota}
OpenAI.
\newblock Openai five.
\newblock {\url{https://blog.openai.com/openai-five/}}, 2018.

\bibitem{PathakAED17}
Deepak Pathak, Pulkit Agrawal, Alexei~A. Efros, and Trevor Darrell.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning ({ICML}'17)}, pages 2778--2787, 2017.

\bibitem{randlov1998learning}
Jette Randl{\o}v and Preben Alstr{\o}m.
\newblock Learning to drive a bicycle using reinforcement learning and shaping.
\newblock In {\em Proceedings of the 15th International Conference on Machine
  Learning {(ICML}'98)}, pages 463--471, 1998.

\bibitem{SchulmanWDRK17}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint}, {arXiv:1707.06347}, 2017.

\bibitem{SinghBC04}
Satinder~P. Singh, Andrew~G. Barto, and Nuttapong Chentanez.
\newblock Intrinsically motivated reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems 17
  ({NIPS}'04)}, pages 1281--1288, 2004.

\bibitem{song2019playing}
Shihong Song, Jiayi Weng, Hang Su, Dong Yan, Haosheng Zou, and Jun Zhu.
\newblock Playing fps games with environment-aware hierarchical reinforcement
  learning.
\newblock In {\em Proceedings of the 28th International Joint Conference on
  Artificial Intelligence (AAAI'19)}, pages 3475--3482, 2019.

\bibitem{SorgSL10}
Jonathan Sorg, Satinder~P. Singh, and Richard~L. Lewis.
\newblock Reward design via online gradient ascent.
\newblock In {\em Advances in Neural Information Processing Systems 23
  ({NIPS'10})}, pages 2190--2198, 2010.

\bibitem{SunCWL18}
Fan{-}Yun Sun, Yen{-}Yu Chang, Yueh{-}Hua Wu, and Shou{-}De Lin.
\newblock Designing non-greedy reinforcement learning agents with diminishing
  reward shaping.
\newblock In {\em Proceedings of the 2018 {AAAI/ACM} Conference on AI, Ethics,
  and Society ({AIES} 2018)}, pages 297--302, 2018.

\bibitem{sutton2000policy}
Richard~S. Sutton, David~A. McAllester, Satinder~P. Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In {\em Proceedings of Advances in Neural Information Processing
  Systems 12 (NIPS'00)}, pages 1057--1063, 1999.

\bibitem{TesslerMM19}
Chen Tessler, Daniel~J. Mankowitz, and Shie Mannor.
\newblock Reward constrained policy optimization.
\newblock In {\em Proceedings of the 7th International Conference on Learning
  Representations ({ICLR}'19)}, 2019.

\bibitem{Wiewiora03JAIR}
Eric Wiewiora.
\newblock Potential-based shaping and q-value initialization are equivalent.
\newblock {\em J. Artif. Intell. Res.}, 19:205--208, 2003.

\bibitem{wiewiora2003principled}
Eric Wiewiora, Garrison~W Cottrell, and Charles Elkan.
\newblock Principled methods for advising reinforcement learning agents.
\newblock In {\em Proceedings of the 20th International Conference on Machine
  Learning (ICML'03)}, pages 792--799, 2003.

\bibitem{WuL18}
Yueh{-}Hua Wu and Shou{-}De Lin.
\newblock A low-cost ethics shaping approach for designing reinforcement
  learning agents.
\newblock In {\em Proceedings of the 32nd {AAAI} Conference on Artificial
  Intelligence (AAAI'18)}, pages 1687--1694, 2018.

\bibitem{zheng2019can}
Zeyu Zheng, Junhyuk Oh, Matteo Hessel, Zhongwen Xu, Manuel Kroiss, Hado van
  Hasselt, David Silver, and Satinder Singh.
\newblock What can learned intrinsic rewards capture?
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning ({ICML}'20)}, 2019.

\bibitem{ZhengOS18}
Zeyu Zheng, Junhyuk Oh, and Satinder Singh.
\newblock On learning intrinsic rewards for policy gradient methods.
\newblock In {\em Advances in Neural Information Processing Systems 31
  ({NeurIPS'18})}, pages 4649--4659, 2018.

\bibitem{HaoshengZouArXiv-1901-09330}
Haosheng Zou, Tongzheng Ren, Dong Yan, Hang Su, and Jun Zhu.
\newblock Reward shaping via meta-learning.
\newblock {\em arXiv preprint}, {arXiv:1901.09330}, 2019.

\end{thebibliography}
