\begin{thebibliography}{75}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alayrac et~al.(2019)Alayrac, Uesato, Huang, Fawzi, Stanforth, and
  Kohli]{alayrac2019labels}
Alayrac, J.-B., Uesato, J., Huang, P.-S., Fawzi, A., Stanforth, R., and Kohli,
  P.
\newblock Are labels required for improving adversarial robustness?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  12192--12202, 2019.

\bibitem[Ali et~al.(2018)Ali, Kolter, and Tibshirani]{ali2018continuous}
Ali, A., Kolter, J.~Z., and Tibshirani, R.~J.
\newblock A continuous-time view of early stopping for least squares
  regression.
\newblock \emph{arXiv preprint arXiv:1810.10082}, 2018.

\bibitem[Athalye et~al.(2017)Athalye, Engstrom, Ilyas, and
  Kwok]{athalye2017synthesizing}
Athalye, A., Engstrom, L., Ilyas, A., and Kwok, K.
\newblock Synthesizing robust adversarial examples.
\newblock \emph{arXiv preprint arXiv:1707.07397}, 2017.

\bibitem[Athalye et~al.(2018)Athalye, Carlini, and
  Wagner]{athalye2018obfuscated}
Athalye, A., Carlini, N., and Wagner, D.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock \emph{arXiv preprint arXiv:1802.00420}, 2018.

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and Mandal]{belkin2019reconciling}
Belkin, M., Hsu, D., Ma, S., and Mandal, S.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (32):\penalty0 15849--15854, 2019.

\bibitem[Buckman et~al.(2018)Buckman, Roy, Raffel, and
  Goodfellow]{buckman2018thermometer}
Buckman, J., Roy, A., Raffel, C., and Goodfellow, I.
\newblock Thermometer encoding: One hot way to resist adversarial examples.
\newblock 2018.

\bibitem[Carlini(2019)]{carlini2019ami}
Carlini, N.
\newblock Is ami (attacks meet interpretability) robust to adversarial
  examples?
\newblock \emph{arXiv preprint arXiv:1902.02322}, 2019.

\bibitem[Carlini \& Wagner(2017{\natexlab{a}})Carlini and
  Wagner]{carlini2017adversarial}
Carlini, N. and Wagner, D.
\newblock Adversarial examples are not easily detected: Bypassing ten detection
  methods.
\newblock In \emph{Proceedings of the 10th ACM Workshop on Artificial
  Intelligence and Security}, pp.\  3--14. ACM, 2017{\natexlab{a}}.

\bibitem[Carlini \& Wagner(2017{\natexlab{b}})Carlini and
  Wagner]{carlini2017towards}
Carlini, N. and Wagner, D.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{2017 IEEE Symposium on Security and Privacy (SP)}, pp.\
  39--57. IEEE, 2017{\natexlab{b}}.

\bibitem[Carmon et~al.(2019)Carmon, Raghunathan, Schmidt, Liang, and
  Duchi]{carmon2019unlabeled}
Carmon, Y., Raghunathan, A., Schmidt, L., Liang, P., and Duchi, J.~C.
\newblock Unlabeled data improves adversarial robustness.
\newblock \emph{arXiv preprint arXiv:1905.13736}, 2019.

\bibitem[Cohen et~al.(2019)Cohen, Rosenfeld, and Kolter]{cohen2019certified}
Cohen, J.~M., Rosenfeld, E., and Kolter, J.~Z.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock \emph{arXiv preprint arXiv:1902.02918}, 2019.

\bibitem[Croce et~al.(2018)Croce, Andriushchenko, and Hein]{croce2018provable}
Croce, F., Andriushchenko, M., and Hein, M.
\newblock Provable robustness of relu networks via maximization of linear
  regions.
\newblock \emph{arXiv preprint arXiv:1810.07481}, 2018.

\bibitem[DeVries \& Taylor(2017)DeVries and Taylor]{devries2017improved}
DeVries, T. and Taylor, G.~W.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock \emph{arXiv preprint arXiv:1708.04552}, 2017.

\bibitem[Dong et~al.(2018)Dong, Liao, Pang, Su, Zhu, Hu, and
  Li]{dong2018boosting}
Dong, Y., Liao, F., Pang, T., Su, H., Zhu, J., Hu, X., and Li, J.
\newblock Boosting adversarial attacks with momentum.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  9185--9193, 2018.

\bibitem[Dvijotham et~al.()Dvijotham, Stanforth, Gowal, Mann, and
  Kohli]{dvijotham2018dual}
Dvijotham, K., Stanforth, R., Gowal, S., Mann, T.~A., and Kohli, P.
\newblock A dual approach to scalable verification of deep networks.

\bibitem[Ehlers(2017)]{ehlers2017formal}
Ehlers, R.
\newblock Formal verification of piece-wise linear feed-forward neural
  networks.
\newblock In \emph{International Symposium on Automated Technology for
  Verification and Analysis}, pp.\  269--286. Springer, 2017.

\bibitem[Engstrom et~al.(2017)Engstrom, Tran, Tsipras, Schmidt, and
  Madry]{engstrom2017rotation}
Engstrom, L., Tran, B., Tsipras, D., Schmidt, L., and Madry, A.
\newblock A rotation and a translation suffice: Fooling cnns with simple
  transformations.
\newblock \emph{arXiv preprint arXiv:1712.02779}, 2017.

\bibitem[Engstrom et~al.(2019)Engstrom, Ilyas, Santurkar, and
  Tsipras]{robustness}
Engstrom, L., Ilyas, A., Santurkar, S., and Tsipras, D.
\newblock Robustness (python library), 2019.
\newblock URL \url{https://github.com/MadryLab/robustness}.

\bibitem[Fazlyab et~al.(2019)Fazlyab, Morari, and Pappas]{fazlyab2019safety}
Fazlyab, M., Morari, M., and Pappas, G.~J.
\newblock Safety verification and robustness analysis of neural networks via
  quadratic constraints and semidefinite programming.
\newblock \emph{arXiv preprint arXiv:1903.01287}, 2019.

\bibitem[Feinman et~al.(2017)Feinman, Curtin, Shintre, and
  Gardner]{feinman2017detecting}
Feinman, R., Curtin, R.~R., Shintre, S., and Gardner, A.~B.
\newblock Detecting adversarial samples from artifacts.
\newblock \emph{arXiv preprint arXiv:1703.00410}, 2017.

\bibitem[Friedman et~al.(2001)Friedman, Hastie, and
  Tibshirani]{friedman2001elements}
Friedman, J., Hastie, T., and Tibshirani, R.
\newblock \emph{The elements of statistical learning}, volume~1.
\newblock Springer series in statistics New York, 2001.

\bibitem[Gehr et~al.(2018)Gehr, Mirman, Drachsler-Cohen, Tsankov, Chaudhuri,
  and Vechev]{gehr2018ai2}
Gehr, T., Mirman, M., Drachsler-Cohen, D., Tsankov, P., Chaudhuri, S., and
  Vechev, M.
\newblock Ai2: Safety and robustness certification of neural networks with
  abstract interpretation.
\newblock In \emph{2018 IEEE Symposium on Security and Privacy (SP)}, pp.\
  3--18. IEEE, 2018.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Goodfellow, I.~J., Shlens, J., and Szegedy, C.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem[Gowal et~al.(2018)Gowal, Dvijotham, Stanforth, Bunel, Qin, Uesato,
  Mann, and Kohli]{gowal2018effectiveness}
Gowal, S., Dvijotham, K., Stanforth, R., Bunel, R., Qin, C., Uesato, J., Mann,
  T., and Kohli, P.
\newblock On the effectiveness of interval bound propagation for training
  verifiably robust models.
\newblock \emph{arXiv preprint arXiv:1810.12715}, 2018.

\bibitem[Guo et~al.(2017)Guo, Rana, Cisse, and Van
  Der~Maaten]{guo2017countering}
Guo, C., Rana, M., Cisse, M., and Van Der~Maaten, L.
\newblock Countering adversarial images using input transformations.
\newblock \emph{arXiv preprint arXiv:1711.00117}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016identity}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European conference on computer vision}, pp.\  630--645.
  Springer, 2016.

\bibitem[Huang et~al.(2017)Huang, Kwiatkowska, Wang, and Wu]{huang2017safety}
Huang, X., Kwiatkowska, M., Wang, S., and Wu, M.
\newblock Safety verification of deep neural networks.
\newblock In \emph{International Conference on Computer Aided Verification},
  pp.\  3--29. Springer, 2017.

\bibitem[Katz et~al.(2017)Katz, Barrett, Dill, Julian, and
  Kochenderfer]{katz2017reluplex}
Katz, G., Barrett, C., Dill, D.~L., Julian, K., and Kochenderfer, M.~J.
\newblock Reluplex: An efficient smt solver for verifying deep neural networks.
\newblock In \emph{International Conference on Computer Aided Verification},
  pp.\  97--117. Springer, 2017.

\bibitem[Krogh \& Hertz(1992)Krogh and Hertz]{krogh1992simple}
Krogh, A. and Hertz, J.~A.
\newblock A simple weight decay can improve generalization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  950--957, 1992.

\bibitem[Kurakin et~al.(2016)Kurakin, Goodfellow, and
  Bengio]{kurakin2016adversarial}
Kurakin, A., Goodfellow, I., and Bengio, S.
\newblock Adversarial examples in the physical world.
\newblock \emph{arXiv preprint arXiv:1607.02533}, 2016.

\bibitem[Lecuyer et~al.(2019)Lecuyer, Atlidakis, Geambasu, Hsu, and
  Jana]{lecuyer2019certified}
Lecuyer, M., Atlidakis, V., Geambasu, R., Hsu, D., and Jana, S.
\newblock Certified robustness to adversarial examples with differential
  privacy.
\newblock In \emph{2019 IEEE Symposium on Security and Privacy (SP)}, pp.\
  656--672. IEEE, 2019.

\bibitem[Lu et~al.(2017)Lu, Sibai, Fabry, and Forsyth]{lu2017no}
Lu, J., Sibai, H., Fabry, E., and Forsyth, D.
\newblock No need to worry about adversarial examples in object detection in
  autonomous vehicles.
\newblock \emph{arXiv preprint arXiv:1707.03501}, 2017.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1706.06083}, 2017.

\bibitem[Maini et~al.(2019)Maini, Wong, and Kolter]{maini2019adversarial}
Maini, P., Wong, E., and Kolter, J.~Z.
\newblock Adversarial robustness against the union of multiple perturbation
  models.
\newblock \emph{arXiv preprint arXiv:1909.04068}, 2019.

\bibitem[Metzen et~al.(2017)Metzen, Genewein, Fischer, and
  Bischoff]{metzen2017detecting}
Metzen, J.~H., Genewein, T., Fischer, V., and Bischoff, B.
\newblock On detecting adversarial perturbations.
\newblock \emph{arXiv preprint arXiv:1702.04267}, 2017.

\bibitem[Mirman et~al.(2018)Mirman, Gehr, and Vechev]{mirman2018differentiable}
Mirman, M., Gehr, T., and Vechev, M.
\newblock Differentiable abstract interpretation for provably robust neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3575--3583, 2018.

\bibitem[Morgan \& Bourlard(1990)Morgan and Bourlard]{morgan1990generalization}
Morgan, N. and Bourlard, H.
\newblock Generalization and parameter estimation in feedforward nets: Some
  experiments.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  630--637, 1990.

\bibitem[Mosbach et~al.(2018)Mosbach, Andriushchenko, Trost, Hein, and
  Klakow]{mosbach2018logit}
Mosbach, M., Andriushchenko, M., Trost, T., Hein, M., and Klakow, D.
\newblock Logit pairing methods can fool gradient-based attacks.
\newblock \emph{arXiv preprint arXiv:1810.12042}, 2018.

\bibitem[Nakkiran et~al.(2019)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{nakkiran2019deep}
Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., and Sutskever, I.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock \emph{arXiv preprint arXiv:1912.02292}, 2019.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{neyshabur2017exploring}
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N.
\newblock Exploring generalization in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5947--5956, 2017.

\bibitem[Papernot et~al.(2016)Papernot, McDaniel, Wu, Jha, and
  Swami]{papernot2016distillation}
Papernot, N., McDaniel, P., Wu, X., Jha, S., and Swami, A.
\newblock Distillation as a defense to adversarial perturbations against deep
  neural networks.
\newblock In \emph{2016 IEEE Symposium on Security and Privacy (SP)}, pp.\
  582--597. IEEE, 2016.

\bibitem[Raghunathan et~al.(2018{\natexlab{a}})Raghunathan, Steinhardt, and
  Liang]{raghunathan2018certified}
Raghunathan, A., Steinhardt, J., and Liang, P.
\newblock Certified defenses against adversarial examples.
\newblock \emph{arXiv preprint arXiv:1801.09344}, 2018{\natexlab{a}}.

\bibitem[Raghunathan et~al.(2018{\natexlab{b}})Raghunathan, Steinhardt, and
  Liang]{raghunathan2018semidefinite}
Raghunathan, A., Steinhardt, J., and Liang, P.~S.
\newblock Semidefinite relaxations for certifying robustness to adversarial
  examples.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10877--10887, 2018{\natexlab{b}}.

\bibitem[Recht et~al.(2018)Recht, Roelofs, Schmidt, and
  Shankar]{recht2018cifar}
Recht, B., Roelofs, R., Schmidt, L., and Shankar, V.
\newblock Do cifar-10 classifiers generalize to cifar-10?
\newblock \emph{arXiv preprint arXiv:1806.00451}, 2018.

\bibitem[Salman et~al.(2019{\natexlab{a}})Salman, Yang, Li, Zhang, Zhang,
  Razenshteyn, and Bubeck]{salman2019provably}
Salman, H., Yang, G., Li, J., Zhang, P., Zhang, H., Razenshteyn, I., and
  Bubeck, S.
\newblock Provably robust deep learning via adversarially trained smoothed
  classifiers.
\newblock \emph{arXiv preprint arXiv:1906.04584}, 2019{\natexlab{a}}.

\bibitem[Salman et~al.(2019{\natexlab{b}})Salman, Yang, Zhang, Hsieh, and
  Zhang]{salman2019convex}
Salman, H., Yang, G., Zhang, H., Hsieh, C.-J., and Zhang, P.
\newblock A convex relaxation barrier to tight robustness verification of
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9832--9842, 2019{\natexlab{b}}.

\bibitem[Schmidt et~al.(2018)Schmidt, Santurkar, Tsipras, Talwar, and
  Madry]{schmidt2018adversarially}
Schmidt, L., Santurkar, S., Tsipras, D., Talwar, K., and Madry, A.
\newblock Adversarially robust generalization requires more data.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5014--5026, 2018.

\bibitem[Shafahi et~al.(2019)Shafahi, Najibi, Ghiasi, Xu, Dickerson, Studer,
  Davis, Taylor, and Goldstein]{shafahi2019adversarial}
Shafahi, A., Najibi, M., Ghiasi, A., Xu, Z., Dickerson, J., Studer, C., Davis,
  L.~S., Taylor, G., and Goldstein, T.
\newblock Adversarial training for free!
\newblock \emph{arXiv preprint arXiv:1904.12843}, 2019.

\bibitem[Singh et~al.(2018)Singh, Gehr, Mirman, P{\"u}schel, and
  Vechev]{singh2018fast}
Singh, G., Gehr, T., Mirman, M., P{\"u}schel, M., and Vechev, M.
\newblock Fast and effective robustness certification.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10802--10813, 2018.

\bibitem[Sinha et~al.(2017)Sinha, Namkoong, and Duchi]{sinha2017certifying}
Sinha, A., Namkoong, H., and Duchi, J.
\newblock Certifying some distributional robustness with principled adversarial
  training.
\newblock \emph{arXiv preprint arXiv:1710.10571}, 2017.

\bibitem[Smith(2017)]{smith2017cyclical}
Smith, L.~N.
\newblock Cyclical learning rates for training neural networks.
\newblock In \emph{2017 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pp.\  464--472. IEEE, 2017.

\bibitem[Song et~al.(2017)Song, Kim, Nowozin, Ermon, and
  Kushman]{song2017pixeldefend}
Song, Y., Kim, T., Nowozin, S., Ermon, S., and Kushman, N.
\newblock Pixeldefend: Leveraging generative models to understand and defend
  against adversarial examples.
\newblock \emph{arXiv preprint arXiv:1710.10766}, 2017.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Strand(1974)]{strand1974theory}
Strand, O.~N.
\newblock Theory and methods related to the singular-function expansion and
  landweberâ€™s iteration for integral equations of the first kind.
\newblock \emph{SIAM Journal on Numerical Analysis}, 11\penalty0 (4):\penalty0
  798--825, 1974.

\bibitem[Suggala et~al.(2018)Suggala, Prasad, and
  Ravikumar]{suggala2018connecting}
Suggala, A., Prasad, A., and Ravikumar, P.~K.
\newblock Connecting optimization and regularization paths.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10608--10619, 2018.

\bibitem[Tao et~al.(2018)Tao, Ma, Liu, and Zhang]{tao2018attacks}
Tao, G., Ma, S., Liu, Y., and Zhang, X.
\newblock Attacks meet interpretability: Attribute-steered detection of
  adversarial samples.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  7717--7728, 2018.

\bibitem[Tjeng et~al.(2019)Tjeng, Xiao, and Tedrake]{tjeng2019evaluating}
Tjeng, V., Xiao, K.~Y., and Tedrake, R.
\newblock Evaluating robustness of neural networks with mixed integer
  programming.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HyGIdiRqtm}.

\bibitem[Tram{\`e}r \& Boneh(2019)Tram{\`e}r and Boneh]{tramer2019adversarial}
Tram{\`e}r, F. and Boneh, D.
\newblock Adversarial training and robustness for multiple perturbations.
\newblock \emph{arXiv preprint arXiv:1904.13000}, 2019.

\bibitem[Wei et~al.(2017)Wei, Yang, and Wainwright]{wei2017early}
Wei, Y., Yang, F., and Wainwright, M.~J.
\newblock Early stopping for kernel boosting algorithms: A general analysis
  with localized complexities.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6065--6075, 2017.

\bibitem[Wong \& Kolter(2017)Wong and Kolter]{wong2017provable}
Wong, E. and Kolter, J.~Z.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock \emph{arXiv preprint arXiv:1711.00851}, 2017.

\bibitem[Wong et~al.(2018)Wong, Schmidt, Metzen, and Kolter]{wong2018scaling}
Wong, E., Schmidt, F., Metzen, J.~H., and Kolter, J.~Z.
\newblock Scaling provable adversarial defenses.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8400--8409, 2018.

\bibitem[Wong et~al.(2019)Wong, Schmidt, and Kolter]{wong2019wasserstein}
Wong, E., Schmidt, F.~R., and Kolter, J.~Z.
\newblock Wasserstein adversarial examples via projected sinkhorn iterations.
\newblock \emph{arXiv preprint arXiv:1902.07906}, 2019.

\bibitem[Wong et~al.(2020)Wong, Rice, and Kolter]{wong2020fast}
Wong, E., Rice, L., and Kolter, J.~Z.
\newblock Fast is better than free: Revisiting adversarial training.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=BJx040EFvH}.

\bibitem[Xiao et~al.(2018{\natexlab{a}})Xiao, Zhu, Li, He, Liu, and
  Song]{xiao2018spatially}
Xiao, C., Zhu, J.-Y., Li, B., He, W., Liu, M., and Song, D.
\newblock Spatially transformed adversarial examples.
\newblock \emph{arXiv preprint arXiv:1801.02612}, 2018{\natexlab{a}}.

\bibitem[Xiao et~al.(2018{\natexlab{b}})Xiao, Tjeng, Shafiullah, and
  Madry]{xiao2018training}
Xiao, K.~Y., Tjeng, V., Shafiullah, N.~M., and Madry, A.
\newblock Training for faster adversarial robustness verification via inducing
  relu stability.
\newblock \emph{arXiv preprint arXiv:1809.03008}, 2018{\natexlab{b}}.

\bibitem[Xie et~al.(2019)Xie, Wu, Maaten, Yuille, and He]{xie2019feature}
Xie, C., Wu, Y., Maaten, L. v.~d., Yuille, A.~L., and He, K.
\newblock Feature denoising for improving adversarial robustness.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  501--509, 2019.

\bibitem[Yang et~al.(2019)Yang, Zhang, Katabi, and Xu]{yang2019me}
Yang, Y., Zhang, G., Katabi, D., and Xu, Z.
\newblock Me-net: Towards effective adversarial robustness with matrix
  estimation.
\newblock \emph{arXiv preprint arXiv:1905.11971}, 2019.

\bibitem[Yin et~al.(2018)Yin, Ramchandran, and Bartlett]{yin2018rademacher}
Yin, D., Ramchandran, K., and Bartlett, P.
\newblock Rademacher complexity for adversarially robust generalization.
\newblock \emph{arXiv preprint arXiv:1810.11914}, 2018.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zhai et~al.(2019)Zhai, Cai, He, Dan, He, Hopcroft, and
  Wang]{zhai2019adversarially}
Zhai, R., Cai, T., He, D., Dan, C., He, K., Hopcroft, J., and Wang, L.
\newblock Adversarially robust generalization just requires more unlabeled
  data.
\newblock \emph{arXiv preprint arXiv:1906.00555}, 2019.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Zhang, Lu, Zhu, and
  Dong]{zhang2019you}
Zhang, D., Zhang, T., Lu, Y., Zhu, Z., and Dong, B.
\newblock You only propagate once: Painless adversarial training using maximal
  principle.
\newblock \emph{arXiv preprint arXiv:1905.00877}, 2019{\natexlab{a}}.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Chen, Xiao, Li, Boning, and
  Hsieh]{zhang2019towards}
Zhang, H., Chen, H., Xiao, C., Li, B., Boning, D., and Hsieh, C.-J.
\newblock Towards stable and efficient training of verifiably robust neural
  networks.
\newblock \emph{arXiv preprint arXiv:1906.06316}, 2019{\natexlab{b}}.

\bibitem[Zhang et~al.(2019{\natexlab{c}})Zhang, Yu, Jiao, Xing, Ghaoui, and
  Jordan]{zhang2019theoretically}
Zhang, H., Yu, Y., Jiao, J., Xing, E.~P., Ghaoui, L.~E., and Jordan, M.~I.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock \emph{arXiv preprint arXiv:1901.08573}, 2019{\natexlab{c}}.

\end{thebibliography}
