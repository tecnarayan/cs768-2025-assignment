\begin{thebibliography}{91}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and
  Le]{sutskever2014sequence}
Ilya Sutskever, Oriol Vinyals, and Quoc~V Le.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  3104--3112, 2014.

\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and Bengio]{bahdanau2015neural}
Dzmitry Bahdanau, Kyung~Hyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock In \emph{3rd International Conference on Learning Representations,
  ICLR 2015}, 2015.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Lan et~al.(2019)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{lan2019albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock {ALBERT}: A lite {BERT} for self-supervised learning of language
  representations.
\newblock \emph{arXiv preprint arXiv:1909.11942}, 2019.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv e-prints}, pages arXiv--1907, 2019.

\bibitem[Joshi et~al.(2020)Joshi, Chen, Liu, Weld, Zettlemoyer, and
  Levy]{joshi2020spanbert}
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel~S Weld, Luke Zettlemoyer, and Omer
  Levy.
\newblock Spanbert: Improving pre-training by representing and predicting
  spans.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:\penalty0 64--77, 2020.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le]{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,
  and Quoc~V Le.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1906.08237}, 2019.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Chen et~al.(2020)Chen, Radford, Child, Wu, Jun, Luan, and
  Sutskever]{chen2020generative}
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and
  Ilya Sutskever.
\newblock Generative pretraining from pixels.
\newblock In \emph{International Conference on Machine Learning}, pages
  1691--1703. PMLR, 2020.

\bibitem[Chen et~al.(2019)Chen, Li, Yu, El~Kholy, Ahmed, Gan, Cheng, and
  Liu]{chen2019uniter}
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El~Kholy, Faisal Ahmed, Zhe Gan,
  Yu~Cheng, and Jingjing Liu.
\newblock Uniter: Learning universal image-text representations.
\newblock 2019.

\bibitem[Lu et~al.(2019)Lu, Batra, Parikh, and Lee]{lu2019vilbert}
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
\newblock Vilbert: Pretraining task-agnostic visiolinguistic representations
  for vision-and-language tasks.
\newblock \emph{arXiv preprint arXiv:1908.02265}, 2019.

\bibitem[Xu et~al.(2015)Xu, Ba, Kiros, Cho, Courville, Salakhudinov, Zemel, and
  Bengio]{xu2015show}
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
  Salakhudinov, Rich Zemel, and Yoshua Bengio.
\newblock Show, attend and tell: Neural image caption generation with visual
  attention.
\newblock In \emph{International conference on machine learning}, pages
  2048--2057, 2015.

\bibitem[Rennie et~al.(2017)Rennie, Marcheret, Mroueh, Ross, and
  Goel]{rennie2017self}
Steven~J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava
  Goel.
\newblock Self-critical sequence training for image captioning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 7008--7024, 2017.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[Li et~al.(2018)Li, Tu, Yang, Lyu, and Zhang]{li2018multi}
Jian Li, Zhaopeng Tu, Baosong Yang, Michael~R Lyu, and Tong Zhang.
\newblock Multi-head attention with disagreement regularization.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 2897--2903, 2018.

\bibitem[Berthelot et~al.(2019)Berthelot, Carlini, Goodfellow, Papernot,
  Oliver, and Raffel]{berthelot2019mixmatch}
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital
  Oliver, and Colin Raffel.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1905.02249}, 2019.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Gong, and
  Choi]{zhang2021knowing}
Shujian Zhang, Chengyue Gong, and Eunsol Choi.
\newblock Knowing more about questions can help: Improving calibration in
  question answering.
\newblock \emph{arXiv preprint arXiv:2106.01494}, 2021{\natexlab{a}}.

\bibitem[Duan et~al.(2021{\natexlab{a}})Duan, Xu, Chen, Wang, Wang, and
  Zhou]{duan2021topicnet}
Zhibin Duan, Yishi Xu, Bo~Chen, Dongsheng Wang, Chaojie Wang, and Mingyuan
  Zhou.
\newblock Topicnet: Semantic graph-guided topic discovery.
\newblock In \emph{NeurIPS 2021: Neural Information Processing Systems}, Dec.
  2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Gong, and
  Choi]{zhang2021learning}
Shujian Zhang, Chengyue Gong, and Eunsol Choi.
\newblock Learning with different amounts of annotation: From zero to many
  labels.
\newblock \emph{arXiv preprint arXiv:2109.04408}, 2021{\natexlab{b}}.

\bibitem[Van~der Maaten and Hinton(2008)]{van2008visualizing}
Laurens Van~der Maaten and Geoffrey Hinton.
\newblock Visualizing data using t-sne.
\newblock \emph{Journal of machine learning research}, 9\penalty0 (11), 2008.

\bibitem[Duan et~al.(2021{\natexlab{b}})Duan, Wang, Chen, Wang, Chen, Li, Ren,
  and Zhou]{duan2021sawtooth}
Zhibin Duan, Dongsheng Wang, Bo~Chen, Chaojie Wang, Wenchao Chen, Yewen Li, Jie
  Ren, and Mingyuan Zhou.
\newblock Sawtooth factorial topic embeddings guided gamma belief network.
\newblock In \emph{International Conference on Machine Learning}, pages
  2903--2913. PMLR, 2021{\natexlab{b}}.

\bibitem[Goodfellow et~al.(2014{\natexlab{a}})Goodfellow, Pouget-Abadie, Mirza,
  Xu, Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in neural information processing systems}, pages
  2672--2680, 2014{\natexlab{a}}.

\bibitem[Lin(1991)]{lin1991divergence}
Jianhua Lin.
\newblock Divergence measures based on the shannon entropy.
\newblock \emph{IEEE Transactions on Information theory}, 37\penalty0
  (1):\penalty0 145--151, 1991.

\bibitem[Villani(2008)]{villani2008optimal}
C{\'e}dric Villani.
\newblock \emph{Optimal Transport: Old and New}, volume 338.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Peyr{\'e} and Cuturi(2019)]{peyre2019computational}
Gabriel Peyr{\'e} and Marco Cuturi.
\newblock Computational optimal transport: With applications to data science.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  11\penalty0 (5-6):\penalty0 355--607, 2019.

\bibitem[Zheng and Zhou(2021)]{zheng2021act}
Huangjie Zheng and Mingyuan Zhou.
\newblock Comparing probability distributions with conditional transport.
\newblock \emph{arXiv preprint arXiv:2012.14100}, 2021.

\bibitem[Ganin et~al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle,
  Laviolette, Marchand, and Lempitsky]{ganin2016domain}
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
  Larochelle, Fran{\c{c}}ois Laviolette, Mario Marchand, and Victor Lempitsky.
\newblock Domain-adversarial training of neural networks.
\newblock \emph{The journal of machine learning research}, 17\penalty0
  (1):\penalty0 2096--2030, 2016.

\bibitem[Tzeng et~al.(2017)Tzeng, Hoffman, Saenko, and
  Darrell]{tzeng2017adversarial}
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.
\newblock Adversarial discriminative domain adaptation.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7167--7176, 2017.

\bibitem[Srivastava et~al.(2015)Srivastava, Greff, and
  Schmidhuber]{srivastava2015training}
Rupesh~Kumar Srivastava, Klaus Greff, and J{\"u}rgen Schmidhuber.
\newblock Training very deep networks.
\newblock In \emph{NIPS}, 2015.

\bibitem[Ganin and Lempitsky(2015)]{ganin2015unsupervised}
Yaroslav Ganin and Victor Lempitsky.
\newblock Unsupervised domain adaptation by backpropagation.
\newblock In \emph{International conference on machine learning}, pages
  1180--1189. PMLR, 2015.

\bibitem[Balaji et~al.(2020)Balaji, Chellappa, and Feizi]{balaji2020robust}
Yogesh Balaji, Rama Chellappa, and Soheil Feizi.
\newblock Robust optimal transport with applications in generative modeling and
  domain adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Cuturi(2013)]{cuturi2013sinkhorn}
Marco Cuturi.
\newblock Sinkhorn distances: Lightspeed computation of optimal transport.
\newblock \emph{Advances in neural information processing systems},
  26:\penalty0 2292--2300, 2013.

\bibitem[Feydy et~al.(2019)Feydy, S{\'e}journ{\'e}, Vialard, Amari, Trouv{\'e},
  and Peyr{\'e}]{feydy2019interpolating}
Jean Feydy, Thibault S{\'e}journ{\'e}, Fran{\c{c}}ois-Xavier Vialard, Shun-ichi
  Amari, Alain Trouv{\'e}, and Gabriel Peyr{\'e}.
\newblock Interpolating between optimal transport and mmd using sinkhorn
  divergences.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 2681--2690. PMLR, 2019.

\bibitem[Liang et~al.(2006)Liang, Taskar, and Klein]{liang2006alignment}
Percy Liang, Ben Taskar, and Dan Klein.
\newblock Alignment by agreement.
\newblock In \emph{Proceedings of the Human Language Technology Conference of
  the NAACL, Main Conference}, pages 104--111, 2006.

\bibitem[Levinboim et~al.(2015)Levinboim, Vaswani, and
  Chiang]{levinboim2015model}
Tomer Levinboim, Ashish Vaswani, and David Chiang.
\newblock Model invertibility regularization: Sequence alignment with or
  without parallel data.
\newblock In \emph{Proceedings of the 2015 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 609--618, 2015.

\bibitem[Alkhouli et~al.(2018)Alkhouli, Bretschner, and
  Ney]{alkhouli2018alignment}
Tamer Alkhouli, Gabriel Bretschner, and Hermann Ney.
\newblock On the alignment problem in multi-head attention-based neural machine
  translation.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation:
  Research Papers}, pages 177--185, 2018.

\bibitem[Murphy(2012)]{murphy2012machine}
Kevin~P Murphy.
\newblock \emph{Machine learning: a probabilistic perspective}.
\newblock MIT press, 2012.

\bibitem[Kullback and Leibler(1951)]{kullback1951information}
Solomon Kullback and Richard~A Leibler.
\newblock On information and sufficiency.
\newblock \emph{The annals of mathematical statistics}, 22\penalty0
  (1):\penalty0 79--86, 1951.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{arjovsky2017wasserstein}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock Wasserstein gan.
\newblock \emph{arXiv preprint arXiv:1701.07875}, 2017.

\bibitem[Bellemare et~al.(2017)Bellemare, Danihelka, Dabney, Mohamed,
  Lakshminarayanan, Hoyer, and Munos]{bellemare2017cramer}
Marc~G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji
  Lakshminarayanan, Stephan Hoyer, and R{\'e}mi Munos.
\newblock The cramer distance as a solution to biased wasserstein gradients.
\newblock \emph{arXiv preprint arXiv:1705.10743}, 2017.

\bibitem[Strubell et~al.(2019)Strubell, Ganesh, and
  McCallum]{strubell2019energy}
Emma Strubell, Ananya Ganesh, and Andrew McCallum.
\newblock Energy and policy considerations for deep learning in nlp.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 3645--3650, 2019.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R
  Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock \emph{arXiv preprint arXiv:1804.07461}, 2018.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang]{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock {SQuAD}: 100,000+ questions for machine comprehension of text.
\newblock \emph{arXiv preprint arXiv:1606.05250}, 2016.

\bibitem[Rajpurkar et~al.(2018)Rajpurkar, Jia, and Liang]{rajpurkar2018know}
Pranav Rajpurkar, Robin Jia, and Percy Liang.
\newblock Know what you don't know: Unanswerable questions for {SQuAD}.
\newblock \emph{arXiv preprint arXiv:1806.03822}, 2018.

\bibitem[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, et~al.]{wolf2019transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz,
  et~al.
\newblock Transformers: State-of-the-art natural language processing.
\newblock \emph{arXiv preprint arXiv:1910.03771}, 2019.

\bibitem[Chen et~al.(2018)Chen, Sun, Athiwaratkun, Cardie, and
  Weinberger]{chen2018adversarial}
Xilun Chen, Yu~Sun, Ben Athiwaratkun, Claire Cardie, and Kilian Weinberger.
\newblock Adversarial deep averaging networks for cross-lingual sentiment
  classification.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  6:\penalty0 557--570, 2018.

\bibitem[Peng et~al.(2018)Peng, Zhang, Jiang, and Huang]{peng2018cross}
Minlong Peng, Qi~Zhang, Yu-gang Jiang, and Xuan-Jing Huang.
\newblock Cross-domain sentiment classification with target domain specific
  information.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 2505--2513,
  2018.

\bibitem[Miller(2019)]{miller2019simplified}
Timothy Miller.
\newblock Simplified neural unsupervised domain adaptation.
\newblock In \emph{Proceedings of the conference. Association for Computational
  Linguistics. North American Chapter. Meeting}, volume 2019, page 414. NIH
  Public Access, 2019.

\bibitem[Desai and Durrett(2020)]{desai2020calibration}
Shrey Desai and Greg Durrett.
\newblock Calibration of pre-trained transformers.
\newblock \emph{arXiv preprint arXiv:2003.07892}, 2020.

\bibitem[Bowman et~al.(2015)Bowman, Angeli, Potts, and
  Manning]{bowman2015large}
Samuel~R Bowman, Gabor Angeli, Christopher Potts, and Christopher~D Manning.
\newblock A large annotated corpus for learning natural language inference.
\newblock In \emph{EMNLP}, 2015.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{williams2018broad}
Adina Williams, Nikita Nangia, and Samuel Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pages 1112--1122, 2018.

\bibitem[Iyer et~al.(2017)Iyer, Dandekar, and Csernai]{iyer2017first}
Shankar Iyer, Nikhil Dandekar, and Korn{\'e}l Csernai.
\newblock First quora dataset release: Question pairs.
\newblock \emph{data. quora. com}, 2017.

\bibitem[Lan et~al.(2017)Lan, Qiu, He, and Xu]{lan2017continuously}
Wuwei Lan, Siyu Qiu, Hua He, and Wei Xu.
\newblock A continuously growing dataset of sentential paraphrases.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing}, pages 1224--1234, 2017.

\bibitem[Zellers et~al.(2018)Zellers, Bisk, Schwartz, and
  Choi]{zellers2018swag}
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi.
\newblock Swag: A large-scale adversarial dataset for grounded commonsense
  inference.
\newblock In \emph{EMNLP}, 2018.

\bibitem[Parikh et~al.(2016)Parikh, T{\"a}ckstr{\"o}m, Das, and
  Uszkoreit]{parikh2016decomposable}
Ankur~P Parikh, Oscar T{\"a}ckstr{\"o}m, Dipanjan Das, and Jakob Uszkoreit.
\newblock A decomposable attention model for natural language inference.
\newblock In \emph{EMNLP}, 2016.

\bibitem[Chen et~al.(2017)Chen, Zhu, Ling, Wei, Jiang, and
  Inkpen]{chen2017enhanced}
Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si~Wei, Hui Jiang, and Diana Inkpen.
\newblock Enhanced lstm for natural language inference.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 1657--1668,
  2017.

\bibitem[Goodfellow et~al.(2014{\natexlab{b}})Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014{\natexlab{b}}.

\bibitem[Morris et~al.(2020)Morris, Lifland, Yoo, Grigsby, Jin, and
  Qi]{morris2020textattack}
John Morris, Eli Lifland, Jin~Yong Yoo, Jake Grigsby, Di~Jin, and Yanjun Qi.
\newblock Textattack: A framework for adversarial attacks, data augmentation,
  and adversarial training in nlp.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 119--126, 2020.

\bibitem[Jin et~al.(2020)Jin, Jin, Zhou, and Szolovits]{jin2020bert}
Di~Jin, Zhijing Jin, Joey~Tianyi Zhou, and Peter Szolovits.
\newblock Is bert really robust? a strong baseline for natural language attack
  on text classification and entailment.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 8018--8025, 2020.

\bibitem[Li et~al.(2019)Li, Ji, Du, Li, and Wang]{li2019textbugger}
J~Li, S~Ji, T~Du, B~Li, and T~Wang.
\newblock Textbugger: Generating adversarial text against real-world
  applications.
\newblock In \emph{26th Annual Network and Distributed System Security
  Symposium}, 2019.

\bibitem[Garg and Ramakrishnan(2020)]{garg2020bae}
Siddhant Garg and Goutham Ramakrishnan.
\newblock Bae: Bert-based adversarial examples for text classification.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 6174--6181, 2020.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2017)Veli{\v{c}}kovi{\'c}, Cucurull,
  Casanova, Romero, Lio, and Bengio]{velivckovic2017graph}
Petar Veli{\v{c}}kovi{\'c}, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
  Pietro Lio, and Yoshua Bengio.
\newblock Graph attention networks.
\newblock \emph{arXiv preprint arXiv:1710.10903}, 2017.

\bibitem[Sen et~al.(2008)Sen, Namata, Bilgic, Getoor, Galligher, and
  Eliassi-Rad]{sen2008collective}
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher,
  and Tina Eliassi-Rad.
\newblock Collective classification in network data.
\newblock \emph{AI magazine}, 29\penalty0 (3):\penalty0 93--93, 2008.

\bibitem[Yang et~al.(2016)Yang, Cohen, and Salakhutdinov]{yang2016revisiting}
Zhilin Yang, William~W Cohen, and Ruslan Salakhutdinov.
\newblock Revisiting semi-supervised learning with graph embeddings.
\newblock \emph{arXiv preprint arXiv:1603.08861}, 2016.

\bibitem[Goyal et~al.(2017)Goyal, Khot, Summers-Stay, Batra, and
  Parikh]{goyal2017making}
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
\newblock Making the {V} in {VQA} matter: Elevating the role of image
  understanding in visual question answering.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 6904--6913, 2017.

\bibitem[Yu et~al.(2019)Yu, Yu, Cui, Tao, and Tian]{yu2019deep}
Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi~Tian.
\newblock Deep modular co-attention networks for visual question answering.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 6281--6290, 2019.

\bibitem[Larochelle et~al.(2007)Larochelle, Erhan, Courville, Bergstra, and
  Bengio]{larochelle2007empirical}
Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua
  Bengio.
\newblock An empirical evaluation of deep architectures on problems with many
  factors of variation.
\newblock In \emph{Proceedings of the 24th international conference on Machine
  learning}, pages 473--480, 2007.

\bibitem[Fan et~al.(2020)Fan, Zhang, Chen, and Zhou]{fan2020bayesian}
Xinjie Fan, Shujian Zhang, Bo~Chen, and Mingyuan Zhou.
\newblock Bayesian attention modules.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Zhang et~al.(2021{\natexlab{c}})Zhang, Fan, Chen, and
  Zhou]{zhang2021bayesian}
Shujian Zhang, Xinjie Fan, Bo~Chen, and Mingyuan Zhou.
\newblock Bayesian attention belief networks.
\newblock \emph{arXiv preprint arXiv:2106.05251}, 2021{\natexlab{c}}.

\bibitem[Mukhoti and Gal(2018)]{mukhoti2018evaluating}
Jishnu Mukhoti and Yarin Gal.
\newblock Evaluating {B}ayesian deep learning methods for semantic
  segmentation.
\newblock \emph{arXiv preprint arXiv:1811.12709}, 2018.

\bibitem[Kim and Canny(2017)]{kim2017interpretable}
Jinkyu Kim and John Canny.
\newblock Interpretable learning for self-driving cars by visualizing causal
  attention.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 2942--2950, 2017.

\bibitem[Choi et~al.(2016)Choi, Bahadori, Sun, Kulas, Schuetz, and
  Stewart]{choi2016retain}
Edward Choi, Mohammad~Taha Bahadori, Jimeng Sun, Joshua Kulas, Andy Schuetz,
  and Walter Stewart.
\newblock Retain: An interpretable predictive model for healthcare using
  reverse time attention mechanism.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3504--3512, 2016.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin,
  Dillon, Lakshminarayanan, and Snoek]{ovadia2019can}
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D~Sculley, Sebastian
  Nowozin, Joshua~V Dillon, Balaji Lakshminarayanan, and Jasper Snoek.
\newblock Can you trust your model's uncertainty? evaluating predictive
  uncertainty under dataset shift.
\newblock \emph{arXiv preprint arXiv:1906.02530}, 2019.

\bibitem[Dolan and Brockett(2005)]{dolan2005automatically}
William~B Dolan and Chris Brockett.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \emph{Proceedings of the Third International Workshop on
  Paraphrasing (IWP2005)}, 2005.

\bibitem[Warstadt et~al.(2019)Warstadt, Singh, and Bowman]{warstadt2019neural}
Alex Warstadt, Amanpreet Singh, and Samuel~R Bowman.
\newblock Neural network acceptability judgments.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  7:\penalty0 625--641, 2019.

\bibitem[Dagan et~al.(2005)Dagan, Glickman, and Magnini]{dagan2005pascal}
Ido Dagan, Oren Glickman, and Bernardo Magnini.
\newblock The pascal recognising textual entailment challenge.
\newblock In \emph{Machine Learning Challenges Workshop}, pages 177--190.
  Springer, 2005.

\bibitem[Williams et~al.(2017)Williams, Nangia, and Bowman]{williams2017broad}
Adina Williams, Nikita Nangia, and Samuel~R Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock \emph{arXiv preprint arXiv:1704.05426}, 2017.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{socher2013recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D Manning,
  Andrew~Y Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pages 1631--1642, 2013.

\bibitem[Cer et~al.(2017)Cer, Diab, Agirre, Lopez-Gazpio, and
  Specia]{cer2017semeval}
Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia.
\newblock Semeval-2017 task 1: Semantic textual similarity-multilingual and
  cross-lingual focused evaluation.
\newblock \emph{arXiv preprint arXiv:1708.00055}, 2017.

\bibitem[Gardner et~al.(2017)Gardner, Grus, Neumann, Tafjord, Dasigi, Liu,
  Peters, Schmitz, and Zettlemoyer]{gardner2017deep}
Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, NH~Liu,
  Matthew Peters, Michael Schmitz, and Luke~S Zettlemoyer.
\newblock A deep semantic natural language processing platform.
\newblock \emph{arXiv preprint arXiv:1803.07640}, 2017.

\bibitem[Loshchilov and Hutter(2018)]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Glorot and Bengio(2010)]{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256, 2010.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Clevert et~al.(2015)Clevert, Unterthiner, and
  Hochreiter]{clevert2015fast}
Djork-Arn{\'e} Clevert, Thomas Unterthiner, and Sepp Hochreiter.
\newblock Fast and accurate deep network learning by exponential linear units
  (elus).
\newblock \emph{arXiv preprint arXiv:1511.07289}, 2015.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Fan et~al.(2021)Fan, Zhang, Tanwisuth, Qian, and
  Zhou]{fan2021contextual}
Xinjie Fan, Shujian Zhang, Korawat Tanwisuth, Xiaoning Qian, and Mingyuan Zhou.
\newblock Contextual dropout: An efficient sample-dependent dropout module.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
  Doll{\'a}r, and Zitnick]{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft {COCO}: Common objects in context.
\newblock In \emph{European conference on computer vision}, pages 740--755.
  Springer, 2014.

\bibitem[Deng et~al.(2018)Deng, Kim, Chiu, Guo, and Rush]{deng2018latent}
Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, and Alexander Rush.
\newblock Latent alignment and variational attention.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9712--9724, 2018.

\bibitem[Teney et~al.(2018)Teney, Anderson, He, and Van
  Den~Hengel]{teney2018tips}
Damien Teney, Peter Anderson, Xiaodong He, and Anton Van Den~Hengel.
\newblock Tips and tricks for visual question answering: Learnings from the
  2017 challenge.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4223--4232, 2018.

\end{thebibliography}
