\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong, Welinder, McGrew, Tobin, Pieter~Abbeel, and Zaremba]{andrychowicz2017hindsight}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Pieter~Abbeel, O., and Zaremba, W.
\newblock Hindsight experience replay.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 30, 2017.

\bibitem[Aradi(2020)]{aradi2020survey}
Aradi, S.
\newblock Survey of deep reinforcement learning for motion planning of autonomous vehicles.
\newblock \emph{IEEE Transactions on Intelligent Transportation Systems}, 23\penalty0 (2):\penalty0 740--759, 2020.

\bibitem[Bakker et~al.(2011)Bakker, Oerlemans, Demerouti, Slot, and Ali]{bakker2011flow}
Bakker, A.~B., Oerlemans, W., Demerouti, E., Slot, B.~B., and Ali, D.~K.
\newblock Flow and performance: A study among talented dutch soccer players.
\newblock \emph{Psychology of Sport and Exercise}, 12\penalty0 (4):\penalty0 442--450, 2011.

\bibitem[Bejani \& Ghatee(2021)Bejani and Ghatee]{bejani2021systematic}
Bejani, M.~M. and Ghatee, M.
\newblock A systematic review on overfitting control in shallow and deep neural networks.
\newblock \emph{Artificial Intelligence Review}, pp.\  1--48, 2021.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, Debiak, Dennison, Farhi, Fischer, Hashme, Hesse, et~al.]{berner2019dota}
Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.06680}, 2019.

\bibitem[Brown(1951)]{brown1951iterative}
Brown, G.~W.
\newblock Iterative solution of games by fictitious play.
\newblock \emph{Act. Anal. Prod Allocation}, 13\penalty0 (1):\penalty0 374, 1951.

\bibitem[Brown \& Sandholm(2018)Brown and Sandholm]{brown2018superhuman}
Brown, N. and Sandholm, T.
\newblock Superhuman ai for heads-up no-limit poker: Libratus beats top professionals.
\newblock \emph{Science}, 359\penalty0 (6374):\penalty0 418--424, 2018.

\bibitem[Cheng et~al.(2016)Cheng, Hung, and Chen]{cheng2016influence}
Cheng, T.-M., Hung, S.-H., and Chen, M.-T.
\newblock The influence of leisure involvement on flow experience during hiking activity: Using psychological commitment as a mediate variable.
\newblock \emph{Asia Pacific Journal of Tourism Research}, 21\penalty0 (1):\penalty0 1--19, 2016.

\bibitem[Cobbe et~al.(2019)Cobbe, Klimov, Hesse, Kim, and Schulman]{cobbe2019quantifying}
Cobbe, K., Klimov, O., Hesse, C., Kim, T., and Schulman, J.
\newblock Quantifying generalization in reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\  1282--1289. PMLR, 2019.

\bibitem[Csikszentmihalyi(2000)]{csikszentmihalyi2000beyond}
Csikszentmihalyi, M.
\newblock \emph{Beyond boredom and anxiety.}
\newblock Jossey-bass, 2000.

\bibitem[Dosovitskiy \& Koltun(2016)Dosovitskiy and Koltun]{dosovitskiy2016learning}
Dosovitskiy, A. and Koltun, V.
\newblock Learning to act by predicting the future.
\newblock \emph{arXiv preprint arXiv:1611.01779}, 2016.

\bibitem[Elo(1978)]{elo1978rating}
Elo, A.
\newblock The rating of chess players, past and present (arco, new york).
\newblock 1978.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International Conference On Machine Learning (ICML)}, pp.\  1126--1135. PMLR, 2017.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{sac}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\  1861--1870, 2018.

\bibitem[Hafner et~al.(2019)Hafner, Lillicrap, Fischer, Villegas, Ha, Lee, and Davidson]{hafner2019learning}
Hafner, D., Lillicrap, T.~P., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J.
\newblock Learning latent dynamics for planning from pixels.
\newblock In \emph{Proceedings of the 36th International Conference on Machine Learning}, volume~97, pp.\  2467--2475, 2019.

\bibitem[Halina \& Guzdial(2022)Halina and Guzdial]{halina2022diversity}
Halina, E. and Guzdial, M.
\newblock Diversity-based deep reinforcement learning towards multidimensional difficulty for fighting game ai.
\newblock \emph{arXiv preprint arXiv:2211.02759}, 2022.

\bibitem[He et~al.(2023)He, Su, Zhang, and Hou]{DBLP:conf/cvpr/HeSZH23}
He, Q., Su, H., Zhang, J., and Hou, X.
\newblock Frustratingly easy regularization on representation can boost deep reinforcement learning.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern Recognition, {CVPR} 2023, Vancouver, BC, Canada, June 17-24, 2023}, pp.\  20215--20225. {IEEE}, 2023.
\newblock \doi{10.1109/CVPR52729.2023.01936}.
\newblock URL \url{https://doi.org/10.1109/CVPR52729.2023.01936}.

\bibitem[He et~al.(2024)He, Zhou, Fang, and Maghsudi]{DBLP:journals/corr/abs-2404-12754}
He, Q., Zhou, T., Fang, M., and Maghsudi, S.
\newblock Adaptive regularization of representation rank as an implicit constraint of bellman equation.
\newblock \emph{CoRR}, abs/2404.12754, 2024.
\newblock \doi{10.48550/ARXIV.2404.12754}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2404.12754}.

\bibitem[Heinrich \& Silver(2016)Heinrich and Silver]{heinrich2016deep}
Heinrich, J. and Silver, D.
\newblock Deep reinforcement learning from self-play in imperfect-information games, 2016.

\bibitem[Heinrich et~al.(2015)Heinrich, Lanctot, and Silver]{Heinrich2015FictitiousSI}
Heinrich, J., Lanctot, M., and Silver, D.
\newblock Fictitious self-play in extensive-form games.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2015.

\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Isola, Cruz, and Abbeel]{houthooft2016vime}
Houthooft, R., Chen, X., Isola, P., Cruz, E., and Abbeel, P.
\newblock Vime: Variational information maximizing exploration.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\  1109--1117, 2016.

\bibitem[Ishii et~al.(2018)Ishii, Ito, Ishihara, Harada, and Thawonmas]{ishii2018monte}
Ishii, R., Ito, S., Ishihara, M., Harada, T., and Thawonmas, R.
\newblock Monte-carlo tree search implementation of fighting game ais having personas.
\newblock In \emph{2018 IEEE Conference on Computational Intelligence and Games (CIG)}, pp.\  1--8. IEEE, 2018.

\bibitem[Jaderberg et~al.(2019)Jaderberg, Czarnecki, Dunning, Marris, Lever, Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, et~al.]{jaderberg2019human}
Jaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castaneda, A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., et~al.
\newblock Human-level performance in 3d multiplayer games with population-based reinforcement learning.
\newblock \emph{Science}, 364\penalty0 (6443):\penalty0 859--865, 2019.

\bibitem[Khan et~al.(2022)Khan, Van~Nguyen, Dai, and Thawonmas]{khan2022darefightingice}
Khan, I., Van~Nguyen, T., Dai, X., and Thawonmas, R.
\newblock Darefightingice competition: A fighting game sound design and ai competition.
\newblock In \emph{2022 IEEE Conference on Games (CoG)}, pp.\  478--485. IEEE, 2022.

\bibitem[Kim et~al.(2020)Kim, Park, and Yang]{kim2020mastering}
Kim, D.-W., Park, S., and Yang, S.-i.
\newblock Mastering fighting game using deep reinforcement learning with self-play.
\newblock In \emph{2020 IEEE Conference on Games (CoG)}, pp.\  576--583. IEEE, 2020.

\bibitem[Kim \& Ahn(2018)Kim and Ahn]{kim2018hybrid}
Kim, M.-J. and Ahn, C.~W.
\newblock Hybrid fighting game ai using a genetic algorithm and monte carlo tree search.
\newblock In \emph{Proceedings of the Genetic And Evolutionary Computation Conference Companion}, pp.\  129--130, 2018.

\bibitem[Lan et~al.(2024)Lan, Zhang, Yi, Guo, Peng, Gao, Wu, Chen, Du, Hu, et~al.]{lan2024contrastive}
Lan, S., Zhang, R., Yi, Q., Guo, J., Peng, S., Gao, Y., Wu, F., Chen, R., Du, Z., Hu, X., et~al.
\newblock Contrastive modules with temporal attention for multi-task reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Lanctot et~al.(2017)Lanctot, Zambaldi, Gruslys, Lazaridou, Tuyls, P{\'e}rolat, Silver, and Graepel]{lanctot2017unified}
Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A., Tuyls, K., P{\'e}rolat, J., Silver, D., and Graepel, T.
\newblock A unified game-theoretic approach to multiagent reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 30, 2017.

\bibitem[Leslie \& Collins(2006)Leslie and Collins]{leslie2006generalised}
Leslie, D.~S. and Collins, E.~J.
\newblock Generalised weakened fictitious play.
\newblock \emph{Games and Economic Behavior}, 56\penalty0 (2):\penalty0 285--298, 2006.

\bibitem[Li et~al.(2023)Li, Lyu, Ma, Wang, Yang, Li, and Li]{li2023normalization}
Li, L., Lyu, J., Ma, G., Wang, Z., Yang, Z., Li, X., and Li, Z.
\newblock Normalization enhances generalization in visual reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2306.00656}, 2023.

\bibitem[Lyu et~al.(2024{\natexlab{a}})Lyu, Bai, Yang, Lu, and Li]{lyu2024cross}
Lyu, J., Bai, C., Yang, J., Lu, Z., and Li, X.
\newblock Cross-domain policy adaptation by capturing representation mismatch.
\newblock \emph{arXiv preprint arXiv:2405.15369}, 2024{\natexlab{a}}.

\bibitem[Lyu et~al.(2024{\natexlab{b}})Lyu, Wan, Li, and Lu]{lyu2024towards}
Lyu, J., Wan, L., Li, X., and Lu, Z.
\newblock Towards understanding how to reduce generalization gap in visual reinforcement learning.
\newblock In \emph{Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems}, pp.\  2369--2371, 2024{\natexlab{b}}.

\bibitem[Lyu et~al.(2024{\natexlab{c}})Lyu, Wan, Li, and Lu]{lyu2024understanding}
Lyu, J., Wan, L., Li, X., and Lu, Z.
\newblock Understanding what affects generalization gap in visual reinforcement learning: Theory and empirical evidence.
\newblock \emph{arXiv preprint arXiv:2402.02701}, 2024{\natexlab{c}}.

\bibitem[McMahan et~al.(2003)McMahan, Gordon, and Blum]{mcmahan2003planning}
McMahan, H.~B., Gordon, G.~J., and Blum, A.
\newblock Planning in the presence of cost functions controlled by an adversary.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\  536--543, 2003.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare, M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Morav{\v{c}}{\'\i}k et~al.(2017)Morav{\v{c}}{\'\i}k, Schmid, Burch, Lis{\`y}, Morrill, Bard, Davis, Waugh, Johanson, and Bowling]{moravvcik2017deepstack}
Morav{\v{c}}{\'\i}k, M., Schmid, M., Burch, N., Lis{\`y}, V., Morrill, D., Bard, N., Davis, T., Waugh, K., Johanson, M., and Bowling, M.
\newblock Deepstack: Expert-level artificial intelligence in heads-up no-limit poker.
\newblock \emph{Science}, 356\penalty0 (6337):\penalty0 508--513, 2017.

\bibitem[Oh et~al.(2021)Oh, Rho, Moon, Son, Lee, and Chung]{oh2021creating}
Oh, I., Rho, S., Moon, S., Son, S., Lee, H., and Chung, J.
\newblock Creating pro-level ai for a real-time fighting game using deep reinforcement learning.
\newblock \emph{IEEE Transactions on Games}, 14\penalty0 (2):\penalty0 212--220, 2021.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{instructgpt}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 35:\penalty0 27730--27744, 2022.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and Darrell]{pathak2017curiosity}
Pathak, D., Agrawal, P., Efros, A.~A., and Darrell, T.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  16--17, 2017.

\bibitem[Peng et~al.(2023{\natexlab{a}})Peng, Hu, Yi, Zhang, Guo, Huang, Tian, Chen, Du, Guo, Chen, and Li]{Peng2023SelfdrivenGL}
Peng, S., Hu, X., Yi, Q., Zhang, R., Guo, J., Huang, D., Tian, Z., Chen, R., Du, Z., Guo, Q., Chen, Y., and Li, L.
\newblock Self-driven grounding: Large language model agents with automatical language-aligned skill learning.
\newblock \emph{ArXiv}, abs/2309.01352, 2023{\natexlab{a}}.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:261530737}.

\bibitem[Peng et~al.(2023{\natexlab{b}})Peng, Hu, Zhang, Guo, Yi, Chen, Du, Li, Guo, and Chen]{Peng2023ConceptualRL}
Peng, S., Hu, X., Zhang, R., Guo, J., Yi, Q., Chen, R., Du, Z., Li, L., Guo, Q., and Chen, Y.
\newblock Conceptual reinforcement learning for language-conditioned tasks.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2023{\natexlab{b}}.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:257427058}.

\bibitem[Schaul et~al.(2015)Schaul, Horgan, Gregor, and Silver]{schaul2015universal}
Schaul, T., Horgan, D., Gregor, K., and Silver, D.
\newblock Universal value function approximators.
\newblock In \emph{International Conference On Machine Learning (ICML)}, pp.\  1312--1320. PMLR, 2015.

\bibitem[Schulman et~al.(2015)Schulman, Moritz, Levine, Jordan, and Abbeel]{schulman2015high}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage estimation.
\newblock \emph{arXiv preprint arXiv:1506.02438}, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms, 2017.

\bibitem[Shafer(2012)]{shafer2012causes}
Shafer, D.~M.
\newblock Causes of state hostility and enjoyment in player versus player and player versus environment video games.
\newblock \emph{Journal of Communication}, 62\penalty0 (4):\penalty0 719--737, 2012.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang, Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Silver et~al.(2018)Silver, Hubert, Schrittwieser, Antonoglou, Lai, Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2018general}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi, and go through self-play.
\newblock \emph{Science}, 362\penalty0 (6419):\penalty0 1140--1144, 2018.

\bibitem[Sun et~al.(2017)Sun, Hoffman, Saenko, and Darrell]{sun2017learning}
Sun, X., Hoffman, J., Saenko, K., and Darrell, T.
\newblock Learning to learn how to learn: Self-adaptive visual navigation using meta-learning.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer Vision}, pp.\  675--684, 2017.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik, Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung, J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wang et~al.(2023)Wang, Liang, Li, Li, Ghanem, Zimmermann, Yi, Zhang, Wang, et~al.]{wang2023brave}
Wang, K., Liang, Y., Li, X., Li, G., Ghanem, B., Zimmermann, R., Yi, H., Zhang, Y., Wang, Y., et~al.
\newblock Brave the wind and the waves: Discovering robust and generalizable graph lottery tickets.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 2023.

\bibitem[Wang(2024)]{wang2024balancing}
Wang, X.
\newblock Balancing the ai strength of roles in self-play training with regret matching+, 2024.

\bibitem[Ye et~al.(2020{\natexlab{a}})Ye, Chen, Zhang, Chen, Yuan, Liu, Chen, Liu, Qiu, Yu, et~al.]{ye2020towards}
Ye, D., Chen, G., Zhang, W., Chen, S., Yuan, B., Liu, B., Chen, J., Liu, Z., Qiu, F., Yu, H., et~al.
\newblock Towards playing full moba games with deep reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 33:\penalty0 621--632, 2020{\natexlab{a}}.

\bibitem[Ye et~al.(2020{\natexlab{b}})Ye, Liu, Sun, Shi, Zhao, Wu, Yu, Yang, Wu, Guo, et~al.]{ye2020mastering}
Ye, D., Liu, Z., Sun, M., Shi, B., Zhao, P., Wu, H., Yu, H., Yang, S., Wu, X., Guo, Q., et~al.
\newblock Mastering complex control in moba games with deep reinforcement learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence (AAAI)}, volume~34, pp.\  6672--6679, 2020{\natexlab{b}}.

\bibitem[Yi et~al.(2022)Yi, Zhang, Peng, Guo, Hu, Du, Zhang, Guo, and Chen]{DBLP:conf/nips/Yi0PG0Dz0C22}
Yi, Q., Zhang, R., Peng, S., Guo, J., Hu, X., Du, Z., Zhang, X., Guo, Q., and Chen, Y.
\newblock Object-category aware reinforcement learning.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022}, 2022.

\bibitem[Yi et~al.(2023)Yi, Zhang, Peng, Guo, Gao, Yuan, Chen, Lan, Hu, Du, Zhang, Guo, and Chen]{DBLP:conf/icml/Yi0PGGYCLHDZGC23}
Yi, Q., Zhang, R., Peng, S., Guo, J., Gao, Y., Yuan, K., Chen, R., Lan, S., Hu, X., Du, Z., Zhang, X., Guo, Q., and Chen, Y.
\newblock Online prototype alignment for few-shot policy transfer.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), \emph{International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  39968--39983. {PMLR}, 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/yi23b.html}.

\bibitem[Zha et~al.(2021)Zha, Xie, Ma, Zhang, Lian, Hu, and Liu]{zha2021douzero}
Zha, D., Xie, J., Ma, W., Zhang, S., Lian, X., Hu, X., and Liu, J.
\newblock Douzero: Mastering doudizhu with self-play deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\  12333--12344, 2021.

\bibitem[Zhao et~al.(2022)Zhao, Zhao, Hu, Zhou, and Li]{zhao2022douzero+}
Zhao, Y., Zhao, J., Hu, X., Zhou, W., and Li, H.
\newblock Douzero+: Improving doudizhu ai by opponent modeling and coach-guided learning.
\newblock In \emph{IEEE Conference on Games (CoG)}, pp.\  127--134, 2022.

\end{thebibliography}
