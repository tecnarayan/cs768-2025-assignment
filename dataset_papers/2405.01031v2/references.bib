@article{arjevani2023lower,
  title={Lower bounds for non-convex stochastic optimization},
  author={Arjevani, Yossi and Carmon, Yair and Duchi, John C and Foster, Dylan J and Srebro, Nathan and Woodworth, Blake},
  journal={Mathematical Programming},
  volume={199},
  number={1-2},
  pages={165--214},
  year={2023},
  publisher={Springer}
}

@article{chang2011libsvm,
  title={LIBSVM: a library for support vector machines},
  author={Chang, Chih-Chung and Lin, Chih-Jen},
  journal={ACM transactions on intelligent systems and technology (TIST)},
  volume={2},
  number={3},
  pages={1--27},
  year={2011},
  publisher={Acm New York, NY, USA}
}

@article{ying2021exponential,
  title={Exponential graph is provably efficient for decentralized deep training},
  author={Ying, Bicheng and Yuan, Kun and Chen, Yiming and Hu, Hanbin and Pan, Pan and Yin, Wotao},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={13975--13987},
  year={2021}
}

@article{vishnoi2012laplacian,
  title={Laplacian Solvers and Their Algorithmic Applications},
  author={Vishnoi, Nisheeth K},
  journal={Theoretical Computer Science},
  volume={8},
  number={1-2},
  pages={1--141},
  year={2012}
}

@article{duchi2018minimax,
  title={Minimax optimal procedures for locally private estimation},
  author={Duchi, John C and Jordan, Michael I and Wainwright, Martin J},
  journal={Journal of the American Statistical Association},
  volume={113},
  number={521},
  pages={182--201},
  year={2018},
  publisher={Taylor \& Francis}
}

@inproceedings{kairouz2021practical,
  title={Practical and private (deep) learning without sampling or shuffling},
  author={Kairouz, Peter and McMahan, Brendan and Song, Shuang and Thakkar, Om and Thakurta, Abhradeep and Xu, Zheng},
  booktitle={International Conference on Machine Learning},
  pages={5213--5225},
  year={2021},
  organization={PMLR}
}

@article{boyd2006randomized,
  title={Randomized gossip algorithms},
  author={Boyd, Stephen and Ghosh, Arpita and Prabhakar, Balaji and Shah, Devavrat},
  journal={IEEE transactions on information theory},
  volume={52},
  number={6},
  pages={2508--2530},
  year={2006},
  publisher={IEEE}
}

@inproceedings{allouah2023robust,
  title={Robust Distributed Learning: Tight Error Bounds and Breakdown Point under Data Heterogeneity},
  author={Allouah, Youssef and Guerraoui, Rachid and Gupta, Nirupam and Pinot, Rafael and Rizk, Geovani},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}

@inproceedings{vaswani2019fast,
  title={Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron},
  author={Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
  booktitle={The 22nd international conference on artificial intelligence and statistics},
  pages={1195--1204},
  year={2019},
  organization={PMLR}
}

@article{cyffers2022muffliato,
  title={Muffliato: Peer-to-peer privacy amplification for decentralized optimization and averaging},
  author={Cyffers, Edwige and Even, Mathieu and Bellet, Aur{\'e}lien and Massouli{\'e}, Laurent},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15889--15902},
  year={2022}
}

@inproceedings{bellet2018personalized,
  title={Personalized and private peer-to-peer machine learning},
  author={Bellet, Aur{\'e}lien and Guerraoui, Rachid and Taziki, Mahsa and Tommasi, Marc},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={473--481},
  year={2018},
  organization={PMLR}
}

@inproceedings{cheu2019distributed,
  title={Distributed differential privacy via shuffling},
  author={Cheu, Albert and Smith, Adam and Ullman, Jonathan and Zeber, David and Zhilyaev, Maxim},
  booktitle={Advances in Cryptology--EUROCRYPT 2019: 38th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Darmstadt, Germany, May 19--23, 2019, Proceedings, Part I 38},
  pages={375--403},
  year={2019},
  organization={Springer}
}

@article{jayaraman2018distributed,
  title={Distributed learning without distress: Privacy-preserving empirical risk minimization},
  author={Jayaraman, Bargav and Wang, Lingxiao and Evans, David and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{agarwal2021skellam,
  title={The skellam mechanism for differentially private federated learning},
  author={Agarwal, Naman and Kairouz, Peter and Liu, Ziyu},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={5052--5064},
  year={2021}
}

@inproceedings{kairouz2021distributed,
  title={The distributed discrete gaussian mechanism for federated learning with secure aggregation},
  author={Kairouz, Peter and Liu, Ziyu and Steinke, Thomas},
  booktitle={International Conference on Machine Learning},
  pages={5201--5212},
  year={2021},
  organization={PMLR}
}

@inproceedings{melis2019exploiting,
  title={Exploiting unintended feature leakage in collaborative learning},
  author={Melis, Luca and Song, Congzheng and De Cristofaro, Emiliano and Shmatikov, Vitaly},
  booktitle={2019 IEEE symposium on security and privacy (SP)},
  pages={691--706},
  year={2019},
  organization={IEEE}
}

@article{shamir1979share,
  title={How to share a secret},
  author={Shamir, Adi},
  journal={Communications of the ACM},
  volume={22},
  number={11},
  pages={612--613},
  year={1979},
  publisher={ACm New York, NY, USA}
}

@article{li2023convergence,
  title={Convergence and Privacy of Decentralized Nonconvex Optimization with Gradient Clipping and Communication Compression},
  author={Li, Boyue and Chi, Yuejie},
  journal={arXiv preprint arXiv:2305.09896},
  year={2023}
}

@inproceedings{allouah2023privacy,
  title={On the Privacy-Robustness-Utility Trilemma in Distributed Learning},
  author={Allouah, Youssef and Guerraoui, Rachid and Gupta, Nirupam and Pinot, Rafa{\"e}l and Stephan, John},
  booktitle={International Conference on Machine Learning},
  year={2023}
}

@article{lian2017can,
  title={Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent},
  author={Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{huang2019dp,
  title={DP-ADMM: ADMM-based distributed learning with differential privacy},
  author={Huang, Zonghao and Hu, Rui and Guo, Yuanxiong and Chan-Tin, Eric and Gong, Yanmin},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={15},
  pages={1002--1012},
  year={2019},
  publisher={IEEE}
}

@inproceedings{cheng2019towards,
  title={Towards decentralized deep learning with differential privacy},
  author={Cheng, Hsin-Pai and Yu, Patrick and Hu, Haojing and Zawad, Syed and Yan, Feng and Li, Shiyu and Li, Hai and Chen, Yiran},
  booktitle={International Conference on Cloud Computing},
  pages={130--145},
  year={2019},
  organization={Springer}
}

@article{fiedler1973algebraic,
  title={Algebraic connectivity of graphs},
  author={Fiedler, Miroslav},
  journal={Czechoslovak mathematical journal},
  volume={23},
  number={2},
  pages={298--305},
  year={1973},
  publisher={Institute of Mathematics, Academy of Sciences of the Czech Republic}
}

@article{de2007old,
  title={Old and new results on algebraic connectivity of graphs},
  author={De Abreu, Nair Maria Maia},
  journal={Linear algebra and its applications},
  volume={423},
  number={1},
  pages={53--73},
  year={2007},
  publisher={Elsevier}
}

@article{gil2013renyi,
  title={R{\'e}nyi divergence measures for commonly used univariate continuous distributions},
  author={Gil, Manuel and Alajaji, Fady and Linder, Tamas},
  journal={Information Sciences},
  volume={249},
  pages={124--131},
  year={2013},
  publisher={Elsevier}
}

@inproceedings{koloskova2020unified,
  title={A unified theory of decentralized sgd with changing topology and local updates},
  author={Koloskova, Anastasia and Loizou, Nicolas and Boreiri, Sadra and Jaggi, Martin and Stich, Sebastian},
  booktitle={International Conference on Machine Learning},
  pages={5381--5393},
  year={2020},
  organization={PMLR}
}

@inproceedings{li2019privacy,
  title={Privacy-preserving distributed average consensus based on additive secret sharing},
  author={Li, Qiongxiu and Cascudo, Ignacio and Christensen, Mads Gr{\ae}sb{\o}ll},
  booktitle={2019 27th European Signal Processing Conference (EUSIPCO)},
  pages={1--5},
  year={2019},
  organization={IEEE}
}

@article{xiao2019local,
  title={Local differential privacy in decentralized optimization},
  author={Xiao, Hanshen and Ye, Yu and Devadas, Srinivas},
  journal={arXiv preprint arXiv:1902.06101},
  year={2019}
}

@article{wang2022decentralized,
  title={Decentralized stochastic optimization with inherent privacy protection},
  author={Wang, Yongqiang and Poor, H Vincent},
  journal={IEEE Transactions on Automatic Control},
  volume={68},
  number={4},
  pages={2293--2308},
  year={2022},
  publisher={IEEE}
}

@inproceedings{bonawitz2017practical,
  title={Practical secure aggregation for privacy-preserving machine learning},
  author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  booktitle={proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages={1175--1191},
  year={2017}
}

@inproceedings{cyffers2022privacy,
  title={Privacy amplification by decentralization},
  author={Cyffers, Edwige and Bellet, Aur{\'e}lien},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={5334--5353},
  year={2022},
  organization={PMLR}
}

@article{imtiaz2019distributed,
  title={Distributed differentially private computation of functions with correlated noise},
  author={Imtiaz, Hafiz and Mohammadi, Jafar and Sarwate, Anand D},
  journal={arXiv preprint arXiv:1904.10059},
  year={2019}
}

@article{sabater2022accurate,
  title={An accurate, scalable and verifiable protocol for federated differentially private averaging},
  author={Sabater, C{\'e}sar and Bellet, Aur{\'e}lien and Ramon, Jan},
  journal={Machine Learning},
  volume={111},
  number={11},
  pages={4249--4293},
  year={2022},
  publisher={Springer}
}

@article{sheller2020federated,
  title={Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient data},
  author={Sheller, Micah J and Edwards, Brandon and Reina, G Anthony and Martin, Jason and Pati, Sarthak and Kotrotsou, Aikaterini and Milchenko, Mikhail and Xu, Weilin and Marcus, Daniel and Colen, Rivka R and others},
  journal={Scientific reports},
  volume={10},
  number={1},
  pages={1--12},
  year={2020},
  publisher={Springer}
}

@inproceedings{allouah2023fixing,
  title={Fixing by Mixing: A Recipe for Optimal Byzantine ML under Heterogeneity},
  author={Allouah, Youssef and Farhadkhani, Sadegh and Guerraoui, Rachid and Gupta, Nirupam and Pinot, Rafa{\"e}l and Stephan, John},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1232--1300},
  year={2023},
  organization={PMLR}
}

@article{agarwal2009information,
  title={Information-theoretic lower bounds on the oracle complexity of convex optimization},
  author={Agarwal, Alekh and Wainwright, Martin J and Bartlett, Peter and Ravikumar, Pradeep},
  journal={Advances in Neural Information Processing Systems},
  volume={22},
  year={2009}
}

@article{gadat2018stochastic,
author = {S{\'e}bastien Gadat and Fabien Panloup and Sofiane Saadane},
title = {{Stochastic heavy ball}},
volume = {12},
journal = {Electronic Journal of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {461 -- 529},
keywords = {Random dynamical systems, second-order methods, Stochastic optimization algorithms},
year = {2018},
doi = {10.1214/18-EJS1395},
URL = {https://doi.org/10.1214/18-EJS1395}
}

@inproceedings{karimi2016linear,
  title={Linear convergence of gradient and proximal-gradient methods under the polyak-{\l}ojasiewicz condition},
  author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  booktitle={Joint European conference on machine learning and knowledge discovery in databases},
  pages={795--811},
  year={2016},
  organization={Springer}
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021},
  publisher={Now Publishers, Inc.}
}

@article{arora2022faster,
  title={Faster Rates of Convergence to Stationary Points in Differentially Private Optimization},
  author={Arora, Raman and Bassily, Raef and Gonz{\'a}lez, Tom{\'a}s and Guzm{\'a}n, Crist{\'o}bal and Menart, Michael and Ullah, Enayat},
  journal={arXiv preprint arXiv:2206.00846},
  year={2022}
}

@article{schmidt2017minimizing,
  title={Minimizing finite sums with the stochastic average gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  volume={162},
  number={1},
  pages={83--112},
  year={2017},
  publisher={Springer}
}

@inproceedings{evfimievski2003limiting,
  title={Limiting privacy breaches in privacy preserving data mining},
  author={Evfimievski, Alexandre and Gehrke, Johannes and Srikant, Ramakrishnan},
  booktitle={Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems},
  pages={211--222},
  year={2003}
}

@inproceedings{
georgiev2022privacy,
title={Privacy Induces Robustness: Information-Computation Gaps and Sparse Mean Estimation},
author={Kristian Georgiev and Samuel B. Hopkins},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=g-OkeNXPy-X}
}

@article{li2022robustness,
  title={On robustness and local differential privacy},
  author={Li, Mengchu and Berrett, Thomas B and Yu, Yi},
  journal={arXiv preprint arXiv:2201.00751},
  year={2022}
}

@inproceedings{chhor2022robust,
  title={Robust estimation of discrete distributions under local differential privacy},
  author={Chhor, Julien and Sentenac, Flore},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={411--446},
  year={2023},
  organization={PMLR}
}

@inproceedings{acharya2021robust,
  title={Robust Testing and Estimation under Manipulation Attacks},
  author={Acharya, Jayadev and Sun, Ziteng and Zhang, Huanyu},
  booktitle={International Conference on Machine Learning},
  pages={43--53},
  year={2021},
  organization={PMLR}
}

@article{hopkins2022robustness,
  title={Robustness Implies Privacy in Statistical Estimation},
  author={Hopkins, Samuel B and Kamath, Gautam and Majid, Mahbod and Narayanan, Shyam},
  journal={arXiv preprint arXiv:2212.05015},
  year={2022}
}

@inproceedings{cheu2021manipulation,
  title={Manipulation attacks in local differential privacy},
  author={Cheu, Albert and Smith, Adam and Ullman, Jonathan},
  booktitle={2021 IEEE Symposium on Security and Privacy (SP)},
  pages={883--900},
  year={2021},
  organization={IEEE}
}

@inproceedings{erlingsson2019amplification,
  title={Amplification by shuffling: From local to central differential privacy via anonymity},
  author={Erlingsson, {\'U}lfar and Feldman, Vitaly and Mironov, Ilya and Raghunathan, Ananth and Talwar, Kunal and Thakurta, Abhradeep},
  booktitle={Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={2468--2479},
  year={2019},
  organization={SIAM}
}

@inproceedings{smith2017interaction,
  title={Is interaction necessary for distributed private learning?},
  author={Smith, Adam and Thakurta, Abhradeep and Upadhyay, Jalaj},
  booktitle={2017 IEEE Symposium on Security and Privacy (SP)},
  pages={58--77},
  year={2017},
  organization={IEEE}
}

@article{li2022communication,
  title={Communication-Efficient and {B}yzantine-Robust Differentially Private Federated Learning},
  author={Li, Min and Xiao, Di and Liang, Jia and Huang, Hui},
  journal={IEEE Communications Letters},
  year={2022},
  publisher={IEEE}
}

@article{xiang2022beta,
  title={$\beta$-Stochastic Sign SGD: A {B}yzantine Resilient and Differentially Private Gradient Compressor for Federated Learning},
  author={Xiang, Ming and Su, Lili},
  journal={arXiv preprint arXiv:2210.00665},
  year={2022}
}

@article{ma2022differentially,
  title={Differentially Private {B}yzantine-robust Federated Learning},
  author={Ma, Xu and Sun, Xiaoqian and Wu, Yuduo and Liu, Zheli and Chen, Xiaofeng and Dong, Changyu},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  year={2022},
  publisher={IEEE}
}

@article{choudhury2019differential,
  title={Differential privacy-enabled federated learning for sensitive health data},
  author={Choudhury, Olivia and Gkoulalas-Divanis, Aris and Salonidis, Theodoros and Sylla, Issa and Park, Yoonyoung and Hsu, Grace and Das, Amar},
  journal={arXiv preprint arXiv:1910.02578},
  year={2019}
}

@article{hu2020personalized,
  title={Personalized federated learning with differential privacy},
  author={Hu, Rui and Guo, Yuanxiong and Li, Hongning and Pei, Qingqi and Gong, Yanmin},
  journal={IEEE Internet of Things Journal},
  volume={7},
  number={10},
  pages={9530--9539},
  year={2020},
  publisher={IEEE}
}

@misc{pauwels2020lecture,
  title={Lecture notes: Statistics, optimization and algorithms in high dimension},
  author={Pauwels, Edouard},
  year={2020}
}

@book{cachin2011introduction,
  title={Introduction to reliable and secure distributed programming},
  author={Cachin, Christian and Guerraoui, Rachid and Rodrigues, Lu{\'\i}s},
  year={2011},
  publisher={Springer Science \& Business Media}
}

@article{rigollet2015high,
  title={High dimensional statistics},
  author={Rigollet, Phillippe and H{\"u}tter, Jan-Christian},
  journal={Lecture notes for course 18S997},
  volume={813},
  number={814},
  pages={46},
  year={2015}
}

@article{vershynin2010introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Vershynin, Roman},
  journal={arXiv preprint arXiv:1011.3027},
  year={2010}
}

@article{zhu2022robust,
  title={Robust estimation via generalized quasi-gradients},
  author={Zhu, Banghua and Jiao, Jiantao and Steinhardt, Jacob},
  journal={Information and Inference: A Journal of the IMA},
  volume={11},
  number={2},
  pages={581--636},
  year={2022},
  publisher={Oxford University Press}
}

@book{steinhardt2018robust,
  title={Robust learning: Information theory and algorithms},
  author={Steinhardt, Jacob},
  year={2018},
  publisher={Stanford University}
}

@inproceedings{diakonikolas2017being,
  title={Being robust (in high dimensions) can be practical},
  author={Diakonikolas, Ilias and Kamath, Gautam and Kane, Daniel M and Li, Jerry and Moitra, Ankur and Stewart, Alistair},
  booktitle={International Conference on Machine Learning},
  pages={999--1008},
  year={2017},
  organization={PMLR}
}

@article{kasiviswanathan2011can,
  title={What can we learn privately?},
  author={Kasiviswanathan, Shiva Prasad and Lee, Homin K and Nissim, Kobbi and Raskhodnikova, Sofya and Smith, Adam},
  journal={SIAM Journal on Computing},
  volume={40},
  number={3},
  pages={793--826},
  year={2011},
  publisher={SIAM}
}

@inproceedings{duchi2013local,
  title={Local privacy and statistical minimax rates},
  author={Duchi, John C and Jordan, Michael I and Wainwright, Martin J},
  booktitle={2013 IEEE 54th Annual Symposium on Foundations of Computer Science},
  pages={429--438},
  year={2013},
  organization={IEEE}
}

@inproceedings{hardt2010geometry,
  title={On the geometry of differential privacy},
  author={Hardt, Moritz and Talwar, Kunal},
  booktitle={Proceedings of the forty-second ACM symposium on Theory of computing},
  pages={705--714},
  year={2010}
}

@inproceedings{bun2014fingerprinting,
  title={Fingerprinting codes and the price of approximate differential privacy},
  author={Bun, Mark and Ullman, Jonathan and Vadhan, Salil},
  booktitle={Proceedings of the forty-sixth annual ACM symposium on Theory of computing},
  pages={1--10},
  year={2014}
}

@book{nesterov2018lectures,
  title={Lectures on convex optimization},
  author={Nesterov, Yurii and others},
  volume={137},
  year={2018},
  publisher={Springer}
}

@article{chaudhuri2019capacity,
  title={Capacity bounded differential privacy},
  author={Chaudhuri, Kamalika and Imola, Jacob and Machanavajjhala, Ashwin},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{minsker2015geometric,
  title={Geometric median and robust estimation in Banach spaces},
  author={Minsker, Stanislav},
  journal={Bernoulli},
  volume={21},
  number={4},
  pages={2308--2335},
  year={2015},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@article{pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{davis2019stochastic,
  title={Stochastic model-based minimization of weakly convex functions},
  author={Davis, Damek and Drusvyatskiy, Dmitriy},
  journal={SIAM Journal on Optimization},
  volume={29},
  number={1},
  pages={207--239},
  year={2019},
  publisher={SIAM}
}

@article{wei2020federated,
  title={Federated learning with differential privacy: Algorithms and performance analysis},
  author={Wei, Kang and Li, Jun and Ding, Ming and Ma, Chuan and Yang, Howard H and Farokhi, Farhad and Jin, Shi and Quek, Tony QS and Poor, H Vincent},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={15},
  pages={3454--3469},
  year={2020},
  publisher={IEEE}
}

@inproceedings{lai2016agnostic,
  title={Agnostic estimation of mean and covariance},
  author={Lai, Kevin A and Rao, Anup B and Vempala, Santosh},
  booktitle={2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS)},
  pages={665--674},
  year={2016},
  organization={IEEE}
}

@article{chen2018robust,
  title={Robust covariance and scatter matrix estimation under Huber’s contamination model},
  author={Chen, Mengjie and Gao, Chao and Ren, Zhao},
  journal={The Annals of Statistics},
  volume={46},
  number={5},
  pages={1932--1960},
  year={2018},
  publisher={JSTOR}
}

@inproceedings{lowy2022private,
  title={Private non-convex federated learning without a trusted server},
  author={Lowy, Andrew and Ghafelebashi, Ali and Razaviyayn, Meisam},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={5749--5786},
  year={2023},
  organization={PMLR}
}

@inproceedings{lowy2021private,
title={Private Federated Learning Without a Trusted Server: Optimal Algorithms for Convex Losses},
author={Andrew Lowy and Meisam Razaviyayn},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=TVY6GoURrw}
}

@inproceedings{ashtiani2022private,
  title={Private and polynomial time algorithms for learning Gaussians and beyond},
  author={Ashtiani, Hassan and Liaw, Christopher},
  booktitle={Conference on Learning Theory},
  pages={1075--1076},
  year={2022},
  organization={PMLR}
}

@article{liu2021robust,
  title={Robust and differentially private mean estimation},
  author={Liu, Xiyang and Kong, Weihao and Kakade, Sham and Oh, Sewoong},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={3887--3901},
  year={2021}
}

@inproceedings{hopkins2022efficient,
  title={Efficient mean estimation with pure differential privacy via a sum-of-squares exponential mechanism},
  author={Hopkins, Samuel B and Kamath, Gautam and Majid, Mahbod},
  booktitle={Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing},
  pages={1406--1417},
  year={2022}
}

@article{chaudhuri2011differentially,
  title={Differentially private empirical risk minimization.},
  author={Chaudhuri, Kamalika and Monteleoni, Claire and Sarwate, Anand D},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={3},
  year={2011}
}

@article{agarwal2018cpsgd,
  title={cpSGD: Communication-efficient and differentially-private distributed SGD},
  author={Agarwal, Naman and Suresh, Ananda Theertha and Yu, Felix Xinnan X and Kumar, Sanjiv and McMahan, Brendan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@InProceedings{pmlr-v130-girgis21a,
  title = 	 { Shuffled Model of Differential Privacy in Federated Learning },
  author =       {Girgis, Antonious and Data, Deepesh and Diggavi, Suhas and Kairouz, Peter and Theertha Suresh, Ananda},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2521--2529},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/girgis21a/girgis21a.pdf},
  url = 	 {https://proceedings.mlr.press/v130/girgis21a.html},
  abstract = 	 { We consider a distributed empirical risk minimization (ERM) optimization problem with communication efficiency and privacy requirements, motivated by the federated learning (FL) framework. We propose a distributed communication-efficient and local differentially private stochastic gradient descent (CLDP-SGD) algorithm and analyze its communication, privacy, and convergence trade-offs. Since each iteration of the CLDP-SGD aggregates the client-side local gradients, we develop (optimal) communication-efficient schemes for mean estimation for several $\ell_p$ spaces under local differential privacy (LDP). To overcome performance limitation of LDP, CLDP-SGD takes advantage of the inherent privacy amplification provided by client subsampling and data subsampling at each selected client (through SGD) as well as the recently developed shuffled model of privacy. For convex loss functions, we prove that the proposed CLDP-SGD algorithm matches the known lower bounds on the \textit{centralized} private ERM while using a finite number of bits per iteration for each client, \emph{i.e.,} effectively getting communication efficiency for “free”. We also provide preliminary experimental results supporting the theory. }
}


@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@inproceedings{
brendan2018learning,
title={Learning Differentially Private Recurrent Language Models},
author={H. Brendan McMahan and Daniel Ramage and Kunal Talwar and Li Zhang},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BJ0hF1Z0b},
}

@inproceedings{li2020secure,
  title={Secure federated averaging algorithm with differential privacy},
  author={Li, Yiwei and Chang, Tsung-Hui and Chi, Chong-Yung},
  booktitle={2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}

@article{steinke2016between,
  title={Between Pure and Approximate Differential Privacy},
  author={Steinke, Thomas and Ullman, Jonathan},
  journal={Journal of Privacy and Confidentiality},
  volume={7},
  number={2},
  year={2016}
}

@inproceedings{liu2022differential,
  title={Differential privacy and robust statistics in high dimensions},
  author={Liu, Xiyang and Kong, Weihao and Oh, Sewoong},
  booktitle={Conference on Learning Theory},
  pages={1167--1246},
  year={2022},
  organization={PMLR}
}

@inproceedings{
karimireddy2022byzantinerobust,
title={{B}yzantine-Robust Learning on Heterogeneous Datasets via Bucketing},
author={Sai Praneeth Karimireddy and Lie He and Martin Jaggi},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=jXKKDEi5vJt}
}

@inproceedings{elmhamdi21jungle,
      title = {Collaborative Learning in the Jungle (Decentralized,  {B}yzantine, Heterogeneous, Asynchronous and Nonconvex  Learning)},
      author = {El Mhamdi, El Mahdi and Farhadkhani, Sadegh and Guerraoui,  Rachid and Guirguis, Arsany Hany Abdelmessih and Hoang, Le  Nguyen and Rouault, Sébastien Louis Alexandre},
      journal = {Advances in Neural Information Processing Systems 34  pre-proceedings (NeurIPS 2021)},
      series = {Advances in Neural Information Processing Systems. 34},
      pages = {14},
      year = {2021},
      note = {Published as part of Advances in Neural Information  Processing Systems 34 proceedings (NeurIPS 2021).},
      abstract = {We study {B}yzantine collaborative learning, where n nodes  seek to collectively learn from each others' local data.  The data distribution may vary from one node to another. No  node is trusted, and f &lt; n nodes can behave arbitrarily. We  prove that collaborative learning is equivalent to a new  form of agreement, which we call averaging agreement. In  this problem, nodes start each with an initial vector and  seek to approximately agree on a common vector, which is  close to the average of honest nodes' initial vectors. We  present two asynchronous solutions to averaging agreement,  each we prove optimal according to some dimension. The  first, based on the minimum-diameter averaging, requires n  ≥ 6f+1, but achieves asymptotically the best-possible  averaging constant up to a multiplicative constant. The  second, based on reliable broadcast and coordinate-wise  trimmed mean, achieves optimal {B}yzantine resilience, i.e.,  n≥3f+1. Each of these algorithms induces an optimal  {B}yzantine collaborative learning protocol. In particular,  our equivalence yields new impossibility theorems on what  any collaborative learning algorithm can achieve in  adversarial and heterogeneous environments.},
      url = {http://infoscience.epfl.ch/record/291214},
}

@article{vial83,
 ISSN = {0364765X, 15265471},
 URL = {http://www.jstor.org/stable/3689591},
 abstract = {In this paper we study two classes of sets, strongly and weakly convex sets. For each class we derive a series of properties which involve either the concept of supporting ball, an obvious extension of the concept of supporting hyperplane, or the normal cone to the set. We also study a class of functions, denoted ρ-convex, which satisfy for arbitrary points x1 and x2 and any value λ ∈ [0, 1] the classical inequality of convex functions up to a term $\rho (1-\lambda)\lambda \|x_{1}-x_{2}\|^{2}$. Depending on the sign of the constant ρ the function is said to be strongly or weakly convex. We provide characteristic properties of this class of sets and we relate it to strongly and weakly convex sets via the epigraph and the level sets. Finally, we give three applications: a separation theorem, a sufficient condition for global optimum of a nonconvex programming problem, and a sufficient geometrical condition for a set to be a manifold.},
 author = {Jean-Philippe Vial},
 journal = {Mathematics of Operations Research},
 number = {2},
 pages = {231--259},
 publisher = {INFORMS},
 title = {Strong and Weak Convexity of Sets and Functions},
 urldate = {2022-08-04},
 volume = {8},
 year = {1983}
}

@InProceedings{wang19c,
  title = 	 {Differentially Private Empirical Risk Minimization with Non-convex Loss Functions},
  author =       {Wang, Di and Chen, Changyou and Xu, Jinhui},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6526--6535},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/wang19c/wang19c.pdf},
  url = 	 {https://proceedings.mlr.press/v97/wang19c.html},
  abstract = 	 {We study the problem of Empirical Risk Minimization (ERM) with (smooth) non-convex loss functions under the differential-privacy (DP) model. Existing approaches for this problem mainly adopt gradient norms to measure the error, which in general cannot guarantee the quality of the solution. To address this issue, we first study the expected excess empirical (or population) risk, which was primarily used as the utility to measure the quality for convex loss functions. Specifically, we show that the excess empirical (or population) risk can be upper bounded by $\tilde{O}(\frac{d\log (1/\delta)}{\log n\epsilon^2})$ in the $(\epsilon, \delta)$-DP settings, where $n$ is the data size and $d$ is the dimensionality of the space. The $\frac{1}{\log n}$ term in the empirical risk bound can be further improved to $\frac{1}{n^{\Omega(1)}}$ (when $d$ is a constant) by a highly non-trivial analysis on the time-average error. To obtain more efficient solutions, we also consider the connection between achieving differential privacy and finding approximate local minimum. Particularly, we show that when the size $n$ is large enough, there are $(\epsilon, \delta)$-DP algorithms which can find an approximate local minimum of the empirical risk with high probability in both the constrained and non-constrained settings. These results indicate that one can escape saddle points privately.}
}


@article{lamport82,
author = {Lamport, Leslie and Shostak, Robert and Pease, Marshall},
title = {The {B}yzantine Generals Problem},
year = {1982},
issue_date = {July 1982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {0164-0925},
url = {https://doi.org/10.1145/357172.357176},
doi = {10.1145/357172.357176},
journal = {ACM Trans. Program. Lang. Syst.},
month = {jul},
pages = {382–401},
numpages = {20}
}

@inproceedings{fallah2020,
 author = {Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {3557--3568},
 publisher = {Curran Associates, Inc.},
 title = {Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach},
 url = {https://proceedings.neurips.cc/paper/2020/file/24389bfe4fe2eba8bf9aa9203a44cdad-Paper.pdf},
 volume = {33},
 year = {2020}
}



@inproceedings{zhu22bridging,
  title     = {Bridging Differential Privacy and {B}yzantine-Robustness via Model Aggregation},
  author    = {Zhu, Heng and Ling, Qing},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on
               Artificial Intelligence, {IJCAI-22}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Lud De Raedt},
  pages     = {2427--2433},
  year      = {2022},
  month     = {7},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2022/337},
  url       = {https://doi.org/10.24963/ijcai.2022/337},
}


@book{rice2006mathematical,
  title={Mathematical statistics and data analysis},
  author={Rice, John A},
  year={2006},
  pages={194},
  publisher={Cengage Learning}
}


@inproceedings{yu2019parallel,
  title={Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning},
  author={Yu, Hao and Yang, Sen and Zhu, Shenghuo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={5693--5700},
  year={2019}
}

@inproceedings{khaled2020tighter,
  title={Tighter theory for local SGD on identical and heterogeneous data},
  author={Khaled, Ahmed and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4519--4529},
  year={2020},
  organization={PMLR}
}

@inproceedings{wang2019subsampled,
  title={Subsampled r{\'e}nyi differential privacy and analytical moments accountant},
  author={Wang, Yu-Xiang and Balle, Borja and Kasiviswanathan, Shiva Prasad},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1226--1235},
  year={2019},
  organization={PMLR}
}

@inproceedings{mironov2017renyi,
  title={R{\'e}nyi differential privacy},
  author={Mironov, Ilya},
  booktitle={2017 IEEE 30th computer security foundations symposium (CSF)},
  pages={263--275},
  year={2017},
  organization={IEEE}
}

@article{dwork2014algorithmic,
  title={The algorithmic foundations of differential privacy},
  author={Dwork, Cynthia and Roth, Aaron and others},
  journal={Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume={9},
  number={3--4},
  pages={211--407},
  year={2014},
  publisher={Now Publishers, Inc.}
}

@inproceedings{noble2022differentially,
  title={Differentially private federated learning on heterogeneous data},
  author={Noble, Maxence and Bellet, Aur{\'e}lien and Dieuleveut, Aymeric},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={10110--10145},
  year={2022},
  organization={PMLR}
}

@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}


@InProceedings{farhadkhani2022byzantine,
  title = 	 {{B}yzantine Machine Learning Made Easy By Resilient Averaging of Momentums},
  author =       {Farhadkhani, Sadegh and Guerraoui, Rachid and Gupta, Nirupam and Pinot, Rafael and Stephan, John},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {6246--6283},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/farhadkhani22a/farhadkhani22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/farhadkhani22a.html},
  abstract = 	 {{B}yzantine resilience emerged as a prominent topic within the distributed machine learning community. Essentially, the goal is to enhance distributed optimization algorithms, such as distributed SGD, in a way that guarantees convergence despite the presence of some misbehaving (a.k.a., <em>{B}yzantine</em>) workers. Although a myriad of techniques addressing the problem have been proposed, the field arguably rests on fragile foundations. These techniques are hard to prove correct and rely on assumptions that are (a) quite unrealistic, i.e., often violated in practice, and (b) heterogeneous, i.e., making it difficult to compare approaches. We present <em>RESAM (RESilient Averaging of Momentums)</em>, a unified framework that makes it simple to establish optimal {B}yzantine resilience, relying only on standard machine learning assumptions. Our framework is mainly composed of two operators: <em>resilient averaging</em> at the server and <em>distributed momentum</em> at the workers. We prove a general theorem stating the convergence of distributed SGD under RESAM. Interestingly, demonstrating and comparing the convergence of many existing techniques become direct corollaries of our theorem, without resorting to stringent assumptions. We also present an empirical evaluation of the practical relevance of RESAM.}
}


@book{villani2009optimal,
author = {Cédric Villani},
address = {Berlin},
booktitle = {Optimal transport : old and new / Cédric Villani},
isbn = {978-3-540-71049-3},
keywords = {Systèmes dynamiques},
language = {eng},
publisher = {Springer},
series = {Grundlehren der mathematischen Wissenschaften},
title = {Optimal transport  : old and new / Cédric Villani},
year = {2009},
}

@inproceedings{cohen2016geometric,
  title={Geometric median in nearly linear time},
  author={Cohen, Michael B and Lee, Yin Tat and Miller, Gary and Pachocki, Jakub and Sidford, Aaron},
  booktitle={Proceedings of the forty-eighth annual ACM symposium on Theory of Computing},
  pages={9--21},
  year={2016}
}

@article{lian2015asynchronous,
  title={Asynchronous parallel stochastic gradient for nonconvex optimization},
  author={Lian, Xiangru and Huang, Yijun and Li, Yuncheng and Liu, Ji},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  pages={2737--2745},
  year={2015}
}

@article{lei2019stochastic,
  title={Stochastic gradient descent for nonconvex learning without bounded gradient assumptions},
  author={Lei, Yunwen and Hu, Ting and Li, Guiying and Tang, Ke},
  journal={IEEE transactions on neural networks and learning systems},
  volume={31},
  number={10},
  pages={4394--4400},
  year={2019},
  publisher={IEEE}
}

@article{tsitsiklis1986distributed,
  title={Distributed asynchronous deterministic and stochastic gradient optimization algorithms},
  author={Tsitsiklis, John and Bertsekas, Dimitri and Athans, Michael},
  journal={IEEE transactions on automatic control},
  volume={31},
  number={9},
  pages={803--812},
  year={1986},
  publisher={IEEE}
}

@article{arnold1979bounds,
  title={Bounds on expectations of linear systematic statistics based on dependent samples},
  author={Arnold, Barry C and Groeneveld, Richard A},
  journal={The Annals of Statistics},
  pages={220--223},
  year={1979},
  publisher={JSTOR}
}

@article{bertsimas2006tight,
  title={Tight bounds on expected order statistics},
  author={Bertsimas, Dimitris and Natarajan, Karthik and Teo, Chung-Piaw},
  journal={Probability in the Engineering and Informational Sciences},
  volume={20},
  number={4},
  pages={667--686},
  year={2006},
  publisher={Cambridge University Press}
}

@book{huber2004robust,
  title={Robust statistics},
  author={Huber, Peter J},
  volume={523},
  year={2004},
  publisher={John Wiley \& Sons}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}
@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@book{lynch1996distributed,
  title={Distributed algorithms},
  author={Lynch, Nancy A},
  year={1996},
  publisher={Elsevier}
}

@InProceedings{DP,
author="Dwork, Cynthia",
editor="Bugliesi, Michele
and Preneel, Bart
and Sassone, Vladimiro
and Wegener, Ingo",
title="Differential Privacy",
booktitle="Automata, Languages and Programming",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--12",
abstract="In 1977 Dalenius articulated a desideratum for statistical databases: nothing about an individual should be learnable from the database that cannot be learned without access to the database. We give a general impossibility result showing that a formalization of Dalenius' goal along the lines of semantic security cannot be achieved. Contrary to intuition, a variant of the result threatens the privacy even of someone not in the database. This state of affairs suggests a new measure, differential privacy, which, intuitively, captures the increased risk to one's privacy incurred by participating in a database. The techniques developed in a sequence of papers [8, 13, 3], culminating in those described in [12], can achieve any desired level of privacy under this measure. In many cases, extremely accurate information about the database can be provided while simultaneously ensuring very high levels of privacy.",
isbn="978-3-540-35908-1"
}

@inproceedings{zhang2013information,
  title={Information-theoretic lower bounds for distributed statistical estimation with communication constraints.},
  author={Zhang, Yuchen and Duchi, John C and Jordan, Michael I and Wainwright, Martin J},
  booktitle={NIPS},
  pages={2328--2336},
  year={2013},
  organization={Citeseer}
}

@article{wu2017lecture,
  title={Lecture notes on information-theoretic methods for high-dimensional statistics},
  author={Wu, Yihong},
  journal={Lecture Notes for ECE598YW (UIUC)},
  volume={16},
  year={2017}
}

@article{yang1999information,
  title={Information-theoretic determination of minimax rates of convergence},
  author={Yang, Yuhong and Barron, Andrew},
  journal={Annals of Statistics},
  pages={1564--1599},
  year={1999},
  publisher={JSTOR}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@inproceedings{su2016fault,
  title={Fault-tolerant multi-agent optimization: optimal iterative distributed algorithms},
  author={Su, Lili and Vaidya, Nitin H},
  booktitle={Proceedings of the 2016 ACM symposium on principles of distributed computing},
  pages={425--434},
  year={2016}
}

@inproceedings{simard2003best,
  title={Best practices for convolutional neural networks applied to visual document analysis.},
  author={Simard, Patrice Y and Steinkraus, David and Platt, John C and others},
  booktitle={Icdar},
  volume={3},
  number={2003},
  year={2003},
  organization={Citeseer}
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={Siam Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@article{averaging,
author = {Polyak, Boris and Juditsky, Anatoli},
year = {1992},
month = {07},
pages = {838-855},
title = {Acceleration of Stochastic Approximation by Averaging},
volume = {30},
journal = {SIAM Journal on Control and Optimization},
doi = {10.1137/0330046}
}

@inproceedings{imagenet,
AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
BOOKTITLE = {CVPR09},
YEAR = {2009},
BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"}

@article{openimages,
   title={The Open Images Dataset V4},
   volume={128},
   ISSN={1573-1405},
   DOI={10.1007/s11263-020-01316-z},
   number={7},
   journal={International Journal of Computer Vision},
   publisher={Springer Science and Business Media LLC},
   author={Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Kolesnikov, Alexander and et al.},
   year={2020},
   month={Mar},
   pages={1956–1981}
}

@article{reduce,
title = "Bandwidth optimal all-reduce algorithms for clusters of workstations",
journal = "Journal of Parallel and Distributed Computing",
volume = "69",
number = "2",
pages = "117 - 124",
year = "2009",
issn = "0743-7315",
doi = "https://doi.org/10.1016/j.jpdc.2008.09.002",
author = "Pitch Patarasuk and Xin Yuan",
keywords = "All-reduce, Collective communication, Tree topology, Cluster of workstations",
abstract = "We consider an efficient realization of the all-reduce operation with large data sizes in cluster environments, under the assumption that the reduce operator is associative and commutative. We derive a tight lower bound of the amount of data that must be communicated in order to complete this operation and propose a ring-based algorithm that only requires tree connectivity to achieve bandwidth optimality. Unlike the widely used butterfly-like all-reduce algorithm that incurs network contention in SMP/multi-core clusters, the proposed algorithm can achieve contention-free communication in almost all contemporary clusters, including SMP/multi-core clusters and Ethernet switched clusters with multiple switches. We demonstrate that the proposed algorithm is more efficient than other algorithms on clusters with different nodal architectures and networking technologies when the data size is sufficiently large."
}


######################################## PRIVACY AND FAULTS IN LEARNING ###############################################
@incollection{DLG,
title = {Deep Leakage from Gradients},
author = {Zhu, Ligeng and Liu, Zhijian and Han, Song},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {14774--14784},
year = {2019},
publisher = {Curran Associates, Inc.},
}

@inproceedings{yin2018byzantine,
  title={{B}yzantine-robust distributed learning: Towards optimal statistical rates},
  author={Yin, Dong and Chen, Yudong and Kannan, Ramchandran and Bartlett, Peter},
  booktitle={International Conference on Machine Learning},
  pages={5650--5659},
  year={2018},
  organization={PMLR}
}

@article{ghosh2021communication,
  title={Communication-Efficient and {B}yzantine-Robust Distributed Learning With Error Feedback},
  author={Ghosh, Avishek and Maity, Raj Kumar and Kadhe, Swanand and Mazumdar, Arya and Ramchandran, Kannan},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={2},
  number={3},
  pages={942--953},
  year={2021},
  publisher={IEEE}
}

@incollection{krum,
title = {Machine Learning with Adversaries: {B}yzantine Tolerant Gradient Descent},
author = {Blanchard, Peva and El Mhamdi, El Mahdi and Guerraoui, Rachid and Stainer, Julien},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {119--129},
year = {2017},
publisher = {Curran Associates, Inc.},
}


@InProceedings{mhamdi18a,
  title = 	 {The Hidden Vulnerability of Distributed Learning in {B}yzantium},
  author =       {El Mhamdi, El Mahdi and Guerraoui, Rachid and Rouault, S{\'e}bastien},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {3521--3530},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/mhamdi18a/mhamdi18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/mhamdi18a.html}
}


@inproceedings{el2020genuinely,
  title={Genuinely distributed {B}yzantine machine learning},
  author={El Mhamdi, El Mahdi and Guerraoui, Rachid  and Guirguis, Arsany  and Hoang, L{\^e} Nguy{\^e}n and Rouault, S{\'e}bastien },
  booktitle={Proceedings of the 39th Symposium on Principles of Distributed Computing},
  pages={355--364},
  year={2020}
}

@misc{median,
      title={{B}yzantine-Robust Distributed Learning: Towards Optimal Statistical Rates}, 
      author={Dong Yin and Yudong Chen and Kannan Ramchandran and Peter Bartlett},
      year={2018},
      eprint={1803.01498},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{phocas,
      title={Phocas: dimensional {B}yzantine-resilient stochastic gradient descent}, 
      author={Cong Xie and Oluwasanmi Koyejo and Indranil Gupta},
      year={2018},
      eprint={1805.09682},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@misc{meamed,
      title={Generalized {B}yzantine-tolerant SGD}, 
      author={Cong Xie and Oluwasanmi Koyejo and Indranil Gupta},
      year={2018},
      eprint={1802.10116},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@article{rousseeuw1985multivariate,
  title={Multivariate estimation with high breakdown point},
  author={Rousseeuw, Peter J},
  journal={Mathematical statistics and applications},
  volume={8},
  number={37},
  pages={283--297},
  year={1985}
}

@inproceedings{paramServer,
author = {Li, Mu and Andersen, David G. and Park, Jun Woo and Smola, Alexander J. and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J. and Su, Bor-Yiing},
title = {Scaling Distributed Machine Learning with the Parameter Server},
year = {2014},
isbn = {9781931971164},
publisher = {USENIX Association},
address = {USA},
booktitle = {Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation},
pages = {583–598},
numpages = {16},
location = {Broomfield, CO},
series = {OSDI’14}
}

@inbook{bottou, place={Cambridge}, series={Publications of the Newton Institute}, title={On-line Learning and Stochastic Approximations}, DOI={10.1017/CBO9780511569920.003}, booktitle={On-Line Learning in Neural Networks}, publisher={Cambridge University Press}, author={Bottou, Léon}, year={1999}, pages={9–42}, collection={Publications of the Newton Institute}}

@article{shokri,
  author    = {Reza Shokri and
               Marco Stronati and
               Vitaly Shmatikov},
  title     = {Membership Inference Attacks against Machine Learning Models},
  journal   = {CoRR},
  volume    = {abs/1610.05820},
  year      = {2016},
  archivePrefix = {arXiv},
  eprint    = {1610.05820},
  timestamp = {Mon, 13 Aug 2018 16:47:19 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{Mlleaks,
      title={ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models}, 
      author={Ahmed Salem and Yang Zhang and Mathias Humbert and Pascal Berrang and Mario Fritz and Michael Backes},
      year={2018},
      eprint={1806.01246},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{ByzProblem,
author = {Lamport, Leslie and Shostak, Robert and Pease, Marshall},
title = {The {B}yzantine Generals Problem},
year = {1982},
issue_date = {July 1982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {0164-0925},
doi = {10.1145/357172.357176},
journal = {ACM Trans. Program. Lang. Syst.},
month = jul,
pages = {382–401},
numpages = {20}
}

@article{mnist,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  username = {mhwombat},
  year = 2010
}

@article{fashion-mnist,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  journal      = {arXiv preprint arXiv:1708.07747},
}

@article{cifar,
title= {CIFAR-100 (Canadian Institute for Advanced Research)},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {2009},
abstract= {This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a "fine" label (the class to which it belongs) and a "coarse" label (the superclass to which it belongs).
Here is the list of classes in the CIFAR-100:
Superclass	Classes
aquatic mammals	beaver, dolphin, otter, seal, whale
fish	aquarium fish, flatfish, ray, shark, trout
flowers	orchids, poppies, roses, sunflowers, tulips
food containers	bottles, bowls, cans, cups, plates
fruit and vegetables	apples, mushrooms, oranges, pears, sweet peppers
household electrical devices	clock, computer keyboard, lamp, telephone, television
household furniture	bed, chair, couch, table, wardrobe
insects	bee, beetle, butterfly, caterpillar, cockroach
large carnivores	bear, leopard, lion, tiger, wolf
large man-made outdoor things	bridge, castle, house, road, skyscraper
large natural outdoor scenes	cloud, forest, mountain, plain, sea
large omnivores and herbivores	camel, cattle, chimpanzee, elephant, kangaroo
medium-sized mammals	fox, porcupine, possum, raccoon, skunk
non-insect invertebrates	crab, lobster, snail, spider, worm
people	baby, boy, girl, man, woman
reptiles	crocodile, dinosaur, lizard, snake, turtle
small mammals	hamster, mouse, rabbit, shrew, squirrel
trees	maple, oak, palm, pine, willow
vehicles 1	bicycle, bus, motorcycle, pickup truck, train
vehicles 2	lawn-mower, rocket, streetcar, tank, tractor

Yes, I know mushrooms aren't really fruit or vegetables and bears aren't really carnivores. },
keywords= {Dataset},
terms= {}
}

@article{SVHN,
author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew},
year = {2011},
month = {01},
pages = {},
title = {Reading Digits in Natural Images with Unsupervised Feature Learning},
journal = {NIPS}
}

@TechReport{LFW,
author = {Gary B. Huang and Manu Ramesh and Tamara Berg and Erik Learned-Miller},
title = {Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments},
institution =  {University of Massachusetts, Amherst},
year =         2007,
number =       {07-49},
month =        {October}
}

@InProceedings{AdvancedComposition, title = {The Composition Theorem for Differential Privacy}, author = {Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod}, booktitle = {Proceedings of the 32nd International Conference on Machine Learning}, pages = {1376--1385}, year = {2015}, editor = {Bach, Francis and Blei, David}, volume = {37}, series = {Proceedings of Machine Learning Research}, address = {Lille, France}, month = {07--09 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v37/kairouz15.pdf}, 
abstract = {Interactive querying of a database degrades the privacy level. In this paper we answer the fundamental question of characterizing the level of privacy degradation as a function of the number of adaptive interactions and the differential privacy levels maintained by the individual queries. Our solution is complete: the privacy degradation guarantee is true for every privacy mechanism, and further, we demonstrate a sequence of privacy mechanisms that do degrade in the characterized manner. The key innovation is the introduction of an operational interpretation (involving hypothesis testing) to differential privacy and the use of the corresponding data processing inequalities. Our result improves over the state of the art and has immediate applications to several problems studied in the literature.} } 
@INPROCEEDINGS{LearningChain,
  author={X. {Chen} and J. {Ji} and C. {Luo} and W. {Liao} and P. {Li}},
  booktitle={2018 IEEE International Conference on Big Data (Big Data)},
  title={When Machine Learning Meets Blockchain: A Decentralized, Privacy-preserving and Secure Design}, 
  year={2018},
  volume={},
  number={},
  pages={1178-1187},}
  
@article{chen2017distributed,
  title={Distributed statistical machine learning in adversarial settings: {B}yzantine gradient descent},
  author={Chen, Yudong and Su, Lili and Xu, Jiaming},
  journal={Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  volume={1},
  number={2},
  pages={1--25},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@inproceedings{federated,
title	= {Federated Learning: Strategies for Improving Communication Efficiency},
author	= {Jakub Konečný and H. Brendan McMahan and Felix X. Yu and Peter Richtarik and Ananda Theertha Suresh and Dave Bacon},
year	= {2016},
booktitle	= {NIPS Workshop on Private Multi-Party Machine Learning}
}

@misc{twoServer_Jaggi,
      title={Secure {B}yzantine-Robust Machine Learning}, 
      author={Lie He and Sai Praneeth Karimireddy and Martin Jaggi},
      year={2020},
      eprint={2006.04747},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{brea,
      title={{B}yzantine-Resilient Secure Federated Learning}, 
      author={Jinhyun So and Basak Guler and A. Salman Avestimehr},
      year={2020},
      eprint={2007.11115},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@INPROCEEDINGS{DP_SGD1,
  author={S. {Song} and K. {Chaudhuri} and A. D. {Sarwate}},
  booktitle={2013 IEEE Global Conference on Signal and Information Processing},
  title={Stochastic gradient descent with differentially private updates}, 
  year={2013},
  volume={},
  number={},
  pages={245-248},
  doi={10.1109/GlobalSIP.2013.6736861}}
  
@misc{DP_SGD2,
      title={Differentially Private Stochastic Coordinate Descent}, 
      author={Georgios Damaskinos and Celestine Mendler-Dünner and Rachid Guerraoui and Nikolaos Papandreou and Thomas Parnell},
      year={2020},
      eprint={2006.07272},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{gradObfuscation,
  author={Gade, Shripad and Vaidya, Nitin H.},
  booktitle={2018 IEEE Conference on Decision and Control (CDC)}, 
  title={Privacy-Preserving Distributed Learning via Obfuscated Stochastic Gradients}, 
  year={2018},
  volume={},
  number={},
  pages={184-191},
  doi={10.1109/CDC.2018.8619133}}


@article{GeneralizedGaussian,
author = { Saralees   Nadarajah },
title = {A generalized normal distribution},
journal = {Journal of Applied Statistics},
volume = {32},
number = {7},
pages = {685-694},
year  = {2005},
publisher = {Taylor & Francis}
}

@inproceedings{bassily2014private,
  title={Private empirical risk minimization: Efficient algorithms and tight error bounds},
  author={Bassily, Raef and Smith, Adam and Thakurta, Abhradeep},
  booktitle={2014 IEEE 55th annual symposium on foundations of computer science},
  pages={464--473},
  year={2014},
  organization={IEEE}
}

@misc{papernot,
      title={Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data}, 
      author={Nicolas Papernot and Martín Abadi and Úlfar Erlingsson and Ian Goodfellow and Kunal Talwar},
      year={2017},
      eprint={1610.05755},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{papernot2018scalable,
  title={Scalable Private Learning with PATE},
  author={Papernot, Nicolas and Song, Shuang and Mironov, Ilya and Raghunathan, Ananth and Talwar, Kunal and Erlingsson, Ulfar},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@inproceedings{shokri2015privacy,
  title={Privacy-preserving deep learning},
  author={Shokri, Reza and Shmatikov, Vitaly},
  booktitle={Proceedings of the 22nd ACM SIGSAC conference on computer and communications security},
  pages={1310--1321},
  year={2015}
}

  
  @article{DP_SGD_Fed2,
  title={Toward Robustness and Privacy in Federated Learning: Experimenting with Local and Central Differential Privacy},
  author={M. Naseri and J. Hayes and Emiliano De Cristofaro},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.03561}
}

@misc{composition,
      title={The Composition Theorem for Differential Privacy}, 
      author={Peter Kairouz and Sewoong Oh and Pramod Viswanath},
      year={2015},
      eprint={1311.0776},
      archivePrefix={arXiv},
      primaryClass={cs.DS}
}

@misc{clipping,
      title={Why gradient clipping accelerates training: A theoretical justification for adaptivity}, 
      author={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},
      year={2020},
      eprint={1905.11881},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{DBLP:journals/corr/ZagoruykoK16,
  author    = {Sergey Zagoruyko and
               Nikos Komodakis},
  title     = {Wide Residual Networks},
  journal   = {CoRR},
  volume    = {abs/1605.07146},
  year      = {2016},
  archivePrefix = {arXiv},
  eprint    = {1605.07146},
  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{encryptGrads,
author = {Tang, Fengyi and Wu, Wei and Liu, Jian and Xian, Ming},
year = {2019},
month = {04},
pages = {411},
title = {Privacy-Preserving Distributed Deep Learning via Homomorphic Re-Encryption},
volume = {8},
journal = {Electronics},
doi = {10.3390/electronics8040411}
}

@misc{HEGrads,
      title={MYSTIKO : : Cloud-Mediated, Private, Federated Gradient Descent}, 
      author={K. R. Jayaram and Archit Verma and Ashish Verma and Gegi Thomas and Colin Sutcher-Shepard},
      year={2020},
      eprint={2012.00740},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}


############################## Non-IID Setting ###################################


@inproceedings{steinhardt2018resilience,
  title={Resilience: A Criterion for Learning in the Presence of Arbitrary Outliers},
  author={Steinhardt, Jacob and Charikar, Moses and Valiant, Gregory},
  booktitle={9th Innovations in Theoretical Computer Science Conference (ITCS 2018)},
  year={2018},
  organization={Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik}
}

@inproceedings{karimireddy2020scaffold,
  title={Scaffold: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International Conference on Machine Learning},
  pages={5132--5143},
  year={2020},
  organization={PMLR}
}

############################## OTHER FILTERS and Fault-Tolerance Works ###################################
@inproceedings{bhatia2015robust,
author = {Bhatia, Kush and Jain, Prateek and Kar, Purushottam},
title = {Robust Regression via Hard Thresholding},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the problem of Robust Least Squares Regression (RLSR) where several response variables can be adversarially corrupted. More specifically, for a data matrix X ∈ ℝp x n and an underlying model w*, the response vector is generated as y = XT w* + b where b ∈ ℝn is the corruption vector supported over at most C · n coordinates. Existing exact recovery results for RLSR focus solely on L1-penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of X.In this work, we study a simple hard-thresholding algorithm called TORRENT which, under mild conditions on X, can recover w* exactly even if b corrupts the response variables in an adversarial manner, i.e. both the support and entries of b are selected adversarially after observing X and w*. Our results hold under deterministic assumptions which are satisfied if X is sampled from any sub-Gaussian distribution. Finally unlike existing results that apply only to a fixed w*, generated independently of X, our results are universal and hold for any w* ∈ ℝp.Next, we propose gradient descent-based extensions of TORRENT that can scale efficiently to large scale problems, such as high dimensional sparse recovery. and prove similar recovery guarantees for these extensions. Empirically we find TORRENT, and more so its extensions, offering significantly faster recovery than the state-of-the-art L1 solvers. For instance, even on moderate-sized datasets (with p = 50K) with around 40% corrupted responses, a variant of our proposed method called TORRENT-HYB is more than 20 x faster than the best L1 solver.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {721–729},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@article{su2019finite,
  title={Finite-time guarantees for {B}yzantine-resilient distributed state estimation with noisy measurements},
  author={Su, Lili and Shahrampour, Shahin},
  journal={IEEE Transactions on Automatic Control},
  volume={65},
  number={9},
  pages={3758--3771},
  year={2019},
  publisher={IEEE}
}

@article{yang2019byrdie,
  title={ByRDiE: {B}yzantine-resilient distributed coordinate descent for decentralized learning},
  author={Yang, Zhixiong and Bajwa, Waheed U},
  journal={IEEE Transactions on Signal and Information Processing over Networks},
  volume={5},
  number={4},
  pages={611--627},
  year={2019},
  publisher={IEEE}
}

@inproceedings{gupta2020fault,
  title={Fault-tolerance in distributed optimization: The case of redundancy},
  author={Gupta, Nirupam and Vaidya, Nitin H},
  booktitle={Proceedings of the 39th Symposium on Principles of Distributed Computing},
  pages={365--374},
  year={2020}
}

@inproceedings{liu2021approximate,
author = {Liu, Shuo and Gupta, Nirupam and Vaidya, Nitin H.},
title = {Approximate {B}yzantine Fault-Tolerance in Distributed Optimization},
year = {2021},
isbn = {9781450385480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3465084.3467902},
abstract = {This paper considers the problem of {B}yzantine fault-tolerance in distributed multi-agent optimization. In this problem, each agent has a local cost function, and in the fault-free case, the goal is to design a distributed algorithm that allows all the agents to find a minimum point of all the agents' aggregate cost function. We consider a scenario where some agents might be {B}yzantine faulty that renders the original goal of computing a minimum point of all the agents' aggregate cost vacuous. A more reasonable objective for an algorithm in this scenario is to allow all the non-faulty agents to compute the minimum point of only the non-faulty agents' aggregate cost. Prior work shows that if there are up to f (out of n) {B}yzantine agents then a minimum point of the non-faulty agents' aggregate cost can be computed exactly if and only if the non-faulty agents' costs satisfy a certain redundancy property called 2f-redundancy. However, 2f-redundancy is an ideal property that can be satisfied only in systems free from noise or uncertainties, which can make the goal of exact fault-tolerance unachievable in some applications. Thus, we introduce the notion of (f,ε)-resilience, a generalization of exact fault-tolerance wherein the objective is to find an approximate minimum point of the non-faulty aggregate cost, with ε accuracy. This approximate fault-tolerance can be achieved under a weaker condition that is easier to satisfy in practice, compared to 2f-redundancy. We obtain necessary and sufficient conditions for achieving (f, ε)-resilience characterizing the correlation between relaxation in redundancy and approximation in resilience. In case when the agents' cost functions are differentiable, we obtain conditions for (f, ε)-resilience of the distributed gradient-descent method when equipped with robust gradient aggregation; such as comparative gradient elimination or coordinate-wise trimmed mean.},
booktitle = {Proceedings of the 2021 ACM Symposium on Principles of Distributed Computing},
pages = {379–389},
numpages = {11},
keywords = {distributed optimization, approximate fault-tolerance, distributed gradient-descent},
location = {Virtual Event, Italy},
series = {PODC'21}
}

@inproceedings{gupta2021byzantine,
  title={{B}yzantine Fault-Tolerant Distributed Machine Learning with Norm-Based Comparative Gradient Elimination},
  author={Gupta, Nirupam and Liu, Shuo and Vaidya, Nitin},
  booktitle={2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)},
  pages={175--181},
  year={2021},
  organization={IEEE}
}

@inproceedings{allen2020byzantine,
  title={{B}yzantine-Resilient Non-Convex Stochastic Gradient Descent},
  author={Allen-Zhu, Zeyuan and Ebrahimianghazani, Faeze and Li, Jerry and Alistarh, Dan},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{prasad2020robust,
  title={Robust estimation via robust gradient estimation},
  author={Prasad, Adarsh and Suggala, Arun Sai and Balakrishnan, Sivaraman and Ravikumar, Pradeep},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={82},
  number={3},
  pages={601--627},
  year={2020},
  publisher={Wiley Online Library}
}

@inproceedings{diakonikolas2019sever,
  title={Sever: A robust meta-algorithm for stochastic optimization},
  author={Diakonikolas, Ilias and Kamath, Gautam and Kane, Daniel and Li, Jerry and Steinhardt, Jacob and Stewart, Alistair},
  booktitle={International Conference on Machine Learning},
  pages={1596--1606},
  year={2019},
  organization={PMLR}
}

@article{bernstein2018signsgd,
  title={signSGD with majority vote is communication efficient and fault tolerant},
  author={Bernstein, Jeremy and Zhao, Jiawei and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1810.05291},
  year={2018}
}

@inproceedings{alistarh2018byzantine,
  title={{B}yzantine stochastic gradient descent},
  author={Alistarh, Dan and Allen-Zhu, Zeyuan and Li, Jerry},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={4618--4628},
  year={2018}
}

@article{gupta2019randomized,
  title={Randomized Reactive Redundancy for {B}yzantine Fault-Tolerance in Parallelized Learning},
  author={Gupta, Nirupam and Vaidya, Nitin H},
  journal={arXiv preprint arXiv:1912.09528},
  year={2019}
}

@inproceedings{data2021byzantine,
  title = 	 {{B}yzantine-Resilient High-Dimensional SGD with Local Iterations on Heterogeneous Data},
  author =       {Data, Deepesh and Diggavi, Suhas},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2478--2488},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/data21a/data21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/data21a.html}
}


@article{data2020data,
  title={Data encoding for {B}yzantine-resilient distributed optimization},
  author={Data, Deepesh and Song, Linqi and Diggavi, Suhas N},
  journal={IEEE Transactions on Information Theory},
  volume={67},
  number={2},
  pages={1117--1140},
  year={2020},
  publisher={IEEE}
}

@inproceedings{charikar2017learning,
  title={Learning from untrusted data},
  author={Charikar, Moses and Steinhardt, Jacob and Valiant, Gregory},
  booktitle={Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing},
  pages={47--60},
  year={2017}
}
################################# ATTACKS ######################################
@inproceedings{empire,
  author    = {Cong Xie and
               Oluwasanmi Koyejo and
               Indranil Gupta},
  title     = {Fall of Empires: Breaking {B}yzantine-tolerant {SGD} by Inner Product
               Manipulation},
  booktitle = {Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial
               Intelligence, {UAI} 2019, Tel Aviv, Israel, July 22-25, 2019},
  pages     = {83},
  year      = {2019},
  timestamp = {Fri, 19 Jul 2019 13:05:12 +0200},
}

@inproceedings{little,
  author    = {Moran Baruch and
               Gilad Baruch and
               Yoav Goldberg},
  title     = {A Little Is Enough: Circumventing Defenses For Distributed Learning},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, 8-14 December 2019,
               Long Beach, CA, {USA}},
  year      = {2019},
}

@InProceedings{pmlr-v28-sutskever13, 
title = {On the importance of initialization and momentum in deep learning}, 
author = {Ilya Sutskever and James Martens and George Dahl and Geoffrey Hinton}, 
booktitle = {Proceedings of the 30th International Conference on Machine Learning}, 
pages = {1139--1147}, 
year = {2013}, 
editor = {Sanjoy Dasgupta and David McAllester}, 
volume = {28}, 
number = {3}, 
series = {Proceedings of Machine Learning Research}, 
address = {Atlanta, Georgia, USA}, 
month = {17--19 Jun}, 
publisher = {PMLR}, 
pdf = {http://proceedings.mlr.press/v28/sutskever13.pdf}, 
abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods. } 
}

@article{momentum,
  author = {Polyak, Boris},
  year = {1964},
  month = {12},
  pages = {1-17},
  title = {Some methods of speeding up the convergence of iteration methods},
  volume = {4},
  journal = {USSR Computational Mathematics and Mathematical Physics},
  doi = {10.1016/0041-5553(64)90137-5}
}

@article{cutkosky2019momentum,
  title={Momentum-Based Variance Reduction in Non-Convex SGD},
  author={Cutkosky, Ashok and Orabona, Francesco},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={15236--15245},
  year={2019}
}

# (As support that two state-of-the-art attacks)
@inproceedings{distributed-momentum,
  author    = {El Mhamdi, El Mahdi and
               Guerraoui, Rachid and
              Rouault, S\'{e}bastien},
  title     = {Distributed Momentum for {B}yzantine-resilient Stochastic Gradient Descent},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Vienna, Austria, May 4–8, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
}


@inproceedings{redundancy,
  author    = {Lingjiao Chen and
               Hongyi Wang and
               Zachary B. Charles and
               Dimitris S. Papailiopoulos},
  editor    = {Jennifer G. Dy and
               Andreas Krause},
  title     = {{DRACO:} {B}yzantine-resilient Distributed Training via
Redundant Gradients},
  booktitle = {Proceedings of the 35th International Conference on
Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden,
July
               10-15, 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {902--911},
  publisher = {{PMLR}},
  year      = {2018},
}

@inproceedings{rajput2019detox,
  title={DETOX: A Redundancy-based Framework for Faster and More Robust Gradient Aggregation},
  author={Rajput, Shashank and Wang, Hongyi and Charles, Zachary and Papailiopoulos, Dimitris},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@inproceedings{suspicion,
  author    = {Cong Xie and
               Sanmi Koyejo and
               Indranil Gupta},
  editor    = {Kamalika Chaudhuri and
               Ruslan Salakhutdinov},
  title     = {Zeno: Distributed Stochastic Gradient Descent with
Suspicion-based
               Fault-tolerance},
  booktitle = {Proceedings of the 36th International Conference on
Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California,
{USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {6893--6901},
  publisher = {{PMLR}},
  year      = {2019},
}

@inproceedings{xie2020zeno++,
  title={Zeno++: Robust fully asynchronous SGD},
  author={Xie, Cong and Koyejo, Sanmi and Gupta, Indranil},
  booktitle={International Conference on Machine Learning},
  pages={10495--10503},
  year={2020},
  organization={PMLR}
}

@article{yao2019federated,
  title={Federated learning with unbiased gradient aggregation and controllable meta updating},
  author={Yao, Xin and Huang, Tianchi and Zhang, Rui-Xiao and Li, Ruiyu and Sun, Lifeng},
  journal={arXiv preprint arXiv:1910.08234},
  year={2019}
}

@inproceedings{gupta2019byzantine,
  title={{B}yzantine fault-tolerant parallelized stochastic gradient descent for linear regression},
  author={Gupta, Nirupam and Vaidya, Nitin H},
  booktitle={2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={415--420},
  year={2019},
  organization={IEEE}
}

@article{diakonikolas2019robust,
  title={Robust estimators in high-dimensions without the computational intractability},
  author={Diakonikolas, Ilias and Kamath, Gautam and Kane, Daniel and Li, Jerry and Moitra, Ankur and Stewart, Alistair},
  journal={SIAM Journal on Computing},
  volume={48},
  number={2},
  pages={742--864},
  year={2019},
  publisher={SIAM}
}

@inproceedings{yin2019defending,
  title={Defending against saddle point attack in {B}yzantine-robust distributed learning},
  author={Yin, Dong and Chen, Yudong and Kannan, Ramchandran and Bartlett, Peter},
  booktitle={International Conference on Machine Learning},
  pages={7074--7084},
  year={2019},
  organization={PMLR}
}

@article{cao2019distributed,
  title={Distributed gradient descent algorithm robust to an arbitrary number of {B}yzantine attackers},
  author={Cao, Xinyang and Lai, Lifeng},
  journal={IEEE Transactions on Signal Processing},
  volume={67},
  number={22},
  pages={5850--5864},
  year={2019},
  publisher={IEEE}
}

@inproceedings{ji2019learning,
  title={Learning to Learn Gradient Aggregation by Gradient Descent.},
  author={Ji, Jinlong and Chen, Xuhui and Wang, Qianlong and Yu, Lixing and Li, Pan},
  booktitle={IJCAI},
  pages={2614--2620},
  year={2019}
}

@inproceedings{li2019rsa,
  title={{RSA}: {B}yzantine-robust stochastic aggregation methods for distributed learning from heterogeneous datasets},
  author={Li, Liping and Xu, Wei and Chen, Tianyi and Giannakis, Georgios B and Ling, Qing},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={1544--1551},
  year={2019}
}

@article{regatti2020bygars,
  title={{ByGARS}: {B}yzantine SGD with Arbitrary Number of Attackers},
  author={Regatti, Jayanth and Chen, Hao and Gupta, Abhishek},
  journal={arXiv preprint arXiv:2006.13421},
  year={2020}
}

@inproceedings{boussetta2021aksel,
  title={{AKSEL}: Fast {B}yzantine SGD},
  author={Boussetta, Amine and El Mhamdi, El Mahdi and Guerraoui, Rachid and Maurer, Alexandre and Rouault, S{\'e}bastien},
  booktitle={24th International Conference on Principles of Distributed Systems (OPODIS 2020)},
  year={2021},
  organization={Schloss Dagstuhl-Leibniz-Zentrum f{\"u}r Informatik}
}

@misc{amplification,
      title={Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity}, 
      author={Úlfar Erlingsson and Vitaly Feldman and Ilya Mironov and Ananth Raghunathan and Kunal Talwar and Abhradeep Thakurta},
      year={2020},
      eprint={1811.12469},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{renyi,
   title={Rényi Differential Privacy},
   ISBN={9781538632178},
   DOI={10.1109/csf.2017.11},
   journal={2017 IEEE 30th Computer Security Foundations Symposium (CSF)},
   publisher={IEEE},
   author={Mironov, Ilya},
   year={2017},
   month={Aug}
}

@techreport{renyiDiv,
  title={On measures of entropy and information},
  author={R{\'e}nyi, Alfr{\'e}d},
  year={1961},
  institution={Hungarian Academy of Sciences Budapest Hungary}
}

@article{6832827,
  author={T. {van Erven} and P. {Harremos}},
  journal={IEEE Transactions on Information Theory}, 
  title={R\'enyi Divergence and Kullback-Leibler Divergence}, 
  year={2014},
  volume={60},
  number={7},
  pages={3797-3820}
 }
 
 @article{KiferM14,
  author    = {Daniel Kifer and
               Ashwin Machanavajjhala},
  title     = {Pufferfish: {A} framework for mathematical privacy definitions},
  journal   = {{ACM} Trans. Database Syst.},
  volume    = {39},
  number    = {1},
  pages     = {3:1--3:36},
  year      = {2014},
  doi       = {10.1145/2514689},
  timestamp = {Tue, 06 Nov 2018 12:51:47 +0100},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{POLYAK19641,
title = {Some methods of speeding up the convergence of iteration methods},
journal = {USSR Computational Mathematics and Mathematical Physics},
volume = {4},
number = {5},
pages = {1-17},
year = {1964},
issn = {0041-5553},
doi = {https://doi.org/10.1016/0041-5553(64)90137-5},
author = {B.T. Polyak},
abstract = {For the solution of the functional equation P (x) = 0 (1) (where P is an operator, usually linear, from B into B, and B is a Banach space) iteration methods are generally used. These consist of the construction of a series x0, …, xn, …, which converges to the solution (see, for example [1]). Continuous analogues of these methods are also known, in which a trajectory x(t), 0 ⩽ t ⩽ ∞ is constructed, which satisfies the ordinary differential equation in B and is such that x(t) approaches the solution of (1) as t → ∞ (see [2]). We shall call the method a k-step method if for the construction of each successive iteration xn+1 we use k previous iterations xn, …, xn−k+1. The same term will also be used for continuous methods if x(t) satisfies a differential equation of the k-th order or k-th degree. Iteration methods which are more widely used are one-step (e.g. methods of successive approximations). They are generally simple from the calculation point of view but often converge very slowly. This is confirmed both by the evaluation of the speed of convergence and by calculation in practice (for more details see below). Therefore the question of the rate of convergence is most important. Some multistep methods, which we shall consider further, which are only slightly more complicated than the corresponding one-step methods, make it possible to speed up the convergence substantially. Note that all the methods mentioned below are applicable also to the problem of minimizing the differentiable functional (x) in Hilbert space, so long as this problem reduces to the solution of the equation grad (x) = 0.}
}



#### Distributed systems

@inproceedings{DistributedNetworks2012,
 author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc\textquotesingle aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc and Ng, Andrew},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Large Scale Distributed Deep Networks},
 volume = {25},
 year = {2012}
}

@misc{Tensorflow2015,
title	= {TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems},
author	= {Martín Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Greg Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Yangqing Jia and Rafal Jozefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dan Mané and Rajat Monga and Sherry Moore and Derek Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda Viégas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
year	= {2015},
}

@inproceedings{podc,
author = {Guerraoui, Rachid and Gupta, Nirupam and Pinot, Rafa\"{e}l and Rouault, S\'{e}bastien and Stephan, John},
title = {Differential Privacy and {B}yzantine Resilience in SGD: Do They Add Up?},
year = {2021},
isbn = {9781450385480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465084.3467919},
doi = {10.1145/3465084.3467919},
abstract = {This paper addresses the problem of combining {B}yzantine resilience with privacy in machine learning (ML). Specifically, we study if a distributed implementation of the renowned Stochastic Gradient Descent (SGD) learning algorithm is feasible withboth differential privacy (DP) and (α,f)-{B}yzantine resilience. To the best of our knowledge, this is the first work to tackle this problem from a theoretical point of view. A key finding of our analyses is that the classical approaches to these two (seemingly) orthogonal issues are incompatible. More precisely, we show that a direct composition of these techniques makes the guarantees of the resulting SGD algorithm depend unfavourably upon the number of parameters of the ML model, making the training of large models practically infeasible. We validate our theoretical results through numerical experiments on publicly-available datasets; showing that it is impractical to ensure DP and {B}yzantine resilience simultaneously.},
booktitle = {Proceedings of the 2021 ACM Symposium on Principles of Distributed Computing},
pages = {391–401},
numpages = {11},
keywords = {sgd, machine learning, differential privacy, {B}yzantine resilience},
location = {Virtual Event, Italy},
series = {PODC'21}
}

@INPROCEEDINGS{PrivacyPreservingDeep2015, author={Shokri, Reza and Shmatikov, Vitaly}, booktitle={2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, title={Privacy-preserving deep learning}, year={2015}, volume={}, number={}, pages={909-910}, doi={10.1109/ALLERTON.2015.7447103}} 

@inproceedings{TrainingLargemodels2015,
 author = {Srivastava, Rupesh K and Greff, Klaus and Schmidhuber, J\"{u}rgen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Training Very Deep Networks},
 volume = {28},
 year = {2015}
}

@InProceedings{Wang2019RDPMomentAcc,
  title = 	 {Subsampled {Rényi} Differential Privacy and Analytical Moments Accountant},
  author =       {Wang, Yu-Xiang and Balle, Borja and Kasiviswanathan, Shiva Prasad},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1226--1235},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/wang19b/wang19b.pdf},
  abstract = 	 {We study the problem of subsampling in differential privacy (DP), a question that is the centerpiece behind many successful differentially private machine learning algorithms.  Specifically, we provide a tight upper bound on the Renyi Differential Privacy (RDP) [Mironov 2017] parameters for algorithms that: (1) subsample the dataset, and then (2) applies a randomized mechanism M to the subsample, in terms of the RDP parameters of M and the subsampling probability parameter. Our results generalize the moments accounting technique, developed by [Abadi et al. 2016] for the Gaussian mechanism, to any subsampled RDP mechanism.}
}



@inproceedings{Balle2018Subsampling,
author = {Balle, Borja and Barthe, Gilles and Gaboardi, Marco},
title = {Privacy Amplification by Subsampling: Tight Analyses via Couplings and Divergences},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Differential privacy comes equipped with multiple analytical tools for the design of private data analyses. One important tool is the so-called "privacy amplification by subsampling" principle, which ensures that a differentially private mechanism run on a random subsample of a population provides higher privacy guarantees than when run on the entire population. Several instances of this principle have been studied for different random subsampling methods, each with an ad-hoc analysis. In this paper we present a general method that recovers and improves prior analyses, yields lower bounds and derives new instances of privacy amplification by subsampling. Our method leverages a characterization of differential privacy as a divergence which emerged in the program verification community. Furthermore, it introduces new tools, including advanced joint convexity and privacy profiles, which might be of independent interest.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {6280–6290},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

  


@inproceedings{Beimel2013SampleComplexity,
author = {Beimel, Amos and Nissim, Kobbi and Stemmer, Uri},
title = {Characterizing the Sample Complexity of Private Learners},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2422436.2422450},
abstract = {In 2008, Kasiviswanathan el al. defined private learning as a combination of PAC learning and differential privacy [16]. Informally, a private learner is applied to a collection of labeled individual information and outputs a hypothesis while preserving the privacy of each individual. Kasiviswanathan et al. gave a generic construction of private learners for (finite) concept classes, with sample complexity logarithmic in the size of the concept class. This sample complexity is higher than what is needed for non-private learners, hence leaving open the possibility that the sample complexity of private learning may be sometimes significantly higher than that of non-private learning. We give a combinatorial characterization of the sample size sufficient and necessary to privately learn a class of concepts. This characterization is analogous to the well known characterization of the sample complexity of non-private learning in terms of the VC dimension of the concept class. We introduce the notion of probabilistic representation of a concept class, and our new complexity measure RepDim corresponds to the size of the smallest probabilistic representation of the concept class. We show that any private learning algorithm for a concept class C with sample complexity m implies RepDim(C) = O(m), and that there exists a private learning algorithm with sample complexity m = O(RepDim(C)).We further demonstrate that a similar characterization holds for the database size needed for privately computing a large class of optimization problems and also for the well studied problem of private data release.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {97–110},
numpages = {14},
keywords = {pac learning, probabilistic representation, differential privacy, sample complexity},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

  

####################### clipping ############################

@article{chen2020understanding,
  title={Understanding gradient clipping in private SGD: A geometric perspective},
  author={Chen, Xiangyi and Wu, Steven Z and Hong, Mingyi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{bu2021convergence,
  title={On the Convergence of Deep Learning with Differential Privacy},
  author={Bu, Zhiqi and Wang, Hua and Long, Qi and Su, Weijie J},
  journal={arXiv preprint arXiv:2106.07830},
  year={2021}
}

@article{mai2021stability,
  title={Stability and Convergence of Stochastic Gradient Clipping: Beyond Lipschitz Continuity and Smoothness},
  author={Mai, Vien V and Johansson, Mikael},
  journal={arXiv preprint arXiv:2102.06489},
  year={2021}
}

@inproceedings{qian2021understanding,
  title={Understanding Gradient Clipping In Incremental Gradient Methods},
  author={Qian, Jiang and Wu, Yuren and Zhuang, Bojin and Wang, Shaojun and Xiao, Jing and others},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1504--1512},
  year={2021},
  organization={PMLR}
}

@misc{TSPrivacy, 
  title = "Tensorflow Privacy Library", 
  howpublished = "Available from \href{https://github.com/tensorflow/privacy}{TFPrivacy}",
  key = "Google"
}

@inproceedings{DBLP:conf/sp/LecuyerAG0J19,
  author    = {Mathias L{\'{e}}cuyer and
               Vaggelis Atlidakis and
               Roxana Geambasu and
               Daniel Hsu and
               Suman Jana},
  title     = {Certified Robustness to Adversarial Examples with Differential Privacy},
  booktitle = {2019 {IEEE} Symposium on Security and Privacy, {SP} 2019, San Francisco,
               CA, USA, May 19-23, 2019},
  pages     = {656--672},
  publisher = {{IEEE}},
  year      = {2019},
  doi       = {10.1109/SP.2019.00044},
  timestamp = {Wed, 16 Oct 2019 14:14:51 +0200}
}


@article{pinot2019unified,
  title={A unified view on differential privacy and robustness to adversarial examples},
  author={Pinot, Rafael and Yger, Florian and Gouy-Pailler, C{\'e}dric and Atif, Jamal},
  journal={arXiv preprint arXiv:1906.07982},
  year={2019}
}

@inproceedings{ShokriPrivacyRiskofAdversarialDefense2019,
author = {Song, Liwei and Shokri, Reza and Mittal, Prateek},
title = {Privacy Risks of Securing Machine Learning Models against Adversarial Examples},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3319535.3354211},
abstract = {The arms race between attacks and defenses for machine learning models has come to
a forefront in recent years, in both the security community and the privacy community.
However, one big limitation of previous research is that the security domain and the
privacy domain have typically been considered separately. It is thus unclear whether
the defense methods in one domain will have any unexpected impact on the other domain.
In this paper, we take a step towards resolving this limitation by combining the two
domains. In particular, we measure the success of membership inference attacks against
six state-of-the-art defense methods that mitigate the risk of adversarial examples
(i.e., evasion attacks). Membership inference attacks determine whether or not an
individual data record has been part of a model's training set. The accuracy of such
attacks reflects the information leakage of training algorithms about individual members
of the training set. Adversarial defense methods against adversarial examples influence
the model's decision boundaries such that model predictions remain unchanged for a
small area around each input. However, this objective is optimized on training data.
Thus, individual data records in the training set have a significant influence on
robust models. This makes the models more vulnerable to inference attacks. To perform
the membership inference attacks, we leverage the existing inference methods that
exploit model predictions. We also propose two new inference methods that exploit
structural properties of robust models on adversarially perturbed data. Our experimental
evaluation demonstrates that compared with the natural training (undefended) approach,
adversarial defense methods can indeed increase the target model's risk against membership
inference attacks. When using adversarial defenses to train the robust models, the
membership inference advantage increases by up to 4.5 times compared to the naturally
undefended models. Beyond revealing the privacy risks of adversarial defenses, we
further investigate the factors, such as model capacity, that influence the membership
information leakage.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {241–257},
numpages = {17},
keywords = {machine learning, membership inference attacks, adversarial examples and defenses},
location = {London, United Kingdom},
series = {CCS '19}
}

  

@inproceedings{MembershipInferenceinRobustnessShokri2019,
  author    = {Liwei Song and
               Reza Shokri and
               Prateek Mittal},
  title     = {Membership Inference Attacks Against Adversarially Robust Deep Learning
               Models},
  booktitle = {2019 {IEEE} Security and Privacy Workshops, {SP} Workshops 2019, San
               Francisco, CA, USA, May 19-23, 2019},
  pages     = {50--56},
  publisher = {{IEEE}},
  year      = {2019},
  doi       = {10.1109/SPW.2019.00021},
  timestamp = {Wed, 16 Oct 2019 14:14:51 +0200},
}


@inproceedings{MaPoisoningandDP2019,
  title     = {Data Poisoning against Differentially-Private Learners: Attacks and Defenses},
  author    = {Ma, Yuzhe and Zhu, Xiaojin and Hsu, Justin},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {4732--4738},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/657},
  url       = {https://doi.org/10.24963/ijcai.2019/657},
}

@inproceedings{WangSRVASLP20YesYouReallyAttackBackdoor,
  author={Hongyi Wang and Kartik Sreenivasan and Shashank Rajput and Harit Vishwakarma and Saurabh Agarwal and Jy-yong Sohn and Kangwook Lee and Dimitris S. Papailiopoulos},
  title={Attack of the Tails: Yes, You Really Can Backdoor Federated Learning},
  year={2020},
  cdate={1577836800000},
  url={https://proceedings.neurips.cc/paper/2020/hash/b8ffa41d4e492f0fad2f13e29e1762eb-Abstract.html},
  booktitle={NeurIPS},
}


@article{Sun2019CanYouBackdoor,
  author    = {Ziteng Sun and
               Peter Kairouz and
               Ananda Theertha Suresh and
               H. Brendan McMahan},
  title     = {Can You Really Backdoor Federated Learning?},
  journal   = {CoRR},
  volume    = {abs/1911.07963},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.07963},
  eprinttype = {arXiv},
  eprint    = {1911.07963},
  timestamp = {Mon, 02 Dec 2019 17:48:37 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-07963.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DworkRobustStats2009,
author = {Dwork, Cynthia and Lei, Jing},
title = {Differential Privacy and Robust Statistics},
year = {2009},
isbn = {9781605585062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1536414.1536466},
doi = {10.1145/1536414.1536466},
abstract = {We show by means of several examples that robust statistical estimators present an
excellent starting point for differentially private estimators. Our algorithms use
a new paradigm for differentially private mechanisms, which we call Propose-Test-Release
(PTR), and for which we give a formal definition and general composition theorems.},
booktitle = {Proceedings of the Forty-First Annual ACM Symposium on Theory of Computing},
pages = {371–380},
numpages = {10},
keywords = {differential privacy, local sensitivity, propose-test-release paradigm, robust statistics},
location = {Bethesda, MD, USA},
series = {STOC '09}
}

  
@INPROCEEDINGS {9519424,
author = {M. Nasr and S. Songi and A. Thakurta and N. Papemoti and N. Carlin},
booktitle = {2021 2021 IEEE Symposium on Security and Privacy (SP)},
title = {Adversary Instantiation: Lower Bounds for Differentially Private Machine Learning},
year = {2021},
volume = {},
issn = {},
pages = {866-882},
keywords = {training;deep learning;privacy;differential privacy;upper bound;toxicology;games},
doi = {10.1109/SP40001.2021.00069},
url = {https://doi.ieeecomputersociety.org/10.1109/SP40001.2021.00069},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {may}
}

@inproceedings{MelisSCS19GradientLeakage,
  author    = {Luca Melis and
               Congzheng Song and
               Emiliano De Cristofaro and
               Vitaly Shmatikov},
  title     = {Exploiting Unintended Feature Leakage in Collaborative Learning},
  booktitle = {2019 {IEEE} Symposium on Security and Privacy, {SP} 2019, San Francisco,
               CA, USA, May 19-23, 2019},
  pages     = {691--706},
  publisher = {{IEEE}},
  year      = {2019},
  url       = {https://doi.org/10.1109/SP.2019.00029},
  doi       = {10.1109/SP.2019.00029},
  timestamp = {Wed, 16 Oct 2019 14:14:51 +0200},
  biburl    = {https://dblp.org/rec/conf/sp/MelisSCS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Hitaj2017PrivacyLeakageFromCollLearning,
author = {Hitaj, Briland and Ateniese, Giuseppe and Perez-Cruz, Fernando},
title = {Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning},
year = {2017},
isbn = {9781450349468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3133956.3134012},
doi = {10.1145/3133956.3134012},
abstract = {Deep Learning has recently become hugely popular in machine learning for its ability
to solve end-to-end learning systems, in which the features and the classifiers are
learned simultaneously, providing significant improvements in classification accuracy
in the presence of highly-structured and large databases.Its success is due to a combination
of recent algorithmic breakthroughs, increasingly powerful computers, and access to
significant amounts of data.Researchers have also considered privacy implications
of deep learning. Models are typically trained in a centralized manner with all the
data being processed by the same training algorithm. If the data is a collection of
users' private data, including habits, personal pictures, geographical positions,
interests, and more, the centralized server will have access to sensitive information
that could potentially be mishandled. To tackle this problem, collaborative deep learning
models have recently been proposed where parties locally train their deep learning
structures and only share a subset of the parameters in the attempt to keep their
respective training sets private. Parameters can also be obfuscated via differential
privacy (DP) to make information extraction even more challenging, as proposed by
Shokri and Shmatikov at CCS'15.Unfortunately, we show that any privacy-preserving
collaborative deep learning is susceptible to a powerful attack that we devise in
this paper. In particular, we show that a distributed, federated, or decentralized
deep learning approach is fundamentally broken and does not protect the training sets
of honest participants. The attack we developed exploits the real-time nature of the
learning process that allows the adversary to train a Generative Adversarial Network
(GAN) that generates prototypical samples of the targeted training set that was meant
to be private (the samples generated by the GAN are intended to come from the same
distribution as the training data). Interestingly, we show that record-level differential
privacy applied to the shared parameters of the model, as suggested in previous work,
is ineffective (i.e., record-level DP is not designed to address our attack).},
booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
pages = {603–618},
numpages = {16},
keywords = {collaborative learning, security, privacy, deep learning},
location = {Dallas, Texas, USA},
series = {CCS '17}
}


@inproceedings{Fredrikson2015ModelInversion,
author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
title = {Model Inversion Attacks That Exploit Confidence Information and Basic Countermeasures},
year = {2015},
isbn = {9781450338325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810103.2813677},
doi = {10.1145/2810103.2813677},
abstract = {Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications
such as predicting lifestyle choices, making medical diagnoses, and facial recognition.
In a model inversion attack, recently introduced in a case study of linear classifiers
in personalized medicine by Fredrikson et al., adversarial access to an ML model is
abused to learn sensitive genomic information about individuals. Whether model inversion
attacks apply to settings outside theirs, however, is unknown. We develop a new class
of model inversion attack that exploits confidence values revealed along with predictions.
Our new attacks are applicable in a variety of settings, and we explore two in depth:
decision trees for lifestyle surveys as used on machine-learning-as-a-service systems
and neural networks for facial recognition. In both cases confidence values are revealed
to those with the ability to make prediction queries to models. We experimentally
show attacks that are able to estimate whether a respondent in a lifestyle survey
admitted to cheating on their significant other and, in the other context, show how
to recover recognizable images of people's faces given only their name and access
to the ML model. We also initiate experimental exploration of natural countermeasures,
investigating a privacy-aware decision tree training algorithm that is a simple variant
of CART learning, as well as revealing only rounded confidence values. The lesson
that emerges is that one can avoid these kinds of MI attacks with negligible degradation
to utility.},
booktitle = {Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
pages = {1322–1333},
numpages = {12},
keywords = {attacks, privacy, machine learning},
location = {Denver, Colorado, USA},
series = {CCS '15}
}

  
@article{zhao2020idlg,
  title={idlg: Improved deep leakage from gradients},
  author={Zhao, Bo and Mopuri, Konda Reddy and Bilen, Hakan},
  journal={arXiv preprint arXiv:2001.02610},
  year={2020}
}

@INPROCEEDINGS{DistributedLabelLeakage2021,
  author={Wainakh, Aidmar and Müßig, Till and Grube, Tim and Mühlhäuser, Max},
  booktitle={2021 IEEE 18th Annual Consumer Communications   Networking Conference (CCNC)}, 
  title={Label Leakage from Gradients in Distributed Machine Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1-4},
  doi={10.1109/CCNC49032.2021.9369498}}

@InProceedings{PhongPPDeepLearning2017,
author="Phong, Le Trieu
and Aono, Yoshinori
and Hayashi, Takuya
and Wang, Lihua
and Moriai, Shiho",
editor="Batten, Lynn
and Kim, Dong Seong
and Zhang, Xuyun
and Li, Gang",
title="Privacy-Preserving Deep Learning: Revisited and Enhanced",
booktitle="Applications and Techniques in Information Security",
year="2017",
publisher="Springer Singapore",
address="Singapore",
pages="100--110",
abstract="We build a privacy-preserving deep learning system in which many learning participants perform neural network-based deep learning over a combined dataset of all, without actually revealing the participants' local data to a curious server. To that end, we revisit the previous work by Shokri and Shmatikov (ACM CCS 2015) and point out that local data information may be actually leaked to an honest-but-curious server. We then move on to fix that problem via building an enhanced system with following properties: (1) no information is leaked to the server; and (2) accuracy is kept intact, compared to that of the ordinary deep learning system also over the combined dataset. Our system makes use of additively homomorphic encryption, and we show that our usage of encryption adds little overhead to the ordinary deep learning system.",
isbn="978-981-10-5421-1"
}

@inproceedings{WangUserLevelPrivacyLeakage2019,
author = {Wang, Zhibo and Mengkai, Song and Zhang, Zhifei and Song, Yang and Wang, Qian and Qi, Hairong},
year = {2019},
month = {04},
pages = {2512-2520},
title = {Beyond Inferring Class Representatives: User-Level Privacy Leakage From Federated Learning},
doi = {10.1109/INFOCOM.2019.8737416}
}


@article{ChenDistribStatML2017,
author = {Chen, Yudong and Su, Lili and Xu, Jiaming},
title = {Distributed Statistical Machine Learning in Adversarial Settings: {B}yzantine Gradient Descent},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3154503},
doi = {10.1145/3154503},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {dec},
articleno = {44},
numpages = {25},
keywords = {security, {B}yzantine adversaries, distributed systems, learning},
}

@misc{pillutla2019robust,
      title={Robust Aggregation for Federated Learning}, 
      author={Krishna Pillutla and Sham M. Kakade and Zaid Harchaoui},
      year={2019},
      eprint={1912.13445},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
 

@inproceedings{ishai2003extending,
  title={Extending oblivious transfers efficiently},
  author={Ishai, Yuval and Kilian, Joe and Nissim, Kobbi and Petrank, Erez},
  booktitle={Annual International Cryptology Conference},
  pages={145--161},
  year={2003},
  organization={Springer}
}

@article{bonawitz2016practical,
  title={Practical secure aggregation for federated learning on user-held data},
  author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  journal={arXiv preprint arXiv:1611.04482},
  year={2016}
}

@inproceedings{graepel2012ml,
  title={ML confidential: Machine learning on encrypted data},
  author={Graepel, Thore and Lauter, Kristin and Naehrig, Michael},
  booktitle={International Conference on Information Security and Cryptology},
  pages={1--21},
  year={2012},
  organization={Springer}
}


@InCollection{Dwork_2006,
  author    = {Cynthia Dwork and Frank McSherry and Kobbi Nissim and Adam Smith},
  title     = {Calibrating Noise to Sensitivity in Private Data Analysis},
  booktitle = {Theory of Cryptography},
  publisher = {Springer Berlin Heidelberg},
  year      = {2006},
  pages     = {265--284},
  doi       = {10.1007/11681878_14},
}


@misc{pillutla2019robust,
      title={Robust Aggregation for Federated Learning}, 
      author={Krishna Pillutla and Sham M. Kakade and Zaid Harchaoui},
      year={2019},
      eprint={1912.13445},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{OpenProblemsinFed2021,
year = {2021},
volume = {14},
journal = {Foundations and Trends® in Machine Learning},
title = {Advances and Open Problems in Federated Learning},
doi = {10.1561/2200000083},
issn = {1935-8237},
number = {1–2},
pages = {1-210},
author = {Peter Kairouz and H. Brendan McMahan and Brendan Avent and Aurélien Bellet and Mehdi Bennis and Arjun Nitin Bhagoji and Kallista Bonawitz and Zachary Charles and Graham Cormode and Rachel Cummings and Rafael G. L. D’Oliveira and Hubert Eichner and Salim El Rouayheb and David Evans and Josh Gardner and Zachary Garrett and Adrià Gascón and Badih Ghazi and Phillip B. Gibbons and Marco Gruteser and Zaid Harchaoui and Chaoyang He and Lie He and Zhouyuan Huo and Ben Hutchinson and Justin Hsu and Martin Jaggi and Tara Javidi and Gauri Joshi and Mikhail Khodak and Jakub Konecný and Aleksandra Korolova and Farinaz Koushanfar and Sanmi Koyejo and Tancrède Lepoint and Yang Liu and Prateek Mittal and Mehryar Mohri and Richard Nock and Ayfer Özgür and Rasmus Pagh and Hang Qi and Daniel Ramage and Ramesh Raskar and Mariana Raykova and Dawn Song and Weikang Song and Sebastian U. Stich and Ziteng Sun and Ananda Theertha Suresh and Florian Tramèr and Praneeth Vepakomma and Jianyu Wang and Li Xiong and Zheng Xu and Qiang Yang and Felix X. Yu and Han Yu and Sen Zhao}
}

@misc{feng2015distributed,
      title={Distributed Robust Learning}, 
      author={Jiashi Feng and Huan Xu and Shie Mannor},
      year={2015},
      eprint={1409.5937},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{feng2017onlinelearning,
  author    = {Jiashi Feng and
               Huan Xu and
               Shie Mannor},
  title     = {Outlier Robust Online Learning},
  journal   = {CoRR},
  volume    = {abs/1701.00251},
  year      = {2017},
  eprinttype = {arXiv},
  eprint    = {1701.00251},
  timestamp = {Mon, 13 Aug 2018 16:48:39 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Karimireddy2021,
      title = {Learning from History for {B}yzantine Robust Optimization},
      author = {Karimireddy, Sai Praneeth and He, Lie and Jaggi, Martin},
      publisher = {JMLR-JOURNAL MACHINE LEARNING RESEARCH},
      journal = {International Conference On Machine Learning, Vol 139},
      address = {San Diego},
      volume = {139},
      series = {Proceedings of Machine Learning Research},
      year = {2021},
      abstract = {{B}yzantine robustness has received significant attention  recently given its importance for distributed and federated  learning. In spite of this, we identify severe flaws in  existing algorithms even when the data across the  participants is identically distributed. First, we show  realistic examples where current state of the art robust  aggregation rules fail to converge even in the absence of  any {B}yzantine attackers. Secondly, we prove that even if  the aggregation rules may succeed in limiting the influence  of the attackers in a single round, the attackers can  couple their attacks across time eventually leading to  divergence. To address these issues, we present two  surprisingly simple strategies: a new robust iterative  clipping procedure, and incorporating worker momentum to  overcome time-coupled attacks. This is the first provably  robust method for the standard stochastic optimization  setting. Our code is open sourced at this link(2).},
}


@book{bertsekas2015parallel,
  title={Parallel and distributed computation: numerical methods},
  author={Bertsekas, Dimitri and Tsitsiklis, John},
  year={2015},
  publisher={Athena Scientific}
}

@inproceedings{
collaborativeElMhamdi21,
title={Collaborative Learning in the Jungle (Decentralized, {B}yzantine, Heterogeneous, Asynchronous and Nonconvex Learning)},
author={El Mhamdi, El Mahdi and Farhadkhani, Sadegh  and Guerraoui, Rachid  and Guirguis, Arsany  and Hoang, L{\^e} Nguy{\^e}n and Rouault, S{\'e}bastien },
booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
year={2021},
}

@article{Geomed21,
  author    = {El Mahdi El Mhamdi and
               Sadegh Farhadkhani and
               Rachid Guerraoui and
               L{\^{e}} Nguy{\^{e}}n Hoang},
  title     = {On the Strategyproofness of the Geometric Median},
  journal   = {CoRR},
  volume    = {abs/2106.02394},
  year      = {2021},
  eprinttype = {arXiv},
  eprint    = {2106.02394},
  timestamp = {Thu, 10 Jun 2021 16:34:18 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


################################## learning with corrupted data ###################################

@inproceedings{diakonikolas2017statistical,
  title={Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures},
  author={Diakonikolas, Ilias and Kane, Daniel M and Stewart, Alistair},
  booktitle={2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)},
  pages={73--84},
  year={2017},
  organization={IEEE}
}

 @techreport{gdpr,
  author ={EU},
  institution={European Parliament and European Council},
  title = {Regulation (EU) 2016/679 of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC},
  year = {2016},
}


@misc{opacus,
  doi = {10.48550/ARXIV.2109.12298},
  
  url = {https://arxiv.org/abs/2109.12298},
  
  author = {Yousefpour, Ashkan and Shilov, Igor and Sablayrolles, Alexandre and Testuggine, Davide and Prasad, Karthik and Malek, Mani and Nguyen, John and Ghosh, Sayan and Bharadwaj, Akash and Zhao, Jessica and Cormode, Graham and Mironov, Ilya},
  
  keywords = {Machine Learning (cs.LG), Cryptography and Security (cs.CR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Opacus: User-Friendly Differential Privacy Library in PyTorch},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@inproceedings{
papernot2018scalablePATE,
title={Scalable Private Learning with {PATE}},
author={Nicolas Papernot and Shuang Song and Ilya Mironov and Ananth Raghunathan and Kunal Talwar and Ulfar Erlingsson},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rkZB1XbRZ},
}

@ARTICLE{GigisShuffle2021,
author={Girgis, Antonious M. and Data, Deepesh and Diggavi, Suhas and Kairouz, Peter and Suresh, Ananda Theertha},
journal={IEEE Journal on Selected Areas in Information Theory}, 
title={Shuffled Model of Federated Learning: Privacy, Accuracy and Communication Trade-Offs}, 
year={2021},
volume={2},
number={1},
pages={464-478},
doi={10.1109/JSAIT.2021.3056102}}

@inproceedings{terrail2022flamby,
title={{FL}amby: Datasets and Benchmarks for Cross-Silo Federated Learning in Realistic Healthcare Settings},
author={Jean Ogier du Terrail and Samy-Safwan Ayed and Edwige Cyffers and Felix Grimberg and Chaoyang He and Regis Loeb and Paul Mangold and Tanguy Marchand and Othmane Marfoq and Erum Mushtaq and Boris Muzellec and Constantin Philippenko and Santiago Silva and Maria Tele{\'n}czuk and Shadi Albarqouni and Salman Avestimehr and Aur{\'e}lien Bellet and Aymeric Dieuleveut and Martin Jaggi and Sai Praneeth Karimireddy and Marco Lorenzi and Giovanni Neglia and Marc Tommasi and Mathieu Andreux},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=GgM5DiAb6A2}
}