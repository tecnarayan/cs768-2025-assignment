\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achille \& Soatto(2018)Achille and Soatto]{achille2018emergence}
Achille, A. and Soatto, S.
\newblock Emergence of invariance and disentanglement in deep representations.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 1947--1980, 2018.

\bibitem[Adragni \& Cook(2009)Adragni and Cook]{adragni2009sufficient}
Adragni, K.~P. and Cook, R.~D.
\newblock Sufficient dimension reduction and prediction in regression.
\newblock \emph{Philosophical Transactions of the Royal Society A:
  Mathematical, Physical and Engineering Sciences}, 367\penalty0
  (1906):\penalty0 4385--4405, 2009.

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and
  Lopez-Paz]{arjovsky2019invariant}
Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D.
\newblock Invariant risk minimization.
\newblock \emph{arXiv preprint arXiv:1907.02893}, 2019.

\bibitem[Bahng et~al.(2020)Bahng, Chun, Yun, Choo, and Oh]{bahng2020learning}
Bahng, H., Chun, S., Yun, S., Choo, J., and Oh, S.~J.
\newblock Learning de-biased representations with biased representations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  528--539. PMLR, 2020.

\bibitem[Beery et~al.(2018)Beery, Van~Horn, and Perona]{beery2018recognition}
Beery, S., Van~Horn, G., and Perona, P.
\newblock Recognition in terra incognita.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  456--473, 2018.

\bibitem[Bellot \& van~der Schaar(2020)Bellot and van~der
  Schaar]{bellot2020generalization}
Bellot, A. and van~der Schaar, M.
\newblock Generalization and invariances in the presence of unobserved
  confounding.
\newblock \emph{arXiv preprint arXiv:2007.10653}, 2020.

\bibitem[Ben-Tal et~al.(2013)Ben-Tal, Den~Hertog, De~Waegenaere, Melenberg, and
  Rennen]{ben2013robust}
Ben-Tal, A., Den~Hertog, D., De~Waegenaere, A., Melenberg, B., and Rennen, G.
\newblock Robust solutions of optimization problems affected by uncertain
  probabilities.
\newblock \emph{Management Science}, 59\penalty0 (2):\penalty0 341--357, 2013.

\bibitem[Blodgett et~al.(2016)Blodgett, Green, and
  O’Connor]{blodgett2016demographic}
Blodgett, S.~L., Green, L., and O’Connor, B.
\newblock Demographic dialectal variation in social media: A case study of
  african-american english.
\newblock In \emph{Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  1119--1130, 2016.

\bibitem[Buolamwini \& Gebru(2018)Buolamwini and Gebru]{buolamwini2018gender}
Buolamwini, J. and Gebru, T.
\newblock Gender shades: Intersectional accuracy disparities in commercial
  gender classification.
\newblock In \emph{Conference on fairness, accountability and transparency},
  pp.\  77--91, 2018.

\bibitem[Cover(1999)]{cover1999elements}
Cover, T.~M.
\newblock \emph{Elements of information theory}.
\newblock John Wiley \& Sons, 1999.

\bibitem[Cvitkovic \& Koliander(2019)Cvitkovic and
  Koliander]{cvitkovic2019minimal}
Cvitkovic, M. and Koliander, G.
\newblock Minimal achievable sufficient statistic learning.
\newblock volume~97, pp.\  1465--1474. PMLR, 2019.

\bibitem[David et~al.(2010)David, Lu, Luu, and P{\'a}l]{david2010impossibility}
David, S.~B., Lu, T., Luu, T., and P{\'a}l, D.
\newblock Impossibility theorems for domain adaptation.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  129--136. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem[Duchi et~al.(2016)Duchi, Glynn, and Namkoong]{duchi2016statistics}
Duchi, J., Glynn, P., and Namkoong, H.
\newblock Statistics of robust optimization: A generalized empirical likelihood
  approach.
\newblock \emph{arXiv preprint arXiv:1610.03425}, 2016.

\bibitem[Duchi et~al.(2019)Duchi, Hashimoto, and
  Namkoong]{duchi2019distributionally}
Duchi, J.~C., Hashimoto, T., and Namkoong, H.
\newblock Distributionally robust losses against mixture covariate shifts.
\newblock 2019.

\bibitem[Dynkin(2000)]{dynkin2000necessary}
Dynkin, E.~B.
\newblock Necessary and sufficient statistics for a family of probability
  distributions.
\newblock \emph{Selected Papers of EB Dynkin with Commentary}, 14:\penalty0
  393, 2000.

\bibitem[Fortuna \& Nunes(2018)Fortuna and Nunes]{fortuna2018survey}
Fortuna, P. and Nunes, S.
\newblock A survey on automatic detection of hate speech in text.
\newblock \emph{ACM Computing Surveys (CSUR)}, 51\penalty0 (4):\penalty0 1--30,
  2018.

\bibitem[Gao \& Kleywegt(2016)Gao and Kleywegt]{gao2016distributionally}
Gao, R. and Kleywegt, A.~J.
\newblock Distributionally robust stochastic optimization with wasserstein
  distance.
\newblock \emph{arXiv preprint arXiv:1604.02199}, 2016.

\bibitem[Geiger(2020)]{geiger2020information}
Geiger, B.~C.
\newblock On information plane analyses of neural network classifiers--a
  review.
\newblock \emph{arXiv preprint arXiv:2003.09671}, 2020.

\bibitem[Goyal et~al.(2017)Goyal, Khot, Summers-Stay, Batra, and
  Parikh]{goyal2017making}
Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D.
\newblock Making the v in vqa matter: Elevating the role of image understanding
  in visual question answering.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  6904--6913, 2017.

\bibitem[Gururangan et~al.(2018)Gururangan, Swayamdipta, Levy, Schwartz,
  Bowman, and Smith]{gururangan2018annotation}
Gururangan, S., Swayamdipta, S., Levy, O., Schwartz, R., Bowman, S., and Smith,
  N.~A.
\newblock Annotation artifacts in natural language inference data.
\newblock In \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 2 (Short Papers)}, pp.\  107--112, 2018.

\bibitem[Hashimoto et~al.(2018)Hashimoto, Srivastava, Namkoong, and
  Liang]{hashimoto2018fairness}
Hashimoto, T., Srivastava, M., Namkoong, H., and Liang, P.
\newblock Fairness without demographics in repeated loss minimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1929--1938, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Hu et~al.(2018)Hu, Niu, Sato, and Sugiyama]{hu2018does}
Hu, W., Niu, G., Sato, I., and Sugiyama, M.
\newblock Does distributionally robust supervised learning give robust
  classifiers?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2029--2037. PMLR, 2018.

\bibitem[Hu \& Hong(2013)Hu and Hong]{hu2013kullback}
Hu, Z. and Hong, L.~J.
\newblock Kullback-leibler divergence constrained distributionally robust
  optimization.
\newblock \emph{Available at Optimization Online}, 2013.

\bibitem[Husain(2020)]{husain2020distributional}
Husain, H.
\newblock Distributional robustness with ipms and links to regularization and
  gans.
\newblock In \emph{Proceedings of the 34th Annual Conference on Neural
  Information Processing Systems (NIPS)}, 2020.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Koenecke et~al.(2020)Koenecke, Nam, Lake, Nudell, Quartey, Mengesha,
  Toups, Rickford, Jurafsky, and Goel]{koenecke2020racial}
Koenecke, A., Nam, A., Lake, E., Nudell, J., Quartey, M., Mengesha, Z., Toups,
  C., Rickford, J.~R., Jurafsky, D., and Goel, S.
\newblock Racial disparities in automated speech recognition.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (14):\penalty0 7684--7689, 2020.

\bibitem[Koh et~al.(2020)Koh, Sagawa, Marklund, Xie, Zhang, Balsubramani, Hu,
  Yasunaga, Phillips, Beery, et~al.]{koh2020wilds}
Koh, P.~W., Sagawa, S., Marklund, H., Xie, S.~M., Zhang, M., Balsubramani, A.,
  Hu, W., Yasunaga, M., Phillips, R.~L., Beery, S., et~al.
\newblock Wilds: A benchmark of in-the-wild distribution shifts.
\newblock \emph{arXiv preprint arXiv:2012.07421}, 2020.

\bibitem[Kolchinsky et~al.(2019)Kolchinsky, Tracey, and
  Van~Kuyk]{kolchinsky2018caveats}
Kolchinsky, A., Tracey, B.~D., and Van~Kuyk, S.
\newblock Caveats for information bottleneck in deterministic scenarios.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Kullback \& Leibler(1951)Kullback and
  Leibler]{kullback1951information}
Kullback, S. and Leibler, R.~A.
\newblock On information and sufficiency.
\newblock \emph{The annals of mathematical statistics}, 22\penalty0
  (1):\penalty0 79--86, 1951.

\bibitem[Lewis(2013)]{lewis2013counterfactuals}
Lewis, D.
\newblock \emph{Counterfactuals}.
\newblock John Wiley \& Sons, 2013.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015deep}
Liu, Z., Luo, P., Wang, X., and Tang, X.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  3730--3738, 2015.

\bibitem[Lopez-Paz(2016)]{lopez2016dependence}
Lopez-Paz, D.
\newblock \emph{From dependence to causation}.
\newblock PhD thesis, University of Cambridge, 2016.

\bibitem[Lovering et~al.(2021)Lovering, Jha, Linzen, and
  Pavlick]{lovering2020info}
Lovering, C., Jha, R., Linzen, T., and Pavlick, E.
\newblock Predicting inductive biases of pre-trained models.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[McCoy et~al.(2019)McCoy, Pavlick, and Linzen]{mccoy2019right}
McCoy, T., Pavlick, E., and Linzen, T.
\newblock Right for the wrong reasons: Diagnosing syntactic heuristics in
  natural language inference.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  3428--3448, 2019.

\bibitem[Michel et~al.(2021)Michel, Hashimoto, and Neubig]{michel2021modeling}
Michel, P., Hashimoto, T., and Neubig, G.
\newblock Modeling the second player in distributionally robust optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Oren et~al.(2019)Oren, Sagawa, Hashimoto, and
  Liang]{oren2019distributionally}
Oren, Y., Sagawa, S., Hashimoto, T.~B., and Liang, P.
\newblock Distributionally robust language modeling.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing (EMNLP)}, Hong Kong, November 2019.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{ott2019fairseq}
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and
  Auli, M.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock In \emph{Proceedings of NAACL-HLT 2019: Demonstrations}, 2019.

\bibitem[Reimers et~al.(2019)Reimers, Gurevych, Reimers, Gurevych, Thakur,
  Reimers, Daxenberger, and Gurevych]{reimers2019sentence}
Reimers, N., Gurevych, I., Reimers, N., Gurevych, I., Thakur, N., Reimers, N.,
  Daxenberger, J., and Gurevych, I.
\newblock Sentence-bert: Sentence embeddings using siamese bert-networks.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing}. Association for Computational Linguistics,
  2019.

\bibitem[Rockafellar et~al.(2000)Rockafellar, Uryasev,
  et~al.]{rockafellar2000optimization}
Rockafellar, R.~T., Uryasev, S., et~al.
\newblock Optimization of conditional value-at-risk.
\newblock \emph{Journal of risk}, 2:\penalty0 21--42, 2000.

\bibitem[Sagawa et~al.(2020{\natexlab{a}})Sagawa, Koh, Hashimoto, and
  Liang]{sagawa2019distributionally}
Sagawa, S., Koh, P.~W., Hashimoto, T.~B., and Liang, P.
\newblock Distributionally robust neural networks for group shifts: On the
  importance of regularization for worst-case generalization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, Addis Ababa, Ethiopia, April 2020{\natexlab{a}}.

\bibitem[Sagawa et~al.(2020{\natexlab{b}})Sagawa, Raghunathan, Koh, and
  Liang]{sagawa2020investigation}
Sagawa, S., Raghunathan, A., Koh, P.~W., and Liang, P.
\newblock An investigation of why overparameterization exacerbates spurious
  correlations.
\newblock In \emph{International Conference on Machine Learning (ICML)}, July
  2020{\natexlab{b}}.

\bibitem[Sap et~al.(2019)Sap, Card, Gabriel, Choi, and Smith]{sap2019risk}
Sap, M., Card, D., Gabriel, S., Choi, Y., and Smith, N.~A.
\newblock The risk of racial bias in hate speech detection.
\newblock In \emph{Proceedings of the 57th annual meeting of the association
  for computational linguistics}, pp.\  1668--1678, 2019.

\bibitem[Schwartz-Ziv \& Tishby(2017)Schwartz-Ziv and
  Tishby]{shwartz2017opening}
Schwartz-Ziv, R. and Tishby, N.
\newblock Opening the black box of deep neural networks via information.
\newblock \emph{arXiv preprint arXiv:1703.00810}, 2017.

\bibitem[Shamir et~al.(2010)Shamir, Sabato, and Tishby]{shamir2010learning}
Shamir, O., Sabato, S., and Tishby, N.
\newblock Learning and generalization with the information bottleneck.
\newblock volume 411, pp.\  2696--2711. Elsevier, 2010.

\bibitem[Sohoni et~al.(2020)Sohoni, Dunnmon, Angus, Gu, and
  R{\'e}]{sohoni2020no}
Sohoni, N., Dunnmon, J., Angus, G., Gu, A., and R{\'e}, C.
\newblock No subclass left behind: Fine-grained robustness in coarse-grained
  classification problems.
\newblock \emph{34th Conference on Neural Information Processing Systems
  (NeurIPS)}, 33, 2020.

\bibitem[Tenenbaum(2018)]{tenenbaum2018building}
Tenenbaum, J.
\newblock Building machines that learn and think like people.
\newblock In \emph{Proceedings of the 17th International Conference on
  Autonomous Agents and MultiAgent Systems}, pp.\  5--5, 2018.

\bibitem[Tishby et~al.(2000)Tishby, Pereira, and Bialek]{tishby2000information}
Tishby, N., Pereira, F.~C., and Bialek, W.
\newblock The information bottleneck method.
\newblock \emph{arXiv preprint physics/0004057}, 2000.

\bibitem[Torralba \& Efros(2011)Torralba and Efros]{torralba2011unbiased}
Torralba, A. and Efros, A.~A.
\newblock Unbiased look at dataset bias.
\newblock In \emph{CVPR 2011}, pp.\  1521--1528. IEEE, 2011.

\bibitem[Wang et~al.(2020)Wang, Guo, Narasimhan, Cotter, Gupta, and
  Jordan]{wang2020robust}
Wang, S., Guo, W., Narasimhan, H., Cotter, A., Gupta, M.~R., and Jordan, M.~I.
\newblock Robust optimization for fairness with noisy protected groups.
\newblock \emph{34th Conference on Neural Information Processing Systems
  (NeurIPS).}, 2020.

\bibitem[Wen et~al.(2014)Wen, Yu, and Greiner]{wen2014robust}
Wen, J., Yu, C.-N., and Greiner, R.
\newblock Robust learning under uncertain test distributions: Relating
  covariate shift to model misspecification.
\newblock In \emph{ICML}, pp.\  631--639, 2014.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{williams2018broad}
Williams, A., Nangia, N., and Bowman, S.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pp.\  1112--1122, 2018.

\end{thebibliography}
