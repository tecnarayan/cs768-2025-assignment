\begin{thebibliography}{145}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[ai2(2019)]{ai2:winogrande}
Winogrande: An adversarial winograd schema challenge at scale.
\newblock 2019.

\bibitem[Allen-Zhu \& Li(2019)Allen-Zhu and Li]{allen2019can}
Allen-Zhu, Z. and Li, Y.
\newblock What can resnet learn efficiently, going beyond kernels?
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Alman \& Song(2023)Alman and Song]{as23}
Alman, J. and Song, Z.
\newblock Fast attention requires bounded entries.
\newblock \emph{arXiv preprint arXiv:2302.13214}, 2023.

\bibitem[Alman et~al.(2022)Alman, Liang, Song, Zhang, and Zhuo]{als+22}
Alman, J., Liang, J., Song, Z., Zhang, R., and Zhuo, D.
\newblock Bypass exponential time preprocessing: Fast neural network training
  via weight-data correlation preprocessing.
\newblock \emph{arXiv preprint arXiv:2211.14227}, 2022.

\bibitem[Alon et~al.(1996)Alon, Matias, and Szegedy]{ams96}
Alon, N., Matias, Y., and Szegedy, M.
\newblock The space complexity of approximating the frequency moments.
\newblock In \emph{Proceedings of the twenty-eighth annual ACM symposium on
  Theory of computing}, pp.\  20--29, 1996.

\bibitem[Aminabadi et~al.(2022)Aminabadi, Rajbhandari, Awan, Li, Li, Zheng,
  Ruwase, Smith, Zhang, Rasley, et~al.]{aminabadi2022deepspeed}
Aminabadi, R.~Y., Rajbhandari, S., Awan, A.~A., Li, C., Li, D., Zheng, E.,
  Ruwase, O., Smith, S., Zhang, M., Rasley, J., et~al.
\newblock Deepspeed-inference: Enabling efficient inference of transformer
  models at unprecedented scale.
\newblock In \emph{2022 SC22: International Conference for High Performance
  Computing, Networking, Storage and Analysis (SC)}, pp.\  646--660. IEEE
  Computer Society, 2022.

\bibitem[Andoni \& Razenshteyn(2015)Andoni and Razenshteyn]{ar15}
Andoni, A. and Razenshteyn, I.
\newblock Optimal data-dependent hashing for approximate near neighbors.
\newblock In \emph{Proceedings of the forty-seventh annual ACM symposium on
  Theory of computing (STOC)}, pp.\  793--801, 2015.

\bibitem[Andoni et~al.(2014)Andoni, Indyk, Nguyen, and Razenshteyn]{ainr14}
Andoni, A., Indyk, P., Nguyen, H.~L., and Razenshteyn, I.
\newblock Beyond locality-sensitive hashing.
\newblock In \emph{Proceedings of the twenty-fifth annual ACM-SIAM symposium on
  Discrete algorithms}, pp.\  1018--1028. SIAM, 2014.

\bibitem[Andoni et~al.(2015)Andoni, Indyk, Laarhoven, Razenshteyn, and
  Schmidt]{ailrs15}
Andoni, A., Indyk, P., Laarhoven, T., Razenshteyn, I., and Schmidt, L.
\newblock Practical and optimal lsh for angular distance.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pp.\  1225--1233. Curran Associates, 2015.

\bibitem[Andoni et~al.(2017)Andoni, Laarhoven, Razenshteyn, and
  Waingarten]{alrw17}
Andoni, A., Laarhoven, T., Razenshteyn, I., and Waingarten, E.
\newblock Optimal hashing-based time-space trade-offs for approximate near
  neighbors.
\newblock In \emph{Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium
  on Discrete Algorithms (SODA)}, pp.\  47--66. SIAM, 2017.

\bibitem[Andoni et~al.(2018)Andoni, Indyk, and Razenshteyn]{air18}
Andoni, A., Indyk, P., and Razenshteyn, I.
\newblock Approximate nearest neighbor search in high dimensions.
\newblock \emph{arXiv preprint arXiv:1806.09823}, 7, 2018.

\bibitem[Arya \& Mount(1993)Arya and Mount]{am93}
Arya, S. and Mount, D.~M.
\newblock Approximate nearest neighbor queries in fixed dimensions.
\newblock In \emph{SODA}, volume~93, pp.\  271--280. Citeseer, 1993.

\bibitem[Balduzzi et~al.(2017)Balduzzi, Frean, Leary, Lewis, Ma, and
  McWilliams]{balduzzi2017shattered}
Balduzzi, D., Frean, M., Leary, L., Lewis, J., Ma, K. W.-D., and McWilliams, B.
\newblock The shattered gradients problem: If resnets are the answer, then what
  is the question?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  342--350. PMLR, 2017.

\bibitem[Bansal et~al.(2022)Bansal, Gopalakrishnan, Dingliwal, Bodapati,
  Kirchhoff, and Roth]{bansal2022rethinking}
Bansal, H., Gopalakrishnan, K., Dingliwal, S., Bodapati, S., Kirchhoff, K., and
  Roth, D.
\newblock Rethinking the role of scale for in-context learning: An
  interpretability-based case study at 66 billion scale.
\newblock \emph{arXiv preprint arXiv:2212.09095}, 2022.

\bibitem[Baum \& Petrie(1966)Baum and Petrie]{baum1966statistical}
Baum, L.~E. and Petrie, T.
\newblock Statistical inference for probabilistic functions of finite state
  markov chains.
\newblock \emph{The annals of mathematical statistics}, 37\penalty0
  (6):\penalty0 1554--1563, 1966.

\bibitem[Bello et~al.(2021)Bello, Fedus, Du, Cubuk, Srinivas, Lin, Shlens, and
  Zoph]{bello2021revisiting}
Bello, I., Fedus, W., Du, X., Cubuk, E.~D., Srinivas, A., Lin, T.-Y., Shlens,
  J., and Zoph, B.
\newblock Revisiting resnets: Improved training and scaling strategies.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 22614--22627, 2021.

\bibitem[Bengio et~al.(2003)Bengio, Ducharme, Vincent, and
  Jauvin]{bengio2003neural}
Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C.
\newblock A neural probabilistic language model.
\newblock \emph{Journal of machine learning research (JMLR)}, 3\penalty0
  (Feb):\penalty0 1137--1155, 2003.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Bras, Gao, and Choi]{Bisk2020}
Bisk, Y., Zellers, R., Bras, R.~L., Gao, J., and Choi, Y.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Thirty-Fourth AAAI Conference on Artificial Intelligence},
  2020.

\bibitem[Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding,
  He, Leahy, McDonell, Phang, Pieler, Prashanth, Purohit, Reynolds, Tow, Wang,
  and Weinbach]{gpt-neox-20b}
Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He,
  H., Leahy, C., McDonell, K., Phang, J., Pieler, M., Prashanth, U.~S.,
  Purohit, S., Reynolds, L., Tow, J., Wang, B., and Weinbach, S.
\newblock {GPT-NeoX-20B}: An open-source autoregressive language model.
\newblock In \emph{Proceedings of the ACL Workshop on Challenges \&
  Perspectives in Creating Large Language Models}, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.06745}.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von
  Arx, Bernstein, Bohg, Bosselut, Brunskill,
  et~al.]{bommasani2021opportunities}
Bommasani, R., Hudson, D.~A., Adeli, E., Altman, R., Arora, S., von Arx, S.,
  Bernstein, M.~S., Bohg, J., Bosselut, A., Brunskill, E., et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Boutsidis et~al.(2016)Boutsidis, Woodruff, and Zhong]{bwz16}
Boutsidis, C., Woodruff, D.~P., and Zhong, P.
\newblock Optimal principal component analysis in distributed and streaming
  models.
\newblock In \emph{S{TOC}'16---{P}roceedings of the 48th {A}nnual {ACM}
  {SIGACT} {S}ymposium on {T}heory of {C}omputing}, 2016.

\bibitem[Boytsov et~al.(2016)Boytsov, Novak, Malkov, and
  Nyberg]{boytsov2016off}
Boytsov, L., Novak, D., Malkov, Y., and Nyberg, E.
\newblock Off the beaten path: Let's replace term-based retrieval with k-nn
  search.
\newblock In \emph{Proceedings of the 25th ACM international on conference on
  information and knowledge management (CIKM)}, pp.\  1099--1108, 2016.

\bibitem[Brand et~al.(2021)Brand, Peng, Song, and Weinstein]{bpsw21}
Brand, J. v.~d., Peng, B., Song, Z., and Weinstein, O.
\newblock Training (overparametrized) neural networks in near-linear time.
\newblock In \emph{ITCS}, 2021.

\bibitem[Brand et~al.(2023)Brand, Song, and Zhou]{bsz23}
Brand, J. v.~d., Song, Z., and Zhou, T.
\newblock Algorithm and hardness for dynamic attention maintenance in large
  language models.
\newblock \emph{arXiv preprint arXiv:2304.02207}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chan et~al.(2022)Chan, Santoro, Lampinen, Wang, Singh, Richemond,
  McClelland, and Hill]{chan2022data}
Chan, S.~C., Santoro, A., Lampinen, A.~K., Wang, J.~X., Singh, A.~K.,
  Richemond, P.~H., McClelland, J., and Hill, F.
\newblock Data distributional properties drive emergent in-context learning in
  transformers.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Chang et~al.(2020)Chang, Yu, Chang, Yang, and Kumar]{chang2020pre}
Chang, W.-C., Yu, F.~X., Chang, Y.-W., Yang, Y., and Kumar, S.
\newblock Pre-training tasks for embedding-based large-scale retrieval.
\newblock \emph{arXiv preprint arXiv:2002.03932}, 2020.

\bibitem[Charikar et~al.(2002)Charikar, Chen, and Farach-Colton]{ccf02}
Charikar, M., Chen, K., and Farach-Colton, M.
\newblock Finding frequent items in data streams.
\newblock In \emph{International Colloquium on Automata, Languages, and
  Programming}, pp.\  693--703. Springer, 2002.

\bibitem[Chen et~al.(2019)Chen, Xu, and Shrivastava]{chen2019fast}
Chen, B., Xu, Y., and Shrivastava, A.
\newblock Fast and accurate stochastic gradient estimation.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Medini, Farwell, Tai,
  Shrivastava, et~al.]{chen2020slide}
Chen, B., Medini, T., Farwell, J., Tai, C., Shrivastava, A., et~al.
\newblock Slide: In defense of smart algorithms over hardware acceleration for
  large-scale deep learning systems.
\newblock \emph{Proceedings of Machine Learning and Systems}, 2:\penalty0
  291--306, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Dao, Winsor, Song, Rudra, and
  R{\'e}]{chen2021scatterbrain}
Chen, B., Dao, T., Winsor, E., Song, Z., Rudra, A., and R{\'e}, C.
\newblock Scatterbrain: Unifying sparse and low-rank attention.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 17413--17426, 2021{\natexlab{a}}.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Liu, Peng, Xu, Li, Dao, Song,
  Shrivastava, and Re]{chen2021mongoose}
Chen, B., Liu, Z., Peng, B., Xu, Z., Li, J.~L., Dao, T., Song, Z., Shrivastava,
  A., and Re, C.
\newblock Mongoose: A learnable lsh framework for efficient neural network
  training.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Chillotti, Dong, Poburinnaya,
  Razenshteyn, and Riazi]{ccd+20}
Chen, H., Chillotti, I., Dong, Y., Poburinnaya, O., Razenshteyn, I., and Riazi,
  M.~S.
\newblock $\{$SANNS$\}$: Scaling up secure approximate k-nearest neighbors
  search.
\newblock In \emph{29th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$
  Security 20)}, pp.\  2111--2128, 2020{\natexlab{b}}.

\bibitem[Chen(2018)]{c18}
Chen, L.
\newblock On the hardness of approximate and exact (bichromatic) maximum inner
  product.
\newblock In \emph{33rd Computational Complexity Conference (CCC)}, 2018.

\bibitem[Cho \& Hariharan(2019)Cho and Hariharan]{cho2019efficacy}
Cho, J.~H. and Hariharan, B.
\newblock On the efficacy of knowledge distillation.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pp.\  4794--4802, 2019.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,
  Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., et~al.
\newblock Pa{LM}: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Clarkson \& Woodruff(2013)Clarkson and Woodruff]{cw13}
Clarkson, K.~L. and Woodruff, D.~P.
\newblock Low-rank approximation and regression in input sparsity time.
\newblock In \emph{STOC}, 2013.

\bibitem[Cohen(2016)]{c16}
Cohen, M.~B.
\newblock Nearly tight oblivious subspace embeddings by trace inequalities.
\newblock In \emph{Proceedings of the twenty-seventh annual ACM-SIAM symposium
  on Discrete algorithms}, pp.\  278--287. SIAM, 2016.

\bibitem[Cook(2012)]{cook2012cuda}
Cook, S.
\newblock \emph{CUDA Programming: A Developer’s Guide to Parallel Computing
  with GPUs}.
\newblock Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1st edition,
  2012.
\newblock ISBN 9780124159334.

\bibitem[Cox \& Cox(2008)Cox and Cox]{cox2008multidimensional}
Cox, M. and Cox, T.
\newblock Multidimensional scaling, 315--347.
\newblock \emph{Handbook of data visualization. Springer, Berlin, Germany},
  2008.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and
  R{\'e}]{dao2022flashattention}
Dao, T., Fu, D.~Y., Ermon, S., Rudra, A., and R{\'e}, C.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Datar et~al.(2004)Datar, Immorlica, Indyk, and Mirrokni]{diim04}
Datar, M., Immorlica, N., Indyk, P., and Mirrokni, V.~S.
\newblock Locality-sensitive hashing scheme based on p-stable distributions.
\newblock In \emph{Proceedings of the twentieth annual symposium on
  Computational geometry (SoCG)}, pp.\  253--262, 2004.

\bibitem[de~Marneffe et~al.(2019)de~Marneffe, Simons, and
  Tonhauser]{Marneffe2019TheCI}
de~Marneffe, M.-C., Simons, M., and Tonhauser, J.
\newblock The commitmentbank: Investigating projection in naturally occurring
  discourse.
\newblock 2019.

\bibitem[Deng et~al.(2023{\natexlab{a}})Deng, Li, and Song]{dls23}
Deng, Y., Li, Z., and Song, Z.
\newblock Attention scheme inspired softmax regression.
\newblock \emph{arXiv preprint arXiv:2304.10411}, 2023{\natexlab{a}}.

\bibitem[Deng et~al.(2023{\natexlab{b}})Deng, Mahadevan, and Song]{dms23}
Deng, Y., Mahadevan, S., and Song, Z.
\newblock Randomized and deterministic attention sparsification algorithms for
  over-parameterized feature dimension.
\newblock \emph{arxiv preprint: arxiv 2304.03426}, 2023{\natexlab{b}}.

\bibitem[Derpanis(2005)]{derpanis2005mean}
Derpanis, K.~G.
\newblock Mean shift clustering.
\newblock \emph{Lecture Notes}, 32:\penalty0 1--4, 2005.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and
  Zettlemoyer]{dettmers2022llm}
Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.
\newblock Llm. int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock \emph{arXiv preprint arXiv:2208.07339}, 2022.

\bibitem[Dong et~al.(2021)Dong, Lee, and Ye]{dly21}
Dong, S., Lee, Y.~T., and Ye, G.
\newblock A nearly-linear time algorithm for linear programs with small
  treewidth: A multiscale representation of robust central path.
\newblock In \emph{Proceedings of the 53rd Annual ACM SIGACT Symposium on
  Theory of Computing}, pp.\  1784--1797, 2021.

\bibitem[Dong et~al.(2019)Dong, Indyk, Razenshteyn, and Wagner]{dirw19}
Dong, Y., Indyk, P., Razenshteyn, I., and Wagner, T.
\newblock Learning space partitions for nearest neighbor search.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Fang et~al.(2021)Fang, Yu, Zhao, and Zhou]{fang2021turbotransformers}
Fang, J., Yu, Y., Zhao, C., and Zhou, J.
\newblock Turbotransformers: an efficient gpu serving system for transformer
  models.
\newblock In \emph{Proceedings of the 26th ACM SIGPLAN Symposium on Principles
  and Practice of Parallel Programming}, pp.\  389--402, 2021.

\bibitem[Frankle \& Carbin(2018)Frankle and Carbin]{frankle2018lottery}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock \emph{arXiv preprint arXiv:1803.03635}, 2018.

\bibitem[Frantar \& Alistarh(2023)Frantar and Alistarh]{frantar2023massive}
Frantar, E. and Alistarh, D.
\newblock Massive language models can be accurately pruned in one-shot.
\newblock \emph{arXiv preprint arXiv:2301.00774}, 2023.

\bibitem[Frantar et~al.(2022)Frantar, Ashkboos, Hoefler, and
  Alistarh]{frantar2022gptq}
Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D.
\newblock Gptq: Accurate post-training quantization for generative pre-trained
  transformers.
\newblock \emph{arXiv preprint arXiv:2210.17323}, 2022.

\bibitem[Frei et~al.(2019)Frei, Cao, and Gu]{frei2019algorithm}
Frei, S., Cao, Y., and Gu, Q.
\newblock Algorithm-dependent generalization bounds for overparameterized deep
  residual networks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Gao et~al.(2021)Gao, Tow, Biderman, Black, DiPofi, Foster, Golding,
  Hsu, McDonell, Muennighoff, Phang, Reynolds, Tang, Thite, Wang, Wang, and
  Zou]{eval-harness}
Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L.,
  Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E.,
  Thite, A., Wang, B., Wang, K., and Zou, A.
\newblock A framework for few-shot language model evaluation, September 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5371628}.

\bibitem[Gao et~al.(2023{\natexlab{a}})Gao, Mahadevan, and Song]{gms23}
Gao, Y., Mahadevan, S., and Song, Z.
\newblock An over-parameterized exponential regression.
\newblock \emph{arXiv preprint arXiv:2303.16504}, 2023{\natexlab{a}}.

\bibitem[Gao et~al.(2023{\natexlab{b}})Gao, Song, and Yang]{gsy23_dp}
Gao, Y., Song, Z., and Yang, X.
\newblock Differentially private attention computation.
\newblock \emph{arXiv preprint arXiv:2305.04701}, 2023{\natexlab{b}}.

\bibitem[Giampiccolo et~al.(2007)Giampiccolo, Magnini, Dagan, and
  Dolan]{giampiccolo-etal-2007-third}
Giampiccolo, D., Magnini, B., Dagan, I., and Dolan, B.
\newblock The third {PASCAL} recognizing textual entailment challenge.
\newblock In \emph{Proceedings of the {ACL}-{PASCAL} Workshop on Textual
  Entailment and Paraphrasing}, pp.\  1--9, Prague, June 2007. Association for
  Computational Linguistics.
\newblock URL \url{https://aclanthology.org/W07-1401}.

\bibitem[Gionis et~al.(1999)Gionis, Indyk, Motwani,
  et~al.]{gionis1999similarity}
Gionis, A., Indyk, P., Motwani, R., et~al.
\newblock Similarity search in high dimensions via hashing.
\newblock In \emph{Vldb}, volume~99, pp.\  518--529, 1999.

\bibitem[Gordon et~al.(2012)Gordon, Kozareva, and
  Roemmele]{gordon-etal-2012-semeval}
Gordon, A., Kozareva, Z., and Roemmele, M.
\newblock {S}em{E}val-2012 task 7: Choice of plausible alternatives: An
  evaluation of commonsense causal reasoning.
\newblock In \emph{*{SEM} 2012: The First Joint Conference on Lexical and
  Computational Semantics {--} Volume 1: Proceedings of the main conference and
  the shared task, and Volume 2: Proceedings of the Sixth International
  Workshop on Semantic Evaluation ({S}em{E}val 2012)}, pp.\  394--398,
  Montr{\'e}al, Canada, 7-8 June 2012. Association for Computational
  Linguistics.
\newblock URL \url{https://aclanthology.org/S12-1052}.

\bibitem[Gu \& Song(2022)Gu and Song]{gs22}
Gu, Y. and Song, Z.
\newblock A faster small treewidth sdp solver.
\newblock \emph{arXiv preprint arXiv:2211.06033}, 2022.

\bibitem[Gu et~al.(2023)Gu, Song, Yin, and Zhang]{gsyz23}
Gu, Y., Song, Z., Yin, J., and Zhang, L.
\newblock Low rank matrix completion via robust alternating minimization in
  nearly linear time.
\newblock \emph{arXiv preprint arXiv:2302.11068}, 2023.

\bibitem[Hall \& Attenberg(2015)Hall and Attenberg]{hall2015fast}
Hall, R. and Attenberg, J.
\newblock Fast and accurate maximum inner product recommendations on
  map-reduce.
\newblock In \emph{Proceedings of the 24th International Conference on World
  Wide Web (WWW)}, pp.\  1263--1268, 2015.

\bibitem[Han et~al.(2015)Han, Mao, and Dally]{han2015deep}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015.

\bibitem[Harris(2013)]{harris2013access}
Harris, M.
\newblock How to access global memory efficiently in {CUDA} {C}/{C}++ kernels.
\newblock \emph{NVIDIA, Jan}, 2013.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2019)He, Liu, Wang, Hu, and Yang]{he2019filter}
He, Y., Liu, P., Wang, Z., Hu, Z., and Yang, Y.
\newblock Filter pruning via geometric median for deep convolutional neural
  networks acceleration.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pp.\  4340--4349, 2019.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, Dean,
  et~al.]{hinton2015distilling}
Hinton, G., Vinyals, O., Dean, J., et~al.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2\penalty0 (7), 2015.

\bibitem[Hoefler et~al.(2021)Hoefler, Alistarh, Ben-Nun, Dryden, and
  Peste]{hoefler2021sparsity}
Hoefler, T., Alistarh, D., Ben-Nun, T., Dryden, N., and Peste, A.
\newblock Sparsity in deep learning: Pruning and growth for efficient inference
  and training in neural networks.
\newblock \emph{J. Mach. Learn. Res.}, 22\penalty0 (241):\penalty0 1--124,
  2021.

\bibitem[Hooker(2021)]{hooker2021hardware}
Hooker, S.
\newblock The hardware lottery.
\newblock \emph{Communications of the ACM}, 64\penalty0 (12):\penalty0 58--65,
  2021.

\bibitem[Hu et~al.(2022)Hu, Song, Weinstein, and Zhuo]{hswz22}
Hu, H., Song, Z., Weinstein, O., and Zhuo, D.
\newblock Training overparametrized neural networks in sublinear time.
\newblock \emph{arXiv preprint arXiv:2208.04508}, 2022.

\bibitem[Indyk \& Motwani(1998{\natexlab{a}})Indyk and Motwani]{im98}
Indyk, P. and Motwani, R.
\newblock Approximate nearest neighbors: towards removing the curse of
  dimensionality.
\newblock In \emph{Proceedings of the thirtieth annual ACM symposium on Theory
  of computing (STOC)}, pp.\  604--613, 1998{\natexlab{a}}.

\bibitem[Indyk \& Motwani(1998{\natexlab{b}})Indyk and
  Motwani]{indyk1998approximate}
Indyk, P. and Motwani, R.
\newblock Approximate nearest neighbors: towards removing the curse of
  dimensionality.
\newblock In \emph{Proceedings of the thirtieth annual ACM symposium on Theory
  of computing}, pp.\  604--613, 1998{\natexlab{b}}.

\bibitem[Indyk \& Wagner(2018)Indyk and Wagner]{iw18}
Indyk, P. and Wagner, T.
\newblock Approximate nearest neighbors in limited space.
\newblock In \emph{Conference On Learning Theory}, pp.\  2012--2036. PMLR,
  2018.

\bibitem[Ivanov et~al.(2021)Ivanov, Dryden, Ben-Nun, Li, and
  Hoefler]{ivanov2021data}
Ivanov, A., Dryden, N., Ben-Nun, T., Li, S., and Hoefler, T.
\newblock Data movement is all you need: A case study on optimizing
  transformers.
\newblock \emph{Proceedings of Machine Learning and Systems}, 3:\penalty0
  711--732, 2021.

\bibitem[Jacob et~al.(2018)Jacob, Kligys, Chen, Zhu, Tang, Howard, Adam, and
  Kalenichenko]{jacob2018quantization}
Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., and
  Kalenichenko, D.
\newblock Quantization and training of neural networks for efficient
  integer-arithmetic-only inference.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  2704--2713, 2018.

\bibitem[Jiang et~al.(2021)Jiang, Song, Weinstein, and Zhang]{jswz21}
Jiang, S., Song, Z., Weinstein, O., and Zhang, H.
\newblock A faster algorithm for solving general lps.
\newblock In \emph{Proceedings of the 53rd Annual ACM SIGACT Symposium on
  Theory of Computing}, pp.\  823--832, 2021.

\bibitem[Johnson et~al.(2019)Johnson, Douze, and J{\'e}gou]{johnson2019billion}
Johnson, J., Douze, M., and J{\'e}gou, H.
\newblock Billion-scale similarity search with {GPUs}.
\newblock \emph{IEEE Transactions on Big Data}, 7\penalty0 (3):\penalty0
  535--547, 2019.

\bibitem[Johnson \& Lindenstrauss(1984)Johnson and Lindenstrauss]{jl84}
Johnson, W.~B. and Lindenstrauss, J.
\newblock Extensions of lipschitz mappings into a hilbert space.
\newblock \emph{Contemporary mathematics}, 26\penalty0 (189-206):\penalty0 1,
  1984.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kkl20}
Kitaev, N., Kaiser, {\L}., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock In \emph{ICLR}, 2020.

\bibitem[Kurtz et~al.(2020)Kurtz, Kopinsky, Gelashvili, Matveev, Carr, Goin,
  Leiserson, Moore, Shavit, and Alistarh]{pmlr-v119-kurtz20a}
Kurtz, M., Kopinsky, J., Gelashvili, R., Matveev, A., Carr, J., Goin, M.,
  Leiserson, W., Moore, S., Shavit, N., and Alistarh, D.
\newblock Inducing and exploiting activation sparsity for fast inference on
  deep neural networks.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  5533--5543. PMLR,
  13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/kurtz20a.html}.

\bibitem[Laurent \& Massart(2000)Laurent and Massart]{lm00}
Laurent, B. and Massart, P.
\newblock Adaptive estimation of a quadratic functional by model selection.
\newblock \emph{Annals of Statistics}, pp.\  1302--1338, 2000.

\bibitem[LeCun et~al.(1989)LeCun, Denker, and Solla]{lecun1989optimal}
LeCun, Y., Denker, J., and Solla, S.
\newblock Optimal brain damage.
\newblock \emph{Advances in neural information processing systems}, 2, 1989.

\bibitem[Lee et~al.(2016)Lee, He, Yih, Gao, Deng, and
  Smolensky]{lee2015reasoning}
Lee, M., He, X., Yih, W.-t., Gao, J., Deng, L., and Smolensky, P.
\newblock Reasoning in vector space: An exploratory study of question
  answering.
\newblock In \emph{ICLR}, 2016.

\bibitem[Lee et~al.(2018)Lee, Ajanthan, and Torr]{lee2018snip}
Lee, N., Ajanthan, T., and Torr, P.~H.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock \emph{arXiv preprint arXiv:1810.02340}, 2018.

\bibitem[Lee et~al.(2019)Lee, Song, and Zhang]{lsz19}
Lee, Y.~T., Song, Z., and Zhang, Q.
\newblock Solving empirical risk minimization in the current matrix
  multiplication time.
\newblock In \emph{Conference on Learning Theory}, pp.\  2140--2157. PMLR,
  2019.

\bibitem[Li et~al.(2019)Li, Li, and Zhang]{li2019re}
Li, P., Li, X., and Zhang, C.-H.
\newblock Re-randomized densification for one permutation hashing and bin-wise
  consistent weighted sampling.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Song, Xia, Yu, and Zhou]{lsx+23}
Li, S., Song, Z., Xia, Y., Yu, T., and Zhou, T.
\newblock The closeness of in-context learning and weight shifting for softmax
  regression.
\newblock \emph{arXiv preprint}, 2023{\natexlab{a}}.

\bibitem[Li \& Li(2022)Li and Li]{pmlr-v162-li22m}
Li, X. and Li, P.
\newblock C-{M}in{H}ash: Improving minwise hashing with circulant permutation.
\newblock In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and
  Sabato, S. (eds.), \emph{Proceedings of the 39th International Conference on
  Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning
  Research}, pp.\  12857--12887. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/li22m.html}.

\bibitem[Li et~al.(2022)Li, You, Bhojanapalli, Li, Rawat, Reddi, Ye, Chern, Yu,
  Guo, and Kumar]{sanjiv}
Li, Z., You, C., Bhojanapalli, S., Li, D., Rawat, A.~S., Reddi, S.~J., Ye, K.,
  Chern, F., Yu, F., Guo, R., and Kumar, S.
\newblock Large models are parsimonious learners: Activation sparsity in
  trained transformers, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.06313}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Song, and Zhou]{lsz23}
Li, Z., Song, Z., and Zhou, T.
\newblock Solving regularized exp, cosh and sinh regression problems.
\newblock \emph{arXiv preprint, 2303.15725}, 2023{\natexlab{b}}.

\bibitem[Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga,
  Zhang, Narayanan, Wu, Kumar, et~al.]{liang2022holistic}
Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang,
  Y., Narayanan, D., Wu, Y., Kumar, A., et~al.
\newblock Holistic evaluation of language models.
\newblock \emph{arXiv preprint arXiv:2211.09110}, 2022.

\bibitem[Liu et~al.(2018)Liu, Sun, Zhou, Huang, and Darrell]{liu2018rethinking}
Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T.
\newblock Rethinking the value of network pruning.
\newblock \emph{arXiv preprint arXiv:1810.05270}, 2018.

\bibitem[Liu et~al.(2022)Liu, Xu, Ji, Zhang, Li, Chen, and
  Shrivastava]{liu2022halos}
Liu, Z., Xu, Z., Ji, A., Zhang, J., Li, J., Chen, B., and Shrivastava, A.
\newblock Halos: Hashing large output space for cheap inference.
\newblock \emph{Proceedings of Machine Learning and Systems}, 4:\penalty0
  110--125, 2022.

\bibitem[Lu et~al.(2013)Lu, Dhillon, Foster, and Ungar]{ldfu13}
Lu, Y., Dhillon, P., Foster, D.~P., and Ungar, L.
\newblock Faster ridge regression via the subsampled randomized hadamard
  transform.
\newblock In \emph{Advances in neural information processing systems (NIPS)},
  pp.\  369--377, 2013.

\bibitem[Malkov et~al.(2014)Malkov, Ponomarenko, Logvinov, and
  Krylov]{malkov2014approximate}
Malkov, Y., Ponomarenko, A., Logvinov, A., and Krylov, V.
\newblock Approximate nearest neighbor algorithm based on navigable small world
  graphs.
\newblock \emph{Information Systems}, 45:\penalty0 61--68, 2014.

\bibitem[Malkov \& Yashunin(2018)Malkov and Yashunin]{malkov2018efficient}
Malkov, Y.~A. and Yashunin, D.~A.
\newblock Efficient and robust approximate nearest neighbor search using
  hierarchical navigable small world graphs.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 42\penalty0 (4):\penalty0 824--836, 2018.

\bibitem[Meng \& Mahoney(2013)Meng and Mahoney]{mm13}
Meng, X. and Mahoney, M.~W.
\newblock Low-distortion subspace embeddings in input-sparsity time and
  applications to robust linear regression.
\newblock In \emph{Proceedings of the forty-fifth annual ACM symposium on
  Theory of computing}, pp.\  91--100, 2013.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and
  Socher]{merity2016pointer}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models, 2016.

\bibitem[Michel et~al.(2019)Michel, Levy, and Neubig]{michel2019sixteen}
Michel, P., Levy, O., and Neubig, G.
\newblock Are sixteen heads really better than one?
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and
  Sabharwal]{OpenBookQA2018}
Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A.
\newblock Can a suit of armor conduct electricity? a new dataset for open book
  question answering.
\newblock In \emph{EMNLP}, 2018.

\bibitem[Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and
  Zettlemoyer]{min2022rethinking}
Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and
  Zettlemoyer, L.
\newblock Rethinking the role of demonstrations: What makes in-context learning
  work?
\newblock \emph{arXiv preprint arXiv:2202.12837}, 2022.

\bibitem[Molchanov et~al.(2016)Molchanov, Tyree, Karras, Aila, and
  Kautz]{molchanov2016pruning}
Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock \emph{arXiv preprint arXiv:1611.06440}, 2016.

\bibitem[Nagel et~al.(2019)Nagel, Baalen, Blankevoort, and
  Welling]{nagel2019data}
Nagel, M., Baalen, M.~v., Blankevoort, T., and Welling, M.
\newblock Data-free quantization through weight equalization and bias
  correction.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  1325--1334, 2019.

\bibitem[Nelson \& Nguy{\^e}n(2013)Nelson and Nguy{\^e}n]{nn13}
Nelson, J. and Nguy{\^e}n, H.~L.
\newblock Osnap: Faster numerical linear algebra algorithms via sparser
  subspace embeddings.
\newblock In \emph{2013 ieee 54th annual symposium on foundations of computer
  science}, pp.\  117--126. IEEE, 2013.

\bibitem[Neyshabur \& Srebro(2015)Neyshabur and Srebro]{ns15}
Neyshabur, B. and Srebro, N.
\newblock On symmetric and asymmetric lshs for inner product search.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1926--1934. PMLR, 2015.

\bibitem[NVIDIA()]{nvidiaft}
NVIDIA.
\newblock Fastertransformer.
\newblock \url{https://github.com/NVIDIA/FasterTransformer}.

\bibitem[NVIDIA(2022)]{nvidia2022nvidia}
NVIDIA.
\newblock Gpu performance background user's guide, 2022.
\newblock URL
  \url{https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html}.

\bibitem[Park et~al.(2022)Park, Park, Kwon, Kim, Lee, and Lee]{park2022nuqmm}
Park, G., Park, B., Kwon, S.~J., Kim, B., Lee, Y., and Lee, D.
\newblock nuqmm: Quantized matmul for efficient inference of large-scale
  generative language models.
\newblock \emph{arXiv preprint arXiv:2206.09557}, 2022.

\bibitem[Pope et~al.(2022)Pope, Douglas, Chowdhery, Devlin, Bradbury, Levskaya,
  Heek, Xiao, Agrawal, and Dean]{pope2022efficiently}
Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Levskaya, A.,
  Heek, J., Xiao, K., Agrawal, S., and Dean, J.
\newblock Efficiently scaling transformer inference.
\newblock \emph{arXiv preprint arXiv:2211.05102}, 2022.

\bibitem[Qin et~al.(2023{\natexlab{a}})Qin, Song, and Wang]{qsw23}
Qin, L., Song, Z., and Wang, Y.
\newblock Fast submodular function maximization.
\newblock \emph{CoRR}, abs/2305.08367, 2023{\natexlab{a}}.

\bibitem[Qin et~al.(2023{\natexlab{b}})Qin, Song, Zhang, and Zhuo]{qszz23}
Qin, L., Song, Z., Zhang, L., and Zhuo, D.
\newblock An online and unified algorithm for projection matrix vector
  multiplication with application to empirical risk minimization.
\newblock In \emph{AISTATS}, 2023{\natexlab{b}}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{2019t5}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv e-prints}, 2019.

\bibitem[Razenshteyn et~al.(2016)Razenshteyn, Song, and Woodruff]{rsw16}
Razenshteyn, I., Song, Z., and Woodruff, D.~P.
\newblock Weighted low rank approximations with provable guarantees.
\newblock In \emph{Proceedings of the forty-eighth annual ACM symposium on
  Theory of Computing}, pp.\  250--263, 2016.

\bibitem[Sarlos(2006)]{s06}
Sarlos, T.
\newblock Improved approximation algorithms for large matrices via random
  projections.
\newblock In \emph{2006 47th annual IEEE symposium on foundations of computer
  science (FOCS)}, pp.\  143--152. IEEE, 2006.

\bibitem[Seo et~al.(2019)Seo, Lee, Kwiatkowski, Parikh, Farhadi, and
  Hajishirzi]{seo2019real}
Seo, M., Lee, J., Kwiatkowski, T., Parikh, A.~P., Farhadi, A., and Hajishirzi,
  H.
\newblock Real-time open-domain question answering with dense-sparse phrase
  index.
\newblock In \emph{ACL}, pp.\  4430--4441, 2019.

\bibitem[Shrivastava et~al.(2021)Shrivastava, Song, and Xu]{ssx21}
Shrivastava, A., Song, Z., and Xu, Z.
\newblock Sublinear least-squares value iteration via locality sensitive
  hashing.
\newblock \emph{arXiv preprint arXiv:2105.08285}, 2021.

\bibitem[Smith(1998)]{smith1998study}
Smith, J.~E.
\newblock A study of branch prediction strategies.
\newblock In \emph{25 years of the international symposia on Computer
  architecture (selected papers)}, pp.\  202--215, 1998.

\bibitem[Sohler \& Woodruff(2011)Sohler and Woodruff]{sw11}
Sohler, C. and Woodruff, D.~P.
\newblock Subspace embeddings for the l1-norm with applications.
\newblock In \emph{Proceedings of the forty-third annual ACM symposium on
  Theory of computing}, pp.\  755--764, 2011.

\bibitem[Song \& Ye(2023)Song and Ye]{sy23}
Song, Z. and Ye, M.
\newblock Efficient asynchronize stochastic gradient algorithm with structured
  data.
\newblock \emph{CoRR}, abs/2305.08001, 2023.

\bibitem[Song \& Yu(2021)Song and Yu]{sy21}
Song, Z. and Yu, Z.
\newblock Oblivious sketching-based central path method for linear programming.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9835--9847. PMLR, 2021.

\bibitem[Song et~al.(2017)Song, Woodruff, and Zhong]{swz17}
Song, Z., Woodruff, D.~P., and Zhong, P.
\newblock Low rank approximation with entrywise l1-norm error.
\newblock In \emph{Proceedings of the 49th Annual ACM SIGACT Symposium on
  Theory of Computing}, pp.\  688--701, 2017.

\bibitem[Song et~al.(2019)Song, Woodruff, and Zhong]{swz19}
Song, Z., Woodruff, D.~P., and Zhong, P.
\newblock Relative error tensor low rank approximation.
\newblock In \emph{Proceedings of the Thirtieth Annual ACM-SIAM Symposium on
  Discrete Algorithms (SODA)}, pp.\  2772--2789. SIAM, 2019.

\bibitem[Song et~al.(2021)Song, Zhang, and Zhang]{szz21}
Song, Z., Zhang, L., and Zhang, R.
\newblock Training multi-layer over-parametrized neural network in subquadratic
  time.
\newblock \emph{arXiv preprint arXiv:2112.07628}, 2021.

\bibitem[Song et~al.(2023{\natexlab{a}})Song, Wang, and Yin]{swy23}
Song, Z., Wang, W., and Yin, C.
\newblock Fast and efficient matching algorithm with deadline instances.
\newblock \emph{CoRR}, abs/2305.08353, 2023{\natexlab{a}}.

\bibitem[Song et~al.(2023{\natexlab{b}})Song, Yang, Yang, and Zhang]{syyz23}
Song, Z., Yang, X., Yang, Y., and Zhang, L.
\newblock Sketching meets differential privacy: fast algorithm for dynamic
  kronecker projection maintenance.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2023{\natexlab{b}}.

\bibitem[Tang et~al.(2019)Tang, Lu, Liu, Mou, Vechtomova, and
  Lin]{tang2019distilling}
Tang, R., Lu, Y., Liu, L., Mou, L., Vechtomova, O., and Lin, J.
\newblock Distilling task-specific knowledge from bert into simple neural
  networks.
\newblock \emph{arXiv preprint arXiv:1903.12136}, 2019.

\bibitem[Tillet et~al.(2019)Tillet, Kung, and Cox]{tillet2019triton}
Tillet, P., Kung, H.-T., and Cox, D.
\newblock Triton: an intermediate language and compiler for tiled neural
  network computations.
\newblock In \emph{Proceedings of the 3rd ACM SIGPLAN International Workshop on
  Machine Learning and Programming Languages}, pp.\  10--19, 2019.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J{\'e}gou]{touvron2021training}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J{\'e}gou,
  H.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10347--10357. PMLR, 2021.

\bibitem[Veit et~al.(2016)Veit, Wilber, and Belongie]{veit2016residual}
Veit, A., Wilber, M.~J., and Belongie, S.
\newblock Residual networks behave like ensembles of relatively shallow
  networks.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Viterbi(1967)]{viterbi1967error}
Viterbi, A.
\newblock Error bounds for convolutional codes and an asymptotically optimum
  decoding algorithm.
\newblock \emph{IEEE transactions on Information Theory}, 13\penalty0
  (2):\penalty0 260--269, 1967.

\bibitem[Wang \& Komatsuzaki(2021)Wang and Komatsuzaki]{gpt-j}
Wang, B. and Komatsuzaki, A.
\newblock {GPT-J-6B}: A 6 billion parameter autoregressive language model.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}, May 2021.

\bibitem[Wang \& Woodruff(2018)Wang and Woodruff]{ww18}
Wang, R. and Woodruff, D.~P.
\newblock Tight bounds for lp oblivious subspace embeddings.
\newblock 2018.

\bibitem[Wang et~al.(2021)Wang, Xiong, Wei, Wang, and Li]{wang2021lightseq}
Wang, X., Xiong, Y., Wei, Y., Wang, M., and Li, L.
\newblock Lightseq: A high performance inference library for transformers.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies: Industry Papers}, pp.\  113--120, 2021.

\bibitem[Woodruff(2014)]{w14}
Woodruff, D.~P.
\newblock Sketching as a tool for numerical linear algebra.
\newblock \emph{Foundations and Trends{\textregistered} in Theoretical Computer
  Science}, 10\penalty0 (1--2):\penalty0 1--157, 2014.

\bibitem[Xiao et~al.(2022)Xiao, Lin, Seznec, Demouth, and
  Han]{xiao2022smoothquant}
Xiao, G., Lin, J., Seznec, M., Demouth, J., and Han, S.
\newblock Smoothquant: Accurate and efficient post-training quantization for
  large language models.
\newblock \emph{arXiv preprint arXiv:2211.10438}, 2022.

\bibitem[Xie et~al.(2022)Xie, Raghunathan, Liang, and Ma]{xie2022an}
Xie, S.~M., Raghunathan, A., Liang, P., and Ma, T.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=RdJVFCHjUMI}.

\bibitem[Xue et~al.(2017)Xue, Dai, Zhang, Huang, and Chen]{xue2017deep}
Xue, H.-J., Dai, X., Zhang, J., Huang, S., and Chen, J.
\newblock Deep matrix factorization models for recommender systems.
\newblock In \emph{IJCAI}, pp.\  3203--3209, 2017.

\bibitem[Yao et~al.(2022)Yao, Aminabadi, Zhang, Wu, Li, and
  He]{yao2022zeroquant}
Yao, Z., Aminabadi, R.~Y., Zhang, M., Wu, X., Li, C., and He, Y.
\newblock Zeroquant: Efficient and affordable post-training quantization for
  large-scale transformers.
\newblock \emph{arXiv preprint arXiv:2206.01861}, 2022.

\bibitem[Yu et~al.(2022)Yu, Jeong, Kim, Kim, and Chun]{yu2022orca}
Yu, G.-I., Jeong, J.~S., Kim, G.-W., Kim, S., and Chun, B.-G.
\newblock Orca: A distributed serving system for $\{$Transformer-Based$\}$
  generative models.
\newblock In \emph{16th USENIX Symposium on Operating Systems Design and
  Implementation (OSDI 22)}, pp.\  521--538, 2022.

\bibitem[Zandieh et~al.(2023)Zandieh, Han, Daliri, and Karbasi]{zhdk23}
Zandieh, A., Han, I., Daliri, M., and Karbasi, A.
\newblock Kdeformer: Accelerating transformers via kernel density estimation.
\newblock \emph{arXiv preprint arXiv:2302.02451}, 2023.

\bibitem[Zhang(2022)]{z22}
Zhang, L.
\newblock Speeding up optimizations via data structures: Faster search, sample
  and maintenance.
\newblock Master's thesis, Carnegie Mellon University, 2022.

\bibitem[Zhang et~al.(2018)Zhang, Wang, Liu, Gao, and He]{zhang2018navigating}
Zhang, M., Wang, W., Liu, X., Gao, J., and He, Y.
\newblock Navigating with graph representations for fast and scalable decoding
  of neural language models.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Zhao et~al.(2019)Zhao, Hu, Dotzel, De~Sa, and
  Zhang]{zhao2019improving}
Zhao, R., Hu, Y., Dotzel, J., De~Sa, C., and Zhang, Z.
\newblock Improving neural network quantization without retraining using
  outlier channel splitting.
\newblock In \emph{International conference on machine learning}, pp.\
  7543--7552. PMLR, 2019.

\end{thebibliography}
