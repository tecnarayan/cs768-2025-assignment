\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019{\natexlab{a}})Allen-Zhu, Li, and
  Liang]{allen2019learning}
Allen-Zhu, Z., Li, Y., and Liang, Y.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock \emph{Advances in neural information processing systems}, 32,
  2019{\natexlab{a}}.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{b}})Allen-Zhu, Li, and
  Song]{allen2019convergence}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  242--252. PMLR, 2019{\natexlab{b}}.

\bibitem[Barakat et~al.(2022)Barakat, Bianchi, and
  Lehmann]{barakat2022analysis}
Barakat, A., Bianchi, P., and Lehmann, J.
\newblock Analysis of a target-based actor-critic algorithm with linear
  function approximation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  991--1040. PMLR, 2022.

\bibitem[Bertsekas(2012)]{bertsekas2012dynamic}
Bertsekas, D.
\newblock \emph{Dynamic programming and optimal control: Volume I}, volume~1.
\newblock Athena scientific, 2012.

\bibitem[Bhandari et~al.(2018)Bhandari, Russo, and Singal]{bhandari2018finite}
Bhandari, J., Russo, D., and Singal, R.
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock In \emph{Conference on learning theory}, pp.\  1691--1692. PMLR,
  2018.

\bibitem[Borkar(2009)]{borkar2009sto}
Borkar, V.~S.
\newblock \emph{Stochastic approximation: a dynamical systems viewpoint},
  volume~48.
\newblock Springer, 2009.

\bibitem[Bowling \& Veloso(2001)Bowling and Veloso]{bowling2001rational}
Bowling, M. and Veloso, M.
\newblock Rational and convergent learning in stochastic games.
\newblock In \emph{International joint conference on artificial intelligence},
  volume~17, pp.\  1021--1026. Citeseer, 2001.

\bibitem[Boyan(2002)]{boyan2002technical}
Boyan, J.~A.
\newblock Technical update: Least-squares temporal difference learning.
\newblock \emph{Machine learning}, 49\penalty0 (2):\penalty0 233--246, 2002.

\bibitem[Bradtke \& Barto(1996)Bradtke and Barto]{bradtke1996linear}
Bradtke, S.~J. and Barto, A.~G.
\newblock Linear least-squares algorithms for temporal difference learning.
\newblock \emph{Machine learning}, 22\penalty0 (1):\penalty0 33--57, 1996.

\bibitem[Brandfonbrener \& Bruna(2019)Brandfonbrener and
  Bruna]{brandfonbrener2019geo}
Brandfonbrener, D. and Bruna, J.
\newblock Geometric insights into the convergence of nonlinear td learning.
\newblock \emph{arXiv preprint arXiv:1905.12185}, 2019.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Cai et~al.(2023)Cai, Yang, Lee, and Wang]{cai2023neural}
Cai, Q., Yang, Z., Lee, J.~D., and Wang, Z.
\newblock Neural temporal difference and q learning provably converge to global
  optima.
\newblock \emph{Mathematics of Operations Research}, 2023.

\bibitem[Cao \& Gu(2019)Cao and Gu]{cao2019generalization}
Cao, Y. and Gu, Q.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Cao \& Gu(2020)Cao and Gu]{cao2020generalization}
Cao, Y. and Gu, Q.
\newblock Generalization error bounds of gradient descent for learning
  over-parameterized deep relu networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  3349--3356, 2020.

\bibitem[Cayci et~al.(2023)Cayci, Satpathi, He, and Srikant]{cayci2023sample}
Cayci, S., Satpathi, S., He, N., and Srikant, R.
\newblock Sample complexity and overparameterization bounds for temporal
  difference learning with neural network approximation.
\newblock \emph{IEEE Transactions on Automatic Control}, 2023.

\bibitem[Dalal et~al.(2018)Dalal, Sz{\"o}r{\'e}nyi, Thoppe, and
  Mannor]{dalal2018finite}
Dalal, G., Sz{\"o}r{\'e}nyi, B., Thoppe, G., and Mannor, S.
\newblock Finite sample analyses for td (0) with function approximation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International conference on machine learning}, pp.\
  1675--1685. PMLR, 2019.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018.

\bibitem[Fan et~al.(2020)Fan, Wang, Xie, and Yang]{fan2020theoretical}
Fan, J., Wang, Z., Xie, Y., and Yang, Z.
\newblock A theoretical analysis of deep q-learning.
\newblock In \emph{Learning for dynamics and control}, pp.\  486--489. PMLR,
  2020.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and
  Meger]{fujimoto2018addressing}
Fujimoto, S., Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International conference on machine learning}, pp.\
  1587--1596. PMLR, 2018.

\bibitem[Godfrey(2019)]{godfrey2019evaluation}
Godfrey, L.~B.
\newblock An evaluation of parametric activation functions for deep learning.
\newblock In \emph{2019 IEEE International Conference on Systems, Man and
  Cybernetics (SMC)}, pp.\  3006--3011. IEEE, 2019.

\bibitem[Jaakkola et~al.(1993)Jaakkola, Jordan, and
  Singh]{jaakkola1993convergence}
Jaakkola, T., Jordan, M., and Singh, S.
\newblock Convergence of stochastic iterative dynamic programming algorithms.
\newblock \emph{Advances in neural information processing systems}, 6, 1993.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Ke et~al.(2023)Ke, Wen, and Zhang]{ke2023provably}
Ke, Z., Wen, Z., and Zhang, J.
\newblock Provably efficient gauss-newton temporal difference learning method
  with function approximation.
\newblock \emph{arXiv preprint arXiv:2302.13087}, 2023.

\bibitem[Konda \& Tsitsiklis(1999)Konda and Tsitsiklis]{konda1999actor}
Konda, V. and Tsitsiklis, J.
\newblock Actor-critic algorithms.
\newblock \emph{Advances in neural information processing systems}, 12, 1999.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Fergus, Tompson, and
  Nachum]{kostrikov2021offline}
Kostrikov, I., Fergus, R., Tompson, J., and Nachum, O.
\newblock Offline reinforcement learning with fisher divergence critic
  regularization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5774--5783. PMLR, 2021.

\bibitem[Lazaric et~al.(2010)Lazaric, Ghavamzadeh, and
  Munos]{lazaric2010finite}
Lazaric, A., Ghavamzadeh, M., and Munos, R.
\newblock Finite-sample analysis of lstd.
\newblock In \emph{ICML-27th International Conference on Machine Learning},
  pp.\  615--622, 2010.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Littman(1994)]{littman1994markov}
Littman, M.~L.
\newblock Markov games as a framework for multi-agent reinforcement learning.
\newblock In \emph{Machine learning proceedings 1994}, pp.\  157--163.
  Elsevier, 1994.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Liu, Ghavamzadeh, Mahadevan, and
  Petrik]{liu2020finite}
Liu, B., Liu, J., Ghavamzadeh, M., Mahadevan, S., and Petrik, M.
\newblock Finite-sample analysis of proximal gradient td algorithms.
\newblock \emph{arXiv preprint arXiv:2006.14364}, 2020{\natexlab{a}}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Zhu, and Belkin]{liu2020linearity}
Liu, C., Zhu, L., and Belkin, M.
\newblock On the linearity of large non-linear models: when and why the tangent
  kernel is constant.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15954--15964, 2020{\natexlab{b}}.

\bibitem[Maei et~al.(2009)Maei, Szepesvari, Bhatnagar, Precup, Silver, and
  Sutton]{maei2009convergent}
Maei, H., Szepesvari, C., Bhatnagar, S., Precup, D., Silver, D., and Sutton,
  R.~S.
\newblock Convergent temporal-difference learning with arbitrary smooth
  function approximation.
\newblock \emph{Advances in neural information processing systems}, 22, 2009.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Perkins \& Pendrith(2002)Perkins and Pendrith]{perkins2002existence}
Perkins, T.~J. and Pendrith, M.~D.
\newblock On the existence of fixed points for q-learning and sarsa in
  partially observable domains.
\newblock In \emph{ICML}, pp.\  490--497, 2002.

\bibitem[Perolat et~al.(2018)Perolat, Piot, and Pietquin]{perolat2018actor}
Perolat, J., Piot, B., and Pietquin, O.
\newblock Actor-critic fictitious play in simultaneous move multistage games.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  919--928. PMLR, 2018.

\bibitem[Prashanth et~al.(2014)Prashanth, Korda, and Munos]{prashanth2014fast}
Prashanth, L., Korda, N., and Munos, R.
\newblock Fast lstd using stochastic approximation: Finite time analysis and
  application to traffic control.
\newblock In \emph{Joint European conference on machine learning and knowledge
  discovery in databases}, pp.\  66--81. Springer, 2014.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897. PMLR, 2015.

\bibitem[Sun et~al.(2022)Sun, Li, and Wang]{sun2022finite}
Sun, T., Li, D., and Wang, B.
\newblock Finite-time analysis of adaptive temporal difference learning with
  deep neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 19592--19604, 2022.

\bibitem[Sutton(1988)]{sutton1988learning}
Sutton, R.~S.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine learning}, 3\penalty0 (1):\penalty0 9--44, 1988.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and
  Mansour]{sutton1999policy}
Sutton, R.~S., McAllester, D., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock \emph{Advances in neural information processing systems}, 12, 1999.

\bibitem[Sutton et~al.(2009a)Sutton, Maei, Precup, Bhatnagar, Silver,
  Szepesv{\'a}ri, and Wiewiora]{sutton2009fast}
Sutton, R.~S., Maei, H.~R., Precup, D., Bhatnagar, S., Silver, D.,
  Szepesv{\'a}ri, C., and Wiewiora, E.
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In \emph{Proceedings of the 26th annual international conference on
  machine learning}, pp.\  993--1000, 2009a.

\bibitem[Sutton et~al.(2009b)Sutton, Szepesv{\'a}ri, and
  Maei]{sutton2009convergent}
Sutton, R.~S., Szepesv{\'a}ri, C., and Maei, H.~R.
\newblock A convergent o (n) algorithm for off-policy temporal-difference
  learning with linear function approximation.
\newblock \emph{Advances in neural information processing systems}, 21\penalty0
  (21):\penalty0 1609--1616, 2009b.

\bibitem[Tagorti \& Scherrer(2015)Tagorti and Scherrer]{tagorti2015rate}
Tagorti, M. and Scherrer, B.
\newblock On the rate of convergence and error bounds for lstd ($\lambda$).
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1521--1529. PMLR, 2015.

\bibitem[Tesauro et~al.(1995)]{tesauro1995temporal}
Tesauro, G. et~al.
\newblock Temporal difference learning and td-gammon.
\newblock \emph{Communications of the ACM}, 38\penalty0 (3):\penalty0 58--68,
  1995.

\bibitem[Tian et~al.(2022)Tian, Paschalidis, and
  Olshevsky]{tian2022performance}
Tian, H., Paschalidis, I., and Olshevsky, A.
\newblock On the performance of temporal difference learning with neural
  networks.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2022.

\bibitem[Touati et~al.(2018)Touati, Bacon, Precup, and
  Vincent]{touati2018convergent}
Touati, A., Bacon, P.-L., Precup, D., and Vincent, P.
\newblock Convergent tree backup and retrace with function approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4955--4964. PMLR, 2018.

\bibitem[Tsitsiklis \& Van~Roy(1996)Tsitsiklis and Van~Roy]{tsitsklis}
Tsitsiklis, J. and Van~Roy, B.
\newblock Analysis of temporal-diffference learning with function
  approximation.
\newblock \emph{Advances in neural information processing systems}, 9, 1996.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{van2016deep}
Van~Hasselt, H., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~30, 2016.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Wu, Y., Tucker, G., and Nachum, O.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Xu \& Gu(2020)Xu and Gu]{xu2020finite}
Xu, P. and Gu, Q.
\newblock A finite-time analysis of q-learning with neural network function
  approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10555--10565. PMLR, 2020.

\bibitem[Zou et~al.(2019)Zou, Xu, and Liang]{zou2019finite}
Zou, S., Xu, T., and Liang, Y.
\newblock Finite-sample analysis for sarsa with linear function approximation.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\end{thebibliography}
