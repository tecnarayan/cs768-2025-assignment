\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bishop and Nasrabadi(2006)]{bishop2006pattern}
C.~M Bishop and N.M. Nasrabadi.
\newblock \emph{{Pattern Recognition and Machine Learning}}, volume~4.
\newblock Springer, 2006.

\bibitem[Brooks et~al.(2011)Brooks, Gelman, Jones, and
  Meng]{brooks2011handbook}
S.~Brooks, A.~Gelman, G.~Jones, and X-L. Meng.
\newblock \emph{{Handbook of Markov Chain Monte Carlo}}.
\newblock CRC press, 2011.

\bibitem[Chen et~al.(2018)Chen, Mackey, Gorham, Briol, and
  Oates]{chen2018stein}
W.Y. Chen, L.~Mackey, J.~Gorham, F.~Briol, and C.~Oates.
\newblock Stein points.
\newblock In \emph{International Conference on Machine Learning}, pages
  844--853. PMLR, 2018.

\bibitem[Chen et~al.(2019)Chen, Barp, Briol, Gorham, Girolami, Mackey, and
  Oates]{chen2019stein}
W.Y. Chen, A.~Barp, F-X. Briol, J.~Gorham, M.~Girolami, L.~Mackey, and
  C.~Oates.
\newblock Stein point markov chain monte carlo.
\newblock In \emph{International Conference on Machine Learning}, pages
  1011--1021. PMLR, 2019.

\bibitem[Chopin and Ducrocq(2021)]{chopin2021fast}
N.~Chopin and G.~Ducrocq.
\newblock Fast compression of {MCMC} output.
\newblock \emph{Entropy}, 23:\penalty0 1017, 2021.

\bibitem[Chwialkowski et~al.(2016)Chwialkowski, Strathmann, and
  Gretton]{chwialkowski2016kernel}
K.~Chwialkowski, H.~Strathmann, and A.~Gretton.
\newblock A kernel test of goodness of fit.
\newblock In \emph{International Conference on Machine Learning}, pages
  2606--2615. PMLR, 2016.

\bibitem[Dua and Graff(2017)]{Dua:2019}
D.~Dua and C.~Graff.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Fong et~al.(2019)Fong, Lyddon, and Holmes]{fong2019scalable}
E.~Fong, S.~Lyddon, and C.~Holmes.
\newblock Scalable nonparametric sampling from multimodal posteriors with the
  posterior bootstrap.
\newblock In \emph{International Conference on Machine Learning}, pages
  1952--1962. PMLR, 2019.

\bibitem[Garreau et~al.(2017)Garreau, Jitkrittum, and
  Kanagawa]{garreau2017large}
D.~Garreau, W.~Jitkrittum, and M.~Kanagawa.
\newblock Large sample analysis of the median heuristic.
\newblock \emph{arXiv preprint arXiv:1707.07269}, 2017.

\bibitem[Gelman et~al.(1995)Gelman, Carlin, Stern, and
  Rubin]{gelman1995bayesian}
A.~Gelman, J.B. Carlin, H.S. Stern, and D.B. Rubin.
\newblock \emph{{Bayesian Data Analysis}}.
\newblock Chapman and Hall/CRC, 1995.

\bibitem[Gershman et~al.(2012)Gershman, Hoffman, and
  Blei]{gershman2012nonparametric}
S.~Gershman, M.~Hoffman, and D.~Blei.
\newblock Nonparametric variational inference.
\newblock \emph{arXiv preprint arXiv:1206.4665}, 2012.

\bibitem[Gorham and Mackey(2015)]{gorham2015measuring}
J.~Gorham and L.~Mackey.
\newblock Measuring sample quality with stein's method.
\newblock \emph{Advances in Neural Information Processing Systems}, 28, 2015.

\bibitem[Gorham and Mackey(2017)]{gorham2017measuring}
J.~Gorham and L.~Mackey.
\newblock Measuring sample quality with kernels.
\newblock In \emph{International Conference on Machine Learning}, pages
  1292--1301. PMLR, 2017.

\bibitem[Green et~al.(2015)Green, {\L}atuszy{\'n}ski, Pereyra, and
  Robert]{green2015bayesian}
P.J. Green, K.~{\L}atuszy{\'n}ski, M.~Pereyra, and C.P. Robert.
\newblock Bayesian computation: a summary of the current state, and samples
  backwards and forwards.
\newblock \emph{Statistics and Computing}, 25:\penalty0 835--862, 2015.

\bibitem[Gretton et~al.(2006)Gretton, Borgwardt, Rasch, Sch{\"o}lkopf, and
  Smola]{gretton2006kernel}
A.~Gretton, K.~Borgwardt, M.~Rasch, B.~Sch{\"o}lkopf, and A.~Smola.
\newblock A kernel method for the two-sample-problem.
\newblock \emph{{Advances in Neural Information Processing Systems}}, 19, 2006.

\bibitem[Haario et~al.(1999)Haario, Saksman, and Tamminen]{haario1999adaptive}
H.~Haario, E.~Saksman, and J.~Tamminen.
\newblock Adaptive proposal distribution for random walk metropolis algorithm.
\newblock \emph{{Computational Statistics}}, 14:\penalty0 375--395, 1999.

\bibitem[Korba et~al.(2021)Korba, Aubin-Frankowski, Majewski, and
  Ablin]{korba2021kernel}
A.~Korba, P-C. Aubin-Frankowski, S.~Majewski, and P.~Ablin.
\newblock Kernel stein discrepancy descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  5719--5730. PMLR, 2021.

\bibitem[Liu and Wang(2016)]{liu2016stein}
Q.~Liu and D.~Wang.
\newblock Stein variational gradient descent: A general purpose bayesian
  inference algorithm.
\newblock In \emph{Advances in neural information processing systems}, pages
  2378--2386, 2016.

\bibitem[Liu et~al.(2016)Liu, Lee, and Jordan]{liu2016kernelized}
Q.~Liu, J.~Lee, and M.~Jordan.
\newblock A kernelized stein discrepancy for goodness-of-fit tests.
\newblock In \emph{{International Conference on Machine Learning}}, pages
  276--284. PMLR, 2016.

\bibitem[Liu et~al.(2023)Liu, Duncan, and Gandy]{liu2023using}
X.~Liu, A.B. Duncan, and A.~Gandy.
\newblock Using perturbation to improve goodness-of-fit tests based on
  kernelized stein discrepancy.
\newblock \emph{arXiv preprint arXiv:2304.14762}, 2023.

\bibitem[Oates et~al.(2017)Oates, Barp, and Girolami]{oates2017posterior}
C.~Oates, A.~Barp, and M.~Girolami.
\newblock Posterior integration on a riemannian manifold.
\newblock \emph{arXiv preprint arXiv:1712.01793}, 2017.

\bibitem[Pompe et~al.(2020)Pompe, Holmes, and
  {\L}atuszy{\'n}ski]{pompe2020framework}
E.~Pompe, C.~Holmes, and K.~{\L}atuszy{\'n}ski.
\newblock A framework for adaptive mcmc targeting multimodal distributions.
\newblock \emph{The Annals of Statistics}, 48:\penalty0 2930--2952, 2020.

\bibitem[Qiu and Wang(2023)]{qiu2023efficient}
Yixuan Qiu and Xiao Wang.
\newblock Efficient multimodal sampling via tempered distribution flow.
\newblock \emph{Journal of the American Statistical Association}, 0\penalty0
  (0):\penalty0 1--15, 2023.
\newblock \doi{10.1080/01621459.2023.2198059}.

\bibitem[Riabiz et~al.(2022)Riabiz, Chen, Cockayne, Swietach, Niederer, Mackey,
  and Oates]{riabiz2020optimal}
M.~Riabiz, W.Y. Chen, J.~Cockayne, P.~Swietach, S.A. Niederer, L.~Mackey, and
  C.J. Oates.
\newblock Optimal thinning of {MCMC} output.
\newblock \emph{Journal of the Royal Statistical Society: Series B}, in press,
  2022.

\bibitem[Robert and Casella(1999)]{robert1999monte}
CP.P Robert and G.~Casella.
\newblock \emph{{Monte Carlo Statistical Methods}}, volume~2.
\newblock Springer, 1999.

\bibitem[Sejdinovic et~al.(2013)Sejdinovic, Sriperumbudur, Gretton, and
  Fukumizu]{sejdinovic2013equivalence}
D.~Sejdinovic, B.~Sriperumbudur, A.~Gretton, and K.~Fukumizu.
\newblock Equivalence of distance-based and rkhs-based statistics in hypothesis
  testing.
\newblock \emph{The Annals of Statistics}, pages 2263--2291, 2013.

\bibitem[South et~al.(2022)South, Riabiz, Teymur, and
  Oates]{south2022postprocessing}
L.F. South, M.~Riabiz, O.~Teymur, and C.J. Oates.
\newblock Postprocessing of mcmc.
\newblock \emph{Annual Review of Statistics and Its Application}, 9:\penalty0
  529--555, 2022.

\bibitem[Stein(1972)]{stein1972bound}
C.~Stein.
\newblock A bound for the error in the normal approximation to the distribution
  of a sum of dependent random variables.
\newblock In \emph{{Proceedings of the sixth Berkeley Symposium on Mathematical
  Statistics and Probability, Volume 2: Probability Theory}}, volume~6, pages
  583--603. University of California Press, 1972.

\bibitem[Wenliang and Kanagawa(2020)]{wenliang2020blindness}
L.K. Wenliang and H.~Kanagawa.
\newblock Blindness of score-based methods to isolated components and mixing
  proportions.
\newblock \emph{arXiv preprint arXiv:2008.10087}, 2020.

\end{thebibliography}
