\begin{thebibliography}{76}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akaike(1973)]{akaike1973information}
Akaike, H.
\newblock \emph{Information Theory and an Extension of the Maximum Likelihood Principle}, pp.\  199--213.
\newblock Springer New York, New York, NY, 1973.

\bibitem[Bae et~al.(2022)Bae, Ng, Lo, Ghassemi, and Grosse]{bae2022influence}
Bae, J., Ng, N., Lo, A., Ghassemi, M., and Grosse, R.~B.
\newblock If influence functions are the answer, then what is the question?
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~35, pp.\  17953--17967. Curran Associates, Inc., 2022.

\bibitem[Barron et~al.(1998)Barron, Rissanen, and Yu]{barron1998minimum}
Barron, A., Rissanen, J., and Yu, B.
\newblock The minimum description length principle in coding and modeling.
\newblock \emph{IEEE Transactions on Information Theory}, 44\penalty0 (6):\penalty0 2743--2760, 1998.
\newblock \doi{10.1109/18.720554}.

\bibitem[Bartlett \& Mendelson(2003)Bartlett and Mendelson]{bartlett2003rademacher}
Bartlett, P.~L. and Mendelson, S.
\newblock Rademacher and gaussian complexities: Risk bounds and structural results.
\newblock \emph{J. Mach. Learn. Res.}, 3\penalty0 (null):\penalty0 463–482, mar 2003.
\newblock ISSN 1532-4435.

\bibitem[Basu et~al.(2020)Basu, Pope, and Feizi]{basu2020influence}
Basu, S., Pope, P., and Feizi, S.
\newblock Influence functions in deep learning are fragile.
\newblock \emph{arXiv preprint arXiv:2006.14651}, 2020.

\bibitem[Bernardo \& Smith(1994)Bernardo and Smith]{bernardo1994bayesian}
Bernardo, J.~M. and Smith, A. F.~M.
\newblock \emph{Bayesian Theory}.
\newblock Wiley, 1994.

\bibitem[Bibas et~al.(2020)Bibas, Fogel, and Feder]{bibas2020deep}
Bibas, K., Fogel, Y., and Feder, M.
\newblock Deep pnml: Predictive normalized maximum likelihood for deep neural networks, 2020.

\bibitem[Bojarski et~al.(2016)Bojarski, Testa, Dworakowski, Firner, Flepp, Goyal, Jackel, Monfort, Muller, Zhang, Zhang, Zhao, and Zieba]{bojarski2016end}
Bojarski, M., Testa, D.~D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., Jackel, L.~D., Monfort, M., Muller, U., Zhang, J., Zhang, X., Zhao, J., and Zieba, K.
\newblock End to end learning for self-driving cars, 2016.

\bibitem[Brunet et~al.(2019)Brunet, Alkalay-Houlihan, Anderson, and Zemel]{pmlr-v97-brunet19a}
Brunet, M.-E., Alkalay-Houlihan, C., Anderson, A., and Zemel, R.
\newblock Understanding the origins of bias in word embeddings.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of the 36th International Conference on Machine Learning}, volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\  803--811. PMLR, 09--15 Jun 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/brunet19a.html}.

\bibitem[Cook(1979)]{75272a7e-1c8b-3ed5-9350-a7ee81abee59}
Cook, R.~D.
\newblock Influential observations in linear regression.
\newblock \emph{Journal of the American Statistical Association}, 74\penalty0 (365):\penalty0 169--174, 1979.
\newblock ISSN 01621459.
\newblock URL \url{http://www.jstor.org/stable/2286747}.

\bibitem[Cover \& Thomas(1991)Cover and Thomas]{elements1991cover}
Cover, T. and Thomas, J.
\newblock \emph{Elements of Information Theory}.
\newblock Wiley Interscience, 1991.

\bibitem[Deng(2012)]{deng2012mnist}
Deng, L.
\newblock The mnist database of handwritten digit images for machine learning research.
\newblock \emph{IEEE Signal Processing Magazine}, 29\penalty0 (6):\penalty0 141--142, 2012.

\bibitem[Dwivedi et~al.(2023)Dwivedi, Singh, Yu, and Wainwright]{JMLR:v24:21-1133}
Dwivedi, R., Singh, C., Yu, B., and Wainwright, M.
\newblock Revisiting minimum description length complexity in overparameterized models.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (268):\penalty0 1--59, 2023.
\newblock URL \url{http://jmlr.org/papers/v24/21-1133.html}.

\bibitem[Esteva et~al.(2017)Esteva, Kuprel, Novoa, Ko, Swetter, Blau, and Thrun]{Esteva2017}
Esteva, A., Kuprel, B., Novoa, R.~A., Ko, J., Swetter, S.~M., Blau, H.~M., and Thrun, S.
\newblock Dermatologist-level classification of skin cancer with deep neural networks.
\newblock \emph{Nature}, 542\penalty0 (7639):\penalty0 115--118, Feb 2017.
\newblock ISSN 1476-4687.
\newblock \doi{10.1038/nature21056}.
\newblock URL \url{https://doi.org/10.1038/nature21056}.

\bibitem[Farnia \& Tse(2017)Farnia and Tse]{farnia2017minimax}
Farnia, F. and Tse, D.
\newblock A minimax approach to supervised learning, 2017.

\bibitem[Feldman(2021)]{feldman2021does}
Feldman, V.
\newblock Does learning require memorization? a short tale about a long tail, 2021.

\bibitem[Feldman \& Zhang(2020)Feldman and Zhang]{feldman2020neural}
Feldman, V. and Zhang, C.
\newblock What neural networks memorize and why: Discovering the long tail via influence estimation, 2020.

\bibitem[Fisher et~al.(2023)Fisher, Liu, Pillutla, Choi, and Harchaoui]{pmlr-v206-fisher23a}
Fisher, J., Liu, L., Pillutla, K., Choi, Y., and Harchaoui, Z.
\newblock Influence diagnostics under self-concordance.
\newblock In Ruiz, F., Dy, J., and van~de Meent, J.-W. (eds.), \emph{Proceedings of The 26th International Conference on Artificial Intelligence and Statistics}, volume 206 of \emph{Proceedings of Machine Learning Research}, pp.\  10028--10076. PMLR, 25--27 Apr 2023.
\newblock URL \url{https://proceedings.mlr.press/v206/fisher23a.html}.

\bibitem[Fogel \& Feder(2018)Fogel and Feder]{8437543}
Fogel, Y. and Feder, M.
\newblock Universal batch learning with log-loss.
\newblock In \emph{2018 IEEE International Symposium on Information Theory (ISIT)}, pp.\  21--25, 2018.
\newblock \doi{10.1109/ISIT.2018.8437543}.

\bibitem[Fogel \& Feder(2019)Fogel and Feder]{fogel2019universal}
Fogel, Y. and Feder, M.
\newblock Universal learning of individual data.
\newblock In \emph{2019 IEEE International Symposium on Information Theory (ISIT)}, pp.\  2289--2293, 2019.
\newblock \doi{10.1109/ISIT.2019.8849222}.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{gal2016dropout}
Gal, Y. and Ghahramani, Z.
\newblock Dropout as a bayesian approximation: Representing model uncertainty in deep learning.
\newblock In Balcan, M.~F. and Weinberger, K.~Q. (eds.), \emph{Proceedings of The 33rd International Conference on Machine Learning}, volume~48 of \emph{Proceedings of Machine Learning Research}, pp.\  1050--1059, New York, New York, USA, 20--22 Jun 2016. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v48/gal16.html}.

\bibitem[George et~al.(2018)George, Laurent, Bouthillier, Ballas, and Vincent]{george2021fast}
George, T., Laurent, C., Bouthillier, X., Ballas, N., and Vincent, P.
\newblock Fast approximate natural gradient descent in a kronecker-factored eigenbasis.
\newblock In \emph{Proceedings of the 32nd Conference on Neural Information Processing Systems}, 2018.

\bibitem[Ghassemi \& Mohamed(2022)Ghassemi and Mohamed]{Ghassemi2022}
Ghassemi, M. and Mohamed, S.
\newblock Machine learning and health need better values.
\newblock \emph{npj Digital Medicine}, 5\penalty0 (1):\penalty0 51, Apr 2022.
\newblock ISSN 2398-6352.
\newblock \doi{10.1038/s41746-022-00595-9}.
\newblock URL \url{https://doi.org/10.1038/s41746-022-00595-9}.

\bibitem[Graves(2011)]{NIPS2011_7eb3c8be}
Graves, A.
\newblock Practical variational inference for neural networks.
\newblock In Shawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., and Weinberger, K. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~24. Curran Associates, Inc., 2011.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf}.

\bibitem[Griewank \& Walther(2008)Griewank and Walther]{griewank2008evaluating}
Griewank, A. and Walther, A.
\newblock \emph{Evaluating derivatives: principles and techniques of algorithmic differentiation}.
\newblock SIAM, 2008.

\bibitem[Grosse et~al.(2023)Grosse, Bae, Anil, Elhage, Tamkin, Tajdini, Steiner, Li, Durmus, Perez, Hubinger, Lukošiūtė, Nguyen, Joseph, McCandlish, Kaplan, and Bowman]{grosse2023studying}
Grosse, R., Bae, J., Anil, C., Elhage, N., Tamkin, A., Tajdini, A., Steiner, B., Li, D., Durmus, E., Perez, E., Hubinger, E., Lukošiūtė, K., Nguyen, K., Joseph, N., McCandlish, S., Kaplan, J., and Bowman, S.~R.
\newblock Studying large language model generalization with influence functions, 2023.

\bibitem[Grunwald(2004)]{grunwald2004tutorial}
Grunwald, P.
\newblock A tutorial introduction to the minimum description length principle, 2004.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{pmlr-v70-guo17a}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th International Conference on Machine Learning}, volume~70 of \emph{Proceedings of Machine Learning Research}, pp.\  1321--1330. PMLR, 06--11 Aug 2017.
\newblock URL \url{https://proceedings.mlr.press/v70/guo17a.html}.

\bibitem[Hampel(1974)]{bd831960-ac2b-396a-8c8f-de3944255f11}
Hampel, F.~R.
\newblock The influence curve and its role in robust estimation.
\newblock \emph{Journal of the American Statistical Association}, 69\penalty0 (346):\penalty0 383--393, 1974.
\newblock ISSN 01621459.
\newblock URL \url{http://www.jstor.org/stable/2285666}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  770--778, 2016.
\newblock \doi{10.1109/CVPR.2016.90}.

\bibitem[Hendrycks \& Dietterich(2019)Hendrycks and Dietterich]{hendrycks2019robustness}
Hendrycks, D. and Dietterich, T.
\newblock Benchmarking neural network robustness to common corruptions and perturbations.
\newblock \emph{Proceedings of the International Conference on Learning Representations}, 2019.

\bibitem[Hinton \& van Camp(1993)Hinton and van Camp]{10.1145/168304.168306}
Hinton, G.~E. and van Camp, D.
\newblock Keeping the neural networks simple by minimizing the description length of the weights.
\newblock In \emph{Proceedings of the Sixth Annual Conference on Computational Learning Theory}, COLT '93, pp.\  5–13, New York, NY, USA, 1993. Association for Computing Machinery.
\newblock ISBN 0897916115.
\newblock \doi{10.1145/168304.168306}.
\newblock URL \url{https://doi.org/10.1145/168304.168306}.

\bibitem[Hinton \& Zemel(1993)Hinton and Zemel]{NIPS1993_9e3cfc48}
Hinton, G.~E. and Zemel, R.
\newblock Autoencoders, minimum description length and helmholtz free energy.
\newblock In Cowan, J., Tesauro, G., and Alspector, J. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~6. Morgan-Kaufmann, 1993.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/1993/file/9e3cfc48eccf81a0d57663e129aef3cb-Paper.pdf}.

\bibitem[Ilyas et~al.(2022)Ilyas, Park, Engstrom, Leclerc, and Madry]{ilyas2022datamodels}
Ilyas, A., Park, S.~M., Engstrom, L., Leclerc, G., and Madry, A.
\newblock Datamodels: Predicting predictions from training data, 2022.

\bibitem[Izmailov et~al.(2019)Izmailov, Podoprikhin, Garipov, Vetrov, and Wilson]{izmailov2019averaging}
Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A.~G.
\newblock Averaging weights leads to wider optima and better generalization, 2019.

\bibitem[Kass \& Raftery(1995)Kass and Raftery]{kass1995bayes}
Kass, R.~E. and Raftery, A.~E.
\newblock Bayes factors.
\newblock \emph{Journal of the American Statistical Association}, 90\penalty0 (430):\penalty0 773--795, 1995.
\newblock \doi{10.1080/01621459.1995.10476572}.
\newblock URL \url{/brokenurl# http://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476572}.

\bibitem[Kim et~al.(2021)Kim, Oh, Kim, Cho, and Yun]{kim2021comparing}
Kim, T., Oh, J., Kim, N., Cho, S., and Yun, S.-Y.
\newblock Comparing kullback-leibler divergence and mean squared error loss in knowledge distillation, 2021.

\bibitem[Koh \& Liang(2017)Koh and Liang]{koh2017understanding}
Koh, P.~W. and Liang, P.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1885--1894. PMLR, 2017.

\bibitem[Koh et~al.(2021)Koh, Steinhardt, and Liang]{koh2021stronger}
Koh, P.~W., Steinhardt, J., and Liang, P.
\newblock Stronger data poisoning attacks break data sanitization defenses, 2021.

\bibitem[Kolmogorov(1965)]{kolmogorov65}
Kolmogorov, A.~N.
\newblock Three approaches to the quantitative definition of information.
\newblock \emph{Problems of Information Transmission}, 1:\penalty0 1--7, 1965.

\bibitem[Krantz \& Parks(2002)Krantz and Parks]{krantz2002implicit}
Krantz, S.~G. and Parks, H.~R.
\newblock \emph{The implicit function theorem: history, theory, and applications}.
\newblock Springer Science \& Business Media, 2002.

\bibitem[Krizhevsky(2009)]{Krizhevsky2009learning}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and Blundell]{balaji2017ensembles}
Lakshminarayanan, B., Pritzel, A., and Blundell, C.
\newblock Simple and scalable predictive uncertainty estimation using deep ensembles.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf}.

\bibitem[Lecun et~al.(1998)Lecun, Bottou, Bengio, and Haffner]{lecun1998lenet}
Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0 2278--2324, 1998.
\newblock \doi{10.1109/5.726791}.

\bibitem[Mackay(1992)]{mackay1992bayesian}
Mackay, D. J.~C.
\newblock \emph{Bayesian methods for adaptive models}.
\newblock PhD thesis, USA, 1992.
\newblock UMI Order No. GAX92-32200.

\bibitem[Maddox et~al.(2019)Maddox, Izmailov, Garipov, Vetrov, and Wilson]{maddox2019swag}
Maddox, W.~J., Izmailov, P., Garipov, T., Vetrov, D.~P., and Wilson, A.~G.
\newblock A simple baseline for bayesian uncertainty in deep learning.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/118921efba23fc329e6560b27861f0c2-Paper.pdf}.

\bibitem[Naeini et~al.(2015)Naeini, Cooper, and Hauskrecht]{naeini2015ece}
Naeini, M.~P., Cooper, G., and Hauskrecht, M.
\newblock Obtaining well calibrated probabilities using bayesian binning.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 29\penalty0 (1), Feb. 2015.
\newblock \doi{10.1609/aaai.v29i1.9602}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/9602}.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and Van~Roy]{NIPS2016_8d8818c8}
Osband, I., Blundell, C., Pritzel, A., and Van~Roy, B.
\newblock Deep exploration via bootstrapped dqn.
\newblock In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~29. Curran Associates, Inc., 2016.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2016/file/8d8818c8e140c64c743113f563cf750f-Paper.pdf}.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin, Dillon, Lakshminarayanan, and Snoek]{ovadia2019trust}
Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon, J., Lakshminarayanan, B., and Snoek, J.
\newblock Can you trust your model\textquotesingle s uncertainty? evaluating predictive uncertainty under dataset shift.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf}.

\bibitem[Park et~al.(2023)Park, Georgiev, Ilyas, Leclerc, and Madry]{park2023trak}
Park, S.~M., Georgiev, K., Ilyas, A., Leclerc, G., and Madry, A.
\newblock Trak: Attributing model behavior at scale.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2023.

\bibitem[Paul et~al.(2021)Paul, Ganguli, and Dziugaite]{el2n2021paul}
Paul, M., Ganguli, S., and Dziugaite, G.~K.
\newblock Deep learning on a data diet: Finding important examples early in training.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~34, pp.\  20596--20607. Curran Associates, Inc., 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2021/file/ac56f8fe9eea3e4a365f29f0f1957c55-Paper.pdf}.

\bibitem[Perotti et~al.(2018)Perotti, Tessone, Clauset, and Caldarelli]{perotti2018thermodynamics}
Perotti, J.~I., Tessone, C.~J., Clauset, A., and Caldarelli, G.
\newblock Thermodynamics of the minimum description length on community detection, 2018.

\bibitem[Platt(2000)]{PlattProbabilisticOutputs1999}
Platt, J.
\newblock Probabilistic outputs for support vector machines and comparison to regularized likelihood methods.
\newblock In \emph{Advances in Large Margin Classifiers}, 2000.

\bibitem[Pruthi et~al.(2020)Pruthi, Liu, Kale, and Sundararajan]{tracin2020pruthi}
Pruthi, G., Liu, F., Kale, S., and Sundararajan, M.
\newblock Estimating training data influence by tracing gradient descent.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  19920--19930. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/e6385d39ec9394f2f3a354d9d2b88eec-Paper.pdf}.

\bibitem[Ren et~al.(2021)Ren, Fort, Liu, Roy, Padhy, and Lakshminarayanan]{ren2021simple}
Ren, J., Fort, S., Liu, J., Roy, A.~G., Padhy, S., and Lakshminarayanan, B.
\newblock A simple fix to mahalanobis distance for improving near-ood detection, 2021.

\bibitem[Rissanen(1978)]{mdl1978rissanen}
Rissanen, J.
\newblock Modeling by shortest data description.
\newblock \emph{Automatica}, 14\penalty0 (5):\penalty0 465--471, 1978.
\newblock ISSN 0005-1098.
\newblock \doi{https://doi.org/10.1016/0005-1098(78)90005-5}.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/0005109878900055}.

\bibitem[Rissanen(1986)]{rissanen1986complexity}
Rissanen, J.
\newblock Stochastic complexity and modeling.
\newblock \emph{The Annals of Statistics}, 14\penalty0 (3):\penalty0 1080--1100, 1986.
\newblock ISSN 00905364.
\newblock URL \url{http://www.jstor.org/stable/3035559}.

\bibitem[Rissanen(1996)]{rissanen1996fisher}
Rissanen, J.
\newblock Fisher information and stochastic complexity.
\newblock \emph{IEEE Transactions on Information Theory}, 42\penalty0 (1):\penalty0 40--47, 1996.
\newblock \doi{10.1109/18.481776}.

\bibitem[Ritter et~al.(2018)Ritter, Botev, and Barber]{ritter2018laplace}
Ritter, H., Botev, A., and Barber, D.
\newblock A scalable laplace approximation for neural networks.
\newblock In \emph{Proceedings of ICLR}, 2018.

\bibitem[Roos \& Rissanen(2008)Roos and Rissanen]{sequentially2008roos}
Roos, T. and Rissanen, J.
\newblock On sequentially normalized maximum likelihood models.
\newblock 01 2008.

\bibitem[Roos et~al.(2008)Roos, Silander, Kontkanen, and Myllymaki]{roos2008bayesian}
Roos, T., Silander, T., Kontkanen, P., and Myllymaki, P.
\newblock Bayesian network structure learning using factorized nml universal models.
\newblock In \emph{2008 Information Theory and Applications Workshop}, pp.\  272--276, 2008.
\newblock \doi{10.1109/ITA.2008.4601061}.

\bibitem[Schulam \& Saria(2019)Schulam and Saria]{pmlr-v89-schulam19a}
Schulam, P. and Saria, S.
\newblock Can you trust this prediction? auditing pointwise reliability after learning.
\newblock In Chaudhuri, K. and Sugiyama, M. (eds.), \emph{Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics}, volume~89 of \emph{Proceedings of Machine Learning Research}, pp.\  1022--1031. PMLR, 16--18 Apr 2019.
\newblock URL \url{https://proceedings.mlr.press/v89/schulam19a.html}.

\bibitem[Schwarz(1978)]{10.1214/aos/1176344136}
Schwarz, G.
\newblock {Estimating the Dimension of a Model}.
\newblock \emph{The Annals of Statistics}, 6\penalty0 (2):\penalty0 461 -- 464, 1978.
\newblock \doi{10.1214/aos/1176344136}.
\newblock URL \url{https://doi.org/10.1214/aos/1176344136}.

\bibitem[Shtar'kov(1987)]{universal1987shtarkov}
Shtar'kov, Y.~M.
\newblock Universal sequential coding of single messages.
\newblock \emph{Problemy Peredachi Informatsii}, 23:\penalty0 175--186, 1987.

\bibitem[Solomonoff(1964)]{SOLOMONOFF19641}
Solomonoff, R.
\newblock A formal theory of inductive inference. part i.
\newblock \emph{Information and Control}, 7\penalty0 (1):\penalty0 1--22, 1964.
\newblock ISSN 0019-9958.
\newblock \doi{https://doi.org/10.1016/S0019-9958(64)90223-2}.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/S0019995864902232}.

\bibitem[Srikanth et~al.(2023)Srikanth, Irvin, Hill, Godoy, Sabane, and Ng]{srikanth2023empirical}
Srikanth, M., Irvin, J., Hill, B.~W., Godoy, F., Sabane, I., and Ng, A.~Y.
\newblock An empirical study of automated mislabel detection in real world vision datasets, 2023.

\bibitem[Stine \& Foster(2001)Stine and Foster]{foster2001competitive}
Stine, R.~A. and Foster, D.~P.
\newblock The competitive complexity ratio.
\newblock In \emph{Proceedings of the 2001 Conference on Information Sciences and Systems}, 2001.

\bibitem[Tanno et~al.(2022)Tanno, F.~Pradier, Nori, and Li]{NEURIPS2022_552260cf}
Tanno, R., F.~Pradier, M., Nori, A., and Li, Y.
\newblock Repairing neural networks by leaving the right past behind.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~35, pp.\  13132--13145. Curran Associates, Inc., 2022.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/552260cfb5e292e511eaa780806ac984-Paper-Conference.pdf}.

\bibitem[Vapnik \& Chervonenkis(1971)Vapnik and Chervonenkis]{vapnik1971uniform}
Vapnik, V.~N. and Chervonenkis, A.~Y.
\newblock On the uniform convergence of relative frequencies of events to their probabilities.
\newblock \emph{Theory of Probability \& Its Applications}, 16\penalty0 (2):\penalty0 264--280, 1971.
\newblock \doi{10.1137/1116025}.
\newblock URL \url{https://doi.org/10.1137/1116025}.

\bibitem[Wei et~al.(2022)Wei, Zhu, Cheng, Liu, Niu, and Liu]{wei2021learning}
Wei, J., Zhu, Z., Cheng, H., Liu, T., Niu, G., and Liu, Y.
\newblock Learning with noisy labels revisited: A study using real-world human annotations.
\newblock In \emph{Proceedings of the International Conference on Learning Representations}, 2022.

\bibitem[Welling \& Teh(2011)Welling and Teh]{10.5555/3104482.3104568}
Welling, M. and Teh, Y.~W.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In \emph{Proceedings of the 28th International Conference on International Conference on Machine Learning}, ICML'11, pp.\  681–688, Madison, WI, USA, 2011. Omnipress.
\newblock ISBN 9781450306195.

\bibitem[Yang et~al.(2022)Yang, Wang, Zou, Zhou, Ding, Peng, Wang, Chen, Li, Sun, Du, Zhou, Zhang, Hendrycks, Li, and Liu]{yang2022openood}
Yang, J., Wang, P., Zou, D., Zhou, Z., Ding, K., Peng, W., Wang, H., Chen, G., Li, B., Sun, Y., Du, X., Zhou, K., Zhang, W., Hendrycks, D., Li, Y., and Liu, Z.
\newblock Openood: Benchmarking generalized out-of-distribution detection.
\newblock 2022.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and Vinyals]{zhang2017understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization, 2017.

\bibitem[Zhang et~al.(2023)Zhang, Yang, Wang, Wang, Lin, Zhang, Sun, Du, Zhou, Zhang, Li, Liu, Chen, and Li]{zhang2023openood}
Zhang, J., Yang, J., Wang, P., Wang, H., Lin, Y., Zhang, H., Sun, Y., Du, X., Zhou, K., Zhang, W., Li, Y., Liu, Z., Chen, Y., and Li, H.
\newblock Openood v1.5: Enhanced benchmark for out-of-distribution detection.
\newblock \emph{arXiv preprint arXiv:2306.09301}, 2023.

\bibitem[Zhou \& Levine(2021)Zhou and Levine]{zhou2021amortized}
Zhou, A. and Levine, S.
\newblock Amortized conditional normalized maximum likelihood: Reliable out of distribution uncertainty estimation, 2021.

\bibitem[Zhu et~al.(2021)Zhu, Song, and Liu]{clusterability2021zhu}
Zhu, Z., Song, Y., and Liu, Y.
\newblock Clusterability as an alternative to anchor points when learning with noisy labels.
\newblock In \emph{Proceedings of ICML}, 2021.

\end{thebibliography}
