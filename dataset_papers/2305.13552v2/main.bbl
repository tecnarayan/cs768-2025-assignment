\begin{thebibliography}{10}

\bibitem{Bach2017}
Francis Bach.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock {\em Journal of Machine Learning Research}, 18(19):1--53, 2017.

\bibitem{beck2017realistic}
R~Beck, C-A Lin, EEO Ishida, F~Gieseke, RS~de~Souza, MV~Costa-Duarte,
  MW~Hattab, and A~Krone-Martins.
\newblock On the realistic validation of photometric redshifts.
\newblock {\em Monthly Notices of the Royal Astronomical Society},
  468(4):4323--4339, 2017.

\bibitem{bernal2021training}
Edgar~A Bernal.
\newblock Training deep normalizing flow models in highly incomplete data
  scenarios with prior regularization.
\newblock {\em arXiv preprint arXiv:2104.01482}, 2021.

\bibitem{NIPS2009_3628}
Youngmin Cho and Lawrence~K. Saul.
\newblock Kernel methods for deep learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  342--350, 2009.

\bibitem{collins2001generalization}
Michael Collins, Sanjoy Dasgupta, and Robert~E Schapire.
\newblock A generalization of principal components analysis to the exponential
  family.
\newblock {\em Advances in neural information processing systems}, 14, 2001.

\bibitem{deisenroth2020mathematics}
Marc~Peter Deisenroth, A~Aldo Faisal, and Cheng~Soon Ong.
\newblock {\em Mathematics for machine learning}.
\newblock Cambridge University Press, 2020.

\bibitem{dinh2017density}
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
\newblock Density estimation using real {NVP}.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{efron2021computer}
Bradley Efron and Trevor Hastie.
\newblock {\em Computer Age Statistical Inference: Algorithms, Evidence, and
  Data Science}, volume~6.
\newblock Cambridge University Press, 2021.

\bibitem{flaxman2017poisson}
Seth Flaxman, Yee~Whye Teh, and Dino Sejdinovic.
\newblock Poisson intensity estimation with reproducing kernels.
\newblock In {\em Artificial Intelligence and Statistics}, pages 270--279.
  PMLR, 2017.

\bibitem{glusenkamp2020unifying}
Thorsten Gl{\"u}senkamp.
\newblock Unifying supervised learning and vaes--automating statistical
  inference in (astro-) particle physics with amortized conditional normalizing
  flows.
\newblock {\em arXiv e-prints}, 2020.

\bibitem{NIPS2014_5ca3e9b1}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~27, 2014.

\bibitem{NEURIPS2022_e7be1f4c}
Insu Han, Amir Zandieh, Jaehoon Lee, Roman Novak, Lechao Xiao, and Amin
  Karbasi.
\newblock Fast neural kernel embeddings for general activations.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~35, pages 35657--35671, 2022.

\bibitem{51790}
Insu Han, Amir Zandieh, Jaehoon Lee, Roman Novak, Lechao Xiao, and Amin
  Karbasi.
\newblock Fast neural kernel embeddings for general activations.
\newblock {\em Advances in neural information processing systems}, 2022.

\bibitem{hendrycks2016gaussian}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units.
\newblock {\em arXiv preprint arXiv:1606.08415}, 2016.

\bibitem{hinton2002training}
Geoffrey~E Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock {\em Neural computation}, 14(8):1771--1800, 2002.

\bibitem{hyvarinen2005estimation}
Aapo Hyv{\"a}rinen and Peter Dayan.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock {\em Journal of Machine Learning Research}, 6(4), 2005.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational {B}ayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{lawrence2018point}
Thomas~Joseph Lawrence.
\newblock Point pattern analysis on a sphere.
\newblock Master's thesis, The University of Western Australia, 2018.

\bibitem{le2007continuous}
Nicolas Le~Roux and Yoshua Bengio.
\newblock Continuous neural networks.
\newblock In {\em Artificial Intelligence and Statistics}, pages 404--411,
  2007.

\bibitem{lecun2006tutorial}
Yann LeCun, Sumit Chopra, Raia Hadsell, M~Ranzato, and Fujie Huang.
\newblock A tutorial on energy-based learning.
\newblock {\em Predicting structured data}, 1(0), 2006.

\bibitem{lee2018deep}
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel~S Schoenholz, Jeffrey
  Pennington, and Jascha Sohl-Dickstein.
\newblock Deep neural networks as {G}aussian processes.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{loconte2023negative}
Lorenzo Loconte, Stefan Mengel, Nicolas Gillis, and Antonio Vergari.
\newblock Negative mixture models via squaring: Representation and learning.
\newblock In {\em The 6th Workshop on Tractable Probabilistic Modeling}, 2023.

\bibitem{mackay1998introduction}
David~JC MacKay.
\newblock Introduction to {G}aussian processes, 1998.

\bibitem{marteau2020non}
Ulysse Marteau-Ferey, Francis Bach, and Alessandro Rudi.
\newblock Non-parametric models for non-negative functions.
\newblock {\em Advances in neural information processing systems},
  33:12816--12826, 2020.

\bibitem{marteau2022sampling}
Ulysse Marteau-Ferey, Francis Bach, and Alessandro Rudi.
\newblock Sampling from arbitrary functions via psd models.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2823--2861. PMLR, 2022.

\bibitem{mccullagh1989generalized}
P.~McCullagh and J.A. Nelder.
\newblock {\em Generalized Linear Models, Second Edition}.
\newblock Monographs on Statistics and Applied Probability Series. Chapman \&
  Hall, 1989.

\bibitem{meronen2020stationary}
Lassi Meronen, Christabella Irwanto, and Arno Solin.
\newblock Stationary activations for uncertainty calibration in deep learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:2338--2350, 2020.

\bibitem{meronen2021periodic}
Lassi Meronen, Martin Trapp, and Arno Solin.
\newblock Periodic activation functions induce stationarity.
\newblock {\em Advances in Neural Information Processing Systems},
  34:1673--1685, 2021.

\bibitem{nash2019autoregressive}
Charlie Nash and Conor Durkan.
\newblock Autoregressive energy machines.
\newblock In {\em International Conference on Machine Learning}, pages
  1735--1744, 2019.

\bibitem{neal1995bayesian}
Radford~M Neal.
\newblock {\em Bayesian learning for neural networks}.
\newblock PhD thesis, University of Toronto, 1995.

\bibitem{nielsen2009statistical}
Frank Nielsen and Vincent Garcia.
\newblock Statistical exponential families: A digest with flash cards.
\newblock {\em arXiv preprint arXiv:0911.4863}, 2009.

\bibitem{novak2018bayesian}
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron,
  Daniel~A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein.
\newblock Bayesian deep convolutional networks with many channels are
  {G}aussian processes.
\newblock In {\em The International Conference on Learning Representations},
  2019.

\bibitem{papamakarios2021normalizing}
George Papamakarios, Eric Nalisnick, Danilo~Jimenez Rezende, Shakir Mohamed,
  and Balaji Lakshminarayanan.
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock {\em The Journal of Machine Learning Research}, 22(1):2617--2680,
  2021.

\bibitem{papamakarios2019neural}
Georgios Papamakarios.
\newblock {\em Neural density estimation and likelihood-free inference}.
\newblock PhD thesis, University of Edinburgh, 2019.

\bibitem{park1991universal}
Jooyoung Park and Irwin~W Sandberg.
\newblock Universal approximation using radial-basis-function networks.
\newblock {\em Neural computation}, 3(2):246--257, 1991.

\bibitem{pearce2019expressive}
Tim Pearce, Russell Tsuchida, Mohamed Zaki, Alexandra Brintrup, and Andy Neely.
\newblock Expressive priors in {B}ayesian neural networks: Kernel combinations
  and periodic functions.
\newblock In {\em Uncertainty in Artificial Intelligence}, 2019.

\bibitem{rahimi2007random}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock {\em Advances in neural information processing systems}, 20, 2007.

\bibitem{ramachandran2018searching}
Prajit Ramachandran, Barret Zoph, and Quoc~V. Le.
\newblock Searching for activation functions, 2018.

\bibitem{rezende2015variational}
Danilo Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In {\em International conference on machine learning}, pages
  1530--1538, 2015.

\bibitem{richardson2020mcflow}
Trevor~W Richardson, Wencheng Wu, Lei Lin, Beilei Xu, and Edgar~A Bernal.
\newblock Mcflow: Monte carlo flow models for data imputation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 14205--14214, 2020.

\bibitem{rudi_ciliberto}
Alessandro Rudi and Carlo Ciliberto.
\newblock {PSD} representations for effective probability models.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~34, pages 19411--19422, 2021.

\bibitem{Sladek2023}
Aleksanteri Sladek.
\newblock {Positive Semi-Definite Probabilistic Circuits}.
\newblock Master's thesis, Aalto University. School of Science, 2023.

\bibitem{steinicke}
W.~Steinicke.
\newblock Revised new general catalogue and index catalogue (revised ngc/ic).
\newblock \url{http://www.klima-luft.de/steinicke/index_e.htm}, 2015.
\newblock Accessed 2nd May 2015.

\bibitem{stimper2023normflows}
Vincent Stimper, David Liu, Andrew Campbell, Vincent Berenz, Lukas Ryll,
  Bernhard Sch{\"o}lkopf, and Jos{\'e}~Miguel Hern{\'a}ndez-Lobato.
\newblock normflows: A {PyTorch} package for normalizing flows.
\newblock {\em arXiv preprint arXiv:2302.12014}, 2023.

\bibitem{stimper2022resampling}
Vincent Stimper, Bernhard Sch{\"o}lkopf, and Jos{\'e}~Miguel
  Hern{\'a}ndez-Lobato.
\newblock Resampling {B}ase {D}istributions of {N}ormalizing {F}lows.
\newblock In {\em Proceedings of The 25th International Conference on
  Artificial Intelligence and Statistics}, volume 151, pages 4915--4936, 2022.

\bibitem{tsuchida2021avoiding}
Russell Tsuchida, Tim Pearce, Chris van~der Heide, Fred Roosta, and Marcus
  Gallagher.
\newblock Avoiding kernel fixed points: Computing with {ELU} and {GELU}
  infinite networks.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 9967--9977, 2021.

\bibitem{tsuchida2018invariance}
Russell Tsuchida, Fred Roosta, and Marcus Gallagher.
\newblock Invariance of weight distributions in rectified {MLP}s.
\newblock In {\em International Conference on Machine Learning}, pages
  5002--5011, 2018.

\bibitem{tsuchida2019richer}
Russell Tsuchida, Fred Roosta, and Marcus Gallagher.
\newblock Richer priors for infinitely wide multi-layer perceptrons.
\newblock {\em arXiv preprint arXiv:1911.12927}, 2019.

\bibitem{wainwright2008graphical}
Martin~J Wainwright, Michael~I Jordan, et~al.
\newblock Graphical models, exponential families, and variational inference.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  1(1--2):1--305, 2008.

\bibitem{walder2017fast}
Christian~J Walder and Adrian~N Bishop.
\newblock Fast bayesian intensity estimation for the permanental process.
\newblock In {\em International Conference on Machine Learning}, pages
  3579--3588. PMLR, 2017.

\bibitem{williams1997computing}
Christopher~KI Williams.
\newblock Computing with infinite networks.
\newblock In {\em Advances in neural information processing systems}, pages
  295--301, 1997.

\bibitem{williams2006gaussian}
Christopher~KI Williams and Carl~Edward Rasmussen.
\newblock {\em Gaussian processes for machine learning}.
\newblock MIT press Cambridge, MA, 2006.

\bibitem{winkler2019learning}
Christina Winkler, Daniel Worrall, Emiel Hoogeboom, and Max Welling.
\newblock Learning likelihoods with conditional normalizing flows, 2019.

\bibitem{NEURIPS2020_11604531}
Liu Ziyin, Tilman Hartwig, and Masahito Ueda.
\newblock Neural networks fail to learn periodic functions and how to fix it.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 1583--1594, 2020.

\end{thebibliography}
