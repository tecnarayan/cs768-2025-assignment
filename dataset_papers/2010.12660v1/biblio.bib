@Inbook{hastie2009elements,
author="Hastie, Trevor
and Tibshirani, Robert
and Friedman, Jerome",
title="Linear Methods for Regression",
bookTitle="The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
year="2009",
publisher="Springer New York",
address="New York, NY",
pages="43--99",
isbn="978-0-387-84858-7",
doi="10.1007/978-0-387-84858-7_3",
url="https://doi.org/10.1007/978-0-387-84858-7_3"
}

@article{lipshutz2020biologically,
      title={A biologically plausible neural network for multi-channel Canonical Correlation Analysis}, 
      author={David Lipshutz and Yanis Bahroun and Siavash Golkar and Anirvan M. Sengupta and Dmitri B. Chklovskii},
      year={2020},
    journal={arXiv preprint arXiv:2010.00525}
}

@article{IQMD,
  title={Computing the reduced rank {W}iener filter by {IQMD}},
  author={Hua, Yingbo and Nikpour, Maziar},
  journal={IEEE Signal Processing Letters},
  volume={6},
  number={9},
  pages={240--242},
  year={1999},
  publisher={IEEE}
}@book{rrr_book,
  title={Multivariate Reduced-Rank Regression: Theory and Applications},
  author={Velu, Raja and Reinsel, Gregory C},
  volume={136},
  year={2013},
  publisher={Springer Science \& Business Media}
}@article{de2012least,
  title={A least-squares framework for component analysis},
  author={De la Torre, Fernando},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={34},
  number={6},
  pages={1041--1055},
  year={2012},
  publisher={IEEE}
}@article{diamantaras1994multilayer,
  title={Multilayer neural networks for reduced-rank approximation},
  author={Diamantaras, Konstantinos I and Kung, Sun-Yuan},
  journal={IEEE Transactions on Neural Networks},
  volume={5},
  number={5},
  pages={684--697},
  year={1994},
  publisher={IEEE}
}
@incollection{rao2005probabilistic,
  title={Probabilistic models of attention based on iconic representations and predictive coding},
  author={Rao, Rajesh PN and Ballard, Dana H},
  booktitle={Neurobiology of Attention},
  pages={553--561},
  year={2005},
  publisher={Elsevier}
}
@article{bittner2017behavioral,
  title={Behavioral time scale synaptic plasticity underlies CA1 place fields},
  author={Bittner, Katie C and Milstein, Aaron D and Grienberger, Christine and Romani, Sandro and Magee, Jeffrey C},
  journal={Science},
  volume={357},
  number={6355},
  pages={1033--1036},
  year={2017},
  publisher={American Association for the Advancement of Science}
}
@article{olshausen1996emergence,
  title={Emergence of simple-cell receptive field properties by learning a sparse code for natural images},
  author={Olshausen, Bruno A and Field, David J},
  journal={Nature},
  volume={381},
  number={6583},
  pages={607--609},
  year={1996},
  publisher={Nature Publishing Group}
}
@article{dan1996efficient,
  title={Efficient coding of natural scenes in the lateral geniculate nucleus: experimental test of a computational theory},
  author={Dan, Yang and Atick, Joseph J and Reid, R Clay},
  journal={Journal of Neuroscience},
  volume={16},
  number={10},
  pages={3351--3362},
  year={1996},
  publisher={Soc Neuroscience}
}
@article{kording2001supervised,
  title={Supervised and unsupervised learning with two sites of synaptic integration},
  author={K{\"o}rding, Konrad P and K{\"o}nig, Peter},
  journal={Journal of Computational Neuroscience},
  volume={11},
  number={3},
  pages={207--215},
  year={2001},
  publisher={Springer}
}
@article{barshan2011supervised,
  title={Supervised principal component analysis: Visualization, classification and regression on subspaces and submanifolds},
  author={Barshan, Elnaz and Ghodsi, Ali and Azimifar, Zohreh and Jahromi, Mansoor Zolghadri},
  journal={Pattern Recognition},
  volume={44},
  number={7},
  pages={1357--1371},
  year={2011},
  publisher={Elsevier}
}
@article{simoncelli2003vision,
  title={Vision and the statistics of the visual environment},
  author={Simoncelli, Eero P},
  journal={Current Opinion in Neurobiology},
  volume={13},
  number={2},
  pages={144--149},
  year={2003},
  publisher={Elsevier}
}
@article{wanner2020whitening,
  title={Whitening of odor representations by the wiring diagram of the olfactory bulb},
  author={Wanner, Adrian A and Friedrich, Rainer W},
  journal={Nature Neuroscience},
  volume={23},
  number={3},
  pages={433--442},
  year={2020},
  publisher={Nature Publishing Group}
}
@article{srinivasan1982predictive,
  title={Predictive coding: a fresh view of inhibition in the retina},
  author={Srinivasan, Mandyam V and Laughlin, Simon B and Dubs, Andreas},
  journal={Proceedings of the Royal Society of London. Series B. Biological Sciences},
  volume={216},
  number={1205},
  pages={427--459},
  year={1982},
  publisher={The Royal Society London}
}



@article{keller2018predictive,
  title={Predictive processing: a canonical cortical computation},
  author={Keller, Georg B and Mrsic-Flogel, Thomas D},
  journal={Neuron},
  volume={100},
  number={2},
  pages={424--435},
  year={2018},
  publisher={Elsevier}
}
@article{gilbert2013top,
  title={Top-down influences on visual processing},
  author={Gilbert, Charles D and Li, Wu},
  journal={Nature Reviews Neuroscience},
  volume={14},
  number={5},
  pages={350--363},
  year={2013},
  publisher={Nature Publishing Group}
}
@article{anderson1951estimating,
  title={Estimating linear restrictions on regression coefficients for multivariate normal distributions},
  author={Anderson, Theodore Wilbur},
  journal={The Annals of Mathematical Statistics},
  volume={22},
  number={3},
  pages={327--351},
  year={1951},
  publisher={Institute of Mathematical Statistics}
}@article{izenman1975reduced,
  title={Reduced-rank regression for the multivariate linear model},
  author={Izenman, Alan J},
  journal={Journal of Multivariate Analysis},
  volume={5},
  number={2},
  pages={248--264},
  year={1975},
  publisher={Elsevier}
}@article{golding2002,
  title={Dendritic spikes as a mechanism for cooperative long-term potentiation},
  author={Golding, Nace L and Staff, Nathan P and Spruston, Nelson},
  journal={Nature},
  volume={418},
  number={6895},
  pages={326--331},
  year={2002},
  publisher={Nature Publishing Group}
}
@article{sjostrom2006,
  title={A cooperative switch determines the sign of synaptic plasticity in distal dendrites of neocortical pyramidal neurons},
  author={Sj{\"o}str{\"o}m, Per Jesper and H{\"a}usser, Michael},
  journal={Neuron},
  volume={51},
  number={2},
  pages={227--238},
  year={2006},
  publisher={Elsevier}
}
@article{hotelling1936relations,
  title={Relations between two sets of variates},
  author={Hotelling, Harold},
  journal={Biometrika},
  volume={28},
  number={3-4},
  pages={321--377},
  year={1936},
  publisher={Oxford University Press}
}
@article{bittner2015,
  title={Conjunctive input processing drives feature selectivity in hippocampal CA1 neurons},
  author={Bittner, Katie C and Grienberger, Christine and Vaidya, Sachin P and Milstein, Aaron D and Macklin, John J and Suh, Junghyup and Tonegawa, Susumu and Magee, Jeffrey C},
  journal={Nature neuroscience},
  volume={18},
  number={8},
  pages={1133},
  year={2015},
  publisher={Nature Publishing Group}
}
@article{takahashimagee,
  title={Pathway interactions and synaptic plasticity in the dendritic tuft regions of {CA1} pyramidal neurons},
  author={Takahashi, Hiroto and Magee, Jeffrey C},
  journal={Neuron},
  volume={62},
  number={1},
  pages={102--111},
  year={2009},
  publisher={Elsevier}
}
@article{Larkum2013,
    title={A cellular mechanism for cortical associations: an organizing principle for the cerebral cortex},
  author={Larkum, Matthew},
  journal={Trends in Neurosciences},
  volume={36},
  number={3},
  pages={141--151},
  year={2013},
  publisher={Elsevier}
}

@article{kompella2012incremental,
  title={Incremental slow feature analysis: Adaptive low-complexity slow feature updating from high-dimensional input streams},
  author={Kompella, Varun Raj and Luciw, Matthew and Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={24},
  number={11},
  pages={2994--3024},
  year={2012},
  publisher={MIT Press}
}
@inproceedings{pehlevan2015normative,
  title={A normative theory of adaptive dimensionality reduction in neural networks},
  author={Pehlevan, Cengiz and Chklovskii, Dmitri},
  booktitle={NeurIPS},
  pages={2269--2277},
  year={2015}
}

@article{berkes2005slow,
  title={Slow feature analysis yields a rich repertoire of complex cell properties},
  author={Berkes, Pietro and Wiskott, Laurenz},
  journal={Journal of Vision},
  volume={5},
  number={6},
  pages={9--9},
  year={2005},
  publisher={The Association for Research in Vision and Ophthalmology}
}

@article{wiskott2002slow,
  title={Slow feature analysis: {U}nsupervised learning of invariances},
  author={Wiskott, Laurenz and Sejnowski, Terrence J},
  journal={Neural Computation},
  volume={14},
  number={4},
  pages={715--770},
  year={2002},
  publisher={MIT Press}
}

@article{pehlevan2018similarity,
  title={Why do similarity matching objectives lead to {H}ebbian/anti-{H}ebbian networks?},
  author={Pehlevan, Cengiz and Sengupta, Anirvan M and Chklovskii, Dmitri B},
  journal={Neural Computation},
  volume={30},
  number={1},
  pages={84--124},
  year={2018},
  publisher={MIT Press}
}

@inproceedings{clark2019unsupervised,
  title={Unsupervised Discovery of Temporal Structure in Noisy Data with Dynamical Components Analysis},
  author={Clark, David and Livezey, Jesse and Bouchard, Kristofer},
  booktitle={NeurIPS},
  pages={14267--14278},
  year={2019}
}

@article{pehlevan2019neuroscience,
  title={Neuroscience-Inspired Online Unsupervised Learning Algorithms: Artificial neural networks},
  author={Pehlevan, Cengiz and Chklovskii, Dmitri B},
  journal={IEEE Signal Processing Magazine},
  volume={36},
  number={6},
  pages={88--96},
  year={2019},
  publisher={IEEE}
}

@article{pehlevan2015hebbian,
  title={A {H}ebbian/anti-{H}ebbian neural network for linear subspace learning: A derivation from multidimensional scaling of streaming data},
  author={Pehlevan, Cengiz and Hu, Tao and Chklovskii, Dmitri B},
  journal={Neural Computation},
  volume={27},
  number={7},
  pages={1461--1495},
  year={2015},
  publisher={MIT Press}
}

@book{cox2000multidimensional,
  title={Multidimensional scaling},
  author={Cox, Trevor F and Cox, Michael AA},
  year={2000},
  publisher={Chapman and hall/CRC}
}

@article{creutzig2008predictive,
  title={Predictive coding and the slowness principle: An information-theoretic approach},
  author={Creutzig, Felix and Sprekeler, Henning},
  journal={Neural Computation},
  volume={20},
  number={4},
  pages={1026--1041},
  year={2008},
  publisher={MIT Press}
}

@article{RRR_MLE,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/2938278},
 abstract = {The purpose of this paper is to present the likelihood methods for the analysis of cointegration in VAR models with Gaussian errors, seasonal dummies, and constant terms. We discuss likelihood ratio tests of cointegration rank and find the asymptotic distribution of the test statistics. We characterize the maximum likelihood estimator of the cointegrating relations and formulate tests of structural hypotheses about these relations. We show that the asymptotic distribution of the maximum likelihood estimator is mixed Gaussian. Once a certain eigenvalue problem is solved and the eigenvectors and eigenvalues calculated, one can conduct inference on the cointegrating rank using some nonstandard distributions, and test hypotheses about cointegrating relations using the χ 2 distribution.},
 author = {Søren Johansen},
 journal = {Econometrica},
 number = {6},
 pages = {1551--1580},
 publisher = {[Wiley, Econometric Society]},
 title = {Estimation and Hypothesis Testing of Cointegration Vectors in Gaussian Vector Autoregressive Models},
 volume = {59},
 year = {1991}
}


@inproceedings{RRR_robust,
  title={Robust maximum likelihood estimation of sparse vector error correction model},
  author={Zhao, Ziping and Palomar, Daniel P},
  booktitle={2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)},
  pages={913--917},
  year={2017},
  organization={IEEE}
}

@Article{stochastic-mm,
author={Razaviyayn, Meisam
and Sanjabi, Maziar
and Luo, Zhi-Quan},
title={A Stochastic Successive Minimization Method for Nonsmooth Nonconvex Optimization with Applications to Transceiver Design in Wireless Communication Networks},
journal={Mathematical Programming},
year={2016},
volume={157},
number={2},
pages={515-545},
abstract={Consider the problem of minimizing the expected value of a cost function parameterized by a random variable. The classical sample average approximation method for solving this problem requires minimization of an ensemble average of the objective at each step, which can be expensive. In this paper, we propose a stochastic successive upper-bound minimization method (SSUM) which minimizes an approximate ensemble average at each iteration. To ensure convergence and to facilitate computation, we require the approximate ensemble average to be a locally tight upper-bound of the expected cost function and be easily optimized. The main contributions of this work include the development and analysis of the SSUM method as well as its applications in linear transceiver design for wireless communication networks and online dictionary learning. Moreover, using the SSUM framework, we extend the classical stochastic (sub-)gradient method to the case of minimizing a nonsmooth nonconvex objective function and establish its convergence.},
issn={1436-4646},
doi={10.1007/s10107-016-1021-7},
url={https://doi.org/10.1007/s10107-016-1021-7}
}

@Article{ORRR,
author={Yangzhuoran Yang
and Ziping Zhao},
year={2020},
title={{RRRR}: Online Robust Reduced-Rank Regression Estimation. {R} package version 1.0.0. https://pkg.yangzhuoranyang.com/RRRR/},}

@article{Gao2019CCA,
    author = {Gao, Chao and Garber, Dan and Srebro, Nathan and Wan, Jialei and Wang, Weiran},
    year = {2019},
    title = {Stochastic canonical correlation analysis},
    journal = {Journal of Machine Learning Research},
    volume = {20},
    pages = {1--46}
}

@inproceedings{snoek2006challenge,
  title={The challenge problem for automated detection of 101 semantic concepts in multimedia},
  author={Snoek, Cees GM and Worring, Marcel and Van Gemert, Jan C and Geusebroek, Jan-Mark and Smeulders, Arnold WM},
  booktitle={Proceedings of the 14th ACM international conference on Multimedia},
  pages={421--430},
  year={2006}
}

@inproceedings{ge2016efficient,
  title={Efficient algorithms for large-scale generalized eigenvector computation and canonical correlation analysis},
  author={Ge, Rong and Jin, Chi and Netrapalli, Praneeth and Sidford, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={2741--2750},
  year={2016}
}

@inproceedings{arora2017stochastic,
  title={Stochastic approximation for canonical correlation analysis},
  author={Arora, Raman and Marinov, Teodor Vanislavov and Mianjy, Poorya and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4775--4784},
  year={2017}
}


@inproceedings{bhatia2018gen,
  title={Gen-{O}ja: Simple \& Efficient Algorithm for Streaming Generalized Eigenvector Computation},
  author={Bhatia, Kush and Pacchiano, Aldo and Flammarion, Nicolas and Bartlett, Peter L and Jordan, Michael I},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7016--7025},
  year={2018}
}

@article{oja1982simplified,
  title={Simplified neuron model as a principal component analyzer},
  author={Oja, Erkki},
  journal={Journal of Mathematical Biology},
  volume={15},
  number={3},
  pages={267--273},
  year={1982},
  publisher={Springer}
}

@article{lai1999neural,
  title={A neural implementation of canonical correlation analysis},
  author={Lai, Pei Ling and Fyfe, Colin},
  journal={Neural Networks},
  volume={12},
  number={10},
  pages={1391--1397},
  year={1999},
  publisher={Elsevier}
}

@article{pezeshki2003network,
  title={A network for recursive extraction of canonical coordinates},
  author={Pezeshki, Ali and Azimi-Sadjadi, Mahmood R and Scharf, Louis L},
  journal={Neural Networks},
  volume={16},
  number={5-6},
  pages={801--808},
  year={2003},
  publisher={Elsevier}
}

@article{via2007learning,
  title={A learning algorithm for adaptive canonical correlation analysis of several data sets},
  author={V{\'\i}a, Javier and Santamar{\'\i}a, Ignacio and P{\'e}rez, Jes{\'u}s},
  journal={Neural Networks},
  volume={20},
  number={1},
  pages={139--152},
  year={2007},
  publisher={Elsevier}
}

@inproceedings{Zhao2019,
    author={Zhao et al., Xinyuan},
    title={A biologically-plausible pyramidal neuron network for multi-variable canonical correlation analysis},
    year={2019},
    booktitle={Cosyne Abstracts 2019},
    address={Lisbon, Portugal}
}

@MISC{cca_tutorial,
    author = {Magnus Borga},
    title = {Canonical correlation a tutorial},
    year = {1999}
}

@article{sparese_rrr,
author = { Lisha   Chen  and  Jianhua Z.   Huang },
title = {Sparse Reduced-Rank Regression for Simultaneous Dimension Reduction and Variable Selection},
journal = {Journal of the American Statistical Association},
volume = {107},
number = {500},
pages = {1533-1545},
year  = {2012},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2012.734178},
URL = {         https://doi.org/10.1080/01621459.2012.734178},
eprint = {         https://doi.org/10.1080/01621459.2012.734178}
}


@article {Larkum756,
	author = {Larkum, Matthew E. and Nevian, Thomas and Sandler, Maya and Polsky, Alon and Schiller, Jackie},
	title = {Synaptic Integration in Tuft Dendrites of Layer 5 Pyramidal Neurons: A New Unifying Principle},
	volume = {325},
	number = {5941},
	pages = {756--760},
	year = {2009},
	doi = {10.1126/science.1171958},
	publisher = {American Association for the Advancement of Science},
	URL = {https://science.sciencemag.org/content/325/5941/756},
	eprint = {https://science.sciencemag.org/content/325/5941/756.full.pdf},
	journal = {Science}
}

@article{pyramidal_review,
author = {Major, Guy and Larkum, Matthew E. and Schiller, Jackie},
title = {Active Properties of Neocortical Pyramidal Neuron Dendrites},
journal = {Annual Review of Neuroscience},
volume = {36},
number = {1},
pages = {1-24},
year = {2013},
doi = {10.1146/annurev-neuro-062111-150343},
note ={PMID: 23841837},
URL = { https://doi.org/10.1146/annurev-neuro-062111-150343},
eprint = {https://doi.org/10.1146/annurev-neuro-062111-150343}
}


@article{Bernander94,
author = {Bernander, O. and Koch, C. and Douglas, R. J.},
title = {Amplification and linearization of distal synaptic input to cortical pyramidal cells},
journal = {Journal of Neurophysiology},
volume = {72},
number = {6},
pages = {2743-2753},
year = {1994},
doi = {10.1152/jn.1994.72.6.2743},
    note ={PMID: 7897486},
URL = { https://doi.org/10.1152/jn.1994.72.6.2743},
eprint = { https://doi.org/10.1152/jn.1994.72.6.2743}
}

@article {Hausser739,
	author = {H{\"a}usser, Michael and Spruston, Nelson and Stuart, Greg J.},
	title = {Diversity and Dynamics of Dendritic Signaling},
	volume = {290},
	number = {5492},
	pages = {739--744},
	year = {2000},
	doi = {10.1126/science.290.5492.739},
	publisher = {American Association for the Advancement of Science},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/290/5492/739},
	eprint = {https://science.sciencemag.org/content/290/5492/739.full.pdf},
	journal = {Science}
}

@article{Spruston2008,
author = {Spruston, Nelson},
doi = {10.1038/nrn2286},
issn = {1471-0048},
journal = {Nature Reviews Neuroscience},
number = {3},
pages = {206--221},
title = {{Pyramidal neurons: dendritic structure and synaptic integration}},
url = {https://doi.org/10.1038/nrn2286},
volume = {9},
year = {2008}
}

@article{deepnet_review,
author = {Kriegeskorte, Nikolaus},
title = {Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information Processing},
journal = {Annual Review of Vision Science},
volume = {1},
number = {1},
pages = {417-446},
year = {2015},
doi = {10.1146/annurev-vision-082114-035447},
    note ={PMID: 28532370},
URL = { https://doi.org/10.1146/annurev-vision-082114-035447},
eprint = { https://doi.org/10.1146/annurev-vision-082114-035447}
}

@book{Hebb,
  title={The Organization of Behavior: A Neuropsychological Theory},
  author={Hebb, Donald Olding},
  year={2005},
  publisher={Psychology Press}
}

@article{Payeur2020,
abstract = {Synaptic plasticity is believed to be a key physiological mechanism for learning. It is well-established that it depends on pre and postsynaptic activity. However, models that rely solely on pre and postsynaptic activity for synaptic changes have, to date, not been able to account for learning complex tasks that demand hierarchical networks. Here, we show that if synaptic plasticity is regulated by high-frequency bursts of spikes, then neurons higher in the hierarchy can coordinate the plasticity of lower-level connections. Using simulations and mathematical analyses, we demonstrate that, when paired with short-term synaptic dynamics, regenerative activity in the apical dendrites, and synaptic plasticity in feedback pathways, a burst-dependent learning rule can solve challenging tasks that require deep network architectures. Our results demonstrate that well-known properties of dendrites, synapses, and synaptic plasticity are sufficient to enable sophisticated learning in hierarchical circuits.},
author = {Payeur, Alexandre and Guerguiev, Jordan and Zenke, Friedemann and Richards, Blake A and Naud, Richard},
doi = {10.1101/2020.03.30.015511},
journal = {bioRxiv},
publisher = {Cold Spring Harbor Laboratory},
title = {{Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits}},
url = {https://www.biorxiv.org/content/early/2020/03/31/2020.03.30.015511},
year = {2020}
}
@article{guergiuev2016deep,
  title={Deep learning with segregated dendrites},
  author={Guergiuev, Jordan and Lillicrap, Timothy P and Richards, Blake A},
  journal={arXiv preprint arXiv:1610.00161},
  year={2016}
}
@article{hardie2009synaptic,
  title={Synaptic depolarization is more effective than back-propagating action potentials during induction of associative long-term potentiation in hippocampal pyramidal neurons},
  author={Hardie, Jason and Spruston, Nelson},
  journal={Journal of Neuroscience},
  volume={29},
  number={10},
  pages={3233--3241},
  year={2009},
  publisher={Soc Neuroscience}
}
@article{gambino2014sensory,
  title={Sensory-evoked {LTP} driven by dendritic plateau potentials in vivo},
  author={Gambino, Fr{\'e}d{\'e}ric and Pag{\`e}s, St{\'e}phane and Kehayas, Vassilis and Baptista, Daniela and Tatti, Roberta and Carleton, Alan and Holtmaat, Anthony},
  journal={Nature},
  volume={515},
  number={7525},
  pages={116--119},
  year={2014},
  publisher={Nature Publishing Group}
}
@article{Rao1999,
abstract = {We describe a model of visual processing in which feedback connections from a higher- to a lower-order visual cortical area carry predictions of lower-level neural activities, whereas the feedforward connections carry the residual errors between the predictions and the actual lower-level activities. When exposed to natural images, a hierarchical network of model neurons implementing such a model developed simple-cell-like receptive fields. A subset of neurons responsible for carrying the residual errors showed endstopping and other extra-classical receptive-field effects. These results suggest that rather than being exclusively feedforward phenomena, nonclassical surround effects in the visual cortex may also result from cortico-cortical feedback as a consequence of the visual system using an efficient hierarchical strategy for encoding natural images.},
author = {Rao, Rajesh P N and Ballard, Dana H},
doi = {10.1038/4580},
issn = {1546-1726},
journal = {Nature Neuroscience},
number = {1},
pages = {79--87},
title = {{Predictive coding in the visual cortex:  a functional interpretation of some extra-classical receptive-field effects}},
url = {https://doi.org/10.1038/4580},
volume = {2},
year = {1999}
}
@article{haga2017dendritic,
  title={Dendritic processing of spontaneous neuronal sequences for one-shot learning},
  author={Haga, Tatsuya and Fukai, Tomoki},
  journal={bioRxiv},
  pages={165613},
  year={2017},
  publisher={Cold Spring Harbor Laboratory}
}
@article{Friston2005,
author = {Friston, Karl},
doi = {10.1098/rstb.2005.1622},
issn = {09628436},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
keywords = {Bayesian,Cortical,Generative models,Hierarchical,Inference,Predictive coding},
pmid = {15937014},
title = {{A theory of cortical responses}},
year = {2005}
}

@incollection{SGD,
  author = {Bottou, L\'{e}on},
  title = {Online Algorithms and Stochastic Approximations},
  booktitle = {Online Learning and Neural Networks},
  editor = {Saad, David},
  publisher = {Cambridge University Press},
  address = {Cambridge, UK},
  year = {1998},
  url = {http://leon.bottou.org/papers/bottou-98x},
  note = {revised, oct 2012}
}

@article{expansion1,
author = {Brecht, Michael and Sakmann, Bert},
title = {Dynamic representation of whisker deflection by synaptic potentials in spiny stellate and pyramidal cells in the barrels and septa of layer 4 rat somatosensory cortex},
journal = {The Journal of Physiology},
volume = {543},
number = {1},
pages = {49-70},
doi = {10.1113/jphysiol.2002.018465},
url = {https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.2002.018465},
eprint = {https://physoc.onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.2002.018465},
abstract = {Whole-cell voltage recordings were made in vivo from excitatory neurons (n= 23) in layer 4 of the barrel cortex in urethane-anaesthetised rats. Their receptive fields (RFs) for a brief whisker deflection were mapped, the position of the cell soma relative to barrel borders was determined for 15 cells and dendritic and axonal arbors were reconstructed for all cells. Three classes of neurons were identified: spiny stellate cells and pyramidal cells located in barrels and pyramidal cells located in septa. Dendritic and, with some exceptions, axonal arborisations of barrel cells were mostly restricted to the borders of a column with a cross sectional area of a barrel, defining a cytoarchitectonic barrel-column. Dendrites and axons of septum cells, in contrast, mostly extended across barrel borders. The subthreshold RFs measured by evoked postsynaptic potentials (PSPs) comprised a principal whisker (PW) and several surround whiskers (SuWs) indicating that deflection of a single whisker is represented in multiple barrels and septa. Barrel cells responded with larger depolarisation to stimulation of the PW (13.7 ± 4.6 mV (mean ±S.D.), n= 10) than septum cells (5.7 ± 2.4 mV, n= 5), the gradient between peak responses to PW and SuW deflection was steeper and the latency of depolarisation onset was shorter (8 ± 1.4 ms vs. 11 ± 2 ms). In barrel cells the response onset and the peak to SuW deflection was delayed depending on the distance to the PW thus indicating that the spatial representation of a single whisker deflection in the barrel map is dynamic and varies on the scale of milliseconds to tens of milliseconds. Septum cells responded later and with comparable latencies to PW and SuW stimulation. Spontaneous (0.053 ± 0.12 action potentials (APs) s−1) and evoked APs (0.14 ± 0.29 APs per principal whisker (PW) stimulus) were sparse. We conclude that PSPs in ensembles of barrel cells represent dynamically the deflection of a single whisker with high temporal and spatial acuity, initially by the excitation in a single PW-barrel followed by multi-barrel excitation. This presumably reflects the divergence of thalamocortical projections to different barrels. Septum cell PSPs preferably represent multiple whisker deflections, but less dynamically and with less spatial acuity.},
year = {2002}
}

@article {expansion2,
	author = {DeWeese, Michael R. and Wehr, Michael and Zador, Anthony M.},
	title = {Binary Spiking in Auditory Cortex},
	volume = {23},
	number = {21},
	pages = {7940--7949},
	year = {2003},
	doi = {10.1523/JNEUROSCI.23-21-07940.2003},
	publisher = {Society for Neuroscience},
	abstract = {Neurons are often assumed to operate in a highly unreliable manner: a neuron can signal the same stimulus with a variable number of action potentials. However, much of the experimental evidence supporting this view was obtained in the visual cortex. We have, therefore, assessed trial-to-trial variability in the auditory cortex of the rat. To ensure single-unit isolation, we used cell-attached recording. Tone-evoked responses were usually transient, often consisting of, on average, only a single spike per stimulus. Surprisingly, the majority of responses were not just transient, but were also binary, consisting of 0 or 1 action potentials, but not more, in response to each stimulus; several dramatic examples consisted of exactly one spike on 100\% of trials, with no trial-to-trial variability in spike count. The variability of such binary responses differs from comparably transient responses recorded in visual cortical areas such as area MT, and represent the lowest trial-to-trial variability mathematically possible for responses of a given firing rate. Our study thus establishes for the first time that transient responses in auditory cortex can be described as a binary process, rather than as a highly variable Poisson process. These results demonstrate that cortical architecture can support a more precise control of spike number than was previously recognized, and they suggest a re-evaluation of models of cortical processing that assume noisiness to be an inevitable feature of cortical codes.},
	issn = {0270-6474},
	URL = {https://www.jneurosci.org/content/23/21/7940},
	eprint = {https://www.jneurosci.org/content/23/21/7940.full.pdf},
	journal = {Journal of Neuroscience}
}

@article{expansion3,
title = "Sparse coding of sensory inputs",
journal = "Current Opinion in Neurobiology",
volume = "14",
number = "4",
pages = "481 - 487",
year = "2004",
issn = "0959-4388",
doi = "https://doi.org/10.1016/j.conb.2004.07.007",
url = "http://www.sciencedirect.com/science/article/pii/S0959438804001035",
author = "Bruno A Olshausen and David J Field",
abstract = "Several theoretical, computational, and experimental studies suggest that neurons encode sensory information using a small number of active neurons at any given point in time. This strategy, referred to as ‘sparse coding’, could possibly confer several advantages. First, it allows for increased storage capacity in associative memories; second, it makes the structure in natural signals explicit; third, it represents complex data in a way that is easier to read out at subsequent levels of processing; and fourth, it saves energy. Recent physiological recordings from sensory neurons have indicated that sparse coding could be a ubiquitous strategy employed in several different modalities across different organisms."
}

@article{expansion4,
title = "Efficient computation via sparse coding in electrosensory neural networks",
journal = "Current Opinion in Neurobiology",
volume = "21",
number = "5",
pages = "752 - 760",
year = "2011",
note = "Networks, circuits and computation",
issn = "0959-4388",
doi = "https://doi.org/10.1016/j.conb.2011.05.016",
url = "http://www.sciencedirect.com/science/article/pii/S0959438811000845",
author = "Maurice J Chacron and André Longtin and Leonard Maler",
abstract = "The electric sense combines spatial aspects of vision and touch with temporal features of audition. Its accessible neural architecture shares similarities with mammalian sensory systems and allows for recordings from successive brain areas to test hypotheses about neural coding. Further, electrosensory stimuli encountered during prey capture, navigation, and communication, can be readily synthesized in the laboratory. These features enable analyses of the neural circuitry that reveal general principles of encoding and decoding, such as segregation of information into separate streams and neural response sparsification. A systems level understanding arises via linkage between cellular differentiation and network architecture, revealed by in vitro and in vivo analyses, while computational modeling reveals how single cell dynamics and connectivity shape the sparsification process."
}

@article{mahalanobis,
  added-at = {2013-09-11T20:35:50.000+0200},
  author = {Mahalanobis, Prasanta Chandra},
  biburl = {https://www.bibsonomy.org/bibtex/2aef303a4aba53e4fcd7b0e58f7c205b6/thoni},
  interhash = {23e3d7fb895ac218d0c5cc52bd7af4d3},
  intrahash = {aef303a4aba53e4fcd7b0e58f7c205b6},
  journal = {Proceedings of the National Institute of Sciences (Calcutta)},
  keywords = {distance generalized mahalanobis},
  pages = {49--55},
  timestamp = {2016-09-06T08:23:07.000+0200},
  title = {On the generalized distance in statistics},
  volume = 2,
  year = 1936
}

@article {Milstein2020,
	author = {Milstein, Aaron D. and Li, Yiding and Bittner, Katie C. and Grienberger, Christine and Soltesz, Ivan and Magee, Jeffrey C. and Romani, Sandro},
	title = {Bidirectional synaptic plasticity rapidly modifies hippocampal representations independent of correlated activity},
	elocation-id = {2020.02.04.934182},
	year = {2020},
	doi = {10.1101/2020.02.04.934182},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {According to standard models of synaptic plasticity, correlated activity between connected neurons drives changes in synaptic strengths to store associative memories. Here we tested this hypothesis in vivo by manipulating the activity of hippocampal place cells and measuring the resulting changes in spatial selectivity. We found that the spatial tuning of place cells was rapidly reshaped via bidirectional synaptic plasticity. To account for the magnitude and direction of plasticity, we evaluated two models {\textendash} a standard model that depended on synchronous pre- and post-synaptic activity, and an alternative model that depended instead on whether active synaptic inputs had previously been potentiated. While both models accounted equally well for the data, they predicted opposite outcomes of a perturbation experiment, which ruled out the standard correlation-dependent model. Finally, network modeling suggested that this form of bidirectional synaptic plasticity enables population activity, rather than pairwise neuronal correlations, to drive plasticity in response to changes in the environment.},
	URL = {https://www.biorxiv.org/content/early/2020/02/05/2020.02.04.934182},
	eprint = {https://www.biorxiv.org/content/early/2020/02/05/2020.02.04.934182.full.pdf},
	journal = {bioRxiv}
}

@article{klausberger2003brain,
  title={Brain-state-and cell-type-specific firing of hippocampal interneurons in vivo},
  author={Klausberger, Thomas and Magill, Peter J and M{\'a}rton, L{\'a}szl{\'o} F and Roberts, J David B and Cobden, Philip M and Buzs{\'a}ki, Gy{\"o}rgy and Somogyi, Peter},
  journal={Nature},
  volume={421},
  number={6925},
  pages={844--848},
  year={2003},
  publisher={Nature Publishing Group}
}

@article{riedemann2019diversity,
  title={Diversity and Function of Somatostatin-Expressing Interneurons in the Cerebral Cortex},
  author={Riedemann, Therese},
  journal={International Journal of Molecular Sciences},
  volume={20},
  number={12},
  pages={2952},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{hua2001optimal,
  title={Optimal reduced-rank estimation and filtering},
  author={Hua, Yingbo and Nikpour, Maziar and Stoica, Petre},
  journal={IEEE Transactions on Signal Processing},
  volume={49},
  number={3},
  pages={457--469},
  year={2001},
  publisher={IEEE}
}

@article{Lillicrap2016,
abstract = {The brain processes information through multiple layers of neurons. This deep architecture is representationally powerful, but complicates learning because it is difficult to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame by multiplying error signals with all the synaptic weights on each neuron's axon and further downstream. However, this involves a precise, symmetric backward connectivity pattern, which is thought to be impossible in the brain. Here we demonstrate that this strong architectural constraint is not required for effective error propagation. We present a surprisingly simple mechanism that assigns blame by multiplying errors by even random synaptic weights. This mechanism can transmit teaching signals across multiple layers of neurons and performs as effectively as backpropagation on a variety of tasks. Our results help reopen questions about how the brain could use error signals and dispel long-held assumptions about algorithmic constraints on learning.},
author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
doi = {10.1038/ncomms13276},
file = {:F$\backslash$:/Downloads/ncomms13276.pdf:pdf},
issn = {20411723},
journal = {Nature Communications},
mendeley-groups = {Backpropagation},
pages = {1--10},
publisher = {Nature Publishing Group},
title = {{Random synaptic feedback weights support error backpropagation for deep learning}},
url = {http://dx.doi.org/10.1038/ncomms13276},
volume = {7},
year = {2016}
}

@article{Burbank2012,
abstract = {Top-down synapses are ubiquitous throughout neocortex and play a central role in cognition, yet little is known about their development and specificity. During sensory experience, lower neocortical areas are activated before higher ones, causing top-down synapses to experience a preponderance of post-synaptic activity preceding pre-synaptic activity. This timing pattern is the opposite of that experienced by bottom-up synapses, which suggests that different versions of spike-timing dependent synaptic plasticity (STDP) rules may be required at top-down synapses. We consider a two-layer neural network model and investigate which STDP rules can lead to a distribution of top-down synaptic weights that is stable, diverse and avoids strong loops. We introduce a temporally reversed rule (rSTDP) where top-down synapses are potentiated if post-synaptic activity precedes pre-synaptic activity. Combining analytical work and integrate-and-fire simulations, we show that only depression-biased rSTDP (and not classical STDP) produces stable and diverse top-down weights. The conclusions did not change upon addition of homeostatic mechanisms, multiplicative STDP rules or weak external input to the top neurons. Our prediction for rSTDP at top-down synapses, which are distally located, is supported by recent neurophysiological evidence showing the existence of temporally reversed STDP in synapses that are distal to the post-synaptic cell body. {\textcopyright} 2012 Burbank, Kreiman.},
author = {Burbank, Kendra S. and Kreiman, Gabriel},
doi = {10.1371/journal.pcbi.1002393},
issn = {1553734X},
journal = {PLoS Computational Biology},
mendeley-groups = {Backpropagation},
title = {{Depression-biased reverse plasticity rule is required for stable learning at top-down connections}},
year = {2012}
}
@article{Burbank2015,
abstract = {The autoencoder algorithm is a simple but powerful unsupervised method for training neural networks. Autoencoder networks can learn sparse distributed codes similar to those seen in cortical sensory areas such as visual area V1, but they can also be stacked to learn increasingly abstract representations. Several computational neuroscience models of sensory areas, including Olshausen {\&} Field's Sparse Coding algorithm, can be seen as autoencoder variants, and autoencoders have seen extensive use in the machine learning community. Despite their power and versatility, autoencoders have been difficult to implement in a biologically realistic fashion. The challenges include their need to calculate differences between two neuronal activities and their requirement for learning rules which lead to identical changes at feedforward and feedback connections. Here, we study a biologically realistic network of integrate-and-fire neurons with anatomical connectivity and synaptic plasticity that closely matches that observed in cortical sensory areas. Our choice of synaptic plasticity rules is inspired by recent experimental and theoretical results suggesting that learning at feedback connections may have a different form from learning at feedforward connections, and our results depend critically on this novel choice of plasticity rules. Specifically, we propose that plasticity rules at feedforward versus feedback connections are temporally opposed versions of spike-timing dependent plasticity (STDP), leading to a symmetric combined rule we call Mirrored STDP (mSTDP). We show that with mSTDP, our network follows a learning rule that approximately minimizes an autoencoder loss function. When trained with whitened natural image patches, the learned synaptic weights resemble the receptive fields seen in V1. Our results use realistic synaptic plasticity rules to show that the powerful autoencoder learning algorithm could be within the reach of real biological networks.},
author = {Burbank, Kendra S.},
doi = {10.1371/journal.pcbi.1004566},
issn = {15537358},
journal = {PLoS Computational Biology},
mendeley-groups = {Backpropagation},
title = {{Mirrored STDP Implements Autoencoder Learning in a Network of Spiking Neurons}},
year = {2015}
}

@article{Akrout2019,
archivePrefix = {arXiv},
arxivId = {1904.05391},
author = {Akrout, Mohamed and Wilson, Collin and Humphreys, Peter C. and Lillicrap, Timothy and Tweed, Douglas},
eprint = {1904.05391},
journal = {arXiv},
mendeley-groups = {Backpropagation},
title = {{Using Weight Mirrors to Improve Feedback Alignment}},
year = {2019}
}

@article{weight_transport_problem1,
abstract = {Functional and mechanistic comparisons are made between several network models of cognitive processing: competitive learning, interactive activation, adaptive resonance, and back propagation. The starting point of this comparison is the article of Rumelhart and Zipser (1985) on feature discovery through competitive learning. All the models which Rumelhart and Zipser (1985) have described were shown in Grossberg (1976b) to exhibit a type of learning which is temporally unstable. Competitive learning mechanisms can be stabilized in response to an arbitrary input environment by being supplemented with mechanisms for learning top-down expectancies, or templates; for matching bottom-up input patterns with the top-down expectancies; and for releasing orienting reactions in a mismatch situation, thereby updating short-term memory and searching for another internal representation. Network architectures which embody all of these mechanisms were called adaptive resonance models by Grossberg (1976c). Self-stabilizing learning models are candidates for use in real-world applications where unpredictable changes can occur in complex input environments. Competitive learning postulates are inconsistent with the postulates of the interactive activation model of McClelland and Rumelhart (1981), and suggest different levels of processing and interaction rules for the analysis of word recognition. Adaptive resonance models use these alternative levels and interaction rules. The selforganizing learning of an adaptive resonance model is compared and contrasted with the teacher-directed learning of a back propagation model. A number of criteria for evaluating real-time network models of cognitive processing are described and applied. {\textcopyright} 1987.},
author = {Grossberg, Stephen},
doi = {10.1016/S0364-0213(87)80025-3},
issn = {03640213},
journal = {Cognitive Science},
mendeley-groups = {Backpropagation},
title = {{Competitive learning: From interactive activation to adaptive resonance}},
year = {1987}
}

@incollection{weight_transport_problem3,
abstract = {Review o BP and a suggestion of how to implement BP in the cortex by including a dedicated error-feedback network.},
author = {Zipser, D and Rumelhart, D E},
booktitle = {Computational Neuroscience},
keywords = {cortex,error back-propagation,learning,review},
mendeley-groups = {Backpropagation},
title = {{The neurobiological significance of the new learning models}},
year = {1990}
}
@misc{weight_transport_problem2,
abstract = {The remarkable properties of some recent computer algorithms for neural networks seemed to promise a fresh approach to understanding the computational properties of the brain. Unfortunately most of these neural nets are unrealistic in important respects. {\textcopyright} 1989 Nature Publishing Group.},
author = {Crick, Francis},
booktitle = {Nature},
doi = {10.1038/337129a0},
issn = {00280836},
mendeley-groups = {Backpropagation},
title = {{The recent excitement about neural networks}},
year = {1989}
}

@article{MNIST,
abstract = {The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.$\backslash$r$\backslash$nIt is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.},
author = {{Lecun Yann} and {Cortes Corinna} and {Burges Christopher}},
isbn = {7842500200015},
journal = {The Courant Institute of Mathematical Sciences},
mendeley-groups = {Datasets},
pages = {1--10},
pmid = {1000253529},
title = {{The MNIST database of Handwritten Digits}},
url = {http://yann.lecun.com/exdb/mnist/},
year = {1998}
}

@online{FMNIST,
author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
mendeley-groups = {Datasets},
title = {{Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms}},
year = {2017}
}


@online{CIFAR,
abstract = {The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset},
author = {Krizhevsky, A and Nair, V and Hinton, G},
mendeley-groups = {Datasets},
title = {{CIFAR-10 and CIFAR-100 datasets}},
url = {https://www.cs.toronto.edu/{~}kriz/cifar.html},
year = {2009}
}

@book{XRMB,
  title={X-ray Microbeam Speech Production Database User's Handbook: Version 1.0 (June 1994)},
  author={Westbury, J.R.},
  url={https://books.google.com/books?id=H1HTjwEACAAJ},
  year={1994},
  publisher={Waisman Center on Mental Retardation \& Human Development}
}

@article{Gidon2020,
abstract = {The active electrical properties of dendrites shape neuronal input and output and are fundamental to brain function. However, our knowledge of active dendrites has been almost entirely acquired from studies of rodents. In this work, we investigated the dendrites of layer 2 and 3 (L2/3) pyramidal neurons of the human cerebral cortex ex vivo. In these neurons, we discovered a class of calcium-mediated dendritic action potentials (dCaAPs) whose waveform and effects on neuronal output have not been previously described. In contrast to typical all-or-none action potentials, dCaAPs were graded; their amplitudes were maximal for threshold-level stimuli but dampened for stronger stimuli. These dCaAPs enabled the dendrites of individual human neocortical pyramidal neurons to classify linearly nonseparable inputs—a computation conventionally thought to require multilayered networks.},
author = {Gidon, Albert and Zolnik, Timothy Adam and Fidzinski, Pawel and Bolduan, Felix and Papoutsi, Athanasia and Poirazi, Panayiota and Holtkamp, Martin and Vida, Imre and Larkum, Matthew Evan},
doi = {10.1126/science.aax6239},
file = {:mnt/home/sgolkar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gidon et al. - 2020 - Dendritic action potentials and computation in human layer 23 cortical neurons.pdf:pdf},
issn = {0036-8075},
journal = {Science},
mendeley-groups = {Calcium plateau},
month = {jan},
number = {6473},
pages = {83--87},
pmid = {31896716},
title = {{Dendritic action potentials and computation in human layer 2/3 cortical neurons}},
url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aax6239},
volume = {367},
year = {2020}
}

@article{Urbanczik2014,
abstract = {Recent modeling of spike-timing-dependent plasticity indicates that plasticity involves as a third factor a local dendritic potential, besides pre- and postsynaptic firing times. We present a simple compartmental neuron model together with a non-Hebbian, biologically plausible learning rule for dendritic synapses where plasticity is modulated by these three factors. In functional terms, the rule seeks to minimize discrepancies between somatic firings and a local dendritic potential. Such prediction errors can arise in our model from stochastic fluctuations as well as from synaptic input, which directly targets the soma. Depending on the nature of this direct input, our plasticity rule subserves supervised or unsupervised learning. When a reward signal modulates the learning rate, reinforcement learning results. Hence a single plasticity rule supports diverse learning paradigms. {\textcopyright} 2014 Elsevier Inc.},
author = {Urbanczik, Robert and Senn, Walter},
doi = {10.1016/j.neuron.2013.11.030},
file = {:mnt/home/sgolkar/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Urbanczik, Senn - 2014 - Learning by the Dendritic Prediction of Somatic Spiking.pdf:pdf},
issn = {08966273},
journal = {Neuron},
mendeley-groups = {Biologically plausible learning},
number = {3},
pages = {521--528},
publisher = {Elsevier Inc.},
title = {{Learning by the Dendritic Prediction of Somatic Spiking}},
url = {http://dx.doi.org/10.1016/j.neuron.2013.11.030},
volume = {81},
year = {2014}
}


@article{Kampa2007,
abstract = {The ability of neurons to modulate the strength of their synaptic connections has been shown to depend on the relative timing of pre- and postsynaptic action potentials. This form of synaptic plasticity, called spike-timing-dependent plasticity (STDP), has become an attractive model for learning at the single-cell level. Yet, despite its popularity in experimental and theoretical neuroscience, the influence of dendritic mechanisms in the induction of STDP has been largely overlooked. Several recent studies have investigated how active dendritic properties and synapse location within the dendritic tree influence STDP. These studies suggest the existence of learning rules that depend on firing mode and subcellular input location, adding unanticipated complexity to STDP. Here, we propose a new look at STDP that is focused on processing at the postsynaptic site in the dendrites, rather than on spike-timing at the cell body. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {Kampa, Bj{\"{o}}rn M. and Letzkus, Johannes J. and Stuart, Greg J.},
doi = {10.1016/j.tins.2007.06.010},
file = {:mnt/home/sgolkar/Downloads/1-s2.0-S0166223607001798-main.pdf:pdf},
issn = {01662236},
journal = {Trends in Neurosciences},
mendeley-groups = {STDP},
number = {9},
pages = {456--463},
pmid = {17765330},
title = {{Dendritic mechanisms controlling spike-timing-dependent synaptic plasticity}},
volume = {30},
year = {2007}
}


@article{Sacramento2018,
abstract = {Deep learning has seen remarkable developments over the last years, many of them inspired by neuroscience. However, the main learning mechanism behind these advances - error backpropagation - appears to be at odds with neurobiology. Here, we introduce a multilayer neuronal network model with simplified dendritic compartments in which error-driven synaptic plasticity adapts the network towards a global desired output. In contrast to previous work our model does not require separate phases and synaptic learning is driven by local dendritic prediction errors continuously in time. Such errors originate at apical dendrites and occur due to a mismatch between predictive input from lateral interneurons and activity from actual top-down feedback. Through the use of simple dendritic compartments and different cell-types our model can represent both error and normal activity within a pyramidal neuron. We demonstrate the learning capabilities of the model in regression and classification tasks, and show analytically that it approximates the error backpropagation algorithm. Moreover, our framework is consistent with recent observations of learning between brain areas and the architecture of cortical microcircuits. Overall, we introduce a novel view of learning on dendritic cortical circuits and on how the brain may solve the long-standing synaptic credit assignment problem.},
archivePrefix = {arXiv},
arxivId = {1810.11393},
author = {Sacramento, Jo{\~{a}}o and Costa, Rui Ponte and Bengio, Yoshua and Senn, Walter},
eprint = {1810.11393},
file = {:mnt/home/sgolkar/Downloads/1810.11393.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {Backpropagation,Biologically plausible learning},
month = {oct},
number = {Nips},
pages = {8721--8732},
title = {{Dendritic cortical microcircuits approximate the backpropagation algorithm}},
url = {http://arxiv.org/abs/1810.11393},
volume = {2018-Decem},
year = {2018}
}

@article{MageeGrienberger2020,
author = {Magee, Jeffrey C. and Grienberger, Christine},
journal = {Annual Review of Neuroscience},
doi = {10.1146/annurev-neuro-090919-022842},
issn = {15454126},
keywords = {dendrites,eligibility traces,learning,memory,neuromodulation,synapses},
pmid = {32075520},
title = {{Synaptic plasticity forms and functions}},
year = {2020},
volume={43},
}

@article{Whittington2017,
abstract = {To efficiently learn from feedback, cortical networks need to update synaptic weights on multiple levels of cortical hierarchy. An effective and well-known algorithm for computing such changes in synaptic weights is the error backpropagation algorithm. However, in this algorithm, the change in synaptic weights is a complex function of weights and activities of neurons not directly connected with the synapse being modified, whereas the changes in biological synapses are determined only by the activity of presynaptic and postsynaptic neurons. Several models have been proposed that approximate the backpropagation algorithm with local synaptic plasticity, but these models require complex external control over the network or relatively complex plasticity rules. Here we show that a network developed in the predictive coding framework can efficiently perform supervised learning fully autonomously, employing only simple local Hebbian plasticity. Furthermore, for certain parameters, the weight change in the predictive codingmodel converges to that of the backpropagation algorithm. This suggests that it is possible for cortical networks with simple Hebbian synaptic plasticity to implement efficient learning algorithms in which synapses in areas on multiple levels of hierarchy are modified to minimize the error on the output.},
author = {Whittington, James C.R. and Bogacz, Rafal},
doi = {10.1162/NECO_a_00949},
file = {:Users/sgolkar/Downloads/NECO_a_00949 (1).pdf:pdf},
issn = {1530888X},
journal = {Neural Computation},
mendeley-groups = {Biologically plausible learning,Backpropagation,Predictive coding},
month = {may},
number = {5},
pages = {1229--1262},
pmid = {28333583},
title = {{An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity}},
url = {http://www.mitpressjournals.org/doi/10.1162/NECO_a_00949},
volume = {29},
year = {2017}
}

@article{pehlevan2020neurons,
  title={Neurons as canonical correlation analyzers},
  author={Pehlevan, Cengiz and Zhao, Xinyuan and Sengupta, Anirvan M and Chklovskii, Dmitri},
  journal={Frontiers in computational neuroscience},
  volume={14},
  pages={55},
  year={2020},
  publisher={Frontiers}
}