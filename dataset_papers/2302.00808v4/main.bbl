\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abounadi et~al.(2001)Abounadi, Bertsekas, and Borkar]{abounadi2001learning}
Abounadi, J., Bertsekas, D., and Borkar, V.~S.
\newblock Learning algorithms for markov decision processes with average cost.
\newblock \emph{SIAM Journal on Control and Optimization}, 40\penalty0 (3):\penalty0 681--698, 2001.

\bibitem[Achiam(2017)]{achiam2017advanced}
Achiam, J.
\newblock {{UC} {B}erkeley {CS} 285 ({F}all 2017), Advanced Policy Gradients}, 2017.
\newblock URL: \url{http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf}. Last visited on 2020/05/24.

\bibitem[Achiam et~al.(2017)Achiam, Held, Tamar, and Abbeel]{achiam2017constrained}
Achiam, J., Held, D., Tamar, A., and Abbeel, P.
\newblock Constrained policy optimization.
\newblock In \emph{Proceedings of the 34th International Conference on Machine Learning-Volume 70}, pp.\  22--31. JMLR. org, 2017.

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.~L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Agnihotri et~al.(2019)Agnihotri, Saraf, and Bapnad]{9030307}
Agnihotri, A., Saraf, P., and Bapnad, K.~R.
\newblock A convolutional neural network approach towards self-driving cars.
\newblock In \emph{2019 IEEE 16th India Council International Conference (INDICON)}, pp.\  1--4, 2019.
\newblock \doi{10.1109/INDICON47234.2019.9030307}.

\bibitem[Akkaya et~al.(2019)Akkaya, Andrychowicz, Chociej, Litwin, McGrew, Petron, Paino, Plappert, Powell, Ribas, et~al.]{akkaya2019solving}
Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., Petron, A., Paino, A., Plappert, M., Powell, G., Ribas, R., et~al.
\newblock Solving rubik's cube with a robot hand.
\newblock \emph{arXiv preprint arXiv:1910.07113}, 2019.

\bibitem[Altman(1999)]{altman1999constrained}
Altman, E.
\newblock \emph{Constrained Markov decision processes}, volume~7.
\newblock CRC Press, 1999.

\bibitem[Aractingi et~al.(2023)Aractingi, L{\'e}ziart, Flayols, Perez, Silander, and Sou{\`e}res]{aractingi2023controlling}
Aractingi, M., L{\'e}ziart, P.-A., Flayols, T., Perez, J., Silander, T., and Sou{\`e}res, P.
\newblock Controlling the solo12 quadruped robot with deep reinforcement learning.
\newblock \emph{scientific Reports}, 13\penalty0 (1):\penalty0 11945, 2023.

\bibitem[Bhatnagar \& Lakshmanan(2012)Bhatnagar and Lakshmanan]{Bhatnagar2012}
Bhatnagar, S. and Lakshmanan, K.
\newblock {A}n {O}nline {A}ctorâ€“{C}ritic {A}lgorithm with {F}unction {A}pproximation for {C}onstrained {M}arkov {D}ecision {P}rocesses.
\newblock \url{https://doi.org/10.1007/s10957-012-9989-5}, 2012.
\newblock [Accessed 08-10-2023].

\bibitem[Borkar(1988)]{borkar1988convex}
Borkar, V.~S.
\newblock A convex analytic approach to markov decision processes.
\newblock \emph{Probability Theory and Related Fields}, 78\penalty0 (4):\penalty0 583--602, 1988.

\bibitem[Calvo-Fullana et~al.(2023)Calvo-Fullana, Paternain, Chamon, and Ribeiro]{calvo2023state}
Calvo-Fullana, M., Paternain, S., Chamon, L.~F., and Ribeiro, A.
\newblock State augmented constrained reinforcement learning: Overcoming the limitations of learning with rewards.
\newblock \emph{IEEE Transactions on Automatic Control}, 2023.

\bibitem[Chen et~al.(2022)Chen, Jain, and Luo]{pmlr-v162-chen22i}
Chen, L., Jain, R., and Luo, H.
\newblock Learning infinite-horizon average-reward {M}arkov decision process with constraints.
\newblock In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), \emph{Proceedings of the 39th International Conference on Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  3246--3270. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/chen22i.html}.

\bibitem[Cho \& Meyer(2001)Cho and Meyer]{cho2001comparison}
Cho, G.~E. and Meyer, C.~D.
\newblock Comparison of perturbation bounds for the stationary distribution of a markov chain.
\newblock \emph{Linear Algebra and its Applications}, 335\penalty0 (1-3):\penalty0 137--150, 2001.

\bibitem[Doyle(2009)]{doyle2009kemeny}
Doyle, P.~G.
\newblock The kemeny constant of a markov chain.
\newblock \emph{arXiv preprint arXiv:0909.2636}, 2009.

\bibitem[Gao et~al.(2018)Gao, Xu, Lin, Yu, Levine, and Darrell]{gao2018reinforcement}
Gao, Y., Xu, H., Lin, J., Yu, F., Levine, S., and Darrell, T.
\newblock Reinforcement learning from imperfect demonstrations.
\newblock \emph{arXiv preprint arXiv:1802.05313}, 2018.

\bibitem[Garcia \& Fernandez(2015)Garcia and Fernandez]{garcia2015comprehensive}
Garcia, J. and Fernandez, F.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 16\penalty0 (1):\penalty0 1437--1480, 2015.

\bibitem[Hordijk \& Kallenberg(1979)Hordijk and Kallenberg]{hordijk1979linear}
Hordijk, A. and Kallenberg, L.
\newblock Linear programming and markov decision chains.
\newblock \emph{Management Science}, 25\penalty0 (4):\penalty0 352--362, 1979.

\bibitem[Hu et~al.(2022)Hu, Liu, Chitlangia, Agnihotri, and Zhao]{Hu_2022_CVPR}
Hu, H., Liu, Z., Chitlangia, S., Agnihotri, A., and Zhao, D.
\newblock Investigating the impact of multi-lidar placement on object detection for autonomous driving.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  2550--2559, June 2022.

\bibitem[Hunter(2005)]{hunter2005stationary}
Hunter, J.~J.
\newblock Stationary distributions and mean first passage times of perturbed markov chains.
\newblock \emph{Linear Algebra and its Applications}, 410:\penalty0 217--243, 2005.

\bibitem[Hunter(2014)]{hunter2014mathematical}
Hunter, J.~J.
\newblock \emph{Mathematical techniques of applied probability: Discrete time models: Basic theory}, volume~1.
\newblock Academic Press, 2014.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximately}
Kakade, S. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, volume~2, pp.\  267--274, 2002.

\bibitem[Kemeny \& Snell(1960)Kemeny and Snell]{kemeny1960finite}
Kemeny, J. and Snell, I.
\newblock \emph{Finite {M}arkov {C}hains}.
\newblock Van Nostrand, New Jersey, 1960.

\bibitem[Levene \& Loizou(2002)Levene and Loizou]{levene2002kemeny}
Levene, M. and Loizou, G.
\newblock Kemeny's constant and the random surfer.
\newblock \emph{The American mathematical monthly}, 109\penalty0 (8):\penalty0 741--745, 2002.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine2016end}
Levine, S., Finn, C., Darrell, T., and Abbeel, P.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0 (1):\penalty0 1334--1373, 2016.

\bibitem[Liao et~al.(2022)Liao, Qi, Wan, Klasnja, and Murphy]{liao2022batch}
Liao, P., Qi, Z., Wan, R., Klasnja, P., and Murphy, S.~A.
\newblock Batch policy learning in average reward markov decision processes.
\newblock \emph{The Annals of Statistics}, 50\penalty0 (6):\penalty0 3364--3387, 2022.

\bibitem[Ma et~al.(2021)Ma, Tang, Xia, Yang, and Zhao]{ma2021average}
Ma, X., Tang, X., Xia, L., Yang, J., and Zhao, Q.
\newblock Average-reward reinforcement learning with trust region methods.
\newblock \emph{arXiv preprint arXiv:2106.03442}, 2021.

\bibitem[Manne(1960)]{manne1960linear}
Manne, A.~S.
\newblock Linear programming and sequential decisions.
\newblock \emph{Management Science}, 6\penalty0 (3):\penalty0 259--267, 1960.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare, M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Moskovitz et~al.(2023)Moskovitz, Singh, Strouse, Sandholm, Salakhutdinov, Dragan, and McAleer]{moskovitz2023confronting}
Moskovitz, T., Singh, A.~K., Strouse, D., Sandholm, T., Salakhutdinov, R., Dragan, A.~D., and McAleer, S.
\newblock Confronting reward model overoptimization with constrained rlhf.
\newblock \emph{arXiv preprint arXiv:2310.04373}, 2023.

\bibitem[Rajeswaran et~al.(2017)Rajeswaran, Kumar, Gupta, Vezzani, Schulman, Todorov, and Levine]{rajeswaran2017learning}
Rajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schulman, J., Todorov, E., and Levine, S.
\newblock Learning complex dexterous manipulation with deep reinforcement learning and demonstrations.
\newblock \emph{arXiv preprint arXiv:1709.10087}, 2017.

\bibitem[Satija et~al.(2020)Satija, Amortila, and Pineau]{pmlr-v119-satija20a}
Satija, H., Amortila, P., and Pineau, J.
\newblock Constrained {M}arkov decision processes via backward value functions.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th International Conference on Machine Learning}, volume 119 of \emph{Proceedings of Machine Learning Research}, pp.\  8502--8511. PMLR, 13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/satija20a.html}.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1889--1897, 2015.

\bibitem[Schulman et~al.(2016)Schulman, Moritz, Levine, Jordan, and Abbeel]{schulman2016high}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage estimation.
\newblock \emph{International Conference on Learning Representations (ICLR)}, 2016.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang, Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Song et~al.(2020)Song, Abdolmaleki, Springenberg, Clark, Soyer, Rae, Noury, Ahuja, Liu, Tirumala, et~al.]{song2020v}
Song, H.~F., Abdolmaleki, A., Springenberg, J.~T., Clark, A., Soyer, H., Rae, J.~W., Noury, S., Ahuja, A., Liu, S., Tirumala, D., et~al.
\newblock V-mpo: on-policy maximum a posteriori policy optimization for discrete and continuous control.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Tessler et~al.(2019)Tessler, Mankowitz, and Mannor]{tessler2018reward}
Tessler, C., Mankowitz, D.~J., and Mannor, S.
\newblock Reward constrained policy optimization.
\newblock \emph{International Conference on Learning Representation (ICLR)}, 2019.

\bibitem[Tibshirani(2017)]{tibshirani2017dykstra}
Tibshirani, R.~J.
\newblock Dykstra's algorithm, admm, and coordinate descent: Connections, insights, and extensions.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Vinitsky et~al.(2018)Vinitsky, Kreidieh, Le~Flem, Kheterpal, Jang, Wu, Liaw, Liang, and Bayen]{vinitsky2018benchmarks}
Vinitsky, E., Kreidieh, A., Le~Flem, L., Kheterpal, N., Jang, K., Wu, F., Liaw, R., Liang, E., and Bayen, A.~M.
\newblock Benchmarks for reinforcement learning in mixed-autonomy traffic.
\newblock In \emph{Proceedings of Conference on Robot Learning}, pp.\  399--409, 2018.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik, Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung, J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Vuong et~al.(2019)Vuong, Zhang, and Ross]{vuong2019supervised}
Vuong, Q., Zhang, Y., and Ross, K.~W.
\newblock Supervised policy update for deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representation (ICLR)}, 2019.

\bibitem[Wan et~al.(2021)Wan, Naik, and Sutton]{wan2021learning}
Wan, Y., Naik, A., and Sutton, R.~S.
\newblock Learning and planning in average-reward markov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10653--10662. PMLR, 2021.

\bibitem[Wei et~al.(2021)Wei, Jahromi, Luo, and Jain]{wei2021learning}
Wei, C.-Y., Jahromi, M.~J., Luo, H., and Jain, R.
\newblock Learning infinite-horizon average-reward mdps with linear function approximation.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  3007--3015. PMLR, 2021.

\bibitem[Wu et~al.(2017)Wu, Mansimov, Grosse, Liao, and Ba]{wu2017scalable}
Wu, Y., Mansimov, E., Grosse, R.~B., Liao, S., and Ba, J.
\newblock Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation.
\newblock In \emph{Advances in neural information processing systems (NIPS)}, pp.\  5285--5294, 2017.

\bibitem[Yang et~al.(2020)Yang, Rosca, Narasimhan, and Ramadge]{yang2020projection}
Yang, T.-Y., Rosca, J., Narasimhan, K., and Ramadge, P.~J.
\newblock Projection-based constrained policy optimization.
\newblock In \emph{International Conference on Learning Representation (ICLR)}, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Wan, Sutton, and Whiteson]{zhang2021average}
Zhang, S., Wan, Y., Sutton, R.~S., and Whiteson, S.
\newblock Average-reward off-policy policy evaluation with function approximation.
\newblock \emph{arXiv preprint arXiv:2101.02808}, 2021.

\bibitem[Zhang \& Ross(2020)Zhang and Ross]{zhang2020average}
Zhang, Y. and Ross, K.
\newblock Average reward reinforcement learning with monotonic policy improvement.
\newblock \emph{Preprint}, 2020.

\end{thebibliography}
