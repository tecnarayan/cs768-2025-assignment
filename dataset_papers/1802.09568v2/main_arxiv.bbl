\begin{thebibliography}{24}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, Kudlur, Levenberg, Monga, Moore, Murray, Steiner,
  Tucker, Vasudevan, Warden, Wicke, Yu, and Zheng]{tensorflow}
M.~Abadi, P.~Barham, J.~Chen, Z.~Chen, A.~Davis, J.~Dean, M.~Devin,
  S.~Ghemawat, G.~Irving, M.~Isard, M.~Kudlur, J.~Levenberg, R.~Monga,
  S.~Moore, D.~G. Murray, B.~Steiner, P.~Tucker, V.~Vasudevan, P.~Warden,
  M.~Wicke, Y.~Yu, and X.~Zheng.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{12th USENIX Symposium on Operating Systems Design and
  Implementation (OSDI 16)}, pages 265--283, 2016.

\bibitem[Agarwal et~al.(2016)Agarwal, Bullins, and Hazan]{agarwal2016second}
N.~Agarwal, B.~Bullins, and E.~Hazan.
\newblock Second order stochastic optimization in linear time.
\newblock \emph{arXiv preprint arXiv:1602.03943}, 2016.

\bibitem[Ando et~al.(2004)Ando, Li, and Mathias]{ando2004geometric}
T.~Ando, C.-K. Li, and R.~Mathias.
\newblock Geometric means.
\newblock \emph{Linear algebra and its applications}, 385:\penalty0 305--334,
  2004.

\bibitem[Cesa-Bianchi et~al.(2004)Cesa-Bianchi, Conconi, and
  Gentile]{cesa2004generalization}
N.~Cesa-Bianchi, A.~Conconi, and C.~Gentile.
\newblock On the generalization ability of on-line learning algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 50\penalty0
  (9):\penalty0 2050--2057, 2004.

\bibitem[Chelba et~al.(2013)Chelba, Mikolov, Schuster, Ge, Brants, Koehn, and
  Robinson]{lm1b}
C.~Chelba, T.~Mikolov, M.~Schuster, Q.~Ge, T.~Brants, P.~Koehn, and
  T.~Robinson.
\newblock One billion word benchmark for measuring progress in statistical
  language modeling.
\newblock Technical report, Google, 2013.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Erdogdu and Montanari(2015)]{erdogdu2015convergence}
M.~A. Erdogdu and A.~Montanari.
\newblock Convergence rates of sub-sampled newton methods.
\newblock In \emph{Proceedings of the 28th International Conference on Neural
  Information Processing Systems-Volume 2}, pages 3052--3060. MIT Press, 2015.

\bibitem[Fletcher(2013)]{fletcher2013practical}
R.~Fletcher.
\newblock \emph{Practical methods of optimization}.
\newblock John Wiley \& Sons, 2013.

\bibitem[Gonen and Shalev-Shwartz(2015)]{gonen2015faster}
A.~Gonen and S.~Shalev-Shwartz.
\newblock Faster sgd using sketched conditioning.
\newblock \emph{arXiv preprint arXiv:1506.02649}, 2015.

\bibitem[Gupta et~al.(2017)Gupta, Koren, and Singer]{gupta2017unified}
V.~Gupta, T.~Koren, and Y.~Singer.
\newblock A unified approach to adaptive regularization in online and
  stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1706.06569}, 2017.

\bibitem[Hazan(2016)]{hazan2016introduction}
E.~Hazan.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends in Optimization}, 2\penalty0
  (3-4):\penalty0 157--325, 2016.

\bibitem[Horn and Johnson(1991)]{horn1991topics}
R.~A. Horn and C.~R. Johnson.
\newblock Topics in matrix analysis, 1991.
\newblock \emph{Cambridge University Presss, Cambridge}, 37:\penalty0 39, 1991.

\bibitem[Kalai and Vempala(2005)]{kalai2005efficient}
A.~Kalai and S.~Vempala.
\newblock Efficient algorithms for online decision problems.
\newblock \emph{Journal of Computer and System Sciences}, 71\penalty0
  (3):\penalty0 291--307, 2005.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Lewis and Overton(2013)]{lewis2013nonsmooth}
A.~S. Lewis and M.~L. Overton.
\newblock Nonsmooth optimization via quasi-newton methods.
\newblock \emph{Mathematical Programming}, 141\penalty0 (1-2):\penalty0
  135--163, 2013.

\bibitem[L{\"o}wner(1934)]{lowner1934monotone}
K.~L{\"o}wner.
\newblock {\"U}ber monotone matrixfunktionen.
\newblock \emph{Mathematische Zeitschrift}, 38\penalty0 (1):\penalty0 177--216,
  1934.

\bibitem[Martens and Grosse(2015)]{martens2015optimizing}
J.~Martens and R.~Grosse.
\newblock Optimizing neural networks with {K}ronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pages
  2408--2417, 2015.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Salakhutdinov, and
  Srebro]{neyshabur2015path}
B.~Neyshabur, R.~R. Salakhutdinov, and N.~Srebro.
\newblock Path-sgd: Path-normalized optimization in deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2422--2430, 2015.

\bibitem[Nocedal(1980)]{nocedal1980updating}
J.~Nocedal.
\newblock Updating quasi-newton matrices with limited storage.
\newblock \emph{Mathematics of computation}, 35\penalty0 (151):\penalty0
  773--782, 1980.

\bibitem[Pilanci and Wainwright(2017)]{pilanci2017newton}
M.~Pilanci and M.~J. Wainwright.
\newblock Newton sketch: {A} near linear-time optimization algorithm with
  linear-quadratic convergence.
\newblock \emph{{SIAM} Journal on Optimization}, 27\penalty0 (1):\penalty0
  205--245, 2017.

\bibitem[Shalev-Shwartz(2012)]{shalev2012online}
S.~Shalev-Shwartz.
\newblock Online learning and online convex optimization.
\newblock \emph{Foundations and Trends in Machine Learning}, 4\penalty0
  (2):\penalty0 107--194, 2012.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6000--6010, 2017.

\bibitem[Xu et~al.(2016)Xu, Yang, Roosta-Khorasani, R{\'e}, and
  Mahoney]{xu2016sub}
P.~Xu, J.~Yang, F.~Roosta-Khorasani, C.~R{\'e}, and M.~W. Mahoney.
\newblock Sub-sampled newton methods with non-uniform sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3000--3008, 2016.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{5th International Conference on Learning Representations,
  ICLR 2017}, 2017.

\end{thebibliography}
