\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amini and Yousefian(2019)]{amini2019iterative}
Mostafa Amini and Farzad Yousefian.
\newblock An iterative regularized incremental projected subgradient method for a class of bilevel optimization problems.
\newblock In \emph{2019 American Control Conference (ACC)}, pages 4069--4074. IEEE, 2019.

\bibitem[Arbel and Mairal(2022)]{arbel2022amortized}
Michael Arbel and Julien Mairal.
\newblock Amortized implicit differentiation for stochastic bilevel optimization.
\newblock In \emph{ICLR}, 2022.

\bibitem[Beck and Sabach(2014)]{beck2014first}
Amir Beck and Shoham Sabach.
\newblock A first order method for finding minimal norm-like solutions of convex optimization problems.
\newblock \emph{Mathematical Programming}, 147\penalty0 (1):\penalty0 25--46, 2014.

\bibitem[Bubeck et~al.(2015)]{bubeck2015convex}
S{\'e}bastien Bubeck et~al.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 8\penalty0 (3-4):\penalty0 231--357, 2015.

\bibitem[Cao et~al.(2023)Cao, Jiang, Abolfazli, Yazdandoost~Hamedani, and Mokhtari]{cao2023projection}
Jincheng Cao, Ruichen Jiang, Nazanin Abolfazli, Erfan Yazdandoost~Hamedani, and Aryan Mokhtari.
\newblock Projection-free methods for stochastic simple bilevel optimization with convex lower-level problem.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Cao et~al.(2024)Cao, Jiang, Hamedani, and Mokhtari]{cao2024accelerated}
Jincheng Cao, Ruichen Jiang, Erfan~Yazdandoost Hamedani, and Aryan Mokhtari.
\newblock An accelerated gradient method for simple bilevel optimization with convex lower-level problem.
\newblock In \emph{NeurIPS}, 2024.

\bibitem[Carmon et~al.(2020)Carmon, Duchi, Hinder, and Sidford]{carmon2020lower}
Yair Carmon, John~C. Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Lower bounds for finding stationary points i.
\newblock \emph{Mathematical Programming}, 184\penalty0 (1):\penalty0 71--120, 2020.

\bibitem[Chang and Lin(2011)]{CC01a}
Chih-Chung Chang and Chih-Jen Lin.
\newblock {LIBSVM}: A library for support vector machines.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology}, 2:\penalty0 27:1--27:27, 2011.
\newblock Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}.

\bibitem[Chen et~al.(2024{\natexlab{a}})Chen, Xu, and Zhang]{chen2024finding}
Lesi Chen, Jing Xu, and Jingzhao Zhang.
\newblock On finding small hyper-gradients in bilevel optimization: Hardness results and improved analysis.
\newblock In \emph{COLT}, 2024{\natexlab{a}}.

\bibitem[Chen et~al.(2024{\natexlab{b}})Chen, Shi, Jiang, and Wang]{chen2024penalty}
Pengyu Chen, Xu~Shi, Rujun Jiang, and Jiulin Wang.
\newblock {Penalty-based methods for simple bilevel optimization under Holderian Error Bounds}.
\newblock In \emph{NeurIPS}, 2024{\natexlab{b}}.

\bibitem[Combettes and Pesquet(2011)]{combettes2011proximal}
Patrick~L Combettes and Jean-Christophe Pesquet.
\newblock Proximal splitting methods in signal processing.
\newblock \emph{Fixed-point algorithms for inverse problems in science and engineering}, pages 185--212, 2011.

\bibitem[Helou and Sim{\~o}es(2017)]{helou2017}
Elias~S. Helou and Lucas~EA Sim{\~o}es.
\newblock $\epsilon$-subgradient algorithms for bilevel convex optimization.
\newblock \emph{Inverse problems}, 33\penalty0 (5):\penalty0 055020, 2017.

\bibitem[Jiang et~al.(2023)Jiang, Abolfazli, Mokhtari, and Hamedani]{jiang2023conditional}
Ruichen Jiang, Nazanin Abolfazli, Aryan Mokhtari, and Erfan~Yazdandoost Hamedani.
\newblock A conditional gradient-based method for simple bilevel optimization with convex lower-level problem.
\newblock In \emph{AISTATS}, 2023.

\bibitem[Kaushik and Yousefian(2021)]{kaushik2021method}
Harshal~D. Kaushik and Farzad Yousefian.
\newblock A method with convergence rates for optimization problems with variational inequality constraints.
\newblock \emph{SIAM Journal on Optimization}, 31\penalty0 (3):\penalty0 2171--2198, 2021.

\bibitem[Kissel et~al.(2020)Kissel, Gottwald, and Diepold]{kissel2020neural}
Matthias Kissel, Martin Gottwald, and Klaus Diepold.
\newblock Neural network training with safe regularization in the null space of batch activations.
\newblock In \emph{Artificial Neural Networks and Machine Learning}, pages 217--228. Springer, 2020.

\bibitem[Lewis et~al.(2004)Lewis, Yang, Russell-Rose, and Li]{lewis2004rcv1}
David~D Lewis, Yiming Yang, Tony Russell-Rose, and Fan Li.
\newblock Rcv1: A new benchmark collection for text categorization research.
\newblock \emph{Journal of machine learning research}, 5\penalty0 (Apr):\penalty0 361--397, 2004.

\bibitem[Lin et~al.(2018)Lin, Mairal, and Harchaoui]{lin2018catalyst}
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.
\newblock Catalyst acceleration for first-order convex optimization: from theory to practice.
\newblock \emph{JMLR}, 18\penalty0 (212):\penalty0 1--54, 2018.

\bibitem[Malitsky(2017)]{malitsky2017chambolle}
Yura Malitsky.
\newblock {Chambolle-pock and Tseng's methods: relationship and extension to the bilevel optimization}.
\newblock \emph{arXiv preprint arXiv:1706.02602}, 2017.

\bibitem[Merchav and Sabach(2023)]{merchav2023convex}
Roey Merchav and Shoham Sabach.
\newblock Convex bi-level optimization problems with nonsmooth outer objective function.
\newblock \emph{SIAM Journal on Optimization}, 33\penalty0 (4):\penalty0 3114--3142, 2023.

\bibitem[Nesterov(2018)]{nesterov2018lectures}
Yurii Nesterov.
\newblock \emph{Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem[Pang(1997)]{pang1997error}
Jong-Shi Pang.
\newblock Error bounds in mathematical programming.
\newblock \emph{Mathematical Programming}, 79\penalty0 (1):\penalty0 299--332, 1997.

\bibitem[Rozemberczki et~al.(2021)Rozemberczki, Scherer, He, Panagopoulos, Riedel, Astefanoaei, Kiss, Beres, Lopez, Collignon, et~al.]{rozemberczki2021pytorch}
Benedek Rozemberczki, Paul Scherer, Yixuan He, George Panagopoulos, Alexander Riedel, Maria Astefanoaei, Oliver Kiss, Ferenc Beres, Guzman Lopez, Nicolas Collignon, et~al.
\newblock Pytorch geometric temporal: Spatiotemporal signal processing with neural machine learning models.
\newblock In \emph{Proceedings of the 30th ACM international conference on information \& knowledge management}, pages 4564--4573, 2021.

\bibitem[Sabach and Shtern(2017)]{sabach2017first}
Shoham Sabach and Shimrit Shtern.
\newblock A first order method for solving convex bilevel optimization problems.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (2):\penalty0 640--660, 2017.

\bibitem[Samadi et~al.(2023)Samadi, Burbano, and Yousefian]{samadi2023achieving}
Sepideh Samadi, Daniel Burbano, and Farzad Yousefian.
\newblock Achieving optimal complexity guarantees for a class of bilevel convex optimization problems.
\newblock \emph{arXiv preprint arXiv:2310.12247}, 2023.

\bibitem[Shen et~al.(2023)Shen, Ho-Nguyen, and K{\i}l{\i}n{\c{c}}-Karzan]{shen2023online}
Lingqing Shen, Nam Ho-Nguyen, and Fatma K{\i}l{\i}n{\c{c}}-Karzan.
\newblock An online convex optimization-based framework for convex bilevel optimization.
\newblock \emph{Mathematical Programming}, 198\penalty0 (2):\penalty0 1519--1582, 2023.

\bibitem[Solodov(2007{\natexlab{a}})]{solodov2007explicit}
Mikhail Solodov.
\newblock An explicit descent method for bilevel convex optimization.
\newblock \emph{Journal of Convex Analysis}, 14\penalty0 (2):\penalty0 227, 2007{\natexlab{a}}.

\bibitem[Solodov(2007{\natexlab{b}})]{solodov2007bundle}
Mikhail~V. Solodov.
\newblock A bundle method for a class of bilevel nonsmooth convex minimization problems.
\newblock \emph{SIAM Journal on Optimization}, 18\penalty0 (1):\penalty0 242--259, 2007{\natexlab{b}}.

\bibitem[Wang et~al.(2024)Wang, Shi, and Jiang]{wang2024near}
Jiulin Wang, Xu~Shi, and Rujun Jiang.
\newblock Near-optimal convex simple bilevel optimization with a bisection method.
\newblock In \emph{AISTATS}, 2024.

\end{thebibliography}
