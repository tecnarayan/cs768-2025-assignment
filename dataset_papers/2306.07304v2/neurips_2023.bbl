\begin{thebibliography}{66}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Tripicchio and D’Avella(2020)]{tripicchio2020deep}
Paolo Tripicchio and Salvatore D’Avella.
\newblock Is deep learning ready to satisfy industry needs?
\newblock \emph{Procedia Manufacturing}, 51:\penalty0 1192--1199, 2020.

\bibitem[Doshi-Velez and Kim(2017)]{doshivelez2017rigorous}
Finale Doshi-Velez and Been Kim.
\newblock Towards a rigorous science of interpretable machine learning.
\newblock \emph{{A}r{X}iv e-print}, 2017.

\bibitem[Jacovi et~al.(2021)Jacovi, Marasovi{\'c}, Miller, and
  Goldberg]{jacovi2021formalizing}
Alon Jacovi, Ana Marasovi{\'c}, Tim Miller, and Yoav Goldberg.
\newblock Formalizing trust in artificial intelligence: Prerequisites, causes
  and goals of human trust in ai.
\newblock In \emph{Proceedings of the 2021 ACM conference on fairness,
  accountability, and transparency}, pages 624--635, 2021.

\bibitem[Simonyan et~al.(2013)Simonyan, Vedaldi, and
  Zisserman]{simonyan2013deep}
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
\newblock Deep inside convolutional networks: Visualising image classification
  models and saliency maps.
\newblock In \emph{Workshop, Proceedings of the International Conference on
  Learning Representations (ICLR)}, 2013.

\bibitem[Smilkov et~al.(2017)Smilkov, Thorat, Kim, Viégas, and
  Wattenberg]{smilkov2017smoothgrad}
Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin
  Wattenberg.
\newblock Smoothgrad: removing noise by adding noise.
\newblock In \emph{Workshop on Visualization for Deep Learning, Proceedings of
  the International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Shrikumar et~al.(2017)Shrikumar, Greenside, and
  Kundaje]{shrikumar2017learning}
Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje.
\newblock Learning important features through propagating activation
  differences.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2017.

\bibitem[Sundararajan et~al.(2017)Sundararajan, Taly, and
  Yan]{sundararajan2017axiomatic}
Mukund Sundararajan, Ankur Taly, and Qiqi Yan.
\newblock Axiomatic attribution for deep networks.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2017.

\bibitem[Selvaraju et~al.(2017)Selvaraju, Cogswell, Das, Vedantam, Parikh, and
  Batra]{Selvaraju_2019}
Ramprasaath~R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam,
  Devi Parikh, and Dhruv Batra.
\newblock Grad-cam: Visual explanations from deep networks via gradient-based
  localization.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, 2017.

\bibitem[Fel et~al.(2021)Fel, Cadene, Chalvidal, Cord, Vigouroux, and
  Serre]{fel2021sobol}
Thomas Fel, Remi Cadene, Mathieu Chalvidal, Matthieu Cord, David Vigouroux, and
  Thomas Serre.
\newblock Look at the variance! efficient black-box explanations with
  sobol-based sensitivity analysis.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Novello et~al.(2022)Novello, Fel, and Vigouroux]{novello2022making}
Paul Novello, Thomas Fel, and David Vigouroux.
\newblock Making sense of dependence: Efficient black-box explanations using
  dependence measure.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2022.

\bibitem[Fel et~al.(2022{\natexlab{a}})Fel, Ducoffe, Vigouroux, Cadene,
  Capelle, Nicodeme, and Serre]{eva}
Thomas Fel, Melanie Ducoffe, David Vigouroux, Remi Cadene, Mikael Capelle,
  Claire Nicodeme, and Thomas Serre.
\newblock Don't lie to me! robust and efficient explainability with verified
  perturbation analysis.
\newblock \emph{Workshop on Formal Verification of Machine Learning,
  Proceedings of the International Conference on Machine Learning (ICML)},
  2022{\natexlab{a}}.

\bibitem[Graziani et~al.(2021)Graziani, Palatnik~de Sousa, Vellasco, Costa~da
  Silva, M{\"u}ller, and Andrearczyk]{graziani2021sharpening}
Mara Graziani, Iam Palatnik~de Sousa, Marley~MBR Vellasco, Eduardo Costa~da
  Silva, Henning M{\"u}ller, and Vincent Andrearczyk.
\newblock Sharpening local interpretable model-agnostic explanations for
  histopathology: improved understandability and reliability.
\newblock In \emph{Medical Image Computing and Computer Assisted Intervention
  (MICCAI)}. Springer, 2021.

\bibitem[Zeiler and Fergus(2014{\natexlab{a}})]{zeiler2014visualizing}
Matthew~D Zeiler and Rob Fergus.
\newblock Visualizing and understanding convolutional networks.
\newblock In \emph{Proceedings of the IEEE European Conference on Computer
  Vision (ECCV)}, 2014{\natexlab{a}}.

\bibitem[Fong and Vedaldi(2017)]{Fong_2017}
Ruth~C. Fong and Andrea Vedaldi.
\newblock Interpretable explanations of black boxes by meaningful perturbation.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, 2017.

\bibitem[Idrissi et~al.(2023)Idrissi, Bousquet, Gamboa, Iooss, and
  Loubes]{idrissi2023coalitional}
Marouane~Il Idrissi, Nicolas Bousquet, Fabrice Gamboa, Bertrand Iooss, and
  Jean-Michel Loubes.
\newblock On the coalitional decomposition of parameters of interest, 2023.

\bibitem[Adebayo et~al.(2018)Adebayo, Gilmer, Muelly, Goodfellow, Hardt, and
  Kim]{adebayo2018sanity}
Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt,
  and Been Kim.
\newblock Sanity checks for saliency maps.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2018.

\bibitem[Sixt et~al.(2020)Sixt, Granz, and Landgraf]{sixt2020explanations}
Leon Sixt, Maximilian Granz, and Tim Landgraf.
\newblock When explanations lie: Why many modified bp attributions fail.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2020.

\bibitem[Slack et~al.(2021)Slack, Hilgard, Singh, and
  Lakkaraju]{slack2021reliable}
Dylan Slack, Anna Hilgard, Sameer Singh, and Himabindu Lakkaraju.
\newblock Reliable post hoc explanations: Modeling uncertainty in
  explainability.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  34, 2021.

\bibitem[Rao et~al.(2022)Rao, B{\"o}hle, and Schiele]{rao2022towards}
Sukrut Rao, Moritz B{\"o}hle, and Bernt Schiele.
\newblock Towards better understanding attribution methods.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2022.

\bibitem[Hase and Bansal(2020)]{hase2020evaluating}
Peter Hase and Mohit Bansal.
\newblock Evaluating explainable ai: Which algorithmic explanations help users
  predict model behavior?
\newblock \emph{Proceedings of the Annual Meeting of the Association for
  Computational Linguistics (ACL)}, 2020.

\bibitem[Shen and Huang(2020)]{shen2020useful}
Hua Shen and Ting-Hao Huang.
\newblock How useful are the machine-generated interpretations to general
  users? a human evaluation on guessing the incorrectly predicted labels.
\newblock In \emph{Proceedings of the AAAI Conference on Human Computation and
  Crowdsourcing}, volume~8, pages 168--172, 2020.

\bibitem[Colin et~al.(2021)Colin, Fel, Cad{\`e}ne, and Serre]{fel2021cannot}
Julien Colin, Thomas Fel, R{\'e}mi Cad{\`e}ne, and Thomas Serre.
\newblock What i cannot predict, i do not understand: A human-centered
  evaluation framework for explainability methods.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem[Kim et~al.(2022)Kim, Meister, Ramaswamy, Fong, and
  Russakovsky]{kim2021hive}
Sunnie S.~Y. Kim, Nicole Meister, Vikram~V. Ramaswamy, Ruth Fong, and Olga
  Russakovsky.
\newblock {HIVE}: Evaluating the human interpretability of visual explanations.
\newblock In \emph{Proceedings of the IEEE European Conference on Computer
  Vision (ECCV)}, 2022.

\bibitem[Nguyen et~al.(2021)Nguyen, Kim, and Nguyen]{nguyen2021effectiveness}
Giang Nguyen, Daeyoung Kim, and Anh Nguyen.
\newblock The effectiveness of feature attribution methods and its correlation
  with automatic evaluation scores.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem[Sixt et~al.(2022)Sixt, Schuessler, Popescu, Wei{\ss}, and
  Landgraf]{sixt2022users}
Leon Sixt, Martin Schuessler, Oana-Iuliana Popescu, Philipp Wei{\ss}, and Tim
  Landgraf.
\newblock Do users benefit from interpretable vision? a user study, baseline,
  and dataset.
\newblock \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2022.

\bibitem[Kim et~al.(2018)Kim, Wattenberg, Gilmer, Cai, Wexler, Viegas,
  et~al.]{kim2018interpretability}
Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda
  Viegas, et~al.
\newblock Interpretability beyond feature attribution: Quantitative testing
  with concept activation vectors (tcav).
\newblock In \emph{International conference on machine learning}. Proceedings
  of the International Conference on Machine Learning (ICML), 2018.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Wexler, Zou, and
  Kim]{ghorbani2019towards}
Amirata Ghorbani, James Wexler, James~Y Zou, and Been Kim.
\newblock Towards automatic concept-based explanations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9273--9282, 2019.

\bibitem[Zhang et~al.(2021)Zhang, Madumal, Miller, Ehinger, and
  Rubinstein]{zhang2021invertible}
Ruihan Zhang, Prashan Madumal, Tim Miller, Krista~A Ehinger, and Benjamin~IP
  Rubinstein.
\newblock Invertible concept-based explanations for cnn models with
  non-negative concept activation vectors.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 11682--11690, 2021.

\bibitem[Fel et~al.(2023)Fel, Picard, Bethune, Boissin, Vigouroux, Colin,
  Cadène, and Serre]{fel2023craft}
Thomas Fel, Agustin Picard, Louis Bethune, Thibaut Boissin, David Vigouroux,
  Julien Colin, Rémi Cadène, and Thomas Serre.
\newblock Craft: Concept recursive activation factorization for explainability.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2023.

\bibitem[Graziani et~al.(2023)Graziani, Nguyen, O'Mahony, M{\"u}ller, and
  Andrearczyk]{graziani2023concept}
Mara Graziani, An-phi Nguyen, Laura O'Mahony, Henning M{\"u}ller, and Vincent
  Andrearczyk.
\newblock Concept discovery and dataset exploration with singular value
  decomposition.
\newblock In \emph{ICLR 2023 Workshop on Pitfalls of limited data and
  computation for Trustworthy ML}, 2023.

\bibitem[Genone and Lombrozo(2012)]{genone2012concept}
James Genone and Tania Lombrozo.
\newblock Concept possession, experimental semantics, and hybrid theories of
  reference.
\newblock \emph{Philosophical Psychology}, 25\penalty0 (5):\penalty0 717--742,
  2012.

\bibitem[Petsiuk et~al.(2018)Petsiuk, Das, and Saenko]{petsiuk2018rise}
Vitali Petsiuk, Abir Das, and Kate Saenko.
\newblock Rise: Randomized input sampling for explanation of black-box models.
\newblock In \emph{Proceedings of the British Machine Vision Conference
  (BMVC)}, 2018.

\bibitem[Bhatt et~al.(2020)Bhatt, Weller, and Moura]{aggregating2020}
Umang Bhatt, Adrian Weller, and José M.~F. Moura.
\newblock Evaluating and aggregating feature-based model explanations.
\newblock In \emph{Proceedings of the International Joint Conference on
  Artificial Intelligence (IJCAI)}, 2020.

\bibitem[Haug et~al.(2021)Haug, Z{\"u}rn, El-Jiz, and
  Kasneci]{haug2021baselines}
Johannes Haug, Stefan Z{\"u}rn, Peter El-Jiz, and Gjergji Kasneci.
\newblock On baselines for local feature attributions.
\newblock \emph{arXiv preprint arXiv:2101.00905}, 2021.

\bibitem[Hsieh et~al.(2021)Hsieh, Yeh, Liu, Ravikumar, Kim, Kumar, and
  Hsieh]{hsieh2020evaluations}
Cheng-Yu Hsieh, Chih-Kuan Yeh, Xuanqing Liu, Pradeep Ravikumar, Seungyeon Kim,
  Sanjiv Kumar, and Cho-Jui Hsieh.
\newblock Evaluations and methods for explanation through robustness analysis.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2021.

\bibitem[Kindermans et~al.(2019)Kindermans, Hooker, Adebayo, Alber, Sch{\"u}tt,
  D{\"a}hne, Erhan, and Kim]{kindermans2019reliability}
Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof~T
  Sch{\"u}tt, Sven D{\"a}hne, Dumitru Erhan, and Been Kim.
\newblock The (un) reliability of saliency methods.
\newblock 2019.

\bibitem[Sturmfels et~al.(2020)Sturmfels, Lundberg, and
  Lee]{sturmfels2020visualizing}
Pascal Sturmfels, Scott Lundberg, and Su-In Lee.
\newblock Visualizing the impact of feature attribution baselines.
\newblock \emph{Distill}, 2020.

\bibitem[Vielhaben et~al.(2023)Vielhaben, Bl{\"u}cher, and
  Strodthoff]{vielhaben2023multi}
Johanna Vielhaben, Stefan Bl{\"u}cher, and Nils Strodthoff.
\newblock Multi-dimensional concept discovery (mcd): A unifying framework with
  completeness guarantees.
\newblock 2023.

\bibitem[Elhage et~al.(2022{\natexlab{a}})Elhage, Hume, Olsson, Schiefer,
  Henighan, Kravec, Hatfield-Dodds, Lasenby, Drain, Chen,
  et~al.]{elhage2022toy}
Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan,
  Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen,
  et~al.
\newblock Toy models of superposition.
\newblock \emph{arXiv preprint arXiv:2209.10652}, 2022{\natexlab{a}}.

\bibitem[Mairal et~al.(2014)Mairal, Bach, Ponce, et~al.]{mairal2014sparse}
Julien Mairal, Francis Bach, Jean Ponce, et~al.
\newblock Sparse modeling for image and vision processing.
\newblock \emph{Foundations and Trends{\textregistered} in Computer Graphics
  and Vision}, 8\penalty0 (2-3):\penalty0 85--283, 2014.

\bibitem[Makhzani and Frey(2014)]{makhzani2013k}
Alireza Makhzani and Brendan Frey.
\newblock K-sparse autoencoders.
\newblock \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2014.

\bibitem[Bricken et~al.(2023)Bricken, Templeton, Batson, Chen, Jermyn, Conerly,
  Turner, Anil, Denison, Askell, Lasenby, Wu, Kravec, Schiefer, Maxwell,
  Joseph, Hatfield-Dodds, Tamkin, Nguyen, McLean, Burke, Hume, Carter,
  Henighan, and Olah]{bricken2023monosemanticity}
Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom
  Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert
  Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas
  Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean,
  Josiah~E Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher
  Olah.
\newblock Towards monosemanticity: Decomposing language models with dictionary
  learning.
\newblock \emph{Transformer Circuits Thread}, 2023.
\newblock
  https://transformer-circuits.pub/2023/monosemantic-features/index.html.

\bibitem[Elhage et~al.(2022{\natexlab{b}})Elhage, Hume, Olsson, Schiefer,
  Henighan, Kravec, Hatfield-Dodds, Lasenby, Drain, Chen, Grosse, McCandlish,
  Kaplan, Amodei, Wattenberg, and Olah]{elhage2022superposition}
Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan,
  Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen,
  Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg,
  and Christopher Olah.
\newblock Toy models of superposition.
\newblock \emph{Transformer Circuits Thread}, 2022{\natexlab{b}}.

\bibitem[Eckart and Young(1936)]{eckart1936approximation}
Carl Eckart and Gale Young.
\newblock The approximation of one matrix by another of lower rank.
\newblock \emph{Psychometrika}, 1\penalty0 (3):\penalty0 211--218, 1936.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{neural-collapse}
Vardan Papyan, XY~Han, and David~L Donoho.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (40):\penalty0 24652--24663, 2020.

\bibitem[Ding et~al.(2005)Ding, He, and Simon]{ding2005equivalence}
Chris Ding, Xiaofeng He, and Horst~D Simon.
\newblock On the equivalence of nonnegative matrix factorization and spectral
  clustering.
\newblock In \emph{Proceedings of the 2005 SIAM international conference on
  data mining}, pages 606--610. SIAM, 2005.

\bibitem[Zhang et~al.(2018)Zhang, Weng, Chen, Hsieh, and
  Daniel]{zhang2018efficient}
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel.
\newblock Efficient neural network robustness certification with general
  activation functions.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2016.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{sandler2018mobilenetv2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4510--4520, 2018.

\bibitem[Parekh et~al.(2022)Parekh, Parekh, Mozharovskyi, d'Alch{\'e} Buc, and
  Richard]{parekh2022listen}
Jayneel Parekh, Sanjeel Parekh, Pavlo Mozharovskyi, Florence d'Alch{\'e} Buc,
  and Ga{\"e}l Richard.
\newblock Listen to interpret: Post-hoc interpretability for audio networks
  with nmf.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2022.

\bibitem[Howard()]{imagenette}
Jeremy Howard.
\newblock Imagenette dataset.
\newblock URL \url{https://github.com/fastai/imagenette/}.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet2002stability}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock \emph{The Journal of Machine Learning Research}, 2002.

\bibitem[Villani et~al.()]{villaniOpt}
C{\'e}dric Villani et~al.
\newblock \emph{Optimal transport: old and new}, volume 338.
\newblock Springer.

\bibitem[Sun et~al.(2022)Sun, Ming, Zhu, and Li]{sun2022out}
Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li.
\newblock Out-of-distribution detection with deep nearest neighbors.
\newblock In \emph{International Conference on Machine Learning}, pages
  20827--20840. PMLR, 2022.

\bibitem[Ghorbani et~al.(2017)Ghorbani, Abid, and
  Zou]{ghorbani2017interpretation}
Amirata Ghorbani, Abubakar Abid, and James Zou.
\newblock Interpretation of neural networks is fragile.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence (AAAI)}, 2017.

\bibitem[Hooker et~al.(2019)Hooker, Erhan, Kindermans, and
  Kim]{hooker2018benchmark}
Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim.
\newblock A benchmark for interpretability methods in deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Jacovi and Goldberg(2020)]{jacovi2020towards}
Alon Jacovi and Yoav Goldberg.
\newblock Towards faithfully interpretable nlp systems: How should we define
  and evaluate faithfulness?
\newblock \emph{Proceedings of the Annual Meeting of the Association for
  Computational Linguistics (ACL)}, 2020.

\bibitem[McInnes et~al.(2018)McInnes, Healy, and Melville]{mcinnes2018umap}
Leland McInnes, John Healy, and James Melville.
\newblock Umap: Uniform manifold approximation and projection for dimension
  reduction.
\newblock \emph{arXiv preprint arXiv:1802.03426}, 2018.

\bibitem[Fel et~al.(2022{\natexlab{b}})Fel, Hervier, Vigouroux, Poche, Plakoo,
  Cadene, Chalvidal, Colin, Boissin, Bethune, Picard, Nicodeme, Gardes,
  Flandin, and Serre]{fel2022xplique}
Thomas Fel, Lucas Hervier, David Vigouroux, Antonin Poche, Justin Plakoo, Remi
  Cadene, Mathieu Chalvidal, Julien Colin, Thibaut Boissin, Louis Bethune,
  Agustin Picard, Claire Nicodeme, Laurent Gardes, Gregory Flandin, and Thomas
  Serre.
\newblock Xplique: A deep learning explainability toolbox.
\newblock \emph{Workshop on Explainable Artificial Intelligence for Computer
  Vision (CVPR)}, 2022{\natexlab{b}}.

\bibitem[Ancona et~al.(2018)Ancona, Ceolini, Öztireli, and
  Gross]{ancona2017better}
Marco Ancona, Enea Ceolini, Cengiz Öztireli, and Markus Gross.
\newblock Towards better understanding of gradient-based attribution methods
  for deep neural networks.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2018.

\bibitem[Sotoudeh and Thakur(2019)]{sotoudeh2019computing}
Matthew Sotoudeh and Aditya~V. Thakur.
\newblock Computing linear restrictions of neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Zeiler and Fergus(2014{\natexlab{b}})]{zeiler2013visualizing}
Matthew~D Zeiler and Rob Fergus.
\newblock Visualizing and understanding convolutional networks.
\newblock In \emph{Proceedings of the IEEE European Conference on Computer
  Vision (ECCV)}, 2014{\natexlab{b}}.

\bibitem[Whitney(1992)]{whitney1992abstract}
Hassler Whitney.
\newblock On the abstract properties of linear dependence.
\newblock \emph{Hassler Whitney Collected Papers}, pages 147--171, 1992.

\bibitem[Fathi~Hafshejani and Moaberfard(2023)]{fathi2023initialization}
Sajad Fathi~Hafshejani and Zahra Moaberfard.
\newblock Initialization for non-negative matrix factorization: a comprehensive
  review.
\newblock \emph{International Journal of Data Science and Analytics}, 2023.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2015.

\bibitem[Dumitrescu and Irofti(2018)]{dumitrescu2018dictionary}
Bogdan Dumitrescu and Paul Irofti.
\newblock \emph{Dictionary learning algorithms and applications}.
\newblock Springer, 2018.

\end{thebibliography}
