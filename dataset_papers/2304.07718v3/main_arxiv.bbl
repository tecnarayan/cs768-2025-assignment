\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2019)Agarwal, Dahleh, and Sarkar]{agarwal2019}
Agarwal, A., Dahleh, M., and Sarkar, T.
\newblock A marketplace for data: An algorithmic solution.
\newblock In \emph{Proceedings of the 2019 ACM Conference on Economics and
  Computation}, pp.\  701--726, 2019.

\bibitem[Arthur \& Vassilvitskii(2007)Arthur and Vassilvitskii]{arthur2007k}
Arthur, D. and Vassilvitskii, S.
\newblock k-means++ the advantages of careful seeding.
\newblock In \emph{Proceedings of the eighteenth annual ACM-SIAM symposium on
  Discrete algorithms}, pp.\  1027--1035, 2007.

\bibitem[Athey et~al.(2019)Athey, Tibshirani, and Wager]{athey2019generalized}
Athey, S., Tibshirani, J., and Wager, S.
\newblock Generalized random forests.
\newblock \emph{The Annals of Statistics}, 47\penalty0 (2):\penalty0
  1148--1178, 2019.

\bibitem[Bachrach et~al.(2010)Bachrach, Markakis, Resnick, Procaccia,
  Rosenschein, and Saberi]{bachrach2010approximating}
Bachrach, Y., Markakis, E., Resnick, E., Procaccia, A.~D., Rosenschein, J.~S.,
  and Saberi, A.
\newblock Approximating power indices: theoretical and empirical analysis.
\newblock \emph{Autonomous Agents and Multi-Agent Systems}, 20\penalty0
  (2):\penalty0 105--122, 2010.

\bibitem[Basu et~al.(2020)Basu, Pope, and Feizi]{basu2020influence}
Basu, S., Pope, P., and Feizi, S.
\newblock Influence functions in deep learning are fragile.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Breiman(1996)]{breiman1996bagging}
Breiman, L.
\newblock Bagging predictors.
\newblock \emph{Machine learning}, 24\penalty0 (2):\penalty0 123--140, 1996.

\bibitem[Breiman(2001)]{breiman2001random}
Breiman, L.
\newblock Random forests.
\newblock \emph{Machine learning}, 45\penalty0 (1):\penalty0 5--32, 2001.

\bibitem[Candes et~al.(2018)Candes, Fan, Janson, and Lv]{candes2018panning}
Candes, E., Fan, Y., Janson, L., and Lv, J.
\newblock Panning for gold:‘model-x’knockoffs for high dimensional
  controlled variable selection.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 80\penalty0 (3):\penalty0 551--577, 2018.

\bibitem[Cook \& Weisberg(1980)Cook and Weisberg]{cook1980characterizations}
Cook, R.~D. and Weisberg, S.
\newblock Characterizations of an empirical influence function for detecting
  influential cases in regression.
\newblock \emph{Technometrics}, 22\penalty0 (4):\penalty0 495--508, 1980.

\bibitem[Cook \& Weisberg(1982)Cook and Weisberg]{cook1982residuals}
Cook, R.~D. and Weisberg, S.
\newblock \emph{Residuals and influence in regression}.
\newblock New York: Chapman and Hall, 1982.

\bibitem[Covert et~al.(2021)Covert, Lundberg, and Lee]{covert2021explaining}
Covert, I., Lundberg, S., and Lee, S.-I.
\newblock Explaining by removing: A unified framework for model explanation.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (209):\penalty0 1--90, 2021.

\bibitem[Efron(1979)]{efron1979bootstrap}
Efron, B.
\newblock Bootstrap methods: Another look at the jackknife.
\newblock \emph{The Annals of Statistics}, pp.\  1--26, 1979.

\bibitem[Efron(1992)]{efron1992jackknife}
Efron, B.
\newblock Jackknife-after-bootstrap standard errors and influence functions.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 54\penalty0 (1):\penalty0 83--111, 1992.

\bibitem[Efron \& Tibshirani(1997)Efron and Tibshirani]{efron1997improvements}
Efron, B. and Tibshirani, R.
\newblock Improvements on cross-validation: the 632+ bootstrap method.
\newblock \emph{Journal of the American Statistical Association}, 92\penalty0
  (438):\penalty0 548--560, 1997.

\bibitem[Efron \& Tibshirani(1995)Efron and Tibshirani]{efron1995cross}
Efron, B. and Tibshirani, R.~J.
\newblock \emph{Cross-validation and the bootstrap: Estimating the error rate
  of a prediction rule}.
\newblock Stanford University, Technical Report NO.477, 1995.

\bibitem[Feldman \& Zhang(2020)Feldman and Zhang]{feldman2020neural}
Feldman, V. and Zhang, C.
\newblock What neural networks memorize and why: Discovering the long tail via
  influence estimation.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2881--2891, 2020.

\bibitem[Feurer et~al.(2021)Feurer, van Rijn, Kadra, Gijsbers, Mallik, Ravi,
  Muller, Vanschoren, and Hutter]{feurer-arxiv19a}
Feurer, M., van Rijn, J.~N., Kadra, A., Gijsbers, P., Mallik, N., Ravi, S.,
  Muller, A., Vanschoren, J., and Hutter, F.
\newblock Openml-python: an extensible python api for openml.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (100):\penalty0 1--5, 2021.
\newblock URL \url{http://jmlr.org/papers/v22/19-920.html}.

\bibitem[Gelman et~al.(1995)Gelman, Carlin, Stern, and
  Rubin]{gelman1995bayesian}
Gelman, A., Carlin, J.~B., Stern, H.~S., and Rubin, D.~B.
\newblock \emph{Bayesian data analysis}.
\newblock Chapman and Hall/CRC, 1995.

\bibitem[Ghorbani \& Zou(2019)Ghorbani and Zou]{ghorbani2019}
Ghorbani, A. and Zou, J.
\newblock Data shapley: Equitable valuation of data for machine learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2242--2251, 2019.

\bibitem[Ghorbani \& Zou(2020)Ghorbani and Zou]{ghorbani2020neuron}
Ghorbani, A. and Zou, J.~Y.
\newblock Neuron shapley: Discovering the responsible neurons.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5922--5932, 2020.

\bibitem[Ghorbani et~al.(2020)Ghorbani, Kim, and
  Zou]{ghorbani2020distributional}
Ghorbani, A., Kim, M., and Zou, J.
\newblock A distributional framework for data valuation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3535--3544. PMLR, 2020.

\bibitem[Hassine et~al.(2019)Hassine, Erbad, and Hamila]{hassine2019important}
Hassine, K., Erbad, A., and Hamila, R.
\newblock Important complexity reduction of random forest in
  multi-classification problem.
\newblock In \emph{2019 15th International Wireless Communications \& Mobile
  Computing Conference (IWCMC)}, pp.\  226--231. IEEE, 2019.

\bibitem[Ilyas et~al.(2022)Ilyas, Park, Engstrom, Leclerc, and
  Madry]{ilyas2022datamodels}
Ilyas, A., Park, S.~M., Engstrom, L., Leclerc, G., and Madry, A.
\newblock Datamodels: Understanding predictions with data and data with
  predictions.
\newblock In \emph{Proceedings of the 39th International Conference on Machine
  Learning}, volume 162, pp.\  9525--9587. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/ilyas22a.html}.

\bibitem[Jaeckel(1972)]{jaeckel1972infinitesimal}
Jaeckel, L.
\newblock The infinitesimal jackknife. memorandum.
\newblock Technical report, MM 72-1215-11, Bell Lab. Murray Hill, NJ, 1972.

\bibitem[Jia et~al.(2019{\natexlab{a}})Jia, Dao, Wang, Hubis, Gurel, Li, Zhang,
  Spanos, and Song]{jia2019b}
Jia, R., Dao, D., Wang, B., Hubis, F.~A., Gurel, N.~M., Li, B., Zhang, C.,
  Spanos, C., and Song, D.
\newblock Efficient task-specific data valuation for nearest neighbor
  algorithms.
\newblock \emph{Proceedings of the VLDB Endowment}, 12\penalty0 (11):\penalty0
  1610--1623, 2019{\natexlab{a}}.

\bibitem[Jia et~al.(2019{\natexlab{b}})Jia, Dao, Wang, Hubis, Hynes, G{\"u}rel,
  Li, Zhang, Song, and Spanos]{jia2019}
Jia, R., Dao, D., Wang, B., Hubis, F.~A., Hynes, N., G{\"u}rel, N.~M., Li, B.,
  Zhang, C., Song, D., and Spanos, C.~J.
\newblock Towards efficient data valuation based on the shapley value.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  1167--1176, 2019{\natexlab{b}}.

\bibitem[Koh \& Liang(2017)Koh and Liang]{koh2017understanding}
Koh, P.~W. and Liang, P.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1885--1894. PMLR, 2017.

\bibitem[Kumar et~al.(2020)Kumar, Venkatasubramanian, Scheidegger, and
  Friedler]{kumar2020problems}
Kumar, I.~E., Venkatasubramanian, S., Scheidegger, C., and Friedler, S.
\newblock Problems with shapley-value-based explanations as feature importance
  measures.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5491--5500. PMLR, 2020.

\bibitem[Kwon \& Zou(2022{\natexlab{a}})Kwon and Zou]{kwon2022beta}
Kwon, Y. and Zou, J.
\newblock Beta shapley: a unified and noise-reduced data valuation framework
  for machine learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  8780--8802. PMLR, 2022{\natexlab{a}}.

\bibitem[Kwon \& Zou(2022{\natexlab{b}})Kwon and Zou]{kwon2022weightedshap}
Kwon, Y. and Zou, J.
\newblock Weighted{SHAP}: analyzing and improving shapley based feature
  attributions.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022{\natexlab{b}}.

\bibitem[Kwon et~al.(2021)Kwon, Rivas, and Zou]{kwon2021efficient}
Kwon, Y., Rivas, M.~A., and Zou, J.
\newblock Efficient computation and analysis of distributional shapley values.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  793--801. PMLR, 2021.

\bibitem[Lin et~al.(2022)Lin, Zhang, L{\'e}cuyer, Li, Panda, and
  Sen]{lin2022measuring}
Lin, J., Zhang, A., L{\'e}cuyer, M., Li, J., Panda, A., and Sen, S.
\newblock Measuring the effect of training data on deep learning predictions
  via randomized experiments.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  13468--13504. PMLR, 2022.

\bibitem[Lundberg \& Lee(2017)Lundberg and Lee]{lundberg2017unified}
Lundberg, S.~M. and Lee, S.-I.
\newblock A unified approach to interpreting model predictions.
\newblock In \emph{Proceedings of the 31st international conference on neural
  information processing systems}, pp.\  4768--4777, 2017.

\bibitem[Maleki et~al.(2013)Maleki, Tran-Thanh, Hines, Rahwan, and
  Rogers]{maleki2013bounding}
Maleki, S., Tran-Thanh, L., Hines, G., Rahwan, T., and Rogers, A.
\newblock Bounding the estimation error of sampling-based shapley value
  approximation.
\newblock \emph{arXiv preprint arXiv:1306.4265}, 2013.

\bibitem[Mallows(1975)]{mallows1975some}
Mallows, C.~L.
\newblock On some topics in robustness.
\newblock \emph{Unpublished memorandum, Bell Telephone Laboratories, Murray
  Hill, NJ}, 37, 1975.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{Pedregosa2011Scikit}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
  O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J.,
  Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2825--2830,
  2011.

\bibitem[Rozemberczki et~al.(2022)Rozemberczki, Watson, Bayer, Yang, Kiss,
  Nilsson, and Sarkar]{rozemberczki2022shapley}
Rozemberczki, B., Watson, L., Bayer, P., Yang, H.-T., Kiss, O., Nilsson, S.,
  and Sarkar, R.
\newblock The shapley value in machine learning.
\newblock \emph{arXiv preprint arXiv:2202.05594}, 2022.

\bibitem[Schoch et~al.(2022)Schoch, Xu, and Ji]{schoch2022cs}
Schoch, S., Xu, H., and Ji, Y.
\newblock {CS}-shapley: Class-wise shapley values for data valuation in
  classification.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Shapley(1953)]{shapley1953}
Shapley, L.~S.
\newblock A value for n-person games.
\newblock \emph{Contributions to the Theory of Games}, 2\penalty0
  (28):\penalty0 307--317, 1953.

\bibitem[Sim et~al.(2020)Sim, Zhang, Chan, and Low]{sim2020collaborative}
Sim, R. H.~L., Zhang, Y., Chan, M.~C., and Low, B. K.~H.
\newblock Collaborative machine learning with incentive-aware model rewards.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8927--8936. PMLR, 2020.

\bibitem[Sim et~al.(2022)Sim, Xu, and Low]{sim2022data}
Sim, R. H.~L., Xu, X., and Low, B. K.~H.
\newblock Data valuation in machine learning:“ingredients”, strategies, and
  open challenges.
\newblock In \emph{Proc. IJCAI}, 2022.

\bibitem[Stier et~al.(2018)Stier, Gianini, Granitzer, and
  Ziegler]{stier2018analysing}
Stier, J., Gianini, G., Granitzer, M., and Ziegler, K.
\newblock Analysing neural network topologies: a game theoretic approach.
\newblock \emph{Procedia Computer Science}, 126:\penalty0 234--243, 2018.

\bibitem[Tang et~al.(2021)Tang, Ghorbani, Yamashita, Rehman, Dunnmon, Zou, and
  Rubin]{tang2021data}
Tang, S., Ghorbani, A., Yamashita, R., Rehman, S., Dunnmon, J.~A., Zou, J., and
  Rubin, D.~L.
\newblock Data valuation for medical imaging using shapley value and
  application to a large-scale chest x-ray dataset.
\newblock \emph{Scientific reports}, 11\penalty0 (1):\penalty0 1--9, 2021.

\bibitem[Tian et~al.(2022)Tian, Liu, Li, Cao, Jia, and Ren]{tian2022private}
Tian, Z., Liu, J., Li, J., Cao, X., Jia, R., and Ren, K.
\newblock Private data valuation and fair payment in data marketplaces.
\newblock \emph{arXiv preprint arXiv:2210.08723}, 2022.

\bibitem[Wager et~al.(2014)Wager, Hastie, and Efron]{wager2014confidence}
Wager, S., Hastie, T., and Efron, B.
\newblock Confidence intervals for random forests: The jackknife and the
  infinitesimal jackknife.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1625--1651, 2014.

\bibitem[Wang(2019)]{wang2019interpret}
Wang, G.
\newblock Interpret federated learning with shapley values.
\newblock \emph{arXiv preprint arXiv:1905.04519}, 2019.

\bibitem[Wang \& Jia(2022)Wang and Jia]{wang2022data}
Wang, T. and Jia, R.
\newblock Data banzhaf: A data valuation framework with maximal robustness to
  learning stochasticity.
\newblock \emph{arXiv preprint arXiv:2205.15466}, 2022.

\bibitem[Wang et~al.(2020)Wang, Rausch, Zhang, Jia, and
  Song]{wang2020principled}
Wang, T., Rausch, J., Zhang, C., Jia, R., and Song, D.
\newblock A principled approach to data valuation for federated learning.
\newblock In \emph{Federated Learning}, pp.\  153--167. Springer, 2020.

\bibitem[Wu et~al.(2022)Wu, Jia, Huang, Chang, et~al.]{wu2022robust}
Wu, M., Jia, R., Huang, W., Chang, X., et~al.
\newblock Robust data valuation via variance reduced data shapley.
\newblock \emph{arXiv preprint arXiv:2210.16835}, 2022.

\bibitem[Xu et~al.(2021)Xu, Lyu, Ma, Miao, Foo, and Low]{xu2021gradient}
Xu, X., Lyu, L., Ma, X., Miao, C., Foo, C.~S., and Low, B. K.~H.
\newblock Gradient driven rewards to guarantee fairness in collaborative
  machine learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 16104--16117, 2021.

\bibitem[Yan \& Procaccia(2021)Yan and Procaccia]{yan2021if}
Yan, T. and Procaccia, A.~D.
\newblock If you like shapley then you’ll love the core.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  5751--5759, 2021.

\bibitem[Yoon et~al.(2020)Yoon, Arik, and Pfister]{yoon2020data}
Yoon, J., Arik, S., and Pfister, T.
\newblock Data valuation using reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10842--10851. PMLR, 2020.

\end{thebibliography}
