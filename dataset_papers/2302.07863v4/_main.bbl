\begin{thebibliography}{10}

\bibitem{wmt14}
Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale~{s} Tamchyna.
\newblock Findings of the 2014 workshop on statistical machine translation.
\newblock In {\em Proceedings of the Ninth Workshop on Statistical Machine Translation}, pages 12--58, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics.

\bibitem{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em arXiv preprint arXiv:2005.14165}, 2020.

\bibitem{cettolo-etal-2017-overview}
Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian St{\"u}ker, Katsuhito Sudoh, Koichiro Yoshino, and Christian Federmann.
\newblock Overview of the {IWSLT} 2017 evaluation campaign.
\newblock In {\em Proceedings of the 14th International Conference on Spoken Language Translation}, pages 2--14, Tokyo, Japan, December 14-15 2017. International Workshop on Spoken Language Translation.

\bibitem{chen2023accelerating}
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper.
\newblock Accelerating large language model decoding with speculative sampling.
\newblock {\em arXiv preprint arXiv:2302.01318}, 2023.

\bibitem{chen2020adabert}
Daoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang, Bofang Li, Bolin Ding, Hongbo Deng, Jun Huang, Wei Lin, and Jingren Zhou.
\newblock Adabert: Task-adaptive bert compression with differentiable neural architecture search.
\newblock {\em arXiv preprint arXiv:2001.04246}, 2020.

\bibitem{chen2018comparison}
Patrick~H Chen and Cho-jui Hsieh.
\newblock A comparison of second-order methods for deep convolutional neural networks.
\newblock {\em openreview under ICLR 2018}, 2018.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{de2022fido}
Michiel de~Jong, Yury Zemlyanskiy, Joshua Ainslie, Nicholas FitzGerald, Sumit Sanghai, Fei Sha, and William Cohen.
\newblock Fido: Fusion-in-decoder optimized for stronger performance and faster inference.
\newblock {\em arXiv preprint arXiv:2212.08153}, 2022.

\bibitem{dettmersgpt3}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{du2022glam}
Nan Du, Yanping Huang, Andrew~M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, et~al.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock In {\em International Conference on Machine Learning}, pages 5547--5569. PMLR, 2022.

\bibitem{elbayad2019depth}
Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli.
\newblock Depth-adaptive transformer.
\newblock {\em arXiv preprint arXiv:1910.10073}, 2019.

\bibitem{fan2019reducing}
Angela Fan, Edouard Grave, and Armand Joulin.
\newblock Reducing transformer depth on demand with structured dropout.
\newblock {\em arXiv preprint arXiv:1909.11556}, 2019.

\bibitem{gal2016dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty in deep learning.
\newblock In {\em international conference on machine learning}, pages 1050--1059. PMLR, 2016.

\bibitem{gale2019state}
Trevor Gale, Erich Elsen, and Sara Hooker.
\newblock The state of sparsity in deep neural networks.
\newblock {\em arXiv preprint arXiv:1902.09574}, 2019.

\bibitem{ghazvininejad2019mask}
Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer.
\newblock Mask-predict: Parallel decoding of conditional masked language models.
\newblock {\em arXiv preprint arXiv:1904.09324}, 2019.

\bibitem{gu2017non}
Jiatao Gu, James Bradbury, Caiming Xiong, Victor~OK Li, and Richard Socher.
\newblock Non-autoregressive neural machine translation.
\newblock {\em arXiv preprint arXiv:1711.02281}, 2017.

\bibitem{gu2019levenshtein}
Jiatao Gu, Changhan Wang, and Junbo Zhao.
\newblock Levenshtein transformer.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock In {\em International conference on machine learning}, pages 1321--1330. PMLR, 2017.

\bibitem{guo2020jointly}
Junliang Guo, Linli Xu, and Enhong Chen.
\newblock Jointly masked sequence-to-sequence model for non-autoregressive neural machine translation.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pages 376--385, 2020.

\bibitem{hendrycks2016baseline}
Dan Hendrycks and Kevin Gimpel.
\newblock A baseline for detecting misclassified and out-of-distribution examples in neural networks.
\newblock {\em arXiv preprint arXiv:1610.02136}, 2016.

\bibitem{hermann2015teaching}
Karl~Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom.
\newblock Teaching machines to read and comprehend.
\newblock In {\em Advances in neural information processing systems}, pages 1693--1701, 2015.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em Workshop paper in NIPS}, 2014.

\bibitem{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock {\em arXiv preprint arXiv:2203.15556}, 2022.

\bibitem{hokamp2020dyne}
Chris Hokamp, Demian~Gholipour Ghalandari, Nghia~The Pham, and John Glover.
\newblock Dyne: Dynamic ensemble decoding for multi-document summarization.
\newblock {\em arXiv preprint arXiv:2006.08748}, 2020.

\bibitem{huy2022autoencoding}
Ngo~Quang Huy, Tu~Minh Phuong, and Ngo~Xuan Bach.
\newblock Autoencoding language model based ensemble learning for commonsense validation and explanation.
\newblock {\em arXiv preprint arXiv:2204.03324}, 2022.

\bibitem{iandola2020squeezebert}
Forrest~N Iandola, Albert~E Shaw, Ravi Krishna, and Kurt~W Keutzer.
\newblock Squeezebert: What can computer vision teach nlp about efficient neural networks?
\newblock {\em arXiv preprint arXiv:2006.11316}, 2020.

\bibitem{jiang2023llm}
Dongfu Jiang, Xiang Ren, and Bill~Yuchen Lin.
\newblock Llm-blender: Ensembling large language models with pairwise ranking and generative fusion.
\newblock {\em arXiv preprint arXiv:2306.02561}, 2023.

\bibitem{jiao2019tinybert}
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
\newblock Tinybert: Distilling bert for natural language understanding.
\newblock {\em arXiv preprint arXiv:1909.10351}, 2019.

\bibitem{kasai2020deep}
Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, and Noah~A Smith.
\newblock Deep encoder, shallow decoder: Reevaluating non-autoregressive machine translation.
\newblock {\em arXiv preprint arXiv:2006.10369}, 2020.

\bibitem{kendall2017uncertainties}
Alex Kendall and Yarin Gal.
\newblock What uncertainties do we need in bayesian deep learning for computer vision?
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{kim2021bert}
Sehoon Kim, Amir Gholami, Zhewei Yao, Michael~W Mahoney, and Kurt Keutzer.
\newblock I-bert: Integer-only bert quantization.
\newblock {\em arXiv preprint arXiv:2101.01321}, 2021.

\bibitem{kim2023full}
Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael~W Mahoney, et~al.
\newblock Full stack optimization of transformer inference: a survey.
\newblock {\em arXiv preprint arXiv:2302.14017}, 2023.

\bibitem{kitaev2019reformer}
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{kurtic2022optimal}
Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael Goin, and Dan Alistarh.
\newblock The optimal bert surgeon: Scalable and accurate second-order pruning for large language models.
\newblock {\em arXiv preprint arXiv:2203.07259}, 2022.

\bibitem{kwon2022fast}
Woosuk Kwon, Sehoon Kim, Michael~W Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gholami.
\newblock A fast post-training pruning framework for transformers.
\newblock {\em arXiv preprint arXiv:2204.09656}, 2022.

\bibitem{lan2019albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.
\newblock Albert: A lite bert for self-supervised learning of language representations.
\newblock {\em arXiv preprint arXiv:1909.11942}, 2019.

\bibitem{lee2018deterministic}
Jason Lee, Elman Mansimov, and Kyunghyun Cho.
\newblock Deterministic non-autoregressive neural sequence modeling by iterative refinement.
\newblock {\em arXiv preprint arXiv:1802.06901}, 2018.

\bibitem{leviathan2023fast}
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
\newblock Fast inference from transformers via speculative decoding.
\newblock In {\em International Conference on Machine Learning}, pages 19274--19286. PMLR, 2023.

\bibitem{li2019hint}
Zhuohan Li, Zi~Lin, Di~He, Fei Tian, Tao Qin, Liwei Wang, and Tie-Yan Liu.
\newblock Hint-based training for non-autoregressive machine translation.
\newblock {\em arXiv preprint arXiv:1909.06708}, 2019.

\bibitem{matsubara2022ensemble}
Yoshitomo Matsubara, Luca Soldaini, Eric Lind, and Alessandro Moschitti.
\newblock Ensemble transformer for efficient and accurate ranking tasks: an application to question answering systems.
\newblock {\em arXiv preprint arXiv:2201.05767}, 2022.

\bibitem{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models, 2016.

\bibitem{michel2019sixteen}
Paul Michel, Omer Levy, and Graham Neubig.
\newblock Are sixteen heads really better than one?
\newblock {\em arXiv preprint arXiv:1905.10650}, 2019.

\bibitem{narayan2018don}
Shashi Narayan, Shay~B Cohen, and Mirella Lapata.
\newblock Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization.
\newblock {\em arXiv preprint arXiv:1808.08745}, 2018.

\bibitem{pai2020qiaoning}
Liu Pai.
\newblock Qiaoning at semeval-2020 task 4: Commonsense validation and explanation system based on ensemble of language model.
\newblock In {\em Proceedings of the Fourteenth Workshop on Semantic Evaluation}, pages 415--421, 2020.

\bibitem{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem{pope2022efficiently}
Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean.
\newblock Efficiently scaling transformer inference.
\newblock {\em arXiv preprint arXiv:2211.05102}, 2022.

\bibitem{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter~J Liu, et~al.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em J. Mach. Learn. Res.}, 21(140):1--67, 2020.

\bibitem{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem{sanh2020movement}
Victor Sanh, Thomas Wolf, and Alexander Rush.
\newblock Movement pruning: Adaptive sparsity by fine-tuning.
\newblock {\em Advances in Neural Information Processing Systems}, 33:20378--20389, 2020.

\bibitem{scao2022bloom}
Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois Yvon, Matthias Gall{\'e}, et~al.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock {\em arXiv preprint arXiv:2211.05100}, 2022.

\bibitem{schuster2022confident}
Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh~Q Tran, Yi~Tay, and Donald Metzler.
\newblock Confident adaptive language modeling.
\newblock {\em arXiv preprint arXiv:2207.07061}, 2022.

\bibitem{shao2020minimizing}
Chenze Shao, Jinchao Zhang, Yang Feng, Fandong Meng, and Jie Zhou.
\newblock Minimizing the bag-of-ngrams difference for non-autoregressive neural machine translation.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~34, pages 198--205, 2020.

\bibitem{shazeer2018adafactor}
Noam Shazeer and Mitchell Stern.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock In {\em International Conference on Machine Learning}, pages 4596--4604, 2018.

\bibitem{shen2020q}
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael~W Mahoney, and Kurt Keutzer.
\newblock {Q-BERT}: Hessian based ultra low precision quantization of bert.
\newblock In {\em AAAI}, pages 8815--8821, 2020.

\bibitem{smith2022using}
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et~al.
\newblock Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model.
\newblock {\em arXiv preprint arXiv:2201.11990}, 2022.

\bibitem{so2019evolved}
David So, Quoc Le, and Chen Liang.
\newblock The evolved transformer.
\newblock In {\em International Conference on Machine Learning}, pages 5877--5886. PMLR, 2019.

\bibitem{so2021primer}
David~R So, Wojciech Ma{\'n}ke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc~V Le.
\newblock Primer: Searching for efficient transformers for language modeling.
\newblock {\em arXiv preprint arXiv:2109.08668}, 2021.

\bibitem{stern2019insertion}
Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit.
\newblock Insertion transformer: Flexible sequence generation via insertion operations.
\newblock In {\em International Conference on Machine Learning}, pages 5976--5985. PMLR, 2019.

\bibitem{sun2019fast}
Zhiqing Sun, Zhuohan Li, Haoqing Wang, Di~He, Zi~Lin, and Zhihong Deng.
\newblock Fast structured decoding for sequence models.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{sun2020mobilebert}
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.
\newblock Mobilebert: a compact task-agnostic bert for resource-limited devices.
\newblock {\em arXiv preprint arXiv:2004.02984}, 2020.

\bibitem{tang2019distilling}
Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin.
\newblock Distilling task-specific knowledge from bert into simple neural networks.
\newblock {\em arXiv preprint arXiv:1903.12136}, 2019.

\bibitem{thoppilan2022lamda}
Romal Thoppilan, Daniel De~Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du, et~al.
\newblock Lamda: Language models for dialog applications.
\newblock {\em arXiv preprint arXiv:2201.08239}, 2022.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages 5998--6008, 2017.

\bibitem{voita2019analyzing}
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov.
\newblock Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned.
\newblock {\em arXiv preprint arXiv:1905.09418}, 2019.

\bibitem{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural language understanding.
\newblock {\em arXiv preprint arXiv:1804.07461}, 2018.

\bibitem{wang2020hat}
Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, and Song Han.
\newblock Hat: Hardware-aware transformers for efficient natural language processing.
\newblock {\em arXiv preprint arXiv:2005.14187}, 2020.

\bibitem{wang2020linformer}
Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock {\em arXiv preprint arXiv:2006.04768}, 2020.

\bibitem{wang2020minilm}
Wenhui Wang, Furu Wei, Li~Dong, Hangbo Bao, Nan Yang, and Ming Zhou.
\newblock Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers.
\newblock {\em arXiv preprint arXiv:2002.10957}, 2020.

\bibitem{wang2019non}
Yiren Wang, Fei Tian, Di~He, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu.
\newblock Non-autoregressive machine translation with auxiliary regularization.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~33, pages 5377--5384, 2019.

\bibitem{wei2019imitation}
Bingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang Lin, Jun Xie, and Xu~Sun.
\newblock Imitation learning for non-autoregressive neural machine translation.
\newblock {\em arXiv preprint arXiv:1906.02041}, 2019.

\bibitem{welleck2019non}
Sean Welleck, Kiant{\'e} Brantley, Hal~Daum{\'e} Iii, and Kyunghyun Cho.
\newblock Non-monotonic sequential text generation.
\newblock In {\em International Conference on Machine Learning}, pages 6716--6726. PMLR, 2019.

\bibitem{williams2009roofline}
Samuel Williams, Andrew Waterman, and David Patterson.
\newblock Roofline: an insightful visual performance model for multicore architectures.
\newblock {\em Communications of the ACM}, 52(4):65--76, 2009.

\bibitem{wolf2020transformers}
Thomas Wolf, Julien Chaumond, Lysandre Debut, Victor Sanh, Clement Delangue, Anthony Moi, Pierric Cistac, Morgan Funtowicz, Joe Davison, Sam Shleifer, et~al.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pages 38--45, 2020.

\bibitem{wu2022extreme}
Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, and Yuxiong He.
\newblock Extreme compression for pre-trained transformers made simple and efficient.
\newblock {\em arXiv preprint arXiv:2206.01859}, 2022.

\bibitem{wu2020lite}
Zhanghao Wu, Zhijian Liu, Ji~Lin, Yujun Lin, and Song Han.
\newblock Lite transformer with long-short range attention.
\newblock {\em arXiv preprint arXiv:2004.11886}, 2020.

\bibitem{xu2021bert}
Jin Xu, Xu~Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin, and Tie-Yan Liu.
\newblock Nas-bert: task-agnostic and adaptive-size bert compression with neural architecture search.
\newblock In {\em Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining}, pages 1933--1943, 2021.

\bibitem{xu2020improving}
Yige Xu, Xipeng Qiu, Ligao Zhou, and Xuanjing Huang.
\newblock Improving bert fine-tuning via self-ensemble and self-distillation.
\newblock {\em arXiv preprint arXiv:2002.10345}, 2020.

\bibitem{xue2020mt5}
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel.
\newblock mt5: A massively multilingual pre-trained text-to-text transformer.
\newblock {\em arXiv preprint arXiv:2010.11934}, 2020.

\bibitem{yao2022zeroquant}
Zhewei Yao, Reza~Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He.
\newblock Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.
\newblock {\em arXiv preprint arXiv:2206.01861}, 2022.

\bibitem{yin2021autotinybert}
Yichun Yin, Cheng Chen, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu.
\newblock Autotinybert: Automatic hyper-parameter optimization for efficient pre-trained language models.
\newblock {\em arXiv preprint arXiv:2107.13686}, 2021.

\bibitem{zadeh2020gobo}
Ali~Hadi Zadeh, Isak Edo, Omar~Mohamed Awad, and Andreas Moshovos.
\newblock Gobo: Quantizing attention-based nlp models for low latency and energy efficient inference.
\newblock In {\em 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, pages 811--824. IEEE, 2020.

\bibitem{zafrir2019q8bert}
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
\newblock {Q8BERT}: Quantized 8bit bert.
\newblock {\em arXiv preprint arXiv:1910.06188}, 2019.

\bibitem{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock {\em arXiv preprint arXiv:2205.01068}, 2022.

\bibitem{zhou2019understanding}
Chunting Zhou, Graham Neubig, and Jiatao Gu.
\newblock Understanding knowledge distillation in non-autoregressive machine translation.
\newblock {\em arXiv preprint arXiv:1911.02727}, 2019.

\end{thebibliography}
