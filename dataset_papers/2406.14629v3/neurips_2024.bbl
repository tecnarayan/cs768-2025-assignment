\begin{thebibliography}{100}

\bibitem{phi3}
Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah,
  Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl,
  et~al.
\newblock Phi-3 technical report: A highly capable language model locally on
  your phone.
\newblock {\em arXiv preprint arXiv:2404.14219}, 2024.

\bibitem{mbpp}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski,
  David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al.
\newblock Program synthesis with large language models.
\newblock {\em arXiv preprint arXiv:2108.07732}, 2021.

\bibitem{bengio2009curriculum}
Yoshua Bengio, J{\'e}r{\^o}me Louradour, Ronan Collobert, and Jason Weston.
\newblock Curriculum learning.
\newblock In {\em Proceedings of the 26th annual international conference on
  machine learning}, pages 41--48, 2009.

\bibitem{besta2024graph}
Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal
  Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert
  Niewiadomski, Piotr Nyczyk, et~al.
\newblock Graph of thoughts: Solving elaborate problems with large language
  models.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~38, pages 17682--17690, 2024.

\bibitem{biswas2005learning}
Gautam Biswas, Krittaya Leelawong, Daniel Schwartz, Nancy Vye, and The
  Teachable Agents~Group at~Vanderbilt.
\newblock Learning by teaching: A new agent paradigm for educational software.
\newblock {\em Applied Artificial Intelligence}, 19(3-4):363--392, 2005.

\bibitem{burns2023weaktostrong}
Collin Burns, Pavel Izmailov, Jan~Hendrik Kirchner, Bowen Baker, Leo Gao,
  Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan
  Leike, et~al.
\newblock Weak-to-strong generalization: Eliciting strong capabilities with
  weak supervision.
\newblock {\em arXiv preprint arXiv:2312.09390}, 2023.

\bibitem{cao2022active}
Xiaofeng Cao, Yaming Guo, Tieru Wu, and Ivor~W Tsang.
\newblock Black-box generalization of machine teaching.
\newblock {\em arXiv preprint arXiv:2206.15205}, 2022.

\bibitem{chen2023codet}
Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and
  Weizhu Chen.
\newblock Codet: Code generation with generated tests.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{humaneval}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira
  Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
  Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock {\em arXiv preprint arXiv:2107.03374}, 2021.

\bibitem{chen2023universal}
Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin,
  Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou.
\newblock Universal self-consistency for large language model generation.
\newblock {\em arXiv preprint arXiv:2311.17311}, 2023.

\bibitem{chen2023selfdebug}
Xinyun Chen, Maxwell Lin, Nathanael Sch{\"a}rli, and Denny Zhou.
\newblock Teaching large language models to self-debug.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{cheng2024exploring}
Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao
  Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, et~al.
\newblock Exploring large language model based intelligent agents: Definitions,
  methods, and prospects.
\newblock {\em arXiv preprint arXiv:2401.03428}, 2024.

\bibitem{chi2001learning}
Michelene~TH Chi, Stephanie~A Siler, Heisawn Jeong, Takashi Yamauchi, and
  Robert~G Hausmann.
\newblock Learning from human tutoring.
\newblock {\em Cognitive science}, 25(4):471--533, 2001.

\bibitem{cobbe2021gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  et~al.
\newblock Training verifiers to solve math word problems.
\newblock {\em arXiv preprint arXiv:2110.14168}, 2021.

\bibitem{cubuk2019autoaugment}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock Autoaugment: Learning augmentation strategies from data.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 113--123, 2019.

\bibitem{didolkar2024metacognitive}
Aniket~Rajiv Didolkar, Anirudh Goyal, Nan~Rosemary Ke, Siyuan Guo, Michal
  Valko, Timothy~P Lillicrap, Danilo~Jimenez Rezende, Yoshua Bengio,
  Michael~Curtis Mozer, and Sanjeev Arora.
\newblock Metacognitive capabilities of {LLM}s: An exploration in mathematical
  problem solving.
\newblock In {\em AI for Math Workshop @ ICML 2024}, 2024.

\bibitem{dong2023self}
Yihong Dong, Xue Jiang, Zhi Jin, and Ge~Li.
\newblock Self-collaboration code generation via chatgpt.
\newblock {\em ACM Trans. Softw. Eng. Methodol.}, jun 2024.

\bibitem{duran2017learning}
David Duran.
\newblock Learning-by-teaching. evidence and implications as a pedagogical
  mechanism.
\newblock {\em Innovations in education and teaching international},
  54(5):476--484, 2017.

\bibitem{elsken2019nassurvey}
Thomas Elsken, Jan~Hendrik Metzen, and Frank Hutter.
\newblock Neural architecture search: A survey.
\newblock {\em Journal of Machine Learning Research}, 20(55):1--21, 2019.

\bibitem{feurer2019hyperparameter}
Matthias Feurer and Frank Hutter.
\newblock Hyperparameter optimization.
\newblock {\em Automated machine learning: Methods, systems, challenges}, pages
  3--33, 2019.

\bibitem{finn2017maml}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em International conference on machine learning}, pages
  1126--1135. PMLR, 2017.

\bibitem{gartner1971children}
Alan Gartner et~al.
\newblock Children teach children: Learning by teaching.
\newblock 1971.

\bibitem{gou2021kd}
Jianping Gou, Baosheng Yu, Stephen~J Maybank, and Dacheng Tao.
\newblock Knowledge distillation: A survey.
\newblock {\em International Journal of Computer Vision}, 129(6):1789--1819,
  2021.

\bibitem{minillm}
Yuxian Gu, Li~Dong, Furu Wei, and Minlie Huang.
\newblock Minillm: Knowledge distillation of large language models.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2023.

\bibitem{hattie2007power}
John Hattie and Helen Timperley.
\newblock The power of feedback.
\newblock {\em Review of educational research}, 77(1):81--112, 2007.

\bibitem{hendel2023context}
Roee Hendel, Mor Geva, and Amir Globerson.
\newblock In-context learning creates task vectors.
\newblock {\em arXiv preprint arXiv:2310.15916}, 2023.

\bibitem{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric
  Tang, Dawn Song, and Jacob Steinhardt.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock In {\em Proceedings of the Neural Information Processing Systems
  Track on Datasets and Benchmarks}, volume~1, 2021.

\bibitem{ho2022large}
Namgyu Ho, Laura Schmid, and Se-Young Yun.
\newblock Large language models are reasoning teachers.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 14852--14882,
  Toronto, Canada, July 2023. Association for Computational Linguistics.

\bibitem{hong2023metagpt}
Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin
  Wang, Ceyao Zhang, Zili Wang, Steven Ka~Shing Yau, Zijuan Lin, Liyang Zhou,
  Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and J{\"u}rgen Schmidhuber.
\newblock Meta{GPT}: Meta programming for a multi-agent collaborative
  framework.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{hospedales2021meta}
Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey.
\newblock Meta-learning in neural networks: A survey.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  44(9):5149--5169, 2021.

\bibitem{huang2022selfimprove}
Jiaxin Huang, Shixiang Gu, Le~Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and
  Jiawei Han.
\newblock Large language models can self-improve.
\newblock In {\em Proceedings of the 2023 Conference on Empirical Methods in
  Natural Language Processing}, pages 1051--1068, Singapore, December 2023.
  Association for Computational Linguistics.

\bibitem{jin2024lft}
Can Jin, Tong Che, Hongwu Peng, Yiyuan Li, and Marco Pavone.
\newblock Learning from teaching regularization: Generalizable correlations
  should be easy to imitate.
\newblock {\em arXiv preprint arXiv:2402.02769}, 2024.

\bibitem{jin2023teach}
Hyoungwook Jin, Seonghee Lee, Hyungyu Shin, and Juho Kim.
\newblock Teach ai how to code: Using large language models as teachable agents
  for programming education.
\newblock In {\em Proceedings of the CHI Conference on Human Factors in
  Computing Systems}, CHI '24, New York, NY, USA, 2024. Association for
  Computing Machinery.

\bibitem{jin2022logical}
Zhijing Jin, Abhinav Lalwani, Tejas Vaidhya, Xiaoyu Shen, Yiwen Ding, Zhiheng
  Lyu, Mrinmaya Sachan, Rada Mihalcea, and Bernhard Schoelkopf.
\newblock Logical fallacy detection.
\newblock In {\em Findings of the Association for Computational Linguistics:
  EMNLP 2022}, pages 7180--7198, Abu Dhabi, United Arab Emirates, December
  2022. Association for Computational Linguistics.

\bibitem{kadavath2022lmmostlyknow}
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan
  Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli
  Tran-Johnson, et~al.
\newblock Language models (mostly) know what they know.
\newblock {\em arXiv preprint arXiv:2207.05221}, 2022.

\bibitem{kojima2022large}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
  Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock {\em Advances in neural information processing systems},
  35:22199--22213, 2022.

\bibitem{leckey2001quantifying}
Janet Leckey and Neville Neill.
\newblock Quantifying quality: the importance of student feedback.
\newblock {\em Quality in Higher Education}, 7(1):19--32, 2001.

\bibitem{lewkowycz2022solving}
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk
  Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo
  Gutman-Solo, et~al.
\newblock Solving quantitative reasoning problems with language models.
\newblock {\em Advances in Neural Information Processing Systems},
  35:3843--3857, 2022.

\bibitem{camel}
Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.
\newblock Camel: Communicative agents for" mind" exploration of large language
  model society.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{li2024language}
Jiaoda Li, Yifan Hou, Mrinmaya Sachan, and Ryan Cotterell.
\newblock What do language models learn in context? the structured task
  hypothesis.
\newblock {\em arXiv preprint arXiv:2406.04216}, 2024.

\bibitem{li2022explanations}
Shiyang Li, Jianshu Chen, yelong shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong
  Wang, Jing Qian, Baolin Peng, Yi~Mao, Wenhu Chen, and Xifeng Yan.
\newblock Explanations from large language models make small reasoners better.
\newblock In {\em 2nd Workshop on Sustainable AI}, 2024.

\bibitem{li2023making}
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and
  Weizhu Chen.
\newblock Making language models better reasoners with step-aware verifier.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 5315--5333, 2023.

\bibitem{li2022alphacode}
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
  R{\'e}mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal~Lago,
  et~al.
\newblock Competition-level code generation with alphacode.
\newblock {\em Science}, 378(6624):1092--1097, 2022.

\bibitem{ted}
Chen Liang, Simiao Zuo, Qingru Zhang, Pengcheng He, Weizhu Chen, and Tuo Zhao.
\newblock Less is more: Task-aware layer-wise distillation for language model
  compression.
\newblock In {\em International Conference on Machine Learning}, pages
  20852--20867. PMLR, 2023.

\bibitem{lightman2023let}
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy
  Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
\newblock Let's verify step by step.
\newblock {\em arXiv preprint arXiv:2305.20050}, 2023.

\bibitem{liu2023chainofhindsight}
Hao Liu, Carmelo Sferrazza, and Pieter Abbeel.
\newblock Chain of hindsight aligns language models with feedback.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{madaan2024selfrefine}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
  Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{mcalpine1999building}
Lynn McAlpine, Cynthia Weston, Catherine Beauchamp, C~Wiseman, and Jacinthe
  Beauchamp.
\newblock Building a metacognitive model of reflection.
\newblock {\em Higher education}, 37:105--131, 1999.

\bibitem{meng2024simpo}
Yu~Meng, Mengzhou Xia, and Danqi Chen.
\newblock Simpo: Simple preference optimization with a reference-free reward.
\newblock {\em arXiv preprint arXiv:2405.14734}, 2024.

\bibitem{min2022rethinking}
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
  Hajishirzi, and Luke Zettlemoyer.
\newblock Rethinking the role of demonstrations: What makes in-context learning
  work?
\newblock {\em arXiv preprint arXiv:2202.12837}, 2022.

\bibitem{moskowitz1985evaluationjigsaw}
Joel~M Moskowitz, Janet~H Malvin, Gary~A Schaeffer, and Eric Schaps.
\newblock Evaluation of jigsaw, a cooperative learning technique.
\newblock {\em Contemporary educational psychology}, 10(2):104--112, 1985.

\bibitem{ning2024skeleton}
Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, and Yu~Wang.
\newblock Skeleton-of-thought: Prompting llms for efficient parallel
  generation.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{opedal2024language}
Andreas Opedal, Alessandro Stolfo, Haruki Shirakami, Ying Jiao, Ryan Cotterell,
  Bernhard Sch{\"o}lkopf, Abulhair Saparov, and Mrinmaya Sachan.
\newblock Do language models exhibit the same cognitive biases in problem
  solving as human learners?
\newblock In {\em International Conference on Machine Learning}, pages
  38762--38778. PMLR, 2024.

\bibitem{o1}
OpenAI.
\newblock Learning to reason with llms, Sep 2024.

\bibitem{pang2024iterative}
Richard~Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He~He, Sainbayar Sukhbaatar,
  and Jason Weston.
\newblock Iterative reasoning preference optimization.
\newblock {\em arXiv preprint arXiv:2404.19733}, 2024.

\bibitem{press2022selfask}
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis.
\newblock Measuring and narrowing the compositionality gap in language models.
\newblock In {\em Findings of the Association for Computational Linguistics:
  EMNLP 2023}, pages 5687--5711, Singapore, December 2023. Association for
  Computational Linguistics.

\bibitem{pryzant2023automatic}
Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng.
\newblock Automatic prompt optimization with {``}gradient descent{''} and beam
  search.
\newblock In {\em Proceedings of the 2023 Conference on Empirical Methods in
  Natural Language Processing}, pages 7957--7968, Singapore, December 2023.
  Association for Computational Linguistics.

\bibitem{chatdev}
Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
  Liu, and Maosong Sun.
\newblock Communicative agents for software development.
\newblock {\em arXiv preprint arXiv:2307.07924}, 2023.

\bibitem{dpo}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano
  Ermon, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a
  reward model.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{reimers2019sentence}
Nils Reimers and Iryna Gurevych.
\newblock Sentence-{BERT}: Sentence embeddings using {S}iamese {BERT}-networks.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 3982--3992, Hong Kong,
  China, November 2019. Association for Computational Linguistics.

\bibitem{richardson2005instruments}
John~TE Richardson.
\newblock Instruments for obtaining student feedback: A review of the
  literature.
\newblock {\em Assessment \& evaluation in higher education}, 30(4):387--415,
  2005.

\bibitem{roscoe2004influence}
Rod~D Roscoe and Michelene~TH Chi.
\newblock The influence of the tutee in learning by peer tutoring.
\newblock In {\em Proceedings of the Annual Meeting of the Cognitive Science
  Society}, volume~26, 2004.

\bibitem{roscoe2007understanding}
Rod~D Roscoe and Michelene~TH Chi.
\newblock Understanding tutor learning: Knowledge-building and
  knowledge-telling in peer tutors’ explanations and questions.
\newblock {\em Review of educational research}, 77(4):534--574, 2007.

\bibitem{roscoe2008tutor}
Rod~D Roscoe and Michelene~TH Chi.
\newblock Tutor learning: The role of explaining and responding to questions.
\newblock {\em Instructional science}, 36:321--350, 2008.

\bibitem{shahriar2021can}
Tasmia Shahriar and Noboru Matsuda.
\newblock “can you clarify what you said?”: Studying the impact of tutee
  agents’ follow-up questions on tutors’ learning.
\newblock In {\em Artificial Intelligence in Education: 22nd International
  Conference, AIED 2021, Utrecht, The Netherlands, June 14--18, 2021,
  Proceedings, Part I 22}, pages 395--407. Springer, 2021.

\bibitem{shi2022mbr-exec}
Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida~I.
  Wang.
\newblock Natural language to code translation with execution.
\newblock In {\em Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 3533--3546, Abu Dhabi, United Arab
  Emirates, December 2022. Association for Computational Linguistics.

\bibitem{shinn2024reflexion}
Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu
  Yao.
\newblock Reflexion: Language agents with verbal reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{shumailov2024aicollapse}
Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson,
  and Yarin Gal.
\newblock Ai models collapse when trained on recursively generated data.
\newblock {\em Nature}, 631(8022):755--759, 2024.

\bibitem{snell2024scaling}
Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar.
\newblock Scaling llm test-time compute optimally can be more effective than
  scaling model parameters.
\newblock {\em arXiv preprint arXiv:2408.03314}, 2024.

\bibitem{snoek2012practical}
Jasper Snoek, Hugo Larochelle, and Ryan~P Adams.
\newblock Practical bayesian optimization of machine learning algorithms.
\newblock {\em Advances in neural information processing systems}, 25, 2012.

\bibitem{sordoni2023deep}
Alessandro Sordoni, Eric Yuan, Marc-Alexandre C\^{o}t\'{e}, Matheus Pereira,
  Adam Trischler, Ziang Xiao, Arian Hosseini, Friederike Niedtner, and Nicolas
  Le~Roux.
\newblock Joint prompt optimization of stacked llms using variational
  inference.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~36, pages 58128--58151. Curran Associates, Inc., 2023.

\bibitem{srivastava2024functional}
Saurabh Srivastava, Anto PV, Shashank Menon, Ajay Sukumar, Alan Philipose,
  Stevin Prince, Sooraj Thomas, et~al.
\newblock Functional benchmarks for robust evaluation of reasoning performance,
  and the reasoning gap.
\newblock {\em arXiv preprint arXiv:2402.19450}, 2024.

\bibitem{sun2024easy}
Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck,
  and Chuang Gan.
\newblock Easy-to-hard generalization: Scalable alignment beyond human
  supervision.
\newblock {\em arXiv preprint arXiv:2403.09472}, 2024.

\bibitem{tian2023justaskcalib}
Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov,
  Huaxiu Yao, Chelsea Finn, and Christopher Manning.
\newblock Just ask for calibration: Strategies for eliciting calibrated
  confidence scores from language models fine-tuned with human feedback.
\newblock In {\em Proceedings of the 2023 Conference on Empirical Methods in
  Natural Language Processing}, pages 5433--5442, Singapore, December 2023.
  Association for Computational Linguistics.

\bibitem{babyllama}
Inar Timiryasov and Jean-Loup Tastet.
\newblock Baby llama: knowledge distillation from an ensemble of teachers
  trained on a small dataset with no performance penalty.
\newblock In {\em Proceedings of the BabyLM Challenge at the 27th Conference on
  Computational Natural Language Learning}, pages 279--289, Singapore, December
  2023. Association for Computational Linguistics.

\bibitem{tong2024dart}
Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He.
\newblock Dart-math: Difficulty-aware rejection tuning for mathematical
  problem-solving.
\newblock {\em arXiv preprint arXiv:2407.13690}, 2024.

\bibitem{wambsganss2022bias}
Thiemo Wambsganss, Vinitra Swamy, Roman Rietsche, and Tanja K{\"a}ser.
\newblock Bias at a second glance: A deep dive into bias for german educational
  peer-review data modeling.
\newblock {\em arXiv preprint arXiv:2209.10335}, 2022.

\bibitem{wan2024teach}
Xingchen Wan, Ruoxi Sun, Hootan Nakhost, and Sercan~O Arik.
\newblock Teach better or show smarter? on instructions and exemplars in
  automatic prompt optimization.
\newblock {\em arXiv preprint arXiv:2406.15708}, 2024.

\bibitem{wang2023mathshepherd}
Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen,
  Yu~Wu, and Zhifang Sui.
\newblock Math-shepherd: Verify and reinforce llms step-by-step without human
  annotations.
\newblock In {\em Proceedings of the 62nd Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 9426--9439, 2024.

\bibitem{wang2017liar}
William~Yang Wang.
\newblock {``}liar, liar pants on fire{''}: A new benchmark dataset for fake
  news detection.
\newblock In {\em Proceedings of the 55th Annual Meeting of the Association for
  Computational Linguistics (Volume 2: Short Papers)}, pages 422--426,
  Vancouver, Canada, July 2017. Association for Computational Linguistics.

\bibitem{wang2022selfconsistency}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc~V Le, Ed~H. Chi, Sharan Narang,
  Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language
  models.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{wang2022selfinstruct}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language models with self-generated
  instructions.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, {\em
  Proceedings of the 61st Annual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers)}, pages 13484--13508, Toronto, Canada,
  July 2023. Association for Computational Linguistics.

\bibitem{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V
  Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock {\em Advances in Neural Information Processing Systems},
  35:24824--24837, 2022.

\bibitem{wisniewski2020powerre}
Benedikt Wisniewski, Klaus Zierer, and John Hattie.
\newblock The power of feedback revisited: A meta-analysis of educational
  feedback research.
\newblock {\em Frontiers in psychology}, 10:487662, 2020.

\bibitem{xie2021explanation}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock {\em arXiv preprint arXiv:2111.02080}, 2021.

\bibitem{xie2024selfevalbeam}
Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James~Xu Zhao, Min-Yen Kan, Junxian He,
  and Michael Xie.
\newblock Self-evaluation guided beam search for reasoning.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{xu2024llmkdsurvey}
Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can
  Xu, Dacheng Tao, and Tianyi Zhou.
\newblock A survey on knowledge distillation of large language models.
\newblock {\em arXiv preprint arXiv:2402.13116}, 2024.

\bibitem{yang2024leandojo}
Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu,
  Saad Godil, Ryan~J Prenger, and Animashree Anandkumar.
\newblock Leandojo: Theorem proving with retrieval-augmented language models.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{yao2023tree}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas~L. Griffiths, Yuan
  Cao, and Karthik~R Narasimhan.
\newblock Tree of thoughts: Deliberate problem solving with large language
  models.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.

\bibitem{yuan2024selfreward}
Weizhe Yuan, Richard~Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing
  Xu, and Jason Weston.
\newblock Self-rewarding language models.
\newblock {\em arXiv preprint arXiv:2401.10020}, 2024.

\bibitem{yuan2023rft}
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang
  Zhou.
\newblock Scaling relationship on learning mathematical reasoning with large
  language models.
\newblock {\em arXiv preprint arXiv:2308.01825}, 2023.

\bibitem{zelikman2022star}
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman.
\newblock Star: Bootstrapping reasoning with reasoning.
\newblock {\em Advances in Neural Information Processing Systems},
  35:15476--15488, 2022.

\bibitem{zhang2023building}
Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua~B.
  Tenenbaum, Tianmin Shu, and Chuang Gan.
\newblock Building cooperative embodied agents modularly with large language
  models.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{zhang2018deepml}
Ying Zhang, Tao Xiang, Timothy~M Hospedales, and Huchuan Lu.
\newblock Deep mutual learning.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4320--4328, 2018.

\bibitem{zheng2023take}
Huaixiu~Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed~H. Chi,
  Quoc~V Le, and Denny Zhou.
\newblock Take a step back: Evoking reasoning via abstraction in large language
  models.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{zhou2022least}
Denny Zhou, Nathanael Sch{\"a}rli, Le~Hou, Jason Wei, Nathan Scales, Xuezhi
  Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc~V Le, and Ed~H.
  Chi.
\newblock Least-to-most prompting enables complex reasoning in large language
  models.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{zhou2022humanlevel}
Yongchao Zhou, Andrei~Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis,
  Harris Chan, and Jimmy Ba.
\newblock Large language models are human-level prompt engineers.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{efficientsurvey}
Zixuan Zhou, Xuefei Ning, Ke~Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming
  Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et~al.
\newblock A survey on efficient inference for large language models.
\newblock {\em arXiv preprint arXiv:2404.14294}, 2024.

\bibitem{zhu2015machine}
Xiaojin Zhu.
\newblock Machine teaching: An inverse problem to machine learning and an
  approach toward optimal education.
\newblock In {\em Proceedings of the AAAI conference on artificial
  intelligence}, volume~29, 2015.

\bibitem{zhu2018overview}
Xiaojin Zhu, Adish Singla, Sandra Zilles, and Anna~N Rafferty.
\newblock An overview of machine teaching.
\newblock {\em arXiv preprint arXiv:1801.05927}, 2018.

\bibitem{ziegler2019rlhf}
Daniel~M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B Brown, Alec Radford, Dario
  Amodei, Paul Christiano, and Geoffrey Irving.
\newblock Fine-tuning language models from human preferences.
\newblock {\em arXiv preprint arXiv:1909.08593}, 2019.

\end{thebibliography}
