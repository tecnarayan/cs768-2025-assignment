\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{MDFFF16}

\bibitem[ABH16]{agarwal2016second}
N. Agarwal, B. Bullins, and E. Hazan.
\newblock Second order stochastic optimization in linear time.
\newblock {\em arXiv preprint arXiv:1602.03943}, 2016.

\bibitem[ACW16]{avron2016faster}
H. Avron, K. Clarkson, and D. Woodruff.
\newblock Faster kernel ridge regression using sketching and preconditioning.
\newblock {\em arXiv preprint arXiv:1611.03220}, 2016.

\bibitem[Aro50]{aronszajn1950theory}
N. Aronszajn.
\newblock Theory of reproducing kernels.
\newblock {\em Transactions of the American mathematical society},
  68(3):337--404, 1950.

\bibitem[B{\etalchar{+}}05]{braun2005spectral}
M.~L. Braun et~al.
\newblock {\em Spectral properties of the kernel matrix and their relation to
  kernel methods in machine learning.}
\newblock PhD thesis, University of Bonn, 2005.

\bibitem[B{\etalchar{+}}15]{bubeck2015convex}
S. Bubeck et~al.
\newblock Convex optimization: Algorithms and complexity.
\newblock {\em Foundations and Trends in Machine Learning}, 8(3-4):231--357,
  2015.

\bibitem[BBG09]{bordes2009sgd}
A. Bordes, L. Bottou, and P. Gallinari.
\newblock {SGD-QN}: Careful quasi-newton stochastic gradient descent.
\newblock {\em JMLR}, 10:1737--1754, 2009.

\bibitem[BE02]{bousquet2002stability}
O. Bousquet and A. Elisseeff.
\newblock Stability and generalization.
\newblock {\em JMLR}, 2:499--526, 2002.

\bibitem[BHNS16]{byrd2016stochastic}
R.~H. Byrd, S. Hansen, J. Nocedal, and Y. Singer.
\newblock A stochastic quasi-newton method for large-scale optimization.
\newblock {\em SIAM Journal on Optimization}, 26(2):1008--1031, 2016.

\bibitem[Bis07]{bishop2007pattern}
C. Bishop.
\newblock Pattern recognition and machine learning.
\newblock {\em Springer, New York}, 2007.

\bibitem[BSW14]{baldi2014searching}
P. Baldi, P. Sadowski, and D. Whiteson.
\newblock Searching for exotic particles in high-energy physics with deep
  learning.
\newblock {\em Nature communications}, 5, 2014.

\bibitem[BV04]{boyd2004convex}
S. Boyd and L. Vandenberghe.
\newblock {\em Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[CARR16]{camoriano2016nytro}
R. Camoriano, T. Angles, A. Rudi, and L. Rosasco.
\newblock {NYTRO}: When subsampling meets early stopping.
\newblock In {\em AISTATS}, pages 1403--1411, 2016.

\bibitem[CAS16]{chen2016hierarchically}
J. Chen, H. Avron, and V. Sindhwani.
\newblock Hierarchically compositional kernels for scalable nonparametric
  learning.
\newblock {\em arXiv preprint arXiv:1608.00860}, 2016.

\bibitem[CK11]{cheng2011arccosine}
C.-C. Cheng and B. Kingsbury.
\newblock Arccosine kernels: Acoustic modeling with infinite neural networks.
\newblock In {\em ICASSP}, pages 5200--5203. IEEE, 2011.

\bibitem[COCF16]{cutajar2016preconditioning}
K. Cutajar, M. Osborne, J. Cunningham, and M. Filippone.
\newblock Preconditioning kernel matrices.
\newblock In {\em ICML}, pages 2529--2538, 2016.

\bibitem[DHS11]{duchi2011adaptive}
J. Duchi, E. Hazan, and Y. Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em JMLR}, 12:2121--2159, 2011.

\bibitem[DJS96]{dennis1996numerical}
J.~E. Dennis~Jr and R.~B. Schnabel.
\newblock {\em Numerical methods for unconstrained optimization and nonlinear
  equations}.
\newblock SIAM, 1996.

\bibitem[DXH{\etalchar{+}}14]{dai2014scalable}
B. Dai, B. Xie, N. He, Y. Liang, A. Raj, M. Balcan, and L. Song.
\newblock Scalable kernel methods via doubly stochastic gradients.
\newblock In {\em NIPS}, pages 3041--3049, 2014.

\bibitem[EM15]{erdogdu2015convergence}
M.~A. Erdogdu and A. Montanari.
\newblock Convergence rates of sub-sampled newton methods.
\newblock In {\em NIPS}, pages 3052--3060, 2015.

\bibitem[FM12]{fasshauer2012stable}
G. Fasshauer and M. McCourt.
\newblock Stable evaluation of gaussian radial basis function interpolants.
\newblock {\em SIAM Journal on Scientific Computing}, 34(2):A737--A762, 2012.

\bibitem[GLF{\etalchar{+}}93]{garofolo1993darpa}
J.~S. Garofolo, L.~F. Lamel, W.~M. Fisher, J.~G. Fiscus, and D.~S. Pallett.
\newblock Darpa timit acoustic-phonetic continous speech corpus cd-rom.
\newblock {\em NIST speech disc}, 1-1.1, 1993.

\bibitem[GOSS16]{gonen2016solving}
A. Gonen, F. Orabona, and S. Shalev-Shwartz.
\newblock Solving ridge regression using sketched preconditioned svrg.
\newblock In {\em ICML}, pages 1397--1405, 2016.

\bibitem[HAS{\etalchar{+}}14]{huang2014kernel}
P.-S. Huang, H. Avron, T.~N. Sainath, V. Sindhwani, and B. Ramabhadran.
\newblock Kernel methods match deep neural networks on timit.
\newblock In {\em ICASSP}, pages 205--209. IEEE, 2014.

\bibitem[HCL{\etalchar{+}}08]{hsieh2008dual}
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S.~S. Keerthi, and S. Sundararajan.
\newblock A dual coordinate descent method for large-scale linear svm.
\newblock In {\em ICML}, pages 408--415, 2008.

\bibitem[HMT11]{halko2011finding}
N. Halko, P.-G. Martinsson, and J.~A. Tropp.
\newblock Finding structure with randomness: Probabilistic algorithms for
  constructing approximate matrix decompositions.
\newblock {\em SIAM review}, 53(2):217--288, 2011.

\bibitem[HYWW13]{healy2013algorithm}
E.~W. Healy, S.~E. Yoho, Y. Wang, and D. Wang.
\newblock An algorithm to improve speech recognition in noise for
  hearing-impaired listeners.
\newblock {\em The Journal of the Acoustical Society of America},
  134(4):3029--3038, 2013.

\bibitem[JZ13]{johnson2013accelerating}
R. Johnson and T. Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em NIPS}, pages 315--323, 2013.

\bibitem[KB14]{kingma2014adam}
D. Kingma and J. Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[KH09]{krizhevsky2009learning}
A. Krizhevsky and G. Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Master's thesis, University of Toronto, 2009.

\bibitem[KSW04]{kivinen2004online}
J. Kivinen, A.~J. Smola, and R.~C. Williamson.
\newblock Online learning with kernels.
\newblock {\em Signal Processing, IEEE Transactions on}, 52(8):2165--2176,
  2004.

\bibitem[K{\"u}h87]{kuhn1987eigenvalues}
T. K{\"u}hn.
\newblock Eigenvalues of integral operators with smooth positive definite
  kernels.
\newblock {\em Archiv der Mathematik}, 49(6):525--534, 1987.

\bibitem[LBBH98]{lecun1998gradient}
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock In {\em Proceedings of the IEEE}, volume~86, pages 2278--2324, 1998.

\bibitem[LML{\etalchar{+}}14]{lu2014scale}
Z. Lu, A. May, K. Liu, A.~B. Garakani, D. Guo, A. Bellet, L. Fan, M. Collins,
  B. Kingsbury, M. Picheny, and F. Sha.
\newblock How to scale up kernel methods to be as good as deep neural nets.
\newblock {\em arXiv preprint arXiv:1411.4000}, 2014.

\bibitem[LN89]{liu1989limited}
D.~C. Liu and J. Nocedal.
\newblock On the limited memory bfgs method for large scale optimization.
\newblock {\em Mathematical programming}, 45(1-3):503--528, 1989.

\bibitem[LSS13]{le2013fastfood}
Q. Le, T. Sarl{\'o}s, and A. Smola.
\newblock Fastfood-approximating kernel expansions in loglinear time.
\newblock In {\em ICML}, 2013.

\bibitem[MDFFF16]{moosavi2016universal}
S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard.
\newblock Universal adversarial perturbations.
\newblock {\em arXiv preprint arXiv:1610.08401}, 2016.

\bibitem[MGL{\etalchar{+}}17]{may2017kernel}
A. May, A.~B. Garakani, Z. Lu, D. Guo, K. Liu, A. Bellet, L. Fan, M. Collins,
  D. Hsu, B. Kingsbury, et~al.
\newblock Kernel approximation methods for speech recognition.
\newblock {\em arXiv preprint arXiv:1701.03577}, 2017.

\bibitem[Min17]{minsker2017some}
S. Minsker.
\newblock On some extensions of bernsteinâ€™s inequality for self-adjoint
  operators.
\newblock {\em Statistics \& Probability Letters}, 2017.

\bibitem[MNJ16]{moritz2016linearly}
P. Moritz, R. Nishihara, and M. Jordan.
\newblock A linearly-convergent stochastic l-bfgs algorithm.
\newblock In {\em AISTATS}, pages 249--258, 2016.

\bibitem[Mur98]{murata1998statistical}
N. Murata.
\newblock A statistical study of on-line learning.
\newblock {\em Online Learning and Neural Networks. Cambridge University Press,
  Cambridge, UK}, pages 63--92, 1998.

\bibitem[NWC{\etalchar{+}}11]{netzer2011reading}
Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In {\em NIPS workshop}, volume 2011, page~4, 2011.

\bibitem[PGB{\etalchar{+}}11]{povey2011kaldi}
D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M.
  Hannemann, P. Motlicek, Y. Qian, P. Schwarz, et~al.
\newblock The kaldi speech recognition toolkit.
\newblock In {\em ASRU}, 2011.

\bibitem[QB16]{queback}
Q. Que and M. Belkin.
\newblock Back to the future: Radial basis function networks revisited.
\newblock In {\em AISTATS}, pages 1375--1383, 2016.

\bibitem[RBV10]{rosasco2010learning}
L. Rosasco, M. Belkin, and E.~D. Vito.
\newblock On learning with integral operators.
\newblock {\em JMLR}, 11(Feb):905--934, 2010.

\bibitem[Ric11]{richardson1911approximate}
L.~F. Richardson.
\newblock The approximate arithmetical solution by finite differences of
  physical problems involving differential equations, with an application to
  the stresses in a masonry dam.
\newblock {\em Philosophical Transactions of the Royal Society of London.
  Series A}, 210:307--357, 1911.

\bibitem[Ros97]{rosenberg1997laplacian}
S. Rosenberg.
\newblock {\em The Laplacian on a Riemannian manifold: an introduction to
  analysis on manifolds}.
\newblock Cambridge University Press, 1997.

\bibitem[RR07]{rahimi2007random}
A. Rahimi and B. Recht.
\newblock Random features for large-scale kernel machines.
\newblock In {\em NIPS}, pages 1177--1184, 2007.

\bibitem[RSB12]{roux2012stochastic}
N.~L. Roux, M. Schmidt, and F.~R. Bach.
\newblock A stochastic gradient method with an exponential convergence \_rate
  for finite training sets.
\newblock In {\em NIPS}, pages 2663--2671, 2012.

\bibitem[RWY14]{raskutti2014early}
G. Raskutti, M. Wainwright, and B. Yu.
\newblock Early stopping and non-parametric regression: an optimal
  data-dependent stopping rule.
\newblock {\em JMLR}, 15(1):335--366, 2014.

\bibitem[SC08]{steinwart2008support}
I. Steinwart and A. Christmann.
\newblock {\em Support vector machines}.
\newblock Springer Science \& Business Media, 2008.

\bibitem[She94]{shewchuk1994introduction}
J.~R. Shewchuk.
\newblock An introduction to the conjugate gradient method without the
  agonizing pain.
\newblock Technical report, Pittsburgh, PA, USA, 1994.

\bibitem[SNB05]{sindhwani2005beyond}
V. Sindhwani, P. Niyogi, and M. Belkin.
\newblock Beyond the point cloud: from transductive to semi-supervised
  learning.
\newblock In {\em ICML}, pages 824--831, 2005.

\bibitem[SS16]{Santin2016}
G. Santin and R. Schaback.
\newblock Approximation of eigenfunctions in kernel-based spaces.
\newblock {\em Advances in Computational Mathematics}, 42(4):973--993, 2016.

\bibitem[SSSSC11]{shalev2011pegasos}
S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter.
\newblock Pegasos: Primal estimated sub-gradient solver for {SVM}.
\newblock {\em Mathematical programming}, 127(1):3--30, 2011.

\bibitem[STC04]{shawe2004kernel}
J. Shawe-Taylor and N. Cristianini.
\newblock {\em Kernel methods for pattern analysis}.
\newblock Cambridge university press, 2004.

\bibitem[SYG07]{schraudolph2007stochastic}
N.~N. Schraudolph, J. Yu, and S. G{\"u}nter.
\newblock A stochastic quasi-newton method for online convex optimization.
\newblock In {\em AISTATS}, pages 436--443, 2007.

\bibitem[SZS{\etalchar{+}}13]{szegedy2013intriguing}
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R.
  Fergus.
\newblock Intriguing properties of neural networks.
\newblock {\em arXiv preprint arXiv:1312.6199}, 2013.

\bibitem[TBRS13]{takac2013mini}
M. Tak{\'a}c, A.~S. Bijral, P. Richt{\'a}rik, and N. Srebro.
\newblock Mini-batch primal and dual methods for {SVM}s.
\newblock In {\em ICML (3)}, pages 1022--1030, 2013.

\bibitem[TH12]{tieleman2012lecture}
T. Tieleman and G. Hinton.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock {\em COURSERA: Neural Networks for Machine Learning}, 4:2, 2012.

\bibitem[Tro15]{tropp2015introduction}
J.~A. Tropp.
\newblock An introduction to matrix concentration inequalities.
\newblock {\em arXiv preprint arXiv:1501.01571}, 2015.

\bibitem[TRVR16]{tu2016large}
S. Tu, R. Roelofs, S. Venkataraman, and B. Recht.
\newblock Large scale kernel learning using block coordinate descent.
\newblock {\em arXiv preprint arXiv:1602.05310}, 2016.

\bibitem[Tsy04]{tsybakov2004optimal}
A.~B. Tsybakov.
\newblock Optimal aggregation of classifiers in statistical learning.
\newblock {\em Annals of Statistics}, pages 135--166, 2004.

\bibitem[WS01]{williams2001using}
C. Williams and M. Seeger.
\newblock Using the {N}ystr{\"o}m method to speed up kernel machines.
\newblock In {\em NIPS}, pages 682--688, 2001.

\bibitem[YRC07]{yao2007early}
Y. Yao, L. Rosasco, and A. Caponnetto.
\newblock On early stopping in gradient descent learning.
\newblock {\em Constructive Approximation}, 26(2):289--315, 2007.

\bibitem[ZBH{\etalchar{+}}16]{zhang2016understanding}
C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em arXiv preprint arXiv:1611.03530}, 2016.

\end{thebibliography}
