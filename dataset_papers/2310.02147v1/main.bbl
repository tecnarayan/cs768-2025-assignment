\begin{thebibliography}{10}

\bibitem{abounadi2001learning}
Jinane Abounadi, Dimitrib Bertsekas, and Vivek~S Borkar.
\newblock Learning algorithms for markov decision processes with average cost.
\newblock {\em SIAM Journal on Control and Optimization}, 40(3):681--698, 2001.

\bibitem{achiam2019towards}
Joshua Achiam, Ethan Knight, and Pieter Abbeel.
\newblock Towards characterizing divergence in deep q-learning.
\newblock {\em arXiv preprint arXiv:1903.08894}, 2019.

\bibitem{avrachenkov2022whittle}
Konstantin~E Avrachenkov and Vivek~S Borkar.
\newblock Whittle index based q-learning for restless bandits with average
  reward.
\newblock {\em Automatica}, 139:110186, 2022.

\bibitem{bertsekas1996neuro}
Dimitri Bertsekas and John~N Tsitsiklis.
\newblock {\em Neuro-dynamic programming}.
\newblock Athena Scientific, 1996.

\bibitem{bertsimas2000restless}
Dimitris Bertsimas and Jos{\'e} Ni{\~n}o-Mora.
\newblock {Restless Bandits, Linear Programming Relaxations, and A Primal-Dual
  Index Heuristic}.
\newblock {\em Operations Research}, 48(1):80--90, 2000.

\bibitem{bhandari2018finite}
Jalaj Bhandari, Daniel Russo, and Raghav Singal.
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock In {\em Conference on learning theory}, pages 1691--1692. PMLR, 2018.

\bibitem{bhatnagar2009natural}
Shalabh Bhatnagar, Richard~S Sutton, Mohammad Ghavamzadeh, and Mark Lee.
\newblock {Natural Actor--Critic Algorithms}.
\newblock {\em Automatica}, 45(11):2471--2482, 2009.

\bibitem{biswas2021learn}
Arpita Biswas, Gaurav Aggarwal, Pradeep Varakantham, and Milind Tambe.
\newblock Learn to intervene: An adaptive learning policy for restless bandits
  in application to preventive healthcare.
\newblock In {\em Proc. of IJCAI}, 2021.

\bibitem{borkar2009stochastic}
Vivek~S Borkar.
\newblock {\em {Stochastic Approximation: A Dynamical Systems Viewpoint}},
  volume~48.
\newblock Springer, 2009.

\bibitem{borkar2018reinforcement}
Vivek~S Borkar and Karan Chadha.
\newblock A reinforcement learning algorithm for restless bandits.
\newblock In {\em 2018 Indian Control Conference (ICC)}, pages 89--94. IEEE,
  2018.

\bibitem{borkar1997actor}
Vivek~S Borkar and Vijaymohan~R Konda.
\newblock {The Actor-Critic Algorithm as Multi-Time-Scale Stochastic
  Approximation}.
\newblock {\em Sadhana}, 22(4):525--543, 1997.

\bibitem{borkar2017index}
Vivek~S Borkar, K~Ravikumar, and Krishnakant Saboo.
\newblock An index policy for dynamic pricing in cloud computing under price
  commitments.
\newblock {\em Applicationes Mathematicae}, 44:215--245, 2017.

\bibitem{cai2023neural}
Qi~Cai, Zhuoran Yang, Jason~D Lee, and Zhaoran Wang.
\newblock Neural temporal difference and q learning provably converge to global
  optima.
\newblock {\em Mathematics of Operations Research}, 2023.

\bibitem{chen2020finite}
Zaiwei Chen, Siva Theja~Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam.
\newblock {Finite-Sample Analysis of Stochastic Approximation Using Smooth
  Convex Envelopes}.
\newblock {\em arXiv e-prints}, pages arXiv--2002, 2020.

\bibitem{chen2019performance}
Zaiwei Chen, Sheng Zhang, Thinh~T Doan, Siva~Theja Maguluri, and John-Paul
  Clarke.
\newblock {Performance of Q-learning with Linear Function Approximation:
  Stability and Finite-Time Analysis}.
\newblock {\em arXiv preprint arXiv:1905.11425}, 2019.

\bibitem{dai2011non}
Wenhan Dai, Yi~Gai, Bhaskar Krishnamachari, and Qing Zhao.
\newblock {The Non-Bayesian Restless Multi-Armed Bandit: A Case of
  Near-Logarithmic Regret}.
\newblock In {\em Proc. of IEEE ICASSP}, 2011.

\bibitem{dalal2020tale}
Gal Dalal, Balazs Szorenyi, and Gugan Thoppe.
\newblock A tale of two-timescale reinforcement learning with the tightest
  finite-time bound.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 3701--3708, 2020.

\bibitem{dalal2018finite}
Gal Dalal, Gugan Thoppe, Bal{\'a}zs Sz{\"o}r{\'e}nyi, and Shie Mannor.
\newblock Finite sample analysis of two-timescale stochastic approximation with
  applications to reinforcement learning.
\newblock In {\em Conference On Learning Theory}, pages 1199--1233. PMLR, 2018.

\bibitem{doan2020nonlinear}
Thinh~T Doan.
\newblock Nonlinear two-time-scale stochastic approximation: Convergence and
  finite-time performance.
\newblock {\em arXiv preprint arXiv:2011.01868}, 2020.

\bibitem{doan2021finite}
Thinh~T Doan.
\newblock Finite-time convergence rates of nonlinear two-time-scale stochastic
  approximation under markovian noise.
\newblock {\em arXiv preprint arXiv:2104.01627}, 2021.

\bibitem{doan2019linear}
Thinh~T Doan and Justin Romberg.
\newblock {Linear Two-Time-Scale Stochastic Approximation A Finite-Time
  Analysis}.
\newblock In {\em Proc. of Allerton}, 2019.

\bibitem{fan2020theoretical}
Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang.
\newblock A theoretical analysis of deep q-learning.
\newblock In {\em Learning for Dynamics and Control}, pages 486--489. PMLR,
  2020.

\bibitem{fu2019towards}
Jing Fu, Yoni Nazarathy, Sarat Moka, and Peter~G Taylor.
\newblock Towards q-learning the whittle index for restless bandits.
\newblock In {\em 2019 Australian \& New Zealand Control Conference (ANZCC)},
  pages 249--254. IEEE, 2019.

\bibitem{gupta2019finite}
Harsh Gupta, Rayadurgam Srikant, and Lei Ying.
\newblock Finite-time performance bounds and adaptive learning rate selection
  for two time-scale reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{jiang2023online}
Bowen Jiang, Bo~Jiang, Jian Li, Tao Lin, Xinbing Wang, and Chenghu Zhou.
\newblock Online restless bandits with unobserved states.
\newblock In {\em Proc. of ICML}, 2023.

\bibitem{jung2019regret}
Young~Hun Jung and Ambuj Tewari.
\newblock {Regret Bounds for Thompson Sampling in Episodic Restless Bandit
  Problems}.
\newblock {\em Proc. of NeurIPS}, 2019.

\bibitem{kaledin2020finite}
Maxim Kaledin, Eric Moulines, Alexey Naumov, Vladislav Tadic, and Hoi-To Wai.
\newblock Finite time analysis of linear two-timescale stochastic approximation
  with markovian noise.
\newblock In {\em Conference on Learning Theory}, pages 2144--2203. PMLR, 2020.

\bibitem{killian2021q}
Jackson~A Killian, Arpita Biswas, Sanket Shah, and Milind Tambe.
\newblock Q-learning lagrange policies for multi-action restless bandits.
\newblock In {\em Proceedings of the 27th ACM SIGKDD Conference on Knowledge
  Discovery \& Data Mining}, pages 871--881, 2021.

\bibitem{killian2021beyond}
Jackson~A Killian, Andrew Perrault, and Milind Tambe.
\newblock {Beyond" To Act or Not to Act": Fast Lagrangian Approaches to General
  Multi-Action Restless Bandits}.
\newblock In {\em Proc.of AAMAS}, 2021.

\bibitem{konda2000actor}
Vijay~R Konda and John~N Tsitsiklis.
\newblock {Actor-Critic Algorithms}.
\newblock In {\em Proc. of NIPS}, 2000.

\bibitem{konda2004convergence}
Vijay~R Konda and John~N Tsitsiklis.
\newblock Convergence rate of linear two-time-scale stochastic approximation.
\newblock {\em The Annals of Applied Probability}, 14(2):796--819, 2004.

\bibitem{levin2017markov}
David~A Levin and Yuval Peres.
\newblock {\em Markov chains and mixing times}, volume 107.
\newblock American Mathematical Soc., 2017.

\bibitem{liu2011logarithmic}
Haoyang Liu, Keqin Liu, and Qing Zhao.
\newblock {Logarithmic Weak Regret of Non-Bayesian Restless Multi-Armed
  Bandit}.
\newblock In {\em Proc of IEEE ICASSP}, 2011.

\bibitem{liu2012learning}
Haoyang Liu, Keqin Liu, and Qing Zhao.
\newblock {Learning in A Changing World: Restless Multi-Armed Bandit with
  Unknown Dynamics}.
\newblock {\em IEEE Transactions on Information Theory}, 59(3):1902--1916,
  2012.

\bibitem{mate2020collapsing}
Aditya Mate, Jackson Killian, Haifeng Xu, Andrew Perrault, and Milind Tambe.
\newblock Collapsing bandits and their application to public health
  intervention.
\newblock {\em Advances in Neural Information Processing Systems},
  33:15639--15650, 2020.

\bibitem{mate2021risk}
Aditya Mate, Andrew Perrault, and Milind Tambe.
\newblock {Risk-Aware Interventions in Public Health: Planning with Restless
  Multi-Armed Bandits}.
\newblock In {\em Proc.of AAMAS}, 2021.

\bibitem{melo2008analysis}
Francisco~S Melo, Sean~P Meyn, and M~Isabel Ribeiro.
\newblock An analysis of reinforcement learning with function approximation.
\newblock In {\em Proceedings of the 25th international conference on Machine
  learning}, pages 664--671, 2008.

\bibitem{meshram2016optimal}
Rahul Meshram, Aditya Gopalan, and D~Manjunath.
\newblock Optimal recommendation to users that react: Online learning for a
  class of pomdps.
\newblock In {\em 2016 IEEE 55th Conference on Decision and Control (CDC)},
  pages 7210--7215. IEEE, 2016.

\bibitem{mokkadem2006convergence}
Abdelkader Mokkadem and Mariane Pelletier.
\newblock Convergence rate and averaging of nonlinear two-time-scale stochastic
  approximation algorithms.
\newblock {\em The Annals of Applied Probability}, 16(3):1671--1702, 2006.

\bibitem{nakhleh2021neurwin}
Khaled Nakhleh, Santosh Ganji, Ping-Chun Hsieh, I~Hou, Srinivas Shakkottai,
  et~al.
\newblock Neurwin: Neural whittle index network for restless bandits via deep
  rl.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{nakhleh2022deeptop}
Khaled Nakhleh, I~Hou, et~al.
\newblock Deeptop: Deep threshold-optimal policy for mdps and rmabs.
\newblock {\em arXiv preprint arXiv:2209.08646}, 2022.

\bibitem{ortner2012regret}
Ronald Ortner, Daniil Ryabko, Peter Auer, and R{\'e}mi Munos.
\newblock {Regret Bounds for Restless Markov Bandits}.
\newblock In {\em Proc. of Algorithmic Learning Theory}, 2012.

\bibitem{pagare2023full}
Tejas Pagare, Vivek Borkar, and Konstantin Avrachenkov.
\newblock Full gradient deep reinforcement learning for average-reward
  criterion.
\newblock {\em arXiv preprint arXiv:2304.03729}, 2023.

\bibitem{papadimitriou1994complexity}
Christos~H Papadimitriou and John~N Tsitsiklis.
\newblock {The Complexity of Optimal Queueing Network Control}.
\newblock In {\em Proc. of IEEE Conference on Structure in Complexity Theory},
  1994.

\bibitem{puterman1994markov}
Martin~L Puterman.
\newblock {\em {Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}}.
\newblock John Wiley \& Sons, 1994.

\bibitem{qu2020finite}
Guannan Qu and Adam Wierman.
\newblock {Finite-Time Analysis of Asynchronous Stochastic Approximation and $
  Q $-Learning}.
\newblock In {\em Proc. of COLT}, 2020.

\bibitem{sharma2020approximate}
Hiteshi Sharma, Mehdi Jafarnia-Jahromi, and Rahul Jain.
\newblock Approximate relative value learning for average-reward continuous
  state mdps.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 956--964.
  PMLR, 2020.

\bibitem{srikant2019finite}
Rayadurgam Srikant and Lei Ying.
\newblock Finite-time error bounds for linear stochastic approximation andtd
  learning.
\newblock In {\em Conference on Learning Theory}, pages 2803--2830. PMLR, 2019.

\bibitem{suttle2021reinforcement}
Wesley Suttle, Kaiqing Zhang, Zhuoran Yang, Ji~Liu, and David Kraemer.
\newblock {Reinforcement Learning for Cost-Aware Markov Decision Processes}.
\newblock In {\em Proc. of ICML}, 2021.

\bibitem{tekin2012online}
Cem Tekin and Mingyan Liu.
\newblock {Online Learning of Rested and Restless Bandits}.
\newblock {\em IEEE Transactions on Information Theory}, 58(8):5588--5611,
  2012.

\bibitem{tsitsiklis1999average}
John~N Tsitsiklis and Benjamin Van~Roy.
\newblock {Average Cost Temporal-Difference Learning}.
\newblock {\em Automatica}, 35(11):1799--1808, 1999.

\bibitem{wan2021learning}
Yi~Wan, Abhishek Naik, and Richard~S Sutton.
\newblock {Learning and Planning in Average-Reward Markov Decision Processes}.
\newblock In {\em Proc. of ICML}, 2021.

\bibitem{wang2020restless}
Siwei Wang, Longbo Huang, and John Lui.
\newblock {Restless-UCB, an Efficient and Low-complexity Algorithm for Online
  Restless Bandits}.
\newblock In {\em Proc. of NeurIPS}, 2020.

\bibitem{weber1990index}
Richard~R Weber and Gideon Weiss.
\newblock {On An Index Policy for Restless Bandits}.
\newblock {\em Journal of applied probability}, pages 637--648, 1990.

\bibitem{wei2020model}
Chen-Yu Wei, Mehdi~Jafarnia Jahromi, Haipeng Luo, Hiteshi Sharma, and Rahul
  Jain.
\newblock {Model-free Reinforcement Learning in Infinite-Horizon Average-Reward
  Markov Decision Processes}.
\newblock In {\em Proc. of ICML}, 2020.

\bibitem{whittle1988restless}
Peter Whittle.
\newblock {Restless Bandits: Activity Allocation in A Changing World}.
\newblock {\em Journal of applied probability}, pages 287--298, 1988.

\bibitem{xiong2022reinforcement}
Guojun Xiong, Jian Li, and Rahul Singh.
\newblock {Reinforcement Learning Augmented Asymptotically Optimal Index
  Policies for Finite-Horizon Restless Bandits}.
\newblock In {\em Proc. of AAAI}, 2022.

\bibitem{xiong2022indexwireless}
Guojun Xiong, Xudong Qin, Bin Li, Rahul Singh, and Jian Li.
\newblock {Index-aware Reinforcement Learning for Adaptive Video Streaming at
  the Wireless Edge}.
\newblock In {\em Proc. of ACM MobiHoc}, 2022.

\bibitem{xiong2022Nips}
Guojun Xiong, Shufan Wang, and Jian Li.
\newblock Learning infinite-horizon average-reward restless multi-action
  bandits via index awareness.
\newblock {\em Proc. of NeurIPS}, 2022.

\bibitem{xiong2023whittle}
Guojun Xiong, Shufan Wang, Jian Li, and Rahul Singh.
\newblock Whittle index based q-learning for wireless edge caching with linear
  function approximation.
\newblock {\em arXiv preprint arXiv:2202.13187}, 2022.

\bibitem{xiong2022reinforcementcache}
Guojun Xiong, Shufan Wang, Gang Yan, and Jian Li.
\newblock {Reinforcement Learning for Dynamic Dimensioning of Cloud Caches: A
  Restless Bandit Approach}.
\newblock In {\em Proc. of IEEE INFOCOM}, 2022.

\bibitem{xu2020finite}
Pan Xu and Quanquan Gu.
\newblock A finite-time analysis of q-learning with neural network function
  approximation.
\newblock In {\em International Conference on Machine Learning}, pages
  10555--10565. PMLR, 2020.

\bibitem{yang2019provably}
Zhuoran Yang, Yongxin Chen, Mingyi Hong, and Zhaoran Wang.
\newblock {Provably Global Convergence of Actor-Critic: A Case for Linear
  Quadratic Regulator with Ergodic Cost}.
\newblock In {\em Proc. of NeurIPS}, 2019.

\bibitem{yu2018deadline}
Zhe Yu, Yunjian Xu, and Lang Tong.
\newblock {Deadline Scheduling as Restless Bandits}.
\newblock {\em IEEE Transactions on Automatic Control}, 63(8):2343--2358, 2018.

\bibitem{zhang2021average}
Shangtong Zhang, Yi~Wan, Richard~S Sutton, and Shimon Whiteson.
\newblock {Average-Reward Off-Policy Policy Evaluation with Function
  Approximation}.
\newblock {\em arXiv preprint arXiv:2101.02808}, 2021.

\bibitem{zhang2021finite}
Sheng Zhang, Zhe Zhang, and Siva~Theja Maguluri.
\newblock {Finite Sample Analysis of Average-Reward TD Learning and $ Q
  $-Learning}.
\newblock {\em Proc. of NeurIPS}, 2021.

\bibitem{zou2019finite}
Shaofeng Zou, Tengyu Xu, and Yingbin Liang.
\newblock Finite-sample analysis for sarsa with linear function approximation.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\end{thebibliography}
