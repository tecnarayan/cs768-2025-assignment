@incollection{NIPS2018_8209,
title = {A Retrieve-and-Edit Framework for Predicting Structured Outputs},
author = {Hashimoto, Tatsunori B and Guu, Kelvin and Oren, Yonatan and Liang, Percy S},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {10052--10062},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.pdf}
}
@inproceedings{hossain-etal-2020-simple,
    title = "Simple and Effective Retrieve-Edit-Rerank Text Generation",
    author = "Hossain, Nabil  and
      Ghazvininejad, Marjan  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.228",
    doi = "10.18653/v1/2020.acl-main.228",
    pages = "2532--2538",
    abstract = "Retrieve-and-edit seq2seq methods typically retrieve an output from the training set and learn a model to edit it to produce the final output. We propose to extend this framework with a simple and effective post-generation ranking approach. Our framework (i) retrieves several potentially relevant outputs for each input, (ii) edits each candidate independently, and (iii) re-ranks the edited candidates to select the final output. We use a standard editing model with simple task-specific re-ranking approaches, and we show empirically that this approach outperforms existing, significantly more complex methodologies. Experiments on two machine translation (MT) datasets show new state-of-art results. We also achieve near state-of-art performance on the Gigaword summarization dataset, where our analyses show that there is significant room for performance improvement with better candidate output selection in future work.",
}
@inproceedings{gu_mt_retrieve_and_edit,
title = "Search engine guided neural machine translation",
abstract = "In this paper, we extend an attention-based neural machine translation (NMT) model by allowing it to access an entire training set of parallel sentence pairs even after training. The proposed approach consists of two stages. In the first stage-retrieval stage-, an off-the-shelf, black-box search engine is used to retrieve a small subset of sentence pairs from a training set given a source sentence. These pairs are further filtered based on a fuzzy matching score based on edit distance. In the second stage-translation stage-, a novel translation model, called search engine guided NMT (SEG-NMT), seamlessly uses both the source sentence and a set of retrieved sentence pairs to perform the translation. Empirical evaluation on three language pairs (En-Fr, En-De, and En-Es) shows that the proposed approach significantly outperforms the baseline approach and the improvement is more significant when more relevant sentence pairs were retrieved.",
author = "Jiatao Gu and Yong Wang and Kyunghyun Cho and Li, {Victor O.K.}",
year = "2018",
language = "English (US)",
series = "32nd AAAI Conference on Artificial Intelligence, AAAI 2018",
publisher = "AAAI press",
pages = "5133--5140",
booktitle = "32nd AAAI Conference on Artificial Intelligence, AAAI 2018",
note = "32nd AAAI Conference on Artificial Intelligence, AAAI 2018 ; Conference date: 02-02-2018 Through 07-02-2018",
}
@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}
@article{Bi2020PALMPA,
  title={PALM: Pre-training an Autoencoding\&Autoregressive Language Model for Context-conditioned Generation},
  author={Bin Bi and Chenliang Li and Chen Wu and Ming Yan and Wei Wang},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.07159},
  url={https://arxiv.org/abs/2004.07159}
}
@inproceedings{kingma_adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{
petroni2020how,
title={How Context Affects Language Models' Factual Predictions},
author={Fabio Petroni and Patrick Lewis and Aleksandra Piktus and Tim Rockt{\"a}schel and Yuxiang Wu and Alexander H. Miller and Sebastian Riedel},
booktitle={Automated Knowledge Base Construction},
year={2020},
url={https://openreview.net/forum?id=025X0zPfn}
}
@article{Zhong2019ReasoningOS,
  title={Reasoning Over Semantic-Level Graph for Fact Checking},
  author={Wanjun Zhong and Jingjing Xu and Duyu Tang and Zenan Xu and Nan Duan and Ming Zhou and Jiahai Wang and Jian Yin},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.03745},
  url={https://arxiv.org/abs/1909.03745}
}
@article{Vijayakumar2016DiverseBS,
	author = {Ashwin Vijayakumar and Michael Cogswell and Ramprasaath Selvaraju and Qing Sun and Stefan Lee and David Crandall and Dhruv Batra},
	title = {Diverse Beam Search for Improved Description of Complex Scenes},
	journal = {AAAI Conference on Artificial Intelligence},
	year = {2018},
	keywords = {Recurrent Neural Networks, Beam Search, Diversity},
	abstract = {A single image captures the appearance and position of multiple entities in a scene as well as their complex interactions. As a consequence, natural language grounded in visual contexts tends to be diverse---with utterances differing as focus shifts to specific objects, interactions, or levels of detail. Recently, neural sequence models such as RNNs and LSTMs have been employed to produce visually-grounded language. Beam Search, the standard work-horse for decoding sequences from these models, is an approximate inference algorithm that decodes the top-B sequences in a greedy left-to-right fashion. In practice, the resulting sequences are often minor rewordings of a common utterance, failing to capture the multimodal nature of source images. To address this shortcoming, we propose Diverse Beam Search (DBS), a diversity promoting alternative to BS for approximate inference. DBS produces sequences that are significantly different from each other by incorporating diversity constraints within groups of candidate sequences during decoding; moreover, it achieves this with minimal computational or memory overhead. We demonstrate that our method improves both diversity and quality of decoded sequences over existing techniques on two visually-grounded language generation tasks---image captioning and visual question generation---particularly on complex scenes containing diverse visual content. We also show similar improvements at language-only machine translation tasks, highlighting the generality of our approach.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17329}
}
@article{Malkov2016EfficientAR,
  title={Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs},
  author={Yury A. Malkov and D. A. Yashunin},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2016},
  volume={42},
  pages={824-836},
  url={https://arxiv.org/abs/1603.09320}
}
@misc{
fan2020augmenting,
title={Augmenting Transformers with {KNN}-Based Composite Memory},
author={Angela Fan and Claire Gardent and Chloe Braud and Antoine Bordes},
year={2020},
url={https://openreview.net/forum?id=H1gx1CNKPH}
}

@incollection{lample_large_2019,
title = {Large Memory Layers with Product Keys},
author = {Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc' Aurelio and Denoyer, Ludovic and Jegou, Herve},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d' Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8548--8559},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf}
}



@inproceedings{joulin2015,
author = {Joulin, Armand and Mikolov, Tomas},
title = {Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {190–198},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS’15},
url={https://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets}
}


@article{robertson2009bm25,
author = {Robertson, Stephen and Zaragoza, Hugo},
title = {The Probabilistic Relevance Framework: BM25 and Beyond},
year = {2009},
issue_date = {April 2009},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {3},
number = {4},
issn = {1554-0669},
url = {https://doi.org/10.1561/1500000019},
doi = {10.1561/1500000019},
journal = {Found. Trends Inf. Retr.},
month = apr,
pages = {333–389},
numpages = {57}
}



@inproceedings{DBLP:conf/nips/NguyenRSGTMD16,
  author    = {Tri Nguyen and
               Mir Rosenberg and
               Xia Song and
               Jianfeng Gao and
               Saurabh Tiwary and
               Rangan Majumder and
               Li Deng},
  editor    = {Tarek Richard Besold and
               Antoine Bordes and
               Artur S. d'Avila Garcez and
               Greg Wayne},
  title     = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},
  booktitle = {Proceedings of the Workshop on Cognitive Computation: Integrating
               neural and symbolic approaches 2016 co-located with the 30th Annual
               Conference on Neural Information Processing Systems {(NIPS} 2016),
               Barcelona, Spain, December 9, 2016},
  series    = {{CEUR} Workshop Proceedings},
  volume    = {1773},
  publisher = {CEUR-WS.org},
  year      = {2016},
  url       = {http://ceur-ws.org/Vol-1773/CoCoNIPS\_2016\_paper9.pdf},
  timestamp = {Wed, 12 Feb 2020 16:44:20 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/NguyenRSGTMD16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Li2019ACUTEEVALID,
  title={ACUTE-EVAL: Improved Dialogue Evaluation with Optimized Questions and Multi-turn Comparisons},
  author={Margaret Li and Jason Weston and Stephen Roller},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.03087},
  url={https://arxiv.org/abs/1909.03087}
}
 @article{Thorne2020AvoidingCF,
  title={Avoiding catastrophic forgetting in mitigating model biases in sentence-pair classification with elastic weight consolidation},
  author={James H. Thorne and Andreas Vlachos},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.14366},
  url={https://arxiv.org/abs/2004.14366}
}

@article{herrero-zazo_ddi_2013,
	title = {The {DDI} corpus: {An} annotated corpus with pharmacological substances and drug–drug interactions},
	volume = {46},
	issn = {1532-0464},
	shorttitle = {The {DDI} corpus},
	url = {http://www.sciencedirect.com/science/article/pii/S1532046413001123},
	doi = {10.1016/j.jbi.2013.07.011},
	abstract = {The management of drug–drug interactions (DDIs) is a critical issue resulting from the overwhelming amount of information available on them. Natural Language Processing (NLP) techniques can provide an interesting way to reduce the time spent by healthcare professionals on reviewing biomedical literature. However, NLP techniques rely mostly on the availability of the annotated corpora. While there are several annotated corpora with biological entities and their relationships, there is a lack of corpora annotated with pharmacological substances and DDIs. Moreover, other works in this field have focused in pharmacokinetic (PK) DDIs only, but not in pharmacodynamic (PD) DDIs. To address this problem, we have created a manually annotated corpus consisting of 792 texts selected from the DrugBank database and other 233 Medline abstracts. This fined-grained corpus has been annotated with a total of 18,502 pharmacological substances and 5028 DDIs, including both PK as well as PD interactions. The quality and consistency of the annotation process has been ensured through the creation of annotation guidelines and has been evaluated by the measurement of the inter-annotator agreement between two annotators. The agreement was almost perfect (Kappa up to 0.96 and generally over 0.80), except for the DDIs in the MedLine database (0.55–0.72). The DDI corpus has been used in the SemEval 2013 DDIExtraction challenge as a gold standard for the evaluation of information extraction techniques applied to the recognition of pharmacological substances and the detection of DDIs from biomedical texts. DDIExtraction 2013 has attracted wide attention with a total of 14 teams from 7 different countries. For the task of recognition and classification of pharmacological names, the best system achieved an F1 of 71.5\%, while, for the detection and classification of DDIs, the best result was F1 of 65.1\%. These results show that the corpus has enough quality to be used for training and testing NLP techniques applied to the field of Pharmacovigilance. The DDI corpus and the annotation guidelines are free for use for academic research and are available at http://labda.inf.uc3m.es/ddicorpus.},
	language = {en},
	number = {5},
	urldate = {2020-04-27},
	journal = {Journal of Biomedical Informatics},
	author = {Herrero-Zazo, María and Segura-Bedmar, Isabel and Martínez, Paloma and Declerck, Thierry},
	month = oct,
	year = {2013},
	keywords = {Biomedical corpora, Drug interaction, Information extraction},
	pages = {914--920}
}

@article{van_mulligen_eu-adr_2012,
	title = {The {EU}-{ADR} corpus: annotated drugs, diseases, targets, and their relationships},
	volume = {45},
	issn = {1532-0480},
	shorttitle = {The {EU}-{ADR} corpus},
	doi = {10.1016/j.jbi.2012.04.004},
	abstract = {Corpora with specific entities and relationships annotated are essential to train and evaluate text-mining systems that are developed to extract specific structured information from a large corpus. In this paper we describe an approach where a named-entity recognition system produces a first annotation and annotators revise this annotation using a web-based interface. The agreement figures achieved show that the inter-annotator agreement is much better than the agreement with the system provided annotations. The corpus has been annotated for drugs, disorders, genes and their inter-relationships. For each of the drug-disorder, drug-target, and target-disorder relations three experts have annotated a set of 100 abstracts. These annotated relationships will be used to train and evaluate text-mining software to capture these relationships in texts.},
	language = {eng},
	number = {5},
	journal = {Journal of Biomedical Informatics},
	author = {van Mulligen, Erik M. and Fourrier-Reglat, Annie and Gurwitz, David and Molokhia, Mariam and Nieto, Ainhoa and Trifiro, Gianluca and Kors, Jan A. and Furlong, Laura I.},
	month = oct,
	year = {2012},
	pmid = {22554700},
	keywords = {Data Mining, Databases, Factual, Documentation, Drug Therapy, Humans, Internet, Medical Informatics, Pharmaceutical Preparations, User-Computer Interface},
	pages = {879--884}
}

@article{bravo_extraction_2015,
	title = {Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research},
	volume = {16},
	issn = {1471-2105},
	shorttitle = {Extraction of relations between genes and diseases from text and large-scale data analysis},
	doi = {10.1186/s12859-015-0472-9},
	abstract = {BACKGROUND: Current biomedical research needs to leverage and exploit the large amount of information reported in scientific publications. Automated text mining approaches, in particular those aimed at finding relationships between entities, are key for identification of actionable knowledge from free text repositories. We present the BeFree system aimed at identifying relationships between biomedical entities with a special focus on genes and their associated diseases.
RESULTS: By exploiting morpho-syntactic information of the text, BeFree is able to identify gene-disease, drug-disease and drug-target associations with state-of-the-art performance. The application of BeFree to real-case scenarios shows its effectiveness in extracting information relevant for translational research. We show the value of the gene-disease associations extracted by BeFree through a number of analyses and integration with other data sources. BeFree succeeds in identifying genes associated to a major cause of morbidity worldwide, depression, which are not present in other public resources. Moreover, large-scale extraction and analysis of gene-disease associations, and integration with current biomedical knowledge, provided interesting insights on the kind of information that can be found in the literature, and raised challenges regarding data prioritization and curation. We found that only a small proportion of the gene-disease associations discovered by using BeFree is collected in expert-curated databases. Thus, there is a pressing need to find alternative strategies to manual curation, in order to review, prioritize and curate text-mining data and incorporate it into domain-specific databases. We present our strategy for data prioritization and discuss its implications for supporting biomedical research and applications.
CONCLUSIONS: BeFree is a novel text mining system that performs competitively for the identification of gene-disease, drug-disease and drug-target associations. Our analyses show that mining only a small fraction of MEDLINE results in a large dataset of gene-disease associations, and only a small proportion of this dataset is actually recorded in curated resources (2\%), raising several issues on data prioritization and curation. We propose that joint analysis of text mined data with data curated by experts appears as a suitable approach to both assess data quality and highlight novel and interesting information.},
	language = {eng},
	journal = {BMC bioinformatics},
	author = {Bravo, Àlex and Piñero, Janet and Queralt-Rosinach, Núria and Rautschka, Michael and Furlong, Laura I.},
	month = feb,
	year = {2015},
	pmid = {25886734},
	pmcid = {PMC4466840},
	keywords = {Data Mining, Databases, Factual, Depression, Disease, Humans, Information Storage and Retrieval, Knowledge Bases, MEDLINE, Publications, Translational Medical Research},
	pages = {55}
}

@inproceedings{krallinger_overview_2017,
	title = {Overview of the {BioCreative} {VI} chemical-protein interaction {Track}},
	author = {Krallinger, Martin and Rabal, Obdulia and Akhondi, Saber Ahmad and Pérez, Martín Pérez and Santamaría, Jésús López and Rodríguez, Gael Pérez and Tsatsaronis, Georgios and Intxaurrondo, Ander and López, José Antonio Baso and Nandal, Umesh and Buel, Erin M. van and Chandrasekhar, A. Poorna and Rodenburg, Marleen and Lægreid, Astrid and Doornenbal, Marius A. and Oyarzábal, Julen and Lourenço, Anália and Valencia, Alfonso},
	year = {2017}
}

@inproceedings{romanov_lessons_2018,
	address = {Brussels, Belgium},
	title = {Lessons from {Natural} {Language} {Inference} in the {Clinical} {Domain}},
	url = {https://www.aclweb.org/anthology/D18-1187},
	doi = {10.18653/v1/D18-1187},
	abstract = {State of the art models using deep neural networks have become very good in learning an accurate mapping from inputs to outputs. However, they still lack generalization capabilities in conditions that differ from the ones encountered during training. This is even more challenging in specialized, and knowledge intensive domains, where training data is limited. To address this gap, we introduce MedNLI - a dataset annotated by doctors, performing a natural language inference task (NLI), grounded in the medical history of patients. We present strategies to: 1) leverage transfer learning using datasets from the open domain, (e.g. SNLI) and 2) incorporate domain knowledge from external data and lexical sources (e.g. medical terminologies). Our results demonstrate performance gains using both strategies.},
	urldate = {2020-04-27},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Romanov, Alexey and Shivade, Chaitanya},
	month = oct,
	year = {2018},
	pages = {1586--1596}
}

@article{baker_automatic_2016,
	title = {Automatic semantic classification of scientific literature according to the hallmarks of cancer},
	volume = {32},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/32/3/432/1743783},
	doi = {10.1093/bioinformatics/btv585},
	abstract = {Abstract.  Motivation: The hallmarks of cancer have become highly influential in cancer research. They reduce the complexity of cancer into 10 principles (e.g.},
	language = {en},
	number = {3},
	urldate = {2020-04-27},
	journal = {Bioinformatics},
	author = {Baker, Simon and Silins, Ilona and Guo, Yufan and Ali, Imran and Högberg, Johan and Stenius, Ulla and Korhonen, Anna},
	month = feb,
	year = {2016},
	note = {Publisher: Oxford Academic},
	pages = {432--440}
}

@article{sogancioglu_biosses_2017,
	title = {{BIOSSES}: a semantic sentence similarity estimation system for the biomedical domain},
	volume = {33},
	issn = {1367-4803},
	shorttitle = {{BIOSSES}},
	url = {https://academic.oup.com/bioinformatics/article/33/14/i49/3953954},
	doi = {10.1093/bioinformatics/btx238},
	abstract = {AbstractMotivation.  The amount of information available in textual format is rapidly increasing in the biomedical domain. Therefore, natural language processin},
	language = {en},
	number = {14},
	urldate = {2020-04-27},
	journal = {Bioinformatics},
	author = {Soğancıoğlu, Gizem and Öztürk, Hakime and Özgür, Arzucan},
	month = jul,
	year = {2017},
	note = {Publisher: Oxford Academic},
	pages = {i49--i58}
}

@inproceedings{peng_transfer_2019,
	address = {Florence, Italy},
	title = {Transfer {Learning} in {Biomedical} {Natural} {Language} {Processing}: {An} {Evaluation} of {BERT} and {ELMo} on {Ten} {Benchmarking} {Datasets}},
	shorttitle = {Transfer {Learning} in {Biomedical} {Natural} {Language} {Processing}},
	url = {https://www.aclweb.org/anthology/W19-5006},
	doi = {10.18653/v1/W19-5006},
	abstract = {Inspired by the success of the General Language Understanding Evaluation benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE) benchmark to facilitate research in the development of pre-training language representations in the biomedicine domain. The benchmark consists of five tasks with ten datasets that cover both biomedical and clinical texts with different dataset sizes and difficulties. We also evaluate several baselines based on BERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and MIMIC-III clinical notes achieves the best results. We make the datasets, pre-trained models, and codes publicly available at https://github.com/ ncbi-nlp/BLUE\_Benchmark.},
	urldate = {2020-04-27},
	booktitle = {Proceedings of the 18th {BioNLP} {Workshop} and {Shared} {Task}},
	publisher = {Association for Computational Linguistics},
	author = {Peng, Yifan and Yan, Shankai and Lu, Zhiyong},
	month = aug,
	year = {2019},
	pages = {58--65}
}

@article{gerner_linnaeus_2010,
	title = {{LINNAEUS}: a species name identification system for biomedical literature},
	volume = {11},
	issn = {1471-2105},
	shorttitle = {{LINNAEUS}},
	doi = {10.1186/1471-2105-11-85},
	abstract = {BACKGROUND: The task of recognizing and identifying species names in biomedical literature has recently been regarded as critical for a number of applications in text and data mining, including gene name recognition, species-specific document retrieval, and semantic enrichment of biomedical articles.
RESULTS: In this paper we describe an open-source species name recognition and normalization software system, LINNAEUS, and evaluate its performance relative to several automatically generated biomedical corpora, as well as a novel corpus of full-text documents manually annotated for species mentions. LINNAEUS uses a dictionary-based approach (implemented as an efficient deterministic finite-state automaton) to identify species names and a set of heuristics to resolve ambiguous mentions. When compared against our manually annotated corpus, LINNAEUS performs with 94\% recall and 97\% precision at the mention level, and 98\% recall and 90\% precision at the document level. Our system successfully solves the problem of disambiguating uncertain species mentions, with 97\% of all mentions in PubMed Central full-text documents resolved to unambiguous NCBI taxonomy identifiers.
CONCLUSIONS: LINNAEUS is an open source, stand-alone software system capable of recognizing and normalizing species name mentions with speed and accuracy, and can therefore be integrated into a range of bioinformatics and text-mining applications. The software and manually annotated corpus can be downloaded freely at http://linnaeus.sourceforge.net/.},
	language = {eng},
	journal = {BMC bioinformatics},
	author = {Gerner, Martin and Nenadic, Goran and Bergman, Casey M.},
	month = feb,
	year = {2010},
	pmid = {20149233},
	pmcid = {PMC2836304},
	keywords = {Computational Biology, Information Storage and Retrieval, PubMed, Software, Vocabulary, Controlled},
	pages = {85}
}

@article{pafilis_species_2013,
	title = {The {SPECIES} and {ORGANISMS} {Resources} for {Fast} and {Accurate} {Identification} of {Taxonomic} {Names} in {Text}},
	volume = {8},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0065390},
	abstract = {The exponential growth of the biomedical literature is making the need for efficient, accurate text-mining tools increasingly clear. The identification of named biological entities in text is a central and difficult task. We have developed an efficient algorithm and implementation of a dictionary-based approach to named entity recognition, which we here use to identify names of species and other taxa in text. The tool, SPECIES, is more than an order of magnitude faster and as accurate as existing tools. The precision and recall was assessed both on an existing gold-standard corpus and on a new corpus of 800 abstracts, which were manually annotated after the development of the tool. The corpus comprises abstracts from journals selected to represent many taxonomic groups, which gives insights into which types of organism names are hard to detect and which are easy. Finally, we have tagged organism names in the entire Medline database and developed a web resource, ORGANISMS, that makes the results accessible to the broad community of biologists. The SPECIES software is open source and can be downloaded from http://species.jensenlab.org along with dictionary files and the manually annotated gold-standard corpus. The ORGANISMS web resource can be found at http://organisms.jensenlab.org.},
	language = {eng},
	number = {6},
	journal = {PloS One},
	author = {Pafilis, Evangelos and Frankild, Sune P. and Fanini, Lucia and Faulwetter, Sarah and Pavloudi, Christina and Vasileiadou, Aikaterini and Arvanitidis, Christos and Jensen, Lars Juhl},
	year = {2013},
	pmid = {23823062},
	pmcid = {PMC3688812},
	keywords = {Classification, Data Mining, Terminology as Topic},
	pages = {e65390}
}

@misc{noauthor_linnaeus_nodate,
	title = {{LINNAEUS}: a species name identification system for biomedical literature. - {PubMed} - {NCBI}},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/20149233},
	urldate = {2020-04-27}
}

@article{smith_overview_2008,
	title = {Overview of {BioCreative} {II} gene mention recognition},
	volume = {9 Suppl 2},
	issn = {1474-760X},
	doi = {10.1186/gb-2008-9-s2-s2},
	abstract = {Nineteen teams presented results for the Gene Mention Task at the BioCreative II Workshop. In this task participants designed systems to identify substrings in sentences corresponding to gene name mentions. A variety of different methods were used and the results varied with a highest achieved F1 score of 0.8721. Here we present brief descriptions of all the methods used and a statistical analysis of the results. We also demonstrate that, by combining the results from all submissions, an F score of 0.9066 is feasible, and furthermore that the best result makes use of the lowest scoring submissions.},
	language = {eng},
	journal = {Genome Biology},
	author = {Smith, Larry and Tanabe, Lorraine K. and Ando, Rie Johnson nee and Kuo, Cheng-Ju and Chung, I.-Fang and Hsu, Chun-Nan and Lin, Yu-Shi and Klinger, Roman and Friedrich, Christoph M. and Ganchev, Kuzman and Torii, Manabu and Liu, Hongfang and Haddow, Barry and Struble, Craig A. and Povinelli, Richard J. and Vlachos, Andreas and Baumgartner, William A. and Hunter, Lawrence and Carpenter, Bob and Tsai, Richard Tzong-Han and Dai, Hong-Jie and Liu, Feng and Chen, Yifei and Sun, Chengjie and Katrenko, Sophia and Adriaans, Pieter and Blaschke, Christian and Torres, Rafael and Neves, Mariana and Nakov, Preslav and Divoli, Anna and Maña-López, Manuel and Mata, Jacinto and Wilbur, W. John},
	year = {2008},
	pmid = {18834493},
	pmcid = {PMC2559986},
	keywords = {Computational Biology, Congresses as Topic, Genes, Societies, Scientific},
	pages = {S2}
}

@inproceedings{baudivs2015modeling,
  title={Modeling of the question answering task in the yodaqa system},
  author={Baudi{\v{s}}, Petr and {\v{S}}ediv{\`y}, Jan},
  booktitle={International Conference of the Cross-Language Evaluation Forum for European Languages},
  pages={222--228},
  year={2015},
  organization={Springer},
  url={https://link.springer.com/chapter/10.1007\%2F978-3-319-24027-5_20}
}

@article{krallinger_chemdner_2015,
	title = {The {CHEMDNER} corpus of chemicals and drugs and its annotation principles},
	volume = {7},
	issn = {1758-2946},
	url = {https://doi.org/10.1186/1758-2946-7-S1-S2},
	doi = {10.1186/1758-2946-7-S1-S2},
	abstract = {The automatic extraction of chemical information from text requires the recognition of chemical entity mentions as one of its key steps. When developing supervised named entity recognition (NER) systems, the availability of a large, manually annotated text corpus is desirable. Furthermore, large corpora permit the robust evaluation and comparison of different approaches that detect chemicals in documents. We present the CHEMDNER corpus, a collection of 10,000 PubMed abstracts that contain a total of 84,355 chemical entity mentions labeled manually by expert chemistry literature curators, following annotation guidelines specifically defined for this task. The abstracts of the CHEMDNER corpus were selected to be representative for all major chemical disciplines. Each of the chemical entity mentions was manually labeled according to its structure-associated chemical entity mention (SACEM) class: abbreviation, family, formula, identifier, multiple, systematic and trivial. The difficulty and consistency of tagging chemicals in text was measured using an agreement study between annotators, obtaining a percentage agreement of 91. For a subset of the CHEMDNER corpus (the test set of 3,000 abstracts) we provide not only the Gold Standard manual annotations, but also mentions automatically detected by the 26 teams that participated in the BioCreative IV CHEMDNER chemical mention recognition task. In addition, we release the CHEMDNER silver standard corpus of automatically extracted mentions from 17,000 randomly selected PubMed abstracts. A version of the CHEMDNER corpus in the BioC format has been generated as well. We propose a standard for required minimum information about entity annotations for the construction of domain specific corpora on chemical and drug entities. The CHEMDNER corpus and annotation guidelines are available at: http://www.biocreative.org/resources/biocreative-iv/chemdner-corpus/},
	number = {1},
	urldate = {2020-04-27},
	journal = {Journal of Cheminformatics},
	author = {Krallinger, Martin and Rabal, Obdulia and Leitner, Florian and Vazquez, Miguel and Salgado, David and Lu, Zhiyong and Leaman, Robert and Lu, Yanan and Ji, Donghong and Lowe, Daniel M. and Sayle, Roger A. and Batista-Navarro, Riza Theresa and Rak, Rafal and Huber, Torsten and Rocktäschel, Tim and Matos, Sérgio and Campos, David and Tang, Buzhou and Xu, Hua and Munkhdalai, Tsendsuren and Ryu, Keun Ho and Ramanan, SV and Nathan, Senthil and Žitnik, Slavko and Bajec, Marko and Weber, Lutz and Irmer, Matthias and Akhondi, Saber A. and Kors, Jan A. and Xu, Shuo and An, Xin and Sikdar, Utpal Kumar and Ekbal, Asif and Yoshioka, Masaharu and Dieb, Thaer M. and Choi, Miji and Verspoor, Karin and Khabsa, Madian and Giles, C. Lee and Liu, Hongfang and Ravikumar, Komandur Elayavilli and Lamurias, Andre and Couto, Francisco M. and Dai, Hong-Jie and Tsai, Richard Tzong-Han and Ata, Caglar and Can, Tolga and Usié, Anabel and Alves, Rui and Segura-Bedmar, Isabel and Martínez, Paloma and Oyarzabal, Julen and Valencia, Alfonso},
	month = jan,
	year = {2015},
	pages = {S2}
}

@article{stubbs_annotating_2015,
	title = {Annotating longitudinal clinical narratives for de-identification: the 2014 i2b2/{UTHealth} {Corpus}},
	volume = {58},
	issn = {1532-0464},
	shorttitle = {Annotating longitudinal clinical narratives for de-identification},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978170/},
	doi = {10.1016/j.jbi.2015.07.020},
	abstract = {The 2014 i2b2/UTHealth natural language processing shared task featured a track focused on the de-identification of longitudinal medical records. For this track, we de-identified a set of 1,304 longitudinal medical records describing 296 patients. This corpus was de-identified under a broad interpretation of the HIPAA guidelines using double-annotation followed by arbitration, rounds of sanity checking, and proof reading. The average token-based F1 measure for the annotators compared to the gold standard was 0.927. The resulting annotations were used both to de-identify the data and to set the gold standard for the de-identification track of the 2014 i2b2/UTHealth shared task. All annotated private health information were replaced with realistic surrogates automatically and then read over and corrected manually. The resulting corpus is the first of its kind made available for de-identification research. This corpus was first used for the 2014 i2b2/UTHealth shared task, during which the systems achieved a mean F-measure of 0.872 and a maximum F-measure of 0.964 using entity-based micro-averaged evaluations.,},
	number = {Suppl},
	urldate = {2020-04-27},
	journal = {Journal of biomedical informatics},
	author = {Stubbs, Amber and Uzuner, Ozlem},
	month = dec,
	year = {2015},
	pmid = {26319540},
	pmcid = {PMC4978170},
	pages = {S20--S29}
}

@article{stubbs_automated_2015,
	title = {Automated systems for the de-identification of longitudinal clinical narratives: {Overview} of 2014 i2b2/{UTHealth} shared task {Track} 1},
	volume = {58},
	issn = {1532-0464},
	shorttitle = {Automated systems for the de-identification of longitudinal clinical narratives},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4989908/},
	doi = {10.1016/j.jbi.2015.06.007},
	abstract = {The 2014 i2b2/UTHealth Natural Language Processing (NLP) shared task featured four tracks. The first of these was the de-identification track focused on identifying protected health information (PHI) in longitudinal clinical narratives. The longitudinal nature of clinical narratives calls particular attention to details of information that, while benign on their own in separate records, can lead to identification of patients in combination in longitudinal records. Accordingly, the 2014 de-identification track addressed a broader set of entities and PHI than covered by the Health Insurance Portability and Accountability Act – the focus of the de-identification shared task that was organized in 2006. Ten teams tackled the 2014 de-identification task and submitted 22 system outputs for evaluation. Each team was evaluated on their best performing system output. Three of the 10 systems achieved F1 scores over .90, and seven of the top 10 scored over .75. The most successful systems combined conditional random fields and hand-written rules. Our findings indicate that automated systems can be very effective for this task, but that de-identification is not yet a solved problem.,},
	number = {Suppl},
	urldate = {2020-04-27},
	journal = {Journal of biomedical informatics},
	author = {Stubbs, Amber and Kotfila, Christopher and Uzuner, Ozlem},
	month = dec,
	year = {2015},
	pmid = {26225918},
	pmcid = {PMC4989908},
	pages = {S11--S19}
}

@article{sun_evaluating_2013,
	title = {Evaluating temporal relations in clinical text: 2012 i2b2 {Challenge}},
	volume = {20},
	issn = {1067-5027},
	shorttitle = {Evaluating temporal relations in clinical text},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3756273/},
	doi = {10.1136/amiajnl-2013-001628},
	abstract = {Background
The Sixth Informatics for Integrating Biology and the Bedside (i2b2) Natural Language Processing Challenge for Clinical Records focused on the temporal relations in clinical narratives. The organizers provided the research community with a corpus of discharge summaries annotated with temporal information, to be used for the development and evaluation of temporal reasoning systems. 18 teams from around the world participated in the challenge. During the workshop, participating teams presented comprehensive reviews and analysis of their systems, and outlined future research directions suggested by the challenge contributions.

Methods
The challenge evaluated systems on the information extraction tasks that targeted: (1) clinically significant events, including both clinical concepts such as problems, tests, treatments, and clinical departments, and events relevant to the patient's clinical timeline, such as admissions, transfers between departments, etc; (2) temporal expressions, referring to the dates, times, durations, or frequencies phrases in the clinical text. The values of the extracted temporal expressions had to be normalized to an ISO specification standard; and (3) temporal relations, between the clinical events and temporal expressions. Participants determined pairs of events and temporal expressions that exhibited a temporal relation, and identified the temporal relation between them.

Results
For event detection, statistical machine learning (ML) methods consistently showed superior performance. While ML and rule based methods seemed to detect temporal expressions equally well, the best systems overwhelmingly adopted a rule based approach for value normalization. For temporal relation classification, the systems using hybrid approaches that combined ML and heuristics based methods produced the best results.},
	number = {5},
	urldate = {2020-04-27},
	journal = {Journal of the American Medical Informatics Association : JAMIA},
	author = {Sun, Weiyi and Rumshisky, Anna and Uzuner, Ozlem},
	month = sep,
	year = {2013},
	pmid = {23564629},
	pmcid = {PMC3756273},
	pages = {806--813}
}

@article{sun_annotating_2013,
	title = {Annotating temporal information in clinical narratives},
	volume = {46 Suppl},
	issn = {1532-0480},
	doi = {10.1016/j.jbi.2013.07.004},
	abstract = {Temporal information in clinical narratives plays an important role in patients' diagnosis, treatment and prognosis. In order to represent narrative information accurately, medical natural language processing (MLP) systems need to correctly identify and interpret temporal information. To promote research in this area, the Informatics for Integrating Biology and the Bedside (i2b2) project developed a temporally annotated corpus of clinical narratives. This corpus contains 310 de-identified discharge summaries, with annotations of clinical events, temporal expressions and temporal relations. This paper describes the process followed for the development of this corpus and discusses annotation guideline development, annotation methodology, and corpus quality.},
	language = {eng},
	journal = {Journal of Biomedical Informatics},
	author = {Sun, Weiyi and Rumshisky, Anna and Uzuner, Ozlem},
	month = dec,
	year = {2013},
	pmid = {23872518},
	pmcid = {PMC3855581},
	keywords = {Annotation, Corpus Building, Documentation, Electronic Health Records, Humans, Medical Informatics, Narration, Natural Language Processing, Temporal Reasoning},
	pages = {S5--12}
}

@article{paperno2016lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  journal={arXiv preprint arXiv:1606.06031},
  year={2016}
}

@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

@article{uzuner_2010_2011,
	title = {2010 i2b2/{VA} challenge on concepts, assertions, and relations in clinical text},
	volume = {18},
	issn = {1067-5027},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3168320/},
	doi = {10.1136/amiajnl-2011-000203},
	abstract = {The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for Clinical Records presented three tasks: a concept extraction task focused on the extraction of medical concepts from patient reports; an assertion classification task focused on assigning assertion types for medical problem concepts; and a relation classification task focused on assigning relation types that hold between medical problems, tests, and treatments. i2b2 and the VA provided an annotated reference standard corpus for the three tasks. Using this reference standard, 22 systems were developed for concept extraction, 21 for assertion classification, and 16 for relation classification., These systems showed that machine learning approaches could be augmented with rule-based systems to determine concepts, assertions, and relations. Depending on the task, the rule-based systems can either provide input for machine learning or post-process the output of machine learning. Ensembles of classifiers, information from unlabeled data, and external knowledge sources can help when the training data are inadequate.},
	number = {5},
	urldate = {2020-04-27},
	journal = {Journal of the American Medical Informatics Association : JAMIA},
	author = {Uzuner, Özlem and South, Brett R and Shen, Shuying and DuVall, Scott L},
	year = {2011},
	pmid = {21685143},
	pmcid = {PMC3168320},
	pages = {552--556}
}

@article{uzuner_evaluating_2007,
	title = {Evaluating the {State}-of-the-{Art} in {Automatic} {De}-identification},
	volume = {14},
	issn = {1067-5027},
	url = {https://academic.oup.com/jamia/article/14/5/550/720189},
	doi = {10.1197/jamia.M2444},
	abstract = {Abstract.  To facilitate and survey studies in automatic de-identification, as a part of the i2b2 (Informatics for Integrating Biology to the Bedside) project,},
	language = {en},
	number = {5},
	urldate = {2020-04-27},
	journal = {Journal of the American Medical Informatics Association},
	author = {Uzuner, Özlem and Luo, Yuan and Szolovits, Peter},
	month = sep,
	year = {2007},
	note = {Publisher: Oxford Academic},
	pages = {550--563}
}

@article{dogan_ncbi_2014,
	title = {{NCBI} {Disease} {Corpus}: {A} {Resource} for {Disease} {Name} {Recognition} and {Concept} {Normalization}},
	volume = {47},
	issn = {1532-0464},
	shorttitle = {{NCBI} {Disease} {Corpus}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3951655/},
	doi = {10.1016/j.jbi.2013.12.006},
	abstract = {Information encoded in natural language in biomedical literature publications is only useful if efficient and reliable ways of accessing and analyzing that information are available. Natural language processing and text mining tools are therefore essential for extracting valuable information, however, the development of powerful, highly effective tools to automatically detect central biomedical concepts such as diseases is conditional on the availability of annotated corpora., This paper presents the disease name and concept annotations of the NCBI disease corpus, a collection of 793 PubMed abstracts fully annotated at the mention and concept level to serve as a research resource for the biomedical natural language processing community. Each PubMed abstract was manually annotated by two annotators with disease mentions and their corresponding concepts in Medical Subject Headings (MeSH®) or Online Mendelian Inheritance in Man (OMIM®). Manual curation was performed using PubTator, which allowed the use of pre-annotations as a pre-step to manual annotations. Fourteen annotators were randomly paired and differing annotations were discussed for reaching a consensus in two annotation phases. In this setting, a high inter-annotator agreement was observed. Finally, all results were checked against annotations of the rest of the corpus to assure corpus-wide consistency., The public release of the NCBI disease corpus contains 6,892 disease mentions, which are mapped to 790 unique disease concepts. Of these, 88\% link to a MeSH identifier, while the rest contain an OMIM identifier. We were able to link 91\% of the mentions to a single disease concept, while the rest are described as a combination of concepts. In order to help researchers use the corpus to design and test disease identification methods, we have prepared the corpus as training, testing and development sets. To demonstrate its utility, we conducted a benchmarking experiment where we compared three different knowledge-based disease normalization methods with a best performance in F-measure of 63.7\%. These results show that the NCBI disease corpus has the potential to significantly improve the state-of-the-art in disease name recognition and normalization research, by providing a high-quality gold standard thus enabling the development of machine-learning based approaches for such tasks.},
	urldate = {2020-04-27},
	journal = {Journal of biomedical informatics},
	author = {Doğan, Rezarta Islamaj and Leaman, Robert and Lu, Zhiyong},
	month = feb,
	year = {2014},
	pmid = {24393765},
	pmcid = {PMC3951655},
	pages = {1--10}
}

@inproceedings{collier_introduction_2004,
	address = {Geneva, Switzerland},
	title = {Introduction to the {Bio}-entity {Recognition} {Task} at {JNLPBA}},
	url = {https://www.aclweb.org/anthology/W04-1213},
	urldate = {2020-04-27},
	booktitle = {Proceedings of the {International} {Joint} {Workshop} on {Natural} {Language} {Processing} in {Biomedicine} and its {Applications} ({NLPBA}/{BioNLP})},
	publisher = {COLING},
	author = {Collier, Nigel and Kim, Jin-Dong},
	month = aug,
	year = {2004},
	pages = {73--78}
}

@article{li_biocreative_2016,
	title = {{BioCreative} {V} {CDR} task corpus: a resource for chemical disease relation extraction},
	volume = {2016},
	issn = {1758-0463},
	shorttitle = {{BioCreative} {V} {CDR} task corpus},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4860626/},
	doi = {10.1093/database/baw068},
	abstract = {Community-run, formal evaluations and manually annotated text corpora are critically important for advancing biomedical text-mining research. Recently in BioCreative V, a new challenge was organized for the tasks of disease named entity recognition (DNER) and chemical-induced disease (CID) relation extraction. Given the nature of both tasks, a test collection is required to contain both disease/chemical annotations and relation annotations in the same set of articles. Despite previous efforts in biomedical corpus construction, none was found to be sufficient for the task. Thus, we developed our own corpus called BC5CDR during the challenge by inviting a team of Medical Subject Headings (MeSH) indexers for disease/chemical entity annotation and Comparative Toxicogenomics Database (CTD) curators for CID relation annotation. To ensure high annotation quality and productivity, detailed annotation guidelines and automatic annotation tools were provided. The resulting BC5CDR corpus consists of 1500 PubMed articles with 4409 annotated chemicals, 5818 diseases and 3116 chemical-disease interactions. Each entity annotation includes both the mention text spans and normalized concept identifiers, using MeSH as the controlled vocabulary. To ensure accuracy, the entities were first captured independently by two annotators followed by a consensus annotation: The average inter-annotator agreement (IAA) scores were 87.49\% and 96.05\% for the disease and chemicals, respectively, in the test set according to the Jaccard similarity coefficient. Our corpus was successfully used for the BioCreative V challenge tasks and should serve as a valuable resource for the text-mining research community., Database URL: http://www.biocreative.org/tasks/biocreative-v/track-3-cdr/},
	urldate = {2020-04-27},
	journal = {Database: The Journal of Biological Databases and Curation},
	author = {Li, Jiao and Sun, Yueping and Johnson, Robin J. and Sciaky, Daniela and Wei, Chih-Hsuan and Leaman, Robert and Davis, Allan Peter and Mattingly, Carolyn J. and Wiegers, Thomas C. and Lu, Zhiyong},
	month = may,
	year = {2016},
	pmid = {27161011},
	pmcid = {PMC4860626}
}

@inproceedings{alsentzer_publicly_2019,
	address = {Minneapolis, Minnesota, USA},
	title = {Publicly {Available} {Clinical} {BERT} {Embeddings}},
	url = {https://www.aclweb.org/anthology/W19-1909},
	doi = {10.18653/v1/W19-1909},
	abstract = {Contextual word embedding models such as ELMo and BERT have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on 3/5 clinical NLP tasks, establishing a new state-of-the-art on the MedNLI dataset. We find that these domain-specific models are not as performant on 2 clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text.},
	booktitle = {Proceedings of the 2nd {Clinical} {Natural} {Language} {Processing} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Alsentzer, Emily and Murphy, John and Boag, William and Weng, Wei-Hung and Jindi, Di and Naumann, Tristan and McDermott, Matthew},
	month = jun,
	year = {2019},
	pages = {72--78}
}

@inproceedings{beltagy_scibert_2019,
	address = {Hong Kong, China},
	title = {{SciBERT}: {A} {Pretrained} {Language} {Model} for {Scientific} {Text}},
	shorttitle = {{SciBERT}},
	url = {https://www.aclweb.org/anthology/D19-1371},
	doi = {10.18653/v1/D19-1371},
	abstract = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.},
	urldate = {2020-04-27},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
	month = nov,
	year = {2019},
	pages = {3615--3620}
}

@article{lee_biobert_2019,
	title = {{BioBERT}: a pre-trained biomedical language representation model for biomedical text mining},
	volume = {36},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/btz682},
	doi = {10.1093/bioinformatics/btz682},
	abstract = {Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62\% F1 score improvement), biomedical relation extraction (2.80\% F1 score improvement) and biomedical question answering (12.24\% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts.We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.},
	number = {4},
	journal = {Bioinformatics},
	author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
	year = {2019},
	note = {\_eprint: https://academic.oup.com/bioinformatics/article-pdf/36/4/1234/32527770/btz682.pdf},
	pages = {1234--1240}
}

@article{clark_transformers_2020,
	title = {Transformers as {Soft} {Reasoners} over {Language}},
	url = {http://arxiv.org/abs/2002.05867},
	abstract = {AI has long pursued the goal of having systems reason over *explicitly provided* knowledge, but building suitable representations has proved challenging. Here we explore whether transformers can similarly learn to reason (or emulate reasoning), but using rules expressed in language, thus bypassing a formal representation. We provide the first demonstration that this is possible, and characterize the extent of this capability. To do this, we use a collection of synthetic datasets that test increasing levels of reasoning complexity (number of rules, presence of negation, and depth of chaining). We find transformers appear to learn rule-based reasoning with high (99\%) accuracy on these datasets, and in a way that generalizes to test data requiring substantially deeper chaining than in the training data (95\%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as a limited "soft theorem prover" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering. All datasets and a live demo are available at http://rule-reasoning.apps.allenai.org/},
	urldate = {2020-02-27},
	journal = {arXiv:2002.05867 [cs]},
	author = {Clark, Peter and Tafjord, Oyvind and Richardson, Kyle},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.05867},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@incollection{wang_superglue_2019,
	title = {{SuperGLUE}: {A} {Stickier} {Benchmark} for {General}-{Purpose} {Language} {Understanding} {Systems}},
	url = {https://arxiv.org/abs/1905.00537},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {3261--3275}
}

@article{li_dont_2019,
	title = {Don't {Say} {That}! {Making} {Inconsistent} {Dialogue} {Unlikely} with {Unlikelihood} {Training}},
	url = {http://arxiv.org/abs/1911.03860},
	abstract = {Generative dialogue models currently suffer from a number of problems which standard maximum likelihood training does not address. They tend to produce generations that (i) rely too much on copying from the context, (ii) contain repetitions within utterances, (iii) overuse frequent words, and (iv) at a deeper level, contain logical flaws. In this work we show how all of these problems can be addressed by extending the recently introduced unlikelihood loss (Welleck et al., 2019) to these cases. We show that appropriate loss functions which regularize generated outputs to match human distributions are effective for the first three issues. For the last important general issue, we show applying unlikelihood to collected data of what a model should not do is effective for improving logical consistency, potentially paving the way to generative models with greater reasoning ability. We demonstrate the efficacy of our approach across several dialogue tasks.},
	urldate = {2020-01-24},
	journal = {arXiv:1911.03860 [cs]},
	author = {Li, Margaret and Roller, Stephen and Kulikov, Ilia and Welleck, Sean and Boureau, Y.-Lan and Cho, Kyunghyun and Weston, Jason},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.03860},
	keywords = {Computer Science - Computation and Language}
}

@inproceedings{artetxe_margin-based_2019,
	address = {Florence, Italy},
	title = {Margin-based {Parallel} {Corpus} {Mining} with {Multilingual} {Sentence} {Embeddings}},
	url = {https://www.aclweb.org/anthology/P19-1309},
	doi = {10.18653/v1/P19-1309},
	abstract = {Machine translation is highly sensitive to the size and quality of the training data, which has led to an increasing interest in collecting and filtering large parallel corpora. In this paper, we propose a new method for this task based on multilingual sentence embeddings. In contrast to previous approaches, which rely on nearest neighbor retrieval with a hard threshold over cosine similarity, our proposed method accounts for the scale inconsistencies of this measure, considering the margin between a given sentence pair and its closest candidates instead. Our experiments show large improvements over existing methods. We outperform the best published results on the BUCC mining task and the UN reconstruction task by more than 10 F1 and 30 precision points, respectively. Filtering the English-German ParaCrawl corpus with our approach, we obtain 31.2 BLEU points on newstest2014, an improvement of more than one point over the best official filtered version.},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Artetxe, Mikel and Schwenk, Holger},
	month = jul,
	year = {2019},
	pages = {3197--3203}
}

@article{artetxe_cross-lingual_2019,
	title = {On the {Cross}-lingual {Transferability} of {Monolingual} {Representations}},
	url = {http://arxiv.org/abs/1910.11856},
	abstract = {State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective -freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.},
	urldate = {2019-12-09},
	journal = {arXiv:1910.11856 [cs]},
	author = {Artetxe, Mikel and Ruder, Sebastian and Yogatama, Dani},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.11856},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@inproceedings{devlin_bert:_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://www.aclweb.org/anthology/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186}
}

@inproceedings{alberti_synthetic_2019,
	address = {Florence, Italy},
	title = {Synthetic {QA} {Corpora} {Generation} with {Roundtrip} {Consistency}},
	url = {https://www.aclweb.org/anthology/P19-1620},
	doi = {10.18653/v1/P19-1620},
	abstract = {We introduce a novel method of generating synthetic question answering corpora by combining models of question generation and answer extraction, and by filtering the results to ensure roundtrip consistency. By pretraining on the resulting corpora we obtain significant improvements on SQuAD2 and NQ, establishing a new state-of-the-art on the latter. Our synthetic data generation models, for both question generation and answer extraction, can be fully reproduced by finetuning a publicly available BERT model on the extractive subsets of SQuAD2 and NQ. We also describe a more powerful variant that does full sequence-to-sequence pretraining for question generation, obtaining exact match and F1 at less than 0.1\% and 0.4\% from human performance on SQuAD2.},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Alberti, Chris and Andor, Daniel and Pitler, Emily and Devlin, Jacob and Collins, Michael},
	month = jul,
	year = {2019},
	pages = {6168--6173}
}

@incollection{hermann_teaching_2015,
	title = {Teaching {Machines} to {Read} and {Comprehend}},
	url = {http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {1693--1701}
}

@article{johnson_billion:_2017,
  title={Billion-scale similarity search with GPUs},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={CoRR},
  volume={abs/1702.08734},
  year={2017},
  url={https://arxiv.org/abs/1702.08734}
}

@inproceedings{joshi_triviaqa:_2017,
	address = {Vancouver, Canada},
	title = {{TriviaQA}: {A} {Large} {Scale} {Distantly} {Supervised} {Challenge} {Dataset} for {Reading} {Comprehension}},
	shorttitle = {{TriviaQA}},
	url = {https://www.aclweb.org/anthology/P17-1147},
	doi = {10.18653/v1/P17-1147},
	abstract = {We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23\% and 40\% vs. 80\%), suggesting that TriviaQA is a challenging testbed that is worth significant future study.},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Joshi, Mandar and Choi, Eunsol and Weld, Daniel and Zettlemoyer, Luke},
	month = jul,
	year = {2017},
	pages = {1601--1611}
}

@inproceedings{khandelwal2020generalization,
    title={Generalization through Memorization: Nearest Neighbor Language Models},
    author={Urvashi Khandelwal and Omer Levy and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=HklBjCEKvH}
}

@article{lample_cross-lingual_2019,
	title = {Cross-lingual {Language} {Model} {Pretraining}},
	url = {http://arxiv.org/abs/1901.07291},
	abstract = {Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9\% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.},
	urldate = {2019-12-09},
	journal = {arXiv:1901.07291 [cs]},
	author = {Lample, Guillaume and Conneau, Alexis},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.07291},
	keywords = {Computer Science - Computation and Language}
}

@inproceedings{lee_semi-supervised_2018,
	address = {Miyazaki, Japan},
	title = {Semi-supervised {Training} {Data} {Generation} for {Multilingual} {Question} {Answering}},
	url = {https://www.aclweb.org/anthology/L18-1437},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the {Eleventh} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC} 2018)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Lee, Kyungjae and Yoon, Kyoungho and Park, Sunghyun and Hwang, Seung-won},
	month = may,
	year = {2018}
}

@inproceedings{lewis_unsupervised_2019,
	address = {Florence, Italy},
	title = {Unsupervised {Question} {Answering} by {Cloze} {Translation}},
	url = {https://www.aclweb.org/anthology/P19-1484},
	doi = {10.18653/v1/P19-1484},
	abstract = {Obtaining training data for Question Answering (QA) is time-consuming and resource-intensive, and existing QA datasets are only available for limited domains and languages. In this work, we explore to what extent high quality training data is actually required for Extractive QA, and investigate the possibility of unsupervised Extractive QA. We approach this problem by first learning to generate context, question and answer triples in an unsupervised manner, which we then use to synthesize Extractive QA training data automatically. To generate such triples, we first sample random context paragraphs from a large corpus of documents and then random noun phrases or Named Entity mentions from these paragraphs as answers. Next we convert answers in context to “fill-in-the-blank” cloze questions and finally translate them into natural questions. We propose and compare various unsupervised ways to perform cloze-to-natural question translation, including training an unsupervised NMT model using non-aligned corpora of natural questions and cloze questions as well as a rule-based approach. We find that modern QA models can learn to answer human questions surprisingly well using only synthetic training data. We demonstrate that, without using the SQuAD training data at all, our approach achieves 56.4 F1 on SQuAD v1 (64.5 F1 when the answer is a Named Entity mention), outperforming early supervised models.},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Lewis, Patrick and Denoyer, Ludovic and Riedel, Sebastian},
	month = jul,
	year = {2019},
	pages = {4896--4910}
}

@inproceedings{rajpurkar_know_2018,
	address = {Melbourne, Australia},
	title = {Know {What} {You} {Don}'t {Know}: {Unanswerable} {Questions} for {SQuAD}},
	shorttitle = {Know {What} {You} {Don}'t {Know}},
	url = {https://www.aclweb.org/anthology/P18-2124},
	doi = {10.18653/v1/P18-2124},
	abstract = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on SQuAD achieves only 66\% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
	month = jul,
	year = {2018},
	pages = {784--789}
}

@inproceedings{rajpurkar_squad:_2016,
	address = {Austin, Texas},
	title = {{SQuAD}: 100,000+ {Questions} for {Machine} {Comprehension} of {Text}},
	shorttitle = {{SQuAD}},
	url = {https://www.aclweb.org/anthology/D16-1264},
	doi = {10.18653/v1/D16-1264},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	month = nov,
	year = {2016},
	pages = {2383--2392}
}

@inproceedings{trischler_newsqa:_2017,
	address = {Vancouver, Canada},
	title = {{NewsQA}: {A} {Machine} {Comprehension} {Dataset}},
	shorttitle = {{NewsQA}},
	url = {https://www.aclweb.org/anthology/W17-2623},
	doi = {10.18653/v1/W17-2623},
	abstract = {We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text in the articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. Analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (13.3\% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available online.},
	urldate = {2019-12-09},
	booktitle = {Proceedings of the 2nd {Workshop} on {Representation} {Learning} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Trischler, Adam and Wang, Tong and Yuan, Xingdi and Harris, Justin and Sordoni, Alessandro and Bachman, Philip and Suleman, Kaheer},
	month = aug,
	year = {2017},
	pages = {191--200}
}
@article{botha_learning_2018,
	title = {Learning {To} {Split} and {Rephrase} {From} {Wikipedia} {Edit} {History}},
	url = {http://arxiv.org/abs/1808.09468},
	abstract = {Split and rephrase is the task of breaking down a sentence into shorter ones that together convey the same meaning. We extract a rich new dataset for this task by mining Wikipedia's edit history: WikiSplit contains one million naturally occurring sentence rewrites, providing sixty times more distinct split examples and a ninety times larger vocabulary than the WebSplit corpus introduced by Narayan et al. (2017) as a benchmark for this task. Incorporating WikiSplit as training data produces a model with qualitatively better predictions that score 32 BLEU points above the prior best result on the WebSplit benchmark.},
	urldate = {2019-11-20},
	journal = {arXiv:1808.09468 [cs]},
	author = {Botha, Jan A. and Faruqui, Manaal and Alex, John and Baldridge, Jason and Das, Dipanjan},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.09468},
	keywords = {Computer Science - Computation and Language}
}

@article{geva_discofuse:_2019,
	title = {{DiscoFuse}: {A} {Large}-{Scale} {Dataset} for {Discourse}-{Based} {Sentence} {Fusion}},
	shorttitle = {{DiscoFuse}},
	url = {http://arxiv.org/abs/1902.10526},
	abstract = {Sentence fusion is the task of joining several independent sentences into a single coherent text. Current datasets for sentence fusion are small and insufficient for training modern neural models. In this paper, we propose a method for automatically-generating fusion examples from raw text and present DiscoFuse, a large scale dataset for discourse-based sentence fusion. We author a set of rules for identifying a diverse set of discourse phenomena in raw text, and decomposing the text into two independent sentences. We apply our approach on two document collections: Wikipedia and Sports articles, yielding 60 million fusion examples annotated with discourse information required to reconstruct the fused text. We develop a sequence-to-sequence model on DiscoFuse and thoroughly analyze its strengths and weaknesses with respect to the various discourse phenomena, using both automatic as well as human evaluation. Finally, we conduct transfer learning experiments with WebSplit, a recent dataset for text simplification. We show that pretraining on DiscoFuse substantially improves performance on WebSplit when viewed as a sentence fusion task.},
	urldate = {2019-11-20},
	journal = {arXiv:1902.10526 [cs]},
	author = {Geva, Mor and Malmi, Eric and Szpektor, Idan and Berant, Jonathan},
	month = mar,
	year = {2019},
	note = {arXiv: 1902.10526},
	keywords = {Computer Science - Computation and Language}
}

@article{nie_adversarial_2019,
	title = {Adversarial {NLI}: {A} {New} {Benchmark} for {Natural} {Language} {Understanding}},
	shorttitle = {Adversarial {NLI}},
	url = {http://arxiv.org/abs/1910.14599},
	abstract = {We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.},
	urldate = {2019-11-20},
	journal = {arXiv:1910.14599 [cs]},
	author = {Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.14599},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@inproceedings{stahlberg_nmt_2019,
	address = {Hong Kong, China},
	title = {On {NMT} {Search} {Errors} and {Model} {Errors}: {Cat} {Got} {Your} {Tongue}?},
	shorttitle = {On {NMT} {Search} {Errors} and {Model} {Errors}},
	url = {https://www.aclweb.org/anthology/D19-1331},
	doi = {10.18653/v1/D19-1331},
	abstract = {We report on search errors and model errors in neural machine translation (NMT). We present an exact inference procedure for neural sequence models based on a combination of beam search and depth-first search. We use our exact search to find the global best model scores under a Transformer base model for the entire WMT15 English-German test set. Surprisingly, beam search fails to find these global best model scores in most cases, even with a very large beam size of 100. For more than 50\% of the sentences, the model in fact assigns its global best score to the empty translation, revealing a massive failure of neural models in properly accounting for adequacy. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations. We conclude that vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modelling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation.},
	urldate = {2019-11-20},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Stahlberg, Felix and Byrne, Bill},
	month = nov,
	year = {2019},
	pages = {3347--3353}
}

@inproceedings{jing_bipar:_2019,
	address = {Hong Kong, China},
	title = {{BiPaR}: {A} {Bilingual} {Parallel} {Dataset} for {Multilingual} and {Cross}-lingual {Reading} {Comprehension} on {Novels}},
	shorttitle = {{BiPaR}},
	url = {https://www.aclweb.org/anthology/D19-1249},
	doi = {10.18653/v1/D19-1249},
	abstract = {This paper presents BiPaR, a bilingual parallel novel-style machine reading comprehension (MRC) dataset, developed to support multilingual and cross-lingual reading comprehension. The biggest difference between BiPaR and existing reading comprehension datasets is that each triple (Passage, Question, Answer) in BiPaR is written parallelly in two languages. We collect 3,667 bilingual parallel paragraphs from Chinese and English novels, from which we construct 14,668 parallel question-answer pairs via crowdsourced workers following a strict quality control procedure. We analyze BiPaR in depth and find that BiPaR offers good diversification in prefixes of questions, answer types and relationships between questions and passages. We also observe that answering questions of novels requires reading comprehension skills of coreference resolution, multi-sentence reasoning, and understanding of implicit causality, etc. With BiPaR, we build monolingual, multilingual, and cross-lingual MRC baseline models. Even for the relatively simple monolingual MRC on this dataset, experiments show that a strong BERT baseline is over 30 points behind human in terms of both EM and F1 score, indicating that BiPaR provides a challenging testbed for monolingual, multilingual and cross-lingual MRC on novels. The dataset is available at https://multinlp.github.io/BiPaR/.},
	urldate = {2019-11-07},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Jing, Yimin and Xiong, Deyi and Yan, Zhen},
	month = nov,
	year = {2019},
	pages = {2452--2462}
}

@inproceedings{hsu_zero-shot_2019,
	address = {Hong Kong, China},
	title = {Zero-shot {Reading} {Comprehension} by {Cross}-lingual {Transfer} {Learning} with {Multi}-lingual {Language} {Representation} {Model}},
	url = {https://www.aclweb.org/anthology/D19-1607},
	doi = {10.18653/v1/D19-1607},
	abstract = {Because it is not feasible to collect training data for every language, there is a growing interest in cross-lingual transfer learning. In this paper, we systematically explore zero-shot cross-lingual transfer learning on reading comprehension tasks with language representation model pre-trained on multi-lingual corpus. The experimental results show that with pre-trained language representation zero-shot learning is feasible, and translating the source data into the target language is not necessary and even degrades the performance. We further explore what does the model learn in zero-shot setting.},
	urldate = {2019-11-07},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Hsu, Tsung-Yuan and Liu, Chi-Liang and Lee, Hung-yi},
	month = nov,
	year = {2019},
	pages = {5935--5942}
}

@inproceedings{lee_learning_2019,
	address = {Hong Kong, China},
	title = {Learning with {Limited} {Data} for {Multilingual} {Reading} {Comprehension}},
	url = {https://www.aclweb.org/anthology/D19-1283},
	doi = {10.18653/v1/D19-1283},
	abstract = {This paper studies the problem of supporting question answering in a new language with limited training resources. As an extreme scenario, when no such resource exists, one can (1) transfer labels from another language, and (2) generate labels from unlabeled data, using translator and automatic labeling function respectively. However, these approaches inevitably introduce noises to the training data, due to translation or generation errors, which require a judicious use of data with varying confidence. To address this challenge, we propose a weakly-supervised framework that quantifies such noises from automatically generated labels, to deemphasize or fix noisy data in training. On reading comprehension task, we demonstrate the effectiveness of our model on low-resource languages with varying similarity to English, namely, Korean and French.},
	urldate = {2019-11-07},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Lee, Kyungjae and Park, Sunghyun and Han, Hojae and Yeo, Jinyoung and Hwang, Seung-won and Lee, Juho},
	month = nov,
	year = {2019},
	pages = {2833--2843}
}

@inproceedings{cui_cross-lingual_2019,
	address = {Hong Kong, China},
	title = {Cross-{Lingual} {Machine} {Reading} {Comprehension}},
	url = {https://www.aclweb.org/anthology/D19-1169},
	doi = {10.18653/v1/D19-1169},
	abstract = {Though the community has made great progress on Machine Reading Comprehension (MRC) task, most of the previous works are solving English-based MRC problems, and there are few efforts on other languages mainly due to the lack of large-scale training data.In this paper, we propose Cross-Lingual Machine Reading Comprehension (CLMRC) task for the languages other than English. Firstly, we present several back-translation approaches for CLMRC task which is straightforward to adopt. However, to exactly align the answer into source language is difficult and could introduce additional noise. In this context, we propose a novel model called Dual BERT, which takes advantage of the large-scale training data provided by rich-resource language (such as English) and learn the semantic relations between the passage and question in bilingual context, and then utilize the learned knowledge to improve reading comprehension performance of low-resource language. We conduct experiments on two Chinese machine reading comprehension datasets CMRC 2018 and DRCD. The results show consistent and significant improvements over various state-of-the-art systems by a large margin, which demonstrate the potentials in CLMRC task. Resources available: https://github.com/ymcui/Cross-Lingual-MRC},
	urldate = {2019-11-07},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Wang, Shijin and Hu, Guoping},
	month = nov,
	year = {2019},
	pages = {1586--1595}
}

@article{singh_xlda:_2019,
	title = {{XLDA}: {Cross}-{Lingual} {Data} {Augmentation} for {Natural} {Language} {Inference} and {Question} {Answering}},
	shorttitle = {{XLDA}},
	url = {http://arxiv.org/abs/1905.11471},
	abstract = {While natural language processing systems often focus on a single language, multilingual transfer learning has the potential to improve performance, especially for low-resource languages. We introduce XLDA, cross-lingual data augmentation, a method that replaces a segment of the input text with its translation in another language. XLDA enhances performance of all 14 tested languages of the cross-lingual natural language inference (XNLI) benchmark. With improvements of up to \$4.8{\textbackslash}\%\$, training with XLDA achieves state-of-the-art performance for Greek, Turkish, and Urdu. XLDA is in contrast to, and performs markedly better than, a more naive approach that aggregates examples in various languages in a way that each example is solely in one language. On the SQuAD question answering task, we see that XLDA provides a \$1.0{\textbackslash}\%\$ performance increase on the English evaluation set. Comprehensive experiments suggest that most languages are effective as cross-lingual augmentors, that XLDA is robust to a wide range of translation quality, and that XLDA is even more effective for randomly initialized models than for pretrained models.},
	urldate = {2019-11-07},
	journal = {arXiv:1905.11471 [cs]},
	author = {Singh, Jasdeep and McCann, Bryan and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
	month = may,
	year = {2019},
	note = {arXiv: 1905.11471},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@inproceedings{gupta_mmqa:_2018,
	title = {{MMQA}: {A} {Multi}-domain {Multi}-lingual {Question}-{Answering} {Framework} for {English} and {Hindi}},
	shorttitle = {{MMQA}},
	abstract = {In this paper, we assess the challenges for multi-domain, multi-lingual question answering, create necessary resources for benchmarking and develop a baseline model. We curate 500 articles in six different domains from the web. These articles form a comparable corpora of 250 English documents and 250 Hindi documents. From these comparable corpora, we have created 5, 495 question-answer pairs with the questions and answers, both being in English and Hindi. The question can be both factoid or short descriptive types. The answers are categorized in 6 coarse and 63 finer types. To the best of our knowledge, this is the very first attempt towards creating multi-domain, multi-lingual question answering evaluation involving English and Hindi. We develop a deep learning based model for classifying an input question into the coarse and finer categories depending upon the expected answer. Answers are extracted through similarity computation and subsequent ranking. For factoid question, we obtain an MRR value of 49.10\% and for short descriptive question, we obtain a BLEU score of 41.37\%. Evaluation of question classification model shows the accuracies of 90.12\% and 80.30\% for coarse and finer classes, respectively.},
	booktitle = {{LREC}},
	author = {Gupta, Deepak and Kumari, Surabhi and Ekbal, Asif and Bhattacharyya, Pushpak},
	year = {2018},
	keywords = {BLEU, Baseline (configuration management), Categorization, Computation, Deep learning, Google Questions and Answers, Question answering, Text corpus}
}

@book{winograd_understanding_1972,
	address = {Orlando, FL, USA},
	title = {Understanding {Natural} {Language}},
	isbn = {0-12-759750-6},
	publisher = {Academic Press, Inc.},
	author = {Winograd, Terry},
	year = {1972}
}

@inproceedings{levesque_winograd_2011,
	title = {The {Winograd} {Schema} {Challenge}},
	booktitle = {{AAAI} {Spring} {Symposium}: {Logical} {Formalizations} of {Commonsense} {Reasoning}},
	author = {Levesque, Hector J.},
	year = {2011}
}

@article{bajaj_ms_2016,
	title = {{MS} {MARCO}: {A} {Human} {Generated} {MAchine} {Reading} {COmprehension} {Dataset}},
	shorttitle = {{MS} {MARCO}},
	url = {http://arxiv.org/abs/1611.09268},
	abstract = {We introduce a large scale MAchine Reading COmprehension dataset, which we name MS MARCO. The dataset comprises of 1,010,916 anonymized questions---sampled from Bing's search query logs---each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages---extracted from 3,563,535 web documents retrieved by Bing---that provide the information necessary for curating the natural language answers. A question in the MS MARCO dataset may have multiple answers or no answers at all. Using this dataset, we propose three different tasks with varying levels of difficulty: (i) predict if a question is answerable given a set of context passages, and extract and synthesize the answer as a human would (ii) generate a well-formed answer (if possible) based on the context passages that can be understood with the question and passage context, and finally (iii) rank a set of retrieved passages given a question. The size of the dataset and the fact that the questions are derived from real user search queries distinguishes MS MARCO from other well-known publicly available datasets for machine reading comprehension and question-answering. We believe that the scale and the real-world nature of this dataset makes it attractive for benchmarking machine reading comprehension and question-answering models.},
	urldate = {2019-09-26},
	journal = {arXiv:1611.09268 [cs]},
	author = {Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and Rosenberg, Mir and Song, Xia and Stoica, Alina and Tiwary, Saurabh and Wang, Tong},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.09268},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval}
}

@inproceedings{chen_reading_2017,
	address = {Vancouver, Canada},
	title = {Reading {Wikipedia} to {Answer} {Open}-{Domain} {Questions}},
	url = {https://www.aclweb.org/anthology/P17-1171},
	doi = {10.18653/v1/P17-1171},
	abstract = {This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},
	month = jul,
	year = {2017},
	pages = {1870--1879}
}

@inproceedings{klementiev_inducing_2012,
	address = {Mumbai, India},
	title = {Inducing {Crosslingual} {Distributed} {Representations} of {Words}},
	url = {https://www.aclweb.org/anthology/C12-1089},
	urldate = {2019-09-26},
	booktitle = {Proceedings of {COLING} 2012},
	publisher = {The COLING 2012 Organizing Committee},
	author = {Klementiev, Alexandre and Titov, Ivan and Bhattarai, Binod},
	month = dec,
	year = {2012},
	pages = {1459--1474}
}

@inproceedings{gan_improving_2019,
	address = {Florence, Italy},
	title = {Improving the {Robustness} of {Question} {Answering} {Systems} to {Question} {Paraphrasing}},
	url = {https://www.aclweb.org/anthology/P19-1610},
	doi = {10.18653/v1/P19-1610},
	abstract = {Despite the advancement of question answering (QA) systems and rapid improvements on held-out test sets, their generalizability is a topic of concern. We explore the robustness of QA models to question paraphrasing by creating two test sets consisting of paraphrased SQuAD questions. Paraphrased questions from the first test set are very similar to the original questions designed to test QA models' over-sensitivity, while questions from the second test set are paraphrased using context words near an incorrect answer candidate in an attempt to confuse QA models. We show that both paraphrased test sets lead to significant decrease in performance on multiple state-of-the-art QA models. Using a neural paraphrasing model trained to generate multiple paraphrased questions for a given source question and a set of paraphrase suggestions, we propose a data augmentation approach that requires no human intervention to re-train the models for improved robustness to question paraphrasing.},
	urldate = {2019-09-26},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Gan, Wee Chung and Ng, Hwee Tou},
	month = jul,
	year = {2019},
	pages = {6065--6075}
}

@inproceedings{akbik_generating_2015,
	address = {Beijing, China},
	title = {Generating {High} {Quality} {Proposition} {Banks} for {Multilingual} {Semantic} {Role} {Labeling}},
	url = {https://www.aclweb.org/anthology/P15-1039},
	doi = {10.3115/v1/P15-1039},
	urldate = {2019-09-23},
	booktitle = {Proceedings of the 53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Akbik, Alan and Chiticariu, Laura and Danilevsky, Marina and Li, Yunyao and Vaithyanathan, Shivakumar and Zhu, Huaiyu},
	month = jul,
	year = {2015},
	pages = {397--407}
}

@inproceedings{cui_span-extraction_2019,
	title = {A {Span}-{Extraction} {Dataset} for {Chinese} {Machine} {Reading} {Comprehension}},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Cui, Yiming and Liu, Ting and Che, Wanxiang and Xiao, Li and Chen, Zhipeng and Ma, Wentao and Wang, Shijin and Hu, Guoping},
	year = {2019}
}

@inproceedings{schwenk_corpus_2018,
	address = {Miyazaki, Japan},
	title = {A {Corpus} for {Multilingual} {Document} {Classification} in {Eight} {Languages}},
	url = {https://www.aclweb.org/anthology/L18-1560},
	urldate = {2019-09-09},
	booktitle = {Proceedings of the {Eleventh} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}-2018)},
	publisher = {European Languages Resources Association (ELRA)},
	author = {Schwenk, Holger and Li, Xian},
	month = may,
	year = {2018}
}

@article{asai_multilingual_2018,
	title = {Multilingual {Extractive} {Reading} {Comprehension} by {Runtime} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1809.03275},
	abstract = {Despite recent work in Reading Comprehension (RC), progress has been mostly limited to English due to the lack of large-scale datasets in other languages. In this work, we introduce the first RC system for languages without RC training data. Given a target language without RC training data and a pivot language with RC training data (e.g. English), our method leverages existing RC resources in the pivot language by combining a competitive RC model in the pivot language with an attentive Neural Machine Translation (NMT) model. We first translate the data from the target to the pivot language, and then obtain an answer using the RC model in the pivot language. Finally, we recover the corresponding answer in the original language using soft-alignment attention scores from the NMT model. We create evaluation sets of RC data in two non-English languages, namely Japanese and French, to evaluate our method. Experimental results on these datasets show that our method significantly outperforms a back-translation baseline of a state-of-the-art product-level machine translation system.},
	urldate = {2019-09-03},
	journal = {arXiv:1809.03275 [cs]},
	author = {Asai, Akari and Eriguchi, Akiko and Hashimoto, Kazuma and Tsuruoka, Yoshimasa},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.03275},
	keywords = {Computer Science - Computation and Language}
}

@article{hardalov_beyond_2019,
	title = {Beyond {English}-only {Reading} {Comprehension}: {Experiments} in {Zero}-{Shot} {Multilingual} {Transfer} for {Bulgarian}},
	shorttitle = {Beyond {English}-only {Reading} {Comprehension}},
	url = {http://arxiv.org/abs/1908.01519},
	abstract = {Recently, reading comprehension models achieved near-human performance on large-scale datasets such as SQuAD, CoQA, MS Macro, RACE, etc. This is largely due to the release of pre-trained contextualized representations such as BERT and ELMo, which can be fine-tuned for the target task. Despite those advances and the creation of more challenging datasets, most of the work is still done for English. Here, we study the effectiveness of multilingual BERT fine-tuned on large-scale English datasets for reading comprehension (e.g., for RACE), and we apply it to Bulgarian multiple-choice reading comprehension. We propose a new dataset containing 2,221 questions from matriculation exams for twelfth grade in various subjects -history, biology, geography and philosophy-, and 412 additional questions from online quizzes in history. While the quiz authors gave no relevant context, we incorporate knowledge from Wikipedia, retrieving documents matching the combination of question + each answer option. Moreover, we experiment with different indexing and pre-training strategies. The evaluation results show accuracy of 42.23\%, which is well above the baseline of 24.89\%.},
	urldate = {2019-08-29},
	journal = {arXiv:1908.01519 [cs]},
	author = {Hardalov, Momchil and Koychev, Ivan and Nakov, Preslav},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.01519},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval}
}

@article{liu_xcmrc:_2019,
	title = {{XCMRC}: {Evaluating} {Cross}-lingual {Machine} {Reading} {Comprehension}},
	shorttitle = {{XCMRC}},
	url = {http://arxiv.org/abs/1908.05416},
	abstract = {We present XCMRC, the first public cross-lingual language understanding (XLU) benchmark which aims to test machines on their cross-lingual reading comprehension ability. To be specific, XCMRC is a Cross-lingual Cloze-style Machine Reading Comprehension task which requires the reader to fill in a missing word (we additionally provide ten noun candidates) in a sentence written in target language (English / Chinese) by reading a given passage written in source language (Chinese / English). Chinese and English are rich-resource language pairs, in order to study low-resource cross-lingual machine reading comprehension (XMRC), besides defining the common XCMRC task which has no restrictions on use of external language resources, we also define the pseudo low-resource XCMRC task by limiting the language resources to be used. In addition, we provide two baselines for common XCMRC task and two for pseudo XCMRC task respectively. We also provide an upper bound baseline for both tasks. We found that for common XCMRC task, translation-based method and multilingual sentence encoder-based method can obtain reasonable performance but still have much room for improvement. As for pseudo low-resource XCMRC task, due to strict restrictions on the use of language resources, our two approaches are far below the upper bound so there are many challenges ahead.},
	urldate = {2019-08-29},
	journal = {arXiv:1908.05416 [cs]},
	author = {Liu, Pengyuan and Deng, Yuning and Zhu, Chenghao and Hu, Han},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.05416},
	keywords = {Computer Science - Computation and Language}
}

@article{hardalov_beyond_2019-1,
	title = {Beyond {English}-only {Reading} {Comprehension}: {Experiments} in {Zero}-{Shot} {Multilingual} {Transfer} for {Bulgarian}},
	shorttitle = {Beyond {English}-only {Reading} {Comprehension}},
	url = {http://arxiv.org/abs/1908.01519},
	abstract = {Recently, reading comprehension models achieved near-human performance on large-scale datasets such as SQuAD, CoQA, MS Macro, RACE, etc. This is largely due to the release of pre-trained contextualized representations such as BERT and ELMo, which can be fine-tuned for the target task. Despite those advances and the creation of more challenging datasets, most of the work is still done for English. Here, we study the effectiveness of multilingual BERT fine-tuned on large-scale English datasets for reading comprehension (e.g., for RACE), and we apply it to Bulgarian multiple-choice reading comprehension. We propose a new dataset containing 2,221 questions from matriculation exams for twelfth grade in various subjects -history, biology, geography and philosophy-, and 412 additional questions from online quizzes in history. While the quiz authors gave no relevant context, we incorporate knowledge from Wikipedia, retrieving documents matching the combination of question + each answer option. Moreover, we experiment with different indexing and pre-training strategies. The evaluation results show accuracy of 42.23\%, which is well above the baseline of 24.89\%.},
	urldate = {2019-08-27},
	journal = {arXiv:1908.01519 [cs]},
	author = {Hardalov, Momchil and Koychev, Ivan and Nakov, Preslav},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.01519},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval}
}

@inproceedings{seo_real-time_2019,
	address = {Florence, Italy},
	title = {Real-{Time} {Open}-{Domain} {Question} {Answering} with {Dense}-{Sparse} {Phrase} {Index}},
	url = {https://www.aclweb.org/anthology/P19-1436},
	abstract = {Existing open-domain question answering (QA) models are not suitable for real-time usage because they need to process several long documents on-demand for every input query, which is computationally prohibitive. In this paper, we introduce query-agnostic indexable representations of document phrases that can drastically speed up open-domain QA. In particular, our dense-sparse phrase encoding effectively captures syntactic, semantic, and lexical information of the phrases and eliminates the pipeline filtering of context documents. Leveraging strategies for optimizing training and inference time, our model can be trained and deployed even in a single 4-GPU server. Moreover, by representing phrases as pointers to their start and end tokens, our model indexes phrases in the entire English Wikipedia (up to 60 billion phrases) using under 2TB. Our experiments on SQuAD-Open show that our model is on par with or more accurate than previous models with 6000x reduced computational cost, which translates into at least 68x faster end-to-end inference benchmark on CPUs. Code and demo are available at nlp.cs.washington.edu/denspi},
	urldate = {2019-08-07},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Seo, Minjoon and Lee, Jinhyuk and Kwiatkowski, Tom and Parikh, Ankur and Farhadi, Ali and Hajishirzi, Hannaneh},
	month = jul,
	year = {2019},
	pages = {4430--4441}
}

@inproceedings{unger_question_2015,
	title = {Question {Answering} over {Linked} {Data} ({QALD}-5)},
	booktitle = {{CLEF}},
	author = {Unger, Christina and Forescu, Corina and Lopez, Vanessa and Ngonga Ngomo, Axel-Cyrille and Cabrio, Elena and Cimiano, Philipp and Walter, Sebastian},
	year = {2015}
}

@inproceedings{cimiano_multilingual_2013,
	title = {Multilingual {Question} {Answering} over {Linked} {Data} ({QALD}-3): {Lab} {Overview}},
	booktitle = {{CLEF}},
	author = {Cimiano, Philipp and López, Vanessa and Unger, Christina and Cabrio, Elena and Ngomo, Axel-Cyrille Ngonga and Walter, Sebastian},
	year = {2013}
}

@article{zimina_mug-qa:_2018,
	title = {{MuG}-{QA}: {Multilingual} {Grammatical} {Question} {Answering} for {RDF} {Data}},
	journal = {2018 IEEE International Conference on Progress in Informatics and Computing (PIC)},
	author = {Zimina, Elizaveta and Nummenmaa, Jyrki and Jarvelin, Kalervo and Peltonen, Jaakko and Stefanidis, Kostas},
	year = {2018},
	pages = {57--61}
}

@article{lee_cross-lingual_2019,
	title = {Cross-{Lingual} {Transfer} {Learning} for {Question} {Answering}},
	url = {http://arxiv.org/abs/1907.06042},
	abstract = {Deep learning based question answering (QA) on English documents has achieved success because there is a large amount of English training examples. However, for most languages, training examples for high-quality QA models are not available. In this paper, we explore the problem of cross-lingual transfer learning for QA, where a source language task with plentiful annotations is utilized to improve the performance of a QA model on a target language task with limited available annotations. We examine two different approaches. A machine translation (MT) based approach translates the source language into the target language, or vice versa. Although the MT-based approach brings improvement, it assumes the availability of a sentence-level translation system. A GAN-based approach incorporates a language discriminator to learn language-universal feature representations, and consequentially transfer knowledge from the source language. The GAN-based approach rivals the performance of the MT-based approach with fewer linguistic resources. Applying both approaches simultaneously yield the best results. We use two English benchmark datasets, SQuAD and NewsQA, as source language data, and show significant improvements over a number of established baselines on a Chinese QA task. We achieve the new state-of-the-art on the Chinese QA dataset.},
	urldate = {2019-07-25},
	journal = {arXiv:1907.06042 [cs]},
	author = {Lee, Chia-Hsuan and Lee, Hung-Yi},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.06042},
	keywords = {Computer Science - Computation and Language}
}

@article{kumar_cross-lingual_2019,
	title = {Cross-{Lingual} {Training} for {Automatic} {Question} {Generation}},
	url = {http://arxiv.org/abs/1906.02525},
	abstract = {Automatic question generation (QG) is a challenging problem in natural language understanding. QG systems are typically built assuming access to a large number of training instances where each instance is a question and its corresponding answer. For a new language, such training instances are hard to obtain making the QG problem even more challenging. Using this as our motivation, we study the reuse of an available large QG dataset in a secondary language (e.g. English) to learn a QG model for a primary language (e.g. Hindi) of interest. For the primary language, we assume access to a large amount of monolingual text but only a small QG dataset. We propose a cross-lingual QG model which uses the following training regime: (i) Unsupervised pretraining of language models in both primary and secondary languages and (ii) joint supervised training for QG in both languages. We demonstrate the efficacy of our proposed approach using two different primary languages, Hindi and Chinese. We also create and release a new question answering dataset for Hindi consisting of 6555 sentences.},
	urldate = {2019-07-25},
	journal = {arXiv:1906.02525 [cs]},
	author = {Kumar, Vishwajeet and Joshi, Nitish and Mukherjee, Arijit and Ramakrishnan, Ganesh and Jyothi, Preethi},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.02525},
	keywords = {Computer Science - Computation and Language}
}

@inproceedings{pouran_ben_veyseh_cross-lingual_2016,
	address = {San Diego, CA, USA},
	title = {Cross-{Lingual} {Question} {Answering} {Using} {Common} {Semantic} {Space}},
	url = {https://www.aclweb.org/anthology/W16-1403},
	doi = {10.18653/v1/W16-1403},
	urldate = {2019-07-25},
	booktitle = {Proceedings of {TextGraphs}-10: the {Workshop} on {Graph}-based {Methods} for {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Pouran Ben Veyseh, Amir},
	month = jun,
	year = {2016},
	pages = {15--19}
}

@inproceedings{liu_xqa:_2019,
	title = {{XQA}: {A} {Cross}-lingual {Open}-domain {Question} {Answering} {Dataset}},
	booktitle = {Proceedings of {ACL} 2019},
	author = {Liu, Jiahua and Lin, Yankai and Liu, Zhiyuan and Sun, Maosong},
	year = {2019}
}

@article{shao_drcd:_2018,
	title = {{DRCD}: a {Chinese} {Machine} {Reading} {Comprehension} {Dataset}},
	shorttitle = {{DRCD}},
	url = {http://arxiv.org/abs/1806.00920},
	abstract = {In this paper, we introduce DRCD (Delta Reading Comprehension Dataset), an open domain traditional Chinese machine reading comprehension (MRC) dataset. This dataset aimed to be a standard Chinese machine reading comprehension dataset, which can be a source dataset in transfer learning. The dataset contains 10,014 paragraphs from 2,108 Wikipedia articles and 30,000+ questions generated by annotators. We build a baseline model that achieves an F1 score of 89.59\%. F1 score of Human performance is 93.30\%.},
	urldate = {2019-07-25},
	journal = {arXiv:1806.00920 [cs]},
	author = {Shao, Chih Chieh and Liu, Trois and Lai, Yuting and Tseng, Yiying and Tsai, Sam},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.00920},
	keywords = {Computer Science - Computation and Language}
}

@inproceedings{he_dureader:_2018,
	address = {Melbourne, Australia},
	title = {{DuReader}: a {Chinese} {Machine} {Reading} {Comprehension} {Dataset} from {Real}-world {Applications}},
	shorttitle = {{DuReader}},
	url = {https://www.aclweb.org/anthology/W18-2605},
	doi = {10.18653/v1/W18-2605},
	abstract = {This paper introduces DuReader, a new large-scale, open-domain Chinese machine reading comprehension (MRC) dataset, designed to address real-world MRC. DuReader has three advantages over previous MRC datasets: (1) data sources: questions and documents are based on Baidu Search and Baidu Zhidao; answers are manually generated. (2) question types: it provides rich annotations for more question types, especially yes-no and opinion questions, that leaves more opportunity for the research community. (3) scale: it contains 200K questions, 420K answers and 1M documents; it is the largest Chinese MRC dataset so far. Experiments show that human performance is well above current state-of-the-art baseline systems, leaving plenty of room for the community to make improvements. To help the community make these improvements, both DuReader and baseline systems have been posted online. We also organize a shared competition to encourage the exploration of more models. Since the release of the task, there are significant improvements over the baselines.},
	urldate = {2019-07-25},
	booktitle = {Proceedings of the {Workshop} on {Machine} {Reading} for {Question} {Answering}},
	publisher = {Association for Computational Linguistics},
	author = {He, Wei and Liu, Kai and Liu, Jing and Lyu, Yajuan and Zhao, Shiqi and Xiao, Xinyan and Liu, Yuan and Wang, Yizhong and Wu, Hua and She, Qiaoqiao and Liu, Xuan and Wu, Tian and Wang, Haifeng},
	month = jul,
	year = {2018},
	pages = {37--46}
}

@inproceedings{yang_wikiqa:_2015,
	address = {Lisbon, Portugal},
	title = {{WikiQA}: {A} {Challenge} {Dataset} for {Open}-{Domain} {Question} {Answering}},
	url = {https://www.aclweb.org/anthology/D15-1237},
	doi = {10.18653/v1/D15-1237},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Yang, Yi and Yih, Wen-tau and Meek, Christopher},
	month = sep,
	year = {2015},
	pages = {2013--2018}
}

@inproceedings{mikolov_distributed_2013,
	address = {USA},
	series = {{NIPS}'13},
	title = {Distributed {Representations} of {Words} and {Phrases} and {Their} {Compositionality}},
	url = {http://dl.acm.org/citation.cfm?id=2999792.2999959},
	booktitle = {Proceedings of the 26th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 2},
	publisher = {Curran Associates Inc.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	year = {2013},
	note = {event-place: Lake Tahoe, Nevada},
	pages = {3111--3119}
}

@inproceedings{pennington_glove:_2014,
	address = {Doha, Qatar},
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	url = {https://www.aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	month = oct,
	year = {2014},
	pages = {1532--1543}
}

@article{welbl_constructing_2018,
	title = {Constructing {Datasets} for {Multi}-hop {Reading} {Comprehension} {Across} {Documents}},
	volume = {6},
	url = {https://www.aclweb.org/anthology/Q18-1021},
	doi = {10.1162/tacl_a_00021},
	abstract = {Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence, paragraph, or document. Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods, but currently no resources exist to train and test this capability. We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods. In our task, a model learns to seek and combine evidence — effectively performing multihop, alias multi-step, inference. We devise a methodology to produce datasets for this task, given a collection of query-answer pairs and thematically linked documents. Two datasets from different domains are induced, and we identify potential pitfalls and devise circumvention strategies. We evaluate two previously proposed competitive models and find that one can integrate information across documents. However, both models struggle to select relevant information; and providing documents guaranteed to be relevant greatly improves their performance. While the models outperform several strong baselines, their best accuracy reaches 54.5\% on an annotated test set, compared to human performance at 85.0\%, leaving ample room for improvement.},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Welbl, Johannes and Stenetorp, Pontus and Riedel, Sebastian},
	year = {2018},
	pages = {287--302}
}

@inproceedings{dua_drop:_2019,
	title = {{DROP}: {A} {Reading} {Comprehension} {Benchmark} {Requiring} {Discrete} {Reasoning} {Over} {Paragraphs}},
	booktitle = {{NAACL}-{HLT}},
	author = {Dua, Dheeru and Wang, Yizhong and Dasigi, Pradeep and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
	year = {2019}
}

@article{howard_fine-tuned_2018,
	title = {Fine-tuned {Language} {Models} for {Text} {Classification}},
	volume = {abs/1801.06146},
	url = {http://arxiv.org/abs/1801.06146},
	journal = {CoRR},
	author = {Howard, Jeremy and Ruder, Sebastian},
	year = {2018}
}

@misc{radford_improving_2018,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	year = {2018},
	url = {https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf}
}

@article{seo_bidirectional_2016,
	title = {Bidirectional {Attention} {Flow} for {Machine} {Comprehension}},
	volume = {abs/1611.01603},
	url = {http://arxiv.org/abs/1611.01603},
	journal = {CoRR},
	author = {Seo, Min Joon and Kembhavi, Aniruddha and Farhadi, Ali and Hajishirzi, Hannaneh},
	year = {2016}
}

@inproceedings{hill_goldilocks_2016,
	title = {The {Goldilocks} {Principle}: {Reading} {Children}'s {Books} with {Explicit} {Memory} {Representations}},
	url = {http://arxiv.org/abs/1511.02301},
	booktitle = {4th {International} {Conference} on {Learning} {Representations}, {ICLR} 2016, {San} {Juan}, {Puerto} {Rico}, {May} 2-4, 2016, {Conference} {Track} {Proceedings}},
	author = {Hill, Felix and Bordes, Antoine and Chopra, Sumit and Weston, Jason},
	year = {2016}
}

@inproceedings{richardson_mctest:_2013,
	title = {{MCTest}: {A} {Challenge} {Dataset} for the {Open}-{Domain} {Machine} {Comprehension} of {Text}},
	url = {https://www.microsoft.com/en-us/research/publication/mctest-challenge-dataset-open-domain-machine-comprehension-text/},
	abstract = {We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text.},
	booktitle = {Proceedings of the 2013 {Conference} on {Emprical} {Methods} in {Natural} {Language} {Processing} ({EMNLP} 2013)},
	author = {Richardson, Matthew},
	month = oct,
	year = {2013}
}

@inproceedings{voorhees_building_2000,
	address = {New York, NY, USA},
	series = {{SIGIR} '00},
	title = {Building a {Question} {Answering} {Test} {Collection}},
	isbn = {1-58113-226-3},
	url = {http://doi.acm.org/10.1145/345508.345577},
	doi = {10.1145/345508.345577},
	booktitle = {Proceedings of the 23rd {Annual} {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Voorhees, Ellen M. and Tice, Dawn M.},
	year = {2000},
	note = {event-place: Athens, Greece},
	pages = {200--207}
}

@inproceedings{voorhees_trec-8_1999,
	title = {The {TREC}-8 {Question} {Answering} {Track} {Report}},
	booktitle = {In {Proceedings} of {TREC}-8},
	author = {Voorhees, Ellen M.},
	year = {1999},
	pages = {77--82}
}

@article{chu_meansum:_2018,
	title = {{MeanSum}: {A} {Neural} {Model} for {Unsupervised} {Multi}-document {Abstractive} {Summarization}},
	shorttitle = {{MeanSum}},
	url = {http://arxiv.org/abs/1810.05739},
	abstract = {Abstractive summarization has been studied using neural sequence transduction methods with datasets of large, paired document-summary examples. However, such datasets are rare and the models trained from them do not generalize to other domains. Recently, some progress has been made in learning sequence-to-sequence mappings with only unpaired examples. In our work, we consider the setting where there are only documents (product or business reviews) with no summaries provided, and propose an end-to-end, neural model architecture to perform unsupervised abstractive summarization. Our proposed model consists of an auto-encoder where the mean of the representations of the input reviews decodes to a reasonable summary-review while not relying on any review-specific features. We consider variants of the proposed architecture and perform an ablation study to show the importance of specific components. We show through automated metrics and human evaluation that the generated summaries are highly abstractive, fluent, relevant, and representative of the average sentiment of the input reviews. Finally, we collect a reference evaluation dataset and show that our model outperforms a strong extractive baseline.},
	urldate = {2019-07-14},
	journal = {arXiv:1810.05739 [cs]},
	author = {Chu, Eric and Liu, Peter J.},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.05739},
	keywords = {Computer Science - Computation and Language}
}

@inproceedings{freitag_unsupervised_2018,
	address = {Brussels, Belgium},
	title = {Unsupervised {Natural} {Language} {Generation} with {Denoising} {Autoencoders}},
	url = {https://www.aclweb.org/anthology/D18-1426},
	abstract = {Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data.},
	urldate = {2019-07-14},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Freitag, Markus and Roy, Scott},
	month = oct,
	year = {2018},
	pages = {3922--3929}
}

@article{surya_unsupervised_2018,
	title = {Unsupervised {Neural} {Text} {Simplification}},
	url = {http://arxiv.org/abs/1810.07931},
	abstract = {The paper presents a first attempt towards unsupervised neural text simplification that relies only on unlabeled text corpora. The core framework is composed of a shared encoder and a pair of attentional-decoders and gains knowledge of simplification through discrimination based-losses and denoising. The framework is trained using unlabeled text collected from en-Wikipedia dump. Our analysis (both quantitative and qualitative involving human evaluators) on a public test data shows that the proposed model can perform text-simplification at both lexical and syntactic levels, competitive to existing supervised methods. Addition of a few labelled pairs also improves the performance further.},
	urldate = {2019-07-14},
	journal = {arXiv:1810.07931 [cs]},
	author = {Surya, Sai and Mishra, Abhijit and Laha, Anirban and Jain, Parag and Sankaranarayanan, Karthik},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.07931},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@article{artetxe_massively_2018,
	title = {Massively {Multilingual} {Sentence} {Embeddings} for {Zero}-{Shot} {Cross}-{Lingual} {Transfer} and {Beyond}},
	url = {http://arxiv.org/abs/1812.10464},
	abstract = {We introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging to more than 30 different language families and written in 28 different scripts. Our system uses a single BiLSTM encoder with a shared BPE vocabulary for all languages, which is coupled with an auxiliary decoder and trained on publicly available parallel corpora. This enables us to learn a classifier on top of the resulting sentence embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification. Our approach sets a new state-of-the-art on zero-shot cross-lingual natural language inference for all the 14 languages in the XNLI dataset but one. We also achieve very competitive results in cross-lingual document classification (MLDoc dataset). Our sentence embeddings are also strong at parallel corpus mining, establishing a new state-of-the-art in the BUCC shared task for 3 of its 4 language pairs. Finally, we introduce a new test set of aligned sentences in 122 languages based on the Tatoeba corpus, and show that our sentence embeddings obtain strong results in multilingual similarity search even for low-resource languages. Our PyTorch implementation, pre-trained encoder and the multilingual test set will be freely available.},
	urldate = {2019-07-14},
	journal = {arXiv:1812.10464 [cs]},
	author = {Artetxe, Mikel and Schwenk, Holger},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.10464},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@article{kwiatkowski_natural_2019,
	title = {Natural {Questions}: a {Benchmark} for {Question} {Answering} {Research}},
	url = {https://tomkwiat.users.x20web.corp.google.com/papers/natural-questions/main-1455-kwiatkowski.pdf},
	journal = {Transactions of the Association of Computational Linguistics},
	author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Kelcey, Matthew and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina N. and Jones, Llion and Chang, Ming-Wei and Dai, Andrew and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
	year = {2019}
}

@article{mozannar_neural_2019,
	title = {Neural {Arabic} {Question} {Answering}},
	url = {http://arxiv.org/abs/1906.05394},
	abstract = {This paper tackles the problem of open domain factual Arabic question answering (QA) using Wikipedia as our knowledge source. This constrains the answer of any question to be a span of text in Wikipedia. Open domain QA for Arabic entails three challenges: annotated QA datasets in Arabic, large scale efficient information retrieval and machine reading comprehension. To deal with the lack of Arabic QA datasets we present the Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD). Our system for open domain question answering in Arabic (SOQAL) is based on two components: (1) a document retriever using a hierarchical TF-IDF approach and (2) a neural reading comprehension model using the pre-trained bi-directional transformer BERT. Our experiments on ARCD indicate the effectiveness of our approach with our BERT-based reader achieving a 61.3 F1 score, and our open domain system SOQAL achieving a 27.6 F1 score.},
	urldate = {2019-07-13},
	journal = {arXiv:1906.05394 [cs]},
	author = {Mozannar, Hussein and Hajal, Karl El and Maamary, Elie and Hajj, Hazem},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.05394},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@inproceedings{lample_unsupervised_2018,
	title = {Unsupervised {Machine} {Translation} {Using} {Monolingual} {Corpora} {Only}},
	url = {https://arxiv.org/abs/1711.00043v2},
	abstract = {Machine translation has recently achieved impressive performance thanks to
recent advances in deep learning and the availability of large-scale parallel
corpora. There have been numerous attempts to extend these successes to
low-resource language pairs, yet requiring tens of thousands of parallel
sentences. In this work, we take this research direction to the extreme and
investigate whether it is possible to learn to translate even without any
parallel data. We propose a model that takes sentences from monolingual corpora
in two different languages and maps them into the same latent space. By
learning to reconstruct in both languages from this shared feature space, the
model effectively learns to translate without using any labeled data. We
demonstrate our model on two widely used datasets and two language pairs,
reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French
datasets, without using even a single parallel sentence at training time.},
	language = {en},
	urldate = {2019-02-18},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Lample, Guillaume and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
	year = {2018}
}

@inproceedings{golub_two-stage_2017,
	title = {Two-{Stage} {Synthesis} {Networks} for {Transfer} {Learning} in {Machine} {Comprehension}},
	url = {https://www.aclweb.org/anthology/papers/D/D17/D17-1087/},
	doi = {10.18653/v1/D17-1087},
	language = {en-us},
	urldate = {2019-06-17},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Golub, David and Huang, Po-Sen and He, Xiaodong and Deng, Li},
	month = sep,
	year = {2017},
	pages = {835--844}
}

@article{min_compositional_2019,
	title = {Compositional {Questions} {Do} {Not} {Necessitate} {Multi}-hop {Reasoning}},
	url = {http://arxiv.org/abs/1906.02900},
	abstract = {Multi-hop reading comprehension (RC) questions are challenging because they require reading and reasoning over multiple paragraphs. We argue that it can be difficult to construct large multi-hop RC datasets. For example, even highly compositional questions can be answered with a single hop if they target specific entity types, or the facts needed to answer them are redundant. Our analysis is centered on HotpotQA, where we show that single-hop reasoning can solve much more of the dataset than previously thought. We introduce a single-hop BERT-based RC model that achieves 67 F1---comparable to state-of-the-art multi-hop models. We also design an evaluation setting where humans are not shown all of the necessary paragraphs for the intended multi-hop reasoning but can still answer over 80\% of questions. Together with detailed error analysis, these results suggest there should be an increasing focus on the role of evidence in multi-hop reasoning and possibly even a shift towards information retrieval style evaluations with large and diverse evidence collections.},
	urldate = {2019-06-20},
	journal = {arXiv:1906.02900 [cs]},
	author = {Min, Sewon and Wallace, Eric and Singh, Sameer and Gardner, Matt and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.02900},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}
@inproceedings{yadav_alignment_2019,
	address = {Minneapolis, Minnesota},
	title = {Alignment over {Heterogeneous} {Embeddings} for {Question} {Answering}},
	url = {https://www.aclweb.org/anthology/N19-1274},
	abstract = {We propose a simple, fast, and mostly-unsupervised approach for non-factoid question answering (QA) called Alignment over Heterogeneous Embeddings (AHE). AHE simply aligns each word in the question and candidate answer with the most similar word in the retrieved supporting paragraph, and weighs each alignment score with the inverse document frequency of the corresponding question/answer term. AHE's similarity function operates over embeddings that model the underlying text at different levels of abstraction: character (FLAIR), word (BERT and GloVe), and sentence (InferSent), where the latter is the only supervised component in the proposed approach. Despite its simplicity and lack of supervision, AHE obtains a new state-of-the-art performance on the “Easy” partition of the AI2 Reasoning Challenge (ARC) dataset (64.6\% accuracy), top-two performance on the “Challenge” partition of ARC (34.1\%), and top-three performance on the WikiQA dataset (74.08\% MRR), outperforming many other complex, supervised approaches. Our error analysis indicates that alignments over character, word, and sentence embeddings capture substantially different semantic information. We exploit this with a simple meta-classifier that learns how much to trust the predictions over each representation, which further improves the performance of unsupervised AHE.},
	urldate = {2019-06-17},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Yadav, Vikas and Bethard, Steven and Surdeanu, Mihai},
	month = jun,
	year = {2019},
	pages = {2681--2691}
}

@article{talmor_multiqa:_2019,
	title = {{MultiQA}: {An} {Empirical} {Investigation} of {Generalization} and {Transfer} in {Reading} {Comprehension}},
	shorttitle = {{MultiQA}},
	url = {http://arxiv.org/abs/1905.13453},
	abstract = {A large number of reading comprehension (RC) datasets has been created recently, but little analysis has been done on whether they generalize to one another, and the extent to which existing datasets can be leveraged for improving performance on new ones. In this paper, we conduct such an investigation over ten RC datasets, training on one or more source RC datasets, and evaluating generalization, as well as transfer to a target RC dataset. We analyze the factors that contribute to generalization, and show that training on a source RC dataset and transferring to a target dataset substantially improves performance, even in the presence of powerful contextual representations from BERT (Devlin et al., 2019). We also find that training on multiple source RC datasets leads to robust generalization and transfer, and can reduce the cost of example collection for a new RC dataset. Following our analysis, we propose MultiQA, a BERT-based model, trained on multiple RC datasets, which leads to state-of-the-art performance on five RC datasets. We share our infrastructure for the benefit of the research community.},
	urldate = {2019-06-12},
	journal = {arXiv:1905.13453 [cs]},
	author = {Talmor, Alon and Berant, Jonathan},
	month = may,
	year = {2019},
	note = {arXiv: 1905.13453},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@article{sun_probing_2019,
	title = {Probing {Prior} {Knowledge} {Needed} in {Challenging} {Chinese} {Machine} {Reading} {Comprehension}},
	url = {http://arxiv.org/abs/1904.09679},
	abstract = {With an ultimate goal of narrowing the gap between human and machine readers in text comprehension, we present the first collection of Challenging Chinese machine reading Comprehension datasets (C{\textasciicircum}3) collected from language and professional certification exams, which contains 13,924 documents and their associated 23,990 multiple-choice questions. Most of the questions in C{\textasciicircum}3 cannot be answered merely by surface-form matching against the given text. As a pilot study, we closely analyze the prior knowledge (i.e., linguistic, domain-specific, and general world knowledge) needed in these real-world reading comprehension tasks. We further explore how to leverage linguistic knowledge including a lexicon of idioms and proverbs, graphs of general world knowledge (e.g., ConceptNet), and domain-specific knowledge such as textbooks to aid machine readers, through fine-tuning a pre-trained language model. Experimental results demonstrate that linguistic and general world knowledge may help improve the performance of the baseline reader in both general and domain-specific tasks. C{\textasciicircum}3 will be available at http://dataset.org/c3/.},
	urldate = {2019-06-06},
	journal = {arXiv:1904.09679 [cs]},
	author = {Sun, Kai and Yu, Dian and Yu, Dong and Cardie, Claire},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.09679},
	keywords = {Computer Science - Computation and Language}
}

@article{faruqui_identifying_2018,
	title = {Identifying {Well}-formed {Natural} {Language} {Questions}},
	url = {http://arxiv.org/abs/1808.09419},
	abstract = {Understanding search queries is a hard problem as it involves dealing with "word salad" text ubiquitously issued by users. However, if a query resembles a well-formed question, a natural language processing pipeline is able to perform more accurate interpretation, thus reducing downstream compounding errors. Hence, identifying whether or not a query is well formed can enhance query understanding. Here, we introduce a new task of identifying a well-formed natural language question. We construct and release a dataset of 25,100 publicly available questions classified into well-formed and non-wellformed categories and report an accuracy of 70.7\% on the test set. We also show that our classifier can be used to improve the performance of neural sequence-to-sequence models for generating questions for reading comprehension.},
	urldate = {2019-06-01},
	journal = {arXiv:1808.09419 [cs]},
	author = {Faruqui, Manaal and Das, Dipanjan},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.09419},
	keywords = {Computer Science - Computation and Language}
}

@article{bojanowski_enriching_2016,
	title = {Enriching {Word} {Vectors} with {Subword} {Information}},
	url = {http://arxiv.org/abs/1607.04606},
	abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character \$n\$-grams. A vector representation is associated to each character \$n\$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
	urldate = {2019-05-30},
	journal = {arXiv:1607.04606 [cs]},
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.04606},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@article{faruqui_identifying_2018-1,
	title = {Identifying {Well}-formed {Natural} {Language} {Questions}},
	url = {http://arxiv.org/abs/1808.09419},
	abstract = {Understanding search queries is a hard problem as it involves dealing with "word salad" text ubiquitously issued by users. However, if a query resembles a well-formed question, a natural language processing pipeline is able to perform more accurate interpretation, thus reducing downstream compounding errors. Hence, identifying whether or not a query is well formed can enhance query understanding. Here, we introduce a new task of identifying a well-formed natural language question. We construct and release a dataset of 25,100 publicly available questions classified into well-formed and non-wellformed categories and report an accuracy of 70.7\% on the test set. We also show that our classifier can be used to improve the performance of neural sequence-to-sequence models for generating questions for reading comprehension.},
	urldate = {2019-05-29},
	journal = {arXiv:1808.09419 [cs]},
	author = {Faruqui, Manaal and Das, Dipanjan},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.09419},
	keywords = {Computer Science - Computation and Language}
}

@article{wu_extract_2019,
	title = {Extract and {Edit}: {An} {Alternative} to {Back}-{Translation} for {Unsupervised} {Neural} {Machine} {Translation}},
	shorttitle = {Extract and {Edit}},
	url = {http://arxiv.org/abs/1904.02331},
	abstract = {The overreliance on large parallel corpora significantly limits the applicability of machine translation systems to the majority of language pairs. Back-translation has been dominantly used in previous approaches for unsupervised neural machine translation, where pseudo sentence pairs are generated to train the models with a reconstruction loss. However, the pseudo sentences are usually of low quality as translation errors accumulate during training. To avoid this fundamental issue, we propose an alternative but more effective approach, extract-edit, to extract and then edit real sentences from the target monolingual corpora. Furthermore, we introduce a comparative translation loss to evaluate the translated target sentences and thus train the unsupervised translation systems. Experiments show that the proposed approach consistently outperforms the previous state-of-the-art unsupervised machine translation systems across two benchmarks (English-French and English-German) and two low-resource language pairs (English-Romanian and English-Russian) by more than 2 (up to 3.63) BLEU points.},
	urldate = {2019-04-08},
	journal = {arXiv:1904.02331 [cs]},
	author = {Wu, Jiawei and Wang, Xin and Wang, William Yang},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.02331},
	keywords = {Computer Science - Computation and Language}
}

@inproceedings{lei_rationalizing_2016,
	address = {Austin, Texas},
	title = {Rationalizing {Neural} {Predictions}},
	url = {https://www.aclweb.org/anthology/D16-1011},
	doi = {10.18653/v1/D16-1011},
	urldate = {2019-04-08},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
	month = nov,
	year = {2016},
	pages = {107--117}
}

@article{buck_ask_2017,
	title = {Ask the {Right} {Questions}: {Active} {Question} {Reformulation} with {Reinforcement} {Learning}},
	shorttitle = {Ask the {Right} {Questions}},
	url = {http://arxiv.org/abs/1705.07830},
	abstract = {We frame Question Answering (QA) as a Reinforcement Learning task, an approach that we call Active Question Answering. We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers. The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. The reformulation system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!. The agent outperforms a state-of-the-art base model, playing the role of the environment, and other benchmarks. We also analyze the language that the agent has learned while interacting with the question answering system. We find that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting (tf-idf) and stemming.},
	urldate = {2019-04-03},
	journal = {arXiv:1705.07830 [cs]},
	author = {Buck, Christian and Bulian, Jannis and Ciaramita, Massimiliano and Gajewski, Wojciech and Gesmundo, Andrea and Houlsby, Neil and Wang, Wei},
	month = may,
	year = {2017},
	note = {arXiv: 1705.07830},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@article{kusner_gans_2016,
	title = {{GANS} for {Sequences} of {Discrete} {Elements} with the {Gumbel}-softmax {Distribution}},
	url = {http://arxiv.org/abs/1611.04051},
	abstract = {Generative Adversarial Networks (GAN) have limitations when the goal is to generate sequences of discrete elements. The reason for this is that samples from a distribution on discrete objects such as the multinomial are not differentiable with respect to the distribution parameters. This problem can be avoided by using the Gumbel-softmax distribution, which is a continuous approximation to a multinomial distribution parameterized in terms of the softmax function. In this work, we evaluate the performance of GANs based on recurrent neural networks with Gumbel-softmax output distributions in the task of generating sequences of discrete elements.},
	urldate = {2019-03-28},
	journal = {arXiv:1611.04051 [cs, stat]},
	author = {Kusner, Matt J. and Hernández-Lobato, José Miguel},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.04051},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{almahairi_augmented_2018,
	title = {Augmented {CycleGAN}: {Learning} {Many}-to-{Many} {Mappings} from {Unpaired} {Data}},
	shorttitle = {Augmented {CycleGAN}},
	url = {http://arxiv.org/abs/1802.10151},
	abstract = {Learning inter-domain mappings from unpaired data can improve performance in structured prediction tasks, such as image segmentation, by reducing the need for paired data. CycleGAN was recently proposed for this problem, but critically assumes the underlying inter-domain mapping is approximately deterministic and one-to-one. This assumption renders the model ineffective for tasks requiring flexible, many-to-many mappings. We propose a new model, called Augmented CycleGAN, which learns many-to-many mappings between domains. We examine Augmented CycleGAN qualitatively and quantitatively on several image datasets.},
	urldate = {2019-03-12},
	journal = {arXiv:1802.10151 [cs]},
	author = {Almahairi, Amjad and Rajeswar, Sai and Sordoni, Alessandro and Bachman, Philip and Courville, Aaron},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.10151},
	keywords = {Computer Science - Machine Learning}
}

@inproceedings{kann_sentence-level_2018,
	address = {Brussels, Belgium},
	title = {Sentence-{Level} {Fluency} {Evaluation}: {References} {Help}, {But} {Can} {Be} {Spared}!},
	shorttitle = {Sentence-{Level} {Fluency} {Evaluation}},
	url = {http://www.aclweb.org/anthology/K18-1031},
	abstract = {Motivated by recent findings on the probabilistic modeling of acceptability judgments, we propose syntactic log-odds ratio (SLOR), a normalized language model score, as a metric for referenceless fluency evaluation of natural language generation output at the sentence level. We further introduce WPSLOR, a novel WordPiece-based version, which harnesses a more compact language model. Even though word-overlap metrics like ROUGE are computed with the help of hand-written references, our referenceless methods obtain a significantly higher correlation with human fluency scores on a benchmark dataset of compressed sentences. Finally, we present ROUGE-LM, a reference-based metric which is a natural extension of WPSLOR to the case of available references. We show that ROUGE-LM yields a significantly higher correlation with human judgments than all baseline metrics, including WPSLOR on its own.},
	urldate = {2019-03-11},
	booktitle = {Proceedings of the 22nd {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Kann, Katharina and Rothe, Sascha and Filippova, Katja},
	month = oct,
	year = {2018},
	pages = {313--323}
}

@inproceedings{dhingra_simple_2018,
	address = {New Orleans, Louisiana},
	title = {Simple and {Effective} {Semi}-{Supervised} {Question} {Answering}},
	url = {http://www.aclweb.org/anthology/N18-2092},
	abstract = {Recent success of deep learning models for the task of extractive Question Answering (QA) is hinged on the availability of large annotated corpora. However, large domain specific annotated corpora are limited and expensive to construct. In this work, we envision a system where the end user specifies a set of base documents and only a few labelled examples. Our system exploits the document structure to create cloze-style questions from these base documents; pre-trains a powerful neural network on the cloze style questions; and further fine-tunes the model on the labeled examples. We evaluate our proposed system across three diverse datasets from different domains, and find it to be highly effective with very little labeled data. We attain more than 50\% F1 score on SQuAD and TriviaQA with less than a thousand labelled examples. We are also releasing a set of 3.2M cloze-style questions for practitioners to use while building QA systems.},
	urldate = {2019-03-05},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 2 ({Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Dhingra, Bhuwan and Danish, Danish and Rajagopal, Dheeraj},
	month = jun,
	year = {2018},
	pages = {582--587}
}

@article{hosking_evaluating_2019,
	title = {Evaluating {Rewards} for {Question} {Generation} {Models}},
	url = {http://arxiv.org/abs/1902.11049},
	abstract = {Recent approaches to question generation have used modifications to a Seq2Seq architecture inspired by advances in machine translation. Models are trained using teacher forcing to optimise only the one-step-ahead prediction. However, at test time, the model is asked to generate a whole sequence, causing errors to propagate through the generation process (exposure bias). A number of authors have proposed countering this bias by optimising for a reward that is less tightly coupled to the training data, using reinforcement learning. We optimise directly for quality metrics, including a novel approach using a discriminator learned directly from the training data. We confirm that policy gradient methods can be used to decouple training from the ground truth, leading to increases in the metrics used as rewards. We perform a human evaluation, and show that although these metrics have previously been assumed to be good proxies for question quality, they are poorly aligned with human judgement and the model simply learns to exploit the weaknesses of the reward source.},
	urldate = {2019-03-04},
	journal = {arXiv:1902.11049 [cs]},
	author = {Hosking, Tom and Riedel, Sebastian},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.11049},
	keywords = {Computer Science - Computation and Language}
}

@inproceedings{koehn_moses:_2007,
	title = {Moses: {Open} {Source} {Toolkit} for {Statistical} {Machine} {Translation}},
	shorttitle = {Moses},
	url = {http://aclweb.org/anthology/P07-2045},
	urldate = {2019-03-03},
	booktitle = {Proceedings of the 45th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} {Companion} {Volume} {Proceedings} of the {Demo} and {Poster} {Sessions}},
	publisher = {Association for Computational Linguistics},
	author = {Koehn, Philipp and Hoang, Hieu and Birch, Alexandra and Callison-Burch, Chris and Federico, Marcello and Bertoldi, Nicola and Cowan, Brooke and Shen, Wade and Moran, Christine and Zens, Richard and Dyer, Chris and Bojar, Ondrej and Constantin, Alexandra and Herbst, Evan},
	year = {2007},
	note = {event-place: Prague, Czech Republic},
	pages = {177--180}
}

@article{du_learning_2017,
	title = {Learning to {Ask}: {Neural} {Question} {Generation} for {Reading} {Comprehension}},
	shorttitle = {Learning to {Ask}},
	url = {https://arxiv.org/abs/1705.00106v1},
	abstract = {We study automatic question generation for sentences from text passages in
reading comprehension. We introduce an attention-based sequence learning model
for the task and investigate the effect of encoding sentence- vs.
paragraph-level information. In contrast to all previous work, our model does
not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead
trainable end-to-end via sequence-to-sequence learning. Automatic evaluation
results show that our system significantly outperforms the state-of-the-art
rule-based system. In human evaluations, questions generated by our system are
also rated as being more natural (i.e., grammaticality, fluency) and as more
difficult to answer (in terms of syntactic and lexical divergence from the
original text and reasoning needed to answer).},
	language = {en},
	urldate = {2019-02-28},
	author = {Du, Xinya and Shao, Junru and Cardie, Claire},
	month = apr,
	year = {2017}
}

@article{yuan_machine_2017,
	title = {Machine {Comprehension} by {Text}-to-{Text} {Neural} {Question} {Generation}},
	url = {http://arxiv.org/abs/1705.02012},
	abstract = {We propose a recurrent neural model that generates natural-language questions from documents, conditioned on answers. We show how to train the model using a combination of supervised and reinforcement learning. After teacher forcing for standard maximum likelihood training, we ﬁne-tune the model using policy gradient techniques to maximize several rewards that measure question quality. Most notably, one of these rewards is the performance of a question-answering system. Our model is trained and evaluated on the recent question-answering dataset SQuAD.},
	language = {en},
	urldate = {2019-02-28},
	journal = {arXiv:1705.02012 [cs]},
	author = {Yuan, Xingdi and Wang, Tong and Gulcehre, Caglar and Sordoni, Alessandro and Bachman, Philip and Subramanian, Sandeep and Zhang, Saizheng and Trischler, Adam},
	month = may,
	year = {2017},
	note = {arXiv: 1705.02012},
	keywords = {Computer Science - Computation and Language}
}

@inproceedings{du_harvesting_2018,
	address = {Melbourne, Australia},
	title = {Harvesting {Paragraph}-level {Question}-{Answer} {Pairs} from {Wikipedia}},
	url = {http://www.aclweb.org/anthology/P18-1177},
	abstract = {We study the task of generating from Wikipedia articles question-answer pairs that cover content beyond a single sentence. We propose a neural network approach that incorporates coreference knowledge via a novel gating mechanism. As compared to models that only take into account sentence-level information (Heilman and Smith, 2010; Du et al., 2017; Zhou et al., 2017), we find that the linguistic knowledge introduced by the coreference representation aids question generation significantly, producing models that outperform the current state-of-the-art. We apply our system (composed of an answer span extraction system and the passage-level QG system) to the 10,000 top ranking Wikipedia articles and create a corpus of over one million question-answer pairs. We provide qualitative analysis for the this large-scale generated corpus from Wikipedia.},
	urldate = {2019-02-28},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Du, Xinya and Cardie, Claire},
	month = jul,
	year = {2018},
	pages = {1907--1917}
}

@inproceedings{zhao_paragraph-level_2018,
	address = {Brussels, Belgium},
	title = {Paragraph-level {Neural} {Question} {Generation} with {Maxout} {Pointer} and {Gated} {Self}-attention {Networks}},
	url = {http://www.aclweb.org/anthology/D18-1424},
	urldate = {2019-02-28},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Yao and Ni, Xiaochuan and Ding, Yuanyuan and Ke, Qifa},
	month = nov,
	year = {2018},
	pages = {3901--3910}
}

@article{olney_question_2012,
	title = {Question {Generation} from {Concept} {Maps}},
	volume = {3},
	issn = {2152-9620},
	url = {1480.html},
	doi = {10.5087/d&d.v3i2.1480},
	abstract = {In this paper we present a question generation approach suitable for tutorial dialogues. The approach is based on previous psychological theories that hypothesize questions are generated from a knowledge representation modeled as a concept map. Our model automatically extracts concept maps from a textbook and uses them to generate questions. The purpose of the study is to generate and evaluate pedagogically-appropriate questions at varying levels of specificity across one or more sentences. The evaluation metrics include scales from the Question Generation Shared Task and Evaluation Challenge and a new scale specific to the pedagogical nature of questions in tutoring.},
	language = {en},
	number = {2},
	urldate = {2019-02-28},
	journal = {Dialogue \& Discourse},
	author = {Olney, Andrew M. and Graesser, Arthur C. and Person, Natalie K.},
	month = mar,
	year = {2012},
	pages = {75--99--99}
}

@article{yao_semantics-based_2012,
	title = {Semantics-based {Question} {Generation} and {Implementation}},
	volume = {3},
	doi = {10.5087/d},
	abstract = {This paper presents a question generation system based on the approach of semantic rewriting. State-of-the-art deep linguistic parsing and generation tools are employed to map natural language sentences into their meaning representations in the form of Minimal Recursion Semantics (mrs) and vice versa. By carefully operating on the semantic structures, we obtain a principled way of generating questions which avoids ad-hoc manipulation of syntactic structures. Based on the (partial) understanding of the sentence meaning, the system generates questions that are semantically grounded and purposeful. As the generator uses a deep linguistic grammar, the grammaticality of the generation results is licensed by the grammar. With a specialized ranking model, the linguistic realizations from the general purpose generation model are further refined for the question generation task. The evaluation results from QGSTEC2010 show promising results for the proposed approach.},
	journal = {D\&D},
	author = {Yao, Xuchen and Bouma, Gosse and Zhang, Yi},
	year = {2012},
	keywords = {Hoc (programming language), Minimal recursion semantics, Natural language, Parsing, Rewriting},
	pages = {11--42}
}

@article{yao_semantics-based_2012-1,
	title = {Semantics-based {Question} {Generation} and {Implementation}},
	volume = {3},
	issn = {2152-9620},
	url = {http://www.elanguage.net/journals/dad/article/view/1439/2825},
	doi = {10.5087/dad.2012.202},
	abstract = {This paper presents a question generation system based on the approach of semantic rewriting. State-of-the-art deep linguistic parsing and generation tools are employed to map natural language sentences into their meaning representations in the form of Minimal Recursion Semantics (mrs) and vice versa. By carefully operating on the semantic structures, we obtain a principled way of generating questions which avoids ad-hoc manipulation of syntactic structures. Based on the (partial) understanding of the sentence meaning, the system generates questions that are semantically grounded and purposeful. As the generator uses a deep linguistic grammar, the grammaticality of the generation results is licensed by the grammar. With a specialized ranking model, the linguistic realizations from the general purpose generation model are further reﬁned for the question generation task. The evaluation results from QGSTEC2010 show promising results for the proposed approach.},
	language = {en},
	number = {2},
	urldate = {2019-02-28},
	journal = {Dialogue \& Discourse},
	author = {Yao, Xuchen and Bouma, Gosse and Zhang, Yi},
	year = {2012},
	pages = {11--42}
}

@misc{noauthor_pdf_nodate,
	title = {({PDF}) {Semantics}-based {Question} {Generation} and {Implementation}},
	url = {https://www.researchgate.net/publication/264851328_Semantics-based_Question_Generation_and_Implementation},
	abstract = {PDF {\textbar} This paper presents a question generation system based on the approach of semantic rewriting. State-of-the-art deep linguistic parsing and generation tools are employed to map natural language sentences into their meaning representations in the form of Minimal Recursion...},
	language = {en},
	urldate = {2019-02-28},
	journal = {ResearchGate},
	doi = {http://dx.doi.org/10.5087/dad.2012.202}
}

@inproceedings{rus_first_2010,
	address = {Stroudsburg, PA, USA},
	series = {{INLG} '10},
	title = {The {First} {Question} {Generation} {Shared} {Task} {Evaluation} {Challenge}},
	url = {http://dl.acm.org/citation.cfm?id=1873738.1873777},
	abstract = {The paper briefly describes the First Shared Task Evaluation Challenge on Question Generation that took place in Spring 2010. The campaign included two tasks: Task A - Question Generation from Paragraphs and Task B - Question Generation from Sentences. An overview of each of the tasks is provided.},
	urldate = {2019-02-28},
	booktitle = {Proceedings of the 6th {International} {Natural} {Language} {Generation} {Conference}},
	publisher = {Association for Computational Linguistics},
	author = {Rus, Vasile and Wyse, Brendan and Piwek, Paul and Lintean, Mihai and Stoyanchev, Svetlana and Moldovan, Cristian},
	year = {2010},
	note = {event-place: Trim, Co. Meath, Ireland},
	keywords = {question generation, shared task evaluation campaign},
	pages = {251--257}
}

@inproceedings{sugawara_what_2018,
	address = {Brussels, Belgium},
	title = {What {Makes} {Reading} {Comprehension} {Questions} {Easier}?},
	url = {http://www.aclweb.org/anthology/D18-1453},
	urldate = {2019-02-25},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Sugawara, Saku and Inui, Kentaro and Sekine, Satoshi and Aizawa, Akiko},
	month = nov,
	year = {2018},
	pages = {4208--4219}
}

@article{marcus2020next,
  title={The next decade in AI: four steps towards robust artificial intelligence},
  author={Marcus, Gary},
  journal={arXiv preprint arXiv:2002.06177},
  year={2020},
  url={https://arxiv.org/abs/2002.06177}
}

@inproceedings{marcus_penn_1994,
	address = {Plainsboro, NJ},
	title = {The {Penn} {Treebank}: annotating predicate argument structure},
	isbn = {978-1-55860-357-8},
	shorttitle = {The {Penn} {Treebank}},
	url = {http://portal.acm.org/citation.cfm?doid=1075812.1075835},
	doi = {10.3115/1075812.1075835},
	abstract = {The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of coindexed null elements in what can be thought of as "underlying" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides some non-context free annotational mechanism to allow the structure of discontinuous constituents to be easily recovered, and allows for a clear, concise tagging system for some semantic roles.},
	language = {en},
	urldate = {2019-02-25},
	booktitle = {Proceedings of the workshop on {Human} {Language} {Technology}  - {HLT} '94},
	publisher = {Association for Computational Linguistics},
	author = {Marcus, Mitchell and Kim, Grace and Marcinkiewicz, Mary Ann and MacIntyre, Robert and Bies, Ann and Ferguson, Mark and Katz, Karen and Schasberger, Britta},
	year = {1994},
	pages = {114}
}

@inproceedings{berant_semantic_2013,
	address = {Seattle, Washington, USA},
	title = {Semantic {Parsing} on {Freebase} from {Question}-{Answer} {Pairs}},
	url = {http://www.aclweb.org/anthology/D13-1160},
	urldate = {2019-02-25},
	booktitle = {Proceedings of the 2013 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Berant, Jonathan and Chou, Andrew and Frostig, Roy and Liang, Percy},
	month = oct,
	year = {2013},
	pages = {1533--1544}
}

@inproceedings{lei_semi-supervised_2016,
	address = {San Diego, California},
	title = {Semi-supervised {Question} {Retrieval} with {Gated} {Convolutions}},
	url = {http://www.aclweb.org/anthology/N16-1153},
	urldate = {2019-02-25},
	booktitle = {Proceedings of the 2016 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Lei, Tao and Joshi, Hrishikesh and Barzilay, Regina and Jaakkola, Tommi and Tymoshenko, Kateryna and Moschitti, Alessandro and Màrquez, Lluís},
	month = jun,
	year = {2016},
	pages = {1279--1289}
}

@inproceedings{shen_sentiment_2018,
	address = {Brussels, Belgium},
	title = {Sentiment {Classification} towards {Question}-{Answering} with {Hierarchical} {Matching} {Network}},
	url = {http://www.aclweb.org/anthology/D18-1401},
	urldate = {2019-02-25},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Shen, Chenlin and Sun, Changlong and Wang, Jingjing and Kang, Yangyang and Li, Shoushan and Liu, Xiaozhong and Si, Luo and Zhang, Min and Zhou, Guodong},
	month = nov,
	year = {2018},
	pages = {3654--3663}
}

@inproceedings{chen_semi-supervised_2018,
	address = {Santa Fe, New Mexico, USA},
	title = {Semi-{Supervised} {Lexicon} {Learning} for {Wide}-{Coverage} {Semantic} {Parsing}},
	url = {http://www.aclweb.org/anthology/C18-1076},
	abstract = {Semantic parsers critically rely on accurate and high-coverage lexicons.},
	urldate = {2019-02-25},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Bo and An, Bo and Sun, Le and Han, Xianpei},
	month = aug,
	year = {2018},
	pages = {892--904}
}

@article{speer_conceptnet_2016,
	title = {{ConceptNet} 5.5: {An} {Open} {Multilingual} {Graph} of {General} {Knowledge}},
	shorttitle = {{ConceptNet} 5.5},
	url = {http://arxiv.org/abs/1612.03975},
	abstract = {Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.},
	urldate = {2019-02-25},
	journal = {arXiv:1612.03975 [cs]},
	author = {Speer, Robyn and Chin, Joshua and Havasi, Catherine},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.03975},
	keywords = {Computer Science - Computation and Language, I.2.7}
}

@inproceedings{mihaylov_knowledgeable_2018,
	address = {Melbourne, Australia},
	title = {Knowledgeable {Reader}: {Enhancing} {Cloze}-{Style} {Reading} {Comprehension} with {External} {Commonsense} {Knowledge}},
	shorttitle = {Knowledgeable {Reader}},
	url = {http://www.aclweb.org/anthology/P18-1076},
	abstract = {We introduce a neural reading comprehension model that integrates external commonsense knowledge, encoded as a key-value memory, in a cloze-style setting. Instead of relying only on document-to-question interaction or discrete features as in prior work, our model attends to relevant external knowledge and combines this knowledge with the context representation before inferring the answer. This allows the model to attract and imply knowledge from an external knowledge source that is not explicitly stated in the text, but that is relevant for inferring the answer. Our model improves results over a very strong baseline on a hard Common Nouns dataset, making it a strong competitor of much more complex models. By including knowledge explicitly, our model can also provide evidence about the background knowledge used in the RC process.},
	urldate = {2019-02-25},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Mihaylov, Todor and Frank, Anette},
	month = jul,
	year = {2018},
	pages = {821--832}
}

@article{weissenborn_dynamic_2017,
	title = {Dynamic {Integration} of {Background} {Knowledge} in {Neural} {NLU} {Systems}},
	url = {http://arxiv.org/abs/1706.02596},
	abstract = {Common-sense and background knowledge is required to understand natural language, but in most neural natural language understanding (NLU) systems, this knowledge must be acquired from training corpora during learning, and then it is static at test time. We introduce a new architecture for the dynamic integration of explicit background knowledge in NLU models. A general-purpose reading module reads background knowledge in the form of free-text statements (together with task-specific text inputs) and yields refined word representations to a task-specific NLU architecture that reprocesses the task inputs with these representations. Experiments on document question answering (DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness and flexibility of the approach. Analysis shows that our model learns to exploit knowledge in a semantically appropriate way.},
	urldate = {2019-02-25},
	journal = {arXiv:1706.02596 [cs]},
	author = {Weissenborn, Dirk and Kočiský, Tomáš and Dyer, Chris},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.02596},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing}
}

@article{xie_large-scale_2017,
	title = {Large-scale {Cloze} {Test} {Dataset} {Created} by {Teachers}},
	url = {http://arxiv.org/abs/1711.03225},
	abstract = {Cloze tests are widely adopted in language exams to evaluate students' language proficiency. In this paper, we propose the first large-scale human-created cloze test dataset CLOTH, containing questions used in middle-school and high-school language exams. With missing blanks carefully created by teachers and candidate choices purposely designed to be nuanced, CLOTH requires a deeper language understanding and a wider attention span than previously automatically-generated cloze datasets. We test the performance of dedicatedly designed baseline models including a language model trained on the One Billion Word Corpus and show humans outperform them by a significant margin. We investigate the source of the performance gap, trace model deficiencies to some distinct properties of CLOTH, and identify the limited ability of comprehending the long-term context to be the key bottleneck.},
	urldate = {2019-02-25},
	journal = {arXiv:1711.03225 [cs]},
	author = {Xie, Qizhe and Lai, Guokun and Dai, Zihang and Hovy, Eduard},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.03225},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@inproceedings{wang_multi-perspective_2018,
	address = {Santa Fe, New Mexico, USA},
	title = {Multi-{Perspective} {Context} {Aggregation} for {Semi}-supervised {Cloze}-style {Reading} {Comprehension}},
	url = {http://www.aclweb.org/anthology/C18-1073},
	abstract = {Cloze-style reading comprehension has been a popular task for measuring the progress of natural language understanding in recent years. In this paper, we design a novel multi-perspective framework, which can be seen as the joint training of heterogeneous experts and aggregate context information from different perspectives. Each perspective is modeled by a simple aggregation module. The outputs of multiple aggregation modules are fed into a one-timestep pointer network to get the final answer. At the same time, to tackle the problem of insufficient labeled data, we propose an efficient sampling mechanism to automatically generate more training examples by matching the distribution of candidates between labeled and unlabeled data. We conduct our experiments on a recently released cloze-test dataset CLOTH (Xie et al., 2017), which consists of nearly 100k questions designed by professional teachers. Results show that our method achieves new state-of-the-art performance over previous strong baselines.},
	urldate = {2019-02-25},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Liang and Li, Sujian and Zhao, Wei and Shen, Kewei and Sun, Meng and Jia, Ruoyu and Liu, Jingming},
	month = aug,
	year = {2018},
	pages = {857--867}
}

@inproceedings{yang_semi-supervised_2017,
	address = {Vancouver, Canada},
	title = {Semi-{Supervised} {QA} with {Generative} {Domain}-{Adaptive} {Nets}},
	url = {http://aclweb.org/anthology/P17-1096},
	abstract = {We study the problem of semi-supervised question answering—-utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the \textit{Generative Domain-Adaptive Nets}. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.},
	urldate = {2019-02-25},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Yang, Zhilin and Hu, Junjie and Salakhutdinov, Ruslan and Cohen, William},
	month = jul,
	year = {2017},
	pages = {1040--1050}
}

@article{dhingra_gated-attention_2016,
	title = {Gated-{Attention} {Readers} for {Text} {Comprehension}},
	url = {http://arxiv.org/abs/1606.01549},
	abstract = {In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task--the CNN {\textbackslash}\& Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention. The code is available at https://github.com/bdhingra/ga-reader.},
	urldate = {2019-02-25},
	journal = {arXiv:1606.01549 [cs]},
	author = {Dhingra, Bhuwan and Liu, Hanxiao and Yang, Zhilin and Cohen, William W. and Salakhutdinov, Ruslan},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.01549},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@misc{noauthor_unsupervised_nodate,
	title = {Unsupervised {QA}},
	url = {https://www.overleaf.com/project/5c4f1a5f7086717f84b6e467},
	abstract = {An online LaTeX editor that's easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2019-02-21}
}

@article{stern_minimal_2017,
	title = {A {Minimal} {Span}-{Based} {Neural} {Constituency} {Parser}},
	url = {http://arxiv.org/abs/1705.03919},
	abstract = {In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).},
	urldate = {2019-02-21},
	journal = {arXiv:1705.03919 [cs]},
	author = {Stern, Mitchell and Andreas, Jacob and Klein, Dan},
	month = may,
	year = {2017},
	note = {arXiv: 1705.03919},
	keywords = {Computer Science - Computation and Language}
}

@article{lewis_generative_2018,
	title = {Generative {Question} {Answering}: {Learning} to {Answer} the {Whole} {Question}},
	shorttitle = {Generative {Question} {Answering}},
	url = {https://openreview.net/forum?id=Bkx0RjA9tX},
	abstract = {Discriminative  question  answering  models  can  overfit  to  superficial  biases  in datasets,  because their loss function saturates when any clue makes the answer likely.  We introduce...},
	urldate = {2019-02-20},
	author = {Lewis, Mike and Fan, Angela},
	month = sep,
	year = {2018}
}

@article{kaushik_how_2018,
	title = {How {Much} {Reading} {Does} {Reading} {Comprehension} {Require}? {A} {Critical} {Investigation} of {Popular} {Benchmarks}},
	shorttitle = {How {Much} {Reading} {Does} {Reading} {Comprehension} {Require}?},
	url = {http://arxiv.org/abs/1808.04926},
	abstract = {Many recent papers address reading comprehension, where examples consist of (question, passage, answer) tuples. Presumably, a model must combine information from both questions and passages to predict corresponding answers. However, despite intense interest in the topic, with hundreds of published papers vying for leaderboard dominance, basic questions about the difficulty of many popular benchmarks remain unanswered. In this paper, we establish sensible baselines for the bAbI, SQuAD, CBT, CNN, and Who-did-What datasets, finding that question- and passage-only models often perform surprisingly well. On \$14\$ out of \$20\$ bAbI tasks, passage-only models achieve greater than \$50{\textbackslash}\%\$ accuracy, sometimes matching the full model. Interestingly, while CBT provides \$20\$-sentence stories only the last is needed for comparably accurate prediction. By comparison, SQuAD and CNN appear better-constructed.},
	urldate = {2019-02-20},
	journal = {arXiv:1808.04926 [cs, stat]},
	author = {Kaushik, Divyansh and Lipton, Zachary C.},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.04926},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning}
}
@article{lample_phrase-based_2018,
	title = {Phrase-{Based} \& {Neural} {Unsupervised} {Machine} {Translation}},
	url = {https://arxiv.org/abs/1804.07755v2},
	abstract = {Machine translation systems achieve near human-level performance on some
languages, yet their effectiveness strongly relies on the availability of large
amounts of parallel sentences, which hinders their applicability to the
majority of language pairs. This work investigates how to learn to translate
when having access to only large monolingual corpora in each language. We
propose two model variants, a neural and a phrase-based model. Both versions
leverage a careful initialization of the parameters, the denoising effect of
language models and automatic generation of parallel data by iterative
back-translation. These models are significantly better than methods from the
literature, while being simpler and having fewer hyper-parameters. On the
widely used WMT'14 English-French and WMT'16 German-English benchmarks, our
models respectively obtain 28.1 and 25.2 BLEU points without using a single
parallel sentence, outperforming the state of the art by more than 11 BLEU
points. On low-resource languages like English-Urdu and English-Romanian, our
methods achieve even better results than semi-supervised and supervised
approaches leveraging the paucity of available bitexts. Our code for NMT and
PBSMT is publicly available.},
	language = {en},
	urldate = {2019-02-18},
	author = {Lample, Guillaume and Ott, Myle and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
	month = apr,
	year = {2018}
}

@inproceedings{heilman_good_2010,
	address = {Stroudsburg, PA, USA},
	series = {{HLT} '10},
	title = {Good {Question}! {Statistical} {Ranking} for {Question} {Generation}},
	isbn = {978-1-932432-65-7},
	url = {http://dl.acm.org/citation.cfm?id=1857999.1858085},
	abstract = {We address the challenge of automatically generating questions from reading materials for educational practice and assessment. Our approach is to overgenerate questions, then rank them. We use manually written rules to perform a sequence of general purpose syntactic transformations (e.g., subject-auxiliary inversion) to turn declarative sentences into questions. These questions are then ranked by a logistic regression model trained on a small, tailored dataset consisting of labeled output from our system. Experimental results show that ranking nearly doubles the percentage of questions rated as acceptable by annotators, from 27\% of all questions to 52\% of the top ranked 20\% of questions.},
	urldate = {2019-02-13},
	booktitle = {Human {Language} {Technologies}: {The} 2010 {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Heilman, Michael and Smith, Noah A.},
	year = {2010},
	pages = {609--617}
}

@article{almahairi_augmented_2018,
	title = {Augmented {CycleGAN}: {Learning} {Many}-to-{Many} {Mappings} from {Unpaired} {Data}},
	shorttitle = {Augmented {CycleGAN}},
	url = {http://arxiv.org/abs/1802.10151},
	abstract = {Learning inter-domain mappings from unpaired data can improve performance in structured prediction tasks, such as image segmentation, by reducing the need for paired data. CycleGAN was recently proposed for this problem, but critically assumes the underlying inter-domain mapping is approximately deterministic and one-to-one. This assumption renders the model ineffective for tasks requiring flexible, many-to-many mappings. We propose a new model, called Augmented CycleGAN, which learns many-to-many mappings between domains. We examine Augmented CycleGAN qualitatively and quantitatively on several image datasets.},
	urldate = {2019-02-11},
	journal = {arXiv:1802.10151 [cs]},
	author = {Almahairi, Amjad and Rajeswar, Sai and Sordoni, Alessandro and Bachman, Philip and Courville, Aaron},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.10151},
	keywords = {Computer Science - Machine Learning}
}

@misc{noauthor_unsupervised_nodate,
	title = {Unsupervised {QA} ({Copy})},
	url = {https://www.overleaf.com/project/5c5d5aba9f687c21b0f59eea},
	abstract = {An online LaTeX editor that's easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2019-02-08}
}

@article{yogatama_learning_2019,
	title = {Learning and {Evaluating} {General} {Linguistic} {Intelligence}},
	url = {http://arxiv.org/abs/1901.11373},
	abstract = {We define general linguistic intelligence as the ability to reuse previously acquired knowledge about a language's lexicon, syntax, semantics, and pragmatic conventions to adapt to new tasks quickly. Using this definition, we analyze state-of-the-art natural language understanding models and conduct an extensive empirical investigation to evaluate them against these criteria through a series of experiments that assess the task-independence of the knowledge being acquired by the learning process. In addition to task performance, we propose a new evaluation metric based on an online encoding of the test data that quantifies how quickly an existing agent (model) learns a new task. Our results show that while the field has made impressive progress in terms of model architectures that generalize to many tasks, these models still require a lot of in-domain training examples (e.g., for fine tuning, training task-specific modules), and are prone to catastrophic forgetting. Moreover, we find that far from solving general tasks (e.g., document question answering), our models are overfitting to the quirks of particular datasets (e.g., SQuAD). We discuss missing components and conjecture on how to make progress toward general linguistic intelligence.},
	urldate = {2019-02-04},
	journal = {arXiv:1901.11373 [cs, stat]},
	author = {Yogatama, Dani and d'Autume, Cyprien de Masson and Connor, Jerome and Kocisky, Tomas and Chrzanowski, Mike and Kong, Lingpeng and Lazaridou, Angeliki and Ling, Wang and Yu, Lei and Dyer, Chris and Blunsom, Phil},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.11373},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@misc{noauthor_scratch3_nodate,
	title = {scratch3},
	url = {http://localhost:8888/notebooks/scratch3.ipynb},
	urldate = {2019-01-16}
}

@article{kim_textbook_2018,
	title = {Textbook {Question} {Answering} with {Knowledge} {Graph} {Understanding} and {Unsupervised} {Open}-set {Text} {Comprehension}},
	url = {http://arxiv.org/abs/1811.00232},
	abstract = {In this work, we introduce a novel algorithm for solving the textbook question answering (TQA) task which describes more realistic QA problems compared to other recent tasks. We mainly focus on two related issues with analysis of TQA dataset. First, it requires to comprehend long lessons to extract knowledge. To tackle this issue of extracting knowledge features from long lessons, we establish knowledge graph from texts and incorporate graph convolutional network (GCN). Second, scientific terms are not spread over the chapters and data splits in TQA dataset. To overcome this so called `out-of-domain' issue, we add novel unsupervised text learning process without any annotations before learning QA problems. The experimental results show that our model significantly outperforms prior state-of-the-art methods. Moreover, ablation studies validate that both methods of incorporating GCN for extracting knowledge from long lessons and our newly proposed unsupervised learning process are meaningful to solve this problem.},
	urldate = {2019-01-10},
	journal = {arXiv:1811.00232 [cs]},
	author = {Kim, Daesik and Kim, Seonhoon and Kwak, Nojun},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.00232},
	keywords = {Computer Science - Computation and Language}
}

@article{bradshaw_adversarial_2017,
	title = {Adversarial {Examples}, {Uncertainty}, and {Transfer} {Testing} {Robustness} in {Gaussian} {Process} {Hybrid} {Deep} {Networks}},
	url = {http://arxiv.org/abs/1707.02476},
	abstract = {Deep neural networks (DNNs) have excellent representative power and are state of the art classifiers on many tasks. However, they often do not capture their own uncertainties well making them less robust in the real world as they overconfidently extrapolate and do not notice domain shift. Gaussian processes (GPs) with RBF kernels on the other hand have better calibrated uncertainties and do not overconfidently extrapolate far from data in their training set. However, GPs have poor representational power and do not perform as well as DNNs on complex domains. In this paper we show that GP hybrid deep networks, GPDNNs, (GPs on top of DNNs and trained end-to-end) inherit the nice properties of both GPs and DNNs and are much more robust to adversarial examples. When extrapolating to adversarial examples and testing in domain shift settings, GPDNNs frequently output high entropy class probabilities corresponding to essentially "don't know". GPDNNs are therefore promising as deep architectures that know when they don't know.},
	urldate = {2019-01-06},
	journal = {arXiv:1707.02476 [stat]},
	author = {Bradshaw, John and Matthews, Alexander G. de G. and Ghahramani, Zoubin},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.02476},
	keywords = {Statistics - Machine Learning}
}

@article{cui_span-extraction_2018,
	title = {A {Span}-{Extraction} {Dataset} for {Chinese} {Machine} {Reading} {Comprehension}},
	url = {http://arxiv.org/abs/1810.07366},
	abstract = {Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, the existing reading comprehension datasets are mostly in English. In this paper, we introduce a Span-Extraction dataset for Chinese Machine Reading Comprehension to add language diversities in this area. The dataset is composed by near 20,000 real questions annotated by human on Wikipedia paragraphs. We also annotated a challenge set which contains the questions that need comprehensive understanding and multi-sentence inference throughout the context. With the release of the dataset, we hosted the Second Evaluation Workshop on Chinese Machine Reading Comprehension (CMRC 2018). We hope the release of the dataset could further accelerate the machine reading comprehension research in Chinese language. The data is available through: https://github.com/ymcui/cmrc2018},
	urldate = {2018-12-06},
	journal = {arXiv:1810.07366 [cs]},
	author = {Cui, Yiming and Liu, Ting and Xiao, Li and Chen, Zhipeng and Ma, Wentao and Che, Wanxiang and Wang, Shijin and Hu, Guoping},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.07366},
	keywords = {Computer Science - Computation and Language}
}

@misc{olvera-lobo_language_2012,
	type = {Journal article ({Paginated})},
	title = {Language resources for translation in multilingual question answering systems},
	copyright = {cc\_by},
	url = {http://eprints.rclis.org/32920/},
	abstract = {In the field of Information Retrieval monolingual and multilingual tools are being created that can greatly assist specialists in their work; as well as helping other users find a wide variety of information. Multilingual tools are evolving but several years of study and research are still needed to improve implementations. One of the main difficulties facing these tools is the task of translating queries made by users and the documentary sources found in response (Diekema, 2003). Given the current expansion in research, development, and the creation of multilingual IR systems, it was considered worthwhile analysing and evaluating the resources used by one type of these systems, the multilingual Question Answering systems. Our article is primarily intended as a general purpose analysis and aims to encompass translation in the study of multilingual QA systems. The second general aim is to identify and analyse the linguistic resources and tools found in these systems. Specific objectives include identifying the main types of language resources and tools useful in the multilingual IR processes associated with multilingual QA systems, and establishing how much use is made of these tools by multilingual QA systems.},
	language = {en},
	urldate = {2018-12-04},
	journal = {Translation journal},
	author = {Olvera-Lobo, María-Dolores and Gutiérrez-Artacho, Juncal},
	month = apr,
	year = {2012}
}

@incollection{gangemi_demoing_2018,
	address = {Cham},
	title = {Demoing {Platypus} – {A} {Multilingual} {Question} {Answering} {Platform} for {Wikidata}},
	volume = {11155},
	isbn = {978-3-319-98191-8 978-3-319-98192-5},
	url = {http://link.springer.com/10.1007/978-3-319-98192-5_21},
	abstract = {In this paper we present Platypus, a natural language question answering system on Wikidata. Our platform can answer complex queries in several languages, using hybrid grammatical and template based techniques. Our demo allows users either to select sample questions, or formulate their own – in any of the 3 languages that we currently support. A user can also try out our Twitter bot, which replies to any tweet that is sent to its account.},
	language = {en},
	urldate = {2018-12-04},
	booktitle = {The {Semantic} {Web}: {ESWC} 2018 {Satellite} {Events}},
	publisher = {Springer International Publishing},
	author = {Pellissier Tanon, Thomas and de Assunção, Marcos Dias and Caron, Eddy and Suchanek, Fabian M.},
	editor = {Gangemi, Aldo and Gentile, Anna Lisa and Nuzzolese, Andrea Giovanni and Rudolph, Sebastian and Maleshkova, Maria and Paulheim, Heiko and Pan, Jeff Z and Alam, Mehwish},
	year = {2018},
	doi = {10.1007/978-3-319-98192-5_21},
	pages = {111--116}
}

@inproceedings{ture_learning_2016,
	address = {Austin, Texas},
	title = {Learning to {Translate} for {Multilingual} {Question} {Answering}},
	url = {https://aclweb.org/anthology/D16-1055},
	urldate = {2018-12-04},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Ture, Ferhan and Boschee, Elizabeth},
	month = nov,
	year = {2016},
	pages = {573--584}
}

@article{noauthor_analysis_nodate,
	title = {Analysis of {Joint} {Multilingual} {Sentence} {Representations} and {Semantic} {K}-{Nearest} {Neighbor} {Graphs}},
	abstract = {Multilingual sentence and document representations are be- coming increasingly important. We build on recent advances in multilingual sentence encoders, with a focus on efficiency and large-scale applicability. Specifically, we construct and investigate the k-nn graph over the joint space of 566 mil- lion news sentences in seven different languages. We show excellent multilingual retrieval quality on the UN corpus of 11.3M sentences, which extends to the zero-shot case where we have never seen a language. We provide a detailed analy- sis of both the multilingual sentence encoder for twenty-one European languages and the learned graph. Our sentence en- coder is language agnostic and supports code switching.}
}

@article{joty_cross-language_2017,
	title = {Cross-language {Learning} with {Adversarial} {Neural} {Networks}: {Application} to {Community} {Question} {Answering}},
	shorttitle = {Cross-language {Learning} with {Adversarial} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1706.06749},
	abstract = {We address the problem of cross-language adaptation for question-question similarity reranking in community question answering, with the objective to port a system trained on one input language to another input language given labeled training data for the first language and only unlabeled data for the second language. In particular, we propose to use adversarial training of neural networks to learn high-level features that are discriminative for the main learning task, and at the same time are invariant across the input languages. The evaluation results show sizable improvements for our cross-language adversarial neural network (CLANN) model over a strong non-adversarial system.},
	urldate = {2018-12-02},
	journal = {arXiv:1706.06749 [cs]},
	author = {Joty, Shafiq and Nakov, Preslav and Màrquez, Lluís and Jaradat, Israa},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.06749},
	keywords = {Computer Science - Computation and Language}
}

@article{chen_adversarial_2016,
	title = {Adversarial {Deep} {Averaging} {Networks} for {Cross}-{Lingual} {Sentiment} {Classification}},
	url = {http://arxiv.org/abs/1606.01614},
	abstract = {In recent years great success has been achieved in sentiment classification for English, thanks in part to the availability of copious annotated resources. Unfortunately, most languages do not enjoy such an abundance of labeled data. To tackle the sentiment classification problem in low-resource languages without adequate annotated data, we propose an Adversarial Deep Averaging Network (ADAN) to transfer the knowledge learned from labeled data on a resource-rich source language to low-resource languages where only unlabeled data exists. ADAN has two discriminative branches: a sentiment classifier and an adversarial language discriminator. Both branches take input from a shared feature extractor to learn hidden representations that are simultaneously indicative for the classification task and invariant across languages. Experiments on Chinese and Arabic sentiment classification demonstrate that ADAN significantly outperforms state-of-the-art systems.},
	urldate = {2018-12-02},
	journal = {arXiv:1606.01614 [cs]},
	author = {Chen, Xilun and Sun, Yu and Athiwaratkun, Ben and Cardie, Claire and Weinberger, Kilian},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.01614},
	keywords = {Computer Science - Computation and Language}
}

@article{conneau_xnli:_2018,
	title = {{XNLI}: {Evaluating} {Cross}-lingual {Sentence} {Representations}},
	shorttitle = {{XNLI}},
	url = {http://arxiv.org/abs/1809.05053},
	abstract = {State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in crosslingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 15 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We ﬁnd that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines.},
	language = {en},
	urldate = {2018-12-02},
	journal = {arXiv:1809.05053 [cs]},
	author = {Conneau, Alexis and Lample, Guillaume and Rinott, Ruty and Williams, Adina and Bowman, Samuel R. and Schwenk, Holger and Stoyanov, Veselin},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.05053},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@article{yang_semi-supervised_2017,
	title = {Semi-{Supervised} {QA} with {Generative} {Domain}-{Adaptive} {Nets}},
	url = {http://arxiv.org/abs/1702.02206},
	abstract = {We study the problem of semi-supervised question answering----utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the Generative Domain-Adaptive Nets. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.},
	urldate = {2018-12-02},
	journal = {arXiv:1702.02206 [cs]},
	author = {Yang, Zhilin and Hu, Junjie and Salakhutdinov, Ruslan and Cohen, William W.},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.02206},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@article{goyal_accurate_2017,
	title = {Accurate, {Large} {Minibatch} {SGD}: {Training} {ImageNet} in 1 {Hour}},
	shorttitle = {Accurate, {Large} {Minibatch} {SGD}},
	url = {http://arxiv.org/abs/1706.02677},
	abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves {\textasciitilde}90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
	urldate = {2018-11-30},
	journal = {arXiv:1706.02677 [cs]},
	author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.02677},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning}
}

@article{sergeev_horovod:_2018,
	title = {Horovod: fast and easy distributed deep learning in {TensorFlow}},
	shorttitle = {Horovod},
	url = {http://arxiv.org/abs/1802.05799},
	abstract = {Training modern deep learning models requires large amounts of computation, often provided by GPUs. Scaling computation from one GPU to many can enable much faster training and research progress but entails two complications. First, the training library must support inter-GPU communication. Depending on the particular methods employed, this communication may entail anywhere from negligible to significant overhead. Second, the user must modify his or her training code to take advantage of inter-GPU communication. Depending on the training library's API, the modification required may be either significant or minimal. Existing methods for enabling multi-GPU training under the TensorFlow library entail non-negligible communication overhead and require users to heavily modify their model-building code, leading many researchers to avoid the whole mess and stick with slower single-GPU training. In this paper we introduce Horovod, an open source library that improves on both obstructions to scaling: it employs efficient inter-GPU communication via ring reduction and requires only a few lines of modification to user code, enabling faster, easier distributed training in TensorFlow. Horovod is available under the Apache 2.0 license at https://github.com/uber/horovod},
	urldate = {2018-11-30},
	journal = {arXiv:1802.05799 [cs, stat]},
	author = {Sergeev, Alexander and Del Balso, Mike},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.05799},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{peters_deep_2018,
	title = {Deep contextualized word representations},
	url = {http://arxiv.org/abs/1802.05365},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	urldate = {2018-11-30},
	journal = {arXiv:1802.05365 [cs]},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.05365},
	keywords = {Computer Science - Computation and Language}
}

@article{clark_simple_2017,
	title = {Simple and {Effective} {Multi}-{Paragraph} {Reading} {Comprehension}},
	url = {http://arxiv.org/abs/1710.10723},
	abstract = {We consider the problem of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Our proposed solution trains models to produce well calibrated confidence scores for their results on individual paragraphs. We sample multiple paragraphs from the documents during training, and use a shared-normalization training objective that encourages the model to produce globally correct output. We combine this method with a state-of-the-art pipeline for training models on document QA data. Experiments demonstrate strong performance on several document QA datasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system.},
	urldate = {2018-11-30},
	journal = {arXiv:1710.10723 [cs]},
	author = {Clark, Christopher and Gardner, Matt},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.10723},
	keywords = {Computer Science - Computation and Language}
}

@article{fitzgerald_large-scale_2018,
	title = {Large-{Scale} {QA}-{SRL} {Parsing}},
	url = {http://arxiv.org/abs/1805.05377},
	abstract = {We present a new large-scale corpus of Question-Answer driven Semantic Role Labeling (QA-SRL) annotations, and the first high-quality QA-SRL parser. Our corpus, QA-SRL Bank 2.0, consists of over 250,000 question-answer pairs for over 64,000 sentences across 3 domains and was gathered with a new crowd-sourcing scheme that we show has high precision and good recall at modest cost. We also present neural models for two QA-SRL subtasks: detecting argument spans for a predicate and generating questions to label the semantic relationship. The best models achieve question accuracy of 82.6\% and span-level accuracy of 77.6\% (under human evaluation) on the full pipelined QA-SRL prediction task. They can also, as we show, be used to gather additional annotations at low cost.},
	urldate = {2018-11-28},
	journal = {arXiv:1805.05377 [cs]},
	author = {FitzGerald, Nicholas and Michael, Julian and He, Luheng and Zettlemoyer, Luke},
	month = may,
	year = {2018},
	note = {arXiv: 1805.05377},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@article{aissa_reinforcement_2018,
	title = {A {Reinforcement} {Learning}-driven {Translation} {Model} for {Search}-{Oriented} {Conversational} {Systems}},
	url = {http://arxiv.org/abs/1809.01495},
	abstract = {Search-oriented conversational systems rely on information needs expressed in natural language (NL). We focus here on the understanding of NL expressions for building keyword-based queries. We propose a reinforcement-learning-driven translation model framework able to 1) learn the translation from NL expressions to queries in a supervised way, and, 2) to overcome the lack of large-scale dataset by framing the translation model as a word selection approach and injecting relevance feedback in the learning process. Experiments are carried out on two TREC datasets and outline the effectiveness of our approach.},
	urldate = {2018-11-28},
	journal = {arXiv:1809.01495 [cs, stat]},
	author = {Aissa, Wafa and Soulier, Laure and Denoyer, Ludovic},
	month = aug,
	year = {2018},
	note = {arXiv: 1809.01495},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@incollection{deudon_learning_2018,
	title = {Learning semantic similarity in a continuous space},
	url = {http://papers.nips.cc/paper/7377-learning-semantic-similarity-in-a-continuous-space.pdf},
	urldate = {2018-11-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Deudon, Michel},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {992--1003}
}

@article{chen_neural_2018,
	title = {Neural {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/1806.07366},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	urldate = {2018-11-25},
	journal = {arXiv:1806.07366 [cs, stat]},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.07366},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{zellers_swag:_2018,
	title = {{SWAG}: {A} {Large}-{Scale} {Adversarial} {Dataset} for {Grounded} {Commonsense} {Inference}},
	shorttitle = {{SWAG}},
	url = {http://arxiv.org/abs/1808.05326},
	abstract = {Given a partial description like "she opened the hood of the car," humans can reason about the situation and anticipate what might come next ("then, she examined the engine"). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning. We present SWAG, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88\%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.},
	urldate = {2018-11-12},
	journal = {arXiv:1808.05326 [cs]},
	author = {Zellers, Rowan and Bisk, Yonatan and Schwartz, Roy and Choi, Yejin},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.05326},
	keywords = {Computer Science - Computation and Language}
}

@article{guu_generating_2017,
	title = {Generating {Sentences} by {Editing} {Prototypes}},
	url = {http://arxiv.org/abs/1709.08878},
	abstract = {We propose a new generative model of sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence. Compared to traditional models that generate from scratch either left-to-right or by first sampling a latent sentence vector, our prototype-then-edit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation. Furthermore, the model gives rise to a latent edit vector that captures interpretable semantics such as sentence similarity and sentence-level analogies.},
	urldate = {2018-11-12},
	journal = {arXiv:1709.08878 [cs, stat]},
	author = {Guu, Kelvin and Hashimoto, Tatsunori B. and Oren, Yonatan and Liang, Percy},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.08878},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning}
}

@article{subramanian_multiple-attribute_2018,
	title = {Multiple-{Attribute} {Text} {Style} {Transfer}},
	url = {http://arxiv.org/abs/1811.00552},
	abstract = {The dominant approach to unsupervised "style transfer" in text is based on the idea of learning a latent representation, which is independent of the attributes specifying its "style". In this paper, we show that this condition is not necessary and is not always met in practice, even with domain adversarial training that explicitly aims at learning such disentangled representations. We thus propose a new model that controls several factors of variation in textual data where this condition on disentanglement is replaced with a simpler mechanism based on back-translation. Our method allows control over multiple attributes, like gender, sentiment, product type, etc., and a more fine-grained control on the trade-off between content preservation and change of style with a pooling operator in the latent space. Our experiments demonstrate that the fully entangled model produces better generations, even when tested on new and more challenging benchmarks comprising reviews with multiple sentences and multiple attributes.},
	urldate = {2018-11-12},
	journal = {arXiv:1811.00552 [cs]},
	author = {Subramanian, Sandeep and Lample, Guillaume and Smith, Eric Michael and Denoyer, Ludovic and Ranzato, Marc'Aurelio and Boureau, Y.-Lan},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.00552},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@article{zhao_integrating_nodate,
	title = {Integrating {Transformer} and {Paraphrase} {Rules} for {Sentence} {Simplification}},
	abstract = {Sentence simpliﬁcation aims to reduce the complexity of a sentence while retaining its original meaning. Current models for sentence simpliﬁcation adopted ideas from machine translation studies and implicitly learned simpliﬁcation mapping rules from normalsimple sentence pairs. In this paper, we explore a novel model based on a multi-layer and multi-head attention architecture and we propose two innovative approaches to integrate the Simple PPDB (A Paraphrase Database for Simpliﬁcation), an external paraphrase knowledge base for simpliﬁcation that covers a wide range of real-world simpliﬁcation rules. The experiments show that the integration provides two major beneﬁts: (1) the integrated model outperforms multiple stateof-the-art baseline models for sentence simpliﬁcation in the literature (2) through analysis of the rule utilization, the model seeks to select more accurate simpliﬁcation rules. The code and models used in the paper are available at https://github.com/ Sanqiang/text\_simplification.},
	language = {en},
	author = {Zhao, Sanqiang and Meng, Rui and He, Daqing and Saptono, Andi and Parmanto, Bambang},
	pages = {10}
}

@inproceedings{pavlick_simple_2016,
	address = {Berlin, Germany},
	title = {Simple {PPDB}: {A} {Paraphrase} {Database} for {Simplification}},
	shorttitle = {Simple {PPDB}},
	url = {http://aclweb.org/anthology/P16-2024},
	doi = {10.18653/v1/P16-2024},
	abstract = {We release the Simple Paraphrase Database, a subset of of the Paraphrase Database (PPDB) adapted for the task of text simpliﬁcation. We train a supervised model to associate simpliﬁcation scores with each phrase pair, producing rankings competitive with state-of-theart lexical simpliﬁcation models. Our new simpliﬁcation database contains 4.5 million paraphrase rules, making it the largest available resource for lexical simpliﬁcation.},
	language = {en},
	urldate = {2018-11-05},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Pavlick, Ellie and Callison-Burch, Chris},
	year = {2016},
	pages = {143--148}
}

@article{colin_generating_nodate,
	title = {Generating {Syntactic} {Paraphrases}},
	abstract = {We study the automatic generation of syntactic paraphrases using four different models for generation: data-to-text generation, textto-text generation, text reduction and text expansion, We derive training data for each of these tasks from the WebNLG dataset and we show (i) that conditioning generation on syntactic constraints effectively permits the generation of syntactically distinct paraphrases for the same input and (ii) that exploiting different types of input (data, text or data+text) further increases the number of distinct paraphrases that can be generated for a given input.},
	language = {en},
	author = {Colin, Emilie and Gardent, Claire},
	pages = {7}
}

@article{xiao_hungarian_2017,
	title = {Hungarian {Layer}: {Logics} {Empowered} {Neural} {Architecture}},
	shorttitle = {Hungarian {Layer}},
	url = {http://arxiv.org/abs/1712.02555},
	abstract = {Neural architecture is a purely numeric framework, which fits the data as a continuous function. However, lacking of logic flow (e.g. {\textbackslash}textit\{if, for, while\}), traditional algorithms (e.g. {\textbackslash}textit\{Hungarian algorithm, A\${\textasciicircum}*\$ searching, decision tress algorithm\}) could not be embedded into this paradigm, which limits the theories and applications. In this paper, we reform the calculus graph as a dynamic process, which is guided by logic flow. Within our novel methodology, traditional algorithms could empower numerical neural network. Specifically, regarding the subject of sentence matching, we reformulate this issue as the form of task-assignment, which is solved by Hungarian algorithm. First, our model applies BiLSTM to parse the sentences. Then Hungarian layer aligns the matching positions. Last, we transform the matching results for soft-max regression by another BiLSTM. Extensive experiments show that our model outperforms other state-of-the-art baselines substantially.},
	urldate = {2018-10-25},
	journal = {arXiv:1712.02555 [cs]},
	author = {Xiao, Han and Chen, Yidong and Shi, Xiaodong},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.02555},
	keywords = {Computer Science - Computation and Language}
}

@inproceedings{fader_paraphrase-driven_2013,
	address = {Sofia, Bulgaria},
	title = {Paraphrase-{Driven} {Learning} for {Open} {Question} {Answering}},
	url = {http://www.aclweb.org/anthology/P13-1158},
	urldate = {2018-10-23},
	booktitle = {Proceedings of the 51st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Fader, Anthony and Zettlemoyer, Luke and Etzioni, Oren},
	year = {2013},
	pages = {1608--1618}
}

@article{lee_set_2018,
	title = {Set {Transformer}},
	url = {http://arxiv.org/abs/1810.00825},
	abstract = {Many machine learning tasks such as multiple instance learning, 3D shape recognition and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the permutation of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating increased performance compared to recent methods for set-structured data.},
	urldate = {2018-10-11},
	journal = {arXiv:1810.00825 [cs, stat]},
	author = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam R. and Choi, Seungjin and Teh, Yee Whye},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.00825},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{wang_integrating_2017,
	title = {Integrating {User} and {Agent} {Models}: {A} {Deep} {Task}-{Oriented} {Dialogue} {System}},
	shorttitle = {Integrating {User} and {Agent} {Models}},
	url = {http://arxiv.org/abs/1711.03697},
	abstract = {Task-oriented dialogue systems can efficiently serve a large number of customers and relieve people from tedious works. However, existing task-oriented dialogue systems depend on handcrafted actions and states or extra semantic labels, which sometimes degrades user experience despite the intensive human intervention. Moreover, current user simulators have limited expressive ability so that deep reinforcement Seq2Seq models have to rely on selfplay and only work in some special cases. To address those problems, we propose a uSer and Agent Model IntegrAtion (SAMIA) framework inspired by an observation that the roles of the user and agent models are asymmetric. Firstly, this SAMIA framework model the user model as a Seq2Seq learning problem instead of ranking or designing rules. Then the built user model is used as a leverage to train the agent model by deep reinforcement learning. In the test phase, the output of the agent model is filtered by the user model to enhance the stability and robustness. Experiments on a real-world coffee ordering dataset verify the effectiveness of the proposed SAMIA framework.},
	urldate = {2018-10-11},
	journal = {arXiv:1711.03697 [cs]},
	author = {Wang, Weiyan and WU, Yuxiang and Zhang, Yu and Lu, Zhongqi and Mo, Kaixiang and Yang, Qiang},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.03697},
	keywords = {Computer Science - Computation and Language}
}

@article{nogueira_task-oriented_2017,
	title = {Task-{Oriented} {Query} {Reformulation} with {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1704.04572},
	abstract = {Search engines play an important role in our everyday lives by assisting us in ﬁnding the information we need. When we input a complex query, however, results are often far from satisfactory. In this work, we introduce a query reformulation system based on a neural network that rewrites a query to maximize the number of relevant documents returned. We train this neural network with reinforcement learning. The actions correspond to selecting terms to build a reformulated query, and the reward is the document recall. We evaluate our approach on three datasets against strong baselines and show a relative improvement of 5-20\% in terms of recall. Furthermore, we present a simple method to estimate a conservative upperbound performance of a model in a particular environment and verify that there is still large room for improvements.},
	language = {en},
	urldate = {2018-10-10},
	journal = {arXiv:1704.04572 [cs]},
	author = {Nogueira, Rodrigo and Cho, Kyunghyun},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.04572},
	keywords = {Computer Science - Information Retrieval}
}
@article{weissenborn_jack_2018,
	title = {Jack the {Reader} - {A} {Machine} {Reading} {Framework}},
	url = {http://arxiv.org/abs/1806.08727},
	abstract = {Many Machine Reading and Natural Language Understanding tasks require reading supporting text in order to answer questions. For example, in Question Answering, the supporting text can be newswire or Wikipedia articles; in Natural Language Inference, premises can be seen as the supporting text and hypotheses as questions. Providing a set of useful primitives operating in a single framework of related tasks would allow for expressive modelling, and easier model comparison and replication. To that end, we present Jack the Reader (Jack), a framework for Machine Reading that allows for quick model prototyping by component reuse, evaluation of new models on existing datasets as well as integrating new datasets and applying them on a growing set of implemented baseline models. Jack is currently supporting (but not limited to) three tasks: Question Answering, Natural Language Inference, and Link Prediction. It is developed with the aim of increasing research efficiency and code reuse.},
	urldate = {2018-10-10},
	journal = {arXiv:1806.08727 [cs, stat]},
	author = {Weissenborn, Dirk and Minervini, Pasquale and Dettmers, Tim and Augenstein, Isabelle and Welbl, Johannes and Rocktäschel, Tim and Bošnjak, Matko and Mitchell, Jeff and Demeester, Thomas and Stenetorp, Pontus and Riedel, Sebastian},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.08727},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{clark_semi-supervised_2018,
	title = {Semi-{Supervised} {Sequence} {Modeling} with {Cross}-{View} {Training}},
	url = {https://arxiv.org/abs/1809.08370},
	language = {en},
	urldate = {2018-10-08},
	author = {Clark, Kevin and Luong, Minh-Thang and Manning, Christopher D. and Le, Quoc V.},
	month = sep,
	year = {2018}
}

@inproceedings{berant_semantic_2014,
	address = {Baltimore, Maryland},
	title = {Semantic {Parsing} via {Paraphrasing}},
	url = {http://aclweb.org/anthology/P14-1133},
	doi = {10.3115/v1/P14-1133},
	language = {en},
	urldate = {2018-10-05},
	booktitle = {Proceedings of the 52nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Berant, Jonathan and Liang, Percy},
	year = {2014},
	pages = {1415--1425}
}

@article{battaglia_relational_2018,
	title = {Relational inductive biases, deep learning, and graph networks},
	url = {http://arxiv.org/abs/1806.01261},
	abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning.},
	urldate = {2018-10-04},
	journal = {arXiv:1806.01261 [cs, stat]},
	author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.01261},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{yang_glomo:_2018,
	title = {{GLoMo}: {Unsupervisedly} {Learned} {Relational} {Graphs} as {Transferable} {Representations}},
	shorttitle = {{GLoMo}},
	url = {http://arxiv.org/abs/1806.05662},
	abstract = {Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (e.g., words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained (including GloVe embeddings, ELMo embeddings, and task-specific RNN hidden unit), or embedding-free units such as image pixels.},
	urldate = {2018-10-04},
	journal = {arXiv:1806.05662 [cs, stat]},
	author = {Yang, Zhilin and Zhao, Jake and Dhingra, Bhuwan and He, Kaiming and Cohen, William W. and Salakhutdinov, Ruslan and LeCun, Yann},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.05662},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{peters_deep_2018,
	title = {Deep contextualized word representations},
	url = {http://arxiv.org/abs/1802.05365},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	urldate = {2018-10-04},
	journal = {arXiv:1802.05365 [cs]},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.05365},
	keywords = {Computer Science - Computation and Language}
}

@article{joshi_extending_2018,
	title = {Extending a {Parser} to {Distant} {Domains} {Using} a {Few} {Dozen} {Partially} {Annotated} {Examples}},
	url = {http://arxiv.org/abs/1805.06556},
	abstract = {We revisit domain adaptation for parsers in the neural era. First we show that recent advances in word representations greatly diminish the need for domain adaptation when the target domain is syntactically similar to the source domain. As evidence, we train a parser on the Wall Street Jour- nal alone that achieves over 90\% F1 on the Brown corpus. For more syntactically dis- tant domains, we provide a simple way to adapt a parser using only dozens of partial annotations. For instance, we increase the percentage of error-free geometry-domain parses in a held-out set from 45\% to 73\% using approximately five dozen training examples. In the process, we demon- strate a new state-of-the-art single model result on the Wall Street Journal test set of 94.3\%. This is an absolute increase of 1.7\% over the previous state-of-the-art of 92.6\%.},
	urldate = {2018-10-04},
	journal = {arXiv:1805.06556 [cs]},
	author = {Joshi, Vidur and Peters, Matthew and Hopkins, Mark},
	month = may,
	year = {2018},
	note = {arXiv: 1805.06556},
	keywords = {Computer Science - Computation and Language}
}

@inproceedings{jia_adversarial_2017,
	address = {Copenhagen, Denmark},
	title = {Adversarial {Examples} for {Evaluating} {Reading} {Comprehension} {Systems}},
	url = {http://aclweb.org/anthology/D17-1215},
	doi = {10.18653/v1/D17-1215},
	abstract = {Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75\% F1 score to 36\%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7\%. We hope our insights will motivate the development of new models that understand language more precisely.},
	language = {en},
	urldate = {2018-10-03},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural}           {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Jia, Robin and Liang, Percy},
	year = {2017},
	pages = {2021--2031}
}

@article{wieting_paranmt-50m:_2017,
	title = {{ParaNMT}-{50M}: {Pushing} the {Limits} of {Paraphrastic} {Sentence} {Embeddings} with {Millions} of {Machine} {Translations}},
	shorttitle = {{ParaNMT}-{50M}},
	url = {https://arxiv.org/abs/1711.05732},
	language = {en},
	urldate = {2018-10-02},
	author = {Wieting, John and Gimpel, Kevin},
	month = nov,
	year = {2017}
}

@article{fader_paraphrase-driven_nodate,
	title = {Paraphrase-{Driven} {Learning} for {Open} {Question} {Answering}},
	abstract = {We study question answering as a machine learning problem, and induce a function that maps open-domain questions to queries over a database of web extractions. Given a large, community-authored, question-paraphrase corpus, we demonstrate that it is possible to learn a semantic lexicon and linear ranking function without manually annotating questions. Our approach automatically generalizes a seed lexicon and includes a scalable, parallelized perceptron parameter estimation scheme. Experiments show that our approach more than quadruples the recall of the seed lexicon, with only an 8\% loss in precision.},
	language = {en},
	author = {Fader, Anthony and Zettlemoyer, Luke and Etzioni, Oren},
	pages = {11}
}

@article{ganitkevitch_ppdb:_nodate,
	title = {{PPDB}: {The} {Paraphrase} {Database}},
	abstract = {We present the 1.0 release of our paraphrase database, PPDB. Its English portion, PPDB:Eng, contains over 220 million paraphrase pairs, consisting of 73 million phrasal and 8 million lexical paraphrases, as well as 140 million paraphrase patterns, which capture many meaning-preserving syntactic transformations. The paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion English words. We also release PPDB:Spa, a collection of 196 million Spanish paraphrases. Each paraphrase pair in PPDB contains a set of associated scores, including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the Google n-grams and the Annotated Gigaword corpus. Our release includes pruning tools that allow users to determine their own precision/recall tradeoff.},
	language = {en},
	author = {Ganitkevitch, Juri and Durme, Benjamin Van and Callison-Burch, Chris},
	pages = {7}
}

@article{choi_quac_2018,
	title = {{QuAC} : {Question} {Answering} in {Context}},
	shorttitle = {{QuAC}},
	url = {http://arxiv.org/abs/1808.07036},
	abstract = {We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http://quac.ai.},
	urldate = {2018-10-02},
	journal = {arXiv:1808.07036 [cs]},
	author = {Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.07036},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@article{reddy_coqa:_2018,
	title = {{CoQA}: {A} {Conversational} {Question} {Answering} {Challenge}},
	shorttitle = {{CoQA}},
	url = {http://arxiv.org/abs/1808.07042},
	abstract = {Humans gather information by engaging in conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning. We evaluate strong conversational and reading comprehension models on CoQA. The best system obtains an F1 score of 65.1\%, which is 23.7 points behind human performance (88.8\%), indicating there is ample room for improvement. We launch CoQA as a challenge to the community at http://stanfordnlp.github.io/coqa/},
	urldate = {2018-10-02},
	journal = {arXiv:1808.07042 [cs]},
	author = {Reddy, Siva and Chen, Danqi and Manning, Christopher D.},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.07042},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@article{wang_integrating_2017,
	title = {Integrating {User} and {Agent} {Models}: {A} {Deep} {Task}-{Oriented} {Dialogue} {System}},
	shorttitle = {Integrating {User} and {Agent} {Models}},
	url = {http://arxiv.org/abs/1711.03697},
	abstract = {Task-oriented dialogue systems can efﬁciently serve a large number of customers and relieve people from tedious works. However, existing task-oriented dialogue systems depend on handcrafted actions and states or extra semantic labels, which sometimes degrades user experience despite the intensive human intervention. Moreover, current user simulators have limited expressive ability so that deep reinforcement Seq2Seq models have to rely on selfplay and only work in some special cases. To address those problems, we propose a uSer and Agent Model IntegrAtion (SAMIA) framework inspired by an observation that the roles of the user and agent models are asymmetric. Firstly, this SAMIA framework model the user model as a Seq2Seq learning problem instead of ranking or designing rules. Then the built user model is used as a leverage to train the agent model by deep reinforcement learning. In the test phase, the output of the agent model is ﬁltered by the user model to enhance the stability and robustness. Experiments on a real-world coffee ordering dataset verify the effectiveness of the proposed SAMIA framework.},
	language = {en},
	urldate = {2018-10-01},
	journal = {arXiv:1711.03697 [cs]},
	author = {Wang, Weiyan and WU, Yuxiang and Zhang, Yu and Lu, Zhongqi and Mo, Kaixiang and Yang, Qiang},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.03697},
	keywords = {Computer Science - Computation and Language}
}

@article{saeidi_interpretation_2018,
	title = {Interpretation of {Natural} {Language} {Rules} in {Conversational} {Machine} {Reading}},
	url = {http://arxiv.org/abs/1809.01494},
	abstract = {Most work in machine reading focuses on question answering problems where the answer is directly expressed in the text to read. However, many real-world question answering problems require the reading of text not because it contains the literal answer, but because it contains a recipe to derive an answer together with the reader’s background knowledge. One example is the task of interpreting regulations to answer “Can I...?” or “Do I have to...?” questions such as “I am working in Canada. Do I have to carry on paying UK National Insurance?” after reading a UK government website about this topic. This task requires both the interpretation of rules and the application of background knowledge. It is further complicated due to the fact that, in practice, most questions are underspeciﬁed, and a human assistant will regularly have to ask clariﬁcation questions such as “How long have you been working abroad?” when the answer cannot be directly derived from the question and text. In this paper, we formalise this task and develop a crowd-sourcing strategy to collect 32k task instances based on real-world rules and crowd-generated questions and scenarios. We analyse the challenges of this task and assess its difﬁculty by evaluating the performance of rule-based and machine-learning baselines. We observe promising results when no background knowledge is necessary, and substantial room for improvement whenever background knowledge is needed.},
	language = {en},
	urldate = {2018-10-01},
	journal = {arXiv:1809.01494 [cs, stat]},
	author = {Saeidi, Marzieh and Bartolo, Max and Lewis, Patrick and Singh, Sameer and Rocktäschel, Tim and Sheldon, Mike and Bouchard, Guillaume and Riedel, Sebastian},
	month = aug,
	year = {2018},
	note = {arXiv: 1809.01494},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{saeidi_interpretation_2018-1,
	title = {Interpretation of {Natural} {Language} {Rules} in {Conversational} {Machine} {Reading}},
	url = {http://arxiv.org/abs/1809.01494},
	abstract = {Most work in machine reading focuses on question answering problems where the answer is directly expressed in the text to read. However, many real-world question answering problems require the reading of text not because it contains the literal answer, but because it contains a recipe to derive an answer together with the reader's background knowledge. One example is the task of interpreting regulations to answer "Can I...?" or "Do I have to...?" questions such as "I am working in Canada. Do I have to carry on paying UK National Insurance?" after reading a UK government website about this topic. This task requires both the interpretation of rules and the application of background knowledge. It is further complicated due to the fact that, in practice, most questions are underspecified, and a human assistant will regularly have to ask clarification questions such as "How long have you been working abroad?" when the answer cannot be directly derived from the question and text. In this paper, we formalise this task and develop a crowd-sourcing strategy to collect 32k task instances based on real-world rules and crowd-generated questions and scenarios. We analyse the challenges of this task and assess its difficulty by evaluating the performance of rule-based and machine-learning baselines. We observe promising results when no background knowledge is necessary, and substantial room for improvement whenever background knowledge is needed.},
	urldate = {2018-10-01},
	journal = {arXiv:1809.01494 [cs, stat]},
	author = {Saeidi, Marzieh and Bartolo, Max and Lewis, Patrick and Singh, Sameer and Rocktäschel, Tim and Sheldon, Mike and Bouchard, Guillaume and Riedel, Sebastian},
	month = aug,
	year = {2018},
	note = {arXiv: 1809.01494},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{yang_hotpotqa:_2018,
	title = {{HotpotQA}: {A} {Dataset} for {Diverse}, {Explainable} {Multi}-hop {Question} {Answering}},
	shorttitle = {{HotpotQA}},
	url = {https://arxiv.org/abs/1809.09600},
	language = {en},
	urldate = {2018-10-01},
	author = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},
	month = sep,
	year = {2018}
}

@article{iyyer_adversarial_2018,
	title = {Adversarial {Example} {Generation} with {Syntactically} {Controlled} {Paraphrase} {Networks}},
	url = {http://arxiv.org/abs/1804.06059},
	abstract = {We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples. Given a sentence and a target syntactic form (e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the sentence with the desired syntax. We show it is possible to create training data for this task by first doing backtranslation at a very large scale, and then using a parser to label the syntactic transformations that naturally occur during this process. Such data allows us to train a neural encoder-decoder model with extra inputs to specify the target syntax. A combination of automated and human evaluations show that SCPNs generate paraphrases that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled) paraphrase systems. Furthermore, they are more capable of generating syntactically adversarial examples that both (1) "fool" pretrained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data.},
	urldate = {2018-09-30},
	journal = {arXiv:1804.06059 [cs]},
	author = {Iyyer, Mohit and Wieting, John and Gimpel, Kevin and Zettlemoyer, Luke},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.06059},
	keywords = {Computer Science - Computation and Language}
}

@inproceedings{bordes_question_2014,
	address = {Doha, Qatar},
	title = {Question {Answering} with {Subgraph} {Embeddings}},
	url = {http://aclweb.org/anthology/D14-1067},
	doi = {10.3115/v1/D14-1067},
	abstract = {This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features. Our model learns low-dimensional embeddings of words and knowledge base constituents; these representations are used to score natural language questions against candidate answers. Training our system using pairs of questions and structured representations of their answers, and pairs of question paraphrases, yields competitive results on a recent benchmark of the literature.},
	language = {en},
	urldate = {2018-09-28},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Bordes, Antoine and Chopra, Sumit and Weston, Jason},
	year = {2014},
	pages = {615--620}
}

@inproceedings{dong_question_2015,
	address = {Beijing, China},
	title = {Question {Answering} over {Freebase} with {Multi}-{Column} {Convolutional} {Neural} {Networks}},
	url = {http://aclweb.org/anthology/P15-1026},
	doi = {10.3115/v1/P15-1026},
	abstract = {Answering natural language questions over a knowledge base is an important and challenging task. Most of existing systems typically rely on hand-crafted features and rules to conduct question understanding and/or answer ranking. In this paper, we introduce multi-column convolutional neural networks (MCCNNs) to understand questions from three different aspects (namely, answer path, answer context, and answer type) and learn their distributed representations. Meanwhile, we jointly learn low-dimensional embeddings of entities and relations in the knowledge base. Question-answer pairs are used to train the model to rank candidate answers. We also leverage question paraphrases to train the column networks in a multi-task learning manner. We use FREEBASE as the knowledge base and conduct extensive experiments on the WEBQUESTIONS dataset. Experimental results show that our method achieves better or comparable performance compared with baseline systems. In addition, we develop a method to compute the salience scores of question words in different column networks. The results help us intuitively understand what MCCNNs learn.},
	language = {en},
	urldate = {2018-09-28},
	booktitle = {Proceedings of the 53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Dong, Li and Wei, Furu and Zhou, Ming and Xu, Ke},
	year = {2015},
	pages = {260--269}
}

@article{dunn_searchqa:_2017,
	title = {{SearchQA}: {A} {New} {Q}\&{A} {Dataset} {Augmented} with {Context} from a {Search} {Engine}},
	shorttitle = {{SearchQA}},
	url = {http://arxiv.org/abs/1704.05179},
	abstract = {We publicly release a new large-scale dataset, called SearchQA, for machine comprehension, or question-answering. Unlike recently released datasets, such as DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to reﬂect a full pipeline of general question-answering. That is, we start not from an existing article and generate a question-answer pair, but start from an existing question-answer pair, crawled from J! Archive, and augment it with text snippets retrieved by Google. Following this approach, we built SearchQA, which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context tuple of the SearchQA comes with additional meta-data such as the snippet’s URL, which we believe will be valuable resources for future research. We conduct human evaluation as well as test two baseline methods, one simple word selection and the other deep learning based, on the SearchQA. We show that there is a meaningful gap between the human and machine performances. This suggests that the proposed dataset could well serve as a benchmark for question-answering.},
	language = {en},
	urldate = {2018-09-28},
	journal = {arXiv:1704.05179 [cs]},
	author = {Dunn, Matthew and Sagun, Levent and Higgins, Mike and Guney, V. Ugur and Cirik, Volkan and Cho, Kyunghyun},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.05179},
	keywords = {Computer Science - Computation and Language}
}

@article{yu_qanet:_2018,
	title = {{QANet}: {Combining} {Local} {Convolution} with {Global} {Self}-{Attention} for {Reading} {Comprehension}},
	shorttitle = {{QANet}},
	url = {http://arxiv.org/abs/1804.09541},
	abstract = {Current end-to-end machine reading and question answering (Q\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\&A architecture called QANet, which does not require recurrent networks: Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score1 on the test set, which is signiﬁcantly better than the best published F1 score of 81.8.},
	language = {en},
	urldate = {2018-09-28},
	journal = {arXiv:1804.09541 [cs]},
	author = {Yu, Adams Wei and Dohan, David and Luong, Minh-Thang and Zhao, Rui and Chen, Kai and Norouzi, Mohammad and Le, Quoc V.},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.09541},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@article{ribeiro_semantically_nodate,
	title = {Semantically {Equivalent} {Adversarial} {Rules} for {Debugging} {NLP} models},
	abstract = {Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) – semantic-preserving perturbations that induce changes in the model’s predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) – simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and ﬂexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual questionanswering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation signiﬁcantly reduces bugs, while maintaining accuracy.},
	language = {en},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	pages = {10}
}

@article{minervini_adversarially_2018,
	title = {Adversarially {Regularising} {Neural} {NLI} {Models} to {Integrate} {Logical} {Background} {Knowledge}},
	url = {http://arxiv.org/abs/1808.08609},
	abstract = {Adversarial examples are inputs to machine learning models designed to cause the model to make a mistake. They are useful for understanding the shortcomings of machine learning models, interpreting their results, and for regularisation. In NLP, however, most example generation strategies produce input text by using known, pre-speciﬁed semantic transformations, requiring signiﬁcant manual effort and in-depth understanding of the problem and domain. In this paper, we investigate the problem of automatically generating adversarial examples that violate a set of given First-Order Logic constraints in Natural Language Inference (NLI). We reduce the problem of identifying such adversarial examples to a combinatorial optimisation problem, by maximising a quantity measuring the degree of violation of such constraints and by using a language model for generating linguisticallyplausible examples. Furthermore, we propose a method for adversarially regularising neural NLI models for incorporating background knowledge. Our results show that, while the proposed method does not always improve results on the SNLI and MultiNLI datasets, it signiﬁcantly and consistently increases the predictive accuracy on adversarially-crafted datasets – up to a 79.6\% relative improvement – while drastically reducing the number of background knowledge violations. Furthermore, we show that adversarial examples transfer among model architectures, and that the proposed adversarial training procedure improves the robustness of NLI models to adversarial examples.},
	language = {en},
	urldate = {2018-09-28},
	journal = {arXiv:1808.08609 [cs, stat]},
	author = {Minervini, Pasquale and Riedel, Sebastian},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.08609},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{dong_learning_2017,
	title = {Learning to {Paraphrase} for {Question} {Answering}},
	url = {http://arxiv.org/abs/1708.06022},
	abstract = {Question answering (QA) systems are sensitive to the many different ways natural language expresses the same information need. In this paper we turn to paraphrases as a means of capturing this knowledge and present a general framework which learns felicitous paraphrases for various QA tasks. Our method is trained end-toend using question-answer pairs as a supervision signal. A question and its paraphrases serve as input to a neural scoring model which assigns higher weights to linguistic expressions most likely to yield correct answers. We evaluate our approach on QA over Freebase and answer sentence selection. Experimental results on three datasets show that our framework consistently improves performance, achieving competitive results despite the use of simple QA models.},
	language = {en},
	urldate = {2018-09-28},
	journal = {arXiv:1708.06022 [cs]},
	author = {Dong, Li and Mallinson, Jonathan and Reddy, Siva and Lapata, Mirella},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.06022},
	keywords = {Computer Science - Computation and Language}
}

@article{buck_ask_2017,
	title = {Ask the {Right} {Questions}: {Active} {Question} {Reformulation} with {Reinforcement} {Learning}},
	shorttitle = {Ask the {Right} {Questions}},
	url = {http://arxiv.org/abs/1705.07830},
	abstract = {We frame Question Answering (QA) as a Reinforcement Learning task, an approach that we call Active Question Answering. We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers. The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. The reformulation system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!. The agent outperforms a stateof-the-art base model, playing the role of the environment, and other benchmarks. We also analyze the language that the agent has learned while interacting with the question answering system. We ﬁnd that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting (tf-idf) and stemming.},
	language = {en},
	urldate = {2018-09-26},
	journal = {arXiv:1705.07830 [cs]},
	author = {Buck, Christian and Bulian, Jannis and Ciaramita, Massimiliano and Gajewski, Wojciech and Gesmundo, Andrea and Houlsby, Neil and Wang, Wei},
	month = may,
	year = {2017},
	note = {arXiv: 1705.07830},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@article{nogueira_end--end_2016,
	title = {End-to-{End} {Goal}-{Driven} {Web} {Navigation}},
	url = {http://arxiv.org/abs/1602.02261},
	abstract = {We propose a goal-driven web navigation as a benchmark task for evaluating an agent with abilities to understand natural language and plan on partially observed environments. In this challenging task, an agent navigates through a website, which is represented as a graph consisting of web pages as nodes and hyperlinks as directed edges, to ﬁnd a web page in which a query appears. The agent is required to have sophisticated high-level reasoning based on natural languages and efﬁcient sequential decision-making capability to succeed. We release a software tool, called WebNav, that automatically transforms a website into this goal-driven web navigation task, and as an example, we make WikiNav, a dataset constructed from the English Wikipedia. We extensively evaluate different variants of neural net based artiﬁcial agents on WikiNav and observe that the proposed goal-driven web navigation well reﬂects the advances in models, making it a suitable benchmark for evaluating future progress. Furthermore, we extend the WikiNav with questionanswer pairs from Jeopardy! and test the proposed agent based on recurrent neural networks against strong inverted index based search engines. The artiﬁcial agents trained on WikiNav outperforms the engined based approaches, demonstrating the capability of the proposed goal-driven navigation as a good proxy for measuring the progress in real-world tasks such as focused crawling and question-answering.},
	language = {en},
	urldate = {2018-09-26},
	journal = {arXiv:1602.02261 [cs]},
	author = {Nogueira, Rodrigo and Cho, Kyunghyun},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.02261},
	keywords = {Computer Science - Artificial Intelligence}
}

@article{faust_resilient_2018,
	title = {Resilient {Computing} with {Reinforcement} {Learning} on a {Dynamical} {System}: {Case} {Study} in {Sorting}},
	shorttitle = {Resilient {Computing} with {Reinforcement} {Learning} on a {Dynamical} {System}},
	url = {http://arxiv.org/abs/1809.09261},
	abstract = {Robots and autonomous agents often complete goal-based tasks with limited resources, relying on imperfect models and sensor measurements. In particular, reinforcement learning (RL) and feedback control can be used to help a robot achieve a goal. Taking advantage of this body of work, this paper formulates general computation as a feedback-control problem, which allows the agent to autonomously overcome some limitations of standard procedural language programming: resilience to errors and early program termination. Our formulation considers computation to be trajectory generation in the program's variable space. The computing then becomes a sequential decision making problem, solved with reinforcement learning (RL), and analyzed with Lyapunov stability theory to assess the agent's resilience and progression to the goal. We do this through a case study on a quintessential computer science problem, array sorting. Evaluations show that our RL sorting agent makes steady progress to an asymptotically stable goal, is resilient to faulty components, and performs less array manipulations than traditional Quicksort and Bubble sort.},
	urldate = {2018-09-26},
	journal = {arXiv:1809.09261 [cs]},
	author = {Faust, Aleksandra and Aimone, James B. and James, Conrad D. and Tapia, Lydia},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.09261},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Systems and Control}
}

@article{song_exploring_2018,
	title = {Exploring {Graph}-structured {Passage} {Representation} for {Multi}-hop {Reading} {Comprehension} with {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1809.02040},
	abstract = {Multi-hop reading comprehension focuses on one type of factoid question, where a system needs to properly integrate multiple pieces of evidence to correctly answer a question. Previous work approximates global evidence with local coreference information, encoding coreference chains with DAG-styled GRU layers within a gated-attention reader. However, coreference is limited in providing information for rich inference. We introduce a new method for better connecting global evidence, which forms more complex graphs compared to DAGs. To perform evidence integration on our graphs, we investigate two recent graph neural networks, namely graph convolutional network (GCN) and graph recurrent network (GRN). Experiments on two standard datasets show that richer global information leads to better answers. Our method performs better than all published results on these datasets.},
	urldate = {2018-09-26},
	journal = {arXiv:1809.02040 [cs]},
	author = {Song, Linfeng and Wang, Zhiguo and Yu, Mo and Zhang, Yue and Florian, Radu and Gildea, Daniel},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.02040},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@article{de_cao_question_2018,
	title = {Question {Answering} by {Reasoning} {Across} {Documents} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1808.09920},
	abstract = {Most research in reading comprehension has focused on answering questions based on individual documents or even single paragraphs. We introduce a method which integrates and reasons relying on information spread within documents and across multiple documents. We frame it as an inference problem on a graph. Mentions of entities are nodes of this graph where edges encode relations between different mentions (e.g., within- and cross-document co-references). Graph convolutional networks (GCNs) are applied to these graphs and trained to perform multi-step reasoning. Our Entity-GCN method is scalable and compact, and it achieves state-of-the-art results on the WikiHop dataset (Welbl et al. 2017).},
	urldate = {2018-09-26},
	journal = {arXiv:1808.09920 [cs, stat]},
	author = {De Cao, Nicola and Aziz, Wilker and Titov, Ivan},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.09920},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning}
}

@article{min_efficient_2018,
	title = {Efficient and {Robust} {Question} {Answering} from {Minimal} {Context} over {Documents}},
	url = {http://arxiv.org/abs/1805.08092},
	abstract = {Neural models for question answering (QA) over documents have achieved significant performance improvements. Although effective, these models do not scale to large corpora due to their complex modeling of interactions between the document and the question. Moreover, recent work has shown that such models are sensitive to adversarial inputs. In this paper, we study the minimal context required to answer the question, and find that most questions in existing datasets can be answered with a small set of sentences. Inspired by this observation, we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model. Our overall system achieves significant reductions in training (up to 15 times) and inference times (up to 13 times), with accuracy comparable to or better than the state-of-the-art on SQuAD, NewsQA, TriviaQA and SQuAD-Open. Furthermore, our experimental results and analyses show that our approach is more robust to adversarial inputs.},
	urldate = {2018-09-26},
	journal = {arXiv:1805.08092 [cs]},
	author = {Min, Sewon and Zhong, Victor and Socher, Richard and Xiong, Caiming},
	month = may,
	year = {2018},
	note = {arXiv: 1805.08092},
	keywords = {Computer Science - Computation and Language}
}

@article{weissenborn_making_2017,
	title = {Making {Neural} {QA} as {Simple} as {Possible} but not {Simpler}},
	url = {http://arxiv.org/abs/1703.04816},
	abstract = {Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to simpler neural baseline systems that would justify their complexity. In this work, we propose a simple heuristic that guides the development of neural baseline systems for the extractive QA task. We find that there are two ingredients necessary for building a high-performing neural QA system: first, the awareness of question words while processing the context and second, a composition function that goes beyond simple bag-of-words modeling, such as recurrent neural networks. Our results show that FastQA, a system that meets these two requirements, can achieve very competitive performance compared with existing models. We argue that this surprising finding puts results of previous systems and the complexity of recent QA datasets into perspective.},
	urldate = {2018-09-26},
	journal = {arXiv:1703.04816 [cs]},
	author = {Weissenborn, Dirk and Wiese, Georg and Seiffe, Laura},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.04816},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing}
}

@inproceedings{talmor_web_2018,
	address = {New Orleans, Louisiana},
	title = {The {Web} as a {Knowledge}-{Base} for {Answering} {Complex} {Questions}},
	url = {http://aclweb.org/anthology/N18-1059},
	doi = {10.18653/v1/N18-1059},
	abstract = {Answering complex questions is a timeconsuming activity for humans that requires reasoning and integration of information. Recent work on reading comprehension made headway in answering simple questions, but tackling complex questions is still an ongoing research challenge. Conversely, semantic parsers have been successful at handling compositionality, but only when the information resides in a target knowledge-base. In this paper, we present a novel framework for answering broad and complex questions, assuming answering simple questions is possible using a search engine and a reading comprehension model. We propose to decompose complex questions into a sequence of simple questions, and compute the ﬁnal answer from the sequence of answers. To illustrate the viability of our approach, we create a new dataset of complex questions, COMPLEXWEBQUESTIONS, and present a model that decomposes questions and interacts with the web to compute an answer. We empirically demonstrate that question decomposition improves performance from 20.8 precision@1 to 27.5 precision@1 on this new dataset.},
	language = {en},
	urldate = {2018-09-25},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of           the {Association} for {Computational} {Linguistics}: {Human} {Language}           {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Talmor, Alon and Berant, Jonathan},
	year = {2018},
	pages = {641--651}
}

@article{khot_answering_2017,
	title = {Answering {Complex} {Questions} {Using} {Open} {Information} {Extraction}},
	url = {http://arxiv.org/abs/1704.05572},
	abstract = {While there has been substantial progress in factoid question-answering (QA), answering complex questions remains challenging, typically requiring both a large body of knowledge and inference techniques. Open Information Extraction (Open IE) provides a way to generate semi-structured knowledge for QA, but to date such knowledge has only been used to answer simple questions with retrievalbased methods. We overcome this limitation by presenting a method for reasoning with Open IE knowledge, allowing more complex questions to be handled. Using a recently proposed support graph optimization framework for QA, we develop a new inference model for Open IE, in particular one that can work effectively with multiple short facts, noise, and the relational structure of tuples. Our model signiﬁcantly outperforms a state-of-the-art structured solver on complex questions of varying difﬁculty, while also removing the reliance on manually curated knowledge.},
	language = {en},
	urldate = {2018-09-25},
	journal = {arXiv:1704.05572 [cs]},
	author = {Khot, Tushar and Sabharwal, Ashish and Clark, Peter},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.05572},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}

@inproceedings{
dinan2018wizard,
title={Wizard of Wikipedia: Knowledge-Powered Conversational Agents},
author={Emily Dinan and Stephen Roller and Kurt Shuster and Angela Fan and Michael Auli and Jason Weston},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=r1l73iRqKm},
}


@incollection{shrivastava2014asymmetric,
title = {Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS)},
author = {Shrivastava, Anshumali and Li, Ping},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {2321--2329},
year = {2014},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5329-asymmetric-lsh-alsh-for-sublinear-time-maximum-inner-product-search-mips.pdf}
}

@inproceedings{gionis1999similarity,
author = {Gionis, Aristides and Indyk, Piotr and Motwani, Rajeev},
title = {Similarity Search in High Dimensions via Hashing},
year = {1999},
isbn = {1558606157},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 25th International Conference on Very Large Data Bases},
pages = {518–529},
numpages = {12},
series = {VLDB ’99}
}

@inproceedings{gu2018search,
	author = {Jiatao Gu and Yong Wang and Kyunghyun Cho and Victor O.K. Li},
	title = {Search Engine Guided Neural Machine Translation},
	booktitle = {AAAI Conference on Artificial Intelligence},
	year = {2018},
	url={https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282}
}

@inproceedings{liu2018generating,
title={Generating Wikipedia by Summarizing Long Sequences},
author={Peter J. Liu* and Mohammad Saleh* and Etienne Pot and Ben Goodrich and Ryan Sepassi and Lukasz Kaiser and Noam Shazeer},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Hyg0vbWC-},
}

@article{raffel2019t5,
  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {arXiv e-prints},
  year = {2019},
  archivePrefix = {arXiv},
  eprint = {1910.10683},
  url = {https://arxiv.org/abs/1910.10683}
}

@article{roberts2020t5cqba,
  author = {Adam Roberts and Colin Raffel and Noam Shazeer},
  title = {How Much Knowledge Can You Pack Into the Parameters of a Language Model?},
  journal = {arXiv e-prints},
  year = {2020},
  archivePrefix = {arXiv},
  eprint = {2002.08910},
  url = {https://arxiv.org/abs/2002.08910}
}

@article{mccann2018decanlp,
  title={The Natural Language Decathlon: Multitask Learning as Question Answering},
  author={Bryan McCann and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},
  journal={arXiv preprint arXiv:1806.08730},
  year={2018},
  url = {https://arxiv.org/abs/1806.08730}
}

@article{lewis2019bart,
    title = {{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
    author = {Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and
              Abdelrahman Mohamed and Omer Levy and Veselin Stoyanov
              and Luke Zettlemoyer },
    journal={arXiv preprint arXiv:1910.13461},
    year = {2019},
    url={https://arxiv.org/abs/1910.13461}
}

@article{Karpukhin20dense,
    title={ Dense Passage Retrieval for Open-domain Question Answering },
    author={ Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau },
    journal={ arXiv preprint arXiv:2004.04906 },
    year={2020},
    url={https://arxiv.org/abs/2004.04906}
}

@article{guu2020realm,
  title={{REALM}: Retrieval-Augmented Language Model Pre-Training},
  author={Kelvin Guu and Kenton Lee and Zora Tung and Panupong Pasupat and Ming-Wei Chang},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.08909},
  url={https://arxiv.org/abs/2002.08909}
}

@article{lee2019latentorqa,
  title={Latent retrieval for weakly supervised open domain question answering},
  author={Lee, Kenton and Chang, Ming-Wei and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1906.00300},
  year={2019}
}

@article{min2020ambigqa,
    title={ {A}mbig{QA}: Answering Ambiguous Open-domain Questions },
    author={ Min, Sewon and Michael, Julian and Hajishirzi, Hannaneh and Zettlemoyer, Luke },
    journal={ arXiv preprint arXiv:2004.10645 },
    year={2020}
}

@inproceedings{grave2016improving,
  author    = {Edouard Grave and
               Armand Joulin and
               Nicolas Usunier},
  title     = {Improving Neural Language Models with a Continuous Cache},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=B184E5qee},
  timestamp = {Thu, 25 Jul 2019 14:25:44 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/GraveJU17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ghazvininejad2018knowledge,
	author = {Marjan Ghazvininejad and Chris Brockett and Ming-Wei Chang and Bill Dolan and Jianfeng Gao and Wen-tau Yih and Michel Galley},
	title = {A Knowledge-Grounded Neural Conversation Model},
	booktitle = {AAAI Conference on Artificial Intelligence},
	year = {2018},
	url={https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16710}
}

@misc{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019},
  url={https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}

@article{nogueira2019passage, 
  title={Passage Re-ranking with {BERT}},
  author={Nogueira, Rodrigo and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:1901.04085},
  year={2019},
  url={https://arxiv.org/abs/1901.04085}
}

 @inproceedings{wang2018r3,
  author    = {Shuohang Wang and
               Mo Yu and
               Xiaoxiao Guo and
               Zhiguo Wang and
               Tim Klinger and
               Wei Zhang and
               Shiyu Chang and
               Gerry Tesauro and
               Bowen Zhou and
               Jing Jiang},
  editor    = {Sheila A. McIlraith and
               Kilian Q. Weinberger},
  title     = {R\({}^{\mbox{3}}\): Reinforced Ranker-Reader for Open-Domain Question
               Answering},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence,
               (AAAI-18), the 30th innovative Applications of Artificial Intelligence
               (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in
               Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
               2-7, 2018},
  pages     = {5981--5988},
  publisher = {{AAAI} Press},
  year      = {2018},
  url       = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16712},
  timestamp = {Mon, 23 Dec 2019 08:52:22 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/WangYGWKZCTZJ18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{wang2018evidence-aggregation,
    title={Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering},
    author={Shuohang Wang and Mo Yu and Jing Jiang and Wei Zhang and Xiaoxiao Guo and Shiyu Chang and Zhiguo Wang and Tim Klinger and Gerald Tesauro and Murray Campbell},
    booktitle={ICLR},
    year={2018},
    url={https://openreview.net/forum?id=rJl3yM-Ab},
}

@article{JDH17,
  title={Billion-scale similarity search with GPUs},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:1702.08734},
  year={2017},
  url={https://arxiv.org/abs/1702.08734}
}

@incollection{sukhbaatar2015end,
title = {End-To-End Memory Networks},
author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2440--2448},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf}
}

@inproceedings{weston2015memory,
  author    = {Jason Weston and
               Sumit Chopra and
               Antoine Bordes},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Memory Networks},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1410.3916},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/WestonCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{graves2014neural,
  author    = {Alex Graves and
               Greg Wayne and
               Ivo Danihelka},
  title     = {Neural Turing Machines},
  journal   = {CoRR},
  volume    = {abs/1410.5401},
  year      = {2014},
  url       = {http://arxiv.org/abs/1410.5401},
  archivePrefix = {arXiv},
  eprint    = {1410.5401},
  timestamp = {Mon, 13 Aug 2018 16:46:28 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GravesWD14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{fevry2020entities,
  title={Entities as Experts: Sparse Memory Access with Entity Supervision},
  author={Thibault F{\'e}vry and Livio Baldini Soares and Nicholas FitzGerald and Eunsol Choi and Tom Kwiatkowski},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.07202},
  url={https://arxiv.org/abs/2004.07202}
}

@incollection{lample2019large,
title = {Large Memory Layers with Product Keys},
author = {Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc' Aurelio and Denoyer, Ludovic and Jegou, Herve},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d' Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8548--8559},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf}
}

@incollection{vaswani2017attention,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@article{massarelli2019decoding,
	Author = {Massarelli, Luca and Petroni, Fabio and Piktus, Aleksandra and Ott, Myle and Rockt{\"a}schel, Tim and Plachouras, Vassilis and Silvestri, Fabrizio and Riedel, Sebastian},
	Date-Added = {2020-02-12 11:48:00 +0000},
	Date-Modified = {2020-02-12 11:48:00 +0000},
	Journal = {arXiv preprint arXiv:1911.03587},
	Title = {How Decoding Strategies Affect the Verifiability of Generated Text},
	Year = {2019},
	url={https://arxiv.org/abs/1911.03587}}

@inproceedings{micikevicius2018mixed,
    title={Mixed Precision Training},
    author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
    booktitle={ICLR},
    year={2018},
    url={https://openreview.net/forum?id=r1gs9JgRZ},
}

@article{grace2017when,
  author    = {Katja Grace and
               John Salvatier and
               Allan Dafoe and
               Baobao Zhang and
               Owain Evans},
  title     = {When Will {AI} Exceed Human Performance? Evidence from {AI} Experts},
  journal   = {CoRR},
  volume    = {abs/1705.08807},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.08807},
  archivePrefix = {arXiv},
  eprint    = {1705.08807},
  timestamp = {Mon, 13 Aug 2018 16:46:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GraceSDZE17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{solaiman2019release,
  title={Release Strategies and the Social Impacts of Language Models},
  author={Irene Solaiman and Miles Brundage and Jack Clark and Amanda Askell and Ariel Herbert-Voss and Jeff Wu and Alec Radford and Jian-Bing Wang},
  journal={ArXiv},
  year={2019},
  volume={abs/1908.09203}
}