\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Liang]{allen2019learning}
Allen-Zhu, Z., Li, Y., and Liang, Y.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{cntkexact}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Arora et~al.(2020)Arora, Du, Li, Salakhutdinov, Wang, and
  Yu]{arora2020harnessing}
Arora, S., Du, S.~S., Li, Z., Salakhutdinov, R., Wang, R., and Yu, D.
\newblock Harnessing the power of infinitely wide deep nets on small-data
  tasks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Coates \& Ng(2012)Coates and Ng]{coates2012learning}
Coates, A. and Ng, A.~Y.
\newblock Learning feature representations with k-means.
\newblock In \emph{Neural networks: Tricks of the Trade}, pp.\  561--580.
  Springer, 2012.

\bibitem[Cubuk et~al.(2019)Cubuk, Zoph, Shlens, and Le]{cubuk2019randaugment}
Cubuk, E.~D., Zoph, B., Shlens, J., and Le, Q.~V.
\newblock Randaugment: Practical data augmentation with no separate search.
\newblock \emph{arXiv preprint arXiv:1909.13719}, 2019.

\bibitem[Dai et~al.(2014)Dai, Xie, He, Liang, Raj, Balcan, and
  Song]{dai2014scalable}
Dai, B., Xie, B., He, N., Liang, Y., Raj, A., Balcan, M., and Song, L.
\newblock Scalable kernel methods via doubly stochastic gradients.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and Singer]{daniely2016toward}
Daniely, A., Frostig, R., and Singer, Y.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Dao et~al.(2019)Dao, Gu, Ratner, Smith, Sa, and Re]{Dao19}
Dao, T., Gu, A., Ratner, A.~J., Smith, V., Sa, C.~D., and Re, C.
\newblock A kernel theory of modern data augmentation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Dem{\v{s}}ar(2006)]{demvsar2006statistical}
Dem{\v{s}}ar, J.
\newblock Statistical comparisons of classifiers over multiple data sets.
\newblock \emph{Journal of Machine Learning Research}, 7\penalty0
  (Jan):\penalty0 1--30, 2006.

\bibitem[Dolan \& Mor\'{e}(2002)Dolan and Mor\'{e}]{Dolan02}
Dolan, E.~D. and Mor\'{e}, J.~J.
\newblock Benchmarking optimization software with performance profiles.
\newblock \emph{Mathematical Programming, Series A}, 91:\penalty0 201--213,
  2002.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and Zhai]{Du19b}
Du, S.~S., Lee, J.~D., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning},
  2019{\natexlab{a}}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos, and Singh]{Du19}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.

\bibitem[Fern{\'a}ndez-Delgado et~al.(2014)Fern{\'a}ndez-Delgado, Cernadas,
  Barro, and Amorim]{fernandez2014we}
Fern{\'a}ndez-Delgado, M., Cernadas, E., Barro, S., and Amorim, D.
\newblock Do we need hundreds of classifiers to solve real world classification
  problems?
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 3133--3181, 2014.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2019linearized}
Ghorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A.
\newblock Linearized two-layers neural networks in high dimension, 2019.

\bibitem[Goodfellow et~al.(2013)Goodfellow, Warde{-}Farley, Mirza, Courville,
  and Bengio]{goodfellow2013maxout}
Goodfellow, I.~J., Warde{-}Farley, D., Mirza, M., Courville, A.~C., and Bengio,
  Y.
\newblock Maxout networks.
\newblock In \emph{International Conference on Machine Learning}, 2013.

\bibitem[Jacot et~al.(2018)Jacot, Hongler, and Gabriel]{ntk}
Jacot, A., Hongler, C., and Gabriel, F.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[LeCun et~al.(1998{\natexlab{a}})LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998{\natexlab{a}}.

\bibitem[LeCun et~al.(1998{\natexlab{b}})LeCun, Cortes, and Burges]{lecunmnist}
LeCun, Y., Cortes, C., and Burges, C.
\newblock The mnist dataset of handwritten digits, 1998{\natexlab{b}}.

\bibitem[Lee et~al.(2018)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl{-}Dickstein]{lee2018deep}
Lee, J., Bahri, Y., Novak, R., Schoenholz, S.~S., Pennington, J., and
  Sohl{-}Dickstein, J.
\newblock Deep neural networks as gaussian processes.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J.,
  and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Li et~al.(2019)Li, Wang, Yu, Du, Hu, Salakhutdinov, and
  Arora]{li2019enhanced}
Li, Z., Wang, R., Yu, D., Du, S.~S., Hu, W., Salakhutdinov, R., and Arora, S.
\newblock Enhanced convolutional neural tangent kernels, 2019.

\bibitem[Ma \& Belkin(2018)Ma and Belkin]{ma2018kernel}
Ma, S. and Belkin, M.
\newblock Kernel machines that adapt to gpus for effective large batch
  training, 2018.

\bibitem[Mairal et~al.(2014)Mairal, Koniusz, Harchaoui, and
  Schmid]{mairal2014convolutional}
Mairal, J., Koniusz, P., Harchaoui, Z., and Schmid, C.
\newblock Convolutional kernel networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Novak et~al.(2019)Novak, Xiao, Bahri, Lee, Yang, Hron, Abolafia,
  Pennington, and Sohl{-}Dickstein]{novak2019bayesian}
Novak, R., Xiao, L., Bahri, Y., Lee, J., Yang, G., Hron, J., Abolafia, D.~A.,
  Pennington, J., and Sohl{-}Dickstein, J.
\newblock Bayesian deep convolutional networks with many channels are gaussian
  processes.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Page(2018)]{myrtle}
Page, D.
\newblock myrtle.ai, 2018.
\newblock URL \url{https://myrtle.ai/how-to-train-your-resnet-4-architecture/}.

\bibitem[Rahimi \& Recht(2008)Rahimi and Recht]{rahimi2008random}
Rahimi, A. and Recht, B.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1177--1184, 2008.

\bibitem[Ratner et~al.(2017)Ratner, Ehrenberg, Hussain, Dunnmon, and
  Re]{Ratner17}
Ratner, A., Ehrenberg, H., Hussain, Z., Dunnmon, J., and Re, C.
\newblock Learning to compose domain-specific transformations for data
  augmentation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and
  Shankar]{recht2019imagenet}
Recht, B., Roelofs, R., Schmidt, L., and Shankar, V.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Shankar et~al.(2018)Shankar, Krauth, Pu, Jonas, Venkataraman, Stoica,
  Recht, and Ragan{-}Kelley]{numpywren}
Shankar, V., Krauth, K., Pu, Q., Jonas, E., Venkataraman, S., Stoica, I.,
  Recht, B., and Ragan{-}Kelley, J.
\newblock numpywren: Serverless linear algebra.
\newblock \emph{CoRR}, abs/1810.09679, 2018.
\newblock URL \url{http://arxiv.org/abs/1810.09679}.

\bibitem[Shawe-Taylor et~al.(2004)Shawe-Taylor, Cristianini,
  et~al.]{shawe2004kernel}
Shawe-Taylor, J., Cristianini, N., et~al.
\newblock \emph{Kernel Methods for Pattern Analysis}.
\newblock Cambridge university press, 2004.

\bibitem[Vasilache et~al.(2018)Vasilache, Zinenko, Theodoridis, Goyal, DeVito,
  Moses, Verdoolaege, Adams, and Cohen]{vasilache2018tensor}
Vasilache, N., Zinenko, O., Theodoridis, T., Goyal, P., DeVito, Z., Moses,
  W.~S., Verdoolaege, S., Adams, A., and Cohen, A.
\newblock Tensor comprehensions: Framework-agnostic high-performance machine
  learning abstractions.
\newblock \emph{arXiv preprint arXiv:1802.04730}, 2018.

\bibitem[Wang et~al.(2019)Wang, Pleiss, Gardner, Tyree, Weinberger, and
  Wilson]{Wang19}
Wang, K.~A., Pleiss, G., Gardner, J., Tyree, S., Weinberger, K., and Wilson,
  A.~G.
\newblock Exact gaussian processes on a million data points.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\end{thebibliography}
