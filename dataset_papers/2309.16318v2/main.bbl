\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ben-Nun \& Hoefler(2019)Ben-Nun and Hoefler]{parallelDNNBenNun}
Ben-Nun, T. and Hoefler, T.
\newblock Demystifying parallel and distributed deep learning: An in-depth
  concurrency analysis.
\newblock \emph{ACM Comput. Surv.}, 52\penalty0 (4), aug 2019.
\newblock ISSN 0360-0300.
\newblock \doi{10.1145/3320060}.
\newblock URL \url{https://doi.org/10.1145/3320060}.

\bibitem[Bini \& Meini(2008)Bini and Meini]{reviewCR2}
Bini, D. and Meini, B.
\newblock The cyclic reduction algorithm: from {P}oisson equation to stochastic
  processes and beyond.
\newblock \emph{Numerical Algorithms}, 51:\penalty0 23--60, 2008.

\bibitem[Brock et~al.(2019)Brock, Donahue, and Simonyan]{brocklarge}
Brock, A., Donahue, J., and Simonyan, K.
\newblock Large scale gan training for high fidelity natural image synthesis.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chen et~al.(2017)Chen, Papandreou, Kokkinos, Murphy, and
  Yuille]{chen2017deeplab}
Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and Yuille, A.~L.
\newblock Deeplab: Semantic image segmentation with deep convolutional nets,
  atrous convolution, and fully connected crfs.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 40\penalty0 (4):\penalty0 834--848, 2017.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and Duvenaud]{neuralODE}
Chen, T.~Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D.~K.
\newblock Neural ordinary differential equations.
\newblock In \emph{Neural Information Processing Systems}, 2018.

\bibitem[Chetlur et~al.(2014)Chetlur, Woolley, Vandermersch, Cohen, Tran,
  Catanzaro, and Shelhamer]{chetlur2014cudnn}
Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catanzaro, B.,
  and Shelhamer, E.
\newblock cudnn: Efficient primitives for deep learning.
\newblock \emph{arXiv preprint arXiv:1410.0759}, 2014.

\bibitem[Deng(2012)]{mnist}
Deng, L.
\newblock The {MNIST} database of handwritten digit images for machine learning
  research.
\newblock \emph{IEEE Signal Processing Magazine}, 29\penalty0 (6):\penalty0
  141--142, 2012.

\bibitem[Frankle et~al.(2021)Frankle, Dziugaite, Roy, and
  Carbin]{Frankle:ICLR2021}
Frankle, J., Dziugaite, G.~K., Roy, D.~M., and Carbin, M.
\newblock Pruning neural networks at initialization: Why are we missing the
  mark?
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Gander(1997)]{reviewCR1}
Gander, W.~Golub, G.~H.
\newblock Cyclic reduction – history and applications.
\newblock In Luk, F.~T. and Plemmons, R.~J. (eds.), \emph{Workshop on
  Scientific Computing}, New York, 1997. Springer Verlag.

\bibitem[Gander(2015)]{pintGander}
Gander, M.~J.
\newblock 50 years of time parallel time integration.
\newblock In Carraro, T., Geiger, M., K{\"o}rkel, S., and Rannacher, R. (eds.),
  \emph{Multiple Shooting and Time Domain Decomposition Methods}, pp.\
  69--113, Cham, 2015. Springer International Publishing.
\newblock ISBN 978-3-319-23321-5.

\bibitem[Germain et~al.(2015)Germain, Gregor, Murray, and
  Larochelle]{germain2015made}
Germain, M., Gregor, K., Murray, I., and Larochelle, H.
\newblock Made: Masked autoencoder for distribution estimation.
\newblock In \emph{International conference on machine learning}, pp.\
  881--889. PMLR, 2015.

\bibitem[G\"{u}nther et~al.(2020)G\"{u}nther, Ruthotto, Schroder, Cyr, and
  Gauger]{layerparallelGunther}
G\"{u}nther, S., Ruthotto, L., Schroder, J.~B., Cyr, E.~C., and Gauger, N.~R.
\newblock Layer-parallel training of deep residual neural networks.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 2\penalty0
  (1):\penalty0 1--23, 2020.
\newblock \doi{10.1137/19M1247620}.
\newblock URL \url{https://doi.org/10.1137/19M1247620}.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016{\natexlab{a}}.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  770--778, 2016{\natexlab{b}}.
\newblock \doi{10.1109/CVPR.2016.90}.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{FIDscore}
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf}.

\bibitem[Ho et~al.(2020{\natexlab{a}})Ho, Jain, and Abbeel]{Ho:NEURIPS2020}
Ho, J., Jain, A., and Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2020{\natexlab{a}}.

\bibitem[Ho et~al.(2020{\natexlab{b}})Ho, Jain, and Abbeel]{ho2020denoising}
Ho, J., Jain, A., and Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6840--6851, 2020{\natexlab{b}}.

\bibitem[Hockney(1965)]{originalCR}
Hockney, R.~W.
\newblock A fast direct solution of {P}oisson's equation using {F}ourier
  analysis.
\newblock \emph{J. ACM}, 12\penalty0 (1):\penalty0 95–113, jan 1965.
\newblock ISSN 0004-5411.
\newblock \doi{10.1145/321250.321259}.
\newblock URL \url{https://doi.org/10.1145/321250.321259}.

\bibitem[Horton et~al.(2022)Horton, Jin, Farhadi, and
  Rastegari]{layer-wise-data-free-cnn-compression}
Horton, M., Jin, Y., Farhadi, A., and Rastegari, M.
\newblock Layer-wise data-free {CNN} compression.
\newblock 2022.
\newblock URL \url{https://arxiv.org/pdf/2011.09058.pdf}.

\bibitem[Huang \& Belongie(2017)Huang and Belongie]{huang2017arbitrary}
Huang, X. and Belongie, S.
\newblock Arbitrary style transfer in real-time with adaptive instance
  normalization.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  1501--1510, 2017.

\bibitem[Jouppi et~al.(2023)Jouppi, Kurian, Li, Ma, Nagarajan, Nai, Patil,
  Subramanian, Swing, Towles, et~al.]{jouppi2023tpu}
Jouppi, N.~P., Kurian, G., Li, S., Ma, P., Nagarajan, R., Nai, L., Patil, N.,
  Subramanian, S., Swing, A., Towles, B., et~al.
\newblock Tpu v4: An optically reconfigurable supercomputer for machine
  learning with hardware support for embeddings.
\newblock \emph{arXiv preprint arXiv:2304.01433}, 2023.

\bibitem[Kenyon \& Capano(2022)Kenyon and Capano]{kenyon2022apple}
Kenyon, C. and Capano, C.
\newblock Apple silicon performance in scientific computing.
\newblock In \emph{2022 IEEE High Performance Extreme Computing Conference
  (HPEC)}, pp.\  1--10. IEEE, 2022.

\bibitem[Kirby et~al.(2020)Kirby, Samsi, Jones, Reuther, Kepner, and
  Gadepally]{layerparallelKirby}
Kirby, A., Samsi, S., Jones, M., Reuther, A., Kepner, J., and Gadepally, V.
\newblock Layer-parallel training with {GPU} concurrency of deep residual
  neural networks via nonlinear multigrid.
\newblock In \emph{2020 IEEE High Performance Extreme Computing Conference
  (HPEC)}, pp.\  1--7, 2020.
\newblock \doi{10.1109/HPEC43674.2020.9286180}.

\bibitem[Krizhevsky(2009)]{cifar10}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem[Krizhevsky et~al.(2017)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2017imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Communications of the ACM}, 60\penalty0 (6):\penalty0 84--90,
  2017.

\bibitem[Lee et~al.(2019)Lee, Ajanthan, and Torr]{Lee:ICLR2019}
Lee, N., Ajanthan, T., and Torr, P. H.~S.
\newblock {SNIP:} single-shot network pruning based on connection sensitivity.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Li et~al.(2017)Li, Kadav, Durdanovic, Samet, and Graf]{Li:ICLR2017}
Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H.~P.
\newblock Pruning filters for efficient {C}onv{N}ets.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{celeba}
Liu, Z., Luo, P., Wang, X., and Tang, X.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{ICCV}, pp.\  3730--3738. IEEE Computer Society, 2015.

\bibitem[Long et~al.(2015)Long, Shelhamer, and Darrell]{long2015fully}
Long, J., Shelhamer, E., and Darrell, T.
\newblock Fully convolutional networks for semantic segmentation.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  3431--3440, 2015.

\bibitem[Massaroli et~al.(2021)Massaroli, Poli, Sonoda, Suzuki, Park,
  Yamashita, and Asama]{layerparallelMassaroli}
Massaroli, S., Poli, M., Sonoda, S., Suzuki, T., Park, J., Yamashita, A., and
  Asama, H.
\newblock Differentiable multiple shooting layers.
\newblock \emph{ArXiv}, abs/2106.03885, 2021.

\bibitem[Moon \& Cyr(2022)Moon and Cyr]{layerparallelCyr}
Moon, E. and Cyr, E.~C.
\newblock Parallel training of {GRU} networks with a multi-grid solver for long
  sequences.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=N1WI0vJLER}.

\bibitem[Naumov(2017)]{Naumov}
Naumov, M.
\newblock Parallel complexity of forward and backward propagation.
\newblock \emph{CoRR}, abs/1712.06577, 2017.
\newblock URL \url{http://arxiv.org/abs/1712.06577}.

\bibitem[Ortega \& Rheinboldt(2000)Ortega and Rheinboldt]{nonlinearjacobi}
Ortega, J.~M. and Rheinboldt, W.~C.
\newblock \emph{Iterative Solution of Nonlinear Equations in Several
  Variables}.
\newblock Society for Industrial and Applied Mathematics, 2000.
\newblock \doi{10.1137/1.9780898719468}.
\newblock URL \url{https://epubs.siam.org/doi/abs/10.1137/1.9780898719468}.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International conference on machine learning}, pp.\
  8748--8763. PMLR, 2021.

\bibitem[Rombach et~al.(2022{\natexlab{a}})Rombach, Blattmann, Lorenz, Esser,
  and Ommer]{Rombach:CVPR2022}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Computer Vision and Pattern Recognition},
  2022{\natexlab{a}}.

\bibitem[Rombach et~al.(2022{\natexlab{b}})Rombach, Blattmann, Lorenz, Esser,
  and Ommer]{rombach2022high}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  10684--10695, 2022{\natexlab{b}}.

\bibitem[Salimans et~al.(2017)Salimans, Karpathy, Chen, and
  Kingma]{salimans2017pixelcnn++}
Salimans, T., Karpathy, A., Chen, X., and Kingma, D.~P.
\newblock Pixelcnn++: Improving the pixelcnn with discretized logistic mixture
  likelihood and other modifications.
\newblock \emph{arXiv preprint arXiv:1701.05517}, 2017.

\bibitem[Song et~al.(2021)Song, Meng, Liao, and Ermon]{song2021accelerating}
Song, Y., Meng, C., Liao, R., and Ermon, S.
\newblock Accelerating feedforward computation via parallel nonlinear equation
  solving.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Suau et~al.(2020)Suau, Zappella, Palakkode, and
  Apostoloff]{Suau:WACV2018}
Suau, X., Zappella, L., Palakkode, V., and Apostoloff, N.
\newblock Principal filter analysis for guided network compression.
\newblock In \emph{Winter Conference on Applications of Computer Vision}, 2020.

\bibitem[Sun et~al.(2020)Sun, Dong, Chen, Dian, Sun, Sun, Li, and
  Dong]{layerparallelSun}
Sun, Q., Dong, H., Chen, Z., Dian, W., Sun, J., Sun, Y., Li, Z., and Dong, B.
\newblock Penalty and augmented {L}agrangian methods for layer-parallel
  training of residual networks.
\newblock \emph{ArXiv}, abs/2009.01462, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2019)Wang, Zhang, and Grosse]{Wang:ICLR2020}
Wang, C., Zhang, G., and Grosse, R.~B.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Wiggers \& Hoogeboom(2020)Wiggers and Hoogeboom]{pmlr-v119-wiggers20a}
Wiggers, A. and Hoogeboom, E.
\newblock Predictive sampling with forecasting autoregressive models.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  10260--10269. PMLR,
  13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/wiggers20a.html}.

\end{thebibliography}
