\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baevski et~al.(2019)Baevski, Edunov, Liu, Zettlemoyer, and
  Auli]{Baevski2019ClozedrivenPO}
Baevski, A., Edunov, S., Liu, Y., Zettlemoyer, L.~S., and Auli, M.
\newblock Cloze-driven pretraining of self-attention networks, 2019.
\newblock URL \url{https://arxiv.org/abs/1903.07785}.

\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and Bengio]{Bahdanau2014NeuralMT}
Bahdanau, D., Cho, K., and Bengio, Y.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock In \emph{Proc. of ICLR}, 2015.
\newblock URL \url{https://arxiv.org/abs/1409.0473}.

\bibitem[Chan et~al.(2019{\natexlab{a}})Chan, Kitaev, Guu, Stern, and
  Uszkoreit]{Chan2019KERMITGI}
Chan, W., Kitaev, N., Guu, K., Stern, M., and Uszkoreit, J.
\newblock {KERMIT}: Generative insertion-based modeling for sequences,
  2019{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/1906.01604}.

\bibitem[Chan et~al.(2019{\natexlab{b}})Chan, Stern, Kiros, and
  Uszkoreit]{Chan2019AnES}
Chan, W., Stern, M., Kiros, J.~R., and Uszkoreit, J.
\newblock An empirical study of generation order for machine translation,
  2019{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/1910.13437}.

\bibitem[Dehghani et~al.(2019)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser]{Dehghani2019UniversalT}
Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L.
\newblock Universal transformers, 2019.
\newblock URL \url{https://arxiv.org/abs/1807.03819}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlins2019bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proc. of NAACL-HLT}, 2019.
\newblock URL \url{https://arxiv.org/abs/810.04805}.

\bibitem[Gehring et~al.(2017)Gehring, Auli, Grangier, Yarats, and
  Dauphin]{Gehring2017ConvolutionalST}
Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y.
\newblock Convolutional sequence to sequence learning.
\newblock In \emph{Proc.\ of ICML}, 2017.
\newblock URL \url{https://arxiv.org/abs/1705.03122}.

\bibitem[Ghazvininejad et~al.(2019)Ghazvininejad, Levy, Liu, and
  Zettlemoyer]{Ghazvininejad2019MaskPredictPD}
Ghazvininejad, M., Levy, O., Liu, Y., and Zettlemoyer, L.~S.
\newblock Mask-predict: Parallel decoding of conditional masked language
  models.
\newblock In \emph{Proc. of EMNLP}, 2019.
\newblock URL \url{https://arxiv.org/abs/1904.09324}.

\bibitem[Ghazvininejad et~al.(2020{\natexlab{a}})Ghazvininejad, Karpukhin,
  Zettlemoyer, and Levy]{Ghazvininejad2020AlignedCE}
Ghazvininejad, M., Karpukhin, V., Zettlemoyer, L., and Levy, O.
\newblock Aligned cross entropy for non-autoregressive machine translation,
  2020{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2004.01655}.

\bibitem[Ghazvininejad et~al.(2020{\natexlab{b}})Ghazvininejad, Levy, and
  Zettlemoyer]{Ghazvininejad2020SemiAutoregressiveTI}
Ghazvininejad, M., Levy, O., and Zettlemoyer, L.
\newblock Semi-autoregressive training improves mask-predict decoding,
  2020{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2001.08785}.

\bibitem[Graves(2016)]{Graves2016AdaptiveCT}
Graves, A.
\newblock Adaptive computation time for recurrent neural networks, 2016.
\newblock URL \url{https://arxiv.org/abs/1603.08983}.

\bibitem[Gu et~al.(2018)Gu, Bradbury, Xiong, Li, and
  Socher]{Gu2017NonAutoregressiveNM}
Gu, J., Bradbury, J., Xiong, C., Li, V. O.~K., and Socher, R.
\newblock Non-autoregressive neural machine translation.
\newblock In \emph{Proc. of ICLR}, 2018.
\newblock URL \url{https://arxiv.org/abs/1711.02281}.

\bibitem[Gu et~al.(2019{\natexlab{a}})Gu, Liu, and Cho]{Gu2019InsertionbasedDW}
Gu, J., Liu, Q., and Cho, K.
\newblock Insertion-based decoding with automatically inferred generation
  order.
\newblock \emph{TACL}, 2019{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/1902.01370}.

\bibitem[Gu et~al.(2019{\natexlab{b}})Gu, Wang, and Zhao]{Gu2019LevenshteinT}
Gu, J., Wang, C., and Zhao, J.
\newblock Levenshtein transformer.
\newblock In \emph{Proc. of NeurIPS}, 2019{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/1905.11006}.

\bibitem[Hassan et~al.(2018)Hassan, Aue, Chen, Chowdhary, Clark, Federmann,
  Huang, Junczys-Dowmunt, Lewis, Li, Liu, Liu, Luo, Menezes, Qin, Seide, Tan,
  Tian, Wu, Wu, Xia, Zhang, Zhang, and Zhou]{Hassan2018AchievingHP}
Hassan, H., Aue, A., Chen, C., Chowdhary, V., Clark, J., Federmann, C., Huang,
  X., Junczys-Dowmunt, M., Lewis, W., Li, M., Liu, S., Liu, T.-Y., Luo, R.,
  Menezes, A., Qin, T., Seide, F., Tan, X., Tian, F., Wu, L., Wu, S., Xia, Y.,
  Zhang, D., Zhang, Z., and Zhou, M.
\newblock Achieving human parity on automatic {Chinese} to {English} news
  translation, 2018.
\newblock URL \url{https://arxiv.org/abs/1803.05567}.

\bibitem[Kaiser et~al.(2018)Kaiser, Roy, Vaswani, Parmar, Bengio, Uszkoreit,
  and Shazeer]{Kaiser2018FastDI}
Kaiser, L., Roy, A., Vaswani, A., Parmar, N., Bengio, S., Uszkoreit, J., and
  Shazeer, N.
\newblock Fast decoding in sequence models using discrete latent variables.
\newblock In \emph{Proc.\ of ICML}, 2018.
\newblock URL \url{https://arxiv.org/abs/1803.03382}.

\bibitem[Kim \& Rush(2016)Kim and Rush]{Kim2016SequenceLevelKD}
Kim, Y. and Rush, A.~M.
\newblock Sequence-level knowledge distillation.
\newblock In \emph{Proc.\ of EMNLP}, 2016.
\newblock URL \url{https://arxiv.org/abs/1606.07947}.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{Kingma2014AdamAM}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{Proc.\ of ICLR}, 2015.
\newblock URL \url{https://arxiv.org/abs/1412.6980}.

\bibitem[Lee et~al.(2018)Lee, Mansimov, and Cho]{Lee2018DeterministicNN}
Lee, J.~D., Mansimov, E., and Cho, K.
\newblock Deterministic non-autoregressive neural sequence modeling by
  iterative refinement.
\newblock In \emph{Proc. of EMNLP}, 2018.
\newblock URL \url{https://arxiv.org/abs/1802.06901}.

\bibitem[Li et~al.(2020)Li, Meng, Yuan, Wu, and Li]{Li2020LAVANA}
Li, X., Meng, Y., Yuan, A., Wu, F., and Li, J.
\newblock {LAVA NAT}: A non-autoregressive translation model with look-around
  decoding and vocabulary attention, 2020.
\newblock URL \url{https://arxiv.org/abs/2002.03084}.

\bibitem[Li et~al.(2019)Li, Lin, He, Tian, Qin, Wang, and
  Liu]{Li2019HintBasedTF}
Li, Z., Lin, Z., He, D., Tian, F., Qin, T., Wang, L., and Liu, T.-Y.
\newblock Hint-based training for non-autoregressive machine translation.
\newblock In \emph{Proc.\ of EMNLP}, 2019.
\newblock URL \url{https://arxiv.org/abs/1909.06708}.

\bibitem[Libovick{\'y} \& Helcl(2018)Libovick{\'y} and
  Helcl]{Libovick2018EndtoEndNN}
Libovick{\'y}, J. and Helcl, J.
\newblock End-to-end non-autoregressive neural machine translation with
  connectionist temporal classification.
\newblock In \emph{Proc.\ of EMNLP}, 2018.
\newblock URL \url{https://arxiv.org/abs/1811.04719}.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{Liu2019RoBERTaAR}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L.~S., and Stoyanov, V.
\newblock {RoBERTa}: A robustly optimized bert pretraining approach, 2019.
\newblock URL \url{https://arxiv.org/abs/1907.11692}.

\bibitem[Luong et~al.(2015)Luong, Pham, and Manning]{luong-etal-2015-effective}
Luong, T., Pham, H., and Manning, C.~D.
\newblock Effective approaches to attention-based neural machine translation.
\newblock In \emph{Proc.\ of EMNLP}, September 2015.
\newblock URL \url{https://www.aclweb.org/anthology/D15-1166}.

\bibitem[Ma et~al.(2019)Ma, Zhou, Li, Neubig, and Hovy]{Ma2019FlowSeqNC}
Ma, X., Zhou, C., Li, X., Neubig, G., and Hovy, E.~H.
\newblock {FlowSeq}: Non-autoregressive conditional sequence generation with
  generative flow.
\newblock In \emph{Proc.\ of EMNLP}, 2019.
\newblock URL \url{https://arxiv.org/abs/1909.02480}.

\bibitem[Mansimov et~al.(2019)Mansimov, Wang, and Cho]{Mansimov2019AGF}
Mansimov, E., Wang, A., and Cho, K.
\newblock A generalized framework of sequence generation with application to
  undirected sequence models, 2019.
\newblock URL \url{https://arxiv.org/abs/1905.12790}.

\bibitem[Micikevicius et~al.(2018)Micikevicius, Narang, Alben, Diamos, Elsen,
  Garcia, Ginsburg, Houston, Kuchaiev, Venkatesh, and
  Wu]{micikevicius2018mixed}
Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D.,
  Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., and Wu, H.
\newblock Mixed precision training.
\newblock In \emph{Proc.\ of ICLR}, 2018.
\newblock URL \url{https://arxiv.org/abs/1710.03740}.

\bibitem[Nakayama et~al.(2019)Nakayama, Kano, Tjandra, Sakti, and
  Nakamura]{Speech2019}
Nakayama, S., Kano, T., Tjandra, A., Sakti, S., and Nakamura, S.
\newblock Recognition and translation of code-switching speech utterances.
\newblock In \emph{Proc.\ of Oriental COCOSDA}, 2019.
\newblock URL
  \url{https://ahcweb01.naist.jp/papers/conference/2019/201910\_OCOCOSDA\_sahoko-n/201910\_OCOCOSDA\_sahoko-n.paper.pdf}.

\bibitem[Ott et~al.(2018)Ott, Edunov, Grangier, and Auli]{Ott2018ScalingNM}
Ott, M., Edunov, S., Grangier, D., and Auli, M.
\newblock Scaling neural machine translation.
\newblock In \emph{Proc.\ of WMT}, 2018.
\newblock URL \url{https://arxiv.org/abs/1806.00187}.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{ott-etal-2019-fairseq}
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and
  Auli, M.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock In \emph{NAACL Demonstrations}, 2019.
\newblock URL \url{https://arxiv.org/abs/1904.01038}.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu]{Papineni2001BleuAM}
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
\newblock {BLEU}: a method for automatic evaluation of machine translation.
\newblock In \emph{Proc.\ of ACL}, 2002.
\newblock URL \url{https://www.aclweb.org/anthology/P02-1040/}.

\bibitem[Post(2018)]{post-2018-call}
Post, M.
\newblock A call for clarity in reporting {BLEU} scores.
\newblock In \emph{Proc.\ of WMT}, 2018.
\newblock URL \url{https://www.aclweb.org/anthology/W18-6319}.

\bibitem[Ran et~al.(2019)Ran, Lin, Li, and Zhou]{ReorderNAT}
Ran, Q., Lin, Y., Li, P., and Zhou, J.
\newblock Guiding non-autoregressive neural machine translation decoding with
  reordering information, 2019.
\newblock URL \url{https://arxiv.org/abs/1911.02215}.

\bibitem[Saharia et~al.(2020)Saharia, Chan, Saxena, and Norouzi]{Saharia2020}
Saharia, C., Chan, W., Saxena, S., and Norouzi, M.
\newblock Non-autoregressive machine translation with latent alignments, 2020.
\newblock URL \url{https://arxiv.org/abs/2004.07437}.

\bibitem[Sennrich et~al.(2016)Sennrich, Haddow, and
  Birch]{sennrich-etal-2016-neural}
Sennrich, R., Haddow, B., and Birch, A.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{Proc.\ of ACL}, 2016.
\newblock URL \url{https://www.aclweb.org/anthology/P16-1162}.

\bibitem[Shao et~al.(2020)Shao, Zhang, Feng, Meng, and Zhou]{minBOW}
Shao, C., Zhang, J., Feng, Y., Meng, F., and Zhou, J.
\newblock Minimizing the bag-of-ngrams difference for non-autoregressive neural
  machine translation.
\newblock In \emph{Proc.\ of AAAI}, 2020.
\newblock URL \url{https://arxiv.org/abs/1911.09320}.

\bibitem[Shu et~al.(2020)Shu, Lee, Nakayama, and Cho]{Shu2019LatentVariableNN}
Shu, R., Lee, J., Nakayama, H., and Cho, K.
\newblock Latent-variable non-autoregressive neural machine translation with
  deterministic inference using a delta posterior.
\newblock In \emph{Proc.\ of AAAI}, 2020.
\newblock URL \url{https://arxiv.org/abs/1908.07181}.

\bibitem[Stern et~al.(2018)Stern, Shazeer, and Uszkoreit]{Stern2018BlockwisePD}
Stern, M., Shazeer, N., and Uszkoreit, J.
\newblock Blockwise parallel decoding for deep autoregressive models.
\newblock In \emph{Proc. of NeurIPS}, 2018.
\newblock URL \url{https://arxiv.org/abs/1811.03115}.

\bibitem[Stern et~al.(2019)Stern, Chan, Kiros, and
  Uszkoreit]{Stern2019InsertionTF}
Stern, M., Chan, W., Kiros, J.~R., and Uszkoreit, J.
\newblock Insertion transformer: Flexible sequence generation via insertion
  operations.
\newblock In \emph{Proc.\ of ICML}, 2019.
\newblock URL \url{https://arxiv.org/abs/1902.03249}.

\bibitem[Sun et~al.(2019)Sun, Li, Wang, He, Lin, and Deng]{Sun2019Fast}
Sun, Z., Li, Z., Wang, H., He, D., Lin, Z., and Deng, Z.
\newblock Fast structured decoding for sequence models.
\newblock In \emph{Proc. of NeurIPS}, 2019.
\newblock URL \url{https://arxiv.org/abs/1910.11555}.

\bibitem[Tu et~al.(2020)Tu, Pang, Wiseman, and Gimpel]{Tu2020ENGINEEI}
Tu, L., Pang, R.~Y., Wiseman, S., and Gimpel, K.
\newblock {ENGINE}: Energy-based inference networks for non-autoregressive
  machine translation.
\newblock In \emph{Proc.\ of ACL}, 2020.
\newblock URL \url{https://arxiv.org/abs/2005.00850}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{Vaswani2017AttentionIA}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Proc. of NeurIPS}, 2017.
\newblock URL \url{https://arxiv.org/abs/1706.03762}.

\bibitem[Wang et~al.(2019)Wang, Tian, He, Qin, Zhai, and
  Liu]{Wang2019NonAutoregressiveMT}
Wang, Y., Tian, F., He, D., Qin, T., Zhai, C., and Liu, T.-Y.
\newblock Non-autoregressive machine translation with auxiliary regularization.
\newblock In \emph{Proc.\ of AAAI}, 2019.
\newblock URL \url{https://arxiv.org/abs/1902.10245}.

\bibitem[Wu et~al.(2019)Wu, Fan, Baevski, Dauphin, and Auli]{wu2018pay}
Wu, F., Fan, A., Baevski, A., Dauphin, Y., and Auli, M.
\newblock Pay less attention with lightweight and dynamic convolutions.
\newblock In \emph{Proc.\ of ICLR}, 2019.
\newblock URL \url{https://arxiv.org/abs/1901.10430}.

\bibitem[Yang et~al.(2019{\natexlab{a}})Yang, Liu, and Zou]{Video2019}
Yang, B., Liu, F., and Zou, Y.
\newblock Non-autoregressive video captioning with iterative refinement,
  2019{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/1911.12018}.

\bibitem[Yang et~al.(2019{\natexlab{b}})Yang, Dai, Yang, Carbonell,
  Salakhutdinov, and Le]{Yang2019XLNetGA}
Yang, Z., Dai, Z., Yang, Y., Carbonell, J.~G., Salakhutdinov, R., and Le, Q.~V.
\newblock {XLNet}: Generalized autoregressive pretraining for language
  understanding.
\newblock In \emph{Proc. of NeurIPS}, 2019{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/1906.08237}.

\bibitem[Zhang et~al.(2020)Zhang, Wang, Li, Gan, Brockett, and
  Dolan]{Zhang2020POINTERCT}
Zhang, Y., Wang, G., Li, C., Gan, Z., Brockett, C., and Dolan, B.
\newblock {POINTER}: Constrained text generation via insertion-based generative
  pre-training, 2020.
\newblock URL \url{https://arxiv.org/abs/2005.00558}.

\bibitem[Zhou et~al.(2020)Zhou, Neubig, and Gu]{UnderstandingKD}
Zhou, C., Neubig, G., and Gu, J.
\newblock Understanding knowledge distillation in non-autoregressive machine
  translation.
\newblock In \emph{Proc.\ of ICLR}, 2020.
\newblock URL \url{https://arxiv.org/abs/1911.02727}.

\bibitem[Zhou \& Keung(2020)Zhou and Keung]{Zhou2020ImprovingNN}
Zhou, J. and Keung, P.
\newblock Improving non-autoregressive neural machine translation with
  monolingual data.
\newblock In \emph{Proc.\ of ACL}, 2020.
\newblock URL \url{https://arxiv.org/abs/2005.00932}.

\end{thebibliography}
