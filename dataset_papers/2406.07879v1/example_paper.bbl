\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bako et~al.(2017)Bako, Vogels, McWilliams, Meyer, NovDoll\'{a}k,
  Harvill, Sen, DeRose, and Rousselle]{WeightGeneration_KPN}
Bako, S., Vogels, T., McWilliams, B., Meyer, M., NovDoll\'{a}k, J., Harvill,
  A., Sen, P., DeRose, T., and Rousselle, F.
\newblock Kernel-predicting convolutional networks for denoising monte carlo
  renderings.
\newblock In \emph{Siggraph}, 2017.

\bibitem[Chen et~al.(2020)Chen, Dai, Liu, Chen, Yuan, and
  Liu]{DynamicConv_DyConv}
Chen, Y., Dai, X., Liu, M., Chen, D., Yuan, L., and Liu, Z.
\newblock Dynamic convolution: Attention over convolution kernels.
\newblock In \emph{CVPR}, 2020.

\bibitem[Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and
  V]{augmentation_randaugment}
Cubuk, E.~D., Zoph, B., Shlens, J., and V, L.~Q.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In \emph{CVPR Workshops}, 2020.

\bibitem[Diba et~al.(2019)Diba, Sharma, Van~Gool, and
  Stiefelhagen]{WeightGeneration_DynamoNet}
Diba, A., Sharma, V., Van~Gool, L., and Stiefelhagen, R.
\newblock Dynamonet: Dynamic action and motion network.
\newblock In \emph{ICCV}, 2019.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Dosovitskiy, Beyer, Kolesnikov,
  Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  Uszkoreit, and Houlsby]{SelfAttention_ViT}
Dosovitskiy, A., Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D.,
  Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
  S., Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{ICLR}, 2021.

\bibitem[Ha et~al.(2017)Ha, Dai, and Le]{WeightGeneration_Hypernetworks}
Ha, D., Dai, A.~M., and Le, Q.~V.
\newblock Hypernetworks.
\newblock In \emph{ICLR}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{CNN_ResNet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[He et~al.(2017)He, Gkioxari, Doll{\'a}r, and
  Girshick]{DL_Instance_MaskRCNN}
He, K., Gkioxari, G., Doll{\'a}r, P., and Girshick, R.
\newblock Mask r-cnn.
\newblock In \emph{ICCV}, 2017.

\bibitem[He et~al.(2023)He, Jiang, Dong, and Ding]{DynamicConv_SDConv}
He, S., Jiang, C., Dong, D., and Ding, L.
\newblock Sd-conv: Towards the parameter-effciency of dynamic convolution.
\newblock In \emph{WACV}, 2023.

\bibitem[Howard et~al.(2019)Howard, Sandler, Chu, Chen, Chen, Tan, Wang, Zhu,
  Pang, Vasudevan, Le, and Adam]{CNN_MobileNetV3}
Howard, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., Wang, W.,
  Zhu, Y., Pang, R., Vasudevan, V., Le, Q.~V., and Adam, H.
\newblock Searching for mobilenetv3.
\newblock In \emph{ICCV}, 2019.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{CNN_MobileNets}
Howard, A.~G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T.,
  Andreetto, M., and Adam, H.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{arXiv preprint arXiv:1704.04861}, 2017.

\bibitem[Hu et~al.(2018{\natexlab{a}})Hu, Shen, Albanie, Sun, and
  Vedaldi]{Attention_GE}
Hu, J., Shen, L., Albanie, S., Sun, G., and Vedaldi, A.
\newblock Gather-excite: Exploiting feature context in convolutional neural
  networks.
\newblock In \emph{NeurIPS}, 2018{\natexlab{a}}.

\bibitem[Hu et~al.(2018{\natexlab{b}})Hu, Shen, and Sun]{Attention_SE}
Hu, J., Shen, L., and Sun, G.
\newblock Squeeze-and-excitation networks.
\newblock In \emph{CVPR}, 2018{\natexlab{b}}.

\bibitem[Huang et~al.(2017)Huang, Liu, van~der Maaten, and
  Weinberger]{CNN_DenseNet}
Huang, G., Liu, Z., van~der Maaten, L., and Weinberger, K.~Q.
\newblock Densely connected convolutional networks.
\newblock In \emph{CVPR}, 2017.

\bibitem[Jaderberg et~al.(2015)Jaderberg, Simonyan, Zisserman, and
  Kavukcuoglu]{WeightGeneration_STNetworks}
Jaderberg, M., Simonyan, K., Zisserman, A., and Kavukcuoglu, K.
\newblock Spatial transformer networks.
\newblock In \emph{NIPS}, 2015.

\bibitem[Jia et~al.(2016)Jia, Brabandere, Tuytelaars, and
  Gool]{WeightGeneration_DynamicFilterNetworks}
Jia, X., Brabandere, B.~D., Tuytelaars, T., and Gool, L.~V.
\newblock Dynamic filter networks.
\newblock In \emph{NIPS}, 2016.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{CNN_AlexNet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{NIPS}, 2012.

\bibitem[Lee et~al.(2019)Lee, Kim, and Nam]{Attention_SRM}
Lee, H., Kim, H.-E., and Nam, H.
\newblock Srm: A style-based recalibration module for convolutional neural
  networks.
\newblock In \emph{ICCV}, 2019.

\bibitem[Li et~al.(2022)Li, Zhou, and Yao]{DynamicConv_ODConv}
Li, C., Zhou, A., and Yao, A.
\newblock Omni-dimensional dynamic convolution.
\newblock In \emph{ICLR}, 2022.

\bibitem[Li et~al.(2020)Li, Yao, and Chen]{GroupConv_PSConv}
Li, D., Yao, A., and Chen, Q.
\newblock Psconv: Squeezing feature pyramid into one compact poly-scale
  convolutional layer.
\newblock In \emph{ECCV}, 2020.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Hu, Wang, Li, She, Zhu, Zhang, and
  Chen]{WeightGeneration_Involution}
Li, D., Hu, J., Wang, C., Li, X., She, Q., Zhu, L., Zhang, T., and Chen, Q.
\newblock Involution: Inverting the inherence of convolution for visual
  recognition.
\newblock In \emph{CVPR}, 2021{\natexlab{a}}.

\bibitem[Li et~al.(2019)Li, Wang, Hu, and Yang]{Attention_SKNets}
Li, X., Wang, W., Hu, X., and Yang, J.
\newblock Selective kernel networks.
\newblock In \emph{CVPR}, 2019.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Chen, Dai, Liu, Chen, Yu, Lu, Liu,
  Chen, and Vasconcelos]{DynamicConv_ResDyConv}
Li, Y., Chen, Y., Dai, X., Liu, M., Chen, D., Yu, Y., Lu, Y., Liu, Z., Chen,
  M., and Vasconcelos, N.
\newblock Revisiting dynamic convolution via matrix decomposition.
\newblock In \emph{ICLR}, 2021{\natexlab{b}}.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
  Doll{\'a}r, and Zitnick]{Dataset_MS_COCO}
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
  Doll{\'a}r, P., and Zitnick, C.~L.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{ECCV}, 2014.

\bibitem[Lin et~al.(2020)Lin, Ma, Liu, and Chang]{DynamicConv_CGC}
Lin, X., Ma, L., Liu, W., and Chang, S.-F.
\newblock Context-gated convolution.
\newblock In \emph{ECCV}, 2020.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{SelfAttention_Swin}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In \emph{ICCV}, 2021.

\bibitem[Liu et~al.(2022)Liu, Mao, Wu, Feichtenhofer, Darrell, and
  Xie]{CNN_ConvNeXt}
Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S.
\newblock A convnet for the 2020s.
\newblock In \emph{CVPR}, 2022.

\bibitem[Ma et~al.(2020)Ma, Zhang, Huang, and Sun]{WeightGeneration_WeightNet}
Ma, N., Zhang, X., Huang, J., and Sun, J.
\newblock Weightnet: Revisiting the design space of weight networks.
\newblock In \emph{ECCV}, 2020.

\bibitem[Mildenhall et~al.(2018)Mildenhall, Barron, Chen, Sharlet, Ng, and
  Carroll]{WeightGeneration_KPN_BurstDenoising}
Mildenhall, B., Barron, J.~T., Chen, J., Sharlet, D., Ng, R., and Carroll, R.
\newblock Burst denoising with kernel prediction networks.
\newblock In \emph{CVPR}, 2018.

\bibitem[Munkhdalai \& Yu(2017)Munkhdalai and
  Yu]{WeightGeneration_MetaNetworks}
Munkhdalai, T. and Yu, H.
\newblock Meta networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Park et~al.(2018)Park, Woo, Lee, and Kweon]{Attention_BAM}
Park, J., Woo, S., Lee, J.-Y., and Kweon, I.~S.
\newblock Bam: Bottleneck attention module.
\newblock In \emph{BMVC}, 2018.

\bibitem[Quader et~al.(2020)Quader, Bhuiyan, Lu, Dai, and
  Li]{DynamicConv_WeightExcitation}
Quader, N., Bhuiyan, M. M.~I., Lu, J., Dai, P., and Li, W.
\newblock Weight excitation: Built-in attention mechanisms in convolutional
  neural networks.
\newblock In \emph{ECCV}, 2020.

\bibitem[Radosavovic et~al.(2020)Radosavovic, Kosaraju, Girshick, He, and
  Doll{\'a}r]{CNN_RegNet}
Radosavovic, I., Kosaraju, R.~P., Girshick, R., He, K., and Doll{\'a}r, P.
\newblock Designing network design spaces.
\newblock In \emph{CVPR}, 2020.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Li]{Dataset_ImageNet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Li, F.-F.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{IJCV}, 2015.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{CNN_MobileNetV2}
Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{CVPR}, 2018.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{CNN_VGGNet}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{ICLR}, 2015.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{CNN_GoogLeNet}
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
  Vanhoucke, V., and Rabinovich, A.
\newblock Going deeper with convolutions.
\newblock In \emph{CVPR}, 2015.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{augmentation_labelsmoothing}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{CVPR}, 2016.

\bibitem[Tan \& Le(2019{\natexlab{a}})Tan and Le]{CNN_EfficientNet}
Tan, M. and Le, Q.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{ICML}, 2019{\natexlab{a}}.

\bibitem[Tan \& Le(2019{\natexlab{b}})Tan and Le]{GroupConv_MixConv}
Tan, M. and Le, Q.~V.
\newblock Mixconv: Mixed depthwise convolutional kernels.
\newblock In \emph{BMVC}, 2019{\natexlab{b}}.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J{\'e}gou]{SelfAttention_DeiT}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J{\'e}gou,
  H.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In \emph{ICML}, 2021.

\bibitem[Wang et~al.(2017)Wang, Jiang, Qian, Yang, Li, Zhang, Wang, and
  Tang]{Attention_ResidualAttention}
Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Wang, X., and Tang,
  X.
\newblock Residual attention network for image classification.
\newblock In \emph{CVPR}, 2017.

\bibitem[Wang et~al.(2019)Wang, Chen, Xu, Liu, Chen, and
  Lin]{WeightGeneration_CARAFE}
Wang, J., Chen, K., Xu, R., Liu, Z., Chen, C.~L., and Lin, D.
\newblock Carafe: Content-aware reassembly of features.
\newblock In \emph{ICCV}, 2019.

\bibitem[Wang et~al.(2020)Wang, Wu, Zhu, Li, Zuo, and Hu]{Attention_ECA}
Wang, Q., Wu, B., Zhu, P., Li, P., Zuo, W., and Hu, Q.
\newblock Eca-net: Efficient channel attention for deep convolutional neural
  networks.
\newblock In \emph{CVPR}, 2020.

\bibitem[Woo et~al.(2018)Woo, Park, Lee, and Kweon]{Attention_CBAM}
Woo, S., Park, J., Lee, J.-Y., and Kweon, I.~S.
\newblock Cbam: Convolutional block attention module.
\newblock In \emph{ECCV}, 2018.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and He]{CNN_ResNeXt}
Xie, S., Girshick, R., Doll{\'a}r, P., Tu, Z., and He, K.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{CVPR}, 2017.

\bibitem[Yang et~al.(2019{\natexlab{a}})Yang, Bender, Le, and
  Ngiam]{DynamicConv_CondConv}
Yang, B., Bender, G., Le, Q.~V., and Ngiam, J.
\newblock Condconv: Conditionally parameterized convolutions for efficient
  inference.
\newblock In \emph{NeurIPS}, 2019{\natexlab{a}}.

\bibitem[Yang et~al.(2019{\natexlab{b}})Yang, Yunhe, Chen, Liu, Shi, Xu, Xu,
  and Xu]{GroupConv_LegoFilters}
Yang, Z., Yunhe, W., Chen, H., Liu, C., Shi, B., Xu, C., Xu, C., and Xu, C.
\newblock Legonet: Efficient convolutional neural networks with lego filters.
\newblock In \emph{ICML}, 2019{\natexlab{b}}.

\bibitem[Yun et~al.(2019)Yun, , Han, Oh, Chun, Choe, and
  Yoo]{augmentation_cutmix}
Yun, S., , Han, D., Oh, S.~J., Chun, S., Choe, J., and Yoo, Y.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In \emph{ICCV}, 2019.

\bibitem[Zhang et~al.(2018{\natexlab{a}})Zhang, Cisse, Dauphin, and
  Lopez-Paz]{augmentation_mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{ICLR}, 2018{\natexlab{a}}.

\bibitem[Zhang et~al.(2018{\natexlab{b}})Zhang, Zhou, Lin, and
  Sun]{CNN_ShuffleNet}
Zhang, X., Zhou, X., Lin, M., and Sun, J.
\newblock Shufflenet: An extremely efficient convolutional neural network for
  mobile devices.
\newblock In \emph{CVPR}, 2018{\natexlab{b}}.

\bibitem[Zhong et~al.(2020)Zhong, Zheng, Kang, Li, and
  Yang]{augmentation_randerazing}
Zhong, Z., Zheng, L., Kang, G., Li, S., and Yang, Y.
\newblock Random erasing data augmentation.
\newblock In \emph{AAAI}, 2020.

\end{thebibliography}
