\begin{thebibliography}{10}

\bibitem{AW21}
{\sc Agapiou, S., and Wang, S.}
\newblock Laplace priors and spatial inhomogeneity in {B}ayesian inverse
  problems, 2021.

\bibitem{AA20}
{\sc Aykroyd, R.~G., and Aljohani, H.}
\newblock A {B}ayesian approach to wavelet-based modelling of discontinuous
  functions applied to inverse problems.
\newblock {\em Comm. Statist. Simulation Comput. 49}, 1 (2020), 207--225.

\bibitem{BD06}
{\sc Bioucas-Dias, J.~M.}
\newblock Bayesian wavelet-based image deconvolution: a {GEM} algorithm
  exploiting a class of heavy-tailed priors.
\newblock {\em IEEE Trans. Image Process. 15}, 4 (2006), 937--951.

\bibitem{C08}
{\sc Castillo, I.}
\newblock Lower bounds for posterior rates with {G}aussian process priors.
\newblock {\em Electron. J. Stat. 2\/} (2008), 1281--1299.

\bibitem{CR15}
{\sc Castillo, I., and Rousseau, J.}
\newblock A {B}ernstein--von {M}ises theorem for smooth functionals in
  semiparametric models.
\newblock {\em Ann. Statist. 43}, 6 (2015), 2353--2383.

\bibitem{CA20}
{\sc Ch{\'e}rief-Abdellatif, B.-E.}
\newblock Convergence rates of variational inference in sparse deep learning.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning\/} (13--18 Jul 2020), vol.~119 of {\em Proceedings of Machine
  Learning Research}, PMLR, pp.~1831--1842.

\bibitem{CDV93}
{\sc Cohen, A., Daubechies, I., and Vial, P.}
\newblock Wavelets on the interval and fast wavelet transforms.
\newblock {\em Appl. Comput. Harmon. Anal. 1}, 1 (1993), 54--81.

\bibitem{DP06}
{\sc Da~Prato, G.}
\newblock {\em An introduction to infinite-dimensional analysis}.
\newblock Universitext. Springer-Verlag, Berlin, 2006.
\newblock Revised and extended from the 2001 original by Da Prato.

\bibitem{DL13}
{\sc Damianou, A., and Lawrence, N.~D.}
\newblock Deep {G}aussian processes.
\newblock In {\em Proceedings of the Sixteenth International Conference on
  Artificial Intelligence and Statistics\/} (29 Apr--01 May 2013), vol.~31 of
  {\em Proceedings of Machine Learning Research}, PMLR, pp.~207--215.

\bibitem{DJ98}
{\sc Donoho, D.~L., and Johnstone, I.~M.}
\newblock Minimax estimation via wavelet shrinkage.
\newblock {\em Ann. Statist. 26}, 3 (1998), 879--921.

\bibitem{FSH21}
{\sc Finocchio, G., and Schmidt-Hieber, J.}
\newblock Posterior contraction for deep {G}aussian process priors, 2021.

\bibitem{F99}
{\sc Freedman, D.}
\newblock On the {B}ernstein-von {M}ises theorem with infinite-dimensional
  parameters.
\newblock {\em Ann. Statist. 27}, 4 (1999), 1119--1140.

\bibitem{MR1790007}
{\sc Ghosal, S., Ghosh, J.~K., and van~der Vaart, A.~W.}
\newblock Convergence rates of posterior distributions.
\newblock {\em Ann. Statist. 28}, 2 (2000), 500--531.

\bibitem{GvdV17}
{\sc Ghosal, S., and van~der Vaart, A.~W.}
\newblock {\em Fundamentals of Nonparametric Bayesian Inference}.
\newblock Cambridge University Press, New York, 2017.

\bibitem{GN16}
{\sc Gin\'{e}, E., and Nickl, R.}
\newblock {\em Mathematical foundations of infinite-dimensional statistical
  models}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics, [40].
  Cambridge University Press, New York, 2016.

\bibitem{MR1082147}
{\sc Hastie, T.~J., and Tibshirani, R.~J.}
\newblock {\em Generalized additive models}, vol.~43 of {\em Monographs on
  Statistics and Applied Probability}.
\newblock Chapman and Hall, Ltd., London, 1990.

\bibitem{HS20}
{\sc Hayakawa, S., and Suzuki, T.}
\newblock On the minimax optimality and superiority of deep neural network
  learning over sparse parameter spaces.
\newblock {\em Neural Networks 123\/} (2020), 343--361.

\bibitem{IF19}
{\sc Imaizumi, M., and Fukumizu, K.}
\newblock Deep neural networks learn non-smooth functions effectively.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics, {AISTATS} 2019\/} (2019), vol.~89 of {\em Proceedings of
  Machine Learning Research}, {PMLR}, pp.~869--878.

\bibitem{IF20}
{\sc Imaizumi, M., and Fukumizu, K.}
\newblock Advantage of deep neural networks for estimating functions with
  singularity on hypersurfaces, 2020.

\bibitem{JDM17}
{\sc Jacobsen, R.~D., and M\o~ller, J.}
\newblock Frequentist and {B}ayesian inference for {G}aussian-log-{G}aussian
  wavelet trees and statistical signal processing applications.
\newblock {\em Stat 6\/} (2017), 248--256.

\bibitem{JohnstoneBook}
{\sc Johnstone, I.}
\newblock Gaussian estimation: Sequence and wavelet models.
\newblock available from \url{https://imjohnstone.su.domains/GE_09_16_19.pdf},
  2019.

\bibitem{J10}
{\sc Johnstone, I.~M.}
\newblock High dimensional {B}ernstein--von {M}ises: simple examples.
\newblock In {\em Borrowing strength: theory powering applications---a
  {F}estschrift for {L}awrence {D}. {B}rown}, vol.~6 of {\em Inst. Math. Stat.
  (IMS) Collect.} Inst. Math. Statist., Beachwood, OH, 2010, pp.~87--98.

\bibitem{KT93}
{\sc Korostel\"{e}v, A.~P., and Tsybakov, A.~B.}
\newblock {\em Minimax theory of image reconstruction}, vol.~82 of {\em Lecture
  Notes in Statistics}.
\newblock Springer-Verlag, New York, 1993.

\bibitem{LM00}
{\sc Laurent, B., and Massart, P.}
\newblock Adaptive estimation of a quadratic functional by model selection.
\newblock {\em Ann. Statist. 28}, 5 (2000), 1302--1338.

\bibitem{MV99}
{\sc M\"{u}ller, P., and Vidakovic, B.}, Eds.
\newblock {\em Bayesian inference in wavelet-based models}, vol.~141 of {\em
  Lecture Notes in Statistics}.
\newblock Springer-Verlag, New York, 1999.

\bibitem{N96}
{\sc Neal, R.~M.}
\newblock {\em Bayesian Learning for Neural Networks}.
\newblock Springer-Verlag, Berlin, Heidelberg, 1996.

\bibitem{PETERSEN2018296}
{\sc Petersen, P., and Voigtlaender, F.}
\newblock Optimal approximation of piecewise smooth functions using deep {ReLU}
  neural networks.
\newblock {\em Neural Networks 108\/} (2018), 296--330.

\bibitem{PR18}
{\sc Polson, N.~G., and Ro{\v{c}}kov{\'a}, V.}
\newblock Posterior concentration for sparse deep learning.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2018),
  pp.~930--941.

\bibitem{RW06}
{\sc Rasmussen, C.~E., and Williams, C. K.~I.}
\newblock {\em Gaussian processes for machine learning}.
\newblock Adaptive Computation and Machine Learning. MIT Press, Cambridge, MA,
  2006.

\bibitem{RS20}
{\sc Ray, K., Szabo, B., and Clara, G.}
\newblock Spike and slab variational {B}ayes for high dimensional logistic
  regression.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2020),
  vol.~33, Curran Associates, Inc., pp.~14423--14434.

\bibitem{RS21}
{\sc Ray, K., and Szabó, B.}
\newblock Variational {B}ayes for high-dimensional linear regression with
  sparse priors.
\newblock {\em Journal of the American Statistical Association 0}, 0 (2021),
  1--12.

\bibitem{RvdV20}
{\sc Ray, K., and van~der Vaart, A.}
\newblock Semiparametric {B}ayesian causal inference.
\newblock {\em Ann. Statist. 48}, 5 (2020), 2999--3020.

\bibitem{R08}
{\sc Rei\ss, M.}
\newblock Asymptotic equivalence for nonparametric regression with multivariate
  and random design.
\newblock {\em Ann. Statist. 36}, 4 (2008), 1957--1982.

\bibitem{RR21}
{\sc Rockova, V., and Rousseau, J.}
\newblock Ideal {B}ayesian spatial adaptation, 2021.

\bibitem{SH20}
{\sc Schmidt-Hieber, J.}
\newblock Nonparametric regression using deep neural networks with {R}e{LU}
  activation function.
\newblock {\em Ann. Statist. 48}, 4 (2020), 1875--1897.

\bibitem{SCHMIDTHIEBER2021119}
{\sc Schmidt-Hieber, J.}
\newblock The {K}olmogorov–{A}rnold representation theorem revisited.
\newblock {\em Neural Networks 137\/} (2021), 119--126.

\bibitem{S19}
{\sc Suzuki, T.}
\newblock Adaptivity of deep {ReLU} network for learning in {B}esov and mixed
  smooth {B}esov spaces: optimal rate and curse of dimensionality.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019\/} (2019), OpenReview.net.

\bibitem{SN21}
{\sc Suzuki, T., and Nitanda, A.}
\newblock Deep learning is adaptive to intrinsic dimensionality of model
  smoothness in anisotropic {B}esov space.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2021),
  vol.~34, Curran Associates, Inc., pp.~3609--3621.

\bibitem{TMNMZ14}
{\sc Tang, J., Meng, Z., Nguyen, X., Mei, Q., and Zhang, M.}
\newblock Understanding the limiting factors of topic modeling via posterior
  contraction analysis.
\newblock In {\em Proceedings of the 31st International Conference on Machine
  Learning\/} (Bejing, China, 22--24 Jun 2014), vol.~32 of {\em Proceedings of
  Machine Learning Research}, PMLR, pp.~190--198.

\bibitem{telgarsky16}
{\sc Telgarsky, M.}
\newblock Benefits of depth in neural networks.
\newblock In {\em 29th Annual Conference on Learning Theory\/} (Columbia
  University, New York, New York, USA, 23--26 Jun 2016), vol.~49 of {\em
  Proceedings of Machine Learning Research}, PMLR, pp.~1517--1539.

\bibitem{TS21}
{\sc Tsuji, K., and Suzuki, T.}
\newblock {Estimation error analysis of deep learning on the regression problem
  on the variable exponent Besov space}.
\newblock {\em Electronic Journal of Statistics 15}, 1 (2021), 1869 -- 1908.

\bibitem{vdV98}
{\sc van~der Vaart, A.~W.}
\newblock {\em Asymptotic statistics}, vol.~3 of {\em Cambridge Series in
  Statistical and Probabilistic Mathematics}.
\newblock Cambridge University Press, Cambridge, 1998.

\bibitem{vdVvZ08}
{\sc van~der Vaart, A.~W., and van Zanten, J.~H.}
\newblock Rates of contraction of posterior distributions based on {G}aussian
  process priors.
\newblock {\em Ann. Statist. 36}, 3 (2008), 1435--1463.

\bibitem{VEtAl09}
{\sc V\"{a}nsk\"{a}, S., Lassas, M., and Siltanen, S.}
\newblock Statistical {X}-ray tomography using empirical {B}esov priors.
\newblock {\em Int. J. Tomogr. Stat. 11}, S09 (2009), 3--32.

\bibitem{WB19b}
{\sc Wang, Y., and Blei, D.}
\newblock Variational {B}ayes under model misspecification.
\newblock In {\em Advances in Neural Information Processing Systems 32}. 2019,
  pp.~13357--13367.

\bibitem{WB19}
{\sc Wang, Y., and Blei, D.~M.}
\newblock Frequentist consistency of variational {B}ayes.
\newblock {\em J. Amer. Statist. Assoc. 114}, 527 (2019), 1147--1161.

\bibitem{YAROTSKY2017103}
{\sc Yarotsky, D.}
\newblock Error bounds for approximations with deep {ReLU} networks.
\newblock {\em Neural Networks 94\/} (2017), 103--114.

\bibitem{pmlryarotsky18a}
{\sc Yarotsky, D.}
\newblock Optimal approximation of continuous functions by very deep {ReLU}
  networks.
\newblock In {\em Proceedings of the 31st Conference On Learning Theory\/}
  (06--09 Jul 2018), vol.~75 of {\em Proceedings of Machine Learning Research},
  PMLR, pp.~639--649.

\end{thebibliography}
