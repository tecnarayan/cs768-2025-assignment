\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{KMK{\etalchar{+}}16b}

\bibitem[AAB{\etalchar{+}}16]{abadi2016tensorflow}
Mart{\'\i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Greg~S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et~al.
\newblock Tensorflow: Large-scale machine learning on heterogeneous distributed
  systems.
\newblock {\em arXiv preprint arXiv:1603.04467}, 2016.

\bibitem[AAZB{\etalchar{+}}17]{agarwal2017finding}
Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma.
\newblock Finding approximate local minima faster than gradient descent.
\newblock In {\em Proceedings of the 49th Annual ACM SIGACT Symposium on Theory
  of Computing}, pages 1195--1199. ACM, 2017.

\bibitem[ABH17]{agarwal2017second}
Naman Agarwal, Brian Bullins, and Elad Hazan.
\newblock Second-order stochastic optimization for machine learning in linear
  time.
\newblock {\em The Journal of Machine Learning Research}, 18(1):4148--4187,
  2017.

\bibitem[AGNZ18]{arora2018stronger}
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi~Zhang.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock {\em arXiv preprint arXiv:1802.05296}, 2018.

\bibitem[Ama98]{amari1998natural}
Shun-Ichi Amari.
\newblock Natural gradient works efficiently in learning.
\newblock {\em Neural computation}, 10(2):251--276, 1998.

\bibitem[AZ17]{allen2017katyusha}
Zeyuan Allen-Zhu.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock In {\em Proceedings of the 49th Annual ACM SIGACT Symposium on Theory
  of Computing}, pages 1200--1205. ACM, 2017.

\bibitem[AZH16]{allen2016variance}
Zeyuan Allen-Zhu and Elad Hazan.
\newblock Variance reduction for faster non-convex optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  699--707, 2016.

\bibitem[B{\etalchar{+}}15]{bubeck2015convex}
S{\'e}bastien Bubeck et~al.
\newblock Convex optimization: Algorithms and complexity.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  8(3-4):231--357, 2015.

\bibitem[CDHS17]{carmon2017convex}
Yair Carmon, John~C Duchi, Oliver Hinder, and Aaron Sidford.
\newblock “convex until proven guilty”: Dimension-free acceleration of
  gradient descent on non-convex functions.
\newblock In {\em International {C}onference on {M}achine {L}earning}, pages
  654--663, 2017.

\bibitem[CLL{\etalchar{+}}15]{chen2015mxnet}
Tianqi Chen, Mu~Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao,
  Bing Xu, Chiyuan Zhang, and Zheng Zhang.
\newblock Mx{N}et: A flexible and efficient machine learning library for
  heterogeneous distributed systems.
\newblock {\em arXiv preprint arXiv:1512.01274}, 2015.

\bibitem[CLSH18]{chen2018convergence}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of a class of adam-type algorithms for non-convex
  optimization.
\newblock {\em arXiv preprint arXiv:1808.02941}, 2018.

\bibitem[CYY{\etalchar{+}}18]{chen2018universal}
Zaiyi Chen, Tianbao Yang, Jinfeng Yi, Bowen Zhou, and Enhong Chen.
\newblock Universal stagewise learning for non-convex problems with convergence
  on averaged solutions.
\newblock {\em arXiv preprint arXiv:1808.06296}, 2018.

\bibitem[DHS11]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of Machine Learning Research}, 12:2121--2159, 2011.

\bibitem[Doz16]{dozat2016incorporating}
Timothy Dozat.
\newblock Incorporating {N}esterov momentum into {A}dam.
\newblock 2016.

\bibitem[EM15]{erdogdu2015convergence}
Murat~A Erdogdu and Andrea Montanari.
\newblock Convergence rates of sub-sampled newton methods.
\newblock In {\em Proceedings of the 28th International Conference on Neural
  Information Processing Systems-Volume 2}, pages 3052--3060. MIT Press, 2015.

\bibitem[Gas17]{gastaldi2017shake}
Xavier Gastaldi.
\newblock Shake-shake regularization.
\newblock {\em arXiv preprint arXiv:1705.07485}, 2017.

\bibitem[GKS18]{gupta2018shampoo}
Vineet Gupta, Tomer Koren, and Yoram Singer.
\newblock Shampoo: Preconditioned stochastic tensor optimization.
\newblock {\em arXiv preprint arXiv:1802.09568}, 2018.

\bibitem[GL13]{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem[HAK07]{hazan2007logarithmic}
Elad Hazan, Amit Agarwal, and Satyen Kale.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock {\em Machine Learning}, 69(2-3):169--192, 2007.

\bibitem[HS97]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem[KB14]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[KMK{\etalchar{+}}16a]{krueger2016zoneout}
David Krueger, Tegan Maharaj, J{\'a}nos Kram{\'a}r, Mohammad Pezeshki, Nicolas
  Ballas, Nan~Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Aaron Courville, and
  Chris Pal.
\newblock Zoneout: Regularizing rnns by randomly preserving hidden activations.
\newblock {\em arXiv preprint arXiv:1606.01305}, 2016.

\bibitem[KMK{\etalchar{+}}16b]{krummenacher2016scalable}
Gabriel Krummenacher, Brian McWilliams, Yannic Kilcher, Joachim~M Buhmann, and
  Nicolai Meinshausen.
\newblock Scalable adaptive stochastic optimization using random projections.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1750--1758, 2016.

\bibitem[KS17]{keskar2017improving}
Nitish~Shirish Keskar and Richard Socher.
\newblock Improving generalization performance by switching from adam to sgd.
\newblock {\em arXiv preprint arXiv:1712.07628}, 2017.

\bibitem[LACBL16]{luo2016efficient}
Haipeng Luo, Alekh Agarwal, Nicolo Cesa-Bianchi, and John Langford.
\newblock Efficient second order online learning by sketching.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  902--910, 2016.

\bibitem[LH16]{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: stochastic gradient descent with restarts.
\newblock {\em arXiv preprint arXiv:1608.03983}, 2016.

\bibitem[LN89]{liu1989limited}
Dong~C Liu and Jorge Nocedal.
\newblock On the limited memory bfgs method for large scale optimization.
\newblock {\em Mathematical Programming}, 45(1-3):503--528, 1989.

\bibitem[LO18]{li2018convergence}
Xiaoyu Li and Francesco Orabona.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock {\em arXiv preprint arXiv:1805.08114}, 2018.

\bibitem[LXTG17]{li2017visualizing}
Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock {\em arXiv preprint arXiv:1712.09913}, 2017.

\bibitem[MBJ18]{martens2018kronecker}
James Martens, Jimmy Ba, and Matt Johnson.
\newblock Kronecker-factored curvature approximations for recurrent neural
  networks.
\newblock 2018.

\bibitem[MDB17]{melis2017state}
G{\'a}bor Melis, Chris Dyer, and Phil Blunsom.
\newblock On the state of the art of evaluation in neural language models.
\newblock {\em arXiv preprint arXiv:1707.05589}, 2017.

\bibitem[MG15]{martens2015optimizing}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In {\em International {C}onference on {M}achine {L}earning}, pages
  2408--2417, 2015.

\bibitem[MKM{\etalchar{+}}94]{marcus1994penn}
Mitchell Marcus, Grace Kim, Mary~Ann Marcinkiewicz, Robert MacIntyre, Ann Bies,
  Mark Ferguson, Karen Katz, and Britta Schasberger.
\newblock The penn treebank: annotating predicate argument structure.
\newblock In {\em Proceedings of the workshop on Human Language Technology},
  pages 114--119. Association for Computational Linguistics, 1994.

\bibitem[MKS17]{merity2017regularizing}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher.
\newblock Regularizing and optimizing lstm language models.
\newblock {\em arXiv preprint arXiv:1708.02182}, 2017.

\bibitem[MKS18]{merity2018analysis}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher.
\newblock An analysis of neural language modeling at multiple scales.
\newblock {\em arXiv preprint arXiv:1803.08240}, 2018.

\bibitem[MRVW16]{mehta2016compadagrad}
Nishant~A Mehta, Alistair Rendell, Anish Varghese, and Christfried Webers.
\newblock Compadagrad: A compressed, complementary, computationally-efficient
  adaptive gradient method.
\newblock {\em arXiv preprint arXiv:1609.03319}, 2016.

\bibitem[PGC{\etalchar{+}}17]{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in {P}y{T}orch.
\newblock In {\em NIPS-W}, 2017.

\bibitem[PJ92]{polyak1992acceleration}
Boris~T Polyak and Anatoli~B Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock {\em SIAM Journal on Control and Optimization}, 30(4):838--855, 1992.

\bibitem[PMB13]{pascanu2013difficulty}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1310--1318, 2013.

\bibitem[RGYSD17]{raghu2017svcca}
Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein.
\newblock Svcca: Singular vector canonical correlation analysis for deep
  understanding and improvement.
\newblock {\em arXiv preprint arXiv:1706.05806}, 2017.

\bibitem[RKK18]{reddi2018convergence}
Sashank~J Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of {A}dam and beyond.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem[SRK{\etalchar{+}}19]{staib2019escaping}
Matthew Staib, Sashank~J Reddi, Satyen Kale, Sanjiv Kumar, and Suvrit Sra.
\newblock Escaping saddle points with adaptive gradient methods.
\newblock {\em arXiv preprint arXiv:1901.09149}, 2019.

\bibitem[SZ14]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[TH12]{tieleman2012lecture}
Tijmen Tieleman and Geoffrey Hinton.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock {\em COURSERA: Neural networks for machine learning}, 4(2):26--31,
  2012.

\bibitem[VSP{\etalchar{+}}17]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5998--6008, 2017.

\bibitem[WRS{\etalchar{+}}17]{wilson2017marginal}
Ashia~C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin
  Recht.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4151--4161, 2017.

\bibitem[WWB18]{ward2018adagrad}
Rachel Ward, Xiaoxia Wu, and Leon Bottou.
\newblock Adagrad stepsizes: Sharp convergence over nonconvex landscapes, from
  any initialization.
\newblock {\em arXiv preprint arXiv:1806.01811}, 2018.

\bibitem[Zei12]{zeiler2012adadelta}
Matthew~D Zeiler.
\newblock Adadelta: an adaptive learning rate method.
\newblock {\em arXiv preprint arXiv:1212.5701}, 2012.

\bibitem[ZS18]{zou2018convergence}
Fangyu Zou and Li~Shen.
\newblock On the convergence of adagrad with momentum for training deep neural
  networks.
\newblock {\em arXiv preprint arXiv:1808.03408}, 2018.

\bibitem[ZTY{\etalchar{+}}18]{zhou2018convergence}
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock {\em arXiv preprint arXiv:1808.05671}, 2018.

\end{thebibliography}
