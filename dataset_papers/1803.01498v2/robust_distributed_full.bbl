\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
M.~Abadi, P.~Barham, J.~Chen, Z.~Chen, A.~Davis, J.~Dean, M.~Devin,
  S.~Ghemawat, G.~Irving, M.~Isard, et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{OSDI}, volume~16, pages 265--283, 2016.

\bibitem[Alon et~al.(1999)Alon, Matias, and Szegedy]{alon1999space}
N.~Alon, Y.~Matias, and M.~Szegedy.
\newblock The space complexity of approximating the frequency moments.
\newblock \emph{Journal of Computer and system sciences}, 58\penalty0
  (1):\penalty0 137--147, 1999.

\bibitem[Berry(1941)]{berry1941accuracy}
A.~C. Berry.
\newblock The accuracy of the gaussian approximation to the sum of independent
  variates.
\newblock \emph{Transactions of the american mathematical society}, 49\penalty0
  (1):\penalty0 122--136, 1941.

\bibitem[Bhatia et~al.(2015)Bhatia, Jain, and Kar]{bhatia2015robust}
K.~Bhatia, P.~Jain, and P.~Kar.
\newblock Robust regression via hard thresholding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  721--729, 2015.

\bibitem[Blanchard et~al.(2017)Blanchard, Mhamdi, Guerraoui, and
  Stainer]{blanchard2017byzantine}
P.~Blanchard, E.~M.~E. Mhamdi, R.~Guerraoui, and J.~Stainer.
\newblock Byzantine-tolerant machine learning.
\newblock \emph{arXiv preprint arXiv:1703.02757}, 2017.

\bibitem[Bubeck et~al.(2013)Bubeck, Cesa-Bianchi, and
  Lugosi]{bubeck2013bandits}
S.~Bubeck, N.~Cesa-Bianchi, and G.~Lugosi.
\newblock Bandits with heavy tail.
\newblock \emph{IEEE Transactions on Information Theory}, 59\penalty0
  (11):\penalty0 7711--7717, 2013.

\bibitem[Bubeck et~al.(2015)]{bubeck2015convex}
S.~Bubeck et~al.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  8\penalty0 (3-4):\penalty0 231--357, 2015.

\bibitem[Charikar et~al.(2017)Charikar, Steinhardt, and
  Valiant]{charikar2017learning}
M.~Charikar, J.~Steinhardt, and G.~Valiant.
\newblock Learning from untrusted data.
\newblock In \emph{Proceedings of the 49th Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 47--60. ACM, 2017.

\bibitem[Chen et~al.(2015)Chen, Gao, and Ren]{chen2015robust}
M.~Chen, C.~Gao, and Z.~Ren.
\newblock Robust covariance matrix estimation via matrix depth.
\newblock \emph{arXiv preprint arXiv:1506.00691}, 2015.

\bibitem[Chen et~al.(2017)Chen, Su, and Xu]{chen2017distributed}
Y.~Chen, L.~Su, and J.~Xu.
\newblock Distributed statistical machine learning in adversarial settings:
  Byzantine gradient descent.
\newblock \emph{arXiv preprint arXiv:1705.05491}, 2017.

\bibitem[Diakonikolas et~al.(2016)Diakonikolas, Kamath, Kane, Li, Moitra, and
  Stewart]{diakonikolas2016robust}
I.~Diakonikolas, G.~Kamath, D.~M. Kane, J.~Li, A.~Moitra, and A.~Stewart.
\newblock Robust estimators in high dimensions without the computational
  intractability.
\newblock In \emph{Foundations of Computer Science (FOCS), 2016 IEEE 57th
  Annual Symposium on}, pages 655--664. IEEE, 2016.

\bibitem[Esseen(1942)]{esseen1942liapounoff}
C.-G. Esseen.
\newblock \emph{On the Liapounoff limit of error in the theory of probability}.
\newblock Almqvist \& Wiksell, 1942.

\bibitem[Feng et~al.(2014)Feng, Xu, and Mannor]{feng2014distributed}
J.~Feng, H.~Xu, and S.~Mannor.
\newblock Distributed robust learning.
\newblock \emph{arXiv preprint arXiv:1409.5937}, 2014.

\bibitem[Hsu and Sabato(2016)]{hsu2016loss}
D.~Hsu and S.~Sabato.
\newblock Loss minimization and parameter estimation with heavy tails.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 543--582, 2016.

\bibitem[Huber(2011)]{huber2011robust}
P.~J. Huber.
\newblock Robust statistics.
\newblock In \emph{International Encyclopedia of Statistical Science}, pages
  1248--1251. Springer, 2011.

\bibitem[Jerrum et~al.(1986)Jerrum, Valiant, and Vazirani]{jerrum1986random}
M.~R. Jerrum, L.~G. Valiant, and V.~V. Vazirani.
\newblock Random generation of combinatorial structures from a uniform
  distribution.
\newblock \emph{Theoretical Computer Science}, 43:\penalty0 169--188, 1986.

\bibitem[Kogler and Traxler(2016)]{kogler2016efficient}
A.~Kogler and P.~Traxler.
\newblock Efficient and robust median-of-means algorithms for location and
  regression.
\newblock In \emph{Symbolic and Numeric Algorithms for Scientific Computing
  (SYNASC), 2016 18th International Symposium on}, pages 206--213. IEEE, 2016.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, McMahan, Yu,
  Richt{\'a}rik, Suresh, and Bacon]{konevcny2016federatedcommunication}
J.~Kone{\v{c}}n{\`y}, H.~B. McMahan, F.~X. Yu, P.~Richt{\'a}rik, A.~T. Suresh,
  and D.~Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock \emph{arXiv preprint arXiv:1610.05492}, 2016.

\bibitem[Lai et~al.(2016)Lai, Rao, and Vempala]{lai2016agnostic}
K.~A. Lai, A.~B. Rao, and S.~Vempala.
\newblock Agnostic estimation of mean and covariance.
\newblock In \emph{Foundations of Computer Science (FOCS), 2016 IEEE 57th
  Annual Symposium on}, pages 665--674. IEEE, 2016.

\bibitem[Lamport et~al.(1982)Lamport, Shostak, and Pease]{lamport1982byzantine}
L.~Lamport, R.~Shostak, and M.~Pease.
\newblock The byzantine generals problem.
\newblock \emph{ACM Transactions on Programming Languages and Systems
  (TOPLAS)}, 4\penalty0 (3):\penalty0 382--401, 1982.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lee et~al.(2015)Lee, Lin, Ma, and Yang]{lee2015distributed}
J.~D. Lee, Q.~Lin, T.~Ma, and T.~Yang.
\newblock Distributed stochastic variance reduced gradient methods and a lower
  bound for communication complexity.
\newblock \emph{arXiv preprint arXiv:1507.07595}, 2015.

\bibitem[Lerasle and Oliveira(2011)]{lerasle2011robust}
M.~Lerasle and R.~I. Oliveira.
\newblock Robust empirical mean estimators.
\newblock \emph{arXiv preprint arXiv:1112.3914}, 2011.

\bibitem[Lugosi and Mendelson(2016)]{lugosi2016risk}
G.~Lugosi and S.~Mendelson.
\newblock Risk minimization by median-of-means tournaments.
\newblock \emph{arXiv preprint arXiv:1608.00757}, 2016.

\bibitem[Lugosi and Mendelson(2017)]{lugosi2017sub}
G.~Lugosi and S.~Mendelson.
\newblock Sub-gaussian estimators of the mean of a random vector.
\newblock \emph{arXiv preprint arXiv:1702.00482}, 2017.

\bibitem[McMahan and Ramage(2017)]{mcmahan2017federated}
B.~McMahan and D.~Ramage.
\newblock Federated learning: Collaborative machine learning without
  centralized training data.
\newblock
  \url{https://research.googleblog.com/2017/04/federated-learning-collaborative.html},
  2017.

\bibitem[McMahan et~al.(2016)McMahan, Moore, Ramage, Hampson,
  et~al.]{mcmahan2016communication}
H.~B. McMahan, E.~Moore, D.~Ramage, S.~Hampson, et~al.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock \emph{arXiv preprint arXiv:1602.05629}, 2016.

\bibitem[Minsker and Strawn(2017)]{minsker2017distributed}
S.~Minsker and N.~Strawn.
\newblock Distributed statistical estimation and rates of convergence in normal
  approximation.
\newblock \emph{arXiv preprint arXiv:1704.02658}, 2017.

\bibitem[Minsker et~al.(2015)]{minsker2015geometric}
S.~Minsker et~al.
\newblock Geometric median and robust estimation in banach spaces.
\newblock \emph{Bernoulli}, 21\penalty0 (4):\penalty0 2308--2335, 2015.

\bibitem[Nemirovskii et~al.(1983)Nemirovskii, Yudin, and
  Dawson]{nemirovskii1983problem}
A.~Nemirovskii, D.~B. Yudin, and E.~R. Dawson.
\newblock Problem complexity and method efficiency in optimization.
\newblock 1983.

\bibitem[Pinelis and Molzon(2016)]{pinelis2016optimal}
I.~Pinelis and R.~Molzon.
\newblock Optimal-order bounds on the rate of convergence to normality in the
  multivariate delta method.
\newblock \emph{Electronic Journal of Statistics}, 10\penalty0 (1):\penalty0
  1001--1063, 2016.

\bibitem[Reddi et~al.(2016)Reddi, Kone{\v{c}}n{\`y}, Richt{\'a}rik,
  P{\'o}cz{\'o}s, and Smola]{reddi2016aide}
S.~J. Reddi, J.~Kone{\v{c}}n{\`y}, P.~Richt{\'a}rik, B.~P{\'o}cz{\'o}s, and
  A.~Smola.
\newblock Aide: Fast and communication efficient distributed optimization.
\newblock \emph{arXiv preprint arXiv:1608.06879}, 2016.

\bibitem[Rosenblatt and Nadler(2016)]{rosenblatt2016optimality}
J.~D. Rosenblatt and B.~Nadler.
\newblock On the optimality of averaging in distributed statistical learning.
\newblock \emph{Information and Inference: A Journal of the IMA}, 5\penalty0
  (4):\penalty0 379--404, 2016.

\bibitem[Shamir et~al.(2014)Shamir, Srebro, and Zhang]{shamir2014communication}
O.~Shamir, N.~Srebro, and T.~Zhang.
\newblock Communication-efficient distributed optimization using an approximate
  newton-type method.
\newblock In \emph{International conference on machine learning}, pages
  1000--1008, 2014.

\bibitem[Shevtsova(2014)]{shevtsova2014absolute}
I.~Shevtsova.
\newblock On the absolute constants in the berry-esseen-type inequalities.
\newblock In \emph{Doklady Mathematics}, volume~89, pages 378--381. Springer,
  2014.

\bibitem[Su and Vaidya(2016{\natexlab{a}})]{su2016fault}
L.~Su and N.~H. Vaidya.
\newblock Fault-tolerant multi-agent optimization: optimal iterative
  distributed algorithms.
\newblock In \emph{Proceedings of the 2016 ACM Symposium on Principles of
  Distributed Computing}, pages 425--434. ACM, 2016{\natexlab{a}}.

\bibitem[Su and Vaidya(2016{\natexlab{b}})]{su2016non}
L.~Su and N.~H. Vaidya.
\newblock Non-bayesian learning in the presence of byzantine agents.
\newblock In \emph{International Symposium on Distributed Computing}, pages
  414--427. Springer, 2016{\natexlab{b}}.

\bibitem[Vershynin(2010)]{vershynin2010introduction}
R.~Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \emph{arXiv preprint arXiv:1011.3027}, 2010.

\bibitem[Wang et~al.(2017)Wang, Roosta-Khorasani, Xu, and
  Mahoney]{wang2017giant}
S.~Wang, F.~Roosta-Khorasani, P.~Xu, and M.~W. Mahoney.
\newblock Giant: Globally improved approximate newton method for distributed
  optimization.
\newblock \emph{arXiv preprint arXiv:1709.03528}, 2017.

\bibitem[Wu(2017)]{wu2017lecture}
Y.~Wu.
\newblock Lecture notes for ece598yw: Information-theoretic methods for
  high-dimensional statistics.
\newblock \url{http://www.stat.yale.edu/~yw562/teaching/it-stats.pdf}, 2017.

\bibitem[Yin et~al.(2017)Yin, Pananjady, Lam, Papailiopoulos, Ramchandran, and
  Bartlett]{yin2017gradient}
D.~Yin, A.~Pananjady, M.~Lam, D.~Papailiopoulos, K.~Ramchandran, and
  P.~Bartlett.
\newblock Gradient diversity: a key ingredient for scalable distributed
  learning.
\newblock \emph{arXiv preprint arXiv:1706.05699}, 2017.

\bibitem[Zhang and Lin(2015)]{zhang2015disco}
Y.~Zhang and X.~Lin.
\newblock Disco: Distributed optimization for self-concordant empirical loss.
\newblock In \emph{International conference on machine learning}, pages
  362--370, 2015.

\bibitem[Zhang et~al.(2012)Zhang, Wainwright, and
  Duchi]{zhang2012communication}
Y.~Zhang, M.~J. Wainwright, and J.~C. Duchi.
\newblock Communication-efficient algorithms for statistical optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1502--1510, 2012.

\bibitem[Zhang et~al.(2015)Zhang, Duchi, and Wainwright]{zhang2015divide}
Y.~Zhang, J.~Duchi, and M.~Wainwright.
\newblock Divide and conquer kernel ridge regression: A distributed algorithm
  with minimax optimal rates.
\newblock \emph{The Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 3299--3340, 2015.

\end{thebibliography}
