\begin{thebibliography}{147}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aghakhani et~al.(2020)Aghakhani, Meng, Wang, Kruegel, and
  Vigna]{aghakhani2020bullseye}
Hojjat Aghakhani, Dongyu Meng, Yu-Xiang Wang, Christopher Kruegel, and Giovanni
  Vigna.
\newblock Bullseye polytope: A scalable clean-label poisoning attack with
  improved transferability.
\newblock \emph{arXiv preprint arXiv:2005.00191}, 2020.

\bibitem[Anonymous(2022)]{anonymous2022adversarially}
Anonymous.
\newblock Adversarially robust models may not transfer better: Sufficient
  conditions for domain transferability from the view of regularization.
\newblock In \emph{Submitted to The Tenth ICLR}, 2022.
\newblock URL \url{https://openreview.net/forum?id=_ixHFNR-FZ}.
\newblock under review.

\bibitem[Athalye et~al.(2018)Athalye, Carlini, and
  Wagner]{athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock In \emph{ICML}, 2018.

\bibitem[Bai et~al.(2021)Bai, Yan, Jiang, Xia, and Wang]{bai2021clustering}
Yang Bai, Xin Yan, Yong Jiang, Shu-Tao Xia, and Yisen Wang.
\newblock Clustering effect of adversarial robust models.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Barreno et~al.(2006)Barreno, Nelson, Sears, Joseph, and
  Tygar]{barreno2006can}
Marco Barreno, Blaine Nelson, Russell Sears, Anthony~D Joseph, and J~Doug
  Tygar.
\newblock Can machine learning be secure?
\newblock In \emph{Proceedings of the 2006 ACM Symposium on Information,
  computer and communications security}, 2006.

\bibitem[Barreno et~al.(2010)Barreno, Nelson, Joseph, and
  Tygar]{barreno2010security}
Marco Barreno, Blaine Nelson, Anthony~D Joseph, and J~Doug Tygar.
\newblock The security of machine learning.
\newblock \emph{Machine Learning}, 81\penalty0 (2):\penalty0 121--148, 2010.

\bibitem[Biggio and Roli(2018)]{biggio2018wild}
Battista Biggio and Fabio Roli.
\newblock Wild patterns: Ten years after the rise of adversarial machine
  learning.
\newblock \emph{Pattern Recognition}, 84:\penalty0 317--331, 2018.

\bibitem[Biggio et~al.(2011)Biggio, Nelson, and Laskov]{biggio2011support}
Battista Biggio, Blaine Nelson, and Pavel Laskov.
\newblock Support vector machines under adversarial label noise.
\newblock In \emph{ACML}, 2011.

\bibitem[Biggio et~al.(2012)Biggio, Nelson, and Laskov]{biggio2012poisoning}
Battista Biggio, Blaine Nelson, and Pavel Laskov.
\newblock Poisoning attacks against support vector machines.
\newblock In \emph{ICML}, 2012.

\bibitem[Biggio et~al.(2013)Biggio, Corona, Maiorca, Nelson, {\v{S}}rndi{\'c},
  Laskov, Giacinto, and Roli]{biggio2013evasion}
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim
  {\v{S}}rndi{\'c}, Pavel Laskov, Giorgio Giacinto, and Fabio Roli.
\newblock Evasion attacks against machine learning at test time.
\newblock In \emph{ECML-KDD}, 2013.

\bibitem[Blum et~al.(2021)Blum, Hanneke, Qian, and Shao]{pmlr-v134-blum21a}
Avrim Blum, Steve Hanneke, Jian Qian, and Han Shao.
\newblock Robust learning under clean-label attack.
\newblock In \emph{COLT}, 2021.

\bibitem[Borgnia et~al.(2021)Borgnia, Cherepanova, Fowl, Ghiasi, Geiping,
  Goldblum, Goldstein, and Gupta]{borgnia2021strong}
Eitan Borgnia, Valeriia Cherepanova, Liam Fowl, Amin Ghiasi, Jonas Geiping,
  Micah Goldblum, Tom Goldstein, and Arjun Gupta.
\newblock Strong data augmentation sanitizes poisoning and backdoor attacks
  without an accuracy tradeoff.
\newblock In \emph{ICASSP}, 2021.

\bibitem[Cai et~al.(2018)Cai, Liu, and Song]{cai2018curriculum}
Qi-Zhi Cai, Chang Liu, and Dawn Song.
\newblock Curriculum adversarial training.
\newblock In \emph{IJCAI}, 2018.

\bibitem[Carlini(2021)]{274598}
Nicholas Carlini.
\newblock Poisoning the unlabeled dataset of semi-supervised learning.
\newblock In \emph{{USENIX} Security Symposium}, 2021.

\bibitem[Carlini and Terzis(2021)]{carlini2021poisoning}
Nicholas Carlini and Andreas Terzis.
\newblock Poisoning and backdooring contrastive learning.
\newblock \emph{arXiv preprint arXiv:2106.09667}, 2021.

\bibitem[Chen et~al.(2018)Chen, Carvalho, Baracaldo, Ludwig, Edwards, Lee,
  Molloy, and Srivastava]{chen2018detecting}
Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin
  Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava.
\newblock Detecting backdoor attacks on deep neural networks by activation
  clustering.
\newblock \emph{arXiv preprint arXiv:1811.03728}, 2018.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{ICML}, 2020.

\bibitem[Chen et~al.(2017)Chen, Liu, Li, Lu, and Song]{chen2017targeted}
Xinyun Chen, Chang Liu, Bo~Li, Kimberly Lu, and Dawn Song.
\newblock Targeted backdoor attacks on deep learning systems using data
  poisoning.
\newblock \emph{arXiv preprint arXiv:1712.05526}, 2017.

\bibitem[Chen et~al.(2021)Chen, Wang, Bender, Ding, Jia, Li, and
  Song]{Chen2021REFIT}
Xinyun Chen, Wenxiao Wang, Chris Bender, Yiming Ding, Ruoxi Jia, Bo~Li, and
  Dawn Song.
\newblock Refit: A unified watermark removal framework for deep learning
  systems with limited data.
\newblock In \emph{ACM Asia Conference on Computer and Communications
  Security}, 2021.

\bibitem[Cherepanova et~al.(2021)Cherepanova, Goldblum, Foley, Duan, Dickerson,
  Taylor, and Goldstein]{cherepanova2021lowkey}
Valeriia Cherepanova, Micah Goldblum, Harrison Foley, Shiyuan Duan, John~P
  Dickerson, Gavin Taylor, and Tom Goldstein.
\newblock Lowkey: Leveraging adversarial attacks to protect social media users
  from facial recognition.
\newblock In \emph{ICLR}, 2021.

\bibitem[Croce et~al.(2021)Croce, Andriushchenko, Sehwag, Debenedetti,
  Flammarion, Chiang, Mittal, and Hein]{croce2021robustbench}
Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti,
  Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein.
\newblock Robustbench: a standardized adversarial robustness benchmark.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems (NeurIPS) Datasets and Benchmarks Track}, 2021.
\newblock URL \url{https://openreview.net/forum?id=SSKZPJCt7B}.

\bibitem[Deng et~al.(2021)Deng, Zhang, Vodrahalli, Kawaguchi, and
  Zou]{deng2021adversarial}
Zhun Deng, Linjun Zhang, Kailas Vodrahalli, Kenji Kawaguchi, and James Zou.
\newblock Adversarial training helps transfer learning via better
  representations.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Diakonikolas et~al.(2019)Diakonikolas, Kamath, Kane, Li, Steinhardt,
  and Stewart]{diakonikolas2019sever}
Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and
  Alistair Stewart.
\newblock Sever: A robust meta-algorithm for stochastic optimization.
\newblock In \emph{ICML}, 2019.

\bibitem[Dietterich(2002)]{dietterich2002machine}
Thomas~G Dietterich.
\newblock Machine learning for sequential data: A review.
\newblock In \emph{Joint IAPR international workshops on statistical techniques
  in pattern recognition (SPR) and structural and syntactic pattern recognition
  (SSPR)}, 2002.

\bibitem[Dobriban et~al.(2020)Dobriban, Hassani, Hong, and
  Robey]{dobriban2020provable}
Edgar Dobriban, Hamed Hassani, David Hong, and Alexander Robey.
\newblock Provable tradeoffs in adversarially robust classification.
\newblock \emph{arXiv preprint arXiv:2006.05161}, 2020.

\bibitem[Dong et~al.(2020)Dong, Deng, Pang, Zhu, and Su]{dong2020adversarial}
Yinpeng Dong, Zhijie Deng, Tianyu Pang, Jun Zhu, and Hang Su.
\newblock Adversarial distributional training for robust deep learning.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Dong et~al.(2021{\natexlab{a}})Dong, Xu, Yang, Pang, Deng, Su, and
  Zhu]{dong2021exploring}
Yinpeng Dong, Ke~Xu, Xiao Yang, Tianyu Pang, Zhijie Deng, Hang Su, and Jun Zhu.
\newblock Exploring memorization in adversarial training.
\newblock \emph{arXiv preprint arXiv:2106.01606}, 2021{\natexlab{a}}.

\bibitem[Dong et~al.(2021{\natexlab{b}})Dong, Yang, Deng, Pang, Xiao, Su, and
  Zhu]{dong2021black}
Yinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang Su, and Jun
  Zhu.
\newblock Black-box detection of backdoor attacks with limited information and
  data.
\newblock In \emph{ICCV}, 2021{\natexlab{b}}.

\bibitem[Du et~al.(2021)Du, Zhang, Han, Liu, Rong, Niu, Huang, and
  Sugiyama]{du2021learning}
Xuefeng Du, Jingfeng Zhang, Bo~Han, Tongliang Liu, Yu~Rong, Gang Niu, Junzhou
  Huang, and Masashi Sugiyama.
\newblock Learning diverse-structured networks for adversarial robustness.
\newblock In \emph{ICML}, 2021.

\bibitem[Evtimov et~al.(2021)Evtimov, Covert, Kusupati, and
  Kohno]{evtimov2021disrupting}
Ivan Evtimov, Ian Covert, Aditya Kusupati, and Tadayoshi Kohno.
\newblock Disrupting model training with adversarial shortcuts.
\newblock In \emph{ICML Workshop}, 2021.

\bibitem[Farokhi(2020)]{farokhi2020regularization}
Farhad Farokhi.
\newblock Regularization helps with mitigating poisoning attacks:
  Distributionally-robust machine learning using the wasserstein distance.
\newblock \emph{arXiv preprint arXiv:2001.10655}, 2020.

\bibitem[Feng et~al.(2019)Feng, Cai, and Zhou]{feng2019learning}
Ji~Feng, Qi-Zhi Cai, and Zhi-Hua Zhou.
\newblock Learning to confuse: generating training time adversarial data with
  auto-encoder.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Fowl et~al.(2021{\natexlab{a}})Fowl, Chiang, Goldblum, Geiping,
  Bansal, Czaja, and Goldstein]{fowl2021preventing}
Liam Fowl, Ping-yeh Chiang, Micah Goldblum, Jonas Geiping, Arpit Bansal, Wojtek
  Czaja, and Tom Goldstein.
\newblock Preventing unauthorized use of proprietary data: Poisoning for secure
  dataset release.
\newblock \emph{arXiv preprint arXiv:2103.02683}, 2021{\natexlab{a}}.

\bibitem[Fowl et~al.(2021{\natexlab{b}})Fowl, Goldblum, Chiang, Geiping, Czaja,
  and Goldstein]{fowl2021adversarial}
Liam Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geiping, Wojtek Czaja, and
  Tom Goldstein.
\newblock Adversarial examples make strong poisons.
\newblock In \emph{NeurIPS}, 2021{\natexlab{b}}.

\bibitem[Franci et~al.(2020)Franci, Cordy, Gubri, Papadakis, and
  Traon]{franci2020effective}
Adriano Franci, Maxime Cordy, Martin Gubri, Mike Papadakis, and Yves~Le Traon.
\newblock Effective and efficient data poisoning in semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:2012.07381}, 2020.

\bibitem[Gao et~al.(2021{\natexlab{a}})Gao, Karbasi, and
  Mahmoody]{gao2021learning}
Ji~Gao, Amin Karbasi, and Mohammad Mahmoody.
\newblock Learning and certification under instance-targeted poisoning.
\newblock In \emph{UAI}, 2021{\natexlab{a}}.

\bibitem[Gao et~al.(2021{\natexlab{b}})Gao, Liu, Zhang, Han, Liu, Niu, and
  Sugiyama]{gao2021maximum}
Ruize Gao, Feng Liu, Jingfeng Zhang, Bo~Han, Tongliang Liu, Gang Niu, and
  Masashi Sugiyama.
\newblock Maximum mean discrepancy test is aware of adversarial attacks.
\newblock In \emph{ICML}, 2021{\natexlab{b}}.

\bibitem[Gao et~al.(2019)Gao, Xu, Wang, Chen, Ranasinghe, and
  Nepal]{gao2019strip}
Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith~C Ranasinghe, and
  Surya Nepal.
\newblock Strip: A defence against trojan attacks on deep neural networks.
\newblock In \emph{Annual Computer Security Applications Conference}, 2019.

\bibitem[Geiping et~al.(2020)Geiping, Fowl, Huang, Czaja, Taylor, Moeller, and
  Goldstein]{geiping2020witches}
Jonas Geiping, Liam Fowl, W~Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael
  Moeller, and Tom Goldstein.
\newblock Witches' brew: Industrial scale data poisoning via gradient matching.
\newblock \emph{arXiv preprint arXiv:2009.02276}, 2020.

\bibitem[Geiping et~al.(2021)Geiping, Fowl, Somepalli, Goldblum, Moeller, and
  Goldstein]{geiping2021doesn}
Jonas Geiping, Liam Fowl, Gowthami Somepalli, Micah Goldblum, Michael Moeller,
  and Tom Goldstein.
\newblock What doesn't kill you makes you robust (er): Adversarial training
  against poisons and backdoors.
\newblock \emph{arXiv preprint arXiv:2102.13624}, 2021.

\bibitem[Gidaris et~al.(2018)Gidaris, Singh, and
  Komodakis]{gidaris2018unsupervised}
Spyros Gidaris, Praveer Singh, and Nikos Komodakis.
\newblock Unsupervised representation learning by predicting image rotations.
\newblock In \emph{ICLR}, 2018.

\bibitem[Globerson and Roweis(2006)]{globerson2006nightmare}
Amir Globerson and Sam Roweis.
\newblock Nightmare at test time: robust learning by feature deletion.
\newblock In \emph{ICML}, 2006.

\bibitem[Goldblum et~al.(2020)Goldblum, Tsipras, Xie, Chen, Schwarzschild,
  Song, Madry, Li, and Goldstein]{goldblum2020dataset}
Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild,
  Dawn Song, Aleksander Madry, Bo~Li, and Tom Goldstein.
\newblock Dataset security for machine learning: Data poisoning, backdoor
  attacks, and defenses.
\newblock \emph{arXiv preprint arXiv:2012.10544}, 2020.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In \emph{ICLR}, 2015.

\bibitem[Gowal et~al.(2021)Gowal, Rebuffi, Wiles, Stimberg, Calian, and
  Mann]{gowal2021improving}
Sven Gowal, Sylvestre-Alvise Rebuffi, Olivia Wiles, Florian Stimberg,
  Dan~Andrei Calian, and Timothy Mann.
\newblock Improving robustness using generated data.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Goyal et~al.(2020)Goyal, Raghunathan, Jain, Simhadri, and
  Jain]{goyal2020drocc}
Sachin Goyal, Aditi Raghunathan, Moksh Jain, Harsha~Vardhan Simhadri, and
  Prateek Jain.
\newblock Drocc: Deep robust one-class classification.
\newblock In \emph{ICML}, 2020.

\bibitem[Gu et~al.(2019)Gu, Liu, Dolan-Gavitt, and Garg]{gu2019badnets}
Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Badnets: Evaluating backdooring attacks on deep neural networks.
\newblock \emph{IEEE Access}, 7:\penalty0 47230--47244, 2019.

\bibitem[Hayase et~al.(2021)Hayase, Kong, Somani, and Oh]{pmlr-v139-hayase21a}
Jonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh.
\newblock Spectre: defending against backdoor attacks using robust statistics.
\newblock In \emph{ICML}, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Hermann and Lampinen(2020)]{hermann2020shapes}
Katherine~L Hermann and Andrew~K Lampinen.
\newblock What shapes feature representations? exploring datasets,
  architectures, and training.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Hong et~al.(2020)Hong, Chandrasekaran, Kaya, Dumitra{\c{s}}, and
  Papernot]{hong2020effectiveness}
Sanghyun Hong, Varun Chandrasekaran, Yi{\u{g}}itcan Kaya, Tudor Dumitra{\c{s}},
  and Nicolas Papernot.
\newblock On the effectiveness of mitigating data poisoning attacks with
  gradient shaping.
\newblock \emph{arXiv preprint arXiv:2002.11497}, 2020.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{CVPR}, 2017.

\bibitem[Huang et~al.(2021{\natexlab{a}})Huang, Ma, Erfani, Bailey, and
  Wang]{huang2021unlearnable}
Hanxun Huang, Xingjun Ma, Sarah~Monazam Erfani, James Bailey, and Yisen Wang.
\newblock Unlearnable examples: Making personal data unexploitable.
\newblock In \emph{ICLR}, 2021{\natexlab{a}}.

\bibitem[Huang et~al.(2021{\natexlab{b}})Huang, Wang, Erfani, Gu, Bailey, and
  Ma]{huang2021exploring}
Hanxun Huang, Yisen Wang, Sarah~Monazam Erfani, Quanquan Gu, James Bailey, and
  Xingjun Ma.
\newblock Exploring architectural ingredients of adversarially robust deep
  neural networks.
\newblock In \emph{NeurIPS}, 2021{\natexlab{b}}.

\bibitem[Huang et~al.(2020)Huang, Geiping, Fowl, Taylor, and
  Goldstein]{huang2020metapoison}
W~Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, and Tom Goldstein.
\newblock Metapoison: Practical general-purpose clean-label data poisoning.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Ilyas et~al.(2019)Ilyas, Santurkar, Tsipras, Engstrom, Tran, and
  Madry]{ilyas2019adversarial}
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon
  Tran, and Aleksander Madry.
\newblock Adversarial examples are not bugs, they are features.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Jetley et~al.(2018)Jetley, Lord, and Torr]{jetley2018friends}
Saumya Jetley, Nicholas Lord, and Philip Torr.
\newblock With friends like these, who needs adversaries?
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Jing and Tian(2020)]{jing2020self}
Longlong Jing and Yingli Tian.
\newblock Self-supervised visual feature learning with deep neural networks: A
  survey.
\newblock \emph{TPAMI}, 2020.

\bibitem[Kandel et~al.(2011)Kandel, Paepcke, Hellerstein, and
  Heer]{kandel2011wrangler}
Sean Kandel, Andreas Paepcke, Joseph Hellerstein, and Jeffrey Heer.
\newblock Wrangler: Interactive visual specification of data transformation
  scripts.
\newblock In \emph{CHI Conference on Human Factors in Computing Systems (CHI)},
  2011.

\bibitem[Kireev et~al.(2021)Kireev, Andriushchenko, and
  Flammarion]{kireev2021effectiveness}
Klim Kireev, Maksym Andriushchenko, and Nicolas Flammarion.
\newblock On the effectiveness of adversarial training against common
  corruptions.
\newblock \emph{ICLR RobustML Workshop}, 2021.

\bibitem[Koh and Liang(2017)]{koh2017understanding}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{ICML}, 2017.

\bibitem[Koh et~al.(2018)Koh, Steinhardt, and Liang]{koh2018stronger}
Pang~Wei Koh, Jacob Steinhardt, and Percy Liang.
\newblock Stronger data poisoning attacks break data sanitization defenses.
\newblock \emph{arXiv preprint arXiv:1811.00741}, 2018.

\bibitem[Krizhevsky et~al.(2009)]{krizhevsky2009learning}
Alex Krizhevsky et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kumar et~al.(2020)Kumar, Nystr{\"o}m, Lambert, Marshall, Goertzel,
  Comissoneru, Swann, and Xia]{kumar2020adversarial}
Ram Shankar~Siva Kumar, Magnus Nystr{\"o}m, John Lambert, Andrew Marshall,
  Mario Goertzel, Andi Comissoneru, Matt Swann, and Sharon Xia.
\newblock Adversarial machine learning-industry perspectives.
\newblock In \emph{IEEE Security and Privacy Workshops (SPW)}, 2020.

\bibitem[Lechner et~al.(2021)Lechner, Hasani, Grosu, Rus, and
  Henzinger]{lechner2021adversarial}
Mathias Lechner, Ramin Hasani, Radu Grosu, Daniela Rus, and Thomas~A Henzinger.
\newblock Adversarial training is not ready for robot learning.
\newblock In \emph{ICRA}, 2021.

\bibitem[Levine and Feizi(2021)]{levine2021deep}
Alexander Levine and Soheil Feizi.
\newblock Deep partition aggregation: Provable defenses against general
  poisoning attacks.
\newblock In \emph{ICLR}, 2021.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Lyu, Koren, Lyu, Li, and
  Ma]{li2021anti}
Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo~Li, and Xingjun Ma.
\newblock Anti-backdoor learning: Training clean models on poisoned data.
\newblock In \emph{NeurIPS}, 2021{\natexlab{a}}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Lyu, Koren, Lyu, Li, and
  Ma]{li2021neural}
Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo~Li, and Xingjun Ma.
\newblock Neural attention distillation: Erasing backdoor triggers from deep
  neural networks.
\newblock In \emph{ICLR}, 2021{\natexlab{b}}.

\bibitem[Liang et~al.(2021)Liang, Zhang, Wang, Yang, Koyejo, and
  Li]{pmlr-v139-liang21b}
Kaizhao Liang, Jacky~Y Zhang, Boxin Wang, Zhuolin Yang, Sanmi Koyejo, and
  Bo~Li.
\newblock Uncovering the connections between adversarial transferability and
  knowledge transferability.
\newblock In \emph{ICML}, 2021.

\bibitem[Liu et~al.(2019)Liu, Si, Zhu, Li, and Hsieh]{liu2019unified}
Xuanqing Liu, Si~Si, Xiaojin Zhu, Yang Li, and Cho-Jui Hsieh.
\newblock A unified framework for data poisoning attack to graph-based
  semi-supervised learning.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Liu et~al.(2018)Liu, Ma, Aafer, Lee, Zhai, Wang, and Zhang]{Trojannn}
Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang,
  and Xiangyu Zhang.
\newblock Trojaning attack on neural networks.
\newblock In \emph{Annual Network and Distributed System Security Symposium
  (NDSS)}, 2018.

\bibitem[Liu et~al.(2020)Liu, Ma, Bailey, and Lu]{liu2020reflection}
Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu.
\newblock Reflection backdoor: A natural backdoor attack on deep neural
  networks.
\newblock In \emph{ECCV}, 2020.

\bibitem[Ma et~al.(2019)Ma, Zhu, and Hsu]{ma2019data}
Yuzhe Ma, Xiaojin~Zhu Zhu, and Justin Hsu.
\newblock Data poisoning against differentially-private learners: Attacks and
  defenses.
\newblock In \emph{IJCAI}, 2019.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2018towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Mehra et~al.(2021)Mehra, Kailkhura, Chen, and
  Hamm]{mehra2021effectiveness}
Akshay Mehra, Bhavya Kailkhura, Pin-Yu Chen, and Jihun Hamm.
\newblock Understanding the limits of unsupervised domain adaptation via data
  poisoning.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Mei and Zhu(2015)]{mei2015using}
Shike Mei and Xiaojin Zhu.
\newblock Using machine teaching to identify optimal training-set attacks on
  machine learners.
\newblock In \emph{AAAI Conference on Artificial Intelligence (AAAI)}, 2015.

\bibitem[Moosavi-Dezfooli et~al.(2017)Moosavi-Dezfooli, Fawzi, Fawzi, and
  Frossard]{moosavi2017universal}
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
  Frossard.
\newblock Universal adversarial perturbations.
\newblock In \emph{CVPR}, 2017.

\bibitem[Mu{\~n}oz-Gonz{\'a}lez et~al.(2017)Mu{\~n}oz-Gonz{\'a}lez, Biggio,
  Demontis, Paudice, Wongrassamee, Lupu, and Roli]{munoz2017towards}
Luis Mu{\~n}oz-Gonz{\'a}lez, Battista Biggio, Ambra Demontis, Andrea Paudice,
  Vasin Wongrassamee, Emil~C Lupu, and Fabio Roli.
\newblock Towards poisoning of deep learning algorithms with back-gradient
  optimization.
\newblock In \emph{ACM Workshop on Artificial Intelligence and Security}, 2017.

\bibitem[Nakkiran(2019)]{nakkiran2019a}
Preetum Nakkiran.
\newblock A discussion of 'adversarial examples are not bugs, they are
  features': Adversarial examples are just bugs, too.
\newblock \emph{Distill}, 2019.
\newblock \doi{10.23915/distill.00019.5}.
\newblock https://distill.pub/2019/advex-bugs-discussion/response-5.

\bibitem[Nelson et~al.(2008)Nelson, Barreno, Chi, Joseph, Rubinstein, Saini,
  Sutton, Tygar, and Xia]{nelson2008exploiting}
Blaine Nelson, Marco Barreno, Fuching~Jack Chi, Anthony~D Joseph, Benjamin~IP
  Rubinstein, Udam Saini, Charles Sutton, JD~Tygar, and Kai Xia.
\newblock Exploiting machine learning to subvert your spam filter.
\newblock In \emph{Usenix Workshop on Large-Scale Exploits and Emergent
  Threats}, 2008.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Newsome et~al.(2006)Newsome, Karp, and Song]{newsome2006paragraph}
James Newsome, Brad Karp, and Dawn Song.
\newblock Paragraph: Thwarting signature learning by training maliciously.
\newblock In \emph{International Workshop on Recent Advances in Intrusion
  Detection}, 2006.

\bibitem[Nguyen and Tran(2020)]{nguyen2020input}
Tuan~Anh Nguyen and Anh Tran.
\newblock Input-aware dynamic backdoor attack.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Nguyen and Tran(2021)]{nguyen2021wanet}
Tuan~Anh Nguyen and Anh~Tuan Tran.
\newblock Wanet - imperceptible warping-based backdoor attack.
\newblock In \emph{ICLR}, 2021.

\bibitem[Noack et~al.(2021)Noack, Ahern, Dou, and Li]{noack2021empirical}
Adam Noack, Isaac Ahern, Dejing Dou, and Boyang Li.
\newblock An empirical study on the relation between network interpretability
  and adversarial robustness.
\newblock \emph{SN Computer Science}, 2021.

\bibitem[Pang et~al.(2020)Pang, Shen, Zhang, Ji, Vorobeychik, Luo, Liu, and
  Wang]{pang2020tale}
Ren Pang, Hua Shen, Xinyang Zhang, Shouling Ji, Yevgeniy Vorobeychik, Xiapu
  Luo, Alex Liu, and Ting Wang.
\newblock A tale of evil twins: Adversarial inputs versus poisoned models.
\newblock In \emph{CCS}, 2020.

\bibitem[Pang et~al.(2021{\natexlab{a}})Pang, Yang, Dong, Su, and
  Zhu]{pang2020bag}
Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu.
\newblock Bag of tricks for adversarial training.
\newblock In \emph{ICLR}, 2021{\natexlab{a}}.

\bibitem[Pang et~al.(2021{\natexlab{b}})Pang, Yang, Dong, Su, and
  Zhu]{pang2021accumulative}
Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu.
\newblock Accumulative poisoning attacks on real-time data.
\newblock In \emph{NeurIPS}, 2021{\natexlab{b}}.

\bibitem[Papernot et~al.(2016)Papernot, McDaniel, Wu, Jha, and
  Swami]{papernot2016distillation}
Nicolas Papernot, Patrick McDaniel, Xi~Wu, Somesh Jha, and Ananthram Swami.
\newblock Distillation as a defense to adversarial perturbations against deep
  neural networks.
\newblock In \emph{IEEE symposium on security and privacy (SP)}, 2016.

\bibitem[Peri et~al.(2020)Peri, Gupta, Huang, Fowl, Zhu, Feizi, Goldstein, and
  Dickerson]{peri2020deep}
Neehar Peri, Neal Gupta, W.~Ronny Huang, Liam Fowl, Chen Zhu, Soheil Feizi, Tom
  Goldstein, and John~P. Dickerson.
\newblock Deep k-nn defense against clean-label data poisoning attacks.
\newblock In Adrien Bartoli and Andrea Fusiello, editors, \emph{ECCV Workshop},
  2020.

\bibitem[Pezeshki et~al.(2020)Pezeshki, Kaba, Bengio, Courville, Precup, and
  Lajoie]{pezeshki2020gradient}
Mohammad Pezeshki, S{\'e}kou-Oumar Kaba, Yoshua Bengio, Aaron Courville, Doina
  Precup, and Guillaume Lajoie.
\newblock Gradient starvation: A learning proclivity in neural networks.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Radiya-Dixit and Tramer(2021)]{radiya2021data}
Evani Radiya-Dixit and Florian Tramer.
\newblock Data poisoning won’t save you from facial recognition.
\newblock In \emph{ICML 2021 Workshop on Adversarial Machine Learning}, 2021.

\bibitem[Rice et~al.(2020)Rice, Wong, and Kolter]{rice2020overfitting}
Leslie Rice, Eric Wong, and Zico Kolter.
\newblock Overfitting in adversarially robust deep learning.
\newblock In \emph{ICML}, 2020.

\bibitem[Rosenfeld et~al.(2020)Rosenfeld, Winston, Ravikumar, and
  Kolter]{rosenfeld2020certified}
Elan Rosenfeld, Ezra Winston, Pradeep Ravikumar, and Zico Kolter.
\newblock Certified robustness to label-flipping attacks via randomized
  smoothing.
\newblock In \emph{ICML}, 2020.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{IJCV}, 2015.

\bibitem[Saha et~al.(2021)Saha, Tejankar, Koohpayegani, and
  Pirsiavash]{saha2021backdoor}
Aniruddha Saha, Ajinkya Tejankar, Soroush~Abbasi Koohpayegani, and Hamed
  Pirsiavash.
\newblock Backdoor attacks on self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2105.10123}, 2021.

\bibitem[Salehi et~al.(2021)Salehi, Arya, Pajoum, Otoofi, Shaeiri, Rohban, and
  Rabiee]{salehi2021arae}
Mohammadreza Salehi, Atrin Arya, Barbod Pajoum, Mohammad Otoofi, Amirreza
  Shaeiri, Mohammad~Hossein Rohban, and Hamid~R Rabiee.
\newblock Arae: Adversarially robust training of autoencoders improves novelty
  detection.
\newblock \emph{Neural Networks}, 2021.

\bibitem[Salman et~al.(2020)Salman, Ilyas, Engstrom, Kapoor, and
  Madry]{salman2020adversarially}
Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry.
\newblock Do adversarially robust imagenet models transfer better?
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Salman et~al.(2021)Salman, Ilyas, Engstrom, Vemprala, Madry, and
  Kapoor]{salman2021unadversarial}
Hadi Salman, Andrew Ilyas, Logan Engstrom, Sai Vemprala, Aleksander Madry, and
  Ashish Kapoor.
\newblock Unadversarial examples: Designing objects for robust vision.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Santurkar et~al.(2019)Santurkar, Ilyas, Tsipras, Engstrom, Tran, and
  Madry]{santurkar2019image}
Shibani Santurkar, Andrew Ilyas, Dimitris Tsipras, Logan Engstrom, Brandon
  Tran, and Aleksander Madry.
\newblock Image synthesis with a single (robust) classifier.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Schmidt et~al.(2018)Schmidt, Santurkar, Tsipras, Talwar, and
  Madry]{schmidt2018adversarially}
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and
  Aleksander Madry.
\newblock Adversarially robust generalization requires more data.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Schwarzschild et~al.(2021)Schwarzschild, Goldblum, Gupta, Dickerson,
  and Goldstein]{schwarzschild2021just}
Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John~P Dickerson, and Tom
  Goldstein.
\newblock Just how toxic is data poisoning? a unified benchmark for backdoor
  and data poisoning attacks.
\newblock In \emph{ICML}, 2021.

\bibitem[Shafahi et~al.(2018)Shafahi, Huang, Najibi, Suciu, Studer, Dumitras,
  and Goldstein]{shafahi2018poison}
Ali Shafahi, W~Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,
  Tudor Dumitras, and Tom Goldstein.
\newblock Poison frogs! targeted clean-label poisoning attacks on neural
  networks.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Shah et~al.(2020)Shah, Tamuly, Raghunathan, Jain, and
  Netrapalli]{shah2020pitfalls}
Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth
  Netrapalli.
\newblock The pitfalls of simplicity bias in neural networks.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Shan et~al.(2020)Shan, Wenger, Zhang, Li, Zheng, and
  Zhao]{shan2020fawkes}
Shawn Shan, Emily Wenger, Jiayun Zhang, Huiying Li, Haitao Zheng, and Ben~Y
  Zhao.
\newblock Fawkes: Protecting privacy against unauthorized deep learning models.
\newblock In \emph{{USENIX} Security Symposium}, 2020.

\bibitem[Shen et~al.(2019)Shen, Zhu, and Ma]{shen2019tensorclog}
Juncheng Shen, Xiaolei Zhu, and De~Ma.
\newblock Tensorclog: An imperceptible poisoning attack on deep neural network
  applications.
\newblock \emph{IEEE Access}, 7:\penalty0 41498--41506, 2019.

\bibitem[Shokri et~al.(2020)]{shokri2020bypassing}
Reza Shokri et~al.
\newblock Bypassing backdoor detection algorithms in deep learning.
\newblock In \emph{EuroS\&P}, 2020.

\bibitem[Simonyan and Zisserman(2015)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{ICLR}, 2015.

\bibitem[Sinha et~al.(2018)Sinha, Namkoong, and Duchi]{sinha2018certifiable}
Aman Sinha, Hongseok Namkoong, and John Duchi.
\newblock Certifiable distributional robustness with principled adversarial
  training.
\newblock In \emph{ICLR}, 2018.

\bibitem[Staib and Jegelka(2017)]{staib2017distributionally}
Matthew Staib and Stefanie Jegelka.
\newblock Distributionally robust deep learning as a generalization of
  adversarial training.
\newblock In \emph{NeurIPS workshop on Machine Learning and Computer Security},
  2017.

\bibitem[Steinhardt et~al.(2017)Steinhardt, Koh, and
  Liang]{steinhardt2017certified}
Jacob Steinhardt, Pang~Wei Koh, and Percy Liang.
\newblock Certified defenses for data poisoning attacks.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Su et~al.(2018)Su, Zhang, Chen, Yi, Chen, and Gao]{su2018robustness}
Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, and Yupeng Gao.
\newblock Is robustness the cost of accuracy?--a comprehensive study on the
  robustness of 18 deep image classification models.
\newblock In \emph{ECCV}, 2018.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock In \emph{ICLR}, 2014.

\bibitem[Tack et~al.(2021)Tack, Yu, Jeong, Kim, Hwang, and
  Shin]{tack2021consistency}
Jihoon Tack, Sihyun Yu, Jongheon Jeong, Minseon Kim, Sung~Ju Hwang, and Jinwoo
  Shin.
\newblock Consistency regularization for adversarial robustness.
\newblock In \emph{ICML 2021 Workshop on Adversarial Machine Learning}, 2021.

\bibitem[Tao et~al.(2022)Tao, Feng, Yi, and Chen]{tao2020false}
Lue Tao, Lei Feng, Jinfeng Yi, and Songcan Chen.
\newblock With false friends like these, who can notice mistakes?
\newblock In \emph{AAAI}, 2022.

\bibitem[Terzi et~al.(2021)Terzi, Achille, Maggipinto, and
  Susto]{terzi2021adversarial}
Matteo Terzi, Alessandro Achille, Marco Maggipinto, and Gian~Antonio Susto.
\newblock Adversarial training reduces information and improves
  transferability.
\newblock In \emph{AAAI}, 2021.

\bibitem[Tian et~al.(2021)Tian, Kuang, Jiang, Wu, and Wang]{tian2021analysis}
Qi~Tian, Kun Kuang, Kelu Jiang, Fei Wu, and Yisen Wang.
\newblock Analysis and applications of class-wise robustness in adversarial
  training.
\newblock In \emph{KDD}, 2021.

\bibitem[Tran et~al.(2018)Tran, Li, and Madry]{tran2018spectral}
Brandon Tran, Jerry Li, and Aleksander Madry.
\newblock Spectral signatures in backdoor attacks.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Tsipras et~al.(2019)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{tsipras2018robustness}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
  Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock In \emph{ICLR}, 2019.

\bibitem[Turner et~al.(2019)Turner, Tsipras, and Madry]{turner2019label}
Alexander Turner, Dimitris Tsipras, and Aleksander Madry.
\newblock Label-consistent backdoor attacks.
\newblock \emph{arXiv preprint arXiv:1912.02771}, 2019.

\bibitem[Utrera et~al.(2021)Utrera, Kravitz, Erichson, Khanna, and
  Mahoney]{utrera2021adversariallytrained}
Francisco Utrera, Evan Kravitz, N.~Benjamin Erichson, Rajiv Khanna, and
  Michael~W. Mahoney.
\newblock Adversarially-trained deep nets transfer better: Illustration on
  image classification.
\newblock In \emph{ICLR}, 2021.

\bibitem[Veldanda and Garg(2020)]{veldanda2020evaluating}
Akshaj Veldanda and Siddharth Garg.
\newblock On evaluating neural network backdoor defenses.
\newblock \emph{arXiv preprint arXiv:2010.12186}, 2020.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Liu, Han, Liu, Gong, Niu, Zhou,
  and Sugiyama]{wang2021probabilistic}
Qizhou Wang, Feng Liu, Bo~Han, Tongliang Liu, Chen Gong, Gang Niu, Mingyuan
  Zhou, and Masashi Sugiyama.
\newblock Probabilistic margins for instance reweighting in adversarial
  training.
\newblock In \emph{NeurIPS}, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2019)Wang, Ma, Bailey, Yi, Zhou, and
  Gu]{wang2019convergence}
Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu.
\newblock On the convergence and robustness of adversarial training.
\newblock In \emph{ICML}, 2019.

\bibitem[Wang et~al.(2020)Wang, Zou, Yi, Bailey, Ma, and Gu]{wang2019improving}
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu.
\newblock Improving adversarial robustness requires revisiting misclassified
  examples.
\newblock In \emph{ICLR}, 2020.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Mianjy, and
  Arora]{pmlr-v139-wang21r}
Yunjuan Wang, Poorya Mianjy, and Raman Arora.
\newblock Robust learning for data poisoning attacks.
\newblock In \emph{ICML}, 2021{\natexlab{b}}.

\bibitem[Weber et~al.(2020)Weber, Xu, Karla{\v{s}}, Zhang, and
  Li]{weber2020rab}
Maurice Weber, Xiaojun Xu, Bojan Karla{\v{s}}, Ce~Zhang, and Bo~Li.
\newblock Rab: Provable robustness against backdoor attacks.
\newblock \emph{arXiv preprint arXiv:2003.08904}, 2020.

\bibitem[Weng et~al.(2020)Weng, Lee, and Wu]{weng2020trade}
Cheng-Hsin Weng, Yan-Ting Lee, and Shan-Hung~Brandon Wu.
\newblock On the trade-off between adversarial and backdoor robustness.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Wong and Kolter(2018)]{wong2018provable}
Eric Wong and Zico Kolter.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock In \emph{ICML}, 2018.

\bibitem[Wu and Wang(2021)]{wu2021Adversarial}
Dongxian Wu and Yisen Wang.
\newblock Adversarial neuron pruning purifies backdoored deep models.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Wu et~al.(2020)Wu, Xia, and Wang]{wu2020adversarial}
Dongxian Wu, Shu-Tao Xia, and Yisen Wang.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Xiao et~al.(2015)Xiao, Biggio, Brown, Fumera, Eckert, and
  Roli]{xiao2015feature}
Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and
  Fabio Roli.
\newblock Is feature selection secure against training data poisoning?
\newblock In \emph{ICML}, 2015.

\bibitem[Xie et~al.(2020)Xie, Tan, Gong, Wang, Yuille, and
  Le]{xie2020adversarial}
Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan~L Yuille, and Quoc~V
  Le.
\newblock Adversarial examples improve image recognition.
\newblock In \emph{CVPR}, 2020.

\bibitem[Xu et~al.(2020)Xu, Liu, Li, and Tang]{xu2020robust}
Han Xu, Xiaorui Liu, Yaxin Li, and Jiliang Tang.
\newblock To be robust or to be fair: Towards fairness in adversarial training.
\newblock \emph{arXiv preprint arXiv:2010.06121}, 2020.

\bibitem[Yang et~al.(2020)Yang, Rashtchian, Zhang, Salakhutdinov, and
  Chaudhuri]{yang2020closer}
Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ~R Salakhutdinov, and
  Kamalika Chaudhuri.
\newblock A closer look at accuracy vs. robustness.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Yi et~al.(2021)Yi, Hou, Sun, Shang, Jiang, Liu, and
  Ma]{pmlr-v139-yi21a}
Mingyang Yi, Lu~Hou, Jiacheng Sun, Lifeng Shang, Xin Jiang, Qun Liu, and
  Zhiming Ma.
\newblock Improved ood generalization via adversarial training and pretraing.
\newblock In \emph{ICML}, 2021.

\bibitem[Yuan and Wu(2021)]{pmlr-v139-yuan21b}
Chia-Hung Yuan and Shan-Hung Wu.
\newblock Neural tangent generalization attacks.
\newblock In \emph{ICML}, 2021.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Cai, Lu, He, and
  Wang]{zhang2021towards}
Bohang Zhang, Tianle Cai, Zhou Lu, Di~He, and Liwei Wang.
\newblock Towards certifying l-infinity robustness using neural networks with
  l-inf-dist neurons.
\newblock In \emph{ICML}, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2019)Zhang, Yu, Jiao, Xing, El~Ghaoui, and
  Jordan]{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El~Ghaoui, and
  Michael Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In \emph{ICML}, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Xu, Han, Niu, Cui, Sugiyama, and
  Kankanhalli]{zhang2020attacks}
Jingfeng Zhang, Xilie Xu, Bo~Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and
  Mohan Kankanhalli.
\newblock Attacks which do not kill training make adversarial learning
  stronger.
\newblock In \emph{ICML}, 2020.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Zhu, Niu, Han, Sugiyama, and
  Kankanhalli]{zhang2020geometry}
Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo~Han, Masashi Sugiyama, and Mohan
  Kankanhalli.
\newblock Geometry-aware instance-reweighted adversarial training.
\newblock In \emph{ICLR}, 2021{\natexlab{b}}.

\bibitem[Zhang and Zhu(2019)]{zhang2019interpreting}
Tianyuan Zhang and Zhanxing Zhu.
\newblock Interpreting adversarially trained convolutional neural networks.
\newblock In \emph{ICML}, 2019.

\bibitem[Zhao et~al.(2017)Zhao, An, Gao, and Zhang]{zhao2017efficient}
Mengchen Zhao, Bo~An, Wei Gao, and Teng Zhang.
\newblock Efficient label contamination attacks against black-box learning
  models.
\newblock In \emph{IJCAI}, 2017.

\bibitem[Zhao et~al.(2021)Zhao, Ma, Wang, Bailey, Li, and Jiang]{zhao2021deep}
Shihao Zhao, Xingjun Ma, Yisen Wang, James Bailey, Bo~Li, and Yu-Gang Jiang.
\newblock What do deep nets learn? class-wise patterns revealed in the input
  space.
\newblock \emph{arXiv preprint arXiv:2101.06898}, 2021.

\bibitem[Zhu et~al.(2019)Zhu, Huang, Li, Taylor, Studer, and
  Goldstein]{zhu2019transferable}
Chen Zhu, W~Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, and Tom
  Goldstein.
\newblock Transferable clean-label poisoning attacks on deep neural nets.
\newblock In \emph{ICML}, 2019.

\bibitem[Zhu et~al.(2021)Zhu, Zhang, Han, Liu, Niu, Yang, Kankanhalli, and
  Sugiyama]{zhu2021understanding}
Jianing Zhu, Jingfeng Zhang, Bo~Han, Tongliang Liu, Gang Niu, Hongxia Yang,
  Mohan Kankanhalli, and Masashi Sugiyama.
\newblock Understanding the interaction of adversarial training with noisy
  labels.
\newblock \emph{arXiv preprint arXiv:2102.03482}, 2021.

\bibitem[Zhu et~al.(2020)Zhu, Zhang, and Evans]{zhu2020learning}
Sicheng Zhu, Xiao Zhang, and David Evans.
\newblock Learning adversarially robust representations via worst-case mutual
  information maximization.
\newblock In \emph{ICML}, 2020.

\end{thebibliography}
