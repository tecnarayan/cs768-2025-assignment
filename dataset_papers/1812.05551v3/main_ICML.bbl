\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Asadi \& Littman(2016)Asadi and Littman]{asadi2016alternative}
Asadi, K. and Littman, M.~L.
\newblock An alternative softmax operator for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1612.05628}, 2016.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,
  Saxton, and Munos]{bellemare2016unifying}
Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and
  Munos, R.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1471--1479, 2016.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare2013arcade}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and
  Munos]{bellemare2017distributional}
Bellemare, M.~G., Dabney, W., and Munos, R.
\newblock A distributional perspective on reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1707.06887}, 2017.

\bibitem[Bertsekas \& Tsitsiklis(1995)Bertsekas and
  Tsitsiklis]{bertsekas1995neuro}
Bertsekas, D.~P. and Tsitsiklis, J.~N.
\newblock Neuro-dynamic programming: an overview.
\newblock In \emph{Decision and Control, 1995., Proceedings of the 34th IEEE
  Conference on}, volume~1, pp.\  560--564. IEEE, 1995.

\bibitem[Brafman \& Tennenholtz(2002)Brafman and Tennenholtz]{brafman2002r}
Brafman, R.~I. and Tennenholtz, M.
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Oct):\penalty0 213--231, 2002.

\bibitem[Dabney et~al.(2018)Dabney, Rowland, Bellemare, and
  Munos]{dabney2018distributional}
Dabney, W., Rowland, M., Bellemare, M.~G., and Munos, R.
\newblock Distributional reinforcement learning with quantile regression.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Dhariwal et~al.(2017)Dhariwal, Hesse, Klimov, Nichol, Plappert,
  Radford, Schulman, Sidor, and Wu]{baselines}
Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A.,
  Schulman, J., Sidor, S., and Wu, Y.
\newblock Openai baselines.
\newblock \url{https://github.com/openai/baselines}, 2017.

\bibitem[Efroni et~al.(2018)Efroni, Dalal, Scherrer, and
  Mannor]{beyond2018efroni}
Efroni, Y., Dalal, G., Scherrer, B., and Mannor, S.
\newblock Beyond the one-step greedy approach in reinforcement learning.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, pp.\  1386--1395, 2018.

\bibitem[Fortunato et~al.(2017)Fortunato, Azar, Piot, Menick, Osband, Graves,
  Mnih, Munos, Hassabis, Pietquin, et~al.]{fortunato2017noisy}
Fortunato, M., Azar, M.~G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih,
  V., Munos, R., Hassabis, D., Pietquin, O., et~al.
\newblock Noisy networks for exploration.
\newblock \emph{arXiv preprint arXiv:1706.10295}, 2017.

\bibitem[Hessel et~al.(2017)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{hessel2017rainbow}
Hessel, M., Modayil, J., Van~Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
  W., Horgan, D., Piot, B., Azar, M., and Silver, D.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1710.02298}, 2017.

\bibitem[Horgan et~al.(2018)Horgan, Quan, Budden, Barth-Maron, Hessel,
  Van~Hasselt, and Silver]{horgan2018distributed}
Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., Van~Hasselt, H.,
  and Silver, D.
\newblock Distributed prioritized experience replay.
\newblock \emph{arXiv preprint arXiv:1803.00933}, 2018.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Jaksch, T., Ortner, R., and Auer, P.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Apr):\penalty0 1563--1600, 2010.

\bibitem[Jiang et~al.(2015)Jiang, Kulesza, Singh, and
  Lewis]{jiang2015dependence}
Jiang, N., Kulesza, A., Singh, S., and Lewis, R.
\newblock The dependence of effective planning horizon on model accuracy.
\newblock In \emph{Proceedings of the 2015 International Conference on
  Autonomous Agents and Multiagent Systems}, pp.\  1181--1189. International
  Foundation for Autonomous Agents and Multiagent Systems, 2015.

\bibitem[John(1994)]{john1994best}
John, G.~H.
\newblock When the best move isn't optimal: Q-learning with exploration.
\newblock Citeseer, 1994.

\bibitem[Kearns \& Singh(2002)Kearns and Singh]{kearns2002near}
Kearns, M. and Singh, S.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine learning}, 49\penalty0 (2-3):\penalty0 209--232, 2002.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Littman et~al.(1997)]{littman1997generalized}
Littman, M.~L. et~al.
\newblock Generalized markov decision processes: Dynamic-programming and
  reinforcement-learning algorithms.
\newblock 1997.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1928--1937, 2016.

\bibitem[Nachum et~al.(2018)Nachum, Norouzi, Tucker, and
  Schuurmans]{nachum2018smoothed}
Nachum, O., Norouzi, M., Tucker, G., and Schuurmans, D.
\newblock Smoothed action value functions for learning gaussian policies.
\newblock \emph{arXiv preprint arXiv:1803.02348}, 2018.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{osband2013more}
Osband, I., Russo, D., and Van~Roy, B.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3003--3011, 2013.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and
  Van~Roy]{osband2016deep}
Osband, I., Blundell, C., Pritzel, A., and Van~Roy, B.
\newblock Deep exploration via bootstrapped dqn.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  4026--4034, 2016.

\bibitem[Petrik \& Scherrer(2009)Petrik and Scherrer]{petrik2009biasing}
Petrik, M. and Scherrer, B.
\newblock Biasing approximate dynamic programming with a lower discount factor.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1265--1272, 2009.

\bibitem[Puterman(1994)]{puterman1994markov}
Puterman, M.~L.
\newblock Markov decision processes. j.
\newblock \emph{Wiley and Sons}, 1994.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{silver2014deterministic}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{ICML}, 2014.

\bibitem[Singh et~al.(2000)Singh, Jaakkola, Littman, and
  Szepesv{\'a}ri]{singh2000convergence}
Singh, S., Jaakkola, T., Littman, M.~L., and Szepesv{\'a}ri, C.
\newblock Convergence results for single-step on-policy reinforcement-learning
  algorithms.
\newblock \emph{Machine learning}, 38\penalty0 (3):\penalty0 287--308, 2000.

\bibitem[Strehl et~al.(2009)Strehl, Li, and Littman]{strehl2009reinforcement}
Strehl, A.~L., Li, L., and Littman, M.~L.
\newblock Reinforcement learning in finite mdps: Pac analysis.
\newblock \emph{Journal of Machine Learning Research}, 10\penalty0
  (Nov):\penalty0 2413--2444, 2009.

\bibitem[Sutton et~al.(1998)Sutton, Barto, et~al.]{sutton1998reinforcement}
Sutton, R.~S., Barto, A.~G., et~al.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 1998.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ
  International Conference on}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{van2016deep}
Van~Hasselt, H., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{AAAI}, volume~2, pp.\ ~5. Phoenix, AZ, 2016.

\bibitem[Van~Seijen et~al.(2009)Van~Seijen, Van~Hasselt, Whiteson, and
  Wiering]{van2009theoretical}
Van~Seijen, H., Van~Hasselt, H., Whiteson, S., and Wiering, M.
\newblock A theoretical and empirical analysis of expected sarsa.
\newblock In \emph{Adaptive Dynamic Programming and Reinforcement Learning,
  2009. ADPRL'09. IEEE Symposium on}, pp.\  177--184. IEEE, 2009.

\end{thebibliography}
