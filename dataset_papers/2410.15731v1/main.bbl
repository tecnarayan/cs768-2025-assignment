\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amestoy et~al.(2000)Amestoy, Duff, L’Excellent, and Koster]{amestoy2000mumps}
Patrick~R Amestoy, Iain~S Duff, Jean-Yves L’Excellent, and Jacko Koster.
\newblock Mumps: a general purpose distributed memory sparse solver.
\newblock In \emph{International Workshop on Applied Parallel Computing}, pages 121--130. Springer, 2000.

\bibitem[Andrychowicz et~al.(2016)Andrychowicz, Denil, Gomez, Hoffman, Pfau, Schaul, Shillingford, and De~Freitas]{andrychowicz2016learning}
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew~W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De~Freitas.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Baker(2019)]{baker2019learning}
Kyri Baker.
\newblock Learning warm-start points for ac optimal power flow.
\newblock In \emph{2019 IEEE 29th International Workshop on Machine Learning for Signal Processing (MLSP)}, pages 1--6. IEEE, 2019.

\bibitem[Bellavia(1998)]{bellavia1998inexact}
Stefania Bellavia.
\newblock Inexact interior-point method.
\newblock \emph{Journal of Optimization Theory and Applications}, 96:\penalty0 109--121, 1998.

\bibitem[Bengio et~al.(2021)Bengio, Lodi, and Prouvost]{bengio2021machine}
Yoshua Bengio, Andrea Lodi, and Antoine Prouvost.
\newblock Machine learning for combinatorial optimization: a methodological tour d’horizon.
\newblock \emph{European Journal of Operational Research}, 290\penalty0 (2):\penalty0 405--421, 2021.

\bibitem[Chen and Burer(2012)]{chen2012globally}
Jieqiu Chen and Samuel Burer.
\newblock Globally solving nonconvex quadratic programming problems via completely positive programming.
\newblock \emph{Mathematical Programming Computation}, 4\penalty0 (1):\penalty0 33--52, 2012.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Chen, Chen, Heaton, Liu, Wang, and Yin]{chen2022learning}
Tianlong Chen, Xiaohan Chen, Wuyang Chen, Howard Heaton, Jialin Liu, Zhangyang Wang, and Wotao Yin.
\newblock Learning to optimize: A primer and a benchmark.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0 (189):\penalty0 1--59, 2022{\natexlab{a}}.

\bibitem[Chen et~al.(2024)Chen, Liu, and Yin]{chen2024learning}
Xiaohan Chen, Jialin Liu, and Wotao Yin.
\newblock Learning to optimize: A tutorial for continuous and mixed-integer optimization.
\newblock \emph{Science China Mathematics}, pages 1--72, 2024.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Liu, Wang, and Yin]{chen2022representing}
Ziang Chen, Jialin Liu, Xinshang Wang, and Wotao Yin.
\newblock On representing linear programs by graph neural networks.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022{\natexlab{b}}.

\bibitem[Chiang(2009)]{chiang2009nonconvex}
Mung Chiang.
\newblock Nonconvex optimization for communication networks.
\newblock \emph{Advances in Applied Mathematics and Global Optimization: In Honor of Gilbert Strang}, pages 137--196, 2009.

\bibitem[Conejo and Baringo(2018)]{conejo2018power}
Antonio~J Conejo and Luis Baringo.
\newblock \emph{Power system operations}, volume~14.
\newblock Springer, 2018.

\bibitem[Dexter et~al.(2022)Dexter, Chowdhury, Avron, and Drineas]{dexter2022convergence}
Gregory Dexter, Agniva Chowdhury, Haim Avron, and Petros Drineas.
\newblock On the convergence of inexact predictor-corrector methods for linear programming.
\newblock In \emph{International Conference on Machine Learning}, pages 5007--5038. PMLR, 2022.

\bibitem[Diehl(2019)]{diehl2019warm}
Frederik Diehl.
\newblock Warm-starting ac optimal power flow with graph neural networks.
\newblock In \emph{33rd Conference on Neural Information Processing Systems (NeurIPS 2019)}, pages 1--6, 2019.

\bibitem[Donti et~al.(2020)Donti, Rolnick, and Kolter]{donti2020dc3}
Priya~L Donti, David Rolnick, and J~Zico Kolter.
\newblock Dc3: A learning method for optimization with hard constraints.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Eisenstat and Walker(1994)]{eisenstat1994globally}
Stanley~C Eisenstat and Homer~F Walker.
\newblock Globally convergent inexact newton methods.
\newblock \emph{SIAM Journal on Optimization}, 4\penalty0 (2):\penalty0 393--422, 1994.

\bibitem[Fioretto et~al.(2020)Fioretto, Mak, and Van~Hentenryck]{fioretto2020predicting}
Ferdinando Fioretto, Terrence~WK Mak, and Pascal Van~Hentenryck.
\newblock Predicting ac optimal power flows: Combining deep learning and lagrangian dual methods.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~34, pages 630--637, 2020.

\bibitem[Forsgren(2006)]{forsgren2006warm}
Anders Forsgren.
\newblock On warm starts for interior methods.
\newblock In \emph{System Modeling and Optimization: Proceedings of the 22nd IFIP TC7 Conference held from July 18--22, 2005, in Turin, Italy 22}, pages 51--66. Springer, 2006.

\bibitem[Gasse et~al.(2022)Gasse, Bowly, Cappart, Charfreitag, Charlin, Ch{\'e}telat, Chmiela, Dumouchelle, Gleixner, Kazachkov, et~al.]{gasse2022machine}
Maxime Gasse, Simon Bowly, Quentin Cappart, Jonas Charfreitag, Laurent Charlin, Didier Ch{\'e}telat, Antonia Chmiela, Justin Dumouchelle, Ambros Gleixner, Aleksandr~M Kazachkov, et~al.
\newblock The machine learning for combinatorial optimization competition (ml4co): Results and insights.
\newblock In \emph{NeurIPS 2021 competitions and demonstrations track}, pages 220--231. PMLR, 2022.

\bibitem[Gregor and LeCun(2010)]{gregor2010learning}
Karol Gregor and Yann LeCun.
\newblock Learning fast approximations of sparse coding.
\newblock In \emph{Proceedings of the 27th international conference on international conference on machine learning}, pages 399--406, 2010.

\bibitem[Greif et~al.(2014)Greif, Moulding, and Orban]{greif2014bounds}
Chen Greif, Erin Moulding, and Dominique Orban.
\newblock Bounds on eigenvalues of matrices arising from interior-point methods.
\newblock \emph{SIAM Journal on Optimization}, 24\penalty0 (1):\penalty0 49--83, 2014.

\bibitem[Han et~al.(2024)Han, Wang, Yang, Niu, Yang, Yan, and Li]{han_frmnet_2024}
Jiayu Han, Wei Wang, Chao Yang, Mengyang Niu, Cheng Yang, Lei Yan, and Zuyi Li.
\newblock {FRMNet}: {A} {Feasibility} {Restoration} {Mapping} {Deep} {Neural} {Network} for {AC} {Optimal} {Power} {Flow}.
\newblock \emph{IEEE Transactions on Power Systems}, pages 1--11, 2024.
\newblock ISSN 0885-8950, 1558-0679.
\newblock \doi{10.1109/TPWRS.2024.3354733}.
\newblock URL \url{https://ieeexplore.ieee.org/document/10411984/}.

\bibitem[Han et~al.(2023)Han, Yang, Chen, Zhou, Zhang, Wang, Sun, and Luo]{hangnn}
Qingyu Han, Linxin Yang, Qian Chen, Xiang Zhou, Dong Zhang, Akang Wang, Ruoyu Sun, and Xiaodong Luo.
\newblock A gnn-guided predict-and-search framework for mixed-integer linear programming.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Huang et~al.(2021)Huang, Pan, Chen, and Low]{huang2021deepopf}
Wanjun Huang, Xiang Pan, Minghua Chen, and Steven~H Low.
\newblock Deepopf-v: Solving ac-opf problems efficiently.
\newblock \emph{IEEE Transactions on Power Systems}, 37\penalty0 (1):\penalty0 800--803, 2021.

\bibitem[Kim et~al.(2023)]{kim2023self}
Hongseok Kim et~al.
\newblock Self-supervised equality embedded deep lagrange dual for approximate constrained optimization.
\newblock \emph{arXiv preprint arXiv:2306.06674}, 2023.

\bibitem[Kingma(2014)]{kingma2014adam}
DP~Kingma.
\newblock Adam: a method for stochastic optimization.
\newblock In \emph{Int Conf Learn Represent}, 2014.

\bibitem[Li et~al.(2024)Li, Yang, Chen, Wang, Chen, Mao, Ma, Wang, Ding, Tang, et~al.]{lipdhg}
Bingheng Li, Linxin Yang, Yupeng Chen, Senmiao Wang, Qian Chen, Haitao Mao, Yao Ma, Akang Wang, Tian Ding, Jiliang Tang, et~al.
\newblock Pdhg-unrolled learning-to-optimize method for large-scale linear programming.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.

\bibitem[Li et~al.(2023)Li, Kolouri, and Mohammadi]{li2023learning}
Meiyi Li, Soheil Kolouri, and Javad Mohammadi.
\newblock Learning to solve optimization problems with hard linear constraints.
\newblock \emph{IEEE Access}, 2023.

\bibitem[Liang et~al.(2023)Liang, Chen, and Low]{liang2023low}
Enming Liang, Minghua Chen, and Steven Low.
\newblock Low complexity homeomorphic projection to ensure neural-network solution feasibility for optimization over (non-) convex set.
\newblock In \emph{Conference on Parsimony and Learning (Recent Spotlight Track)}, 2023.

\bibitem[Liu et~al.(2023)Liu, Chen, Wang, Yin, and Cai]{liu2023towards}
Jialin Liu, Xiaohan Chen, Zhangyang Wang, Wotao Yin, and HanQin Cai.
\newblock Towards constituting mathematical structures for learning to optimize.
\newblock In \emph{International Conference on Machine Learning}, pages 21426--21449. PMLR, 2023.

\bibitem[Liu et~al.(2024)Liu, Pu, Ge, and Ye]{liu2024learning}
Tianhao Liu, Shanwen Pu, Dongdong Ge, and Yinyu Ye.
\newblock Learning to pivot as a smart expert.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 8073--8081, 2024.

\bibitem[Lv et~al.(2017)Lv, Jiang, and Li]{lv2017learning}
Kaifeng Lv, Shunhua Jiang, and Jian Li.
\newblock Learning gradient descent: Better generalization and longer horizons.
\newblock In \emph{International Conference on Machine Learning}, pages 2247--2255. PMLR, 2017.

\bibitem[Nesterov and Nemirovskii(1994)]{nesterov1994interior}
Yurii Nesterov and Arkadii Nemirovskii.
\newblock \emph{Interior-point polynomial algorithms in convex programming}.
\newblock SIAM, 1994.

\bibitem[Nocedal and Wright(1999)]{nocedal1999numerical}
Jorge Nocedal and Stephen~J Wright.
\newblock \emph{Numerical optimization}.
\newblock Springer, 1999.

\bibitem[Pan et~al.(2023)Pan, Chen, Zhao, and Low]{pan2023deepopf}
Xiang Pan, Minghua Chen, Tianyu Zhao, and Steven~H. Low.
\newblock Deepopf: A feasibility-optimized deep neural network approach for ac optimal power flow problems.
\newblock \emph{IEEE Systems Journal}, 17\penalty0 (1):\penalty0 673--683, 2023.
\newblock \doi{10.1109/JSYST.2022.3201041}.

\bibitem[Park and Van~Hentenryck(2023)]{park2023self}
Seonho Park and Pascal Van~Hentenryck.
\newblock Self-supervised primal-dual learning for constrained optimization.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pages 4052--4060, 2023.

\bibitem[Qian et~al.(2024)Qian, Ch{\'e}telat, and Morris]{qian2024exploring}
Chendi Qian, Didier Ch{\'e}telat, and Christopher Morris.
\newblock Exploring the power of graph neural networks in solving linear optimization problems.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 1432--1440. PMLR, 2024.

\bibitem[Ruiz(2001)]{ruiz2001scaling}
Daniel Ruiz.
\newblock A scaling algorithm to equilibrate both rows and columns norms in matrices.
\newblock Technical report, CM-P00040415, 2001.

\bibitem[Schaal and Atkeson(2010)]{schaal2010learning}
Stefan Schaal and Christopher~G Atkeson.
\newblock Learning control in robotics.
\newblock \emph{IEEE Robotics \& Automation Magazine}, 17\penalty0 (2):\penalty0 20--29, 2010.

\bibitem[Stellato et~al.(2020)Stellato, Banjac, Goulart, Bemporad, and Boyd]{osqp}
B.~Stellato, G.~Banjac, P.~Goulart, A.~Bemporad, and S.~Boyd.
\newblock {OSQP}: an operator splitting solver for quadratic programs.
\newblock \emph{Mathematical Programming Computation}, 12\penalty0 (4):\penalty0 637--672, 2020.
\newblock \doi{10.1007/s12532-020-00179-2}.
\newblock URL \url{https://doi.org/10.1007/s12532-020-00179-2}.

\bibitem[W{\"a}chter and Biegler(2006)]{wachter2006implementation}
Andreas W{\"a}chter and Lorenz~T Biegler.
\newblock On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming.
\newblock \emph{Mathematical programming}, 106:\penalty0 25--57, 2006.

\bibitem[Yu et~al.(2019)Yu, Si, Hu, and Zhang]{yu2019review}
Yong Yu, Xiaosheng Si, Changhua Hu, and Jianxun Zhang.
\newblock A review of recurrent neural networks: Lstm cells and network architectures.
\newblock \emph{Neural computation}, 31\penalty0 (7):\penalty0 1235--1270, 2019.

\bibitem[Zeng et~al.(2024)Zeng, Kim, Ren, and Kim]{zeng_qcqp-net_2024}
Sihan Zeng, Youngdae Kim, Yuxuan Ren, and Kibaek Kim.
\newblock {QCQP}-{Net}: {Reliably} {Learning} {Feasible} {Alternating} {Current} {Optimal} {Power} {Flow} {Solutions} {Under} {Constraints}, January 2024.
\newblock URL \url{http://arxiv.org/abs/2401.06820}.
\newblock arXiv:2401.06820 [cs, math] version: 1.

\bibitem[Zhang and Zhang(2022)]{zhang2022learning}
Ling Zhang and Baosen Zhang.
\newblock Learning to solve the ac optimal power flow via a lagrangian approach.
\newblock In \emph{2022 North American Power Symposium (NAPS)}, pages 1--6, 2022.
\newblock \doi{10.1109/NAPS56150.2022.10012237}.

\end{thebibliography}
