\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{gpt3}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.: Language models
  are few-shot learners. Advances in neural information processing systems
  \textbf{33},  1877--1901 (2020)

\bibitem{xdetr}
Cai, Z., Kwon, G., Ravichandran, A., Bas, E., Tu, Z., Bhotika, R., Soatto, S.:
  X-detr: A versatile architecture for instance-wise vision-language tasks.
  arXiv preprint arXiv:2204.05626  (2022)

\bibitem{openpose}
Cao, Z., Simon, T., Wei, S.E., Sheikh, Y.: Realtime multi-person 2d pose
  estimation using part affinity fields. In: Proceedings of the IEEE conference
  on computer vision and pattern recognition. pp. 7291--7299 (2017)

\bibitem{detr}
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.:
  End-to-end object detection with transformers. In: European conference on
  computer vision. pp. 213--229. Springer (2020)

\bibitem{dino}
Caron, M., Touvron, H., Misra, I., J\'egou, H., Mairal, J., Bojanowski, P.,
  Joulin, A.: Emerging properties in self-supervised vision transformers. In:
  Proceedings of the International Conference on Computer Vision (ICCV) (2021)

\bibitem{mtl}
Caruana, R.: Multitask learning. Machine learning  \textbf{28}(1),  41--75
  (1997)

\bibitem{igpt}
Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., Sutskever, I.:
  Generative pretraining from pixels. In: International Conference on Machine
  Learning. pp. 1691--1703. PMLR (2020)

\bibitem{simclr}
Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for
  contrastive learning of visual representations. In: International conference
  on machine learning. pp. 1597--1607. PMLR (2020)

\bibitem{pix2seq}
Chen, T., Saxena, S., Li, L., Fleet, D.J., Hinton, G.: Pix2seq: A language
  modeling framework for object detection. In: International Conference on
  Learning Representations (2022),
  \url{https://openreview.net/forum?id=e42KbIw6Wb}

\bibitem{mocov3}
Chen, X., Xie, S., He, K.: An empirical study of training self-supervised
  vision transformers. In: Proceedings of the IEEE/CVF International Conference
  on Computer Vision. pp. 9640--9649 (2021)

\bibitem{cpn}
Chen, Y., Wang, Z., Peng, Y., Zhang, Z., Yu, G., Sun, J.: Cascaded pyramid
  network for multi-person pose estimation. In: Proceedings of the IEEE
  conference on computer vision and pattern recognition. pp. 7103--7112 (2018)

\bibitem{dpt}
Chen, Z., Zhu, Y., Zhao, C., Hu, G., Zeng, W., Wang, J., Tang, M.: Dpt:
  Deformable patch-based transformer for visual recognition. In: Proceedings of
  the 29th ACM International Conference on Multimedia. pp. 2899--2907 (2021)

\bibitem{higherhr}
Cheng, B., Xiao, B., Wang, J., Shi, H., Huang, T.S., Zhang, L.: Higherhrnet:
  Scale-aware representation learning for bottom-up human pose estimation. In:
  Proceedings of the IEEE/CVF conference on computer vision and pattern
  recognition. pp. 5386--5395 (2020)

\bibitem{mltr}
Cheng, X., Lin, H., Wu, X., Yang, F., Shen, D., Wang, Z., Shi, N., Liu, H.:
  Mltr: Multi-label classification with transformer. arXiv preprint
  arXiv:2106.06195  (2021)

\bibitem{mmpose}
Contributors, M.: Openmmlab pose estimation toolbox and benchmark.
  \url{https://github.com/open-mmlab/mmpose} (2020)

\bibitem{bert}
Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep
  bidirectional transformers for language understanding. arXiv preprint
  arXiv:1810.04805  (2018)

\bibitem{vit}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.:
  An image is worth 16x16 words: Transformers for image recognition at scale.
  arXiv preprint arXiv:2010.11929  (2020)

\bibitem{mcar}
Gao, B.B., Zhou, H.Y.: Learning to discover multi-class attentional regions for
  multi-label image recognition. IEEE Transactions on Image Processing
  \textbf{30},  5920--5932 (2021)

\bibitem{smca}
Gao, P., Zheng, M., Wang, X., Dai, J., Li, H.: Fast convergence of detr with
  spatially modulated co-attention. In: Proceedings of the IEEE/CVF
  International Conference on Computer Vision. pp. 3621--3630 (2021)

\bibitem{prompt}
Gao, T., Fisch, A., Chen, D.: Making pre-trained language models better
  few-shot learners. In: Association for Computational Linguistics (ACL) (2021)

\bibitem{must}
Ghiasi, G., Zoph, B., Cubuk, E.D., Le, Q.V., Lin, T.Y.: Multi-task
  self-training for learning general representations. In: Proceedings of the
  IEEE/CVF International Conference on Computer Vision. pp. 8856--8865 (2021)

\bibitem{mae}
He, K., Chen, X., Xie, S., Li, Y., Doll{\'a}r, P., Girshick, R.: Masked
  autoencoders are scalable vision learners. arXiv:2111.06377  (2021)

\bibitem{maskrcnn}
He, K., Gkioxari, G., Doll{\'a}r, P., Girshick, R.: Mask r-cnn. In: Proceedings
  of the IEEE international conference on computer vision. pp. 2961--2969
  (2017)

\bibitem{resnet}
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
  recognition. In: Proceedings of the IEEE conference on computer vision and
  pattern recognition. pp. 770--778 (2016)

\bibitem{randsac}
Hua, T., Tian, Y., Ren, S., Zhao, H., Sigal, L.: Self-supervision through
  random segments with autoregressive coding (randsac). arXiv preprint
  arXiv:2203.12054  (2022)

\bibitem{vpt}
Jia, M., Tang, L., Chen, B.C., Cardie, C., Belongie, S., Hariharan, B., Lim,
  S.N.: Visual prompt tuning. arXiv preprint arXiv:2203.12119  (2022)

\bibitem{mdetr}
Kamath, A., Singh, M., LeCun, Y., Synnaeve, G., Misra, I., Carion, N.:
  Mdetr-modulated detection for end-to-end multi-modal understanding. In:
  Proceedings of the IEEE/CVF International Conference on Computer Vision. pp.
  1780--1790 (2021)

\bibitem{pifpaf}
Kreiss, S., Bertoni, L., Alahi, A.: Pifpaf: Composite fields for human pose
  estimation. In: Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition. pp. 11977--11986 (2019)

\bibitem{prtr}
Li, K., Wang, S., Zhang, X., Xu, Y., Xu, W., Tu, Z.: Pose recognition with
  cascade transformers. In: Proceedings of the IEEE/CVF Conference on Computer
  Vision and Pattern Recognition. pp. 1944--1953 (2021)

\bibitem{mst}
Li, Z., Chen, Z., Yang, F., Li, W., Zhu, Y., Zhao, C., Deng, R., Wu, L., Zhao,
  R., Tang, M., et~al.: Mst: Masked self-supervised transformer for visual
  representation. Advances in Neural Information Processing Systems
  \textbf{34} (2021)

\bibitem{transfering}
Li, Z., Zhao, X., Zhao, C., Tang, M., Wang, J.: Transfering low-frequency
  features for domain adaptation. In: 2022 IEEE International Conference on
  Multimedia and Expo (ICME). pp. 01--06. IEEE (2022)

\bibitem{univip}
Li, Z., Zhu, Y., Yang, F., Li, W., Zhao, C., Chen, Y., Chen, Z., Xie, J., Wu,
  L., Zhao, R., et~al.: Univip: A unified framework for self-supervised visual
  pre-training. In: Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition. pp. 14627--14636 (2022)

\bibitem{focal}
Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll{\'a}r, P.: Focal loss for
  dense object detection. In: Proceedings of the IEEE international conference
  on computer vision. pp. 2980--2988 (2017)

\bibitem{coco}
Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
  Doll{\'a}r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In:
  European conference on computer vision. pp. 740--755. Springer (2014)

\bibitem{dab}
Liu, S., Li, F., Zhang, H., Yang, X., Qi, X., Su, H., Zhu, J., Zhang, L.:
  {DAB}-{DETR}: Dynamic anchor boxes are better queries for {DETR}. In:
  International Conference on Learning Representations (2022),
  \url{https://openreview.net/forum?id=oMI9PjOb9Jl}

\bibitem{query2label}
Liu, S., Zhang, L., Yang, X., Su, H., Zhu, J.: Query2label: A simple
  transformer way to multi-label classification (2021)

\bibitem{swin}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin
  transformer: Hierarchical vision transformer using shifted windows. In:
  Proceedings of the IEEE/CVF International Conference on Computer Vision. pp.
  10012--10022 (2021)

\bibitem{adamw}
Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv
  preprint arXiv:1711.05101  (2017)

\bibitem{yolopose}
Maji, D., Nagori, S., Mathew, M., Poddar, D.: Yolo-pose: Enhancing yolo for
  multi person pose estimation using object keypoint similarity loss. arXiv
  preprint arXiv:2204.06806  (2022)

\bibitem{conditional}
Meng, D., Chen, X., Fan, Z., Zeng, G., Li, H., Yuan, Y., Sun, L., Wang, J.:
  Conditional detr for fast training convergence. In: Proceedings of the
  IEEE/CVF International Conference on Computer Vision. pp. 3651--3660 (2021)

\bibitem{pixelcnn}
Van~den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals, O., Graves, A.,
  et~al.: Conditional image generation with pixelcnn decoders. Advances in
  neural information processing systems  \textbf{29} (2016)

\bibitem{personlab}
Papandreou, G., Zhu, T., Chen, L.C., Gidaris, S., Tompson, J., Murphy, K.:
  Personlab: Person pose estimation and instance segmentation with a bottom-up,
  part-based, geometric embedding model. In: Proceedings of the European
  conference on computer vision (ECCV). pp. 269--286 (2018)

\bibitem{clip}
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.: Learning transferable visual
  models from natural language supervision. In: International Conference on
  Machine Learning. pp. 8748--8763. PMLR (2021)

\bibitem{fasterrcnn}
Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object
  detection with region proposal networks. Advances in neural information
  processing systems  \textbf{28} (2015)

\bibitem{giou}
Rezatofighi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I., Savarese, S.:
  Generalized intersection over union: A metric and a loss for bounding box
  regression. In: Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition. pp. 658--666 (2019)

\bibitem{asl}
Ridnik, T., Ben-Baruch, E., Zamir, N., Noy, A., Friedman, I., Protter, M.,
  Zelnik-Manor, L.: Asymmetric loss for multi-label classification. In:
  Proceedings of the IEEE/CVF International Conference on Computer Vision. pp.
  82--91 (2021)

\bibitem{mldecoder}
Ridnik, T., Sharir, G., Ben-Cohen, A., Ben-Baruch, E., Noy, A.: Ml-decoder:
  Scalable and versatile classification head. arXiv preprint arXiv:2111.12933
  (2021)

\bibitem{mtl2}
Ruder, S.: An overview of multi-task learning in deep neural networks. arXiv
  preprint arXiv:1706.05098  (2017)

\bibitem{poet}
Stoffl, L., Vidal, M., Mathis, A.: End-to-end trainable multi-instance pose
  estimation with transformers. arXiv preprint arXiv:2103.12115  (2021)

\bibitem{hrnet}
Sun, K., Xiao, B., Liu, D., Wang, J.: Deep high-resolution representation
  learning for human pose estimation. In: Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition. pp. 5693--5703 (2019)

\bibitem{tsp}
Sun, Z., Cao, S., Yang, Y., Kitani, K.M.: Rethinking transformer-based set
  prediction for object detection. In: Proceedings of the IEEE/CVF
  International Conference on Computer Vision. pp. 3611--3620 (2021)

\bibitem{directpose}
Tian, Z., Chen, H., Shen, C.: Directpose: Direct end-to-end multi-person pose
  estimation. arXiv preprint arXiv:1911.07451  (2019)

\bibitem{deit}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J{\'e}gou, H.:
  Training data-efficient image transformers \& distillation through attention.
  In: International Conference on Machine Learning. pp. 10347--10357. PMLR
  (2021)

\bibitem{transformer}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
  Kaiser, {\L}., Polosukhin, I.: Attention is all you need. Advances in neural
  information processing systems  \textbf{30} (2017)

\bibitem{fpdetr}
Wang, W., Cao, Y., Zhang, J., Tao, D.: Fp-detr: Detection transformer advanced
  by fully pre-training. In: International Conference on Learning
  Representations (2021)

\bibitem{pvt}
Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P.,
  Shao, L.: Pyramid vision transformer: A versatile backbone for dense
  prediction without convolutions. In: Proceedings of the IEEE/CVF
  International Conference on Computer Vision. pp. 568--578 (2021)

\bibitem{anchor}
Wang, Y., Zhang, X., Yang, T., Sun, J.: Anchor detr: Query design for
  transformer-based detector. arXiv preprint arXiv:2109.07107  (2021)

\bibitem{simplebaseline}
Xiao, B., Wu, H., Wei, Y.: Simple baselines for human pose estimation and
  tracking. In: Proceedings of the European conference on computer vision
  (ECCV). pp. 466--481 (2018)

\bibitem{groupvit}
Xu, J., De~Mello, S., Liu, S., Byeon, W., Breuel, T., Kautz, J., Wang, X.:
  Groupvit: Semantic segmentation emerges from text supervision. arXiv preprint
  arXiv:2202.11094  (2022)

\bibitem{xlnet}
Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., Le, Q.V.:
  Xlnet: Generalized autoregressive pretraining for language understanding.
  Advances in neural information processing systems  \textbf{32} (2019)

\bibitem{dark}
Zhang, F., Zhu, X., Dai, H., Ye, M., Zhu, C.: Distribution-aware coordinate
  representation for human pose estimation. In: Proceedings of the IEEE/CVF
  conference on computer vision and pattern recognition. pp. 7093--7102 (2020)

\bibitem{detic}
Zhou, X., Girdhar, R., Joulin, A., Kr{\"a}henb{\"u}hl, P., Misra, I.: Detecting
  twenty-thousand classes using image-level supervision. In: arXiv preprint
  arXiv:2201.02605 (2021)

\bibitem{centernet}
Zhou, X., Wang, D., Kr{\"a}henb{\"u}hl, P.: Objects as points. arXiv preprint
  arXiv:1904.07850  (2019)

\bibitem{deformable}
Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable
  transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159
   (2020)

\end{thebibliography}
