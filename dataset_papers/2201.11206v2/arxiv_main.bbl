\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{abbasi2011improved}
Abbasi-Yadkori, Y., P{\'a}l, D., and Szepesv{\'a}ri, C.
\newblock Improved algorithms for linear stochastic bandits.
\newblock \emph{Advances in neural information processing systems},
  24:\penalty0 2312--2320, 2011.

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Krishnamurthy, and
  Sun]{agarwal2020flambe}
Agarwal, A., Kakade, S., Krishnamurthy, A., and Sun, W.
\newblock Flambe: Structural complexity and representation learning of low rank
  mdps.
\newblock \emph{arXiv preprint arXiv:2006.10814}, 2020.

\bibitem[Agarwal et~al.(2021)Agarwal, Chaudhuri, Jain, Nagaraj, and
  Netrapalli]{agarwal2021online}
Agarwal, N., Chaudhuri, S., Jain, P., Nagaraj, D., and Netrapalli, P.
\newblock Online target q-learning with reverse experience replay: Efficiently
  finding the optimal policy for linear mdps.
\newblock \emph{arXiv preprint arXiv:2110.08440}, 2021.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang, and
  Yang]{ayoub2020model}
Ayoub, A., Jia, Z., Szepesvari, C., Wang, M., and Yang, L.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  463--474. PMLR, 2020.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  263--272. PMLR, 2017.

\bibitem[Dann \& Brunskill(2015)Dann and Brunskill]{dann2015sample}
Dann, C. and Brunskill, E.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1510.08906}, 2015.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Dann, C., Lattimore, T., and Brunskill, E.
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1703.07710}, 2017.

\bibitem[Dann et~al.(2019)Dann, Li, Wei, and Brunskill]{dann2019policy}
Dann, C., Li, L., Wei, W., and Brunskill, E.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1507--1516. PMLR, 2019.

\bibitem[Du et~al.(2019)Du, Kakade, Wang, and Yang]{du2019good}
Du, S.~S., Kakade, S.~M., Wang, R., and Yang, L.~F.
\newblock Is a good representation sufficient for sample efficient
  reinforcement learning?
\newblock \emph{arXiv preprint arXiv:1910.03016}, 2019.

\bibitem[Du et~al.(2021)Du, Kakade, Lee, Lovett, Mahajan, Sun, and
  Wang]{du2021bilinear}
Du, S.~S., Kakade, S.~M., Lee, J.~D., Lovett, S., Mahajan, G., Sun, W., and
  Wang, R.
\newblock Bilinear classes: A structural framework for provable generalization
  in rl.
\newblock \emph{arXiv preprint arXiv:2103.10897}, 2021.

\bibitem[Foster et~al.(2021)Foster, Kakade, Qian, and
  Rakhlin]{foster2021statistical}
Foster, D.~J., Kakade, S.~M., Qian, J., and Rakhlin, A.
\newblock The statistical complexity of interactive decision making.
\newblock \emph{arXiv preprint arXiv:2112.13487}, 2021.

\bibitem[Freedman(1975)]{freedman1975tail}
Freedman, D.~A.
\newblock On tail probabilities for martingales.
\newblock \emph{the Annals of Probability}, pp.\  100--118, 1975.

\bibitem[Hao et~al.(2021)Hao, Lattimore, Szepesv{\'a}ri, and
  Wang]{hao2021online}
Hao, B., Lattimore, T., Szepesv{\'a}ri, C., and Wang, M.
\newblock Online sparse reinforcement learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  316--324. PMLR, 2021.

\bibitem[Huang et~al.(2022)Huang, Chen, Zhao, Qin, Jiang, and
  Liu]{huang2022towards}
Huang, J., Chen, J., Zhao, L., Qin, T., Jiang, N., and Liu, T.-Y.
\newblock Towards deployment-efficient reinforcement learning: Lower bound and
  optimality.
\newblock \emph{arXiv preprint arXiv:2202.06450}, 2022.

\bibitem[Jia et~al.(2020)Jia, Yang, Szepesvari, and Wang]{jia2020model}
Jia, Z., Yang, L., Szepesvari, C., and Wang, M.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{Learning for Dynamics and Control}, pp.\  666--686. PMLR,
  2020.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R.~E.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1704--1713. PMLR, 2017.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I.
\newblock Is q-learning provably efficient?
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pp.\  4868--4878, 2018.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Krishnamurthy, Simchowitz, and
  Yu]{jin2020reward}
Jin, C., Krishnamurthy, A., Simchowitz, M., and Yu, T.
\newblock Reward-free exploration for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4870--4879. PMLR, 2020{\natexlab{a}}.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Yang, Wang, and
  Jordan]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pp.\  2137--2143. PMLR,
  2020{\natexlab{b}}.

\bibitem[Jin et~al.(2021)Jin, Liu, and Miryoosefi]{jin2021bellman}
Jin, C., Liu, Q., and Miryoosefi, S.
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock \emph{arXiv preprint arXiv:2102.00815}, 2021.

\bibitem[Kakade(2003)]{kakade2003sample}
Kakade, S.~M.
\newblock \emph{On the sample complexity of reinforcement learning}.
\newblock PhD thesis, UCL (University College London), 2003.

\bibitem[Kearns \& Singh(2002)Kearns and Singh]{kearns2002near}
Kearns, M. and Singh, S.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine learning}, 49\penalty0 (2):\penalty0 209--232, 2002.

\bibitem[M{\'e}nard et~al.(2020)M{\'e}nard, Domingues, Jonsson, Kaufmann,
  Leurent, and Valko]{menard2020fast}
M{\'e}nard, P., Domingues, O.~D., Jonsson, A., Kaufmann, E., Leurent, E., and
  Valko, M.
\newblock Fast active learning for pure exploration in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2007.13442}, 2020.

\bibitem[Modi et~al.(2020)Modi, Jiang, Tewari, and Singh]{modi2020sample}
Modi, A., Jiang, N., Tewari, A., and Singh, S.
\newblock Sample complexity of reinforcement learning using linearly combined
  model ensembles.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  2010--2020. PMLR, 2020.

\bibitem[Modi et~al.(2021)Modi, Chen, Krishnamurthy, Jiang, and
  Agarwal]{modi2021model}
Modi, A., Chen, J., Krishnamurthy, A., Jiang, N., and Agarwal, A.
\newblock Model-free representation learning and exploration in low-rank mdps.
\newblock \emph{arXiv preprint arXiv:2102.07035}, 2021.

\bibitem[Shamir(2013)]{shamir2013complexity}
Shamir, O.
\newblock On the complexity of bandit and derivative-free stochastic convex
  optimization.
\newblock In \emph{Conference on Learning Theory}, pp.\  3--24. PMLR, 2013.

\bibitem[Tarbouriech et~al.(2020)Tarbouriech, Pirotta, Valko, and
  Lazaric]{tarbouriech2020provably}
Tarbouriech, J., Pirotta, M., Valko, M., and Lazaric, A.
\newblock A provably efficient sample collection strategy for reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2007.06437}, 2020.

\bibitem[Tsybakov(2009)]{tsybakov2009introduction}
Tsybakov, A.~B.
\newblock Introduction to nonparametric estimation., 2009.

\bibitem[Vershynin(2010)]{vershynin2010introduction}
Vershynin, R.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \emph{arXiv preprint arXiv:1011.3027}, 2010.

\bibitem[Wagenmaker et~al.(2021{\natexlab{a}})Wagenmaker, Chen, Simchowitz, Du,
  and Jamieson]{wagenmaker2021first}
Wagenmaker, A., Chen, Y., Simchowitz, M., Du, S.~S., and Jamieson, K.
\newblock First-order regret in reinforcement learning with linear function
  approximation: A robust estimation approach.
\newblock \emph{arXiv preprint arXiv:2112.03432}, 2021{\natexlab{a}}.

\bibitem[Wagenmaker et~al.(2021{\natexlab{b}})Wagenmaker, Simchowitz, and
  Jamieson]{wagenmaker2021beyond}
Wagenmaker, A., Simchowitz, M., and Jamieson, K.
\newblock Beyond no regret: Instance-dependent pac reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2108.02717}, 2021{\natexlab{b}}.

\bibitem[Wang et~al.(2020)Wang, Du, Yang, and Salakhutdinov]{wang2020reward}
Wang, R., Du, S.~S., Yang, L.~F., and Salakhutdinov, R.
\newblock On reward-free reinforcement learning with linear function
  approximation.
\newblock \emph{arXiv preprint arXiv:2006.11274}, 2020.

\bibitem[Wang et~al.(2019)Wang, Wang, Du, and Krishnamurthy]{wang2019optimism}
Wang, Y., Wang, R., Du, S.~S., and Krishnamurthy, A.
\newblock Optimism in reinforcement learning with generalized linear function
  approximation.
\newblock \emph{arXiv preprint arXiv:1912.04136}, 2019.

\bibitem[Wang et~al.(2021)Wang, Wang, and Kakade]{wang2021exponential}
Wang, Y., Wang, R., and Kakade, S.~M.
\newblock An exponential lower bound for linearly-realizable mdps with constant
  suboptimality gap.
\newblock \emph{arXiv preprint arXiv:2103.12690}, 2021.

\bibitem[Weisz et~al.(2021)Weisz, Amortila, and
  Szepesv{\'a}ri]{weisz2021exponential}
Weisz, G., Amortila, P., and Szepesv{\'a}ri, C.
\newblock Exponential lower bounds for planning in mdps with
  linearly-realizable optimal action-value functions.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  1237--1264. PMLR, 2021.

\bibitem[Wu et~al.(2021)Wu, Braverman, and Yang]{wu2021gap}
Wu, J., Braverman, V., and Yang, L.~F.
\newblock Gap-dependent unsupervised exploration for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2108.05439}, 2021.

\bibitem[Yang \& Wang(2019)Yang and Wang]{yang2019sample}
Yang, L. and Wang, M.
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6995--7004. PMLR, 2019.

\bibitem[Zanette \& Brunskill(2019)Zanette and Brunskill]{zanette2019tighter}
Zanette, A. and Brunskill, E.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7304--7312. PMLR, 2019.

\bibitem[Zanette et~al.(2020{\natexlab{a}})Zanette, Brandfonbrener, Brunskill,
  Pirotta, and Lazaric]{zanette2020frequentist}
Zanette, A., Brandfonbrener, D., Brunskill, E., Pirotta, M., and Lazaric, A.
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1954--1964. PMLR, 2020{\natexlab{a}}.

\bibitem[Zanette et~al.(2020{\natexlab{b}})Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
Zanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10978--10989. PMLR, 2020{\natexlab{b}}.

\bibitem[Zanette et~al.(2020{\natexlab{c}})Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020provably}
Zanette, A., Lazaric, A., Kochenderfer, M.~J., and Brunskill, E.
\newblock Provably efficient reward-agnostic navigation with linear value
  iteration.
\newblock \emph{arXiv preprint arXiv:2008.07737}, 2020{\natexlab{c}}.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Zhou, and Gu]{zhang2021reward}
Zhang, W., Zhou, D., and Gu, Q.
\newblock Reward-free model-based reinforcement learning with linear function
  approximation.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Du, and Ji]{zhang2020nearly}
Zhang, Z., Du, S.~S., and Ji, X.
\newblock Nearly minimax optimal reward-free reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2010.05901}, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Ji, and
  Du]{zhang2020reinforcement}
Zhang, Z., Ji, X., and Du, S.~S.
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock \emph{arXiv preprint arXiv:2009.13503}, 2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Yang, Ji, and
  Du]{zhang2021variance}
Zhang, Z., Yang, J., Ji, X., and Du, S.~S.
\newblock Variance-aware confidence set: Variance-dependent bound for linear
  bandits and horizon-free bound for linear mixture mdp.
\newblock \emph{arXiv preprint arXiv:2101.12745}, 2021{\natexlab{b}}.

\bibitem[Zhou et~al.(2020)Zhou, Gu, and Szepesvari]{zhou2020nearly}
Zhou, D., Gu, Q., and Szepesvari, C.
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock \emph{arXiv preprint arXiv:2012.08507}, 2020.

\bibitem[Zhou et~al.(2021)Zhou, He, and Gu]{zhou2021provably}
Zhou, D., He, J., and Gu, Q.
\newblock Provably efficient reinforcement learning for discounted mdps with
  feature mapping.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12793--12802. PMLR, 2021.

\end{thebibliography}
