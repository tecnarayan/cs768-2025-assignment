\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bard et~al.(2019)Bard, Foerster, Chandar, Burch, Lanctot, Song,
  Parisotto, Dumoulin, Moitra, Hughes, Dunning, Mourad, Larochelle, Bellemare,
  and Bowling]{bard2019hanabi}
Nolan Bard, Jakob~N. Foerster, Sarath Chandar, Neil Burch, Marc Lanctot,
  H.~Francis Song, Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward
  Hughes, Iain Dunning, Shibl Mourad, Hugo Larochelle, Marc~G. Bellemare, and
  Michael Bowling.
\newblock The hanabi challenge: {A} new frontier for {AI} research.
\newblock \emph{CoRR}, abs/1902.00506, 2019.
\newblock URL \url{http://arxiv.org/abs/1902.00506}.

\bibitem[Bernstein et~al.(2002)Bernstein, Givan, Immerman, and
  Zilberstein]{bernstein2002DecPOMDP}
Daniel~S. Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein.
\newblock The complexity of decentralized control of markov decision processes.
\newblock \emph{Mathematics of Operations Research}, 27\penalty0 (4):\penalty0
  819--840, 2002.
\newblock \doi{10.1287/moor.27.4.819.297}.
\newblock URL \url{https://doi.org/10.1287/moor.27.4.819.297}.

\bibitem[Boutilier(1996)]{boutilier1996planning}
Craig Boutilier.
\newblock Planning, learning and coordination in multiagent decision processes.
\newblock In \emph{Proceedings of the 6th conference on Theoretical aspects of
  rationality and knowledge}, pages 195--210. Morgan Kaufmann Publishers Inc.,
  1996.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Chen et~al.(2019)Chen, Zhou, Wen, Yang, Su, Zhang, Zhang, Wang, and
  Liu]{Chen2019FactorizedQF}
Y.~Chen, M.~Zhou, Ying Wen, Y.~Yang, Y.~Su, W.~Zhang, Dell Zhang, J.~Wang, and
  Han Liu.
\newblock Factorized q-learning for large-scale multi-agent systems.
\newblock In \emph{DAI '19}, 2019.

\bibitem[Cobbe et~al.(2019)Cobbe, Hesse, Hilton, and
  Schulman]{cobbe2019leveraging}
Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.01588}, 2019.

\bibitem[GitHub(2021)]{openai}
GitHub.
\newblock openai/gym dependents, 2021.
\newblock URL
  \url{https://web.archive.org/web/20210527224052/https://github.com/openai/gym/network/dependents?dependent_type=PACKAGE}.

\bibitem[Gupta et~al.(2017)Gupta, Egorov, and
  Kochenderfer]{gupta2017cooperative}
Jayesh~K Gupta, Maxim Egorov, and Mykel Kochenderfer.
\newblock Cooperative multi-agent control using deep reinforcement learning.
\newblock In \emph{International Conference on Autonomous Agents and Multiagent
  Systems}, pages 66--83. Springer, 2017.

\bibitem[Ha(2020)]{slimevolleygym}
David Ha.
\newblock Slime volleyball gym environment.
\newblock \url{https://github.com/hardmaru/slimevolleygym}, 2020.

\bibitem[Hill et~al.(2018)Hill, Raffin, Ernestus, Gleave, Kanervisto, Traore,
  Dhariwal, Hesse, Klimov, Nichol, Plappert, Radford, Schulman, Sidor, and
  Wu]{stable-baselines}
Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi
  Kanervisto, Rene Traore, Prafulla Dhariwal, Christopher Hesse, Oleg Klimov,
  Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor,
  and Yuhuai Wu.
\newblock Stable baselines.
\newblock \url{https://github.com/hill-a/stable-baselines}, 2018.

\bibitem[Horgan et~al.(2018)Horgan, Quan, Budden, Barth-Maron, Hessel,
  Van~Hasselt, and Silver]{horgan2018distributed}
Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado
  Van~Hasselt, and David Silver.
\newblock Distributed prioritized experience replay.
\newblock \emph{arXiv preprint arXiv:1803.00933}, 2018.

\bibitem[Huang et~al.(2020)Huang, Dossa, and Ye]{cleanrl}
Shengyi Huang, Rousslan Dossa, and Chang Ye.
\newblock Cleanrl: High-quality single-file implementation of deep
  reinforcement learning algorithms.
\newblock \url{https://github.com/vwxyzjn/cleanrl/}, 2020.

\bibitem[Hughes et~al.(2018)Hughes, Leibo, Phillips, Tuyls, Due{\~n}ez-Guzman,
  Casta{\~n}eda, Dunning, Zhu, McKee, Koster, et~al.]{hughes2018inequity}
Edward Hughes, Joel~Z Leibo, Matthew Phillips, Karl Tuyls, Edgar
  Due{\~n}ez-Guzman, Antonio~Garc{\'\i}a Casta{\~n}eda, Iain Dunning, Tina Zhu,
  Kevin McKee, Raphael Koster, et~al.
\newblock Inequity aversion improves cooperation in intertemporal social
  dilemmas.
\newblock In \emph{Advances in neural information processing systems}, pages
  3326--3336, 2018.

\bibitem[Lanctot et~al.(2019)Lanctot, Lockhart, Lespiau, Zambaldi, Upadhyay,
  P{\'{e}}rolat, Srinivasan, Timbers, Tuyls, Omidshafiei, Hennes, Morrill,
  Muller, Ewalds, Faulkner, Kram{\'{a}}r, Vylder, Saeta, Bradbury, Ding,
  Borgeaud, Lai, Schrittwieser, Anthony, Hughes, Danihelka, and
  Ryan{-}Davis]{openspiel2019}
Marc Lanctot, Edward Lockhart, Jean{-}Baptiste Lespiau, Vin{\'{\i}}cius~Flores
  Zambaldi, Satyaki Upadhyay, Julien P{\'{e}}rolat, Sriram Srinivasan, Finbarr
  Timbers, Karl Tuyls, Shayegan Omidshafiei, Daniel Hennes, Dustin Morrill,
  Paul Muller, Timo Ewalds, Ryan Faulkner, J{\'{a}}nos Kram{\'{a}}r, Bart~De
  Vylder, Brennan Saeta, James Bradbury, David Ding, Sebastian Borgeaud,
  Matthew Lai, Julian Schrittwieser, Thomas~W. Anthony, Edward Hughes, Ivo
  Danihelka, and Jonah Ryan{-}Davis.
\newblock Openspiel: {A} framework for reinforcement learning in games.
\newblock \emph{CoRR}, abs/1908.09453, 2019.
\newblock URL \url{http://arxiv.org/abs/1908.09453}.

\bibitem[Laszuk(2020)]{AI-Traineree}
Dawid Laszuk.
\newblock Ai-traineree.
\newblock \url{https://github.com/laszukdawid/ai-traineree}, 2020.

\bibitem[Leibo et~al.(2017)Leibo, Zambaldi, Lanctot, Marecki, and
  Graepel]{leibo2017multi}
Joel~Z Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore
  Graepel.
\newblock Multi-agent reinforcement learning in sequential social dilemmas.
\newblock \emph{arXiv preprint arXiv:1702.03037}, 2017.

\bibitem[Liang et~al.(2017)Liang, Liaw, Moritz, Nishihara, Fox, Goldberg,
  Gonzalez, Jordan, and Stoica]{liang2017rllib}
Eric Liang, Richard Liaw, Philipp Moritz, Robert Nishihara, Roy Fox, Ken
  Goldberg, Joseph~E Gonzalez, Michael~I Jordan, and Ion Stoica.
\newblock Rllib: Abstractions for distributed reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1712.09381}, 2017.

\bibitem[Liang et~al.(2018)Liang, Liaw, Nishihara, Moritz, Fox, Goldberg,
  Gonzalez, Jordan, and Stoica]{liang2018rllib}
Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken
  Goldberg, Joseph~E. Gonzalez, Michael~I. Jordan, and Ion Stoica.
\newblock {RLlib}: Abstractions for distributed reinforcement learning.
\newblock In \emph{International Conference on Machine Learning ({ICML})},
  2018.

\bibitem[Liu et~al.(2019)Liu, Lever, Merel, Tunyasuvunakool, Heess, and
  Graepel]{deepmind_soccer}
Siqi Liu, Guy Lever, Josh Merel, Saran Tunyasuvunakool, Nicolas Heess, and
  Thore Graepel.
\newblock Emergent coordination through competition.
\newblock \emph{CoRR}, abs/1902.07151, 2019.
\newblock URL \url{http://arxiv.org/abs/1902.07151}.

\bibitem[Lowe et~al.(2017)Lowe, Wu, Tamar, Harb, Abbeel, and
  Mordatch]{lowe2017multi}
Ryan Lowe, Yi~Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock \emph{Neural Information Processing Systems (NIPS)}, 2017.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Mordatch and Abbeel(2017)]{mordatch2017emergence}
Igor Mordatch and Pieter Abbeel.
\newblock Emergence of grounded compositional language in multi-agent
  populations.
\newblock \emph{arXiv preprint arXiv:1703.04908}, 2017.

\bibitem[Nota(2020)]{nota2020autonomous}
Chris Nota.
\newblock The autonomous learning library.
\newblock \url{https://github.com/cpnota/autonomous-learning-library}, 2020.

\bibitem[OpenAI(2018)]{OpenAI_dota}
OpenAI.
\newblock Openai five.
\newblock \url{https://blog.openai.com/openai-five/}, 2018.

\bibitem[Osborne and Rubinstein(1994)]{osborne1994gametheory}
Martin~J Osborne and Ariel Rubinstein.
\newblock \emph{A course in game theory}.
\newblock MIT press, 1994.

\bibitem[Palmer(2020)]{Palmer2020IndependentLA}
G.~Palmer.
\newblock Independent learning approaches: Overcoming multi-agent learning
  pathologies in team-games.
\newblock 2020.

\bibitem[Pokle(2018)]{Pokle2018AnalysisOE}
Stefanie Anna Baby Ling Li~Ashwini Pokle.
\newblock Analysis of emergent behavior in multi agent environments using deep
  reinforcement learning.
\newblock 2018.

\bibitem[Raffin(2020)]{rl-zoo3}
Antonin Raffin.
\newblock Rl baselines3 zoo.
\newblock \url{https://github.com/DLR-RM/rl-baselines3-zoo}, 2020.

\bibitem[Raffin et~al.(2019)Raffin, Hill, Ernestus, Gleave, Kanervisto, and
  Dormann]{stable-baselines3}
Antonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi
  Kanervisto, and Noah Dormann.
\newblock Stable baselines3.
\newblock \url{https://github.com/DLR-RM/stable-baselines3}, 2019.

\bibitem[Samvelyan et~al.(2019)Samvelyan, Rashid, de~Witt, Farquhar, Nardelli,
  Rudner, Hung, Torr, Foerster, and Whiteson]{smac2019}
Mikayel Samvelyan, Tabish Rashid, Christian~Schr{\"{o}}der de~Witt, Gregory
  Farquhar, Nantas Nardelli, Tim G.~J. Rudner, Chia{-}Man Hung, Philip H.~S.
  Torr, Jakob~N. Foerster, and Shimon Whiteson.
\newblock The starcraft multi-agent challenge.
\newblock \emph{CoRR}, abs/1902.04043, 2019.
\newblock URL \url{http://arxiv.org/abs/1902.04043}.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Shapley(1953)]{shapley1953stochasticgames}
L.~S. Shapley.
\newblock Stochastic games.
\newblock \emph{Proceedings of the National Academy of Sciences}, 39\penalty0
  (10):\penalty0 1095--1100, 1953.
\newblock ISSN 0027-8424.
\newblock \doi{10.1073/pnas.39.10.1095}.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Subramanian et~al.(2020)Subramanian, Poupart, Taylor, and
  Hegde]{Subramanian2020MultiTM}
Sriram~Ganapathi Subramanian, P.~Poupart, Matthew~E. Taylor, and N.~Hegde.
\newblock Multi type mean field reinforcement learning.
\newblock In \emph{AAMAS}, 2020.

\bibitem[Terry et~al.(2020{\natexlab{a}})Terry, Grammel, Hari, Santos, and
  Black]{terry2020revisiting}
J~K Terry, Nathaniel Grammel, Ananth Hari, Luis Santos, and Benjamin Black.
\newblock Revisiting parameter sharing in multi-agent deep reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2005.13625}, 2020{\natexlab{a}}.

\bibitem[Terry and Black(2020)]{terry2020arcade}
J~K Terry and Benjamin Black.
\newblock Multiplayer support for the arcade learning environment.
\newblock \emph{arXiv preprint arXiv:2009.09341}, 2020.

\bibitem[Terry et~al.(2020{\natexlab{b}})Terry, Black, and Hari]{SuperSuit}
J~K Terry, Benjamin Black, and Ananth Hari.
\newblock Supersuit: Simple microwrappers for reinforcement learning
  environments.
\newblock \emph{arXiv preprint arXiv:2008.08932}, 2020{\natexlab{b}}.

\bibitem[Terry et~al.(2020{\natexlab{c}})Terry, Black, and Hari]{SuperSuit2020}
J~K Terry, Benjamin Black, and Ananth Hari.
\newblock Supersuit: Simple microwrappers for reinforcement learning
  environments.
\newblock \emph{arXiv preprint arXiv:2008.08932}, 2020{\natexlab{c}}.

\bibitem[Terry et~al.(2020{\natexlab{d}})Terry, Grammel, Hari, Santos, and
  Black]{TerryParameterSharing}
J~K Terry, Nathaniel Grammel, Ananth Hari, Luis Santos, and Benjamin
  Black.
\newblock Revisiting parameter sharing in multi-agent deep reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2005.13625}, 2020{\natexlab{d}}.

\bibitem[Tesauro(1995)]{gerald1995temporal}
Gerald Tesauro.
\newblock Temporal difference learning and td-gammon.
\newblock \emph{Commun. ACM}, 38\penalty0 (3):\penalty0 58â€“68, March 1995.
\newblock ISSN 0001-0782.
\newblock \doi{10.1145/203330.203343}.
\newblock URL \url{https://doi.org/10.1145/203330.203343}.

\bibitem[Vidal et~al.(2002)Vidal, Shakernia, Kim, Shim, and
  Sastry]{vidal2002probabilistic}
Rene Vidal, Omid Shakernia, H~Jin Kim, David~Hyunchul Shim, and Shankar Sastry.
\newblock Probabilistic pursuit-evasion games: theory, implementation, and
  experimental evaluation.
\newblock \emph{IEEE transactions on robotics and automation}, 18\penalty0
  (5):\penalty0 662--669, 2002.

\bibitem[Vinitsky et~al.(2019)Vinitsky, Jaques, Leibo, Castenada, and
  Hughes]{SSDOpenSource}
Eugene Vinitsky, Natasha Jaques, Joel Leibo, Antonio Castenada, and Edward
  Hughes.
\newblock An open source implementation of sequential social dilemma games.
\newblock
  \url{https://github.com/eugenevinitsky/sequential_social_dilemma_games/},
  2019.
\newblock GitHub repository.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Oriol Vinyals, Igor Babuschkin, Wojciech~M Czarnecki, Micha{\"e}l Mathieu,
  Andrew Dudzik, Junyoung Chung, David~H Choi, Richard Powell, Timo Ewalds,
  Petko Georgiev, et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Weng et~al.(2020)Weng, Zhang, Duburcq, You, Yan, Su, and
  Zhu]{tianshou}
Jiayi Weng, Minghao Zhang, Alexis Duburcq, Kaichao You, Dong Yan, Hang Su, and
  Jun Zhu.
\newblock Tianshou.
\newblock \url{https://github.com/thu-ml/tianshou}, 2020.

\bibitem[Zha et~al.(2019)Zha, Lai, Cao, Huang, Wei, Guo, and Hu]{zha2019rlcard}
Daochen Zha, Kwei-Herng Lai, Yuanpu Cao, Songyi Huang, Ruzhe Wei, Junyu Guo,
  and Xia Hu.
\newblock Rlcard: A toolkit for reinforcement learning in card games.
\newblock \emph{arXiv preprint arXiv:1910.04376}, 2019.

\bibitem[Zheng et~al.(2017)Zheng, Yang, Cai, Zhang, Wang, and
  Yu]{zheng2017magent}
Lianmin Zheng, Jiacheng Yang, Han Cai, Weinan Zhang, Jun Wang, and Yong Yu.
\newblock Magent: A many-agent reinforcement learning platform for artificial
  collective intelligence.
\newblock \emph{arXiv preprint arXiv:1712.00600}, 2017.

\end{thebibliography}
