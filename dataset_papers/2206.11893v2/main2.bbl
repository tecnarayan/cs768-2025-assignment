\begin{thebibliography}{26}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bai et~al.(2019)Bai, Kolter, and Koltun]{trellisnet}
Shaojie Bai, J~Zico Kolter, and Vladlen Koltun.
\newblock Trellis networks for sequence modeling.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2019.

\bibitem[Dauphin et~al.(2017)Dauphin, Fan, Auli, and
  Grangier]{dauphin2017language}
Yann~N Dauphin, Angela Fan, Michael Auli, and David Grangier.
\newblock Language modeling with gated convolutional networks.
\newblock In \emph{International conference on machine learning}, pages
  933--941. PMLR, 2017.

\bibitem[Erichson et~al.(2021)Erichson, Azencot, Queiruga, Hodgkinson, and
  Mahoney]{erichson2021lipschitz}
N~Benjamin Erichson, Omri Azencot, Alejandro Queiruga, Liam Hodgkinson, and
  Michael~W Mahoney.
\newblock Lipschitz recurrent neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Goel et~al.(2022)Goel, Gu, Donahue, and R{\'e}]{goel2022sashimi}
Karan Goel, Albert Gu, Chris Donahue, and Christopher R{\'e}.
\newblock It's raw! audio generation with state-space models.
\newblock \emph{arXiv preprint arXiv:2202.09729}, 2022.

\bibitem[Gu et~al.(2020{\natexlab{a}})Gu, Dao, Ermon, Rudra, and
  R{\'{e}}]{gu2020hippo}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R{\'{e}}.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020{\natexlab{a}}.

\bibitem[Gu et~al.(2020{\natexlab{b}})Gu, Gulcehre, Paine, Hoffman, and
  Pascanu]{gu2020improving}
Albert Gu, Caglar Gulcehre, Tom~Le Paine, Matt Hoffman, and Razvan Pascanu.
\newblock Improving the gating mechanism of recurrent neural networks.
\newblock In \emph{The International Conference on Machine Learning ({ICML})},
  2020{\natexlab{b}}.

\bibitem[Gu et~al.(2021)Gu, Johnson, Goel, Saab, Dao, Rudra, and
  R\'{e}]{gu2021lssl}
Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and
  Christopher R\'{e}.
\newblock Combining recurrent, convolutional, and continuous-time models with
  the structured learnable linear state space layer.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Gu et~al.(2022{\natexlab{a}})Gu, Goel, and R{\'e}]{gu2022efficiently}
Albert Gu, Karan Goel, and Christopher R{\'e}.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2022{\natexlab{a}}.

\bibitem[Gu et~al.(2022{\natexlab{b}})Gu, Johnson, Timalsina, Rudra, and
  R\'e]{gu2022hippo}
Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R\'e.
\newblock How to train your hippo: State space models with generalized basis
  projections.
\newblock \emph{arXiv preprint arXiv:2206.12037}, 2022{\natexlab{b}}.

\bibitem[Gupta(2022)]{gupta2022diagonal}
Ankit Gupta.
\newblock Diagonal state spaces are as effective as structured state spaces.
\newblock \emph{arXiv preprint arXiv:2203.14343}, 2022.

\bibitem[Hochreiter and Schmidhuber(1997)]{lstm}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pages
  448--456. PMLR, 2015.

\bibitem[Kidger et~al.(2020)Kidger, Morrill, Foster, and
  Lyons]{kidger2020neural}
Patrick Kidger, James Morrill, James Foster, and Terry Lyons.
\newblock Neural controlled differential equations for irregular time series.
\newblock \emph{arXiv preprint arXiv:2005.08926}, 2020.

\bibitem[Morrill et~al.(2021)Morrill, Salvi, Kidger, Foster, and
  Lyons]{morrill2021neural}
James Morrill, Cristopher Salvi, Patrick Kidger, James Foster, and Terry Lyons.
\newblock Neural rough differential equations for long time series.
\newblock \emph{The International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Nonaka and Seita(2021)]{nonaka2021depth}
Naoki Nonaka and Jun Seita.
\newblock In-depth benchmarking of deep neural network architectures for ecg
  diagnosis.
\newblock In \emph{Machine Learning for Healthcare Conference}, pages 414--439.
  PMLR, 2021.

\bibitem[Pan(2001)]{pan2001structured}
Victor Pan.
\newblock \emph{Structured matrices and polynomials: unified superfast
  algorithms}.
\newblock Springer Science \& Business Media, 2001.

\bibitem[Ralston and Rabinowitz(2001)]{ralston2001first}
Anthony Ralston and Philip Rabinowitz.
\newblock \emph{A first course in numerical analysis}.
\newblock Courier Corporation, 2001.

\bibitem[Romero et~al.(2021)Romero, Kuzina, Bekkers, Tomczak, and
  Hoogendoorn]{romero2021ckconv}
David~W Romero, Anna Kuzina, Erik~J Bekkers, Jakub~M Tomczak, and Mark
  Hoogendoorn.
\newblock Ckconv: Continuous kernel convolution for sequential data.
\newblock \emph{arXiv preprint arXiv:2102.02611}, 2021.

\bibitem[Romero et~al.(2022)Romero, Bruintjes, Tomczak, Bekkers, Hoogendoorn,
  and van Gemert]{romero2022flexconv}
David~W Romero, Robert-Jan Bruintjes, Jakub~M Tomczak, Erik~J Bekkers, Mark
  Hoogendoorn, and Jan~C van Gemert.
\newblock Flexconv: Continuous kernel convolutions with differentiable kernel
  sizes.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2022.

\bibitem[Rusch and Mishra(2021)]{rusch2021unicornn}
T~Konstantin Rusch and Siddhartha Mishra.
\newblock Unicornn: A recurrent model for learning very long time dependencies.
\newblock \emph{The International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Shazeer(2020)]{shazeer2020glu}
Noam Shazeer.
\newblock Glu variants improve transformer.
\newblock \emph{arXiv preprint arXiv:2002.05202}, 2020.

\bibitem[Tan et~al.(2021)Tan, Bergmeir, Petitjean, and Webb]{Tan2020TSER}
Chang~Wei Tan, Christoph Bergmeir, Francois Petitjean, and Geoffrey~I Webb.
\newblock Time series extrinsic regression.
\newblock \emph{Data Mining and Knowledge Discovery}, pages 1--29, 2021.
\newblock \doi{https://doi.org/10.1007/s10618-021-00745-9}.

\bibitem[Tay et~al.(2021)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang,
  Ruder, and Metzler]{tay2021long}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena : A benchmark for efficient transformers.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=qVyeW-grC2k}.

\bibitem[Trinh et~al.(2018)Trinh, Dai, Luong, and Le]{trinh2018learning}
Trieu~H Trinh, Andrew~M Dai, Minh-Thang Luong, and Quoc~V Le.
\newblock Learning longer-term dependencies in {RNNs} with auxiliary losses.
\newblock In \emph{The International Conference on Machine Learning ({ICML})},
  2018.

\bibitem[Voelker et~al.(2019)Voelker, Kaji{\'c}, and
  Eliasmith]{voelker2019legendre}
Aaron Voelker, Ivana Kaji{\'c}, and Chris Eliasmith.
\newblock Legendre memory units: Continuous-time representation in recurrent
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  15544--15553, 2019.

\end{thebibliography}
