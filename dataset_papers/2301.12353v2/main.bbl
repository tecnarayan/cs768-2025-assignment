\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bao et~al.(2023)Bao, Li, Shen, Tai, Wu, and
  Xiang]{Bao2019ApproximationAO}
Bao, C., Li, Q., Shen, Z., Tai, C., Wu, L., and Xiang, X.
\newblock Approximation analysis of convolutional neural networks.
\newblock \emph{East Asian Journal on Applied Mathematics}, 13\penalty0
  (3):\penalty0 524--549, 2023.
\newblock ISSN 2079--7370.
\newblock \doi{https://doi.org/10.4208/eajam.2022-270.070123}.
\newblock URL
  \url{http://global-sci.org/intro/article_detail/eajam/21721.html}.

\bibitem[Barron(1993)]{barron1993}
Barron, A.~R.
\newblock Universal approximation bounds for superpositions of a sigmoidal
  function.
\newblock \emph{IEEE Transactions on Information Theory}, 39\penalty0
  (3):\penalty0 930--945, May 1993.
\newblock ISSN 0018-9448.
\newblock URL \url{https://doi.org/10.1109/18.256500}.

\bibitem[{Barron} \& {Klusowski}(2018){Barron} and
  {Klusowski}]{barron2018approximation}
{Barron}, A.~R. and {Klusowski}, J.~M.
\newblock Approximation and estimation for high-dimensional deep learning
  networks.
\newblock \emph{arXiv e-prints}, art. arXiv:1809.03090, September 2018.
\newblock URL \url{https://arxiv.org/abs/1809.03090}.

\bibitem[Bartlett et~al.(1998)Bartlett, Maiorov, and
  Meir]{Bartlett98almostlinear}
Bartlett, P., Maiorov, V., and Meir, R.
\newblock Almost linear {VC}-dimension bounds for piecewise polynomial
  networks.
\newblock \emph{Neural Computation}, 10\penalty0 (8):\penalty0 2159â€“2173,
  1998.
\newblock URL \url{https://doi.org/10.1162/089976698300017016}.

\bibitem[B{\"o}lcskei et~al.(2019)B{\"o}lcskei, Grohs, Kutyniok, and
  Petersen]{doi:10.1137/18M118709X}
B{\"o}lcskei, H., Grohs, P., Kutyniok, G., and Petersen, P.
\newblock Optimal approximation with sparsely connected deep neural networks.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 1\penalty0
  (1):\penalty0 8--45, 2019.
\newblock \doi{10.1137/18M118709X}.
\newblock URL \url{https://doi.org/10.1137/18M118709X}.

\bibitem[Chen et~al.(2019)Chen, Jiang, Liao, and Zhao]{Wenjing}
Chen, M., Jiang, H., Liao, W., and Zhao, T.
\newblock Efficient approximation of deep {ReLU} networks for functions on low
  dimensional manifolds.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/fd95ec8df5dbeea25aa8e6c808bad583-Paper.pdf}.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and
  Duvenaud]{NEURIPS2018_69386f6b}
Chen, R. T.~Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D.~K.
\newblock Neural ordinary differential equations.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf}.

\bibitem[Chui et~al.(2018)Chui, Lin, and Zhou]{10.3389/fams.2018.00014}
Chui, C.~K., Lin, S.-B., and Zhou, D.-X.
\newblock Construction of neural networks for realization of localized deep
  learning.
\newblock \emph{Frontiers in Applied Mathematics and Statistics}, 4:\penalty0
  14, 2018.
\newblock ISSN 2297-4687.
\newblock \doi{10.3389/fams.2018.00014}.
\newblock URL
  \url{https://www.frontiersin.org/article/10.3389/fams.2018.00014}.

\bibitem[Cybenko(1989)]{Cybenko1989ApproximationBS}
Cybenko, G.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of Control, Signals, and Systems}, 2:\penalty0
  303--314, 1989.
\newblock URL \url{https://doi.org/10.1007/BF02551274}.

\bibitem[E(2017)]{WeinanE2017dynamicalsystems}
E, W.
\newblock A proposal on machine learning via dynamical systems.
\newblock \emph{Communications in Mathematics and Statistics}, 5:\penalty0
  1--11, 2017.
\newblock URL \url{https://doi.org/10.1007/s40304-017-0103-z}.

\bibitem[{E} et~al.(2022){E}, {Ma}, and {Wu}]{2019arXiv190608039E}
{E}, W., {Ma}, C., and {Wu}, L.
\newblock The {Barron} space and the flow-induced function spaces for neural
  network models.
\newblock \emph{Constructive Approximation}, 55:\penalty0 369--406, 2022.
\newblock URL \url{https://doi.org/10.1007/s00365-021-09549-y}.

\bibitem[Gribonval et~al.(2022)Gribonval, Kutyniok, Nielsen, and
  Voigtlaender]{2019arXiv190501208G}
Gribonval, R., Kutyniok, G., Nielsen, M., and Voigtlaender, F.
\newblock Approximation spaces of deep neural networks.
\newblock \emph{Constructive Approximation}, 55:\penalty0 259--367, 2022.
\newblock URL \url{https://doi.org/10.1007/s00365-021-09543-4}.

\bibitem[{G{\"u}hring} et~al.(2020){G{\"u}hring}, {Kutyniok}, and
  {Petersen}]{2019arXiv190207896G}
{G{\"u}hring}, I., {Kutyniok}, G., and {Petersen}, P.
\newblock Error bounds for approximations with deep {ReLU} neural networks in
  ${W}^{s,p}$ norms.
\newblock \emph{Analysis and Applications}, 18\penalty0 (05):\penalty0
  803--859, 2020.
\newblock \doi{10.1142/S0219530519410021}.
\newblock URL \url{https://doi.org/10.1142/S0219530519410021}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{7780459}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  770--778, June 2016.
\newblock URL \url{https://doi.org/10.1109/CVPR.2016.90}.

\bibitem[Holmes(2007)]{Holmes:2007}
Holmes, P.
\newblock {H}istory of dynamical systems.
\newblock \emph{Scholarpedia}, 2\penalty0 (5):\penalty0 1843, 2007.
\newblock URL \url{https://doi.org/10.4249/scholarpedia.1843}.

\bibitem[Hornik(1991)]{HORNIK1991251}
Hornik, K.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock \emph{Neural Networks}, 4\penalty0 (2):\penalty0 251--257, 1991.
\newblock ISSN 0893-6080.
\newblock \doi{https://doi.org/10.1016/0893-6080(91)90009-T}.
\newblock URL
  \url{http://www.sciencedirect.com/science/article/pii/089360809190009T}.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and White]{HORNIK1989359}
Hornik, K., Stinchcombe, M., and White, H.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural Networks}, 2\penalty0 (5):\penalty0 359--366, 1989.
\newblock ISSN 0893-6080.
\newblock \doi{https://doi.org/10.1016/0893-6080(89)90020-8}.
\newblock URL
  \url{http://www.sciencedirect.com/science/article/pii/0893608089900208}.

\bibitem[{Jiao} et~al.(2021){Jiao}, {Lai}, {Lu}, {Wang}, {Zhijian Yang}, and
  {Yang}]{jiao2021deep}
{Jiao}, Y., {Lai}, Y., {Lu}, X., {Wang}, F., {Zhijian Yang}, J., and {Yang}, Y.
\newblock Deep neural networks with {ReLU-Sine-Exponential} activations break
  curse of dimensionality on {H}{\"o}lder class.
\newblock \emph{arXiv e-prints}, art. arXiv:2103.00542, February 2021.
\newblock URL \url{https://arxiv.org/abs/2103.00542}.

\bibitem[{Li} et~al.(2022){Li}, {Lin}, and {Shen}]{2022arXiv220808707L}
{Li}, Q., {Lin}, T., and {Shen}, Z.
\newblock Deep neural network approximation of invariant functions through
  dynamical systems.
\newblock \emph{arXiv e-prints}, art. arXiv:2208.08707, August 2022.
\newblock URL \url{https://arxiv.org/abs/2208.08707}.

\bibitem[{Li} et~al.(2023){Li}, {Lin}, and {Shen}]{2019arXiv191210382L}
{Li}, Q., {Lin}, T., and {Shen}, Z.
\newblock Deep learning via dynamical systems: An approximation perspective.
\newblock \emph{Journal of the European Mathematical Society}, 25\penalty0
  (5):\penalty0 1671--1709, 2023.
\newblock URL \url{https://doi.org/10.4171/JEMS/1221}.

\bibitem[{Lin} et~al.(2022){Lin}, {Shen}, and {Li}]{2022arXiv221114047L}
{Lin}, T., {Shen}, Z., and {Li}, Q.
\newblock On the universal approximation property of deep fully convolutional
  neural networks.
\newblock \emph{arXiv e-prints}, art. arXiv:2211.14047, November 2022.
\newblock URL \url{https://arxiv.org/abs/2211.14047}.

\bibitem[Liu et~al.(2020)Liu, Jiang, He, Chen, Liu, Gao, and Han]{Liu2020On}
Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkgz2aEKDr}.

\bibitem[Lu et~al.(2021)Lu, Shen, Yang, and Zhang]{shijun:3}
Lu, J., Shen, Z., Yang, H., and Zhang, S.
\newblock Deep network approximation for smooth functions.
\newblock \emph{SIAM Journal on Mathematical Analysis}, 53\penalty0
  (5):\penalty0 5465--5506, 2021.
\newblock URL \url{https://doi.org/10.1137/20M134695X}.

\bibitem[Montanelli \& Yang(2020)Montanelli and Yang]{MO}
Montanelli, H. and Yang, H.
\newblock Error bounds for deep {ReLU} networks using the {Kolmogorov-Arnold}
  superposition theorem.
\newblock \emph{Neural Networks}, 129:\penalty0 1--6, 2020.
\newblock ISSN 0893-6080.
\newblock \doi{https://doi.org/10.1016/j.neunet.2019.12.013}.
\newblock URL
  \url{http://www.sciencedirect.com/science/article/pii/S0893608019304058}.

\bibitem[{Montanelli} et~al.(2021){Montanelli}, {Yang}, and {Du}]{bandlimit}
{Montanelli}, H., {Yang}, H., and {Du}, Q.
\newblock Deep {ReLU} networks overcome the curse of dimensionality for
  bandlimited functions.
\newblock \emph{Journal of Computational Mathematics}, 39\penalty0
  (6):\penalty0 801--815, 2021.
\newblock ISSN 1991-7139.
\newblock \doi{https://doi.org/10.4208/jcm.2007-m2019-0239}.
\newblock URL \url{http://global-sci.org/intro/article_detail/jcm/19912.html}.

\bibitem[Nakada \& Imaizumi(2020)Nakada and Imaizumi]{Ryumei}
Nakada, R. and Imaizumi, M.
\newblock Adaptive approximation and generalization of deep neural network with
  intrinsic dimensionality.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (174):\penalty0 1--38, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-002.html}.

\bibitem[Plummer et~al.(2022)Plummer, Dryden, Frost, Hoefler, and
  Saenko]{2006.10598}
Plummer, B.~A., Dryden, N., Frost, J., Hoefler, T., and Saenko, K.
\newblock Neural parameter allocation search.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=srtIXtySfT4}.

\bibitem[Savarese \& Maire(2019)Savarese and Maire]{savarese2018learning}
Savarese, P. and Maire, M.
\newblock Learning implicitly recurrent {CNN}s through parameter sharing.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJgYxn09Fm}.

\bibitem[Shen et~al.(2019)Shen, Yang, and Zhang]{shijun:1}
Shen, Z., Yang, H., and Zhang, S.
\newblock Nonlinear approximation via compositions.
\newblock \emph{Neural Networks}, 119:\penalty0 74--84, 2019.
\newblock ISSN 0893-6080.
\newblock \doi{https://doi.org/10.1016/j.neunet.2019.07.011}.
\newblock URL
  \url{http://www.sciencedirect.com/science/article/pii/S0893608019301996}.

\bibitem[Shen et~al.(2020)Shen, Yang, and Zhang]{shijun:2}
Shen, Z., Yang, H., and Zhang, S.
\newblock Deep network approximation characterized by number of neurons.
\newblock \emph{Communications in Computational Physics}, 28\penalty0
  (5):\penalty0 1768--1811, 2020.
\newblock ISSN 1991-7120.
\newblock URL \url{https://doi.org/10.4208/cicp.OA-2020-0149}.

\bibitem[Shen et~al.(2021{\natexlab{a}})Shen, Yang, and Zhang]{shijun:4}
Shen, Z., Yang, H., and Zhang, S.
\newblock Deep network with approximation error being reciprocal of width to
  power of square root of depth.
\newblock \emph{Neural Computation}, 33\penalty0 (4):\penalty0 1005--1036, 03
  2021{\natexlab{a}}.
\newblock ISSN 0899-7667.
\newblock \doi{10.1162/neco_a_01364}.
\newblock URL \url{https://doi.org/10.1162/neco\_a\_01364}.

\bibitem[Shen et~al.(2021{\natexlab{b}})Shen, Yang, and Zhang]{shijun:5}
Shen, Z., Yang, H., and Zhang, S.
\newblock Neural network approximation: {T}hree hidden layers are enough.
\newblock \emph{Neural Networks}, 141:\penalty0 160--173, 2021{\natexlab{b}}.
\newblock ISSN 0893-6080.
\newblock URL \url{https://doi.org/10.1016/j.neunet.2021.04.011}.

\bibitem[Shen et~al.(2022{\natexlab{a}})Shen, Yang, and
  Zhang]{shijun:arbitrary:error:with:fixed:size}
Shen, Z., Yang, H., and Zhang, S.
\newblock Deep network approximation: Achieving arbitrary accuracy with fixed
  number of neurons.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (276):\penalty0 1--60, 2022{\natexlab{a}}.
\newblock URL \url{http://jmlr.org/papers/v23/21-1404.html}.

\bibitem[Shen et~al.(2022{\natexlab{b}})Shen, Yang, and
  Zhang]{shijun:intrinsic:parameters}
Shen, Z., Yang, H., and Zhang, S.
\newblock Deep network approximation in terms of intrinsic parameters.
\newblock In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and
  Sabato, S. (eds.), \emph{Proceedings of the 39th International Conference on
  Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning
  Research}, pp.\  19909--19934. PMLR, 17--23 Jul 2022{\natexlab{b}}.
\newblock URL \url{https://proceedings.mlr.press/v162/shen22g.html}.

\bibitem[{Shen} et~al.(2022){Shen}, {Yang}, and
  {Zhang}]{shijun:net:arc:beyond:width:depth}
{Shen}, Z., {Yang}, H., and {Zhang}, S.
\newblock Neural network architecture beyond width and depth.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and
  Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  5669--5681. Curran Associates, Inc., 2022.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2022/hash/257be12f31dfa7cc158dda99822c6fd1-Abstract-Conference.html}.

\bibitem[Suzuki(2019)]{suzuki2018adaptivity}
Suzuki, T.
\newblock Adaptivity of deep {ReLU} network for learning in {Besov} and mixed
  smooth {Besov} spaces: optimal rate and curse of dimensionality.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=H1ebTsActm}.

\bibitem[Wallingford et~al.(2022)Wallingford, Li, Achille, Ravichandran,
  Fowlkes, Bhotika, and Soatto]{9879069}
Wallingford, M., Li, H., Achille, A., Ravichandran, A., Fowlkes, C., Bhotika,
  R., and Soatto, S.
\newblock Task adaptive parameter sharing for multi-task learning.
\newblock In \emph{2022 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  7551--7560, 2022.
\newblock URL \url{https://doi.org/10.1109/CVPR52688.2022.00741}.

\bibitem[Wang et~al.(2020)Wang, Bai, Wu, Shi, Huang, King, Lyu, and
  Cheng]{NEURIPS2020_42cd63cb}
Wang, J., Bai, H., Wu, J., Shi, X., Huang, J., King, I., Lyu, M., and Cheng, J.
\newblock Revisiting parameter sharing for automatic neural channel number
  search.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  5991--6002. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/42cd63cb189c30ed03e42ce2c069566c-Paper.pdf}.

\bibitem[{Wang} et~al.(2020){Wang}, {Cheng}, {Sapiro}, and
  {Qiu}]{2020arXiv200902386W}
{Wang}, Z., {Cheng}, X., {Sapiro}, G., and {Qiu}, Q.
\newblock {ACDC}: Weight sharing in atom-coefficient decomposed convolution.
\newblock \emph{arXiv e-prints}, art. arXiv:2009.02386, September 2020.
\newblock URL \url{https://arxiv.org/abs/2009.02386}.

\bibitem[Yarotsky(2017)]{yarotsky2017}
Yarotsky, D.
\newblock Error bounds for approximations with deep {ReLU} networks.
\newblock \emph{Neural Networks}, 94:\penalty0 103--114, 2017.
\newblock ISSN 0893-6080.
\newblock \doi{https://doi.org/10.1016/j.neunet.2017.07.002}.
\newblock URL
  \url{http://www.sciencedirect.com/science/article/pii/S0893608017301545}.

\bibitem[Yarotsky(2018)]{yarotsky18a}
Yarotsky, D.
\newblock Optimal approximation of continuous functions by very deep {ReLU}
  networks.
\newblock In Bubeck, S., Perchet, V., and Rigollet, P. (eds.),
  \emph{Proceedings of the 31st Conference On Learning Theory}, volume~75 of
  \emph{Proceedings of Machine Learning Research}, pp.\  639--649. PMLR, 06--09
  Jul 2018.
\newblock URL \url{http://proceedings.mlr.press/v75/yarotsky18a.html}.

\bibitem[Yarotsky \& Zhevnerchuk(2020)Yarotsky and
  Zhevnerchuk]{yarotsky:2019:06}
Yarotsky, D. and Zhevnerchuk, A.
\newblock The phase diagram of approximation rates for deep neural networks.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  13005--13015. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/979a3f14bae523dc5101c52120c535e9-Paper.pdf}.

\bibitem[Zhang et~al.(2022)Zhang, Yang, Liu, and Guan]{9859706}
Zhang, L., Yang, Q., Liu, X., and Guan, H.
\newblock Rethinking hard-parameter sharing in multi-domain learning.
\newblock In \emph{2022 IEEE International Conference on Multimedia and Expo
  (ICME)}, pp.\  01--06, 2022.
\newblock URL \url{https://doi.org/10.1109/ICME52920.2022.9859706}.

\bibitem[Zhang(2020)]{shijun:thesis}
Zhang, S.
\newblock Deep neural network approximation via function compositions.
\newblock \emph{PhD Thesis, National University of Singapore}, 2020.
\newblock URL \url{https://scholarbank.nus.edu.sg/handle/10635/186064}.

\bibitem[Zhou(2020)]{ZHOU2019}
Zhou, D.-X.
\newblock Universality of deep convolutional neural networks.
\newblock \emph{Applied and Computational Harmonic Analysis}, 48\penalty0
  (2):\penalty0 787--794, 2020.
\newblock ISSN 1063-5203.
\newblock \doi{https://doi.org/10.1016/j.acha.2019.06.004}.
\newblock URL
  \url{http://www.sciencedirect.com/science/article/pii/S1063520318302045}.

\end{thebibliography}
