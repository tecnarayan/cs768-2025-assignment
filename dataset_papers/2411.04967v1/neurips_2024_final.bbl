\begin{thebibliography}{100}

\bibitem{EfficientNetV2}
Mingxing Tan and Quoc Le.
\newblock Efficientnetv2: Smaller models and faster training.
\newblock In {\em International conference on machine learning}, pages 10096--10106. PMLR, 2021.

\bibitem{li2022efficientformer}
Yanyu Li, Geng Yuan, Yang Wen, Ju~Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren.
\newblock Efficientformer: Vision transformers at mobilenet speed.
\newblock {\em Advances in Neural Information Processing Systems}, 35:12934--12949, 2022.

\bibitem{imagen}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily~L Denton, Kamyar Ghasemipour, Raphael Gontijo~Lopes, Burcu Karagol~Ayan, Tim Salimans, et~al.
\newblock Photorealistic text-to-image diffusion models with deep language understanding.
\newblock {\em Advances in Neural Information Processing Systems}, 35:36479--36494, 2022.

\bibitem{dit}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 4195--4205, 2023.

\bibitem{li2023snapfusion}
Yanyu Li, Huan Wang, Qing Jin, Ju~Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren.
\newblock Snapfusion: Text-to-image diffusion model on mobile devices within two seconds.
\newblock {\em arXiv preprint arXiv:2306.00980}, 2023.

\bibitem{liu2023hyperhuman}
Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li, Dahua Lin, Xihui Liu, Ziwei Liu, and Sergey Tulyakov.
\newblock Hyperhuman: Hyper-realistic human generation with latent structural diffusion.
\newblock {\em arXiv preprint arXiv:2310.08579}, 2023.

\bibitem{gigagan}
Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park.
\newblock Scaling up gans for text-to-image synthesis.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 10124--10134, 2023.

\bibitem{sdxl}
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M{\"u}ller, Joe Penna, and Robin Rombach.
\newblock Sdxl: Improving latent diffusion models for high-resolution image synthesis.
\newblock {\em arXiv preprint arXiv:2307.01952}, 2023.

\bibitem{kondor2018generalization}
Risi Kondor and Shubhendu Trivedi.
\newblock On the generalization of equivariance and convolution in neural networks to the action of compact groups.
\newblock In {\em International Conference on Machine Learning}, pages 2747--2755. PMLR, 2018.

\bibitem{ViT2021dosovitskiy}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{dai2021coatnet}
Zihang Dai, Hanxiao Liu, Quoc~V Le, and Mingxing Tan.
\newblock Coatnet: Marrying convolution and attention for all data sizes.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan, editors, {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{li2023rethinking}
Yanyu Li, Ju~Hu, Yang Wen, Georgios Evangelidis, Kamyar Salahi, Yanzhi Wang, Sergey Tulyakov, and Jian Ren.
\newblock Rethinking vision transformers for mobilenet size and speed.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 16889--16900, 2023.

\bibitem{ldm}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 10684--10695, 2022.

\bibitem{li2024scalability}
Hao Li, Yang Zou, Ying Wang, Orchid Majumder, Yusheng Xie, R~Manmatha, Ashwin Swaminathan, Zhuowen Tu, Stefano Ermon, and Stefano Soatto.
\newblock On the scalability of diffusion-based text-to-image generation.
\newblock {\em arXiv preprint arXiv:2404.02883}, 2024.

\bibitem{hoogeboom2023simple}
Emiel Hoogeboom, Jonathan Heek, and Tim Salimans.
\newblock simple diffusion: End-to-end diffusion for high resolution images.
\newblock In {\em International Conference on Machine Learning}, pages 13213--13232. PMLR, 2023.

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in Neural Information Processing Systems}, 33:6840--6851, 2020.

\bibitem{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock {\em arXiv preprint arXiv:2011.13456}, 2020.

\bibitem{dhariwal2021diffusion}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock {\em Advances in Neural Information Processing Systems}, 34:8780--8794, 2021.

\bibitem{karras2022elucidating}
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
\newblock Elucidating the design space of diffusion-based generative models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:26565--26577, 2022.

\bibitem{glide}
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen.
\newblock Glide: Towards photorealistic image generation and editing with text-guided diffusion models.
\newblock {\em arXiv preprint arXiv:2112.10741}, 2021.

\bibitem{saharia2022palette}
Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi.
\newblock Palette: Image-to-image diffusion models.
\newblock In {\em ACM SIGGRAPH 2022 Conference Proceedings}, pages 1--10, 2022.

\bibitem{dalle2}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock {\em arXiv preprint arXiv:2204.06125}, 2022.

\bibitem{chang2023muse}
Huiwen Chang, Han Zhang, Jarred Barber, AJ~Maschinot, Jose Lezama, Lu~Jiang, Ming-Hsuan Yang, Kevin Murphy, William~T Freeman, Michael Rubinstein, et~al.
\newblock Muse: Text-to-image generation via masked generative transformers.
\newblock {\em arXiv preprint arXiv:2301.00704}, 2023.

\bibitem{ediffi}
Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et~al.
\newblock ediffi: Text-to-image diffusion models with an ensemble of expert denoisers.
\newblock {\em arXiv preprint arXiv:2211.01324}, 2022.

\bibitem{zhang2023sine}
Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris~N Metaxas, and Jian Ren.
\newblock Sine: Single image editing with text-to-image diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 6027--6037, 2023.

\bibitem{sheynin2023emu}
Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman.
\newblock Emu edit: Precise image editing via recognition and generation tasks.
\newblock {\em arXiv preprint arXiv:2311.10089}, 2023.

\bibitem{ho2022imagen}
Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik~P Kingma, Ben Poole, Mohammad Norouzi, David~J Fleet, et~al.
\newblock Imagen video: High definition video generation with diffusion models.
\newblock {\em arXiv preprint arXiv:2210.02303}, 2022.

\bibitem{gupta2023photorealistic}
Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li~Fei-Fei, Irfan Essa, Lu~Jiang, and Jos{\'e} Lezama.
\newblock Photorealistic video generation with diffusion models.
\newblock {\em arXiv preprint arXiv:2312.06662}, 2023.

\bibitem{girdhar2023emu}
Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai~Saketh Rambhatla, Akbar Shah, Xi~Yin, Devi Parikh, and Ishan Misra.
\newblock Emu video: Factorizing text-to-video generation by explicit image conditioning.
\newblock {\em arXiv preprint arXiv:2311.10709}, 2023.

\bibitem{menapace2024snap}
Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et~al.
\newblock Snap video: Scaled spatiotemporal transformers for text-to-video synthesis.
\newblock {\em arXiv preprint arXiv:2402.14797}, 2024.

\bibitem{tu2022maxvit}
Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li.
\newblock Maxvit: Multi-axis vision transformer.
\newblock {\em ECCV}, 2022.

\bibitem{hatamizadeh2023fastervit}
Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, Jose~M Alvarez, Jan Kautz, and Pavlo Molchanov.
\newblock Fastervit: Fast vision transformers with hierarchical attention.
\newblock {\em arXiv preprint arXiv:2306.06189}, 2023.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern recognition}, pages 248--255. Ieee, 2009.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em Advances in neural information processing systems}, 25, 2012.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{jiao2019survey}
Licheng Jiao, Fan Zhang, Fang Liu, Shuyuan Yang, Lingling Li, Zhixi Feng, and Rong Qu.
\newblock A survey of deep learning-based object detection.
\newblock {\em IEEE access}, 7:128837--128868, 2019.

\bibitem{minaee2021image}
Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, Nasser Kehtarnavaz, and Demetri Terzopoulos.
\newblock Image segmentation using deep learning: A survey.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence}, 44(7):3523--3542, 2021.

\bibitem{SE_Nets_CVPR_2018}
Jie Hu, Li~Shen, and Gang Sun.
\newblock Squeeze-and-excitation networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 7132--7141, 2018.

\bibitem{kag2022condensing}
Anil Kag and Venkatesh Saligrama.
\newblock Condensing cnns with partial differential equations.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 610--619, 2022.

\bibitem{sun2020neupde}
Yifan Sun, Linan Zhang, and Hayden Schaeffer.
\newblock Neupde: Neural network based ordinary and partial differential equations for modeling time-dependent data.
\newblock In {\em Mathematical and Scientific Machine Learning}, pages 352--372. PMLR, 2020.

\bibitem{chen2018neural}
Ricky~TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David~K Duvenaud.
\newblock Neural ordinary differential equations.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{ding2022scaling}
Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding.
\newblock Scaling up your kernels to 31x31: Revisiting large kernel design in cnns.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 11963--11975, 2022.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{wang2020linformer}
Sinong Wang, Belinda~Z Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock {\em arXiv preprint arXiv:2006.04768}, 2020.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted windows.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 10012--10022, 2021.

\bibitem{dong2022cswin}
Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu~Yuan, Dong Chen, and Baining Guo.
\newblock Cswin transformer: A general vision transformer backbone with cross-shaped windows.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 12124--12134, 2022.

\bibitem{pan2022edgevits}
Junting Pan, Adrian Bulat, Fuwen Tan, Xiatian Zhu, Lukasz Dudziak, Hongsheng Li, Georgios Tzimiropoulos, and Brais Martinez.
\newblock Edgevits: Competing light-weight cnns on mobile devices with vision transformers.
\newblock In {\em European Conference on Computer Vision}, pages 294--311. Springer, 2022.

\bibitem{hatamizadeh2023global}
Ali Hatamizadeh, Hongxu Yin, Greg Heinrich, Jan Kautz, and Pavlo Molchanov.
\newblock Global context vision transformers.
\newblock In {\em International Conference on Machine Learning}, pages 12633--12646. PMLR, 2023.

\bibitem{sauer2023stylegan}
Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila.
\newblock Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis.
\newblock {\em arXiv preprint arXiv:2301.09515}, 2023.

\bibitem{yu2022scaling}
Jiahui Yu, Yuanzhong Xu, Jing~Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu~Karagol Ayan, et~al.
\newblock Scaling autoregressive models for content-rich text-to-image generation.
\newblock {\em arXiv preprint arXiv:2206.10789}, 2022.

\bibitem{gafni2022make}
Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman.
\newblock Make-a-scene: Scene-based text-to-image generation with human priors.
\newblock In {\em European Conference on Computer Vision}, pages 89--106. Springer, 2022.

\bibitem{dai2023emu}
Xiaoliang Dai, Ji~Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et~al.
\newblock Emu: Enhancing image generation models using photogenic needles in a haystack.
\newblock {\em arXiv preprint arXiv:2309.15807}, 2023.

\bibitem{sohl2015deep}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In {\em International Conference on Machine Learning}, pages 2256--2265. PMLR, 2015.

\bibitem{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{hong2023improving}
Susung Hong, Gyuseong Lee, Wooseok Jang, and Seungryong Kim.
\newblock Improving sample quality of diffusion models using self-attention guidance.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 7462--7471, 2023.

\bibitem{gu2023matryoshka}
Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Joshua~M Susskind, and Navdeep Jaitly.
\newblock Matryoshka diffusion models.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{chen2023gentron}
Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua.
\newblock Gentron: Delving deep into diffusion transformers for image and video generation.
\newblock {\em arXiv preprint arXiv:2312.04557}, 2023.

\bibitem{peebles2023scalable}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 4195--4205, 2023.

\bibitem{karras2023analyzing}
Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine.
\newblock Analyzing and improving the training dynamics of diffusion models.
\newblock {\em arXiv preprint arXiv:2312.02696}, 2023.

\bibitem{hatamizadeh2023diffit}
Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat.
\newblock Diffit: Diffusion vision transformers for image generation.
\newblock {\em arXiv preprint arXiv:2312.02139}, 2023.

\bibitem{feng2023ernie}
Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, Weichong Yin, Shikun Feng, et~al.
\newblock Ernie-vilg 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 10135--10145, 2023.

\bibitem{xue2024raphael}
Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu~Liu, and Ping Luo.
\newblock Raphael: Text-to-image generation via large mixture of diffusion paths.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{ma2023deepcache}
Xinyin Ma, Gongfan Fang, and Xinchao Wang.
\newblock Deepcache: Accelerating diffusion models for free.
\newblock {\em arXiv preprint arXiv:2312.00858}, 2023.

\bibitem{li2023distrifusion}
Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu, Kai Li, and Song Han.
\newblock Distrifusion: Distributed parallel inference for high-resolution diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2024.

\bibitem{wimbauer2023cache}
Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji~Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et~al.
\newblock Cache me if you can: Accelerating diffusion models through block caching.
\newblock {\em arXiv preprint arXiv:2312.03209}, 2023.

\bibitem{webster2023duplication}
Ryan Webster, Julien Rabin, Loic Simon, and Frederic Jurie.
\newblock On the de-duplication of laion-2b.
\newblock {\em arXiv preprint arXiv:2303.12733}, 2023.

\bibitem{gokaslan2023commoncanvas}
Aaron Gokaslan, A~Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel, Jonathan Frankle, Cory Stephenson, and Volodymyr Kuleshov.
\newblock Commoncanvas: An open diffusion model trained with creative-commons images.
\newblock {\em arXiv preprint arXiv:2310.16825}, 2023.

\bibitem{pixart}
Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li.
\newblock Pixart-$\alpha$: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023.

\bibitem{wrstchen}
Pablo Pernias, Dominic Rampas, Mats~Leon Richter, Christopher Pal, and Marc Aubreville.
\newblock W{\"u}rstchen: An efficient architecture for large-scale text-to-image diffusion models.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{pixartsigma}
Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li.
\newblock Pixart-$\backslash$sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation.
\newblock {\em arXiv preprint arXiv:2403.04692}, 2024.

\bibitem{ImageNetILSVRC15}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International journal of computer vision}, 115:211--252, 2015.

\bibitem{EfficientNet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural networks.
\newblock In {\em International conference on machine learning}, pages 6105--6114. PMLR, 2019.

\bibitem{Liu_2022_CVPR}
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
\newblock A convnet for the 2020s.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 11976--11986, June 2022.

\bibitem{MobilenetV3}
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo~Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et~al.
\newblock Searching for mobilenetv3.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 1314--1324, 2019.

\bibitem{hendrycks2016gelu}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock {\em arXiv preprint arXiv:1606.08415}, 2016.

\bibitem{vit22b-dehghani23a}
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas~Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et~al.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock In {\em International Conference on Machine Learning}, pages 7480--7512. PMLR, 2023.

\bibitem{touvron2022things}
Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Jakob Verbeek, and Hervé Jégou.
\newblock Three things everyone should know about vision transformers, 2022.

\bibitem{lin2023scaleaware}
Weifeng Lin, Ziheng Wu, Jiayu Chen, Jun Huang, and Lianwen Jin.
\newblock Scale-aware modulation meet transformer, 2023.

\bibitem{zhu2023biformer}
Lei Zhu, Xinjiang Wang, Zhanghan Ke, Wayne Zhang, and Rynson Lau.
\newblock Biformer: Vision transformer with bi-level routing attention.
\newblock {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2023.

\bibitem{fan2023rmt}
Qihang Fan, Huaibo Huang, Mingrui Chen, Hongmin Liu, and Ran He.
\newblock Rmt: Retentive networks meet vision transformers.
\newblock In {\em CVPR}, 2024.

\bibitem{iclr2024MogaNet}
Siyuan Li, Zedong Wang, Zicheng Liu, Cheng Tan, Haitao Lin, Di~Wu, Zhiyuan Chen, Jiangbin Zheng, and Stan~Z. Li.
\newblock Moganet: Multi-order gated aggregation network.
\newblock In {\em International Conference on Learning Representations}, 2024.

\bibitem{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{rezende2014stochastic}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep generative models.
\newblock In {\em International conference on machine learning}, pages 1278--1286. PMLR, 2014.

\bibitem{chung2024scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock {\em Journal of Machine Learning Research}, 25(70):1--53, 2024.

\bibitem{uvit}
Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu.
\newblock All are worth words: A vit backbone for diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 22669--22679, 2023.

\bibitem{rope}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock {\em Neurocomputing}, 568:127063, 2024.

\bibitem{rmsnorm}
Biao Zhang and Rico Sennrich.
\newblock Root mean square layer normalization.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{esser2024scaling}
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M{\"u}ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et~al.
\newblock Scaling rectified flow transformers for high-resolution image synthesis.
\newblock {\em arXiv preprint arXiv:2403.03206}, 2024.

\bibitem{pvt_v2_Wang_2022}
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.
\newblock Pvt v2: Improved baselines with pyramid vision transformer.
\newblock {\em Computational Visual Media}, 8(3):415–424, March 2022.

\bibitem{yang2023moat}
Chenglin Yang, Siyuan Qiao, Qihang Yu, Xiaoding Yuan, Yukun Zhu, Alan Yuille, Hartwig Adam, and Liang-Chieh Chen.
\newblock {MOAT}: Alternating mobile convolution and attention brings strong vision models.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{cai2022efficientvit}
Han Cai, Chuang Gan, and Song Han.
\newblock Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition.
\newblock {\em arXiv preprint arXiv:2205.14756}, 2022.

\bibitem{ridnik2021imagenet21k}
Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor.
\newblock Imagenet-21k pretraining for the masses, 2021.

\bibitem{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash equilibrium.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{cfg}
Jonathan Ho and Tim Salimans.
\newblock Classifier-free diffusion guidance.
\newblock {\em arXiv preprint arXiv:2207.12598}, 2022.

\bibitem{ghosh2023geneval}
Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt.
\newblock Geneval: An object-focused framework for evaluating text-to-image alignment, 2023.

\bibitem{RandAugment}
Ekin~Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le.
\newblock Randaugment: Practical automated data augmentation with a reduced search space.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin, editors, {\em Advances in Neural Information Processing Systems}, volume~33, pages 18613--18624. Curran Associates, Inc., 2020.

\bibitem{zhang2018mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N. Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{rw2019timm}
Ross Wightman.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem{brock2021high}
Andy Brock, Soham De, Samuel~L Smith, and Karen Simonyan.
\newblock High-performance large-scale image recognition without normalization.
\newblock In {\em International Conference on Machine Learning}, pages 1059--1071. PMLR, 2021.

\bibitem{kim2024densenets}
Donghyun Kim, Byeongho Heo, and Dongyoon Han.
\newblock Densenets reloaded: Paradigm shift beyond resnets and vits, 2024.

\bibitem{deitPaper}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through attention.
\newblock In {\em International conference on machine learning}, pages 10347--10357. PMLR, 2021.

\bibitem{ade20k}
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba.
\newblock Scene parsing through ade20k dataset.
\newblock In {\em 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 5122--5130, 2017.

\bibitem{mmseg2020}
MMSegmentation Contributors.
\newblock {MMSegmentation}: Openmmlab semantic segmentation toolbox and benchmark.
\newblock \url{https://github.com/open-mmlab/mmsegmentation}, 2020.

\bibitem{xiao2018unifiedUperNet}
Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun.
\newblock Unified perceptual parsing for scene understanding.
\newblock In {\em European Conference on Computer Vision}. Springer, 2018.

\bibitem{blip2_paper}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock {BLIP}-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, {\em Proceedings of the 40th International Conference on Machine Learning}, volume 202 of {\em Proceedings of Machine Learning Research}, pages 19730--19742. PMLR, 23--29 Jul 2023.

\end{thebibliography}
