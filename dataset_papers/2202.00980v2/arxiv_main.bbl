\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anil et~al.(2019)Anil, Gupta, Koren, and Singer]{anil2019memory}
Anil, R., Gupta, V., Koren, T., and Singer, Y.
\newblock Memory efficient adaptive optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Arora et~al.(2018)Arora, Li, and Lyu]{arora2018theoretical}
Arora, S., Li, Z., and Lyu, K.
\newblock Theoretical analysis of auto rate-tuning by batch normalization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Chen et~al.(2020)Chen, Wu, and Hong]{Chen20}
Chen, X., Wu, Z.~S., and Hong, M.
\newblock Understanding gradient clipping in private {SGD:} {A} geometric
  perspective.
\newblock \emph{CoRR}, abs/2006.15429, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.15429}.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,
  Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Cohen et~al.(2020)Cohen, Kaur, Li, Kolter, and
  Talwalkar]{cohen2020gradient}
Cohen, J., Kaur, S., Li, Y., Kolter, J.~Z., and Talwalkar, A.
\newblock Gradient descent on neural networks typically occurs at the edge of
  stability.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Hazan et~al.(2015)Hazan, Levy, and Shalev-Shwartz]{hazan2015beyond}
Hazan, E., Levy, K., and Shalev-Shwartz, S.
\newblock Beyond convexity: Stochastic quasi-convex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1594--1602, 2015.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016gaussian}
Hendrycks, D. and Gimpel, K.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Hoffer et~al.(2018)Hoffer, Banner, Golan, and Soudry]{hoffer2018norm}
Hoffer, E., Banner, R., Golan, I., and Soudry, D.
\newblock Norm matters: efficient and accurate normalization schemes in deep
  networks.
\newblock \emph{arXiv preprint arXiv:1803.01814}, 2018.

\bibitem[Huang et~al.(2017)Huang, Liu, Lang, and Li]{Huang2017ProjectionBW}
Huang, L., Liu, X., Lang, B., and Li, B.
\newblock Projection based weight normalization for deep neural networks.
\newblock \emph{ArXiv}, abs/1710.02338, 2017.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pp.\
  448--456. PMLR, 2015.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{3rd International Conference on Learning Representations,
  {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
  Proceedings}, 2015.

\bibitem[Levy(2016)]{levy2016power}
Levy, K.~Y.
\newblock The power of normalization: Faster evasion of saddle points.
\newblock \emph{arXiv preprint arXiv:1611.04831}, 2016.

\bibitem[Lewkowycz et~al.(2020)Lewkowycz, Bahri, Dyer, Sohl-Dickstein, and
  Gur-Ari]{lewkowycz2020large}
Lewkowycz, A., Bahri, Y., Dyer, E., Sohl-Dickstein, J., and Gur-Ari, G.
\newblock The large learning rate phase of deep learning: the catapult
  mechanism.
\newblock \emph{arXiv preprint arXiv:2003.02218}, 2020.

\bibitem[Li \& Arora(2019)Li and Arora]{li2019exponential}
Li, Z. and Arora, S.
\newblock An exponential learning rate schedule for deep learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Li et~al.(2020)Li, Lyu, and Arora]{li2020reconciling}
Li, Z., Lyu, K., and Arora, S.
\newblock Reconciling modern deep learning with traditional optimization
  analyses: The intrinsic learning rate.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Liu et~al.(2020)Liu, Liu, Gao, Chen, and Han]{Liu20}
Liu, L., Liu, X., Gao, J., Chen, W., and Han, J.
\newblock Understanding the difficulty of training transformers.
\newblock In Webber, B., Cohn, T., He, Y., and Liu, Y. (eds.),
  \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural
  Language Processing, {EMNLP} 2020, Online, November 16-20, 2020}, pp.\
  5747--5763. Association for Computational Linguistics, 2020.

\bibitem[Lobacheva et~al.(2021)Lobacheva, Kodryan, Chirkova, Malinin, and
  Vetrov]{lobacheva2021periodic}
Lobacheva, E., Kodryan, M., Chirkova, N., Malinin, A., and Vetrov, D.~P.
\newblock On the periodic behavior of neural network training with batch
  normalization and weight decay.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and Bengio]{pascanu13}
Pascanu, R., Mikolov, T., and Bengio, Y.
\newblock On the difficulty of training recurrent neural networks.
\newblock In Dasgupta, S. and McAllester, D. (eds.), \emph{Proceedings of the
  30th International Conference on Machine Learning}, volume~28 of
  \emph{Proceedings of Machine Learning Research}, pp.\  1310--1318, Atlanta,
  Georgia, USA, 17--19 Jun 2013. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v28/pascanu13.html}.

\bibitem[Polyak(1987)]{polyak1987introduction}
Polyak, B.~T.
\newblock Introduction to optimization. optimization software.
\newblock \emph{Inc., Publications Division, New York}, 1, 1987.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2019.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang]{rajpurkar2016squad}
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock \emph{arXiv preprint arXiv:1606.05250}, 2016.

\bibitem[Rajpurkar et~al.(2018)Rajpurkar, Jia, and Liang]{rajpurkar2018know}
Rajpurkar, P., Jia, R., and Liang, P.
\newblock Know what you don't know: Unanswerable questions for squad.
\newblock \emph{arXiv preprint arXiv:1806.03822}, 2018.

\bibitem[Reddi et~al.(2019)Reddi, Kale, and Kumar]{reddi2018convergence}
Reddi, S.~J., Kale, S., and Kumar, S.
\newblock On the convergence of {ADAM} and beyond.
\newblock \emph{arXiv preprint arXiv:1904.09237}, 2019.

\bibitem[Salimans \& Kingma(2016)Salimans and Kingma]{salimans2016weight}
Salimans, T. and Kingma, D.~P.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 901--909, 2016.

\bibitem[Shazeer \& Stern(2018)Shazeer and Stern]{shazeer2018adafactor}
Shazeer, N. and Stern, M.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4596--4604. PMLR, 2018.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{tieleman2012lecture}
Tieleman, T. and Hinton, G.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, 4\penalty0
  (2):\penalty0 26--31, 2012.

\bibitem[Ulyanov et~al.(2016)Ulyanov, Vedaldi, and
  Lempitsky]{ulyanov2016instance}
Ulyanov, D., Vedaldi, A., and Lempitsky, V.
\newblock Instance normalization: The missing ingredient for fast stylization.
\newblock \emph{arXiv preprint arXiv:1607.08022}, 2016.

\bibitem[van Handel(2016)]{vanprobability}
van Handel, R.
\newblock Probability in high dimension.
\newblock 2016.

\bibitem[Van~Laarhoven(2017)]{van2017l2}
Van~Laarhoven, T.
\newblock L2 regularization versus batch and weight normalization.
\newblock \emph{arXiv preprint arXiv:1706.05350}, 2017.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5998--6008, 2017.

\bibitem[Wan et~al.(2020)Wan, Zhu, Zhang, and Sun]{wan2020spherical}
Wan, R., Zhu, Z., Zhang, X., and Sun, J.
\newblock Spherical motion dynamics: Learning dynamics of neural network with
  normalization, weight decay, and sgd.
\newblock \emph{arXiv preprint arXiv:2006.08419}, 2020.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{N18-1101}
Williams, A., Nangia, N., and Bowman, S.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pp.\  1112--1122. Association for
  Computational Linguistics, 2018.

\bibitem[Wu \& He(2018)Wu and He]{wu2018group}
Wu, Y. and He, K.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pp.\  3--19, 2018.

\bibitem[You et~al.(2020)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{You2020lamb}
You, Y., Li, J., Reddi, S.~J., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X.,
  Demmel, J., Keutzer, K., and Hsieh, C.
\newblock Large batch optimization for deep learning: Training {BERT} in 76
  minutes.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.

\bibitem[Zhang et~al.(2018)Zhang, Wang, Xu, and Grosse]{zhang2018three}
Zhang, G., Wang, C., Xu, B., and Grosse, R.
\newblock Three mechanisms of weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, He, Sra, and
  Jadbabaie]{ZhangHSJ20}
Zhang, J., He, T., Sra, S., and Jadbabaie, A.
\newblock Why gradient clipping accelerates training: {A} theoretical
  justification for adaptivity.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net,
  2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Karimireddy, Veit, Kim, Reddi,
  Kumar, and Sra]{Zhang20}
Zhang, J., Karimireddy, S.~P., Veit, A., Kim, S., Reddi, S.~J., Kumar, S., and
  Sra, S.
\newblock Why are adaptive methods good for attention models?
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020{\natexlab{b}}.

\end{thebibliography}
