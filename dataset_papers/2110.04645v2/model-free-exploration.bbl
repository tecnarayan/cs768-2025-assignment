\begin{thebibliography}{}

\bibitem[Agarwal et~al., 2020]{agarwal2019optimality}
Agarwal, A., Kakade, S., and Yang, L.~F. (2020).
\newblock Model-based reinforcement learning with a generative model is minimax
  optimal.
\newblock {\em Conference on Learning Theory}, pages 67--83.

\bibitem[Agrawal and Jia, 2017]{agrawal2017posterior}
Agrawal, S. and Jia, R. (2017).
\newblock Posterior sampling for reinforcement learning: worst-case regret
  bounds.
\newblock {\em arXiv preprint arXiv:1705.07041}.

\bibitem[Auer et~al., 2002]{auer2002nonstochastic}
Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R.~E. (2002).
\newblock The nonstochastic multiarmed bandit problem.
\newblock {\em SIAM journal on computing}, 32(1):48--77.

\bibitem[Auer and Ortner, 2010]{auer2010ucb}
Auer, P. and Ortner, R. (2010).
\newblock {UCB} revisited: Improved regret bounds for the stochastic
  multi-armed bandit problem.
\newblock {\em Periodica Mathematica Hungarica}, 61(1-2):55--65.

\bibitem[Azar et~al., 2011]{ghavamzadeh2011speedy}
Azar, M.~G., Kappen, H.~J., Ghavamzadeh, M., and Munos, R. (2011).
\newblock Speedy {Q}-learning.
\newblock In {\em Advances in neural information processing systems}, pages
  2411--2419.

\bibitem[Azar et~al., 2013]{azar2013minimax}
Azar, M.~G., Munos, R., and Kappen, H.~J. (2013).
\newblock Minimax {PAC} bounds on the sample complexity of reinforcement
  learning with a generative model.
\newblock {\em Machine learning}, 91(3):325--349.

\bibitem[Azar et~al., 2017]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R. (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 263--272. JMLR. org.

\bibitem[Bai et~al., 2019]{bai2019provably}
Bai, Y., Xie, T., Jiang, N., and Wang, Y.-X. (2019).
\newblock Provably efficient $q$-learning with low switching cost.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8002--8011.

\bibitem[Bartlett and Tewari, 2009]{bartlett2009regal}
Bartlett, P. and Tewari, A. (2009).
\newblock Regal: a regularization based algorithm for reinforcement learning in
  weakly communicating {MDP}s.
\newblock In {\em Uncertainty in Artificial Intelligence: Proceedings of the
  25th Conference}, pages 35--42. AUAI Press.

\bibitem[Beck and Srikant, 2012]{beck2012error}
Beck, C.~L. and Srikant, R. (2012).
\newblock Error bounds for constant step-size {Q}-learning.
\newblock {\em Systems \& control letters}, 61(12):1203--1208.

\bibitem[Bertsekas, 2017]{bertsekas2017dynamic}
Bertsekas, D.~P. (2017).
\newblock {\em Dynamic programming and optimal control (4th edition)}.
\newblock Athena Scientific.

\bibitem[Chen et~al., 2020]{chen2020finite}
Chen, Z., Maguluri, S.~T., Shakkottai, S., and Shanmugam, K. (2020).
\newblock Finite-sample analysis of stochastic approximation using smooth
  convex envelopes.
\newblock {\em arXiv preprint arXiv:2002.00874}.

\bibitem[Chen et~al., 2021]{chen2021lyapunov}
Chen, Z., Maguluri, S.~T., Shakkottai, S., and Shanmugam, K. (2021).
\newblock A {L}yapunov theory for finite-sample guarantees of asynchronous
  {Q}-learning and {TD}-learning variants.
\newblock {\em arXiv preprint arXiv:2102.01567}.

\bibitem[Dann et~al., 2017]{dann2017unifying}
Dann, C., Lattimore, T., and Brunskill, E. (2017).
\newblock Unifying {PAC} and regret: Uniform {PAC} bounds for episodic
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:1703.07710}.

\bibitem[Domingues et~al., 2021]{domingues2021episodic}
Domingues, O.~D., M{\'e}nard, P., Kaufmann, E., and Valko, M. (2021).
\newblock Episodic reinforcement learning in finite {MDP}s: Minimax lower
  bounds revisited.
\newblock In {\em Algorithmic Learning Theory}, pages 578--598. PMLR.

\bibitem[Dong et~al., 2019]{dong2019q}
Dong, K., Wang, Y., Chen, X., and Wang, L. (2019).
\newblock {Q}-learning with {UCB} exploration is sample efficient for
  infinite-horizon {MDP}.
\newblock {\em arXiv preprint arXiv:1901.09311}.

\bibitem[Du et~al., 2017]{du2017stochastic}
Du, S.~S., Chen, J., Li, L., Xiao, L., and Zhou, D. (2017).
\newblock Stochastic variance reduction methods for policy evaluation.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1049--1058. JMLR. org.

\bibitem[Du et~al., 2020]{du2019good}
Du, S.~S., Kakade, S.~M., Wang, R., and Yang, L.~F. (2020).
\newblock Is a good representation sufficient for sample efficient
  reinforcement learning?
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Du et~al., 2019]{du2019provably}
Du, S.~S., Luo, Y., Wang, R., and Zhang, H. (2019).
\newblock Provably efficient {Q}-learning with function approximation via
  distribution shift error checking oracle.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8058--8068.

\bibitem[Efroni et~al., 2019]{efroni2019tight}
Efroni, Y., Merlis, N., Ghavamzadeh, M., and Mannor, S. (2019).
\newblock Tight regret bounds for model-based reinforcement learning with
  greedy policies.
\newblock {\em arXiv preprint arXiv:1905.11527}.

\bibitem[Even-Dar and Mansour, 2003]{even2003learning}
Even-Dar, E. and Mansour, Y. (2003).
\newblock Learning rates for {Q}-learning.
\newblock {\em Journal of machine learning Research}, 5(Dec):1--25.

\bibitem[Fan et~al., 2019]{fan2019theoretical}
Fan, J., Wang, Z., Xie, Y., and Yang, Z. (2019).
\newblock A theoretical analysis of deep {Q}-learning.
\newblock {\em arXiv e-prints}, pages arXiv--1901.

\bibitem[Freedman, 1975]{freedman1975tail}
Freedman, D.~A. (1975).
\newblock On tail probabilities for martingales.
\newblock {\em the Annals of Probability}, pages 100--118.

\bibitem[Gower et~al., 2020]{gower2020variance}
Gower, R.~M., Schmidt, M., Bach, F., and Richt{\'a}rik, P. (2020).
\newblock Variance-reduced methods for machine learning.
\newblock {\em Proceedings of the IEEE}, 108(11):1968--1983.

\bibitem[He et~al., 2020]{he2020nearly}
He, J., Zhou, D., and Gu, Q. (2020).
\newblock Nearly minimax optimal reinforcement learning for discounted {MDP}s.
\newblock {\em arXiv preprint arXiv:2010.00587}.

\bibitem[Jaakkola et~al., 1994]{jaakkola1994convergence}
Jaakkola, T., Jordan, M.~I., and Singh, S.~P. (1994).
\newblock Convergence of stochastic iterative dynamic programming algorithms.
\newblock In {\em Advances in neural information processing systems}, pages
  703--710.

\bibitem[Jafarnia-Jahromi et~al., 2020]{jafarnia2020model}
Jafarnia-Jahromi, M., Wei, C.-Y., Jain, R., and Luo, H. (2020).
\newblock A model-free learning algorithm for infinite-horizon average-reward
  {MDP}s with near-optimal regret.
\newblock {\em arXiv preprint arXiv:2006.04354}.

\bibitem[Jaksch et~al., 2010]{jaksch2010near}
Jaksch, T., Ortner, R., and Auer, P. (2010).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 11(4).

\bibitem[Jin et~al., 2018a]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I. (2018a).
\newblock Is {Q}-learning provably efficient?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4863--4873.

\bibitem[Jin et~al., 2018b]{jin2018qarxiv}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I. (2018b).
\newblock Is {Q}-learning provably efficient?
\newblock {\em arXiv preprint arXiv:1807.03765}.

\bibitem[Jin et~al., 2020]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I. (2020).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In {\em Conference on Learning Theory}, pages 2137--2143. PMLR.

\bibitem[Johnson and Zhang, 2013]{johnson2013accelerating}
Johnson, R. and Zhang, T. (2013).
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em Advances in neural information processing systems}, pages
  315--323.

\bibitem[Kakade, 2003]{kakade2003sample}
Kakade, S. (2003).
\newblock {\em On the sample complexity of reinforcement learning}.
\newblock PhD thesis, University of London.

\bibitem[Khamaru et~al., 2020]{khamaru2020temporal}
Khamaru, K., Pananjady, A., Ruan, F., Wainwright, M.~J., and Jordan, M.~I.
  (2020).
\newblock Is temporal difference learning optimal? an instance-dependent
  analysis.
\newblock {\em arXiv preprint arXiv:2003.07337}.

\bibitem[Lai and Robbins, 1985]{lai1985asymptotically}
Lai, T.~L. and Robbins, H. (1985).
\newblock Asymptotically efficient adaptive allocation rules.
\newblock {\em Advances in applied mathematics}, 6(1):4--22.

\bibitem[Lattimore and Szepesv{\'a}ri, 2020]{lattimore2020bandit}
Lattimore, T. and Szepesv{\'a}ri, C. (2020).
\newblock {\em Bandit algorithms}.
\newblock Cambridge University Press.

\bibitem[Li et~al., 2021a]{li2021tightening}
Li, G., Cai, C., Chen, Y., Gu, Y., Wei, Y., and Chi, Y. (2021a).
\newblock Is {Q}-learning minimax optimal? a tight sample complexity analysis.
\newblock {\em arXiv preprint arXiv:2102.06548}.

\bibitem[Li et~al., 2021b]{li2021sample}
Li, G., Chen, Y., Chi, Y., Gu, Y., and Wei, Y. (2021b).
\newblock Sample-efficient reinforcement learning is feasible for linearly
  realizable {MDP}s with limited revisiting.
\newblock {\em Advances in Neural Information Processing Systems},
  34:16671--16685.

\bibitem[Li et~al., 2020a]{li2020breaking}
Li, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y. (2020a).
\newblock Breaking the sample size barrier in model-based reinforcement
  learning with a generative model.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33.

\bibitem[Li et~al., 2020b]{li2020sample}
Li, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y. (2020b).
\newblock Sample complexity of asynchronous {Q}-learning: Sharper analysis and
  variance reduction.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)}.

\bibitem[Liu and Su, 2020]{liu2020gamma}
Liu, S. and Su, H. (2020).
\newblock $\gamma$-regret for non-episodic reinforcement learning.
\newblock {\em arXiv:2002.05138}.

\bibitem[Menard et~al., 2021]{menard2021ucb}
Menard, P., Domingues, O.~D., Shang, X., and Valko, M. (2021).
\newblock {UCB} momentum {Q}-learning: Correcting the bias without forgetting.
\newblock {\em arXiv preprint arXiv:2103.01312}.

\bibitem[Murphy, 2005]{murphy2005generalization}
Murphy, S. (2005).
\newblock A generalization error for {Q}-learning.
\newblock {\em Journal of Machine Learning Research}, 6:1073--1097.

\bibitem[Nguyen et~al., 2017]{nguyen2017sarah}
Nguyen, L.~M., Liu, J., Scheinberg, K., and Tak{\'a}{\v{c}}, M. (2017).
\newblock {SARAH}: A novel method for machine learning problems using
  stochastic recursive gradient.
\newblock In {\em International Conference on Machine Learning}, pages
  2613--2621. PMLR.

\bibitem[Osband and Van~Roy, 2016]{osband2016lower}
Osband, I. and Van~Roy, B. (2016).
\newblock On lower bounds for regret in reinforcement learning.
\newblock {\em arXiv preprint arXiv:1608.02732}.

\bibitem[Pacchiano et~al., 2020]{pacchiano2020optimism}
Pacchiano, A., Ball, P., Parker-Holder, J., Choromanski, K., and Roberts, S.
  (2020).
\newblock On optimism in model-based reinforcement learning.
\newblock {\em arXiv preprint arXiv:2006.11911}.

\bibitem[Puterman, 2014]{puterman2014markov}
Puterman, M.~L. (2014).
\newblock {\em Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons.

\bibitem[Qu and Wierman, 2020]{qu2020finite}
Qu, G. and Wierman, A. (2020).
\newblock Finite-time analysis of asynchronous stochastic approximation and
  {Q}-learning.
\newblock {\em Conference on Learning Theory}, pages 3185--3205.

\bibitem[Robbins and Monro, 1951]{robbins1951stochastic}
Robbins, H. and Monro, S. (1951).
\newblock A stochastic approximation method.
\newblock {\em The annals of mathematical statistics}, pages 400--407.

\bibitem[Shi et~al., 2022]{shi2022pessimistic}
Shi, L., Li, G., Wei, Y., Chen, Y., and Chi, Y. (2022).
\newblock Pessimistic {Q}-learning for offline reinforcement learning: Towards
  optimal sample complexity.
\newblock {\em arXiv preprint arXiv:2202.13890}.

\bibitem[Sidford et~al., 2018a]{sidford2018near}
Sidford, A., Wang, M., Wu, X., Yang, L., and Ye, Y. (2018a).
\newblock Near-optimal time and sample complexities for solving markov decision
  processes with a generative model.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5186--5196.

\bibitem[Sidford et~al., 2018b]{sidford2018variance}
Sidford, A., Wang, M., Wu, X., and Ye, Y. (2018b).
\newblock Variance reduced value iteration and faster algorithms for solving
  {M}arkov decision processes.
\newblock In {\em Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 770--787. SIAM.

\bibitem[Strehl et~al., 2006]{strehl2006pac}
Strehl, A.~L., Li, L., Wiewiora, E., Langford, J., and Littman, M.~L. (2006).
\newblock {PAC} model-free reinforcement learning.
\newblock In {\em Proceedings of the 23rd international conference on Machine
  learning}, pages 881--888.

\bibitem[Szepesv{\'a}ri, 1997]{szepesvari1997asymptotic}
Szepesv{\'a}ri, C. (1997).
\newblock The asymptotic convergence-rate of {Q}-learning.
\newblock In {\em NIPS}, volume~10, pages 1064--1070. Citeseer.

\bibitem[Tao, 2012]{Tao2012RMT}
Tao, T. (2012).
\newblock {\em Topics in Random Matrix Theory}.
\newblock Graduate Studies in Mathematics. American Mathematical Society,
  Providence, Rhode Island.

\bibitem[Tropp, 2011]{tropp2011freedman}
Tropp, J. (2011).
\newblock Freedman's inequality for matrix martingales.
\newblock {\em Electronic Communications in Probability}, 16:262--270.

\bibitem[Tsitsiklis, 1994]{tsitsiklis1994asynchronous}
Tsitsiklis, J.~N. (1994).
\newblock Asynchronous stochastic approximation and {Q}-learning.
\newblock {\em Machine learning}, 16(3):185--202.

\bibitem[Wai et~al., 2019]{wai2019variance}
Wai, H.-T., Hong, M., Yang, Z., Wang, Z., and Tang, K. (2019).
\newblock Variance reduced policy evaluation with smooth function
  approximation.
\newblock {\em Advances in Neural Information Processing Systems},
  32:5784--5795.

\bibitem[Wainwright, 2019a]{wainwright2019stochastic}
Wainwright, M.~J. (2019a).
\newblock Stochastic approximation with cone-contractive operators: Sharp
  $\ell_{\infty}$-bounds for {Q}-learning.
\newblock {\em arXiv preprint arXiv:1905.06265}.

\bibitem[Wainwright, 2019b]{wainwright2019variance}
Wainwright, M.~J. (2019b).
\newblock Variance-reduced {Q}-learning is minimax optimal.
\newblock {\em arXiv preprint arXiv:1906.04697}.

\bibitem[Wang et~al., 2021]{wang2021sample}
Wang, B., Yan, Y., and Fan, J. (2021).
\newblock Sample-efficient reinforcement learning for linearly-parameterized
  mdps with a generative model.
\newblock {\em Advances in Neural Information Processing Systems},
  34:23009--23022.

\bibitem[Watkins and Dayan, 1992]{watkins1992q}
Watkins, C.~J. and Dayan, P. (1992).
\newblock {Q}-learning.
\newblock {\em Machine learning}, 8(3-4):279--292.

\bibitem[Watkins, 1989]{watkins1989learning}
Watkins, C. J. C.~H. (1989).
\newblock Learning from delayed rewards.
\newblock {\em PhD thesis, King's College, University of Cambridge}.

\bibitem[Weng et~al., 2020]{weng2020momentum}
Weng, B., Xiong, H., Zhao, L., Liang, Y., and Zhang, W. (2020).
\newblock Momentum {Q}-learning with finite-sample convergence guarantee.
\newblock {\em arXiv preprint arXiv:2007.15418}.

\bibitem[Xiong et~al., 2020]{xiong2020finite}
Xiong, H., Zhao, L., Liang, Y., and Zhang, W. (2020).
\newblock Finite-time analysis for double {Q}-learning.
\newblock {\em Advances in Neural Information Processing Systems}, 33.

\bibitem[Xu et~al., 2019]{xu2019reanalysis}
Xu, T., Wang, Z., Zhou, Y., and Liang, Y. (2019).
\newblock Reanalysis of variance reduced temporal difference learning.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Yang et~al., 2021]{yang2021q}
Yang, K., Yang, L., and Du, S. (2021).
\newblock Q-learning with logarithmic regret.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1576--1584. PMLR.

\bibitem[Yin et~al., 2021]{yin2021near}
Yin, M., Bai, Y., and Wang, Y.-X. (2021).
\newblock Near-optimal offline reinforcement learning via double variance
  reduction.
\newblock {\em arXiv preprint arXiv:2102.01748}.

\bibitem[Zanette and Brunskill, 2019]{zanette2019tighter}
Zanette, A. and Brunskill, E. (2019).
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In {\em International Conference on Machine Learning}, pages
  7304--7312. PMLR.

\bibitem[Zhang et~al., 2020a]{zhang2020model_game}
Zhang, K., Kakade, S., Basar, T., and Yang, L. (2020a).
\newblock Model-based multi-agent {RL} in zero-sum {M}arkov games with
  near-optimal sample complexity.
\newblock {\em Advances in Neural Information Processing Systems}, 33.

\bibitem[Zhang et~al., 2020b]{zhang2020reinforcement}
Zhang, Z., Ji, X., and Du, S.~S. (2020b).
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock {\em arXiv preprint arXiv:2009.13503}.

\bibitem[Zhang et~al., 2020c]{zhang2020almost}
Zhang, Z., Zhou, Y., and Ji, X. (2020c).
\newblock Almost optimal model-free reinforcement learning via
  reference-advantage decomposition.
\newblock {\em Advances in Neural Information Processing Systems}, 33.

\bibitem[Zhang et~al., 2020d]{zhang2020model}
Zhang, Z., Zhou, Y., and Ji, X. (2020d).
\newblock Model-free reinforcement learning: from clipped pseudo-regret to
  sample complexity.
\newblock {\em arXiv preprint arXiv:2006.03864}.

\end{thebibliography}
