\begin{thebibliography}{10}

\bibitem{abel2017near}
David Abel, D.~Hershkowitz, and Michael Littman.
\newblock Near optimal behavior via approximate state abstraction.
\newblock In {\em Proceedings of the 33rd International Conference on
  International Conference on Machine Learning}, pages 2915--2923, 2016.

\bibitem{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 263--272. JMLR. org, 2017.

\bibitem{barto1995learning}
Andrew Barto, Steven Bradtke, and Satinder Singh.
\newblock Learning to act using real-time dynamic programming.
\newblock {\em Artificial intelligence}, 72(1-2):81--138, 1995.

\bibitem{bertsekas1996neuro}
D.~Bertsekas and J.~Tsitsiklis.
\newblock {\em Neuro-dynamic programming}.
\newblock Athena Scientific, 1996.

\bibitem{bonet2000planning}
Blai Bonet and Hector Geffner.
\newblock Planning with incomplete information as heuristic search in belief
  space.
\newblock In {\em Proceedings of the Fifth International Conference on
  Artificial Intelligence Planning Systems}, pages 52--61. AAAI Press, 2000.

\bibitem{bonet2003labeled}
Blai Bonet and Hector Geffner.
\newblock Labeled rtdp: Improving the convergence of real-time dynamic
  programming.
\newblock In {\em ICAPS}, volume~3, pages 12--21, 2003.

\bibitem{browne2012survey}
Cameron Browne, Edward Powley, Daniel Whitehouse, Simon Lucas, Peter Cowling,
  Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and
  Simon Colton.
\newblock A survey of {M}onte {C}arlo tree search methods.
\newblock {\em IEEE Transactions on Computational Intelligence and AI in
  games}, 4(1):1--43, 2012.

\bibitem{bulitko2006learning}
Vadim Bulitko and Greg Lee.
\newblock Learning in real-time search: A unifying framework.
\newblock {\em Journal of Artificial Intelligence Research}, 25:119--157, 2006.

\bibitem{coquelin2007bandit}
Pierre-Arnaud Coquelin and R{\'e}mi Munos.
\newblock Bandit algorithms for tree search.
\newblock {\em arXiv preprint cs/0703062}, 2007.

\bibitem{dann2017unifying}
Christoph Dann, Tor Lattimore, and Emma Brunskill.
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5713--5723, 2017.

\bibitem{dean1997model}
Thomas Dean, Robert Givan, and Sonia Leach.
\newblock Model reduction techniques for computing approximately optimal
  solutions for {M}arkov decision processes.
\newblock In {\em Proceedings of the 13th conference on Uncertainty in
  artificial intelligence}, pages 124--131, 1997.

\bibitem{dearden1997abstraction}
Richard Dearden and Craig Boutilier.
\newblock Abstraction and approximate decision-theoretic planning.
\newblock {\em Artificial Intelligence}, 89(1-2):219--283, 1997.

\bibitem{efroni2019combine}
Y.~Efroni, G.~Dalal, B.~Scherrer, and S.~Mannor.
\newblock How to combine tree-search methods in reinforcement learning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, pages 3494--3501, 2019.

\bibitem{efroni2018multiple}
Yonathan Efroni, Gal Dalal, Bruno Scherrer, and Shie Mannor.
\newblock Multiple-step greedy policies in approximate and online reinforcement
  learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5238--5247, 2018.

\bibitem{efroni2019tight}
Yonathan Efroni, Nadav Merlis, Mohammad Ghavamzadeh, and Shie Mannor.
\newblock Tight regret bounds for model-based reinforcement learning with
  greedy policies.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  12203--12213, 2019.

\bibitem{even2003approximate}
Eyal Even-Dar and Yishay Mansour.
\newblock Approximate equivalence of markov decision processes.
\newblock In {\em Learning Theory and Kernel Machines}, pages 581--594, 2003.

\bibitem{geist2013algorithmic}
Matthieu Geist and Olivier Pietquin.
\newblock Algorithmic survey of parametric value function approximation.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  24(6):845--867, 2013.

\bibitem{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is q-learning provably efficient?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4863--4873, 2018.

\bibitem{kearns2002sparse}
Michael Kearns, Yishay Mansour, and Andrew Ng.
\newblock A sparse sampling algorithm for near-optimal planning in large
  {M}arkov decision processes.
\newblock {\em Machine learning}, 49(2-3):193--208, 2002.

\bibitem{kearns2002near}
Michael Kearns and Satinder Singh.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock {\em Machine learning}, 49(2-3):209--232, 2002.

\bibitem{kocsis2006bandit}
Levente Kocsis and Csaba Szepesv{\'a}ri.
\newblock Bandit based {M}onte-{C}arlo planning.
\newblock In {\em European conference on machine learning}, pages 282--293,
  2006.

\bibitem{kolobov2012lrtdp}
Andrey Kolobov, Daniel~S Weld, et~al.
\newblock Lrtdp versus uct for online probabilistic planning.
\newblock In {\em Twenty-Sixth AAAI Conference on Artificial Intelligence},
  2012.

\bibitem{li2006towards}
L.~Li, T.~Walsh, and M.~Littman.
\newblock Towards a unified theory of state abstraction for {MDP}s.
\newblock In {\em Proceedings of the 9th International Symposium on Artificial
  Intelligence and Mathematics}, pages 531--539, 2006.

\bibitem{mcmahan2005bounded}
Brendan McMahan, Maxim Likhachev, and Geoffrey Gordon.
\newblock Bounded real-time dynamic programming: Rtdp with monotone upper
  bounds and performance guarantees.
\newblock In {\em Proceedings of the 22nd international conference on Machine
  learning}, pages 569--576. ACM, 2005.

\bibitem{munos2007performance}
R{\'e}mi Munos.
\newblock Performance bounds in l\_p-norm for approximate value iteration.
\newblock {\em SIAM journal on control and optimization}, 46(2):541--561, 2007.

\bibitem{munos2014bandits}
R{\'e}mi Munos.
\newblock From bandits to {M}onte-{C}arlo tree search: The optimistic principle
  applied to optimization and planning.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  7(1):1--129, 2014.

\bibitem{scherrer2012approximate}
Bruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon, and Matthieu Geist.
\newblock Approximate modified policy iteration.
\newblock In {\em Proceedings of the 29th International Conference on Machine
  Learning}, pages 1207--1214, 2012.

\bibitem{sidford2018variance}
Aaron Sidford, Mengdi Wang, Xian Wu, and Yinyu Ye.
\newblock Variance reduced value iteration and faster algorithms for solving
  {M}arkov decision processes.
\newblock In {\em Proceedings of the 29th Annual ACM-SIAM Symposium on Discrete
  Algorithms}, pages 770--787, 2018.

\bibitem{strehl2006pac}
A.~Strehl, L.~Li, and M.~Littman.
\newblock {PAC} reinforcement learning bounds for {RTDP} and rand-{RTDP}.
\newblock In {\em Proceedings of AAAI workshop on learning for search}, 2006.

\bibitem{strehl2009reinforcement}
Alexander Strehl, Lihong Li, and Michael Littman.
\newblock Reinforcement learning in finite {MDP}s: {PAC} analysis.
\newblock {\em Journal of Machine Learning Research}, 10(Nov):2413--2444, 2009.

\end{thebibliography}
