\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{PABM{\etalchar{+}}17}

\bibitem[AW80]{aitkin1980mixture}
Murray Aitkin and Granville~Tunnicliffe Wilson.
\newblock Mixture models, outliers, and the {EM} algorithm.
\newblock {\em Technometrics}, 22(3):325--331, 1980.

\bibitem[BGRS21]{barazandeh2021efficient}
Babak Barazandeh, Ali Ghafelebashi, Meisam Razaviyayn, and Ram Sriharsha.
\newblock Efficient algorithms for estimating the parameters of mixed linear
  regression models.
\newblock {\em arXiv preprint arXiv:2105.05953}, 2021.

\bibitem[BHK09]{banks2009cherry}
David~L Banks, Leanna House, and Kevin Killourhy.
\newblock Cherry-picking for complex data: robust structure discovery.
\newblock {\em Philosophical Transactions of the Royal Society A: Mathematical,
  Physical and Engineering Sciences}, 367(1906):4339--4359, 2009.

\bibitem[Bis06]{bishop2006pattern}
Christopher~M Bishop.
\newblock {\em Pattern recognition and machine learning}.
\newblock Springer, 2006.

\bibitem[BWY17]{balakrishnan2017statistical}
Sivaraman Balakrishnan, Martin~J Wainwright, and Bin Yu.
\newblock Statistical guarantees for the {EM} algorithm: From population to
  sample-based analysis.
\newblock {\em The Annals of Statistics}, 45(1):77--120, 2017.

\bibitem[CL13]{chaganty2013spectral}
Arun~Tejasvi Chaganty and Percy Liang.
\newblock Spectral experts for estimating mixtures of linear regressions.
\newblock In {\em International Conference on Machine Learning}, pages
  1040--1048. PMLR, 2013.

\bibitem[CLS20]{chen2020learning}
Sitan Chen, Jerry Li, and Zhao Song.
\newblock Learning mixtures of linear regressions in subexponential time via
  fourier moments.
\newblock In {\em Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory
  of Computing}, pages 587--600, 2020.

\bibitem[CM97]{chatterjee1997robust}
Samprit Chatterjee and Martin M{\"a}chler.
\newblock Robust regression: A weighted least squares approach.
\newblock {\em Communications in Statistics-Theory and Methods},
  26(6):1381--1394, 1997.

\bibitem[Coh80]{cohen1980influence}
Elizabeth~Ann Cohen.
\newblock {\em The influence of nonharmonic partials on tone perception}.
\newblock Stanford University, 1980.

\bibitem[CSCG07]{chai2007locally}
Xiujuan Chai, Shiguang Shan, Xilin Chen, and Wen Gao.
\newblock Locally linear regression for pose-invariant face recognition.
\newblock {\em IEEE Transactions on image processing}, 16(7):1716--1725, 2007.

\bibitem[CWZ{\etalchar{+}}21]{chang2021supervised}
Wennan Chang, Changlin Wan, Yong Zang, Chi Zhang, and Sha Cao.
\newblock Supervised clustering of high-dimensional data using regularized
  mixture modeling.
\newblock {\em Briefings in bioinformatics}, 22(4):bbaa291, 2021.

\bibitem[CYC14]{chen2014convex}
Yudong Chen, Xinyang Yi, and Constantine Caramanis.
\newblock A convex formulation for mixed regression with two components:
  Minimax optimal rates.
\newblock In {\em Conference on Learning Theory}, pages 560--604. PMLR, 2014.

\bibitem[DEF{\etalchar{+}}21]{diamandis2021wasserstein}
Theo Diamandis, Yonina Eldar, Alireza Fallah, Farzan Farnia, and Asuman
  Ozdaglar.
\newblock A wasserstein minimax framework for mixed linear regression.
\newblock In {\em International Conference on Machine Learning}, pages
  2697--2706. PMLR, 2021.

\bibitem[DH00]{deb2000estimates}
Partha Deb and Ann~M Holmes.
\newblock Estimates of use and costs of behavioural health care: a comparison
  of standard and finite mixture models.
\newblock {\em Health economics}, 9(6):475--489, 2000.

\bibitem[DTZ17]{daskalakis2017ten}
Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis.
\newblock Ten steps of {EM} suffice for mixtures of two gaussians.
\newblock In {\em Conference on Learning Theory}, pages 704--710. PMLR, 2017.

\bibitem[DV89]{de1989mixtures}
Richard~D De~Veaux.
\newblock Mixtures of linear regressions.
\newblock {\em Computational Statistics \& Data Analysis}, 8(3):227--245, 1989.

\bibitem[FS10]{faria2010fitting}
Susana Faria and Gilda Soromenho.
\newblock Fitting mixtures of linear regressions.
\newblock {\em Journal of Statistical Computation and Simulation},
  80(2):201--225, 2010.

\bibitem[GK20]{ghosh2020alternating}
Avishek Ghosh and Ramchandran Kannan.
\newblock Alternating minimization converges super-linearly for mixed linear
  regression.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1093--1103. PMLR, 2020.

\bibitem[GS99]{gaffney1999trajectory}
Scott Gaffney and Padhraic Smyth.
\newblock Trajectory clustering with mixtures of regression models.
\newblock In {\em Proceedings of the fifth ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 63--72, 1999.

\bibitem[HJ18]{hand2018convex}
Paul Hand and Babhru Joshi.
\newblock A convex program for mixed linear regression with a recovery
  guarantee for well-separated data.
\newblock {\em Information and Inference: A Journal of the IMA}, 7(3):563--579,
  2018.

\bibitem[HTFF09]{hastie2009elements}
Trevor Hastie, Robert Tibshirani, Jerome~H Friedman, and Jerome~H Friedman.
\newblock {\em The elements of statistical learning: data mining, inference,
  and prediction}, volume~2.
\newblock Springer, 2009.

\bibitem[Hub81]{huber1981robust}
PJ~Huber.
\newblock {\em Robust statistics.}
\newblock John Wiley and Sons, New York, 1981.

\bibitem[HW77]{holland1977robust}
Paul~W Holland and Roy~E Welsch.
\newblock Robust regression using iteratively reweighted least-squares.
\newblock {\em Communications in Statistics-theory and Methods}, 6(9):813--827,
  1977.

\bibitem[HY12a]{huang2012mixture}
Mian Huang and Weixin Yao.
\newblock Mixture of regression models with varying mixing proportions: a
  semiparametric approach.
\newblock {\em Journal of the American Statistical Association},
  107(498):711--724, 2012.

\bibitem[HY12b]{hunter2012semiparametric}
David~R Hunter and Derek~S Young.
\newblock Semiparametric mixtures of regressions.
\newblock {\em Journal of Nonparametric Statistics}, 24(1):19--38, 2012.

\bibitem[HYW17]{hu2017robust}
Hao Hu, Weixin Yao, and Yichao Wu.
\newblock The robust {EM}-type algorithms for log-concave mixtures of
  regression models.
\newblock {\em Computational statistics \& data analysis}, 111:14--26, 2017.

\bibitem[IMP14]{ingrassia2014model}
Salvatore Ingrassia, Simona~C Minotti, and Antonio Punzo.
\newblock Model-based clustering via linear cluster-weighted models.
\newblock {\em Computational Statistics \& Data Analysis}, 71:159--182, 2014.

\bibitem[JG21]{jiang2021nonparametric}
Hansheng Jiang and Adityanand Guntuboyina.
\newblock A nonparametric maximum likelihood approach to mixture of regression.
\newblock {\em arXiv preprint arXiv:2108.09816}, 2021.

\bibitem[KC07]{khalili2007variable}
Abbas Khalili and Jiahua Chen.
\newblock Variable selection in finite mixture of regression models.
\newblock {\em Journal of the american Statistical association},
  102(479):1025--1038, 2007.

\bibitem[KC20]{kwon2020converges}
Jeongyeol Kwon and Constantine Caramanis.
\newblock {EM} converges for a mixture of many linear regressions.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1727--1736. PMLR, 2020.

\bibitem[KHC21]{kwon2021minimax}
Jeongyeol Kwon, Nhat Ho, and Constantine Caramanis.
\newblock On the minimax optimality of the {EM} algorithm for learning
  two-component mixed linear regression.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1405--1413. PMLR, 2021.

\bibitem[KQC{\etalchar{+}}19]{kwon2019global}
Jeongyeol Kwon, Wei Qian, Constantine Caramanis, Yudong Chen, and Damek Davis.
\newblock Global convergence of the {EM} algorithm for mixtures of two
  component linear regression.
\newblock In {\em Conference on Learning Theory}, pages 2055--2110. PMLR, 2019.

\bibitem[KYB19]{klusowski2019estimating}
Jason~M Klusowski, Dana Yang, and WD~Brinda.
\newblock Estimating the coefficients of a mixture of two linear regressions by
  expectation maximization.
\newblock {\em IEEE Transactions on Information Theory}, 65(6):3515--3524,
  2019.

\bibitem[LL18]{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning mixtures of linear regressions with nearly optimal
  complexity.
\newblock In {\em Conference On Learning Theory}, pages 1125--1144. PMLR, 2018.

\bibitem[LSL19]{li2019drug}
Qianyun Li, Runmin Shi, and Faming Liang.
\newblock Drug sensitivity prediction with high-dimensional mixture regression.
\newblock {\em PloS one}, 14(2):e0212108, 2019.

\bibitem[MGJK19]{mukhoty2019globally}
Bhaskar Mukhoty, Govind Gopakumar, Prateek Jain, and Purushottam Kar.
\newblock Globally-convergent iteratively reweighted least squares for robust
  regression problems.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 313--322. PMLR, 2019.

\bibitem[MLR19]{mclachlan2019finite}
Geoffrey~J McLachlan, Sharon~X Lee, and Suren~I Rathnayake.
\newblock Finite mixture models.
\newblock {\em Annual review of statistics and its application}, 6:355--378,
  2019.

\bibitem[PABM{\etalchar{+}}17]{pimentel2017mixture}
Daniel Pimentel-Alarc{\'o}n, Laura Balzano, Roummel Marcia, Robert Nowak, and
  Rebecca Willett.
\newblock Mixture regression as subspace clustering.
\newblock In {\em 2017 International Conference on Sampling Theory and
  Applications (SampTA)}, pages 456--459. IEEE, 2017.

\bibitem[PMSG22]{pal2022learning}
Soumyabrata Pal, Arya Mazumdar, Rajat Sen, and Avishek Ghosh.
\newblock On learning mixture of linear regressions in the non-realizable
  setting.
\newblock In {\em International Conference on Machine Learning}, pages
  17202--17220. PMLR, 2022.

\bibitem[SJA16]{sedghi2016provable}
Hanie Sedghi, Majid Janzamin, and Anima Anandkumar.
\newblock Provable tensor methods for learning mixtures of generalized linear
  models.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1223--1231.
  PMLR, 2016.

\bibitem[SLF22]{sun2022robust}
Yifan Sun, Ziye Luo, and Xinyan Fan.
\newblock Robust structured heterogeneity analysis approach for
  high-dimensional data.
\newblock {\em Statistics in Medicine}, 2022.

\bibitem[SS19]{shen2019iterative}
Yanyao Shen and Sujay Sanghavi.
\newblock Iterative least trimmed squares for mixed linear regression.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[SYX14]{song2014robust}
Weixing Song, Weixin Yao, and Yanru Xing.
\newblock Robust mixture regression model fitting by laplace distribution.
\newblock {\em Computational Statistics \& Data Analysis}, 71:128--137, 2014.

\bibitem[Wil11]{wilcox2011introduction}
Rand Wilcox.
\newblock {\em Introduction to Robust Estimation and Hypothesis Testing}.
\newblock Elsevier, 2011.

\bibitem[WK00]{wedel2000market}
Michel Wedel and Wagner~A Kamakura.
\newblock {\em Market segmentation: Conceptual and methodological foundations}.
\newblock Springer Science \& Business Media, 2000.

\bibitem[YCS14]{yi2014alternating}
Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi.
\newblock Alternating minimization for mixed linear regression.
\newblock In {\em International Conference on Machine Learning}, pages
  613--621. PMLR, 2014.

\bibitem[YCS16]{yi2016solving}
Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi.
\newblock Solving a mixture of many random linear equations by tensor
  decomposition and alternating minimization.
\newblock {\em arXiv preprint arXiv:1608.05749}, 2016.

\bibitem[YH10]{young2010mixtures}
Derek~S Young and David~R Hunter.
\newblock Mixtures of regressions with predictor-dependent mixing proportions.
\newblock {\em Computational Statistics \& Data Analysis}, 54(10):2253--2266,
  2010.

\bibitem[YWY14]{yao2014robust}
Weixin Yao, Yan Wei, and Chun Yu.
\newblock Robust mixture regression using the t-distribution.
\newblock {\em Computational Statistics \& Data Analysis}, 71:116--127, 2014.

\bibitem[ZJD16]{zhong2016mixed}
Kai Zhong, Prateek Jain, and Inderjit~S Dhillon.
\newblock Mixed linear regression with multiple components.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem[ZMCL20]{zhang2020estimation}
Linjun Zhang, Rong Ma, T~Tony Cai, and Hongzhe Li.
\newblock Estimation, confidence intervals, and large-scale hypotheses testing
  for high-dimensional mixed linear regression.
\newblock {\em arXiv preprint arXiv:2011.03598}, 2020.

\end{thebibliography}
