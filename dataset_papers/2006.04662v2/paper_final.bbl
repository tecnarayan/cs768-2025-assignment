\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ben-David et~al.(2007)Ben-David, Blitzer, Crammer, and
  Pereira]{ben2007analysis}
S.~Ben-David, J.~Blitzer, K.~Crammer, and F.~Pereira.
\newblock Analysis of representations for domain adaptation.
\newblock In \emph{NeurIPS}, 2007.

\bibitem[Ben-Tal et~al.(2013)Ben-Tal, Den~Hertog, De~Waegenaere, Melenberg, and
  Rennen]{ben2013robust}
A.~Ben-Tal, D.~Den~Hertog, A.~De~Waegenaere, B.~Melenberg, and G.~Rennen.
\newblock Robust solutions of optimization problems affected by uncertain
  probabilities.
\newblock \emph{Management Science}, 59\penalty0 (2):\penalty0 341--357, 2013.

\bibitem[Borgwardt et~al.(2006)Borgwardt, Gretton, Rasch, Kriegel,
  Sch{\"o}lkopf, and Smola]{borgwardt2006integrating}
K.~M. Borgwardt, A.~Gretton, M.~J. Rasch, H.-P. Kriegel, B.~Sch{\"o}lkopf, and
  A.~J. Smola.
\newblock Integrating structured biological data by kernel maximum mean
  discrepancy.
\newblock \emph{Bioinformatics}, 22\penalty0 (14):\penalty0 e49--e57, 2006.

\bibitem[Buda et~al.(2018)Buda, Maki, and Mazurowski]{buda2018systematic}
M.~Buda, A.~Maki, and M.~A. Mazurowski.
\newblock A systematic study of the class imbalance problem in convolutional
  neural networks.
\newblock \emph{Neural Networks}, 106:\penalty0 249--259, 2018.

\bibitem[Byrd and Lipton(2019)]{byrd2018effect}
J.~Byrd and Z.~C. Lipton.
\newblock What is the effect of importance weighting in deep learning?
\newblock In \emph{ICML}, 2019.

\bibitem[Cao et~al.(2019)Cao, Wei, Gaidon, Arechiga, and Ma]{cao2019learning}
K.~Cao, C.~Wei, A.~Gaidon, N.~Arechiga, and T.~Ma.
\newblock Learning imbalanced datasets with label-distribution-aware margin
  loss.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Daume~III and Marcu(2006)]{daume2006domain}
H.~Daume~III and D.~Marcu.
\newblock Domain adaptation for statistical classifiers.
\newblock \emph{Journal of Artificial Intelligence Research}, 26:\penalty0
  101--126, 2006.

\bibitem[du~Plessis et~al.(2013)du~Plessis, Niu, and Sugiyama]{christo13taai}
M.~C. du~Plessis, G.~Niu, and M.~Sugiyama.
\newblock Clustering unclustered data: Unsupervised binary labeling of two
  datasets having different class balances.
\newblock In \emph{TAAI}, 2013.

\bibitem[Ganin et~al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle,
  Laviolette, March, and Lempitsky]{ganin2016domain}
Y.~Ganin, E.~Ustinova, H.~Ajakan, P.~Germain, H.~Larochelle, F.~Laviolette,
  M.~March, and V.~Lempitsky.
\newblock Domain-adversarial training of neural networks.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (59):\penalty0 1--35, 2016.

\bibitem[Ghifary et~al.(2017)Ghifary, Balduzzi, Kleijn, and
  Zhang]{ghifary2017scatter}
M.~Ghifary, D.~Balduzzi, W.~B. Kleijn, and M.~Zhang.
\newblock Scatter component analysis: A unified framework for domain adaptation
  and domain generalization.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 39\penalty0 (7):\penalty0 1414--1430, 2017.

\bibitem[Gong et~al.(2016)Gong, Zhang, Liu, Tao, Glymour, and
  Sch{\"o}lkopf]{gong2016domain}
M.~Gong, K.~Zhang, T.~Liu, D.~Tao, C.~Glymour, and B.~Sch{\"o}lkopf.
\newblock Domain adaptation with conditional transferable components.
\newblock In \emph{ICML}, 2016.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{goodfellow2016deep}
I.~Goodfellow, Y.~Bengio, and A.~Courville.
\newblock \emph{Deep learning}.
\newblock The MIT press, 2016.

\bibitem[Gretton et~al.(2012)Gretton, Borgwardt, Rasch, Sch{\"o}lkopf, and
  Smola]{gretton2012kernel}
A.~Gretton, K.~M. Borgwardt, M.~J. Rasch, B.~Sch{\"o}lkopf, and A.~Smola.
\newblock A kernel two-sample test.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (Mar):\penalty0 723--773, 2012.

\bibitem[Han et~al.(2018{\natexlab{a}})Han, Yao, Niu, Zhou, Tsang, Zhang, and
  Sugiyama]{han2018masking}
B.~Han, J.~Yao, G.~Niu, M.~Zhou, I.~Tsang, Y.~Zhang, and M.~Sugiyama.
\newblock Masking: A new perspective of noisy supervision.
\newblock In \emph{NeurIPS}, 2018{\natexlab{a}}.

\bibitem[Han et~al.(2018{\natexlab{b}})Han, Yao, Yu, Niu, Xu, Hu, Tsang, and
  Sugiyama]{han2018co}
B.~Han, Q.~Yao, X.~Yu, G.~Niu, M.~Xu, W.~Hu, I.~Tsang, and M.~Sugiyama.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock In \emph{NeurIPS}, 2018{\natexlab{b}}.

\bibitem[Han et~al.(2020)Han, Niu, Yu, Yao, Xu, Tsang, and
  Sugiyama]{han2020sigua}
B.~Han, G.~Niu, X.~Yu, Q.~Yao, M.~Xu, I.~W. Tsang, and M.~Sugiyama.
\newblock {SIGUA}: Forgetting may make learning with noisy labels more robust.
\newblock In \emph{ICML}, 2020.

\bibitem[He and Garcia(2009)]{he2009learning}
H.~He and E.~A. Garcia.
\newblock Learning from imbalanced data.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering},
  21\penalty0 (9):\penalty0 1263--1284, 2009.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Hu et~al.(2018)Hu, Niu, Sato, and Sugiyama]{hu2018does}
W.~Hu, G.~Niu, I.~Sato, and M.~Sugiyama.
\newblock Does distributionally robust supervised learning give robust
  classifiers?
\newblock In \emph{ICML}, 2018.

\bibitem[Huang et~al.(2016)Huang, Li, Change~Loy, and Tang]{huang2016learning}
C.~Huang, Y.~Li, C.~Change~Loy, and X.~Tang.
\newblock Learning deep representation for imbalanced classification.
\newblock In \emph{CVPR}, 2016.

\bibitem[Huang et~al.(2007)Huang, Gretton, Borgwardt, Sch{\"o}lkopf, and
  Smola]{huang2007correcting}
J.~Huang, A.~Gretton, K.~Borgwardt, B.~Sch{\"o}lkopf, and A.~Smola.
\newblock Correcting sample selection bias by unlabeled data.
\newblock In \emph{NeurIPS}, 2007.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{ICML}, 2015.

\bibitem[Japkowicz and Stephen(2002)]{japkowicz2002class}
N.~Japkowicz and S.~Stephen.
\newblock The class imbalance problem: A systematic study.
\newblock \emph{Intelligent data analysis}, 6\penalty0 (5):\penalty0 429--449,
  2002.

\bibitem[Jiang et~al.(2018)Jiang, Zhou, Leung, Li, and
  Fei-Fei]{jiang2017mentornet}
L.~Jiang, Z.~Zhou, T.~Leung, L.-J. Li, and L.~Fei-Fei.
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock In \emph{ICML}, 2018.

\bibitem[Kanamori et~al.(2009)Kanamori, Hido, and Sugiyama]{kanamori2009least}
T.~Kanamori, S.~Hido, and M.~Sugiyama.
\newblock A least-squares approach to direct importance estimation.
\newblock \emph{Journal of Machine Learning Research}, 10\penalty0
  (7):\penalty0 1391--1445, 2009.

\bibitem[Kingma and Ba(2015)]{kingma15iclr}
D.~P. Kingma and J.~L. Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lipton et~al.(2018)Lipton, Wang, and Smola]{lipton2018detecting}
Z.~C. Lipton, Y.-X. Wang, and A.~Smola.
\newblock Detecting and correcting for label shift with black box predictors.
\newblock In \emph{ICML}, 2018.

\bibitem[Liu and Tao(2016)]{liu2016classification}
T.~Liu and D.~Tao.
\newblock Classification with noisy labels by importance reweighting.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 38\penalty0 (3):\penalty0 447--461, 2016.

\bibitem[Lu et~al.(2019)Lu, Niu, Menon, and Sugiyama]{lu2018minimal}
N.~Lu, G.~Niu, A.~K. Menon, and M.~Sugiyama.
\newblock On the minimal supervision for training any binary classifier from
  only unlabeled data.
\newblock In \emph{ICLR}, 2019.

\bibitem[Lu et~al.(2020)Lu, Zhang, Niu, and Sugiyama]{lu2019mitigating}
N.~Lu, T.~Zhang, G.~Niu, and M.~Sugiyama.
\newblock Mitigating overfitting in supervised classification from two
  unlabeled datasets: A consistent risk correction approach.
\newblock In \emph{AISTATS}, 2020.

\bibitem[Maaten and Hinton(2008)]{maaten2008visualizing}
L.~v.~d. Maaten and G.~Hinton.
\newblock Visualizing data using t-sne.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0
  (Nov):\penalty0 2579--2605, 2008.

\bibitem[Menon et~al.(2015)Menon, Van~Rooyen, Ong, and
  Williamson]{menon2015learning}
A.~Menon, B.~Van~Rooyen, C.~S. Ong, and B.~Williamson.
\newblock Learning from corrupted binary labels via class-probability
  estimation.
\newblock In \emph{ICML}, 2015.

\bibitem[Namkoong and Duchi(2016)]{namkoong2016stochastic}
H.~Namkoong and J.~C. Duchi.
\newblock Stochastic gradient methods for distributionally robust optimization
  with f-divergences.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Namkoong and Duchi(2017)]{namkoong2017variance}
H.~Namkoong and J.~C. Duchi.
\newblock Variance-based regularization with convex objectives.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Natarajan et~al.(2013)Natarajan, Dhillon, Ravikumar, and
  Tewari]{natarajan2013learning}
N.~Natarajan, I.~S. Dhillon, P.~K. Ravikumar, and A.~Tewari.
\newblock Learning with noisy labels.
\newblock In \emph{NeurIPS}, 2013.

\bibitem[Pan and Yang(2009)]{pan2009survey}
S.~Pan and Q.~Yang.
\newblock A survey on transfer learning.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering},
  22\penalty0 (10):\penalty0 1345--1359, 2009.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Krishna~Menon, Nock, and
  Qu]{patrini2017making}
G.~Patrini, A.~Rozza, A.~Krishna~Menon, R.~Nock, and L.~Qu.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In \emph{CVPR}, 2017.

\bibitem[Quionero-Candela et~al.(2009)Quionero-Candela, Sugiyama, Schwaighofer,
  and Lawrence]{quionero2009dataset}
J.~Quionero-Candela, M.~Sugiyama, A.~Schwaighofer, and N.~Lawrence.
\newblock \emph{Dataset shift in machine learning}.
\newblock The MIT Press, 2009.

\bibitem[Ren et~al.(2018)Ren, Zeng, Yang, and Urtasun]{ren2018learning}
M.~Ren, W.~Zeng, B.~Yang, and R.~Urtasun.
\newblock Learning to reweight examples for robust deep learning.
\newblock In \emph{ICML}, 2018.

\bibitem[Robbins and Monro(1951)]{robbins1951stochastic}
H.~Robbins and S.~Monro.
\newblock A stochastic approximation method.
\newblock \emph{Annals of mathematical statistics}, pages 400--407, 1951.

\bibitem[Saito et~al.(2017)Saito, Ushiku, and Harada]{saito2017asymmetric}
K.~Saito, Y.~Ushiku, and T.~Harada.
\newblock Asymmetric tri-training for unsupervised domain adaptation.
\newblock In \emph{ICML}, 2017.

\bibitem[Sch\"{o}lkopf and Smola(2001)]{scholkopf01LK}
B.~Sch\"{o}lkopf and A.~Smola.
\newblock \emph{Learning with Kernels}.
\newblock The MIT Press, 2001.

\bibitem[Scott et~al.(2013)Scott, Blanchard, and
  Handy]{scott2013classification}
C.~Scott, G.~Blanchard, and G.~Handy.
\newblock Classification with asymmetric label noise: Consistency and maximal
  denoising.
\newblock In \emph{COLT}, 2013.

\bibitem[Shimodaira(2000)]{shimodaira2000improving}
H.~Shimodaira.
\newblock Improving predictive inference under covariate shift by weighting the
  log-likelihood function.
\newblock \emph{Journal of Statistical Planning and Inference}, 90\penalty0
  (2):\penalty0 227--244, 2000.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Sugiyama and Kawanabe(2012)]{sugiyama2012machine}
M.~Sugiyama and M.~Kawanabe.
\newblock \emph{Machine learning in non-stationary environments: Introduction
  to covariate shift adaptation}.
\newblock The MIT press, 2012.

\bibitem[Sugiyama et~al.(2007{\natexlab{a}})Sugiyama, Krauledat, and
  M{\"u}ller]{sugiyama2007covariate}
M.~Sugiyama, M.~Krauledat, and K.~M{\"u}ller.
\newblock Covariate shift adaptation by importance weighted cross validation.
\newblock \emph{Journal of Machine Learning Research}, 8\penalty0 (5):\penalty0
  985--1005, 2007{\natexlab{a}}.

\bibitem[Sugiyama et~al.(2007{\natexlab{b}})Sugiyama, Nakajima, Kashima,
  Buenau, and Kawanabe]{sugiyama2008direct}
M.~Sugiyama, S.~Nakajima, H.~Kashima, P.~Buenau, and M.~Kawanabe.
\newblock Direct importance estimation with model selection and its application
  to covariate shift adaptation.
\newblock In \emph{NeurIPS}, 2007{\natexlab{b}}.

\bibitem[Sugiyama et~al.(2008)Sugiyama, Suzuki, Nakajima, Kashima, von
  B{\"u}nau, and Kawanabe]{sugiyama2008direct1}
M.~Sugiyama, T.~Suzuki, S.~Nakajima, H.~Kashima, P.~von B{\"u}nau, and
  M.~Kawanabe.
\newblock Direct importance estimation for covariate shift adaptation.
\newblock \emph{Annals of the Institute of Statistical Mathematics},
  60\penalty0 (4):\penalty0 699--746, 2008.

\bibitem[Sugiyama et~al.(2012)Sugiyama, Suzuki, and
  Kanamori]{sugiyama2012density}
M.~Sugiyama, T.~Suzuki, and T.~Kanamori.
\newblock \emph{Density ratio estimation in machine learning}.
\newblock Cambridge University Press, 2012.

\bibitem[Van~Rooyen et~al.(2015)Van~Rooyen, Menon, and
  Williamson]{van2015learning}
B.~Van~Rooyen, A.~Menon, and R.~C. Williamson.
\newblock Learning with symmetric label noise: The importance of being
  unhinged.
\newblock In \emph{NeurIPS}, 2015.

\bibitem[Wen et~al.(2014)Wen, Yu, and Greiner]{wen2014robust}
J.~Wen, C.-N. Yu, and R.~Greiner.
\newblock Robust learning under uncertain test distributions: Relating
  covariate shift to model misspecification.
\newblock In \emph{ICML}, 2014.

\bibitem[Xia et~al.(2019)Xia, Liu, Wang, Han, Gong, Niu, and
  Sugiyama]{xia2019anchor}
X.~Xia, T.~Liu, N.~Wang, B.~Han, C.~Gong, G.~Niu, and M.~Sugiyama.
\newblock Are anchor points really indispensable in label-noise learning?
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Xia et~al.(2020)Xia, Liu, Han, Wang, Gong, Liu, Niu, Tao, and
  Sugiyama]{xia2020part}
X.~Xia, T.~Liu, B.~Han, N.~Wang, M.~Gong, H.~Liu, G.~Niu, D.~Tao, and
  M.~Sugiyama.
\newblock Part-dependent label noise: Towards instance-dependent label noise.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017}
H.~Xiao, K.~Rasul, and R.~Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747v2}, 2017.

\bibitem[Yao et~al.(2020{\natexlab{a}})Yao, Yang, Han, Niu, and
  Kwok]{yao2020icml}
Q.~Yao, H.~Yang, B.~Han, G.~Niu, and J.~T. Kwok.
\newblock Searching to exploit memorization effect in learning with noisy
  labels.
\newblock In \emph{ICML}, 2020{\natexlab{a}}.

\bibitem[Yao et~al.(2020{\natexlab{b}})Yao, Liu, Han, Gong, Deng, Niu, and
  Sugiyama]{yao2020dual}
Y.~Yao, T.~Liu, B.~Han, M.~Gong, J.~Deng, G.~Niu, and M.~Sugiyama.
\newblock Dual {T}: Reducing estimation error for transition matrix in
  label-noise learning.
\newblock In \emph{NeurIPS}, 2020{\natexlab{b}}.

\bibitem[Yu et~al.(2019)Yu, Han, Yao, Niu, Tsang, and Sugiyama]{yu2019does}
X.~Yu, B.~Han, J.~Yao, G.~Niu, I.~W. Tsang, and M.~Sugiyama.
\newblock How does disagreement help generalization against label corruption?
\newblock In \emph{ICML}, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{ICLR}, 2017.

\bibitem[Zhang et~al.(2013)Zhang, Sch{\"o}lkopf, Muandet, and
  Wang]{zhang2013domain}
K.~Zhang, B.~Sch{\"o}lkopf, K.~Muandet, and Z.~Wang.
\newblock Domain adaptation under target and conditional shift.
\newblock In \emph{ICML}, 2013.

\bibitem[Zhang et~al.(2020)Zhang, Yamane, Lu, and Sugiyama]{zhang2020one}
T.~Zhang, I.~Yamane, N.~Lu, and M.~Sugiyama.
\newblock A one-step approach to covariate shift adaptation.
\newblock In \emph{ACML}, 2020.

\end{thebibliography}
