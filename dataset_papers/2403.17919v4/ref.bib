@misc{diao2023active,
      title={Active Prompting with Chain-of-Thought for Large Language Models}, 
      author={Shizhe Diao and Pengcheng Wang and Yong Lin and Tong Zhang},
      year={2023},
      eprint={2302.12246},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{toma2023clinical,
  title={Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding},
  author={Toma, Augustin and Lawler, Patrick R and Ba, Jimmy and Krishnan, Rahul G and Rubin, Barry B and Wang, Bo},
  journal={arXiv preprint arXiv:2305.12031},
  year={2023}
}
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@misc{meng2024pissa,
      title={PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models}, 
      author={Fanxu Meng and Zhaohui Wang and Muhan Zhang},
      year={2024},
      eprint={2404.02948},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{luo2024badam,
      title={BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models}, 
      author={Qijun Luo and Hengxu Yu and Xiao Li},
      year={2024},
      eprint={2404.02827},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
song2024increasing,
title={Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning},
author={Haobo SONG and Hao Zhao and Soumajit Majumder and Tao Lin},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=H3IUunLy8s}
}

@misc{ji2024advlora,
      title={AdvLoRA: Adversarial Low-Rank Adaptation of Vision-Language Models}, 
      author={Yuheng Ji and Yue Liu and Zhicheng Zhang and Zhao Zhang and Yuting Zhao and Gang Zhou and Xingwei Zhang and Xinwang Liu and Xiaolong Zheng},
      year={2024},
      eprint={2404.13425},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Li2023ChatDoctorAM,
  title={ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge},
  author={Yunxiang Li and Zihan Li and Kai Zhang and Ruilong Dan and You Zhang},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.14070}
}

@article{fan2024not,
  title={Not all Layers of LLMs are Necessary during Inference},
  author={Fan, Siqi and Jiang, Xin and Li, Xiang and Meng, Xuying and Han, Peng and Shang, Shuo and Sun, Aixin and Wang, Yequan and Wang, Zhongyuan},
  journal={arXiv preprint arXiv:2403.02181},
  year={2024}
}
@article{chuang2023dola,
  title={DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models},
  author={Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and Kim, Yoon and Glass, James and He, Pengcheng},
  journal={arXiv preprint arXiv:2309.03883},
  year={2023},
}

@article{luo2023latent,
  title={Latent consistency models: Synthesizing high-resolution images with few-step inference},
  author={Luo, Simian and Tan, Yiqin and Huang, Longbo and Li, Jian and Zhao, Hang},
  journal={arXiv preprint arXiv:2310.04378},
  year={2023}
}
@article{Ho2020DenoisingDP,
  title={Denoising Diffusion Probabilistic Models},
  author={Jonathan Ho and Ajay Jain and P. Abbeel},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.11239},
  url={https://api.semanticscholar.org/CorpusID:219955663}
}
@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}


@misc{zhong2023agieval,
      title={AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models}, 
      author={Wanjun Zhong and Ruixiang Cui and Yiduo Guo and Yaobo Liang and Shuai Lu and Yanlin Wang and Amin Saied and Weizhu Chen and Nan Duan},
      year={2023},
      eprint={2304.06364},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@misc{paster2023openwebmath,
      title={OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text},
      author={Keiran Paster and Marco Dos Santos and Zhangir Azerbayev and Jimmy Ba},
      year={2023},
      eprint={2310.06786},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
@inproceedings{Liu2023RadiologyGPTAL,
  title={Radiology-GPT: A Large Language Model for Radiology},
  author={Zheng Liu and Aoxiao Zhong and Yiwei Li and Longtao Yang and Chao Ju and Zihao Wu and Chong Ma and Peng Shu and Cheng Chen and Sekeun Kim and Haixing Dai and Lin Zhao and Dajiang Zhu and Jun Liu and Wei Liu and Dinggang Shen and Xiang Li and Quanzheng Li and Tianming Liu},
  year={2023}
}
@article{zhang2023instruction,
  title={Instruction Tuning for Large Language Models: A Survey},
  author={Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others},
  journal={arXiv preprint arXiv:2308.10792},
  year={2023}
}

@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}
@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}
@article{Fedus2021SwitchTS,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={William Fedus and Barret Zoph and Noam M. Shazeer},
  journal={J. Mach. Learn. Res.},
  year={2021},
  volume={23},
  pages={120:1-120:39}
}
@article{Brown2020Language,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and T. J. Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.14165}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{jin2019pubmedqa,
  title={PubMedQA: A Dataset for Biomedical Research Question Answering},
  author={Jin, Qiao and Dhingra, Bhuwan and Liu, Zhengping and Cohen, William and Lu, Xinghua},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2567--2577},
  year={2019}
}

@misc{cobbe2021training,
      title={Training Verifiers to Solve Math Word Problems}, 
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{malladi2023finetuning,
	title        = {Fine-Tuning Language Models with Just Forward Passes},
	author       = {Sadhika Malladi and Tianyu Gao and Eshaan Nichani and Alex Damian and Jason D. Lee and Danqi Chen and Sanjeev Arora},
	year         = 2023,
	booktitle    = {Thirty-seventh Conference on Neural Information Processing Systems},
	url          = {https://openreview.net/forum?id=Vota6rFhBQ}
}
@article{hendrycksmath2021,
	title        = {Measuring Mathematical Problem Solving With the MATH Dataset},
	author       = {Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
	year         = 2021,
	journal      = {NeurIPS}
}
@inproceedings{paster2024openwebmath,
	title        = {OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text},
	author       = {Keiran Paster and Marco Dos Santos and Zhangir Azerbayev and Jimmy Ba},
	year         = 2024,
	booktitle    = {The Twelfth International Conference on Learning Representations}
}
@inproceedings{ouyang2022instructgpt,
	title        = {Training language models to follow instructions with human feedback},
	author       = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
	year         = 2022,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 35,
	pages        = {27730--27744},
	editor       = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh}
}
@article{Schulman2017ProximalPO,
	title        = {Proximal Policy Optimization Algorithms},
	author       = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
	year         = 2017,
	journal      = {ArXiv},
	volume       = {abs/1707.06347},
	url          = {https://api.semanticscholar.org/CorpusID:28695052}
}
@article{dong2023raft,
	title        = {{RAFT}: Reward rAnked FineTuning for Generative Foundation Model Alignment},
	author       = {Hanze Dong and Wei Xiong and Deepanshu Goyal and Yihan Zhang and Winnie Chow and Rui Pan and Shizhe Diao and Jipeng Zhang and KaShun SHUM and Tong Zhang},
	year         = 2023,
	journal      = {Transactions on Machine Learning Research},
	issn         = {2835-8856},
	url          = {https://openreview.net/forum?id=m7p5O7zblY},
	note         = {}
}
@inproceedings{dao2022flashattention,
	title        = {Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
	author       = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
	year         = 2022,
	booktitle    = {Advances in Neural Information Processing Systems}
}
@article{dao2023flashattention2,
	title        = {Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
	author       = {Dao, Tri},
	year         = 2023
}
@article{lee2020biobert,
	title        = {BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
	author       = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
	year         = 2020,
	journal      = {Bioinformatics},
	publisher    = {Oxford University Press},
	volume       = 36,
	number       = 4,
	pages        = {1234--1240}
}
@misc{huang2020clinicalbert,
	title        = {ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission},
	author       = {Kexin Huang and Jaan Altosaar and Rajesh Ranganath},
	year         = 2020,
	eprint       = {1904.05342},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{beltagy-etal-2019-scibert,
	title        = {{S}ci{BERT}: A Pretrained Language Model for Scientific Text},
	author       = {Beltagy, Iz  and Lo, Kyle  and Cohan, Arman},
	year         = 2019,
	month        = nov,
	booktitle    = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	publisher    = {Association for Computational Linguistics},
	address      = {Hong Kong, China},
	pages        = {3615--3620},
	doi          = {10.18653/v1/D19-1371},
	url          = {https://aclanthology.org/D19-1371},
	editor       = {Inui, Kentaro  and Jiang, Jing  and Ng, Vincent  and Wan, Xiaojun},
	abstract     = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at \url{https://github.com/allenai/scibert/}.}
}
@inproceedings{alsentzer-etal-2019-publicly,
	title        = {Publicly Available Clinical {BERT} Embeddings},
	author       = {Alsentzer, Emily  and Murphy, John  and Boag, William  and Weng, Wei-Hung  and Jindi, Di  and Naumann, Tristan  and McDermott, Matthew},
	year         = 2019,
	month        = jun,
	booktitle    = {Proceedings of the 2nd Clinical Natural Language Processing Workshop},
	publisher    = {Association for Computational Linguistics},
	address      = {Minneapolis, Minnesota, USA},
	pages        = {72--78},
	doi          = {10.18653/v1/W19-1909},
	url          = {https://aclanthology.org/W19-1909},
	editor       = {Rumshisky, Anna  and Roberts, Kirk  and Bethard, Steven  and Naumann, Tristan},
	abstract     = {Contextual word embedding models such as ELMo and BERT have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on 3/5 clinical NLP tasks, establishing a new state-of-the-art on the MedNLI dataset. We find that these domain-specific models are not as performant on 2 clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text.}
}
@misc{alpaca,
	title        = {Stanford Alpaca: An Instruction-following LLaMA model},
	author       = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
	year         = 2023,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}}
}
@misc{zheng2023judging,
	title        = {Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
	author       = {Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
	year         = 2023,
	eprint       = {2306.05685},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@online{noauthor_introducing_nodate,
	title        = {Introducing {ChatGPT}},
	author       = {OpenAI},
	url          = {https://openai.com/blog/chatgpt},
	urldate      = {2024-01-07},
	abstract     = {We’ve trained a model called {ChatGPT} which interacts in a conversational way. The dialogue format makes it possible for {ChatGPT} to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.},
	langid       = {american},
	file         = {Snapshot:/Users/xiangliu/Zotero/storage/9MDJ63DL/chatgpt.html:text/html}
}
@article{Vaswani_Shazeer_Parmar_Uszkoreit_Jones_Gomez_Kaiser_Polosukhin_2017,
	title        = {Attention is All you Need},
	author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, AidanN. and Kaiser, Lukasz and Polosukhin, Illia},
	year         = 2017,
	month        = {Jun},
	journal      = {Neural Information Processing Systems,Neural Information Processing Systems},
	language     = {en-US}
}
@article{radford2019gpt2,
	title        = {Language Models are Unsupervised Multitask Learners},
	author       = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year         = 2019
}
@article{raffel2020t5,
	title        = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	author       = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	year         = 2020,
	month        = {jan},
	journal      = {J. Mach. Learn. Res.},
	publisher    = {JMLR.org},
	volume       = 21,
	number       = 1,
	issn         = {1532-4435},
	issue_date   = {January 2020},
	abstract     = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	articleno    = 140,
	numpages     = 67,
	keywords     = {transfer learning, deep learning, multi-task learning, attention based models, natural language processing}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@inproceedings{biderman2023pythia,
	title        = {Pythia: A suite for analyzing large language models across training and scaling},
	author       = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
	year         = 2023,
	booktitle    = {International Conference on Machine Learning},
	pages        = {2397--2430},
	organization = {PMLR}
}
@inproceedings{howard-ruder-2018-universal,
	title        = {Universal Language Model Fine-tuning for Text Classification},
	author       = {Howard, Jeremy  and Ruder, Sebastian},
	year         = 2018,
	month        = jul,
	booktitle    = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Melbourne, Australia},
	pages        = {328--339},
	doi          = {10.18653/v1/P18-1031},
	url          = {https://aclanthology.org/P18-1031},
	editor       = {Gurevych, Iryna  and Miyao, Yusuke},
	abstract     = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.}
}
@misc{ren2021zerooffload,
	title        = {ZeRO-Offload: Democratizing Billion-Scale Model Training},
	author       = {Jie Ren and Samyam Rajbhandari and Reza Yazdani Aminabadi and Olatunji Ruwase and Shuangyan Yang and Minjia Zhang and Dong Li and Yuxiong He},
	year         = 2021,
	eprint       = {2101.06840},
	archiveprefix = {arXiv},
	primaryclass = {cs.DC}
}
@article{diao2023lmflow,
	title        = {Lmflow: An extensible toolkit for finetuning and inference of large foundation models},
	author       = {Diao, Shizhe and Pan, Rui and Dong, Hanze and Shum, Ka Shun and Zhang, Jipeng and Xiong, Wei and Zhang, Tong},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2306.12420}
}
@inproceedings{gururangan-etal-2020-dont,
	title        = {Don{'}t Stop Pretraining: Adapt Language Models to Domains and Tasks},
	author       = {Gururangan, Suchin  and Marasovi{\'c}, Ana  and Swayamdipta, Swabha  and Lo, Kyle  and Beltagy, Iz  and Downey, Doug  and Smith, Noah A.},
	year         = 2020,
	month        = jul,
	booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {8342--8360},
	doi          = {10.18653/v1/2020.acl-main.740},
	url          = {https://aclanthology.org/2020.acl-main.740},
	editor       = {Jurafsky, Dan  and Chai, Joyce  and Schluter, Natalie  and Tetreault, Joel},
	abstract     = {Language models pretrained on text from a wide variety of sources form the foundation of today{'}s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task{'}s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.}
}
@inproceedings{devlin-etal-2019-bert,
	title        = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author       = {Devlin, Jacob  and Chang, Ming-Wei  and Lee, Kenton  and Toutanova, Kristina},
	year         = 2019,
	month        = jun,
	booktitle    = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Minneapolis, Minnesota},
	pages        = {4171--4186},
	doi          = {10.18653/v1/N19-1423},
	url          = {https://aclanthology.org/N19-1423},
	editor       = {Burstein, Jill  and Doran, Christy  and Solorio, Thamar},
	abstract     = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}
@misc{liu2021autofreeze,
	title        = {AutoFreeze: Automatically Freezing Model Blocks to Accelerate Fine-tuning},
	author       = {Yuhan Liu and Saurabh Agarwal and Shivaram Venkataraman},
	year         = 2021,
	eprint       = {2102.01386},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{li2023textbooks,
	title        = {Textbooks Are All You Need II: phi-1.5 technical report},
	author       = {Yuanzhi Li and Sébastien Bubeck and Ronen Eldan and Allie Del Giorno and Suriya Gunasekar and Yin Tat Lee},
	year         = 2023,
	eprint       = {2309.05463},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{jiang2023mistral,
	title        = {Mistral 7B},
	author       = {Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
	year         = 2023,
	eprint       = {2310.06825},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{zhang2024tinyllama,
	title        = {TinyLlama: An Open-Source Small Language Model},
	author       = {Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu},
	year         = 2024,
	eprint       = {2401.02385},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inbook{Bengio_Lamblin_Popovici_Larochelle_2007,
	title        = {Greedy Layer-Wise Training of Deep Networks},
	author       = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
	year         = 2007,
	month        = {Sep},
	booktitle    = {Advances in Neural Information Processing Systems 19},
	pages        = {153–160},
	doi          = {10.7551/mitpress/7503.003.0024},
	url          = {http://dx.doi.org/10.7551/mitpress/7503.003.0024},
	language     = {en-US}
}
@article{Hinton_Osindero_Teh_2006,
	title        = {A Fast Learning Algorithm for Deep Belief Nets},
	author       = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
	year         = 2006,
	month        = {Jul},
	journal      = {Neural Computation},
	pages        = {1527–1554},
	doi          = {10.1162/neco.2006.18.7.1527},
	url          = {http://dx.doi.org/10.1162/neco.2006.18.7.1527},
	language     = {en-US}
}
@article{brocker2017freezeout,
	title        = {FreezeOut: Accelerate Training by Progressively Freezing Layers.},
	author       = {Brock, Andrew and Lim, Theodore and Ritchie, James M. and Weston, Nick},
	year         = 2017,
	journal      = {CoRR},
	volume       = {abs/1706.04983},
	url          = {http://dblp.uni-trier.de/db/journals/corr/corr1706.html#BrockLRW17},
	added-at     = {2018-08-13T00:00:00.000+0200},
	biburl       = {https://www.bibsonomy.org/bibtex/2503d852b0d196a234c758d369d157abc/dblp},
	ee           = {http://arxiv.org/abs/1706.04983},
	interhash    = {31fb3f71a0ab68c915b1f4a3e6651c5c},
	intrahash    = {503d852b0d196a234c758d369d157abc},
	keywords     = {dblp},
	timestamp    = {2018-08-14T12:52:50.000+0200}
}
@inproceedings{OpenBookQA2018,
	title        = {Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
	author       = {Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
	year         = 2018,
	booktitle    = {EMNLP}
}
@article{10.1145/3474381,
	title        = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
	author       = {Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
	year         = 2021,
	month        = {aug},
	journal      = {Commun. ACM},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 64,
	number       = 9,
	pages        = {99–106},
	doi          = {10.1145/3474381},
	issn         = {0001-0782},
	url          = {https://doi.org/10.1145/3474381},
	issue_date   = {September 2021},
	abstract     = {Commonsense reasoning remains a major challenge in AI, and yet, recent progresses on benchmarks may seem to suggest otherwise. In particular, the recent neural language models have reported above 90\% accuracy on the Winograd Schema Challenge (WSC), a commonsense benchmark originally designed to be unsolvable for statistical models that rely simply on word associations. This raises an important question---whether these models have truly acquired robust commonsense capabilities or they rely on spurious biases in the dataset that lead to an overestimation of the true capabilities of machine commonsense.To investigate this question, we introduce WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) large-scale crowdsourcing, followed by (2) systematic bias reduction using a novel AFLITE algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. Our experiments demonstrate that state-of-the-art models achieve considerably lower accuracy (59.4\%-79.1\%) on WINOGRANDE compared to humans (94\%), confirming that the high performance on the original WSC was inflated by spurious biases in the dataset.Furthermore, we report new state-of-the-art results on five related benchmarks with emphasis on their dual implications. On the one hand, they demonstrate the effectiveness of WINOGRANDE when used as a resource for transfer learning. On the other hand, the high performance on all these benchmarks suggests the extent to which spurious biases are prevalent in all such datasets, which motivates further research on algorithmic bias reduction.},
	numpages     = 8
}
@article{Bisk_Zellers_Le_bras_Gao_Choi_2020,
	title        = {PIQA: Reasoning about Physical Commonsense in Natural Language},
	author       = {Bisk, Yonatan and Zellers, Rowan and Le bras, Ronan and Gao, Jianfeng and Choi, Yejin},
	year         = 2020,
	month        = {Apr.},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 34,
	number       = {05},
	pages        = {7432--7439},
	doi          = {10.1609/aaai.v34i05.6239},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/6239},
	abstractnote = {&lt;p&gt;To apply eyeshadow without a brush, should I use a &lt;em&gt;cotton swab or a toothpick&lt;/em&gt;? Questions requiring this kind of &lt;strong&gt;physical commonsense&lt;/strong&gt; pose a challenge to today’s natural language understanding systems. While recent pretrained models (such as BERT) have made progress on question answering over more &lt;em&gt;abstract&lt;/em&gt; domains – such as news articles and encyclopedia entries, where text is plentiful – in more &lt;em&gt;physical&lt;/em&gt; domains, text is inherently limited due to reporting bias. Can AI systems learn to reliably answer physical commonsense questions without experiencing the physical world?&lt;/p&gt;&lt;p&gt;In this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset &lt;strong&gt;Physical Interaction: Question Answering&lt;/strong&gt; or &lt;strong&gt;PIQA&lt;/strong&gt;. Though humans find the dataset easy (95% accuracy), large pretrained models struggle (∼75%). We provide analysis about the dimensions of knowledge that existing models lack, which offers significant opportunities for future research.&lt;/p&gt;}
}
@inproceedings{zellers2019hellaswag,
	title        = {HellaSwag: Can a Machine Really Finish Your Sentence?},
	author       = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
	year         = 2019,
	booktitle    = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}
}
@article{Clark2018ThinkYH,
	title        = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
	author       = {Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
	year         = 2018,
	journal      = {ArXiv},
	volume       = {abs/1803.05457}
}
@inproceedings{clark-etal-2019-boolq,
	title        = {{B}ool{Q}: Exploring the Surprising Difficulty of Natural Yes/No Questions},
	author       = {Clark, Christopher  and Lee, Kenton  and Chang, Ming-Wei  and Kwiatkowski, Tom  and Collins, Michael  and Toutanova, Kristina},
	year         = 2019,
	month        = jun,
	booktitle    = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Minneapolis, Minnesota},
	pages        = {2924--2936},
	doi          = {10.18653/v1/N19-1300},
	url          = {https://aclanthology.org/N19-1300},
	editor       = {Burstein, Jill  and Doran, Christy  and Solorio, Thamar},
	abstract     = {In this paper we study yes/no questions that are naturally occurring {---} meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4{\%} accuracy compared to 90{\%} accuracy of human annotators (and 62{\%} majority-baseline), leaving a significant gap for future work.}
}
@inproceedings{li2023smartfrz,
	title        = {Smart{FRZ}: An Efficient Training Framework using Attention-Based Layer Freezing},
	author       = {Sheng Li and Geng Yuan and Yue Dai and Youtao Zhang and Yanzhi Wang and Xulong Tang},
	year         = 2023,
	booktitle    = {The Eleventh International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=i9UlAr1T_xl}
}
@inproceedings{lin-etal-2020-exploring,
	title        = {Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning},
	author       = {Lin, Zhaojiang  and Madotto, Andrea  and Fung, Pascale},
	year         = 2020,
	month        = nov,
	booktitle    = {Findings of the Association for Computational Linguistics: EMNLP 2020},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {441--459},
	doi          = {10.18653/v1/2020.findings-emnlp.41},
	url          = {https://aclanthology.org/2020.findings-emnlp.41},
	editor       = {Cohn, Trevor  and He, Yulan  and Liu, Yang},
	abstract     = {Fine-tuning pre-trained generative language models to down-stream language generation tasks has shown promising results. However, this comes with the cost of having a single, large model for each task, which is not ideal in low-memory/power scenarios (e.g., mobile). In this paper, we propose an effective way to fine-tune multiple down-stream generation tasks simultaneously using a single, large pretrained model. The experiments on five diverse language generation tasks show that by just using an additional 2-3{\%} parameters for each task, our model can maintain or even improve the performance of fine-tuning the whole model.}
}
@inproceedings{brown_language_2020,
	title        = {Language {Models} are {Few}-{Shot} {Learners}},
	author       = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year         = 2020,
	booktitle    = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher    = {Curran Associates, Inc.},
	volume       = 33,
	pages        = {1877--1901},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	editor       = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.}
}

@inproceedings{10.5555/3618408.3619590,
	title        = {Robust Speech Recognition via Large-Scale Weak Supervision},
	author       = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
	year         = 2023,
	booktitle    = {Proceedings of the 40th International Conference on Machine Learning},
	location     = {Honolulu, Hawaii, USA},
	publisher    = {JMLR.org},
	series       = {ICML'23},
	abstract     = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
	articleno    = 1182,
	numpages     = 27
}
@article{Chowdhery2022PaLMSL,
	title        = {PaLM: Scaling Language Modeling with Pathways},
	author       = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam M. Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Benton C. Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garc{\'i}a and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark D{\'i}az and Orhan Firat and Michele Catasta and Jason Wei and Kathleen S. Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
	year         = 2022,
	journal      = {J. Mach. Learn. Res.},
	volume       = 24,
	pages        = {240:1--240:113}
}
@article{peng2023instruction,
	title        = {Instruction Tuning with GPT-4},
	author       = {Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2304.03277}
}
@article{2019t5,
	title        = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	author       = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
	year         = 2019,
	journal      = {arXiv e-prints},
	archiveprefix = {arXiv},
	eprint       = {1910.10683}
}

@misc{lialin2023relora,
	title        = {ReLoRA: High-Rank Training Through Low-Rank Updates},
	author       = {Vladislav Lialin and Namrata Shivagunde and Sherin Muckatira and Anna Rumshisky},
	year         = 2023,
	eprint       = {2307.05695},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{rozière2023code,
	title        = {Code Llama: Open Foundation Models for Code},
	author       = {Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
	year         = 2023,
	eprint       = {2308.12950},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{houlsby2019peft,
	title        = {Parameter-efficient transfer learning for NLP},
	author       = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
	year         = 2019,
	booktitle    = {International Conference on Machine Learning},
	pages        = {2790--2799},
	organization = {PMLR}
}
@inproceedings{hu2022lora,
	title        = {Lo{RA}: Low-Rank Adaptation of Large Language Models},
	author       = {Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
	year         = 2022,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=nZeVKeeFYf9}
}
@misc{openai2023gpt4,
	title        = {GPT-4 Technical Report},
	author       = {OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mo Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
	year         = 2023,
	eprint       = {2303.08774},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@article{kloek1978bayesian,
	title        = {Bayesian estimates of equation system parameters: an application of integration by Monte Carlo},
	author       = {Kloek, Teun and Van Dijk, Herman K},
	year         = 1978,
	journal      = {Econometrica: Journal of the Econometric Society},
	publisher    = {JSTOR},
	pages        = {1--19}
}
@inproceedings{zhao2015stochastic,
	title        = {Stochastic optimization with importance sampling for regularized loss minimization},
	author       = {Zhao, Peilin and Zhang, Tong},
	year         = 2015,
	booktitle    = {international conference on machine learning},
	pages        = {1--9},
	organization = {PMLR}
}
@article{you2019lamb,
	title        = {Large batch optimization for deep learning: Training bert in 76 minutes},
	author       = {You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1904.00962}
}
@article{you2017lars,
	title        = {Large batch training of convolutional networks},
	author       = {You, Yang and Gitman, Igor and Ginsburg, Boris},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1708.03888}
}
@inproceedings{rajbhandari2020zero,
	title        = {Zero: Memory optimizations toward training trillion parameter models},
	author       = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
	year         = 2020,
	booktitle    = {SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
	pages        = {1--16},
	organization = {IEEE}
}
@inproceedings{rasley2020deepspeed,
	title        = {Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
	author       = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
	year         = 2020,
	booktitle    = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	pages        = {3505--3506}
}
@article{raffel2020exploring,
	title        = {Exploring the limits of transfer learning with a unified text-to-text transformer},
	author       = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
	year         = 2020,
	journal      = {The Journal of Machine Learning Research},
	publisher    = {JMLRORG},
	volume       = 21,
	number       = 1,
	pages        = {5485--5551}
}

@article{dettmers2023qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023}
}

@article{ding2022delta,
  title={Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models},
  author={Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
  journal={arXiv preprint arXiv:2203.06904},
  year={2022}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@article{workshop2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@article{falcon180b,
  title={The Falcon Series of Language Models: Towards Open Frontier Models},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Alhammadi, Maitha and Daniele, Mazzotta and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
  year={2023}
}

@inproceedings{hambardzumyan2021warp,
 address = {Online},
 author = {Hambardzumyan, Karen  and
Khachatrian, Hrant  and
May, Jonathan},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
 doi = {10.18653/v1/2021.acl-long.381},
 pages = {4921--4933},
 publisher = {Association for Computational Linguistics},
 title = {{WARP}: {W}ord-level {A}dversarial {R}e{P}rogramming},
 url = {https://aclanthology.org/2021.acl-long.381},
 year = {2021}
}

@inproceedings{zhong2021factual,
 address = {Online},
 author = {Zhong, Zexuan  and
Friedman, Dan  and
Chen, Danqi},
 booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 doi = {10.18653/v1/2021.naacl-main.398},
 pages = {5017--5033},
 publisher = {Association for Computational Linguistics},
 title = {Factual Probing Is [{MASK}]: Learning vs. Learning to Recall},
 url = {https://aclanthology.org/2021.naacl-main.398},
 year = {2021}
}

@article{han2021ptr,
 author = {Han, Xu and Zhao, Weilin and Ding, Ning and Liu, Zhiyuan and Sun, Maosong},
 journal = {ArXiv preprint},
 title = {{PTR: Prompt Tuning with Rules for Text Classification}},
 url = {https://arxiv.org/abs/2105.11259},
 volume = {abs/2105.11259},
 year = {2021}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@inproceedings{qin2021learning,
 address = {Online},
 author = {Qin, Guanghui  and
Eisner, Jason},
 booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 doi = {10.18653/v1/2021.naacl-main.410},
 pages = {5203--5212},
 publisher = {Association for Computational Linguistics},
 title = {Learning How to Ask: Querying {LM}s with Mixtures of Soft Prompts},
 url = {https://aclanthology.org/2021.naacl-main.410},
 year = {2021}
}

@article{liu2021gpt,
  title={GPT understands, too},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2103.10385},
  year={2021}
}

@article{diao2023mixture,
  title={Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models Memories},
  author={Diao, Shizhe and Xu, Tianyang and Xu, Ruijia and Wang, Jiawei and Zhang, Tong},
  journal={arXiv preprint arXiv:2306.05406},
  year={2023}
}

@article{gou2023mixture,
  title={Mixture of cluster-conditional lora experts for vision-language instruction tuning},
  author={Gou, Yunhao and Liu, Zhili and Chen, Kai and Hong, Lanqing and Xu, Hang and Li, Aoxue and Yeung, Dit-Yan and Kwok, James T and Zhang, Yu},
  journal={arXiv preprint arXiv:2312.12379},
  year={2023}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{reddi2019amsgrad,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}

@article{loshchilov2017adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@inproceedings{shum2023automatic,
  title={Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data},
  author={Shum, Kashun and Diao, Shizhe and Zhang, Tong},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={12113--12139},
  year={2023}
}

@article{liu2023sophia,
  title={Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training},
  author={Liu, Hong and Li, Zhiyuan and Hall, David and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2305.14342},
  year={2023}
}

@article{zhou2020randomized,
  title={A randomized block-coordinate adam online learning optimization algorithm},
  author={Zhou, Yangfan and Zhang, Mingchuan and Zhu, Junlong and Zheng, Ruijuan and Wu, Qingtao},
  journal={Neural Computing and Applications},
  volume={32},
  number={16},
  pages={12671--12684},
  year={2020},
  publisher={Springer}
}

@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{diao2022black,
  title={Black-Box Prompt Learning for Pre-trained Language Models},
  author={Diao, Shizhe and Huang, Zhichao and Xu, Ruijia and Li, Xuechun and Yong, LIN and Zhou, Xiao and Zhang, Tong},
  journal={Transactions on Machine Learning Research},
  year={2022}
}

@inproceedings{diao2021taming,
  title={Taming pre-trained language models with n-gram representations for low-resource domain adaptation},
  author={Diao, Shizhe and Xu, Ruijia and Su, Hongjin and Jiang, Yilei and Song, Yan and Zhang, Tong},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={3336--3349},
  year={2021}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{wangni2018gradient,
  title={Gradient sparsification for communication-efficient distributed optimization},
  author={Wangni, Jianqiao and Wang, Jialei and Liu, Ji and Zhang, Tong},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{zhao2024galore,
  title={GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}

@article{zhou2023randomized,
  title={Randomized block-coordinate adaptive algorithms for nonconvex optimization problems},
  author={Zhou, Yangfan and Huang, Kaizhu and Li, Jiang and Cheng, Cheng and Wang, Xuguang and Hussian, Amir and Liu, Xin},
  journal={Engineering Applications of Artificial Intelligence},
  volume={121},
  pages={105968},
  year={2023},
  publisher={Elsevier}
}

@article{JMLR:v15:srivastava14a:dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@inproceedings{cheng2016wide,
  title={Wide \& deep learning for recommender systems},
  author={Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and others},
  booktitle={Proceedings of the 1st workshop on deep learning for recommender systems},
  pages={7--10},
  year={2016}
}