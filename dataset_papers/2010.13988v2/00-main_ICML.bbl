\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abou-Moustafa \& Szepesv{\'a}ri(2019)Abou-Moustafa and
  Szepesv{\'a}ri]{abou2019exponential}
Karim Abou-Moustafa and Csaba Szepesv{\'a}ri.
\newblock An exponential efron-stein inequality for lq stable learning rules.
\newblock \emph{arXiv preprint arXiv:1903.05457}, 2019.

\bibitem[Bartlett \& Mendelson(2002)Bartlett and
  Mendelson]{bartlett2002rademacher}
Peter~L Bartlett and Shahar Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 463--482, 2002.

\bibitem[Bousquet \& Elisseeff(2002)Bousquet and
  Elisseeff]{bousquet2002stability}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock \emph{Journal of machine learning research}, 2\penalty0
  (Mar):\penalty0 499--526, 2002.

\bibitem[Bousquet et~al.(2020)Bousquet, Klochkov, and
  Zhivotovskiy]{bousquet2020sharper}
Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy.
\newblock Sharper bounds for uniformly stable algorithms.
\newblock In \emph{Conference on Learning Theory}, pp.\  610--626, 2020.

\bibitem[Chatterjee(2020)]{chatterjee2020coherent}
Satrajit Chatterjee.
\newblock Coherent gradients: An approach to understanding generalization in
  gradient descent-based optimization.
\newblock \emph{arXiv preprint arXiv:2002.10657}, 2020.

\bibitem[Chen et~al.(2020)Chen, He, and Su]{chen2020label}
Shuxiao Chen, Hangfeng He, and Weijie~J. Su.
\newblock Label-aware neural tangent kernel: Toward better generalization and
  local elasticity.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: {A} large-scale hierarchical image database.
\newblock In \emph{2009 IEEE Conference on Computer Vision and Pattern
  Recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Devroye \& Wagner(1979)Devroye and Wagner]{devroye1979distribution}
Luc Devroye and Terry Wagner.
\newblock Distribution-free inequalities for the deleted and holdout error
  estimates.
\newblock \emph{IEEE Transactions on Information Theory}, 25\penalty0
  (2):\penalty0 202--207, 1979.

\bibitem[Feldman \& Vondrak(2018)Feldman and
  Vondrak]{feldman2018generalization}
Vitaly Feldman and Jan Vondrak.
\newblock Generalization bounds for uniformly stable algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9747--9757, 2018.

\bibitem[Feldman \& Vondrak(2019)Feldman and Vondrak]{feldman2019high}
Vitaly Feldman and Jan Vondrak.
\newblock High probability generalization bounds for uniformly stable
  algorithms with nearly optimal rate.
\newblock In \emph{Conference on Learning Theory}, pp.\  1270--1279, 2019.

\bibitem[Fort et~al.(2019)Fort, Nowak, Jastrzebski, and
  Narayanan]{fort2019stiffness}
Stanislav Fort, Pawe{\l}~Krzysztof Nowak, Stanislaw Jastrzebski, and Srini
  Narayanan.
\newblock Stiffness: A new perspective on generalization in neural networks.
\newblock \emph{arXiv preprint arXiv:1901.09491}, 2019.

\bibitem[Hardt et~al.(2015)Hardt, Recht, and Singer]{hardt2015train}
M~Hardt, B~Recht, and Y~Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent. arxiv 2015.
\newblock \emph{arXiv preprint arXiv:1509.01240}, 2015.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
Moritz Hardt, Ben Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1225--1234. PMLR, 2016.

\bibitem[He \& Su(2020)He and Su]{he2020local}
Hangfeng He and Weijie~J. Su.
\newblock The local elasticity of neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Kearns \& Ron(1999)Kearns and Ron]{kearns1999algorithmic}
Michael Kearns and Dana Ron.
\newblock Algorithmic stability and sanity-check bounds for leave-one-out
  cross-validation.
\newblock \emph{Neural computation}, 11\penalty0 (6):\penalty0 1427--1453,
  1999.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2015adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{ICLR}, 2015.

\bibitem[Koh \& Liang(2017)Koh and Liang]{koh2017understanding}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1885--1894, 2017.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
A~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master?s thesis, Department of Computer Science, University of
  Toronto}, 2009.

\bibitem[Kutin \& Niyogi(2002)Kutin and Niyogi]{kutin2002almost}
Samuel Kutin and Partha Niyogi.
\newblock Almost-everywhere algorithmic stability and generalization error.
\newblock In \emph{Proceedings of the Eighteenth conference on Uncertainty in
  artificial intelligence}, pp.\  275--282, 2002.

\bibitem[Kutin \& Niyogi(2012)Kutin and Niyogi]{kutin2012almost}
Samuel Kutin and Partha Niyogi.
\newblock Almost-everywhere algorithmic stability and generalization error.
\newblock \emph{arXiv preprint arXiv:1301.0579}, 2012.

\bibitem[Kuzborskij \& Lampert(2018)Kuzborskij and Lampert]{kuzborskij2018data}
Ilja Kuzborskij and Christoph Lampert.
\newblock Data-dependent stability of stochastic gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2815--2824. PMLR, 2018.

\bibitem[Lei \& Ying(2020)Lei and Ying]{lei2020fine}
Yunwen Lei and Yiming Ying.
\newblock Fine-grained analysis of stability and generalization for stochastic
  gradient descent.
\newblock \emph{arXiv preprint arXiv:2006.08157}, 2020.

\bibitem[Lugosi \& Pawlak(1994)Lugosi and Pawlak]{lugosi1994posterior}
G{\'a}bor Lugosi and Miroslaw Pawlak.
\newblock On the posterior-probability estimate of the error rate of
  nonparametric classification rules.
\newblock \emph{IEEE Transactions on Information Theory}, 40\penalty0
  (2):\penalty0 475--481, 1994.

\bibitem[Madden et~al.(2020)Madden, Dall'Anese, and Becker]{madden2020high}
Liam Madden, Emiliano Dall'Anese, and Stephen Becker.
\newblock High probability convergence and uniform stability bounds for
  nonconvex stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:2006.05610}, 2020.

\bibitem[Mukherjee et~al.(2006)Mukherjee, Niyogi, Poggio, and
  Rifkin]{mukherjee2006learning}
Sayan Mukherjee, Partha Niyogi, Tomaso Poggio, and Ryan Rifkin.
\newblock Learning theory: stability is sufficient for generalization and
  necessary and sufficient for consistency of empirical risk minimization.
\newblock \emph{Advances in Computational Mathematics}, 25\penalty0
  (1-3):\penalty0 161--193, 2006.

\bibitem[Shalev-Shwartz et~al.(2010)Shalev-Shwartz, Shamir, Srebro, and
  Sridharan]{shalev2010learnability}
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan.
\newblock Learnability, stability and uniform convergence.
\newblock \emph{The Journal of Machine Learning Research}, 11:\penalty0
  2635--2670, 2010.

\bibitem[Vapnik(1979)]{vapnik1979estimation}
V~Vapnik.
\newblock Estimation of dependences based on empirical data nauka, 1979.

\bibitem[Vapnik(2013)]{vapnik2013nature}
Vladimir Vapnik.
\newblock \emph{The nature of statistical learning theory}.
\newblock Springer science \& business media, 2013.

\bibitem[Wainwright(2019)]{wainwright2019high}
Martin~J Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Zhu et~al.(2014)Zhu, Anguelov, and Ramanan]{zhu2014capturing}
Xiangxin Zhu, Dragomir Anguelov, and Deva Ramanan.
\newblock Capturing long-tail distributions of object subcategories.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  915--922, 2014.

\end{thebibliography}
