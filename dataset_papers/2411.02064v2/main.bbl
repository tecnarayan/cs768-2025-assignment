\begin{thebibliography}{}

\bibitem[Abbasnejad et~al., 2015]{abbasnejad2015loss}
Abbasnejad, E., Domke, J., and Sanner, S. (2015).
\newblock Loss-calibrated monte carlo action selection.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~29.

\bibitem[Arango et~al., 2021]{arango2021hpo}
Arango, S.~P., Jomaa, H.~S., Wistuba, M., and Grabocka, J. (2021).
\newblock Hpo-b: A large-scale reproducible benchmark for black-box hpo based on openml.
\newblock In {\em Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)}.

\bibitem[Balandat et~al., 2020]{balandat2020botorch}
Balandat, M., Karrer, B., Jiang, D., Daulton, S., Letham, B., Wilson, A.~G., and Bakshy, E. (2020).
\newblock Botorch: A framework for efficient monte-carlo bayesian optimization.
\newblock {\em Advances in neural information processing systems}, 33.

\bibitem[Berger, 2013]{berger2013statistical}
Berger, J.~O. (2013).
\newblock {\em Statistical decision theory and Bayesian analysis}.
\newblock Springer Science \& Business Media.

\bibitem[Bica et~al., 2021]{bica2021real}
Bica, I., Alaa, A.~M., Lambert, C., and Van Der~Schaar, M. (2021).
\newblock From real-world patient data to individualized treatment effects using machine learning: current and future methods to address underlying challenges.
\newblock {\em Clinical Pharmacology \& Therapeutics}, 109(1):87--100.

\bibitem[Bille, 2005]{bille2005survey}
Bille, P. (2005).
\newblock A survey on tree edit distance and related problems.
\newblock {\em Theoretical computer science}, 337(1-3):217--239.

\bibitem[Blacker et~al., 2011]{blacker2011pharmaceutical}
Blacker, A.~J., Williams, M.~T., and Williams, M.~T. (2011).
\newblock {\em Pharmaceutical process development: current chemical and engineering challenges}, volume~9.
\newblock Royal Society of Chemistry.

\bibitem[Blau et~al., 2023]{blau2023cross}
Blau, T., Bonilla, E., Chades, I., and Dezfouli, A. (2023).
\newblock Cross-entropy estimators for sequential experiment design with reinforcement learning.
\newblock {\em arXiv preprint arXiv:2305.18435}.

\bibitem[Blau et~al., 2022]{blau2022optimizing}
Blau, T., Bonilla, E.~V., Chades, I., and Dezfouli, A. (2022).
\newblock Optimizing sequential experimental design with deep reinforcement learning.
\newblock In {\em International conference on machine learning}, pages 2107--2128. PMLR.

\bibitem[Bruinsma et~al., 2023]{bruinsmaautoregressive}
Bruinsma, W., Markou, S., Requeima, J., Foong, A.~Y., Andersson, T., Vaughan, A., Buonomo, A., Hosking, S., and Turner, R.~E. (2023).
\newblock Autoregressive conditional neural processes.
\newblock In {\em The Eleventh International Conference on Learning Representations}.

\bibitem[Burger et~al., 2021]{burger2021sequentially}
Burger, M., Hauptmann, A., Helin, T., Hyv{\"o}nen, N., and Puska, J.-P. (2021).
\newblock Sequentially optimized projections in x-ray imaging.
\newblock {\em Inverse Problems}, 37(7):075006.

\bibitem[Chaloner and Verdinelli, 1995]{chaloner1995bayesian}
Chaloner, K. and Verdinelli, I. (1995).
\newblock Bayesian experimental design: A review.
\newblock {\em Statistical science}, pages 273--304.

\bibitem[Chang et~al., 2024]{chang2024amortized}
Chang, P.~E., Loka, N., Huang, D., Remes, U., Kaski, S., and Acerbi, L. (2024).
\newblock Amortized probabilistic conditioning for optimization, simulation and inference.
\newblock {\em arXiv preprint arXiv:2410.15320}.

\bibitem[Chen et~al., 2021]{chen2021decision}
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. (2021).
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock {\em Advances in neural information processing systems}, 34.

\bibitem[Cheng and Shen, 2005]{cheng2005bayesian}
Cheng, Y. and Shen, Y. (2005).
\newblock Bayesian adaptive designs for clinical trials.
\newblock {\em Biometrika}, 92(3):633--646.

\bibitem[Cobb et~al., 2018]{cobb2018loss}
Cobb, A.~D., Roberts, S.~J., and Gal, Y. (2018).
\newblock Loss-calibrated approximate inference in bayesian neural networks.
\newblock {\em arXiv preprint arXiv:1805.03901}.

\bibitem[Filstroff et~al., 2024]{filstroff2024targeted}
Filstroff, L., Sundin, I., Mikkola, P., Tiulpin, A., Kylm{\"a}oja, J., and Kaski, S. (2024).
\newblock Targeted active learning for bayesian decision-making.
\newblock {\em Transactions on Machine Learning Research}.

\bibitem[Foster et~al., 2021]{foster2021deep}
Foster, A., Ivanova, D.~R., Malik, I., and Rainforth, T. (2021).
\newblock Deep adaptive design: Amortizing sequential bayesian experimental design.
\newblock In {\em International Conference on Machine Learning}, pages 3384--3395. PMLR.

\bibitem[Foster et~al., 2019]{foster2019variational}
Foster, A., Jankowiak, M., Bingham, E., Horsfall, P., Teh, Y.~W., Rainforth, T., and Goodman, N. (2019).
\newblock Variational bayesian optimal experimental design.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Foster et~al., 2020]{foster2020unified}
Foster, A., Jankowiak, M., Oâ€™Meara, M., Teh, Y.~W., and Rainforth, T. (2020).
\newblock A unified stochastic gradient approach to designing bayesian-optimal experiments.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 2959--2969. PMLR.

\bibitem[Garnelo et~al., 2018]{garnelo2018conditional}
Garnelo, M., Rosenbaum, D., Maddison, C., Ramalho, T., Saxton, D., Shanahan, M., Teh, Y.~W., Rezende, D., and Eslami, S.~A. (2018).
\newblock Conditional neural processes.
\newblock In {\em International conference on machine learning}, pages 1704--1713. PMLR.

\bibitem[Garnett, 2023]{garnett2023bayesian}
Garnett, R. (2023).
\newblock {\em Bayesian optimization}.
\newblock Cambridge University Press.

\bibitem[Huang et~al., 2023a]{huang2024learning}
Huang, D., Bharti, A., Souza, A., Acerbi, L., and Kaski, S. (2023a).
\newblock Learning robust statistics for simulation-based inference under model misspecification.
\newblock {\em Advances in Neural Information Processing Systems}, 36.

\bibitem[Huang et~al., 2023b]{huang2023practical}
Huang, D., Haussmann, M., Remes, U., John, S., Clart{\'e}, G., Luck, K., Kaski, S., and Acerbi, L. (2023b).
\newblock Practical equivariances via relational conditional neural processes.
\newblock {\em Advances in Neural Information Processing Systems}, 36.

\bibitem[Ivanova et~al., 2021]{ivanova2021implicit}
Ivanova, D.~R., Foster, A., Kleinegesse, S., Gutmann, M.~U., and Rainforth, T. (2021).
\newblock Implicit deep adaptive design: Policy-based experimental design without likelihoods.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\bibitem[Ivanova et~al., 2024]{ivanova2024step}
Ivanova, D.~R., Hedman, M., Guan, C., and Rainforth, T. (2024).
\newblock {Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design}.
\newblock {\em ICLR 2024 Workshop on Data-centric Machine Learning Research (DMLR)}.

\bibitem[Ivanova et~al., 2023]{ivanova2023co}
Ivanova, D.~R., Jennings, J., Rainforth, T., Zhang, C., and Foster, A. (2023).
\newblock Co-bed: information-theoretic contextual optimization via bayesian experimental design.
\newblock In {\em International Conference on Machine Learning}. PMLR.

\bibitem[Kleinegesse and Gutmann, 2020]{kleinegesse2020bayesian}
Kleinegesse, S. and Gutmann, M.~U. (2020).
\newblock Bayesian experimental design for implicit models by mutual information neural estimation.
\newblock In {\em International conference on machine learning}, pages 5316--5326. PMLR.

\bibitem[Ku{\'s}mierczyk et~al., 2019]{kusmierczyk2019variational}
Ku{\'s}mierczyk, T., Sakaya, J., and Klami, A. (2019).
\newblock Variational bayesian decision-making for continuous utilities.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Lacoste-Julien et~al., 2011]{lacoste2011approximate}
Lacoste-Julien, S., Husz{\'a}r, F., and Ghahramani, Z. (2011).
\newblock Approximate inference for the loss-calibrated bayesian.
\newblock In {\em Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics}, pages 416--424. JMLR Workshop and Conference Proceedings.

\bibitem[Li, 2017]{li2017deep}
Li, Y. (2017).
\newblock Deep reinforcement learning: An overview.
\newblock {\em arXiv preprint arXiv:1701.07274}.

\bibitem[Lim et~al., 2022]{lim2022policy}
Lim, V., Novoseller, E., Ichnowski, J., Huang, H., and Goldberg, K. (2022).
\newblock Policy-based bayesian experimental design for non-differentiable implicit models.
\newblock {\em arXiv preprint arXiv:2203.04272}.

\bibitem[Lindley, 1956]{lindley1956measure}
Lindley, D.~V. (1956).
\newblock On a measure of the information provided by an experiment.
\newblock {\em The Annals of Mathematical Statistics}, 27(4):986--1005.

\bibitem[Lindley, 1972]{lindley1972bayesian}
Lindley, D.~V. (1972).
\newblock {\em Bayesian statistics: A review}.
\newblock SIAM.

\bibitem[Maraval et~al., 2024]{maraval2024end}
Maraval, A., Zimmer, M., Grosnit, A., and Bou~Ammar, H. (2024).
\newblock End-to-end meta-bayesian optimisation with transformer neural processes.
\newblock {\em Advances in Neural Information Processing Systems}, 36.

\bibitem[Markou et~al., 2022]{markoupractical}
Markou, S., Requeima, J., Bruinsma, W., Vaughan, A., and Turner, R.~E. (2022).
\newblock Practical conditional neural process via tractable dependent predictions.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Mo et~al., 2021]{mo2021evaluating}
Mo, Y., Guan, Y., Verma, P., Guo, J., Fortunato, M.~E., Lu, Z., Coley, C.~W., and Jensen, K.~F. (2021).
\newblock Evaluating and clustering retrosynthesis pathways with learned strategy.
\newblock {\em Chemical science}, 12(4):1469--1478.

\bibitem[Morais and Pillow, 2022]{morais2022loss}
Morais, M.~J. and Pillow, J.~W. (2022).
\newblock Loss-calibrated expectation propagation for approximate bayesian decision-making.
\newblock {\em arXiv preprint arXiv:2201.03128}.

\bibitem[M{\"u}ller et~al., 2023]{muller2023pfns4bo}
M{\"u}ller, S., Feurer, M., Hollmann, N., and Hutter, F. (2023).
\newblock Pfns4bo: In-context learning for bayesian optimization.
\newblock In {\em International Conference on Machine Learning}, pages 25444--25470. PMLR.

\bibitem[M{\"u}ller et~al., 2021]{mullertransformers}
M{\"u}ller, S., Hollmann, N., Arango, S.~P., Grabocka, J., and Hutter, F. (2021).
\newblock Transformers can do bayesian inference.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Neiswanger et~al., 2021]{neiswanger2021bayesian}
Neiswanger, W., Wang, K.~A., and Ermon, S. (2021).
\newblock Bayesian algorithm execution: Estimating computable properties of black-box functions using mutual information.
\newblock In {\em International Conference on Machine Learning}, pages 8005--8015. PMLR.

\bibitem[Neiswanger et~al., 2022]{neiswanger2022generalizing}
Neiswanger, W., Yu, L., Zhao, S., Meng, C., and Ermon, S. (2022).
\newblock Generalizing bayesian optimization with decision-theoretic entropies.
\newblock {\em Advances in Neural Information Processing Systems}, 35.

\bibitem[Nguyen and Grover, 2022]{nguyen2022transformer}
Nguyen, T. and Grover, A. (2022).
\newblock Transformer neural processes: Uncertainty-aware meta learning via sequence modeling.
\newblock In {\em International Conference on Machine Learning}, pages 16569--16594. PMLR.

\bibitem[Parzen, 1999]{parzen1999stochastic}
Parzen, E. (1999).
\newblock {\em Stochastic processes}.
\newblock SIAM.

\bibitem[Paszke et~al., 2019]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et~al. (2019).
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Advances in neural information processing systems}, 32.

\bibitem[Rainforth et~al., 2018]{rainforth2018nesting}
Rainforth, T., Cornish, R., Yang, H., Warrington, A., and Wood, F. (2018).
\newblock On nesting monte carlo estimators.
\newblock In {\em International Conference on Machine Learning}, pages 4267--4276. PMLR.

\bibitem[Rainforth et~al., 2024]{rainforth2024modern}
Rainforth, T., Foster, A., Ivanova, D.~R., and Bickford~Smith, F. (2024).
\newblock Modern bayesian experimental design.
\newblock {\em Statistical Science}, 39(1):100--114.

\bibitem[Rasmussen and Williams, 2006]{rasmussen2006gaussian}
Rasmussen, C.~E. and Williams, C.~K. (2006).
\newblock {\em Gaussian Processes for Machine Learning}.
\newblock MIT Press.

\bibitem[Ryan et~al., 2016]{ryan2016review}
Ryan, E.~G., Drovandi, C.~C., McGree, J.~M., and Pettitt, A.~N. (2016).
\newblock A review of modern computational algorithms for bayesian optimal design.
\newblock {\em International Statistical Review}, 84(1):128--154.

\bibitem[Schulman et~al., 2017]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017).
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}.

\bibitem[Smith et~al., 2023]{smith2023prediction}
Smith, F.~B., Kirsch, A., Farquhar, S., Gal, Y., Foster, A., and Rainforth, T. (2023).
\newblock Prediction-oriented bayesian active learning.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 7331--7348. PMLR.

\bibitem[Stevens, 2011]{stevens2011progress}
Stevens, S.~J. (2011).
\newblock {\em Progress toward the synthesis of providencin}.
\newblock PhD thesis, Colorado State University.

\bibitem[Sundin et~al., 2018]{sundin2018improving}
Sundin, I., Peltola, T., Micallef, L., Afrabandpey, H., Soare, M., Mamun~Majumder, M., Daee, P., He, C., Serim, B., Havulinna, A., et~al. (2018).
\newblock Improving genomics-based predictions for precision medicine through active elicitation of expert knowledge.
\newblock {\em Bioinformatics}, 34(13):i395--i403.

\bibitem[Szymku{\'c} et~al., 2016]{szymkuc2016computer}
Szymku{\'c}, S., Gajewska, E.~P., Klucznik, T., Molga, K., Dittwald, P., Startek, M., Bajczyk, M., and Grzybowski, B.~A. (2016).
\newblock Computer-assisted synthetic planning: the end of the beginning.
\newblock {\em Angewandte Chemie International Edition}, 55(20):5904--5937.

\bibitem[Vadera et~al., 2021]{vadera2021post}
Vadera, M.~P., Ghosh, S., Ng, K., and Marlin, B.~M. (2021).
\newblock Post-hoc loss-calibration for bayesian neural networks.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 1403--1412. PMLR.

\bibitem[Vaswani et~al., 2017]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I. (2017).
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30.

\bibitem[Williams, 1992]{williams1992simple}
Williams, R.~J. (1992).
\newblock Simple statistical gradient-following algorithms for connectionist reinforcement learning.
\newblock {\em Machine learning}, 8:229--256.

\bibitem[Zheng et~al., 2022]{zheng2022online}
Zheng, Q., Zhang, A., and Grover, A. (2022).
\newblock Online decision transformer.
\newblock In {\em international conference on machine learning}. PMLR.

\bibitem[Zhong et~al., 2024]{zhong2024goal}
Zhong, S., Shen, W., Catanach, T., and Huan, X. (2024).
\newblock Goal-oriented bayesian optimal experimental design for nonlinear models using markov chain monte carlo.
\newblock {\em arXiv preprint arXiv:2403.18072}.

\end{thebibliography}
