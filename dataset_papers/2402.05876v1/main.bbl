\begin{thebibliography}{}

\bibitem[Bai et~al., 2019]{bai2019provably}
Bai, Y., Xie, T., Jiang, N., and Wang, Y.-X. (2019).
\newblock Provably efficient {Q}-learning with low switching cost.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32.

\bibitem[Beck and Srikant, 2012]{beck2012error}
Beck, C.~L. and Srikant, R. (2012).
\newblock Error bounds for constant step-size {Q}-learning.
\newblock {\em Systems \& control letters}, 61(12):1203--1208.

\bibitem[Bertsekas, 2017]{bertsekas2017dynamic}
Bertsekas, D.~P. (2017).
\newblock {\em Dynamic programming and optimal control (4th edition)}.
\newblock Athena Scientific.

\bibitem[Chen et~al., 2019]{chen2019performance}
Chen, Z., Zhang, S., Doan, T.~T., Maguluri, S.~T., and Clarke, J.-P. (2019).
\newblock Performance of {Q}-learning with linear function approximation:
  Stability and finite-time analysis.
\newblock {\em arXiv preprint arXiv:1905.11425}.

\bibitem[Even-Dar and Mansour, 2003]{even2003learning}
Even-Dar, E. and Mansour, Y. (2003).
\newblock Learning rates for {Q}-learning.
\newblock {\em Journal of machine learning Research}, 5(Dec):1--25.

\bibitem[Fan et~al., 2020]{fan2019theoretical}
Fan, J., Wang, Z., Xie, Y., and Yang, Z. (2020).
\newblock A theoretical analysis of deep {Q}-learning.
\newblock In {\em Learning for Dynamics and Control}, pages 486--489. PMLR.

\bibitem[Fan et~al., 2021]{fan2021fedrlfaulttolerant}
Fan, X., Ma, Y., Dai, Z., Jing, W., Tan, C., and Low, B. K.~H. (2021).
\newblock Fault-tolerant federated reinforcement learning with theoretical
  guarantee.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~34, pages 1007--1021.

\bibitem[Freedman, 1975]{freedman1975tail}
Freedman, D.~A. (1975).
\newblock On tail probabilities for martingales.
\newblock {\em The Annals of Probability}, 3(1):100--118.

\bibitem[Fujimoto and Gu, 2021]{fujimoto2021minimalist}
Fujimoto, S. and Gu, S.~S. (2021).
\newblock A minimalist approach to offline reinforcement learning.
\newblock {\em Advances in neural information processing systems},
  34:20132--20145.

\bibitem[Fujimoto et~al., 2019]{fujimoto2019off}
Fujimoto, S., Meger, D., and Precup, D. (2019).
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em International Conference on Machine Learning}, pages
  2052--2062. PMLR.

\bibitem[Jin et~al., 2018]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I. (2018).
\newblock Is {Q}-learning provably efficient?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4863--4873.

\bibitem[Jin et~al., 2022]{jin2022fedrlhete}
Jin, H., Peng, Y., Yang, W., Wang, S., and Zhang, Z. (2022).
\newblock Federated reinforcement learning with environment heterogeneity.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 18--37.

\bibitem[Jin et~al., 2021]{jin2021pessimism}
Jin, Y., Yang, Z., and Wang, Z. (2021).
\newblock Is pessimism provably efficient for offline {RL}?
\newblock In {\em International Conference on Machine Learning}, pages
  5084--5096.

\bibitem[Khodadadian et~al., 2022]{khodadadian22fedrlspeedup}
Khodadadian, S., Sharma, P., Joshi, G., and Maguluri, S.~T. (2022).
\newblock Federated reinforcement learning: Linear speedup under {M}arkovian
  sampling.
\newblock In {\em International Conference on Machine Learning}, pages
  10997--11057.

\bibitem[Kidambi et~al., 2020]{kidambi2020morel}
Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T. (2020).
\newblock Morel: Model-based offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2005.05951}.

\bibitem[Kim and Oh, 2023]{kim23model}
Kim, B. and Oh, M.-H. (2023).
\newblock Model-based offline reinforcement learning with count-based
  conservatism.
\newblock In {\em International Conference on Machine Learning}, volume 202 of
  {\em Proceedings of Machine Learning Research}, pages 16728--16746. PMLR.

\bibitem[Kostrikov et~al., 2022]{kostrikovoffline}
Kostrikov, I., Nair, A., and Levine, S. (2022).
\newblock Offline reinforcement learning with implicit q-learning.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Kumar et~al., 2020]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S. (2020).
\newblock Conservative {Q}-learning for offline reinforcement learning.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H., editors, {\em Advances in Neural Information Processing Systems},
  volume~33, pages 1179--1191. Curran Associates, Inc.

\bibitem[Levine et~al., 2020]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J. (2020).
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock {\em arXiv preprint arXiv:2005.01643}.

\bibitem[Li et~al., 2024]{li2021syncq}
Li, G., Cai, C., Chen, Y., Wei, Y., and Chi, Y. (2024).
\newblock Is {Q}-learning minimax optimal? a tight sample complexity analysis.
\newblock {\em Operations Research}, 72(1):222--236.

\bibitem[Li et~al., 2022]{li2022settling}
Li, G., Shi, L., Chen, Y., Chi, Y., and Wei, Y. (2022).
\newblock Settling the sample complexity of model-based offline reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:2204.05275}.

\bibitem[Li et~al., 2021]{li2021asyncq}
Li, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y. (2021).
\newblock Sample complexity of asynchronous {Q}-learning: Sharper analysis and
  variance reduction.
\newblock {\em IEEE Transactions on Information Theory}, 68(1):448--473.

\bibitem[Liu et~al., 2020]{liu2020provably}
Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E. (2020).
\newblock Provably good batch reinforcement learning without great exploration.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~34.

\bibitem[Mitzenmacher and Upfal, 2005]{mitzenmacher2005probability}
Mitzenmacher, M. and Upfal, E. (2005).
\newblock {\em Probability and computing}.
\newblock Cambridge University Press.

\bibitem[Puterman, 2014]{puterman2014markov}
Puterman, M.~L. (2014).
\newblock {\em Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons.

\bibitem[Qu and Wierman, 2020]{qu2020finite}
Qu, G. and Wierman, A. (2020).
\newblock Finite-time analysis of asynchronous stochastic approximation and
  {Q}-learning.
\newblock In {\em Conference on Learning Theory}, pages 3185--3205. PMLR.

\bibitem[Rashidinejad et~al., 2021]{rashidinejad2021bridging}
Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S. (2021).
\newblock Bridging offline reinforcement learning and imitation learning: A
  tale of pessimism.
\newblock {\em Advances in Neural Information Processing Systems}.

\bibitem[Shi et~al., 2023]{shi23perturb}
Shi, C., Xiong, W., Shen, C., and Yang, J. (2023).
\newblock Provably efficient offline reinforcement learning with perturbed data
  sources.
\newblock In {\em International Conference on Machine Learning}, volume 202 of
  {\em Proceedings of Machine Learning Research}, pages 31353--31388. PMLR.

\bibitem[Shi et~al., 2022]{shi2022pessimistic}
Shi, L., Li, G., Wei, Y., Chen, Y., and Chi, Y. (2022).
\newblock Pessimistic {Q}-learning for offline reinforcement learning: Towards
  optimal sample complexity.
\newblock In {\em International Conference on Machine Learning}, volume 162,
  pages 19967--20025. PMLR.

\bibitem[Siegel et~al., 2020]{siegel2020keep}
Siegel, N., Springenberg, J.~T., Berkenkamp, F., Abdolmaleki, A., Neunert, M.,
  Lampe, T., Hafner, R., Heess, N., and Riedmiller, M. (2020).
\newblock Keep doing what worked: Behavior modelling priors for offline
  reinforcement learning.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Uehara et~al., 2023]{uehara2023offline}
Uehara, M., Kallus, N., Lee, J.~D., and Sun, W. (2023).
\newblock Offline minimax soft-q-learning under realizability and partial
  coverage.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~37.

\bibitem[Wainwright, 2019]{wainwright2019stochastic}
Wainwright, M.~J. (2019).
\newblock Stochastic approximation with cone-contractive operators: Sharp
  $\ell_{\infty}$-bounds for {Q}-learning.
\newblock {\em arXiv preprint arXiv:1905.06265}.

\bibitem[Wang et~al., 2023]{wang23fedtd}
Wang, H., Mitra, A., Hassani, H., Pappas, G.~J., and Anderson, J. (2023).
\newblock Federated temporal difference learning with linear function
  approximation under environmental heterogeneity.
\newblock {\em arXiv preprint arXiv:2302.02212}.

\bibitem[Wang et~al., 2019]{wang2019q}
Wang, Y., Dong, K., Chen, X., and Wang, L. (2019).
\newblock {Q}-learning with {UCB} exploration is sample efficient for
  infinite-horizon {MDP}.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Watkins and Dayan, 1992]{watkins1992q}
Watkins, C.~J. and Dayan, P. (1992).
\newblock {Q}-learning.
\newblock {\em Machine learning}, 8(3-4):279--292.

\bibitem[Woo et~al., 2023]{woo23blessing}
Woo, J., Joshi, G., and Chi, Y. (2023).
\newblock The blessing of heterogeneity in federated q-learning: Linear speedup
  and beyond.
\newblock In {\em International Conference on Machine Learning}, volume 202 of
  {\em Proceedings of Machine Learning Research}, pages 37157--37216. PMLR.

\bibitem[Wu et~al., 2019]{wu2019behavior}
Wu, Y., Tucker, G., and Nachum, O. (2019).
\newblock Behavior regularized offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:1911.11361}.

\bibitem[Wu et~al., 2021]{wu2021byzantine}
Wu, Z., Shen, H., Chen, T., and Ling, Q. (2021).
\newblock Byzantine-resilient decentralized policy evaluation with linear
  function approximation.
\newblock {\em IEEE Transactions on Signal Processing}, 69:3839--3853.

\bibitem[Xie et~al., 2021a]{xie2021bellman}
Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A. (2021a).
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~35.

\bibitem[Xie et~al., 2021b]{xie2021policy}
Xie, T., Jiang, N., Wang, H., Xiong, C., and Bai, Y. (2021b).
\newblock Policy finetuning: Bridging sample-efficient offline and online
  reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 35.

\bibitem[Xu and Gu, 2020]{xu2020finite}
Xu, P. and Gu, Q. (2020).
\newblock A finite-time analysis of {Q}-learning with neural network function
  approximation.
\newblock In {\em International Conference on Machine Learning}, pages
  10555--10565. PMLR.

\bibitem[Yan et~al., 2023]{yan2022efficacy}
Yan, Y., Li, G., Chen, Y., and Fan, J. (2023).
\newblock The efficacy of pessimism in asynchronous {Q}-learning.
\newblock {\em IEEE Transactions on Information Theory}, 69(11):7185--7219.

\bibitem[Yang et~al., 2023]{yang23fednpg}
Yang, T., Cen, S., Wei, Y., Chen, Y., and Chi, Y. (2023).
\newblock Federated natural policy gradient methods for multi-task
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:2311.00201}.

\bibitem[Yin and Wang, 2021]{yin2021towards}
Yin, M. and Wang, Y.-X. (2021).
\newblock Towards instance-optimal offline reinforcement learning with
  pessimism.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~35.

\bibitem[Yu et~al., 2020]{yu2020mopo}
Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J., Levine, S., Finn, C., and Ma,
  T. (2020).
\newblock {MOPO}: Model-based offline policy optimization.
\newblock {\em arXiv preprint arXiv:2005.13239}.

\bibitem[Zanette et~al., 2021]{zanette2021provable}
Zanette, A., Wainwright, M.~J., and Brunskill, E. (2021).
\newblock Provable benefits of actor-critic methods for offline reinforcement
  learning.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~35.

\bibitem[Zhang et~al., 2024]{zhang2024finite}
Zhang, C., Wang, H., Mitra, A., and Anderson, J. (2024).
\newblock Finite-time analysis of on-policy heterogeneous federated
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:2401.15273}.

\bibitem[Zheng et~al., 2023]{zheng2023federated}
Zheng, Z., Gao, F., Xue, L., and Yang, J. (2023).
\newblock Federated {Q}-learning: Linear regret speedup with low communication
  cost.
\newblock {\em arXiv preprint arXiv:2312.15023}.

\bibitem[Zhou et~al., 2023]{zhou23fed}
Zhou, D., Zhang, Y., Sonabend-W, A., Wang, Z., Lu, J., and Cai, T. (2023).
\newblock Federated offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2206.05581}.

\end{thebibliography}
