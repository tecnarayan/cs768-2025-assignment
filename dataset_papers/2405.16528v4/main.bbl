\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Gholami et~al.(2022)Gholami, Kim, Dong, Yao, Mahoney, and Keutzer]{surveyquant}
Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael~W Mahoney, and Kurt Keutzer.
\newblock A survey of quantization methods for efficient neural network inference.
\newblock In \emph{Low-Power Computer Vision}, pages 291--326. Chapman and Hall/CRC, 2022.

\bibitem[Ma et~al.(2024)Ma, Wang, Ma, Wang, Wang, Huang, Dong, Wang, Xue, and Wei]{ma2024era1bitllmslarge}
Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li~Dong, Ruiping Wang, Jilong Xue, and Furu Wei.
\newblock The era of 1-bit llms: All large language models are in 1.58 bits, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.17764}.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2023qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, \emph{Advances in Neural Information Processing Systems}, volume~36, pages 10088--10115. Curran Associates, Inc., 2023.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/1feb87871436031bdc0f2beaa62a049b-Paper-Conference.pdf}.

\bibitem[Liao et~al.(2024)Liao, Herold, Khadivi, and Monz]{liao2024apiq}
Baohao Liao, Christian Herold, Shahram Khadivi, and Christof Monz.
\newblock Apiq: Finetuning of 2-bit quantized large language model, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.05147}.

\bibitem[Hu et~al.()Hu, Wallis, Allen-Zhu, Li, Wang, Wang, Chen, et~al.]{hu2021lora}
Edward~J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, Weizhu Chen, et~al.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[Hayou et~al.()Hayou, Ghosh, and Yu]{hayou2024loraplus}
Soufiane Hayou, Nikhil Ghosh, and Bin Yu.
\newblock Lora+: Efficient low rank adaptation of large models.
\newblock In \emph{Forty-first International Conference on Machine Learning}.

\bibitem[Lialin et~al.(2023)Lialin, Shivagunde, Muckatira, and Rumshisky]{lialin2023relora}
Vladislav Lialin, Namrata Shivagunde, Sherin Muckatira, and Anna Rumshisky.
\newblock Relora: High-rank training through low-rank updates, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.05695}.

\bibitem[Zhao et~al.(2024{\natexlab{a}})Zhao, Zhang, Chen, Wang, Anandkumar, and Tian]{zhao2024galore}
Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian.
\newblock {G}a{L}ore: Memory-efficient {LLM} training by gradient low-rank projection.
\newblock In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, \emph{Proceedings of the 41st International Conference on Machine Learning}, volume 235 of \emph{Proceedings of Machine Learning Research}, pages 61121--61143. PMLR, 21--27 Jul 2024{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v235/zhao24s.html}.

\bibitem[Kingma and Ba(2017)]{kingma2017adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization, 2017.
\newblock arXiv:1412.6980.

\bibitem[Barkarson et~al.(2022)Barkarson, Steingr{\'\i}msson, and Hafsteinsd{\'o}ttir]{barkarson-etal-2022-evolving}
Starka{\dh}ur Barkarson, Stein{\th}{\'o}r Steingr{\'\i}msson, and Hildur Hafsteinsd{\'o}ttir.
\newblock Evolving large text corpora: Four versions of the {I}celandic {G}igaword corpus.
\newblock In Nicoletta Calzolari, Fr{\'e}d{\'e}ric B{\'e}chet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, H{\'e}l{\`e}ne Mazo, Jan Odijk, and Stelios Piperidis, editors, \emph{Proceedings of the Thirteenth Language Resources and Evaluation Conference}, pages 2371--2381, Marseille, France, June 2022. European Language Resources Association.
\newblock URL \url{https://aclanthology.org/2022.lrec-1.254}.

\bibitem[Sun et~al.(2020)Sun, Wang, Li, Feng, Tian, Wu, and Wang]{wang-etal-2018-glue}
Yu~Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang.
\newblock Ernie 2.0: A continual pre-training framework for language understanding.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~34, pages 8968--8975, 2020.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021trainingverifierssolvemath}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.14168}.

\bibitem[Lv et~al.(2024)Lv, Yang, Liu, Gao, Guo, and Qiu]{lv2023parameter}
Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu.
\newblock Full parameter fine-tuning for large language models with limited resources, 2024.
\newblock URL \url{https://arxiv.org/abs/2306.09782}.

\bibitem[Li et~al.({\natexlab{a}})Li, Yu, Liang, Karampatziakis, He, Chen, and Zhao]{li2023loftq}
Yixiao Li, Yifan Yu, Chen Liang, Nikos Karampatziakis, Pengcheng He, Weizhu Chen, and Tuo Zhao.
\newblock Loftq: Lora-fine-tuning-aware quantization for large language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, {\natexlab{a}}.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
  Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models, 2023.
\newblock arXiv:2307.09288.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{2019t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de~Las~Casas, Hendricks, Welbl, Clark, Hennigan, Noland, Millican, van~den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae, Vinyals, and Sifre]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las~Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van~den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack~W. Rae, Oriol Vinyals, and Laurent Sifre.
\newblock Training compute-optimal large language models, 2022.
\newblock arXiv:2203.15556.

\bibitem[Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang2019glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R. Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019.
\newblock URL \url{https://arxiv.org/abs/1804.07461}.

\bibitem[He et~al.()He, Gao, and Chen]{he2023debertav3}
Pengcheng He, Jianfeng Gao, and Weizhu Chen.
\newblock Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing.
\newblock In \emph{The Eleventh International Conference on Learning Representations}.

\bibitem[Dettmers et~al.({\natexlab{a}})Dettmers, Lewis, Shleifer, and Zettlemoyer]{dettmers20228bit}
Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer.
\newblock 8-bit optimizers via block-wise quantization.
\newblock In \emph{International Conference on Learning Representations}, {\natexlab{a}}.

\bibitem[Zafrir et~al.(2019)Zafrir, Boudoukh, Izsak, and Wasserblat]{Zafrir_2019}
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
\newblock Q8bert: Quantized 8bit bert.
\newblock In \emph{2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)}, pages 36--39. IEEE, 2019.

\bibitem[Shen et~al.(2020)Shen, Dong, Ye, Ma, Yao, Gholami, Mahoney, and Keutzer]{shen2019qbert}
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael~W Mahoney, and Kurt Keutzer.
\newblock Q-bert: Hessian based ultra low precision quantization of bert.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~34, pages 8815--8821, 2020.

\bibitem[Bai et~al.(2022)Bai, Hou, Shang, Jiang, King, and Lyu]{bai2021efficient}
Haoli Bai, Lu~Hou, Lifeng Shang, Xin Jiang, Irwin King, and Michael~R Lyu.
\newblock Towards efficient post-training quantization of pre-trained language models.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 1405--1418, 2022.

\bibitem[Shazeer and Stern(2018)]{shazeer2018adafactor}
Noam Shazeer and Mitchell Stern.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock In \emph{International Conference on Machine Learning}, pages 4596--4604. PMLR, 2018.

\bibitem[Larsen et~al.(2022)Larsen, Fort, Becker, and Ganguli]{larsen2022degrees}
Brett~W. Larsen, Stanislav Fort, Nic Becker, and Surya Ganguli.
\newblock How many degrees of freedom do we need to train deep networks: a loss landscape perspective, 2022.
\newblock URL \url{https://arxiv.org/abs/2107.05802}.

\bibitem[Gur-Ari et~al.(2018)Gur-Ari, Roberts, and Dyer]{gurari2018gradient}
Guy Gur-Ari, Daniel~A. Roberts, and Ethan Dyer.
\newblock Gradient descent happens in a tiny subspace, 2018.
\newblock URL \url{https://arxiv.org/abs/1812.04754}.

\bibitem[Liu et~al.(2023)Liu, Oguz, Zhao, Chang, Stock, Mehdad, Shi, Krishnamoorthi, and Chandra]{liu2023llm}
Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
\newblock Llm-qat: Data-free quantization aware training for large language models, 2023.
\newblock arXiv:2205.17888.

\bibitem[Jung et~al.(2019)Jung, Son, Lee, Son, Han, Kwak, Hwang, and Choi]{jung2018learning}
Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun Kwak, Sung~Ju Hwang, and Changkyu Choi.
\newblock Learning to quantize deep networks by optimizing quantization intervals with task loss.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 4350--4359, 2019.

\bibitem[Egiazarian et~al.(2024)Egiazarian, Panferov, Kuznedelev, Frantar, Babenko, and Alistarh]{egiazarian2024extreme}
Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, and Dan Alistarh.
\newblock Extreme compression of large language models via additive quantization, 2024.
\newblock URL \url{https://arxiv.org/abs/2401.06118}.

\bibitem[Wang et~al.(2023)Wang, Ma, Dong, Huang, Wang, Ma, Yang, Wang, Wu, and Wei]{wang2023bitnet}
Hongyu Wang, Shuming Ma, Li~Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi~Wu, and Furu Wei.
\newblock Bitnet: Scaling 1-bit transformers for large language models.
\newblock \emph{arXiv e-prints}, pages arXiv--2310, 2023.

\bibitem[Frantar et~al.(2023)Frantar, Ashkboos, Hoefler, and Alistarh]{frantar2023gptq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock Gptq: Accurate post-training quantization for generative pre-trained transformers, 2023.
\newblock URL \url{https://arxiv.org/abs/2210.17323}.

\bibitem[Dettmers et~al.({\natexlab{b}})Dettmers, Svirschevski, Egiazarian, Kuznedelev, Frantar, Ashkboos, Borzunov, Hoefler, and Alistarh]{dettmers2023spqr}
Tim Dettmers, Ruslan~A Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh.
\newblock Spqr: A sparse-quantized representation for near-lossless llm weight compression.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, {\natexlab{b}}.

\bibitem[Tseng et~al.()Tseng, Chee, Sun, Kuleshov, and De~Sa]{tseng2024quip}
Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De~Sa.
\newblock Quip\#: Even better llm quantization with hadamard incoherence and lattice codebooks.
\newblock In \emph{Forty-first International Conference on Machine Learning}.

\bibitem[Xiao et~al.(2023)Xiao, Lin, Seznec, Wu, Demouth, and Han]{xiao2024smoothquant}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for large language models.
\newblock In \emph{International Conference on Machine Learning}, pages 38087--38099. PMLR, 2023.

\bibitem[Park et~al.()Park, Kim, Lee, Kim, Kwon, Kwon, Kim, Lee, Lee, et~al.]{park2024lutgemm}
Gunho Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok Kwon, Se~Jung Kwon, Byeongwook Kim, Youngjoo Lee, Dongsoo Lee, et~al.
\newblock Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[Lee et~al.(2023)Lee, Kim, Kwon, and Lee]{pmlr-v202-lee23h}
Jung~Hyun Lee, Jeonghoon Kim, Se~Jung Kwon, and Dongsoo Lee.
\newblock Flexround: Learnable rounding based on element-wise division for post-training quantization.
\newblock In \emph{International Conference on Machine Learning}, pages 18913--18939. PMLR, 2023.

\bibitem[Heo et~al.()Heo, Kim, Kwon, Kim, Kwon, and Lee]{heo2024rethinking}
Jung~Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se~Jung Kwon, and Dongsoo Lee.
\newblock Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[Shao et~al.()Shao, Chen, Zhang, Xu, Zhao, Li, Zhang, Gao, Qiao, and Luo]{shao2024omniquant}
Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu~Qiao, and Ping Luo.
\newblock Omniquant: Omnidirectionally calibrated quantization for large language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[Wang et~al.(2018)Wang, Choi, Brand, Chen, and Gopalakrishnan]{wang2018training}
Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan.
\newblock Training deep neural networks with 8-bit floating point numbers.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Chmiel et~al.(2023)Chmiel, Banner, Hoffer, Ben-Yaacov, and Soudry]{chmiel2022logarithmic}
Brian Chmiel, Ron Banner, Elad Hoffer, Hilla Ben-Yaacov, and Daniel Soudry.
\newblock Accurate neural training with 4-bit matrix multiplications at standard formats.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Banner et~al.(2018)Banner, Hubara, Hoffer, and Soudry]{banner2018scalable}
Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry.
\newblock Scalable methods for 8-bit training of neural networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Perez et~al.()Perez, Zhang, Briggs, Blake, Levy-Kramer, Balanca, Luschi, Barlow, and Fitzgibbon]{perez2023training}
Sergio~P Perez, Yan Zhang, James Briggs, Charlie Blake, Josh Levy-Kramer, Paul Balanca, Carlo Luschi, Stephen Barlow, and Andrew~William Fitzgibbon.
\newblock Training and inference of large language models using 8-bit floating point.
\newblock In \emph{Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023)}.

\bibitem[Wortsman et~al.(2023)Wortsman, Dettmers, Zettlemoyer, Morcos, Farhadi, and Schmidt]{wortsman2023stable}
Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi, and Ludwig Schmidt.
\newblock Stable and low-precision training for large-scale vision-language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 10271--10298, 2023.

\bibitem[Peng et~al.(2023)Peng, Wu, Wei, Zhao, Yang, Liu, Xiong, Yang, Ni, Hu, Li, Zhang, Li, Ning, Wang, Zhang, Liu, Chau, Hu, and Cheng]{peng2023fp8lm}
Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze~Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, Ruihang Li, Miaosen Zhang, Chen Li, Jia Ning, Ruizhe Wang, Zheng Zhang, Shuguang Liu, Joe Chau, Han Hu, and Peng Cheng.
\newblock Fp8-lm: Training fp8 large language models, 2023.
\newblock arXiv:2310.18313.

\bibitem[Xi et~al.()Xi, Chen, Zhao, TEH, Chen, and Zhu]{xi2024jetfire}
Haocheng Xi, Yuxiang Chen, Kang Zhao, KAI~JUN TEH, Jianfei Chen, and Jun Zhu.
\newblock Jetfire: Efficient and accurate transformer pretraining with int8 data flow and per-block quantization.
\newblock In \emph{Forty-first International Conference on Machine Learning}.

\bibitem[Li et~al.({\natexlab{b}})Li, Yu, Liang, Karampatziakis, He, Chen, and Zhao]{li2023loftqlorafinetuningawarequantizationlarge}
Yixiao Li, Yifan Yu, Chen Liang, Nikos Karampatziakis, Pengcheng He, Weizhu Chen, and Tuo Zhao.
\newblock Loftq: Lora-fine-tuning-aware quantization for large language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, {\natexlab{b}}.

\bibitem[Guo et~al.(2023)Guo, Greengard, Xing, and Kim]{guo2024lqloralowrankplusquantized}
Han Guo, Philip Greengard, Eric Xing, and Yoon Kim.
\newblock Lq-lora: Low-rank plus quantized matrix decomposition for efficient language model finetuning.
\newblock \emph{ICLR 2024}, 2023.

\bibitem[Zhao et~al.(2024{\natexlab{b}})Zhao, Zhang, Chen, Schäfer, and Anandkumar]{zhao2024inrank}
Jiawei Zhao, Yifei Zhang, Beidi Chen, Florian Schäfer, and Anima Anandkumar.
\newblock Inrank: Incremental low-rank learning, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2306.11250}.

\end{thebibliography}
