\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and Lopez-Paz]{arjovsky2019invariant}
Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D.
\newblock Invariant risk minimization.
\newblock \emph{arXiv preprint arXiv:1907.02893}, 2019.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Beery et~al.(2020)Beery, Liu, Morris, Piavis, Kapoor, Joshi, Meister, and Perona]{beery2020synthetic}
Beery, S., Liu, Y., Morris, D., Piavis, J., Kapoor, A., Joshi, N., Meister, M., and Perona, P.
\newblock Synthetic examples improve generalization for rare classes.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pp.\  863--873, 2020.

\bibitem[Blanchard et~al.(2011)Blanchard, Lee, and Scott]{blanchard2011generalizing}
Blanchard, G., Lee, G., and Scott, C.
\newblock Generalizing from several related classification tasks to a new unlabeled sample.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Boudiaf et~al.(2022)Boudiaf, Mueller, Ben~Ayed, and Bertinetto]{boudiaf2022parameter}
Boudiaf, M., Mueller, R., Ben~Ayed, I., and Bertinetto, L.
\newblock Parameter-free online test-time adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  8344--8353, 2022.

\bibitem[Burns \& Steinhardt(2021)Burns and Steinhardt]{burns2021limitations}
Burns, C. and Steinhardt, J.
\newblock Limitations of post-hoc feature alignment for robustness.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  2525--2533, 2021.

\bibitem[Chen et~al.(2022)Chen, Wang, Darrell, and Ebrahimi]{chen2022contrastive}
Chen, D., Wang, D., Darrell, T., and Ebrahimi, S.
\newblock Contrastive test-time adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  295--305, 2022.

\bibitem[Eastwood et~al.(2022)Eastwood, Mason, Williams, and Sch{\"o}lkopf]{eastwoodsource}
Eastwood, C., Mason, I., Williams, C., and Sch{\"o}lkopf, B.
\newblock Source-free adaptation to measurement shift via bottom-up feature restoration.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Fleuret et~al.(2021)]{fleuret2021uncertainty}
Fleuret, F. et~al.
\newblock Uncertainty reduction for model adaptation in semantic segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  9613--9623, 2021.

\bibitem[Gandelsman et~al.()Gandelsman, Sun, Chen, and Efros]{gandelsmantest}
Gandelsman, Y., Sun, Y., Chen, X., and Efros, A.~A.
\newblock Test-time training with masked autoencoders.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[Ganin \& Lempitsky(2015)Ganin and Lempitsky]{ganin2015unsupervised}
Ganin, Y. and Lempitsky, V.
\newblock Unsupervised domain adaptation by backpropagation.
\newblock In \emph{International conference on machine learning}, pp.\  1180--1189. PMLR, 2015.

\bibitem[Gao et~al.(2022)Gao, Zhang, Liu, Darrell, Shelhamer, and Wang]{gao2022back}
Gao, J., Zhang, J., Liu, X., Darrell, T., Shelhamer, E., and Wang, D.
\newblock Back to the source: Diffusion-driven test-time adaptation.
\newblock \emph{arXiv preprint arXiv:2207.03442}, 2022.

\bibitem[Geirhos et~al.(2018)Geirhos, Rubisch, Michaelis, Bethge, Wichmann, and Brendel]{geirhos2018imagenet}
Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F.~A., and Brendel, W.
\newblock Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness.
\newblock \emph{arXiv preprint arXiv:1811.12231}, 2018.

\bibitem[Gong et~al.(2022{\natexlab{a}})Gong, Jeong, Kim, Kim, Shin, and Lee]{gong2022note}
Gong, T., Jeong, J., Kim, T., Kim, Y., Shin, J., and Lee, S.-J.
\newblock Note: Robust continual test-time adaptation against temporal correlation.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2022{\natexlab{a}}.

\bibitem[Gong et~al.(2022{\natexlab{b}})Gong, Jeong, Kim, Kim, Shin, and Lee]{gong2022robust}
Gong, T., Jeong, J., Kim, T., Kim, Y., Shin, J., and Lee, S.-J.
\newblock Robust continual test-time adaptation: Instance-aware bn and prediction-balanced memory.
\newblock \emph{arXiv preprint arXiv:2208.05117}, 2022{\natexlab{b}}.

\bibitem[Goyal et~al.(2022)Goyal, Sun, Raghunathan, and Kolter]{goyal2022test}
Goyal, S., Sun, M., Raghunathan, A., and Kolter, Z.
\newblock Test-time adaptation via conjugate pseudo-labels.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[Gulrajani \& Lopez-Paz(2021)Gulrajani and Lopez-Paz]{gulrajani2021in}
Gulrajani, I. and Lopez-Paz, D.
\newblock In search of lost domain generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=lQdXeXDoWtI}.

\bibitem[Hendrycks \& Dietterich(2019)Hendrycks and Dietterich]{hendrycks2018benchmarking}
Hendrycks, D. and Dietterich, T.
\newblock Benchmarking neural network robustness to common corruptions and perturbations.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HJz6tiCqYm}.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Mu, Cubuk, Zoph, Gilmer, and Lakshminarayanan]{hendrycks2019augmix}
Hendrycks, D., Mu, N., Cubuk, E.~D., Zoph, B., Gilmer, J., and Lakshminarayanan, B.
\newblock Augmix: A simple data processing method to improve robustness and uncertainty.
\newblock \emph{arXiv preprint arXiv:1912.02781}, 2019.

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Basart, Mu, Kadavath, Wang, Dorundo, Desai, Zhu, Parajuli, Guo, et~al.]{hendrycks2021many}
Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., et~al.
\newblock The many faces of robustness: A critical analysis of out-of-distribution generalization.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  8340--8349, 2021{\natexlab{a}}.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Zhao, Basart, Steinhardt, and Song]{hendrycks2021nae}
Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D.
\newblock Natural adversarial examples.
\newblock \emph{CVPR}, 2021{\natexlab{b}}.

\bibitem[Hendrycks et~al.(2022)Hendrycks, Zou, Mazeika, Tang, Li, Song, and Steinhardt]{hendrycks2022robustness}
Hendrycks, D., Zou, A., Mazeika, M., Tang, L., Li, B., Song, D., and Steinhardt, J.
\newblock Pixmix: Dreamlike pictures comprehensively improve safety measures.
\newblock \emph{CVPR}, 2022.

\bibitem[Iwasawa \& Matsuo(2021)Iwasawa and Matsuo]{iwasawa2021test}
Iwasawa, Y. and Matsuo, Y.
\newblock Test-time classifier adjustment module for model-agnostic domain generalization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 2427--2440, 2021.

\bibitem[Jiang \& Lin(2023)Jiang and Lin]{jiang2023test}
Jiang, L. and Lin, T.
\newblock Test-time robust personalization for federated learning.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Koh et~al.(2021)Koh, Sagawa, Marklund, Xie, Zhang, Balsubramani, Hu, Yasunaga, Phillips, Gao, et~al.]{koh2021wilds}
Koh, P.~W., Sagawa, S., Marklund, H., Xie, S.~M., Zhang, M., Balsubramani, A., Hu, W., Yasunaga, M., Phillips, R.~L., Gao, I., et~al.
\newblock Wilds: A benchmark of in-the-wild distribution shifts.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5637--5664. PMLR, 2021.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kundu et~al.(2022)Kundu, Kulkarni, Bhambri, Mehta, Kulkarni, Jampani, and Radhakrishnan]{kundu2022balancing}
Kundu, J.~N., Kulkarni, A.~R., Bhambri, S., Mehta, D., Kulkarni, S.~A., Jampani, V., and Radhakrishnan, V.~B.
\newblock Balancing discriminability and transferability for source-free domain adaptation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  11710--11728. PMLR, 2022.

\bibitem[Lee et~al.(2022)Lee, Jung, Yim, and Yoon]{lee2022confidence}
Lee, J., Jung, D., Yim, J., and Yoon, S.
\newblock Confidence score for source-free unsupervised domain adaptation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  12365--12377. PMLR, 2022.

\bibitem[Li et~al.(2017)Li, Yang, Song, and Hospedales]{li2017deeper}
Li, D., Yang, Y., Song, Y.-Z., and Hospedales, T.~M.
\newblock Deeper, broader and artier domain generalization.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pp.\  5542--5550, 2017.

\bibitem[Li et~al.(2020)Li, Jiao, Cao, Wong, and Wu]{li2020model}
Li, R., Jiao, Q., Cao, W., Wong, H.-S., and Wu, S.
\newblock Model adaptation: Unsupervised domain adaptation without source data.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  9641--9650, 2020.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Chen, Xie, Yang, Yuan, Pu, and Zhuang]{li2021free}
Li, X., Chen, W., Xie, D., Yang, S., Yuan, P., Pu, S., and Zhuang, Y.
\newblock A free lunch for unsupervised domain adaptive object detection without source data.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, pp.\  8474--8481, 2021{\natexlab{a}}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Hao, Di, Gundavarapu, and Wang]{li2021test}
Li, Y., Hao, M., Di, Z., Gundavarapu, N.~B., and Wang, X.
\newblock Test-time personalization with a transformer for human pose estimation.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 2583--2597, 2021{\natexlab{b}}.

\bibitem[Liang et~al.(2020)Liang, Hu, and Feng]{liang2020we}
Liang, J., Hu, D., and Feng, J.
\newblock Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\  6028--6039, 2020.

\bibitem[Liu et~al.(2021)Liu, Kothari, van Delft, Bellot-Gurlet, Mordan, and Alahi]{liu2021ttt++}
Liu, Y., Kothari, P., van Delft, B., Bellot-Gurlet, B., Mordan, T., and Alahi, A.
\newblock {TTT}++: When does self-supervised test-time training fail or thrive?
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 21808--21820, 2021.

\bibitem[Long et~al.(2015)Long, Cao, Wang, and Jordan]{long2015learning}
Long, M., Cao, Y., Wang, J., and Jordan, M.
\newblock Learning transferable features with deep adaptation networks.
\newblock In \emph{International conference on machine learning}, pp.\  97--105. PMLR, 2015.

\bibitem[Mehrabi et~al.(2021)Mehrabi, Morstatter, Saxena, Lerman, and Galstyan]{mehrabi2021survey}
Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., and Galstyan, A.
\newblock A survey on bias and fairness in machine learning.
\newblock \emph{ACM Computing Surveys (CSUR)}, 54\penalty0 (6):\penalty0 1--35, 2021.

\bibitem[Muandet et~al.(2013)Muandet, Balduzzi, and Sch{\"o}lkopf]{muandet2013domain}
Muandet, K., Balduzzi, D., and Sch{\"o}lkopf, B.
\newblock Domain generalization via invariant feature representation.
\newblock In \emph{International conference on machine learning}, pp.\  10--18. PMLR, 2013.

\bibitem[Niu et~al.(2022{\natexlab{a}})Niu, Wu, Zhang, Chen, Zheng, Zhao, and Tan]{niu2022efficient}
Niu, S., Wu, J., Zhang, Y., Chen, Y., Zheng, S., Zhao, P., and Tan, M.
\newblock Efficient test-time model adaptation without forgetting.
\newblock In \emph{The Internetional Conference on Machine Learning}, 2022{\natexlab{a}}.

\bibitem[Niu et~al.(2022{\natexlab{b}})Niu, Wu, Zhang, Chen, Zheng, Zhao, and Tan]{niu22efficient}
Niu, S., Wu, J., Zhang, Y., Chen, Y., Zheng, S., Zhao, P., and Tan, M.
\newblock Efficient test-time model adaptation without forgetting.
\newblock In \emph{Proceedings of the 39th International Conference on Machine Learning}, pp.\  16888--16905, 2022{\natexlab{b}}.

\bibitem[Niu et~al.(2023)Niu, Wu, Zhang, Wen, Chen, Zhao, and Tan]{niu2023towards}
Niu, S., Wu, J., Zhang, Y., Wen, Z., Chen, Y., Zhao, P., and Tan, M.
\newblock Towards stable test-time adaptation in dynamic wild world.
\newblock In \emph{Internetional Conference on Learning Representations}, 2023.

\bibitem[Peng et~al.(2019)Peng, Bai, Xia, Huang, Saenko, and Wang]{peng2019moment}
Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., and Wang, B.
\newblock Moment matching for multi-source domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pp.\  1406--1415, 2019.

\bibitem[Recht et~al.(2018)Recht, Roelofs, Schmidt, and Shankar]{recht2018cifar10.1}
Recht, B., Roelofs, R., Schmidt, L., and Shankar, V.
\newblock Do cifar-10 classifiers generalize to cifar-10?
\newblock 2018.
\newblock \url{https://arxiv.org/abs/1806.00451}.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and Shankar]{recht2019imagenet}
Recht, B., Roelofs, R., Schmidt, L., and Shankar, V.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In \emph{International Conference on Machine Learning}, pp.\  5389--5400. PMLR, 2019.

\bibitem[Rusak et~al.(2022)Rusak, Schneider, Pachitariu, Eck, Gehler, Bringmann, Brendel, and Bethge]{rusak2022if}
Rusak, E., Schneider, S., Pachitariu, G., Eck, L., Gehler, P.~V., Bringmann, O., Brendel, W., and Bethge, M.
\newblock If your data distribution shifts, use self-learning.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock URL \url{https://openreview.net/forum?id=vqRzLv6POg}.

\bibitem[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and Liang]{sagawa2019distributionally}
Sagawa, S., Koh, P.~W., Hashimoto, T.~B., and Liang, P.
\newblock Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization.
\newblock \emph{arXiv preprint arXiv:1911.08731}, 2019.

\bibitem[Schneider et~al.(2020)Schneider, Rusak, Eck, Bringmann, Brendel, and Bethge]{schneider2020improving}
Schneider, S., Rusak, E., Eck, L., Bringmann, O., Brendel, W., and Bethge, M.
\newblock Improving robustness against common corruptions by covariate shift adaptation.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 11539--11551, 2020.

\bibitem[Sinha et~al.(2023)Sinha, Gehler, Locatello, and Schiele]{sinha2023test}
Sinha, S., Gehler, P., Locatello, F., and Schiele, B.
\newblock Test: Test-time self-training under distribution shift.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pp.\  2759--2769, 2023.

\bibitem[Su et~al.(2022)Su, Xu, and Jia]{su2022revisiting}
Su, Y., Xu, X., and Jia, K.
\newblock Revisiting realistic test-time training: Sequential inference and adaptation by anchored clustering.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Sun et~al.(2022)Sun, Murphy, Ebrahimi, and D'Amour]{sun2022beyond}
Sun, Q., Murphy, K., Ebrahimi, S., and D'Amour, A.
\newblock Beyond invariance: Test-time label-shift adaptation for distributions with" spurious" correlations.
\newblock \emph{arXiv preprint arXiv:2211.15646}, 2022.

\bibitem[Sun et~al.(2020)Sun, Wang, Liu, Miller, Efros, and Hardt]{sun2020test}
Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt, M.
\newblock Test-time training with self-supervision for generalization under distribution shifts.
\newblock In \emph{International conference on machine learning}, pp.\  9229--9248. PMLR, 2020.

\bibitem[Tsai et~al.(2018)Tsai, Hung, Schulter, Sohn, Yang, and Chandraker]{tsai2018learning}
Tsai, Y.-H., Hung, W.-C., Schulter, S., Sohn, K., Yang, M.-H., and Chandraker, M.
\newblock Learning to adapt structured output space for semantic segmentation.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  7472--7481, 2018.

\bibitem[Vapnik(1998)]{vapnik1998statistical}
Vapnik, V.~N.
\newblock \emph{Statistical Learning Theory}.
\newblock Wiley-Interscience, 1998.

\bibitem[Venkateswara et~al.(2017)Venkateswara, Eusebio, Chakraborty, and Panchanathan]{venkateswara2017deep}
Venkateswara, H., Eusebio, J., Chakraborty, S., and Panchanathan, S.
\newblock Deep hashing network for unsupervised domain adaptation.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  5018--5027, 2017.

\bibitem[Wang et~al.(2021)Wang, Shelhamer, Liu, Olshausen, and Darrell]{wang2021tent}
Wang, D., Shelhamer, E., Liu, S., Olshausen, B., and Darrell, T.
\newblock Tent: Fully test-time adaptation by entropy minimization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=uXl3bZLkr3c}.

\bibitem[Wang et~al.(2022)Wang, Fink, Van~Gool, and Dai]{wang2022continual}
Wang, Q., Fink, O., Van~Gool, L., and Dai, D.
\newblock Continual test-time domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  7201--7211, 2022.

\bibitem[Wu \& He(2018)Wu and He]{wu2018group}
Wu, Y. and He, K.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European conference on computer vision (ECCV)}, pp.\  3--19, 2018.

\bibitem[Yang et~al.(2022)Yang, Wang, Wang, Jui, et~al.]{yang2022attracting}
Yang, S., Wang, Y., Wang, K., Jui, S., et~al.
\newblock Attracting and dispersing: A simple approach for source-free domain adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Yao et~al.(2022)Yao, Choi, Cao, Lee, Koh, and Finn]{yao2022wild}
Yao, H., Choi, C., Cao, B., Lee, Y., Koh, P.~W., and Finn, C.
\newblock Wild-time: A benchmark of in-the-wild distribution shift over time.
\newblock In \emph{Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2022.

\bibitem[You et~al.(2019)You, Wang, Long, and Jordan]{you2019towards}
You, K., Wang, X., Long, M., and Jordan, M.
\newblock Towards accurate model selection in deep unsupervised domain adaptation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  7124--7133. PMLR, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and Lopez-Paz]{zhang2017mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\bibitem[Zhang et~al.(2021)Zhang, Marklund, Dhawan, Gupta, Levine, and Finn]{zhang2021adaptive}
Zhang, M., Marklund, H., Dhawan, N., Gupta, A., Levine, S., and Finn, C.
\newblock Adaptive risk minimization: Learning to adapt to domain shift.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 23664--23678, 2021.

\bibitem[Zhang et~al.()Zhang, Levine, and Finn]{zhang2022memo}
Zhang, M.~M., Levine, S., and Finn, C.
\newblock Memo: Test time robustness via adaptation and augmentation.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[Zhou \& Levine(2021)Zhou and Levine]{zhou2021bayesian}
Zhou, A. and Levine, S.
\newblock Bayesian adaptation for covariate shift.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 914--927, 2021.

\end{thebibliography}
