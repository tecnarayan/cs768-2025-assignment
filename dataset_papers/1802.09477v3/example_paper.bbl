\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anschel et~al.(2017)Anschel, Baram, and Shimkin]{AVGDQN}
Anschel, O., Baram, N., and Shimkin, N.
\newblock Averaged-dqn: Variance reduction and stabilization for deep
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  176--185, 2017.

\bibitem[Barth-Maron et~al.(2018)Barth-Maron, Hoffman, Budden, Dabney, Horgan,
  TB, Muldal, Heess, and Lillicrap]{barth-maron2018distributional}
Barth-Maron, G., Hoffman, M.~W., Budden, D., Dabney, W., Horgan, D., TB, D.,
  Muldal, A., Heess, N., and Lillicrap, T.
\newblock Distributional policy gradients.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and
  Munos]{bellemare2017distributional}
Bellemare, M.~G., Dabney, W., and Munos, R.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  449--458, 2017.

\bibitem[Bellman(1957)]{bellman}
Bellman, R.
\newblock \emph{Dynamic Programming}.
\newblock Princeton University Press, 1957.

\bibitem[Bertsekas(1995)]{bertsekas1995dynamic}
Bertsekas, D.~P.
\newblock \emph{Dynamic programming and optimal control}, volume~1.
\newblock Athena scientific Belmont, MA, 1995.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{OpenAIGym}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym, 2016.

\bibitem[Dhariwal et~al.(2017)Dhariwal, Hesse, Plappert, Radford, Schulman,
  Sidor, and Wu]{baselines}
Dhariwal, P., Hesse, C., Plappert, M., Radford, A., Schulman, J., Sidor, S.,
  and Wu, Y.
\newblock Openai baselines.
\newblock \url{https://github.com/openai/baselines}, 2017.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, et~al.]{espeholt2018impala}
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron,
  Y., Firoiu, V., Harley, T., Dunning, I., et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock \emph{arXiv preprint arXiv:1802.01561}, 2018.

\bibitem[Fox et~al.(2016)Fox, Pakman, and Tishby]{fox2015glearning}
Fox, R., Pakman, A., and Tishby, N.
\newblock Taming the noise in reinforcement learning via soft updates.
\newblock In \emph{Proceedings of the Thirty-Second Conference on Uncertainty
  in Artificial Intelligence}, pp.\  202--211. AUAI Press, 2016.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock \emph{arXiv preprint arXiv:1801.01290}, 2018.

\bibitem[He et~al.(2016)He, Liu, Schwing, and Peng]{optimalitytightening}
He, F.~S., Liu, Y., Schwing, A.~G., and Peng, J.
\newblock Learning to play in a day: Faster deep reinforcement learning by
  optimality tightening.
\newblock \emph{arXiv preprint arXiv:1611.01606}, 2016.

\bibitem[{Henderson} et~al.(2017){Henderson}, {Islam}, {Bachman}, {Pineau},
  {Precup}, and {Meger}]{hendersonRL2017}
{Henderson}, P., {Islam}, R., {Bachman}, P., {Pineau}, J., {Precup}, D., and
  {Meger}, D.
\newblock {Deep Reinforcement Learning that Matters}.
\newblock \emph{arXiv preprint arXiv:1709.06560}, 2017.

\bibitem[Horgan et~al.(2018)Horgan, Quan, Budden, Barth-Maron, Hessel, van
  Hasselt, and Silver]{horgan2018distributed}
Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., van Hasselt, H.,
  and Silver, D.
\newblock Distributed prioritized experience replay.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{adam}
Kingma, D. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Konda \& Tsitsiklis(2003)Konda and Tsitsiklis]{konda2003onactor}
Konda, V.~R. and Tsitsiklis, J.~N.
\newblock On actor-critic algorithms.
\newblock \emph{SIAM journal on Control and Optimization}, 42\penalty0
  (4):\penalty0 1143--1166, 2003.

\bibitem[Lee et~al.(2013)Lee, Defourny, and Powell]{lee2013biascor}
Lee, D., Defourny, B., and Powell, W.~B.
\newblock Bias-corrected q-learning to control max-operator bias in q-learning.
\newblock In \emph{Adaptive Dynamic Programming And Reinforcement Learning
  (ADPRL), 2013 IEEE Symposium on}, pp.\  93--99. IEEE, 2013.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{DDPG}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Lin(1992)]{expreplay1992}
Lin, L.-J.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 293--321, 1992.

\bibitem[Mannor \& Tsitsiklis(2011)Mannor and Tsitsiklis]{mannor2011mean}
Mannor, S. and Tsitsiklis, J.~N.
\newblock Mean-variance optimization in markov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  177--184, 2011.

\bibitem[Mannor et~al.(2007)Mannor, Simester, Sun, and
  Tsitsiklis]{mannor2007bias}
Mannor, S., Simester, D., Sun, P., and Tsitsiklis, J.~N.
\newblock Bias and variance approximation in value function estimates.
\newblock \emph{Management Science}, 53\penalty0 (2):\penalty0 308--322, 2007.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{DQN}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{a3c}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1928--1937, 2016.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{munos2016safe}
Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1054--1062, 2016.

\bibitem[Nachum et~al.(2018)Nachum, Norouzi, Tucker, and Schuurmans]{Smoothie}
Nachum, O., Norouzi, M., Tucker, G., and Schuurmans, D.
\newblock Smoothed action value functions for learning gaussian policies.
\newblock \emph{arXiv preprint arXiv:1803.02348}, 2018.

\bibitem[O'Donoghue et~al.(2017)O'Donoghue, Osband, Munos, and
  Mnih]{UncertaintyBellman}
O'Donoghue, B., Osband, I., Munos, R., and Mnih, V.
\newblock The uncertainty bellman equation and exploration.
\newblock \emph{arXiv preprint arXiv:1709.05380}, 2017.

\bibitem[Pendrith et~al.(1997)Pendrith, Ryan, et~al.]{pendrith1997estimator}
Pendrith, M.~D., Ryan, M.~R., et~al.
\newblock \emph{Estimator variance in reinforcement learning: Theoretical
  problems and practical solutions}.
\newblock University of New South Wales, School of Computer Science and
  Engineering, 1997.

\bibitem[Petrik \& Scherrer(2009)Petrik and Scherrer]{petrik2009discount}
Petrik, M. and Scherrer, B.
\newblock Biasing approximate dynamic programming with a lower discount factor.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1265--1272, 2009.

\bibitem[Popov et~al.(2017)Popov, Heess, Lillicrap, Hafner, Barth-Maron,
  Vecerik, Lampe, Tassa, Erez, and Riedmiller]{popov2017data}
Popov, I., Heess, N., Lillicrap, T., Hafner, R., Barth-Maron, G., Vecerik, M.,
  Lampe, T., Tassa, Y., Erez, T., and Riedmiller, M.
\newblock Data-efficient deep reinforcement learning for dexterous
  manipulation.
\newblock \emph{arXiv preprint arXiv:1704.03073}, 2017.

\bibitem[Precup et~al.(2001)Precup, Sutton, and Dasgupta]{precup2001off}
Precup, D., Sutton, R.~S., and Dasgupta, S.
\newblock Off-policy temporal-difference learning with function approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  417--424, 2001.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and
  Silver]{PrioritizedExpReplay}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock In \emph{International Conference on Learning Representations},
  Puerto Rico, 2016.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{trpo}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{ppo}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{DPG}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  387--395, 2014.

\bibitem[Singh et~al.(2000)Singh, Jaakkola, Littman, and
  Szepesv{\'a}ri]{singh2000convergence}
Singh, S., Jaakkola, T., Littman, M.~L., and Szepesv{\'a}ri, C.
\newblock Convergence results for single-step on-policy reinforcement-learning
  algorithms.
\newblock \emph{Machine learning}, 38\penalty0 (3):\penalty0 287--308, 2000.

\bibitem[Sutton(1988)]{sutton1988tdlearning}
Sutton, R.~S.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine learning}, 3\penalty0 (1):\penalty0 9--44, 1988.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton1998reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}, volume~1.
\newblock MIT press Cambridge, 1998.

\bibitem[Thrun \& Schwartz(1993)Thrun and Schwartz]{thrun1993bias}
Thrun, S. and Schwartz, A.
\newblock Issues in using function approximation for reinforcement learning.
\newblock In \emph{Proceedings of the 1993 Connectionist Models Summer School
  Hillsdale, NJ. Lawrence Erlbaum}, 1993.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ
  International Conference on}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Uhlenbeck \& Ornstein(1930)Uhlenbeck and
  Ornstein]{uhlenbeck1930theory}
Uhlenbeck, G.~E. and Ornstein, L.~S.
\newblock On the theory of the brownian motion.
\newblock \emph{Physical review}, 36\penalty0 (5):\penalty0 823, 1930.

\bibitem[Van~Hasselt(2010)]{hasselt2010double}
Van~Hasselt, H.
\newblock Double q-learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2613--2621, 2010.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{DoubleDQN}
Van~Hasselt, H., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{AAAI}, pp.\  2094--2100, 2016.

\bibitem[Van~Seijen et~al.(2009)Van~Seijen, Van~Hasselt, Whiteson, and
  Wiering]{van2009theoretical}
Van~Seijen, H., Van~Hasselt, H., Whiteson, S., and Wiering, M.
\newblock A theoretical and empirical analysis of expected sarsa.
\newblock In \emph{Adaptive Dynamic Programming and Reinforcement Learning,
  2009. ADPRL'09. IEEE Symposium on}, pp.\  177--184. IEEE, 2009.

\bibitem[Watkins(1989)]{watkins1989qlearning}
Watkins, C. J. C.~H.
\newblock \emph{Learning from delayed rewards}.
\newblock PhD thesis, King's College, Cambridge, 1989.

\bibitem[Wu et~al.(2017)Wu, Mansimov, Grosse, Liao, and Ba]{acktr}
Wu, Y., Mansimov, E., Grosse, R.~B., Liao, S., and Ba, J.
\newblock Scalable trust-region method for deep reinforcement learning using
  kronecker-factored approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5285--5294, 2017.

\end{thebibliography}
