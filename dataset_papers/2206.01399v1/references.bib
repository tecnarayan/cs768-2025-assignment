@book{Cover2006,
  added-at = {2009-04-20T21:27:16.000+0200},
  at = {2008-03-31 06:17:47},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  howpublished = {Hardcover},
  isbn = {0471241954},
  keywords = {information-theory book},
  month = {July},
  priority = {0},
  publisher = {Wiley-Interscience},
  title = {Elements of Information Theory 2nd Edition (Wiley Series in Telecommunications and Signal Processing)},
  year = 2006
}




% Algorithms for multi-class classification
@ARTICLE{algo_multiclass:Crammer01,
    author = {Koby Crammer and Yoram Singer and Nello Cristianini and John Shawe-taylor and Bob Williamson},
    title = {On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines},
    journal = {Journal of Machine Learning Research},
    year = {2001},
    volume = {2},
    pages = {265–292},
    numpages = {28}
}
@techreport{algo_multiclass:Weston98,
    author = {Jason Weston and Chris Watkins},
    title = {Multi-class Support Vector Machines},
    year = {1998},
}


@article{algo_multiclass:Lee04,
author = {Yoonkyung Lee and Yi Lin and Grace Wahba},
title = {Multicategory Support Vector Machines: Theory and Application to the Classification of Microarray Data and Satellite Radiance Data},
journal = {Journal of the American Statistical Association},
volume = {99},
number = {465},
pages = {67-81},
year  = {2004},
publisher = {Taylor & Francis},
doi = {10.1198/016214504000000098},
URL = { 
        https://doi.org/10.1198/016214504000000098
    
},
eprint = { 
        https://doi.org/10.1198/016214504000000098
}
}

@article{algo_multiclass:Bred99,
author = {Erin J. Bredensteiner and  Kristin P. Bennett},
year = {1999},
pages = {},
title = {Multicategory Classification by Support Vector Machines},
volume = {12},
 pages={53-79},
isbn = {978-1-4613-7367-4},
journal = {Computational Optimization and Applications},
doi = {10.1023/A:1008663629662}
}

@article{algo_multiclass:Dietterich95,
author = {Thomas G Dietterich and Ghulum Bakiri},
title = {Solving Multiclass Learning Problems via Error-Correcting Output Codes},
year = {1994},
issue_date = {August 1994},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {2},
number = {1},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
pages = {263–286},
numpages = {24}
}


%Algorithm comparisons

@article{algo_comparison:Rifkin04,
author = {Ryan Rifkin and Aldebaro Klautau},
year = {2004},
month = {12},
pages = {101-141},
title = {In Defense of One-Vs-All Classification},
volume = {5},
journal = {Journal of Machine Learning Research}
}

@article{algo_comparison:Johannes02,
  title={Round Robin Classification},
  author={Johannes F{\"u}rnkranz},
  journal={Journal of Machine Learning Research},
  year={2002},
  volume={2},
  pages={721-747}
}

@article{algo_comparison:Allwein00,
  title={Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers},
  author={Erin Allwein and Robert E. Schapire and Yoram Singer},
  journal={Journal of Machine Learning Research},
  year={2000},
  volume={1},
  pages={113-141}
}

%Loss functions nets

@ARTICLE{nets_loss_function:Hou16,
       author = {{Hou}, Le and {Yu}, Chen-Ping and {Samaras}, Dimitris},
        title = "{Squared Earth Mover's Distance-based Loss for Training Deep Neural Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2016,
        month = nov,
          eid = {arXiv:1611.05916},
        pages = {arXiv:1611.05916},
archivePrefix = {arXiv},
       eprint = {1611.05916},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv161105916H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{nets_loss_function:Kry17,
author = {Gajowniczek, Krzysztof and Chmielewski, Leszek and Or{\l}owski, Arkadiusz and Z{\k{a}}bkowski, Tomasz},
year = {2017},
month = {10},
pages = {128-136},
title = {Generalized Entropy Cost Function in Neural Networks},
booktitle = { International Conference on Artificial Neural Networks},
isbn = {978-3-319-68611-0},
doi = {10.1007/978-3-319-68612-7_15}
}

@article{nets_loss_function:Kum18,
  title={Robust Loss Functions for Learning Multi-class Classifiers},
  author={Himanshu Kumar and P. Shanti Sastry},
  journal={2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  year={2018},
  pages={687-692}
}

@article{nets_loss_function:Bos20,
author = {Bosman, Anna and Engelbrecht, Andries and Helbig, Mardé},
year = {2020},
month = {03},
pages = {},
title = {Visualising Basins of Attraction for the Cross-Entropy and the Squared Error Neural Network Loss Functions},
volume = {400},
journal = {Neurocomputing},
doi = {10.1016/j.neucom.2020.02.113}
}

@INPROCEEDINGS{nets_loss_function:Dem20,
  author={Demirkaya, Ahmet and Chen, Jiasi and Oymak, Samet},
  booktitle={2020 54th Annual Conference on Information Sciences and Systems (CISS)}, 
  title={Exploring the Role of Loss Functions in Multiclass Classification}, 
  year={2020},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/CISS48834.2020.1570627167}}

@article{nets_loss_function:Kline05,
  title={Revisiting squared-error and cross-entropy functions for training neural network classifiers},
  author={Doug M. Kline and Victor L. Berardi},
  journal={Neural Computing \& Applications},
  year={2005},
  volume={14},
  pages={310-318}
}

@ARTICLE{nets_loss_function:Hui20,
       author = {{Hui}, Like and {Belkin}, Mikhail},
        title = "{Evaluation of Neural Architectures Trained with Square Loss vs Cross-Entropy in Classification Tasks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2020,
        month = jun,
          eid = {arXiv:2006.07322},
        pages = {arXiv:2006.07322},
archivePrefix = {arXiv},
       eprint = {2006.07322},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200607322H},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


%Most relevant theoretical multiclass classification

@inproceedings{multi_class_theory:Thram20,
 author = {Thrampoulidis, Christos and Oymak, Samet and Soltanolkotabi, Mahdi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {8907--8920},
 publisher = {Curran Associates, Inc.},
 title = {Theoretical Insights Into Multiclass Classification: A High-dimensional Asymptotic View},
 url = {https://proceedings.neurips.cc/paper/2020/file/6547884cea64550284728eb26b0947ef-Paper.pdf},
 volume = {33},
 year = {2020}
}


@ARTICLE{multi_class_theory:Wang21,
       author = {{Wang}, Ke and {Muthukumar}, Vidya and {Thrampoulidis}, Christos},
        title = "{Benign Overfitting in Multiclass Classification: All Roads Lead to Interpolation}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Information Theory, Computer Science - Machine Learning},
         year = 2021,
        month = jun,
          eid = {arXiv:2106.10865},
        pages = {arXiv:2106.10865},
archivePrefix = {arXiv},
       eprint = {2106.10865},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv210610865W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


% Binary theory
@article{margin:bartlett2002rademacher,
  title={Rademacher and Gaussian complexities: Risk bounds and structural results},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Nov},
  pages={463--482},
  year={2002}
}

@article{binary:Muth20,
  title={Classification vs regression in overparameterized regimes: Does the loss function matter?},
  author={Vidya Muthukumar and Adhyyan Narang and Vignesh Subramanian and Mikhail Belkin and Daniel J. Hsu and Anant Sahai},
  journal={Journal of Machine Learning Research},
  year={2021},
  volume={22},
  pages={222:1-222:69}
}


%Implicit bias

@book{implicit_bias:engl1996regularization,
  title={Regularization of inverse problems},
  author={Engl, Heinz Werner and Hanke, Martin and Neubauer, Andreas},
  volume={375},
  year={1996},
  publisher={Springer Science \& Business Media}
}

@ARTICLE{implicit_bias:srebro,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
}

@article{implicit_bias:wu2020direction,
  title={Direction matters: On the implicit bias of stochastic gradient descent with moderate learning rate},
  author={Wu, Jingfeng and Zou, Difan and Braverman, Vladimir and Gu, Quanquan},
  journal={arXiv preprint arXiv:2011.02538},
  year={2020}
}

@inproceedings{implicit_bias:ji2019,
  title={The implicit bias of gradient descent on nonseparable data},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={1772--1798},
  year={2019}
}


@article{implicit_bias:woodworth2019kernel,
  title={Kernel and Deep Regimes in Overparametrized Models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:1906.05827},
  year={2019}
}

@inproceedings{implicit_bias:gunasekar2018characterizing,
  title={Characterizing Implicit Bias in Terms of Optimization Geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={1832--1841},
  year={2018}
}

@inproceedings{implicit_bias:nacson2019convergence,
  title={Convergence of Gradient Descent on Separable Data},
  author={Nacson, Mor Shpigel and Lee, Jason and Gunasekar, Suriya and Savarese, Pedro Henrique Pamplona and Srebro, Nathan and Soudry, Daniel},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3420--3428},
  year={2019}
}


@inproceedings{implicit_bias:azizan2020study,
  title={A Study of Generalization of Stochastic Mirror Descent Algorithms on Overparameterized Nonlinear Models},
  author={Azizan, Navid and Lale, Sahin and Hassibi, Babak},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3132--3136},
  year={2020},
  organization={IEEE}
}



%Consistency

@article{consistency:Zhang2004StatisticalAO,
  title={Statistical Analysis of Some Multi-Category Large Margin Classification Methods},
  author={Tong Zhang},
  journal={Journal of Machine Learning Research},
  year={2004},
  volume={5},
  pages={1225-1251}
}

@ARTICLE{consistency:Pires16,
       author = { Pires, Bernardo {\'A}vila and {Szepesv{\'a}ri}, Csaba},
        title = "{Multiclass Classification Calibration Functions}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = 2016,
        month = sep,
          eid = {arXiv:1609.06385},
        pages = {arXiv:1609.06385},
archivePrefix = {arXiv},
       eprint = {1609.06385},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160906385A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{consistency:Pires2013CostsensitiveMC,
author = {Pires, Bernardo \'{A}vila and Ghavamzadeh, Mohammad and Szepesv\'{a}ri, Csaba},
title = {Cost-Sensitive Multiclass Classification Risk Bounds},
year = {2013},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages={1391-1399},
location = {Atlanta, GA, USA},
}

@article{consistency:Tewari05,
author = {Tewari, Ambuj and Bartlett, Peter},
year = {2005},
month = {01},
pages = {143-157},
title = {On the Consistency of Multiclass Classification Methods.},
volume = {8},
journal = {Journal of Machine Learning Research}
}

@article{consistency:Chen06,
author = {Chen, Di-Rong and Sun, Tao},
title = {Consistency of Multiclass Empirical Risk Minimization Methods Based on Convex Loss},
year = {2006},
issue_date = {12/1/2006},
publisher = {JMLR.org},
volume = {7},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
month = {dec},
pages = {2435–2447},
numpages = {13}
}

%Underparameterized multi class theory

@article{multi_class_theory:Kolt02,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2700001},
 author = {Vladimir Koltchinskii and Dmitry Panchenko},
 journal = {The Annals of Statistics},
 number = {1},
 pages = {1--50},
 publisher = {Institute of Mathematical Statistics},
 title = {Empirical Margin Distributions and Bounding the Generalization Error of Combined Classifiers},
 volume = {30},
 year = {2002}
}

@article{multi_class_theory:Gue02,
author = {Guermeur, Yann},
year = {2002},
month = {06},
pages = {168-179},
title = "{Combining Discriminant Models with New Multi-Class SVMs}",
volume = {5},
journal = {Pattern Anal. Appl.},
doi = {10.1007/s100440200015}
}

@inproceedings{multi_class_theory:Li18,
 author = {Li, Jian and Liu, Yong and Yin, Rong and Zhang, Hua and Ding, Lizhong and Wang, Weiping},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Multi-Class Learning: From Theory to Algorithm},
 url = {https://proceedings.neurips.cc/paper/2018/file/1141938ba2c2b13f5505d7c424ebae5f-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{multi_class_theory:Cortes16,
 author = {Cortes, Corinna and Kuznetsov, Vitaly and Mohri, Mehryar and Yang, Scott},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Structured Prediction Theory Based on Factor Graph Complexity},
 url = {https://proceedings.neurips.cc/paper/2016/file/535ab76633d94208236a2e829ea6d888-Paper.pdf},
 volume = {29},
 year = {2016}
}

@inproceedings{multi_class_theory:Lei15,
 author = {Lei, Yunwen and Dogan, Urun and Binder, Alexander and Kloft, Marius},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = "{Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to Novel Algorithms}",
 url = {https://proceedings.neurips.cc/paper/2015/file/3a029f04d76d32e79367c4b3255dda4d-Paper.pdf},
 volume = {28},
 year = {2015}
}


@inproceedings{multi_class_theory:Mau16,
 author = {Andreas Maurer},
 booktitle = {Algorithmic Learning Theory},
 editor = {Ronald Ortner, Hans Ulrich Simon and Sandra Zilles},
 pages = {3-17},
 publisher = {Springer International Publishing},
 title = "{ A vector-contraction inequality for Rademacher complexities}",
 year = {2016}
}

@ARTICLE{multi_class_theory:Lei19,
  author={Lei, Yunwen and Dogan, Ürün and Zhou, Ding-Xuan and Kloft, Marius},
  journal={IEEE Transactions on Information Theory}, 
  title={Data-Dependent Generalization Bounds for Multi-Class Classification}, 
  year={2019},
  volume={65},
  number={5},
  pages={2995-3021},
  doi={10.1109/TIT.2019.2893916}}

@inproceedings{multi_class_theory:Kuz14,
 author = {Kuznetsov, Vitaly and Mohri, Mehryar and Syed, Umar},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Multi-Class Deep Boosting},
 url = {https://proceedings.neurips.cc/paper/2014/file/7bb060764a818184ebb1cc0d43d382aa-Paper.pdf},
 volume = {27},
 year = {2014}
}

@inproceedings{multi_class_theory:Kuznetsov2015RademacherCM,
  title={Rademacher Complexity Margin Bounds for Learning with a Large Number of Classes},
  booktitle = {ICML Workshop on Extreme Classification: Learning
with a Very Large Number of Labels},
  author={Vitaly Kuznetsov and Mehryar Mohri and Umar Syed},
  year={2015}
}

%extreme
@inproceedings{extreme:choromanska2013extreme,
  title={Extreme multi class classification},
  author={Choromanska, Anna and Agarwal, Alekh and Langford, John},
  booktitle={NIPS Workshop: eXtreme Classification, submitted},
  volume={1},
  pages={2--1},
  year={2013}
}

@inproceedings{extreme:yen2016pd,
  title={Pd-sparse: A primal and dual sparse approach to extreme multiclass and multilabel classification},
  author={Yen, Ian En-Hsu and Huang, Xiangru and Ravikumar, Pradeep and Zhong, Kai and Dhillon, Inderjit},
  booktitle={International conference on machine learning},
  pages={3069--3077},
  year={2016},
  organization={PMLR}
}

@article{extreme:rawat2019sampled,
  title={Sampled softmax with random fourier features},
  author={Rawat, Ankit Singh and Chen, Jiecao and Yu, Felix Xinnan X and Suresh, Ananda Theertha and Kumar, Sanjiv},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


%Binary classification CGMT

@article{classification_cgmt:salehi2019impact,
  title={The impact of regularization on high-dimensional logistic regression},
  author={Salehi, Fariborz and Abbasi, Ehsan and Hassibi, Babak},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{classification_cgmt:deng21,
    author = {Deng, Zeyu and Kammoun, Abla and Thrampoulidis, Christos},
    title = "{A model of double descent for high-dimensional binary linear classification}",
    journal = {Information and Inference: A Journal of the IMA},
    year = {2021},
    month = {04},
    issn = {2049-8772},
    doi = {10.1093/imaiai/iaab002},
    url = {https://doi.org/10.1093/imaiai/iaab002},
    note = {iaab002},
    eprint = {https://academic.oup.com/imaiai/advance-article-pdf/doi/10.1093/imaiai/iaab002/36872804/iaab002.pdf},
}


@article{classification_cgmt:kammoun2021precise,
  title={On the precise error analysis of support vector machines},
  author={Kammoun, Abla and AlouiniFellow, Mohamed-Slim},
  journal={IEEE Open Journal of Signal Processing},
  volume={2},
  pages={99--118},
  year={2021},
  publisher={IEEE}
}

@inproceedings{classification_cgmt:taheri2020sharp,
  title={Sharp asymptotics and optimal performance for inference in binary models},
  author={Taheri, Hossein and Pedarsani, Ramtin and Thrampoulidis, Christos},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3739--3749},
  year={2020},
  organization={PMLR}
}

@article{classification_cgmt:montanari2019generalization,
  title={The generalization error of max-margin linear classifiers: High-dimensional asymptotics in the overparametrized regime},
  author={Montanari, Andrea and Ruan, Feng and Sohn, Youngtak and Yan, Jun},
  journal={arXiv preprint arXiv:1911.01544},
  year={2019}
}

@inproceedings{classification_cgmt:kini2020analytic,
  title={Analytic study of double descent in binary classification: The impact of loss},
  author={Kini, Ganesh Ramachandra and Thrampoulidis, Christos},
  booktitle={2020 IEEE International Symposium on Information Theory (ISIT)},
  pages={2527--2532},
  year={2020},
  organization={IEEE}
}

@inproceedings{classification_cgmt:taheri2021fundamental,
  title={Fundamental limits of ridge-regularized empirical risk minimization in high dimensions},
  author={Taheri, Hossein and Pedarsani, Ramtin and Thrampoulidis, Christos},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2773--2781},
  year={2021},
  organization={PMLR}
}

% Binary classification
@article{binary_classification:liang2021interpolating,
  title={Interpolating classifiers make few mistakes},
  author={Liang, Tengyuan and Recht, Benjamin},
  journal={arXiv preprint arXiv:2101.11815},
  year={2021}
}

@article{binary_classification:chatterji2021finite,
  title={Finite-sample analysis of interpolating linear classifiers in the overparameterized regime},
  author={Chatterji, Niladri S and Long, Philip M},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={129},
  pages={1--30},
  year={2021}
}


@article{binary_classification:mai2019high,
  title={High dimensional classification via regularized and unregularized empirical risk minimization: Precise error and optimal loss},
  author={Mai, Xiaoyi and Liao, Zhenyu},
  journal={arXiv preprint arXiv:1905.13742},
  year={2019}
}

@inproceedings{binary_classification:wang2021benign,
  title="{Benign overfitting in binary classification of Gaussian mixtures}",
  author={Wang, Ke and Thrampoulidis, Christos},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4030--4034},
  year={2021},
  organization={IEEE}
}

@article{binary_classification:cao2021risk,
  title={Risk bounds for over-parameterized maximum margin classification on sub-gaussian mixtures},
  author={Cao, Yuan and Gu, Quanquan and Belkin, Mikhail},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{binary_classification:hsu2021proliferation,
  title={On the proliferation of support vectors in high dimensions},
  author={Hsu, Daniel and Muthukumar, Vidya and Xu, Ji},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={91--99},
  year={2021},
  organization={PMLR}
}

@article{binary_classification:mcrae2021harmless,
  title={Harmless interpolation in regression and classification with structured features},
  author={McRae, Andrew D and Karnik, Santhosh and Davenport, Mark A and Muthukumar, Vidya},
  journal={arXiv preprint arXiv:2111.05198},
  year={2021}
}

@article{binary_classification:shamir2022implicit,
  title={The Implicit Bias of Benign Overfitting},
  author={Shamir, Ohad},
  journal={arXiv preprint arXiv:2201.11489},
  year={2022}
}


% Empirical work

@article{d2020underspecification,
  title={Underspecification presents challenges for credibility in modern machine learning},
  author={D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D and others},
  journal={arXiv preprint arXiv:2011.03395},
  year={2020}
}

@article{reg:zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}


@article{reg:neyshabur2014search,
  title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={arXiv preprint arXiv:1412.6614},
  year={2014}
}

% Regression

@article{reg:belkin2018understand,
  title={To understand deep learning we need to understand kernel learning},
  author={Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik},
  journal={ICML},
  year={2018}
}


@article{reg:belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}


@article{reg:geiger2019jamming,
  title={Jamming transition as a paradigm to understand the loss landscape of deep neural networks},
  author={Geiger, Mario and Spigler, Stefano and d'Ascoli, St{\'e}phane and Sagun, Levent and Baity-Jesi, Marco and Biroli, Giulio and Wyart, Matthieu},
  journal={Physical Review E},
  volume={100},
  number={1},
  pages={012115},
  year={2019},
  publisher={APS}
}

}
@article{reg:bartlett2020benign,
  title={Benign overfitting in linear regression},
  author={Bartlett, Peter L and Long, Philip M and Lugosi, G{\'a}bor and Tsigler, Alexander},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30063--30070},
  year={2020},
  publisher={National Academy of Sciences}
}

@article{reg:tsigler2020benign,
  title={Benign overfitting in ridge regression},
  author={Tsigler, Alexander and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2009.14286},
  year={2020}
}


@article{reg:hastie2019surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={arXiv preprint arXiv:1903.08560},
  year={2019}
}

@article{reg:belkin2020two,
  title={Two models of double descent for weak features},
  author={Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={2},
  number={4},
  pages={1167--1180},
  year={2020},
  publisher={SIAM}
}

@article{reg:mei2019generalization,
  title={The generalization error of random features regression: {P}recise asymptotics and double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={arXiv preprint arXiv:1908.05355},
  year={2019}
}

@article{reg:mitra2019understanding,
  title={Understanding overfitting peaks in generalization error: Analytical risk curves for $\ell_2 $ and $\ell_1 $ penalized interpolation},
  author={Mitra, Partha P},
  journal={arXiv preprint arXiv:1906.03667},
  year={2019}
}

@article{reg:muthukumar2020harmless,
  title={Harmless interpolation of noisy data in regression},
  author={Muthukumar, Vidya and Vodrahalli, Kailas and Subramanian, Vignesh and Sahai, Anant},
  journal={IEEE Journal on Selected Areas in Information Theory},
  year={2020},
  volume={1},
  number={1},
  pages={67-83}
}


@article{reg:bibas,
  author    = {Koby Bibas and
               Yaniv Fogel and
               Meir Feder},
  title     = {A New Look at an Old Problem: {A} Universal Learning Approach to Linear
               Regression},
  journal   = {CoRR},
  volume    = {abs/1905.04708},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.04708},
  archivePrefix = {arXiv},
  eprint    = {1905.04708},
  timestamp = {Tue, 28 May 2019 12:48:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-04708.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{reg:nakkiran,
       author = {{Nakkiran}, Preetum},
        title = "{More Data Can Hurt for Linear Regression: Sample-wise Double Descent}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Statistics Theory},
         year = 2019,
        month = dec,
          eid = {arXiv:1912.07242},
        pages = {arXiv:1912.07242},
archivePrefix = {arXiv},
       eprint = {1912.07242},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191207242N},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{reg:kobak2020optimal,
  title={The Optimal Ridge Penalty for Real-world High-dimensional Data Can Be Zero or Negative due to the Implicit Ridge Regularization.},
  author={Kobak, Dmitry and Lomond, Jonathan and Sanchez, Benoit},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={169--1},
  year={2020}
}

@article{reg:wu2020optimal,
  title={On the Optimal Weighted $\ell_2$ Regularization in Overparameterized Linear Regression},
  author={Wu, Denny and Xu, Ji},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={10112--10123},
  year={2020}
}

@inproceedings{reg:richards2021asymptotics,
  title={Asymptotics of ridge (less) regression under general source condition},
  author={Richards, Dominic and Mourtada, Jaouad and Rosasco, Lorenzo},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3889--3897},
  year={2021},
  organization={PMLR}
}

% l1


@article{reg:wang2021tight,
  title={Tight bounds for minimum $\ell_1$-norm interpolation of noisy data},
  author={Wang, Guillaume and Donhauser, Konstantin and Yang, Fanny},
  journal={arXiv preprint arXiv:2111.05987},
  year={2021}
}

@article{reg:li2021minimum,
  title={Minimum $\ell_1$-norm interpolators: Precise asymptotics and multiple descent},
  author={Li, Yue and Wei, Yuting},
  journal={arXiv preprint arXiv:2110.09502},
  year={2021}
}


% Survey papers
@article{survey:bartlett2021deep,
  title={Deep learning: a statistical viewpoint},
  author={Bartlett, Peter L and Montanari, Andrea and Rakhlin, Alexander},
  journal={Acta numerica},
  volume={30},
  pages={87--201},
  year={2021},
  publisher={Cambridge University Press}
}

@article{survey:dar2021farewell,
  title={A farewell to the bias-variance tradeoff? an overview of the theory of overparameterized machine learning},
  author={Dar, Yehuda and Muthukumar, Vidya and Baraniuk, Richard G},
  journal={arXiv preprint arXiv:2109.02355},
  year={2021}
}

@article{survey:belkin2021fit,
  title={Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation},
  author={Belkin, Mikhail},
  journal={Acta Numerica},
  volume={30},
  pages={203--248},
  year={2021},
  publisher={Cambridge University Press}
}

%% margin proof

@MISC {Marginproof,
    TITLE = {Concentration and anti-concentration of gap between largest and second largest value in Gaussian iid sample},
    AUTHOR = {Iosif Pinelis (https://mathoverflow.net/users/36721/iosif-pinelis)},
    HOWPUBLISHED = {MathOverflow},
    NOTE = {URL:https://mathoverflow.net/q/379688 (version: 2020-12-25)},
    EPRINT = {https://mathoverflow.net/q/379688},
    URL = {https://mathoverflow.net/q/379688}
}

%Neurips 2021
@inproceedings{covariate_shift,
 author = {Tripuraneni, Nilesh and Adlam, Ben and Pennington, Jeffrey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {13883--13897},
 publisher = {Curran Associates, Inc.},
 title = {Overparameterization Improves Robustness to Covariate Shift in High Dimensions},
 url = {https://proceedings.neurips.cc/paper/2021/file/73fed7fd472e502d8908794430511f4d-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{label_imbalance,
 author = {Kini, Ganesh Ramachandra and Paraskevas, Orestis and Oymak, Samet and Thrampoulidis, Christos},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {18970--18983},
 publisher = {Curran Associates, Inc.},
 title = {Label-Imbalanced and Group-Sensitive Classification under Overparameterization},
 url = {https://proceedings.neurips.cc/paper/2021/file/9dfcf16f0adbc5e2a55ef02db36bac7f-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{overparameterized_meta_learning,
 author = {Sun, Yue and Narang, Adhyyan and Gulluk, Ibrahim and Oymak, Samet and Fazel, Maryam},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {28156--28168},
 publisher = {Curran Associates, Inc.},
 title = {Towards Sample-efficient Overparameterized Meta-learning},
 url = {https://proceedings.neurips.cc/paper/2021/file/ed46558a56a4a26b96a68738a0d28273-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{rudelson2013hanson,
  title="{Hanson-Wright inequality and sub-Gaussian concentration}",
  author={Rudelson, Mark and Vershynin, Roman},
  journal={Electronic Communications in Probability},
  volume={18},
  pages={1--9},
  year={2013},
  publisher={Institute of Mathematical Statistics and Bernoulli Society}
}

@book{gallager1968information,
  title={Information theory and reliable communication},
  author={Gallager, Robert G},
  volume={588},
  year={1968},
  publisher={Springer}
}