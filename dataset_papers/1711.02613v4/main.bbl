\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba \& Caruana(2014)Ba and Caruana]{ba2013do}
Ba, L.~J. and Caruana, R.
\newblock Do deep nets really need to be deep?
\newblock In \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Buciluǎ et~al.(2006)Buciluǎ, Caruana, and
  Niculescu-Mizil]{bucila2006model}
Buciluǎ, C., Caruana, R., and Niculescu-Mizil, A.
\newblock Model compression.
\newblock In \emph{ACM SIGKDD {I}nternational {C}onference on {K}nowledge
  {D}iscovery and {D}ata {M}ining}, 2006.

\bibitem[Chollet(2016)]{chollet2016xception}
Chollet, F.
\newblock {Xception}: Deep learning with depthwise separable convolutions.
\newblock \emph{arXiv:1610.02357}, 2016.

\bibitem[Cordts et~al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
  Franke, Roth, and Schiele]{Cordts2016Cityscapes}
Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,
  Franke, U., Roth, S., and Schiele, B.
\newblock The cityscapes dataset for semantic urban scene understanding.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2016.

\bibitem[Cybenko(1989)]{cybenko1989approximation}
Cybenko, G.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of Control, Signals, and Systems (MCSS)},
  2\penalty0 (4):\penalty0 303--314, 1989.

\bibitem[Denil et~al.(2013)Denil, Shakibi, Dinh, Ranzato, and
  de~Freitas]{denil2013predicting}
Denil, M., Shakibi, B., Dinh, L., Ranzato, M., and de~Freitas, N.
\newblock Predicting parameters in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2013.

\bibitem[Garipov et~al.(2016)Garipov, Podoprikhin, Novikov, and
  Vetrov]{garipov2016ultimate}
Garipov, T., Podoprikhin, D., Novikov, A., and Vetrov, D.~P.
\newblock Ultimate tensorization: compressing convolutional and {FC} layers
  alike.
\newblock \emph{arXiv:1611.03214}, 2016.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{han2015deep}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and {H}uffman coding.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2016{\natexlab{a}}.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{he2016identity}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European Conference on Computer Vision}, 2016{\natexlab{b}}.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv:1503.02531}, 2015.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{howard2017mobilenets}
Howard, A.~G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T.,
  Andreetto, M., and Adam, H.
\newblock Mobile{N}ets: Efficient convolutional neural networks for mobile
  vision applications.
\newblock \emph{arXiv:1704.04861}, 2017.

\bibitem[Iandola et~al.(2016)Iandola, Moskewicz, Ashraf, Han, Dally, and
  Keutzer]{iandola2016squeeze}
Iandola, F.~N., Moskewicz, M.~W., Ashraf, K., Han, S., Dally, W.~J., and
  Keutzer, K.
\newblock Squeeze{N}et: Alex{N}et-level accuracy with $50\times$ fewer
  parameters and $<1${M}{B} model size.
\newblock \emph{arXiv:1602.07360}, 2016.

\bibitem[Ioannou et~al.(2017)Ioannou, Robertson, Cipolla, and
  Criminisi]{deeproots17}
Ioannou, Y., Robertson, D., Cipolla, R., and Criminisi, A.
\newblock Deep roots: Improving {CNN} efficiency with hierarchical filter
  groups.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2017.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning}, 2015.

\bibitem[Jaderberg et~al.(2014)Jaderberg, Vedaldi, and
  Zisserman]{jaderberg2014speeding}
Jaderberg, M., Vedaldi, A., and Zisserman, A.
\newblock Speeding up convolutional neural networks with low rank expansions.
\newblock In \emph{British Machine Vision Conference}, 2014.

\bibitem[Jin et~al.(2015)Jin, Dundar, and Culurciello]{jin2014flattened}
Jin, J., Dundar, A., and Culurciello, E.
\newblock Flattened convolutional neural networks for feedforward acceleration.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Krizhevsky(2009)]{cifar}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock Master's thesis, University of Toronto, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{alexnet}
Krizhevsky, A., Sutskever, I., and Hinton, G.
\newblock Image{N}et classification with deep convolutional neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2012.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Li et~al.(2017)Li, Wang, Lv, and Yang]{li2017sep}
Li, Z., Wang, X., Lv, X., and Yang, T.
\newblock {S}{E}{P}-{N}ets: Small and effective pattern networks.
\newblock \emph{arXiv:1706.03912}, 2017.

\bibitem[Moczulski et~al.(2016)Moczulski, Denil, Appleyard, and
  de~Freitas]{moczulski2015acdc}
Moczulski, M., Denil, M., Appleyard, J., and de~Freitas, N.
\newblock {ACDC:} a structured efficient linear layer.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Romera et~al.(2017{\natexlab{a}})Romera, {\'A}lvarez, Bergasa, and
  Arroyo]{Romera2017a}
Romera, E., {\'A}lvarez, J.~M., Bergasa, L.~M., and Arroyo, R.
\newblock Efficient convnet for real-time semantic segmentation.
\newblock \emph{IEEE Intelligent Vehicles Symposium}, 2017{\natexlab{a}}.

\bibitem[Romera et~al.(2017{\natexlab{b}})Romera, {\'A}lvarez, Bergasa, and
  Arroyo]{Romera2017b}
Romera, E., {\'A}lvarez, J.~M., Bergasa, L.~M., and Arroyo, R.
\newblock {E}{R}{F}{N}et: Efficient residual factorized convnet for real-time
  semantic segmentation.
\newblock \emph{IEEE Transactions on Intelligent Transportation Systems},
  2017{\natexlab{b}}.

\bibitem[Romero et~al.(2015)Romero, Ballas, Kahou, Chassang, Gatta, and
  Bengio]{romero2014fitnets}
Romero, A., Ballas, N., Kahou, S.~E., Chassang, A., Gatta, C., and Bengio, Y.
\newblock Fit{N}ets: Hints for thin deep nets.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 115\penalty0
  (3):\penalty0 211--252, 2015.
\newblock \doi{10.1007/s11263-015-0816-y}.

\bibitem[Sainath et~al.(2013)Sainath, Kingsbury, Sindhwani, Arisoy, and
  Ramabhadran]{sainath2013low}
Sainath, T.~N., Kingsbury, B., Sindhwani, V., Arisoy, E., and Ramabhadran, B.
\newblock Low-rank matrix factorization for deep neural network training with
  high-dimensional output targets.
\newblock In \emph{IEEE International Conference on Acoustics, Speech and
  Signal Processing}, 2013.

\bibitem[Sifre(2014)]{sifre2014rigid}
Sifre, L.
\newblock \emph{Rigid-Motion Scattering for Image Classification}.
\newblock PhD thesis, \'Ecole Polytechnique, 2014.

\bibitem[Urban et~al.(2017)Urban, Geras, Kahou, Aslan, Wang, Caruana, Mohamed,
  Philipose, and Richardson]{urban2016do}
Urban, G., Geras, K.~J., Kahou, S.~E., Aslan, O., Wang, S., Caruana, R.,
  Mohamed, A., Philipose, M., and Richardson, M.
\newblock Do deep convolutional nets really need to be deep and convolutional?
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Wang et~al.(2016)Wang, Liu, and Foroosh]{wang2016factorized}
Wang, M., Liu, B., and Foroosh, H.
\newblock Design of efficient convolutional layers using single intra-channel
  convolution, topological subdivisioning and spatial bottleneck structure.
\newblock \emph{arXiv:1608.04337}, 2016.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'{a}}r, Tu, and
  He]{xie2016aggregated}
Xie, S., Girshick, R., Doll{\'{a}}r, P., Tu, Z., and He, K.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2017.

\bibitem[Yang et~al.(2015)Yang, Moczulski, Denil, de~Freitas, Smola, Song, and
  Wang]{yang2015deep}
Yang, Z., Moczulski, M., Denil, M., de~Freitas, N., Smola, A., Song, L., and
  Wang, Z.
\newblock Deep fried convnets.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, 2015.

\bibitem[Yu \& Koltun(2016)Yu and Koltun]{dilated2016}
Yu, F. and Koltun, V.
\newblock Multi-scale context aggregation by dilated convolutions.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock In \emph{British Machine Vision Conference}, 2016.

\bibitem[Zagoruyko \& Komodakis(2017)Zagoruyko and
  Komodakis]{zagoruyko2016paying}
Zagoruyko, S. and Komodakis, N.
\newblock Paying more attention to attention: Improving the performance of
  convolutional neural networks via attention transfer.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[{Zhang} et~al.(2018){Zhang}, {Zhou}, {Lin}, and
  {Sun}]{zhang2017shuffle}
{Zhang}, X., {Zhou}, X., {Lin}, M., and {Sun}, J.
\newblock Shuffle{N}et: An extremely efficient convolutional neural network for
  mobile devices.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2018.

\bibitem[Zoph et~al.(2018)Zoph, Vasudevan, Shlens, and Le]{zoph2017learning}
Zoph, B., Vasudevan, V., Shlens, J., and Le, Q.~V.
\newblock Learning transferable architectures for scalable image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2018.

\end{thebibliography}
