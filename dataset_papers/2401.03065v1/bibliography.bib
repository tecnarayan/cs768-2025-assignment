
@article{zheng2023codegeex,
  title={Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x},
  author={Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan and Xue, Yufei and Wang, Zihan and Shen, Lei and Wang, Andi and Li, Yang and others},
  journal={arXiv preprint arXiv:2303.17568},
  year={2023}
}

@article{luo2023wizardcoder,
  title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct},
  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  journal={arXiv preprint arXiv:2306.08568},
  year={2023}
}

@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{allal2023santacoder,
  title={SantaCoder: don't reach for the stars!},
  author={Allal, Loubna Ben and Li, Raymond and Kocetkov, Denis and Mou, Chenghao and Akiki, Christopher and Ferrandis, Carlos Munoz and Muennighoff, Niklas and Mishra, Mayank and Gu, Alex and Dey, Manan and others},
  journal={arXiv preprint arXiv:2301.03988},
  year={2023}
}


@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@inproceedings{wang2021codet5,
  title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={8696--8708},
  year={2021}
}

@inproceedings{nijkamp2022codegen,
  title={CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{li2023starcoder,
  title={StarCoder: may the source be with you!},
  author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
  journal={arXiv preprint arXiv:2305.06161},
  year={2023}
}

@article{wang2023codet5+,
  title={CodeT5+: Open code large language models for code understanding and generation},
  author={Wang, Yue and Le, Hung and Gotmare, Akhilesh Deepak and Bui, Nghi DQ and Li, Junnan and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2305.07922},
  year={2023}
}

@inproceedings{xu2022systematic,
  title={A systematic evaluation of large language models of code},
  author={Xu, Frank F and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent Josua},
  booktitle={Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},
  pages={1--10},
  year={2022}
}

# Benchmarks
@article{cassano2022multipl,
  title={MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation},
  author={Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and Phipps-Costin, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q and others},
  journal={arXiv preprint arXiv:2208.08227},
  year={2022}
}

@article{athiwaratkun2022multi,
  title={Multi-lingual evaluation of code generation models},
  author={Athiwaratkun, Ben and Gouda, Sanjay Krishna and Wang, Zijian and Li, Xiaopeng and Tian, Yuchen and Tan, Ming and Ahmad, Wasi Uddin and Wang, Shiqi and Sun, Qing and Shang, Mingyue and others},
  journal={arXiv preprint arXiv:2210.14868},
  year={2022}
}

@article{wang2022recode,
  title={ReCode: Robustness Evaluation of Code Generation Models},
  author={Wang, Shiqi and Li, Zheng and Qian, Haifeng and Yang, Chenghao and Wang, Zijian and Shang, Mingyue and Kumar, Varun and Tan, Samson and Ray, Baishakhi and Bhatia, Parminder and others},
  journal={arXiv preprint arXiv:2212.10264},
  year={2022}
}

@article{hendrycks2021measuring,
  title={Measuring coding challenge competence with apps},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  journal={arXiv preprint arXiv:2105.09938},
  year={2021}
}

@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{shinn2023reflexion,
  title={Reflexion: an autonomous agent with dynamic memory and self-reflection},
  author={Shinn, Noah and Labash, Beck and Gopinath, Ashwin},
  journal={arXiv preprint arXiv:2303.11366},
  year={2023}
}

@article{liu2023repobench,
  title={RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems},
  author={Liu, Tianyang and Xu, Canwen and McAuley, Julian},
  journal={arXiv preprint arXiv:2306.03091},
  year={2023}
}

@article{agashe2019juice,
  title={Juice: A large scale distantly supervised dataset for open domain context-based code generation},
  author={Agashe, Rajas and Iyer, Srinivasan and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.02216},
  year={2019}
}

@article{patil2023gorilla,
  title={Gorilla: Large language model connected with massive apis},
  author={Patil, Shishir G and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2305.15334},
  year={2023}
}

@article{roziere2020unsupervised,
  title={Unsupervised translation of programming languages},
  author={Roziere, Baptiste and Lachaux, Marie-Anne and Chanussot, Lowik and Lample, Guillaume},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20601--20611},
  year={2020}
}

@article{zhu2022xlcost,
  title={Xlcost: A benchmark dataset for cross-lingual code intelligence},
  author={Zhu, Ming and Jain, Aneesh and Suresh, Karthik and Ravindran, Roshan and Tipirneni, Sindhu and Reddy, Chandan K},
  journal={arXiv preprint arXiv:2206.08474},
  year={2022}
}

@article{zhang2023toolcoder,
  title={ToolCoder: Teach Code Generation Models to use APIs with search tools},
  author={Zhang, Kechi and Li, Ge and Li, Jia and Li, Zhuo and Jin, Zhi},
  journal={arXiv preprint arXiv:2305.04032},
  year={2023}
}

@article{yin2022natural,
  title={Natural language to code generation in interactive data science notebooks},
  author={Yin, Pengcheng and Li, Wen-Ding and Xiao, Kefan and Rao, Abhishek and Wen, Yeming and Shi, Kensen and Howland, Joshua and Bailey, Paige and Catasta, Michele and Michalewski, Henryk and others},
  journal={arXiv preprint arXiv:2212.09248},
  year={2022}
}

@inproceedings{lai2023ds,
  title={DS-1000: A natural and reliable benchmark for data science code generation},
  author={Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
  booktitle={International Conference on Machine Learning},
  pages={18319--18345},
  year={2023},
  organization={PMLR}
}

@inproceedings{jain2022jigsaw,
  title={Jigsaw: Large language models meet program synthesis},
  author={Jain, Naman and Vaidyanath, Skanda and Iyer, Arun and Natarajan, Nagarajan and Parthasarathy, Suresh and Rajamani, Sriram and Sharma, Rahul},
  booktitle={Proceedings of the 44th International Conference on Software Engineering},
  pages={1219--1231},
  year={2022}
}

@article{husain2019codesearchnet,
  title={Codesearchnet challenge: Evaluating the state of semantic code search},
  author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
  journal={arXiv preprint arXiv:1909.09436},
  year={2019}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{li2023textbooks,
  title={Textbooks are all you need ii: phi-1.5 technical report},
  author={Li, Yuanzhi and Bubeck, S{\'e}bastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
  journal={arXiv preprint arXiv:2309.05463},
  year={2023}
}

@article{gunasekar2023textbooks,
  title={Textbooks Are All You Need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{openai2023gpt,
  title={GPT-4 technical report. arXiv 2303.08774},
  author={OpenAI, R},
  journal={View in Article},
  year={2023}
}

@misc{Phind,
 author={Michael Royzen and Justin Wei and Russell Coleman},
 title={Phind},
 year={2023},
 url={https://www.phind.com},
}


@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

% Test cases
@article{key2022speak,
  title={I speak, you verify: Toward trustworthy neural program synthesis},
  author={Key, Darren and Li, Wen-Ding and Ellis, Kevin},
  journal={arXiv preprint arXiv:2210.00848},
  year={2022}
}

@article{chen2022codet,
  title={Codet: Code generation with generated tests},
  author={Chen, Bei and Zhang, Fengji and Nguyen, Anh and Zan, Daoguang and Lin, Zeqi and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2207.10397},
  year={2022}
}

@inproceedings{zhang2023coder,
  title={Coder reviewer reranking for code generation},
  author={Zhang, Tianyi and Yu, Tao and Hashimoto, Tatsunori and Lewis, Mike and Yih, Wen-tau and Fried, Daniel and Wang, Sida},
  booktitle={International Conference on Machine Learning},
  pages={41832--41846},
  year={2023},
  organization={PMLR}
}

@article{liu2023code,
  title={Code Execution with Pre-trained Language Models},
  author={Liu, Chenxiao and Lu, Shuai and Chen, Weizhu and Jiang, Daxin and Svyatkovskiy, Alexey and Fu, Shengyu and Sundaresan, Neel and Duan, Nan},
  journal={arXiv preprint arXiv:2305.05383},
  year={2023}
}

@article{dong2023codescore,
  title={Codescore: Evaluating code generation by learning code execution},
  author={Dong, Yihong and Ding, Jiazheng and Jiang, Xue and Li, Zhuo and Li, Ge and Jin, Zhi},
  journal={arXiv preprint arXiv:2301.09043},
  year={2023}
}

@article{tian2023test,
  title={Test-Case-Driven Programming Understanding in Large Language Models for Better Code Generation},
  author={Tian, Zhao and Chen, Junjie},
  journal={arXiv preprint arXiv:2309.16120},
  year={2023}
}

@inproceedings{ni2023lever,
  title={Lever: Learning to verify language-to-code generation with execution},
  author={Ni, Ansong and Iyer, Srini and Radev, Dragomir and Stoyanov, Veselin and Yih, Wen-tau and Wang, Sida and Lin, Xi Victoria},
  booktitle={International Conference on Machine Learning},
  pages={26106--26128},
  year={2023},
  organization={PMLR}
}

@article{shi2022natural,
  title={Natural language to code translation with execution},
  author={Shi, Freda and Fried, Daniel and Ghazvininejad, Marjan and Zettlemoyer, Luke and Wang, Sida I},
  journal={arXiv preprint arXiv:2204.11454},
  year={2022}
}

@article{zhong2022codegen,
  title={CodeGen-Test: An Automatic Code Generation Model Integrating Program Test Information},
  author={Zhong, Maosheng and Liu, Gen and Li, Hongwei and Kuang, Jiangling and Zeng, Jinshan and Wang, Mingwen},
  journal={arXiv preprint arXiv:2202.07612},
  year={2022}
}

@article{zhang2023algo,
  title={ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers},
  author={Zhang, Kexun and Wang, Danqing and Xia, Jingtao and Wang, William Yang and Li, Lei},
  journal={arXiv preprint arXiv:2305.14591},
  year={2023}
}

@article{chen2023teaching,
  title={Teaching large language models to self-debug},
  author={Chen, Xinyun and Lin, Maxwell and Sch{\"a}rli, Nathanael and Zhou, Denny},
  journal={arXiv preprint arXiv:2304.05128},
  year={2023}
}

@article{haluptzok2022language,
  title={Language models can teach themselves to program better},
  author={Haluptzok, Patrick and Bowers, Matthew and Kalai, Adam Tauman},
  journal={arXiv preprint arXiv:2207.14502},
  year={2022}
}

@article{olausson2023demystifying,
  title={Demystifying GPT Self-Repair for Code Generation},
  author={Olausson, Theo X and Inala, Jeevana Priya and Wang, Chenglong and Gao, Jianfeng and Solar-Lezama, Armando},
  journal={arXiv preprint arXiv:2306.09896},
  year={2023}
}


@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{wang2022execution,
  title={Execution-based evaluation for open-domain code generation},
  author={Wang, Zhiruo and Zhou, Shuyan and Fried, Daniel and Neubig, Graham},
  journal={arXiv preprint arXiv:2212.10481},
  year={2022}
}


@article{le2022coderl,
  title={Coderl: Mastering code generation through pretrained models and deep reinforcement learning},
  author={Le, Hung and Wang, Yue and Gotmare, Akhilesh Deepak and Savarese, Silvio and Hoi, Steven Chu Hong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={21314--21328},
  year={2022}
}

@article{fan2023large, title={Large Language Models for Software Engineering: Survey and Open Problems}, author={Fan, Angela and Gokkaya, Beliz and Harman, Mark and Lyubarskiy, Mitya and Sengupta, Shubho and Yoo, Shin and Zhang, Jie M}, journal={arXiv preprint arXiv:2310.03533}, year={2023} }

@inproceedings{zan2023large,
  title={Large language models meet NL2Code: A survey},
  author={Zan, Daoguang and Chen, Bei and Zhang, Fengji and Lu, Dianjie and Wu, Bingchao and Guan, Bei and Yongji, Wang and Lou, Jian-Guang},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={7443--7464},
  year={2023}
}

@article{jimenez2023swe,
  title={SWE-bench: Can Language Models Resolve Real-World GitHub Issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2310.06770},
  year={2023}
}

@article{liu2023your,
  title={Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation},
  author={Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  journal={arXiv preprint arXiv:2305.01210},
  year={2023}
}

@article{gudibande2023false,
  title={The false promise of imitating proprietary llms},
  author={Gudibande, Arnav and Wallace, Eric and Snell, Charlie and Geng, Xinyang and Liu, Hao and Abbeel, Pieter and Levine, Sergey and Song, Dawn},
  journal={arXiv preprint arXiv:2305.15717},
  year={2023}
}

@inproceedings{zhang2023self,
    title = "Self-Edit: Fault-Aware Code Editor for Code Generation",
    author = "Zhang, Kechi  and
      Li, Zhuo  and
      Li, Jia  and
      Li, Ge  and
      Jin, Zhi",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    pages = "769--787"
}

@article{peng2023check,
  title={Check your facts and try again: Improving large language models with external knowledge and automated feedback},
  author={Peng, Baolin and Galley, Michel and He, Pengcheng and Cheng, Hao and Xie, Yujia and Hu, Yu and Huang, Qiuyuan and Liden, Lars and Yu, Zhou and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2302.12813},
  year={2023}
}

@article{madaan2023self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2303.17651},
  year={2023}
}

@article{shen2023pangu,
  title={Pangu-coder2: Boosting large language models for code with ranking feedback},
  author={Shen, Bo and Zhang, Jiaxin and Chen, Taihong and Zan, Daoguang and Geng, Bing and Fu, An and Zeng, Muhan and Yu, Ailun and Ji, Jichuan and Zhao, Jingyang and others},
  journal={arXiv preprint arXiv:2307.14936},
  year={2023}
}

@article{zhang2023planning,
  title={Planning with large language models for code generation},
  author={Zhang, Shun and Chen, Zhenfang and Shen, Yikang and Ding, Mingyu and Tenenbaum, Joshua B and Gan, Chuang},
  journal={arXiv preprint arXiv:2303.05510},
  year={2023}
}

@inproceedings{malik2019nl2type,
  title={NL2Type: inferring JavaScript function types from natural language information},
  author={Malik, Rabee Sohail and Patra, Jibesh and Pradel, Michael},
  booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)},
  pages={304--315},
  year={2019},
  organization={IEEE}
}

@article{wei2023typet5,
  title={TypeT5: Seq2seq Type Inference using Static Analysis},
  author={Wei, Jiayi and Durrett, Greg and Dillig, Isil},
  journal={arXiv preprint arXiv:2303.09564},
  year={2023}
}

@inproceedings{mir2022type4py,
  title={Type4py: Practical deep similarity learning-based type inference for python},
  author={Mir, Amir M and Lato{\v{s}}kinas, Evaldas and Proksch, Sebastian and Gousios, Georgios},
  booktitle={Proceedings of the 44th International Conference on Software Engineering},
  pages={2241--2252},
  year={2022}
}

@article{liu2020atom,
  title={ATOM: Commit message generation based on abstract syntax tree and hybrid ranking},
  author={Liu, Shangqing and Gao, Cuiyun and Chen, Sen and Nie, Lun Yiu and Liu, Yang},
  journal={IEEE Transactions on Software Engineering},
  volume={48},
  number={5},
  pages={1800--1817},
  year={2020},
  publisher={IEEE}
}

% code summarization
@article{hasan2021codesc,
  title={Codesc: A large code-description parallel dataset},
  author={Hasan, Masum and Muttaqueen, Tanveer and Ishtiaq, Abdullah Al and Mehrab, Kazi Sajeed and Haque, Md Mahim Anjum and Hasan, Tahmid and Ahmad, Wasi Uddin and Iqbal, Anindya and Shahriyar, Rifat},
  journal={arXiv preprint arXiv:2105.14220},
  year={2021}
}

@article{barone2017parallel,
  title={A parallel corpus of python functions and documentation strings for automated code documentation and code generation},
  author={Barone, Antonio Valerio Miceli and Sennrich, Rico},
  journal={arXiv preprint arXiv:1707.02275},
  year={2017}
}

@inproceedings{iyer2016summarizing,
  title={Summarizing source code using a neural attention model},
  author={Iyer, Srinivasan and Konstas, Ioannis and Cheung, Alvin and Zettlemoyer, Luke},
  booktitle={54th Annual Meeting of the Association for Computational Linguistics 2016},
  pages={2073--2083},
  year={2016},
  organization={Association for Computational Linguistics}
}

@inproceedings{leclair2019neural,
  title={A neural model for generating natural language summaries of program subroutines},
  author={LeClair, Alexander and Jiang, Siyuan and McMillan, Collin},
  booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)},
  pages={795--806},
  year={2019},
  organization={IEEE}
}

@article{alon2018code2seq,
  title={code2seq: Generating sequences from structured representations of code},
  author={Alon, Uri and Brody, Shaked and Levy, Omer and Yahav, Eran},
  journal={arXiv preprint arXiv:1808.01400},
  year={2018}
}

@article{ahmad-etal-2021-avatar,
  title={AVATAR: A Parallel Corpus for Java-Python Program Translation},
  author={Ahmad, Wasi Uddin and Tushar, Md Golam Rahman and Chakraborty, Saikat and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2108.11590},
  year={2021}
}

% Optimization
@article{garg2022deepperf,
  title={DeepPERF: A Deep Learning-Based Approach For Improving Software Performance},
  author={Garg, Spandan and Moghaddam, Roshanak Zilouchian and Clement, Colin B and Sundaresan, Neel and Wu, Chen},
  journal={arXiv preprint arXiv:2206.13619},
  year={2022}
}

@article{madaan2023learning,
  title={Learning performance-improving code edits},
  author={Madaan, Aman and Shypula, Alexander and Alon, Uri and Hashemi, Milad and Ranganathan, Parthasarathy and Yang, Yiming and Neubig, Graham and Yazdanbakhsh, Amir},
  journal={arXiv preprint arXiv:2302.07867},
  year={2023}
}

% Repair
@inproceedings{gupta2017deepfix,
  title={Deepfix: Fixing common c language errors by deep learning},
  author={Gupta, Rahul and Pal, Soham and Kanade, Aditya and Shevade, Shirish},
  booktitle={Proceedings of the aaai conference on artificial intelligence},
  volume={31},
  number={1},
  year={2017}
}

@article{jin2023inferfix,
  title={Inferfix: End-to-end program repair with llms},
  author={Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey},
  journal={arXiv preprint arXiv:2303.07263},
  year={2023}
}

@article{haque2022fixeval,
  title={Fixeval: Execution-based evaluation of program fixes for competitive programming problems},
  author={Haque, Md Mahim Anjum and Ahmad, Wasi Uddin and Lourentzou, Ismini and Brown, Chris},
  year={2022}
}

@article{tufano2019empirical,
  title={An empirical study on learning bug-fixing patches in the wild via neural machine translation},
  author={Tufano, Michele and Watson, Cody and Bavota, Gabriele and Penta, Massimiliano Di and White, Martin and Poshyvanyk, Denys},
  journal={ACM Transactions on Software Engineering and Methodology (TOSEM)},
  volume={28},
  number={4},
  pages={1--29},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{xia2022practical,
  title={Practical program repair in the era of large pre-trained language models},
  author={Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming},
  journal={arXiv preprint arXiv:2210.14179},
  year={2022}
}

@article{jiang2023impact,
  title={Impact of code language models on automated program repair},
  author={Jiang, Nan and Liu, Kevin and Lutellier, Thibaud and Tan, Lin},
  journal={arXiv preprint arXiv:2302.05020},
  year={2023}
}

@inproceedings{berabi2021tfix,
  title={Tfix: Learning to fix coding errors with a text-to-text transformer},
  author={Berabi, Berkay and He, Jingxuan and Raychev, Veselin and Vechev, Martin},
  booktitle={International Conference on Machine Learning},
  pages={780--791},
  year={2021},
  organization={PMLR}
}

@article{tony2023llmseceval,
  title={LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations},
  author={Tony, Catherine and Mutas, Markus and Ferreyra, Nicol{\'a}s E D{\'\i}az and Scandariato, Riccardo},
  journal={arXiv preprint arXiv:2303.09384},
  year={2023}
}

@inproceedings{pearce2022asleep,
  title={Asleep at the keyboard? assessing the security of github copilot’s code contributions},
  author={Pearce, Hammond and Ahmad, Baleegh and Tan, Benjamin and Dolan-Gavitt, Brendan and Karri, Ramesh},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)},
  pages={754--768},
  year={2022},
  organization={IEEE}
}

@article{liguori2022can,
  title={Can we generate shellcodes via natural language? An empirical study},
  author={Liguori, Pietro and Al-Hossami, Erfan and Cotroneo, Domenico and Natella, Roberto and Cukic, Bojan and Shaikh, Samira},
  journal={Automated Software Engineering},
  volume={29},
  number={1},
  pages={30},
  year={2022},
  publisher={Springer}
}

@inproceedings{shrivastava2023repository,
  title={Repository-level prompt generation for large language models of code},
  author={Shrivastava, Disha and Larochelle, Hugo and Tarlow, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={31693--31715},
  year={2023},
  organization={PMLR}
}

@article{zhang2023repocoder,
  title={Repocoder: Repository-level code completion through iterative retrieval and generation},
  author={Zhang, Fengji and Chen, Bei and Zhang, Yue and Liu, Jin and Zan, Daoguang and Mao, Yi and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2303.12570},
  year={2023}
}

@article{ding2022cocomic,
  title={Cocomic: Code completion by jointly modeling in-file and cross-file context},
  author={Ding, Yangruibo and Wang, Zijian and Ahmad, Wasi Uddin and Ramanathan, Murali Krishna and Nallapati, Ramesh and Bhatia, Parminder and Roth, Dan and Xiang, Bing},
  journal={arXiv preprint arXiv:2212.10007},
  year={2022}
}

@inproceedings{tufano2022methods2test,
  title={Methods2Test: A dataset of focal methods mapped to test cases},
  author={Tufano, Michele and Deng, Shao Kun and Sundaresan, Neel and Svyatkovskiy, Alexey},
  booktitle={Proceedings of the 19th International Conference on Mining Software Repositories},
  pages={299--303},
  year={2022}
}

@inproceedings{watson2020learning,
  title={On learning meaningful assert statements for unit test cases},
  author={Watson, Cody and Tufano, Michele and Moran, Kevin and Bavota, Gabriele and Poshyvanyk, Denys},
  booktitle={Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
  pages={1398--1409},
  year={2020}
}

@article{berglund2023reversal,
  title={The Reversal Curse: LLMs trained on" A is B" fail to learn" B is A"},
  author={Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain},
  journal={arXiv preprint arXiv:2309.12288},
  year={2023}
}

@article{wu2023reasoning,
  title={Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks},
  author={Wu, Zhaofeng and Qiu, Linlu and Ross, Alexis and Aky{\"u}rek, Ekin and Chen, Boyuan and Wang, Bailin and Kim, Najoung and Andreas, Jacob and Kim, Yoon},
  journal={arXiv preprint arXiv:2307.02477},
  year={2023}
}

@inproceedings{shi2023large,
  title={Large language models can be easily distracted by irrelevant context},
  author={Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed H and Sch{\"a}rli, Nathanael and Zhou, Denny},
  booktitle={International Conference on Machine Learning},
  pages={31210--31227},
  year={2023},
  organization={PMLR}
}

@article{hosseini2021understanding,
  title={Understanding by understanding not: Modeling negation in language models},
  author={Hosseini, Arian and Reddy, Siva and Bahdanau, Dzmitry and Hjelm, R Devon and Sordoni, Alessandro and Courville, Aaron},
  journal={arXiv preprint arXiv:2105.03519},
  year={2021}
}


@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{miceli2023larger,
  title={The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python},
  author={Miceli-Barone, Antonio Valerio and Barez, Fazl and Konstas, Ioannis and Cohen, Shay B},
  journal={arXiv preprint arXiv:2305.15507},
  year={2023}
}

@article{arkoudas2023gpt,
  title={GPT-4 Can't Reason},
  author={Arkoudas, Konstantine},
  journal={arXiv preprint arXiv:2308.03762},
  year={2023}
}

@article{liu2023evaluating,
  title={Evaluating the logical reasoning ability of chatgpt and gpt-4},
  author={Liu, Hanmeng and Ning, Ruoxi and Teng, Zhiyang and Liu, Jian and Zhou, Qiji and Zhang, Yue},
  journal={arXiv preprint arXiv:2304.03439},
  year={2023}
}

@article{zhang2023survey,
      title={A Survey on Language Models for Code}, 
      author={Ziyin Zhang and Chaoyu Chen and Bingchang Liu and Cong Liao and Zi Gong and Hang Yu and Jianguo Li and Rui Wang},
      year={2023},
      eprint={2311.07989},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{zhang2022paradox,
  title={On the paradox of learning to reason from data},
  author={Zhang, Honghua and Li, Liunian Harold and Meng, Tao and Chang, Kai-Wei and Broeck, Guy Van den},
  journal={arXiv preprint arXiv:2205.11502},
  year={2022}
}

@article{olausson2023linc,
  title={LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers},
  author={Olausson, Theo X and Gu, Alex and Lipkin, Benjamin and Zhang, Cedegao E and Solar-Lezama, Armando and Tenenbaum, Joshua B and Levy, Roger},
  journal={arXiv preprint arXiv:2310.15164},
  year={2023}
}

@article{lee2023teaching,
  title={Teaching arithmetic to small transformers},
  author={Lee, Nayoung and Sreenivasan, Kartik and Lee, Jason D and Lee, Kangwook and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2307.03381},
  year={2023}
}

@article{zhou2023algorithms,
  title={What Algorithms can Transformers Learn? A Study in Length Generalization},
  author={Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2310.16028},
  year={2023}
}

@article{dziri2023faith,
  title={Faith and Fate: Limits of Transformers on Compositionality},
  author={Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jian, Liwei and Lin, Bill Yuchen and West, Peter and Bhagavatula, Chandra and Bras, Ronan Le and Hwang, Jena D and others},
  journal={arXiv preprint arXiv:2305.18654},
  year={2023}
}

@article{merrill2023expresssive,
  title={The Expresssive Power of Transformers with Chain of Thought},
  author={Merrill, William and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2310.07923},
  year={2023}
}

@article{giannou2023looped,
  title={Looped transformers as programmable computers},
  author={Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2301.13196},
  year={2023}
}

@article{zhang2023can,
  title={Can Transformers Learn to Solve Problems Recursively?},
  author={Zhang, Shizhuo Dylan and Tigges, Curt and Biderman, Stella and Raginsky, Maxim and Ringer, Talia},
  journal={arXiv preprint arXiv:2305.14699},
  year={2023}
}

@article{uesato2022solving,
  title={Solving math word problems with process-and outcome-based feedback},
  author={Uesato, Jonathan and Kushman, Nate and Kumar, Ramana and Song, Francis and Siegel, Noah and Wang, Lisa and Creswell, Antonia and Irving, Geoffrey and Higgins, Irina},
  journal={arXiv preprint arXiv:2211.14275},
  year={2022}
}


@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}

@article{tyen2023llms,
  title={LLMs cannot find reasoning errors, but can correct them!},
  author={Tyen, Gladys and Mansoor, Hassan and Chen, Peter and Mak, Tony and C{\u{a}}rbune, Victor},
  journal={arXiv preprint arXiv:2311.08516},
  year={2023}
}

@article{fried2022incoder,
      title={InCoder: A Generative Model for Code Infilling and Synthesis}, 
      author={Daniel Fried and Armen Aghajanyan and Jessy Lin and Sida Wang and Eric Wallace and Freda Shi and Ruiqi Zhong and Wen-tau Yih and Luke Zettlemoyer and Mike Lewis},
      year={2022},
      journal={preprint arXiv:2204.05999},
      primaryClass={cs.SE}
}

@misc{deepseek-coder,
  author = {DeepSeek AI},
  title = {DeepSeek Coder: Let the Code Write Itself},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/deepseek-ai/DeepSeek-Coder}},
}

@inproceedings{bender-koller-2020-climbing,
    title = "Climbing towards {NLU}: {On} Meaning, Form, and Understanding in the Age of Data",
    author = "Bender, Emily M.  and
      Koller, Alexander",
    booktitle = "Proc of ACL",
    month = jul,
    year = "2020",
    doi = "10.18653/v1/2020.acl-main.463",
}

@article{merrill2021provable,
  title={Provable Limitations of Acquiring Meaning from Ungrounded Form: What Will Future Language Models Understand?},
  author={William Cooper Merrill and Yoav Goldberg and Roy Schwartz and Noah A. Smith},
  journal={Transactions of the Association for Computational Linguistics},
  year={2021},
  volume={9},
  pages={1047-1060},
}

@article{mizrahi2023state,
  title={State of What Art? A Call for Multi-Prompt LLM Evaluation},
  author={Mizrahi, Moran and Kaplan, Guy and Malkin, Dan and Dror, Rotem and Shahaf, Dafna and Stanovsky, Gabriel},
  journal={arXiv preprint arXiv:2401.00595},
  year={2023}
}