%
@article{howard2017mobilenets,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}
%
@inproceedings{pytorch2015,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={8026--8037},
  year={2019}
}
%
@article{bengio2013estimating,
  title={Estimating or propagating gradients through stochastic neurons for conditional computation},
  author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  journal={arXiv preprint arXiv:1308.3432},
  year={2013}
}
%
@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}
%
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={248--255},
  year={2009},
}
@inproceedings{wang2020sparse,
  title={SparseRT: Accelerating Unstructured Sparsity on GPUs for Deep Learning Inference},
  author={Ziheng Wang},
  booktitle={Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques (ICPACT)},
  pages={31--42},
  years={2020},
}

%object detection
@InProceedings{he2017mask,
author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
title = {Mask R-CNN},
booktitle = {IEEE International Conference on Computer Vision (ICCV)},
year = {2017}
}
%
@inproceedings{girshick2015fast,
  title={Fast r-cnn},
  author={Girshick, Ross},
  booktitle={IEEE International Conference on Computer Vision (ICCV)},
  pages={1440--1448},
  year={2015}
}
%semantic segmentation
@inproceedings{girshick2014rich,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle={IEEE International Conference on Computer Vision (ICCV)},
  pages={580--587},
  year={2014}
}
%
@article{croitoru2019unsupervised,
  title={Unsupervised learning of foreground object segmentation},
  author={Croitoru, Ioana and Bogolin, Simion-Vlad and Leordeanu, Marius},
  journal={International Journal of Computer Vision (IJCV)},
  volume={127},
  number={9},
  pages={1279--1302},
  year={2019}
}
%image classification
@inproceedings{simonyan2015very,
  author    = {Karen Simonyan and Andrew Zisserman},
  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2015},
}
%
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={770--778},
  year={2016}
}
%quantization
@article{hubara2016binarized,
  title={Binarized neural networks},
  author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={29},
  year={2016}
}
%
@inproceedings{zhou2017incremental,
  title={Incremental network quantization: Towards lossless cnns with low-precision weights},
  author={Zhou, Aojun and Yao, Anbang and Guo, Yiwen and Xu, Lin and Chen, Yurong},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017}
}
%compact-design
@inproceedings{sandler2018mobilenetv2,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={4510--4520},
  year={2018}
}
%
@inproceedings{ding2021diverse,
  title={Diverse branch block: Building a convolution as an inception-like unit},
  author={Ding, Xiaohan and Zhang, Xiangyu and Han, Jungong and Ding, Guiguang},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={10886--10895},
  year={2021}
}
%tensor factorization
@inproceedings{hayashi2019exploring,
  title={Exploring Unexplored Tensor Network Decompositions for Convolutional Neural Networks},
  author={Hayashi, Kohei and Yamaguchi, Taiki and Sugawara, Yohei and Maeda, Shin-ichi},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={5552--5562},
  year={2019}
}
%
@inproceedings{kim2019efficient,
  title={Efficient neural network compression},
  author={Kim, Hyeji and Khan, Muhammad Umar Karim and Kyung, Chong-Min},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={12569--12577},
  year={2019}
}
%structured sparsity
@inproceedings{luo2017thinet,
  title={Thinet: A filter level pruning method for deep neural network compression},
  author={Luo, Jian-Hao and Wu, Jianxin and Lin, Weiyao},
  booktitle={IEEE International Conference on Computer Vision (ICCV)},
  pages={5058--5066},
  year={2017}
}
%
@article{luo2020autopruner,
  title={Autopruner: An end-to-end trainable filter pruning method for efficient deep model inference},
  author={Luo, Jian-Hao and Wu, Jianxin},
  journal={Pattern Recognition (PR)},
  pages={107461},
  year={2020},
}
%
@inproceedings{luo2020neural,
  title={Neural Network Pruning with Residual-Connections and Limited-Data},
  author={Luo, Jian-Hao and Wu, Jianxin},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},  
  pages={1458--1467},
  year={2020}
}
%
@inproceedings{ning2020dsa,
  title={Dsa: More efficient budgeted pruning via differentiable sparsity allocation},
  author={Ning, Xuefei and Zhao, Tianchen and Li, Wenshuo and Lei, Peng and Wang, Yu and Yang, Huazhong},
  booktitle={European Conference on Computer Vision (ECCV)},
  pages={592--607},
  year={2020},
  organization={Springer}
}
%
@inproceedings{he2018soft,
  title     = {Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks},
  author    = {He, Yang and Kang, Guoliang and Dong, Xuanyi and Fu, Yanwei and Yang, Yi},
  booktitle = {International Joint Conference on Artificial Intelligence (IJCAI)},
  pages     = {2234--2240},
  year      = {2018}
}
@inproceedings{he2019filter,
  title={Filter pruning via geometric median for deep convolutional neural networks acceleration},
  author={He, Yang and Liu, Ping and Wang, Ziwei and Hu, Zhilan and Yang, Yi},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={4340--4349},
  year={2019}
}
@inproceedings{he2020learning,
  title={Learning Filter Pruning Criteria for Deep Convolutional Neural Networks Acceleration},
  author={He, Yang and Ding, Yuhang and Liu, Ping and Zhu, Linchao and Zhang, Hanwang and Yang, Yi},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},  
  pages={2009--2018},
  year={2020}
}
%
@inproceedings{ding2021resrep,
  title={Resrep: Lossless cnn pruning via decoupling remembering and forgetting},
  author={Ding, Xiaohan and Hao, Tianxiang and Tan, Jianchao and Liu, Ji and Han, Jungong and Guo, Yuchen and Ding, Guiguang},
  booktitle={IEEE International Conference on Computer Vision (ICCV)},
  pages={4510--4520},
  year={2021}
}
%
@inproceedings{ding2019centripetal,
  title={Centripetal sgd for pruning very deep convolutional networks with complicated structure},
  author={Ding, Xiaohan and Ding, Guiguang and Guo, Yuchen and Han, Jungong},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={4943--4953},
  year={2019}
}
%
@inproceedings{ding2019approximated,
  title={Approximated oracle filter pruning for destructive cnn width optimization},
  author={Ding, Xiaohan and Ding, Guiguang and Guo, Yuchen and Han, Jungong and Yan, Chenggang},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={1607--1616},
  year={2019},
  organization={PMLR}
}
%
@inproceedings{ding2018auto,
  title={Auto-balanced filter pruning for efficient convolutional neural networks},
  author={Ding, Xiaohan and Ding, Guiguang and Han, Jungong and Tang, Sheng},
  booktitle={AAAI Conference on Artificial Intelligence (AAAI)},
  volume={32},
  number={1},
  year={2018}
}
%
@article{zhang2022carrying,
  title={Carrying Out CNN Channel Pruning in a White Box},
  author={Zhang, Yuxin and Lin, Mingbao and Lin, Chia-Wen and Chen, Jie and Wu, Yongjian and Tian, Yonghong and Ji, Rongrong},
  journal={IEEE Transactions on Neural Networks and Learning Systems (TNNLS)},
  year={2022},
  publisher={IEEE}
}
%
@inproceedings{lin2020hrank,
  title={Hrank: Filter pruning using high-rank feature map},
  author={Lin, Mingbao and Ji, Rongrong and Wang, Yan and Zhang, Yichen and Zhang, Baochang and Tian, Yonghong and Shao, Ling},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={1529--1538},
  year={2020}
}
%
@inproceedings{liu2019metapruning,
  title={MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning},
  author={Liu, Zechun and Mu, Haoyuan and Zhang, Xiangyu and Guo, Zichao and Yang, Xin and Cheng, Tim Kwang-Ting and Sun, Jian},
  booktitle={IEEE International Conference on Computer Vision (ICCV)},
  pages={3296--3305},
  year={2019}
}
%
@inproceedings{lin2020channel,
  title={Channel Pruning via Automatic Structure Search},
  author={Lin, Mingbao and Ji, Rongrong and Zhang, Yuxin and Zhang, Baochang and Wu, Yongjian and Tian, Yonghong},
  booktitle={International Joint Conference on Artificial Intelligence (IJCAI)},
  pages = {673--679},
  year={2020}
}
%unstructured sparsity
@inproceedings{ding2019global,
  title={Global sparse momentum sgd for pruning very deep neural networks},
  author={Ding, Xiaohan and Zhou, Xiangxin and Guo, Yuchen and Han, Jungong and Liu, Ji and others},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={6382--6394},
  year={2019}
}
%
@inproceedings{lecun1989optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={598--605},
  year={1989}
}
%
@article{hoefler2021sparsity,
  title={Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks},
  author={Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
  journal={Journal of Machine Learning Research},
  volume={22},
  pages={1â€“124},
  year={2021},
}
%
@inproceedings{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={1135--1143},
  year={2015}
}
%
@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European Conference on Computer Vision (ECCV)},
  pages={740--755},
  year={2014},
}
%
%
@inproceedings{lee2018snip,
    title={Snip: Single-shot network pruning based on connection sensitivity},
    author={Namhoon Lee and Thalaiyasingam Ajanthan and Philip Torr},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2019},
}
%
@inproceedings{evci2020rigging,
  title={Rigging the lottery: Making all tickets winners},
  author={Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={2943--2952},
  year={2020},
}
%
@inproceedings{kusupati2020soft,
  author    = {Kusupati, Aditya and Ramanujan, Vivek and Somani, Raghav and Wortsman, Mitchell and Jain, Prateek and Kakade, Sham and Farhadi, Ali},
  title     = {Soft threshold weight reparameterization for learnable sparsity},
  booktitle = {International Conference on Machine Learning (ICML)},
  pages = {5544-5555},
  year      = {2020},
}
%
@inproceedings{wang2020picking,
    title={Picking winning tickets before training by preserving gradient flow},
    author={Chaoqi Wang and Guodong Zhang and Roger Grosse},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2020},
}
%
@inproceedings{zhu2017prune,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Zhu, Michael and Gupta, Suyog},
  booktitle={International Conference on Learning Representations Workshop (ICLRW)},
  year={2017}
}
%
@article{mocanu2018scalable,
  title={Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
  author={Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
  journal={Nature Communications},
  volume={9},
  pages={1--12},
  year={2018}
}
%
@inproceedings{molchanov2016pruning,
  author    = {Pavlo Molchanov and Stephen Tyree and Tero Karras and Timo Aila and Jan Kautz},
  title     = {Pruning convolutional neural networks for resource efficient inference},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2017},
}
%
@inproceedings{tanaka2020pruning,
  title={Pruning neural networks without any data by iteratively conserving synaptic flow},
  author={Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L and Ganguli, Surya},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020}
}
@inproceedings{jayakumar2020top,
  title={Top-kast: Top-k always sparse training},
  author={Jayakumar, Siddhant and Pascanu, Razvan and Rae, Jack and Osindero, Simon and Elsen, Erich},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={20744--20754},
  year={2020}
}
%
@inproceedings{frankle2018lottery,
    title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
    author={Jonathan Frankle and Michael Carbin},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2019},
}
%
@inproceedings{lin2020dynamic,
  title={Dynamic model pruning with feedback},
  author={Lin, Tao and Stich, Sebastian U and Barba, Luis and Dmitriev, Daniil and Jaggi, Martin},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020},
}
%
@inproceedings{dettmers2019sparse,
  title={Sparse networks from scratch: Faster training without losing performance},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}
%
@inproceedings{liu2021sparse,
  title={Sparse Training via Boosting Pruning Plasticity with Neuroregeneration},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Atashgahi, Zahra and Yin, Lu and Kou, Huanyu and Shen, Li and Pechenizkiy, Mykola and Wang, Zhangyang and Mocanu, Decebal Constantin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2021}
}
%
@inproceedings{mostafa2019parameter,
  title={Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization},
  author={Mostafa, Hesham and Wang, Xin},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={4646--4655},
  year={2019}
}
%
@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={10347--10357},
  year={2021},
}
%
%NM sparsity
@article{hubara2021accelerated,
  title={Accelerated sparse neural training: A provable and efficient method to find n: m transposable masks},
  author={Hubara, Itay and Chmiel, Brian and Island, Moshe and Banner, Ron and Naor, Joseph and Soudry, Daniel},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={34},
  year={2021}
}
%
@inproceedings{zhou2021learning,
  title={Learning N: M fine-grained structured sparse neural networks from scratch},
  author={Zhou, Aojun and Ma, Yukun and Zhu, Junnan and Liu, Jianbo and Zhang, Zhijie and Yuan, Kun and Sun, Wenxiu and Li, Hongsheng},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021}
}
%
@article{chmiel2022optimal,
  title={Optimal Fine-Grained N: M sparsity for Activations and Neural Gradients},
  author={Chmiel, Brian and Hubara, Itay and Banner, Ron and Soudry, Daniel},
  journal={arXiv preprint arXiv:2203.10991},
  year={2022}
}
%
@article{pool2021channel,
  title={Channel Permutations for N: M Sparsity},
  author={Pool, Jeff and Yu, Chong},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={34},
  year={2021}
}
%
@article{sun2021dominosearch,
  title={DominoSearch: Find layer-wise fine-grained N: M sparse schemes from dense neural networks},
  author={Sun, Wei and Zhou, Aojun and Stuijk, Sander and Wijnhoven, Rob and Nelson, Andrew O and Corporaal, Henk and others},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={34},
  year={2021}
}
%
@misc{nvidia2020a100,
  author = {Nvidia},
  title = { Nvidia a100 tensor core gpu architecture},
  howpublished = "\url{https://www.nvidia.com/content/dam/en- zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf}",
  year = {2020}, 
}
%
@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}
%
@misc{ronny2020nvidia,
  author = {Ronny Krashinsky, Olivier Giroux et al.},
  title = {Nvidia Ampere Sparse Tensor Core},
  howpublished = "\url{https://
developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/}",
  year = {2020}, 
}
%
@inproceedings{louizos2017learning,
  title={Learning sparse neural networks through $ L\_0 $ regularization},
  author={Louizos, Christos and Welling, Max and Kingma, Diederik P},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017}
}