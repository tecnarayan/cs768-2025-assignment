% $ biblatex auxiliary file $
% $ biblatex bbl format version 2.8 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \sortlist[entry]{none/global/}
    \entry{10.5555/3042817.3043083}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=7045b009b04d57bd2e19b5dfa0864d4f}{%
           family={Pascanu},
           familyi={P\bibinitperiod},
           given={Razvan},
           giveni={R\bibinitperiod}}}%
        {{hash=a2d359b12ca2fadf0b40136a73f021bb}{%
           family={Mikolov},
           familyi={M\bibinitperiod},
           given={Tomas},
           giveni={T\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {JMLR.org}%
      }
      \strng{namehash}{ddea4a14df69745e0b708bc055e9ad09}
      \strng{fullhash}{219d705befe0e5043ea8e48c4048f89a}
      \strng{authornamehash}{ddea4a14df69745e0b708bc055e9ad09}
      \strng{authorfullhash}{219d705befe0e5043ea8e48c4048f89a}
      \field{labelalpha}{PMB12}
      \field{sortinit}{1}
      \field{sortinithash}{27a2bc5dfb9ed0a0422134d636544b5d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.}
      \field{booktitle}{Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28}
      \field{eprinttype}{arXiv}
      \field{month}{11}
      \field{series}{ICML'13}
      \field{title}{{Understanding the exploding gradient problem}}
      \field{year}{2012}
      \field{pages}{III--1310--III\bibrangedash 1318}
      \range{pages}{-1}
      \verb{eprint}
      \verb 1211.5063
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1211.5063
      \endverb
    \endentry
    \entry{Zhu2015a}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=5dc6c11cf197b8ab96858ae332d2eb57}{%
           family={Zhu},
           familyi={Z\bibinitperiod},
           given={Chenxi},
           giveni={C\bibinitperiod}}}%
        {{hash=9057ec1bc6c2faaf12dfb4b37a8d37d0}{%
           family={Qiu},
           familyi={Q\bibinitperiod},
           given={Xipeng},
           giveni={X\bibinitperiod}}}%
        {{hash=ed0f3403ef6238bb67876d4c6ad27464}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Xuanjing},
           giveni={X\bibinitperiod}}}%
      }
      \strng{namehash}{0cac4797e4988502d33f9910aa65c308}
      \strng{fullhash}{2c237755623fbdad162f0a0c58bdce10}
      \strng{authornamehash}{0cac4797e4988502d33f9910aa65c308}
      \strng{authorfullhash}{2c237755623fbdad162f0a0c58bdce10}
      \field{labelalpha}{ZQH15}
      \field{sortinit}{2}
      \field{sortinithash}{0aa614ace9f3a40ef5a67e7f7a184048}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper focuses on improving a specific opinion spam detection task, deceptive spam. In addition to traditional word form and other shallow syntactic features, we introduce two types of deep level linguistic features. The first type of features are derived from a shallow discourse parser trained on Penn Discourse Treebank (PDTB), which can capture inter-sentence information. The second type is based on the relationship between sentiment analysis and spam detection. The experi-mental results over the benchmark dataset demonstrate that both of the proposed deep features achieve improved performance over the baseline.}
      \field{booktitle}{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
      \field{isbn}{9783319252063}
      \field{issn}{16113349}
      \field{title}{{Transition-based dependency parsing with long distance collocations}}
      \field{year}{2015}
      \verb{doi}
      \verb 10.1007/978-3-319-25207-0_2
      \endverb
    \endentry
    \entry{Dbrowska2008longDistanceDependencies}{article}{}
      \name{author}{1}{}{%
        {{hash=fa7fe3238711c38ea5e24bd3bd0f9ce8}{%
           family={Dabrowska},
           familyi={D\bibinitperiod},
           given={Ewa},
           giveni={E\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Walter de Gruyter {GmbH}}%
      }
      \strng{namehash}{fa7fe3238711c38ea5e24bd3bd0f9ce8}
      \strng{fullhash}{fa7fe3238711c38ea5e24bd3bd0f9ce8}
      \strng{authornamehash}{fa7fe3238711c38ea5e24bd3bd0f9ce8}
      \strng{authorfullhash}{fa7fe3238711c38ea5e24bd3bd0f9ce8}
      \field{labelalpha}{Dab08}
      \field{sortinit}{2}
      \field{sortinithash}{0aa614ace9f3a40ef5a67e7f7a184048}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Cognitive Linguistics}
      \field{month}{1}
      \field{number}{3}
      \field{title}{Questions with long-distance dependencies: A usage-based perspective}
      \field{volume}{19}
      \field{year}{2008}
      \verb{doi}
      \verb 10.1515/cogl.2008.015
      \endverb
    \endentry
    \entry{khandelwal2018LongDistanceLSTM}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=f7240c53b0110287ad0a277d9b6dace6}{%
           family={Khandelwal},
           familyi={K\bibinitperiod},
           given={Urvashi},
           giveni={U\bibinitperiod}}}%
        {{hash=ab9793659f2bfad54eefb759c28350df}{%
           family={He},
           familyi={H\bibinitperiod},
           given={He},
           giveni={H\bibinitperiod}}}%
        {{hash=90315a68d91c0dd0775cb493bc6967a6}{%
           family={Qi},
           familyi={Q\bibinitperiod},
           given={Peng},
           giveni={P\bibinitperiod}}}%
        {{hash=3147296c99a3f829087becd1a4eaec08}{%
           family={Jurafsky},
           familyi={J\bibinitperiod},
           given={Dan},
           giveni={D\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Stroudsburg, PA, USA}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{3746a4e1eaccb129ece00fc077b5ac62}
      \strng{fullhash}{e8389b9c56d9610d72727c1f76411c78}
      \strng{authornamehash}{3746a4e1eaccb129ece00fc077b5ac62}
      \strng{authorfullhash}{e8389b9c56d9610d72727c1f76411c78}
      \field{labelalpha}{Kha+18}
      \field{sortinit}{2}
      \field{sortinithash}{0aa614ace9f3a40ef5a67e7f7a184048}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}
      \field{title}{{Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context}}
      \field{year}{2018}
      \field{pages}{284\bibrangedash 294}
      \range{pages}{11}
      \verb{doi}
      \verb 10.18653/v1/P18-1027
      \endverb
    \endentry
    \entry{Al-Rfou2019}{article}{}
      \name{author}{5}{}{%
        {{hash=6fdea7728f40679d54135ffed015aec3}{%
           family={Al-Rfou},
           familyi={A\bibinithyphendelim R\bibinitperiod},
           given={Rami},
           giveni={R\bibinitperiod}}}%
        {{hash=046847946225c06cd03d6dc99be95b57}{%
           family={Choe},
           familyi={C\bibinitperiod},
           given={Dokook},
           giveni={D\bibinitperiod}}}%
        {{hash=6a8ed325a5622cf4ec06f17777376a1b}{%
           family={Constant},
           familyi={C\bibinitperiod},
           given={Noah},
           giveni={N\bibinitperiod}}}%
        {{hash=027761345a2d9bc7d8e905d5120c63c6}{%
           family={Guo},
           familyi={G\bibinitperiod},
           given={Mandy},
           giveni={M\bibinitperiod}}}%
        {{hash=2fd2982e30ebcec93ec1cf76e0d797fd}{%
           family={Jones},
           familyi={J\bibinitperiod},
           given={Llion},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{33e6a66c057bd9efe031fcc035fad019}
      \strng{fullhash}{efffef92d9bda9cba5fd9efd52315e06}
      \strng{authornamehash}{33e6a66c057bd9efe031fcc035fad019}
      \strng{authorfullhash}{efffef92d9bda9cba5fd9efd52315e06}
      \field{labelalpha}{AlR+19}
      \field{sortinit}{3}
      \field{sortinithash}{197da6d6c34c6b20ce45c4d4baace5a4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model (Vaswani et al. 2017) with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.}
      \field{issn}{2374-3468}
      \field{journaltitle}{Proceedings of the AAAI Conference on Artificial Intelligence}
      \field{month}{7}
      \field{title}{{Character-Level Language Modeling with Deeper Self-Attention}}
      \field{volume}{33}
      \field{year}{2019}
      \field{pages}{3159\bibrangedash 3166}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1609/aaai.v33i01.33013159
      \endverb
    \endentry
    \entry{Dai2019}{article}{}
      \name{author}{6}{}{%
        {{hash=74ae4c193de464cda59c8066619514fd}{%
           family={Dai},
           familyi={D\bibinitperiod},
           given={Zihang},
           giveni={Z\bibinitperiod}}}%
        {{hash=6b1fa069ba3a41402c63a24209e47289}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Zhilin},
           giveni={Z\bibinitperiod}}}%
        {{hash=be6dc43fe1771d8870237ce7a47c6f7f}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Yiming},
           giveni={Y\bibinitperiod}}}%
        {{hash=4e523aba9ce388c6b542f2f6e52293c2}{%
           family={Carbonell},
           familyi={C\bibinitperiod},
           given={Jaime},
           giveni={J\bibinitperiod}}}%
        {{hash=1633c392d651912a53a929a28bd31bb4}{%
           family={Le},
           familyi={L\bibinitperiod},
           given={Quoc\bibnamedelima V.},
           giveni={Q\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
        {{hash=bd2be300d445e9f6db7808f9533e66cb}{%
           family={Salakhutdinov},
           familyi={S\bibinitperiod},
           given={Ruslan},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{64f332e195de5d241c9866cc6621f71f}
      \strng{fullhash}{6b67d2aa28d04dfbbef32b5455dff716}
      \strng{authornamehash}{64f332e195de5d241c9866cc6621f71f}
      \strng{authorfullhash}{6b67d2aa28d04dfbbef32b5455dff716}
      \field{labelalpha}{Dai+19}
      \field{sortinit}{3}
      \field{sortinithash}{197da6d6c34c6b20ce45c4d4baace5a4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.}
      \field{eprinttype}{arXiv}
      \field{month}{1}
      \field{title}{{Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context}}
      \field{year}{2019}
      \verb{eprint}
      \verb 1901.02860
      \endverb
    \endentry
    \entry{kitaev2020reformer}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=84fcf20261b758feb2c1ca534214ea1e}{%
           family={Kitaev},
           familyi={K\bibinitperiod},
           given={Nikita},
           giveni={N\bibinitperiod}}}%
        {{hash=f2bc899b1160163417da7bf510f15d33}{%
           family={Kaiser},
           familyi={K\bibinitperiod},
           given={Lukasz},
           giveni={L\bibinitperiod}}}%
        {{hash=738351d7ac35edab0cd231cb64407ab4}{%
           family={Levskaya},
           familyi={L\bibinitperiod},
           given={Anselm},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{5eb5da834a75a52f05e954729cb14f67}
      \strng{fullhash}{0dda11ba21797c87ebcf517782ac2d93}
      \strng{authornamehash}{5eb5da834a75a52f05e954729cb14f67}
      \strng{authorfullhash}{0dda11ba21797c87ebcf517782ac2d93}
      \field{labelalpha}{KKL20}
      \field{sortinit}{3}
      \field{sortinithash}{197da6d6c34c6b20ce45c4d4baace5a4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{International Conference on Learning Representations}
      \field{title}{Reformer: The Efficient Transformer}
      \field{year}{2020}
      \verb{url}
      \verb https://openreview.net/forum?id=rkgNKkHtvB
      \endverb
    \endentry
    \entry{Aicher2019}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=ea724ee5e1f3b3f33c213ab0240af331}{%
           family={Aicher},
           familyi={A\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
        {{hash=f1ed230b41c81f59b1f4f291ec5fdb3b}{%
           family={Foti},
           familyi={F\bibinitperiod},
           given={Nicholas\bibnamedelima J},
           giveni={N\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=40c768a3c3b0a3c5476a3841cc896e37}{%
           family={Fox},
           familyi={F\bibinitperiod},
           given={Emily\bibnamedelima B},
           giveni={E\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
      }
      \strng{namehash}{b8b97845e82fe9894aee6bbdce1edc7b}
      \strng{fullhash}{8d7a4eac66cbca91a0e365abc52bd754}
      \strng{authornamehash}{b8b97845e82fe9894aee6bbdce1edc7b}
      \strng{authorfullhash}{8d7a4eac66cbca91a0e365abc52bd754}
      \field{labelalpha}{AFF19}
      \field{sortinit}{4}
      \field{sortinithash}{d221becdd544034cf9082056cd6e6d2d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Truncated backpropagation through time (TBPTT) is a popular method for learning in recurrent neural networks (RNNs) that saves computation and memory at the cost of bias by truncating backpropagation after a fixed number of lags. In practice, choosing the optimal truncation length is difficult: TBPTT will not converge if the truncation length is too small, or will converge slowly if it is too large. We propose an adaptive TBPTT scheme that converts the problem from choosing a temporal lag to one of choosing a tolerable amount of gradient bias. For many realistic RNNs, the TBPTT gradients decay geometrically in expectation for large lags; under this condition, we can control the bias by varying the truncation length adaptively. For RNNs with smooth activation functions, we prove that this bias controls the convergence rate of SGD with biased gradients for our non-convex loss. Using this theory, we develop a practical method for adaptively estimating the truncation length during training. We evaluate our adaptive TBPTT method on synthetic data and language modeling tasks and find that our adaptive TBPTT ameliorates the computational pitfalls of fixed TBPTT.}
      \field{booktitle}{UAI}
      \field{eprinttype}{arXiv}
      \field{month}{5}
      \field{title}{{Adaptively Truncating Backpropagation Through Time to Control Gradient Bias}}
      \field{year}{2019}
      \verb{eprint}
      \verb 1905.07473
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1905.07473
      \endverb
    \endentry
    \entry{Vorontsov2017OnOA}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=3326f6af7bc9d8c3eb7c4dec35959595}{%
           family={Vorontsov},
           familyi={V\bibinitperiod},
           given={Eugene},
           giveni={E\bibinitperiod}}}%
        {{hash=f84993413c8e452b708b59c38557511c}{%
           family={Trabelsi},
           familyi={T\bibinitperiod},
           given={Chiheb},
           giveni={C\bibinitperiod}}}%
        {{hash=985529db9314d3a3eea5f04641fdb9eb}{%
           family={Kadoury},
           familyi={K\bibinitperiod},
           given={Samuel},
           giveni={S\bibinitperiod}}}%
        {{hash=8aa7cbba50b308a24e8ecf9881586f75}{%
           family={Pal},
           familyi={P\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{631bdee9734723b6ea9af5d7735e0ac8}
      \strng{fullhash}{c5e66bcc13c384d40cd22113d9c9257b}
      \strng{authornamehash}{631bdee9734723b6ea9af5d7735e0ac8}
      \strng{authorfullhash}{c5e66bcc13c384d40cd22113d9c9257b}
      \field{labelalpha}{Vor+17}
      \field{sortinit}{5}
      \field{sortinithash}{c9df3c9fb8f555dd9201cedc5e343021}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and may therefore be a desirable property. This paper explores issues with optimization convergence, speed and gradient stability when encouraging or enforcing orthogonality. To perform this analysis, we propose a weight matrix factorization and parameterization strategy through which we can bound matrix norms and therein control the degree of expansivity induced during backpropagation. We find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance.}
      \field{booktitle}{ICML}
      \field{eprinttype}{arXiv}
      \field{month}{1}
      \field{title}{{On orthogonality and learning recurrent networks with long term dependencies}}
      \field{year}{2017}
      \verb{eprint}
      \verb 1702.00071
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1702.00071
      \endverb
    \endentry
    \entry{Arjovsky2015}{article}{}
      \name{author}{3}{}{%
        {{hash=ab4a5e058dea2d852ff02aa4e10045c3}{%
           family={Arjovsky},
           familyi={A\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=dbd6555228f6c241a2693cc967170f14}{%
           family={Shah},
           familyi={S\bibinitperiod},
           given={Amar},
           giveni={A\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{c0253375e3f94eb82dddef2732bbe5a4}
      \strng{fullhash}{7f87d09a3dcd044c90fb43f4482dcf09}
      \strng{authornamehash}{c0253375e3f94eb82dddef2732bbe5a4}
      \strng{authorfullhash}{7f87d09a3dcd044c90fb43f4482dcf09}
      \field{labelalpha}{ASB15}
      \field{sortinit}{6}
      \field{sortinithash}{02bbed3ed82f61ae046619460488516d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies.}
      \field{eprinttype}{arXiv}
      \field{month}{11}
      \field{title}{{Unitary Evolution Recurrent Neural Networks}}
      \field{year}{2015}
      \verb{eprint}
      \verb 1511.06464
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1511.06464
      \endverb
    \endentry
    \entry{Widsom16}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=3c66a78947d02b0c04e0b3afb371b584}{%
           family={Wisdom},
           familyi={W\bibinitperiod},
           given={Scott},
           giveni={S\bibinitperiod}}}%
        {{hash=ce66b3504be56796deee03c3fdb44938}{%
           family={Powers},
           familyi={P\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
        {{hash=f7569671cd18bd712ea205405a3f7ff4}{%
           family={Hershey},
           familyi={H\bibinitperiod},
           given={John\bibnamedelima R},
           giveni={J\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=5e9c3ee010ad094d84c29b23fcc5af04}{%
           family={Roux},
           familyi={R\bibinitperiod},
           given={Jonathan\bibnamedelima Le},
           giveni={J\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=63beb9e3c33e5043b1cc20a3ad9f8630}{%
           family={Atlas},
           familyi={A\bibinitperiod},
           given={Les},
           giveni={L\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Red Hook, NY, USA}%
      }
      \list{publisher}{1}{%
        {Curran Associates Inc.}%
      }
      \strng{namehash}{5b79ab48f819125c7ccba9f7c82a1342}
      \strng{fullhash}{a0ebec318ab57998b774eeff2aedc1d4}
      \strng{authornamehash}{5b79ab48f819125c7ccba9f7c82a1342}
      \strng{authorfullhash}{a0ebec318ab57998b774eeff2aedc1d4}
      \field{labelalpha}{Wis+16}
      \field{sortinit}{6}
      \field{sortinithash}{02bbed3ed82f61ae046619460488516d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recurrent neural networks are powerful models for processing sequential data, but they are generally plagued by vanishing and exploding gradient problems. Unitary recurrent neural networks (uRNNs), which use unitary recurrence matrices, have recently been proposed as a means to avoid these issues. However, in previous experiments, the recurrence matrices were restricted to be a product of parameterized unitary matrices, and an open question remains: when does such a parameterization fail to represent all unitary matrices, and how does this restricted representational capacity limit what can be learned? To address this question, we propose full-capacity uRNNs that optimize their recurrence matrix over all unitary matrices, leading to significantly improved performance over uRNNs that use a restricted-capacity recurrence matrix. Our contribution consists of two main components. First, we provide a theoretical argument to determine if a unitary parameterization has restricted capacity. Using this argument, we show that a recently proposed unitary parameterization has restricted capacity for hidden state dimension greater than 7. Second, we show how a complete, full-capacity unitary recurrence matrix can be optimized over the differentiable manifold of unitary matrices. The resulting multiplicative gradient step is very simple and does not require gradient clipping or learning rate adaptation. We confirm the utility of our claims by empirically evaluating our new full-capacity uRNNs on both synthetic and natural data, achieving superior performance compared to both LSTMs and the original restricted-capacity uRNNs.}
      \field{booktitle}{Proceedings of the 30th International Conference on Neural Information Processing Systems}
      \field{eprinttype}{arXiv}
      \field{isbn}{9781510838819}
      \field{month}{10}
      \field{series}{NIPS'16}
      \field{title}{{Full-Capacity Unitary Recurrent Neural Networks}}
      \field{year}{2016}
      \field{pages}{4887\bibrangedash 4895}
      \range{pages}{9}
      \verb{eprint}
      \verb 1611.00035
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1611.00035
      \endverb
    \endentry
    \entry{8668730}{article}{}
      \name{author}{7}{}{%
        {{hash=e185e98ee3d0228730eb44d3476c64be}{%
           family={Jing},
           familyi={J\bibinitperiod},
           given={Li},
           giveni={L\bibinitperiod}}}%
        {{hash=2adc0c92c308f233c731321d55efe58f}{%
           family={Gulcehre},
           familyi={G\bibinitperiod},
           given={Caglar},
           giveni={C\bibinitperiod}}}%
        {{hash=9c44b2c0b3c422bd48aa27d5e6a6f08a}{%
           family={Peurifoy},
           familyi={P\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
        {{hash=62a82074c485f12c29bb533d5445542c}{%
           family={Shen},
           familyi={S\bibinitperiod},
           given={Yichen},
           giveni={Y\bibinitperiod}}}%
        {{hash=b9b3c445e1013a97fc6318e06799b37b}{%
           family={Tegmark},
           familyi={T\bibinitperiod},
           given={Max},
           giveni={M\bibinitperiod}}}%
        {{hash=d4bb6bd2724f026f198fb2fddbe7a5f7}{%
           family={Soljačić},
           familyi={S\bibinitperiod},
           given={Marin},
           giveni={M\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{04bc0aa41f126aef9f65e3fd6e28ec74}
      \strng{fullhash}{078839501da2fb42b91528a901d2c158}
      \strng{authornamehash}{04bc0aa41f126aef9f65e3fd6e28ec74}
      \strng{authorfullhash}{078839501da2fb42b91528a901d2c158}
      \field{labelalpha}{Jin+17}
      \field{sortinit}{6}
      \field{sortinithash}{02bbed3ed82f61ae046619460488516d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a novel recurrent neural network (RNN) based model that combines the remembering ability of unitary RNNs with the ability of gated RNNs to effectively forget redundant/irrelevant information in its memory. We achieve this by extending unitary RNNs with a gating mechanism. Our model is able to outperform LSTMs, GRUs and Unitary RNNs on several long-term dependency benchmark tasks. We empirically both show the orthogonal/unitary RNNs lack the ability to forget and also the ability of GORU to simultaneously remember long term dependencies while forgetting irrelevant information. This plays an important role in recurrent neural networks. We provide competitive results along with an analysis of our model on many natural sequential tasks including the bAbI Question Answering, TIMIT speech spectrum prediction, Penn TreeBank, and synthetic tasks that involve long-term dependencies such as algorithmic, parenthesis, denoising and copying tasks.}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{Neural Computation}
      \field{month}{6}
      \field{number}{4}
      \field{title}{{Gated Orthogonal Recurrent Units: On Learning to Forget}}
      \field{volume}{31}
      \field{year}{2017}
      \field{pages}{765\bibrangedash 783}
      \range{pages}{19}
      \verb{eprint}
      \verb 1706.02761
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1706.02761
      \endverb
    \endentry
    \entry{Hyland2016}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=7bed2e2e734723b030b288541ef913e2}{%
           family={Hyland},
           familyi={H\bibinitperiod},
           given={Stephanie\bibnamedelima L},
           giveni={S\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=ca596a0a9d87ca9f7999db4d71c7e577}{%
           family={Rätsch},
           familyi={R\bibinitperiod},
           given={Gunnar},
           giveni={G\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {AAAI Press}%
      }
      \strng{namehash}{0d28ce6a2fe11e69210e5725a33cb414}
      \strng{fullhash}{0d28ce6a2fe11e69210e5725a33cb414}
      \strng{authornamehash}{0d28ce6a2fe11e69210e5725a33cb414}
      \strng{authorfullhash}{0d28ce6a2fe11e69210e5725a33cb414}
      \field{labelalpha}{HR16}
      \field{sortinit}{7}
      \field{sortinithash}{6ac7fe70cdec8cba8f65694411ce2532}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A major challenge in the training of recurrent neural networks is the so-called vanishing or exploding gradient problem. The use of a norm-preserving transition operator can address this issue, but parametrization is challenging. In this work we focus on unitary operators and describe a parametrization using the Lie algebra {\$}\backslashmathfrak{\{}u{\}}(n){\$} associated with the Lie group {\$}U(n){\$} of {\$}n \backslashtimes n{\$} unitary matrices. The exponential map provides a correspondence between these spaces, and allows us to define a unitary matrix using {\$}n{\^{}}2{\$} real coefficients relative to a basis of the Lie algebra. The parametrization is closed under additive updates of these coefficients, and thus provides a simple space in which to do gradient descent. We demonstrate the effectiveness of this parametrization on the problem of learning arbitrary unitary operators, comparing to several baselines and outperforming a recently-proposed lower-dimensional parametrization. We additionally use our parametrization to generalize a recently-proposed unitary recurrent neural network to arbitrary unitary matrices, using it to solve standard long-memory tasks.}
      \field{booktitle}{Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence}
      \field{eprinttype}{arXiv}
      \field{month}{7}
      \field{series}{AAAI'17}
      \field{title}{{Learning Unitary Operators with Help From u(n)}}
      \field{year}{2016}
      \field{pages}{2050\bibrangedash 2058}
      \range{pages}{9}
      \verb{eprint}
      \verb 1607.04903
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1607.04903
      \endverb
    \endentry
    \entry{Wang2019}{article}{}
      \name{author}{3}{}{%
        {{hash=f605810d16ffcd928d4922cb3dc343c7}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Daochen},
           giveni={D\bibinitperiod}}}%
        {{hash=2db1ed6b0ea0e0cd8635024ff435326e}{%
           family={Higgott},
           familyi={H\bibinitperiod},
           given={Oscar},
           giveni={O\bibinitperiod}}}%
        {{hash=779d39baa19bb96d78a124cb78e6ee92}{%
           family={Brierley},
           familyi={B\bibinitperiod},
           given={Stephen},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{8ec69f80a50f83308e4325d51a4dd6cf}
      \strng{fullhash}{27bc78f5804d9135681503998b22f66d}
      \strng{authornamehash}{8ec69f80a50f83308e4325d51a4dd6cf}
      \strng{authorfullhash}{27bc78f5804d9135681503998b22f66d}
      \field{labelalpha}{WHB19}
      \field{sortinit}{8}
      \field{sortinithash}{2f1b0e69e7617f99664f1cd00bb9603c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0031-9007}
      \field{journaltitle}{Physical Review Letters}
      \field{month}{4}
      \field{number}{14}
      \field{title}{{Accelerated Variational Quantum Eigensolver}}
      \field{volume}{122}
      \field{year}{2019}
      \field{pages}{140504}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1103/PhysRevLett.122.140504
      \endverb
      \verb{url}
      \verb https://link.aps.org/doi/10.1103/PhysRevLett.122.140504
      \endverb
    \endentry
    \entry{McClean_2016}{article}{}
      \name{author}{4}{}{%
        {{hash=4173ffc8c6347e4a7e4190b3dd776235}{%
           family={McClean},
           familyi={M\bibinitperiod},
           given={Jarrod\bibnamedelima R},
           giveni={J\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=8c1a9ea5a335989d4957424b7cc2f7b9}{%
           family={Romero},
           familyi={R\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod}}}%
        {{hash=7bb1dcf8a93d1ecc6f9531ece93ef1fa}{%
           family={Babbush},
           familyi={B\bibinitperiod},
           given={Ryan},
           giveni={R\bibinitperiod}}}%
        {{hash=083dcdd0db2687433b22fabfb2e55c17}{%
           family={Aspuru-Guzik},
           familyi={A\bibinithyphendelim G\bibinitperiod},
           given={Alán},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {{\{}IOP{\}} Publishing}%
      }
      \strng{namehash}{3c056945941c07a8c76bde1da05113d4}
      \strng{fullhash}{13102fc88dfa6662542675940a063abf}
      \strng{authornamehash}{3c056945941c07a8c76bde1da05113d4}
      \strng{authorfullhash}{13102fc88dfa6662542675940a063abf}
      \field{labelalpha}{McC+16}
      \field{sortinit}{8}
      \field{sortinithash}{2f1b0e69e7617f99664f1cd00bb9603c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Many quantum algorithms have daunting resource requirements when compared to what is available today. To address this discrepancy, a quantum-classical hybrid optimization scheme known as ‘the quantum variational eigensolver' was developed (Peruzzo et al 2014 Nat. Commun. 5 4213) with the philosophy that even minimal quantum resources could be made useful when used in conjunction with classical routines. In this work we extend the general theory of this algorithm and suggest algorithmic improvements for practical implementations. Specifically, we develop a variational adiabatic ansatz and explore unitary coupled cluster where we establish a connection from second order unitary coupled cluster to universal gate sets through a relaxation of exponential operator splitting. We introduce the concept of quantum variational error suppression that allows some errors to be suppressed naturally in this algorithm on a pre-threshold quantum device. Additionally, we analyze truncation and correlated sampling in Hamiltonian averaging as ways to reduce the cost of this procedure. Finally, we show how the use of modern derivative free optimization techniques can offer dramatic computational savings of up to three orders of magnitude over previously used optimization techniques.}
      \field{journaltitle}{New Journal of Physics}
      \field{month}{2}
      \field{number}{2}
      \field{title}{{The theory of variational hybrid quantum-classical algorithms}}
      \field{volume}{18}
      \field{year}{2016}
      \field{pages}{23023}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1088/1367-2630/18/2/023023
      \endverb
      \verb{url}
      \verb https://doi.org/10.1088%7B%5C%%7D2F1367-2630%7B%5C%%7D2F18%7B%5C%%7D2F2%7B%5C%%7D2F023023
      \endverb
    \endentry
    \entry{Peruzzo2014}{article}{}
      \name{author}{8}{}{%
        {{hash=da632f662bb1ab5c3e05ddb80b6cd201}{%
           family={Peruzzo},
           familyi={P\bibinitperiod},
           given={Alberto},
           giveni={A\bibinitperiod}}}%
        {{hash=44d80c9bccbe7462fe524bbf9cd54a08}{%
           family={McClean},
           familyi={M\bibinitperiod},
           given={Jarrod},
           giveni={J\bibinitperiod}}}%
        {{hash=c0ad2cf76d13d7b9fb852ca865569241}{%
           family={Shadbolt},
           familyi={S\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
        {{hash=cbadbf9460f01d20432bc4fb34345309}{%
           family={Yung},
           familyi={Y\bibinitperiod},
           given={Man-Hong},
           giveni={M\bibinithyphendelim H\bibinitperiod}}}%
        {{hash=30e428aa9e63bc24127324a4c1fefabc}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Xiao-Qi},
           giveni={X\bibinithyphendelim Q\bibinitperiod}}}%
        {{hash=c3162d73d1db1d7976b0ab0c9acddcfe}{%
           family={Love},
           familyi={L\bibinitperiod},
           given={Peter\bibnamedelima J.},
           giveni={P\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=083dcdd0db2687433b22fabfb2e55c17}{%
           family={Aspuru-Guzik},
           familyi={A\bibinithyphendelim G\bibinitperiod},
           given={Alán},
           giveni={A\bibinitperiod}}}%
        {{hash=509ba563d00dcb64646ee9679ab824d3}{%
           family={O'Brien},
           familyi={O\bibinitperiod},
           given={Jeremy\bibnamedelima L.},
           giveni={J\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
      }
      \strng{namehash}{bddcbb30e0909355294b8653d4787a0e}
      \strng{fullhash}{d5b67b3dac12d49e7fb90b201def6b74}
      \strng{authornamehash}{bddcbb30e0909355294b8653d4787a0e}
      \strng{authorfullhash}{d5b67b3dac12d49e7fb90b201def6b74}
      \field{labelalpha}{Per+14}
      \field{sortinit}{8}
      \field{sortinithash}{2f1b0e69e7617f99664f1cd00bb9603c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Quantum computers promise to efficiently solve important problems that are intractable on a conventional computer. For quantum systems, where the dimension of the problem space grows exponentially, finding the eigenvalues of certain operators is one such intractable problem and remains a fundamental challenge. The quantum phase estimation algorithm can efficiently find the eigenvalue of a given eigenvector but requires fully coherent evolution. We present an alternative approach that greatly reduces the requirements for coherent evolution and we combine this method with a new approach to state preparation based on ans$\backslash$"atze and classical optimization. We have implemented the algorithm by combining a small-scale photonic quantum processor with a conventional computer. We experimentally demonstrate the feasibility of this approach with an example from quantum chemistry: calculating the ground state molecular energy for He-H+, to within chemical accuracy. The proposed approach, by drastically reducing the coherence time requirements, enhances the potential of the quantum resources available today and in the near future.}
      \field{eprinttype}{arXiv}
      \field{issn}{2041-1723}
      \field{journaltitle}{Nature Communications}
      \field{month}{9}
      \field{number}{1}
      \field{title}{{A variational eigenvalue solver on a photonic quantum processor}}
      \field{volume}{5}
      \field{year}{2014}
      \field{pages}{4213}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1038/ncomms5213
      \endverb
      \verb{eprint}
      \verb 1304.3061
      \endverb
      \verb{url}
      \verb http://www.nature.com/articles/ncomms5213%20http://arxiv.org/abs/1304.3061%20http://dx.doi.org/10.1038/ncomms5213
      \endverb
    \endentry
    \entry{Jiang2018}{article}{}
      \name{author}{4}{}{%
        {{hash=67db50e40049bac70a6d264e681d86de}{%
           family={Jiang},
           familyi={J\bibinitperiod},
           given={Zhang},
           giveni={Z\bibinitperiod}}}%
        {{hash=44d80c9bccbe7462fe524bbf9cd54a08}{%
           family={McClean},
           familyi={M\bibinitperiod},
           given={Jarrod},
           giveni={J\bibinitperiod}}}%
        {{hash=7bb1dcf8a93d1ecc6f9531ece93ef1fa}{%
           family={Babbush},
           familyi={B\bibinitperiod},
           given={Ryan},
           giveni={R\bibinitperiod}}}%
        {{hash=ddcc46c5456340da59be4077c7102a27}{%
           family={Neven},
           familyi={N\bibinitperiod},
           given={Hartmut},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{2b21342b018b60e7ce2ec68cf5054fb9}
      \strng{fullhash}{df3a678391d0e0857175004b176ef24a}
      \strng{authornamehash}{2b21342b018b60e7ce2ec68cf5054fb9}
      \strng{authorfullhash}{df3a678391d0e0857175004b176ef24a}
      \field{labelalpha}{Jia+18}
      \field{sortinit}{8}
      \field{sortinithash}{2f1b0e69e7617f99664f1cd00bb9603c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Locality-preserving fermion-to-qubit mappings are especially useful for simulating lattice fermion models (e.g., the Hubbard model) on a quantum computer. They avoid the overhead associated with non-local parity terms in mappings such as the Jordan-Wigner transformation. As a result, they often provide solutions with lower circuit depth and gate complexity. A major obstacle to achieving near-term quantum computation is quantum noises. Interestingly, locality-preserving mappings encode the fermionic state in the common +1 eigenstate of a set of stabilizers, akin to quantum error-correcting codes. Here, we discuss a couple of known locality-preserving mappings and their abilities to correct/detect single-qubit errors. We also introduce a locality-preserving map, whose stabilizers are products of Majorana operators on closed paths of the fermionic hopping graph. The code can correct all single-qubit errors on a 2-dimensional square lattice, while previous locality-preserving codes can only detect single-qubit errors on the same lattice. Our code also has the advantage of having lower-weight logical operators. We expect that error-mitigating schemes with low overhead to be useful to the success of near-term quantum algorithms such as the variational quantum eigensolver.}
      \field{eprinttype}{arXiv}
      \field{month}{12}
      \field{title}{{Majorana loop stabilizer codes for error correction of fermionic quantum simulations}}
      \field{year}{2018}
      \verb{eprint}
      \verb 1812.08190
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1812.08190
      \endverb
    \endentry
    \entry{Cade2019}{article}{}
      \name{author}{4}{}{%
        {{hash=21eb774ee96d1978689c4f04a7b511d1}{%
           family={Cade},
           familyi={C\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod}}}%
        {{hash=e1decbf5568b6eb528f0a0fe84854668}{%
           family={Mineh},
           familyi={M\bibinitperiod},
           given={Lana},
           giveni={L\bibinitperiod}}}%
        {{hash=bd78a1f9d6e5f3fbbf33e0967436bb75}{%
           family={Montanaro},
           familyi={M\bibinitperiod},
           given={Ashley},
           giveni={A\bibinitperiod}}}%
        {{hash=6bfe2fcbe7e4d55af6ce6075218e9c5e}{%
           family={Stanisic},
           familyi={S\bibinitperiod},
           given={Stasja},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{122450f8aa3f51115507f5dc51e639d3}
      \strng{fullhash}{2f2725eac05495941ff7c9de50412e0c}
      \strng{authornamehash}{122450f8aa3f51115507f5dc51e639d3}
      \strng{authorfullhash}{2f2725eac05495941ff7c9de50412e0c}
      \field{labelalpha}{Cad+19}
      \field{sortinit}{9}
      \field{sortinithash}{2999ebab86052e6d71b434385f8b4ed2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The Fermi-Hubbard model is of fundamental importance in condensed-matter physics, yet is extremely challenging to solve numerically. Finding the ground state of the Hubbard model using variational methods has been predicted to be one of the first applications of near-term quantum computers. Here we carry out a detailed analysis and optimisation of the complexity of variational quantum algorithms for finding the ground state of the Hubbard model, including costs associated with mapping to a real-world hardware platform. The depth complexities we find are substantially lower than previous work. We performed extensive numerical experiments for systems with up to 12 sites. The results suggest that the variational ans$\backslash$"atze we used -- an efficient variant of the Hamiltonian Variational ansatz and a novel generalisation thereof -- will be able to find the ground state of the Hubbard model with high fidelity in relatively low quantum circuit depth. Our experiments include the effect of realistic measurements and depolarising noise. If our numerical results on small lattice sizes are representative of the somewhat larger lattices accessible to near-term quantum hardware, they suggest that optimising over quantum circuits with a gate depth less than a thousand could be sufficient to solve instances of the Hubbard model beyond the capacity of classical exact diagonalisation.}
      \field{eprinttype}{arXiv}
      \field{month}{12}
      \field{title}{{Strategies for solving the Fermi-Hubbard model on near-term quantum computers}}
      \field{year}{2019}
      \verb{eprint}
      \verb 1912.06007
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1912.06007
      \endverb
    \endentry
    \entry{Zoufal2019a}{article}{}
      \name{author}{3}{}{%
        {{hash=4725580e4d58363473dfd98243388664}{%
           family={Zoufal},
           familyi={Z\bibinitperiod},
           given={Christa},
           giveni={C\bibinitperiod}}}%
        {{hash=10ed3ab988aae1110cbc18468281cf87}{%
           family={Lucchi},
           familyi={L\bibinitperiod},
           given={Aurélien},
           giveni={A\bibinitperiod}}}%
        {{hash=43697d55280b4841e685e5008d047e19}{%
           family={Woerner},
           familyi={W\bibinitperiod},
           given={Stefan},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{f8813e9bb12e031778636e0dfa745591}
      \strng{fullhash}{c0e396bbd2e787580aae6b70fff1a837}
      \strng{authornamehash}{f8813e9bb12e031778636e0dfa745591}
      \strng{authorfullhash}{c0e396bbd2e787580aae6b70fff1a837}
      \field{labelalpha}{ZLW19}
      \field{sortinit}{1}
      \field{sortinithash}{27a2bc5dfb9ed0a0422134d636544b5d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Quantum algorithms have the potential to outperform their classical counterparts in a variety of tasks. The realization of the advantage often requires the ability to load classical data efficiently into quantum states. However, the best known methods require O(2 n) gates to load an exact representation of a generic data structure into an n-qubit state. This scaling can easily predominate the complexity of a quantum algorithm and, thereby, impair potential quantum advantage. Our work presents a hybrid quantum-classical algorithm for efficient, approximate quantum state loading. More precisely, we use quantum Generative Adversarial Networks (qGANs) to facilitate efficient learning and loading of generic probability distributions - implicitly given by data samples - into quantum states. Through the interplay of a quantum channel, such as a variational quantum circuit, and a classical neural network, the qGAN can learn a representation of the probability distribution underlying the data samples and load it into a quantum state. The loading requires O(poly(n)) gates and can thus enable the use of potentially advantageous quantum algorithms, such as Quantum Amplitude Estimation. We implement the qGAN distribution learning and loading method with Qiskit and test it using a quantum simulation as well as actual quantum processors provided by the IBM Q Experience. Furthermore, we employ quantum simulation to demonstrate the use of the trained quantum channel in a quantum finance application.}
      \field{eprinttype}{arXiv}
      \field{issn}{20566387}
      \field{journaltitle}{npj Quantum Information}
      \field{title}{{Quantum Generative Adversarial Networks for learning and loading random distributions}}
      \field{year}{2019}
      \verb{doi}
      \verb 10.1038/s41534-019-0223-2
      \endverb
      \verb{eprint}
      \verb 1904.00043
      \endverb
    \endentry
    \entry{Gyongyosi2019}{article}{}
      \name{author}{2}{}{%
        {{hash=7953c0fabe0e21906bca992f99a78fc5}{%
           family={Gyongyosi},
           familyi={G\bibinitperiod},
           given={Laszlo},
           giveni={L\bibinitperiod}}}%
        {{hash=e415eaa7f3c7c07a265f51feaed86e91}{%
           family={Imre},
           familyi={I\bibinitperiod},
           given={Sandor},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{017c1f44da25abe45f724eb5090b7ab6}
      \strng{fullhash}{017c1f44da25abe45f724eb5090b7ab6}
      \strng{authornamehash}{017c1f44da25abe45f724eb5090b7ab6}
      \strng{authorfullhash}{017c1f44da25abe45f724eb5090b7ab6}
      \field{labelalpha}{GI19}
      \field{sortinit}{1}
      \field{sortinithash}{27a2bc5dfb9ed0a0422134d636544b5d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{2045-2322}
      \field{journaltitle}{Scientific Reports}
      \field{month}{12}
      \field{number}{1}
      \field{title}{{Training Optimization for Gate-Model Quantum Neural Networks}}
      \field{volume}{9}
      \field{year}{2019}
      \field{pages}{12679}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1038/s41598-019-48892-w
      \endverb
      \verb{url}
      \verb http://www.nature.com/articles/s41598-019-48892-w
      \endverb
    \endentry
    \entry{Allauddin}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=a789dd0ff4a219860ec643a70cff72f2}{%
           family={Allauddin},
           familyi={A\bibinitperiod},
           given={R.},
           giveni={R\bibinitperiod}}}%
        {{hash=1768193b8a1ecf9aa16ce77656faeb30}{%
           family={Gaddam},
           familyi={G\bibinitperiod},
           given={K.},
           giveni={K\bibinitperiod}}}%
        {{hash=a45cc8000f55692735dda5fd6407e0dd}{%
           family={Behrman},
           familyi={B\bibinitperiod},
           given={E.C.},
           giveni={E\bibinitperiod}}}%
        {{hash=c8cfc57e44a8bc2991570934c256e4b2}{%
           family={Steck},
           familyi={S\bibinitperiod},
           given={J.E.},
           giveni={J\bibinitperiod}}}%
        {{hash=d92871b512ccde2a8859e3a3c60f81e0}{%
           family={Skinner},
           familyi={S\bibinitperiod},
           given={S.R.},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{bb7e5ca59e8323b9c640758a2d76c4ff}
      \strng{fullhash}{060392fc9702f858613eb7a2cd7d509e}
      \strng{authornamehash}{bb7e5ca59e8323b9c640758a2d76c4ff}
      \strng{authorfullhash}{060392fc9702f858613eb7a2cd7d509e}
      \field{labelalpha}{All+}
      \field{sortinit}{1}
      \field{sortinithash}{27a2bc5dfb9ed0a0422134d636544b5d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No.02CH37290)}
      \field{isbn}{0-7803-7278-6}
      \field{title}{{Advantages of quantum recurrent networks: an examination of stable states}}
      \field{pages}{2732\bibrangedash 2737}
      \range{pages}{6}
      \verb{doi}
      \verb 10.1109/IJCNN.2002.1007579
      \endverb
      \verb{url}
      \verb http://ieeexplore.ieee.org/document/1007579/
      \endverb
    \endentry
    \entry{Rebentrost2018}{article}{}
      \name{author}{4}{}{%
        {{hash=a16ddc4e1334ba284e93d2d9662ed1c3}{%
           family={Rebentrost},
           familyi={R\bibinitperiod},
           given={Patrick},
           giveni={P\bibinitperiod}}}%
        {{hash=841b2f477190aecfcee0f0c5a2f76038}{%
           family={Bromley},
           familyi={B\bibinitperiod},
           given={Thomas\bibnamedelima R.},
           giveni={T\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=2adaf440c89e3d8a196dc4d6ecfa1f6d}{%
           family={Weedbrook},
           familyi={W\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
        {{hash=501c0933970931f080455b29b43e560d}{%
           family={Lloyd},
           familyi={L\bibinitperiod},
           given={Seth},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{de576dd095f70abd934d259abe8c2f6c}
      \strng{fullhash}{79c9383013a65822d1dcad57df5fb891}
      \strng{authornamehash}{de576dd095f70abd934d259abe8c2f6c}
      \strng{authorfullhash}{79c9383013a65822d1dcad57df5fb891}
      \field{labelalpha}{Reb+18}
      \field{sortinit}{1}
      \field{sortinithash}{27a2bc5dfb9ed0a0422134d636544b5d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{2469-9926}
      \field{journaltitle}{Physical Review A}
      \field{month}{10}
      \field{number}{4}
      \field{title}{{Quantum Hopfield neural network}}
      \field{volume}{98}
      \field{year}{2018}
      \field{pages}{042308}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1103/PhysRevA.98.042308
      \endverb
      \verb{url}
      \verb https://link.aps.org/doi/10.1103/PhysRevA.98.042308
      \endverb
    \endentry
    \entry{Cao2017}{article}{}
      \name{author}{3}{}{%
        {{hash=9d14b450758cf26d28129c893c5dbb06}{%
           family={Cao},
           familyi={C\bibinitperiod},
           given={Yudong},
           giveni={Y\bibinitperiod}}}%
        {{hash=3c5d311ff27dc6672101a461ced10bdc}{%
           family={Guerreschi},
           familyi={G\bibinitperiod},
           given={Gian\bibnamedelima Giacomo},
           giveni={G\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=083dcdd0db2687433b22fabfb2e55c17}{%
           family={Aspuru-Guzik},
           familyi={A\bibinithyphendelim G\bibinitperiod},
           given={Alán},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{ca24edd932063f74122cbb2f3bd24580}
      \strng{fullhash}{3d2da983f441902ea1f535af67b74f54}
      \strng{authornamehash}{ca24edd932063f74122cbb2f3bd24580}
      \strng{authorfullhash}{3d2da983f441902ea1f535af67b74f54}
      \field{labelalpha}{CGA17}
      \field{sortinit}{1}
      \field{sortinithash}{27a2bc5dfb9ed0a0422134d636544b5d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Even the most sophisticated artificial neural networks are built by aggregating substantially identical units called neurons. A neuron receives multiple signals, internally combines them, and applies a non-linear function to the resulting weighted sum. Several attempts to generalize neurons to the quantum regime have been proposed, but all proposals collided with the difficulty of implementing non-linear activation functions, which is essential for classical neurons, due to the linear nature of quantum mechanics. Here we propose a solution to this roadblock in the form of a small quantum circuit that naturally simulates neurons with threshold activation. Our quantum circuit defines a building block, the "quantum neuron", that can reproduce a variety of classical neural network constructions while maintaining the ability to process superpositions of inputs and preserve quantum coherence and entanglement. In the construction of feedforward networks of quantum neurons, we provide numerical evidence that the network not only can learn a function when trained with superposition of inputs and the corresponding output, but that this training suffices to learn the function on all individual inputs separately. When arranged to mimic Hopfield networks, quantum neural networks exhibit properties of associative memory. Patterns are encoded using the simple Hebbian rule for the weights and we demonstrate attractor dynamics from corrupted inputs. Finally, the fact that our quantum model closely captures (traditional) neural network dynamics implies that the vast body of literature and results on neural networks becomes directly relevant in the context of quantum machine learning.}
      \field{eprinttype}{arXiv}
      \field{month}{11}
      \field{title}{{Quantum Neuron: an elementary building block for machine learning on quantum computers}}
      \field{year}{2017}
      \verb{eprint}
      \verb 1711.11240
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1711.11240
      \endverb
    \endentry
    \entry{Elman1990}{article}{}
      \name{author}{1}{}{%
        {{hash=6bb3b700f57838129a9d2e4e7c8f00f8}{%
           family={Elman},
           familyi={E\bibinitperiod},
           given={J},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{6bb3b700f57838129a9d2e4e7c8f00f8}
      \strng{fullhash}{6bb3b700f57838129a9d2e4e7c8f00f8}
      \strng{authornamehash}{6bb3b700f57838129a9d2e4e7c8f00f8}
      \strng{authorfullhash}{6bb3b700f57838129a9d2e4e7c8f00f8}
      \field{labelalpha}{Elm90}
      \field{sortinit}{1}
      \field{sortinithash}{27a2bc5dfb9ed0a0422134d636544b5d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{03640213}
      \field{journaltitle}{Cognitive Science}
      \field{month}{6}
      \field{number}{2}
      \field{title}{{Finding structure in time}}
      \field{volume}{14}
      \field{year}{1990}
      \field{pages}{179\bibrangedash 211}
      \range{pages}{33}
      \verb{doi}
      \verb 10.1016/0364-0213(90)90002-E
      \endverb
      \verb{url}
      \verb http://doi.wiley.com/10.1016/0364-0213(90)90002-E
      \endverb
    \endentry
    \entry{Du2018OnTP}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=3049249a1274ffe6fce64925da575c31}{%
           family={Du},
           familyi={D\bibinitperiod},
           given={Simon\bibnamedelima S.},
           giveni={S\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=7a3f363490f9d59f622fce1c4c663da3}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Jason\bibnamedelima D.},
           giveni={J\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
      }
      \strng{namehash}{25fa813aed732ceb35db12367e9a876c}
      \strng{fullhash}{25fa813aed732ceb35db12367e9a876c}
      \strng{authornamehash}{25fa813aed732ceb35db12367e9a876c}
      \strng{authorfullhash}{25fa813aed732ceb35db12367e9a876c}
      \field{labelalpha}{DL18}
      \field{sortinit}{1}
      \field{sortinithash}{27a2bc5dfb9ed0a0422134d636544b5d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{ICML}
      \field{title}{On the Power of Over-parametrization in Neural Networks with Quadratic Activation}
      \field{year}{2018}
    \endentry
    \entry{Arora2018OnTO}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=8e08b59145b9531815169c7ed0a0b055}{%
           family={Arora},
           familyi={A\bibinitperiod},
           given={Sanjeev},
           giveni={S\bibinitperiod}}}%
        {{hash=049bbc3640fa48f818ad49526f7efcf7}{%
           family={Cohen},
           familyi={C\bibinitperiod},
           given={Nadav},
           giveni={N\bibinitperiod}}}%
        {{hash=0b3d32703edd2b7248e8e8f33c5893db}{%
           family={Hazan},
           familyi={H\bibinitperiod},
           given={Elad},
           giveni={E\bibinitperiod}}}%
      }
      \strng{namehash}{ab405688b3c276098991bcafe5da7076}
      \strng{fullhash}{cf8a2727b3f7a476bc83601f2bf6b428}
      \strng{authornamehash}{ab405688b3c276098991bcafe5da7076}
      \strng{authorfullhash}{cf8a2727b3f7a476bc83601f2bf6b428}
      \field{labelalpha}{ACH18}
      \field{sortinit}{1}
      \field{sortinithash}{27a2bc5dfb9ed0a0422134d636544b5d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{ICML}
      \field{title}{On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization}
      \field{year}{2018}
    \endentry
    \entry{Benedetti2019}{article}{}
      \name{author}{4}{}{%
        {{hash=a72591df50023022dfff6f8200ef27fe}{%
           family={Benedetti},
           familyi={B\bibinitperiod},
           given={Marcello},
           giveni={M\bibinitperiod}}}%
        {{hash=274e282577fc2d1652647222f033f539}{%
           family={Lloyd},
           familyi={L\bibinitperiod},
           given={Erika},
           giveni={E\bibinitperiod}}}%
        {{hash=e637e1d0e0a1068efffc644459f15f55}{%
           family={Sack},
           familyi={S\bibinitperiod},
           given={Stefan},
           giveni={S\bibinitperiod}}}%
        {{hash=4dbefc3f9d1da32047d9349fa33d431a}{%
           family={Fiorentini},
           familyi={F\bibinitperiod},
           given={Mattia},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{78f945f62a9a0d68e82b1c6b7a9e6050}
      \strng{fullhash}{b331929d98c2f14c09f0ffb1fc66acd0}
      \strng{authornamehash}{78f945f62a9a0d68e82b1c6b7a9e6050}
      \strng{authorfullhash}{b331929d98c2f14c09f0ffb1fc66acd0}
      \field{labelalpha}{Ben+19}
      \field{sortinit}{1}
      \field{sortinithash}{27a2bc5dfb9ed0a0422134d636544b5d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Hybrid quantum-classical systems make it possible to utilize existing quantum computers to their fullest extent. Within this framework, parameterized quantum circuits can be regarded as machine learning models with remarkable expressive power. This Review presents the components of these models and discusses their application to a variety of data-driven tasks, such as supervised learning and generative modeling. With an increasing number of experimental demonstrations carried out on actual quantum hardware and with software being actively developed, this rapidly growing field is poised to have a broad spectrum of real-world applications.}
      \field{eprinttype}{arXiv}
      \field{month}{6}
      \field{title}{{Parameterized quantum circuits as machine learning models}}
      \field{year}{2019}
      \verb{doi}
      \verb 10.1088/2058-9565/ab4eb5
      \endverb
      \verb{eprint}
      \verb 1906.07682
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1906.07682%20http://dx.doi.org/10.1088/2058-9565/ab4eb5
      \endverb
    \endentry
    \entry{Guerreschi2019}{article}{}
      \name{author}{1}{}{%
        {{hash=3c5d311ff27dc6672101a461ced10bdc}{%
           family={Guerreschi},
           familyi={G\bibinitperiod},
           given={Gian\bibnamedelima Giacomo},
           giveni={G\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
      }
      \strng{namehash}{3c5d311ff27dc6672101a461ced10bdc}
      \strng{fullhash}{3c5d311ff27dc6672101a461ced10bdc}
      \strng{authornamehash}{3c5d311ff27dc6672101a461ced10bdc}
      \strng{authorfullhash}{3c5d311ff27dc6672101a461ced10bdc}
      \field{labelalpha}{Gue19}
      \field{sortinit}{1}
      \field{sortinithash}{27a2bc5dfb9ed0a0422134d636544b5d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{2469-9926}
      \field{journaltitle}{Physical Review A}
      \field{month}{2}
      \field{number}{2}
      \field{title}{{Repeat-until-success circuits with fixed-point oblivious amplitude amplification}}
      \field{volume}{99}
      \field{year}{2019}
      \field{pages}{022306}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1103/PhysRevA.99.022306
      \endverb
      \verb{url}
      \verb https://link.aps.org/doi/10.1103/PhysRevA.99.022306
      \endverb
    \endentry
    \entry{Tacchino2019}{article}{}
      \name{author}{4}{}{%
        {{hash=e5985f7aa76d155dd17b6e97c5a06524}{%
           family={Tacchino},
           familyi={T\bibinitperiod},
           given={Francesco},
           giveni={F\bibinitperiod}}}%
        {{hash=fa8ecd6f771849c9a17bd4d6ab7a508d}{%
           family={Macchiavello},
           familyi={M\bibinitperiod},
           given={Chiara},
           giveni={C\bibinitperiod}}}%
        {{hash=23db5e898eec3ed73af58e331cdfbeb4}{%
           family={Gerace},
           familyi={G\bibinitperiod},
           given={Dario},
           giveni={D\bibinitperiod}}}%
        {{hash=40598842267c6eda63266044051fef9c}{%
           family={Bajoni},
           familyi={B\bibinitperiod},
           given={Daniele},
           giveni={D\bibinitperiod}}}%
      }
      \strng{namehash}{f37da7e6b09e24caeacc5b75dd8f62bf}
      \strng{fullhash}{abfe2f24a5018f4ee54f129dfe7b27a0}
      \strng{authornamehash}{f37da7e6b09e24caeacc5b75dd8f62bf}
      \strng{authorfullhash}{abfe2f24a5018f4ee54f129dfe7b27a0}
      \field{labelalpha}{Tac+19}
      \field{sortinit}{2}
      \field{sortinithash}{0aa614ace9f3a40ef5a67e7f7a184048}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{2056-6387}
      \field{journaltitle}{npj Quantum Information}
      \field{month}{12}
      \field{number}{1}
      \field{title}{{An artificial neuron implemented on an actual quantum processor}}
      \field{volume}{5}
      \field{year}{2019}
      \field{pages}{26}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1038/s41534-019-0140-4
      \endverb
      \verb{url}
      \verb http://www.nature.com/articles/s41534-019-0140-4
      \endverb
    \endentry
    \entry{DePaulaNeto2019}{article}{}
      \name{author}{4}{}{%
        {{hash=449a9e040f57084d930b2db3f1b0b719}{%
           family={{de Paula Neto}},
           familyi={d\bibinitperiod},
           given={Fernando\bibnamedelima M.},
           giveni={F\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=1e0c85c789f3d429214cc0f6e77f50f9}{%
           family={Ludermir},
           familyi={L\bibinitperiod},
           given={Teresa\bibnamedelima B.},
           giveni={T\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=a83827b3cf4ddc2f563b86b96935d705}{%
           family={Oliveira},
           familyi={O\bibinitperiod},
           given={Wilson\bibnamedelima R.},
           giveni={W\bibinitperiod\bibinitdelim R\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
        {{hash=077f0c60da8044a7b98a2e81ad94ee94}{%
           family={Silva},
           familyi={S\bibinitperiod},
           given={Adenilton\bibnamedelima J.},
           giveni={A\bibinitperiod\bibinitdelim J\bibinitperiod},
           prefix={da},
           prefixi={d\bibinitperiod}}}%
      }
      \strng{namehash}{dc60ed7f7d2ef310e91c27e2d719a671}
      \strng{fullhash}{122c66687d0656da639304762df126e2}
      \strng{authornamehash}{dc60ed7f7d2ef310e91c27e2d719a671}
      \strng{authorfullhash}{122c66687d0656da639304762df126e2}
      \field{labelalpha}{de +19}
      \field{sortinit}{2}
      \field{sortinithash}{0aa614ace9f3a40ef5a67e7f7a184048}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{2162-237X}
      \field{journaltitle}{IEEE Transactions on Neural Networks and Learning Systems}
      \field{title}{{Implementing Any Nonlinear Quantum Neuron}}
      \field{year}{2019}
      \field{pages}{1\bibrangedash 6}
      \range{pages}{6}
      \verb{doi}
      \verb 10.1109/TNNLS.2019.2938899
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/8848854/
      \endverb
    \endentry
    \entry{code}{online}{}
      \name{author}{1}{}{%
        {{hash=1ec652491db06beba3928734411dd45f}{%
           family={Bausch},
           familyi={B\bibinitperiod},
           given={Johannes},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{1ec652491db06beba3928734411dd45f}
      \strng{fullhash}{1ec652491db06beba3928734411dd45f}
      \strng{authornamehash}{1ec652491db06beba3928734411dd45f}
      \strng{authorfullhash}{1ec652491db06beba3928734411dd45f}
      \field{labelalpha}{Bau20}
      \field{sortinit}{3}
      \field{sortinithash}{197da6d6c34c6b20ce45c4d4baace5a4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{QRNN}
      \field{urlday}{25}
      \field{urlmonth}{6}
      \field{urlyear}{2020}
      \field{year}{2020}
      \field{urldateera}{ce}
      \verb{url}
      \verb https://bitbucket.org/rumschuettel/rvqe.git
      \endverb
    \endentry
    \entry{Wierichs2020}{article}{}
      \name{author}{3}{}{%
        {{hash=b3b26992d4e0cffa2e1fbdafeef6e03d}{%
           family={Wierichs},
           familyi={W\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=b2283e26f29ee3d76c9354deb486064e}{%
           family={Gogolin},
           familyi={G\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
        {{hash=8f19ce35aa344360a3946f941a66c9a4}{%
           family={Kastoryano},
           familyi={K\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{b57e0fa876e8050a86eb4940bba75a7e}
      \strng{fullhash}{5b49bc951539dd6b02ff214b6afaa426}
      \strng{authornamehash}{b57e0fa876e8050a86eb4940bba75a7e}
      \strng{authorfullhash}{5b49bc951539dd6b02ff214b6afaa426}
      \field{labelalpha}{WGK20}
      \field{sortinit}{3}
      \field{sortinithash}{197da6d6c34c6b20ce45c4d4baace5a4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We compare the BFGS optimizer, ADAM and Natural Gradient Descent (NatGrad) in the context of Variational Quantum Eigensolvers (VQEs). We systematically analyze their performance on the QAOA ansatz for the Transverse Field Ising Model (TFIM) as well as on overparametrized circuits with the ability to break the symmetry of the Hamiltonian. The BFGS algorithm is frequently unable to find a global minimum for systems beyond about 20 spins and ADAM easily gets trapped in local minima. On the other hand, NatGrad shows stable performance on all considered system sizes, albeit at a significantly higher cost per epoch. In sharp contrast to most classical gradient based learning, the performance of all optimizers is found to decrease upon seemingly benign overparametrization of the ansatz class, with BFGS and ADAM failing more often and more severely than NatGrad. Additional tests for the Heisenberg XXZ model corroborate the accuracy problems of BFGS in high dimensions, but they reveal some shortcomings of NatGrad as well. Our results suggest that great care needs to be taken in the choice of gradient based optimizers and the parametrization for VQEs.}
      \field{eprinttype}{arXiv}
      \field{month}{4}
      \field{title}{{Avoiding local minima in variational quantum eigensolvers with the natural gradient optimizer}}
      \field{year}{2020}
      \verb{eprint}
      \verb 2004.14666
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2004.14666
      \endverb
    \endentry
    \entry{Farhi2018}{article}{}
      \name{author}{2}{}{%
        {{hash=5e61ee12360fc49d9013fbf8fe18db89}{%
           family={Farhi},
           familyi={F\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod}}}%
        {{hash=ddcc46c5456340da59be4077c7102a27}{%
           family={Neven},
           familyi={N\bibinitperiod},
           given={Hartmut},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{57f3974107d5250f7a12e05879d6cfb7}
      \strng{fullhash}{57f3974107d5250f7a12e05879d6cfb7}
      \strng{authornamehash}{57f3974107d5250f7a12e05879d6cfb7}
      \strng{authorfullhash}{57f3974107d5250f7a12e05879d6cfb7}
      \field{labelalpha}{FN18}
      \field{sortinit}{3}
      \field{sortinithash}{197da6d6c34c6b20ce45c4d4baace5a4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We introduce a quantum neural network, QNN, that can represent labeled data, classical or quantum, and be trained by supervised learning. The quantum circuit consists of a sequence of parameter dependent unitary transformations which acts on an input quantum state. For binary classification a single Pauli operator is measured on a designated readout qubit. The measured output is the quantum neural network's predictor of the binary label of the input state. First we look at classifying classical data sets which consist of n-bit strings with binary labels. The input quantum state is an n-bit computational basis state corresponding to a sample string. We show how to design a circuit made from two qubit unitaries that can correctly represent the label of any Boolean function of n bits. For certain label functions the circuit is exponentially long. We introduce parameter dependent unitaries that can be adapted by supervised learning of labeled data. We study an example of real world data consisting of downsampled images of handwritten digits each of which has been labeled as one of two distinct digits. We show through classical simulation that parameters can be found that allow the QNN to learn to correctly distinguish the two data sets. We then discuss presenting the data as quantum superpositions of computational basis states corresponding to different label values. Here we show through simulation that learning is possible. We consider using our QNN to learn the label of a general quantum state. By example we show that this can be done. Our work is exploratory and relies on the classical simulation of small quantum systems. The QNN proposed here was designed with near-term quantum processors in mind. Therefore it will be possible to run this QNN on a near term gate model quantum computer where its power can be explored beyond what can be explored with simulation.}
      \field{eprinttype}{arXiv}
      \field{month}{2}
      \field{title}{{Classification with Quantum Neural Networks on Near Term Processors}}
      \field{year}{2018}
      \verb{eprint}
      \verb 1802.06002
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1802.06002
      \endverb
    \endentry
    \entry{Grant2019}{article}{}
      \name{author}{4}{}{%
        {{hash=b1d176833e606713973a4617ef10e618}{%
           family={Grant},
           familyi={G\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod}}}%
        {{hash=87c1e1f5d917ee25158301ca68f9794d}{%
           family={Wossnig},
           familyi={W\bibinitperiod},
           given={Leonard},
           giveni={L\bibinitperiod}}}%
        {{hash=bbed4c280130bd2ccc3c81a498a2d679}{%
           family={Ostaszewski},
           familyi={O\bibinitperiod},
           given={Mateusz},
           giveni={M\bibinitperiod}}}%
        {{hash=a72591df50023022dfff6f8200ef27fe}{%
           family={Benedetti},
           familyi={B\bibinitperiod},
           given={Marcello},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{05e40d60659c19fd2074b38b72e4b0a7}
      \strng{fullhash}{94c0a2107854bc5a961f54aaca368e51}
      \strng{authornamehash}{05e40d60659c19fd2074b38b72e4b0a7}
      \strng{authorfullhash}{94c0a2107854bc5a961f54aaca368e51}
      \field{labelalpha}{Gra+19}
      \field{sortinit}{3}
      \field{sortinithash}{197da6d6c34c6b20ce45c4d4baace5a4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Parametrized quantum circuits initialized with random initial parameter values are characterized by barren plateaus where the gradient becomes exponentially small in the number of qubits. In this technical note we theoretically motivate and empirically validate an initialization strategy which can resolve the barren plateau problem for practical applications. The technique involves randomly selecting some of the initial parameter values, then choosing the remaining values so that the circuit is a sequence of shallow blocks that each evaluates to the identity. This initialization limits the effective depth of the circuits used to calculate the first parameter update so that they cannot be stuck in a barren plateau at the start of training. In turn, this makes some of the most compact ans$\backslash$"atze usable in practice, which was not possible before even for rather basic problems. We show empirically that variational quantum eigensolvers and quantum neural networks initialized using this strategy can be trained using a gradient based method.}
      \field{eprinttype}{arXiv}
      \field{month}{3}
      \field{title}{{An initialization strategy for addressing barren plateaus in parametrized quantum circuits}}
      \field{year}{2019}
      \verb{doi}
      \verb 10.22331/q-2019-12-09-214
      \endverb
      \verb{eprint}
      \verb 1903.05076
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1903.05076%20http://dx.doi.org/10.22331/q-2019-12-09-214
      \endverb
    \endentry
    \entry{Kerenidis2018}{article}{}
      \name{author}{2}{}{%
        {{hash=8ba951e8169455059a851c60ef526395}{%
           family={Kerenidis},
           familyi={K\bibinitperiod},
           given={Iordanis},
           giveni={I\bibinitperiod}}}%
        {{hash=bb8061beba57bd23a89455781371b46b}{%
           family={Luongo},
           familyi={L\bibinitperiod},
           given={Alessandro},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{b5b644506cea5db935eda33a7a12b34f}
      \strng{fullhash}{b5b644506cea5db935eda33a7a12b34f}
      \strng{authornamehash}{b5b644506cea5db935eda33a7a12b34f}
      \strng{authorfullhash}{b5b644506cea5db935eda33a7a12b34f}
      \field{labelalpha}{KL20}
      \field{sortinit}{4}
      \field{sortinithash}{d221becdd544034cf9082056cd6e6d2d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Quantum machine learning carries the promise to revolutionize information and communication technologies. While a number of quantum algorithms with potential exponential speedups have been proposed already, it is quite difficult to provide convincing evidence that quantum computers with quantum memories will be in fact useful to solve real-world problems. Our work makes considerable progress towards this goal. We design quantum techniques for Dimensionality Reduction and for Classification, and combine them to provide an efficient and high accuracy quantum classifier that we test on the MNIST dataset. More precisely, we propose a quantum version of Slow Feature Analysis (QSFA), a dimensionality reduction technique that maps the dataset in a lower dimensional space where we can apply a novel quantum classification procedure, the Quantum Frobenius Distance (QFD). We simulate the quantum classifier (including errors) and show that it can provide classification of the MNIST handwritten digit dataset, a widely used dataset for benchmarking classification algorithms, with {\$}98.5\backslash{\%}{\$} accuracy, similar to the classical case. The running time of the quantum classifier is polylogarithmic in the dimension and number of data points. We also provide evidence that the other parameters on which the running time depends (condition number, Frobenius norm, error threshold, etc.) scale favorably in practice, thus ascertaining the efficiency of our algorithm.}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{Phys. Rev. A}
      \field{month}{5}
      \field{title}{{Quantum classification of the MNIST dataset via Slow Feature Analysis}}
      \field{year}{2020}
      \verb{eprint}
      \verb 1805.08837
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1805.08837
      \endverb
    \endentry
    \entry{Harrow2009a}{article}{}
      \name{author}{3}{}{%
        {{hash=3b6d9e507f2fddf3546501dc8331497f}{%
           family={Harrow},
           familyi={H\bibinitperiod},
           given={Aram\bibnamedelima W.},
           giveni={A\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
        {{hash=3765d849007f662dc14f33a696aa6a7b}{%
           family={Hassidim},
           familyi={H\bibinitperiod},
           given={Avinatan},
           giveni={A\bibinitperiod}}}%
        {{hash=501c0933970931f080455b29b43e560d}{%
           family={Lloyd},
           familyi={L\bibinitperiod},
           given={Seth},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{237551882eb3b379bc2666b2058a67f4}
      \strng{fullhash}{8fe435753dcef4ab74b5aab8ecc25a85}
      \strng{authornamehash}{237551882eb3b379bc2666b2058a67f4}
      \strng{authorfullhash}{8fe435753dcef4ab74b5aab8ecc25a85}
      \field{labelalpha}{HHL08}
      \field{sortinit}{4}
      \field{sortinithash}{d221becdd544034cf9082056cd6e6d2d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Solving linear systems of equations is a common problem that arises both on its own and as a subroutine in more complex problems: given a matrix A and a vector b, find a vector x such that Ax=b. We consider the case where one doesn't need to know the solution x itself, but rather an approximation of the expectation value of some operator associated with x, e.g., x'Mx for some matrix M. In this case, when A is sparse, N by N and has condition number kappa, classical algorithms can find x and estimate x'Mx in O(N sqrt(kappa)) time. Here, we exhibit a quantum algorithm for this task that runs in poly(log N, kappa) time, an exponential improvement over the best classical algorithm.}
      \field{eprinttype}{arXiv}
      \field{issn}{0031-9007}
      \field{journaltitle}{Physical Review Letters}
      \field{month}{11}
      \field{number}{15}
      \field{title}{{Quantum algorithm for solving linear systems of equations}}
      \field{volume}{103}
      \field{year}{2008}
      \field{pages}{150502}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1103/PhysRevLett.103.150502
      \endverb
      \verb{eprint}
      \verb 0811.3171
      \endverb
      \verb{url}
      \verb https://link.aps.org/doi/10.1103/PhysRevLett.103.150502%20http://arxiv.org/abs/0811.3171%20http://dx.doi.org/10.1103/PhysRevLett.103.150502
      \endverb
    \endentry
    \entry{berkes2005pattern}{article}{}
      \name{author}{1}{}{%
        {{hash=dc12bb6ec9059f9728ad6185de93996c}{%
           family={Berkes},
           familyi={B\bibinitperiod},
           given={Pietro},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{dc12bb6ec9059f9728ad6185de93996c}
      \strng{fullhash}{dc12bb6ec9059f9728ad6185de93996c}
      \strng{authornamehash}{dc12bb6ec9059f9728ad6185de93996c}
      \strng{authorfullhash}{dc12bb6ec9059f9728ad6185de93996c}
      \field{labelalpha}{Ber05}
      \field{sortinit}{4}
      \field{sortinithash}{d221becdd544034cf9082056cd6e6d2d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Pattern recognition with slow feature analysis}
      \field{year}{2005}
    \endentry
    \entry{Tang2019}{inproceedings}{}
      \name{author}{1}{}{%
        {{hash=0dab0a5fa796c20f4453ef26c88d75e1}{%
           family={Tang},
           familyi={T\bibinitperiod},
           given={Ewin},
           giveni={E\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {New York, New York, USA}%
      }
      \list{publisher}{1}{%
        {ACM Press}%
      }
      \strng{namehash}{0dab0a5fa796c20f4453ef26c88d75e1}
      \strng{fullhash}{0dab0a5fa796c20f4453ef26c88d75e1}
      \strng{authornamehash}{0dab0a5fa796c20f4453ef26c88d75e1}
      \strng{authorfullhash}{0dab0a5fa796c20f4453ef26c88d75e1}
      \field{labelalpha}{Tan19}
      \field{sortinit}{4}
      \field{sortinithash}{d221becdd544034cf9082056cd6e6d2d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing - STOC 2019}
      \field{isbn}{9781450367059}
      \field{title}{{A quantum-inspired classical algorithm for recommendation systems}}
      \field{year}{2019}
      \field{pages}{217\bibrangedash 228}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1145/3313276.3316310
      \endverb
      \verb{url}
      \verb http://dl.acm.org/citation.cfm?doid=3313276.3316310
      \endverb
    \endentry
    \entry{lecun2010mnist}{article}{}
      \name{author}{3}{}{%
        {{hash=6a1aa6b7eab12b931ca7c7e3f927231d}{%
           family={LeCun},
           familyi={L\bibinitperiod},
           given={Yann},
           giveni={Y\bibinitperiod}}}%
        {{hash=17acda211a651e90e228f1776ee07818}{%
           family={Cortes},
           familyi={C\bibinitperiod},
           given={Corinna},
           giveni={C\bibinitperiod}}}%
        {{hash=270087258d6a002033f05032fbdf6fad}{%
           family={Burges},
           familyi={B\bibinitperiod},
           given={CJ},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{9e4c6012409dc8dd9b2aa198a2059804}
      \strng{fullhash}{b2aa156a476841eb9aa413b99e0bd259}
      \strng{authornamehash}{9e4c6012409dc8dd9b2aa198a2059804}
      \strng{authorfullhash}{b2aa156a476841eb9aa413b99e0bd259}
      \field{labelalpha}{LCB10}
      \field{sortinit}{4}
      \field{sortinithash}{d221becdd544034cf9082056cd6e6d2d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist}
      \field{title}{MNIST handwritten digit database}
      \field{volume}{2}
      \field{year}{2010}
    \endentry
    \entry{tSNE}{article}{}
      \name{author}{2}{}{%
        {{hash=184aa20aa5ca3192a62bf066e384c5ab}{%
           family={Maaten},
           familyi={M\bibinitperiod},
           given={Laurens},
           giveni={L\bibinitperiod},
           prefix={van\bibnamedelima der},
           prefixi={v\bibinitperiod\bibinitdelim d\bibinitperiod}}}%
        {{hash=881b7655f3886f90a9400902a521acdb}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey\bibnamedelima E.},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \strng{namehash}{c0b80b1e0ff8551ba24193f9760a0d3a}
      \strng{fullhash}{c0b80b1e0ff8551ba24193f9760a0d3a}
      \strng{authornamehash}{c0b80b1e0ff8551ba24193f9760a0d3a}
      \strng{authorfullhash}{c0b80b1e0ff8551ba24193f9760a0d3a}
      \field{labelalpha}{MH08}
      \field{sortinit}{4}
      \field{sortinithash}{d221becdd544034cf9082056cd6e6d2d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{title}{Visualizing Data using t-SNE}
      \field{volume}{9}
      \field{year}{2008}
      \field{pages}{2579\bibrangedash 2605}
      \range{pages}{27}
    \endentry
    \entry{UMAP}{article}{}
      \name{author}{3}{}{%
        {{hash=7e7b0b236efe84b199d2de2730598422}{%
           family={{McInnes}},
           familyi={M\bibinitperiod},
           given={L.},
           giveni={L\bibinitperiod}}}%
        {{hash=a96c6ef29823b2593284c4b42c06156f}{%
           family={{Healy}},
           familyi={H\bibinitperiod},
           given={J.},
           giveni={J\bibinitperiod}}}%
        {{hash=a29511ec7f83f33752f934a514d8a020}{%
           family={{Melville}},
           familyi={M\bibinitperiod},
           given={J.},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{ac235f42c7386c2818ae5f2a05b16ff3}
      \strng{fullhash}{51aeb33481e86088f2c8acc34b894cb2}
      \strng{authornamehash}{ac235f42c7386c2818ae5f2a05b16ff3}
      \strng{authorfullhash}{51aeb33481e86088f2c8acc34b894cb2}
      \field{labelalpha}{MHM18}
      \field{sortinit}{4}
      \field{sortinithash}{d221becdd544034cf9082056cd6e6d2d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{eprintclass}{stat.ML}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{ArXiv e-prints}
      \field{month}{2}
      \field{title}{{UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}}
      \field{year}{2018}
      \verb{eprint}
      \verb 1802.03426
      \endverb
      \keyw{Statistics - Machine Learning,Computer Science - Computational Geometry,Computer Science - Learning}
    \endentry
    \entry{Bausch2019c}{article}{}
      \name{author}{3}{}{%
        {{hash=1ec652491db06beba3928734411dd45f}{%
           family={Bausch},
           familyi={B\bibinitperiod},
           given={Johannes},
           giveni={J\bibinitperiod}}}%
        {{hash=d6b81dfa9fc35f9111836568b656b889}{%
           family={Subramanian},
           familyi={S\bibinitperiod},
           given={Sathyawageeswar},
           giveni={S\bibinitperiod}}}%
        {{hash=b676060ef28394af62630bf64776637d}{%
           family={Piddock},
           familyi={P\bibinitperiod},
           given={Stephen},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{bf33b4717ca57709fed7c5fe5be3f269}
      \strng{fullhash}{9d4e667656c2583f9aebc1e675a8af3c}
      \strng{authornamehash}{bf33b4717ca57709fed7c5fe5be3f269}
      \strng{authorfullhash}{9d4e667656c2583f9aebc1e675a8af3c}
      \field{labelalpha}{BSP19}
      \field{sortinit}{5}
      \field{sortinithash}{c9df3c9fb8f555dd9201cedc5e343021}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Probabilistic language models, e.g. those based on an LSTM, often face the problem of finding a high probability prediction from a sequence of random variables over a set of words. This is commonly addressed using a form of greedy decoding such as beam search, where a limited number of highest-likelihood paths (the beam width) of the decoder are kept, and at the end the maximum-likelihood path is chosen. The resulting algorithm has linear runtime in the beam width. However, the input is not necessarily distributed such that a high-likelihood input symbol at any given time step also leads to the global optimum. Limiting the beam width can thus result in a failure to recognise long-range dependencies. In practice, only an exponentially large beam width can guarantee that the global optimum is found: for an input of length {\$}n{\$} and average parser branching ratio {\$}R{\$}, the baseline classical algorithm needs to query the input on average {\$}R{\^{}}n{\$} times. In this work, we construct a quantum algorithm to find the globally optimal parse with high constant success probability. Given the input to the decoder is distributed like a power-law with exponent {\$}k{>}0{\$}, our algorithm yields a runtime {\$}R{\^{}}{\{}n f(R,k){\}}{\$}, where {\$}f\backslashle 1/2{\$}, and {\$}f\backslashrightarrow 0{\$} exponentially quickly for growing {\$}k{\$}. This implies that our algorithm always yields a super-Grover type speedup, i.e. it is more than quadratically faster than its classical counterpart. We further modify our procedure to recover a quantum beam search variant, which enables an even stronger empirical speedup, while sacrificing accuracy. Finally, we apply this quantum beam search decoder to Mozilla's implementation of Baidu's DeepSpeech neural net, which we show to exhibit such a power law word rank frequency, underpinning the applicability of our model.}
      \field{eprinttype}{arXiv}
      \field{month}{9}
      \field{title}{{A Quantum Search Decoder for Natural Language Processing}}
      \field{year}{2019}
      \verb{eprint}
      \verb 1909.05023
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1909.05023
      \endverb
    \endentry
    \entry{Flurin2020}{article}{}
      \name{author}{4}{}{%
        {{hash=a520eb49c3389711740bb1d50d4844a3}{%
           family={Flurin},
           familyi={F\bibinitperiod},
           given={E.},
           giveni={E\bibinitperiod}}}%
        {{hash=8925eaddc1ff0de9492f08242e4f3266}{%
           family={Martin},
           familyi={M\bibinitperiod},
           given={L.\bibnamedelimi S.},
           giveni={L\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=6f72918d03bc5aa6d50ea89f895e3102}{%
           family={Hacohen-Gourgy},
           familyi={H\bibinithyphendelim G\bibinitperiod},
           given={S.},
           giveni={S\bibinitperiod}}}%
        {{hash=9eeaa58b7c22e44d3475b0d4a352c2fe}{%
           family={Siddiqi},
           familyi={S\bibinitperiod},
           given={I.},
           giveni={I\bibinitperiod}}}%
      }
      \strng{namehash}{b966de30bba055b33e1d1bfaeb394070}
      \strng{fullhash}{618dc2d5a103a75a3a7dc4ee4d98e9c3}
      \strng{authornamehash}{b966de30bba055b33e1d1bfaeb394070}
      \strng{authorfullhash}{618dc2d5a103a75a3a7dc4ee4d98e9c3}
      \field{labelalpha}{Flu+20}
      \field{sortinit}{5}
      \field{sortinithash}{c9df3c9fb8f555dd9201cedc5e343021}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{2160-3308}
      \field{journaltitle}{Physical Review X}
      \field{month}{1}
      \field{number}{1}
      \field{title}{{Using a Recurrent Neural Network to Reconstruct Quantum Dynamics of a Superconducting Qubit from Physical Observations}}
      \field{volume}{10}
      \field{year}{2020}
      \field{pages}{011006}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1103/PhysRevX.10.011006
      \endverb
      \verb{url}
      \verb https://link.aps.org/doi/10.1103/PhysRevX.10.011006
      \endverb
    \endentry
    \entry{Nielsen2010}{book}{}
      \name{author}{2}{}{%
        {{hash=790b57a148d0102f1420b73dd7d8cb95}{%
           family={Nielsen},
           familyi={N\bibinitperiod},
           given={Michael\bibnamedelima A.},
           giveni={M\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=36b0efc2456ce64558c3e84e97461a10}{%
           family={Chuang},
           familyi={C\bibinitperiod},
           given={Isaac\bibnamedelima L.},
           giveni={I\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cambridge}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{f00d3adcb5103f5ad0e0df199bb9a86d}
      \strng{fullhash}{f00d3adcb5103f5ad0e0df199bb9a86d}
      \strng{authornamehash}{f00d3adcb5103f5ad0e0df199bb9a86d}
      \strng{authorfullhash}{f00d3adcb5103f5ad0e0df199bb9a86d}
      \field{labelalpha}{NC10}
      \field{sortinit}{5}
      \field{sortinithash}{c9df3c9fb8f555dd9201cedc5e343021}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{9780511976667}
      \field{title}{{Quantum Computation and Quantum Information}}
      \field{year}{2010}
      \field{pages}{676}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1017/CBO9780511976667
      \endverb
      \verb{file}
      \verb :home/jkrb2/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nielsen, Chuang - 2010 - Quantum Computation and Quantum Information.djvu:djvu
      \endverb
    \endentry
    \entry{Grover2005}{article}{}
      \name{author}{1}{}{%
        {{hash=943d6432899f7bff826bcde1c3c5529e}{%
           family={Grover},
           familyi={G\bibinitperiod},
           given={Lov\bibnamedelima K.},
           giveni={L\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
      }
      \strng{namehash}{943d6432899f7bff826bcde1c3c5529e}
      \strng{fullhash}{943d6432899f7bff826bcde1c3c5529e}
      \strng{authornamehash}{943d6432899f7bff826bcde1c3c5529e}
      \strng{authorfullhash}{943d6432899f7bff826bcde1c3c5529e}
      \field{labelalpha}{Gro05}
      \field{sortinit}{5}
      \field{sortinithash}{c9df3c9fb8f555dd9201cedc5e343021}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0031-9007}
      \field{journaltitle}{Physical Review Letters}
      \field{month}{10}
      \field{number}{15}
      \field{title}{{Fixed-Point Quantum Search}}
      \field{volume}{95}
      \field{year}{2005}
      \field{pages}{150501}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1103/PhysRevLett.95.150501
      \endverb
      \verb{url}
      \verb https://link.aps.org/doi/10.1103/PhysRevLett.95.150501
      \endverb
    \endentry
  \endsortlist
  \missing{fig:qrnn}
\endrefsection
\endinput

