%
% machine translation / pretraining (NLP)
%

@article{paszke2017automatic,
  title={Automatic differentiation in pytorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  journal={NIPS 2017 Autodiff Workshop},
  year={2017}
}

@article{kudo2018sentencepiece,
  title={Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing},
  author={Kudo, Taku and Richardson, John},
  journal={arXiv preprint arXiv:1808.06226},
  year={2018}
}

@article{lample2018unsupervised,
  title={Unsupervised machine translation using monolingual corpora only},
  author={Lample, Guillaume and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  journal={ICLR},
  year={2018}
}

@inproceedings{lample2018phrase,
  title={Phrase-Based \& Neural Unsupervised Machine Translation},
  author={Lample, Guillaume and Ott, Myle and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  booktitle={EMNLP},
  year={2018}
}

@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={CoRR},
  volume={abs/1810.04805},
  year={2018}
}

@article{lample2019cross,
  title={Cross-lingual Language Model Pretraining},
  author={Lample, Guillaume and Conneau, Alexis},
  journal={arXiv preprint arXiv:1901.07291},
  year={2019}
}

@inproceedings{lample2018word,
  title={Word translation without parallel data},
  author={Lample, Guillaume and Conneau, Alexis and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'e}gou, Herv{\'e}},
  booktitle = {ICLR},
  year={2018}
}


@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{unsupNMTartetxe,
  title = {Unsupervised neural machine translation},
  author = {Mikel Artetxe and Gorka Labaka and Eneko Agirre and Kyunghyun Cho},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year = {2018}
}

@inproceedings{artetxe2017learning,
  title={Learning bilingual word embeddings with (almost) no bilingual data},
  author={Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  volume={1},
  pages={451--462},
  year={2017}
}

@article{artetxe2018unsupervised,
  title={Unsupervised statistical machine translation},
  author={Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
  journal={arXiv preprint arXiv:1809.01272},
  year={2018}
}

@inproceedings{song2019mass,
  title={MASS: Masked Sequence to Sequence Pre-training for Language Generation},
  author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  booktitle={International Conference on Machine Learning},
  pages={5926--5936},
  year={2019}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@inproceedings{vincent2008extracting,
  title={Extracting and composing robust features with denoising autoencoders},
  author={Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1096--1103},
  year={2008}
}

@inproceedings{koehn2004pharaoh,
  title={Pharaoh: a beam search decoder for phrase-based statistical machine translation models},
  author={Koehn, Philipp},
  booktitle={Conference of the Association for Machine Translation in the Americas},
  pages={115--124},
  year={2004},
  organization={Springer}
}

@inproceedings{sennrich2015improving,
  title={Improving neural machine translation models with monolingual data},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics},
  pages={86-96},
  year={2015}
}

@article{guzman2019two,
  title={Two new evaluation datasets for low-resource machine translation: Nepali-english and sinhala-english},
  author={Guzm{\'a}n, Francisco and Chen, Peng-Jen and Ott, Myle and Pino, Juan and Lample, Guillaume and Koehn, Philipp and Chaudhary, Vishrav and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:1902.01382},
  year={2019}
}

@inproceedings{moses,
  author={Philipp Koehn and Hieu Hoang and Alexandra Birch and Chris Callison-Burch and Marcello Federico and Nicola Bertoldi and Brooke Cowan and Wade Shen and Christine Moran and Richard Zens and Chris Dyer, Ondrej Bojar and Alexandra Constantin and Evan Herbst},
  title={Moses: Open Source Toolkit for Statistical Machine Translation},
  booktitle={Annual Meeting of the Association for Computational Linguistics (ACL), demo session},
  year={2007},
}

@inproceedings{bleu,
  title={BLEU: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting on association for computational linguistics},
  pages={311--318},
  year={2002},
  organization={Association for Computational Linguistics}
}

@inproceedings{sennrich2015neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics},
  pages = {1715-1725},
  year={2015}
}

@article{gulcehre2015using,
  title={On using monolingual corpora in neural machine translation},
  author={Gulcehre, Caglar and Firat, Orhan and Xu, Kelvin and Cho, Kyunghyun and Barrault, Loic and Lin, Huei-Chi and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1503.03535},
  year={2015}
}

@inproceedings{he2016dual,
  title={Dual learning for machine translation},
  author={He, Di and Xia, Yingce and Qin, Tao and Wang, Liwei and Yu, Nenghai and Liu, Tie-Yan and Ma, Wei-Ying},
  booktitle={Advances in neural information processing systems},
  pages={820--828},
  year={2016}
}

@inproceedings{zheng2017maximum,
  title={Maximum Expected Likelihood Estimation for Zero-resource Neural Machine Translation.},
  author={Zheng, Hao and Cheng, Yong and Liu, Yang},
  booktitle={IJCAI},
  year={2017}
}

%
% neural code geneneration with constraints on the decoding to match a grammar
%

@article{yin2017syntactic,
  title={A syntactic neural model for general-purpose code generation},
  author={Yin, Pengcheng and Neubig, Graham},
  journal={arXiv preprint arXiv:1704.01696},
  year={2017}
} % code generation, use target syntax as prior knowledge

@article{amodio2017neural,
  title={Neural attribute machines for program generation},
  author={Amodio, Matthew and Chaudhuri, Swarat and Reps, Thomas},
  journal={arXiv preprint arXiv:1705.09231},
  year={2017}
} % use constraints on the generator to ensure that generations respect the underlying grammar

@article{rabinovich2017abstract,
  title={Abstract syntax networks for code generation and semantic parsing},
  author={Rabinovich, Maxim and Stern, Mitchell and Klein, Dan},
  journal={arXiv preprint arXiv:1704.07535},
  year={2017}
} % Abstract Syntax Networks, generate AST

@article{alon2019structural,
  title={Structural Language Models for Any-Code Generation},
  author={Alon, Uri and Sadaka, Roy and Levy, Omer and Yahav, Eran},
  journal={arXiv preprint arXiv:1910.00577},
  year={2019}
} % same as above

@article{alon2018code2seq,
  title={code2seq: Generating sequences from structured representations of code},
  author={Alon, Uri and Brody, Shaked and Levy, Omer and Yahav, Eran},
  journal={ICLR},
  year={2019}
} use AST as representation

%
% neural code geneneration
%

@article{chen2017towards,
  title={Towards synthesizing complex programs from input-output examples},
  author={Chen, Xinyun and Liu, Chang and Song, Dawn},
  journal={arXiv preprint arXiv:1706.01284},
  year={2017}
} % generate programs from input-output examples, with a neural approach

@inproceedings{gupta2017deepfix,
  title={Deepfix: Fixing common c language errors by deep learning},
  author={Gupta, Rahul and Pal, Soham and Kanade, Aditya and Shevade, Shirish},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017}
} % fix errors in C code

@article{wang2017dynamic,
  title={Dynamic neural program embedding for program repair},
  author={Wang, Ke and Singh, Rishabh and Su, Zhendong},
  journal={arXiv preprint arXiv:1711.07163},
  year={2017}
} % detect errors in code

@article{chen2019sequencer,
  title={Sequencer: Sequence-to-sequence learning for end-to-end program repair},
  author={Chen, Zimin and Kommrusch, Steve James and Tufano, Michele and Pouchet, Louis-No{\"e}l and Poshyvanyk, Denys and Monperrus, Martin},
  journal={IEEE Transactions on Software Engineering},
  year={2019},
  publisher={IEEE}
} % same as wang2017dynamic

@article{tarlow2019learning,
  title={Learning to Fix Build Errors with Graph2Diff Neural Networks},
  author={Tarlow, Daniel and Moitra, Subhodeep and Rice, Andrew and Chen, Zimin and Manzagol, Pierre-Antoine and Sutton, Charles and Aftandilian, Edward},
  journal={arXiv preprint arXiv:1911.01205},
  year={2019}
} % fix build errors

@inproceedings{allamanis2014learning,
  title={Learning natural coding conventions},
  author={Allamanis, Miltiadis and Barr, Earl T and Bird, Christian and Sutton, Charles},
  booktitle={Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
  pages={281--293},
  year={2014}
} % propose suggestions to improve code

@inproceedings{chen2018tree,
  title={Tree-to-tree neural networks for program translation},
  author={Chen, Xinyun and Liu, Chang and Song, Dawn},
  booktitle={Advances in neural information processing systems},
  pages={2547--2557},
  year={2018}
} % quite relevant. use the dataset of nguyen2013lexical for C#-Java, and create a CoffeeScript-Javascript dataset using an existing transcompiler. will not work on new language pairs. moreover, evaluation metric is bad (perfect match, and token accuracy)

@article{feng2020codebert,
  title={Codebert: A pre-trained model for programming and natural languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},
  journal={arXiv preprint arXiv:2002.08155},
  year={2020}
} % bert pretraining, downstream tasks including NL code search and docstring generation. also evaluate on probing tasks

@article{li2017code,
  title={Code completion with neural attention and pointer networks},
  author={Li, Jian and Wang, Yue and Lyu, Michael R and King, Irwin},
  journal={IJCAI},
  year={2018}
} % same as bhoopchand2016learning

@article{bhoopchand2016learning,
  title={Learning python code suggestion with a sparse pointer network},
  author={Bhoopchand, Avishkar and Rockt{\"a}schel, Tim and Barr, Earl and Riedel, Sebastian},
  journal={arXiv preprint arXiv:1611.08307},
  year={2016}
} % use a sparse pointer network to propose python code suggestions

%
% neural decompilation
%

@inproceedings{katz2018using,
  title={Using recurrent neural networks for decompilation},
  author={Katz, Deborah S and Ruchti, Jason and Schulte, Eric},
  booktitle={2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  pages={346--356},
  year={2018},
  organization={IEEE}
} % predict C code from their binaries, not language specific

@article{katz2019towards,
  title={Towards neural decompilation},
  author={Katz, Omer and Olshaker, Yuval and Goldberg, Yoav and Yahav, Eran},
  journal={arXiv preprint arXiv:1905.08325},
  year={2019}
} % quite complicated approach

@inproceedings{fu2019coda,
  title={Coda: An end-to-end neural program decompiler},
  author={Fu, Cheng and Chen, Huili and Liu, Haolan and Chen, Xinyun and Tian, Yuandong and Koushanfar, Farinaz and Zhao, Jishen},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3703--3714},
  year={2019}
} % tries to address limitations of katz2018, with two sequential phases: code sketch generation and iterative error correction

%
% neural MT for code
%

@article{barone2017parallel,
  title={A parallel corpus of Python functions and documentation strings for automated code documentation and code generation},
  author={Barone, Antonio Valerio Miceli and Sennrich, Rico},
  journal={arXiv preprint arXiv:1707.02275},
  year={2017}
} % release a parallel corpus of python functions with their docstring. only evaluate on BLEU

@inproceedings{hu2018deep,
  title={Deep code comment generation},
  author={Hu, Xing and Li, Ge and Xia, Xin and Lo, David and Jin, Zhi},
  booktitle={Proceedings of the 26th Conference on Program Comprehension},
  pages={200--210},
  year={2018}
} % propose a neural approach, DeepCom, to automatically generate code comments for java methods

@inproceedings{kim2019translating,
  title={Translating CUDA to OpenCL for hardware generation using neural machine translation},
  author={Kim, Yonghae and Kim, Hyesoon},
  booktitle={2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
  pages={285--286},
  year={2019},
  organization={IEEE}
} % only 2 pages, cannot understand what they do

%
% non neural MT for code
%

@inproceedings{oda2015learning,
  title={Learning to generate pseudo-code from source code using statistical machine translation (t)},
  author={Oda, Yusuke and Fudaba, Hiroyuki and Neubig, Graham and Hata, Hideaki and Sakti, Sakriani and Toda, Tomoki and Nakamura, Satoshi},
  booktitle={2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  pages={574--584},
  year={2015},
  organization={IEEE}
} % generate pseudo-code from source code, with SMT (Moses). not very convenient, had to hire programmers to add pseudo code to existing functions. work on tokens or on trees, they show that working with AST works better. probably more gains with NMT

@inproceedings{karaivanov2014phrase,
  title={Phrase-based statistical translation of programming languages},
  author={Karaivanov, Svetoslav and Raychev, Veselin and Vechev, Martin},
  booktitle={Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming \& Software},
  pages={173--184},
  year={2014}
} % use a small training set of Java-C# functions, show that it works using SMT. very easy direction. created a tool to mine parallel functions, hard to generalize

@inproceedings{nguyen2013lexical,
  title={Lexical statistical machine translation for language migration},
  author={Nguyen, Anh Tuan and Nguyen, Tung Thanh and Nguyen, Tien N},
  booktitle={Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
  pages={651--654},
  year={2013}
} % does something similar to karaivanov2014phrase, by extracting parallel data from two java projects ported to C# (db4o and Lucene)

@techreport{aggarwal2015using,
  title={Using machine translation for converting Python 2 to Python 3 code},
  author={Aggarwal, Karan and Salameh, Mohammad and Hindle, Abram},
  year={2015},
  institution={PeerJ PrePrints}
} % create a dataset python 2 to python 3 using the 2to3 library (Peterson et al., 2014) (very small difference between these two languages)

%
% baseline
%

@article{cocojpt,
  title={JPT: ASimple JAVA-PYTHON TRANSLATOR},
  author={Coco, Eman J and Osman, Hadeel A and Osman, Niemah I},
  journal={Computer Applications: An International Journal},
  year={2018}
}

%
% other
%

@article{maaten2008visualizing,
  title={Visualizing data using t-SNE},
  author={Maaten, Laurens van der and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={Nov},
  pages={2579--2605},
  year={2008}
}

@inproceedings{hindle2012naturalness,
  title={On the naturalness of software},
  author={Hindle, Abram and Barr, Earl T and Su, Zhendong and Gabel, Mark and Devanbu, Premkumar},
  booktitle={2012 34th International Conference on Software Engineering (ICSE)},
  pages={837--847},
  year={2012},
  organization={IEEE}
}

@article{ling2016latent,
  title={Latent predictor networks for code generation},
  author={Ling, Wang and Grefenstette, Edward and Hermann, Karl Moritz and Ko{\v{c}}isk{\`y}, Tom{\'a}{\v{s}} and Senior, Andrew and Wang, Fumin and Blunsom, Phil},
  journal={arXiv preprint arXiv:1603.06744},
  year={2016}
}

@article{brockschmidt2018generative,
  title={Generative code modeling with graphs},
  author={Brockschmidt, Marc and Allamanis, Miltiadis and Gaunt, Alexander L and Polozov, Oleksandr},
  journal={ICLR},
  year={2019}
}

@article{chen2018execution,
  title={Execution-guided neural program synthesis},
  author={Chen, Xinyun and Liu, Chang and Song, Dawn},
  year={2018}
}

@inproceedings{allamanis2016convolutional,
  title={A convolutional attention network for extreme summarization of source code},
  author={Allamanis, Miltiadis and Peng, Hao and Sutton, Charles},
  booktitle={International conference on machine learning},
  pages={2091--2100},
  year={2016}
}
