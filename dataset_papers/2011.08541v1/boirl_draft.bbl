\begin{thebibliography}{10}

\bibitem{abbeel2004apprenticeship}
P.~Abbeel and A.~Y. Ng.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In {\em Proc. {ICML}}, 2004.

\bibitem{amin2017repeated}
K.~Amin, N.~Jiang, and S.~Singh.
\newblock Repeated inverse reinforcement learning.
\newblock In {\em Proc. {NeurIPS}}, pages 1815--1824, 2017.

\bibitem{amin2016towards}
K.~Amin and S.~Singh.
\newblock Towards resolving unidentifiability in inverse reinforcement
  learning.
\newblock {arXiv}:1601.06569, 2016.

\bibitem{bogert2016expectation}
K.~Bogert, J.~F.-S. Lin, P.~Doshi, and D.~Kulic.
\newblock Expectation-maximization for inverse reinforcement learning with
  hidden data.
\newblock In {\em Proc. {AAMAS}}, pages 1034--1042, 2016.

\bibitem{boularias2012structured}
A.~Boularias, O.~Kr{\"o}mer, and J.~Peters.
\newblock Structured apprenticeship learning.
\newblock In {\em Proc. {ECML/PKDD}}, pages 227--242, 2012.

\bibitem{brochu2010bayesian}
E.~Brochu, T.~Brochu, and N.~de~Freitas.
\newblock A {B}ayesian interactive optimization approach to procedural
  animation design.
\newblock In {\em Proc. {SCA}}, pages 103--112, 2010.

\bibitem{openaigym}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba.
\newblock {OpenAI Gym}.
\newblock {arXiv}:1606.01540, 2016.

\bibitem{brown2019deep}
D.~S. Brown and S.~Niekum.
\newblock Deep {Bayesian} reward learning from preferences.
\newblock {arXiv}:1912.04472, 2019.

\bibitem{brown2019machine}
D.~S. Brown and S.~Niekum.
\newblock Machine teaching for inverse reinforcement learning: Algorithms and
  applications.
\newblock In {\em Proc. {AAAI}}, pages 7749--7758, 2019.

\bibitem{dai20}
Z.~Dai, B.~K.~H. Low, and P.~Jaillet.
\newblock Federated {Bayesian} optimization via {Thompson} sampling.
\newblock In {\em Proc. {NeurIPS}}, 2020.

\bibitem{daxberger2017distributed}
E.~A. Daxberger and B.~K.~H. Low.
\newblock Distributed batch {Gaussian} process optimization.
\newblock In {\em Proc. {ICML}}, pages 951--960, 2017.

\bibitem{finn2016guided}
C.~Finn, S.~Levine, and P.~Abbeel.
\newblock Guided cost learning: Deep inverse optimal control via policy
  optimization.
\newblock In {\em Proc. {ICML}}, pages 49--58, 2016.

\bibitem{fosgerau2013link}
M.~Fosgerau, E.~Frejinger, and A.~Karlstrom.
\newblock A link based network route choice model with unrestricted choice set.
\newblock {\em Transportation Research Part B: Methodological}, 56:70--80,
  2013.

\bibitem{fu2017learning}
J.~Fu, K.~Luo, and S.~Levine.
\newblock Learning robust rewards with adversarial inverse reinforcement
  learning.
\newblock {arXiv}:1710.11248, 2017.

\bibitem{hadfield2016cooperative}
D.~Hadfield-Menell, S.~J. Russell, P.~Abbeel, and A.~Dragan.
\newblock Cooperative inverse reinforcement learning.
\newblock In {\em Proc. {NeurIPS}}, pages 3909--3917, 2016.

\bibitem{ho2016generative}
J.~Ho and S.~Ermon.
\newblock Generative adversarial imitation learning.
\newblock In {\em Proc. {NeurIPS}}, pages 4565--4573, 2016.

\bibitem{NghiaAAAI18}
T.~N. Hoang, Q.~M. Hoang, and B.~K.~H. Low.
\newblock Decentralized high-dimensional {Bayesian} optimization with factor
  graphs.
\newblock In {\em Proc. {AAAI}}, pages 3231--3238, 2018.

\bibitem{dmitrii20a}
D.~Kharkovskii, C.~K. Ling, and B.~K.~H. Low.
\newblock Nonmyopic {Gaussian} process optimization with macro-actions.
\newblock In {\em Proc. AISTATS}, pages 4593--4604, 2020.

\bibitem{Kok2020}
B.~C. Kok and H.~Soh.
\newblock Trust in robots: Challenges and opportunities.
\newblock {\em Current Robotics Reports}, 1(4):1--13, 2020.

\bibitem{levine2011nonlinear}
S.~Levine, Z.~Popovic, and V.~Koltun.
\newblock Nonlinear inverse reinforcement learning with {G}aussian processes.
\newblock In {\em Proc. {NeurIPS}}, pages 19--27, 2011.

\bibitem{ling16}
C.~K. Ling, K.~H. Low, and P.~Jaillet.
\newblock {Gaussian} process planning with {Lipschitz} continuous reward
  functions: Towards unifying {Bayesian} optimization, active learning, and
  beyond.
\newblock In {\em Proc. {AAAI}}, pages 1860--1866, 2016.

\bibitem{lizotte08}
D.~J. Lizotte.
\newblock {\em Practical {B}ayesian Optimization}.
\newblock PhD thesis, University of Alberta, 2008.

\bibitem{lopes2009active}
M.~Lopes, F.~Melo, and L.~Montesano.
\newblock Active learning for reward estimation in inverse reinforcement
  learning.
\newblock In {\em Proc. {ECML/PKDD}}, pages 31--46, 2009.

\bibitem{mannion2017policy}
P.~Mannion, S.~Devlin, K.~Mason, J.~Duggan, and E.~Howley.
\newblock Policy invariance under reward transformations for multi-objective
  reinforcement learning.
\newblock {\em Neurocomputing}, 263:60--73, 2017.

\bibitem{NIPS2017_6800}
A.~M. Metelli, M.~Pirotta, and M.~Restelli.
\newblock Compatible reward inverse reinforcement learning.
\newblock In {\em Proc. {NeurIPS}}, pages 2050--2059, 2017.

\bibitem{mockus1978application}
J.~Mockus, V.~Tiesis, and A.~Zilinskas.
\newblock The application of {B}ayesian methods for seeking the extremum.
\newblock In L.~C.~W. Dixon and G.~P. Szeg{\"{o}}, editors, {\em Towards Global
  Optimization 2}, pages 117--129. North-Holland Publishing Company, 1978.

\bibitem{neu2012apprenticeship}
G.~Neu and C.~Szepesv{\'a}ri.
\newblock Apprenticeship learning using inverse reinforcement learning and
  gradient methods.
\newblock {arXiv}:1206.5264, 2012.

\bibitem{ng1999policy}
A.~Y. Ng, D.~Harada, and S.~Russell.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In {\em Proc. {ICML}}, pages 278--287, 1999.

\bibitem{nguyen2015inverse}
Q.~P. Nguyen, B.~K.~H. Low, and P.~Jaillet.
\newblock Inverse reinforcement learning with locally consistent reward
  functions.
\newblock In {\em Proc. {NeurIPS}}, pages 1747--1755, 2015.

\bibitem{osborne2009gaussian}
M.~A. Osborne, R.~Garnett, and S.~J. Roberts.
\newblock Gaussian processes for global optimization.
\newblock In {\em Proc. {LION3}}, pages 1--15, 2009.

\bibitem{roboticsenv-2018}
M.~Plappert, M.~Andrychowicz, A.~Ray, B.~McGrew, B.~Baker, G.~Powell,
  J.~Schneider, J.~Tobin, M.~Chociej, P.~Welinder, V.~Kumar, and W.~Zaremba.
\newblock Multi-goal reinforcement learning: Challenging robotics environments
  and request for research.
\newblock {arXiv}:1206.5264, 2018.

\bibitem{ramachandran2007bayesian}
D.~Ramachandran and E.~Amir.
\newblock Bayesian inverse reinforcement learning.
\newblock In {\em Proc. {IJCAI}}, pages 2586--2591, 2007.

\bibitem{rana2017}
S.~Rana, C.~Li, S.~Gupta, V.~Nguyen, and S.~Venkatesh.
\newblock High dimensional {B}ayesian optimization with elastic {G}aussian
  process.
\newblock In {\em Proc. {ICML}}, pages 2883--2891, 2017.

\bibitem{schulman2017proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {arXiv}:1707.06347, 2017.

\bibitem{shahriari2015taking}
B.~Shahriari, K.~Swersky, Z.~Wang, R.~P. Adams, and N.~De~Freitas.
\newblock Taking the human out of the loop: A review of {B}ayesian
  optimization.
\newblock {\em Proceedings of the IEEE}, 104(1):148--175, 2015.

\bibitem{williams2006gaussian}
C.~K. Williams and C.~E. Rasmussen.
\newblock {\em {Gaussian Processes} for {Machine Learning}}.
\newblock MIT Press, 2006.

\bibitem{wulfmeier2015maximum}
M.~Wulfmeier, P.~Ondruska, and I.~Posner.
\newblock Maximum entropy deep inverse reinforcement learning.
\newblock {arXiv}:1507.04888, 2015.

\bibitem{ziebart2008maximum}
B.~D. Ziebart, A.~L. Maas, J.~A. Bagnell, and A.~K. Dey.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In {\em Proc. {AAAI}}, pages 1433--1438, 2008.

\end{thebibliography}
