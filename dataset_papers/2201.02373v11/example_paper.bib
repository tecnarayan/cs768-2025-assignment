@inproceedings{sutton:nips12,
  added-at = {2008-03-11T14:52:34.000+0100},
  author = {Sutton, R. S. and Mcallester, D. and Singh, S. and Mansour, Y.},
  biburl = {https://www.bibsonomy.org/bibtex/29b842a7da658738b1390cf208b0c9c42/idsia},
  booktitle = {Advances in Neural Information Processing Systems 12},
  citeulike-article-id = {2380307},
  interhash = {4fec5717a76a52dd85fc6876c407a36f},
  intrahash = {9b842a7da658738b1390cf208b0c9c42},
  keywords = {inaki},
  pages = {1057--1063},
  priority = {2},
  publisher = {MIT Press},
  timestamp = {2008-03-11T14:56:16.000+0100},
  title = {Policy gradient methods for reinforcement learning with function approximation},
  volume = 12,
  year = 2000
}
@article{li2020multi,
  title={Multi-Agent Trust Region Policy Optimization},
  author={Li, Hepeng and He, Haibo},
  journal={arXiv e-prints},
  pages={arXiv--2010},
  year={2020}
}
@inproceedings{zhang2020bi,
  title={Bi-level Actor-Critic for Multi-agent Coordination},
  author={Zhang, Haifeng and Chen, Weizhe and Huang, Zeren and Li, Minne and Yang, Yaodong and Zhang, Weinan and Wang, Jun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={7325--7332},
  year={2020}
}
@inproceedings{mahmood2018benchmarking,
  title={Benchmarking reinforcement learning algorithms on real-world robots},
  author={Mahmood, A Rupam and Korenkevych, Dmytro and Vasan, Gautham and Ma, William and Bergstra, James},
  booktitle={Conference on robot learning},
  pages={561--591},
  year={2018},
  organization={PMLR}
}
@article{zhou2021malib,
  title={MALib: A Parallel Framework for Population-based Multi-agent Reinforcement Learning},
  author={Zhou, Ming and Wan, Ziyu and Wang, Hanjing and Wen, Muning and Wu, Runzhe and Wen, Ying and Yang, Yaodong and Zhang, Weinan and Wang, Jun},
  journal={arXiv preprint arXiv:2106.07551},
  year={2021}
}

@inproceedings{yang2018mean,
  title={Mean field multi-agent reinforcement learning},
  author={Yang, Yaodong and Luo, Rui and Li, Minne and Zhou, Ming and Zhang, Weinan and Wang, Jun},
  booktitle={International Conference on Machine Learning},
  pages={5571--5580},
  year={2018},
  organization={PMLR}
}

@InProceedings{spg-david,
  title = 	 {Learning in Nonzero-Sum Stochastic Games with Potentials},
  author =       {Mguni, David H and Wu, Yutong and Du, Yali and Yang, Yaodong and Wang, Ziyi and Li, Minne and Wen, Ying and Jennings, Joel and Wang, Jun},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {7688--7699},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
}


@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}


@article{tomar2020mirror,
  title={Mirror descent policy optimization},
  author={Tomar, Manan and Shani, Lior and Efroni, Yonathan and Ghavamzadeh, Mohammad},
  journal={arXiv preprint arXiv:2005.09814},
  year={2020}
}

@inproceedings{fan2020theoretical,
  title={A theoretical analysis of deep Q-learning},
  author={Fan, Jianqing and Wang, Zhaoran and Xie, Yuchen and Yang, Zhuoran},
  booktitle={Learning for Dynamics and Control},
  pages={486--489},
  year={2020},
  organization={PMLR}
}



@article{berner2019dota,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Debiak, Przemyslaw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}




@inproceedings{wen2018probabilistic,
  title={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},
  author={Wen, Ying and Yang, Yaodong and Luo, Rui and Wang, Jun and Pan, Wei},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{gr2,
  title     = {Modelling Bounded Rationality in Multi-Agent Interactions by Generalized Recursive Reasoning},
  author    = {Wen, Ying and Yang, Yaodong and Wang, Jun},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Christian Bessiere},
  pages     = {414--421},
  year      = {2020},
  month     = {7},
  note      = {Main track}
}

@inproceedings{yang2020multi,
  title={Multi-agent determinantal q-learning},
  author={Yang, Yaodong and Wen, Ying and Wang, Jun and Chen, Liheng and Shao, Kun and Mguni, David and Zhang, Weinan},
  booktitle={International Conference on Machine Learning},
  pages={10757--10766},
  year={2020},
  organization={PMLR}
}

@article{peng1703multiagent,
  title={Multiagent bidirectionally-coordinated nets for learning to play starcraft combat games. arXiv 2017},
year={2017},
  author={Peng, P and Yuan, Q and Wen, Y and Yang, Y and Tang, Z and Long, H and Wang, J},
  journal={arXiv preprint arXiv:1703.10069}
}


@article{kuba2021settling,
  title={Settling the Variance of Multi-Agent Policy Gradients},
  author={Kuba, Jakub Grudzien and Wen, Muning and Yang, Yaodong and Meng, Linghui and Gu, Shangding and Zhang, Haifeng and Mguni, David Henry and Wang, Jun},
  journal={arXiv preprint arXiv:2108.08612},
  year={2021}
}

@article{kakade2001natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}
@article{yang2020overview,
  title={An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective},
  author={Yang, Yaodong and Wang, Jun},
  journal={arXiv preprint arXiv:2011.00583},
  year={2020}
}
@article{nash1951non,
  title={Non-cooperative games},
  author={Nash, John},
  journal={Annals of mathematics},
  pages={286--295},
  year={1951},
  publisher={JSTOR}
}
@inproceedings{duan2016benchmarking,
  title={Benchmarking deep reinforcement learning for continuous control},
  author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={1329--1338},
  year={2016},
  organization={PMLR}
}

@article{MeanAC,
  title={Mean actor critic},
  author={Asadi, Kavosh and Allen, Cameron and Roderick, Melrose and Mohamed, Abdel-rahman and Konidaris, George and Littman, Michael and Amazon, Brown University},
  journal={stat},
  volume={1050},
  pages={1},
  year={2017}
}

@inproceedings{tucker2018mirage,
  title={The mirage of action-dependent baselines in reinforcement learning},
  author={Tucker, George and Bhupatiraju, Surya and Gu, Shixiang and Turner, Richard and Ghahramani, Zoubin and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={5015--5024},
  year={2018},
  organization={PMLR}
}

@article{papoudakiscomparative,
  title={Comparative Evaluation of Cooperative Multi-Agent Deep Reinforcement Learning Algorithms},
  author={Papoudakis, Georgios and Christianos, Filippos and Sch{\"a}fer, Lukas and Albrecht, Stefano V}
}



@article{claus1998dynamics,
  title={The dynamics of reinforcement learning in cooperative multiagent systems},
  author={Claus, Caroline and Boutilier, Craig},
  journal={AAAI/IAAI},
  volume={1998},
  number={746-752},
  pages={2},
  year={1998}
}


@inproceedings{marl,
  title={Fully decentralized multi-agent reinforcement learning with networked agents},
  author={Zhang, Kaiqing and Yang, Zhuoran and Liu, Han and Zhang, Tong and Basar, Tamer},
  booktitle={International Conference on Machine Learning},
  pages={5872--5881},
  year={2018},
  organization={PMLR}
}





@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}




@inproceedings{lawrence2002efficient,
  title={Efficient gradient estimation for motor control learning},
  author={Lawrence, Gregory and Cowan, Noah and Russell, Stuart},
  booktitle={Proceedings of the Nineteenth conference on Uncertainty in Artificial Intelligence},
  pages={354--361},
  year={2002}
}




@inproceedings{ctde,
  title={Contrasting Centralized and Decentralized Critics in Multi-Agent Reinforcement Learning},
  author={Lyu, Xueguang and Xiao, Yuchen and Daley, Brett and Amato, Christopher},
  booktitle={Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages={844--852},
  year={2021}
}





@article{mappo,
  title={The Surprising Effectiveness of MAPPO in Cooperative, Multi-Agent Games},
  author={Chao Yu and A. Velu and Eugene Vinitsky and Yu Wang and A. Bayen and Yi Wu},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.01955}
}



@article{ppo,
  title={Proximal Policy Optimization Algorithms},
  author={John Schulman and F. Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  journal={ArXiv},
  year={2017},
  volume={abs/1707.06347}
}

@article{gemp2020eigengame,
  title={Eigengame: Pca as a nash equilibrium},
  author={Gemp, Ian and McWilliams, Brian and Vernade, Claire and Graepel, Thore},
  journal={arXiv preprint arXiv:2010.00554},
  year={2020}
}

@incollection{littman1994markov,
  title={Markov games as a framework for multi-agent reinforcement learning},
  author={Littman, Michael L},
  booktitle={Machine learning proceedings 1994},
  pages={157--163},
  year={1994},
  publisher={Elsevier}
}

@inproceedings{trpo,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}



@inproceedings{sugiyama,
  title={Analysis and Improvement of Policy Gradient Estimation.},
  author={Zhao, Tingting and Hachiya, Hirotaka and Niu, Gang and Sugiyama, Masashi},
  booktitle={NIPS},
  pages={262--270},
  year={2011},
  organization={Citeseer}
}


@inproceedings{grathwohl2018backpropagation,
  title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},
  author={Grathwohl, Will and Choi, Dami and Wu, Yuhuai and Roeder, Geoff and Duvenaud, David},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{wen2019,
  title={Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning},
  author={Wen, Ying and Yang, Yaodong and Luo, Rui and Wang, Jun and Pan, Wei},
  booktitle={International Conference on Learning Representations},
  year={2018}
}



@article{ciosek2020,
title = {Expected Policy Gradients for Reinforcement Learning},
author= {Kamil Ciosek, Shimon Whiteson},
journal={Journal of Machine Learning Research},
volume={21},
pages={1-51},
year={2020}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@inproceedings{glynn1987likelilood,
  title={Likelilood ratio gradient estimation: an overview},
  author={Glynn, Peter W},
  booktitle={Proceedings of the 19th conference on Winter simulation},
  pages={366--375},
  year={1987}
}

@incollection{dayan1991reinforcement,
  title={Reinforcement comparison},
  author={Dayan, Peter},
  booktitle={Connectionist Models},
  pages={45--51},
  year={1991},
  publisher={Elsevier}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@article{chung2020beyond,
  title={Beyond variance reduction: Understanding the true impact of baselines on policy optimization},
  author={Chung, Wesley and Thomas, Valentin and Machado, Marlos C and Roux, Nicolas Le},
  journal={arXiv preprint arXiv:2008.13773},
  year={2020}
}
@article{schroeder2020independent,
  title={Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?},
  author={Schroeder de Witt, Christian and Gupta, Tarun and Makoviichuk, Denys and Makoviychuk, Viktor and Torr, Philip HS and Sun, Mingfei and Whiteson, Shimon},
  journal={arXiv e-prints},
  pages={arXiv--2011},
  year={2020}
}


@article{hsu2020revisiting,
  title={Revisiting design choices in proximal policy optimization},
  author={Hsu, Chloe Ching-Yun and Mendler-D{\"u}nner, Celestine and Hardt, Moritz},
  journal={arXiv preprint arXiv:2009.10897},
  year={2020}
}

@article{greensmith2004variance,
  title={Variance reduction techniques for gradient estimates in reinforcement learning},
  author={Greensmith, Evan and Bartlett, Peter L and Baxter, Jonathan},
  journal={Journal of Machine Learning Research},
  volume={5},
  number={Nov},
  pages={1471--1530},
  year={2004}
}
@article{baxter2001infinite,
  title={Infinite-horizon policy-gradient estimation},
  author={Baxter, Jonathan and Bartlett, Peter L},
  journal={Journal of Artificial Intelligence Research},
  volume={15},
  pages={319--350},
  year={2001}
}
@inproceedings{weaver2001optimal,
  title={The optimal reward baseline for gradient-based reinforcement learning},
  author={Weaver, Lex and Tao, Nigel},
  booktitle={Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence},
  pages={538--545},
  year={2001}
}



@article{wang2019neural,
  title={Neural policy gradient methods: Global optimality and rates of convergence},
  author={Wang, Lingxiao and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1909.01150},
  year={2019}
}





@article{liu2019neural,
  title={Neural proximal/trust region policy optimization attains globally optimal policy},
  author={Liu, Boyi and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1906.10306},
  year={2019}
}


@article{nemirovskij1983problem,
  title={Problem complexity and method efficiency in optimization},
  author={Nemirovskij, Arkadij Semenovi{\v{c}} and Yudin, David Borisovich},
  year={1983},
  publisher={Wiley-Interscience}
}


@inproceedings{shani2020adaptive,
  title={Adaptive trust region policy optimization: Global convergence and faster rates for regularized mdps},
  author={Shani, Lior and Efroni, Yonathan and Mannor, Shie},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5668--5675},
  year={2020}
}


@article{lan2021policy,
  title={Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes},
  author={Lan, Guanghui},
  journal={arXiv preprint arXiv:2102.00135},
  year={2021}
}


@article{zhan2021policy,
  title={Policy mirror descent for regularized reinforcement learning: A generalized framework with linear convergence},
  author={Zhan, Wenhao and Cen, Shicong and Huang, Baihe and Chen, Yuxin and Lee, Jason D and Chi, Yuejie},
  journal={arXiv preprint arXiv:2105.11066},
  year={2021}
}

@inproceedings{chu2019probability,
  title={Probability functional descent: A unifying perspective on GANs, variational inference, and reinforcement learning},
  author={Chu, Casey and Blanchet, Jose and Glynn, Peter},
  booktitle={International Conference on Machine Learning},
  pages={1213--1222},
  year={2019},
  organization={PMLR}
}


@article{bellman1957markov,
  title={A markov decision process. journal of Mathematical Mechanics},
  author={Bellman, RE},
  year={1957}
}

@inproceedings{wang2020truly,
  title={Truly proximal policy optimization},
  author={Wang, Yuhui and He, Hao and Tan, Xiaoyang},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={113--122},
  year={2020},
  organization={PMLR}
}


@article{vaswani2021functional,
  title={A functional mirror ascent view of policy gradient methods with function approximation},
  author={Vaswani, Sharan and Bachem, Olivier and Totaro, Simone and Mueller, Robert and Geist, Matthieu and Machado, Marlos C and Castro, Pablo Samuel and Roux, Nicolas Le},
  journal={arXiv preprint arXiv:2108.05828},
  year={2021}
}

@article{abdolmaleki2018maximum,
  title={Maximum a posteriori policy optimisation},
  author={Abdolmaleki, Abbas and Springenberg, Jost Tobias and Tassa, Yuval and Munos, Remi and Heess, Nicolas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1806.06920},
  year={2018}
}


@incollection{fukushima1982neocognitron,
  title={Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition},
  author={Fukushima, Kunihiko and Miyake, Sei},
  booktitle={Competition and cooperation in neural nets},
  pages={267--285},
  year={1982},
  publisher={Springer}
}

@article{beck2003mirror,
  title={Mirror descent and nonlinear projected subgradient methods for convex optimization},
  author={Beck, Amir and Teboulle, Marc},
  journal={Operations Research Letters},
  volume={31},
  number={3},
  pages={167--175},
  year={2003},
  publisher={Elsevier}
}

@article{neu2017unified,
  author    = {Gergely Neu and
               Anders Jonsson and
               Vicen{\c{c}} G{\'{o}}mez},
  title     = {A unified view of entropy-regularized Markov decision processes},
  journal   = {CoRR},
  volume    = {abs/1705.07798},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.07798},
  eprinttype = {arXiv},
  eprint    = {1705.07798},
  timestamp = {Mon, 13 Aug 2018 16:47:28 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/NeuJG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Watkins1992,
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
  added-at = {2020-01-01T20:16:30.000+0100},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/2416ac9f845c6ccea5a7eacee4dedead8/lanteunis},
  day = 01,
  doi = {10.1007/BF00992698},
  interhash = {a4436f9e14335d677f156049cb798253},
  intrahash = {416ac9f845c6ccea5a7eacee4dedead8},
  issn = {1573-0565},
  journal = {Machine Learning},
  keywords = {DRLAlgoComparison q-learning reinforcement_learning},
  month = may,
  number = 3,
  pages = {279--292},
  timestamp = {2020-01-01T20:16:30.000+0100},
  title = {Q-learning},
  url = {https://doi.org/10.1007/BF00992698},
  volume = 8,
  year = 1992
}





@article{wang2016sample,
  title={Sample efficient actor-critic with experience replay},
  author={Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
  journal={arXiv preprint arXiv:1611.01224},
  year={2016}
}

@article{sehnke2010parameter,
  title={Parameter-exploring policy gradients},
  author={Sehnke, Frank and Osendorfer, Christian and R{\"u}ckstie{\ss}, Thomas and Graves, Alex and Peters, Jan and Schmidhuber, J{\"u}rgen},
  journal={Neural Networks},
  volume={23},
  number={4},
  pages={551--559},
  year={2010},
  publisher={Elsevier}
}



@article{daskalakis2009complexity,
  title={The complexity of computing a Nash equilibrium},
  author={Daskalakis, Constantinos and Goldberg, Paul W and Papadimitriou, Christos H},
  journal={SIAM Journal on Computing},
  volume={39},
  number={1},
  pages={195--259},
  year={2009},
  publisher={SIAM}
}


@inproceedings{foerster2018counterfactual,
  title={Counterfactual multi-agent policy gradients},
  author={Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{peters2006policy,
  title={Policy gradient methods for robotics},
  author={Peters, Jan and Schaal, Stefan},
  booktitle={2006 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={2219--2225},
  year={2006},
  organization={IEEE}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}


@article{brownEssentials,
  title={Essentials of game theory: A concise multidisciplinary introduction},
  author={Leyton-Brown, Kevin and Shoham, Yoav},
  journal={Synthesis lectures on artificial intelligence and machine learning},
  volume={2},
  number={1},
  pages={1--88},
  year={2008},
  publisher={Morgan \& Claypool Publishers}
}



@article{samvelyanstarcraft,
  title={The starcraft multi-agent challenge},
  author={Samvelyan, Mikayel and Rashid, Tabish and De Witt, Christian Schroeder and Farquhar, Gregory and Nardelli, Nantas and Rudner, Tim GJ and Hung, Chia-Man and Torr, Philip HS and Foerster, Jakob and Whiteson, Shimon},
  journal={arXiv preprint arXiv:1902.04043},
  year={2019}
}

@article{de2020deep,
  publtype={informal},
  author={Christian Schröder de Witt and Bei Peng and Pierre-Alexandre Kamienny and Philip H. S. Torr and Wendelin Böhmer and Shimon Whiteson},
  title={Deep Multi-Agent Reinforcement Learning for Decentralized Continuous Cooperative Control},
  year={2020},
  cdate={1577836800000},
  journal={CoRR},
  volume={abs/2003.06709}
}




@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={In Proc. 19th International Conference on Machine Learning},
  year={2002},
  organization={Citeseer}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{lowe2017multi,
  title={Multi-agent actor-critic for mixed cooperative-competitive environments},
  author={Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={6382--6393},
  year={2017}
}

@inproceedings{son2019qtran,
  title={Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning},
  author={Son, Kyunghwan and Kim, Daewoo and Kang, Wan Ju and Hostallero, David Earl and Yi, Yung},
  booktitle={International Conference on Machine Learning},
  pages={5887--5896},
  year={2019},
  organization={PMLR}
}

@inproceedings{rashid2018qmix,
  title={Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning},
  author={Rashid, Tabish and Samvelyan, Mikayel and Schroeder, Christian and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={4295--4304},
  year={2018},
  organization={PMLR}
}

@inproceedings{sunehag2018value,
  title={Value-Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward},
  author={Sunehag, Peter and Lever, Guy and Gruslys, Audrunas and Czarnecki, Wojciech Marian and Zambaldi, Vinicius and Jaderberg, Max and Lanctot, Marc and Sonnerat, Nicolas and Leibo, Joel Z and Tuyls, Karl and others},
  booktitle={Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
  pages={2085--2087},
  year={2018}
}

@article{schulman2015high,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1506.02438},
  year={2015}
}
@inproceedings{maddpg,
  title={Multi-agent actor-critic for mixed cooperative-competitive environments},
  author={Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={6382--6393},
  year={2017}
}

@inproceedings{konda2000actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay R and Tsitsiklis, John N},
  booktitle={Advances in neural information processing systems},
  pages={1008--1014},
  year={2000},
  organization={Citeseer}
}
@inproceedings{sac,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}





@article{wen2021game,
  title={A Game-Theoretic Approach to Multi-Agent Trust Region Optimization},
  author={Wen, Ying and Chen, Hui and Yang, Yaodong and Tian, Zheng and Li, Minne and Chen, Xu and Wang, Jun},
  journal={arXiv preprint arXiv:2106.06828},
  year={2021}
}


@article{bertsekas2019multiagent,
  title={Multiagent rollout algorithms and reinforcement learning},
  author={Bertsekas, Dimitri},
  journal={arXiv preprint arXiv:1910.00120},
  year={2019}
}


@article{wang2020rode,
  title={RODE: Learning Roles to Decompose Multi-Agent Tasks},
  author={Wang, Tonghan and Gupta, Tarun and Mahajan, Anuj and Peng, Bei and Whiteson, Shimon and Zhang, Chongjie},
  journal={International Conference on LearningRepresentations},
  year={2021}
}

@article{de2020independent,
  title={Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?},
  author={de Witt, Christian Schroeder and Gupta, Tarun and Makoviichuk, Denys and Makoviychuk, Viktor and Torr, Philip HS and Sun, Mingfei and Whiteson, Shimon},
  journal={arXiv preprint arXiv:2011.09533},
  year={2020}
}



@inproceedings{gu2017q,
  title={Q-PrOP: Sample-efficient policy gradient with an off-policy critic},
  author={Gu, S and Lillicrap, T and Ghahramani, Z and Turner, RE and Levine, S},
  booktitle={5th International Conference on Learning Representations, ICLR 2017-Conference Track Proceedings},
  year={2017}
}



@inproceedings{kimura1997reinforcement,
  title={Reinforcement Learning in POMDPs with Function Approximation},
  author={Kimura, Hajime and Miyazaki, Kazuteru and Kobayashi, Shigenobu},
  booktitle={Proceedings of the Fourteenth International Conference on Machine Learning},
  pages={152--160},
  year={1997}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018}
}

@article{wang2020off,
  title={Off-Policy Multi-Agent Decomposed Policy Gradients},
  author={Wang, Yihan and Han, Beining and Wang, Tonghan and Dong, Heng and Zhang, Chongjie},
  journal={arXiv preprint arXiv:2007.12322},
  year={2020}
}


@article{paszke2019pytorch,
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={8026--8037},
  year={2019}
}

@article{hu2021noisy,
  title={Noisy-MAPPO: Noisy Advantage Values for Cooperative Multi-agent Actor-Critic methods},
  author={Hu, Siyue and Hu, Jian},
  journal={arXiv preprint arXiv:2106.14334},
  year={2021}
}



@inproceedings{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={International conference on machine learning},
  pages={387--395},
  year={2014},
  organization={PMLR}
}


@article{wang2016learning,
  title={Learning to reinforcement learn},
  author={Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
  journal={arXiv preprint arXiv:1611.05763},
  year={2016}
}


@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016},
  organization={PMLR}
}

@article{SpinningUp2018,
    author = {Achiam, Joshua},
    title = {{Spinning Up in Deep Reinforcement Learning}},
    year = {2018}
}

@article{peng2020facmac,
  title={FACMAC: Factored Multi-Agent Centralised Policy Gradients},
  author={Peng, Bei and Rashid, Tabish and Schroeder de Witt, Christian A and Kamienny, Pierre-Alexandre and Torr, Philip HS and B{\"o}hmer, Wendelin and Whiteson, Shimon},
  journal={arXiv e-prints},
  pages={arXiv--2003},
  year={2020}
}

@article{weng2021tianshou,
  title={Tianshou: a Highly Modularized Deep Reinforcement Learning Library},
  author={Weng, Jiayi and Chen, Huayu and Yan, Dong and You, Kaichao and Duburcq, Alexis and Zhang, Minghao and Su, Hang and Zhu, Jun},
  journal={arXiv preprint arXiv:2107.14171},
  year={2021}
}

@article{kuba2021trust,
  title={Trust region policy optimisation in multi-agent reinforcement learning},
  author={Kuba, Jakub Grudzien and Chen, Ruiqing and Wen, Munning and Wen, Ying and Sun, Fanglei and Wang, Jun and Yang, Yaodong},
  journal={arXiv preprint arXiv:2109.11251},
  year={2021}
}

@article{ausubel1993generalized,
  title={A generalized theorem of the maximum},
  author={Ausubel, Lawrence M and Deneckere, Raymond J},
  journal={Economic Theory},
  volume={3},
  number={1},
  pages={99--107},
  year={1993},
  publisher={Springer}
}

@article{gateaux1922diverses,
  title={Sur diverses questions de calcul fonctionnel},
  author={Gateaux, Ren{\'e}},
  journal={Bulletin de la Soci{\'e}t{\'e} Math{\'e}matique de France},
  volume={50},
  pages={1--37},
  year={1922}
}


@inproceedings{henderson2018deep,
  title={Deep reinforcement learning that matters},
  author={Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{arjona2018rudder,
  title={Rudder: Return decomposition for delayed rewards},
  author={Arjona-Medina, Jose A and Gillhofer, Michael and Widrich, Michael and Unterthiner, Thomas and Brandstetter, Johannes and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:1806.07857},
  year={2018}
}

@inproceedings{engstrom2020implementation,
  title={Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO},
  author={Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander},
  booktitle={International Conference on Learning Representations},
  year={2020}
}


@article{queeney2021generalized,
  title={Generalized Proximal Policy Optimization with Sample Reuse},
  author={Queeney, James and Paschalidis, Ioannis and Cassandras, Christos},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@inproceedings{van2016deep,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={30},
  number={1},
  year={2016}
}

@article{hamilton1982global,
  title={Global and local variational derivatives and integral representations of G{\^a}teaux differentials},
  author={Hamilton, EP and Nashed, MZ},
  journal={Journal of Functional Analysis},
  volume={49},
  number={1},
  pages={128--144},
  year={1982},
  publisher={Elsevier}
}