@inproceedings{top,
	author = {Li, Junyan and Zhang, Li Lyna and Xu, Jiahang and Wang, Yujing and Yan, Shaoguang and Xia, Yunqing and Yang, Yuqing and Cao, Ting and Sun, Hao and Deng, Weiwei and Zhang, Qi and Yang, Mao},
	title = {Constraint-Aware and Ranking-Distilled Token Pruning for Efficient Transformer Inference},
	year = {2023},
	booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	pages = {1280–1290},
	series = {KDD '23}
}
@inproceedings{ltp,
	author = {Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt},
	title = {Learned Token Pruning for Transformers},
	year = {2022},
	isbn = {9781450393850},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	pages = {784–794},
	numpages = {11},
	keywords = {deep learning, natural language processing, network pruning},
	location = {Washington DC, USA},
	series = {KDD '22}
}

@inproceedings{transkimmer,
	title = {Transkimmer: Transformer Learns to Layer-wise Skim},
	author = {Guan, Yue  and
	Li, Zhengyi  and
	Leng, Jingwen  and
	Lin, Zhouhan  and
	Guo, Minyi},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	year = {2022},
	publisher = {Association for Computational Linguistics},
	pages = {7275--7286},
	
}
@inproceedings{swiftpruner,
	title={SwiftPruner: Reinforced Evolutionary Pruning for Efficient Ad Relevance},
	author={Zhang, Li Lyna and Homma, Youkow and Wang, Yujing and Wu, Min and Yang, Mao and Zhang, Ruofei and Cao, Ting and Shen, Wei},
	booktitle={Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
	pages={3654--3663},
	year={2022}
}
@inproceedings{cofi,
	title={Structured Pruning Learns Compact and Accurate Models},
	author={Xia, Mengzhou and Zhong, Zexuan and Chen, Danqi},
	booktitle={Association for Computational Linguistics (ACL)},
	year={2022}
}
@inproceedings{sanh2020distilbert,
	title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, 
	author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
	year={2020},
	eprint={1910.01108},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@misc{jiao2020tinybert,
	title={TinyBERT: Distilling BERT for Natural Language Understanding}, 
	author={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
	year={2020},
	booktitle={EMNLP}
}

@article{zafrir2019q8bert,
	title={Q8bert: Quantized 8bit bert},
	author={Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
	journal={arXiv preprint arXiv:1910.06188},
	year={2019}
}
@inproceedings{movement,
	title={Movement pruning: Adaptive sparsity by fine-tuning},
	author={Sanh, Victor and Wolf, Thomas and Rush, Alexander M},
	booktitle={NeurIPS},
	year={2020}
}
@misc{gordon2020compressing,
	title={Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning}, 
	author={Mitchell A. Gordon and Kevin Duh and Nicholas Andrews},
	year={2020},
	eprint={2002.08307},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@inproceedings{shen2020q,
	title={Q-bert: Hessian based ultra low precision quantization of bert},
	author={Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	year={2020}
}
@article{kim2021bert,
	title={I-bert: Integer-only bert quantization},
	author={Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
	journal={arXiv preprint arXiv:2101.01321},
	year={2021}
}
@article{bert,
	title={Bert: Pre-training of deep bidirectional transformers for language understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	journal={arXiv preprint arXiv:1810.04805},
	year={2018}
}
@article{albert,
	title={Albert: A lite bert for self-supervised learning of language representations},
	author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	journal={arXiv preprint arXiv:1909.11942},
	year={2019}
}
@inproceedings{nn_pruning,
	author = {Francois Lagunas and Ella Charlaix and Victor Sanh and Alexander M. Rush},
	title = {Block Pruning For Faster Transformers},
	year = {2021},
	
	booktitle = {EMNLP},
	
}
@article{roberta,
	title={Roberta: A robustly optimized bert pretraining approach},
	author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	journal={arXiv preprint arXiv:1907.11692},
	year={2019}
}
@article{t5,
	author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
	title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	journal = {Journal of Machine Learning Research},
	year    = {2020},
	volume  = {21},
	number  = {140},
	pages   = {1-67},
	url     = {http://jmlr.org/papers/v21/20-074.html}
}
@inproceedings{llmpruner,
	title={LLM-Pruner: On the Structural Pruning of Large Language Models},
	author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
	journal={arXiv preprint arXiv:2305.11627},
	year={2023}
}
@article{loraprune,
	title={Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning},
	author={Zhang, Mingyang and Shen, Chunhua and Yang, Zhen and Ou, Linlin and Yu, Xinyi and Zhuang, Bohan and others},
	journal={arXiv preprint arXiv:2305.18403},
	year={2023}
}
@article{sparsegpt,
	title={{SparseGPT}: Massive Language Models Can Be Accurately Pruned in One-Shot}, 
	author={Elias Frantar and Dan Alistarh},
	year={2023},
	journal={arXiv preprint arXiv:2301.00774}
}

@article{wanda,
	title={A Simple and Effective Pruning Approach for Large Language Models},
	author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
	journal={arXiv preprint arXiv:2306.11695},
	year={2023}
}
@misc{gpt4,
	title={GPT-4 Technical Report}, 
	author={OpenAI and : and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mo Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
	year={2023},
	eprint={2303.08774},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@inproceedings{lora,
	title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
	author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
	booktitle={International Conference on Learning Representations},
	year={2022},
	url={https://openreview.net/forum?id=nZeVKeeFYf9}
}


@misc{llama,
	title={LLaMA: Open and Efficient Foundation Language Models}, 
	author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
	year={2023},
	eprint={2302.13971},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@misc{llama2,
	title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
	author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
	year={2023},
	eprint={2307.09288},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@article{transformer,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}

@article{ste,
	title={Estimating or propagating gradients through stochastic neurons for conditional computation},
	author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
	journal={arXiv preprint arXiv:1308.3432},
	year={2013}
}
@article{wei2021finetuned,
	title={Finetuned language models are zero-shot learners},
	author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
	journal={arXiv preprint arXiv:2109.01652},
	year={2021}
}
@inproceedings{
	sanh2022multitask,
	title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
	author={Victor Sanh and Albert Webson and Colin Raffel and Stephen Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Teven Le Scao and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M Rush},
	booktitle={International Conference on Learning Representations},
	year={2022},
}
@article{peng2023instruction,
	title={Instruction tuning with gpt-4},
	author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
	journal={arXiv preprint arXiv:2304.03277},
	year={2023}
}
@article{instructgpt,
	title={Training language models to follow instructions with human feedback},
	author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
	journal={Advances in Neural Information Processing Systems},
	volume={35},
	pages={27730--27744},
	year={2022}
}
@article{compressprompt,
	title={Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt},
	author={Xu, Zhaozhuo and Liu, Zirui and Chen, Beidi and Tang, Yuxin and Wang, Jue and Zhou, Kaixiong and Hu, Xia and Shrivastava, Anshumali},
	journal={arXiv preprint arXiv:2305.11186},
	year={2023}
}
@article{goyal2022news,
	title={News summarization and evaluation in the era of gpt-3},
	author={Goyal, Tanya and Li, Junyi Jessy and Durrett, Greg},
	journal={arXiv preprint arXiv:2209.12356},
	year={2022}
}
@article{cot,
	title={Chain-of-thought prompting elicits reasoning in large language models},
	author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
	journal={Advances in Neural Information Processing Systems},
	volume={35},
	pages={24824--24837},
	year={2022}
}
@inproceedings{schick-schutze-2021-exploiting,
	title = {Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference},
	author = {Schick, Timo  and
	Sch{\"u}tze, Hinrich},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	year = {2021},
}
@article{chowdhery2022palm,
	title={Palm: Scaling language modeling with pathways},
	author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
	journal={arXiv preprint arXiv:2204.02311},
	year={2022}
}
@article{chung2022scaling,
	title={Scaling instruction-finetuned language models},
	author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
	journal={arXiv preprint arXiv:2210.11416},
	year={2022}
}
@misc{alpaca,
	author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
	title = {Stanford Alpaca: An Instruction-following LLaMA model},
	year = {2023},
	publisher = {GitHub},
	journal = {GitHub repository},
	howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}
@article{selfinstruct,
	title={Self-instruct: Aligning language model with self generated instructions},
	author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
	journal={arXiv preprint arXiv:2212.10560},
	year={2022}
}
@misc{vicuna,
	title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
	url = {https://lmsys.org/blog/2023-03-30-vicuna/},
	author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
	month = {March},
	year = {2023}
}
@article{liu2023llm,
	title={LLM-QAT: Data-Free Quantization Aware Training for Large Language Models},
	author={Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and Chang, Ernie and Stock, Pierre and Mehdad, Yashar and Shi, Yangyang and Krishnamoorthi, Raghuraman and Chandra, Vikas},
	journal={arXiv preprint arXiv:2305.17888},
	year={2023}
}

@article{c4,
	author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
	title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	journal = {arXiv e-prints},
	year = {2019},
	archivePrefix = {arXiv},
	eprint = {1910.10683},
}
@article{frantar-gptq,
	title={{GPTQ}: Accurate Post-training Compression for Generative Pretrained Transformers}, 
	author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
	year={2022},
	journal={arXiv preprint arXiv:2210.17323}
}
@article{yao2022zeroquant,
	title={Zeroquant: Efficient and affordable post-training quantization for large-scale transformers},
	author={Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
	journal={Advances in Neural Information Processing Systems},
	volume={35},
	pages={27168--27183},
	year={2022}
}
@inproceedings{xiao2023smoothquant,
	title={Smoothquant: Accurate and efficient post-training quantization for large language models},
	author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
	booktitle={International Conference on Machine Learning},
	pages={38087--38099},
	year={2023},
	organization={PMLR}
}
@article{llama,
	title={Llama: Open and efficient foundation language models},
	author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
	journal={arXiv preprint arXiv:2302.13971},
	year={2023}
}
@article{zhao2023survey,
	title={A survey of large language models},
	author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
	journal={arXiv preprint arXiv:2303.18223},
	year={2023}
}
@article{huang2022towards,
	title={Towards reasoning in large language models: A survey},
	author={Huang, Jie and Chang, Kevin Chen-Chuan},
	journal={arXiv preprint arXiv:2212.10403},
	year={2022}
}
@article{chang2023survey,
	title={A survey on evaluation of large language models},
	author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Zhu, Kaijie and Chen, Hao and Yang, Linyi and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
	journal={arXiv preprint arXiv:2307.03109},
	year={2023}
}
@inproceedings{gpt3,
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	booktitle = {Advances in Neural Information Processing Systems},
	title = {Language Models are Few-Shot Learners},
	volume = {33},
	year = {2020}
}
@article{opt,
	title={Opt: Open pre-trained transformer language models},
	author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
	journal={arXiv preprint arXiv:2205.01068},
	year={2022}
}
@article{palm,
	title={Palm: Scaling language modeling with pathways},
	author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
	journal={arXiv preprint arXiv:2204.02311},
	year={2022}
}
@article{bloom,
	title={Bloom: A 176b-parameter open-access multilingual language model},
	author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
	journal={arXiv preprint arXiv:2211.05100},
	year={2022}
}
@article{xu2023compress,
	title={Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt},
	author={Xu, Zhaozhuo and Liu, Zirui and Chen, Beidi and Tang, Yuxin and Wang, Jue and Zhou, Kaixiong and Hu, Xia and Shrivastava, Anshumali},
	journal={arXiv preprint arXiv:2305.11186},
	year={2023}
}
@inproceedings{lagrangian,
	title = {Structured Pruning of Large Language Models},
	author = {Wang, Ziheng  and	Wohlwend, Jeremy  and	Lei, Tao},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	year = {2020},
}

@inproceedings{l0,
	title={Learning Sparse Neural Networks through $L_0$ Regularization},
	author={Christos Louizos and Max Welling and Diederik P. Kingma},
	booktitle={International Conference on Learning Representations},
	year={2018}
}
@article{hu2023llm,
	title={LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models},
	author={Hu, Zhiqiang and Lan, Yihuai and Wang, Lei and Xu, Wanyu and Lim, Ee-Peng and Lee, Roy Ka-Wei and Bing, Lidong and Poria, Soujanya},
	journal={arXiv preprint arXiv:2304.01933},
	year={2023}
}
@article{dosovitskiy2020image,
	title={An image is worth 16x16 words: Transformers for image recognition at scale},
	author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
	journal={arXiv preprint arXiv:2010.11929},
	year={2020}
}
@article{gao2023llama,
	title={Llama-adapter v2: Parameter-efficient visual instruction model},
	author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others},
	journal={arXiv preprint arXiv:2304.15010},
	year={2023}
}
@misc{dong2023large,
	title={Large Language Model for Science: A Study on P vs. NP}, 
	author={Qingxiu Dong and Li Dong and Ke Xu and Guangyan Zhou and Yaru Hao and Zhifang Sui and Furu Wei},
	year={2023},
	eprint={2309.05689},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@article{race,
	title={RACE: Large-scale ReAding Comprehension Dataset From Examinations},
	author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
	journal={arXiv preprint arXiv:1704.04683},
	year={2017}
}
@article{mmlu,
	title={Measuring massive multitask language understanding},
	author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	journal={arXiv preprint arXiv:2009.03300},
	year={2020}
}
@article{bigbench,
	title={Challenging big-bench tasks and whether chain-of-thought can solve them},
	author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
	journal={arXiv preprint arXiv:2210.09261},
	year={2022}
}
@inproceedings{storycloze,
	title={Lsdsem 2017 shared task: The story cloze test},
	author={Mostafazadeh, Nasrin and Roth, Michael and Louis, Annie and Chambers, Nathanael and Allen, James},
	booktitle={Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics},
	pages={46--51},
	year={2017}
}
@inproceedings{piqa,
	author = {Yonatan Bisk and Rowan Zellers and
	Ronan Le Bras and Jianfeng Gao
	and Yejin Choi},
	title = {PIQA: Reasoning about Physical Commonsense in
	Natural Language},
	booktitle = {Thirty-Fourth AAAI Conference on
	Artificial Intelligence},
	year = {2020},
}
@inproceedings{zellers2019hellaswag,
	title={HellaSwag: Can a Machine Really Finish Your Sentence?},
	author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
	booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	year={2019}
}
@InProceedings{ai2:winogrande,
	title = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
	authors={Keisuke, Sakaguchi and Ronan, Le Bras and Chandra, Bhagavatula and Yejin, Choi
	},
	year={2019}
}
@article{allenai:arc,
	author    = {Peter Clark  and Isaac Cowhey and Oren Etzioni and Tushar Khot and
	Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
	title     = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
	journal   = {arXiv:1803.05457v1},
	year      = {2018},
}
@inproceedings{OpenBookQA2018,
	title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
	author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
	booktitle={EMNLP},
	year={2018}
}
@inproceedings{clark2019boolq,
	title =     {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
	author =    {Clark, Christopher and Lee, Kenton and Chang, Ming-Wei, and Kwiatkowski, Tom and Collins, Michael, and Toutanova, Kristina},
	booktitle = {NAACL},
	year =      {2019},
}
@software{eval-harness,
	author       = {Gao, Leo and
	Tow, Jonathan and
	Biderman, Stella and
	Black, Sid and
	DiPofi, Anthony and
	Foster, Charles and
	Golding, Laurence and
	Hsu, Jeffrey and
	McDonell, Kyle and
	Muennighoff, Niklas and
	Phang, Jason and
	Reynolds, Laria and
	Tang, Eric and
	Thite, Anish and
	Wang, Ben and
	Wang, Kevin and
	Zou, Andy},
	title        = {A framework for few-shot language model evaluation},
	month        = sep,
	year         = 2021,
	publisher    = {Zenodo},
	version      = {v0.0.1},
	doi          = {10.5281/zenodo.5371628},
	url          = {https://doi.org/10.5281/zenodo.5371628}
}
@article{chia2023instructeval,
	title={INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models},
	author={Chia, Yew Ken and Hong, Pengfei and Bing, Lidong and Poria, Soujanya},
	journal={arXiv preprint arXiv:2306.04757},
	year={2023}
}
@inproceedings{srinivas2022cyclical,
	title={Cyclical pruning for sparse neural networks},
	author={Srinivas, Suraj and Kuzmin, Andrey and Nagel, Markus and van Baalen, Mart and Skliar, Andrii and Blankevoort, Tijmen},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={2762--2771},
	year={2022}
}
@inproceedings{
	pool2021channel,
	title={Channel Permutations for N:M Sparsity},
	author={Jeff Pool and Chong Yu},
	booktitle={Advances in Neural Information Processing Systems},
	editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
	year={2021},
	url={https://openreview.net/forum?id=WAO1STUPWPP}
}
@article{hubara2021accelerated,
	title={Accelerated sparse neural training: A provable and efficient method to find n: m transposable masks},
	author={Hubara, Itay and Chmiel, Brian and Island, Moshe and Banner, Ron and Naor, Joseph and Soudry, Daniel},
	journal={Advances in neural information processing systems},
	volume={34},
	pages={21099--21111},
	year={2021}
}
@article{wang2022self,
  title={Self-instruct: Aligning language model with self generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@article{zhou2023solving,
	title={Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification},
	author={Zhou, Aojun and Wang, Ke and Lu, Zimu and Shi, Weikang and Luo, Sichun and Qin, Zipeng and Lu, Shaoqing and Jia, Anya and Song, Linqi and Zhan, Mingjie and others},
	journal={arXiv preprint arXiv:2308.07921},
	year={2023}
}

@article{longllama,
	title={Focused Transformer: Contrastive Training for Context Scaling}, 
	author={Szymon Tworkowski and Konrad Staniszewski and Mikołaj Pacek and Yuhuai Wu and Henryk Michalewski and Piotr Miłoś},
	year={2023},
	eprint={2307.03170},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@article{wang2023augmenting,
	title={Augmenting Language Models with Long-Term Memory},
	author={Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},
	journal={arXiv preprint arXiv:2306.07174},
	year={2023}
}
@article{han2023lminfinite,
	title={LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models},
	author={Han, Chi and Wang, Qifan and Xiong, Wenhan and Chen, Yu and Ji, Heng and Wang, Sinong},
	journal={arXiv preprint arXiv:2308.16137},
	year={2023}
}

@article{streamingllm,
	title={Efficient Streaming Language Models with Attention Sinks},
	author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
	journal={arXiv},
	year={2023}
}
@article{dao2023flashattention2,
	title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
	author={Dao, Tri},
	year={2023}
}
@misc{ntk,
	title={Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degration},
	author={LocalLLaMA},
	year={2023},
	url={{https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/}}
}

@misc{dynamicntk,
	title={Dynamically scaled rope further increases performance of long context LLaMA with zero fine-tuning},
	author={LocalLLaMA},
	year={2023},
url={{https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/}}
}
@article{yarn,
	title={Yarn: Efficient context window extension of large language models},
	author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
	journal={arXiv preprint arXiv:2309.00071},
	year={2023}
}
@article{pi,
	title={Extending context window of large language models via positional interpolation},
	author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
	journal={arXiv preprint arXiv:2306.15595},
	year={2023}
}

@article{rope,
	title={Roformer: Enhanced transformer with rotary position embedding},
	author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
	journal={arXiv preprint arXiv:2104.09864},
	year={2021}
}

@article{alibi,
	title={Train short, test long: Attention with linear biases enables input length extrapolation},
	author={Press, Ofir and Smith, Noah A and Lewis, Mike},
	journal={arXiv preprint arXiv:2108.12409},
	year={2021}
}
@article{haviv2022transformer,
	title={Transformer language models without positional encodings still learn positional information},
	author={Haviv, Adi and Ram, Ori and Press, Ofir and Izsak, Peter and Levy, Omer},
	journal={arXiv preprint arXiv:2203.16634},
	year={2022}
}
@article{sun2022length,
	title={A length-extrapolatable transformer},
	author={Sun, Yutao and Dong, Li and Patra, Barun and Ma, Shuming and Huang, Shaohan and Benhaim, Alon and Chaudhary, Vishrav and Song, Xia and Wei, Furu},
	journal={arXiv preprint arXiv:2212.10554},
	year={2022}
}

@inproceedings{borgeaud2022improving,
	title={Improving language models by retrieving from trillions of tokens},
	author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
	booktitle={International conference on machine learning},
	pages={2206--2240},
	year={2022},
	organization={PMLR}
}

@article{parallelwindow,
	title={Parallel context windows improve in-context learning of large language models},
	author={Ratner, Nir and Levine, Yoav and Belinkov, Yonatan and Ram, Ori and Abend, Omri and Karpas, Ehud and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav},
	journal={arXiv preprint arXiv:2212.10947},
	year={2022}
}

@article{tancik2020fourier,
	title={Fourier features let networks learn high frequency functions in low dimensional domains},
	author={Tancik, Matthew and Srinivasan, Pratul and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan and Ng, Ren},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	pages={7537--7547},
	year={2020}
}

@article{jacot2018neural,
	title={Neural tangent kernel: Convergence and generalization in neural networks},
	author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
	journal={Advances in neural information processing systems},
	volume={31},
	year={2018}
}

@inproceedings{spos,
	title={Single path one-shot neural architecture search with uniform sampling},
	author={Guo, Zichao and Zhang, Xiangyu and Mu, Haoyuan and Heng, Wen and Liu, Zechun and Wei, Yichen and Sun, Jian},
	booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XVI 16},
	pages={544--560},
	year={2020},
	organization={Springer}
}

@article{longlora,
	title={LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models},
	author={Yukang Chen and Shengju Qian and Haotian Tang and Xin Lai and Zhijian Liu and Song Han and Jiaya Jia},
	journal={arXiv:2309.12307},
	year={2023}
}

@misc{mistral,
	title={Mistral 7B}, 
	author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
	year={2023},
	eprint={2310.06825},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@article{phi,
	title={Textbooks Are All You Need},
	author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
	journal={arXiv preprint arXiv:2306.11644},
	year={2023}
}
@article{textbooks2,
	title={Textbooks Are All You Need II: \textbf{phi-1.5} technical report},
	author={Li, Yuanzhi and Bubeck, S{\'e}bastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
	journal={arXiv preprint arXiv:2309.05463},
	year={2023}
}
@article{simonyan2014very,
	title={Very deep convolutional networks for large-scale image recognition},
	author={Simonyan, Karen and Zisserman, Andrew},
	journal={arXiv preprint arXiv:1409.1556},
	year={2014}
}
@article{rogers2021primer,
	title={A primer in BERTology: What we know about how BERT works},
	author={Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	journal={Transactions of the Association for Computational Linguistics},
	volume={8},
	pages={842--866},
	year={2021},
	publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{liu2023scaling,
	title={Scaling Laws of RoPE-based Extrapolation},
	author={Liu, Xiaoran and Yan, Hang and Zhang, Shuo and An, Chenxin and Qiu, Xipeng and Lin, Dahua},
	journal={arXiv preprint arXiv:2310.05209},
	year={2023}
}

@misc{xiong2023effective,
	title={Effective Long-Context Scaling of Foundation Models}, 
	author={Wenhan Xiong and Jingyu Liu and Igor Molybog and Hejia Zhang and Prajjwal Bhargava and Rui Hou and Louis Martin and Rashi Rungta and Karthik Abinav Sankararaman and Barlas Oguz and Madian Khabsa and Han Fang and Yashar Mehdad and Sharan Narang and Kshitiz Malik and Angela Fan and Shruti Bhosale and Sergey Edunov and Mike Lewis and Sinong Wang and Hao Ma},
	year={2023},
	eprint={2309.16039},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@article{huang2023boosting,
	title={Boosting LLM Reasoning: Push the Limits of Few-shot Learning with Reinforced In-Context Pruning},
	author={Huang, Xijie and Zhang, Li Lyna and Cheng, Kwang-Ting and Yang, Mao},
	journal={arXiv preprint arXiv:2312.08901},
	year={2023}
}
@article{fei2023extending,
	title={Extending Context Window of Large Language Models via Semantic Compression},
	author={Fei, Weizhi and Niu, Xueyan and Zhou, Pingyi and Hou, Lu and Bai, Bo and Deng, Lei and Han, Wei},
	journal={arXiv preprint arXiv:2312.09571},
	year={2023}
}

@article{ge2023context,
	title={In-context Autoencoder for Context Compression in a Large Language Model},
	author={Ge, Tao and Hu, Jing and Wang, Xun and Chen, Si-Qing and Wei, Furu},
	journal={arXiv preprint arXiv:2307.06945},
	year={2023}
}

@misc{zhu2023pose,
	title={PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training}, 
	author={Dawei Zhu and Nan Yang and Liang Wang and Yifan Song and Wenhao Wu and Furu Wei and Sujian Li},
	year={2023},
	eprint={2309.10400},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}


@misc{codellama,
	title={Code Llama: Open Foundation Models for Code}, 
	author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
	year={2023},
	eprint={2308.12950},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@article{weng2023prompt,
	title   = {LLM-powered Autonomous Agents},
	author  = {Weng, Lilian},
	journal = {lilianweng.github.io},
	year    = {2023},
	month   = {Jun},
	url     = {https://lilianweng.github.io/posts/2023-06-23-agent/}"
}

@inproceedings{park2023generative,
	title={Generative agents: Interactive simulacra of human behavior},
	author={Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
	booktitle={Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
	pages={1--22},
	year={2023}
}
@article{cube,
	title={SuperScaler: Supporting Flexible DNN Parallelization via a Unified Abstraction},
	author={Lin, Zhiqi and Miao, Youshan and Liu, Guodong and Shi, Xiaoxiang and Zhang, Quanlu and Yang, Fan and Maleki, Saeed and Zhu, Yi and Cao, Xu and Li, Cheng and others},
	journal={arXiv preprint arXiv:2301.08984},
	year={2023}
}

@inproceedings{guo2020single,
	title={Single path one-shot neural architecture search with uniform sampling},
	author={Guo, Zichao and Zhang, Xiangyu and Mu, Haoyuan and Heng, Wen and Liu, Zechun and Wei, Yichen and Sun, Jian},
	booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XVI 16},
	pages={544--560},
	year={2020},
	organization={Springer}
}
@article{pg19,
	author = {Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and
	Hillier, Chloe and Lillicrap, Timothy P},
	title = {Compressive Transformers for Long-Range Sequence Modelling},
	journal = {arXiv preprint},
	url = {https://arxiv.org/abs/1911.05507},
	year = {2019},
}
@misc{proof-pile,
	author={Zhangir Azerbayev and Edward Ayers and Bartosz Piotrowski},
	title={Proof-pile},
	url={https://github.com/zhangir-azerbayev/ProofNet},
	year={2022},

}

@article{jiang2024mixtral,
	title={Mixtral of Experts},
	author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
	journal={arXiv preprint arXiv:2401.04088},
	year={2024}
}
@article{zhang2024soaring,
	title={Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon},
	author={Zhang, Peitian and Liu, Zheng and Xiao, Shitao and Shao, Ninglu and Ye, Qiwei and Dou, Zhicheng},
	journal={arXiv preprint arXiv:2401.03462},
	year={2024}
}
@software{redpajama,
	author = {Together Computer},
	title = {RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset},
	month = April,
	year = 2023,
	url = {https://github.com/togethercomputer/RedPajama-Data}
}
@misc{mistraldata,
	author={},
	title={Long-data collections},
	url={https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T},
	year={2024},
	
}

@misc{mistrallite,
	author={Amazon},
	title={MistralLite},
	url={https://huggingface.co/amazon/MistralLite},
	year={2023},
	
}

@misc{huggingfaceboard,
	author={Hugging Face},
	title={Open LLM Leaderboard},
	url={https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard},
	year={2024},
	
}
@misc{books,
	author={Shawn Presser},
	url={https://twitter.com/theshawwn/status/1320282149329784833},
	year={2020},
	
}
@misc{together,
	author={Together},
	url={https://huggingface.co/togethercomputer/LLaMA-2-7B-32K},
	year={2023},
	
}

@article{mmlu,
	title={Measuring massive multitask language understanding},
	author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	journal={arXiv preprint arXiv:2009.03300},
	year={2020}
}
@inproceedings{zellers2019hellaswag,
	title={HellaSwag: Can a Machine Really Finish Your Sentence?},
	author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
	booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	year={2019}
}
@article{allenai:arc,
	author    = {Peter Clark  and Isaac Cowhey and Oren Etzioni and Tushar Khot and
	Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
	title     = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
	journal   = {arXiv:1803.05457v1},
	year      = {2018},
}
@article{pile,
	title={The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling},
	author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
	journal={arXiv preprint arXiv:2101.00027},
	year={2020}
}
@article{govreport,
	title={Efficient attentions for long document summarization},
	author={Huang, Luyang and Cao, Shuyang and Parulian, Nikolaus and Ji, Heng and Wang, Lu},
	journal={arXiv preprint arXiv:2104.02112},
	year={2021}
}
@article{lin2021truthfulqa,
	title={Truthfulqa: Measuring how models mimic human falsehoods},
	author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
	journal={arXiv preprint arXiv:2109.07958},
	year={2021}
}
@article{passkey,
	title={Landmark Attention: Random-Access Infinite Context Length for Transformers},
	author={Mohtashami, Amirkeivan and Jaggi, Martin},
	journal={arXiv preprint arXiv:2305.16300},
	year={2023}
}
@misc{madaan2023selfrefine,
	title={Self-Refine: Iterative Refinement with Self-Feedback}, 
	author={Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Shashank Gupta and Bodhisattwa Prasad Majumder and Katherine Hermann and Sean Welleck and Amir Yazdanbakhsh and Peter Clark},
	year={2023},
	eprint={2303.17651},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@inproceedings{Huang2023FewerIM,
	title={Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning},
	author={Xijie Huang and Li Lyna Zhang and Kwang-Ting Cheng and Fan Yang and Mao Yang},
	year={2023},
	url={https://api.semanticscholar.org/CorpusID:266210460}
}