@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K},
  year={2008}
}

@article{propp1996exact,
  title={Exact sampling with coupled Markov chains and applications to statistical mechanics},
  author={Propp, James Gary and Wilson, David Bruce},
  journal={Random Structures and Algorithms},
  year={1996},
  publisher={Wiley Online Library}
}

@book{rockafellar1970convex,
  title={Convex analysis},
  author={Rockafellar, Ralph Tyrell},
  year={1970},
  publisher={Princeton university press}
}

@inproceedings{osband2013more,
	title={({M}ore) efficient reinforcement learning via posterior sampling},
	author={Osband, Ian and Russo, Dan and Van Roy, Benjamin},
	booktitle={Advances in Neural Information Processing Systems},
	pages={3003--3011},
	year={2013}
}

@article{o2018variational,
  title={Variational {B}ayesian Reinforcement Learning with Regret Bounds},
  author={O'Donoghue, Brendan},
  journal={arXiv preprint arXiv:1807.09647},
  year={2018}
}

@inproceedings{perolat2015approximate,
  title={Approximate dynamic programming for two-player zero-sum markov games},
  author={Perolat, Julien and Scherrer, Bruno and Piot, Bilal and Pietquin, Olivier},
  booktitle={International Conference on Machine Learning},
  pages={1321--1329},
  year={2015},
  organization={PMLR}
}

@book{osborne1994course,
  title={A course in game theory},
  author={Osborne, Martin J and Rubinstein, Ariel},
  year={1994},
  publisher={MIT press}
}

@article{parikh2014proximal,
  title={Proximal algorithms},
  author={Parikh, Neal and Boyd, Stephen},
  journal={Foundations and Trends{\textregistered} in Optimization},
  volume={1},
  number={3},
  pages={127--239},
  year={2014},
  publisher={Now Publishers, Inc.}
}

@article{o2020stochastic,
  title={Stochastic matrix games with bandit feedback},
  author={O'Donoghue, Brendan and Lattimore, Tor and Osband, Ian},
  journal={arXiv preprint arXiv:2006.05145},
  year={2020}
}

@article{neumann1928theorie,
  title={Zur theorie der gesellschaftsspiele},
  author={Von Neumann, John},
  journal={Mathematische annalen},
  volume={100},
  number={1},
  pages={295--320},
  year={1928},
  publisher={Springer}
}

@article{mehta2008transfer,
  title={Transfer in variable-reward hierarchical reinforcement learning},
  author={Mehta, Neville and Natarajan, Sriraam and Tadepalli, Prasad and Fern, Alan},
  journal={Machine Learning},
  volume={73},
  number={3},
  pages={289},
  year={2008},
  publisher={Springer}
}

@book{cover1999elements,
  title={Elements of information theory},
  author={Cover, Thomas M},
  year={1999},
  publisher={John Wiley \& Sons}
}

@inproceedings{ratliff2006maximum,
  title={Maximum margin planning},
  author={Ratliff, Nathan D and Bagnell, J Andrew and Zinkevich, Martin A},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={729--736},
  year={2006}
}

@book{adams1995hitchhiker,
  title={The Hitchhiker's Guide to the Galaxy},
  author={Adams, D.},
  isbn={9781417642595},
  url={http://books.google.com/books?id=W-xMPgAACAAJ},
  year={1995},
  publisher={San Val}
}

@inproceedings{azar2017minimax,
  title={Minimax Regret Bounds for Reinforcement Learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017}
}

@article{gidel2018frank,
  title={Frank-Wolfe splitting via augmented Lagrangian method},
  author={Gidel, Gauthier and Pedregosa, Fabian and Lacoste-Julien, Simon},
  journal={arXiv preprint arXiv:1804.03176},
  year={2018}
}

@article{tassa2018deepmind,
  title={Deepmind control suite},
  author={Tassa, Yuval and Doron, Yotam and Muldal, Alistair and Erez, Tom and Li, Yazhe and Casas, Diego de Las and Budden, David and Abdolmaleki, Abbas and Merel, Josh and Lefrancq, Andrew and others},
  journal={arXiv preprint arXiv:1801.00690},
  year={2018}
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@article{diamond2016cvxpy,
  author  = {Steven Diamond and Stephen Boyd},
  title   = {{CVXPY}: {A} {P}ython-embedded modeling language for convex optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {83},
  pages   = {1--5},
}

@article{hallak2015contextual,
  title={Contextual Markov decision processes},
  author={Hallak, Assaf and Di Castro, Dotan and Mannor, Shie},
  journal={arXiv preprint arXiv:1502.02259},
  year={2015}
}

@inproceedings{hazan2016variance,
  title={Variance-reduced and projection-free stochastic optimization},
  author={Hazan, Elad and Luo, Haipeng},
  booktitle={International Conference on Machine Learning},
  pages={1263--1271},
  year={2016}
}

@inproceedings{hazan2012projection,
  title={Projection-free online learning},
  author={Hazan, Elad E and Kale, Satyen},
  booktitle={29th International Conference on Machine Learning, ICML 2012},
  pages={521--528},
  year={2012}
}
  
@article{beck2017linearly,
  title={Linearly convergent away-step conditional gradient for non-strongly convex functions},
  author={Beck, Amir and Shtern, Shimrit},
  journal={Mathematical Programming},
  volume={164},
  number={1-2},
  pages={1--27},
  year={2017},
  publisher={Springer}
}

@article{achiam2018variational,
  title={Variational option discovery algorithms},
  author={Achiam, Joshua and Edwards, Harrison and Amodei, Dario and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1807.10299},
  year={2018}
}

@inproceedings{
hausman2018learning,
title={Learning an Embedding Space for Transferable Robot Skills},
author={Karol Hausman and Jost Tobias Springenberg and Ziyu Wang and Nicolas Heess and Martin Riedmiller},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rk07ZXZRb},
}

@inproceedings{florensa2016stochastic,
  title={Stochastic Neural Networks for Hierarchical Reinforcement Learning},
  author={Florensa, Carlos and Duan, Yan and Abbeel, Pieter},
  booktitle={International Conference on Learning Representations},
  year={2016},
}

@article{tirumala2020behavior,
  title={Behavior Priors for Efficient Reinforcement Learning},
  author={Tirumala, Dhruva and Galashov, Alexandre and Noh, Hyeonwoo and Hasenclever, Leonard and Pascanu, Razvan and Schwarz, Jonathan and Desjardins, Guillaume and Czarnecki, Wojciech Marian and Ahuja, Arun and Teh, Yee Whye and others},
  journal={arXiv preprint arXiv:2010.14274},
  year={2020}
}

@article{garber2013linearly,
  title={A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization},
  author={Garber, Dan and Hazan, Elad},
  journal={arXiv preprint arXiv:1301.4666},
  year={2013}
}


@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={International conference on Machine learning},
  pages={267--274},
  year={2002},
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{osband2016deep,
  title={Deep exploration via bootstrapped DQN},
  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Roy, Benjamin Van},
  booktitle={Proceedings of the 30th International Conference on Neural Information Processing Systems},
  pages={4033--4041},
  year={2016}
}

@inproceedings{osband2019behaviour,
  title={Behaviour Suite for Reinforcement Learning},
  author={Osband, Ian and Doron, Yotam and Hessel, Matteo and Aslanides, John and Sezener, Eren and Saraiva, Andre and McKinney, Katrina and Lattimore, Tor and Szepesvari, Csaba and Singh, Satinder and others},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{espeholt2018impala,
  title={Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures},
  author={Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Vlad and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},
  booktitle={International Conference on Machine Learning},
  pages={1407--1416},
  year={2018},
  organization={PMLR}
}

@InProceedings{pmlr-v15-boularias11a, title = {Relative Entropy Inverse Reinforcement Learning}, author = {Boularias, Abdeslam and Kober, Jens and Peters, Jan}, booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics}, pages = {182--189}, year = {2011}, editor = {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav}, volume = {15}, series = {Proceedings of Machine Learning Research}, address = {Fort Lauderdale, FL, USA}, month = {11--13 Apr}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v15/boularias11a/boularias11a.pdf}, url = { http://proceedings.mlr.press/v15/boularias11a.html }, abstract = {We consider the problem of imitation learning where the examples, demonstrated by an expert, cover only a small part of a large state space. Inverse Reinforcement Learning (IRL) provides an efficient tool for generalizing the demonstration, based on the assumption that the expert is optimally acting in a Markov Decision Process (MDP). Most of the past work on IRL requires that a (near)-optimal policy can be computed for different reward functions. However, this requirement can hardly be satisfied in systems with a large, or continuous, state space. In this paper, we propose a model-free IRL algorithm, where the relative entropy between the empirical distribution of the state-action trajectories under a baseline policy and their distribution under the learned policy is minimized by stochastic gradient descent. We compare this new approach to well-known IRL algorithms using learned MDP models. Empirical results on simulated car racing, gridworld and ball-in-a-cup problems show that our approach is able to learn good policies from a small number of demonstrations. [pdf]} }

@inproceedings{jaggi2013revisiting,
  title={Revisiting Frank-Wolfe: Projection-free sparse convex optimization.},
  author={Jaggi, Martin},
  booktitle={Proceedings of the 30th international conference on Machine learning},
  organization={ACM},
  year={2013}
}

@article{even2003learning,
  title={Learning rates for Q-learning},
  author={Even-Dar, Eyal and Mansour, Yishay},
  journal={Journal of machine learning Research},
  volume={5},
  number={Dec},
  pages={1--25},
  year={2003}
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@article{propp1998get,
  title={How to get a perfectly random sample from a generic Markov chain and generate a random spanning tree of a directed graph},
  author={Propp, James Gary and Wilson, David Bruce},
  journal={Journal of Algorithms},
  volume={27},
  number={2},
  pages={170--217},
  year={1998},
  publisher={Elsevier}
}

@inproceedings{syed2008apprenticeship,
  title={Apprenticeship learning using linear programming},
  author={Syed, Umar and Bowling, Michael and Schapire, Robert E},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1032--1039},
  year={2008},
  organization={ACM}
}

@inproceedings{dann2015sample,
  title={Sample complexity of episodic fixed-horizon reinforcement learning},
  author={Dann, Christoph and Brunskill, Emma},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2818--2826},
  year={2015}
}


@article{jaggi_workshop,
title={An affine invariant linear convergence analysis for Frank-Wolfe algorithms},
  author={Lacoste-Julien, S. and Jaggi, M.},
  journal={NIPS 2013 Workshop on Greedy Algorithms, Frank-Wolfe and Friends },
  year={2014}
}

@article{wolfe_ascg,
  title={Convergence theory in nonlinear programming},
  author={Wolfe, Philip},
  journal={Integer and nonlinear programming},
  pages={1--36},
  year={1970},
  publisher={North Holland, Amsterdam}
}


@article{ng2000algorithms,
  title={Algorithms for inverse reinforcement learning.},
  author={Ng, Andrew Y and Russell, Stuart J},
  journal={International Conference on Machine Learning (ICML)},
  year={2000}
}

@inproceedings{garber2015faster,
  title={Faster rates for the frank-wolfe method over strongly-convex sets},
  author={Garber, Dan and Hazan, Elad},
  booktitle={32nd International Conference on Machine Learning, ICML 2015},
  year={2015}
}

@article{frank1956algorithm,
  title={An algorithm for quadratic programming},
  author={Frank, Marguerite and Wolfe, Philip},
  journal={Naval research logistics quarterly},
  volume={3},
  number={1-2},
  pages={95--110},
  year={1956},
  publisher={Wiley Online Library}
}

@article{guelat1986some,
  title={Some comments on Wolfe's ‘away step’},
  author={Gu{\'e}lat, Jacques and Marcotte, Patrice},
  journal={Mathematical Programming},
  volume={35},
  number={1},
  pages={110--119},
  year={1986},
  publisher={Springer}
}

@article{canon1968tight,
  title={A tight upper bound on the rate of convergence of Frank-Wolfe algorithm},
  author={Canon, Michael D and Cullum, Clifton D},
  journal={SIAM Journal on Control},
  volume={6},
  number={4},
  pages={509--516},
  year={1968},
  publisher={SIAM}
}

@article{pena2018polytope,
  title={Polytope conditioning and linear convergence of the Frank--Wolfe algorithm},
  author={Pena, Javier and Rodriguez, Daniel},
  journal={Mathematics of Operations Research},
  volume={44},
  number={1},
  pages={1--18},
  year={2018},
  publisher={INFORMS}
}

@article{beck2004conditional,
  title={A conditional gradient method with linear rate of convergence for solving convex linear systems},
  author={Beck, Amir and Teboulle, Marc},
  journal={Mathematical Methods of Operations Research},
  volume={59},
  number={2},
  pages={235--247},
  year={2004},
  publisher={Springer}
}

@article{beck2003mirror,
  title={Mirror descent and nonlinear projected subgradient methods for convex optimization},
  author={Beck, Amir and Teboulle, Marc},
  journal={Operations Research Letters},
  volume={31},
  pages={167--175},
  year={2003}
}

@article{teboulle1992entropic,
  title={Entropic proximal mappings with applications to nonlinear programming},
  author={Teboulle, Marc},
  journal={Mathematics of Operations Research},
  volume={17},
  number={3},
  pages={670--690},
  year={1992},
  publisher={INFORMS}
}

@article{bertsekas1997nonlinear,
  title={Nonlinear programming},
  author={Bertsekas, Dimitri P},
  journal={Journal of the Operational Research Society},
  volume={48},
  number={3},
  pages={334--334},
  year={1997},
  publisher={Taylor \& Francis}
}

@article{jaksch2010near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Apr},
  pages={1563--1600},
  year={2010}
}

@article{strehl2009reinforcement,
  title={Reinforcement learning in finite MDPs: PAC analysis},
  author={Strehl, Alexander L and Li, Lihong and Littman, Michael L},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={Nov},
  pages={2413--2444},
  year={2009}
}
@inproceedings{ortner2012online,
  title={Online regret bounds for undiscounted continuous reinforcement learning},
  author={Ortner, Ronald and Ryabko, Daniil},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1763--1771},
  year={2012}
}


@article{hansen2013strategy,
  title={Strategy iteration is strongly polynomial for 2-player turn-based stochastic games with a constant discount factor},
  author={Hansen, Thomas Dueholm and Miltersen, Peter Bro and Zwick, Uri},
  journal={Journal of the ACM (JACM)},
  volume={60},
  number={1},
  pages={1},
  year={2013},
  publisher={ACM}
}

@article{ye2011simplex,
  title={The simplex and policy-iteration methods are strongly polynomial for the Markov decision problem with a fixed discount rate},
  author={Ye, Yinyu},
  journal={Mathematics of Operations Research},
  volume={36},
  number={4},
  pages={593--603},
  year={2011},
  publisher={INFORMS}
}

@inproceedings{dudik2004performance,
  title={Performance guarantees for regularized maximum entropy density estimation},
  author={Dudik, Miroslav and Phillips, Steven J and Schapire, Robert E},
  booktitle={International Conference on Computational Learning Theory},
  pages={472--486},
  year={2004},
  organization={Springer}
}

@article{della1997inducing,
  title={Inducing features of random fields},
  author={Della Pietra, Stephen and Della Pietra, Vincent and Lafferty, John},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={19},
  number={4},
  pages={380--393},
  year={1997},
  publisher={IEEE}
}

@inproceedings{barreto2017successor,
  title={Successor features for transfer in reinforcement learning},
  author={Barreto, Andr{\'e} and Dabney, Will and Munos, R{\'e}mi and Hunt, Jonathan J and Schaul, Tom and van Hasselt, Hado P and Silver, David},
  booktitle={Advances in neural information processing systems},
  pages={4055--4065},
  year={2017}
}

@article{barreto2020fast,
  title={Fast reinforcement learning with generalized policy updates},
  author={Barreto, Andr{\'e} and Hou, Shaobo and Borsa, Diana and Silver, David and Precup, Doina},
  journal={Proceedings of the National Academy of Sciences},
  year={2020},
  publisher={National Acad Sciences}
}

@inproceedings{zahavy2020planning,
  title={Planning in Hierarchical Reinforcement Learning: Guarantees for Using Local Policies},
  author={Zahavy, Tom and Hasidim, Avinatan and Kaplan, Haim and Mansour, Yishay},
  booktitle={Algorithmic Learning Theory},
  pages={906--934},
  year={2020}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={1984},
  publisher={John Wiley \& Sons}
}


@article{levitin1966constrained,
  title={Constrained minimization methods},
  author={Levitin, Evgeny S and Polyak, Boris T},
  journal={USSR Computational mathematics and mathematical physics},
  volume={6},
  number={5},
  pages={1--50},
  year={1966},
  publisher={Elsevier}
}

@article{weissman2003inequalities,
  title={Inequalities for the L1 deviation of the empirical distribution},
  author={Weissman, Tsachy and Ordentlich, Erik and Seroussi, Gadiel and Verdu, Sergio and Weinberger, Marcelo J},
  year={2003}
}

@article{zahavy2019average,
  title={Average reward reinforcement learning with unknown mixing times},
  author={Zahavy, Tom and Cohen, Alon and Kaplan, Haim and Mansour, Yishay},
  journal={The Conference on Uncertainty in Artificial Intelligence (UAI)},
  year={2020}
}

@inproceedings{nemirovsky1983problem,
  booktitle={Problem complexity and method efficiency in optimization},
  author={Nemirovsky, Arkadii Semenovich and Yudin, David Borisovich},
  year={1983},
  publisher={Wiley, New York}
}


@article{modi2017markov,
  title={Markov Decision Processes with Continuous Side Information},
  author={Modi, Aditya and Jiang, Nan and Singh, Satinder and Tewari, Ambuj},
  journal={arXiv preprint arXiv:1711.05726},
  year={2017}
}

@inproceedings{syed2008game,
  title={A game-theoretic approach to apprenticeship learning},
  author={Syed, Umar and Schapire, Robert E},
  booktitle={Advances in neural information processing systems},
  pages={1449--1456},
  year={2008}
}

@inproceedings{abbeel2004apprenticeship,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={1},
  year={2004},
  organization={ACM}
}

@inproceedings{freund1996game,
  title={Game theory, on-line prediction and boosting},
  author={Freund, Yoav and Schapire, Robert E},
  booktitle={COLT},
  volume={96},
  pages={325--332},
  year={1996},
  organization={Citeseer}
}

@article{garber2016linearly,
  title={A linearly convergent variant of the conditional gradient algorithm under strong convexity, with applications to online and stochastic optimization},
  author={Garber, Dan and Hazan, Elad},
  journal={SIAM Journal on Optimization},
  volume={26},
  number={3},
  pages={1493--1528},
  year={2016},
  publisher={SIAM}
}

@article{zahavy2019apprenticeship,
  title={Apprenticeship Learning via Frank-Wolfe},
  author={Zahavy, Tom and Cohen, Alon and Kaplan, Haim and Mansour, Yishay},
  journal={AAAI, 2020},
  year={2020}
}

@article{sprague2003multiple,
  title={Multiple-goal reinforcement learning with modular sarsa (0)},
  author={Sprague, Nathan and Ballard, Dana},
  year={2003}
}

@inproceedings{russell2003q,
  title={Q-decomposition for reinforcement learning agents},
  author={Russell, Stuart J and Zimdars, Andrew},
  booktitle={Proceedings of the 20th International Conference on Machine Learning (ICML-03)},
  pages={656--663},
  year={2003}
}

@inproceedings{barreto2018transfer,
  title={Transfer in deep reinforcement learning using successor features and generalised policy improvement},
  author={Barreto, Andre and Borsa, Diana and Quan, John and Schaul, Tom and Silver, David and Hessel, Matteo and Mankowitz, Daniel and Zidek, Augustin and Munos, Remi},
  booktitle={International Conference on Machine Learning},
  pages={501--510},
  year={2018},
  organization={PMLR}
}


@article{pugh2016quality,
  title={Quality diversity: A new frontier for evolutionary computation},
  author={Pugh, Justin K and Soros, Lisa B and Stanley, Kenneth O},
  journal={Frontiers in Robotics and AI},
  volume={3},
  pages={40},
  year={2016},
  publisher={Frontiers}
}

@article{mouret2015illuminating,
  title={Illuminating search spaces by mapping elites},
  author={Mouret, Jean-Baptiste and Clune, Jeff},
  journal={arXiv preprint arXiv:1504.04909},
  year={2015}
}

@article{zheng2018learning,
  title={On learning intrinsic rewards for policy gradient methods},
  author={Zheng, Zeyu and Oh, Junhyuk and Singh, Satinder},
  journal={arXiv preprint arXiv:1804.06459},
  year={2018}
}

@article{zahavy2020self,
  title={A self-tuning actor-critic algorithm},
  author={Zahavy, Tom and Xu, Zhongwen and Veeriah, Vivek and Hessel, Matteo and Oh, Junhyuk and van Hasselt, Hado P and Silver, David and Singh, Satinder},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{xu2018meta,
  title={Meta-gradient reinforcement learning},
  author={Xu, Zhongwen and van Hasselt, Hado and Silver, David},
  journal={arXiv preprint arXiv:1805.09801},
  year={2018}
}



@article{grimm2019disentangled,
  title={Disentangled Cumulants Help Successor Representations Transfer to New Tasks},
  author={Grimm, Christopher and Higgins, Irina and Barreto, Andre and Teplyashin, Denis and Wulfmeier, Markus and Hertweck, Tim and Hadsell, Raia and Singh, Satinder},
  journal={arXiv preprint arXiv:1911.10866},
  year={2019}
}

@article{gregor2016variational,
  title={Variational intrinsic control},
  author={Gregor, Karol and Rezende, Danilo Jimenez and Wierstra, Daan},
  journal={International Conference on Learning Representations, Workshop Track},
  year={2017},
  url={https://openreview.net/forum?id=Skc-Fo4Yg}
}

@inproceedings{o2020making,
  title={Making sense of reinforcement learning and probabilistic inference},
  author={O'Donoghue, Brendan and Osband, Ian and Ionescu, Catalin},
  booktitle={International Conference on Learning Representations},
  year={2020}
}


@inproceedings{eysenbach2018diversity,
title={Diversity is All You Need: Learning Skills without a Reward Function},
author={Benjamin Eysenbach and Abhishek Gupta and Julian Ibarz and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SJx63jRqFm},
}


@inproceedings{
hansen2019fast,
title={Fast Task Inference with Variational Intrinsic Successor Features},
author={Steven Hansen and Will Dabney and Andre Barreto and David Warde-Farley and Tom Van de Wiele and Volodymyr Mnih},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJeAHkrYDS}
}




@article{sutton99between,
  author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
  title = {Between {MDPs} and semi-{MDPs}: a framework for temporal abstraction
in reinforcement learning},
 journal = {Artificial Intelligence},
 volume = {112},
 issue = {1-2},
 month = {August},
 year = {1999},
 pages = {181--211},
 doi = {http://dx.doi.org/10.1016/S0004-3702(99)00052-1},
 publisher = {Elsevier Science Publishers Ltd.},
 address = {Essex, UK}
 } 

 
 @Article{dietterich2000hierarchical,
  author = {T. G. Dietterich},
  title = {Hierarchical Reinforcement Learning with the {MAXQ} Value
  Function Decomposition},
  journal = {Journal of Artificial Intelligence Research},
  year = {2000},
  volume = {13},
  pages = {227--303}
}


@article{barto2003recent,
 author = {Barto, Andrew G. and Mahadevan, Sridhar},
 title = {Recent Advances in Hierarchical Reinforcement Learning},
 journal = {Discrete Event Dynamic Systems},
 volume = {13},
 number = {1-2},
 year = {2003},
 pages = {41--77},
 publisher = {Kluwer Academic Publishers}
} 

@article{nilim2005robust,
  title={Robust control of Markov decision processes with uncertain transition matrices},
  author={Nilim, Arnab and El Ghaoui, Laurent},
  journal={Operations Research},
  volume={53},
  number={5},
  pages={780--798},
  year={2005},
  publisher={INFORMS}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{sutton1999between,
  title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1-2},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}

@article{xu2012robustness,
  title={Robustness and generalization},
  author={Xu, Huan and Mannor, Shie},
  journal={Machine learning},
  volume={86},
  number={3},
  pages={391--423},
  year={2012},
  publisher={Springer}
}

@inproceedings{singh1995reinforcement,
  title={Reinforcement learning with soft state aggregation},
  author={Singh, Satinder P and Jaakkola, Tommi and Jordan, Michael I},
  booktitle={Advances in neural information processing systems},
  pages={361--368},
  year={1995}
}


@article{el1997robust,
  title={Robust solutions to least-squares problems with uncertain data},
  author={El Ghaoui, Laurent and Lebret, Herv{\'e}},
  journal={SIAM Journal on matrix analysis and applications},
  volume={18},
  number={4},
  pages={1035--1064},
  year={1997},
  publisher={SIAM}
}


@inproceedings{xu2009robust,
  title={Robust regression and lasso},
  author={Xu, Huan and Caramanis, Constantine and Mannor, Shie},
  booktitle={Advances in neural information processing systems},
  pages={1801--1808},
  year={2009}
}

@inproceedings{chow2015risk,
  title={Risk-sensitive and robust decision-making: a cvar optimization approach},
  author={Chow, Yinlam and Tamar, Aviv and Mannor, Shie and Pavone, Marco},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1522--1530},
  year={2015}
}



@article{matusch2020evaluating,
  title={Evaluating Agents without Rewards},
  author={Matusch, Brendon and Ba, Jimmy and Hafner, Danijar},
  journal={arXiv preprint arXiv:2012.11538},
  year={2020}
}

@article{simon1991bounded,
  title={Bounded rationality and organizational learning},
  author={Simon, Herbert A},
  journal={Organization science},
  volume={2},
  number={1},
  pages={125--134},
  year={1991},
  publisher={INFORMS}
}

@book{watkins2004fundamentals,
  title={Fundamentals of matrix computations},
  author={Watkins, David S},
  volume={64},
  year={2004},
  publisher={John Wiley \& Sons}
}


@article{baumli2020relative,
  title={Relative Variational Intrinsic Control},
  author={Baumli, Kate and Warde-Farley, David and Hansen, Steven and Mnih, Volodymyr},
  journal={arXiv preprint arXiv:2012.07827},
  year={2020}
}


@article{kumar2020one,
  title={One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL},
  author={Kumar, Saurabh and Kumar, Aviral and Levine, Sergey and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}



@inproceedings{
zahavy2021discovering,
title={Discovering a set of policies for the worst case reward},
author={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=PUkhWz65dy5}
}
 
 @book{altman1999constrained,
  title={Constrained Markov decision processes},
  author={Altman, Eitan},
  volume={7},
  year={1999},
  publisher={CRC Press}
}


@inproceedings{
tessler2018reward,
title={Reward Constrained Policy Optimization},
author={Chen Tessler and Daniel J. Mankowitz and Shie Mannor},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SkfrvsA9FX},
}



@article{borkar2005actor,
  title={An actor-critic algorithm for constrained Markov decision processes},
  author={Borkar, Vivek S},
  journal={Systems \& control letters},
  volume={54},
  number={3},
  pages={207--213},
  year={2005},
  publisher={Elsevier}
}


@article{dayan1993improving,
  title={Improving generalization for temporal difference learning: The successor representation},
  author={Dayan, Peter},
  journal={Neural Computation},
  volume={5},
  number={4},
  pages={613--624},
  year={1993},
  publisher={MIT Press}
}

@article{bhatnagar2012online,
  title={An online actor--critic algorithm with function approximation for constrained markov decision processes},
  author={Bhatnagar, Shalabh and Lakshmanan, K},
  journal={Journal of Optimization Theory and Applications},
  volume={153},
  number={3},
  pages={688--708},
  year={2012},
  publisher={Springer}
}

@inproceedings{
calian2020balancing,
title={Balancing Constraints and Rewards with Meta-Gradient D4PG},
author={Dan A. Calian and Daniel J Mankowitz and Tom Zahavy and Zhongwen Xu and Junhyuk Oh and Nir Levine and Timothy Mann},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=TQt98Ya7UMP}
}


@article{singh2010intrinsically,
  title={Intrinsically motivated reinforcement learning: An evolutionary perspective},
  author={Singh, Satinder and Lewis, Richard L and Barto, Andrew G and Sorg, Jonathan},
  journal={IEEE Transactions on Autonomous Mental Development},
  volume={2},
  number={2},
  pages={70--82},
  year={2010},
  publisher={IEEE}
}

@book{levin2017markov,
  title={Markov Chains and Mixing Times},
  author={Levin, D.A. and Peres, Y. and Wilmer, E.L.},
  year={2017},
  publisher={American Mathematical Society}
}

@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}

@inproceedings{hazan2019provably,
  title={Provably efficient maximum entropy exploration},
  author={Hazan, Elad and Kakade, Sham and Singh, Karan and Van Soest, Abby},
  booktitle={International Conference on Machine Learning},
  pages={2681--2691},
  year={2019},
  organization={PMLR}
}

@article{agarwal2020pc,
  title={PC-PG: Policy cover directed exploration for provable policy gradient learning},
  author={Agarwal, Alekh and Henaff, Mikael and Kakade, Sham and Sun, Wen},
  journal={arXiv preprint arXiv:2007.08459},
  year={2020}
}

@article{osborn1953applied,
  title={Applied imagination.},
  author={Osborn, Alex F},
  year={1953},
  publisher={Scribner's}
}

@misc{cmdpblog,
  title={Constrained MDPs and the reward hypothesis},
  author={Szepesv{\'a}ri, Csaba},
  year={2020},
  journal={Musings about machine learning and other things (blog)},
  url={https://readingsml.blogspot.com/2020/03/constrained-mdps-and-reward-hypothesis.html}
}

@misc{kasparov,
  title={Swapping Songs With Chess Grandmaster Garry Kasparov},
  author={Corinna da Fonseca-Wollheim},
  year={2020},
  journal={The New York Times},
  url={https://www.nytimes.com/2020/12/18/arts/music/garry-kasparov-classical-music.html}
}

@misc{chessgame,
  title={Game 8: Leko wins to take the lead},
  author={Rolf Behovits},
  year={2004},
  journal={Chess news},
  url={https://en.chessbase.com/post/game-8-leko-wins-to-take-the-lead}
}
@InProceedings{pmlr-v28-jaggi13, title = {Revisiting {Frank-Wolfe}: Projection-Free Sparse Convex Optimization}, author = {Martin Jaggi}, booktitle = {Proceedings of the 30th International Conference on Machine Learning}, pages = {427--435}, year = {2013}, editor = {Sanjoy Dasgupta and David McAllester}, volume = {28}, number = {1}, series = {Proceedings of Machine Learning Research}, address = {Atlanta, Georgia, USA}, month = {17--19 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v28/jaggi13.pdf}, url = {http://proceedings.mlr.press/v28/jaggi13.html}, abstract = {We provide stronger and more general primal-dual convergence results for Frank-Wolfe-type algorithms (a.k.a. conditional gradient) for constrained convex optimization, enabled by a simple framework of duality gap certificates. Our analysis also holds if the linear subproblems are only solved approximately (as well as if the gradients are inexact), and is proven to be worst-case optimal in the sparsity of the obtained solutions. On the application side, this allows us to unify a large variety of existing sparse greedy methods, in particular for optimization over convex hulls of an atomic set, even if those sets can only be approximated, including sparse (or structured sparse) vectors or matrices, low-rank matrices, permutation matrices, or max-norm bounded matrices. We present a new general framework for convex optimization over matrix factorizations, where every Frank-Wolfe iteration will consist of a low-rank update, and discuss the broad application areas of this approach.} }.

@article{jaggi2015global,
  title={On the global linear convergence of frank-wolfe optimization variants},
  author={Jaggi, Martin and Lacoste-Julien, Simon},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}

@article{bojun2020steady,
  title={Steady State Analysis of Episodic Reinforcement Learning},
  author={Bojun, Huang},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{shani2021online,
  title={Online Apprenticeship Learning},
  author={Shani, Lior and Zahavy, Tom and Mannor, Shie},
  journal={arXiv preprint arXiv:2102.06924},
  year={2021}
}

@inproceedings{abernethy2017frankwolfe,
 author = {Abernethy, Jacob D and Wang, Jun-Kun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On Frank-Wolfe and Equilibrium Computation},
 url = {https://proceedings.neurips.cc/paper/2017/file/7371364b3d72ac9a3ed8638e6f0be2c9-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{yang2020off,
  title={Off-policy evaluation via the regularized lagrangian},
  author={Yang, Mengjiao and Nachum, Ofir and Dai, Bo and Li, Lihong and Schuurmans, Dale},
  journal={arXiv preprint arXiv:2007.03438},
  year={2020}
}

@article{nachum2019dualdice,
  title={Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections},
  author={Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
  journal={arXiv preprint arXiv:1906.04733},
  year={2019}
}

@inproceedings{NEURIPS2019_a02ffd91,
 author = {Cheung, Wang Chi},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Regret Minimization for Reinforcement Learning with Vectorial Feedback and Complex Objectives},
 url = {https://proceedings.neurips.cc/paper/2019/file/a02ffd91ece5e7efeb46db8f10a74059-Paper.pdf},
 volume = {32},
 year = {2019}
}



@article{zhang2020variational,
  title={Variational policy gradient method for reinforcement learning with general utilities},
  author={Zhang, Junyu and Koppel, Alec and Bedi, Amrit Singh and Szepesvari, Csaba and Wang, Mengdi},
  journal={arXiv preprint arXiv:2007.02151},
  year={2020}
}

@article{ho2016generative,
  title={Generative adversarial imitation learning},
  author={Ho, Jonathan and Ermon, Stefano},
  journal={arXiv preprint arXiv:1606.03476},
  year={2016}
}


@inproceedings{cai2020provably,
  title={Provably efficient exploration in policy optimization},
  author={Cai, Qi and Yang, Zhuoran and Jin, Chi and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={1283--1294},
  year={2020},
  organization={PMLR}
}

@inproceedings{shani2020optimistic,
  title={Optimistic policy optimization with bandit feedback},
  author={Shani, Lior and Efroni, Yonathan and Rosenberg, Aviv and Mannor, Shie},
  booktitle={International Conference on Machine Learning},
  pages={8604--8613},
  year={2020},
  organization={PMLR}
}

@inproceedings{geist2019theory,
  title={A theory of regularized markov decision processes},
  author={Geist, Matthieu and Scherrer, Bruno and Pietquin, Olivier},
  booktitle={International Conference on Machine Learning},
  pages={2160--2169},
  year={2019},
  organization={PMLR}
}

@inproceedings{shani2020adaptive,
  title={Adaptive trust region policy optimization: Global convergence and faster rates for regularized mdps},
  author={Shani, Lior and Efroni, Yonathan and Mannor, Shie},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={5668--5675},
  year={2020}
}

@inproceedings{agarwal2020optimality,
  title={Optimality and approximation with policy gradient methods in markov decision processes},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  booktitle={Conference on Learning Theory},
  pages={64--66},
  year={2020},
  organization={PMLR}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}

@article{hazan2007logarithmic,
  title={Logarithmic regret algorithms for online convex optimization},
  author={Hazan, Elad and Agarwal, Amit and Kale, Satyen},
  journal={Machine Learning},
  volume={69},
  number={2-3},
  pages={169--192},
  year={2007},
  publisher={Springer}
}

@inproceedings{zinkevich2003online,
  title={Online convex programming and generalized infinitesimal gradient ascent},
  author={Zinkevich, Martin},
  booktitle={Proceedings of the 20th international conference on machine learning (icml-03)},
  pages={928--936},
  year={2003}
}

@article{hazan2016introduction,
  title={Introduction to Online Convex Optimization},
  author={Hazan, Elad},
  journal={Foundations and Trends in Optimization},
  volume={2},
  number={3-4},
  pages={157--325},
  year={2016},
  publisher={Now Publishers Inc. Hanover, MA, USA}
}

@article{abdolmaleki2018maximum,
  title={Maximum a posteriori policy optimisation},
  author={Abdolmaleki, Abbas and Springenberg, Jost Tobias and Tassa, Yuval and Munos, Remi and Heess, Nicolas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1806.06920},
  year={2018}
}

@article{tomar2020mirror,
  title={Mirror descent policy optimization},
  author={Tomar, Manan and Shani, Lior and Efroni, Yonathan and Ghavamzadeh, Mohammad},
  journal={arXiv preprint arXiv:2005.09814},
  year={2020}
}

@article{xiao2019wasserstein,
  title={Wasserstein adversarial imitation learning},
  author={Xiao, Huang and Herman, Michael and Wagner, Joerg and Ziesche, Sebastian and Etesami, Jalal and Linh, Thai Hong},
  journal={arXiv preprint arXiv:1906.08113},
  year={2019}
}

@inproceedings{zhang2020wasserstein,
  title={Wasserstein Distance guided Adversarial Imitation Learning with Reward Shape Exploration},
  author={Zhang, Ming and Wang, Yawei and Ma, Xiaoteng and Xia, Li and Yang, Jun and Li, Zhiheng and Li, Xiu},
  booktitle={2020 IEEE 9th Data Driven Control and Learning Systems Conference (DDCLS)},
  pages={1165--1170},
  year={2020},
  organization={IEEE}
}

@inproceedings{rosenberg2019online,
  title={Online convex optimization in adversarial markov decision processes},
  author={Rosenberg, Aviv and Mansour, Yishay},
  booktitle={International Conference on Machine Learning},
  pages={5478--5486},
  year={2019},
  organization={PMLR}
}

@InProceedings{pmlr-v22-neu12, title = {The adversarial stochastic shortest path problem with unknown transition probabilities}, author = {Gergely Neu and Andras Gyorgy and Csaba Szepesvari}, booktitle = {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics}, pages = {805--813}, year = {2012}, editor = {Neil D. Lawrence and Mark Girolami}, volume = {22}, series = {Proceedings of Machine Learning Research}, address = {La Palma, Canary Islands}, month = {21--23 Apr}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v22/neu12/neu12.pdf}, url = {http://proceedings.mlr.press/v22/neu12.html}, abstract = {We consider online learning in a special class of episodic Markovian decision processes, namely, loop-free stochastic shortest path problems. In this problem, an agent has to traverse through a finite directed acyclic graph with random transitions while maximizing the obtained rewards along the way. We assume that the reward function can change arbitrarily between consecutive episodes, and is entirely revealed to the agent at the end of each episode. Previous work was concerned with the case when the stochastic dynamics is known ahead of time, whereas the main novelty of this paper is that this assumption is lifted. We propose an algorithm called “follow the perturbed optimistic policy” that combines ideas from the “follow the perturbed leader” method for online learning of arbitrary sequences and “upper confidence reinforcement learning”, an algorithm for regret minimization in Markovian decision processes (with a fixed reward function). We prove that the expected cumulative regret of our algorithm is of order L X A\sqrtT up to logarithmic factors, where L is the length of the longest path in the graph, \X is the state space, \A is the action space and T is the number of episodes. To our knowledge this is the first algorithm that learns and controls stochastic and adversarial components in an online fashion at the same time.} }

@article{miryoosefi2019reinforcement,
  title={Reinforcement learning with convex constraints},
  author={Miryoosefi, Sobhan and Brantley, Kiant{\'e} and Daum{\'e} III, Hal and Dud{\'\i}k, Miroslav and Schapire, Robert},
  journal={arXiv preprint arXiv:1906.09323},
  year={2019}
}


@article{efroni2020exploration,
  title={Exploration-exploitation in constrained mdps},
  author={Efroni, Yonathan and Mannor, Shie and Pirotta, Matteo},
  journal={arXiv preprint arXiv:2003.02189},
  year={2020}
}


@inproceedings{
calian2021balancing,
title={Balancing Constraints and Rewards with Meta-Gradient D4{\{}PG{\}}},
author={Dan A. Calian and Daniel J Mankowitz and Tom Zahavy and Zhongwen Xu and Junhyuk Oh and Nir Levine and Timothy Mann},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=TQt98Ya7UMP}
}

@article{belogolovsky2019inverse,
  title={Inverse Reinforcement Learning in Contextual MDPs},
  author={Belogolovsky, Stav and Korsunsky, Philip and Mannor, Shie and Tessler, Chen and Zahavy, Tom},
  journal={Machine Learning},
  year={2021},
  publisher={Springer}
}

@inproceedings{ho2016model,
  title={Model-free imitation learning with policy optimization},
  author={Ho, Jonathan and Gupta, Jayesh and Ermon, Stefano},
  booktitle={International Conference on Machine Learning},
  pages={2760--2769},
  year={2016},
  organization={PMLR}
}

@article{lee2019efficient,
  title={Efficient exploration via state marginal matching},
  author={Lee, Lisa and Eysenbach, Benjamin and Parisotto, Emilio and Xing, Eric and Levine, Sergey and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1906.05274},
  year={2019}
}


@InProceedings{impala18a,
  title = 	 {{IMPALA}: Scalable Distributed Deep-{RL} with Importance Weighted Actor-Learner Architectures},
  author =       {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Vlad and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  year = 	 {2018},
}

@misc{Tieleman2012,
  title={{Lecture 6.5---RmsProp: Divide the gradient by a running average of its recent magnitude}},
  author={Tieleman, T. and Hinton, G.},
  howpublished={COURSERA: Neural Networks for Machine Learning},
  year={2012}
}

@inproceedings{agrawal2014fast,
  title={Fast algorithms for online stochastic convex programming},
  author={Agrawal, Shipra and Devanur, Nikhil R},
  booktitle={Proceedings of the twenty-sixth annual ACM-SIAM symposium on Discrete algorithms},
  pages={1405--1424},
  year={2014},
  organization={SIAM}
}

@article{jin2021towards,
  title={Towards Tight Bounds on the Sample Complexity of Average-reward MDPs},
  author={Jin, Yujia and Sidford, Aaron},
  journal={arXiv preprint arXiv:2106.07046},
  year={2021}
}

@inproceedings{lattimore2012pac,
  title={PAC bounds for discounted MDPs},
  author={Lattimore, Tor and Hutter, Marcus},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={320--334},
  year={2012},
  organization={Springer}
}

@inproceedings{jin2020efficiently,
  title={Efficiently solving MDPs with stochastic mirror descent},
  author={Jin, Yujia and Sidford, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={4890--4900},
  year={2020},
  organization={PMLR}
}
@book{kullback1997information,
  title={Information theory and statistics},
  author={Kullback, Solomon},
  year={1997},
  publisher={Courier Corporation}
}
@article{bregman1967relaxation,
  title={The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming},
  author={Bregman, Lev M},
  journal={USSR computational mathematics and mathematical physics},
  volume={7},
  number={3},
  pages={200--217},
  year={1967},
  publisher={Elsevier}
}

@inproceedings{menard2021fast,
  title={Fast active learning for pure exploration in reinforcement learning},
  author={M{\'e}nard, Pierre and Domingues, Omar Darwiche and Jonsson, Anders and Kaufmann, Emilie and Leurent, Edouard and Valko, Michal},
  booktitle={International Conference on Machine Learning},
  pages={7599--7608},
  year={2021},
  organization={PMLR}
}

@inproceedings{kaufmann2021adaptive,
  title={Adaptive reward-free exploration},
  author={Kaufmann, Emilie and M{\'e}nard, Pierre and Domingues, Omar Darwiche and Jonsson, Anders and Leurent, Edouard and Valko, Michal},
  booktitle={Algorithmic Learning Theory},
  pages={865--891},
  year={2021},
  organization={PMLR}
}

@article{cesa2004generalization,
  title={On the generalization ability of on-line learning algorithms},
  author={Cesa-Bianchi, Nicolo and Conconi, Alex and Gentile, Claudio},
  journal={IEEE Transactions on Information Theory},
  volume={50},
  number={9},
  pages={2050--2057},
  year={2004},
  publisher={IEEE}
}

@article{geist2021concave,
  title={Concave Utility Reinforcement Learning: the Mean-field Game viewpoint},
  author={Geist, Matthieu and P{\'e}rolat, Julien and Lauri{\`e}re, Mathieu and Elie, Romuald and Perrin, Sarah and Bachem, Olivier and Munos, R{\'e}mi and Pietquin, Olivier},
  journal={arXiv preprint arXiv:2106.03787},
  year={2021}
}

@article{strehl2008analysis,
  title={An analysis of model-based interval estimation for Markov decision processes},
  author={Strehl, Alexander L and Littman, Michael L},
  journal={Journal of Computer and System Sciences},
  volume={74},
  number={8},
  pages={1309--1331},
  year={2008},
  publisher={Elsevier}
}

@inproceedings{huang2016following,
  title={Following the leader and fast rates in linear prediction: Curved constraint sets and other regularities},
  author={Huang, Ruitong and Lattimore, Tor and Gy{\"o}rgy, Andr{\'a}s and Szepesv{\'a}ri, Csaba},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4970--4978},
  year={2016}
}

@inproceedings{nemirovskij1983problem,
  title={Problem complexity and method efficiency in optimization},
  author={Nemirovskij, Arkadij Semenovi{\v{c}} and Yudin, David Borisovich},
  year={1983},
  booktitle={Wiley-Interscience}
}

@article{freund1997decision,
  title={A decision-theoretic generalization of on-line learning and an application to boosting},
  author={Freund, Yoav and Schapire, Robert E},
  journal={Journal of computer and system sciences},
  volume={55},
  number={1},
  pages={119--139},
  year={1997},
  publisher={Elsevier}
}

@inproceedings{mcmahan2011follow,
  title={Follow-the-regularized-leader and mirror descent: Equivalence theorems and l1 regularization},
  author={McMahan, Brendan},
  booktitle={Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages={525--533},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{hessel2021podracer,
  title={Podracer architectures for scalable Reinforcement Learning},
  author={Hessel, Matteo and Kroiss, Manuel and Clark, Aidan and Kemaev, Iurii and Quan, John and Keck, Thomas and Viola, Fabio and van Hasselt, Hado},
  journal={arXiv preprint arXiv:2104.06272},
  year={2021}
}