\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Asi and Duchi(2019)]{asi2019stochastic}
Hilal Asi and John~C. Duchi.
\newblock Stochastic (approximate) proximal point methods: Convergence,
  optimality, and adaptivity.
\newblock \emph{{SIAM} J. Optim.}, 29\penalty0 (3):\penalty0 2257--2290, 2019.

\bibitem[Asi et~al.(2020)Asi, Chadha, Cheng, and Duchi]{asi2020minibatch}
Hilal Asi, Karan~N. Chadha, Gary Cheng, and John~C. Duchi.
\newblock Minibatch stochastic approximate proximal point methods.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, virtual, 2020.

\bibitem[Bassily et~al.(2020)Bassily, Feldman, Guzm{\'{a}}n, and
  Talwar]{bassily2020stability}
Raef Bassily, Vitaly Feldman, Crist{\'{o}}bal Guzm{\'{a}}n, and Kunal Talwar.
\newblock Stability of stochastic gradient descent on nonsmooth convex losses.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, virtual, 2020.

\bibitem[Bhowmick et~al.(2018)Bhowmick, Duchi, Freudiger, Kapoor, and
  Rogers]{bhowmick2018protection}
Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, and Ryan
  Rogers.
\newblock Protection against reconstruction and its applications in private
  federated learning.
\newblock \emph{arXiv preprint arXiv:1812.00984}, 2018.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet2002stability}
Olivier Bousquet and Andr{\'{e}} Elisseeff.
\newblock Stability and generalization.
\newblock \emph{J. Mach. Learn. Res.}, 2:\penalty0 499--526, 2002.

\bibitem[Bousquet et~al.(2020)Bousquet, Klochkov, and
  Zhivotovskiy]{bousquet2020sharper}
Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy.
\newblock Sharper bounds for uniformly stable algorithms.
\newblock In \emph{Proceedings of the Conference on Learning Theory (COLT)},
  pages 610--626, Virtual Event [Graz, Austria], 2020.

\bibitem[Chen et~al.(2020)Chen, Li, and Li]{chen2020toward}
Xiangyi Chen, Xiaoyun Li, and Ping Li.
\newblock Toward communication efficient adaptive gradient method.
\newblock In \emph{Proceedings of the {ACM-IMS} Foundations of Data Science
  Conference (FODS)}, pages 119--128, Virtual Event, 2020.

\bibitem[Cohen et~al.(2017)Cohen, Afshar, Tapson, and van
  Schaik]{cohen2017emnist}
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr{\'{e}} van Schaik.
\newblock {EMNIST:} extending {MNIST} to handwritten letters.
\newblock In \emph{Proceedings of the 2017 International Joint Conference on
  Neural Networks (IJCNN)}, pages 2921--2926, Anchorage, AK, 2017.

\bibitem[Davis and Drusvyatskiy(2019)]{davis2019stochastic}
Damek Davis and Dmitriy Drusvyatskiy.
\newblock Stochastic model-based minimization of weakly convex functions.
\newblock \emph{{SIAM} J. Optim.}, 29\penalty0 (1):\penalty0 207--239, 2019.

\bibitem[Deng and Gao(2021)]{deng2021minibatch}
Qi~Deng and Wenzhi Gao.
\newblock Minibatch and momentum model-based methods for stochastic weakly
  convex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 23115--23127, virtual, 2021.

\bibitem[Donevski et~al.(2021)Donevski, Nielsen, and
  Popovski]{donevski2021addressing}
Igor Donevski, Jimmy~Jessen Nielsen, and Petar Popovski.
\newblock On addressing heterogeneity in federated learning for autonomous
  vehicles connected to a drone orchestrator.
\newblock \emph{arXiv preprint arXiv:2108.02712}, 2021.

\bibitem[Elisseeff et~al.(2005)Elisseeff, Evgeniou, and
  Pontil]{elisseeff2005stability}
Andr{\'{e}} Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil.
\newblock Stability of randomized learning algorithms.
\newblock \emph{J. Mach. Learn. Res.}, 6:\penalty0 55--79, 2005.

\bibitem[Feldman and Vondr{\'{a}}k(2018)]{feldman2018generalization}
Vitaly Feldman and Jan Vondr{\'{a}}k.
\newblock Generalization bounds for uniformly stable algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 9770--9780, Montr{\'{e}}al, Canada, 2018.

\bibitem[Feldman and Vondr{\'{a}}k(2019)]{feldman2019high}
Vitaly Feldman and Jan Vondr{\'{a}}k.
\newblock High probability generalization bounds for uniformly stable
  algorithms with nearly optimal rate.
\newblock In \emph{Proceedings of the Conference on Learning Theory (COLT)},
  pages 1270--1279, Phoenix, AZ, 2019.

\bibitem[Go et~al.(2009)Go, Bhayani, and Huang]{go2009twitter}
Alec Go, Richa Bhayani, and Lei Huang.
\newblock Twitter sentiment classification using distant supervision.
\newblock \emph{CS224N project report, Stanford}, 1\penalty0 (12):\penalty0
  2009, 2009.

\bibitem[Hard et~al.(2020)Hard, Partridge, Nguyen, Subrahmanya, Shah, Zhu,
  Lopez{-}Moreno, and Mathews]{hard2020training}
Andrew Hard, Kurt Partridge, Cameron Nguyen, Niranjan Subrahmanya, Aishanee
  Shah, Pai Zhu, Ignacio Lopez{-}Moreno, and Rajiv Mathews.
\newblock Training keyword spotting models on non-iid data with federated
  learning.
\newblock In \emph{Proceedings of the 21st Annual Conference of the
  International Speech Communication Association (Interspeech)}, pages
  4343--4347, Virtual Event, Shanghai, China, 2020.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
Moritz Hardt, Ben Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{Proceedings of the 33nd International Conference on Machine
  Learning (ICML)}, pages 1225--1234, New York City, NY, 2016.

\bibitem[He et~al.(2021)He, Shah, Tang, Sivashunmugam, Bhogaraju, Shimpi, Shen,
  Chu, Soltanolkotabi, and Avestimehr]{he2021fedcv}
Chaoyang He, Alay~Dilipbhai Shah, Zhenheng Tang, Di~Fan1Adarshan~Naiynar
  Sivashunmugam, Keerti Bhogaraju, Mita Shimpi, Li~Shen, Xiaowen Chu, Mahdi
  Soltanolkotabi, and Salman Avestimehr.
\newblock {FedCV}: A federated learning framework for diverse computer vision
  tasks.
\newblock \emph{arXiv preprint arXiv:2111.11066}, 2021.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2020scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank~J. Reddi,
  Sebastian~U. Stich, and Ananda~Theertha Suresh.
\newblock {SCAFFOLD:} stochastic controlled averaging for federated learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (ICML)}, pages 5132--5143, Virtual Event, 2020.

\bibitem[Khaled et~al.(2020)Khaled, Mishchenko, and
  Richt{\'{a}}rik]{khaled2020tighter}
Ahmed Khaled, Konstantin Mishchenko, and Peter Richt{\'{a}}rik.
\newblock Tighter theory for local {SGD} on identical and heterogeneous data.
\newblock In \emph{Proceedings of the 23rd International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pages 4519--4529, Online
  [Palermo, Sicily, Italy], 2020.

\bibitem[Khanduri et~al.(2021)Khanduri, Sharma, Yang, Hong, Liu, Rajawat, and
  Varshney]{khanduri2021stem}
Prashant Khanduri, Pranay Sharma, Haibo Yang, Mingyi Hong, Jia Liu, Ketan
  Rajawat, and Pramod~K. Varshney.
\newblock {STEM:} {A} stochastic two-sided momentum algorithm achieving
  near-optimal sample and communication complexities for federated learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 6050--6061, virtual, 2021.

\bibitem[Klochkov and Zhivotovskiy(2021)]{klochkov2021stability}
Yegor Klochkov and Nikita Zhivotovskiy.
\newblock Stability and deviation optimal risk bounds with convergence rate
  $o(1/n)$.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 5065--5076, virtual, 2021.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, McMahan, Ramage, and
  Richt{\'a}rik]{konevcny2016federated}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Daniel Ramage, and Peter
  Richt{\'a}rik.
\newblock Federated optimization: Distributed machine learning for on-device
  intelligence.
\newblock \emph{arXiv preprint arXiv:1610.02527}, 2016.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'{e}}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proc. {IEEE}}, 86\penalty0 (11):\penalty0 2278--2324, 1998.

\bibitem[Lei and Ying(2020)]{lei2020fine}
Yunwen Lei and Yiming Ying.
\newblock Fine-grained analysis of stability and generalization for stochastic
  gradient descent.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (ICML)}, pages 5809--5819, Virtual Event, 2020.

\bibitem[Li et~al.(2014)Li, Zhang, Chen, and Smola]{li2014efficient}
Mu~Li, Tong Zhang, Yuqiang Chen, and Alexander~J. Smola.
\newblock Efficient mini-batch training for stochastic optimization.
\newblock In \emph{Proceedings of the 20th {ACM} {SIGKDD} International
  Conference on Knowledge Discovery and Data Mining (KDD)}, pages 661--670, New
  York, NY, 2014.

\bibitem[Li et~al.(2019)Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2019feddane}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
  Virginia Smith.
\newblock {FedDANE}: {A} federated newton-type method.
\newblock In \emph{Proceedings of the 53rd Asilomar Conference on Signals,
  Systems, and Computers (Asilomar)}, pages 1227--1231, 2019.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Sahu, Talwalkar, and
  Smith]{li2020federated}
Tian Li, Anit~Kumar Sahu, Ameet Talwalkar, and Virginia Smith.
\newblock Federated learning: Challenges, methods, and future directions.
\newblock \emph{{IEEE} Signal Process. Mag.}, 37\penalty0 (3):\penalty0 50--60,
  2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2020federatedprox}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
  Virginia Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock In \emph{Proceedings of Machine Learning and Systems (MLSys)},
  Austin, TX, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2020{\natexlab{c}})Li, Huang, Yang, Wang, and
  Zhang]{li2020convergence}
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.
\newblock On the convergence of {FedAvg} on non-iid data.
\newblock In \emph{Proceedings of the 8th International Conference on Learning
  Representations (ICLR)}, Addis Ababa, Ethiopia, 2020{\natexlab{c}}.

\bibitem[Li et~al.(2022)Li, Karimi, and Li]{li2022distributed}
Xiaoyun Li, Belhal Karimi, and Ping Li.
\newblock On distributed adaptive optimization with gradient compression.
\newblock In \emph{Proceedings of the 10th International Conference on Learning
  Representations (ICLR)}, 2022.

\bibitem[Liang et~al.(2019)Liang, Shen, Liu, Pan, Chen, and
  Cheng]{liang2019variance}
Xianfeng Liang, Shuheng Shen, Jingchang Liu, Zhen Pan, Enhong Chen, and Yifei
  Cheng.
\newblock Variance reduced local sgd with lower communication complexity.
\newblock \emph{arXiv preprint arXiv:1912.12844}, 2019.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
  Blaise~Ag{\"{u}}era y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pages 1273--1282, Fort
  Lauderdale, FL, 2017.

\bibitem[Mukherjee et~al.(2006)Mukherjee, Niyogi, Poggio, and
  Rifkin]{mukherjee2006learning}
Sayan Mukherjee, Partha Niyogi, Tomaso~A. Poggio, and Ryan~M. Rifkin.
\newblock Learning theory: stability is sufficient for generalization and
  necessary and sufficient for consistency of empirical risk minimization.
\newblock \emph{Adv. Comput. Math.}, 25\penalty0 (1-3):\penalty0 161--193,
  2006.

\bibitem[Nguyen et~al.(2020)Nguyen, Sehwag, Hosseinalipour, Brinton, Chiang,
  and Poor]{nguyen2020fast}
Hung~T Nguyen, Vikash Sehwag, Seyyedali Hosseinalipour, Christopher~G Brinton,
  Mung Chiang, and H~Vincent Poor.
\newblock Fast-convergent federated learning.
\newblock \emph{IEEE Journal on Selected Areas in Communications}, 39\penalty0
  (1):\penalty0 201--218, 2020.

\bibitem[Pathak and Wainwright(2020)]{pathak2020fedsplit}
Reese Pathak and Martin~J. Wainwright.
\newblock {FedSplit}: an algorithmic framework for fast federated optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, virtual, 2020.

\bibitem[Pennington et~al.(2014)Pennington, Socher, and
  Manning]{pennington2014glove}
Jeffrey Pennington, Richard Socher, and Christopher~D. Manning.
\newblock Glove: Global vectors for word representation.
\newblock In \emph{Proceedings of the 2014 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 1532--1543, Doha, Qatar, 2014.

\bibitem[Reddi et~al.(2021)Reddi, Charles, Zaheer, Garrett, Rush,
  Kone{\v{c}}n{\'y}, Kumar, and McMahan]{reddi2021adaptive}
Sashank~J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
  Jakub Kone{\v{c}}n{\'y}, Sanjiv Kumar, and Hugh~Brendan McMahan.
\newblock Adaptive federated optimization.
\newblock In \emph{Proceedings of the 9th International Conference on Learning
  Representations (ICLR)}, Virtual Event, Austria, 2021.

\bibitem[Rivasplata et~al.(2018)Rivasplata, Szepesv{\'{a}}ri, Shawe{-}Taylor,
  Parrado{-}Hern{\'{a}}ndez, and Sun]{rivasplata2018pac}
Omar Rivasplata, Csaba Szepesv{\'{a}}ri, John Shawe{-}Taylor, Emilio
  Parrado{-}Hern{\'{a}}ndez, and Shiliang Sun.
\newblock Pac-bayes bounds for stable algorithms with instance-dependent
  priors.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 9234--9244, Montr{\'{e}}al, Canada, 2018.

\bibitem[Shalev{-}Shwartz et~al.(2010)Shalev{-}Shwartz, Shamir, Srebro, and
  Sridharan]{shalev2010learnability}
Shai Shalev{-}Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan.
\newblock Learnability, stability and uniform convergence.
\newblock \emph{J. Mach. Learn. Res.}, 11:\penalty0 2635--2670, 2010.

\bibitem[Stich(2019)]{stich2019local}
Sebastian~U. Stich.
\newblock Local {SGD} converges fast and communicates little.
\newblock In \emph{Proceedings of the 7th International Conference on Learning
  Representations (ICLR)}, New Orleans, LA, 2019.

\bibitem[Stich and Karimireddy(2020)]{stich2020error}
Sebastian~U Stich and Sai~Praneeth Karimireddy.
\newblock The error-feedback framework: Better rates for sgd with delayed
  gradients and compressed updates.
\newblock \emph{J. Mach. Learn. Res.}, 21:\penalty0 237:1--36, 2020.

\bibitem[Tong et~al.(2020)Tong, Liang, Zhu, and Bi]{tong2020federated}
Qianqian Tong, Guannan Liang, Tan Zhu, and Jinbo Bi.
\newblock Federated nonconvex sparse learning.
\newblock \emph{arXiv preprint arXiv:2101.00052}, 2020.

\bibitem[Wang et~al.(2017)Wang, Wang, and Srebro]{wang2017memory}
Jialei Wang, Weiran Wang, and Nathan Srebro.
\newblock Memory and communication efficient distributed stochastic
  optimization with minibatch prox.
\newblock In \emph{Proceedings of the 30th Conference on Learning Theory
  (COLT)}, pages 1882--1919, Amsterdam, The Netherlands, 2017.

\bibitem[Woodworth et~al.(2020)Woodworth, Patel, and
  Srebro]{woodworth2020minibatch}
Blake~E. Woodworth, Kumar~Kshitij Patel, and Nati Srebro.
\newblock Minibatch vs local {SGD} for heterogeneous distributed learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, virtual, 2020.

\bibitem[Yang et~al.(2019)Yang, Liu, Chen, and Tong]{yang2019federated}
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.
\newblock Federated machine learning: Concept and applications.
\newblock \emph{{ACM} Trans. Intell. Syst. Technol.}, 10\penalty0 (2):\penalty0
  12:1--12:19, 2019.

\bibitem[Yu et~al.(2019)Yu, Yang, and Zhu]{yu2019parallel}
Hao Yu, Sen Yang, and Shenghuo Zhu.
\newblock Parallel restarted {SGD} with faster convergence and less
  communication: Demystifying why model averaging works for deep learning.
\newblock In \emph{Proceedings of the Thirty-Third {AAAI} Conference on
  Artificial Intelligence (AAAI)}, pages 5693--5700, Honolulu, HI, 2019.

\bibitem[Yuan et~al.(2021)Yuan, Zaheer, and Reddi]{yuan2021federated}
Honglin Yuan, Manzil Zaheer, and Sashank~J. Reddi.
\newblock Federated composite optimization.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning (ICML)}, pages 12253--12266, Virtual Event, 2021.

\bibitem[Zhang(2003)]{zhang2003leave}
Tong Zhang.
\newblock Leave-one-out bounds for kernel methods.
\newblock \emph{Neural Comput.}, 15\penalty0 (6):\penalty0 1397--1437, 2003.

\bibitem[Zhang et~al.(2020)Zhang, Hong, Dhople, Yin, and Liu]{zhang2020fedpd}
Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu.
\newblock {FedPD}: A federated learning framework with optimal rates and
  adaptivity to non-iid data.
\newblock \emph{arXiv preprint arXiv:2005.11418}, 2020.

\end{thebibliography}
