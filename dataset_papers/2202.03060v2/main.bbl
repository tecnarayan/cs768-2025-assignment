\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akshay et~al.(2013)Akshay, Bertrand, Haddad, and
  Helouet]{akshay2013steady}
Akshay, S., Bertrand, N., Haddad, S., and Helouet, L.
\newblock The steady-state control problem for {M}arkov decision processes.
\newblock In \emph{International Conference on Quantitative Evaluation of
  Systems}, 2013.

\bibitem[Arora \& Barak(2009)Arora and Barak]{arora2009complexity}
Arora, S. and Barak, B.
\newblock \emph{Computational complexity: a modern approach}.
\newblock Cambridge University Press, 2009.

\bibitem[Astrom(1965)]{astrom1965optimal}
Astrom, K.~J.
\newblock Optimal control of {M}arkov decision processes with incomplete state
  estimation.
\newblock \emph{Journal Mathematical Analysis and Applications}, 1965.

\bibitem[Bertsekas \& Tsitsiklis(2002)Bertsekas and
  Tsitsiklis]{bertsekas2002introduction}
Bertsekas, D.~P. and Tsitsiklis, J.~N.
\newblock \emph{Introduction to probability}.
\newblock Athena Scientific Belmont, MA, 2002.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020gpt}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
  D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
  S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
  I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Campos et~al.(2021)Campos, Sprechmann, Hansen, Barreto, Kapturowski,
  Vitvitskyi, Badia, and Blundell]{campos2021coverage}
Campos, V., Sprechmann, P., Hansen, S., Barreto, A., Kapturowski, S.,
  Vitvitskyi, A., Badia, A.~P., and Blundell, C.
\newblock Coverage as a principle for discovering transferable behavior in
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2102.13515}, 2021.

\bibitem[Cheung(2019{\natexlab{a}})]{cheung2019exploration}
Cheung, W.~C.
\newblock Exploration-exploitation trade-off in reinforcement learning on
  online {M}arkov decision processes with global concave rewards.
\newblock \emph{arXiv preprint arXiv:1905.06466}, 2019{\natexlab{a}}.

\bibitem[Cheung(2019{\natexlab{b}})]{cheung2019regret}
Cheung, W.~C.
\newblock Regret minimization for reinforcement learning with vectorial
  feedback and complex objectives.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2019{\natexlab{b}}.

\bibitem[Deisenroth et~al.(2013)Deisenroth, Neumann, Peters,
  et~al.]{deisenroth2013survey}
Deisenroth, M.~P., Neumann, G., Peters, J., et~al.
\newblock A survey on policy search for robotics.
\newblock \emph{Foundations and Trends in Robotics}, 2013.

\bibitem[Erhan et~al.(2009)Erhan, Manzagol, Bengio, Bengio, and
  Vincent]{erhan2009difficulty}
Erhan, D., Manzagol, P.-A., Bengio, Y., Bengio, S., and Vincent, P.
\newblock The difficulty of training deep architectures and the effect of
  unsupervised pre-training.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, 2009.

\bibitem[Erhan et~al.(2010)Erhan, Courville, Bengio, and
  Vincent]{erhan2010does}
Erhan, D., Courville, A., Bengio, Y., and Vincent, P.
\newblock Why does unsupervised pre-training help deep learning?
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, 2010.

\bibitem[Guo et~al.(2021)Guo, Azar, Saade, Thakoor, Piot, Pires, Valko,
  Mesnard, Lattimore, and Munos]{guo2021geometric}
Guo, Z.~D., Azar, M.~G., Saade, A., Thakoor, S., Piot, B., Pires, B.~A., Valko,
  M., Mesnard, T., Lattimore, T., and Munos, R.
\newblock Geometric entropic exploration.
\newblock \emph{arXiv preprint arXiv:2101.02055}, 2021.

\bibitem[Hallak et~al.(2015)Hallak, Di~Castro, and
  Mannor]{hallak2015contextual}
Hallak, A., Di~Castro, D., and Mannor, S.
\newblock Contextual {M}arkov decision processes.
\newblock \emph{arXiv preprint arXiv:1502.02259}, 2015.

\bibitem[Hazan et~al.(2019)Hazan, Kakade, Singh, and
  Van~Soest]{hazan2019maxent}
Hazan, E., Kakade, S., Singh, K., and Van~Soest, A.
\newblock Provably efficient maximum entropy exploration.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2019.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997lstm}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural Computation}, 1997.

\bibitem[Jin et~al.(2020)Jin, Krishnamurthy, Simchowitz, and Yu]{jin2020reward}
Jin, C., Krishnamurthy, A., Simchowitz, M., and Yu, T.
\newblock Reward-free exploration for reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2020.

\bibitem[Kaelbling et~al.(1998)Kaelbling, Littman, and
  Cassandra]{kaelbling1998planning}
Kaelbling, L.~P., Littman, M.~L., and Cassandra, A.~R.
\newblock Planning and acting in partially observable stochastic domains.
\newblock \emph{Artificial Intelligence}, 1998.

\bibitem[Kocsis \& Szepesv{\'a}ri(2006)Kocsis and
  Szepesv{\'a}ri]{kocsis2006bandit}
Kocsis, L. and Szepesv{\'a}ri, C.
\newblock Bandit based {M}onte-{C}arlo planning.
\newblock In \emph{European Conference on Machine Learning}, 2006.

\bibitem[Kwon et~al.(2021)Kwon, Efroni, Caramanis, and Mannor]{kwon2021latent}
Kwon, J., Efroni, Y., Caramanis, C., and Mannor, S.
\newblock {RL} for latent {MDP}s: Regret guarantees and a lower bound.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Laroche et~al.(2022)Laroche, Combes, and Buckman]{laroche2022non}
Laroche, R., Combes, R. T.~d., and Buckman, J.
\newblock Non-{M}arkovian policies occupancy measures.
\newblock \emph{arXiv preprint arXiv:2205.13950}, 2022.

\bibitem[Laskin et~al.(2021)Laskin, Yarats, Liu, Lee, Zhan, Lu, Cang, Pinto,
  and Abbeel]{laskin2021urlb}
Laskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang, C., Pinto,
  L., and Abbeel, P.
\newblock {URLB}: Unsupervised reinforcement learning benchmark.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}, 2021.

\bibitem[Lee et~al.(2019)Lee, Eysenbach, Parisotto, Xing, Levine, and
  Salakhutdinov]{lee2019smm}
Lee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine, S., and Salakhutdinov,
  R.
\newblock Efficient exploration via state marginal matching.
\newblock \emph{arXiv preprint arXiv:1906.05274}, 2019.

\bibitem[Liu \& Abbeel(2021{\natexlab{a}})Liu and Abbeel]{liu2021aps}
Liu, H. and Abbeel, P.
\newblock {APS}: Active pretraining with successor features.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2021{\natexlab{a}}.

\bibitem[Liu \& Abbeel(2021{\natexlab{b}})Liu and Abbeel]{liu2021behavior}
Liu, H. and Abbeel, P.
\newblock Behavior from the void: Unsupervised active pre-training.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2021{\natexlab{b}}.

\bibitem[Lusena et~al.(2001)Lusena, Goldsmith, and
  Mundhenk]{lusena2001complexity}
Lusena, C., Goldsmith, J., and Mundhenk, M.
\newblock Nonapproximability results for partially observable {M}arkov decision
  processes.
\newblock \emph{Journal of Artificial Intelligence Research}, 2001.

\bibitem[Mundhenk et~al.(2000)Mundhenk, Goldsmith, Lusena, and
  Allender]{mundhenk2000complexity}
Mundhenk, M., Goldsmith, J., Lusena, C., and Allender, E.
\newblock Complexity of finite-horizon {M}arkov decision process problems.
\newblock \emph{Journal of the ACM (JACM)}, 2000.

\bibitem[Mutti \& Restelli(2020)Mutti and Restelli]{mutti2020intrinsically}
Mutti, M. and Restelli, M.
\newblock An intrinsically-motivated approach for learning highly exploring and
  fast mixing policies.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2020.

\bibitem[Mutti et~al.(2021)Mutti, Pratissoli, and Restelli]{mutti2020policy}
Mutti, M., Pratissoli, L., and Restelli, M.
\newblock Task-agnostic exploration via policy gradient of a non-parametric
  state entropy estimate.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2021.

\bibitem[Mutti et~al.(2022)Mutti, Mancassola, and
  Restelli]{mutti2021unsupervised}
Mutti, M., Mancassola, M., and Restelli, M.
\newblock Unsupervised reinforcement learning in multiple environments.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2022.

\bibitem[Nedergaard \& Cook(2022)Nedergaard and Cook]{nedergaard2022k}
Nedergaard, A. and Cook, M.
\newblock k-means maximum entropy exploration.
\newblock \emph{arXiv preprint arXiv:2205.15623}, 2022.

\bibitem[Papadimitriou \& Tsitsiklis(1987)Papadimitriou and
  Tsitsiklis]{papadimitriou1987complexity}
Papadimitriou, C.~H. and Tsitsiklis, J.~N.
\newblock The complexity of {M}arkov decision processes.
\newblock \emph{Mathematics of Operations Research}, 1987.

\bibitem[Peters \& Schaal(2008)Peters and Schaal]{peters2008reinforcement}
Peters, J. and Schaal, S.
\newblock Reinforcement learning of motor skills with policy gradients.
\newblock \emph{Neural networks}, 2008.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Seo et~al.(2021)Seo, Chen, Shin, Lee, Abbeel, and Lee]{seo2021state}
Seo, Y., Chen, L., Shin, J., Lee, H., Abbeel, P., and Lee, K.
\newblock State entropy maximization with random encoders for efficient
  exploration.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2021.

\bibitem[Strehl \& Littman(2008)Strehl and Littman]{strehl2008analysis}
Strehl, A.~L. and Littman, M.~L.
\newblock An analysis of model-based interval estimation for {M}arkov decision
  processes.
\newblock \emph{Journal of Computer and System Sciences}, 2008.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tarbouriech \& Lazaric(2019)Tarbouriech and
  Lazaric]{tarbouriech2019active}
Tarbouriech, J. and Lazaric, A.
\newblock Active exploration in {M}arkov decision processes.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, 2019.

\bibitem[Tarbouriech et~al.(2021)Tarbouriech, Pirotta, Valko, and
  Lazaric]{tarbouriech2020gosprl}
Tarbouriech, J., Pirotta, M., Valko, M., and Lazaric, A.
\newblock A provably efficient sample collection strategy for reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Watkins \& Dayan(1992)Watkins and Dayan]{watkins1992q}
Watkins, C.~J. and Dayan, P.
\newblock Q-learning.
\newblock \emph{Machine learning}, 1992.

\bibitem[Williams \& Zipser(1989)Williams and Zipser]{williams1989learning}
Williams, R.~J. and Zipser, D.
\newblock A learning algorithm for continually running fully recurrent neural
  networks.
\newblock \emph{Neural computation}, 1989.

\bibitem[Yarats et~al.(2021)Yarats, Fergus, Lazaric, and
  Pinto]{yarats2021reinforcement}
Yarats, D., Fergus, R., Lazaric, A., and Pinto, L.
\newblock Reinforcement learning with prototypical representations.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2021.

\bibitem[Zhang et~al.(2021)Zhang, Cai, Huang, and Li]{zhang2020exploration}
Zhang, C., Cai, Y., Huang, L., and Li, J.
\newblock Exploration by maximizing {R}{\'e}nyi entropy for reward-free {RL}
  framework.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Koppel, Bedi, Szepesvari, and
  Wang]{zhang2020variational}
Zhang, J., Koppel, A., Bedi, A.~S., Szepesvari, C., and Wang, M.
\newblock Variational policy gradient method for reinforcement learning with
  general utilities.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\end{thebibliography}
