\begin{thebibliography}{10}

\bibitem{flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock In {\em NeurIPS}, volume~35, 2022.

\bibitem{awadalla2023openflamingo}
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et~al.
\newblock Openflamingo: An open-source framework for training large autoregressive vision-language models.
\newblock {\em arXiv preprint arXiv:2308.01390}, 2023.

\bibitem{Bai2023QwenVL}
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.
\newblock Qwen-vl: A frontier large vision-language model with versatile abilities.
\newblock {\em ArXiv}, abs/2308.12966, 2023.

\bibitem{bao2021beit}
Hangbo Bao, Li~Dong, Songhao Piao, and Furu Wei.
\newblock Beit: Bert pre-training of image transformers.
\newblock {\em arXiv preprint arXiv:2106.08254}, 2021.

\bibitem{Chen2023Shikra}
Ke~Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao.
\newblock Shikra: Unleashing multimodal llm's referential dialogue magic.
\newblock {\em ArXiv}, abs/2306.15195, 2023.

\bibitem{Dai2023InstructBLIP}
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng~Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang~Albert Li, Pascale Fung, and Steven C.~H. Hoi.
\newblock Instructblip: Towards general-purpose vision-language models with instruction tuning.
\newblock {\em ArXiv}, abs/2305.06500, 2023.

\bibitem{ViT}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In {\em ICLR}, 2021.

\bibitem{Driess2023PaLME}
Danny Driess, F.~Xia, Mehdi S.~M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan~Ho Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Peter~R. Florence.
\newblock Palm-e: An embodied multimodal language model.
\newblock In {\em International Conference on Machine Learning}, 2023.

\bibitem{fu2023mme}
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu~Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et~al.
\newblock Mme: A comprehensive evaluation benchmark for multimodal large language models.
\newblock {\em arXiv preprint arXiv:2306.13394}, 2023.

\bibitem{Gao2023LLaMAAdapterV2}
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, W.~Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu~Jiao Qiao.
\newblock Llama-adapter v2: Parameter-efficient visual instruction model.
\newblock {\em ArXiv}, abs/2304.15010, 2023.

\bibitem{SEED}
Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan.
\newblock Planting a seed of vision in large language model.
\newblock {\em arXiv preprint arXiv:2307.08041}, 2023.

\bibitem{balanced_vqa_v2}
Yash Goyal, Tejas Khot, Douglas Summers{-}Stay, Dhruv Batra, and Devi Parikh.
\newblock Making the {V} in {VQA} matter: Elevating the role of image understanding in {V}isual {Q}uestion {A}nswering.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR)}, 2017.

\bibitem{VQGAN}
Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo~Zhang, Dongdong Chen, Lu~Yuan, and Baining Guo.
\newblock Vector quantized diffusion model for text-to-image synthesis.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 10696--10706, 2022.

\bibitem{VG}
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David~A Shamma, et~al.
\newblock Visual genome: Connecting language and vision using crowdsourced dense image annotations.
\newblock {\em International journal of computer vision}, 123:32--73, 2017.

\bibitem{li2023otter}
Bo~Li, Yuanhan Zhang, Liangyu Chen, et~al.
\newblock Otter: A multi-modal model with in-context instruction tuning.
\newblock {\em arXiv preprint arXiv:2305.03726}, 2023.

\bibitem{li2023seedbench}
Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan.
\newblock Seed-bench: Benchmarking multimodal llms with generative comprehension.
\newblock {\em arXiv preprint arXiv:2307.16125}, 2023.

\bibitem{demon}
Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, Hanwang Zhang, and Yueting Zhuang.
\newblock Fine-tuning multimodal llms to follow zero-shot demonstrative instructions.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{BLIP2}
Junnan Li, Dongxu Li, Silvio Savarese, et~al.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock In {\em ICML}, 2023.

\bibitem{Li2023BLIP2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven C.~H. Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock {\em ArXiv}, abs/2301.12597, 2023.

\bibitem{liang2022mind}
Victor~Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James~Y Zou.
\newblock Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning.
\newblock {\em Advances in Neural Information Processing Systems}, 35:17612--17625, 2022.

\bibitem{lin2014coco}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}, pages 740--755. Springer, 2014.

\bibitem{LLaVA-1.5}
Haotian Liu, Chunyuan Li, Yuheng Li, et~al.
\newblock Improved baselines with visual instruction tuning.
\newblock {\em arXiv preprint arXiv:2310.03744}, 2023.

\bibitem{Liu2023LLava15}
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee.
\newblock Improved baselines with visual instruction tuning.
\newblock {\em ArXiv}, abs/2310.03744, 2023.

\bibitem{LLaVA}
Haotian Liu, Chunyuan Li, Qingyang Wu, et~al.
\newblock Visual instruction tuning.
\newblock In {\em NeurIPS}, 2023.

\bibitem{Liu2023Llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock {\em ArXiv}, abs/2304.08485, 2023.

\bibitem{liu2023mmbench}
Yuan Liu, Haodong Duan, Yuanhan Zhang, et~al.
\newblock Mmbench: Is your multi-modal model an all-around player?
\newblock {\em arXiv preprint arXiv:2307.06281}, 2023.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted windows.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 10012--10022, 2021.

\bibitem{loshchilov2018adamW}
Ilya Loshchilov and Frank Hutter.
\newblock Fixing weight decay regularization in adam.
\newblock 2018.

\bibitem{Lu2022UnifiedIO}
Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi.
\newblock Unified-io: A unified model for vision, language, and multi-modal tasks.
\newblock {\em ArXiv}, abs/2206.08916, 2022.

\bibitem{mishra2019ocrvqa}
Anand Mishra, Shashank Shekhar, Ajeet~Kumar Singh, and Anirban Chakraborty.
\newblock Ocr-vqa: Visual question answering by reading text in images.
\newblock In {\em 2019 international conference on document analysis and recognition (ICDAR)}, pages 947--952. IEEE, 2019.

\bibitem{radford2021clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In {\em International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem{ren2024grounded}
Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He~Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et~al.
\newblock Grounded sam: Assembling open-world models for diverse visual tasks.
\newblock {\em arXiv preprint arXiv:2401.14159}, 2024.

\bibitem{singh2019textvqa}
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu~Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.
\newblock Towards vqa models that can read.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 8317--8326, 2019.

\bibitem{evaclip}
Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao.
\newblock Eva-clip: Improved training techniques for clip at scale.
\newblock {\em arXiv preprint arXiv:2303.15389}, 2023.

\bibitem{VQVAE}
Aaron Van Den~Oord, Oriol Vinyals, et~al.
\newblock Neural discrete representation learning.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{ye2023mplugowl}
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et~al.
\newblock mplug-owl: Modularization empowers large language models with multimodality.
\newblock {\em arXiv preprint arXiv:2304.14178}, 2023.

\bibitem{yin2023survey}
Shukang Yin, Chaoyou Fu, Sirui Zhao, et~al.
\newblock A survey on multimodal large language models.
\newblock {\em arXiv preprint arXiv:2306.13549}, 2023.

\bibitem{yu2016refcoco}
Licheng Yu, Patrick Poirson, Shan Yang, Alexander~C Berg, and Tamara~L Berg.
\newblock Modeling context in referring expressions.
\newblock In {\em Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14}, pages 69--85. Springer, 2016.

\bibitem{yu2023mmvet}
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.
\newblock Mm-vet: Evaluating large multimodal models for integrated capabilities.
\newblock {\em arXiv preprint arXiv:2308.02490}, 2023.

\bibitem{Zhang2015ASO}
Zheng Zhang, Yong Xu, Jian Yang, Xuelong Li, and David~Dian Zhang.
\newblock A survey of sparse representation: Algorithms and applications.
\newblock {\em IEEE Access}, 3:490--530, 2015.

\bibitem{zhao2023mmicl}
Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang.
\newblock Mmicl: Empowering vision-language model with multi-modal in-context learning.
\newblock {\em arXiv preprint arXiv:2309.07915}, 2023.

\bibitem{zheng2023vicuna}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric.~P Xing, Hao Zhang, Joseph~E. Gonzalez, and Ion Stoica.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

\bibitem{MiniGPT-4}
Deyao Zhu, Jun Chen, Xiaoqian Shen, et~al.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced large language models.
\newblock {\em arXiv preprint arXiv:2304.10592}, 2023.

\bibitem{Zhu2023MiniGPT4}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced large language models.
\newblock {\em ArXiv}, abs/2304.10592, 2023.

\bibitem{MMC4}
Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir~Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William~Yang Wang, and Yejin Choi.
\newblock Multimodal c4: An open, billion-scale corpus of images interleaved with text.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\end{thebibliography}
