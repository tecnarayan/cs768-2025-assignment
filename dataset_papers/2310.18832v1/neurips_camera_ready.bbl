\begin{thebibliography}{99}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Microsoft(2021)]{microsoft-rai}
Microsoft.
\newblock Microsoft.
\newblock \url{https://www.microsoft.com/en-us/ai/responsible-ai}, 2021.
\newblock Accessed: Date Accessed.

\bibitem[Google(2020)]{google-rai}
Google.
\newblock Google.
\newblock \url{https://ai.google/responsibility/responsible-ai-practices/},
  2020.
\newblock Accessed: Date Accessed.

\bibitem[Namkoong and Duchi(2017)]{namkoong2017variance}
Hongseok Namkoong and John~C Duchi.
\newblock Variance-based regularization with convex objectives.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Duchi and Namkoong(2018)]{duchi2018learning}
John Duchi and Hongseok Namkoong.
\newblock Learning models with uniform performance via distributionally robust
  optimization.
\newblock \emph{arXiv preprint arXiv:1810.08750}, 2018.

\bibitem[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and
  Liang]{sagawa2019distributionally}
Shiori Sagawa, Pang~Wei Koh, Tatsunori~B Hashimoto, and Percy Liang.
\newblock Distributionally robust neural networks for group shifts: On the
  importance of regularization for worst-case generalization.
\newblock \emph{arXiv preprint arXiv:1911.08731}, 2019.

\bibitem[Zhai et~al.(2021{\natexlab{a}})Zhai, Dan, Suggala, Kolter, and
  Ravikumar]{zhai2021boosted}
Runtian Zhai, Chen Dan, Arun Suggala, J~Zico Kolter, and Pradeep Ravikumar.
\newblock Boosted cvar classification.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 21860--21871, 2021{\natexlab{a}}.

\bibitem[Hashimoto et~al.(2018)Hashimoto, Srivastava, Namkoong, and
  Liang]{hashimoto2018fairness}
Tatsunori~B. Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang.
\newblock Fairness without demographics in repeated loss minimization.
\newblock \emph{International Conference On Machine Learning}, 2018.

\bibitem[Zhai et~al.(2021{\natexlab{b}})Zhai, Dan, Kolter, and
  Ravikumar]{pmlr-v139-zhai21a}
Runtian Zhai, Chen Dan, Zico Kolter, and Pradeep Ravikumar.
\newblock Doro: Distributional and outlier robust optimization.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 12345--12355. PMLR,
  18-24 Jul 2021{\natexlab{b}}.
\newblock URL \url{https://proceedings.mlr.press/v139/zhai21a.html}.

\bibitem[Roh et~al.(2020)Roh, Lee, Whang, and Suh]{roh2020frtrain}
Yuji Roh, Kangwook Lee, Steven~Euijong Whang, and Changho Suh.
\newblock Fr-train: A mutual information-based approach to fair and robust
  training.
\newblock \emph{arXiv preprint arXiv: Arxiv-2002.10234}, 2020.

\bibitem[Arora et~al.(2012)Arora, Hazan, and Kale]{arora2012multiplicative}
Sanjeev Arora, Elad Hazan, and Satyen Kale.
\newblock The multiplicative weights update method: a meta-algorithm and
  applications.
\newblock \emph{Theory of computing}, 8\penalty0 (1):\penalty0 121--164, 2012.

\bibitem[McMahan(2011)]{pmlr-v15-mcmahan11b}
Brendan McMahan.
\newblock Follow-the-regularized-leader and mirror descent: Equivalence
  theorems and l1 regularization.
\newblock In Geoffrey Gordon, David Dunson, and Miroslav Dudík, editors,
  \emph{Proceedings of the Fourteenth International Conference on Artificial
  Intelligence and Statistics}, volume~15 of \emph{Proceedings of Machine
  Learning Research}, pages 525--533, Fort Lauderdale, FL, USA, 11--13 Apr
  2011. PMLR.

\bibitem[Bubeck(2011)]{bubeck2011introduction}
S{\'e}bastien Bubeck.
\newblock Introduction to online optimization.
\newblock \emph{Lecture notes}, 2:\penalty0 1--86, 2011.

\bibitem[Hu et~al.(2018)Hu, Niu, Sato, and Sugiyama]{hu2018does}
Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama.
\newblock Does distributionally robust supervised learning give robust
  classifiers?
\newblock In \emph{International Conference on Machine Learning}, pages
  2029--2037. PMLR, 2018.

\bibitem[Bühlmann(2018)]{bühlmann2018invariance}
Peter Bühlmann.
\newblock Invariance, causality and robustness.
\newblock \emph{arXiv preprint arXiv: 1812.08233}, 2018.

\bibitem[Shafieezadeh-Abadeh et~al.(2015)Shafieezadeh-Abadeh, Esfahani, and
  Kuhn]{shafieezadehabadeh2015distributionally}
Soroosh Shafieezadeh-Abadeh, Peyman~Mohajerin Esfahani, and D.~Kuhn.
\newblock Distributionally robust logistic regression.
\newblock \emph{Neural Information Processing Systems}, 2015.

\bibitem[Gao and Kleywegt(2022)]{gao2022distributionally}
Rui Gao and Anton Kleywegt.
\newblock Distributionally robust stochastic optimization with wasserstein
  distance.
\newblock \emph{Mathematics of Operations Research}, 2022.

\bibitem[Namkoong and Duchi(2016)]{Namkoong2016StochasticGM}
Hongseok Namkoong and John~C. Duchi.
\newblock Stochastic gradient methods for distributionally robust optimization
  with f-divergences.
\newblock In \emph{Neural Information Processing Systems}, 2016.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:7481496}.

\bibitem[Ben-Tal et~al.(2011)Ben-Tal, den Hertog, Waegenaere, Melenberg, and
  Rennen]{BenTal2011RobustSO}
Aharon Ben-Tal, Dick den Hertog, Anja~De Waegenaere, Bertrand Melenberg, and
  Gijs Rennen.
\newblock Robust solutions of optimization problems affected by uncertain
  probabilities.
\newblock \emph{Advanced Risk \& Portfolio Management{\textregistered} Research
  Paper Series}, 2011.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:761793}.

\bibitem[Mandal et~al.(2020)Mandal, Deng, Jana, Wing, and
  Hsu]{mandal2020fairness}
Debmalya Mandal, Samuel Deng, Suman Jana, Jeannette Wing, and Daniel~J Hsu.
\newblock Ensuring fairness beyond the training data.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 18445--18456. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/d6539d3b57159babf6a72e106beb45bd-Paper.pdf}.

\bibitem[Martinez et~al.(2020)Martinez, Bertran, and
  Sapiro]{pmlr-v119-martinez20a}
Natalia Martinez, Martin Bertran, and Guillermo Sapiro.
\newblock Minimax pareto fairness: A multi objective perspective.
\newblock In Hal~Daumé III and Aarti Singh, editors, \emph{Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 6755--6764. PMLR,
  13-18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/martinez20a.html}.

\bibitem[Oneto et~al.(2018)Oneto, Donini, Elders, and Pontil]{oneto2018taking}
L.~Oneto, Michele Donini, Amon Elders, and M.~Pontil.
\newblock Taking advantage of multitask learning for fair classification.
\newblock \emph{AAAI/ACM Conference on AI, Ethics, and Society}, 2018.
\newblock \doi{10.1145/3306618.3314255}.

\bibitem[Koh et~al.(2021)Koh, Sagawa, Marklund, Xie, Zhang, Balsubramani, Hu,
  Yasunaga, Phillips, Gao, et~al.]{koh2021wilds}
Pang~Wei Koh, Shiori Sagawa, Henrik Marklund, Sang~Michael Xie, Marvin Zhang,
  Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard~Lanas Phillips,
  Irena Gao, et~al.
\newblock Wilds: A benchmark of in-the-wild distribution shifts.
\newblock In \emph{International Conference on Machine Learning}, pages
  5637--5664. PMLR, 2021.

\bibitem[Cao et~al.(2019)Cao, Wei, Gaidon, Arechiga, and Ma]{cao2019learning}
Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma.
\newblock Learning imbalanced datasets with label-distribution-aware margin
  loss.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 1567--1578, 2019.

\bibitem[Menon et~al.(2021)Menon, Jayasumana, Rawat, Jain, Veit, and
  Kumar]{menon2021longtail}
Aditya~Krishna Menon, Sadeep Jayasumana, Ankit~Singh Rawat, Himanshu Jain,
  Andreas Veit, and Sanjiv Kumar.
\newblock Long-tail learning via logit adjustment.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Kini et~al.(2021)Kini, Paraskevas, Oymak, and
  Thrampoulidis]{kini2021labelimbalanced}
Ganesh~Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos
  Thrampoulidis.
\newblock Label-imbalanced and group-sensitive classification under
  overparameterization.
\newblock In \emph{Thirty-Fifth Conference on Neural Information Processing
  Systems}, 2021.

\bibitem[Sinha et~al.(2017)Sinha, Namkoong, Volpi, and
  Duchi]{sinha2017certifying}
Aman Sinha, Hongseok Namkoong, Riccardo Volpi, and John Duchi.
\newblock Certifying some distributional robustness with principled adversarial
  training.
\newblock \emph{arXiv preprint arXiv:1710.10571}, 2017.

\bibitem[Gao et~al.(2022)Gao, Chen, and Kleywegt]{gao2022wasserstein}
Rui Gao, Xi~Chen, and Anton~J Kleywegt.
\newblock Wasserstein distributionally robust optimization and variation
  regularization.
\newblock \emph{Operations Research}, 2022.

\bibitem[Staib and Jegelka(2019)]{staib2019distributionally}
Matthew Staib and Stefanie Jegelka.
\newblock Distributionally robust optimization and generalization in kernel
  methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Zhu et~al.(2020)Zhu, Jitkrittum, Diehl, and
  Sch{\"o}lkopf]{zhu2020kernel}
Jia-Jie Zhu, Wittawat Jitkrittum, Moritz Diehl, and Bernhard Sch{\"o}lkopf.
\newblock Kernel distributionally robust optimization.
\newblock \emph{arXiv preprint arXiv:2006.06981}, 2020.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Namkoong, and Xia]{li2021evaluating}
Mike Li, Hongseok Namkoong, and Shangzhou Xia.
\newblock Evaluating model performance under worst-case subpopulations.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan,
  editors, \emph{Advances in Neural Information Processing Systems},
  2021{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=nehzxAdyJxF}.

\bibitem[Qi et~al.(2020)Qi, Xu, Jin, Yin, and Yang]{qi2020attentional}
Qi~Qi, Yi~Xu, Rong Jin, Wotao Yin, and Tianbao Yang.
\newblock Attentional biased stochastic gradient for imbalanced classification.
\newblock \emph{arXiv preprint arXiv:2012.06951}, 2020.

\bibitem[Kumar et~al.(2023)Kumar, Majmundar, Nagaraj, and
  Suggala]{kumar2023stochastic}
Ramnath Kumar, Kushal Majmundar, Dheeraj Nagaraj, and Arun~Sai Suggala.
\newblock Stochastic re-weighted gradient descent via distributionally robust
  optimization.
\newblock \emph{arXiv preprint arXiv:2306.09222}, 2023.

\bibitem[Jin et~al.(2021)Jin, Zhang, Wang, and Wang]{jin2021nonconvex}
Jikai Jin, Bohang Zhang, Haiyang Wang, and Liwei Wang.
\newblock Non-convex distributionally robust optimization: Non-asymptotic
  analysis.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan,
  editors, \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=gZLhHMyxa-}.

\bibitem[Breiman(1999)]{breiman1999prediction}
Leo Breiman.
\newblock Prediction games and arcing algorithms.
\newblock \emph{Neural computation}, 11\penalty0 (7):\penalty0 1493--1517,
  1999.

\bibitem[Friedman et~al.(2000)Friedman, Hastie, Tibshirani,
  et~al.]{friedman2000additive}
Jerome Friedman, Trevor Hastie, Robert Tibshirani, et~al.
\newblock Additive logistic regression: a statistical view of boosting (with
  discussion and a rejoinder by the authors).
\newblock \emph{The annals of statistics}, 28\penalty0 (2):\penalty0 337--407,
  2000.

\bibitem[Friedman(2001)]{friedman2001greedy}
Jerome~H Friedman.
\newblock Greedy function approximation: a gradient boosting machine.
\newblock \emph{Annals of statistics}, pages 1189--1232, 2001.

\bibitem[Freund and Schapire(1995)]{freund1995desicion}
Yoav Freund and Robert~E Schapire.
\newblock A desicion-theoretic generalization of on-line learning and an
  application to boosting.
\newblock In \emph{European conference on computational learning theory}, pages
  23--37. Springer, 1995.

\bibitem[Freund et~al.(1996)Freund, Schapire, et~al.]{freund1996experiments}
Yoav Freund, Robert~E Schapire, et~al.
\newblock Experiments with a new boosting algorithm.
\newblock In \emph{icml}, volume~96, pages 148--156. Citeseer, 1996.

\bibitem[Mason et~al.(2000)Mason, Baxter, Bartlett, and
  Frean]{mason2000boosting}
Llew Mason, Jonathan Baxter, Peter~L Bartlett, and Marcus~R Frean.
\newblock Boosting algorithms as gradient descent.
\newblock In \emph{Advances in neural information processing systems}, pages
  512--518, 2000.

\bibitem[Schapire(1999)]{schapire1999boosting}
Robert~E Schapire.
\newblock The boosting approach to machine learning: An overview.
\newblock In \emph{MSRI workshop on Nonlinear Estimation and Classification},
  1999.

\bibitem[Demiriz et~al.(2002)Demiriz, Bennett, and Shawe-Taylor]{demiriz2002lp}
Ayhan Demiriz, Kristin~P Bennett, and John Shawe-Taylor.
\newblock Lpboost: A boosting algorithm with linear programming.
\newblock In \emph{International Conference on Machine Learning}, pages
  143--150. AAAI Press, 2002.

\bibitem[Mason et~al.(1999)Mason, Baxter, Bartlett, and Frean]{grad-boost}
Llew Mason, Jonathan Baxter, Peter Bartlett, and Marcus Frean.
\newblock Boosting algorithms as gradient descent.
\newblock In S.~Solla, T.~Leen, and K.~M\"{u}ller, editors, \emph{Advances in
  Neural Information Processing Systems}, volume~12. MIT Press, 1999.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/1999/file/96a93ba89a5b5c6c226e49b88973f46e-Paper.pdf}.

\bibitem[Chen and Guestrin(2016)]{chen2016xgboost}
Tianqi Chen and Carlos Guestrin.
\newblock Xgboost: A scalable tree boosting system.
\newblock \emph{arXiv preprint arXiv: Arxiv-1603.02754}, 2016.

\bibitem[Zhang et~al.(2022)Zhang, Zhang, Courville, Bengio, Ravikumar, and
  Suggala]{zhang2022building}
Dinghuai Zhang, Hongyang Zhang, Aaron Courville, Yoshua Bengio, Pradeep
  Ravikumar, and Arun~Sai Suggala.
\newblock Building robust ensembles via margin boosting.
\newblock In \emph{International Conference on Machine Learning}, pages
  26669--26692. PMLR, 2022.

\bibitem[Meunier et~al.(2021)Meunier, Scetbon, Pinot, Atif, and
  Chevaleyre]{pmlr-v139-meunier21a}
Laurent Meunier, Meyer Scetbon, Rafael~B Pinot, Jamal Atif, and Yann
  Chevaleyre.
\newblock Mixed nash equilibria in the adversarial examples game.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 7677--7687. PMLR,
  18-24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/meunier21a.html}.

\bibitem[Balcan et~al.(2023)Balcan, Pukdee, Ravikumar, and
  Zhang]{balcan2023nash}
Maria-Florina Balcan, Rattana Pukdee, Pradeep Ravikumar, and Hongyang Zhang.
\newblock Nash equilibria and pitfalls of adversarial training in adversarial
  robustness games.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 9607--9636. PMLR, 2023.

\bibitem[Bennouna and Parys(2022)]{bennouna2022holistic}
M.~A. Bennouna and Bart P. G.~Van Parys.
\newblock Holistic robust data-driven decisions.
\newblock \emph{ARXIV.ORG}, 2022.
\newblock \doi{10.48550/arXiv.2207.09560}.

\bibitem[Dwork et~al.(2012)Dwork, Hardt, Pitassi, Reingold, and
  Zemel]{dwork2012fairness}
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel.
\newblock Fairness through awareness.
\newblock In \emph{Proceedings of the 3rd innovations in theoretical computer
  science conference}, pages 214--226, 2012.

\bibitem[Zemel et~al.(2013)Zemel, Wu, Swersky, Pitassi, and
  Dwork]{zemel2013learning}
Rich Zemel, Yu~Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork.
\newblock Learning fair representations.
\newblock In \emph{International conference on machine learning}, pages
  325--333. PMLR, 2013.

\bibitem[Hardt et~al.(2016{\natexlab{a}})Hardt, Price, and
  Srebro]{hardt2016equality}
Moritz Hardt, Eric Price, and Nati Srebro.
\newblock Equality of opportunity in supervised learning.
\newblock \emph{Advances in neural information processing systems}, 29,
  2016{\natexlab{a}}.

\bibitem[Zafar et~al.(2017)Zafar, Valera, Gomez~Rodriguez, and
  Gummadi]{zafar2017fairness}
Muhammad~Bilal Zafar, Isabel Valera, Manuel Gomez~Rodriguez, and Krishna~P
  Gummadi.
\newblock Fairness beyond disparate treatment \& disparate impact: Learning
  classification without disparate mistreatment.
\newblock In \emph{Proceedings of the 26th international conference on world
  wide web}, pages 1171--1180, 2017.

\bibitem[Kusner et~al.(2017)Kusner, Loftus, Russell, and
  Silva]{kusner2017counterfactual}
Matt~J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva.
\newblock Counterfactual fairness.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Rawls(2020)]{rawls2020theory}
John Rawls.
\newblock \emph{A theory of justice: Revised edition}.
\newblock Harvard university press, 2020.

\bibitem[Barocas et~al.(2017)Barocas, Hardt, and
  Narayanan]{barocas2017fairness}
Solon Barocas, Moritz Hardt, and Arvind Narayanan.
\newblock Fairness in machine learning.
\newblock \emph{Nips tutorial}, 1:\penalty0 2017, 2017.

\bibitem[Chouldechova and Roth(2018)]{chouldechova2018frontiers}
Alexandra Chouldechova and Aaron Roth.
\newblock The frontiers of fairness in machine learning.
\newblock \emph{arXiv preprint arXiv:1810.08810}, 2018.

\bibitem[Mehrabi et~al.(2021)Mehrabi, Morstatter, Saxena, Lerman, and
  Galstyan]{mehrabi2021survey}
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram
  Galstyan.
\newblock A survey on bias and fairness in machine learning.
\newblock \emph{ACM Computing Surveys (CSUR)}, 54\penalty0 (6):\penalty0 1--35,
  2021.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Namkoong, and Xia]{namkoong2021}
Mike Li, Hongseok Namkoong, and Shangzhou Xia.
\newblock Evaluating model performance under worst-case subpopulations.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 17325--17334. Curran Associates, Inc., 2021{\natexlab{b}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2021/file/908075ea2c025c335f4865f7db427062-Paper.pdf}.

\bibitem[Warmuth et~al.(2006)Warmuth, Liao, and R{\"a}tsch]{warmuth2006totally}
Manfred~K Warmuth, Jun Liao, and Gunnar R{\"a}tsch.
\newblock Totally corrective boosting algorithms that maximize the margin.
\newblock In \emph{Proceedings of the 23rd international conference on Machine
  learning}, pages 1001--1008, 2006.

\bibitem[Bartlett et~al.(1998)Bartlett, Freund, Lee, and
  Schapire]{bartlett1998boosting}
Peter Bartlett, Yoav Freund, Wee~Sun Lee, and Robert~E Schapire.
\newblock Boosting the margin: A new explanation for the effectiveness of
  voting methods.
\newblock \emph{The annals of statistics}, 26\penalty0 (5):\penalty0
  1651--1686, 1998.

\bibitem[Mohri et~al.(2019)Mohri, Sivek, and Suresh]{mohri2019agnostic}
Mehryar Mohri, Gary Sivek, and Ananda~Theertha Suresh.
\newblock Agnostic federated learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  4615--4625. PMLR, 2019.

\bibitem[Hu et~al.(2016)Hu, Niu, Sato, and Sugiyama]{hu2016distributionally}
Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama.
\newblock Does distributionally robust supervised learning give robust
  classifiers?
\newblock \emph{International Conference On Machine Learning}, 2016.

\bibitem[Lacasse et~al.(2006)Lacasse, Laviolette, Marchand, Germain, and
  Usunier]{lacasse2006pac}
Alexandre Lacasse, Fran{\c{c}}ois Laviolette, Mario Marchand, Pascal Germain,
  and Nicolas Usunier.
\newblock Pac-bayes bounds for the risk of the majority vote and the variance
  of the gibbs classifier.
\newblock \emph{Advances in Neural information processing systems}, 19, 2006.

\bibitem[Germain et~al.(2015)Germain, Lacasse, Laviolette, Marchand, and
  Roy]{germain2015risk}
Pascal Germain, Alexandre Lacasse, Francois Laviolette, Mario Marchand, and
  Jean-Francis Roy.
\newblock Risk bounds for the majority vote: From a pac-bayesian analysis to a
  learning algorithm.
\newblock \emph{arXiv preprint arXiv:1503.08329}, 2015.

\bibitem[Masegosa et~al.(2020)Masegosa, Lorenzen, Igel, and
  Seldin]{masegosa2020second}
Andr{\'e}s Masegosa, Stephan Lorenzen, Christian Igel, and Yevgeny Seldin.
\newblock Second order pac-bayesian bounds for the weighted majority vote.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5263--5273, 2020.

\bibitem[Cesa-Bianchi and Lugosi(2006)]{cesa2006prediction}
Nicolo Cesa-Bianchi and Gabor Lugosi.
\newblock \emph{Prediction, learning, and games}.
\newblock Cambridge university press, 2006.

\bibitem[Huang et~al.(2017)Huang, Lattimore, Gy{\"o}rgy, and
  Szepesv{\'a}ri]{huang2017following}
Ruitong Huang, Tor Lattimore, Andr{\'a}s Gy{\"o}rgy, and Csaba Szepesv{\'a}ri.
\newblock Following the leader and fast rates in online linear prediction:
  Curved constraint sets and other regularities.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 5325--5355, 2017.

\bibitem[Jaggi(2013)]{jaggi2013revisiting}
Martin Jaggi.
\newblock Revisiting frank-wolfe: Projection-free sparse convex optimization.
\newblock In \emph{International conference on machine learning}, pages
  427--435. PMLR, 2013.

\bibitem[Parikh et~al.(2014)Parikh, Boyd, et~al.]{parikh2014proximal}
Neal Parikh, Stephen Boyd, et~al.
\newblock Proximal algorithms.
\newblock \emph{Foundations and trends{\textregistered} in Optimization},
  1\penalty0 (3):\penalty0 127--239, 2014.

\bibitem[Shapiro et~al.(2021)Shapiro, Dentcheva, and
  Ruszczynski]{shapiro2021lectures}
Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski.
\newblock \emph{Lectures on stochastic programming: modeling and theory}.
\newblock SIAM, 2021.

\bibitem[Lahoti et~al.(2020)Lahoti, Beutel, Chen, Lee, Prost, Thain, Wang, and
  Chi]{lahoti2020fairness}
Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain,
  Xuezhi Wang, and Ed~Chi.
\newblock Fairness without demographics through adversarially reweighted
  learning.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 728--740, 2020.

\bibitem[Angwin et~al.(2016)Angwin, Larson, Mattu, and
  Kirchner]{angwin2016machine}
Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner.
\newblock Machine bias: There's software used across the country to predict
  future criminals. and its biased against blacks.
\newblock \emph{ProPublica}, May 2016.
\newblock URL
  \url{https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}.

\bibitem[Qi et~al.(2021)Qi, Guo, Xu, Jin, and Yang]{qi2021onlinedro}
Qi~Qi, Zhishuai Guo, Yi~Xu, Rong Jin, and Tianbao Yang.
\newblock An online method for a class of distributionally robust optimization
  with non-convex objectives.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 10067--10080. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2021/file/533fa796b43291fc61a9e812a50c3fb6-Paper.pdf}.

\bibitem[Shen and Sanghavi(2018)]{shen2018learning}
Yanyao Shen and Sujay Sanghavi.
\newblock Learning with bad training data via iterative trimmed loss
  minimization.
\newblock \emph{International Conference On Machine Learning}, 2018.

\bibitem[Kalra and Paddock(2016)]{kalra2016driving}
Nidhi Kalra and Susan~M. Paddock.
\newblock Driving to safety: How many miles of driving would it take to
  demonstrate autonomous vehicle reliability?
\newblock \emph{Transportation Research Part A: Policy and Practice},
  94:\penalty0 182--193, 2016.

\bibitem[Fuster et~al.(2018)Fuster, Goldsmith-Pinkham, Ramadorai, and
  Walther]{fuster2018predictably}
Andreas Fuster, Paul Goldsmith-Pinkham, Tarun Ramadorai, and Alexander Walther.
\newblock Predictably unequal? the effects of machine learning on credit
  markets.
\newblock \emph{Social Science Research Network}, \penalty0 (3072038), 2018.

\bibitem[Ma et~al.(2022)Ma, Wang, and Liu]{robvsfair2022}
Xinsong Ma, Zekai Wang, and Weiwei Liu.
\newblock On the tradeoff between robustness and fairness.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~35,
  pages 26230--26241. Curran Associates, Inc., 2022.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/a80ebbb4ec9e9b39789318a0a61e2e43-Paper-Conference.pdf}.

\bibitem[Chan(2023)]{chan2023gpt}
Anastasia Chan.
\newblock Gpt-3 and instructgpt: technological dystopianism, utopianism, and
  “contextual” perspectives in ai ethics and industry.
\newblock \emph{AI and Ethics}, 3\penalty0 (1):\penalty0 53--64, 2023.

\bibitem[Bender et~al.(2021)Bender, Gebru, McMillan-Major, and
  Shmitchell]{bender2021dangers}
Emily~M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret
  Shmitchell.
\newblock On the dangers of stochastic parrots: Can language models be too big.
\newblock In \emph{Proceedings of the 2021 ACM conference on fairness,
  accountability, and transparency}, pages 610--623, 2021.

\bibitem[Louizos et~al.(2015)Louizos, Swersky, Li, Welling, and
  Zemel]{louizos2015variational}
Christos Louizos, Kevin Swersky, Yujia Li, M.~Welling, and R.~Zemel.
\newblock The variational fair autoencoder.
\newblock \emph{International Conference on Learning Representations}, 2015.

\bibitem[Hardt et~al.(2016{\natexlab{b}})Hardt, Price, Price, and
  Srebro]{NIPS2016_9d268236}
Moritz Hardt, Eric Price, Eric Price, and Nati Srebro.
\newblock Equality of opportunity in supervised learning.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016{\natexlab{b}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf}.

\bibitem[Wang et~al.(2018)Wang, He, Lipton, and Xing]{wang2018learning}
Haohan Wang, Zexue He, Zachary~Chase Lipton, and E.~Xing.
\newblock Learning robust representations by projecting superficial statistics
  out.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Addepalli et~al.(2022)Addepalli, Nasery, Babu, Netrapalli, and
  Jain]{addepalli2022learning}
Sravanti Addepalli, Anshul Nasery, R.~Venkatesh Babu, Praneeth Netrapalli, and
  Prateek Jain.
\newblock Learning an invertible output mapping can mitigate simplicity bias in
  neural networks.
\newblock \emph{arXiv preprint arXiv: 2210.01360}, 2022.

\bibitem[Cha et~al.(2022)Cha, Lee, Park, and Chun]{cha2022domain}
Junbum Cha, Kyungjae Lee, Sungrae Park, and Sanghyuk Chun.
\newblock Domain generalization by mutual-information regularization with
  pre-trained models.
\newblock \emph{European Conference on Computer Vision}, 2022.
\newblock \doi{10.48550/arXiv.2203.10789}.

\bibitem[Hazan(2016)]{hazan2016introduction}
Elad Hazan.
\newblock Introduction to online convex optimization.
\newblock \emph{Found. Trends Optim.}, 2016.
\newblock \doi{10.1561/2400000013}.

\bibitem[Gupta et~al.(2020)Gupta, Suggala, Prasad, Netrapalli, and
  Ravikumar]{gupta2020learning}
Kartik Gupta, Arun~Sai Suggala, Adarsh Prasad, Praneeth Netrapalli, and Pradeep
  Ravikumar.
\newblock Learning minimax estimators via online learning.
\newblock \emph{ARXIV.ORG}, 2020.

\bibitem[Borwein(2016)]{Borwein2016AVC}
Jonathan~Michael Borwein.
\newblock A very complicated proof of the minimax theorem.
\newblock 2016.

\bibitem[Wald(1945)]{Wald1945GeneralizationOA}
Abraham Wald.
\newblock Generalization of a theorem by v. neumann concerning zero sum two
  person games.
\newblock \emph{Annals of Mathematics}, 46:\penalty0 281, 1945.

\bibitem[Simons(1995)]{Simons1995MinimaxTA}
Stephen Simons.
\newblock Minimax theorems and their proofs.
\newblock 1995.

\bibitem[Raghavan(1994)]{raghavan1994zero}
TES Raghavan.
\newblock Zero-sum two-person games.
\newblock \emph{Handbook of game theory with economic applications},
  2:\penalty0 735--768, 1994.

\bibitem[Cotter et~al.(2019)Cotter, Gupta, and Narasimhan]{cotter2019making}
Andrew Cotter, Maya Gupta, and Harikrishna Narasimhan.
\newblock On making stochastic classifiers deterministic.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Wu et~al.(2022)Wu, Chen, and Liu]{wu2022metric}
Jimmy Wu, Yatong Chen, and Yang Liu.
\newblock Metric-fair classifier derandomization.
\newblock In \emph{International Conference on Machine Learning}, pages
  23999--24016. PMLR, 2022.

\bibitem[McMahan(2017)]{mcmahan2017survey}
H~Brendan McMahan.
\newblock A survey of algorithms and analysis for adaptive online learning.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 3117--3166, 2017.

\bibitem[Boyd and Vandenberghe(2004)]{boyd2004convex}
Stephen Boyd and Lieven Vandenberghe.
\newblock \emph{Convex Optimization}.
\newblock Cambridge University Press, 2004.

\bibitem[Rockafellar(1970)]{rockafellar1970convex}
R~Tyrrell Rockafellar.
\newblock \emph{Convex analysis}.
\newblock Number~28. Princeton university press, 1970.

\bibitem[Bartlett et~al.(2006)Bartlett, Jordan, and
  McAuliffe]{bartlett2006convexity}
Peter~L Bartlett, Michael~I Jordan, and Jon~D McAuliffe.
\newblock Convexity, classification, and risk bounds.
\newblock \emph{Journal of the American Statistical Association}, 101\penalty0
  (473):\penalty0 138--156, 2006.

\bibitem[Tewari and Bartlett(2007)]{tewari2007consistency}
Ambuj Tewari and Peter~L Bartlett.
\newblock On the consistency of multiclass classification methods.
\newblock \emph{Journal of Machine Learning Research}, 8\penalty0 (5), 2007.

\bibitem[Wainwright(2019)]{wainwright2019high}
Martin~J Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge university press, 2019.

\bibitem[Freund and Grigas(2014)]{freund14new}
Robert~M. Freund and Paul Grigas.
\newblock New analysis and results for the frank–wolfe method.
\newblock \emph{Mathematical Programming}, 155:\penalty0 199--230, 2014.
\newblock \doi{10.1007/s10107-014-0841-6}.
\newblock URL
  \url{http://link.springer.com/article/10.1007/s10107-014-0841-6/fulltext.html}.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
Sergey Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock \emph{British Machine Vision Conference}, 2016.
\newblock \doi{10.5244/C.30.87}.

\end{thebibliography}
