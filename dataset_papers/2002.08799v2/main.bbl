\begin{thebibliography}{10}

\bibitem{abernethy2009new}
Jacob Abernethy, Francis Bach, Theodoros Evgeniou, and Jean-Philippe Vert.
\newblock A new approach to collaborative filtering: Operator estimation with
  spectral regularization.
\newblock {\em Journal of Machine Learning Research}, 10(Mar):803--826, 2009.

\bibitem{adams2003sobolev}
Robert~A Adams and John~JF Fournier.
\newblock {\em Sobolev spaces}, volume 140.
\newblock Elsevier, 2003.

\bibitem{altae2017low}
Han Altae-Tran, Bharath Ramsundar, Aneesh~S Pappu, and Vijay Pande.
\newblock Low data drug discovery with one-shot learning.
\newblock {\em ACS central science}, 3(4):283--293, 2017.

\bibitem{antoniou2018train}
Antreas Antoniou, Harrison Edwards, and Amos Storkey.
\newblock How to train your maml.
\newblock {\em International conference on learning representations}, 2019.

\bibitem{aronszajn1950theory}
Nachman Aronszajn.
\newblock Theory of reproducing kernels.
\newblock {\em Transactions of the American mathematical society},
  68(3):337--404, 1950.

\bibitem{bakir2007predicting}
G{\"o}khan Bakir, Thomas Hofmann, Bernhard Sch{\"o}lkopf, Alexander~J Smola,
  and Ben Taskar.
\newblock {\em Predicting structured data}.
\newblock MIT press, 2007.

\bibitem{bartlett2006}
Peter~L Bartlett, Michael~I Jordan, and Jon~D McAuliffe.
\newblock Convexity, classification, and risk bounds.
\newblock {\em Journal of the American Statistical Association},
  101(473):138--156, 2006.

\bibitem{berlinet2011reproducing}
Alain Berlinet and Christine Thomas-Agnan.
\newblock {\em Reproducing kernel Hilbert spaces in probability and
  statistics}.
\newblock Springer Science \& Business Media, 2011.

\bibitem{bertinetto2018meta}
Luca Bertinetto, Joao~F Henriques, Philip~HS Torr, and Andrea Vedaldi.
\newblock Meta-learning with differentiable closed-form solvers.
\newblock {\em International conference on learning representations}, 2019.

\bibitem{cai2020weighted}
Diana Cai, Rishit Sheth, Lester Mackey, and Nicolo Fusi.
\newblock Weighted meta-learning.
\newblock {\em arXiv preprint arXiv:2003.09465}, 2020.

\bibitem{caponnetto2007}
Andrea Caponnetto and Ernesto De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock {\em Foundations of Computational Mathematics}, 7(3):331--368, 2007.

\bibitem{chen2018gradnorm}
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich.
\newblock Gradnorm: Gradient normalization for adaptive loss balancing in deep
  multitask networks.
\newblock In {\em International Conference on Machine Learning}, pages
  794--803. PMLR, 2018.

\bibitem{ciliberto2019localized}
Carlo Ciliberto, Francis Bach, and Alessandro Rudi.
\newblock Localized structured prediction.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{ciliberto2020general}
Carlo Ciliberto, Lorenzo Rosasco, and Alessandro Rudi.
\newblock A general framework for consistent structured prediction with
  implicit loss embeddings.
\newblock {\em Journal of Machine Learning Research}, 21(98):1--67, 2020.

\bibitem{denevi2020advantage}
Giulia Denevi, Massimiliano Pontil, and Carlo Ciliberto.
\newblock The advantage of conditional meta-learning for biased regularization
  and fine-tuning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{fei2006one}
Li~Fei-Fei, Rob Fergus, and Pietro Perona.
\newblock One-shot learning of object categories.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  28(4), 2006.

\bibitem{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}. JMLR. org, 2017.

\bibitem{gretton2012kernel}
Arthur Gretton, Karsten~M Borgwardt, Malte~J Rasch, Bernhard Sch{\"o}lkopf, and
  Alexander Smola.
\newblock A kernel two-sample test.
\newblock {\em Journal of Machine Learning Research}, 13(Mar):723--773, 2012.

\bibitem{ha2016hypernetworks}
David Ha, Andrew Dai, and Quoc~V Le.
\newblock Hypernetworks.
\newblock {\em arXiv preprint arXiv:1609.09106}, 2016.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hochreiter2001learning}
Sepp Hochreiter, A~Steven Younger, and Peter~R Conwell.
\newblock Learning to learn using gradient descent.
\newblock In {\em International Conference on Artificial Neural Networks},
  pages 87--94. Springer, 2001.

\bibitem{jerfel2019reconciling}
Ghassen Jerfel, Erin Grant, Tom Griffiths, and Katherine~A Heller.
\newblock Reconciling meta-learning and continual learning with online mixtures
  of tasks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9119--9130, 2019.

\bibitem{jiang2018learning}
Xiang Jiang, Mohammad Havaei, Farshid Varno, Gabriel Chartrand, Nicolas
  Chapados, and Stan Matwin.
\newblock Learning to learn with conditional class dependencies.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{kendall2018multi}
Alex Kendall, Yarin Gal, and Roberto Cipolla.
\newblock Multi-task learning using uncertainty to weigh losses for scene
  geometry and semantics.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7482--7491, 2018.

\bibitem{cifar100}
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
\newblock Cifar 100 dataset.
\newblock \url{https://www.cs.toronto.edu/~kriz/cifar.html}, 2009.

\bibitem{lake2011one}
Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum.
\newblock One shot learning of simple visual concepts.
\newblock In {\em Proceedings of the annual meeting of the cognitive science
  society}, volume~33, 2011.

\bibitem{lee2019learning}
Hae~Beom Lee, Hayeon Lee, Donghyun Na, Saehoon Kim, Minseop Park, Eunho Yang,
  and Sung~Ju Hwang.
\newblock Learning to balance: Bayesian meta-learning for imbalanced and
  out-of-distribution tasks.
\newblock {\em International Conference on Learning Representations}, 2020.

\bibitem{lee2019meta}
Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto.
\newblock Meta-learning with differentiable convex optimization.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 10657--10665, 2019.

\bibitem{li2016learning}
Ke~Li and Jitendra Malik.
\newblock Learning to optimize.
\newblock {\em arXiv preprint arXiv:1606.01885}, 2016.

\bibitem{li2017meta}
Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li.
\newblock Meta-sgd: Learning to learn quickly for few-shot learning.
\newblock {\em arXiv preprint arXiv:1707.09835}, 2017.

\bibitem{luise2018differential}
Giulia Luise, Alessandro Rudi, Massimiliano Pontil, and Carlo Ciliberto.
\newblock Differential properties of sinkhorn approximation for learning with
  wasserstein distance.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5859--5870, 2018.

\bibitem{meanti2020kernel}
Giacomo Meanti, Luigi Carratino, Lorenzo Rosasco, and Alessandro Rudi.
\newblock Kernel methods through the roof: handling billions of points
  efficiently.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{mroueh2012multiclass}
Youssef Mroueh, Tomaso Poggio, Lorenzo Rosasco, and Jean-Jeacques Slotine.
\newblock Multiclass learning with simplex coding.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2789--2797, 2012.

\bibitem{nichol2018first}
Alex Nichol, Joshua Achiam, and John Schulman.
\newblock On first-order meta-learning algorithms.
\newblock {\em arXiv preprint arXiv:1803.02999}, 2018.

\bibitem{nowozin2011structured}
Sebastian Nowozin, Christoph~H Lampert, et~al.
\newblock Structured learning and prediction in computer vision.
\newblock {\em Foundations and Trends{\textregistered} in Computer Graphics and
  Vision}, 6(3--4):185--365, 2011.

\bibitem{oreshkin2018tadam}
Boris Oreshkin, Pau~Rodr{\'\i}guez L{\'o}pez, and Alexandre Lacoste.
\newblock Tadam: Task dependent adaptive metric for improved few-shot learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  721--731, 2018.

\bibitem{qiao2018few}
Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan~L Yuille.
\newblock Few-shot image recognition by predicting parameters from activations.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 7229--7238, 2018.

\bibitem{rajeswaran2019meta}
Aravind Rajeswaran, Chelsea Finn, Sham~M Kakade, and Sergey Levine.
\newblock Meta-learning with implicit gradients.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{ravi2016optimization}
Sachin Ravi and Hugo Larochelle.
\newblock Optimization as a model for few-shot learning.
\newblock {\em International conference on learning representations}, 2017.

\bibitem{rodriguez2020embedding}
Pau Rodr{\'\i}guez, Issam Laradji, Alexandre Drouin, and Alexandre Lacoste.
\newblock Embedding propagation: Smoother manifold for few-shot classification.
\newblock {\em arXiv preprint arXiv:2003.04151}, 2020.

\bibitem{rudi2017falkon}
Alessandro Rudi, Luigi Carratino, and Lorenzo Rosasco.
\newblock Falkon: An optimal large scale kernel method.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3888--3898, 2017.

\bibitem{rudi2018manifold}
Alessandro Rudi, Carlo Ciliberto, GianMaria Marconi, and Lorenzo Rosasco.
\newblock Manifold structured prediction.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5610--5621, 2018.

\bibitem{rusu2018meta}
Andrei~A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu,
  Simon Osindero, and Raia Hadsell.
\newblock Meta-learning with latent embedding optimization.
\newblock {\em International conference on learning representations}, 2019.

\bibitem{santoro2016meta}
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy
  Lillicrap.
\newblock Meta-learning with memory-augmented neural networks.
\newblock In {\em International conference on machine learning}, 2016.

\bibitem{shalev2014understanding}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock {\em Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem{snell2017prototypical}
Jake Snell, Kevin Swersky, and Richard Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4077--4087, 2017.

\bibitem{sriperumbudur2010hilbert}
Bharath~K Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard
  Sch{\"o}lkopf, and Gert~RG Lanckriet.
\newblock Hilbert space embeddings and metrics on probability measures.
\newblock {\em Journal of Machine Learning Research}, 11(Apr):1517--1561, 2010.

\bibitem{taskar2004max}
Ben Taskar, Carlos Guestrin, and Daphne Koller.
\newblock Max-margin markov networks.
\newblock In {\em Advances in neural information processing systems}, pages
  25--32, 2004.

\bibitem{thrun1996learning}
Sebastian Thrun.
\newblock Is learning the n-th thing any easier than learning the first?
\newblock In {\em Advances in neural information processing systems}, pages
  640--646, 1996.

\bibitem{tsochantaridis2005}
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun.
\newblock Large margin methods for structured and interdependent output
  variables.
\newblock In {\em Journal of Machine Learning Research}, pages 1453--1484,
  2005.

\bibitem{vilalta2002perspective}
Ricardo Vilalta and Youssef Drissi.
\newblock A perspective view and survey of meta-learning.
\newblock {\em Artificial intelligence review}, 18(2):77--95, 2002.

\bibitem{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et~al.
\newblock Matching networks for one shot learning.
\newblock In {\em Advances in neural information processing systems}, 2016.

\bibitem{vuorio2019multimodal}
Risto Vuorio, Shao-Hua Sun, Hexiang Hu, and Joseph~J Lim.
\newblock Multimodal model-agnostic meta-learning via task-aware modulation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1--12, 2019.

\bibitem{wang2019tafe}
Xin Wang, Fisher Yu, Ruth Wang, Trevor Darrell, and Joseph~E Gonzalez.
\newblock Tafe-net: Task-aware feature embeddings for low shot learning.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1831--1840, 2019.

\bibitem{yao2019hierarchically}
Huaxiu Yao, Ying Wei, Junzhou Huang, and Zhenhui Li.
\newblock Hierarchically structured meta-learning.
\newblock In {\em International Conference on Machine Learning}, pages
  7045--7054, 2019.

\bibitem{yao2007early}
Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto.
\newblock On early stopping in gradient descent learning.
\newblock {\em Constructive Approximation}, 26(2):289--315, 2007.

\bibitem{zintgraf2018fast}
Luisa~M Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon
  Whiteson.
\newblock Fast context adaptation via meta-learning.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning-Volume 70}. JMLR. org, 2019.

\end{thebibliography}
