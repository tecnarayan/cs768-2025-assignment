\begin{thebibliography}{76}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abati et~al.(2020)Abati, Tomczak, Blankevoort, Calderara, Cucchiara,
  and Bejnordi]{abati2020conditional}
Abati, D., Tomczak, J., Blankevoort, T., Calderara, S., Cucchiara, R., and
  Bejnordi, E.
\newblock Conditional channel gated networks for task-aware continual learning.
\newblock In \emph{CVPR}, pp.\  3931--3940, 2020.

\bibitem[Ahn et~al.(2019)Ahn, Cha, Lee, and Moon]{ahn2019neurIPS}
Ahn, H., Cha, S., Lee, D., and Moon, T.
\newblock Uncertainty-based continual learning with adaptive regularization.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Aljundi et~al.(2017)Aljundi, Chakravarty, and
  Tuytelaars]{Aljundi2016expert}
Aljundi, R., Chakravarty, P., and Tuytelaars, T.
\newblock Expert gate: Lifelong learning with a network of experts.
\newblock In \emph{CVPR}, 2017.

\bibitem[Aljundi et~al.(2019)Aljundi, Belilovsky, Tuytelaars, Charlin, Caccia,
  Lin, and Caccia]{aljundi2019online}
Aljundi, R., Belilovsky, E., Tuytelaars, T., Charlin, L., Caccia, M., Lin, M.,
  and Caccia, L.
\newblock Online continual learning with maximal interfered retrieval.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Bennani et~al.(2020)Bennani, Doan, and
  Sugiyama]{bennani2020generalisation}
Bennani, M.~A., Doan, T., and Sugiyama, M.
\newblock Generalisation guarantees for continual learning with orthogonal
  gradient descent.
\newblock \emph{Lifelong Learning Workshop at the ICML}, 2020.

\bibitem[Buzzega et~al.(2020)Buzzega, Boschini, Porrello, Abati, and
  Calderara]{NEURIPS2020_b704ea2c_derpp}
Buzzega, P., Boschini, M., Porrello, A., Abati, D., and Calderara, S.
\newblock Dark experience for general continual learning: a strong, simple
  baseline.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Castro et~al.(2018)Castro, Mar{\'\i}n-Jim{\'e}nez, Guil, Schmid, and
  Alahari]{castro2018end}
Castro, F.~M., Mar{\'\i}n-Jim{\'e}nez, M.~J., Guil, N., Schmid, C., and
  Alahari, K.
\newblock End-to-end incremental learning.
\newblock In \emph{ECCV}, pp.\  233--248, 2018.

\bibitem[Cha et~al.(2021)Cha, Lee, and Shin]{Cha_2021_ICCV_co2l}
Cha, H., Lee, J., and Shin, J.
\newblock Co2l: Contrastive continual learning.
\newblock In \emph{ICCV}, 2021.

\bibitem[Chaudhry et~al.(2019{\natexlab{a}})Chaudhry, Ranzato, Rohrbach, and
  Elhoseiny]{Chaudhry2019ICLR}
Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, M.
\newblock Efficient lifelong learning with a-gem.
\newblock In \emph{ICLR}, 2019{\natexlab{a}}.

\bibitem[Chaudhry et~al.(2019{\natexlab{b}})Chaudhry, Rohrbach, Elhoseiny,
  Ajanthan, Dokania, Torr, and Ranzato]{chaudhry2019continual}
Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P.~K., Torr,
  P.~H., and Ranzato, M.
\newblock Continual learning with tiny episodic memories.
\newblock 2019{\natexlab{b}}.

\bibitem[Chaudhry et~al.(2020)Chaudhry, Khan, Dokania, and
  Torr]{chaudhry2020continual}
Chaudhry, A., Khan, N., Dokania, P.~K., and Torr, P. H.~S.
\newblock Continual learning in low-rank orthogonal subspaces, 2020.

\bibitem[Chaudhry et~al.(2021)Chaudhry, Gordo, Dokania, Torr, and
  Lopez-Paz]{Chaudhry_Gordo_Dokania_Torr_Lopez-Paz_2021_hal}
Chaudhry, A., Gordo, A., Dokania, P., Torr, P., and Lopez-Paz, D.
\newblock Using hindsight to anchor past knowledge in continual learning.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  35\penalty0 (8):\penalty0 6993--7001, May 2021.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/16861}.

\bibitem[Chen \& Liu(2018)Chen and Liu]{chen2018lifelong}
Chen, Z. and Liu, B.
\newblock Lifelong machine learning.
\newblock \emph{Synthesis Lectures on Artificial Intelligence and Machine
  Learning}, 12\penalty0 (3):\penalty0 1--207, 2018.

\bibitem[Dhar et~al.(2019)Dhar, Singh, Peng, Wu, and Chellappa]{Dhar2019CVPR}
Dhar, P., Singh, R.~V., Peng, K., Wu, Z., and Chellappa, R.
\newblock Learning without memorizing.
\newblock In \emph{CVPR}, 2019.

\bibitem[Fang et~al.(2022)Fang, Li, Lu, Dong, Han, and Liu]{fang2022out}
Fang, Z., Li, Y., Lu, J., Dong, J., Han, B., and Liu, F.
\newblock Is out-of-distribution detection learnable?
\newblock \emph{aNeurIPS-2022}, 2022.

\bibitem[Guo et~al.(2022)Guo, Liu, and Zhao]{guo2022online}
Guo, Y., Liu, B., and Zhao, D.
\newblock Online continual learning through mutual information maximization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8109--8126. PMLR, 2022.

\bibitem[Hayes \& Kanan(2020)Hayes and Kanan]{hayes2020lifelong}
Hayes, T.~L. and Kanan, C.
\newblock Lifelong machine learning with deep streaming linear discriminant
  analysis.
\newblock In \emph{CVPR Workshop on Continual Learning}, 2020.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016baseline}
Hendrycks, D. and Gimpel, K.
\newblock A baseline for detecting misclassified and out-of-distribution
  examples in neural networks.
\newblock \emph{arXiv preprint arXiv:1610.02136}, 2016.

\bibitem[Henning et~al.(2021)Henning, Cervera, D'Angelo, Von~Oswald, Traber,
  Ehret, Kobayashi, Grewe, and Sacramento]{henning2021posterior}
Henning, C., Cervera, M., D'Angelo, F., Von~Oswald, J., Traber, R., Ehret, B.,
  Kobayashi, S., Grewe, B.~F., and Sacramento, J.
\newblock Posterior meta-replay for continual learning.
\newblock \emph{NeurIPS}, 34, 2021.

\bibitem[Hou et~al.(2019)Hou, Pan, Loy, Wang, and Lin]{hou2019learning}
Hou, S., Pan, X., Loy, C.~C., Wang, Z., and Lin, D.
\newblock Learning a unified classifier incrementally via rebalancing.
\newblock In \emph{CVPR}, pp.\  831--839, 2019.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{Houlsby2019Parameter}
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De~Laroussilhe, Q.,
  Gesmundo, A., Attariyan, M., and Gelly, S.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2790--2799. PMLR, 2019.

\bibitem[Hu et~al.(2019)Hu, Lin, Liu, Tao, Tao, Ma, Zhao, and
  Yan]{hu2019overcoming}
Hu, W., Lin, Z., Liu, B., Tao, C., Tao, Z., Ma, J., Zhao, D., and Yan, R.
\newblock Overcoming catastrophic forgetting for continual learning via model
  adaptation.
\newblock In \emph{ICLR}, 2019.

\bibitem[Hung et~al.(2019)Hung, Tu, Wu, Chen, Chan, and Chen]{hung2019compact}
Hung, C.-Y., Tu, C.-H., Wu, C.-E., Chen, C.-H., Chan, Y.-M., and Chen, C.-S.
\newblock Compacting, picking and growing for unforgetting continual learning.
\newblock In \emph{NeurIPS}, volume~32, 2019.

\bibitem[Kamra et~al.(2017)Kamra, Gupta, and Liu]{Kamra2017deep}
Kamra, N., Gupta, U., and Liu, Y.
\newblock {Deep Generative Dual Memory Network for Continual Learning}.
\newblock \emph{arXiv preprint arXiv:1710.10368}, 2017.

\bibitem[Ke et~al.(2020)Ke, Liu, and Huang]{ke2020continual}
Ke, Z., Liu, B., and Huang, X.
\newblock Continual learning of a mixed sequence of similar and dissimilar
  tasks.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Ke et~al.(2021)Ke, Liu, Ma, Xu, and Shu]{ke2021achieving}
Ke, Z., Liu, B., Ma, N., Xu, H., and Shu, L.
\newblock Achieving forgetting prevention and knowledge transfer in continual
  learning.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Kemker \& Kanan(2018)Kemker and Kanan]{Kemker2018fearnet}
Kemker, R. and Kanan, C.
\newblock {FearNet: Brain-Inspired Model for Incremental Learning}.
\newblock In \emph{ICLR}, 2018.

\bibitem[Kim \& Liu(2020)Kim and Liu]{kim2020continual}
Kim, G. and Liu, B.
\newblock Continual learning via principal components projection, 2020.
\newblock URL \url{https://openreview.net/forum?id=SkxlElBYDS}.

\bibitem[Kim et~al.(2022{\natexlab{a}})Kim, Ke, and Liu]{kim2022multi}
Kim, G., Ke, Z., and Liu, B.
\newblock A multi-head model for continual learning via out-of-distribution
  replay.
\newblock In \emph{Conference on Lifelong Learning Agents}, pp.\  548--563.
  PMLR, 2022{\natexlab{a}}.

\bibitem[Kim et~al.(2022{\natexlab{b}})Kim, Xiao, Konishi, Ke, and
  Liu]{kim2022theoretical}
Kim, G., Xiao, C., Konishi, T., Ke, Z., and Liu, B.
\newblock A theoretical study on solving continual learning.
\newblock \emph{NeurIPS-2022}, 2022{\natexlab{b}}.

\bibitem[Kim et~al.(2023)Kim, Xiao, Konishi, Ke, and Liu]{kim2023open}
Kim, G., Xiao, C., Konishi, T., Ke, Z., and Liu, B.
\newblock Open-world continual learning: Unifying novelty detection and
  continual learning.
\newblock \emph{arXiv:2304.10038 [cs.LG]}, 2023.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{Kirkpatrick2017overcoming}
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu,
  A.~A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{Krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Technical Report TR-2009, University of Toronto, Toronto.},
  2009.

\bibitem[Le \& Yang(2015)Le and Yang]{Le2015TinyIV}
Le, Y. and Yang, X.
\newblock Tiny imagenet visual recognition challenge, 2015.

\bibitem[Lee et~al.(2019)Lee, Lee, Shin, and Lee]{lee2019overcoming}
Lee, K., Lee, K., Shin, J., and Lee, H.
\newblock Overcoming catastrophic forgetting with unlabeled data in the wild.
\newblock In \emph{CVPR}, 2019.

\bibitem[Lee et~al.(2021)Lee, Goldt, and Saxe]{lee2021continual}
Lee, S., Goldt, S., and Saxe, A.
\newblock Continual learning in the teacher-student setup: Impact of task
  similarity.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6109--6119. PMLR, 2021.

\bibitem[Li \& Hoiem(2016)Li and Hoiem]{Li2016LwF}
Li, Z. and Hoiem, D.
\newblock {Learning Without Forgetting}.
\newblock In \emph{ECCV}, pp.\  614--629. Springer, 2016.

\bibitem[Lin et~al.(2022)Lin, Yang, Fan, and Zhang]{lin2022beyond}
Lin, S., Yang, L., Fan, D., and Zhang, J.
\newblock Beyond not-forgetting: Continual learning with backward knowledge
  transfer.
\newblock \emph{NeurIPS-2022}, 2022.

\bibitem[Liu et~al.(2023)Liu, Mazumder, Robertson, and Grigsby]{liu2023ai}
Liu, B., Mazumder, S., Robertson, E., and Grigsby, S.
\newblock Ai autonomy: Self-initiated open-world continual learning and
  adaptation.
\newblock \emph{AI Magazine}, 2023.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Parisot, Slabaugh, Jia, Leonardis,
  and Tuytelaars]{Liu2020}
Liu, Y., Parisot, S., Slabaugh, G., Jia, X., Leonardis, A., and Tuytelaars, T.
\newblock More classifiers, less forgetting: A generic multi-classifier
  paradigm for incremental learning.
\newblock In \emph{ECCV}, pp.\  699--716. Springer International Publishing,
  2020{\natexlab{a}}.
\newblock \doi{10.1007/978-3-030-58574-7_42}.
\newblock URL \url{https://doi.org/10.1007/978-3-030-58574-7_42}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Su, Liu, Schiele, and
  Sun]{liu2020mnemonics}
Liu, Y., Su, Y., Liu, A.-A., Schiele, B., and Sun, Q.
\newblock Mnemonics training: Multi-class incremental learning without
  forgetting.
\newblock In \emph{CVPR}, 2020{\natexlab{b}}.

\bibitem[Liu et~al.(2021)Liu, Schiele, and Sun]{Liu2020AANets}
Liu, Y., Schiele, B., and Sun, Q.
\newblock Adaptive aggregation networks for class-incremental learning.
\newblock In \emph{CVPR}, 2021.

\bibitem[Lopez-Paz \& Ranzato(2017)Lopez-Paz and Ranzato]{Lopez2017gradient}
Lopez-Paz, D. and Ranzato, M.
\newblock {Gradient Episodic Memory for Continual Learning}.
\newblock In \emph{NeurIPS}, pp.\  6470--6479, 2017.

\bibitem[Mallya \& Lazebnik(2017)Mallya and Lazebnik]{Mallya2017packnet}
Mallya, A. and Lazebnik, S.
\newblock {PackNet: Adding Multiple Tasks to a Single Network by Iterative
  Pruning}.
\newblock \emph{arXiv preprint arXiv:1711.05769}, 2017.

\bibitem[McCloskey \& Cohen(1989)McCloskey and Cohen]{McCloskey1989}
McCloskey, M. and Cohen, N.~J.
\newblock {Catastrophic interference in connectionist networks: The sequential
  learning problem}.
\newblock In \emph{Psychology of learning and motivation}, volume~24, pp.\
  109--165. Elsevier, 1989.

\bibitem[Ostapenko et~al.(2019)Ostapenko, Puscas, Klein, Jahnichen, and
  Nabi]{ostapenko2019learning}
Ostapenko, O., Puscas, M., Klein, T., Jahnichen, P., and Nabi, M.
\newblock Learning to remember: A synaptic plasticity driven framework for
  continual learning.
\newblock In \emph{CVPR}, pp.\  11321--11329, 2019.

\bibitem[Ostapenko et~al.(2021)Ostapenko, Rodriguez, Caccia, and
  Charlin]{ostapenko2021continual}
Ostapenko, O., Rodriguez, P., Caccia, M., and Charlin, L.
\newblock Continual learning via local module composition.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 30298--30312, 2021.

\bibitem[Ostapenko et~al.(2022)Ostapenko, Lesort, Rodr{\'\i}guez, Arefin,
  Douillard, Rish, and Charlin]{ostapenko2022foundational}
Ostapenko, O., Lesort, T., Rodr{\'\i}guez, P., Arefin, M.~R., Douillard, A.,
  Rish, I., and Charlin, L.
\newblock Continual learning with foundation models: An empirical study of
  latent replay.
\newblock \emph{Conference on Lifelong Learning Agents}, 2022.

\bibitem[Pentina \& Lampert(2014)Pentina and Lampert]{pentina2014pac}
Pentina, A. and Lampert, C.
\newblock A pac-bayesian bound for lifelong learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  991--999. PMLR, 2014.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock \emph{arXiv preprint arXiv:2103.00020}, 2021.

\bibitem[Rajasegaran et~al.(2020{\natexlab{a}})Rajasegaran, Hayat, Khan, Khan,
  Shao, and Yang]{rajasegaran2020adaptive}
Rajasegaran, J., Hayat, M., Khan, S., Khan, F.~S., Shao, L., and Yang, M.-H.
\newblock An adaptive random path selection approach for incremental learning,
  2020{\natexlab{a}}.

\bibitem[Rajasegaran et~al.(2020{\natexlab{b}})Rajasegaran, Khan, Hayat, Khan,
  and Shah]{rajasegaran2020itaml}
Rajasegaran, J., Khan, S., Hayat, M., Khan, F.~S., and Shah, M.
\newblock itaml: An incremental task-agnostic meta-learning approach.
\newblock In \emph{CVPR}, 2020{\natexlab{b}}.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Kolesnikov, and Lampert]{Rebuffi2017}
Rebuffi, S.-A., Kolesnikov, A., and Lampert, C.~H.
\newblock {iCaRL: Incremental classifier and representation learning}.
\newblock In \emph{CVPR}, pp.\  5533--5542, 2017.

\bibitem[Ritter et~al.(2018)Ritter, Botev, and Barber]{ritter2018online}
Ritter, H., Botev, A., and Barber, D.
\newblock Online structured laplace approximations for overcoming catastrophic
  forgetting.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Rolnick et~al.(2019)Rolnick, Ahuja, Schwarz, Lillicrap, and
  Wayne]{rolnick2019neurIPS}
Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T.~P., and Wayne, G.
\newblock Experience replay for continual learning.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Rostami et~al.(2019)Rostami, Kolouri, and Pilly]{Rostami2019ijcai}
Rostami, M., Kolouri, S., and Pilly, P.~K.
\newblock Complementary learning for overcoming catastrophic forgetting using
  experience replay.
\newblock In \emph{IJCAI}, 2019.

\bibitem[Rusu et~al.(2016)Rusu, Rabinowitz, Desjardins, Soyer, Kirkpatrick,
  Kavukcuoglu, Pascanu, and Hadsell]{Rusu2016}
Rusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,
  Kavukcuoglu, K., Pascanu, R., and Hadsell, R.
\newblock {Progressive neural networks}.
\newblock \emph{arXiv preprint arXiv:1606.04671}, 2016.

\bibitem[Schwarz et~al.(2018)Schwarz, Luketina, Czarnecki, Grabska-Barwinska,
  Teh, Pascanu, and Hadsell]{schwarz2018progress}
Schwarz, J., Luketina, J., Czarnecki, W.~M., Grabska-Barwinska, A., Teh, Y.~W.,
  Pascanu, R., and Hadsell, R.
\newblock Progress \& compress: A scalable framework for continual learning.
\newblock \emph{arXiv preprint arXiv:1805.06370}, 2018.

\bibitem[Seff et~al.(2017)Seff, Beatson, Suo, and Liu]{Seff2017continual}
Seff, A., Beatson, A., Suo, D., and Liu, H.
\newblock {Continual learning in generative adversarial nets}.
\newblock \emph{arXiv preprint arXiv:1705.08395}, 2017.

\bibitem[Serr{\`{a}} et~al.(2018)Serr{\`{a}}, Sur{\'{i}}s, Miron, and
  Karatzoglou]{Serra2018overcoming}
Serr{\`{a}}, J., Sur{\'{i}}s, D., Miron, M., and Karatzoglou, A.
\newblock {Overcoming catastrophic forgetting with hard attention to the task}.
\newblock In \emph{ICML}, 2018.

\bibitem[Shin et~al.(2017)Shin, Lee, Kim, and Kim]{Shin2017continual}
Shin, H., Lee, J.~K., Kim, J., and Kim, J.
\newblock {Continual learning with deep generative replay}.
\newblock In \emph{NIPS}, pp.\  2994--3003, 2017.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J{\'e}gou]{touvron2021training_deit}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J{\'e}gou,
  H.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10347--10357. PMLR, 2021.

\bibitem[van~de Ven \& Tolias(2019)van~de Ven and Tolias]{van2019three}
van~de Ven, G.~M. and Tolias, A.~S.
\newblock Three scenarios for continual learning.
\newblock \emph{arXiv preprint arXiv:1904.07734}, 2019.

\bibitem[von Oswald et~al.(2020)von Oswald, Henning, Sacramento, and
  Grewe]{von2019continual_hypernet}
von Oswald, J., Henning, C., Sacramento, J., and Grewe, B.~F.
\newblock Continual learning with hypernetworks.
\newblock \emph{ICLR}, 2020.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Li, Feng, and Zhang]{wang2022vim}
Wang, H., Li, Z., Feng, L., and Zhang, W.
\newblock Vim: Out-of-distribution with virtual-logit matching.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  4921--4930, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Zhang, Yang, Yu, Li, Hong, Zhang,
  Li, Zhong, and Zhu]{wang2022memory}
Wang, L., Zhang, X., Yang, K., Yu, L., Li, C., Hong, L., Zhang, S., Li, Z.,
  Zhong, Y., and Zhu, J.
\newblock Memory replay with data compression for continual learning.
\newblock \emph{Proceedings of International Conference on Learning
  Representations (ICLR)}, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{c}})Wang, Zhang, Lee, Zhang, Sun, Ren, Su,
  Perot, Dy, and Pfister]{wang2022learning_l2p}
Wang, Z., Zhang, Z., Lee, C.-Y., Zhang, H., Sun, R., Ren, X., Su, G., Perot,
  V., Dy, J., and Pfister, T.
\newblock Learning to prompt for continual learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  139--149, 2022{\natexlab{c}}.

\bibitem[Wortsman et~al.(2020)Wortsman, Ramanujan, Liu, Kembhavi, Rastegari,
  Yosinski, and Farhadi]{supsup2020}
Wortsman, M., Ramanujan, V., Liu, R., Kembhavi, A., Rastegari, M., Yosinski,
  J., and Farhadi, A.
\newblock Supermasks in superposition.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{NeurIPS}, 2020.

\bibitem[Wu et~al.(2018)Wu, Herranz, Liu, van~de Weijer, Raducanu,
  et~al.]{wu2018memory}
Wu, C., Herranz, L., Liu, X., van~de Weijer, J., Raducanu, B., et~al.
\newblock Memory replay gans: Learning to generate new categories without
  forgetting.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Wu et~al.(2022)Wu, Swaminathan, Li, Ravichandran, Vasconcelos,
  Bhotika, and Soatto]{Wu_2022_CVPR}
Wu, T.-Y., Swaminathan, G., Li, Z., Ravichandran, A., Vasconcelos, N., Bhotika,
  R., and Soatto, S.
\newblock Class-incremental learning with strong pre-trained models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pp.\  9601--9610, June 2022.

\bibitem[Wu et~al.(2019)Wu, Chen, Wang, Ye, Liu, Guo, and Fu]{wu2019large}
Wu, Y., Chen, Y., Wang, L., Ye, Y., Liu, Z., Guo, Y., and Fu, Y.
\newblock Large scale incremental learning.
\newblock In \emph{CVPR}, 2019.

\bibitem[Xu \& Zhu(2018)Xu and Zhu]{xu2018reinforced}
Xu, J. and Zhu, Z.
\newblock Reinforced continual learning.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Yan et~al.(2021)Yan, Xie, and He]{yan2021dynamically}
Yan, S., Xie, J., and He, X.
\newblock Der: Dynamically expandable representation for class incremental
  learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  3014--3023, 2021.

\bibitem[Zeng et~al.(2019)Zeng, Chen, Cui, and Yu]{zeng2019continuous}
Zeng, G., Chen, Y., Cui, B., and Yu, S.
\newblock Continuous learning of context-dependent processing in neural
  networks.
\newblock \emph{Nature Machine Intelligence}, 2019.

\bibitem[Zenke et~al.(2017)Zenke, Poole, and Ganguli]{Zenke2017continual}
Zenke, F., Poole, B., and Ganguli, S.
\newblock {Continual learning through synaptic intelligence}.
\newblock In \emph{ICML}, pp.\  3987--3995, 2017.

\bibitem[Zhu et~al.(2021)Zhu, Zhang, Wang, Yin, and Liu]{Zhu_2021_CVPR_pass}
Zhu, F., Zhang, X.-Y., Wang, C., Yin, F., and Liu, C.-L.
\newblock Prototype augmentation and self-supervision for incremental learning.
\newblock In \emph{CVPR}, 2021.

\end{thebibliography}
