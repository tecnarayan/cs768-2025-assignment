Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models