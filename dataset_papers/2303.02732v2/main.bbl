\begin{thebibliography}{}

\bibitem[Allen, 1974]{allen1974relationship}
Allen, D.~M. (1974).
\newblock The relationship between variable selection and data agumentation and
  a method for prediction.
\newblock {\em technometrics}, 16(1):125--127.

\bibitem[Arlot and Celisse, 2010]{arlot2010survey}
Arlot, S. and Celisse, A. (2010).
\newblock A survey of cross-validation procedures for model selection.
\newblock {\em Statistics surveys}, 4:40--79.

\bibitem[Beirami et~al., 2017]{beirami2017optimal}
Beirami, A., Razaviyayn, M., Shahrampour, S., and Tarokh, V. (2017).
\newblock On optimal generalizability in parametric learning.
\newblock {\em Advances in Neural Information Processing Systems}, 30.

\bibitem[Bourtoule et~al., 2021]{bourtoule2021machine}
Bourtoule, L., Chandrasekaran, V., Choquette-Choo, C.~A., Jia, H., Travers, A.,
  Zhang, B., Lie, D., and Papernot, N. (2021).
\newblock Machine unlearning.
\newblock In {\em 2021 IEEE Symposium on Security and Privacy (SP)}, pages
  141--159. IEEE.

\bibitem[Cao and Yang, 2015]{cao2015towards}
Cao, Y. and Yang, J. (2015).
\newblock Towards making systems forget with machine unlearning.
\newblock In {\em 2015 IEEE Symposium on Security and Privacy}, pages 463--480.
  IEEE.

\bibitem[Chi et~al., 2019]{chi2019nonconvex}
Chi, Y., Lu, Y.~M., and Chen, Y. (2019).
\newblock Nonconvex optimization meets low-rank matrix factorization: An
  overview.
\newblock {\em IEEE Transactions on Signal Processing}, 67(20):5239--5269.

\bibitem[Efron, 1982]{efron1982jackknife}
Efron, B. (1982).
\newblock {\em The jackknife, the bootstrap and other resampling plans}.
\newblock SIAM.

\bibitem[Geisser, 1975]{geisser1975predictive}
Geisser, S. (1975).
\newblock The predictive sample reuse method with applications.
\newblock {\em Journal of the American statistical Association},
  70(350):320--328.

\bibitem[Ghosh et~al., 2020]{ghosh2020approximate}
Ghosh, S., Stephenson, W., Nguyen, T.~D., Deshpande, S., and Broderick, T.
  (2020).
\newblock Approximate cross-validation for structured models.
\newblock {\em Advances in Neural Information Processing Systems},
  33:8741--8752.

\bibitem[Ginart et~al., 2019]{ginart2019making}
Ginart, A., Guan, M., Valiant, G., and Zou, J.~Y. (2019).
\newblock Making ai forget you: Data deletion in machine learning.
\newblock {\em Advances in neural information processing systems}, 32.

\bibitem[Giordano et~al., 2019]{giordano2019swiss}
Giordano, R., Stephenson, W., Liu, R., Jordan, M., and Broderick, T. (2019).
\newblock A swiss army infinitesimal jackknife.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1139--1147. PMLR.

\bibitem[Guo et~al., 2020]{guo2020certified}
Guo, C., Goldstein, T., Hannun, A., and Van Der~Maaten, L. (2020).
\newblock Certified data removal from machine learning models.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, pages 3832--3842.

\bibitem[Izzo et~al., 2021]{izzo2021approximate}
Izzo, Z., Smart, M.~A., Chaudhuri, K., and Zou, J. (2021).
\newblock Approximate data deletion from machine learning models.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2008--2016. PMLR.

\bibitem[Jabbar and Khan, 2015]{jabbar2015methods}
Jabbar, H. and Khan, R.~Z. (2015).
\newblock Methods to avoid over-fitting and under-fitting in supervised machine
  learning (comparative study).
\newblock {\em Computer Science, Communication and Instrumentation Devices},
  70:163--172.

\bibitem[Jaeckel, 1972]{jaeckel1972infinitesimal}
Jaeckel, L. (1972).
\newblock The infinitesimal jackknife. memorandum.
\newblock Technical report, MM 72-1215-11, Bell Lab. Murray Hill, NJ.

\bibitem[Koh and Liang, 2017]{koh2017understanding}
Koh, P.~W. and Liang, P. (2017).
\newblock Understanding black-box predictions via influence functions.
\newblock In {\em International conference on machine learning}, pages
  1885--1894. PMLR.

\bibitem[Loh and Wainwright, 2013]{loh2013regularized}
Loh, P.-L. and Wainwright, M.~J. (2013).
\newblock Regularized m-estimators with nonconvexity: Statistical and
  algorithmic theory for local optima.
\newblock {\em Advances in Neural Information Processing Systems}, 26.

\bibitem[McCullagh and Nelder, 1989]{mccullagh1989generalized}
McCullagh, P. and Nelder, J. (1989).
\newblock Generalized linear models.

\bibitem[Neel et~al., 2021]{neel2021descent}
Neel, S., Roth, A., and Sharifi-Malvajerdi, S. (2021).
\newblock Descent-to-delete: Gradient-based methods for machine unlearning.
\newblock In {\em Algorithmic Learning Theory}, pages 931--962. PMLR.

\bibitem[Obuchi and Kabashima, 2016]{obuchi2016cross}
Obuchi, T. and Kabashima, Y. (2016).
\newblock Cross validation in lasso and its acceleration.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2016(5):053304.

\bibitem[Rad and Maleki, 2020]{rad2020scalable}
Rad, K.~R. and Maleki, A. (2020).
\newblock A scalable estimate of the out-of-sample prediction error via
  approximate leave-one-out cross-validation.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 82(4):965--996.

\bibitem[Stephenson and Broderick, 2020]{stephenson2020approximate}
Stephenson, W. and Broderick, T. (2020).
\newblock Approximate cross-validation in high dimensions with guarantees.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2424--2434. PMLR.

\bibitem[Stone, 1974]{stone1974cross}
Stone, M. (1974).
\newblock Cross-validatory choice and assessment of statistical predictions.
\newblock {\em Journal of the royal statistical society: Series B
  (Methodological)}, 36(2):111--133.

\bibitem[Tsai et~al., 2014]{tsai2014incremental}
Tsai, C.-H., Lin, C.-Y., and Lin, C.-J. (2014).
\newblock Incremental and decremental training for linear classification.
\newblock In {\em Proceedings of the 20th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 343--352.

\bibitem[Wang et~al., 2018]{wang2018approximate}
Wang, S., Zhou, W., Lu, H., Maleki, A., and Mirrokni, V. (2018).
\newblock Approximate leave-one-out for fast parameter tuning in high
  dimensions.
\newblock In {\em International Conference on Machine Learning}, pages
  5228--5237. PMLR.

\bibitem[Wilson et~al., 2020]{wilson2020approximate}
Wilson, A., Kasy, M., and Mackey, L. (2020).
\newblock Approximate cross-validation: Guarantees for model assessment and
  selection.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 4530--4540. PMLR.

\bibitem[Wright and Recht, 2022]{wright2022optimization}
Wright, S.~J. and Recht, B. (2022).
\newblock {\em Optimization for data analysis}.
\newblock Cambridge University Press.

\end{thebibliography}
