\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Krishnamurthy, and
  Sun]{agarwal2020flambe}
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun.
\newblock Flambe: Structural complexity and representation learning of low rank
  mdps.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 20095--20107, 2020.

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'a}ri, and Munos]{antos2008learning}
Andr{\'a}s Antos, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Learning near-optimal policies with bellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock \emph{Machine Learning}, 71\penalty0 (1):\penalty0 89--129, 2008.

\bibitem[Chen and Jiang(2019)]{chen2019information}
Jinglin Chen and Nan Jiang.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1042--1051, 2019.

\bibitem[Chen et~al.(2022)Chen, Yu, Zhu, Yu, Chen, Wang, Wu, Wu, Qin, Ding,
  et~al.]{chen2022adversarial}
Xiong-Hui Chen, Yang Yu, Zheng-Mao Zhu, Zhihua Yu, Zhenjun Chen, Chenghe Wang,
  Yinan Wu, Hongqiu Wu, Rong-Jun Qin, Ruijin Ding, et~al.
\newblock Adversarial counterfactual environment model learning.
\newblock \emph{arXiv preprint arXiv:2206.04890}, 2022.

\bibitem[Cheng et~al.(2022)Cheng, Xie, Jiang, and
  Agarwal]{cheng2022adversarially}
Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal.
\newblock Adversarially trained actor critic for offline reinforcement
  learning.
\newblock \emph{International Conference on Machine Learning}, 2022.

\bibitem[Farahmand et~al.(2010)Farahmand, Munos, and
  Szepesv{\'a}ri]{farahmand2010error}
Amir~Massoud Farahmand, R{\'e}mi Munos, and Csaba Szepesv{\'a}ri.
\newblock Error propagation for approximate policy and value iteration.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2010.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Fujimoto and Gu(2021)]{fujimoto2021minimalist}
Scott Fujimoto and Shixiang~Shane Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 20132--20145, 2021.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and
  Meger]{fujimoto2018addressing}
Scott Fujimoto, Herke Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning}, pages
  1587--1596. PMLR, 2018.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning}, pages
  2052--2062, 2019.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pages
  1861--1870. PMLR, 2018.

\bibitem[Hafner et~al.(2023)Hafner, Pasukonis, Ba, and
  Lillicrap]{hafner2023mastering}
Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap.
\newblock Mastering diverse domains through world models.
\newblock \emph{arXiv preprint arXiv:2301.04104}, 2023.

\bibitem[Jin et~al.(2021)Jin, Yang, and Wang]{jin2021pessimism}
Ying Jin, Zhuoran Yang, and Zhaoran Wang.
\newblock Is pessimism provably efficient for offline rl?
\newblock In \emph{International Conference on Machine Learning}, pages
  5084--5096. PMLR, 2021.

\bibitem[Kakade(2001)]{kakade2001natural}
Sham~M Kakade.
\newblock A natural policy gradient.
\newblock \emph{Advances in Neural Information Processing Systems}, 14, 2001.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{kidambi2020morel}
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
\newblock Morel: Model-based offline reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{3rd International Conference on Learning Representations},
  2015.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Nair, and
  Levine]{kostrikov2021offline}
Ilya Kostrikov, Ashvin Nair, and Sergey Levine.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock \emph{arXiv preprint arXiv:2110.06169}, 2021.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and
  Levine]{kumar2019stabilizing}
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 11784--11794, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1179--1191, 2020.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{lange2012batch}
Sascha Lange, Thomas Gabel, and Martin Riedmiller.
\newblock Batch reinforcement learning.
\newblock \emph{Reinforcement learning: State-of-the-art}, pages 45--73, 2012.

\bibitem[Laroche et~al.(2019)Laroche, Trichelair, and
  Des~Combes]{laroche2019safe}
Romain Laroche, Paul Trichelair, and Remi~Tachet Des~Combes.
\newblock Safe policy improvement with baseline bootstrapping.
\newblock In \emph{International Conference on Machine Learning}, pages
  3652--3661. PMLR, 2019.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Liu et~al.(2022)Liu, Chung, Szepesv{\'{a}}ri, and
  Jin]{liu2022partially}
Qinghua Liu, Alan Chung, Csaba Szepesv{\'{a}}ri, and Chi Jin.
\newblock When is partially observable reinforcement learning not scary?
\newblock In \emph{Conference on Learning Theory}, volume 178, pages
  5175--5220. {PMLR}, 2022.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020off}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Off-policy policy gradient with stationary distribution correction.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 1180--1190.
  PMLR, 2020{\natexlab{a}}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020provably}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Provably good batch off-policy reinforcement learning without great
  exploration.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1264--1274, 2020{\natexlab{b}}.

\bibitem[Munos(2003)]{munos2003error}
R{\'e}mi Munos.
\newblock Error bounds for approximate policy iteration.
\newblock In \emph{Proceedings of the Twentieth International Conference on
  International Conference on Machine Learning}, pages 560--567, 2003.

\bibitem[Munos and Szepesv{\'a}ri(2008)]{munos2008finite}
R{\'e}mi Munos and Csaba Szepesv{\'a}ri.
\newblock Finite-time bounds for fitted value iteration.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (5), 2008.

\bibitem[Rashidinejad et~al.(2021)Rashidinejad, Zhu, Ma, Jiao, and
  Russell]{rashidinejad2021bridging}
Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell.
\newblock Bridging offline reinforcement learning and imitation learning: A
  tale of pessimism.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 11702--11716, 2021.

\bibitem[Rigter et~al.(2022)Rigter, Lacerda, and Hawes]{rigter2022rambo}
Marc Rigter, Bruno Lacerda, and Nick Hawes.
\newblock Rambo-rl: Robust adversarial model-based offline reinforcement
  learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 16082--16097, 2022.

\bibitem[Shi et~al.(2022)Shi, Li, Wei, Chen, and Chi]{shi2022pessimistic}
Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi.
\newblock Pessimistic q-learning for offline reinforcement learning: Towards
  optimal sample complexity.
\newblock In \emph{International Conference on Machine Learning}, pages
  19967--20025. PMLR, 2022.

\bibitem[Siegel et~al.(2020)Siegel, Springenberg, Berkenkamp, Abdolmaleki,
  Neunert, Lampe, Hafner, Heess, and Riedmiller]{siegel2020keep}
Noah~Y Siegel, Jost~Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki,
  Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin
  Riedmiller.
\newblock Keep doing what worked: Behavioral modelling priors for offline
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2002.08396}, 2020.

\bibitem[Silver et~al.(2018)Silver, Allen, Tenenbaum, and
  Kaelbling]{silver2018residual}
Tom Silver, Kelsey Allen, Josh Tenenbaum, and Leslie Kaelbling.
\newblock Residual policy learning.
\newblock \emph{arXiv preprint arXiv:1812.06298}, 2018.

\bibitem[Uehara and Sun(2021)]{uehara2021pessimistic}
Masatoshi Uehara and Wen Sun.
\newblock Pessimistic model-based offline reinforcement learning under partial
  coverage.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[van~de Geer(2000)]{van2000empirical}
Sara~A van~de Geer.
\newblock \emph{Empirical Processes in M-estimation}, volume~6.
\newblock Cambridge university press, 2000.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Xiao et~al.(2021)Xiao, Wu, Mei, Dai, Lattimore, Li, Szepesvari, and
  Schuurmans]{xiao2021optimality}
Chenjun Xiao, Yifan Wu, Jincheng Mei, Bo~Dai, Tor Lattimore, Lihong Li, Csaba
  Szepesvari, and Dale Schuurmans.
\newblock On the optimality of batch policy optimization algorithms.
\newblock In \emph{International Conference on Machine Learning}, pages
  11362--11371. PMLR, 2021.

\bibitem[Xie and Jiang(2020)]{xie2020q}
Tengyang Xie and Nan Jiang.
\newblock Q* approximation schemes for batch reinforcement learning: A
  theoretical comparison.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, pages
  550--559. PMLR, 2020.

\bibitem[Xie and Jiang(2021)]{xie2021batch}
Tengyang Xie and Nan Jiang.
\newblock Batch value-function approximation with only realizability.
\newblock In \emph{International Conference on Machine Learning}, pages
  11404--11413. PMLR, 2021.

\bibitem[Xie et~al.(2021{\natexlab{a}})Xie, Cheng, Jiang, Mineiro, and
  Agarwal]{xie2021bellman}
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal.
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 6683--6694, 2021{\natexlab{a}}.

\bibitem[Xie et~al.(2021{\natexlab{b}})Xie, Jiang, Wang, Xiong, and
  Bai]{xie2021policy}
Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu~Bai.
\newblock Policy finetuning: Bridging sample-efficient offline and online
  reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 27395--27407, 2021{\natexlab{b}}.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{yu2020mopo}
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James~Y Zou, Sergey
  Levine, Chelsea Finn, and Tengyu Ma.
\newblock Mopo: Model-based offline policy optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 14129--14142, 2020.

\bibitem[Yu et~al.(2021)Yu, Kumar, Rafailov, Rajeswaran, Levine, and
  Finn]{yu2021combo}
Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine,
  and Chelsea Finn.
\newblock Combo: Conservative offline model-based policy optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 28954--28967, 2021.

\bibitem[Zanette et~al.(2021)Zanette, Wainwright, and
  Brunskill]{zanette2021provable}
Andrea Zanette, Martin~J Wainwright, and Emma Brunskill.
\newblock Provable benefits of actor-critic methods for offline reinforcement
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Zhang(2006)]{zhang2006eps}
Tong Zhang.
\newblock From $\varepsilon$-entropy to kl-entropy: Analysis of minimum
  information complexity density estimation.
\newblock \emph{The Annals of Statistics}, 34\penalty0 (5):\penalty0
  2180--2210, 2006.

\end{thebibliography}
