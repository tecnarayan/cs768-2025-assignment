@article{boyko2021tt,
  title={TT-QI: Faster value iteration in tensor train format for stochastic optimal control},
  author={Boyko, AI and Oseledets, IV and Ferrer, G},
  journal={Computational Mathematics and Mathematical Physics},
  volume={61},
  number={5},
  pages={836--846},
  year={2021},
  publisher={Springer}
}
@article{jamil2013literature,
  title={A literature survey of benchmark functions for global optimisation problems},
  author={Jamil, Momin and Yang, Xin-She},
  journal={International Journal of Mathematical Modelling and Numerical Optimisation},
  volume={4},
  number={2},
  pages={150--194},
  year={2013},
  publisher={Inderscience Publishers Ltd}
}

@article{oseledets2009breaking,
  title={Breaking the curse of dimensionality, or how to use SVD in many dimensions},
  author={Oseledets, Ivan V and Tyrtyshnikov, Eugene E},
  journal={SIAM Journal on Scientific Computing},
  volume={31},
  number={5},
  pages={3744--3759},
  year={2009},
  publisher={SIAM}
}

@inproceedings{NEURIPS2018_worldmodels,
 author = {Ha, David and Schmidhuber, J\"{u}rgen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Recurrent World Models Facilitate Policy Evolution},
 urlspec = {https://proceedings.neurips.cc/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{Georgiev2001OnSE,
  title={On Some Extensions Of The Natural Gradient Algorithm},
  booktitle = {Third International Conference on Independent Component Analysis and Blind. Signal Separation},
  author={Pando G. Georgiev and Andrzej Cichocki and Shun‐ichi Amari},
  year={2001}
}

@book{blind,
author = {Cichocki, Andrzej and Amari, Shun-ichi},
title = {Adaptive Blind Signal and Image Processing: Learning Algorithms and Applications},
year = {2002},
isbn = {0470845899},
publisher = {John Wiley \& Sons, Inc.},
chapter = {6},
pages = {231-272},
address = {USA},
abstract = {}
}


@InProceedings{pmlr-v97-maheswaranathan19a,
  title = 	 {Guided evolutionary strategies: augmenting random search with surrogate gradients},
  author =       {Maheswaranathan, Niru and Metz, Luke and Tucker, George and Choi, Dami and Sohl-Dickstein, Jascha},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4264--4273},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/maheswaranathan19a/maheswaranathan19a.pdf},
  urlspec = 	 {https://proceedings.mlr.press/v97/maheswaranathan19a.html},
  abstract = 	 {Many applications in machine learning require optimizing a function whose true gradient is unknown or computationally expensive, but where surrogate gradient information, directions that may be correlated with the true gradient, is cheaply available. For example, this occurs when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in reinforcement learning or training networks with discrete variables). We propose Guided Evolutionary Strategies (GES), a method for optimally using surrogate gradient directions to accelerate random search. GES defines a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients and estimates a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace and use this to derive a setting of the hyperparameters that works well across problems. We evaluate GES on several example problems, demonstrating an improvement over both standard evolutionary strategies and first-order methods that directly follow the surrogate gradient.}
}


@article{Nevergrad,
author = {Bennet, Pauline and Doerr, Carola and Moreau, Antoine and Rapin, Jeremy and Teytaud, Fabien and Teytaud, Olivier},
title = {Nevergrad: Black-Box Optimization Platform},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
url = {https://doi.org/10.1145/3460310.3460312},
doi = {10.1145/3460310.3460312},
abstract = {Nevergrad is an open source platform for black-box optimization.Join the user group!And if you like Nevergrad, please support us by adding a star on GitHub (https://github.com/facebookresearch/nevergrad).},
journal = {SIGEVOlution},
month = {apr},
pages = {8–15},
numpages = {8}
}

  



@article{HU2015534,
title = {Application of evolutionary computation for rule discovery in stock algorithmic trading: A literature review},
journal = {Applied Soft Computing},
volume = {36},
pages = {534-551},
year = {2015},
issn = {1568-4946},
doispec = {https://doi.org/10.1016/j.asoc.2015.07.008},
urlspec = {https://www.sciencedirect.com/science/article/pii/S156849461500438X},
author = {Yong Hu and Kang Liu and Xiangzhou Zhang and Lijun Su and E.W.T. Ngai and Mei Liu},
keywords = {Literature review, Evolutionary computation, Algorithmic trading, Stock trading rule, Rule discovery, Classification framework},
abstract = {Despite the wide application of evolutionary computation (EC) techniques to rule discovery in stock algorithmic trading (AT), a comprehensive literature review on this topic is unavailable. Therefore, this paper aims to provide the first systematic literature review on the state-of-the-art application of EC techniques for rule discovery in stock AT. Out of 650 articles published before 2013 (inclusive), 51 relevant articles from 24 journals were confirmed. These papers were reviewed and grouped into three analytical method categories (fundamental analysis, technical analysis, and blending analysis) and three EC technique categories (evolutionary algorithm, swarm intelligence, and hybrid EC techniques). A significant bias toward the applications of genetic algorithm-based (GA) and genetic programming-based (GP) techniques in technical trading rule discovery is observed. Other EC techniques and fundamental analysis lack sufficient study. Furthermore, we summarize the information on the evaluation scheme of selected papers and particularly analyze the researches which compare their models with buy and hold strategy (B&H). We observe an interesting phenomenon where most of the existing techniques perform effectively in the downtrend and poorly in the uptrend, and considering the distribution of research in the classification framework, we suggest that this phenomenon can be attributed to the inclination of factor selections and problem in transaction cost selections. We also observe the significant influence of the transaction cost change on the margins of excess return. Other influenced factors are also presented in detail. The absence of ways for market trend prediction and the selection of transaction cost are two major limitations of the studies reviewed. In addition, the combination of trading rule discovery techniques and portfolio selection is a major research gap. Our review reveals the research focus and gaps in applying EC techniques for rule discovery in stock AT and suggests a roadmap for future research.}
}

@Inbook{Schwefel1977,
author={Schwefel, Hans-Paul},
title={Evolutionsstrategien f{\"u}r die numerische Optimierung},
bookTitle={Numerische Optimierung von Computer-Modellen mittels der Evolutionsstrategie: Mit einer vergleichenden Einf{\"u}hrung in die Hill-Climbing- und Zufallsstrategie},
year={1977},
publisher={Birkh{\"a}user Basel},
address={Basel},
pages={123-176},
abstract={Das Unterfangen, biologische Strukturen und Prozesse nachzuahmen, mit dem Ziel, technische Aufgaben zu bew{\"a}ltigen, ist so alt wie die Technik selbst. Die Sage von D{\"a}dalus und Ikarus ist ein fr{\"u}hes Zeugnis solchen menschlichen Bestrebens. Im Zeichen der Verwissenschaftlichung hat sich daraus ein eigener Zweig der Naturwissenschaften gebildet: die Bionik (siehe z.B. Hertel (1963), G{\'e}rardin (1968), Beier und Gla{\ss} (1968), Nachtigall (1971), Heynert (1972)). In ihr geht es um das Erkennen (vorliegender) biologischer L{\"o}sungen f{\"u}r bestimmte, in der Technik ebenfalls auftretende Aufgabenstellungen und um ad{\"a}quate Nachahmung der Vorbilder. Ausgegangen wird dabei stets von der Vermutung, da{\ss} die Evolution besonders gute, eventuell sogar optimale, L{\"o}sungen gefunden hat. Diese Annahme hat sich in vielen F{\"a}llen als richtig bzw.n{\"u}tzlich erwiesen. Nur wenige Bem{\"u}hungen sind bekannt, die Entwicklungsmethode der Natur selbst nachzuahmen (Ashby (1960), Bremermann (1962--1973), Rechenberg (1964,1973); siehe auch Kapitel 4), weil sie seltsamerweise von vornherein als besonders schlecht, das hei{\ss}t aufwendig, angesehen wird.},
isbn={978-3-0348-5927-1},
doispec={10.1007/978-3-0348-5927-1_5},
urlspec={https://doi.org/10.1007/978-3-0348-5927-1_5}
}

@ARTICLE{AMARI-NG1,
author={Amari, Shun-ichi},
journal={Neural Computation}, 
title={Natural Gradient Works Efficiently in Learning}, 
year={1998},
volume={10},
number={2},
pages={251-276},
doispec={10.1162/089976698300017746}
}
  
@INPROCEEDINGS{AMARI-NG2,
  author={Amari, S. and Douglas, S.C.},
  booktitle={Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)}, 
  title={Why natural gradient?}, 
  year={1998},
  volume={2},
  number={},
  pages={1213-1216 vol.2},
  doispec={10.1109/ICASSP.1998.675489}
}

@inproceedings{ActiveSubspace,
  title={Active Subspaces - Emerging Ideas for Dimension Reduction in Parameter Studies},
  author={Paul G. Constantine},
  booktitle={SIAM spotlights},
  doispec = {10.1137/1.9781611973860},
  year={2015}
}
  
  
@ARTICLE{4634579,
  author={Hansen, Nikolaus and Niederberger, AndrÉ S. P. and Guzzella, Lino and Koumoutsakos, Petros},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={A Method for Handling Uncertainty in Evolutionary Optimization With an Application to Feedback Control of Combustion}, 
  year={2009},
  volume={13},
  number={1},
  pages={180-197},
  doispec={10.1109/TEVC.2008.924423}}

@Article{Khoromskij2011,
author={Khoromskij, Boris N.},
title={O(dlog{\thinspace}N)-Quantics Approximation of N-d Tensors in High-Dimensional Numerical Modeling},
journal={Constructive Approximation},
year={2011},
month={Oct},
day={01},
volume={34},
number={2},
pages={257-280},
abstract={},
issn={1432-0940},
doispec={10.1007/s00365-011-9131-1},
urlspec={https://doi.org/10.1007/s00365-011-9131-1}
}

@inproceedings{10.1145/1068009.1068366,
author = {{Hasenj\"{a}ger, Martina and Sendhoff, Bernhard and Sonoda, Toyotaka and Arima, Toshiyuki}},
title = {Three Dimensional Evolutionary Aerodynamic Design Optimization with CMA-ES},
year = {2005},
isbn = {1595930108},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
urlspec = {https://doi.org/10.1145/1068009.1068366},
doispec = {10.1145/1068009.1068366},
abstract = {In this paper, we present the application of evolutionary optimization methods to a demanding, industrially relevant engineering domain, the three-dimensional optimization of gas turbine stator blades. This optimization problem is high-dimensional search and computationally very expensive. We show that, despite of its difficulty, the problem is feasible. Our approach not only successfully optimizes the aerodynamic design but also yields interesting results from an engineering point of view.},
booktitle = {Proceedings of the 7th Annual Conference on Genetic and Evolutionary Computation},
pages = {2173–2180},
numpages = {8},
keywords = {real world application, covariance matrix adaptation, evolutionary strategies, design optimization},
location = {Washington DC, USA},
series = {GECCO '05}}

@inproceedings{10.1145/1143997.1144285,
author = {Subramanian, Harish and Ramamoorthy, Subramanian and Stone, Peter and Kuipers, Benjamin J.},
title = {Designing Safe, Profitable Automated Stock Trading Agents Using Evolutionary Algorithms},
year = {2006},
isbn = {1595931864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
urlspec = {https://doi.org/10.1145/1143997.1144285},
doispec = {10.1145/1143997.1144285},
abstract = {Trading rules are widely used by practitioners as an effective means to mechanize aspects of their reasoning about stock price trends. However, due to the simplicity of these rules, each rule is susceptible to poor behavior in specific types of adverse market conditions. Naive combinations of such rules are not very effective in mitigating the weaknesses of component rules. We demonstrate that sophisticated approaches to combining these trading rules enable us to overcome these problems and gainfully utilize them in autonomous agents. We achieve this combination through the use of genetic algorithms and genetic programs. Further, we show that it is possible to use qualitative characterizations of stochastic dynamics to improve the performance of these agents by delineating safe, or feasible, regions. We present the results of experiments conducted within the Penn-Lehman Automated Trading project. In this way we are able to demonstrate that autonomous agents can achieve consistent profitability in a variety of market conditions, in ways that are human competitive.},
booktitle = {Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation},
pages = {1777–1784},
numpages = {8},
keywords = {genetic algorithms, genetic programming, fitness evaluation, finance, application},
location = {Seattle, Washington, USA},
series = {GECCO '06}
}

@INPROCEEDINGS{8682231,

  author={Zhao, Qibin and Sugiyama, Masashi and Yuan, Longhao and Cichocki, Andrzej},

  booktitle={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 

  title={Learning Efficient Tensor Representations with Ring-structured Networks}, 

  year={2019},

  volume={},

  number={},

  pages={8608-8612},

  doispec={10.1109/ICASSP.2019.8682231}
}

@INPROCEEDINGS{4425031,
  author={Jebalia, M. and Auger, A. and Schoenauer, M. and James, F. and Postel, M.},
  booktitle={2007 IEEE Congress on Evolutionary Computation}, 
  title={Identification of the Isotherm Function in Chromatography Using CMA-ES}, 
  year={2007},
  volume={},
  number={},
  pages={4289-4296},
  doispec={10.1109/CEC.2007.4425031}}

@ARTICLE{phan2020a,
  author={Phan, Anh-Huy and Cichocki, Andrzej and Uschmajew, André and Tichavský, Petr and Luta, George and Mandic, Danilo P.},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Tensor Networks for Latent Variable Analysis: Novel Algorithms for Tensor Train Approximation}, 
  year={2020},
  volume={31},
  number={11},
  pages={4622-4636},
  doispec={10.1109/TNNLS.2019.2956926}
}

@article{ppo-Schulman2017ProximalPO,
  title={Proximal Policy Optimization Algorithms},
  author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  journal={ArXiv},
  year={2017},
  volume={abs/1707.06347}
}

@article{JMLR:v15:wierstra14a,
  author  = {Daan Wierstra and Tom Schaul and Tobias Glasmachers and Yi Sun and Jan Peters and J\"{u}rgen Schmidhuber},
  title   = {Natural Evolution Strategies},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {27},
  pages   = {949-980},
  urlspec     = {http://jmlr.org/papers/v15/wierstra14a.html}
}

@InProceedings{trpo-pmlr-v37-schulman15,
  title = 	 {Trust Region Policy Optimization},
  author = 	 {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1889--1897},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schulman15.pdf},
  urlspec = 	 {https://proceedings.mlr.press/v37/schulman15.html},
  abstract = 	 {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
}

@phdthesis{Coulom-2002a,
 author = "R\'emi Coulom",
 title = "Reinforcement Learning Using Neural Networks, with Applications
          to Motor Control",
 school = "Institut National Polytechnique de Grenoble",
 year = 2002
}

@article{Ramesh2021ZeroShotTG,
  title={Zero-Shot Text-to-Image Generation},
  author={Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.12092}
}

@article{PhysRevResearch.2.033429,
  title = {Ab initio solution of the many-electron Schr\"odinger equation with deep neural networks},
  author = {Pfau, David and Spencer, James S. and Matthews, Alexander G. D. G. and Foulkes, W. M. C.},
  journal = {Phys. Rev. Research},
  volume = {2},
  issue = {3},
  pages = {033429},
  numpages = {20},
  year = {2020},
  month = {Sep},
  publisher = {American Physical Society},
  doispec = {10.1103/PhysRevResearch.2.033429},
  urlspec = {https://link.aps.org/doi/10.1103/PhysRevResearch.2.033429}
}

@article{Brockman2016OpenAIG,
  title={OpenAI Gym},
  author={Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.01540}
}

@article{nes2014,
  author  = {Daan Wierstra and Tom Schaul and Tobias Glasmachers and Yi Sun and Jan Peters and J\"{u}rgen Schmidhuber},
  title   = {Natural Evolution Strategies},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {27},
  pages   = {949-980},
  url     = {http://jmlr.org/papers/v15/wierstra14a.html}
}

@article{kolda2003ds,
author = {Kolda, Tamara G. and Lewis, Robert Michael and Torczon, Virginia},
title = {Optimization by Direct Search: New Perspectives on Some Classical and Modern Methods},
journal = {SIAM Review},
volume = {45},
number = {3},
pages = {385-482},
year = {2003},
doispec = {10.1137/S003614450242889},
urlspec = {https://doi.org/10.1137/S003614450242889}
}

@article{constraint-es2,
title = {Handling bound constraints in CMA-ES: An experimental study},
journal = {Swarm and Evolutionary Computation},
volume = {52},
pages = {100627},
year = {2020},
issn = {2210-6502},
doispec = {https://doi.org/10.1016/j.swevo.2019.100627},
urlspec = {https://www.sciencedirect.com/science/article/pii/S2210650219301622},
author = {Rafał Biedrzycki},
keywords = {Bound constraints, CMA-ES},
abstract = {},
}

@article{constraint-es1,
author = {Kramer, Oliver},
title = {A Review of Constraint-Handling Techniques for Evolution Strategies},
year = {2010},
issue_date = {January 2010},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2010},
issn = {1687-9724},
urlspec = {https://doi.org/10.1155/2010/185063},
doispec = {},
journal = {Appl. Comp. Intell. Soft Comput.},
month = jan,
articleno = {3},
numpages = {19}
}

@InProceedings{pmlr-v80-choromanski18a,
  title = 	 {Structured Evolution with Compact Architectures for Scalable Policy Optimization},
  author =       {Choromanski, Krzysztof and Rowland, Mark and Sindhwani, Vikas and Turner, Richard and Weller, Adrian},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {970--978},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/choromanski18a/choromanski18a.pdf},
  urlspec = 	 {https://proceedings.mlr.press/v80/choromanski18a.html},
  abstract = 	 {}
}

@article{TRES, title={Trust Region Evolution Strategies}, volume={33}, urlspec={https://ojs.aaai.org/index.php/AAAI/article/view/4345}, doispec={10.1609/aaai.v33i01.33014352}, abstractNote={&lt;p&gt;Evolution Strategies (ES), a class of black-box optimization algorithms, has recently been demonstrated to be a viable alternative to popular MDP-based RL techniques such as Qlearning and Policy Gradients. ES achieves fairly good performance on challenging reinforcement learning problems and is easier to scale in a distributed setting. However, standard ES algorithms perform one gradient update per data sample, which is not very efficient. In this paper, with the purpose of more efficient using of sampled data, we propose a novel iterative procedure that optimizes a surrogate objective function, enabling to reuse data sample for multiple epochs of updates. We prove monotonic improvement guarantee for such procedure. By making several approximations to the theoretically-justified procedure, we further develop a practical algorithm called Trust Region Evolution Strategies (TRES). Our experiments demonstrate the effectiveness of TRES on a range of popular MuJoCo locomotion tasks in the OpenAI Gym, achieving better performance than ES algorithm.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Liu, Guoqing and Zhao, Li and Yang, Feidiao and Bian, Jiang and Qin, Tao and Yu, Nenghai and Liu, Tie-Yan}, year={2019}, month={Jul.}, pages={4352-4359} }

@Article{Nesterov2017,
author={Nesterov, Yurii
and Spokoiny, Vladimir},
title={Random Gradient-Free Minimization of Convex Functions},
journal={Foundations of Computational Mathematics},
year={2017},
month={Apr},
day={01},
volume={17},
number={2},
pages={527-566},
abstract={In this paper, we prove new complexity bounds for methods of convex optimization based only on computation of the function value. The search directions of our schemes are normally distributed random Gaussian vectors. It appears that such methods usually need at most n times more iterations than the standard gradient methods, where n is the dimension of the space of variables. This conclusion is true for both nonsmooth and smooth problems. For the latter class, we present also an accelerated scheme with the expected rate of convergence {\$}{\$}O{\backslash}Big ({\{}n^2 {\backslash}over k^2{\}}{\backslash}Big ){\$}{\$}, where k is the iteration counter. For stochastic optimization, we propose a zero-order scheme and justify its expected rate of convergence {\$}{\$}O{\backslash}Big ({\{}n {\backslash}over k^{\{}1/2{\}}{\}}{\backslash}Big ){\$}{\$}. We give also some bounds for the rate of convergence of the random gradient-free methods to stationary points of nonconvex functions, for both smooth and nonsmooth cases. Our theoretical results are supported by preliminary computational experiments.},
issn={1615-3383},
doispec={10.1007/s10208-015-9296-2},
urlspec={https://doi.org/10.1007/s10208-015-9296-2}
}

@incollection{goreinov2010find,
  title={How to find a good submatrix},
  author={Goreinov, Sergei A and Oseledets, Ivan V and Savostyanov, Dimitry V and Tyrtyshnikov, Eugene E and Zamarashkin, Nikolay L},
  booktitle={Matrix Methods: Theory, Algorithms And Applications: Dedicated to the Memory of Gene Golub},
  pages={247--256},
  year={2010},
  publisher={World Scientific}
}

@inproceedings{erl,
 author = {Khadka, Shauharda and Tumer, Kagan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Evolution-Guided Policy Gradient in Reinforcement Learning},
 urlspec = {https://proceedings.neurips.cc/paper/2018/file/85fc37b18c57097425b52fc7afbb6969-Paper.pdf},
 volume = {31},
 year = {2018}
}



@article{Zhang2021OnFPGATW,
  title={On-FPGA Training with Ultra Memory Reduction: A Low-Precision Tensor Method},
  author={Kaiqi Zhang and Cole Hawkins and Xiyuan Zhang and Cong Hao and Zheng Zhang},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.03420}
}

@article{goreinov1997theory,
  title={A theory of pseudoskeleton approximations},
  author={Goreinov, Sergei A and Tyrtyshnikov, Eugene E and Zamarashkin, Nickolai L},
  journal={Linear algebra and its applications},
  volume={261},
  number={1-3},
  pages={1--21},
  year={1997},
  publisher={Elsevier}
}

@article{holtz2012alternating,
  title={The alternating linear scheme for tensor optimization in the tensor train format},
  author={Holtz, Sebastian and Rohwedder, Thorsten and Schneider, Reinhold},
  journal={SIAM Journal on Scientific Computing},
  volume={34},
  number={2},
  pages={A683--A713},
  year={2012},
  publisher={SIAM}
}

@article{oseledets2011tensor,
  title={Tensor-train decomposition},
  author={Oseledets, I. V.},
  journal={SIAM Journal on Scientific Computing},
  volume={33},
  number={5},
  pages={2295--2317},
  year={2011},
  publisher={SIAM}
}

@article{oseledets2010ttcross,
  title={{TT-cross} approximation for multidimensional arrays},
  author={Oseledets, Ivan V and Tyrtyshnikov, Eugene E},
  journal={Linear Algebra and its Applications},
  volume={432},
  number={1},
  pages={70--88},
  year={2010},
  publisher={Elsevier}
}

@article{holland92,
 ISSN = {00368733, 19467087},
 urlspec = {http://www.jstor.org/stable/24939139},
 author = {John H. Holland},
 journal = {Scientific American},
 number = {1},
 pages = {66--73},
 publisher = {Scientific American, a division of Nature America, Inc.},
 title = {Genetic Algorithms},
 volume = {267},
 year = {1992}
}

@Article{Qian2021,
author={Qian, Hong
and Yu, Yang},
title={Derivative-free reinforcement learning: a review},
journal={Frontiers of Computer Science},
year={2021},
month={Sep},
day={01},
volume={15},
number={6},
pages={156336},
abstract={},
issn={2095-2236},
doispec={10.1007/s11704-020-0241-4},
urlspec={https://doi.org/10.1007/s11704-020-0241-4}
}

@article{oseledets2010approximation,
   author={Oseledets, I. V.},
   title={Approximation of $2^d \times 2^d$ matrices using tensor decomposition},
   journal={SIAM J. Matrix Anal. Appl.},
   volume={31},
   number={4},
   pages={2130-2145},
   year={2010},
   database={wos,scopus},
}

@article{tyrtyshnikov2000incomplete,
  title={Incomplete cross approximation in the mosaic--skeleton method},
  author={Tyrtyshnikov, E. E.},
  journal={Computing},
  volume={64},
  number={4},
  pages={367--380},
  year={2000},
  publisher={Springer}
}



@inproceedings{savostyanov2011fast,
  title={Fast adaptive interpolation of multi-dimensional arrays in tensor train format},
  author={Savostyanov, Dmitry and Oseledets, Ivan},
  booktitle={The 2011 International Workshop on Multidimensional (nD) Systems},
  pages={1--8},
  year={2011},
  organization={IEEE}
}

@inproceedings{
gangwani2018genetic,
title={Genetic Policy Optimization},
author={Tanmay Gangwani and Jian Peng},
booktitle={International Conference on Learning Representations},
year={2018},
urlspec={https://openreview.net/forum?id=ByOnmlWC-},
}

@INPROCEEDINGS{mujoco,

  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
  title={MuJoCo: A physics engine for model-based control}, 
  year={2012},
  volume={},
  number={},
  pages={5026-5033},
  doispec={10.1109/IROS.2012.6386109}}


@article{dolgov2020parallel,
  title={Parallel cross interpolation for high-precision calculation of high-dimensional integrals},
  author={Dolgov, Sergey and Savostyanov, Dmitry},
  journal={Computer Physics Communications},
  volume={246},
  pages={106869},
  year={2020},
  publisher={Elsevier}
}
@article{Hubbs2020ORGymAR,
  title={OR-Gym: A Reinforcement Learning Library for Operations Research Problem},
  author={Christian D. Hubbs and Hector D. Perez and Owais Sarwar and N. Sahinidis and I. Grossmann and J. Wassick},
  journal={ArXiv},
  year={2020},
  volume={abs/2008.06319}
}

@article{Salimans2017EvolutionSA,
  title={Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
  author={Tim Salimans and Jonathan Ho and Xi Chen and Ilya Sutskever},
  journal={ArXiv},
  year={2017},
  volume={abs/1703.03864}
}

@Inbook{Hansen2006,
author="Hansen, Nikolaus",
title="The CMA Evolution Strategy: A Comparing Review",
bookTitle="Towards a New Evolutionary Computation: Advances in the Estimation of Distribution Algorithms",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="75--102",
abstract="",
isbn="978-3-540-32494-2",
doispec="10.1007/3-540-32494-1_4",
urlspec="https://doi.org/10.1007/3-540-32494-1_4"
}



@article{Mnih2015,
    author={Mnih, Volodymyr
    and Kavukcuoglu, Koray
    and Silver, David
    and Rusu, Andrei A.
    and Veness, Joel
    and Bellemare, Marc G.
    and Graves, Alex
    and Riedmiller, Martin
    and Fidjeland, Andreas K.
    and Ostrovski, Georg
    and Petersen, Stig
    and Beattie, Charles
    and Sadik, Amir
    and Antonoglou, Ioannis
    and King, Helen
    and Kumaran, Dharshan
    and Wierstra, Daan
    and Legg, Shane
    and Hassabis, Demis},
    title={Human-level control through deep reinforcement learning},
    journal={Nature},
    year={2015},
    month={Feb},
    day={01},
    volume={518},
    number={7540},
    pages={529-533},
    abstract={An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
    issn={1476-4687},
    doispec={10.1038/nature14236},
    urlspec={https://doi.org/10.1038/nature14236}
}

@article{holland73,
author = {Holland, John H.},
title = {Genetic Algorithms and the Optimal Allocation of Trials},
journal = {SIAM Journal on Computing},
volume = {2},
number = {2},
pages = {88-105},
year = {1973},
doispec = {10.1137/0202009},
urlspec = {https://doi.org/10.1137/0202009},
eprint = {https://doi.org/10.1137/0202009}
}

@article{mikhalev2018rectangular,
  title={Rectangular maximum-volume submatrices and their applications},
  author={Mikhalev, Aleksandr and Oseledets, Ivan V},
  journal={Linear Algebra and its Applications},
  volume={538},
  pages={187--211},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{NIPS1999_6449f44a,
 author = {Konda, Vijay and Tsitsiklis, John},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 pages = {},
 publisher = {MIT Press},
 title = {Actor-Critic Algorithms},
 urlspec = {https://proceedings.neurips.cc/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf},
 volume = {12},
 year = {2000}
}

@inproceedings{SuttonNIPS99,
    author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
    title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
    year = {1999},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
    abstract = {},
    booktitle = {Proceedings of the 12th International Conference on Neural Information Processing Systems},
    pages = {1057–1063},
    numpages = {7},
    location = {Denver, CO},
    series = {NIPS'99}
}

@inproceedings{es-not-fd-approx,
author = {Lehman, Joel and Chen, Jay and Clune, Jeff and Stanley, Kenneth O.},
title = {ES is More than Just a Traditional Finite-Difference Approximator},
year = {2018},
isbn = {9781450356183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
urlspec = {https://doi.org/10.1145/3205455.3205474},
doispec = {10.1145/3205455.3205474},
abstract = {},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {450–457},
numpages = {8},
keywords = {finite differences, robustness, evolution strategies, neuroevolution},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@inproceedings{NEURIPS2019_asebo,
 author = {Choromanski, Krzysztof M and Pacchiano, Aldo and Parker-Holder, Jack and Tang, Yunhao and Sindhwani, Vikas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {From Complexity to Simplicity: Adaptive ES-Active Subspaces for Blackbox Optimization},
 urlspec = {https://proceedings.neurips.cc/paper/2019/file/88bade49e98db8790df275fcebb37a13-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{Schweidtmann_2018,
   title={Deterministic Global Optimization with Artificial Neural Networks Embedded},
   volume={180},
   ISSN={1573-2878},
   urlspec={http://dx.doi.org/10.1007/s10957-018-1396-0},
   doispec={10.1007/s10957-018-1396-0},
   number={3},
   journal={Journal of Optimization Theory and Applications},
   publisher={Springer Science and Business Media LLC},
   author={Schweidtmann, Artur M. and Mitsos, Alexander},
   year={2018},
   month={Oct},
   pages={925–948}
}

@article{MAL-067,
urlspec = {http://dx.doi.org/10.1561/2200000067},
year = {2017},
volume = {9},
journal = {Foundations and Trends® in Machine Learning},
title = {Tensor Networks for Dimensionality Reduction and Large-scale Optimization: Part 2 Applications and Future Perspectives},
doispec = {10.1561/2200000067},
issn = {1935-8237},
number = {6},
pages = {431-673},
author = {Andrzej Cichocki and Anh-Huy Phan and Qibin Zhao and Namgil Lee and Ivan Oseledets and Masashi Sugiyama and Danilo P. Mandic}
}

@article{CichockiBookPart1MAL059,
urlspec = {http://dx.doi.org/10.1561/2200000059},
year = {2016},
volume = {9},
journal = {Foundations and Trends® in Machine Learning},
title = {Tensor Networks for Dimensionality Reduction and Large-scale Optimization: Part 1 Low-Rank Tensor Decompositions},
doispec = {10.1561/2200000059},
issn = {1935-8237},
number = {4-5},
pages = {249-429},
author = {Andrzej Cichocki and Namgil Lee and Ivan Oseledets and Anh-Huy Phan and Qibin Zhao and Danilo P. Mandic}
}

@Article{Batselier2021,
author={Batselier, Kim
and Cichocki, Andrzej
and Wong, Ngai},
title={MERACLE: Constructive Layer-Wise Conversion of a Tensor Train into a MERA},
journal={Communications on Applied Mathematics and Computation},
year={2021},
month={Jun},
day={01},
volume={3},
number={2},
pages={257-279},
abstract={},
issn={2661-8893},
doispec={10.1007/s42967-020-00090-6},
urlspec={https://doi.org/10.1007/s42967-020-00090-6}
}

@inproceedings{NEURIPS2018_ARS,
 author = {Mania, Horia and Guy, Aurelia and Recht, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Simple random search of static linear policies is competitive for reinforcement learning},
 urlspec = {https://proceedings.neurips.cc/paper/2018/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{ALARIE2021100011,
title = {Two decades of blackbox optimization applications},
journal = {EURO Journal on Computational Optimization},
volume = {9},
pages = {100011},
year = {2021},
issn = {2192-4406},
doispec = {https://doi.org/10.1016/j.ejco.2021.100011},
author = {Stéphane Alarie and Charles Audet and Aïmen E. Gheribi and Michael Kokkolaras and Sébastien {Le Digabel}},
keywords = {Blackbox optimization, Derivative-free optimization, Mesh adaptive direct search, Applications},
abstract = {This article reviews blackbox optimization applications of direct search optimization methods over the past twenty years. Emphasis is placed on the Mesh Adaptive Direct Search (Mads) derivative-free optimization algorithm. The main focus is on applications in three specific fields: energy, materials science, and computational engineering design. Nevertheless, other applications in science and engineering, including patents, are also considered. The breadth of applications demonstrates the versatility of Mads and highlights the evolution of its accompanying software NOMAD as a standard tool for blackbox optimization.}
}

@article{CAIAFA2010557,
title = {Generalizing the column–row matrix decomposition to multi-way arrays},
journal = {Linear Algebra and its Applications},
volume = {433},
number = {3},
pages = {557-573},
year = {2010},
issn = {0024-3795},
doispec = {https://doi.org/10.1016/j.laa.2010.03.020},
urlspec = {https://www.sciencedirect.com/science/article/pii/S0024379510001394},
author = {Cesar F. Caiafa and Andrzej Cichocki},
keywords = {Large-scale problems, Low-rank approximation, -way array, Pseudo-skeleton (CUR) matrix decomposition, Tucker tensor decomposition},
abstract = {In this paper, we provide two generalizations of the CUR matrix decomposition Y=CUR (also known as pseudo-skeleton approximation method [1]) to the case of N-way arrays (tensors). These generalizations, which we called Fiber Sampling Tensor Decomposition types 1 and 2 (FSTD1 and FSTD2), provide explicit formulas for the parameters of a rank-(R,R,…,R) Tucker representation (the core tensor of size R×R×⋯×R and the matrix factors of sizes In×R, n=1,2,…N) based only on some selected entries of the original tensor. FSTD1 uses PN-1(P⩾R)n-mode fibers of the original tensor while FSTD2 uses exactly R fibers in each mode as matrix factors, as suggested by the existence theorem provided in Oseledets et al. (2008) [2], with a core tensor defined in terms of the entries of a subtensor of size R×R×⋯×R. For N=2 our results are reduced to the already known CUR matrix decomposition where the core matrix is defined as the inverse of the intersection submatrix, i.e. U=W-1. Additionally, we provide an adaptive type algorithm for the selection of proper fibers in the FSTD1 model which is useful for large scale applications. Several numerical results are presented showing the performance of our FSTD1 Adaptive Algorithm compared to two recently proposed approximation methods for 3-way tensors.}
}

@article{hackbusch2009new,
  title={A new scheme for the tensor representation},
  author={Hackbusch, Wolfgang and K{\"u}hn, Stefan},
  journal={Journal of Fourier analysis and applications},
  volume={15},
  number={5},
  pages={706--722},
  year={2009},
  publisher={Springer}
}

@article{oseledets2011algebraic,
  title={Algebraic wavelet transform via quantics tensor train decomposition},
  author={Oseledets, Ivan V and Tyrtyshnikov, Eugene E},
  journal={SIAM Journal on Scientific Computing},
  volume={33},
  number={3},
  pages={1315--1328},
  year={2011},
  publisher={SIAM}
}



@article{ahmadi2021cross,
  title={Cross Tensor Approximation Methods for Compression and Dimensionality Reduction},
  author={Ahmadi-Asl, Salman and Caiafa, Cesar F and Cichocki, Andrzej and Phan, Anh Huy and Tanaka, Toshihisa and Oseledets, Ivan and Wang, Jun},
  journal={IEEE Access},
  volume={9},
  pages={150809--150838},
  year={2021},
  publisher={IEEE}
}

@inbook{Thiruvathukal2022,
  doispec = {10.1201/9781003162810},
  urlspec = {https://doi.org/10.1201/9781003162810},
  year = {2022},
  month = jan,
  publisher = {Chapman and Hall/{CRC}},
  author = {George K. Thiruvathukal and Yung-Hsiang Lu and Jaeyoun Kim and Yiran Chen and Bo Chen},
  title = {Low-Power Computer Vision}
}

@InProceedings{Jacob_2018_CVPR,
author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
title = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
} 

@article{Deiana2021ApplicationsAT,
  title={Applications and Techniques for Fast Machine Learning in Science},
  author={Allison McCarn Deiana and Nhan Tran and Joshua C. Agar and Michaela Blott and Giuseppe Di Guglielmo and Javier Duarte and Philip C. Harris and Scott Hauck and Miaoyuan Liu and Mark S. Neubauer and Jennifer Ngadiuba and Seda Ogrenci Memik and Maurizio Pierini and Thea Klaeboe Aarrestad and Steffen B{\"a}hr and J{\"u}rgen Becker and A. Berthold and Richard J. Bonventre and Tomas E. Muller Bravo and Markus Diefenthaler and Zhen Dong and Nick Fritzsche and Amir Gholami and Ekaterina Govorkova and Kyle J. Hazelwood and Christian Herwig and Babar Khan and Sehoon Kim and Thomas Klijnsma and Yaling Liu and Kin Ho Lo and Tri Nguyen and Gianantonio Pezzullo and Seyedramin Rasoulinezhad and Ryan A. Rivera and Kate Scholberg and Justin Selig and Sougata Sen and Dmitri B. Strukov and William Tang and Savannah Thais and Kai Lukas Unger and Ricardo Vilalta and Belina von Krosigk and Thomas K. Warburton and Maria Acosta Flechas and Anthony Aportela and Thomas Calvet and Leonardo Cristella and Daniel Diaz and Caterina Doglioni and Maria Domenica Galati and Elham E Khoda and Farah Fahim and Davide Giri and Benjamin Hawks and Duc Hoang and Burt Holzman and Shih-Chieh Hsu and Sergo Jindariani and Iris DeLoach Johnson and Raghav Kansal and Ryan Kastner and Erik Katsavounidis and Jeffrey D. Krupa and Pan Li and Sandeep Madireddy and Ethan Marx and Patric McCormack and Andres Meza and Jovan Mitrevski and Mohammed A. Mohammed and Farouk Mokhtar and Eric Moreno and Srishti Nagu and Rohin Narayan and Noah Palladino and Zhiqiang Que and Sang Eon Park and Subramanian Ramamoorthy and Dylan S. Rankin and Simon Rothman and Ashish Sharma and Sioni Summers and Pietro Vischia and J. R. Vlimant and Olivia Weng},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.13041}
}

@inproceedings{conti2018,
    author = {Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
    title = {Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents},
    year = {2018},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.},
    booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
    pages = {5032–5043},
    numpages = {12},
    location = {Montr\'{e}al, Canada},
    series = {NIPS'18}
    }

      

@inproceedings{self-guided-es,
    author = {Liu, Fei-Yu and Li, Zi-Niu and Qian, Chao},
    title = {Self-Guided Evolution Strategies with Historical Estimated Gradients},
    year = {2021},
    isbn = {9780999241165},
    abstract = {Evolution Strategies (ES) are a class of black-box optimization algorithms and have been widely applied to solve problems, e.g., in reinforcement learning (RL), where the true gradient is unavailable. ES estimate the gradient of an objective function with respect to the parameters by randomly sampling search directions and evaluating parameter perturbations in these directions. However, the gradient estimator of ES tends to have a high variance for high-dimensional optimization, thus requiring a large number of samples and making ES inefficient. In this paper, we propose a new ES algorithm SGES, which utilizes historical estimated gradients to construct a low-dimensional subspace for sampling search directions, and adjusts the importance of this subspace adaptively. We prove that the variance of the gradient estimator of SGES can be much smaller than that of Vanilla ES; meanwhile, its bias can be well bounded. Empirical results on benchmark black-box functions and a set of popular RL tasks exhibit the superior performance of SGES over state-of-the-art ES algorithms.},
    booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
    articleno = {205},
    numpages = {7},
    location = {Yokohama, Yokohama, Japan},
    series = {IJCAI'20}
    }

@article{Rechenberg1975,
author = {Rechenberg, Ingo},
title = {Evolutionsstrategie — Optimierung technischer Systeme nach Prinzipien der biologischen Evolution. 170 S. mit 36 Abb. Frommann-Holzboog-Verlag. Stuttgart 1973. Broschiert},
journal = {Feddes Repertorium},
volume = {86},
number = {5},
pages = {337-337},
doispec = {https://doi.org/10.1002/fedr.19750860506},
urlspec = {https://onlinelibrary.wiley.com/doi/abs/10.1002/fedr.19750860506},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/fedr.19750860506},
year = {1975}
}

@inproceedings{10.5555/3326943.3327053,
author = {Khadka, Shauharda and Tumer, Kagan},
title = {Evolution-Guided Policy Gradient in Reinforcement Learning},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep Reinforcement Learning (DRL) algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically suffer from three core difficulties: temporal credit assignment with sparse rewards, lack of effective exploration, and brittle convergence properties that are extremely sensitive to hyperparameters. Collectively, these challenges severely limit the applicability of these approaches to real-world problems. Evolutionary Algorithms (EAs), a class of black box optimization techniques inspired by natural evolution, are well suited to address each of these three challenges. However, EAs typically suffer from high sample complexity and struggle to solve problems that require optimization of a large number of parameters. In this paper, we introduce Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that leverages the population of an EA to provide diversified data to train an RL agent, and reinserts the RL agent into the EA population periodically to inject gradient information into the EA. ERL inherits EA's ability of temporal credit assignment with a fitness metric, effective exploration with a diverse set of policies, and stability of a population-based approach and complements it with off-policy DRL's ability to leverage gradients for higher sample efficiency and faster learning. Experiments in a range of challenging continuous control benchmarks demonstrate that ERL significantly outperforms prior DRL and EA methods.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {1196–1208},
numpages = {13},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

﻿@Article{Storn1997-DE,
author={Storn, Rainer
and Price, Kenneth},
title={Differential Evolution -- A Simple and Efficient Heuristic for global Optimization over Continuous Spaces},
journal={Journal of Global Optimization},
year={1997},
month={Dec},
day={01},
volume={11},
number={4},
pages={341-359},
abstract={A new heuristic approach for minimizing possiblynonlinear and non-differentiable continuous spacefunctions is presented. By means of an extensivetestbed it is demonstrated that the new methodconverges faster and with more certainty than manyother acclaimed global optimization methods. The newmethod requires few control variables, is robust, easyto use, and lends itself very well to parallelcomputation.},
issn={1573-2916},
doi={10.1023/A:1008202821328},
url={https://doi.org/10.1023/A:1008202821328}
}



@inproceedings{alpha_star_es,
    author = {Arulkumaran, Kai and Cully, Antoine and Togelius, Julian},
    title = {AlphaStar: An Evolutionary Computation Perspective},
    year = {2019},
    isbn = {9781450367486},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    urlspec = {https://doi.org/10.1145/3319619.3321894},
    doispec = {10.1145/3319619.3321894},
    abstract = {In January 2019, DeepMind revealed AlphaStar to the world---the first artificial intelligence (AI) system to beat a professional player at the game of StarCraft II---representing a milestone in the progress of AI. AlphaStar draws on many areas of AI research, including deep learning, reinforcement learning, game theory, and evolutionary computation (EC). In this paper we analyze AlphaStar primarily through the lens of EC, presenting a new look at the system and relating it to many concepts in the field. We highlight some of its most interesting aspects---the use of Lamarckian evolution, competitive co-evolution, and quality diversity. In doing so, we hope to provide a bridge between the wider EC community and one of the most significant AI systems developed in recent times.},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
    pages = {314–315},
    numpages = {2},
    keywords = {co-evolution, lamarckian evolution, quality diversity},
    location = {Prague, Czech Republic},
    series = {GECCO '19}
    }

      

@article{HEIDRICHMEISNER2009152,
title = {Neuroevolution strategies for episodic reinforcement learning},
journal = {Journal of Algorithms},
volume = {64},
number = {4},
pages = {152-168},
year = {2009},
note = {Special Issue: Reinforcement Learning},
issn = {0196-6774},
doispec = {https://doi.org/10.1016/j.jalgor.2009.04.002},
urlspec = {https://www.sciencedirect.com/science/article/pii/S0196677409000364},
author = {Verena Heidrich-Meisner and Christian Igel},
keywords = {Reinforcement learning, Evolution strategy, Covariance matrix adaptation, Partially observable Markov decision process, Direct policy search},
abstract = {Because of their convincing performance, there is a growing interest in using evolutionary algorithms for reinforcement learning. We propose learning of neural network policies by the covariance matrix adaptation evolution strategy (CMA-ES), a randomized variable-metric search algorithm for continuous optimization. We argue that this approach, which we refer to as CMA Neuroevolution Strategy (CMA-NeuroES), is ideally suited for reinforcement learning, in particular because it is based on ranking policies (and therefore robust against noise), efficiently detects correlations between parameters, and infers a search direction from scalar reinforcement signals. We evaluate the CMA-NeuroES on five different (Markovian and non-Markovian) variants of the common pole balancing problem. The results are compared to those described in a recent study covering several RL algorithms, and the CMA-NeuroES shows the overall best performance.}
}

@Article{Vinyals2019,
author={Vinyals, Oriol
and Babuschkin, Igor
and Czarnecki, Wojciech M.
and Mathieu, Micha{\"e}l
and Dudzik, Andrew
and Chung, Junyoung
and Choi, David H.
and Powell, Richard
and Ewalds, Timo
and Georgiev, Petko
and Oh, Junhyuk
and Horgan, Dan
and Kroiss, Manuel
and Danihelka, Ivo
and Huang, Aja
and Sifre, Laurent
and Cai, Trevor
and Agapiou, John P.
and Jaderberg, Max
and Vezhnevets, Alexander S.
and Leblond, R{\'e}mi
and Pohlen, Tobias
and Dalibard, Valentin
and Budden, David
and Sulsky, Yury
and Molloy, James
and Paine, Tom L.
and Gulcehre, Caglar
and Wang, Ziyu
and Pfaff, Tobias
and Wu, Yuhuai
and Ring, Roman
and Yogatama, Dani
and W{\"u}nsch, Dario
and McKinney, Katrina
and Smith, Oliver
and Schaul, Tom
and Lillicrap, Timothy
and Kavukcuoglu, Koray
and Hassabis, Demis
and Apps, Chris
and Silver, David},
title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
journal={Nature},
year={2019},
month={Nov},
day={01},
volume={575},
number={7782},
pages={350-354},
abstract={Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1--3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8{\%} of officially ranked human players.},
issn={1476-4687},
doispec={10.1038/s41586-019-1724-z},
urlspec={https://doi.org/10.1038/s41586-019-1724-z}
}

@article{HEIN201787,
title = {Particle swarm optimization for generating interpretable fuzzy reinforcement learning policies},
journal = {Engineering Applications of Artificial Intelligence},
volume = {65},
pages = {87-98},
year = {2017},
issn = {0952-1976},
doispec = {https://doi.org/10.1016/j.engappai.2017.07.005},
urlspec = {https://www.sciencedirect.com/science/article/pii/S0952197617301537},
author = {Daniel Hein and Alexander Hentschel and Thomas Runkler and Steffen Udluft},
keywords = {Interpretable, Reinforcement learning, Fuzzy policy, Fuzzy controller, Particle swarm optimization},
abstract = {Fuzzy controllers are efficient and interpretable system controllers for continuous state and action spaces. To date, such controllers have been constructed manually or trained automatically either using expert-generated problem-specific cost functions or incorporating detailed knowledge about the optimal control strategy. Both requirements for automatic training processes are not found in most real-world reinforcement learning (RL) problems. In such applications, online learning is often prohibited for safety reasons because it requires exploration of the problem’s dynamics during policy training. We introduce a fuzzy particle swarm reinforcement learning (FPSRL) approach that can construct fuzzy RL policies solely by training parameters on world models that simulate real system dynamics. These world models are created by employing an autonomous machine learning technique that uses previously generated transition samples of a real system. To the best of our knowledge, this approach is the first to relate self-organizing fuzzy controllers to model-based batch RL. FPSRL is intended to solve problems in domains where online learning is prohibited, system dynamics are relatively easy to model from previously generated default policy transition samples, and it is expected that a relatively easily interpretable control policy exists. The efficiency of the proposed approach with problems from such domains is demonstrated using three standard RL benchmarks, i.e., mountain car, cart-pole balancing, and cart-pole swing-up. Our experimental results demonstrate high-performing, interpretable fuzzy policies.}
}

@inproceedings{Faust2019EvolvingRT,
  title={Evolving Rewards to Automate Reinforcement Learning},
  author={Aleksandra Faust and Anthony G. Francis and Dar Mehta},
  booktitle={6th ICML Workshop on Automated Machine Learning},
  urlspec={https://sites.google.com/view/automl2019icml/home},
  year={2019},
}

@article{10.1016/j.engappai.2020.103525,
    author = {Liu, Tundong and Li, Liduan and Shao, Guifang and Wu, Xiaomin and Huang, Meng},
    title = {A Novel Policy Gradient Algorithm with PSO-Based Parameter Exploration for Continuous Control},
    year = {2020},
    issue_date = {Apr 2020},
    publisher = {Pergamon Press, Inc.},
    address = {USA},
    volume = {90},
    number = {C},
    issn = {0952-1976},
    urlspec = {https://doi.org/10.1016/j.engappai.2020.103525},
    doispec = {10.1016/j.engappai.2020.103525},
    journal = {Eng. Appl. Artif. Intell.},
    month = {apr},
    numpages = {11},
    keywords = {Parameter exploration, Model-free reinforcement learning, Policy gradient, Continuous control, Gradient-free algorithm, Convergence improvement}
}



@inproceedings{mahajan2021tesseract,
author = {Mahajan, Anuj and Samvelyan, Mikayel and Mao, Lei and Makoviychuk, Viktor and Garg, Animesh and Kossaifi, Jean and Whiteson, Shimon and Zhu, Yuke and Anandkumar, Animashree},
booktitle = {International Conference on Machine Learning (ICML)},
pages = {7301--7312},
title = {Tesseract: Tensorised Actors for Multi-Agent Reinforcement Learning},
volume = {139},
year = {2021}
}
@article{Gorodetsky2018,
author = {Alex Gorodetsky and Sertac Karaman and Youssef Marzouk},
title ={High-dimensional stochastic optimal control using continuous tensor decompositions},
journal = {The International Journal of Robotics Research},
volume = {37},
number = {2-3},
pages = {340-377},
year = {2018},
doispec = {10.1177/0278364917753994},

urlspec = { https://doi.org/10.1177/0278364917753994},
eprint = {https://doi.org/10.1177/0278364917753994}
}

@ARTICLE{9524335,

  author={Meunier, Laurent and Rakotoarison, Herilalaina and Wong, Pak Kan and Roziere, Baptiste and Rapin, Jérémy and Teytaud, Olivier and Moreau, Antoine and Doerr, Carola},

  journal={IEEE Transactions on Evolutionary Computation}, 

  title={Black-Box Optimization Revisited: Improving Algorithm Selection Wizards through Massive Benchmarking}, 

  year={2021},

  volume={},

  number={},

  pages={1-1},

  doispec={10.1109/TEVC.2021.3108185}}


@INPROCEEDINGS{7379496,

  author={Nichols, Barry D.},

  booktitle={2015 IEEE International Conference on Systems, Man, and Cybernetics}, 

  title={Continuous Action-Space Reinforcement Learning Methods Applied to the Minimum-Time Swing-Up of the Acrobot}, 

  year={2015},

  volume={},

  number={},

  pages={2084-2089},

  doispec={10.1109/SMC.2015.364}}


@InProceedings{pmlr-v32-qin14,
  title = 	 {Sparse Reinforcement Learning via Convex Optimization},
  author = 	 {Qin, Zhiwei and Li, Weichang and Janoos, Firdaus},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {424--432},
  year = 	 {2014},
  editor = 	 {Xing, Eric P. and Jebara, Tony},
  volume = 	 {32},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/qin14.pdf},
  urlspec = 	 {https://proceedings.mlr.press/v32/qin14.html},
  abstract = 	 {We propose two new algorithms for the sparse reinforcement learning problem based on different formulations.  The first algorithm is an off-line method based on the alternating direction method of multipliers for solving a constrained formulation that explicitly controls the projected Bellman residual.  The second algorithm is an online stochastic approximation algorithm that employs the regularized dual averaging technique, using the Lagrangian formulation.  The convergence of both algorithms are established. We demonstrate the performance of these algorithms through two classical examples.}
}


@inbook{HerrmannPriceJoyce+2020+45+62,
author = {J. Michael Herrmann and Adam Price and Thomas Joyce},
doispec = {doi:10.1515/9783110671353-003},
urlspec = {https://doi.org/10.1515/9783110671353-003},
title = {3. Ant colony optimization and reinforcement learning},
booktitle = {Computational Intelligence: Theoretical Advances and Advanced Applications},
year = {2020},
publisher = {De Gruyter},
pages = {45--62}
}

@Article{NeldMead65,
  Title                    = {A simplex method for function minimization},
  Author                   = {John A. Nelder and Roger Mead},
  Journal                  = {Computer Journal},
  Year                     = {1965},
  Pages                    = {308--313},
  Volume                   = {7}
}

@INPROCEEDINGS{1562233,

  author={Atiya, A.F. and Parlos, A.G. and Ingber, L.},

  booktitle={2003 46th Midwest Symposium on Circuits and Systems}, 

  title={A reinforcement learning method based on adaptive simulated annealing}, 

  year={2003},

  volume={1},

  number={},

  pages={121-124 Vol. 1},

  doispec={10.1109/MWSCAS.2003.1562233}}

@InProceedings{evo-tn-topology-search,
  title = 	 {Evolutionary Topology Search for Tensor Network Decomposition},
  author =       {Li, Chao and Sun, Zhun},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {5947--5957},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/li20l/li20l.pdf},
  urlspec = 	 {https://proceedings.mlr.press/v119/li20l.html},
  abstract = 	 {Tensor network (TN) decomposition is a promising framework to represent extremely high-dimensional problems with few parameters. However, it is challenging to search the (near-)optimal topological structures for TN decomposition, since the number of candidate solutions exponentially grows with increasing the order of a tensor. In this paper, we claim that the issue can be practically tackled by evolutionary algorithms in an affordable manner. We encode the complex topological structures into binary strings, and develop a simple genetic meta-algorithm to search the optimal topology on Hamming space. The experimental results by both synthetic and real-world data demonstrate that our method can effectively discover the ground-truth topology or even better structures with a small number of generations, and significantly boost the representational power of TN decomposition compared with well-known tensor-train (TT) or tensor-ring (TR) models.}
}

@article{SimpleGASuch2017-DeepNG,
  title={Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning},
  author={Felipe Petroski Such and Vashisht Madhavan and Edoardo Conti and Joel Lehman and Kenneth O. Stanley and Jeff Clune},
  journal={ArXiv},
  year={2017},
  volume={abs/1712.06567}
}


@INPROCEEDINGS{4400335,
  author={Wawrzynski, Pawel},
  booktitle={EUROCON 2007 - The International Conference on "Computer as a Tool"}, 
  title={Learning to Control a 6-Degree-of-Freedom Walking Robot}, 
  year={2007},
  volume={},
  number={},
  pages={698-705},
  doi={10.1109/EURCON.2007.4400335}
}


@article{JMLR:v22:18-220,
  author  = {Erich Merrill and Alan Fern and Xiaoli Fern and Nima Dolatnia},
  title   = {An Empirical Study of Bayesian Optimization: Acquisition Versus Partition},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {4},
  pages   = {1-25},
  url     = {http://jmlr.org/papers/v22/18-220.html}
}

@inproceedings{ErezTT11,
  author    = {Tom Erez and
               Yuval Tassa and
               Emanuel Todorov},
  editor    = {Hugh F. Durrant{-}Whyte and
               Nicholas Roy and
               Pieter Abbeel},
  title     = {Infinite-Horizon Model Predictive Control for Periodic Tasks with
               Contacts},
  booktitle = {Robotics: Science and Systems VII, University of Southern California,
               Los Angeles, CA, USA, June 27-30, 2011},
  year      = {2011},
  url       = {http://www.roboticsproceedings.org/rss07/p10.html},
  doi       = {10.15607/RSS.2011.VII.010},
  timestamp = {Fri, 29 Jan 2021 22:08:13 +0100},
  biburl    = {https://dblp.org/rec/conf/rss/ErezTT11.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{doi:10.1177/027836498400300207,
author = {Marc H. Raibert and H. Benjamin Brown, Jr and Michael Chepponis},
title ={Experiments in Balance with a 3D One-Legged Hopping Machine},
journal = {The International Journal of Robotics Research},
volume = {3},
number = {2},
pages = {75-92},
year = {1984},
doi = {10.1177/027836498400300207},

URL = { 
        https://doi.org/10.1177/027836498400300207
    
},
eprint = { 
        https://doi.org/10.1177/027836498400300207
    
}
,
    abstract = {},
}


      

