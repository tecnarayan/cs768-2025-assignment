\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
  Ghemawat, S., Irving, G., Isard, M., et~al.
\newblock Tensorflow: a system for large-scale machine learning.
\newblock In \emph{OSDI}, 2016.

\bibitem[Agarwal et~al.(2018)Agarwal, Bullins, Chen, Hazan, Singh, Zhang, and
  Zhang]{agarwal2018case}
Agarwal, N., Bullins, B., Chen, X., Hazan, E., Singh, K., Zhang, C., and Zhang,
  Y.
\newblock The case for full-matrix adaptive regularization.
\newblock \emph{arXiv preprint arXiv:1806.02958}, 2018.

\bibitem[Cesa-Bianchi et~al.(2004)Cesa-Bianchi, Conconi, and
  Gentile]{cesa2004generalization}
Cesa-Bianchi, N., Conconi, A., and Gentile, C.
\newblock On the generalization ability of on-line learning algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 50\penalty0
  (9):\penalty0 2050--2057, 2004.

\bibitem[Chelba et~al.(2013)Chelba, Mikolov, Schuster, Ge, Brants, Koehn, and
  Robinson]{chelba2013one}
Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and
  Robinson, T.
\newblock One billion word benchmark for measuring progress in statistical
  language modeling.
\newblock \emph{arXiv preprint arXiv:1312.3005}, 2013.

\bibitem[Cutkosky \& Boahen(2017{\natexlab{a}})Cutkosky and
  Boahen]{cutkosky2017online}
Cutkosky, A. and Boahen, K.
\newblock Online learning without prior information.
\newblock In \emph{Conference on Learning Theory}, pp.\  643--677,
  2017{\natexlab{a}}.

\bibitem[Cutkosky \& Boahen(2017{\natexlab{b}})Cutkosky and
  Boahen]{cutkosky2017stochastic}
Cutkosky, A. and Boahen, K.~A.
\newblock Stochastic and adversarial online learning without hyperparameters.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5059--5067, 2017{\natexlab{b}}.

\bibitem[Cutkosky \& Orabona(2018)Cutkosky and Orabona]{cutkosky2018black}
Cutkosky, A. and Orabona, F.
\newblock Black-box reductions for parameter-free online learning in {B}anach
  spaces.
\newblock In \emph{COLT}, 2018.
\newblock URL \url{https://arxiv.org/abs/1802.06293}.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Gonen \& Shalev-Shwartz(2015)Gonen and
  Shalev-Shwartz]{gonen2015faster}
Gonen, A. and Shalev-Shwartz, S.
\newblock Faster sgd using sketched conditioning.
\newblock \emph{arXiv preprint arXiv:1506.02649}, 2015.

\bibitem[Gupta et~al.(2018)Gupta, Koren, and Singer]{gupta2018shampoo}
Gupta, V., Koren, T., and Singer, Y.
\newblock Shampoo: Preconditioned stochastic tensor optimization.
\newblock \emph{arXiv preprint arXiv:1802.09568}, 2018.

\bibitem[Hazan et~al.(2007)Hazan, Agarwal, and Kale]{hazan2007logarithmic}
Hazan, E., Agarwal, A., and Kale, S.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock \emph{Machine Learning}, 69\penalty0 (2-3):\penalty0 169--192, 2007.

\bibitem[Hazan et~al.(2016)]{hazan2016introduction}
Hazan, E. et~al.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends{\textregistered} in Optimization},
  2\penalty0 (3-4):\penalty0 157--325, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Kempka et~al.(2019)Kempka, Kot{\l}owski, and
  Warmuth]{kempka2019adaptive}
Kempka, M., Kot{\l}owski, W., and Warmuth, M.~K.
\newblock Adaptive scale-invariant online algorithms for learning linear
  models.
\newblock \emph{arXiv preprint arXiv:1902.07528}, 2019.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.~L.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{Proc. 3rd Int. Conf. Learn. Representations}, 2014.

\bibitem[Koren \& Livni(2017)Koren and Livni]{koren2017affine}
Koren, T. and Livni, R.
\newblock Affine-invariant online optimization and the low-rank experts
  problem.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4747--4755, 2017.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Luo et~al.(2016)Luo, Agarwal, Cesa-Bianchi, and
  Langford]{luo2016efficient}
Luo, H., Agarwal, A., Cesa-Bianchi, N., and Langford, J.
\newblock Efficient second order online learning by sketching.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  902--910, 2016.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{martens2015optimizing}
Martens, J. and Grosse, R.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pp.\
  2408--2417, 2015.

\bibitem[McMahan \& Streeter(2012)McMahan and Streeter]{mcmahan2012no}
McMahan, B. and Streeter, M.
\newblock No-regret algorithms for unconstrained online convex optimization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2402--2410, 2012.

\bibitem[McMahan(2017)]{mcmahan2017survey}
McMahan, H.~B.
\newblock A survey of algorithms and analysis for adaptive online learning.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 3117--3166, 2017.

\bibitem[McMahan \& Orabona(2014)McMahan and Orabona]{mcmahan2014unconstrained}
McMahan, H.~B. and Orabona, F.
\newblock Unconstrained online linear learning in hilbert spaces: Minimax
  algorithms and normal approximations.
\newblock In \emph{Conference on Learning Theory}, pp.\  1020--1039, 2014.

\bibitem[McMahan \& Streeter(2010)McMahan and Streeter]{mcmahan2010adaptive}
McMahan, H.~B. and Streeter, M.
\newblock Adaptive bound optimization for online convex optimization.
\newblock \emph{arXiv preprint arXiv:1002.4908}, 2010.

\bibitem[Orabona(2013)]{orabona2013dimension}
Orabona, F.
\newblock Dimension-free exponentiated gradient.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1806--1814, 2013.

\bibitem[Orabona(2014)]{orabona2014simultaneous}
Orabona, F.
\newblock Simultaneous model selection and optimization through parameter-free
  stochastic learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1116--1124, 2014.

\bibitem[Orabona \& P{\'a}l(2016)Orabona and P{\'a}l]{orabona2016coin}
Orabona, F. and P{\'a}l, D.
\newblock Coin betting and parameter-free online learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  577--585, 2016.

\bibitem[Orabona \& Tommasi(2017)Orabona and Tommasi]{orabona2017training}
Orabona, F. and Tommasi, T.
\newblock Training deep networks without learning rates through coin betting.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2160--2170, 2017.

\bibitem[Shalev-Shwartz et~al.(2012)]{shalev2012online}
Shalev-Shwartz, S. et~al.
\newblock Online learning and online convex optimization.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  4\penalty0 (2):\penalty0 107--194, 2012.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5998--6008, 2017.

\bibitem[Vaswani et~al.(2018)Vaswani, Bengio, Brevdo, Chollet, Gomez, Gouws,
  Jones, Kaiser, Kalchbrenner, Parmar, Sepassi, Shazeer, and
  Uszkoreit]{tensor2tensor}
Vaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez, A.~N., Gouws, S.,
  Jones, L., Kaiser, L., Kalchbrenner, N., Parmar, N., Sepassi, R., Shazeer,
  N., and Uszkoreit, J.
\newblock Tensor2tensor for neural machine translation.
\newblock \emph{CoRR}, abs/1803.07416, 2018.
\newblock URL \url{http://arxiv.org/abs/1803.07416}.

\bibitem[Zinkevich(2003)]{zinkevich2003online}
Zinkevich, M.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{Proceedings of the 20th International Conference on Machine
  Learning (ICML-03)}, pp.\  928--936, 2003.

\end{thebibliography}
