\begin{thebibliography}{100}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{}\fi

\bibitem[{Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l and
  Szepesv{\'a}ri}]{abbasi2011improved}
\text{Abbasi-Yadkori, Y.}, \text{P{\'a}l, D.} and \text{Szepesv{\'a}ri, C.}
  (2011).
\newblock Improved algorithms for linear stochastic bandits.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Agarwal et~al.(2020{\natexlab{a}})Agarwal, Jiang and
  Kakade}]{agarwal2019reinforcement}
\text{Agarwal, A.}, \text{Jiang, N.} and \text{Kakade, S.~M.}
  (2020{\natexlab{a}}).
\newblock \textit{Reinforcement learning: {T}heory and algorithms}.
\newblock MIT.

\bibitem[{Agarwal et~al.(2020{\natexlab{b}})Agarwal, Schuurmans and
  Norouzi}]{agarwal2020optimistic}
\text{Agarwal, R.}, \text{Schuurmans, D.} and \text{Norouzi, M.}
  (2020{\natexlab{b}}).
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Antos et~al.(2007)Antos, Szepesv{\'a}ri and Munos}]{antos2007fitted}
\text{Antos, A.}, \text{Szepesv{\'a}ri, C.} and \text{Munos, R.} (2007).
\newblock Fitted {Q}-iteration in continuous action-space {MDP}s.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Antos et~al.(2008)Antos, Szepesv{\'a}ri and
  Munos}]{antos2008learning}
\text{Antos, A.}, \text{Szepesv{\'a}ri, C.} and \text{Munos, R.} (2008).
\newblock Learning near-optimal policies with {B}ellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock \textit{Machine Learning}, \textbf{71} 89--129.

\bibitem[{Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang and
  Yang}]{ayoub2020model}
\text{Ayoub, A.}, \text{Jia, Z.}, \text{Szepesvari, C.}, \text{Wang, M.} and
  \text{Yang, L.~F.} (2020).
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock \textit{arXiv preprint arXiv:2006.01107}.

\bibitem[{Azar et~al.(2017)Azar, Osband and Munos}]{azar2017minimax}
\text{Azar, M.~G.}, \text{Osband, I.} and \text{Munos, R.} (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock \textit{arXiv preprint arXiv:1703.05449}.

\bibitem[{Bellemare et~al.(2013)Bellemare, Naddaf, Veness and
  Bowling}]{bellemare2013arcade}
\text{Bellemare, M.~G.}, \text{Naddaf, Y.}, \text{Veness, J.} and
  \text{Bowling, M.} (2013).
\newblock The {A}rcade {L}earning {E}nvironment: An evaluation platform for
  general agents.
\newblock \textit{Journal of Artificial Intelligence Research}, \textbf{47}
  253--279.

\bibitem[{Buckman et~al.(2020)Buckman, Gelada and
  Bellemare}]{buckman2020importance}
\text{Buckman, J.}, \text{Gelada, C.} and \text{Bellemare, M.~G.} (2020).
\newblock The importance of pessimism in fixed-dataset policy optimization.
\newblock \textit{arXiv preprint arXiv:2009.06799}.

\bibitem[{Cai et~al.(2020)Cai, Yang, Jin and Wang}]{cai2020provably}
\text{Cai, Q.}, \text{Yang, Z.}, \text{Jin, C.} and \text{Wang, Z.} (2020).
\newblock Provably efficient exploration in policy optimization.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Chakraborty and Murphy(2014)}]{chakraborty2014dynamic}
\text{Chakraborty, B.} and \text{Murphy, S.~A.} (2014).
\newblock Dynamic treatment regimes.
\newblock \textit{Annual Review of Statistics and Its Application}, \textbf{1}
  447--464.

\bibitem[{Chen and Jiang(2019)}]{chen2019information}
\text{Chen, J.} and \text{Jiang, N.} (2019).
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock \textit{arXiv preprint arXiv:1905.00360}.

\bibitem[{Chowdhury and Gopalan(2017)}]{chowdhury2017kernelized}
\text{Chowdhury, S.~R.} and \text{Gopalan, A.} (2017).
\newblock On kernelized multi-armed bandits.
\newblock \textit{arXiv preprint arXiv:1704.00445}.

\bibitem[{Donoho and Johnstone(1994)}]{donoho1994ideal}
\text{Donoho, D.~L.} and \text{Johnstone, J.~M.} (1994).
\newblock Ideal spatial adaptation by wavelet shrinkage.
\newblock \textit{Biometrika}, \textbf{81} 425--455.

\bibitem[{Duan et~al.(2020)Duan, Jia and Wang}]{duan2020minimaxoptimal}
\text{Duan, Y.}, \text{Jia, Z.} and \text{Wang, M.} (2020).
\newblock Minimax-optimal off-policy evaluation with linear function
  approximation.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Fan and Li(2001)}]{fan2001variable}
\text{Fan, J.} and \text{Li, R.} (2001).
\newblock Variable selection via nonconcave penalized likelihood and its oracle
  properties.
\newblock \textit{Journal of the American Statistical Association}, \textbf{96}
  1348--1360.

\bibitem[{Fan et~al.(2020)Fan, Wang, Xie and Yang}]{fan2020theoretical}
\text{Fan, J.}, \text{Wang, Z.}, \text{Xie, Y.} and \text{Yang, Z.} (2020).
\newblock A theoretical analysis of deep {Q}-learning.
\newblock In \textit{Learning for Dynamics and Control}.

\bibitem[{Farahmand et~al.(2016)Farahmand, Ghavamzadeh, Szepesv{\'a}ri and
  Mannor}]{farahmand2016regularized}
\text{Farahmand, A.-m.}, \text{Ghavamzadeh, M.}, \text{Szepesv{\'a}ri, C.} and
  \text{Mannor, S.} (2016).
\newblock Regularized policy iteration with nonparametric function spaces.
\newblock \textit{Journal of Machine Learning Research}, \textbf{17}
  4809--4874.

\bibitem[{Farahmand et~al.(2010)Farahmand, Szepesv{\'a}ri and
  Munos}]{farahmand2010error}
\text{Farahmand, A.-m.}, \text{Szepesv{\'a}ri, C.} and \text{Munos, R.} (2010).
\newblock Error propagation for approximate policy and value iteration.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Farajtabar et~al.(2018)Farajtabar, Chow and
  Ghavamzadeh}]{farajtabar2018more}
\text{Farajtabar, M.}, \text{Chow, Y.} and \text{Ghavamzadeh, M.} (2018).
\newblock More robust doubly robust off-policy evaluation.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Fu et~al.(2020{\natexlab{a}})Fu, Kumar, Nachum, Tucker and
  Levine}]{fu2020d4rl}
\text{Fu, J.}, \text{Kumar, A.}, \text{Nachum, O.}, \text{Tucker, G.} and
  \text{Levine, S.} (2020{\natexlab{a}}).
\newblock {D4RL}: Datasets for deep data-driven reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2004.07219}.

\bibitem[{Fu et~al.(2020{\natexlab{b}})Fu, Yang and Wang}]{fu2020single}
\text{Fu, Z.}, \text{Yang, Z.} and \text{Wang, Z.} (2020{\natexlab{b}}).
\newblock Single-timescale actor-critic provably finds globally optimal policy.
\newblock \textit{arXiv preprint arXiv:2008.00483}.

\bibitem[{Fujimoto et~al.(2019)Fujimoto, Meger and Precup}]{fujimoto2019off}
\text{Fujimoto, S.}, \text{Meger, D.} and \text{Precup, D.} (2019).
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Gottesman et~al.(2019)Gottesman, Johansson, Komorowski, Faisal,
  Sontag, Doshi-Velez and Celi}]{gottesman2019guidelines}
\text{Gottesman, O.}, \text{Johansson, F.}, \text{Komorowski, M.},
  \text{Faisal, A.}, \text{Sontag, D.}, \text{Doshi-Velez, F.} and \text{Celi,
  L.} (2019).
\newblock Guidelines for reinforcement learning in healthcare.
\newblock \textit{Nature Medicine}, \textbf{25} 16--32.

\bibitem[{Gulcehre et~al.(2020)Gulcehre, Wang, Novikov, Paine, Colmenarejo,
  Zolna, Agarwal, Merel, Mankowitz, Paduraru et~al.}]{gulcehre2020rl}
\text{Gulcehre, C.}, \text{Wang, Z.}, \text{Novikov, A.}, \text{Paine, T.~L.},
  \text{Colmenarejo, S.~G.}, \text{Zolna, K.}, \text{Agarwal, R.}, \text{Merel,
  J.}, \text{Mankowitz, D.}, \text{Paduraru, C.} \text{et~al.} (2020).
\newblock {RL} {U}nplugged: Benchmarks for offline reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2006.13888}.

\bibitem[{Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck and
  Abbeel}]{houthooft2016vime}
\text{Houthooft, R.}, \text{Chen, X.}, \text{Duan, Y.}, \text{Schulman, J.},
  \text{De~Turck, F.} and \text{Abbeel, P.} (2016).
\newblock {VIME}: Variational information maximizing exploration.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Jaksch et~al.(2010)Jaksch, Ortner and Auer}]{jaksch2010near}
\text{Jaksch, T.}, \text{Ortner, R.} and \text{Auer, P.} (2010).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \textit{Journal of Machine Learning Research}, \textbf{11} 8--36.

\bibitem[{Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu and Picard}]{jaques2019way}
\text{Jaques, N.}, \text{Ghandeharioun, A.}, \text{Shen, J.~H.},
  \text{Ferguson, C.}, \text{Lapedriza, A.}, \text{Jones, N.}, \text{Gu, S.}
  and \text{Picard, R.} (2019).
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock \textit{arXiv preprint arXiv:1907.00456}.

\bibitem[{Jiang and Huang(2020)}]{jiang2020minimax}
\text{Jiang, N.} and \text{Huang, J.} (2020).
\newblock Minimax value interval for off-policy evaluation and policy
  optimization.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Jiang and Li(2016)}]{jiang2016doubly}
\text{Jiang, N.} and \text{Li, L.} (2016).
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Jin et~al.(2018)Jin, Allen-Zhu, Bubeck and Jordan}]{jin2018q}
\text{Jin, C.}, \text{Allen-Zhu, Z.}, \text{Bubeck, S.} and \text{Jordan,
  M.~I.} (2018).
\newblock Is {Q}-learning provably efficient?
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Jin et~al.(2020)Jin, Yang, Wang and Jordan}]{jin2020provably}
\text{Jin, C.}, \text{Yang, Z.}, \text{Wang, Z.} and \text{Jordan, M.~I.}
  (2020).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \textit{Conference on Learning Theory}.

\bibitem[{Kallus and Uehara(2019)}]{kallus2019efficiently}
\text{Kallus, N.} and \text{Uehara, M.} (2019).
\newblock Efficiently breaking the curse of horizon in off-policy evaluation
  with double reinforcement learning.
\newblock \textit{arXiv preprint arXiv:1909.05850}.

\bibitem[{Kallus and Uehara(2020)}]{kallus2020doubly}
\text{Kallus, N.} and \text{Uehara, M.} (2020).
\newblock Doubly robust off-policy value and gradient estimation for
  deterministic policies.
\newblock \textit{arXiv preprint arXiv:2006.03900}.

\bibitem[{Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli and
  Joachims}]{kidambi2020morel}
\text{Kidambi, R.}, \text{Rajeswaran, A.}, \text{Netrapalli, P.} and
  \text{Joachims, T.} (2020).
\newblock {MOReL}: Model-based offline reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2005.05951}.

\bibitem[{Kumar et~al.(2019)Kumar, Fu, Soh, Tucker and
  Levine}]{kumar2019stabilizing}
\text{Kumar, A.}, \text{Fu, J.}, \text{Soh, M.}, \text{Tucker, G.} and
  \text{Levine, S.} (2019).
\newblock Stabilizing off-policy {Q}-learning via bootstrapping error
  reduction.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Kumar et~al.(2020)Kumar, Zhou, Tucker and
  Levine}]{kumar2020conservative}
\text{Kumar, A.}, \text{Zhou, A.}, \text{Tucker, G.} and \text{Levine, S.}
  (2020).
\newblock Conservative {Q}-learning for offline reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2006.04779}.

\bibitem[{Lange et~al.(2012)Lange, Gabel and Riedmiller}]{lange2012batch}
\text{Lange, S.}, \text{Gabel, T.} and \text{Riedmiller, M.} (2012).
\newblock Batch reinforcement learning.
\newblock In \textit{Reinforcement learning}. Springer, 45--73.

\bibitem[{Laroche et~al.(2019)Laroche, Trichelair and
  Des~Combes}]{laroche2019safe}
\text{Laroche, R.}, \text{Trichelair, P.} and \text{Des~Combes, R.~T.} (2019).
\newblock Safe policy improvement with baseline bootstrapping.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Lattimore and Szepesv{\'a}ri(2020)}]{lattimore2020bandit}
\text{Lattimore, T.} and \text{Szepesv{\'a}ri, C.} (2020).
\newblock \textit{Bandit algorithms}.
\newblock Cambridge.

\bibitem[{Le~Cam(2012)}]{le2012asymptotic}
\text{Le~Cam, L.} (2012).
\newblock \textit{Asymptotic methods in statistical decision theory}.
\newblock Springer.

\bibitem[{LeCun et~al.(2015)LeCun, Bengio and Hinton}]{lecun2015deep}
\text{LeCun, Y.}, \text{Bengio, Y.} and \text{Hinton, G.} (2015).
\newblock Deep learning.
\newblock \textit{Nature}, \textbf{521} 436--444.

\bibitem[{Levine et~al.(2020)Levine, Kumar, Tucker and Fu}]{levine2020offline}
\text{Levine, S.}, \text{Kumar, A.}, \text{Tucker, G.} and \text{Fu, J.}
  (2020).
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \textit{arXiv preprint arXiv:2005.01643}.

\bibitem[{Li et~al.(2020)Li, Wei, Chi, Gu and Chen}]{li2020sample}
\text{Li, G.}, \text{Wei, Y.}, \text{Chi, Y.}, \text{Gu, Y.} and \text{Chen,
  Y.} (2020).
\newblock Sample complexity of asynchronous {Q}-learning: Sharper analysis and
  variance reduction.
\newblock \textit{arXiv preprint arXiv:2006.03041}.

\bibitem[{Liao et~al.(2020)Liao, Qi and Murphy}]{liao2020batch}
\text{Liao, P.}, \text{Qi, Z.} and \text{Murphy, S.} (2020).
\newblock Batch policy learning in average reward {M}arkov decision processes.
\newblock \textit{arXiv preprint arXiv:2007.11771}.

\bibitem[{Liu et~al.(2019)Liu, Cai, Yang and Wang}]{liu2019neural}
\text{Liu, B.}, \text{Cai, Q.}, \text{Yang, Z.} and \text{Wang, Z.} (2019).
\newblock Neural trust region/proximal policy optimization attains globally
  optimal policy.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Liu et~al.(2018)Liu, Li, Tang and Zhou}]{liu2018breaking}
\text{Liu, Q.}, \text{Li, L.}, \text{Tang, Z.} and \text{Zhou, D.} (2018).
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Liu et~al.(2020)Liu, Swaminathan, Agarwal and
  Brunskill}]{liu2020provably}
\text{Liu, Y.}, \text{Swaminathan, A.}, \text{Agarwal, A.} and \text{Brunskill,
  E.} (2020).
\newblock Provably good batch reinforcement learning without great exploration.
\newblock \textit{arXiv preprint arXiv:2007.08202}.

\bibitem[{Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski et~al.}]{mnih2015human}
\text{Mnih, V.}, \text{Kavukcuoglu, K.}, \text{Silver, D.}, \text{Rusu, A.~A.},
  \text{Veness, J.}, \text{Bellemare, M.~G.}, \text{Graves, A.},
  \text{Riedmiller, M.}, \text{Fidjeland, A.~K.}, \text{Ostrovski, G.}
  \text{et~al.} (2015).
\newblock Human-level control through deep reinforcement learning.
\newblock \textit{Nature}, \textbf{518} 529--533.

\bibitem[{Munos and Szepesv{\'a}ri(2008)}]{munos2008finite}
\text{Munos, R.} and \text{Szepesv{\'a}ri, C.} (2008).
\newblock Finite-time bounds for fitted value iteration.
\newblock \textit{Journal of Machine Learning Research}, \textbf{9} 815--857.

\bibitem[{Nachum et~al.(2019{\natexlab{a}})Nachum, Chow, Dai and
  Li}]{nachum2019dualdice}
\text{Nachum, O.}, \text{Chow, Y.}, \text{Dai, B.} and \text{Li, L.}
  (2019{\natexlab{a}}).
\newblock Dual{DICE}: Behavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Nachum and Dai(2020)}]{nachum2020reinforcement}
\text{Nachum, O.} and \text{Dai, B.} (2020).
\newblock Reinforcement learning via {F}enchel-{R}ockafellar duality.
\newblock \textit{arXiv preprint arXiv:2001.01866}.

\bibitem[{Nachum et~al.(2019{\natexlab{b}})Nachum, Dai, Kostrikov, Chow, Li and
  Schuurmans}]{nachum2019algaedice}
\text{Nachum, O.}, \text{Dai, B.}, \text{Kostrikov, I.}, \text{Chow, Y.},
  \text{Li, L.} and \text{Schuurmans, D.} (2019{\natexlab{b}}).
\newblock Algae{DICE}: Policy gradient from arbitrary experience.
\newblock \textit{arXiv preprint arXiv:1912.02074}.

\bibitem[{Nair et~al.(2020)Nair, Dalal, Gupta and
  Levine}]{nair2020accelerating}
\text{Nair, A.}, \text{Dalal, M.}, \text{Gupta, A.} and \text{Levine, S.}
  (2020).
\newblock Accelerating online reinforcement learning with offline datasets.
\newblock \textit{arXiv preprint arXiv:2006.09359}.

\bibitem[{Osband and Van~Roy(2014)}]{osband2014model}
\text{Osband, I.} and \text{Van~Roy, B.} (2014).
\newblock Model-based reinforcement learning and the eluder dimension.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Qu and Wierman(2020)}]{qu2020finite}
\text{Qu, G.} and \text{Wierman, A.} (2020).
\newblock Finite-time analysis of asynchronous stochastic approximation and
  {Q}-learning.
\newblock \textit{arXiv preprint arXiv:2002.00260}.

\bibitem[{Russo and Van~Roy(2013)}]{russo2013eluder}
\text{Russo, D.} and \text{Van~Roy, B.} (2013).
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Russo and Van~Roy(2016)}]{russo2016information}
\text{Russo, D.} and \text{Van~Roy, B.} (2016).
\newblock An information-theoretic analysis of {T}hompson sampling.
\newblock \textit{Journal of Machine Learning Research}, \textbf{17}
  2442--2471.

\bibitem[{Russo and Van~Roy(2018)}]{russo2018learning}
\text{Russo, D.} and \text{Van~Roy, B.} (2018).
\newblock Learning to optimize via information-directed sampling.
\newblock \textit{Operations Research}, \textbf{66} 230--252.

\bibitem[{Scherrer et~al.(2015)Scherrer, Ghavamzadeh, Gabillon, Lesner and
  Geist}]{scherrer2015approximate}
\text{Scherrer, B.}, \text{Ghavamzadeh, M.}, \text{Gabillon, V.}, \text{Lesner,
  B.} and \text{Geist, M.} (2015).
\newblock Approximate modified policy iteration and its application to the game
  of {T}etris.
\newblock \textit{Journal of Machine Learning Research}, \textbf{16}
  1629--1676.

\bibitem[{Schmidhuber(1991)}]{schmidhuber1991curious}
\text{Schmidhuber, J.} (1991).
\newblock Curious model-building control systems.
\newblock In \textit{International Joint Conference on Neural Networks}.

\bibitem[{Schmidhuber(2010)}]{schmidhuber2010formal}
\text{Schmidhuber, J.} (2010).
\newblock Formal theory of creativity, fun, and intrinsic motivation
  (1990--2010).
\newblock \textit{IEEE Transactions on Autonomous Mental Development},
  \textbf{2} 230--247.

\bibitem[{Shalev-Shwartz et~al.(2016)Shalev-Shwartz, Shammah and
  Shashua}]{shalev2016safe}
\text{Shalev-Shwartz, S.}, \text{Shammah, S.} and \text{Shashua, A.} (2016).
\newblock Safe, multi-agent, reinforcement learning for autonomous driving.
\newblock \textit{arXiv preprint arXiv:1610.03295}.

\bibitem[{Siegel et~al.(2020)Siegel, Springenberg, Berkenkamp, Abdolmaleki,
  Neunert, Lampe, Hafner and Riedmiller}]{siegel2020keep}
\text{Siegel, N.~Y.}, \text{Springenberg, J.~T.}, \text{Berkenkamp, F.},
  \text{Abdolmaleki, A.}, \text{Neunert, M.}, \text{Lampe, T.}, \text{Hafner,
  R.} and \text{Riedmiller, M.} (2020).
\newblock Keep doing what worked: Behavioral modelling priors for offline
  reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2002.08396}.

\bibitem[{Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam and
  Lanctot}]{silver2016mastering}
\text{Silver, D.}, \text{Huang, A.}, \text{Maddison, C.~J.}, \text{Guez, A.},
  \text{Sifre, L.}, \text{Van Den~Driessche, G.}, \text{Schrittwieser, J.},
  \text{Antonoglou, I.}, \text{Panneershelvam, V.} and \text{Lanctot, M.}
  (2016).
\newblock Mastering the game of {G}o with deep neural networks and tree search.
\newblock \textit{Nature}, \textbf{529} 484--489.

\bibitem[{Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou,
  Huang, Guez, Hubert, Baker, Lai and Bolton}]{silver2017mastering}
\text{Silver, D.}, \text{Schrittwieser, J.}, \text{Simonyan, K.},
  \text{Antonoglou, I.}, \text{Huang, A.}, \text{Guez, A.}, \text{Hubert, T.},
  \text{Baker, L.}, \text{Lai, M.} and \text{Bolton, A.} (2017).
\newblock Mastering the game of {G}o without human knowledge.
\newblock \textit{Nature}, \textbf{550} 354.

\bibitem[{Srinivas et~al.(2009)Srinivas, Krause, Kakade and
  Seeger}]{srinivas2009gaussian}
\text{Srinivas, N.}, \text{Krause, A.}, \text{Kakade, S.~M.} and \text{Seeger,
  M.} (2009).
\newblock Gaussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock \textit{arXiv preprint arXiv:0912.3995}.

\bibitem[{Steinwart and Christmann(2008)}]{steinwart2008support}
\text{Steinwart, I.} and \text{Christmann, A.} (2008).
\newblock \textit{Support vector machines}.
\newblock Springer Science \& Business Media.

\bibitem[{Still and Precup(2012)}]{still2012information}
\text{Still, S.} and \text{Precup, D.} (2012).
\newblock An information-theoretic approach to curiosity-driven reinforcement
  learning.
\newblock \textit{Theory in Biosciences}, \textbf{131} 139--148.

\bibitem[{Sun et~al.(2020)Sun, Kretzschmar, Dotiwalla, Chouard, Patnaik, Tsui,
  Guo, Zhou, Chai, Caine et~al.}]{sun2020scalability}
\text{Sun, P.}, \text{Kretzschmar, H.}, \text{Dotiwalla, X.}, \text{Chouard,
  A.}, \text{Patnaik, V.}, \text{Tsui, P.}, \text{Guo, J.}, \text{Zhou, Y.},
  \text{Chai, Y.}, \text{Caine, B.} \text{et~al.} (2020).
\newblock Scalability in perception for autonomous driving: {W}aymo open
  dataset.
\newblock In \textit{Computer Vision and Pattern Recognition}.

\bibitem[{Sun et~al.(2011)Sun, Gomez and Schmidhuber}]{sun2011planning}
\text{Sun, Y.}, \text{Gomez, F.} and \text{Schmidhuber, J.} (2011).
\newblock Planning to be surprised: Optimal {B}ayesian exploration in dynamic
  environments.
\newblock In \textit{International Conference on Artificial General
  Intelligence}.

\bibitem[{Sutton and Barto(2018)}]{sutton2018reinforcement}
\text{Sutton, R.~S.} and \text{Barto, A.~G.} (2018).
\newblock \textit{Reinforcement learning: An introduction}.
\newblock MIT.

\bibitem[{Szepesv{\'a}ri(2010)}]{szepesvari2010algorithms}
\text{Szepesv{\'a}ri, C.} (2010).
\newblock \textit{Algorithms for reinforcement learning}.
\newblock Morgan \& Claypool.

\bibitem[{Tang et~al.(2019)Tang, Feng, Li, Zhou and Liu}]{tang2019doubly}
\text{Tang, Z.}, \text{Feng, Y.}, \text{Li, L.}, \text{Zhou, D.} and \text{Liu,
  Q.} (2019).
\newblock Doubly robust bias reduction in infinite horizon off-policy
  estimation.
\newblock \textit{arXiv preprint arXiv:1910.07186}.

\bibitem[{Thomas and Brunskill(2016)}]{thomas2016data}
\text{Thomas, P.} and \text{Brunskill, E.} (2016).
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Todorov et~al.(2012)Todorov, Erez and Tassa}]{todorov2012mujoco}
\text{Todorov, E.}, \text{Erez, T.} and \text{Tassa, Y.} (2012).
\newblock Mujoco: A physics engine for model-based control.
\newblock In \textit{International Conference on Intelligent Robots and
  Systems}.

\bibitem[{Tropp(2015)}]{10.1561/2200000048}
\text{Tropp, J.~A.} (2015).
\newblock An introduction to matrix concentration inequalities.
\newblock \textit{Foundations and Trends in Machine Learning}, \textbf{8}
  1â€“230.

\bibitem[{Uehara et~al.(2020)Uehara, Huang and Jiang}]{uehara2020minimax}
\text{Uehara, M.}, \text{Huang, J.} and \text{Jiang, N.} (2020).
\newblock Minimax weight and {Q}-function learning for off-policy evaluation.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Vershynin(2018)}]{vershynin2018high}
\text{Vershynin, R.} (2018).
\newblock \textit{High-dimensional probability: An introduction with
  applications in data science}, vol.~47.
\newblock Cambridge university press.

\bibitem[{Vinyals et~al.(2017)Vinyals, Ewalds, Bartunov, Georgiev, Vezhnevets,
  Yeo, Makhzani, K{\"u}ttler, Agapiou, Schrittwieser
  et~al.}]{vinyals2017starcraft}
\text{Vinyals, O.}, \text{Ewalds, T.}, \text{Bartunov, S.}, \text{Georgiev,
  P.}, \text{Vezhnevets, A.~S.}, \text{Yeo, M.}, \text{Makhzani, A.},
  \text{K{\"u}ttler, H.}, \text{Agapiou, J.}, \text{Schrittwieser, J.}
  \text{et~al.} (2017).
\newblock Star{C}raft {II}: A new challenge for reinforcement learning.
\newblock \textit{arXiv preprint arXiv:1708.04782}.

\bibitem[{Wang et~al.(2019)Wang, Cai, Yang and Wang}]{wang2019neural}
\text{Wang, L.}, \text{Cai, Q.}, \text{Yang, Z.} and \text{Wang, Z.} (2019).
\newblock Neural policy gradient methods: Global optimality and rates of
  convergence.
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Wang et~al.(2020{\natexlab{a}})Wang, Foster and
  Kakade}]{wang2020statistical}
\text{Wang, R.}, \text{Foster, D.~P.} and \text{Kakade, S.~M.}
  (2020{\natexlab{a}}).
\newblock What are the statistical limits of offline {RL} with linear function
  approximation?
\newblock \textit{arXiv preprint arXiv:2010.11895}.

\bibitem[{Wang et~al.(2020{\natexlab{b}})Wang, Salakhutdinov and
  Yang}]{wang2020reinforcement}
\text{Wang, R.}, \text{Salakhutdinov, R.~R.} and \text{Yang, L.}
  (2020{\natexlab{b}}).
\newblock Reinforcement learning with general value function approximation:
  Provably efficient approach via bounded eluder dimension.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Wang et~al.(2020{\natexlab{c}})Wang, Novikov, Zolna, Merel,
  Springenberg, Reed, Shahriari, Siegel, Gulcehre, Heess
  et~al.}]{wang2020critic}
\text{Wang, Z.}, \text{Novikov, A.}, \text{Zolna, K.}, \text{Merel, J.~S.},
  \text{Springenberg, J.~T.}, \text{Reed, S.~E.}, \text{Shahriari, B.},
  \text{Siegel, N.}, \text{Gulcehre, C.}, \text{Heess, N.} \text{et~al.}
  (2020{\natexlab{c}}).
\newblock Critic regularized regression.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Wu et~al.(2019)Wu, Tucker and Nachum}]{wu2019behavior}
\text{Wu, Y.}, \text{Tucker, G.} and \text{Nachum, O.} (2019).
\newblock Behavior regularized offline reinforcement learning.
\newblock \textit{arXiv preprint arXiv:1911.11361}.

\bibitem[{Xie and Jiang(2020{\natexlab{a}})}]{xie2020batch}
\text{Xie, T.} and \text{Jiang, N.} (2020{\natexlab{a}}).
\newblock Batch value-function approximation with only realizability.
\newblock \textit{arXiv preprint arXiv:2008.04990}.

\bibitem[{Xie and Jiang(2020{\natexlab{b}})}]{xie2020q}
\text{Xie, T.} and \text{Jiang, N.} (2020{\natexlab{b}}).
\newblock {$Q^{\star}$}-approximation schemes for batch reinforcement learning:
  A theoretical comparison.
\newblock \textit{arXiv preprint arXiv:2003.03924}.

\bibitem[{Xie et~al.(2019)Xie, Ma and Wang}]{xie2019towards}
\text{Xie, T.}, \text{Ma, Y.} and \text{Wang, Y.-X.} (2019).
\newblock Towards optimal off-policy evaluation for reinforcement learning with
  marginalized importance sampling.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Yang and Wang(2019)}]{yang2019sample}
\text{Yang, L.} and \text{Wang, M.} (2019).
\newblock Sample-optimal parametric {Q}-learning using linearly additive
  features.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Yang et~al.(2020{\natexlab{a}})Yang, Nachum, Dai, Li and
  Schuurmans}]{yang2020off}
\text{Yang, M.}, \text{Nachum, O.}, \text{Dai, B.}, \text{Li, L.} and
  \text{Schuurmans, D.} (2020{\natexlab{a}}).
\newblock Off-policy evaluation via the regularized {L}agrangian.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Yang et~al.(2020{\natexlab{b}})Yang, Jin, Wang, Wang and
  Jordan}]{yang2020bridging}
\text{Yang, Z.}, \text{Jin, C.}, \text{Wang, Z.}, \text{Wang, M.} and
  \text{Jordan, M.~I.} (2020{\natexlab{b}}).
\newblock Bridging exploration and general function approximation in
  reinforcement learning: Provably efficient kernel and neural value
  iterations.
\newblock \textit{arXiv preprint arXiv:2011.04622}.

\bibitem[{Yang et~al.(2020{\natexlab{c}})Yang, Jin, Wang, Wang and
  Jordan}]{yang2020function}
\text{Yang, Z.}, \text{Jin, C.}, \text{Wang, Z.}, \text{Wang, M.} and
  \text{Jordan, M.~I.} (2020{\natexlab{c}}).
\newblock On function approximation in reinforcement learning: Optimism in the
  face of large state spaces.
\newblock \textit{arXiv preprint arXiv:2011.04622}.

\bibitem[{Yin et~al.(2020)Yin, Bai and Wang}]{yin2020near}
\text{Yin, M.}, \text{Bai, Y.} and \text{Wang, Y.-X.} (2020).
\newblock Near optimal provable uniform convergence in off-policy evaluation
  for reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2007.03760}.

\bibitem[{Yin and Wang(2020)}]{yin2020asymptotic}
\text{Yin, M.} and \text{Wang, Y.-X.} (2020).
\newblock Asymptotically efficient off-policy evaluation for tabular
  reinforcement learning.
\newblock In \textit{International Conference on Artificial Intelligence and
  Statistics}.

\bibitem[{Yu(1997)}]{yu1997assouad}
\text{Yu, B.} (1997).
\newblock Assouad, {F}ano, and {L}e {C}am.
\newblock In \textit{Festschrift for Lucien Le Cam}. Springer, 423--435.

\bibitem[{Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn and
  Ma}]{yu2020mopo}
\text{Yu, T.}, \text{Thomas, G.}, \text{Yu, L.}, \text{Ermon, S.}, \text{Zou,
  J.}, \text{Levine, S.}, \text{Finn, C.} and \text{Ma, T.} (2020).
\newblock {MOPO}: Model-based offline policy optimization.
\newblock \textit{arXiv preprint arXiv:2005.13239}.

\bibitem[{Zanette et~al.(2021)Zanette, Cheng and
  Agarwal}]{zanette2021cautiously}
\text{Zanette, A.}, \text{Cheng, C.-A.} and \text{Agarwal, A.} (2021).
\newblock Cautiously optimistic policy optimization and exploration with linear
  function approximation.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Zhang et~al.(2020{\natexlab{a}})Zhang, Koppel, Bedi, Szepesv{\'a}ri
  and Wang}]{zhang2020variational}
\text{Zhang, J.}, \text{Koppel, A.}, \text{Bedi, A.~S.}, \text{Szepesv{\'a}ri,
  C.} and \text{Wang, M.} (2020{\natexlab{a}}).
\newblock Variational policy gradient method for reinforcement learning with
  general utilities.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Zhang et~al.(2020{\natexlab{b}})Zhang, Dai, Li and
  Schuurmans}]{zhang2019gendice}
\text{Zhang, R.}, \text{Dai, B.}, \text{Li, L.} and \text{Schuurmans, D.}
  (2020{\natexlab{b}}).
\newblock Gen{DICE}: Generalized offline estimation of stationary values.
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Zou(2006)}]{zou2006adaptive}
\text{Zou, H.} (2006).
\newblock The adaptive {L}asso and its oracle properties.
\newblock \textit{Journal of the American Statistical Association},
  \textbf{101} 1418--1429.

\end{thebibliography}
